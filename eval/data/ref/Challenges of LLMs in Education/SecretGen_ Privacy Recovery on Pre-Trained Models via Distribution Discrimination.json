{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2207.12263",
    "title": "SecretGen: Privacy Recovery on Pre-Trained Models via Distribution Discrimination",
    "abstract": "Transfer learning through the use of pre-trained models has become a growing trend for the machine learning community. Consequently, numerous pre-trained models are released online to facilitate further research. However, it raises extensive concerns on whether these pre-trained models would leak privacy-sensitive information of their training data. Thus, in this work, we aim to answer the following questions: \"Can we effectively recover private information from these pre-trained models? What are the sufficient conditions to retrieve such sensitive information?\" We first explore different statistical information which can discriminate the private training distribution from other distributions. Based on our observations, we propose a novel private data reconstruction framework, SecretGen, to effectively recover private information. Compared with previous methods which can recover private data with the ground true prediction of the targeted recovery instance, SecretGen does not require such prior knowledge, making it more practical. We conduct extensive experiments on different datasets under diverse scenarios to compare SecretGen with other baselines and provide a systematic benchmark to better understand the impact of different auxiliary information and optimization operations. We show that without prior knowledge about true class prediction, SecretGen is able to recover private data with similar performance compared with the ones that leverage such prior knowledge. If the prior knowledge is given, SecretGen will significantly outperform baseline methods. We also propose several quantitative metrics to further quantify the privacy vulnerability of pre-trained models, which will help the model selection for privacy-sensitive applications. Our code is available at: https://github.com/AI-secure/SecretGen.",
    "bib_name": "yuan2022secretgenprivacyrecoverypretrained",
    "md_text": "# SecretGen: Privacy Recovery on Pre-Trained Models via Distribution Discrimination\nZhuowen Yuan1, Fan Wu1, Yunhui Long1, Chaowei Xiao2, and Bo Li1\n1 University of Illinois Urbana-Champaign 2 Arizona State University\nAbstract. Transfer learning through the use of pre-trained models has become a growing trend for the machine learning community. Consequently, numerous pre-trained models are released online to facilitate further research. However, it raises extensive concerns on whether these pretrained models would leak privacy-sensitive information of their training data. Thus, in this work, we aim to answer the following questions: \u201cCan we effectively recover private information from these pre-trained models? What are the sufficient conditions to retrieve such sensitive information?\u201d We first explore different statistical information which can discriminate the private training distribution from other distributions. Based on our observations, we propose a novel private data reconstruction framework, SecretGen, to effectively recover private information. Compared with previous methods which can recover private data with the ground truth label of the targeted recovery instance, SecretGen does not require such prior knowledge, making it more practical. We conduct extensive experiments on different datasets under diverse scenarios to compare SecretGen with other baselines and provide a systematic benchmark to better understand the impact of different auxiliary information and optimization operations. We show that without prior knowledge about true class prediction, SecretGen is able to recover private data with similar performance compared with the ones that leverage such prior knowledge. If the prior knowledge is given, SecretGen will significantly outperform baseline methods. We also propose several quantitative metrics to further quantify the privacy vulnerability of pre-trained models, which will help the model selection for privacy-sensitive applications. Our code is available at: https://github.com/AI-secure/SecretGen.\nKeywords: Privacy; Pre-trained models; Transfer learning\n# 1 Introduction\nAs machine learning has achieved great successes in different domains, such as robotics [24], audio recognition [7], and face recognition [15], how to train the learning models efficiently given the available large-scale dataset becomes a timely problem. Transfer learning, which focuses on transferring knowledge across domains, is a promising learning paradigm [2]. In particular, many pretrained models are available currently, such as TensorFlow Hubs [1] and PyTorch Hubs [22], which can be flexibly used for fine-tuning later for different\ndownstream tasks. As a result, the training paradigm with transfer learning has enabled efficient usage of the large-scale dataset without requiring training every model from scratch. However, such an efficient transfer learning paradigm also leads to additional privacy concerns. For instance, if the training data of the pre-trained models contain privacy-sensitive information, an adversary who downloads the pre-trained models could potentially perform different privacy attacks to infer the private information. In particular, membership inference attacks [18,19] have been studied to infer whether a private instance is in the training set, and model inversion attacks have been studied to reconstruct the private training instances under certain assumptions [28,11,10,26], which raises more privacy and safety concerns. To better understand the privacy vulnerabilities of such pre-trained models, a comprehensive analysis of different types of privacy attacks, especially the severe model inversion attacks, is required. Currently, there are several limitations of existing privacy model inversion attacks. First, the current state-of-the-art model inversion attack (i.e., GMI) [28] requires the ground truth label of the reconstructed instances, which is less practical. Furthermore, it is a known challenging problem to label the generated instances based on GANs [12]. Second, many existing model inversion attacks require whitebox access to the target pre-trained model, making it less practical in real-world applications. Thus, in this paper, we mainly aim to ask: Can we reconstruct private sensitive training instances without requiring such information? To answer it, we propose a general private data recovery framework SecretGen, which consists of a generation backbone, a pseudo label predictor, and a latent vector selector. We first use a pseudo label predictor to generate a pseudo label for each private instance. Specifically, we randomly sample latent vectors and feed them into the generation backbone to get recovered instances. To stabilize prediction quality, we apply different transformations (e.g.cutouts) to such instances before feeding them into the targeted model to get the final predicted pseudo labels. We then propose a latent vector selector via a proposed selection algorithm to further optimize and constrain the recovery space. Finally, we perform joint optimization to train the end-to-end framework as shown in Fig. 1. We conduct comprehensive experiments to evaluate the proposed SecretGen compared with multiple baselines. We show that SecretGen significantly outperforms baselines given the same ground truth label. Even without such information, SecretGen still achieves comparable performance compared to baselines which leverage the ground truth label information. In addition, to evaluate the performance of recovered data on downstream tasks, we propose different evaluation protocols considering different usage of the recovered private data, and we show that our observations are consistent for different protocols. We also evaluate the robustness of SecretGen against the purification defense [27]. Finally, we perform different ablation studies to show the effectiveness of our design choices. We make the following contributions:\n# 2 Related Work\nRevealing privacy-sensitive information from a trained model has aroused extensive research interest. Membership inference attacks and model inversion attacks are two major categories of such attacks. In membership inference attacks [18,19], the adversary aims to decide whether a sample is a member of the training set, while in model inversion attacks [28,11,10,26], the adversary attempts to reconstruct the training set under certain assumptions. [11] was the first to propose model inversion attacks aiming at recovering private training data. The authors demonstrated that personal genetic markers could be effectively recovered given the output of the model and auxiliary knowledge. [10] extended model inversion to more complex models, including shallow neural networks for face recognition. The recovered data with their proposed method are identified as the original person at a much higher rate than random guessing. However, the reconstructed images are blurry and not visually recognizable to humans. [26] proposed a training-based attack by training an auto-encoder on public data. The attack can be performed with blackbox accesses to the target model and partial (truncated) model predictions. More recently, [28] proposed generative model inversion attack (GMI). The authors distill public knowledge by training a conditional GAN on public data and then solve an optimization problem to maximize the probability of the recovered image for the ground truth class label. GMI significantly outperforms previous methods in re-identification rate of the recovered data, as well as guaranteeing the recovered data are visually plausible. However, they still require the ground truth label for the target image and whitebox access to the victim model, which is often not accessible to the adversary. Another recent work distributional model inversion attack (DMI) [3] recovers the private data distribution for each target class by constructing representative samples. However, DMI does not support recovering every private instance given its non-sensitive version (i.e. instance-level model inversion), which is the adversary\u2019s goal in our setting.\n# 3 Methodology\n<div style=\"text-align: center;\">3 Methodology</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4889/48896cd6-2df7-4162-a5d2-767e07e3db41.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Overview of the proposed SecretGen. The blue modules represent the proposed algorithms. The Target Model could allow either whitebox or blackbox access.</div>\ng. 1. Overview of the proposed SecretGen. The blue modules represent the prosed algorithms. The Target Model could allow either whitebox or blackbox access.\n# 3.1 Problem Formulation\nWe focus on recovering the privacy-sensitive training data based on the trained classification models. Throughout the paper, we will refer to the model that is subjective to attacks as the target model F, which is trained on private training data Dtrain pri , aiming to perform evaluation on private test data Dtest pri . The target model returns a prediction vector F(x) given an input instance x. The prediction vector represents a probability distribution over C classes, where C denotes the number of classes of the whole private dataset Dpri = Dtrain pri \u222aDtest pri . The adversary\u2019s goal is to recover the private training data Dtrain pri given the trained target model F and certain prior information, e.g., partially corrupted images from Dtrain pri . In particular, such corrupted images only contain non-sensitive background information (pixels) xns with the sensitive region xs cropped out. These corrupted images are usually easy to obtain, given that such corruption is often applied to protect the privacy of individuals in practice [28]. Specifically, in our evaluation, we consider cropping the whole face using two face datasets, leaving only the non-sensitive background regions (Section 4). Regarding the adversary\u2019s ability, we consider (1) whitebox access to the target model, where all parameters and intermediate computations of the target model are visible to the adversary, and (2) blackbox access to the target model, where the adversary can only obtain the final prediction from the target model F. Additionally, we assume that the adversary also has access to some public data Dpub from the similar distribution for general training purpose.\n# 3.2 Method Overview\nAn overview of SecretGen is illustrated in Fig. 1, where SecretGen takes nonsensitive information xns as input and returns the recovered images that contain privacy-sensitive training information (e.g., human faces). SecretGen is composed of three components: generation backbone, pseudo label predictor, and latent vector selector, which are jointly optimized under a unified framework. The generation backbone leverages a conditional GAN trained on public data as a backbone to generate realistic images based on the prior information (e.g., cropped images), and the generation process is controlled by the latent vector z sent to the GAN\u2019s generator G. The pseudo label predictor predicts the most possible pseudo label for each recovered private image based on the distributional statistics of recovered images. The latent vector selector selects the optimal latent vector \u02c6z which is the most likely to contain privacy-sensitive information based on the proposed selection algorithm. Finally, we perform joint optimization on the selected \u02c6z, taking the pseudo label provided by the pseudo label predictor as the prediction target, to reconstruct image G(\u02c6z\u2217, xns). In the next following sections, we will describe each component in detail.\n# 3.3 Generation Backbone of SecretGen\nTo recover the privacy-sensitive training data, we train a generation backbone for conditional image recovery on public data Dpub. In particular, we will start from certain prior knowledge, such as the corrupted private data containing only the nonsensitive information xns. We then perform the same corruption operation corr on Dpub to construct the training set for the generation backbone: Dpub corr = {corr(x)|x \u2208Dpub}. Next, we train a conditional GAN which is composed of two networks: generator G and discriminator D. G is conditioned on xns \u2208Dpub corr and z is the latent vector which is sampled from a prior distribution during training. Throughout the paper, we use the prior distribution as standard Gaussian distribution. We leverage the Wasserstein-GAN loss [13] for GAN training:\nWe also incorporate a diversity loss term Ldiv [25] for training the generator to prevent mode collapsing by sampling different latent vectors, say, z1 and z2:\nwhere f is the feature extractor of the target model, which returns the feature embeddings of the input images in the whitebox setting. In the blackbox setting, we use a feature extractor trained on public data fpub for this process. The overall loss term for the generator is as following:\n(1)\n(2)\n(3)\nAfter the generation backbone is trained, we freeze the parameters for both G and D before we enter the next stage. We denote \u02c6x as the recovered image, i.e., \u02c6x = G(z, xns).\n# 3.4 Pseudo Label Predictor\nThe main challenge in this data reconstruction process is that we have no knowledge about the ground truth label of the private images (related work assumes that they have access to the ground truth label [28,26,11,10], while we do not). To tackle this problem, we propose a pseudo label predictor which infers the label prediction with proposed discrimination metrics. We will first introduce the design of our discrimination metric, and then we elaborate on how the pseudo label predictor is optimized. Discrimination Metric. Given the certain prior knowledge xns, we randomly sample n latent vectors {zi}n i=1 from the prior distribution. We generate n recovered images using our generation backbone: {\u02c6xi}n i=1, where \u02c6xi = G(zi, xns). In order to improve the prediction stability, we consider prediction under different transformations. Concretely, let the list of considered transformation functions be T = {ti}m i=1. On each recovered image \u02c6xi, we perform m transformations independently to obtain m transformed images {\u02dcxj i}m j=1, where \u02dcxj i = tj(\u02c6xi). We additionally define \u02dcx0 i = \u02c6xi. Let Fc(\u00b7) denote the model\u2019s prediction confidence for class label c based on target model F. We define the discrimination metric M on label c as follows:\nThe discrimination metric returns a score indicating how likely it is for a label c to be the consistent prediction across different transformations. Based on existing studies of contrastive learning [4], we will select the class c with the highest discrimination metric score as the final label prediction. In particular, we define the list of transformations as a sequence of fix-sized cutouts. We split an image into fix-sized patches and define tj as the transformation that cuts out the j-th patch of the given image, as illustrated in Fig. 2. Intuitively, the discrimination metric M(c; n, m) should preserve the following properties. First, M(c; n, m) is likely to have a higher score when c equals the label associated with the corrupted image xns since the model has learned some correlation between the non-sensitive background information in xns and the label of the original image. Such correlation should be stronger if the target model is more overfitted to private training data. Second, when the recovered image \u02dcxj i is close to the training data, Fc(\u02dcxj i) should be consistently higher on the correct label because training data are often more resistant to transformations than non-training data [6]. Based on these intuitions, we use the discrimination metric as the foundation of the pseudo label predictor in SecretGen.\n(4)\nPseudo Label Predictor. Given the discrimination metric M, we next describe in detail how we leverage M to infer the pseudo prediction label considering different sampled latent vectors, which aims to approximate the ground truth. We first sample a set of n latent vectors randomly and compute M for all class labels. The pseudo label predictor chooses the label with the maximum discrimination metric score as the predicted label \u02c6c:\n(5)\nWe defer the detailed algorithm for label prediction with M to the appendix. Note that there are various design choices for the discrimination metric M, e.g., the average confidence on only the recovered images without including their transformed versions. It is clear that more advanced M will provide more accurate pseudo label predictors. We will analyze the performance of the pseudo label predictor given different designs of M in Section 4.5.\n# 3.5 Latent Vector Selector\nIn addition to the availability of ground truth labels, another challenge during private data recovery is that we may not have whitebox access to the target model. In systems where machine learning is used as a service (MLaaS), the adversary can only query the model and the prediction vector is returned from the service provider. All internal computations and model parameters are unknown to the adversary. In previous work [28], the adversary can directly optimize the latent vector z to maximize the target model\u2019s confidence given a known ground truth label, which is less practical. Without the whitebox access, performing back-propagation with the target model is infeasible in our practical case. To tackle this problem, we design a latent vector selector to first randomly sample n random latent vectors, and then select the ones which lead to their recovered data classified as the predicted label from the pseudo label predictor. If there is no latent vector that leads to the recovered images which can be classified as the predicted label consistently, the selector returns a randomly sampled latent vector from the prior distribution. Otherwise, it returns the latent vector which has the highest confidence of the predicted label. We omit the detailed algorithm to the appendix.\n# 3.6 SecretGen Optimization\nTo put every proposed component within SecretGen together, we perform joint optimization to maximize the consistent label prediction likelihood of recovered images indicated by the discrimination metric (i.e., identity loss), while keeping the recovered images realistic (i.e., discriminator loss). In the whitebox setting, we perform backpropagation on the target model with identity loss Lid. Lid\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05b1/05b19216-d0f3-443a-adb6-9ad595f48cfe.png\" style=\"width: 50%;\"></div>\nFig. 2. Sequential cutout for the recovered image as transformations. The image is first split into m fix-sized patches. Operations of cutting out each patch are viewed as transformations respectively.\nencourages the generated images to achieve consistently high label predict likelihood given the target model for class label c.\nWe utilize discriminator loss as regularization to penalize unrealistic images.\n  h Then we initialize z with \u02c6z returned by our latent vector selector and optimize z with the following objective function:\nIn the blackbox setting, we perform the latent vector selection optimization only with the discriminator loss since the target model is not locally accessible:\nNote that in the blackbox setting where we have the ground truth labels, the identity loss is still minimized by the latent vector selector through random sampling based on the prediction vector of the target model, guaranteeing that the recovered image is close to the density region of the ground truth identity.\n# 3.7 Discussion\nOur proposed SecretGen works under a wide range of scenarios regarding different types of prior knowledge. See Table 1 for the scenarios under which SecretGen and existing methods can be applied. Although EMI theoretically works in blackbox cases with ground truth labels, its performance and efficiency dramatically suffer against deep models. Although SecretGen still requires non-sensitive private data as prior knowledge, such assumption is realistic as image corruption is often leveraged for privacy protection by individuals [28]. Furthermore, if the knowledge of ground truth labels is available, it can be incorporated into SecretGen conveniently. More details are deferred to the appendix. In conclusion, SecretGen is more practical without requiring whitebox access to the target model or the ground truth label. In addition, SecretGen is very efficient and applicable to high-dimensional image data considering deep models as the target model as shown in our evaluation (Section 4).\nTable 1. Comparison with existing methods on the information required by the adversary to recover private training data. The symbol \u2713\u2717means that in theory, the method can work without the information, but the actual performance on deep models is bad.\nMethods Non-sensitive Data? Whitebox Access? Ground Truth Label?\nPII [25]\n\u2713\n\u2717\n\u2717\nEMI [10]\n\u2717\n\u2713\u2717\n\u2713\nGMI [28]\n\u2713\u2717\n\u2713\n\u2713\nSecretGen\n\u2713\n\u2717\n\u2717\n(6)\n(8)\n(9)\n# 4 Experiments\nIn this section, we first present the experimental setup. Then, we introduce the evaluation protocols and report the attack performance respectively. We also evaluate the robustness of SecretGen against the purification defense [27]. In the end, we describe some ablation studies to better understand our method.\n# 4.1 Experimental Setup\nDatasets. We evaluate SecretGen on two face datasets: (1) CelebA [20] which contains 202,599 face images of 10,177 identities. We filter out those identities with 25 or fewer images and randomly select 25,000 images of 1,000 identities as private data Dpri. We also randomly select 50,000 images of 2,000 identities from the rest as adversary\u2019s public data Dpub. There exist no overlapped identities between Dpub and Dpri. (2) FaceScrub [21] which consists of 106,863 face images of 530 identities. We use the images of 250 identities as Dpri and images of another 250 identities as Dpub. We further split Dpri into Dtrain pri and Dtest pri for training and testing. All the images are cropped and resized to 64 \u00d7 64. Prior Information. We consider two types of prior information that the adversary has access to: corrupted images by center mask and face T mask following the standard setting in [28]. Center mask blocks the center part of the private image, but the mouth information may still be exposed. Face T mask completely hides the identity revealing features of the face image. Model Architectures. We perform evaluation on target models with various architectures: (1) VGG16 [23]; (2) ResNet152 [15]; (3) face.evoLVe [5] with an IR50 backbone; (4) ViT-B 16 [9] We utilize IR152 [5] as the evaluation model to predict the identity of input images. Both VGG16 and ResNet152 are pretrained on ImageNet [8]. face.evoLVe and the evaluation model are pre-trained on MS-Celeb-1M [14]. ViT-B 16 is pre-trained on ImageNet21k [8]. The architecture of SecretGen generation backbone is adopted from [28]. Baselines. We compare SecretGen with the state-of-the-art model inversion attack GMI [28]. GMI assumes the adversary has access to the ground truth labels and performs optimization with identity loss and discriminator loss. We also compare our results with pure image inpainting (PII) [25], which only optimizes the discriminator loss for generating realistic images. Latent vectors of both GMI and PII are sampled randomly from Gaussian distribution. We do not compare with EMI [10], since it has been demonstrated in [28] that the effectiveness of EMI is quite limited against deep models. We defer additional details regarding model training and attack to the appendix.\n# 4.2 Evaluation Protocols\nWe consider two principles for evaluating the privacy attack performance: \u201chow much privacy sensitive identity information can be recovered\u201d and \u201chow well the recovered data can perform in downstream tasks\u201d.\nCorresponding to the two principles, we evaluate the privacy attack performance by attack accuracy under the following two protocols: \u2013 Protocol 1: Train the evaluation model on the private data, and evaluate on the recovered data. \u2013 Protocol 2: Train the evaluation model on the recovered data, and evaluate on the private data.\nProtocol 1 was introduced in [28], which evaluates instance-level privacy recovery. However, we demonstrate that even if some instances are not recovered correctly, the recovered data can be used for downstream tasks, e.g., training another classification model. The adversary can potentially use the trained evaluation model for malicious purposes, e.g., performing unauthorized face recognition on private identities with significantly higher accuracy than the target model itself. Thus, we propose Protocol 2, which aims to evaluate distribution-level privacy recovery. In addition, a common goal of the adversary to reconstruct the private data is to leverage such data for other downstream tasks, and therefore Protocol 2 explicitly reflects the utility of the recovered data. For Protocol 1, we train the evaluation model on Dtrain pri and the resulting evaluation model achieves 98.0% classification accuracy over the private identities on Dtest pri . For Protocol 2, we first perform the attack on all corrupted private images Dpri\u2014for each corrupted image xns \u2208Dpri, we recover an image \u02c6x = G(\u02c6z\u2217, xns) via SecretGen, with label \u02c6c = arg maxc\u2208[1,C] Fc(\u02c6x). We then compose the recovered images into a recovered private set Drec, which is separated into Dtrain rec and Dvalid rec by 4:1. We train the evaluation model on Dtrain rec with Dvalid rec as the validation set. We then evaluate the model performance on Dtest pri . We also report Peak Signal-to-Noise Ratio (PSNR) [16] between original and recovered private data, which reflects the pixel-level reconstruction quality of our attack. Note that the recovered data can still reveal identity information even if the generated image is not close to the ground truth image pixel-wise. For example, the recovered images can exhibit variations in pose and light condition while keeping the identity.\n# 4.3 Attack Performance\nWhitebox Attacks. Table 2 compares the performance of SecretGen with baseline methods on CelebA. See the appendix for results on FaceScrub. We can see that SecretGen significantly outperforms GMI under both Protocol 1 and Protocol 2 if the ground truth label is given. Without such information, with the proposed pipeline especially the pseudo label predictor, SecretGen still achieves comparable performance with GMI under Protocol 1. Under Protocol 2, GMI with ground truth label performs better than SecretGen without ground truth label. The reason is that if the predicted pseudo label is incorrect, our pseudo label predictor and optimization push the recovery to be closer to the wrong identity. However, we still outperform PII by a large margin. We also observe that attack accuracy under Protocol 2 is much higher than that under Protocol 1. The reason is that Protocol 1 and 2 work at different\n<div style=\"text-align: center;\">Table 2. Whitebox attack performance on CelebA. See the Ground Truth Label column for whether ground truth label is provided for each attack method.</div>\nTarget Model Methods Ground Truth\nLabel\nCenter Mask\nFace T Mask\nProtocol 1 Protocol 2 PSNR Protocol 1 Protocol 2 PSNR\nVGG16\nPII\n\u2717\n0.423\n0.561\n27.583\n0.166\n0.363\n26.276\nGMI\n\u2713\n0.569\n0.955\n27.587\n0.305\n0.928\n26.240\nSecretGen\n\u2717\n0.584\n0.928\n27.955\n0.312\n0.793\n26.632\nSecretGen\n\u2713\n0.639\n0.965\n28.071\n0.377\n0.931\n26.821\nResNet152\nPII\n\u2717\n0.403\n0.719\n26.892\n0.170\n0.555\n26.117\nGMI\n\u2713\n0.556\n0.965\n27.177\n0.295\n0.946\n26.482\nSecretGen\n\u2717\n0.595\n0.948\n27.506\n0.324\n0.884\n26.821\nSecretGen\n\u2713\n0.618\n0.971\n27.587\n0.349\n0.945\n26.967\nface.evoLVe\nPII\n\u2717\n0.267\n0.455\n27.317\n0.122\n0.343\n26.356\nGMI\n\u2713\n0.595\n0.946\n27.444\n0.467\n0.935\n26.563\nSecretGen\n\u2717\n0.551\n0.841\n27.613\n0.274\n0.630\n26.562\nSecretGen\n\u2713\n0.788\n0.963\n27.781\n0.695\n0.954\n26.827\nViT\nPII\n\u2717\n0.380\n0.389\n26.698\n0.173\n0.306\n26.377\nGMI\n\u2713\n0.482\n0.893\n24.907\n0.214\n0.715\n24.624\nSecretGen\n\u2717\n0.451\n0.634\n26.811\n0.246\n0.528\n26.471\nSecretGen\n\u2713\n0.551\n0.950\n26.607\n0.326\n0.913\n26.609\nlevels: Protocol 1 evaluates how much \u201cdetailed\u201d information the recovered images contain, while Protocol 2 evaluates how much distributional information we can recover by training another model based on the reconstructed data. Clearly, Protocol 2 is relatively easier by recovering distributional level information and thus achieves higher scores. We believe such observations will inspire interesting future work and narrow down such a gap. Blackbox Attacks. In the blackbox setting, the adversary is not capable of performing backpropagation with the target model. We make the following changes to our attack pipeline: (1) In Section 3.3, when training the generation backbone, we use a public feature extractor from [5] pre-trained on MS-Celeb-1M to substitute the target model for extracting the feature embeddings in computing the diversity loss (Ldiv, Eqn. (2)); (2) In Section 3.6, when performing SecretGen optimization, we remove the identity loss Lid and optimize the selected latent vector only with discriminator loss Ldisc. Table 3 compares our results with PII under the blackbox setting on CelebA. The only difference for PII under blackbox and whitebox scenarios is whether the target model is accessed when training the generation backbone. We can see that with the ground truth labels, SecretGen significantly outperforms PII. Without ground truth labels, which is the most general case, we still outperform PII by a large margin. As far as we are concerned, we are the first to propose an effective model inversion attack against deep classification models under the blackbox case without ground truth label. We note that GMI (with ground truth label) performs better on face.evoLVe than SecretGen (without ground truth label), as shown in Table 2 and Table 3. Under this setting, attack performance is largely dependent on the pseudo label predictor. We demonstrate that the label prediction accuracy of face.evoLVe is significantly lower than that of VGG16 and ResNet152 in the appendix. We believe the reason is that face.evoLVe is less overfitted due to the difference in\nTable 3. Blackbox attack performance on CelebA. We report results for both cases where the adversary has or does not have ground truth labels. (Note: GMI does not support blackbox attack, and PII in the blackbox setting does not use the target model.)\nMethods Target Model Ground Truth\nLabel\nCenter Mask\nFace T Mask\nProtocol 1 Protocol 2 PSNR Protocol 1 Protocol 2 PSNR\nPII\nAny\n\u2717\n0.216\n0.759\n27.319\n0.081\n0.484\n25.705\nSecretGen\nVGG16\n\u2717\n0.351\n0.915\n27.638\n0.164\n0.837\n26.045\n\u2713\n0.380\n0.955\n27.737\n0.377\n0.927\n26.821\nResNet152\n\u2717\n0.334\n0.933\n27.737\n0.152\n0.765\n26.144\n\u2713\n0.347\n0.959\n27.840\n0.172\n0.886\n26.284\nface.evoLVe\n\u2717\n0.447\n0.711\n27.568\n0.156\n0.353\n25.787\n\u2713\n0.603\n0.894\n27.694\n0.305\n0.586\n26.002\nViT\n\u2717\n0.285\n0.709\n27.480\n0.119\n0.685\n25.828\n\u2713\n0.335\n0.924\n27.665\n0.160\n0.902\n26.123\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14fd/14fd24d1-c1bd-48ea-ad29-d5761d43f77d.png\" style=\"width: 50%;\"></div>\nFig. 3. Qualitative results of SecretGen on CelebA. \u201cbb\u201d/\u201cwb\u201d indicates the method requires blackbox/whitebox access to the model. \u201cgt\u201d indicates the method requires ground truth labels. pre-training datasets. (face.evoLVe is pre-trained on MS-Celeb-1M while others are on ImageNet.) Qualitative Results. In Fig. 3 we exhibit the images recovered with SecretGen on CelebA to demonstrate that our recovered images are both identityrevealing and visually plausible. We also show qualitative results of PII and GMI for comparison. From the figure, we see that although all of the three methods generate realistic images, PII cannot effectively recover the original identity of private data, while SecretGen is more effective in identity revealing. More examples are shown in the appendix.\n# 4.4 Robustness Evaluation\nWe evaluate the robustness of our proposed method against purification defense [27], which has been shown to effectively defend against model inversion attacks while inducing negligible utility loss. We use Purifier I in [27] which is specialized for model inversion attacks. We follow the default architectures and settings for training the purifier. See Table 4 for quantitative results on CelebA against VGG16 under the blackbox setting. We also assume the ground truth label is not provided. We do not evaluate the whitebox setting because the ad-\n<div style=\"text-align: center;\">Table 4. Robustness evaluation for SecretGen against prediction purification on CelebA. Target model: VGG16. Blackbox setting.</div>\nMethods\nCenter Mask\nFace T Mask\nProtocol 1 Protocol 2 PSNR Protocol 1 Protocol 2 PSNR\nPII\n0.216\n0.759\n27.319\n0.081\n0.484\n25.705\nSecretGen\n0.351\n0.915\n27.638\n0.164\n0.837\n26.045\nSecretGen (purified)\n0.328\n0.913\n27.590\n0.151\n0.747\n26.007\nversary can simply remove the purifier and directly attack the original private model. It can be seen that attack accuracy slightly decreases after the defense, but still outperforms the baseline by a large margin. Therefore, our method is robust against [27].\n# 4.5 Ablation Studies\nDiscrimination Metrics. As discussed in Section 3.4, there may exist various choices for the discrimination metric. One intuitive choice may be derived by removing the transformations from our current discrimination metric M (Eqn. (4)), and the simplified discrimination metric is defined as follows:\nWe perform an end-to-end ablation study on face.evoLVe and CelebA. We remove the transformations in our pseudo label predictor and substitute M with M\u2032. Quantitative results on face.evoLVe are shown in Table 5. See the appendix for results regarding other model architectures. We conclude that incorporating transformations improves the performance of our framework for most model architectures that we used for evaluation.\nTable 5. Attack accuracy of SecretGen with and without transformations on CelebA Evaluated on 3,200 private instances under Protocol 1. Target model: face.evoLVe.\nMetric\nCenter Mask\nFace T Mask\nAttack Acc PSNR Attack Acc PSNR\nw/o transformation\n0.528\n27.505\n0.256\n26.527\nw/ transformation\n0.550\n27.522\n0.273\n26.527\nTo further understand why and how transformations help, we compare the performance of pseudo label predictor equipped with M and M\u2032. We evaluate the performance of pseudo label predictor using label prediction accuracy, which measures the percentage of the predicted labels matching the ground truth labels. We plot out the label prediction accuracy with M and M\u2032 on 3,200 recovered images for face.evoLVe with varying n in Fig. 4. We observe that our pseudo label predictor can predict the pseudo labels more accurately if transformations are incorporated. See the appendix for results of other model architectures.\n(10)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1a9/e1a94ff5-be7d-47cd-b631-27d2d81e0f3e.png\" style=\"width: 50%;\"></div>\nFig. 4. Label prediction accuracy with and without transformations on CelebA. We plot the label prediction accuracy w.r.t. the number of random latent vectors n. Target model: face.evoLVe.\n<div style=\"text-align: center;\">Fig. 4. Label prediction accuracy with and without transformations on CelebA. We plot the label prediction accuracy w.r.t. the number of random latent vectors n. Target model: face.evoLVe.</div>\nData Transformations. Next, we discuss the performance of various data transformations on CelebA. We plot out label prediction accuracy w.r.t. n for various transformations including the proposed sequential cutout, horizontal flipping, gray-scale, and color jittering in Fig. 5. We also plot the results without transformations. We can see that sequential cutout performs better than other transformations in terms of label prediction accuracy. Although it is also possible to adopt other transformations within our pipeline, it is non-trivial to select the best hyper-parameters for other transformations (e.g., cropping and color jittering). We leave the analysis of how different transformations impact attack performance as future work. Overfitting Levels. We also evaluate the impact of higher overfitting levels of the target model on the performance of SecretGen, since the overfitting phenomenon is key to model inversion attacks. Note that results reported in Table 2 and Table 3 are based on standard well-trained models. We demonstrate that highly overfitted models are more vulnerable to our proposed attack. We describe the relevant experiment setup and quantitative results in the appendix.\n# 5 Conclusion\nIn this paper, we propose an effective private data recovery framework SecretGen, which can effectively recover private information under a wide range of scenarios. To our full knowledge, we are the first to propose an effective model inversion attack without prior knowledge of ground truth labels, which can achieve comparable results with previous methods that require ground truth labels. If we are given such prior knowledge, we significantly outperform previous methods. Our attack can also be applied under the blackbox setting where the target model is provided as a service and not locally available. We perform a comprehensive analysis of the performance of SecretGen and our design choices. We also demonstrate that our attack is robust against the purification defense. We hope to raise people\u2019s concerns about possible negative effects of releasing pre-trained models online. For future work, we are interested in whether we can perform privacy recovery simply with the target model and develop defenses against our attack. Acknowledgements. This work is partially supported by NSF grant No.1910100, NSF CNS No.2046726, C3 AI, and the Alfred P. Sloan Foundation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c3c5/c3c564f2-8dcf-45f4-8dda-3de2dd2fadb6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. Label prediction accuracy with different transformations on CelebA. We plot the label prediction accuracy w.r.t. the number of randomly sampled latent vectors n. Target model: face.evoLVe.</div>\nFig. 5. Label prediction accuracy with different transformations on CelebA. We plot the label prediction accuracy w.r.t. the number of randomly sampled latent vectors n. Target model: face.evoLVe.\n# References\n# A Additional Algorithm Details\nIn Section 3.4 and Section 3.5, we introduced pseudo label predictor and latent vector selector of SecretGen. In this section, we concretely demonstrate the detailed algorithms for these two components.\n# A.1 Detailed Algorithm for Pseudo Label Predictor\nAlgorithm 1 elaborates how SecretGen\u2019s pseudo label predictor infers a pseudo label for the target private image. The pseudo label predictor randomly samples n latent vectors from the prior distribution of the generation backbone iteratively. For each latent vector, we perform m transformations to the recovered image. We compute the average confidence for each class label c \u2208[1, C] on the recovered image and their transformed versions. Then we select the class label \u02c6c with the highest average confidence as the predicted pseudo label.\nAlgorithm 1 Label Prediction\nInput: xns: corrupted private data, T\n= {ti}m\ni=1: m transformations, F: target\nmodel, n: number of randomly sampled latent vectors, C: number of classes\nv = 0\n\\triangleright Initialize v to zero vector of length C\nfor i \u21901 to n do\nzi \u223cN(0, 1)\n\\triangleright Randomly sample one latent vector from Gaussian\n\u02dcx0\ni \u2190G(zi, xns)\n\\triangleright Generate the recovered image with the generation backbone\n\u02dcxj\ni \u2190tj(\u02dcx0\ni ) for j = 1 . . . m\n\\triangleright Generate m transformed images\nv \u2190v + \ufffdm\nj=0 F(\u02dcxj\ni) \\triangleright Sum over the prediction vectors of the m transformations\nend for\nv \u2190\n1\nn(m+1)v\n\u02c6c \u2190arg maxc\u2208[1,c] vc\n\\triangleright Select the class c that has the highest value under measure M (Eq. 4)\nreturn \u02c6c\n# A.2 Detailed Algorithm for Latent Vector Selector\nAlgorithm 2 describes how we select the latent vector given the target label. We refer to the target label as the predicted pseudo label or ground truth label (if available). We first randomly sample n latent vectors and filter out the latent vectors that can generate recovered images that will be predicted as the target label by the target model. If there exist such latent vectors, we choose the latent vector which leads to the recovery with the highest confidence of the target label; otherwise, the algorithm returns a random latent vector sampled from Gaussian.\nAlgorithm 2 Latent Vector Selection\nInput: xns: corrupted private data, F: target model, n: number of randomly sampled\nlatent vectors, y: predicted label or ground truth label\nzbank \u2190sample n latent vectors from N(0, 1)\nzbank y \u2190{z \u2208zbank| arg maxc\u2208[1,C] Fc(G(z, xns)) = y}\n\\triangleright Select latent vectors that can generate recovered images that will be predicted to\nhave label y\nif zbank y is not empty then\n\u02c6z \u2190arg maxz\u2208zbank y Fy(G(z, xns))\n\\triangleright Select the latent vector that has the\nhighest confidence for label y\nelse\n\u02c6z \u223cN(0, 1)\nend if\nreturn \u02c6z\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5075/50758af9-b45f-4f8d-8a55-97c444d87110.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Back propagation</div>\nFig. 6. Overview of the proposed SecretGen if ground truth label is available. The blue modules represent the proposed algorithms. The Target Model could allow either whitebox or blackbox access. The ground truth label is fed to the latent vector selector, which returns the initial latent vector for SecretGen optimization. In the whitebox setting, the ground truth label is also leveraged for identity loss.\n# A.3 SecretGen with Known Ground Truth Labels\nAs we have presented in Section 3, SecretGen predicts a pseudo label for the target private image through a discrimination metric without requiring the knowledge of the ground truth label. Here, we consider the setting when the ground truth label is available, and we point out that the knowledge can be incorporated into our pipeline conveniently. See Fig. 6 for an overview of SecretGen if ground truth labels for target private images are known. Under this scenario, the pseudo label predictor is not required, and we substitute the input of the latent vector selector with the known ground truth label. Besides, the ground truth label is leveraged for computing identity loss during SecretGen optimization in the whitebox setting.\n# B Additional Experimental Setup\nTarget Model Training. The target models are trained on Dtrain pri . For training ViT-B 16, we use the SGD optimizer with learning rate 3 \u00d7 10\u22122, batch size 64 for 10,000 steps. We use cosine annealing schedule with 500 warm-up steps. The other models are trained with the SGD optimizer with learning rate 10\u22122, batch size 64, momentum 0.9 and weight decay 10\u22124. Attack Procedures. We leverage WGAN-GP [13] and train our generation backbone with the Adam optimizer [17] with batch size 64, learning rate 4\u00d710\u22123, \u03b21 = 0.5, \u03b22 = 0.999. We set \u03bbdiv in Eqn. (3) to 0.5 based on quantitative observations. For both our pseudo label predictor and latent vector selector, we set the number of randomly sampled latent vectors to 200. Although we demonstrate that our pipeline works better under larger n, the efficiency suffers. See Section 4.5 for concrete results. We set the patch size for cutouts to 16 \u00d7 16 and the resulting number of transformations m is 16. See Appendix C.2 for more discussion regarding the selection of m. At the SecretGen optimization stage, we set \u03bbid in Eqn. (8) to 100 and use the SGD optimizer to optimize the selected latent vector for 1,500 iterations with learning rate 0.02 and momentum 0.9.\n# C Additional Results\nIn this section, we report additional results for ablation studies. We also show some additional qualitative results of SecretGen.\n# C.1 Attack Performance on FaceScrub\nTable 6 compares attack performance of SecretGen and baselines on FaceScrub. We use VGG16 as the target model. It can be seen that SecretGen still significantly outperforms PII and GMI. The conclusion aligns with that on CelebA.\n<div style=\"text-align: center;\">Table 6. Whitebox attack performance of SecretGen and baselines on FaceScrub. Target model: VGG16. The given prior information is corrupted images by center mask.</div>\nMethod\nGround Truth Label\nProtocol 1\nProtocol 2\nPSNR\nPII\n\u2717\n0.274\n0.818\n26.269\nGMI\n\u2713\n0.398\n0.972\n26.028\nSecretGen\n\u2717\n0.396\n0.942\n26.459\nSecretGen\n\u2713\n0.453\n0.973\n26.516\n# C.2 Ablation Studies on the Number of Transformations\nWe demonstrate that the number of transformations m in the discrimination metric (Eqn.(4)) affects the performance of our pseudo label predictor. We adjust\nm by changing the patch size of our sequential cutout. Since our recovered image is 64 \u00d7 64, the resulting patch size is thus 64 \u221am \u00d7 64 \u221am. We fix n to 200 and plot label prediction accuracy on the recovered images w.r.t. m in Fig. 7. We observe that when m is small, the resulting label prediction accuracy suffers due to the aggressiveness of cutting out a large patch. However, efficiency suffers as m increases. See Table 7 for the average running time of pseudo label predictor for one target private image on a single NVIDIA RTX 2080 Ti GPU. Note that the relationship between m and label prediction accuracy is not consistent along different model architectures. Since the attacker has no idea about the optimal m for the target model, in practice we set m to 16 and we demonstrate that performance of SecretGen is promising in Section 4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/56ac/56aca2cc-afe2-4d5a-9a77-9fd75f654bae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7. Label prediction accuracy of the pseudo label predictor w.r.t. different number of transformations on CelebA. For each private image, we sample n = 200 latent vectors, and then perform m transformations corresponding to each latent vector.</div>\nTable 7. Average running time of pseudo label predictor for one target private image. For each private image, we sample n = 200 latent vectors and then perform m transformations corresponding to each latent vector. Target model: VGG16.\nm\n4\n9\n16\n25\n36\nRunning Time (s)\n18.583\n26.747\n38.222\n53.463\n72.876\n# C.3 Ablation Studies on Transformations\nIn this section, we present a series of quantitative results of the ablation studies on transformations. Performance without Transformations. In Section 4.5, we demonstrated that SecretGen performs better if transformations are incorporated in the discrimination metric for face.evoLVe. Here we present the results for other model architectures. We remove transformations from the original discrimination metric M (Eqn. (4)) and denote the resulting discrimination metric without transformations as M\u2032 (Eqn. (10)). We evaluate end-to-end attack accuracy for M and M\u2032 on the first 3,200 images from Dtrain pri under Protocol 1. The results are illustrated in Table 8, which demonstrate that incorporating transformations\nimproves attack accuracy for VGG16. For ResNet152, the results are similar. We also report PSNR scores for the recovered images for reference. Performance with Various Transformations. We further evaluate the performance of our pseudo label predictor incorporated with different kinds of transformations. We consider using more aggressive transformations including horizontal flipping, grayscale, and color jittering (brightness 1.5, contrast 1.5, saturation 1.5). We plot label prediction accuracy for the first 3,200 private images from Dtrain pri with different transformations w.r.t. the number of random latent vectors in Fig. 8. We also plot the label prediction accuracy without transformations for reference. We demonstrate that the performance of aggressive transformations is worse than the case when no transformations are incorporated, while sequential cutout which is used in our pipeline performs the best.\n<div style=\"text-align: center;\">Table 8. Attack accuracy of SecretGen with and without transformations on CelebA. Evaluated with the evaluation model under Protocol 1.</div>\nTarget Model\nSetting\nCenter Mask\nFace T Mask\nAttack Acc PSNR Attack Acc PSNR\nVGG16\nw/o transformation\n0.584\n27.863\n0.319\n26.596\nw/ transformation\n0.599\n27.874\n0.328\n26.598\nResNet152\nw/o transformation\n0.594\n27.447\n0.337\n26.792\nw/ transformation\n0.592\n27.442\n0.338\n26.788\nface.evoLVe\nw/o transformation\n0.528\n27.505\n0.256\n26.527\nw/ transformation\n0.550\n27.522\n0.273\n26.527\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dbc7/dbc7aec2-918b-4605-9485-6847febd9a22.png\" style=\"width: 50%;\"></div>\nFig. 8. Label prediction accuracy without transformation (denoted as \u201cnone\u201d in le end) and with different kinds of transformations on CelebA. We plot label predictio accuracy w.r.t. the number of randomly sampled latent vectors.\n<div style=\"text-align: center;\">Fig. 8. Label prediction accuracy without transformation (denoted as \u201cnone\u201d in legend) and with different kinds of transformations on CelebA. We plot label prediction accuracy w.r.t. the number of randomly sampled latent vectors.</div>\n# C.4 Impact of Overfitting Levels\nWe evaluate target models of two architectures: VGG16 and ResNet152. We train the target model from scratch on Dtrain pri under the settings in Section 4.1 for 200 epochs and we save the model at epoch 50, 100, 200. Classification accuracy of overfitted models of the same architecture are similar, which are 0.746, 0.748, 0.748 (VGG16) and 0.675, 0.676, 0.675 (ResNet152), respectively. We substitute the target model with a public feature extractor from [5] when training the generation backbone to avoid training multiple generation backbones for each model. In Fig. 9 we plot label prediction accuracy w.r.t. different training epochs of the target model. The results indicate that target models of higher overfitted levels are more vulnerable to SecretGen pseudo label predictor. In the ideal case, the classification model learns a general mapping from face images to identity labels such that the non-sensitive information will be ignored. However, in reality, the correlation between the non-sensitive region and ground truth label grows stronger as the number of training epochs increases, as currently the overfitting problem has not been completely solved in machine learning.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bd09/bd091790-3675-4e04-8864-7e448685f832.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9. Label prediction accuracy of pseudo label predictor against target models of different overfitting levels on CelebA. We plot label prediction accuracy w.r.t. the number of training epochs of the target model.</div>\n# C.5 More Qualitative Results\nWe present some qualitative results of SecretGen in Fig. 10. We can see that when both ground truth label and whitebox access are available, SecretGen can produce recoveries that are closer to the target private image than GMI. In other settings, SecretGen also outperforms the baseline PII in reconstructing privacysensitive attributes of the target image. Besides, SecretGen generates visually plausible images under all settings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c713/c7134ba9-1a03-4c7f-a448-a7278423a952.png\" style=\"width: 50%;\"></div>\nFig. 10. More qualitative results on CelebA. \u201cbb\u201d/\u201cwb\u201d indicates that the method requires blackbox/whitebox access to the model. \u201cgt\u201d indicates that the method requires ground truth labels. Images in the first column are ground truth private images. Images in the second column are prior information available to the adversary.\n",
    "paper_type": "method",
    "attri": {
        "background": "Transfer learning using pre-trained models has raised concerns about the potential leakage of private information from training data. Existing methods for recovering private data often require ground truth labels or whitebox access, which limits their practicality. This paper introduces SecretGen, a framework that addresses these issues by recovering private information without such requirements.",
        "problem": {
            "definition": "The problem addressed in this paper is the reconstruction of privacy-sensitive training data from pre-trained models while lacking access to ground truth labels and whitebox access to the model.",
            "key obstacle": "The main challenge is that existing model inversion attacks require either ground truth labels or whitebox access to the target model, which are not always available in real-world scenarios."
        },
        "idea": {
            "intuition": "The idea stems from the observation that statistical information can be used to discriminate private training distributions from others, enabling the recovery of private data.",
            "opinion": "The proposed idea involves a novel private data reconstruction framework, SecretGen, which utilizes a pseudo label predictor and a latent vector selector to recover private information effectively.",
            "innovation": "SecretGen innovates by eliminating the need for ground truth labels and whitebox access, allowing for practical recovery of private data while maintaining comparable performance to existing methods that do require such information."
        },
        "method": {
            "method name": "SecretGen",
            "method abbreviation": "SG",
            "method definition": "SecretGen is a private data recovery framework designed to reconstruct sensitive training data without requiring ground truth labels or whitebox access to the target model.",
            "method description": "SecretGen reconstructs private data by combining a generation backbone, a pseudo label predictor, and a latent vector selector in a unified optimization framework.",
            "method steps": [
                "1. Generate pseudo labels for private instances using the pseudo label predictor.",
                "2. Sample latent vectors and use the generation backbone to create recovered instances.",
                "3. Apply transformations to stabilize predictions and refine the recovery process.",
                "4. Optimize the latent vector selection based on predicted labels to enhance recovery accuracy."
            ],
            "principle": "The effectiveness of SecretGen lies in its ability to leverage statistical patterns and transformations to predict pseudo labels, which guides the recovery of private instances without needing direct access to sensitive information."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using two face datasets, CelebA and FaceScrub, comparing SecretGen against baseline methods like GMI and PII under various scenarios and access conditions.",
            "evaluation method": "Performance was assessed through two protocols: Protocol 1 evaluates recovery accuracy against private data, while Protocol 2 assesses the utility of recovered data in downstream tasks. Metrics included attack accuracy and Peak Signal-to-Noise Ratio (PSNR)."
        },
        "conclusion": "SecretGen demonstrates a significant advancement in private data recovery, effectively reconstructing sensitive information without requiring ground truth labels. The framework shows robustness against various defenses and offers comparable performance to existing methods that rely on more stringent access conditions.",
        "discussion": {
            "advantage": "The key advantages of SecretGen include its ability to recover private data without requiring ground truth labels or whitebox access, making it more practical for real-world applications.",
            "limitation": "One limitation is that the performance is contingent on the quality of the pseudo label predictor, which may vary depending on the overfitting level of the target model.",
            "future work": "Future research directions include exploring methods for privacy recovery with minimal information and developing defenses against such recovery attacks."
        },
        "other info": {
            "acknowledgements": "This work is partially supported by NSF grant No.1910100, NSF CNS No.2046726, C3 AI, and the Alfred P. Sloan Foundation.",
            "code availability": "The code for SecretGen is available at: https://github.com/AI-secure/SecretGen."
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The paper discusses privacy concerns related to the reconstruction of privacy-sensitive training data from pre-trained models, highlighting the potential leakage of private information."
        },
        {
            "section number": "4.5",
            "key information": "The proposed framework, SecretGen, addresses ethical implications by enabling private data recovery without requiring ground truth labels or whitebox access, enhancing the practicality of privacy-preserving techniques."
        },
        {
            "section number": "7.3",
            "key information": "Future research directions mentioned in the paper include exploring methods for privacy recovery with minimal information and developing defenses against recovery attacks, emphasizing the need for multidisciplinary collaboration in privacy protection."
        },
        {
            "section number": "1",
            "key information": "The introduction of the paper emphasizes the importance of addressing privacy concerns in AI applications, particularly in the context of educational technologies that may handle sensitive data."
        }
    ],
    "similarity_score": 0.48056679315523443,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0632_artif/papers/SecretGen_ Privacy Recovery on Pre-Trained Models via Distribution Discrimination.json"
}