{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.13016",
    "title": "Iterative Forward Tuning Boosts In-Context Learning in Language Models",
    "abstract": "Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.",
    "bib_name": "yang2024iterativeforwardtuningboosts",
    "md_text": "# terative Forward Tuning Boosts In-Context Learning in Language Models Jiaxi Yang1,2,\u2217\u2021, Binyuan Hui3,\u2217, Min Yang1\u2020, Bailin Wang4 Bowen Li5, Binhua Li3, Fei Huang3, Yongbin Li3\u2020  Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n# Iterative Forward Tuning Boosts In-Context Learning in Language Models Jiaxi Yang1,2,\u2217\u2021, Binyuan Hui3,\u2217, Min Yang1\u2020, Bailin Wang4\n# Iterative Forward Tuning Boosts In-Context Learning in Language Models\nBowen Li, Binhua Li, Fei Huang, Yongbin Li 1 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences 3 Alibaba Group, 4 MIT CSAIL, 5 Shanghai AI Laboratory {jx.yang, min.yang}@siat.ac.cn binyuan.hby@alibaba-inc.com https://github.com/Yangjiaxi/DeepThinking\nAbstract\nDespite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: DeepThinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by \u201cthinking\u201d demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\n 4 Jun 2024\narXiv:2305.13016v3\n# 1 Introduction\nLarge language models (LLMs), e.g. OpenAI GPTs (OpenAI, 2023), LLaMA (Touvron et al., 2023) and Qwen (Bai et al., 2023), demonstrate the mysterious in-context learning (ICL) ability, where LLMs make predictions directly by prepending demonstrations to the original input without updating model parameters. LLMs are expected to learn the patterns hidden in demonstrations and\n\u2217Equal contribution. \u2021Work done during an intern at Alibaba Group. \u2020Corresponding authors.\nmake predictions accordingly. As illustrated in Figure 1 (a), an LLM can correctly perform inference on an unseen task by conditioning on several demonstrations. The ICL paradigm empowers LLMs to achieve impressive results in various downstream tasks with a few demonstrations, making Language-Model-as-a-Service (LMaaS) (Sun et al., 2022) possible. Since the performance of ICL is sensitive to specific prompt settings, considerable efforts have been developed to improve the performance of ICL by refining the prompt design from different perspectives, such as demonstration selection (Liu et al., 2022; Li and Qiu, 2023), instruction design (Wei et al., 2022a; Ye et al., 2023), and intermediate chain-of-thought (CoT) reasoning (Wei et al., 2022b; Zhang et al., 2023; Lu et al., 2023). These methods can facilitate LLMs to reduce inference variance and avoid poor worst-case accuracy to some extent by performing prompt engineering. The working mechanism of ICL also draws a lot of attention. Dai et al. (2023) shed light on the connections between ICL and explicit fine-tuning. Specifically, ICL computes meta-gradients via forward computation, while explicit fine-tuning obtains gradients by back-propagation. A dual form exists between attention and gradient descent-based optimization (Irie et al., 2022a), directly connecting the test input to demonstrations. Wang et al. (2023a) argue that label words in demonstrations act as anchors, enabling mapping from demonstrations to test input through information aggregation and label propagation. However, these studies assume that the models process demonstrations only once (i.e., perform a single forward computation), which is incoordinate with the human decision-making process by learning from analogy. Humans usually learn from analogy via an iterative thinking process, such as analyzing demonstrations, reflecting on them, and forming abstract concepts. The models\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/08fa/08fad7b4-5167-4b9d-99c6-cc4a913b48ab.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/84c7/84c748ac-ab0f-4e90-a0e7-f8862d1c018b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"> (a) In-Context Learning</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/583b/583b3319-5388-493f-b9d4-4c4fa7492edf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The illustrations of vanilla ICL and our proposed two-stage framework through Deep-Thinking. The vanilla ICL method processes demonstrations only once, while our \u201cDeep-Thinking\u201d method enables multiple rounds of information accumulation during the reasoning process.</div>\nlearned from demonstrations in inference time by \u201cthinking for longer\u201d or \u201cthinking multiple times\u201d (Schwarzschild et al., 2021). These findings inspire us to ask a question: Can we boost the performance of ICL by learning from demonstrations through several (iterative) forward inferences? In this paper, we propose a two-stage framework to boost the ICL ability in LLMs. Instead of simply concatenating demonstrations and test input together for inference, we decouple the ICL process into a Deep-Thinking stage for demonstration training and a test stage, as illustrated in Figure 1 (b). In the Deep-Thinking stage, we introduce a new attention module that manipulates the updates of Key-Value matrices (Vaswani et al., 2017) within the Transformer\u2019s self-attention (Vaswani et al., 2017) mechanism. This modification leverages Key-Value matrices as a bridge to change the information flow to accumulate and learn information over multiple forward iterations without any training. During the test stage, since the concepts contained in demonstrations are already stored in final Key-Value matrices, we only need to feed the test input into the model and utilize the Key-Value cache for inference. This Deep-Thinking strategy is motivated by humans\u2019 repeat logical thinking and reasoning process. LLMs are expected to extend their abilities to solve unseen, complex tasks by \u201cthinking\u201d demonstrations multiple times. To verify the effectiveness of the proposed DeepThinking, we initially conduct evaluations on conventional ICL benchmarks across language models of various sizes. The experiments show that\nDeep-Thinking significantly outperforms vanilla ICL in a variety of model sizes and tasks, surpassing previous state-of-the-art (SOTA) methods focused on selecting demonstrations. In addition, we introduce two more challenging benchmarks (i.e., MMLU (Hendrycks et al., 2021) and BBH (Srivastava et al., 2023)) and conduct experiments on advanced LLMs, including LLaMA2 (Touvron et al., 2023) and Pythia (Biderman et al., 2023). We argue that on these challenging benchmarks, demonstration selection becomes impractical due to the lack of a potential candidate pool. Deep-Thinking obtains a significant advantage over vanilla ICL.\n# 2 Preliminaries: In-Context Learning\nThis paper focuses on in-context learning tasks. Formally, given a nature language test input xtest with a few (N-shot) input-output demonstrations Cdemos = {(xi, yi)}N i=1, the goal of in-context learning is to predict the label \u02c6y of xtest from a predefined candidate label set Y = {y1, y2, ..., ym} conditioned on N demonstrations. Given an LLM M (e.g., a GPT model), the prediction process can be formulated as follows:\n\u02c6y = arg max yj\u2208Y PM(yj|Cdemos, xtest),\n(1)\nwhere P is the output probability of the LLM M. Generally, an LLM adopts the Transformer as the backbone, which consists of a stack of several Transformer blocks (Vaswani et al., 2017).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4ed/b4edb2ac-e1de-4340-ab53-81615b218687.png\" style=\"width: 50%;\"></div>\nFigure 2: The overview of proposed two-stage ICL framework. It divides the ICL process into Deep-Thinking stage and test stage, which take demonstrations and test query as input, respectively. It replaces the vanilla self-attention mechanism with the proposed Iterative Enhanced Attention (IEA). IEA utilizes the Key-Value matrices as bridge of memories, capable of receiving historical (from the previous iteration) memories. It can mix memories with present information to perform attention, and update memories for the next iteration. During testing, predictions are performed using memories that have been refined through multiple iterations. Notably, throughout this process, the LLM parameters remain frozen and no additional parameters are introduced.\n# 3 Methodology\nIn this paper, we propose a two-stage ICL framework that improves performance through multiple forward iterations. As shown in Figure 2, we assign the demonstrations and test input to the DeepThinking and test stages, respectively, where KeyValue matrices serve as a bridge between the two stages. Next, we describe these two stages in detail.\n# 3.1 The Deep-Thinking Stage\nIn the Deep-Thinking stage, given the demonstrations, we perform multiple forward passes in an iterative way by manipulating the Key-Value matrices in the self-attention (Vaswani et al., 2017) module. We use Xl t to denote the output representation of the entire demonstration sequence at layer l and the t-th forward pass. Notably, Xl t receives not only the output Xl\u22121 t from the previous Transformer block, but also the Key-Value matrices\n\ufffdKl t\u22121, \ufffdVl t\u22121* produced by the same self-attention module at the (t\u22121)-th forward pass. Accordingly, the Key-Value matrices will be updated as \ufffdKl t, \ufffdVl t.\n \ufffd  \ufffd Iterative Enhanced Attention To handle multiple forward iteration information in Deep-Thinking, we proposed a modified attention mechanism, named Iterative Enhanced Attention (IEA). Each block of IEA is illustrated in Figure 2. The information flowing through a block can be observed from both horizontal and vertical processes. The horizontal process represents the calculation of the input parameters in a conventional manner, while the vertical process stands for the manipulation of the Key-Value matrices. Specifically, the input Xl\u22121 t is firstly projected by key, value and query weight matrices, respectively:\nKl t = WKXl\u22121 t , Vl t = WV Xl\u22121 t , Ql t = WQXl\u22121 t (2\n(2)\n*Key-Value matrices, represented in blue, act as memory carriers throughout the Deep-Thinking iterations and serve as inputs in the final test stage.\nwhere Kl t, Vl t represent the present Key-Value matrices of vanilla self-attention, projected from input X. For the horizontal process, we concatenate the present Key-Value matrices with the history KeyValue matrices \ufffdKl t\u22121, \ufffdVl t\u22121 as the mixed Key-Value to compute attention map and obtain the output Xl t of current layer as follows:\n(3)\n\ufffd \ufffd where F refers to the operations after selfattention, namely the Feed-Forward Network (FFN) (Vaswani et al., 2017), layer normalization (Ba et al., 2016) and residual connection (He et al., 2015). Furthermore, the update process is jointly contributed by the present and history Key-Value matrices. From a high-level abstract perspective, the update process can be formalized as follows:\n\ufffd \ufffd \ufffd where \ufffdKl t and \ufffdVl t are updated Key-Value matrices. For the update function, we adopt a simple gating mechanism that utilizes the hyper-parameter \u03b7 to control the fusion rate of history and present information. Modeling demonstrations takes up to T iterations, where the value of T can be predefined by users. After the iterative forward process, we can obtain final updated Key-Value matrices \ufffdKl T , \ufffdVl T . By stacking updated Key-Value matrices of all layers in a given LLM, we have\n(5)\n\ufffd\ufffd   \ufffd\ufffd   which can be stored statically. L denotes the number of Transformer blocks in an LLM.\n# 3.2 The Test Stage\nConsidering that we now have the Key-Value matrices \ufffdKT , \ufffdVT that have been updated for T iterations, the information contained in them can be regarded as a highly condensed modeling of the demonstrations. The inference process can be performed using the same formulation as given by Eq.(2)-Eq.(3). Specifically, the inference process for l-th layer can be formalized as:\n\ufffd \ufffd In this way, we can obtain the representation XL test produced by the final layer, which is used to make predictions.\n# 4 Experiments\n# 4.1 Conventional In-context Learning Tasks\nWe first evaluate the proposed Deep-Thinking against other enhanced ICL methods in a fair comparison of conventional ICL tasks. We select five popular tasks, including SST2 (Socher et al., 2013), SST5 (Socher et al., 2013), MR (Pang and Lee, 2005), AGNews (Zhang et al., 2015) and TREC (Li and Roth, 2002; Hovy et al., 2001). For a fair comparison, we choose GPT2-L as the base model, which is widely used by previous studies (Li and Qiu, 2023).\nCompared Methods We use several demonstration selection methods as baselines, which can be classified into distinct approaches. (1) Geometrybased techniques, such as Herding (Chen et al., 2010) and K-Center Greedy (Sener and Savarese, 2018), concentrate on spatial proximity within the feature space for constructing demonstrations. (2) Uncertainty-based methods posit that demonstrations with higher uncertainty exert more substantial influence on the model, encompassing techniques such as Entropy, Least Confidence, Margin (Coleman et al., 2019), and CAL (Margatina et al., 2021). (3) Gradient matching-based methods, such as CRAIG (Mirzasoleiman et al., 2020) and GradMatch (Killamsetty et al., 2021), aim to replicate the gradient distribution of the full dataset with a subset. (4) Submodularity-based methods assess informativeness and diversity for selection, including such as FacilityLocation and GraphCut (Iyer and Bilmes, 2013). (5) LENS (Li and Qiu, 2023) adopts a \u201cfilter-then-search\u201d approach, utilizing the \u201cInfoScore\u201d metric to select the best demonstrations. Notably, our fairest baseline is the Random method (vanilla ICL), where we use the exact same demonstrations without any selection process.\nFurther Comparison To assess the performance of Deep-Thinking across a range of LMs across different sizes, we extend the base model of DeepThinking on conventional ICL tasks to include OPT (125M, 350M, 2.7B) (Zhang et al., 2022), GPT-2 (Medium, Large, and XL) (Radford et al., 2019), GPT-Neo (2.7B) (Black et al., 2021) and LLaMA2 (7B, 13B) (Touvron et al., 2023). This extension aims to demonstrate the effectiveness of DeepThinking across a spectrum of LM scales.\nMethod\nSST2\nSST5\nTREC\nMR\nAGNews\nAverage\nIn-context learning w/o dev set. \u2662\nRandom\n57.9\n27.5\n30.3\n59.5\n33.6\n41.8\nHerding (Chen et al., 2010)\n62.0\n24.8\n26.4\n54.1\n38.7\n41.2\nK-Center Greedy (Sener and Savarese, 2018)\n58.6\n25.1\n31.3\n59.0\n42.3\n43.3\nEntropy (Coleman et al., 2019)\n62.4\n25.5\n26.2\n54.1\n30.6\n39.8\nLeastConfidence (Coleman et al., 2019)\n58.4\n26.0\n23.5\n55.9\n31.6\n39.1\nMargin (Coleman et al., 2019)\n62.4\n26.1\n24.2\n54.1\n38.1\n41.0\nCAL (Margatina et al., 2021)\n59.3\n25.3\n31.8\n66.2\n42.3\n45.0\nCRAIG (Mirzasoleiman et al., 2020)\n63.4\n26.4\n32.0\n59.3\n37.4\n43.7\nGradMatch (Killamsetty et al., 2021)\n57.0\n26.3\n25.8\n56.6\n32.6\n39.7\nFacilityLocation (Iyer and Bilmes, 2013)\n65.5\n23.9\n35.7\n61.7\n42.5\n45.9\nGraphCut (Iyer and Bilmes, 2013)\n65.0\n25.3\n34.7\n66.3\n41.9\n46.6\nDeep-Thinking\n85.7\n39.2\n54.2\n71.6\n72.9\n64.7\nIn-context learning w/ dev set. \u2661\nLENS (Li and Qiu, 2023)\n86.3\n44.9\n59.0\n83.1\n77.9\n70.2\nRandom\u2663\n77.9\n38.0\n56.6\n81.8\n74.6\n65.8\nDeep-Thinking\u2663\n88.1\n45.2\n61.6\n84.8\n80.3\n72.0\nMethod\n<div style=\"text-align: center;\">In-context learning w/ dev set. \u2661</div>\nTable 1: Experimental results across conventional ICL tasks with different ICL methods. \u2662denotes that each method was assessed over ten random seeds, and the reported values are the average performance across these seeds. \u2661signifies the evaluation of multiple sets of random demonstrations on the dev set, with the best-performing set selected (Li and Qiu, 2023). \u2663indicates the methods utilized the same demonstrations, ensuring that any improvement stemmed solely from the Deep-Thinking stage.\n# 4.2 Challenging Benchmarks\nIn contrast to the conventional ICL benchmarks that entail a candidate pool for sample selection, realworld complex tasks frequently present scenarios where only a limited and fixed set of demonstrations is available. This particular challenge renders many existing methods of demonstration selection impractical in such scenarios. To deal with the challenges, we choose MMLU (Hendrycks et al., 2021) and BBH (Srivastava et al., 2023) to extend the evaluation of Deep-Thinking to more rigorous and multifaceted scenarios. Concretely, MMLU encompasses a diverse set of 57 tasks, spanning elementary mathematics, US history, computer science, law, and various other domains. In contrast, BBH is tailored to address a suite of 23 challenging tasks within the BIG-Bench framework. In addressing these challenging benchmarks, we employ more advanced LLMs, including LLaMA2 (Touvron et al., 2023) (7B and 13B) and Pythia (Biderman et al., 2023) (70M, 410M, 1.4B, 6.9B and 12B) as base models, given their balanced ability and versatility in handling a wide range of tasks.\n# 4.3 Implementation Details and Evaluation\nAll experiments are conducted on a single NVIDIA A100 GPU. For all baselines and Deep-Thinking, we run each method over ten random seeds and\nreport the average performance. For conventional ICL tasks, we follow (Li and Qiu, 2023) that the number of demonstrations for SST2, SST5, TREC, MR, and AGNews is 8, 10, 12, 8, and 8, respectively. For MMLU and BBH, the demonstrations come from the dataset\u2019s inherent demonstrations. Specifically five demonstrations for MMLU and three demonstrations for BBH per task. In the in-context setting without a dev set, we fix the iteration number T at 5, with the gating parameter \u03b7 set to 0.01. In the in-context setting with a dev set, we relax the max iteration number T to 15, using the dev set to determine the final hyper-parameters. For the dev set, we randomly select a sample size identical to the test set and keep it fixed. For evaluation, similar to previous methods, we concatenate the test input with each candidate\u2019s answer and submit them to the LLM. The final answer is selected by summing the probabilities of the tokens belonging to the answer part and choosing the candidate answer with the highest probability.\n# 4.4 Main Results\nResults on Conventional ICL Tasks We first compare Deep-Thinking with previous methods on conventional ICL tasks. Table 1 shows that Deep-Thinking consistently outperforms baseline methods. In addition, a significant improvement is observed when comparing its performance with\nModel\nMethod\nSST2\nSST5\nTREC\nMR\nAGNews\nAverage\nOPT-125M\nICL\n55.7\n26.7\n25.0\n50.4\n41.7\n39.9\nOurs\n72.0\n33.2\n47.0\n65.8\n50.6\n53.7\nOPT-350M\nICL\n54.1\n26.6\n37.2\n71.2\n42.9\n46.4\nOurs\n79.7\n31.8\n45.8\n73.4\n64.3\n59.0\nGPT2-Medium\n355M\nICL\n59.6\n23.7\n33.4\n65.0\n51.7\n46.7\nOurs\n86.9\n38.1\n43.6\n80.3\n80.0\n65.8\nGPT2-XL\n1.5B\nICL\n60.3\n41.1\n34.4\n66.7\n56.9\n51.9\nOurs\n89.3\n43.8\n60.6\n86.1\n82.6\n72.5\nOPT-2.7B\nICL\n62.4\n45.8\n37.0\n86.2\n77.8\n61.8\nOurs\n72.4\n47.7\n50.0\n89.0\n85.8\n69.0\nGPT-Neo\n2.7B\nICL\n84.8\n39.7\n49.6\n85.2\n71.6\n66.2\nOurs\n88.1\n45.5\n59.2\n89.1\n83.5\n73.1\nLLaMA2\n7B\nICL\n89.5\n46.3\n82.8\n91.2\n84.6\n78.9\nOurs\n90.0\n48.1\n84.8\n92.2\n88.9\n80.8\nLLaMA2\n13B\nICL\n95.2\n46.4\n84.8\n92.5\n87.0\n81.2\nOurs\n96.0\n49.9\n86.2\n94.7\n88.8\n83.1\nTable 2: Experimental results cross different LLMs on conventional ICL tasks. To ensure that any observed improvement stems exclusively from the Deep-Thinking stage, all variables are held constant across experiments.\nand without the utilization of a development set. Particularly, Deep-Thinking surpasses the Random baseline by an average margin of 6.2% under the dev setting. This improvement is solely attributed to the iterative forward operations, providing empirical evidence of the effectiveness of the proposed method.\nTransferablity across Different LMs In the aforementioned experiments, we employ the same LM (GPT-L) as the base model. To assess the transferability of Deep-Thinking across LMs of varying scales and pre-trained corpora, we expand our experimental scope to include diverse settings. Table 2 validates the transferability and generalizability of Deep-Thinking across different base models, maintaining competitiveness even when applied to stronger models such as LLaMA2.\nResults on Challenging Benchmarks Table 3 presents the averaged performance of DeepThinking and vanilla ICL on MMLU and BBH benchmarks. Notably, the performance boost is consistent across all selected models, affirming that Deep-Thinking beneficially impacts a wide spectrum of frontier LMs, independent of their specific designs or training data. This effect is particularly evident in smaller-sized models such as Pythia, where the relative performance uplift is significant. This trend aligns with observations from conventional ICL tasks, further highlighting the broad\napplicability and effectiveness of Deep-Thinking in enhancing models\u2019 in-context learning ability.\n# 4.5 Fine-grained Task Analysis\nTo investigate whether Deep-Thinking shows greater advantages in tasks requiring complex reasoning, we conduct a detailed analysis as shown in Figure 3. The MMLU benchmark categorizes its 57 subtasks into four major classes: STEM, Humanities, Social Science, and Others. STEM tasks rigorously assess the model\u2019s reasoning abilities, whereas the other three categories predominantly serve as tests of knowledge retention. STEM tasks pose greater challenges for vanilla ICL methods; however, Deep-Thinking consistently demonstrates improvements across all categories, indicating a relatively more substantial gain in STEM. For instance, LLaMA2-7B exhibits a 3.9% increase over ICL in STEM, while registering improvements of 2.3%, 2.6%, and 2.5% in Humanities, Social Science, and Others, respectively. This highlights the effectiveness of Deep-Thinking in enhancing models\u2019 capabilities to address complex reasoning and problem-solving tasks.\n# 4.6 Impact of Hyper-parameters Impact of Demonstration Numbers As de\n# 4.6 Impact of Hyper-parameters\npicted in Figure 4, we conducted experiments on the SST2 and AGNews datasets to explore the impact of increasing the number of demonstrations on\nModel\n&\nMethod\nLLaMA2\nPythia\n7B\n13B\n70M\n410M\n1.4B\n6.9B\n12B\nICL\nOurs\nICL\nOurs\nICL\nOurs\nICL\nOurs\nICL\nOurs\nICL\nOurs\nICL\nOurs\nMMLU\n41.9\n44.6\n45.1\n47.6\n24.6\n29.6\n27.0\n30.8\n30.6\n33.6\n33.5\n37.1\n36.2\n39.5\nBBH\n46.1\n49.8\n49.7\n53.6\n34.9\n39.8\n37.5\n42.3\n38.2\n43.8\n39.2\n43.4\n41.1\n45.3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/74af/74afc454-7446-481d-8619-20959d700ab5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1 Figure 3: Comparison of model performance across four major classes of the MMLU benchmarks. Due to space constraints and to ensure clarity in presentation, we solely report the results of four out of the seven models.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/92b2/92b2b133-4c95-4b19-a2d5-c0cde9da2e25.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: An illustration of the impact of increasing the number of demonstrations on the effectiveness of vanilla ICL and Deep-Thinking.</div>\nthe efficacy of ICL. We perform ten runs of the experiments and calculate the variance. The results indicate that both vanilla ICL and Deep-Thinking benefit from an increase in the number of demonstrations. However, Deep-Thinking consistently outperforms vanilla ICL, achieving significantly better results even with a smaller number of demonstrations. Additionally, Deep-Thinking demonstrates a smaller variance, indicating greater robustness. This suggests that it is more cost-effective to \u201cthink\u201d more from existing demonstrations than merely increasing the number of demonstrations.\nThe Sensitivity of Random Seed The randomness in vanilla ICL and Deep-Thinking stems solely from the random selection of demonstrations. To further investigate the robustness of the methods, we conduct multiple experiments on the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d3a/6d3aaac4-de08-4675-8dc6-c386ec775a1a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1 Figure 5: The performance distribution of performance for vanilla ICL and Deep-Thinking, comparing effects of random seeds (left) and random orders (right).</div>\nSST dataset by randomly choosing eight different demonstrations, keeping other variables (except the seed) constant. As illustrated in Figure 5 (left), vanilla ICL is significantly affected by randomness, whereas Deep-Thinking achieves stronger performance with less variance. Deep-Thinking, by iterating multiple times, bridges the gap by maximizing the utility of demonstrations.\n# The Sensitivity of Demonstrations Orders\norder of demonstrations in ICL is crucial and can significantly impact performance (Lu et al., 2022a; Liu et al., 2022). In particular, different orders of demonstrations can lead to performance close to the state-of-the-art or merely random guesses. We examine the effect of demonstration order on ICL and Deep-Thinking on the SST-2 dataset. As shown in Figure 5 (right), the results show that vanilla ICL\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/724a/724a4a53-b944-40a7-99cc-c7d333b70a63.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Sensitivity of accuracy to hyperparameter \u03b7 and alignment of development (Dev) and test set performance over iteration steps (T).</div>\nFigure 6: Sensitivity of accuracy to hyperparameter \u03b7 and alignment of development (Dev) and test set performance over iteration steps (T).\nis highly sensitive to the order, with a significant variance in outcomes indicating large instability. Deep-Thinking benefits from iterative learning of demonstrations, overcoming order biases, and thus shows more consistent performance.\n# Impact of Gate \u03b7 and Iteration Steps T The\nImpact of Gate \u03b7 and Iteration Steps T The gate \u03b7 is crucial in dictating how much of the memory is retained during the Deep-Thinking stage and the degree of openness to new information for the next iteration. A larger \u03b7 signifies greater changes, thus requiring fewer iterations T, and vice versa. To investigate the optimal \u03b7, we enumerate values in [0.001, 0.01, 0.1]. As shown in Figure 6 (left), setting \u03b7 = 0.01 achieves a balance between the number of iterations and performance. We can analogize \u03b7 to the learning rate and T to the number of training steps. Inspired by this comparison, as described in Table 1, we use a dev set to determine the optimal number of iterations T. Figure 6 (right) shows that there is a basic alignment between the dev and test sets.\n# 5 Related Work\nIn-context learning (ICL) with LLMs has made a breakthrough and become mainstream in tackling various tasks (Li et al., 2023; Dong et al., 2022; Qiao et al., 2023). Recently, great efforts have been made to improve the performance of ICL from different perspectives, such as demonstrations selection (Liu et al., 2022; Li and Qiu, 2023), prompt template design (Wei et al., 2022a), and intermediate chain-of-thought (CoT) reasoning (Wei et al., 2022b; Zhang et al., 2023). For demonstration selection, Liu et al. (2022) performed demonstration selection through a kNNbased retriever, choosing the closest example to test input. Wu et al. (2022) proposed self-adaptive ICL with a general select-and-rank framework for\ndemonstration selection. In addition to example selection, Lu et al. (2022b) investigated the sensitivity of ICL to the permutation of demonstrations and proposed entropy metrics to determine their order. The above ICL methods are usually restricted by the number of demonstrations. To mitigate such a challenge, Hao et al. (2022) attempted to scale ICL by grouping demonstrations, which could increase the number of demonstrations to 1,000. The formatting function also plays a crucial role in ICL, especially for tasks requiring complex reasoning steps, such as commonsense reasoning. Wei et al. (2022b) introduced chain-of-thoughts (CoT) prompting to provide guidance. Zhang et al. (2023) stimulated the model\u2019s ability for gradual reasoning by adding the \u201cLet\u2019s think step-by-step\u201d prefix. Instead of generating reasoning steps, Press et al. (2023) investigated the compositional reasoning abilities by allowing LLMs to generate follow-up questions. Subsequently, Madaan et al. (2023) introduced a new framework to enhance the initial outputs generated by LLMs via iterative feedback and refinement. Meanwhile, some studies (Xie et al., 2022; Dai et al., 2023; Wang et al., 2023b) attempt to uncover the underlying working mechanism of ICL. In particular, Xie et al. (2022) showed that ICL happened via Bayesian inference, where certain concepts were implicitly predicted before the final prediction. Subsequently, Dai et al. (2023) revealed that there are connections between ICL and explicit fine-tuning and explained LLMs as meta-optimizers (Irie et al., 2022b). Unlike existing methods, to the best of our knowledge, we are the first to decouple ICL into two stages and focus on how to deeply learn from fixed demonstrations rather than on demonstration selection or prompt engineering. This is advantageous for the world situation where provided samples are scarce, i.e., there is no large candidate set of demonstrations.\n# 6 Conclusion\nIn this paper, we introduce a novel two-stage framework aimed at enhancing the ICL capabilities of LLMs by leveraging iterative forward inferences to learn from demonstrations. By decoupling the ICL process into a dedicated Deep-Thinking stage for demonstration training and a subsequent test stage, we effectively mimic the decision-making process of humans by learning from analogy. This approach aligns with how humans engage in repeated\nlogical thinking. The empirical evaluations across conventional ICL benchmarks and more challenging datasets demonstrate that our Deep-Thinking strategy significantly outperforms previous ICL approaches, particularly in scenarios where demonstration selection is impractical.\n# Limitations\nWhile our method has demonstrated promising results and significant advancements across various aspects, it is imperative to conduct a thorough analysis of its limitations. In this section, we explore the potential constraints of our method. Firstly, owing to limited computational resources and time constraints, we were unable to evaluate our method on larger language models, such as LLaMA2-70B. This limitation may impact the generalizability of our findings to larger-scale language models. Secondly, our evaluation primarily focused on conventional ICL tasks and challenging benchmarks. To enhance the comprehensiveness of our findings, we intend to broaden the scope of evaluation to encompass a diverse range of dataset types, including math reasoning, code generation, and open-ended text generation. This extension aims to provide further validation of our method\u2019s generalizability.\n# Acknowledgments\nMin Yang was supported by National Key Research and Development Program of China (2022YFF0902100), National Natural Science Foundation of China (62376262), the Natural Science Foundation of Guangdong Province of China (2024A1515030166), Shenzhen Science and Technology Innovation Program (KQTD20190929172835662), Shenzhen Basic Research Foundation (JCYJ20210324115614039). This work was supported by Alibaba Group through Alibaba Research Intern Program.\n# References\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR.\nUSVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR. Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. Yutian Chen, Max Welling, and Alex Smola. 2010. Super-samples from kernel herding. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pages 109\u2013116. Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. 2019. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4005\u20134019. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234. Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Eduard Hovy, Laurie Gerber, Ulf Hermjakob, ChinYew Lin, and Deepak Ravichandran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research. Kazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. 2022a. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In International Conference on Machine Learning, pages 9639\u20139659. PMLR.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.\nYutian Chen, Max Welling, and Alex Smola. 2010. Super-samples from kernel herding. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pages 109\u2013116.\nCody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. 2019. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4005\u20134019. Association for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR).\nKazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. 2022b. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In International Conference on Machine Learning, pages 9639\u20139659. PMLR.\nRishabh K Iyer and Jeff A Bilmes. 2013. Submodular optimization with submodular cover and submodular knapsack constraints. Advances in neural information processing systems, 26.\nKrishnateja Killamsetty, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. 2021. Grad-match: Gradient matching based data subset selection for efficient deep model training. In International Conference on Machine Learning, pages 5464\u20135474. PMLR.\nJinyang Li, Binyuan Hui, GE QU, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023. Can LLM already serve as a database interface? a BIg bench for large-scale database grounded text-to-SQLs. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. In Thirty-seventh Conference on Neural Information Processing Systems.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022a. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b. Fantastically ordered\nprompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098.\nprompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems. Katerina Margatina, Giorgos Vernikos, Lo\u00efc Barrault, and Nikolaos Aletras. 2021. Active learning by acquiring contrastive examples. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650\u2013663, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, Dublin, Ireland. Association for Computational Linguistics. Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. 2020. Coresets for data-efficient training of machine learning models. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6950\u20136960. PMLR. OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 115\u2013124. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5368\u20135393, Toronto, Canada. Association for Computational Linguistics. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nprompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems. Katerina Margatina, Giorgos Vernikos, Lo\u00efc Barrault, and Nikolaos Aletras. 2021. Active learning by acquiring contrastive examples. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650\u2013663, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, Dublin, Ireland. Association for Computational Linguistics. Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. 2020. Coresets for data-efficient training of machine learning models. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6950\u20136960. PMLR. OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 115\u2013124. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5368\u20135393, Toronto, Canada. Association for Computational Linguistics. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. 2021. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34:6695\u20136706. Ozan Sener and Silvio Savarese. 2018. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In Proceedings of The International Conference on Machine Learning. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023a. Label words are anchors: An information flow perspective for understanding in-context learning. In Conference on Empirical Methods in Natural Language Processing. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023b. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2022. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. arXiv preprint arXiv:2212.10375. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 174\u2013184. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, pages 649\u2013657. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations.\n# A Data Statistics and Templates For In-context Learning Tasks\n<div style=\"text-align: center;\">We choose five datasets for evaluating in-context learning methods following (Min et al., 2022; Li and Qiu, 2023). We show the prompting templates and dataset statistics in Table 4.</div>\nTask\nOriginal Dev Size\nTest Size\nTemplate\nLabels\nSST2\n67349\n873\nReview: {query}\nSentiment: {label}\nnegative / positive\nSST5\n8544\n2210\nReview: {query}\nSentiment: {label}\nterrible / negative / neutral / positive /\ngreat\nTREC\n5452\n500\nQuestion: {query}\nType: {label}\nAbbreviation\n/\nEntity\n/\nDescription\n/\nPerson / Location / Number\nMR\n8530\n1066\nReview: {query}\nSentiment: {label}\nnegative / positive\nAGNews\n120000\n7601\nArticle: {query}\nCategory: {label}\nWorld / Sports / Business / Technology\nTable 4: The statistics of standard in-context learning tasks, including detailed task sizes, prompting templates, and labels.\n# B Data Statistics For MMLU\nWe obtained the MMLU dataset from the Hugging Face Hub, specifically from the repository cais/mmlu\u2020. According to the dataset\u2019s card, MMLU encompasses 57 tasks spanning diverse knowledge domains. Each task includes a minimum of 100 test examples. For in-context demonstrations, five examples per task provided by original dataset are used. We present detailed statistics for each sub-task, including the classification scheme, in Table 5.\n# C Data Statistics For BBH\nFor the BigBench-Hard (BBH) dataset, we sourced the data from the maveriq/bigbenchhard\u2021. We have excluded certain tasks due to their incompatibility with multiple-choice or classification formats. Specifically, the tasks omitted include: Dyck Languages, Multistep Arithmetic, Object Counting, Word Sorting, and Reasoning about Colored Objects. Table 6 provides a detailed statistics for each selected task.\nSTEM\nastronomy: 152, college_physics: 102, conceptual_physics: 235, high_school_physics: 151,\ncollege_chemistry: 100, high_school_chemistry: 203, college_biology: 144,\nhigh_school_biology: 310, college_computer_science: 100, computer_security: 100,\nhigh_school_computer_science: 100, machine_learning: 112, abstract_algebra: 100,\ncollege_mathematics: 100, elementary_mathematics: 378, high_school_mathematics: 270,\nhigh_school_statistics: 216, electrical_engineering: 145\nHumanities\nhigh_school_european_history: 165, high_school_us_history: 204, high_school_world_history: 237,\nprehistory: 324, formal_logic: 126, logical_fallacies: 163, moral_disputes: 346,\nmoral_scenarios: 895, philosophy: 311, world_religions: 171, international_law: 121,\njurisprudence: 108, professional_law: 1534\nSocial Sciences\nhigh_school_government_and_politics: 193, public_relations: 110, security_studies: 245,\nus_foreign_policy: 100, human_sexuality: 131, sociology: 201, econometrics: 114,\nhigh_school_macroeconomics: 390, high_school_microeconomics: 238,\nhigh_school_geography: 198, high_school_psychology: 545, professional_psychology: 612\nOthers (Business, Health, misc.)\nglobal_facts: 100, miscellaneous: 783, professional_accounting: 282, business_ethics: 100,\nmanagement: 103, marketing: 234, anatomy: 135, clinical_knowledge: 265,\ncollege_medicine: 173, human_aging: 223, medical_genetics: 100, nutrition: 306,\nprofessional_medicine: 272, virology: 166\ne 5: The number of samples for each subtask in the MMLU benchmar\nBBH boolean_expressions: 250, causal_judgement: 187, date_understanding: 212, disambiguation_qa: 247, formal_fallacies: 250, geometric_shapes: 200, hyperbaton: 250, logical_deduction_three_objects: 250, logical_deduction_five_objects: 250, logical_deduction_seven_objects: 250, movie_recommendation: 231, navigate: 250, penguins_in_a_table: 145, ruin_names: 247, salient_translation_error_detection: 250, snarks: 177, sports_understanding: 250, temporal_sequences: 250, web_of_lies: 250 tracking_shuffled_objects_three_objects: 250, tracking_shuffled_objects_five_objects: 250, tracking_shuffled_objects_seven_objects: 250\n# D Minimal Implementation\nThe provided minimal implementation code showcases our proposed method, primarily integrating wit the Hugging Face Transformers library.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e439/e4393891-95d2-4656-a508-9590fa6e4126.png\" style=\"width: 50%;\"></div>\n# def main():\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the advancements in in-context learning (ICL) for large language models (LLMs), highlighting the limitations of existing methods that primarily focus on single demonstration processing. It emphasizes the necessity for a new approach that incorporates multiple iterations of demonstration processing, akin to human learning through analogy.",
        "problem": {
            "definition": "The problem revolves around the inefficiency of current ICL methods that rely on a single iteration of demonstration processing, which limits the model's ability to generalize and learn effectively from examples.",
            "key obstacle": "The main challenge lies in the assumption that LLMs can learn adequately from demonstrations through a single forward pass, which does not align with the iterative decision-making processes observed in human learning."
        },
        "idea": {
            "intuition": "The idea is inspired by the human cognitive process of learning through repeated reflection and analogy, suggesting that multiple forward inferences could enhance learning outcomes in LLMs.",
            "opinion": "The proposed two-stage framework, consisting of a Deep-Thinking stage and a test stage, allows LLMs to process demonstrations iteratively, thereby improving their understanding and prediction capabilities.",
            "innovation": "The key innovation of this method is the introduction of an iterative enhanced attention mechanism that enables multiple rounds of information accumulation without additional training, contrasting with existing ICL methods."
        },
        "method": {
            "method name": "Deep-Thinking",
            "method abbreviation": "DT",
            "method definition": "Deep-Thinking is a two-stage framework that separates the ICL process into a demonstration training phase and a testing phase, utilizing iterative forward passes to enhance model learning.",
            "method description": "The method leverages an iterative enhanced attention mechanism to manipulate Key-Value matrices, allowing multiple iterations of information processing during inference.",
            "method steps": "1. Initialize Key-Value matrices. 2. Perform multiple forward passes in the Deep-Thinking stage. 3. Update Key-Value matrices iteratively. 4. Use the final Key-Value matrices in the test stage for inference.",
            "principle": "This method is effective because it mimics human-like reasoning through iterative learning, allowing the model to refine its understanding of demonstrations across multiple iterations."
        },
        "experiments": {
            "evaluation setting": "The evaluation was conducted on conventional ICL benchmarks such as SST2, SST5, MR, AGNews, and TREC, using various LLMs including GPT-2 and LLaMA2, with both fixed and flexible demonstration sets.",
            "evaluation method": "The performance of the method was assessed by comparing it against several baseline methods, utilizing metrics such as accuracy across multiple random seeds for robustness."
        },
        "conclusion": "The experiments demonstrate that the Deep-Thinking framework significantly enhances ICL performance, particularly in scenarios where demonstration selection is impractical, thereby validating its effectiveness and contribution to the field.",
        "discussion": {
            "advantage": "The primary advantage of Deep-Thinking is its ability to outperform existing ICL methods by effectively utilizing multiple iterations of demonstrations, leading to improved accuracy and robustness.",
            "limitation": "One limitation is the inability to evaluate the method on larger models due to computational constraints, which may affect the generalizability of the findings.",
            "future work": "Future research should focus on extending the evaluation of Deep-Thinking to a wider array of tasks, including those requiring complex reasoning, to further validate its applicability and effectiveness."
        },
        "other info": {
            "acknowledgments": "Min Yang was supported by various research grants, and the work was supported by Alibaba Group through the Alibaba Research Intern Program.",
            "data_statistics": {
                "ICL_tasks": {
                    "SST2": {
                        "original_dev_size": 67349,
                        "test_size": 873
                    },
                    "SST5": {
                        "original_dev_size": 8544,
                        "test_size": 2210
                    },
                    "MR": {
                        "original_dev_size": 8530,
                        "test_size": 1066
                    },
                    "AGNews": {
                        "original_dev_size": 120000,
                        "test_size": 7601
                    },
                    "TREC": {
                        "original_dev_size": 5452,
                        "test_size": 500
                    }
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the advancements in in-context learning (ICL) for large language models (LLMs), highlighting the limitations of existing methods that primarily focus on single demonstration processing."
        },
        {
            "section number": "1.3",
            "key information": "The proposed two-stage framework, consisting of a Deep-Thinking stage and a test stage, allows LLMs to process demonstrations iteratively, thereby improving their understanding and prediction capabilities."
        },
        {
            "section number": "3.1",
            "key information": "The primary advantage of Deep-Thinking is its ability to outperform existing ICL methods by effectively utilizing multiple iterations of demonstrations, leading to improved accuracy and robustness."
        },
        {
            "section number": "3.4",
            "key information": "The method leverages an iterative enhanced attention mechanism to manipulate Key-Value matrices, allowing multiple iterations of information processing during inference."
        },
        {
            "section number": "6.2",
            "key information": "One limitation is the inability to evaluate the method on larger models due to computational constraints, which may affect the generalizability of the findings."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrate that the Deep-Thinking framework significantly enhances ICL performance, particularly in scenarios where demonstration selection is impractical, thereby validating its effectiveness and contribution to the field."
        }
    ],
    "similarity_score": 0.7285391104445524,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Iterative Forward Tuning Boosts In-Context Learning in Language Models.json"
}