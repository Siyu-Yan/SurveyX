{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2208.01066",
    "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
    "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \"most\" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",
    "bib_name": "garg2023transformerslearnincontextcase",
    "md_text": "# What Can Transformers Learn In-Context? A Case Study of Simple Function Classes\nShivam Garg* Stanford University shivamg@cs.stanford.edu Dimitris Tsipras* Stanford University tsipras@stanford.edu\nIn-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \u201cmost\u201d functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions\u2014that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes\u2014namely sparse linear functions, two-layer neural networks, and decision trees\u2014with performance that matches or exceeds task-specific learning algorithms. 1\n# 1 Introduction\nLarge language models such as GPT-3 [Brown et al., 2020] are able to perform in-context learning: given prompt containing examples from a task (input-output pairs) and a new query input, the language mod can generate the corresponding output. For example, these models are able to produce English translation of French words after being prompted on a few such translations, e.g.:\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd his capability is quite intriguing as it allows models to adapt to a wide range of downstream tasks on-they\u2014i.e., without the need to perform any parameter updates after the model is trained [Brown et al., 2020,\nGregory Valiant Stanford University valiant@stanford.edu\nLieber et al., 2021, Rae et al., 2021, Black et al., 2022]. However, it is unclear to what extent these models have developed the ability to learn new tasks from in-context examples alone as opposed to simply indexing into a vast set of known tasks from the training data (e.g., see Min et al. [2022]). 2 To make progress towards understanding in-context learning, we consider the well-defined problem of learning a function class from in-context examples. That is, we say that a model can in-context learn a function class F if, for \u201cmost\u201d functions f \u2208F, the model can approximate f (xquery) for a new query input xquery by conditioning on a prompt sequence (x1, f (x1), . . . , xk, f (xk), xquery) containing in-context examples and the query input. Formally, let DX be a distribution over inputs and DF be a distribution over functions in F. A prompt P is a sequence (x1, f (x1), . . . , xk, f (xk), xquery) where inputs (i.e., xi and xquery) are drawn i.i.d. from DX and f is drawn from DF. We say that a model M can in-context learn the function class F up to \u03f5, with respect to (DF, DX ), if it can predict f (xquery) with an average error\n\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd where \u2113(\u00b7, \u00b7) is some appropriate loss function, such as the squared error. Within this framework, we can now concretely ask:\nCan we train a model to in-context learn a certain function class?\nNote that, here, being able to in-context learn a function class is a property of model M alone, independent of how it was trained. Training such a model can be viewed as an instance of meta-learning [Schmidhuber, 1987, Naik and Mammone, 1992, Thrun and Pratt, 2012], a general paradigm for learning a model or method that can learn from data. We empirically study this question, focusing on Transformer models [Vaswani et al., 2017, Radford et al., 2018]\u2014the architecture behind recent large language models\u2014trained from scratch to in-context learn a range of simple, well-defined function classes (e.g. linear functions). Specifically, we sample prompts containing in-context examples (input-output pairs) generated using functions in a given class and train models to predict the function value at the corresponding query inputs. (see illustration in Figure 1). Our findings are as follows.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/beb5/beb5c98f-9f79-43a0-afa9-910ac2bb6904.png\" style=\"width: 50%;\"></div>\nFigure 1: Can we train a model that in-context learns a function class (here linear functions)? We train Transformers by repeatedly sampling a random function f from that class, as well as random inputs x1, . . . , xk and training the model to predict each f (xi) given the prompt x1, f (x1), . . . , xi\u22121, f (xi\u22121), xi (wrt squared loss). Then, during inference, we evaluate the model\u2019s ability to predict accurately on new, unseen functions.\n(1)\nTransformers can in-context learn linear functions. We show empirically that we can train a standard Transformer from scratch to in-context learn the class of linear functions, with respect to the input distribution DX being an isotropic Gaussian in 20 dimensions, and DF being the distribution over linear functions with weight vectors drawn from an isotropic Gaussian (the model was trained on prompts generated from the same distributions DX and DF). Specifically, the trained model achieves error comparable to the optimal least squares estimator, suggesting that it encodes an effective learning algorithm, at least for the distribution used to generate the training prompts.\nweight vectors drawn from an isotropic Gaussian (the model was trained on prompts generated from the same distributions DX and DF). Specifically, the trained model achieves error comparable to the optimal least squares estimator, suggesting that it encodes an effective learning algorithm, at least for the distribution used to generate the training prompts. Generalization to out-of-distribution prompts. To understand the extent to which the trained model encodes an algorithm that works beyond the training distribution, we consider in-context learning under two types of distribution shifts: (a) a shift between the prompts encountered during training and inference (e.g., training on prompts without any noise in the in-context example outputs but testing with noisy outputs), (b) a shift between the in-context examples and the query input during inference (e.g., in-context examples lie in one orthant and the query input lies in another). We find that the performance of our model is quite robust to such shifts, indicating that it has learned to perform linear regression with some generality. More complex function classes. We also consider the function classes of 3-sparse linear functions, two-layer ReLU neural networks with 100 hidden units, and decision trees of depth 4, all with 20 dimensional inputs. We show that we can again train Transformer models that can in-context learn these classes (with respect to isotropic Gaussian inputs and appropriately defined distributions over functions). For sparse linear functions, the trained model is able to exploit sparsity, obtaining performance better than least squares and comparable to Lasso. For neural networks, the corresponding model is able to obtain performance comparable to neural networks of the same architecture trained using gradient descent on in-context examples. Moreover, it is also able to in-context learn linear functions. For decision trees, the trained model can learn unseen trees with as few as 100 in-context examples, whereas greedy learning and tree boosting algorithms are unable to achieve competitive performance (for the distribution of prompts studies here). Note that learning these function classes requires involved algorithms (e.g., gradient descent with the Lasso objective), and our results show that Transformers can encode algorithms with similar performance in a single forward pass. Role of model capacity and problem dimension. Finally, we explore how the ability of Transformers to in-context learn linear functions scales with model capacity and problem dimensionality. We find that increasing the capacity of the model improves performance significantly, and also allows the model to incontext learn higher-dimensional functions. Moreover, increasing the capacity often significantly improves performance with distribution shifts, even when the absolute improvement in the standard error is small.\n# 2 Training models for in-context learning\nWe now describe a general methodology for training a model that can in-context learn a function class F with respect to a distribution DF over functions, and DX over inputs. To do so, we start by constructing random training prompts as follows. For each prompt, we first sample a random function f from the class according to DF, then create a set of random inputs x1, . . . , xk+1 drawn independently from DX , and finally evaluate f on these inputs to produce the prompt P = (x1, f (x1), . . . , xk+1, f (xk+1)). For example, in the case of linear functions, inputs could be drawn from the isotropic Gaussian distribution N(0, Id), and a random function chosen by sampling weight vector w from N(0, Id) and setting f (x) = w\u22a4x. Now, given such prompts, we train a model to predict f (xi) for a given xi based on a set of preceding in-context examples. Concretely, let Pi denote the prompt prefix containing i in-context examples (the first i input-output pairs) and the (i + 1)th input: Pi = (x1, f (x1), x2, f (x2), . . . , xi, f (xi), xi+1). Then, we train a\nwhere \u2113(\u00b7, \u00b7) is an appropriately chosen loss function. Below, we describe how this general methodology can be implemented for a concrete model family (see Appendix A for additional details).\nModel structure. We use a decoder-only Transformer architecture [Vaswani et al., 2017] from the GPT-2 family [Radford et al., 2019]. Our model consists of 12 layers, 8 attention heads, and a 256-dimensional embedding space (9.5M parameters). This architecture takes as input a sequence of vectors in its embedding space and predicts the next vector in the sequence within the same space (in language modeling, these vectors correspond to input tokens). We apply this architecture to our prompt format of (x1, f (x1), . . . , xk+1, f (xk+1)) as follows. We map each prompt output f (xi) to the same dimension as prompt inputs xi by appending zeros, and map the prompt inputs and outputs into the latent embedding space of the Transformer through a (learnable) linear transformation. We then use another (learnable) linear transformation to map the vector predicted by the model to a scalar. Note that the Transformer architecture allows us to compute the prediction (M\u03b8(Pi)) for all prompt prefixes in a single forward pass. Training. We train the model according to the training objective in (2) using squared error as the loss function. We do so by sampling a batch of random prompts at each training step and then updating the model through a gradient update (we use a batch size of 64 and train for 500k total steps). This training is done from scratch, that is, we do not fine-tune a pre-trained language model, nor do we train on actual text. Curriculum learning. Many natural function classes contain functions of varying complexity. We exploit this by training our model using a curriculum [Bengio et al., 2009, Elman, 1993, Sanger, 1994, Wu et al., 2020], where we train on a simpler distribution of functions in the beginning (e.g., linear functions with weight vectors restricted to a low-dimensional subspace) and gradually increase the function complexity. This speeds up training drastically, often allowing us to train models that would be significantly more expensive to train without a curriculum (see Section 6 for details).\nTraining. We train the model according to the training objective in (2) using squared error as the loss function. We do so by sampling a batch of random prompts at each training step and then updating the model through a gradient update (we use a batch size of 64 and train for 500k total steps). This training is done from scratch, that is, we do not fine-tune a pre-trained language model, nor do we train on actual text.\nCurriculum learning. Many natural function classes contain functions of varying complexity. We exploit this by training our model using a curriculum [Bengio et al., 2009, Elman, 1993, Sanger, 1994, Wu et al., 2020], where we train on a simpler distribution of functions in the beginning (e.g., linear functions with weight vectors restricted to a low-dimensional subspace) and gradually increase the function complexity. This speeds up training drastically, often allowing us to train models that would be significantly more expensive to train without a curriculum (see Section 6 for details).\n# 3 In-context learning of linear functions\nIn the previous section, we describe a general methodology for training Transformer models to in-context learn a class of functions. Here, we focus on a simple function class\u2014namely linear functions\u2014and study how well models trained using our methodology can in-context learn this class.\nPrompt distribution. We consider the class of linear functions F = \ufffd f | f (x) = w\u22a4x, w \u2208Rd\ufffd , in d dimensions where d = 20. We sample x1, . . . , xk, xquery, and w independently from the isotropic Gaussian distribution N(0, Id). We then compute each yi = w\u22a4xi and construct the prompt as P = (x1, y1, x2, y2, . . . , xk, yk, xquery).\nBaselines. To contextualize the performance of our trained model, we compare it to other learning algorithms: (a) the least squares estimator, computing the minimum-norm linear fit to the in-context examples (xi, yi), (b) n-Nearest Neighbors, averaging the yi values for the n nearest neighbors of xquery, (c) averaging the values yixi to estimate w and compute the inner product of this estimate with xquery. Least squares is the optimal estimator for this problem and thus serves as a lower bound to the best error one can achieve. The other two baselines are consistent (but sub-optimal) estimators that are easier to compute and thus provide an estimate of the performance achieved by simple approaches. See Appendix A.3 for more details.\n# 1 Transformers can in-context learn linear functions\nWe show the in-context learning ability of the resulting model along with the relevant baselines in Figure 2. The trained Transformer is able to in-context learn the class of linear functions with respect to the prompt distribution specified above, performing comparably to the optimal least squares estimator for any number of in-context examples considered. While the simpler baselines achieve non-trivial error, they are far worse, indicating that the trained model encodes a more complex algorithm.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3eb1/3eb11ed2-6a89-4a13-9bea-a5616466dbd7.png\" style=\"width: 50%;\"></div>\nFigure 2: Evaluating the trained Transformer on in-context learning linear functions. We plot the normalized squared error of the Transformer ((M(P) \u2212w\u22a4xquery)2/d), along with the relevant baselines, as a function of the number of in-context examples. Transformer\u2019s error decreases at a rate comparable to least squares. When the number of in-context examples reaches the problem dimension d (here 20), least squares achieves 0 error while the Transformer achieves an error of 0.02, improving to 0.0006 at 2d in-context examples. While the simple baselines obtain better-than-trivial error (zero estimator, dashed line), their performance is relatively poor. (Error averaged over 1280 prompts. 90% confidence intervals over 1000 bootstrap trials.)\nCan memorization of training prompts explain model performance? Note that the probability of the model encountering a training prompt similar to the one used for testing is astronomically low\u2014the prompt inputs alone lie in a 800-dimensional space when predicting with 2d in-context examples (d = 20). Moreover, even considering the possibility that the model encountered a similar weight vector during training cannot explain its performance. That is, the model encounters 32 million random weight vectors during training and even using the best of these vectors would lead to an expected error of around 0.2 (computed empirically, see Appendix B.7 for details). However, the model is able to achieve an error of less than 0.001 for a prompt with 2d in-context examples. Further, in Section 6, we show that the model is able to obtain a similar error even when trained on prompts generated using only 10, 000 distinct weight vectors, in which case the best weight vector seen during training would yield an even worse error of around 0.5. Thus, the model cannot be relying on memorization of training prompts or weight vectors, and instead encodes an algorithm capable of in-context learning linear functions that are very different from those seen during training.\n# 3.2 What functions is the model learning in-context?\nRecall that the goal of our model is: given the prompt P = (x1, w\u22a4x1, . . . , xk, w\u22a4xk, xquery), output w\u22a4xquery. Thus, if we fix the prefix given by the k in-context examples, we can view the output of the model as a function \u02c6fw,x1:k(xquery), that approximates w\u22a4xquery. When k < d (fewer in-context examples than dimensions), the ground truth cannot be recovered perfectly and the ideal model should approximate (projx1:k(w))\u22a4xquery, where projx1:k(w) is the projection of w onto the subspace spanned by x1, . . . , xk. Here, we will evaluate how accurately the model approximates this.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0e68/0e68332a-3043-41c9-94e3-4b3b502d4b38.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) function visualizations</div>\nFigure 3: Understanding the prefix-conditioned function. (a) We plot the model prediction as we fix the in-context examples and vary the query input along a random direction (for three random prompts). The shaded regions denote the intervals in which the norm of a randomly training input lies with probability 0.99. When the scale of the query input is close to this range, the model prediction is close to the ground truth linear function (or its projection to the space of in-context inputs when k < d). (b) We compute the gradient of the model prediction with respect to the query input, and plot its (normalized) inner product with the true w and projected w, averaged over 1280 random prompts. The gradient aligns almost perfectly with w when k \u2265d, and with projected w for all k, indicating that the model locally aligns with the ground truth.\nVisualizing along a random direction. For a randomly sampled fixed prefix, we visualize \u02c6fw,x1:k(xquery) as we vary the query input along a random direction x (Figure 3a). That is, we pick a random unit vector x, and evaluate \u02c6fw,x1:k(\u03bbx) as we vary \u03bb, the distance of the query input from origin. We observe that \u02c6fw,x1:d(\u03bbx) and \u02c6fw,x1:2d(\u03bbx) closely match the ground truth and \u02c6fw,x1:d/2(\u03bbx) matches the projected ground truth, when the distance from origin is not too large compared to the norm of a typical randomly sampled input. In fact, in Appendix B.1, we show that the model is quite robust to scaling the query input: the error doesn\u2019t increase much as we scale up the query input by a factor of up to 2, or scale down by a factor of up to 16, and degrades slowly after that.\nLocal correctness. So far, we have seen that the model is able to make predictions close to the ground truth for randomly drawn query inputs and in-context examples. We will now turn our attention to the local change of \u02c6f around xquery by considering the gradient of the function \u02c6fw,x1:k(xquery) with respect to xquery (our model is fully differentiable so we can compute the gradient directly). Since \u02c6f computed by the model should ideally approximate projx1:k(w)\u22a4x, this gradient should lie in the direction of the projected ground truth projx1:k(w). In Figure 3b, we show the inner product between the gradient and projx1:k(w) (both normalized), averaged over 1280 random prompts, and observe that they align almost perfectly. Since projx1:k(w) = w almost surely when k \u2265d, we observe that the gradient also aligns with w perfectly in this regime. Thus the model is locally correct with respect to changes in the query input.\n# 4 Extrapolating beyond the training distrib\nIn the previous section, we demonstrated that we can train a model to in-context learn linear functions with respect to the distribution of prompts encountered during training. That is, we evaluate the in-context learning ability of the model with respect to distributions DX and DF that were also used to train the model. Here, we evaluate the in-context learning performance of our model on prompt distributions different from the one used for training. Our overarching goal here is to better understand the learning algorithm encoded by our model by analysing how it responds to different prompt distributions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c6d0/c6d0fb60-7b7c-4dde-9153-e1351e4ab11b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) gradients</div>\nFormally, we will refer to the distribution of functions used during training as Dtrain F and the corresponding distribution of prompt inputs as Dtrain X . Then, during inference, functions are sampled from a (potentially different) distribution Dtest F , while prompt inputs from a distribution Dtest X . Moreover, deviating again from our analysis so far, we also consider a separate distribution Dtest query, from which the query input is sampled, potentially dependent on the rest of the in-context inputs x1, . . . , xk (which are still sampled from Dtest X ). Within this framework, we consider the same model as last section, and evaluate its performance on prompts that deviate from those encountered during training, either by 1. sampling prompt inputs or functions from a different distribution, that is Dtrain X /F \u0338= Dtest X /F or 2. introducing a mismatch between in-context examples and the query input, that is Dtest query \u0338= Dtest X . We describe each such prompt structure below and present a subset of the results in Figure 4 (see Appendix B.2 for additional details and full results). Overall, the model performs reasonably accurate in-context learning with respect to these prompt distributions, indicating that it has indeed learnt to perform linear regression to some generality. Recall that we generate a training prompt P = (x1, wTx1, . . . , xk, wTxk, xquery) by drawing the prompt inputs (xi and xquery), and the weight vector (w) i.i.d. from N(0, Id), with d = 20. For all the settings below, except prompt scaling, we normalize the inputs so that their expected squared norm is equal to that of inputs encountered during training.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2786/278617f8-45f5-4996-bfcd-6c906ff57b3a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Noisy linear regression</div>\n<div style=\"text-align: center;\">(a) Skewed covariance</div>\nFigure 4: In-context learning on out-of-distribution prompts. We evaluate the trained model on prompts that deviate from those seen during training by: (a) sampling prompt inputs from a non-isotropic Gaussian, (b) adding label noise to in-context examples, (c) restricting in-context examples to a single (random) orthant. In all cases, the model error degrades gracefully and remains close to that of the least squares estimator, indicating that its in-context learning ability extrapolates beyond the training distribution.\nSkewed covariance. We sample prompt inputs from N(0, \u03a3) where \u03a3 is a skewed covariance matrix with eigenbasis chosen uniformly at random and ith eigenvalue proportional to 1/i2. The model matches the performance of least squares until k = 10, mimicking the sharp drop in the error in this regime, but its error plateaus afterwards (see Figure 4a). Thus, it is not perfectly robust to this distribution mismatch but still does relatively well, achieving less than half the error of the nearest neighbor baseline in most cases.\nSkewed covariance. We sample prompt inputs from N(0, \u03a3) where \u03a3 is a skewed covariance matrix with eigenbasis chosen uniformly at random and ith eigenvalue proportional to 1/i2. The model matches the performance of least squares until k = 10, mimicking the sharp drop in the error in this regime, but its error plateaus afterwards (see Figure 4a). Thus, it is not perfectly robust to this distribution mismatch but still does relatively well, achieving less than half the error of the nearest neighbor baseline in most cases. Low-dimensional subspace. We sample prompt inputs from a random 10 dimensional subspace. In this case, the model achieves low error after 10 in-context examples, closely matching the behavior of the optimal least squares estimator (the model achieves an error of 0.036, 0.0014, and 0.00057 at 10, 20, and 40 in-context examples respectively)\u2014see Appendix Figure 8b. Crucially, unlike the training prompts, when k is between 10 and 20, the prompt inputs are linearly dependent, and a model achieving low error in this regime indicates that it encodes a valid orthogonalization procedure for these inputs.\nLow-dimensional subspace. We sample prompt inputs from a random 10 dimensional subspace. In this case, the model achieves low error after 10 in-context examples, closely matching the behavior of the optimal least squares estimator (the model achieves an error of 0.036, 0.0014, and 0.00057 at 10, 20, and 40 in-context examples respectively)\u2014see Appendix Figure 8b. Crucially, unlike the training prompts, when k is between 10 and 20, the prompt inputs are linearly dependent, and a model achieving low error in this regime indicates that it encodes a valid orthogonalization procedure for these inputs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2845/28451497-1afa-4dbe-bb89-a16e864ea7a0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Different orthants</div>\nNoisy linear regression. We add noise to each prompt output, that is, the ith output is equal to wTxi + \u03f5i where \u03f5i \u223cN(0, 1). The trained model closely tracks the performance of least squares when the number of in-context examples is not close to the input dimension 20 (see Figure 4b). Interestingly, the model also exhibits the double descent error curve [Belkin et al., 2019] that is known to manifest for the least squares estimator [Nakkiran, 2019]. Note that in this noisy setting, the optimal estimator corresponds to solving least squares with appropriate \u21132-regularization. However, since the model was trained on noiseless data, we cannot expect it to learn this.\nQuery input orthogonal to in-context inputs. We sample the query input from the subspace orthogonal to the subspace spanned by in-context example inputs. Here, there is no information relevant to the query input in the in-context examples and thus the model would ideally predict something close to 0 to minimize the error. Indeed, the model outputs such a prediction, achieving an error close to 1 (Appendix Figure 8d).\nQuery input matches an in-context example. We choose the query input to match one of the in-context examples inputs chosen uniformly at random. In this case, the model achieves errors 0.001, 0.001, 0.0005 for 10, 20, 40 examples respectively thus making close to the correct prediction, without being affected by the additional in-context examples present (Appendix Figure 8e).\n# 5 More complex function classes\nWe now turn our attention to in-context learning for more complex function classes, namely sparse linear functions, decision trees, and two-layer ReLU neural networks. Here, we are back in the setting where the distribution of prompts during inference is same as that during training (except the setting of neural networks where we evaluate on linear functions as well). The overall methodology remains the same: we sample random functions from these families and train a Transformer from scratch to approximate these functions given in-context examples. (See Appendix A.3 for more details and baselines.)\nSparse linear functions. First, we consider functions of the form f (x) = w\u22a4x where w \u2208Rd and has exactly s non-zero coordinates. To sample a prompt P = (x1, f (x1), . . . , xk, f (xk), xquery), we draw prompt inputs xi and xquery, and a weight vector w from N(0, Id), and then zero out all but s coordinates of w uniformly at random. We choose d = 20 and s = 3. In this setting, the least squares estimator is no longer\noptimal\u2014one can perform better by leveraging the weight vector sparsity. One estimator that leverages sparsity is Lasso [Tibshirani, 1996], which involves solving the least squares objective with an \u21131-norm regularizer for the weight vector. We plot the performance of our model in Figure 5a, and observe that it is also able to leverage sparsity, nearly matching the performance of Lasso. Our model achieves errors 0.58 and 0.09 while Lasso achieves errors 0.62 and 0.08 for k = 5 and 10 respectively. Note that, unlike least squares, Lasso does not have a closed form expression and involves iterative minimization of the regularized objective, yet the Transformer is able to achieve comparable performance in a single forward pass.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a84/7a847b2b-5959-4ec3-909c-0d7d8362fc01.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f7b/8f7b2365-f132-40e1-9d63-43e5f5f73463.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Sparse linear functions</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9844/98442666-ff1a-4f77-a21f-441f520fd7ef.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) 2-layer NN</div>\nFigure 5: Training a Transformer to in-context learn more complex function classes. (a) A Transformer trained on prompts generated using sparse linear functions can in-context learn this class, with error decreasing at a rate similar to Lasso, and significantly better than minimum norm least squares. (b) A Transformer trained on prompts generated using random decision trees can in-context learn this class, with much better performance than greedy tree learning or tree boosting. (c) A Transformer trained on prompts generated using random 2-layer ReLU neural networks can in-context learn this class. The error decreases at a rate similar to the baseline which involves training a neural network using a variant of gradient descent with in-context examples as the training data. (d) The same model (from (c)) can in-context learn the class of linear functions. The error decreases at a rate slower than least squares, but comparable to a neural network trained using a variant of gradient descent. In all cases, the errors are normalized so that the trivial zero estimator achieves an error of 1 (dashed line).\nDecision trees. Next, we consider the class of depth 4 decision trees with 20 dimensional inputs. A function f in this class is represented by a full binary tree (with 16 leaf nodes) where each non-leaf node is associated with a coordinate, and each leaf node is associated with a target value. To evaluate f on an input x, we traverse the tree starting from the root node, and go to the right child if the coordinate associated with the current node is positive and go to the left child otherwise (that is, the threshold at each node is 0). f (x) is given by the value associated with the leaf node reached at the end. To sample a random prompt P = (x1, f (x1), . . . , xk, f (xk), xquery), we draw prompt inputs xis and xquery from N(0, Id), and f corresponds to a tree where the coordinates associated with the non-leaf nodes are drawn uniformly at random from\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/70d7/70d73d73-b4e2-42dd-b884-ee27e388c76c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aee0/aee05b9c-4a11-4a2c-ad0a-46a35c7b83c1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) 2-layer NN, eval on linear functions</div>\n{1, 2, . . . , d} and the values associated with the leaf nodes are drawn from N(0, 1). In Figure 5b, we show that Transformers can be trained to in-context learn this class, with performance much better than greedy tree learning and boosting (via XGBoost [Chen and Guestrin, 2016]). With k = 100 in-context examples, the Transformer achieves an error of 0.12 whereas greedy learning achieves an error of 0.80 and XGBoost achieves an error of 0.62. Since the decision trees in our function class predict solely based on the sign of each coordinate of xi, we also consider a baseline where we provide the greedy learning and XGBoost algorithms with the signs of each xi instead. This significantly improves their performance\u2014at 100 in-context examples, greedy achieves an error of 0.50 and XGBoost an error if 0.31\u2014but they still perform much worse than the trained Transformer Note that, in general, we do not have a good understanding of the space of efficient algorithms for learning decision trees, and the conditions under which known heuristics work [Blanc et al., 2021, Brutzkus et al., 2020]. At the same time, we found that Transformers can be trained to directly discover such an algorithm for the prompt distribution we considered. This suggests an intriguing possibility where we might be able to reverse engineer the algorithm encoded by a Transformer to obtain new sample efficient algorithms for existing learning problems.\nTwo-layer ReLU neural networks. Finally, we consider the class of two layer ReLU neural networks containing functions of the form f (x) = \u2211r i=1 \u03b1i \u03c3(w\u22a4 i x), where \u03b1i \u2208R, wi \u2208Rd and \u03c3(\u00b7) = max(0, \u00b7) is the ReLU activation function. To draw a random prompt P = (x1, f (x1), . . . , xk, f (xk), xquery), we sample prompt inputs xis and xquery from N(0, Id), along with network parameters ais and wis from N(0, 2/r) and N(0, Id) respectively. We set the input dimension d to 20 and the number of the hidden nodes r to 100. In Figure 5c, we show that Transformers can be trained to in-context learn this class of functions. In fact, the Transformer performs comparably to the baseline which involves training a two-layer neural network of the same architecture on in-context examples using Adam [Kingma and Ba, 2014], a variant of gradient descent (see Appendix A.3 for details). Specifically, for k = 100 in-context examples, both the Transformer and the neural network trained on in-context examples achieve an error of 0.17. Moreover, the model trained to in-context learn two-layer neural networks is also able to in-context learn linear functions (for which it is not explicitly trained), albeit with a rate slower than least squares, but comparable to a neural network trained on in-context examples generated using a linear function (Figure 5d). For k = 20, 50, and 100 in-context examples respectively, the Transformer achieves error 0.34, 0.05, and 0.01, and the two-layer network achieves error 0.37, 0.04, and 0.003 (the least squares estimator achieves error 0 for k \u226520).\n# 6 Investigating what matters for in-context learning\nWe now return to the setting of training models to in-context learn linear functions and explore differen factors that lead to successful in-context learning.\nProblem Dimension and Capacity. In Section 3 and 4, we saw that Transformer models can be trained to in-context learn 20-dimensional linear functions accurately and relatively robustly. To explore the interplay between problem dimensionality and capacity, we also consider models with fewer parameters (see Appendix A.1) and train each architecture on {10, 30, 40, 50}-dimensional problems. In Figure 6, we plot the model error with 2d in-context examples as we vary the problem dimension d and the model capacity. In the standard setting, i.e., when the training and inference time prompt distributions are the same, we observe that the error decreases as we increase the capacity or reduce the problem dimensionality (see Figure 6a). Thus, model capacity helps perform accurate in-context learning. For out-of-distribution prompts, we observe that the settings where the input covariance is skewed or where in-context example inputs and query inputs lie in different orthants are particularly challenging, especially for higher dimensional problems. However, the error decreases considerably (in most cases) as we increase the model capacity, even when absolute decrease in the standard error is small (see Figure 6b and 6c). See Appendix B.3 for additional plots.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd82/fd823a9a-1437-49f1-aa55-31bd388d570a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Different orthants</div>\n<div style=\"text-align: center;\">(a) Standard</div>\nFigure 6: Understanding the effect of model capacity and problem dimension on in-context learning performance for in-distribution (a) and out-of-distribution (b,c) prompts. We train Transformers to in-context learn linear functions and plot the error with 2d in-context examples as we vary problem dimension d and model capacity. Capacity helps with in-context learning in most cases, especially on out-of-distribution prompts (even when the absolute gains in the in-distribution setting are small). We train 3 models in each case with different random seeds, and show the median error (solid lines), and the minimum and maximum errors (shaded region). (See Appendix B.4 for training variance analysis.)\nCurriculum. We train our models using curriculum learning. That is, we initially draw the prompt inputs from a fixed 5 dimensional subspace (by setting some of the coordinates to 0) with prompt length 11 (number of input-output pairs), and increase the subspace dimension by 1 and prompt length by 2 every 2, 000 training steps, until the subspace dimension reaches the ambient dimension d and prompt length reaches 2d + 1 (see Appendix A.2 for details). This process can also be viewed as gradually increasing the complexity of the function class. This speeds up training drastically, especially for higher dimensional problems: for dimension 50, the loss barely decreases through the 500k training steps without curriculum but reaches close to the optimum with curriculum. For the 20 dimensional problem where we were able to train the model without curriculum within the training (step count) budget, we did not observe any qualitative difference in accuracy or robustness compared to the model trained with curriculum. We include plots comparing the speed and accuracy of training with and without curriculum in Appendix B.5. Notably, when training Transformers without curriculum, there is an initial\u2014relatively long\u2014period in training where the loss does not decrease, followed by a period of sharp decrease. The length of this period varies with training randomness and seems to increase on average with problem dimension. Understanding the model just before and after this transition moment is a promising future direction, which can give insights into the emergence of in-context learning. Interestingly, Olsson et al. [2022] observe a similar jump in the in-context learning ability of a language model which they attribute to the formation of \u201cinduction heads\u201d. Number of distinct prompts or functions seen during training. To estimate the amount of training data required for in-context learning, we perform two ablation studies. In the first study, we limit the number of distinct prompts seen during training. That is, we create a set of np randomly generated prompts (as described in Section 2), and sample prompts from this set during training (here, we train without curriculum, as it would introduce additional prompts during the warmup phase). In the second study, we only limit the number of distinct functions used for training. That is we create a set of nw randomly chosen vectors (corresponding to nw linear functions) and sample weight vectors uniformly from that set to generate the training prompts (the inputs are still sampled from N(0, Id) for each training prompt). We find that the amount of training data required is relatively small: non-trivial in-context learning is possible with np = 100k or nw = 1k, and the error drops close to that of the unrestricted model (discussed in Section 3) with np = 1M or nw = 10k (details in Appendix B.6). For context, in Section 3, the model is trained on fresh prompts each step, thus encountering 32M distinct functions and prompts (500k training steps with 64 prompts/batch).\n<div style=\"text-align: center;\">(c) Skewed covariance</div>\n# 7 Related work\nIn-context learning. Since Brown et al. [2020] demonstrated the in-context learning ability of GPT-3, there has been a significant interest in improving and understanding this capability [Liu et al., 2021, Min et al., 2021a, Zhao et al., 2021, Lu et al., 2021b, Rubin et al., 2021, Min et al., 2021b, Chen et al., 2021, Mishra et al., 2021, Lampinen et al., 2022]. The works most relevant to ours are as follows. Xie et al. [2022] propose a Bayesian inference framework explaining how in-context learning works despite formatting differences between training and inference distributions. Razeghi et al. [2022] show that in-context learning for numerical reasoning tasks is better for instances whose terms are more prevalent in training data. Min et al. [2021a] demonstrate tasks where in-context learning works even when the prompt outputs are chosen randomly, questioning to what extent these models are truly learning new tasks on-the-fly, while Rong [2021] gives examples of novel tasks on which these models demonstrate on-the-fly learning ability. Chan et al. [2022] demonstrate that distributional properties such as long-tailedness are crucial for in-context learning on an image-based few-shot dataset. Olsson et al. [2022] and Elhage et al. [2021] consider a different framing of in-context learning, referring to any model behavior that utilizes information in a prompt to make predictions that improve with prompt size. They hypothesize the existence of special circuits inside Transformer models responsible for in-context learning, that can complete prompts by copying previous similar patterns in the prompt sequence. Pesut [2022] and Dinh et al. [2022, Table 16] consider in-context learning for small tabular datasets and learning problems in one and two dimensions, and show that GPT-3 can obtain non-trivial accuracy. Our work contributes to and complements this line of work, by posing in-context learning as a well-defined problem of learning function classes at inference time, and empirically investigating training models that in-context learn simple function classes.\nTransformers. There is a long line of work investigating the capabilities [Vaswani et al., 2017, Dehghani et al., 2018, Yun et al., 2019, P\u00e9rez et al., 2019, Yao et al., 2021, Bhattamishra et al., 2020b, Zhang et al., 2022], limitations [Hahn, 2020, Bhattamishra et al., 2020a], applications [Lu et al., 2021a, Dosovitskiy et al., 2020, Parmar et al., 2018], and internal workings [Elhage et al., 2021, Snell et al., 2021, Weiss et al., 2021, Edelman et al., 2022, Olsson et al., 2022] of Transformer models. Most similar to our work, M\u00fcller et al. [2021] and Nguyen and Grover [2022] demonstrate the ability of Transformer models to solve prediction tasks using the input context, albeit in different settings. M\u00fcller et al. [2021] introduce a \u201cPrior-data fitted transformer network\u201d that is trained to approximate Bayesian inference with priors such as Gaussian processes and Bayesian neural networks, and use it to perform downstream tasks such as tabular dataset classification and few-shot image classification. Nguyen and Grover [2022] introduce Transformer neural processes, building on prior work on neural processes [Garnelo et al., 2018b,a, Kim et al., 2019], and show that they achieve state-of-the art performance on tasks such as image completion and contextual multi-armed bandits. Our work complements these works, focusing on understanding the in-context learning ability of Transformers for various simple function classes and the extent to which this ability extrapolates beyond the training distribution.\nMeta learning. Training a model to perform in-context learning can be viewed as an instance of the more general learning-to-learn or meta-learning paradigm [Schmidhuber, 1987, Naik and Mammone, 1992, Thrun and Pratt, 2012]. Typical approaches from this extensive line of work (see [Hospedales et al., 2020] for a survey) include: training a meta-learner on how to update the parameters of a downstream learner [Bengio et al., 1995, Li and Malik, 2016], learning parameter initializations from which one can quickly train for many downstream tasks [Finn et al., 2017, Ravi and Larochelle, 2017], learning latent embeddings that allow for effective similarity search [Snell et al., 2017]. Most relevant to our setting are approaches that directly take as input examples from a downstream task and a query input and produce the corresponding output [Hochreiter et al., 2001, Mishra et al., 2018, Santoro et al., 2016, Garnelo et al., 2018b,a, Kirsch and Schmidhuber, 2021]. Our work contributes to this line of work, by investigating the learning-to-learn abilities of Transformer models in a well-defined setting.\nData-driven algorithm design. Another line of work aims to discover algorithms that perform well on a distribution of inputs [Horvitz et al., 2001, Xu et al., 2008, Vinyals et al., 2015, Bello et al., 2016, Khalil et al., 2017, Selsam et al., 2018, Schwarzschild et al., 2021] (as opposed to algorithms with guarantees on their worst-case performance). See Balcan [2020] for a survey on advancements on the theoretical foundations of such algorithms. Our work can be viewed as part of this line of work, as we train Transformer models to discover algorithms for different learning problems.\n# 8 Discussion\nIn this work, we formalize and study the question: can we train models that learn different classes of functions in-context? We show that Transformer models trained from scratch can in-context learn the class of linear functions, with performance comparable to the optimal least squares estimator, even under distribution shifts. Moreover, we show that in-context learning is also possible for sparse linear functions, decision trees, and two-layer neural networks; learning problems which are solved in practice with involved iterative algorithms such as gradient descent. At the same time, understanding the implications of our results for language models requires further investigation. A pertinent question regarding the in-context learning capabilities of language models is how they leverage in-context examples [Min et al., 2022]. Our results demonstrate that Transformers can encode complex learning algorithms that utilize in-context examples in a far-from-trivial manner. In fact, this is the case for standard Transformer architectures trained with standard optimization procedures. The extent to which such non-trivial in-context learning behavior exists in large language models is still open, but we believe that our work takes a step towards formalizing and investigating this question. Our work lays the groundwork for several future directions.\nUnderstanding the learning algorithms encoded in Transformers. The models we train are able to perform in-context learning, and are thus themselves encoding learning algorithms. A worthwhile research direction would be to investigate the internal workings of these models and better understand the exact learning algorithms that they encode. Moreover, for settings such as decision trees, we do not have a good understanding of what the optimal learning algorithms are. Nevertheless, in Section 5 we found that Transformers are able to discover sample efficient algorithms when being trained to perform in-context\nlearning. This suggests an intriguing possibility where we might be able to reverse engineer the Transforme to obtain better learning algorithms for such problems.\n# Acknowledgements\nWe thank Niladri Chatterji, Micah Goldblum, Rohith Kuditipudi, Shibani Santurkar, Carmen Strassle, Mirac Sugzun, and Li-Yang Tan for helpful conversations, and anonymous reviewers for helpful comments. SG was funded by a Stanford Interdisciplinary Graduate Fellowship. DT was funded by Open Philanthropy, and partially supported by NSF Award CCF-1813049. GV was supported by NSF Awards CCF-1704417, CCF-1813049, Frontier Award 1804222 and DOE award DE-SC0019205. We performed our experiments on the Stanford NLP cluster.\n# References\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-contextlearning-and-induction-heads/index.html.\n# A Experimental setup\nHere, we provide additional details on our experimental setup.\n# A.1 Model architecture\n<div style=\"text-align: center;\">We use architectures from the GPT-2 family [Radford et al., 2018] as implemented by HuggingFace [Wolf t al., 2020] 3 . Specifically, we consider the following set of configurations.</div>\nModel\nEmbedding size\n#Layers\n#Heads\n(Total parameters)\nTiny\n64\n3\n2\n0.2M\nSmall\n128\n6\n4\n1.2M\nStandard\n256\n12\n8\n9.5M\nWe use the Standard model for the bulk of our experiments and only consider the smaller models for the capacity explorations in Section 6 and Appendix B.3. Since we train on each input once (we sample new inputs at each training step), overfitting to the training data is not an issue. Therefore, we set the Dropout probability to 0. Out of the box, these models take as input a sequence of vectors in embedding space and output a sequence of vectors in the same space. However, the tasks we study are functions from a lower dimensional vector space (e.g., 10-50 dimensions) to a scalar value. Thus, in order to use a prompt such as x1, f (x1), x2, f (x2) . . ., we need to map xis and f (xi)s to vectors in embedding space. We do so by first turning the scalars f (xi) into vectors of the same dimension as xi by appending 0s and then applying a learnable linear transformation to map all these vectors into the embedding space. Finally, we map the model output into a scalar value through a dot product with a learnable vector. We treat the prediction of the model at the position corresponding to xi (that is absolute position 2i \u22121) as the prediction of f (xi). Due to the structure of these models, this prediction only depends on (xj, f (xj)) for j < i and xi. We ignore the model predictions at positions corresponding to f (xi).\n# A.2 Training\nEach training prompt is produced by sampling a random function f from the function class we are training on, then sampling inputs xi from the isotropic Gaussian distribution N(0, Id) and constructing a prompt as (x1, f (x1), . . . , xk, f (xk)). Given a prompt, we obtain model predictions \u02c6yi (meant to approximate f (xi)) for each input, and compute the loss\nAt each training step, we average the loss over a batch of randomly generated prompts (with different functions and prompt inputs), and perform an update step. We use the Adam optimizer [Kingma and Ba, 2014], and train for 500,000 total steps with a batch size of 64. We use a learning rate of 10\u22124 for all function classes and models.\nCurriculum learning. To accelerate training, we start by training on prompt inputs xi lying in a smaller dimensional subspace, and with fewer inputs per prompt, and gradually increase the subspace dimension and number of prompt inputs. Specifically, we zero out all but the first dcur coordinates of xi, sample prompts of size kcur and leave the rest of the training process the same. We use the same schedule for all training runs for the function classes of linear functions and sparse linear functions, starting with dcur = 5, kcur = 11, and increasing dcur and kcur by 1 and 2 respectively, every 2000 steps, until dcur = d, kcur = 2d + 1. We use a slightly different schedule for 2 layer neural networks and decision trees as we want prompts with more\ninputs for these function classes. For these classes, we start with dcur = 5, kcur = 26, and increase dcur and kcur by 1 and 5 respectively, every 2000 steps, until dcur = d, kcur = 5d + 1. Overall, with curriculum, a training prompt (x1, f (x1), . . . , xkcur, f (xkcur) is generated by sampling a random function f from the function class, drawing inputs xi by sampling i.i.d. from N(0, Id) and zeroing out all but the first dcur coordinates. Given model predictions \u02c6yi, the loss is given by\nSampling random functions. For the class of linear functions, we sample random function f (x) = w\u22a4x by drawing w \u223cN(0, Id). For our main setting (Section 3 and 4), we set d = 20. For the class of two-layer neural networks, we sample f (x) = \u2211r i=1 \u03b1i\u03c3(w\u22a4 i x), where \u03b1is and wis are drawn i.i.d. from N(0, 2/r) and N(0, Id) respectively. We set d = 20 and r = 100. For the class of k-sparse linear functions, we sample f (x) = w\u22a4x by drawing w \u223cN(0, Id) and zeroing out all but k coordinates of w chosen uniformly at random from the first dcur coordinates (as defined in the curriculum learning description above). We set d = 20 and k = 3. For the class of decision trees, the random function f is represented by a decision tree of depth 4 (with 16 leaf nodes), with 20 dimensional inputs. Each non-leaf node of the tree is associated with a coordinate selected uniformly at random from {1, 2, . . . , d}, and each leaf node is associated with a value drawn randomly from N(0, 1). To evaluate f on an input x, we traverse the tree starting from the root node, and go to the right child if the coordinate associated with the current node is positive and go to the left child otherwise. f (x) is given by the value associated with the leaf node reached at the end.\nSampling random functions. For the class of linear functions, we sample random function f (x) = w\u22a4x by drawing w \u223cN(0, Id). For our main setting (Section 3 and 4), we set d = 20. For the class of two-layer neural networks, we sample f (x) = \u2211r i=1 \u03b1i\u03c3(w\u22a4 i x), where \u03b1is and wis are drawn i.i.d. from N(0, 2/r) and N(0, Id) respectively. We set d = 20 and r = 100. For the class of k-sparse linear functions, we sample f (x) = w\u22a4x by drawing w \u223cN(0, Id) and zeroing out all but k coordinates of w chosen uniformly at random from the first dcur coordinates (as defined in the curriculum learning description above). We set d = 20 and k = 3. For the class of decision trees, the random function f is represented by a decision tree of depth 4 (with 16 leaf nodes), with 20 dimensional inputs. Each non-leaf node of the tree is associated with a coordinate selected uniformly at random from {1, 2, . . . , d}, and each leaf node is associated with a value drawn randomly from N(0, 1). To evaluate f on an input x, we traverse the tree starting from the root node, and go to the right child if the coordinate associated with the current node is positive and go to the left child otherwise. f (x) is given by the value associated with the leaf node reached at the end. Computational resources. We train using a single NVIDIA GeForce RTX 3090 GPU and most training runs take 5-20 hours depending on model size and context length. For instance, for the class of linear functions, training the standard model takes 17 hours for d = 50, 7 hours for d = 20 and 5.5 hours for d = 10. For decision trees, training the standard model takes 17 hours. The time it takes for decision trees and 50 dimensional linear functions is higher due to larger context lengths (we train for d dimensional linear functions with 2d + 1 input-output pairs per prompt).\nComputational resources. We train using a single NVIDIA GeForce RTX 3090 GPU and most training runs take 5-20 hours depending on model size and context length. For instance, for the class of linear functions, training the standard model takes 17 hours for d = 50, 7 hours for d = 20 and 5.5 hours for d = 10. For decision trees, training the standard model takes 17 hours. The time it takes for decision trees and 50 dimensional linear functions is higher due to larger context lengths (we train for d dimensional linear functions with 2d + 1 input-output pairs per prompt).\n# A.3 Baselines\nLeast squares. Minimum norm least squares is the optimal estimator for the linear regression problem Given a prompt P = (x1, y1, . . . , xk, yk, xquery), let X be a k \u00d7 d matrix with row i given by xi, and let y be a k dimensional vector with the ith entry yi. Set \u02c6wT = X+y, where X+ denotes the Moore-Penrose pseudoinverse of X. The estimator predicts M(P) = \u02c6wTxquery.\nAveraging estimator. This corresponds to M(P) = \u02c6wTxquery where \u02c6w = 1 k \u2211k i=1 xiyi. This estimator is consistent (yet sub-optimal) when xis are drawn from N(0, Id). Unlike least squares, this estimator does not involve an inverse computation, and might be easier for a model to encode.\nNearest neighbors. This corresponds to setting M(P) = 1 n \u2211i\u2208S yi. Here, S is the set of indices of the n nearest neighbors of xquery among x1 to xk. For k < n, we average over all the yis from 1 to k, and for k = 0, we set M(P) = 0. We consider the nearest neighbors baselines as it might be easier for a Transformer model to encode using self-attention compared to least squares.\nWe try different values of \u03b1 \u2208{1, 10\u22121, 10\u22122, 10\u22123, 10\u22124}, and report the best solution (achieving the smallest error with 10 in-context examples) corresponding to \u03b1 = 10\u22122. To solve the optimization problem, we use the Lasso implementation from Scikit-learn [Pedregosa et al., 2011] 4.\nGreedy Tree Learning. We use this baseline for the class of decision trees. This corresponds to greedily learning a decision tree using the in-context examples, and using it to classify the query input. To construct the tree, at each node (starting from a root node), we choose a coordinate for partitioning the examples into two sets, so as to minimize the variance of yis in each set, averaged across the two sets. The value associated with a leaf node is the average yi value of the examples belonging to it. We use Scikit-learn\u2019s decision tree regressor [Pedregosa et al., 2011] 5 implementation for this, with all the arguments set to their default value except the max_depth argument which is set to 2. We considered values {1, 2, 3, 4, 5, 6, unbounded} for the maximum depth and chose the value that performs best at 100 in-context examples which was 2 (which differs from the decision trees sampled from the function class which have depth 4). We also considered a baseline where we learn this tree using only the signs of each xi coordinate\u2014after all, the decision tree we are trying to learn depends only on the signs of xi. In this case, we found the optimal depth to be 4. Tree boosting. For the class of decision trees, we also consider a tree boosting baseline that corresponds to learning an ensemble of decision trees (see Friedman [2001] for a description of the general framework). Specifically, we use the XGBoost library [Chen and Guestrin, 2016] 6, an implementation commonly used for a wide range of real-world machine learning tasks. We performed a hyperpameter search by considering {1, 2, 5, 10, 50, 100, 200, 400} estimators in the ensemble (equivalent to number of boosting rounds), a learning rate of {0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}, and a maximum depth of {1, 2, 3, 4, 6, 10, 16}. In general, we found the performance of the learning algorithm to be quite robust. We chose the hyperparameters obtaining the best performance with 100 training examples, corresponding to 50 estimators, a maximum depth of 4, and a learning rate f 0.1. We found these hyperparameters to also be optimal when learning based on the signs of each xi.\nLearning neural networks with gradient descent. We use this baseline for the class of two-layer neural networks (Section 5). This corresponds to training a two-layer neural network on the in-context examples, and outputting its prediction on the query point. That is, M(P) = \u02c6f (xquery), where\nHere, \u03c3(\u00b7) is the ReLU activation. We find parameters \u02c6\u03b1i, \u02c6wi by minimizing the squared error of the prediction for the in-context examples\nHere, \u03c3(\u00b7) is the ReLU activation. We find parameters \u02c6\u03b1i, \u02c6wi by minimizing the squared error of the prediction for the in-context examples\n\ufffd \ufffd using the Adam optimizer. We use a batch size of 10 (we use full batch when the number of in-context examples is less than 10) with 5000 optimization steps, and set r = 100. We use a learning rate of 5 \u00b7 10\u22123 in the case when the data is generated using a neural network, and a learning rate of 5 \u00b7 10\u22122 when the data is generated using a linear function. We consider the setting with 100 in-context examples, and do a hyperparameter grid search over learning rate \u2208{5 \u00b7 10\u22124, 5 \u00b7 10\u22123, 5 \u00b7 10\u22122, 5 \u00b7 10\u22121, 5}, r \u2208{100, 400}, batch size \u2208{10, 100}, optimization algorithm \u2208{adam, sgd}. All the hyperparameter settings in this grid led to a similar or worse performance compared to the hyperparameter setting we choose.\n# B Additional experimental results\n# B.1 Robustness to query scale\nIn Figure 7, we show that the trained model is quite robust to scaling the query input (while keeping the in-context examples fixed): the error does not increase much as we scale up the query input by a factor of up to 2, or scale down by a factor of up to 16, and degrades slowly after that.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c13d/c13dde3e-3f12-4af3-8f30-b19b357be038.png\" style=\"width: 50%;\"></div>\nFigure 7: Robustness to the scale of query input. For a fixed set of in-context examp error as we scale the query input by a scalar.\n<div style=\"text-align: center;\">Figure 7: Robustness to the scale of query input. For a fixed set of in-context examples, we measure the model\u2019 error as we scale the query input by a scalar.</div>\n# B.2 Out-of-distribution prompts\nHere, we describe the structure of our out-of-distribution prompts (cf. Section 4), and show the corresponding plots (Figure 8). To avoid conflating factors, we normalize the prompt inputs such that their expected norm is equal to the expected norm of inputs during training and investigate the role of scaling these inputs separately. We summarize how these prompts deviate from those seen during training in the table below.\nPrompting strategy\nDtrain\nX\n\u0338= Dtest\nX\nDtrain\nF\n\u0338= Dtest\nF\nDtest\nquery \u0338= Dtest\nX\nSkewed covariance\n\u2713\nd/2-dimensional subspace\n\u2713\nScale inputs\n\u2713\nNoisy output\n\u2713\nScale weights\n\u2713\nDifferent Orthants\n\u2713\n\u2713\nOrthogonal query\n\u2713\nQuery matches example\n\u2713\nSkewed covariance. (Figure 8a) We sample inputs from N(0, \u03a3) where \u03a3 is a skewed covariance matrix with eigenbasis chosen uniformly at random and ith eigenvalue proportional to 1/i2. Low-dimensional subspace. (Figure 8b) We sample prompt inputs from a random d/2 dimensional subspace. That is, we pick a random d/2 dimensional subspace, and draw the prompt inputs from an isotropic Gaussian distribution restricted to this subspace. As a result, it is possible to achieve zero error after d/2 in-context examples. Prompt scale. (Figure 9) We consider the setting where the prompt scale between training and inference is different. We either scale the prompt inputs or the weight vectors, by a factor {1/3, 1/2, 2, 3}. Noisy linear regression. (Figure 8c) We add noise to each prompt output, that is, the ith output is equal to wTxi + \u03f5i where \u03f5i \u223cN(0, d/20). Different orthants for in-context and query inputs. (Figure 8f) We fix the sign of each coordinate to be positive or negative for all in-context inputs xi (at random), and draw xquery (as before) i.i.d. from N(0, Id). As a result, all in-context inputs lie in the same orthant, while the query input lies in another orthant with high probability.\nDifferent orthants for in-context and query inputs. (Figure 8f) We fix the sign of each coordinate to be positive or negative for all in-context inputs xi (at random), and draw xquery (as before) i.i.d. from N(0, Id) As a result, all in-context inputs lie in the same orthant, while the query input lies in another orthant with high probability.\nQuery input orthogonal to in-context inputs. (Figure 8d) We choose the query input randomly in the space orthogonal to the space spanned by in-context example inputs. That is, we draw the query inpu from an isotropic Gaussian distribution restricted to the subspace orthogonal to the space spanned by the in-context examples. Thus, the optimal normalized error is 1 for any number of in-context examples (there can be at most d \u22121 in-context examples for an orthogonal query to exist).\nQuery input matches an in-context example. (Figure 8e) We set the query input equal to one of the in-context examples chosen uniformly at random. Thus it\u2019s possible to achieve zero error since the in-context examples include the correct prediction for the query input already.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33bf/33bfa674-942d-4ae3-8e5d-89cee75d7f0f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) d/2-dimensional subspace</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8354/83544dac-7eac-49fd-81f1-8935dc8412b5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c66/5c66ab7f-8161-434e-8ec1-e1d46f5cea04.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) orthogonal query</div>\n<div style=\"text-align: center;\">(e) query matches in-context example</div>\nFigure 8: In-context learning on out-of-distribution prompts. We evaluate the model trained to in-context learn linear functions on prompt distribution that deviates from the training prompt distribution. In general, the model error degrades gracefully and closely tracks that of the least squares estimator.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0335/03357bc7-6863-429c-b80b-f222a2868bdb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c93/5c93a028-c628-4929-acc2-e3da66a7ff5c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) scaled x, Transformer</div>\nFigure 9: In-context learning robustness to prompt scaling. We evaluate the model trained to in-context learn linear functions when we scaled the prompt inputs x or the weight of the function class w. The model appear to be quite robust to scaling w but their performance degrades when scaling the inputs up or down by a factor of 3.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a2a4/a2a4f7b9-b3f7-4d5f-9f34-44d3c2d18f73.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) noisy output</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d2e/6d2e7284-d4c2-458d-9c86-30b3503bb802.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) scaled w, Transfomer</div>\n# .3 Effect of problem dimension and model capacity\nWe plot the model error for additional out-of-distribution prompts in Figure 10 for 2d in-context examples (with the exception of orthogonal queries where we use d \u22121 in-context examples). Similar to the settings in Section 6 (skewed covariance and different orthants), accuracy improves with capacity in most cases. One exception is scaling x (Figure 10e), in which case we do not see any clear trend. In the case of noisy output (Figure 10b), the accuracy almost saturates at 1.2M parameters, close to the error of the least squares estimator. In the case of orthogonal query input (Figure 10c), the model achieves the optimum error of 1 even with the tiny model with 0.2M parameters.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b15a/b15ad75f-b0d5-4324-8b4b-4ea39c6f4ecf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) scale x by a factor of 2</div>\n<div style=\"text-align: center;\">(d) query matches in-context examples</div>\nFigure 10: The effect of model capacity and problem dimension for in-context learning performance on out-ofdistribution prompts. We train Transformer models of varying capacity to in-context learn linear function in varying dimensions d. We plot the error with 2d in-context examples (or d \u22121 for orthogonal queries). We find that capacity helps in most cases, with the exception of scaling x where we find no clear trend. For each setting, we train 3 models with different random seeds, and show the median error (solid lines), and the minimum and maximum errors (shaded region). (See Figures 6b, 6c in the main text for the corresponding plots on different-orthants and skewed-covariance.)\n<div style=\"text-align: center;\">(f) scale w by a factor of 2</div>\n# B.4 Training variance\nIn Figure 11, we show the variance in error across training runs for the standard Transformer model (9.5M parameters). We plot the squared error for 3 models (with different random seeds) each for d \u2208 {10, 20, 30, 40, 50}, trained to in-context learn linear functions. The error is quite concentrated in the standard setting as well as for most out-of-distribution prompts. In the different-orthants and skewed-covariance settings, we observe a high variance for higher dimensional problems (d \u226530). However, in Section 6, we saw that the error in these settings usually decreases as we increase the model size. In the setting where we scale x, there is high variance even when d = 10.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8125/81252318-2b22-4945-98e0-6be15ae9eaeb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) d/2-dimensional subspace</div>\nFigure 11: Errors for models trained with different random seeds. For each dimension, we train three models wi different random seeds and show the corresponding error curves.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/766b/766bd60a-4ed8-484c-8410-861c69410403.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/70db/70db6bf9-8441-4870-9b88-5427ad264008.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(g) query matches in-context example</div>\n<div style=\"text-align: center;\">(i) scaled w by a factor of 2</div>\n<div style=\"text-align: center;\">Figure 11: (continued) Errors for models trained with different random seeds. For each dimension, we train three models with different random seeds and show the corresponding error curves.</div>\nFigure 11: (continued) Errors for models trained with different random seeds. For each dimension, we train thr models with different random seeds and show the corresponding error curves.\n# B.5 Curriculum\nIn Figure 12, we show the training loss of the Transformer model trained to in-context learn linear functions, with and without a curriculum. Specifically, given a random training prompt sequence (x1, f (x1), x2, f (x2), . . ., xkcur, f (xkcur)), let \u02c6yi be the model\u2019s prediction for the ith input (meant to approximate f (xi)). For each such prompt, we consider the loss given by the normalized squared error averaged over all prompt prefixes\nAt each training step, we plot the loss averaged over a batch of 64 random prompts. For training with curriculum, kcur is gradually increased to 2d + 1 as described in Section A.2. For training without curriculum kcur = 2d + 1 at all times. Note that the loss often increases in the beginning as we train the model with curriculum. This is due to a sharp increase in the loss at steps where we increase the effective dimensionality (dcur) of prompt inputs (xi). There are two reasons for this increase: (i) variance of the target output (f (xi) = w\u22a4xi) increases, so even the optimum loss is larger, (ii) the model performance is worse for the prompt inputs with increased effective dimension. After each such step where we increment dcur, the loss starts to decrease again until the next increment. The overall trend in the loss looks upward when the sharp increase dominates the decrease that follows. Some observations worth highlighting are as follows.\nCurriculum drastically speed-ups training. For functions in 20 or more dimensions, curriculum allows us to train a low-error model often 4 times faster. Moreover, training without curriculum does not always succeed within our training budget (500k steps), e.g., for one run with d = 30 and all runs with d = 50, the loss does not decrease at all without curriculum.\nInitial lull without curriculum. For training without curriculum, we observe that the loss does not decrease for relatively a long period in the beginning, and starts to decrease sharply thereafter. There is a large variance in the length of this period for any fixed dimension, and the average length seems to increase with dimension. This period is almost non-existent for smaller dimensions (e.g., see the plot for d = 10), and therefore we do not observe such a period while training with curriculum where we start training with inputs lying in a 5 dimensional subspace.\nCurriculum does not affect final performance significantly. For our core setting (d = 20), where we are able to train the model to low error even without curriculum, we do not observe any qualitative differences in the error in most cases (both with and without distribution shifts). One exception is the case with skewed covariance, where the model trained without curriculum seems to do slightly better. We plot the error curves for the standard, different orthants and skewed covariance cases in Figure 13.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1db4/1db4dce4-a8d9-4bc5-b44d-67c665b277e1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/091d/091d2ac9-5b55-45d9-b2b0-bbe23249ba64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/07a8/07a8b83b-0583-426a-a495-b09e6eba5783.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) 10 dimensions</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0767/0767a56b-e58e-4f58-a1dc-24df275da4b7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) 50 dimensions</div>\n<div style=\"text-align: center;\">(d) 40 dimensions</div>\nFigure 12: Loss progression during training with and without curriculum. For each dimension, we show the loss progression with 3 random seeds each for training with and without curriculum. The vertical dashed line shows the point at which the effective dimension of prompt inputs dcur reaches the actual dimension d, after which training with and without curriculum have the same prompt distribution. The horizontal dashed line shows the optimum expected loss. There is a drastic speedup in training with curriculum. Without curriculum, there is an initial relatively long period where the loss does not decrease. For each dimension, there is a large variance in the length of this period, and the average length seems to increase with dimension.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4c3/b4c3221c-45a5-4dcb-8742-2b5355bd5fc9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) different orthants</div>\nFigure 13: In-context learning performance for models trained with and without curriculum. We show the perfor mance for models trained with and without curriculum for in-context learning linear functions (d = 20). We did not observe any major qualitative difference in performance between the two settings in most cases. One exception is the case with skewed covariance where the model trained without curriculum does better.\n<div style=\"text-align: center;\">(c) skewed</div>\nHere, we investigate the effect of amount of training data required for in-context learning linear functions. First, we consider the effect of number of distinct prompts encountered during training. For this, we create a set Sp of np randomly generated prompts, where each prompt in Sp is generated by sampling a weight vector and prompt inputs from N(0, Id). We generate random prompts during training by sampling uniformly from this set. As before, we train the model for 500k steps with a batch size of 64. We observe (see Figure 14) that a model trained with np = 100k is able to achieve non-trivial error and a model trained with np = 1M achieves error close to that of the unrestricted model (trained with 32M distinct prompts). Recall that with curriculum learning, we zero out some of the coordinates of prompt inputs in the beginning of training, which will increase the total number of prompts the model sees during training. Therefore we do not use curriculum learning for this study to avoid inflating the number of distinct prompts seen during training. Second, we consider the effect of number of distinct weight vectors (equivalently, distinct functions) encountered during training. For this, we create a set Sw of nw weight vectors where each weight w is drawn i.i.d. from N(0, Id). To generate a training prompt, (x1, w\u22a4x1, . . . , xk, w\u22a4xk), we draw prompt inputs (xis) i.i.d. from N(0, Id) as in the unrestricted setting, and sample w uniformly at random from Sw. Thus while we sample from a finite set of weight vectors, we sample fresh inputs at each step. As before, we train the model for 500k steps with a batch size of 64. Here, we observe (see Figure 14) that the model trained with as few as 10k distinct weight vectors achieves error close to the unrestricted model (trained with 32M distinct functions). We use curriculum learning for this study as in our standard setting. Recall that with curriculum learning, we only zero out some coordinates of prompt inputs in the beginning, so this does not change the number of distinct weight vectors seen by the model during training.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d263/d2634122-48b5-4b49-86a2-e5846b34ccc7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bbb/3bbb119a-6372-475c-a719-8ceed9805782.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1K 10K 100K 1M 10M32M #prompts used for training</div>\n<div style=\"text-align: center;\">100 1K 10K 100K 32M #weight vectors used for training</div>\nFigure 14: Effect of number of distinct prompts/functions seen during training. We plot the squared error for models trained to in-context linear functions, as we increase the number of distinct prompts and distinct weight vectors (equivalently, distinct functions) seen during training. (Note that 32M corresponds to the unrestricted model where we sample fresh prompts at each training step.) The models are able to achieve error close to that of the unrestricted model with 1M distinct prompts or 10k distinct weight vectors.\n# B.7 Can memorization explain model performance?\nIn Section 3.1, we discussed that memorization of prompts seen during training cannot explain model performance. This is because the probability of the model encountering a training prompt similar to the one used for testing is astronomically low\u2014the prompt inputs alone lie in a 800-dimensional space when predicting with 2d in-context examples (d = 20). Moreover, even considering the possibility that the model encountered a similar weight vector during training cannot explain its performance. Let Sw be the set of weight vectors used to generate training prompts. At inference time, given a prompt with in-context examples generated using a weight vector w\u22c6, suppose the model is somehow able to find the best weight vector \u02c6w in Sw minimizing the normalized squared error on query inputs:\nTaking expectation over the weight vector w\u22c6, we get the expected normalized squared error of the mod (with respect to randomly drawn in-context examples and query inputs):\nTo empirically estimate this quantity, we sample nw weight vectors from N(0, Id) (with d = 20) that form the set Sw, and 500 weight vectors from N(0, Id) to estimate the outer expectation. We do this 20 times, freshly sampling the 500 weight vectors and the vectors comprising Sw each time, and compute the mean of the 20 estimates obtained. When nw = 32M (number of weight vectors encountered in our standard training setup), we get a mean of 0.216 (standard deviation 0.004). However, our model is able to achieve an expected error of less than 0.001 for prompts with 2d in-context examples. Similarly, when nw = 10, 000, we get a mean of 0.505 (standard deviation 0.006), while a model trained on prompts generated using 10, 000 distinct weight vectors is able to achieve a much smaller error (see Figure 14). Thus we can conclude that the model cannot be relying on memorization of the training prompts or weight vectors, and is encoding a more sophisticated algorithm capable of in-context learning linear functions that are very different from those seen during training.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of understanding in-context learning in large language models, particularly focusing on the ability of Transformers to learn function classes from in-context examples without parameter updates.",
        "problem": {
            "definition": "The problem is defined as training a model to in-context learn a function class, where the model must approximate unseen functions based on a prompt sequence of input-output pairs.",
            "key obstacle": "The main challenge is to determine how well the model can generalize from training prompts to unseen functions, especially under distribution shifts."
        },
        "idea": {
            "intuition": "The idea stems from the observation that large language models can adapt to various tasks at inference time, but the extent of their learning capabilities from in-context examples is not well understood.",
            "opinion": "The proposed method involves training Transformers from scratch to learn function classes, with a focus on linear functions and their generalizations.",
            "innovation": "This work innovates by demonstrating that Transformers can in-context learn complex function classes, achieving performance comparable to specialized algorithms, even under distribution shifts."
        },
        "method": {
            "method name": "In-Context Learning with Transformers",
            "method abbreviation": "ICT",
            "method definition": "The method involves training a Transformer model to predict function values based on in-context examples without fine-tuning on actual text data.",
            "method description": "The core of the method is to train Transformers to generate predictions for functions based on a sequence of in-context input-output pairs.",
            "method steps": [
                "Sample a random function from the target class.",
                "Generate random inputs for the function.",
                "Construct a prompt with input-output pairs.",
                "Train the model to predict outputs based on the prompt."
            ],
            "principle": "The method is effective because it leverages the model's architecture to encode algorithms capable of learning from examples, allowing for generalization to unseen data."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using various function classes including linear functions, sparse linear functions, decision trees, and two-layer neural networks, with performance measured against baseline methods.",
            "evaluation method": "The performance was assessed by measuring the error in predictions made by the model on unseen functions, comparing it to the optimal least squares estimator and other baseline methods."
        },
        "conclusion": "The results indicate that Transformers can effectively in-context learn various function classes, achieving performance that matches or exceeds traditional learning algorithms, even when facing distribution shifts.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to encode complex learning algorithms within the Transformer architecture, allowing for efficient in-context learning.",
            "limitation": "One limitation is that the method may not generalize well to all types of function classes, particularly those with more complex structures.",
            "future work": "Future research could explore the internal mechanisms of the model to better understand the learning algorithms it encodes, as well as investigate its performance on more complex learning tasks."
        },
        "other info": {
            "acknowledgements": "The authors thank colleagues for helpful discussions and acknowledge funding sources for their research.",
            "experimental setup": {
                "model architecture": "The experiments utilized a decoder-only Transformer architecture from the GPT-2 family with varying capacities.",
                "training details": "Training involved sampling random prompts and functions, optimizing using the Adam optimizer, and employing curriculum learning to enhance training efficiency."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of understanding in-context learning in large language models, particularly focusing on the ability of Transformers to learn function classes from in-context examples without parameter updates."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method involves training Transformers from scratch to learn function classes, demonstrating that Transformers can in-context learn complex function classes, achieving performance comparable to specialized algorithms."
        },
        {
            "section number": "2",
            "key information": "The problem is defined as training a model to in-context learn a function class, where the model must approximate unseen functions based on a prompt sequence of input-output pairs."
        },
        {
            "section number": "3.1",
            "key information": "The method is effective because it leverages the model's architecture to encode algorithms capable of learning from examples, allowing for generalization to unseen data."
        },
        {
            "section number": "3.3",
            "key information": "The core of the method is to train Transformers to generate predictions for functions based on a sequence of in-context input-output pairs."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that the method may not generalize well to all types of function classes, particularly those with more complex structures."
        },
        {
            "section number": "6.4",
            "key information": "Future research could explore the internal mechanisms of the model to better understand the learning algorithms it encodes, as well as investigate its performance on more complex learning tasks."
        }
    ],
    "similarity_score": 0.7425214906375189,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/What Can Transformers Learn In-Context_ A Case Study of Simple Function Classes.json"
}