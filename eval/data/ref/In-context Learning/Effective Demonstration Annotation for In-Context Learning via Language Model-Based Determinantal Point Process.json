{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.02103",
    "title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process",
    "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language Models (LLMs), existing works are highly dependent on large-scale labeled support sets, not always feasible in practical scenarios. To refine this approach, we focus primarily on an innovative selective annotation mechanism, which precedes the standard demonstration retrieval. We introduce the Language Model-based Determinant Point Process (LM-DPP) that simultaneously considers the uncertainty and diversity of unlabeled instances for optimal selection. Consequently, this yields a subset for annotation that strikes a trade-off between the two factors. We apply LM-DPP to various language models, including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples. Further analysis reveals that LLMs benefit most significantly from subsets that are both low uncertainty and high diversity.",
    "bib_name": "wang2024effectivedemonstrationannotationincontext",
    "md_text": "# Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process Retriever x3,y. . . xtes x3 x7 x2 Our work: Small labeled data\nPeng Wang\u2660, Xiaobin Wang\u2661, Chao Lou\u2662, Shengyu Mao\u2660, Pengjun Xie\u2661, Yong Jiang\u2661\u2217 \u2660Zhejiang University, \u2661Alibaba Group, \u2662ShanghaiTech University peng2001@zju.edu.cn, yongjiang.jy@alibaba-inc.com Selective  Annotation Demonstrations\nPeng Wang\u2660, Xiaobin Wang\u2661, Chao Lou\u2662, Shengyu Mao\u2660, Pengjun Xie\u2661, Yong Jiang\u2661\u2217 Selective  Annotation Demonstrations\n# Peng Wang\u2660, Xiaobin Wang\u2661, Chao Lou\u2662, Shengyu Mao\u2660, Pengjun Xie\u2661, Yong Jiang\u2661\u2217 Selective  Annotation Demonstrations\n\u2660Zhejiang University, \u2661Alibaba Group, \u2662ShanghaiTech University peng2001@zju.edu.cn, yongjiang.jy@alibaba-inc.com\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca66/ca66d7b1-310d-4b44-9403-606487f89f04.png\" style=\"width: 50%;\"></div>\n# Abstract\nIn-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language Models (LLMs), existing works are highly dependent on large-scale labeled support sets, not always feasible in practical scenarios. To refine this approach, we focus primarily on an innovative selective annotation mechanism, which precedes the standard demonstration retrieval. We introduce the Language Model-based Determinant Point Process (LM-DPP) that simultaneously considers the uncertainty and diversity of unlabeled instances for optimal selection. Consequently, this yields a subset for annotation that strikes a trade-off between the two factors. We apply LM-DPP to various language models, including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples. Further analysis reveals that LLMs benefit most significantly from subsets that are both low uncertainty and high diversity.\narXiv:2408.02103v1\n# 1 Introduction\nAs large pre-trained language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022a; Tay et al., 2023; Touvron et al., 2023; Workshop, 2023) grow in scale, they not only exhibit enhanced linguistic capabilities and expanded world knowledge but also demonstrate a novel ability for in-context learning. Specifically, LLMs have shown proficiency in learning from a limited set of input-output examples (known as demonstrations (Brown et al., 2020)), and effectively applying these learned mappings to new, unseen instances. This novel few-shot learning paradigm,\nCorresponding Author.\n<div style=\"text-align: center;\">Figure 1: Left (Step 1): Without assuming access to a large amount of labeled data, we employ active data collection, selectively annotating demonstration examples. Right (Step 2): Prompt construction and model inference.</div>\nFigure 1: Left (Step 1): Without assuming access to a large amount of labeled data, we employ active data collection, selectively annotating demonstration examples. Right (Step 2): Prompt construction and model inference.\nwhich avoids parameter updates, has become a popular and efficient method for utilizing LLMs (Liu et al., 2021b; Dong et al., 2023; Liu et al., 2021a). Previous studies have investigated which instances can serve as effective prompts for ICL (Liu et al., 2021a; Zhang et al., 2022b; Li and Qiu, 2023). They have demonstrated that retrieving specific similar contexts for individual test queries can significantly improve performance (instance level) and ground truth matters for support examples. To assign appropriate demonstrations to all test queries, support sets necessitate diversity and broad coverage, usually achieved through large labeled data, following the principle that Monte Carlo estimation accuracy improves with larger samples. Nonetheless, these extensive datasets are often impractical to obtain. We investigate the selection of demonstrations from the perspective of Active Learning (AL) (Cohn et al., 1996; Settles, 2009). Based on the core principle that not all data points are of equal value, AL aims to identify the most effective instances in an unlabeled data pool for annotation. Margatina et al. (2023) elucidates that high semantic similarity, low uncertainty, and high diversity comprise an effective and efficient annotation strategy. Similarly, Gonen et al. (2022) demonstrates\nthat lower prompt perplexity is closely associated with better performance. While Su et al. (2022)\u2019s Vote-k framework adopts a data-centric perspective (i.e., selecting examples that balance diversity and representativeness), it neglects the assessment of uncertainty and the inter-relationship among context examples. In this paper, we pursue a more universally applicable yet straightforward solution, incorporating confidence signals of LLMs to select annotation instances that are maximally diverse and exhibit low uncertainty. To address this need, we introduce a generic approach, LM-DPP, which jointly models uncertainty and diversity within the support set through a conditional Determinantal Point Process. Specifically, we employ LLMs\u2019 perplexity to score each candidate instance in the support set, which serves as a measure of the LLMs\u2019 uncertainty. Then a Gram matrix is constructed to balance the uncertainty and diversity of candidate instances and polynomialtime maximum a posteriori (MAP) inference (Chen et al., 2018) is applied to identify the most useful subset of instances to be annotated. From the perspective of selective annotation, we consider extremely low-resource ICL scenarios as those in which the available annotated examples are limited to a few dozen instances. Our focus centers on identifying which specific set of demonstrations can most effectively harness the capabilities of LLMs within this challenging context. We validate our method through extensive experiments on 9 NLU and 2 Generation datasets. We also demonstrate the versatility of LM-DPP by adapting it to the large language model GPT3 (175B). The experimental results illustrate that our approach can effectively balance two critical factors, uncertainty and diversity. In summary, our contributions are as follows.\n\u2022 We revisit the setup of ICL from the perspective of selective annotation. We introduce a novel approach, LM-DPP, to select instances that balance uncertainty and diversity for annotation, aiming to reduce the human engineering workload.\n The experimental results indicate that the proposed method outperforms the previous bestperforming selection methods by a large relative improvement and exhibits commendable generalizability across model size (\u00a74.2) and annotation budget (\u00a74.3) scaling.\n\u2022 Comprehensive analysis confirms that LLMs can benefit from a demonstration set that exhibits both low uncertainty and diversity (\u00a74.1) and gold annotation matters for ICL performance (\u00a75.2).\n# 2 Methodology\nIn this section, we introduce technical details of LM-DPP for selecting annotation instances exhibiting both high diversity and low uncertainty. Formally, given a set of unlabeled samples X = {xi}N i=1, LM-DPP aims to select a subset L \u2282X for annotation, where |L| = M is the annotation budget, such that the Language Models (LLMs) maintains high ICL performance on the test set Dtest. As shown in Figure 2, given a Pre-trained Language Model (PLM) G, we first score candidate instances xi using the perplexity of the LLMs (\u00a72.1). We then compute vector representations for the candidate instances, utilizing a conditional kernel matrix to balance diversity and low uncertainty (\u00a72.2). Subsequently, we perform a greedy MAP inference algorithm to filter the candidate annotation set (\u00a72.3).\n# 2.1 Uncertainty\nAs off-the-shelf LLMs do not contain a classification head fine-tuned for specific tasks, calculating entropy, a common measure of uncertainty used in AL, across all possible outputs is challenging, if not unfeasible. Alternatively, we adopt the SPELL method proposed by (Gonen et al., 2022), using the perplexity of the LLMs, to score candidate examples \ufffdx. The scoring function r(\ufffdx) is defined as:\n\ufffd (1)\n\ufffd \ufffd \ufffd\ufffd Recent research also delineates that LLMs are essentially a form of lossless data compression (Del\u00e9tang et al., 2023), and perplexity, serving as a proxy for the occurrence of the prompt in some form in the training data, inherently indicates the model\u2019s expectancy of the prompt. Therefore, perplexitybased demonstration selection can, to some extent, avoid LLM sampling from low-frequency distributions. We also conduct pilot experiments (Appendix B) that select instances of high uncertainty, observing a substantial decrease in performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a2e5/a2e5eca4-a0d1-4913-92e0-60ffe469d32c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: An illustration of our proposed approach. There are three steps in LM-DPP: (1) Estimate the perplexi for each unlabeled data point, with the reciprocal denoted as r(xi). (2) Employ conditional DPP to jointly mod uncertainty and diversity, selecting a small set of examples for annotation before test time. (3) At test time, th context is constructed by retrieving relevant examples from the small annotated pool.</div>\n# 2.2 DPP Modeling\nWe consider similarity as the primary qualitative feature of the DPP diversification process. In this section, we present the decomposition of DPP that more directly elucidates the tension between diversity and the uncertainty measure for each candidate instance. Since the DPP kernel, L is typically written as a Gram matrix, L = BT B, where the columns of B represent vectors from the candidate set X. We define Bi as the product of the LLMs uncertainty term ri \u2208R+ and the normalized diversity feature vector \u03d5i \u2208RD, with |\u03d5i| = 1. The new DPP kernel matrix can now be written as Kij = ri\u03d5T i \u03d5jrj = rirj\u27e8\u03d5T i \u03d5j\u27e9(Ye et al., 2023). ri can be regarded as the intrinsic evaluation of the LLMs for the candidate instance and \u27e8\u03d5T i \u03d5j\u27e9as the measure of similarity between instances xi and xj. Therefore, we arrive at L = Diag(r) \u00b7 \u03d5 \u00b7 Diag(r), and the unnormalized log probability for the subset S is log det(LS) = \ufffd i\u2208S log(r2 i ) + log det(\u03d5S). To adjust the trade-off between uncertainty and diversity, we introduce a balancing parameter \u03bb, thus modifying the log probability of LS to: \ufffd\n(2)\nThis corresponds to a DPP with kernel L\u2032 = Diag(exp(\u03b1r)) \u00b7 \u03d5 \u00b7 Diag(exp(\u03b1r)), where \u03b1 = \u03bb/(2(1 \u2212\u03bb)). In Equ. (2), the first term corresponds to the low perplexity of the selected instances, while the second term increases with the diversity of the selected instances. Without the diversity model, we would choose examples of low uncertainty, but the DPP would tend to repeatedly select similar examples. Without the low uncertainty model, although we could obtain a highly diverse set, we might fail to include in S those examples most favorable to the LLMs. By combining\nthem, we can achieve a more balanced outcome.\n# 2.3 Inference\nThe solution to the MAP for DPP, which is to find the set of examples with the highest probability, is a complex process and an NP-hard problem. (Chen et al., 2018) have proposed an improved greedy algorithm that can quickly solve it approximately. In specific, this algorithm greedily selects the demonstration from the candidate set that maximizes the marginal gain to be added to the final result subset, until the stopping condition is satisfied. That is, each time an example j is chosen to be added to the candidate set Smap, which is initialized as an empty set. The formalization is as follows:\nBy performing a Cholesky decomposition on LSmap, and incrementally updating the Cholesky factor, the complexity of solving det(LSmap) can be reduced from O(K3) to O(K). Therefore, the complexity of each iteration is O(NK). This implies that it is possible to return K annotation examples within O(NK2) time. Once we have selected and annotated a subset of examplesL from the unlabeled support set, following recent work (Liu et al., 2021a), we retrieve examples from L that are semantically similar to the test query samples. We use Sentence-BERT (Reimers and Gurevych, 2019) representations for L and Dtest again and employ cosine similarity as the metric. The underlying principle is that demonstrations most similar to the test example will best assist the model in answering the query. For the order of demonstrations, we adhere to the configuration established by Su et al. (2022), where the order of the retrieved\nModel\nBudget\nMethod\nNatural Language Inference\nClassification\nMulti-Choice\nAvg\nRTE\nMNLI\nMRPC\nQNLI\nSST-5\nDBpedia\nTREC\nHellaswag\nCOPA\nGPT-J\n6B\n|L| = 16\nRandom\n48.243.1\n40.923.0\n64.755.0\n51.863.5\n46.493.6\n82.727.7\n56.9416.1\n67.771.5\n83.112.0\n60.316.6\nKmeans\n46.582.6\n39.841.0\n59.488.6\n51.472.1\n41.804.7\n88.770.8\n68.463.5\n66.902.2\n83.401.3\n60.743.8\nVote-k\n47.860.9\n40.042.9\n59.967.3\n51.373.9\n40.243.7\n89.263.5\n72.077.9\n68.562.9\n83.401.6\n61.424.4\nFast Vote-k\n48.340.7\n39.263.9\n58.895.0\n50.391.7\n50.805.8\n89.653.4\n75.105.5\n67.383.8\n83.100.8\n62.543.8\nLM-DPP (ours)\n49.811.5\n40.921.7\n64.361.4\n52.962.0\n47.665.0\n89.063.0\n75.202.6\n69.442.6\n83.602.1\n63.672.6\n|L| = 100\nRandom\n47.642.2\n39.412.8\n63.593.1\n51.113.5\n47.430.9\n90.301.5\n76.361.3\n67.880.8\n84.031.7\n63.082.2\nKmeans\n48.220.5\n41.743.8\n64.405.0\n51.523.1\n46.181.6\n90.551.7\n77.095.6\n67.630.5\n83.301.8\n63.403.1\nVote-k\n49.121.3\n40.262.9\n61.244.1\n50.623.1\n47.851.2\n86.922.0\n82.182.5\n67.791.8\n82.122.8\n63.122.6\nFast Vote-k\n51.934.1\n39.534.2\n65.731.2\n50.412.6\n49.390.9\n91.602.1\n81.455.4\n68.231.0\n83.843.9\n64.683.2\nLM-DPP (ours)\n54.442.6\n42.312.4\n67.101.3\n53.261.5\n49.621.0\n91.032.2\n82.013.2\n68.921.5\n83.801.7\n65.832.0\nLLAMA-2\n7B\n|L| = 16\nRandom\n54.701.4\n38.811.4\n60.421.9\n53.032.1\n54.104.1\n86.826.0\n67.4814.4\n77.252.1\n88.582.5\n64.575.6\nKmeans\n54.881.3\n36.624.9\n60.948.0\n52.541.8\n53.322.7\n90.041.8\n76.958.4\n77.252.1\n89.061.4\n65.734.5\nVote-k\n52.830.5\n41.214.8\n62.891.3\n55.570.4\n53.422.6\n87.791.6\n79.102.5\n77.242.4\n87.701.3\n66.422.3\nFast Vote-k\n52.251.2\n38.284.0\n59.674.4\n53.131.7\n53.324.3\n88.281.8\n75.464.7\n77.152.9\n88.481.9\n65.113.3\nLM-DPP (ours)\n58.993.5\n38.285.6\n63.094.5\n53.812.6\n55.373.3\n93.651.5\n76.284.5\n77.251.2\n88.671.1\n67.263.5\n|L| = 100\nRandom\n58.011.2\n39.855.1\n60.484.0\n51.661.9\n54.501.6\n92.871.2\n83.692.6\n76.763.1\n87.911.2\n67.302.8\nKmeans\n56.541.3\n42.292.9\n64.852.2\n53.322.1\n54.781.9\n93.752.0\n84.962.9\n78.032.3\n87.701.5\n68.472.2\nVote-k\n58.400.7\n42.193.2\n65.334.0\n53.711.4\n57.132.3\n90.821.5\n84.382.7\n78.423.3\n86.141.6\n68.502.5\nFast Vote-k\n61.720.3\n39.551.5\n63.181.4\n51.951.0\n56.152.1\n93.460.7\n85.741.9\n77.833.0\n88.181.5\n68.641.7\nLM-DPP (ours)\n58.992.7\n41.315.3\n66.802.3\n56.150.9\n57.623.0\n94.820.4\n83.502.2\n78.912.1\n89.361.8\n69.722.6\nTable 1: Results with GPT-J and LlaMA-2-7B on NLU task. We compare various selective annotation methods with {100, 16} annotated examples. Bold numbers indicate the highest accuracy among all methods, while those underlined indicate the second-best. The subscript denotes the standard deviation.\nexamples is such that s(qi, x) \u2264s(qj, x) whenever i < j. s(qi, x) denotes the similarity between the retrieved example qi and the test example x. This setup potentially leverages the recency bias inherent in LLMs (Zhao et al., 2021).\n# 3 Experiments\n# 3.1 Experimental Settings\nDatasets We conduct experiments on 9 NLU and 2 Generation tasks involving different task formulations, including Sentiment Classification: SST-5 (Socher et al., 2013); Natural Language Inference: RTE (Bentivogli et al., 2009), MNLI (Williams et al., 2017), MRPC (Dolan et al., 2004), QNLI (Wang et al., 2018); Topic Classification: TREC (Hovy et al., 2001), DBpedia (Lehmann et al., 2015); Multiple-choice Question Answering: Hellaswag (Zellers et al., 2019), COPA (Roemmele et al., 2011); Abstractive Summarization: XSUM (Narayan et al., 2018) and Open Domain QA: NQ (Kwiatkowski et al., 2019). In the main experiment, the budget of annotation is set as ({16, 100}). For datasets with publicly available test data, we use the test data for evaluation. For others, we follow previous work (Lan et al., 2019; Su et al., 2022) and use the dev set for evaluation. Baselines We compare LM-DPP with four strong selective annotation methods. And in our study, we primarily utilize GPT-J-6B (Wang and Komat-\nBaselines We compare LM-DPP with four strong selective annotation methods. And in our study, we primarily utilize GPT-J-6B (Wang and Komat-\nMethods\nRandom\nKmeans\nVote-k\nFast Vote-k\nLM-DPP\nL = 16\nNQ\nACC.\n21.744.39\n22.783.63\n22.793.37\n22.013.75\n23.833.10\nXSUM\nR-L\n24.570.03\n23.650.29\n24.881.03\n24.741.20\n26.341.07\nFactCC\n35.074.26\n36.722.41\n32.491.44\n34.682.86\n33.533.70\nL = 100\nNQ\nACC.\n23.573.54\n22.923.13\n24.484.01\n23.703.51\n24.613.74\nXSUM\nR-L\n25.110.41\n24.470.46\n24.660.84\n24.631.37\n27.290.55\nFactCC\n35.645.86\n34.862.97\n36.122.40\n36.533.84\n35.162.01\nTable 2: Results with LlaMA-2-7B on Generation Task. suzaki, 2021) and LlaMA-2-7B (Touvron et al., 2023) as scoring and inference language models, More details about baselines and implementation can be found in Appendix A.3, A.2 respectively. Metrics We compare the predicted answers with the true outcomes and report the accuracy (Acc.) for all NLU tasks and exact matching scores (Rajpurkar et al., 2016) for NQ. For summarization tasks, we assess factual consistency using FactCC (Kryscinski et al., 2020) 1, a BERT-based (Devlin et al., 2019) metric for evaluating output faithfulness. Simultaneously, for quality assessment, we report the ROUGE-L F1 score (Lin, 2004) to evaluate the summary against the reference.\n# 3.2 Main Results\nNLU Task From Table 1, we can observe that LM-DPP consistently improves the on-average accuracy across a variety of NLU tasks under\n1https://huggingface.co/manueldeprada/FactCC\ndifferent annotation budgets (|L| = 16, |L| = 100). Specifically, with a larger budget, LM-DPP achieves an average absolute gain of 1.15% on GPT-J and 1.08% on LlaMA, compared to the bestperforming baseline. This demonstrates that balancing uncertainty and diversity ensures that the chosen demonstrations are more likely to contain complementary information that enhances performance. On GPT-J, LM-DPP exhibits the lowest average standard deviation (2.6, 2.0), and on LlaMA2, it shows greater stability than the Random baseline, albeit marginally lower than Vote-k. This indicates that LM-DPP can maintain a relatively stable performance across different experimental setups, substantially increasing the reliability and robustness of contextual learning. Furthermore, we observe that as the annotation budget increases, performance fluctuations decrease across different selection methods. Generation Task Experiments on LlaMA-2 (as shown in Table 2) reveal that LM-DPP achieves notable improvement on the NQ task across various annotation budgets, especially at L = 16, where it surpasses the best baseline by 1.04%. In the XSUM task, applying LM-DPP consistently enhances Rouge scores, particularly achieving a 2.18% increase at L = 100. This underscores the efficacy of the proposed method in improving the generality and reference similarity of generated text. However, this improvement comes at the cost of some degree of factual consistency with the reference, potentially due to the pursuit of diversity reducing the focus on task-specific relevance (see Appendix C.2 for a more detailed analysis). Overall, LM-DPP boosts the model\u2019s generalization and accuracy and highlights the potential for performance optimization with increased annotation budgets. Despite some variability in factual consistency, these insights pave the way for future research on efficiently allocating annotation resources in NLG tasks (Dong et al., 2022).\ndifferent annotation budgets (|L| = 16, |L| = 100). Specifically, with a larger budget, LM-DPP achieves an average absolute gain of 1.15% on GPT-J and 1.08% on LlaMA, compared to the bestperforming baseline. This demonstrates that balancing uncertainty and diversity ensures that the chosen demonstrations are more likely to contain complementary information that enhances performance. On GPT-J, LM-DPP exhibits the lowest average standard deviation (2.6, 2.0), and on LlaMA2, it shows greater stability than the Random baseline, albeit marginally lower than Vote-k. This indicates that LM-DPP can maintain a relatively stable performance across different experimental setups, substantially increasing the reliability and robustness of contextual learning. Furthermore, we observe that as the annotation budget increases, performance fluctuations decrease across different selection methods.\nGeneration Task Experiments on LlaMA-2 (as shown in Table 2) reveal that LM-DPP achieves notable improvement on the NQ task across various annotation budgets, especially at L = 16, where it surpasses the best baseline by 1.04%. In the XSUM task, applying LM-DPP consistently enhances Rouge scores, particularly achieving a 2.18% increase at L = 100. This underscores the efficacy of the proposed method in improving the generality and reference similarity of generated text. However, this improvement comes at the cost of some degree of factual consistency with the reference, potentially due to the pursuit of diversity reducing the focus on task-specific relevance (see Appendix C.2 for a more detailed analysis). Overall, LM-DPP boosts the model\u2019s generalization and accuracy and highlights the potential for performance optimization with increased annotation budgets. Despite some variability in factual consistency, these insights pave the way for future research on efficiently allocating annotation resources in NLG tasks (Dong et al., 2022).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a43e/a43e82c8-422b-4cd5-a783-901a36052c5e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: LlaMA-2-7B Results with L = 4.</div>\nSmaller In-Context Examples We investigate the impact of the number of examples and labels on ICL performance. As shown in Figure 3, LM-DPP\n\u03bb\nMRPC\nQNLI\nTREC\nDBpedia\nHellaswag\n0.0\n62.57\n51.43\n79.40\n90.67\n67.16\n0.2\n66.42\n52.64\n78.82\n89.47\n66.73\n0.4\n65.34\n53.21\n77.69\n90.22\n65.05\n0.5\n66.89\n53.38\n81.43\n91.52\n68.89\n0.6\n67.10\n53.26\n82.01\n91.03\n68.92\n0.8\n66.39\n52.18\n81.24\n90.77\n67.42\n0.9\n66.51\n52.97\n79.36\n84.25\n66.27\n1.0\n66.14\n51.45\n81.57\n79.49\n59.73\nTable 3: The GPT-J performance of different trade-off factors \u03bb. (\u03bb = {0.0, 1.0}) correspond respectively to the vanilla DPP and the Perplexity baseline (\u00a7A.3).\nsurpasses the other baselines in terms of accuracy and stability on MRPC and TREC but is slightly inferior to Vote-k on DBpedia. Further analysis suggests that a well-balanced demonstration set does not always result in improved performance or reduced variance (see Appendix C.3 for more details). In TREC, performance increases with more labels, whereas in MRPC, demonstrations with a single label (all being equivalent) lead to better performance than a balanced demonstration set, with less variance.\n# 4 Analysis\n# 4.1 Impacts of the Trade-off Between Uncertainty and Diversity\nWe analyze to investigate how the trade-off between diversity and uncertainty impacts the performance of downstream tasks. With an annotation budget of 100, we test the performance under different (\u03bb) values utilizing GPT-J as the inference model. As evident from Table 3, a complete inclination towards uncertainty (\u03bb = 1.0) generally yields poorer outcomes across all tasks, likely due to selective annotation excessively concentrating on a small portion of data, thereby diminishing ICL\u2019s generalization capacity. Optimal effects are often observed at (\u03bb) values of 0.5 or 0.6 (which approximate a balance between the two factors), suggesting that moderate uncertainty coupled with a degree of diversity is beneficial for the model\u2019s downstream task performance. Moreover, different tasks demonstrate varied sensitivities to the (\u03bb) value. For instance, QNLI shows minor performance shifts (\u00b11.95%), whereas DBpedia exhibits significant performance variations at certain (\u03bb) values (exceeding \u00b110.00%), indicating that the optimal selection of (\u03bb) may relate to the tasks\u2019 characteristics and difficulty levels. Despite such\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3306/33063e24-c04d-4a46-8db3-450ab52db8e1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Comparisons of various selection methods with ({16, 100, 300, 800}) annotated examples on four representative tasks: RTE, MRPC paraphrase detection, QNLI, and Hellaswag commonsense answering for GPT-J.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ad68/ad68e0f3-2619-473b-a4b5-b868281ae87c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Results of GPT-3-Turbo (175B) with 100 annotated examples. LM-DPP consistently improves in-context learning on various datasets.</div>\nvariability, we find that introducing this trade-off factor consistently surpasses the vanilla DPP and Perplexity baselines, which consider only diversity or uncertainty, thereby validating the effectiveness of LM-DPP.\n# 4.2 Transferability across Different LMs\nSmall model for scoring Scoring every sample from the extensive unlabeled pool using a more resource-intensive LLM could be computationally demanding, particularly when the size of the unlabeled sample pool is substantial. Therefore, we attempt to use GPT2 (Radford et al., 2019) (117M, which possesses basic language modeling capabilities) as a surrogate for the source language model GPT-J, while maintaining GPT-J for the inference model. Across 9 NLU tasks (annotation size=100), the average accuracy was 64.76 (details in Appendix C.1). This indicates that LM-DPP exhibits strong transferability across different inference LMs, which means that the selected demonstrations can be reused.\nTransfer to LLMs To gain some intuition on the effect of model size, we endeavor to transfer the proposed method to LLMs that are aligned\nwith human expectations (gpt-3.5-turbo-instruct) (Ouyang et al., 2022). In specific, we take the logprobs returned by the official API as a reference for measuring uncertainty, from which we calculate r(xi) and perform standard LM-DPP. As depicted in Figure 5, we report the experimental results of GPT-3.5-Turbo (175B) with LM-DPP on several datasets and compare them with the Random and Fast Vote-k baseline. In comparison to random selection, our results indicate that LM-DPP can significantly enhance the performance of GPT-3.5, as evidenced by the 5.6% improvement in TREC accuracy, 1.8% in MNLI, 0.2% in SST-5, and 0.6% in COPA. The proposed LM-DPP approach surpasses Fast Vote-k by an average of 3.25%, indicating that considering representativeness alone is not sufficient to extract a high-quality demonstration subset.\n# 4.3 Varying budget of annotated examples\nWe further investigate how the size of the annotation set affects the performance of in-context learning. Under annotation sizes of ({16, 100, 300, 800}), we compare LM-DPP with Random selection, Fast Vote-k, and Vote-k, and report the results in Figure 4. It is observable that with increasing annotation budgets, most selective methods generally show a consistent overall improvement trend. This is in line with the expectation that more labeled data is more likely to retrieve relevant examples to assist LLMs in accurately answering, thereby improving the performance of in-context learning. The proposed approach, LM-DPP, outperforms other methods at an annotation size of 16 on RTE, Hellaswag, and QNLI, suggesting that even with extremely low annotation budgets, LMDPP can ensure the effectiveness and diversity of context. Additionally, with a sufficient annotation budget (L = 800), LM-DPP exhibits commendable performance, achieving the best results on two\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5788/578887e0-c382-4700-8991-7d67465e4c8d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: The time consumed to select 300 demonstrations from the RTE dataset (comprising 2491 instances).</div>\ndatasets, MRPC and QNLI. In contrast, the performance decline of Vote-k on QNLI may be attributed to the annotation of noisy data (high perplexity), with some case analyses provided in the appendix A.1. This reaffirms the necessity of balancing uncertainty and diversity.\n# 4.4 Time Efficiency\nWe explore the execution efficiency of both the baseline methods and LM-DPP. As illustrated in Figure 6, the LM-Free approach significantly reduces the time required to select demonstrations compared to methods that require scoring by LM. Selecting 300 samples takes 4039.1s with Vote-k, 382.6s with LM-DPP, and only 0.3s with random selection. Since LM-DPP only requires a single forward pass per sample, we can optimize time efficiency in two ways: (1) preemptively compute perplexity for data samples in practical scenarios and devise methods to reset or update cached demonstration samples periodically. (2) using smallerparameter scoring models (see \u00a74.2) can achieve more than tenfold acceleration (24.4s).\n# 5 Discussion\n# 5.1 Case Study\nWe compare demonstrations selected via LM-DPP against Random in CosmosQA dataset (Huang et al., 2019). It reveals that demonstrations selected by the LM-DPP exhibit greater diversity in content, covering 16 distinct topics such as natural disasters, personal emotions, political views, social interactions, and school life, compared to only 8 topics covered by random selection (Figure 7). The selected demonstrations not only span a broad range of subjects but also offer a variety in style,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3a2f/3a2f488a-7f70-44b2-b3cb-57b9a32a9913.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Case Study of selected demonstrations under the condition of annotation_size=16.</div>\nHellaswag\nCOPA\nDBpedia\nTREC\nQNLI\nMNLI\nRandom\u2020\n67.88\n84.03\n90.30\n76.36\n51.11\n39.41\nLM-DPP\u2020\n68.92\n83.80\n91.03\n82.01\n53.26\n42.31\nUN-LM-DPP\n68.48 -0.64\n83.20 -0.72\n90.74 -0.32\n76.48 -6.74\n53.37 +0.21\n41.09 -2.88\nTable 4: The GPT-J performance on various datasets. \u2020Resulting numbers are taken from Table 1. The annotation budget is 100. In UN-LM-DPP, the annotation set consists of two parts: Di and Du, with standard ICL being implemented.\nincluding personal narratives, descriptive events, emotional expressions, and dialogues. This diversity enhances the model\u2019s ability to interpret and respond to questions.\n# 5.2 Does annotation benefit from gold labels?\nMin et al. (2022) observed that random substitution of labels in demonstrations minimally impacts the performance across a suite of tasks, while Yoo et al. (2022) highlighted that the integrity of input label mapping is a crucial factor. In this section, we explore whether Gold Labels (i.e., providing correct labels) are essential for achieving high performance in ICL. Specifically, we divide the selective annotation process into several steps. Step 1: Annotate 50 instances to construct an in-domain dev set Di (containing gold labels). Step 2: For the unannotated instances, we pair each input xi with every possible label y \u2208C (C is the label set) to construct a train set D\u2032 carrying pseudo-labels. Step 3: Given the prompts Z \u2208D\u2032, the ICL accuracy on the indomain dev set Di is denoted as Acc(Z). We select the Top-50 Z, represented as Du. Therefore, the final annotation set (|L| = 100) comprises two parts: Di with gold labels, and Du selected posthoc. This process is referred to as UN-LM-DPP, followed by conducting standard ICL experiments. As shown in Table 4, we observe that UN-LMDPP, compared to LM-DPP with gold annotations, exhibits a certain performance decline in most\ntasks but still surpasses Random selection in some datasets. The performance fluctuation varies significantly across different tasks, depending on the specific characteristics of the datasets, as evidenced by a decrease of -6.74% in TREC, yet only -2.88% in MNLI.\nDataset\nHellaswag\nCOPA\nDBpedia\nTREC\nQNLI\nMNLI\nGold-Labeled\n47.63%\n38.86%\n25.11%\n11.52% 52.30% 37.43%\nTable 5: The proportion of golden-labeled examples identified within an unlabeled setting in UN-LM-DPP.\nTable 5: The proportion of golden-labeled examples identified within an unlabeled setting in UN-LM-DPP.\nThis suggests that, to a certain extent, ICL generally benefits from gold demonstrations. In addition, we report the proportion of gold demonstrations within the constructed Du during Step 2, with the results presented in Table 5. In QNLI, there is a 52.30% gold label ratio, and surprisingly, we observe a slight performance improvement compared to LM-DPP. It is evident that within similar tasks, a higher ratio of gold-standard examples correlates with a smaller decline in ICL performance. However, this is not a generalized finding across the board, and we consider annotation-free ICL as a direction for future work.\n# 6 Related Work and Background\nDeterminantal Point Process The Determinantal Point Process (DPP) is an elegant probabilistic model that captures negative correlations and allows for efficient algorithms in sampling, marginalization, and conditioning (Kulesza, 2012). Formally, a point process P is a probability measure on the power set of V, that is, the set of all discrete items 2V. If Y is a random subset drawn according to P, then for every S \u2286Y :\nP(S \u2286Y ) = det(LS)\n(4)\nfor some kernel matrix L \u2208Rn\u00d7n that is symmetric, real and positive semidefinite. LS denotes the submatrix of L obtained by restricting to the rows and columns indexed by S. The operator det(\u00b7) represents the determinant of a matrix. Typically, the DPP kernel L can be written as a Gram matrix, Lij = K(ai, aj), where K(\u00b7, \u00b7) is the kernel associated with the determinantal point process, often expressed as \u03d5(ai)T \u03d5(aj), \u03d5 is the feature map of a reproducing kernel (Ye et al., 2023). Under distribution P, our objective is maximum a posteriori (MAP) inference, which is to find the\nsubset of items with the highest probability, corresponding to the most diverse subset of items.\n(5)\nAlthough finding the mode of a DPP is NP-hard, pioneering works (Kulesza, 2012; Lee et al., 2009; Chen et al., 2018; Gillenwater et al., 2012) have largely relied on greedy algorithms or sampling methods, and have succeeded in performing greedy MAP inference within polynomial time.\nIn-context Learning The capacity for in-context learning has been observed in large-scale Pretrained Language Models (PLMs) such as GPT3, representing a few-shot learning paradigm that does not require any parameter updates. It involves pre-pending a small number of demonstrations as prompts before the test input, allowing LLMs to discern patterns and \u201clearn\u201d to predict. Formally, let \u02c6x be the test query to be addressed, and s(\u00b7, \u00b7) be the cosine similarity. Standard ICL prompts the language model G with a set of example input-output pairs {(x1, y1) . . . (xm, ym)} and predicts the answer \u02c6y for the query. Typically, the pairs (xi, yi) are retrieved from a train set D within the same domain through similarity.\nRecent works have aimed to enhance ICL by selecting valuable demonstrations (Liu et al., 2021a; Rubin et al., 2022), optimizing the order of demonstrations (Lu et al., 2022), etc. Su et al. (2022) utilize selective annotation to significantly reduce annotation costs while ensuring high ICL performance. Yang et al. (2023) explore the corpus-level in-context learning via DPP and mention the need to use gold labels to score candidate samples. CEIL (Ye et al., 2023) train the demonstration retriever with a learnable conditional DPP. However, these existing works are highly dependent on large annotated support sets.\n# 7 Conclusion and Future Work\nIn this work, we focus primarily on an innovative selective annotation mechanism and introduce an efficient annotation practice, LM-DPP. It selects both diverse and low-uncertainty examples for annotation and demonstrates promising results in various LMs. Moreover, empirical results validate the\ngeneralizability of LM-DPP across model size and annotation budget scaling. In the future, we plan to apply LM-DPP to more NLP tasks and explore annotation-free selection methods.\n# Limitations\nThe proposed work still has some limitations.\nSelection Method. Previous studies have elucidated that low uncertainty ensures familiarity of the LLMs with the demonstrations (Gonen et al., 2022), while diversity ensures that the selected demonstrations may encompass a broad range of information, thereby enhancing the overall effectiveness of ICL (Margatina et al., 2023). However, we still lack pilot experiments tailored to these factors to examine their impact on ICL performance thoroughly.\nSelection Method. Previous studies have elucidated that low uncertainty ensures familiarity of the LLMs with the demonstrations (Gonen et al., 2022), while diversity ensures that the selected demonstrations may encompass a broad range of information, thereby enhancing the overall effectiveness of ICL (Margatina et al., 2023). However, we still lack pilot experiments tailored to these factors to examine their impact on ICL performance thoroughly. Retrieval Method. We have implemented prompt retrieval based on similarity (TopK). However, it is currently unclear whether the proposed method applies to other prompt retrieval methods, such as Random Retrieval, Coverage-based Retrieval (Gupta et al., 2023), and Retrieval based on Mutual Information (Sorensen et al., 2022). We plan to extend our work to cover more scenarios. Retriever. Retriever is indeed one of the variables in our experiments. However, we have solely employed a retriever based on the SentenceBert architecture. Validating our experimental results on a more diverse array of retrievers constitutes future extension work. Language. We also acknowledge that all datasets considered in this work are in English, which does not ensure that our work can be broadly generalized to other languages.\nLanguage. We also acknowledge that all datasets considered in this work are in English, which does not ensure that our work can be broadly generalized to other languages.\n# Potential Risk\nPrevious works have shown Large language models contain rich biased data (Bender et al., 2021). Since we use LLMs like LlaMA, GPT-J, and GPT3, the proposed LM-DPP approach may elicit some content with offensive language or discrimination.\n# References\nEmily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Association for Computing Machinery.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. TAC, 7:8.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. TAC, 7:8. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\n# Laming Chen, Guoxin Zhang, and Hanning Zhou. 2018. Fast greedy map inference for determinantal point process to improve recommendation diversity.\nLaming Chen, Guoxin Zhang, and Hanning Zhou. 2018. Fast greedy map inference for determinantal point process to improve recommendation diversity.\nakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\n# D. A. Cohn, Z. Ghahramani, and M. I. Jordan. 1996. Active learning with statistical models.\nGr\u00e9goire Del\u00e9tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. 2023. Language modeling is compression. arXiv preprint arXiv:2309.10668.\nWilliam Dolan, Chris Quirk, Chris Brockett, and Bill Dolan. 2004. Unsupervised construction of large\nparaphrase corpora: Exploiting massively parallel news sources.\nChenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and Min Yang. 2022. A survey of natural language generation. ACM Computing Surveys, 55(8):1\u201338. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning. Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. 2012. Near-optimal map inference for determinantal point processes. In Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc. Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity estimation. Shivanshu Gupta, Matt Gardner, and Sameer Singh. 2023. Coverage-based example selection for incontext learning. Eduard Hovy, Laurie Gerber, Ulf Hermjakob, ChinYew Lin, and Deepak Ravichandran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the first international conference on Human language technology research. Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2391\u20132401, Hong Kong, China. Association for Computational Linguistics. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332\u20139346, Online. Association for Computational Linguistics. Alex Kulesza. 2012. Determinantal point processes for machine learning. Foundations and Trends\u00ae in Machine Learning, 5(2\u20133):123\u2013286. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466.\n# Shivanshu Gupta, Matt Gardner, and Sameer Singh. 2023. Coverage-based example selection for incontext learning.\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, ChinYew Lin, and Deepak Ravichandran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the first international conference on Human language technology research.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2391\u20132401, Hong Kong, China. Association for Computational Linguistics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332\u20139346, Online. Association for Computational Linguistics.\nAlex Kulesza. 2012. Determinantal point processes for machine learning. Foundations and Trends\u00ae in Machine Learning, 5(2\u20133):123\u2013286.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942. Jon Lee, Vahab Mirrokni, Viswanath Nagarjan, and Maxim Sviridenko. 2009. Non-monotone submodular maximization under matroid and knapsack constraints. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S\u00f6ren Auer, et al. 2015. Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167\u2013195. Xiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What makes good in-context examples for gpt-3? Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Katerina Margatina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. 2023. Active learning principles for in-context learning with large language models. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7e7/a7e79dc5-5fb5-408f-a9f1-4e381a74e49e.png\" style=\"width: 50%;\"></div>\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax. Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. BigScience Workshop. 2023. Bloom: A 176bparameter open-access multilingual language model. Zhao Yang, Yuanzhe Zhang, Dianbo Sui, Cao Liu, Jun Zhao, and Kang Liu. 2023. Representative demonstration selection for in-context learning with twostage determinantal point process. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5443\u20135456, Singapore. Association for Computational Linguistics. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022a. Opt: Open pre-trained transformer language models. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models.\n# A Appendix\n# A.1 Details with perplexity estimation\nQNLI\n|L| = 16\n|L| = 100\nPerplexityavg\n75.16\n95.43\nPerplexitymax\n143.48\n278.62\nTable 6: Annotation Set (selected by Vote-k) Perplexity Statistics.\nWe report the perplexity of annotated instances when (|L| = {16, 100}) (as shown in Table 6). It\u2019s observed that as the annotation cost increases to 100, there is a corresponding significant rise in perplexity. For instance, in COPA, the Perplexityavg increases by 4.01, and Perplexitymax rises by 125.70. A similar phenomenon is also observed in DBpedia. This indicates to some extent that introducing demonstrations with high perplexity can lead to a decrease in ICL performance.\n# A.2 Implementation Details\nThe inference method we employed is direct (a regular inference used in (Brown et al., 2020)), which involves presenting demonstrations and candidate answers to the LLMs to select the candidate with the highest likelihood. For each test dataset, a specific prompt template (Table 12) is used for scoring and inference. For each test instance, we include as many retrieved samples as possible in the preceding prompt, up until the maximum token length was reached (e.g., 2048 for GPTJ, 4096 for LlaMA-2-7B). Sentence-BERT (Reimers and Gurevych, 2019) is used as the demonstration retriever. Following (Rubin et al., 2022), we adopt the paraphrase-mpnet-base-v2 to encode the test input xtest and the inputs of the train set. All experiments are conducted on a single Tesla V100 GPU with 32GB of memory. Empirically, obtaining embeddings for unlabeled examples using Sentence BERT as described in Section 2.1 varies between 0.2 to 2 hours, contingent upon the dataset size. In Section 2.2, our approach requires approximately 6 seconds to generate the annotation set on a single CPU. Notably, ICL obviates the need for model training.\nDataset\nTask Type\nSplit\nSST-5\nSentiment Classification\n8544/1101/2210\nRTE\nNatural Language Inference\n2491/277/3000\nMNLI\nNatural Language Inference\n392702/19647/19643\nMRPC\nNatural Language Inference\n3668/408/1725\nQNLI\nNatural Language Inference\n104743/5463/5463\nTREC\nTopic Classification\n5452/0/500\nDBpedia\nTopic Classification\n560000/0/70000\nHellaswag\nMultiple-choice Question Answering\n39905/10042/10003\nCOPA\nMultiple-choice Question Answering\n1000/0/500\nCosmosQA Multiple-choice Question Answering\n9471/1221/1140\nXSUM\nAbstractive Summarization\n204045/11332/11334\nNQ\nOpen Domain QA\n307373/7830/0\nTable 7: Dataset Statistics in the Experiments\nTable 7: Dataset Statistics in the Experiments.\nWe also acknowledge that acquiring unlabelled samples in practice is a process marked by significant variance(Su et al., 2022). To simulate this realistic scenario, we randomly sample 3K instances from the training set multiple times to serve as the pool of samples awaiting annotation. In all the experimental setups described in this paper, we utilize four distinct seeds (0, 1, 42, 123), and the values presented in the tables (figures) reflect the average across four runs. Additionally, we provide the corresponding standard deviations for these values.\n# A.3 Baselines\nRandom A randomly selected annotation baseline is necessary, as it directly picks unlabeled training instances at random. Ideally, data points selected by any heuristic algorithm should yield better performance compared to it.\nPerplexity (Gonen et al., 2022) reported that lower perplexity correlates with better performance. We rank candidate instances by their perplexity and select the top |L| instances with the lowest perplexity as our annotation set.\nK-means As a representative selection method in the series of diversity approaches, we employ clustering techniques. Following (Yu et al., 2022), we first encode all data points using an Encoder, then perform k-means clustering with |L| clusters and select instances accordingly.\nVote-k (Su et al., 2022) selects |L|/10 samples through a graph-based voting mechanism, after which the |L|/10 labeled samples are used as context for the LLMs, to calculate confidence scores for the other unlabeled candidate instances. Finally, the instances are grouped according to percentile\n  KPMG and the Recruitment and Employment Confederation (REC) reported that the rate of expansion in hiring employees sank to a four-month low The number of job vacancies made available also fell to their slowest in 2015. Although starting salaries for permanent employees continued to  grow, the pace of growth sank to its lowest since April's nine-month high. Recruitment agencies reported that the pay of temporary and  contracted staff also continued to grow, although at its slowest since January.  The availability of temporary staff saw its fastest drop in seven months, leading recruitment consultants to report difficulties in hiring suitable  people. KPMG partner Bernard Brown said: \"The UK job market saw a slight slowdown in May, as those on boards took time to digest the election  result and work out the ramifications for their business. \u201c The public sector continues to suffer, with pay growth rising by just 0.2% in the last  reported quarter.\" \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f943/f943550e-1261-40ee-b530-c254a329a21c.png\" style=\"width: 50%;\"></div>\nRouge F1: 43.24\nRouge F1: 58.06    \n# In-context evidence in LM-DPP\nThe availability of temporary staff saw its fastest drop in seven months, leading recruitment consultants to report difficulties in hiring suitable  people.  KPMG partner Bernard Brown said: \"The UK job market saw a slight slowdown in May, as those on boards took time to digest the election result  and work out the ramifications for their business.   \nexample  Figure 8: Case analysis in XSUM, we compare the performance of Random and LM-DPP on generation qua fact consistency\nquestion \u768411 v ranks of confidence scores, and selection is made through voting within each group.\nFast Vote-k A rapid and efficient alternative to Vote-k, it circumvents the use of LLMs to compute confidence scores. It directly selects the |L| samples with the highest voting scores.\n# A.4 Dataset Statistics\nTable 7 presents the data statistics of the datasets employed in our experiments.\n# A.5 Prompt Template\nThe prompt templates utilized for each task are reported in Table 12.\n# B High Uncertainty\nLM-DPPhigh_uncertainty\nRTE\nMNLI\nMRPC\nQNLI\nSST-5\n51.29\n42.91\n66.17\n52.30\n48.74\nDBpedia TREC HellaSwag COPA\n93.18\n81.40\n66.95\n83.80\nTable 8: Results of selecting high-uncertainty instances (GPTJ + annotation_size=100+LM-DPP). Improvements in high uncertainty are underlined.\n FactCC: 98.06\neditd1111da sdd ariables Apart from the MNLI and DBpedia datasets, selecting instances of high uncertainty led to a certain degree of performance degradation (Table 8). Therefore, we prioritize the selection of lowuncertainty instances in our experiments and hope to inspire further work in the area of perplexity estimation.\n# C Analysis and supplement\n# C Analysis and supplement C.1 Small Model for scoring\n# C.1 Small Model for scoring\n<div style=\"text-align: center;\">C.1 Small Model for scoring</div>\nLM-DPPgpt2_scoring\nRTE\nMNLI\nMRPC\nQNLI\nSST-5\n51.96\n41.79\n66.81\n51.43\n47.32\nDBpedia TREC HellaSwag COPA\nAvg\n90.67\n81.85\n67.94\n83.09\n64.76\nTable 9: Results of using GPT2 as a surrogate.\nTable 9: Results of using GPT2 as a surrogate.\nTable 9 presents the results of using GPT2 as a surrogate.\n# C.2 Fact Consistency in XSUM\nUpon closer analysis (as shown in Figure 8), we find that in pursuit of diversity and uncertainty in demonstrations, LM-DPP may retrieve content that is topically related but not completely factually\nExamples\nLM-DPP:\nequivalent, equivalent,\nequivalent, equivalent\nRandom:\nequivalent, not equivalent,\nnot equivalent, not equivalent\nTable 10: In MRPC, the four demonstration label examples selected by Random and LM-DPP.\nconsistent. For example, while the source text emphasizes a \"The UK job market saw a slight slowdown in May,\" the LM-DPP generated summary mentions \"fell in May,\" shifting the focal point of the information and potentially misleading readers to interpret a deterioration in actual employment conditions rather than a deceleration in growth rate. This discrepancy is also reflected in the context evidence cited by LM-DPP, which notes \"the availability of temporary staff saw its fastest drop in seven months,\" further reinforcing a negative portrayal of employment circumstances, despite not fully reflecting the source\u2019s focus or theme. We further observe that balancing the Rouge scores with FactCC scores, ensuring factual consistency while maintaining high levels of abstractiveness and textual similarity, presents a significant challenge for LM-DPP. This observation suggests that future research might need to explore more nuanced demonstration selection strategies or introduce stronger fact-checking and correction mechanisms to mitigate the potential risks to factual consistency arising from the pursuit of diversity and uncertainty. This provides valuable insights on how to further optimize the method moving forward.\n# C.3 Impact of label coverage\nAt L = 4, the Acc. of Random and LM-DPP on MRPC and TREC are respectively (47.30, 40.63) and (61.36, 49.64). Combined with Tables 10 and 11, it can be seen that as the label coverage increases, performance on MRPC decreases, while TREC shows an expected pattern. This may be related to the difficulty of the task; moreover, from the perspective of data, an imbalanced label distribution might more closely approximate the statistical characteristics of real-world data. In certain cases, imbalanced examples could reflect key signals of specific categories, aiding the model in learning effective decision boundaries more swiftly. We look forward to further research in this area.\nRandom\nInput: What are the factors leading to the high teen preg-\nnancy rate in Spartanburg , South Carolina?\nLabel: description and abstract concept\nInput: Who invented Make-up ?\nLabel: human being\nInput: Who is the current UN Secretary General ?\nLabel: human being\nInput: What does God create in the first sentence of the\nBible ?\nLabel: entity\nLM-DPP\nInput: How much caffeine is in a 16 oz cup of coffee ?\nLabel: numeric value\nInput: What is the fastest growing state in the U.S.A. in\n1998 ?\nLabel: location\nInput: What British female pop singing star of the 1960s\nand early 1970s was a child actress in the 1940s and \u201950s\nLabel: human being\nInput: Why was Muhammad Ali stripped of his title and\nbarred from boxing in 1967 ?\nLabel: description and abstract concept\nRandom\nInput: What are the factors leading to the high teen preg-\nnancy rate in Spartanburg , South Carolina?\nLabel: description and abstract concept\nInput: Who invented Make-up ?\nLabel: human being\nInput: Who is the current UN Secretary General ?\nLabel: human being\nInput: What does God create in the first sentence of the\nBible ?\nLabel: entity\nLM-DPP\nInput: How much caffeine is in a 16 oz cup of coffee ?\nLabel: numeric value\nInput: What is the fastest growing state in the U.S.A. in\n1998 ?\nLabel: location\nInput: What British female pop singing star of the 1960s\nand early 1970s was a child actress in the 1940s and \u201950s\nLabel: human being\nInput: Why was Muhammad Ali stripped of his title and\nbarred from boxing in 1967 ?\nLabel: description and abstract concept\nTable 11: In TREC, the four demonstration examples selected by Random and LM-DPP.\nDataset\nPrompt Template\nExample\nSST-5\nHow do you feel about the following sentence?\n\\n {Input} \\n answer:{Output}\nInput: this is a stunning film, a one-of-a-kind tour de force.\nOutput: very positive\nRTE\n{Input1}. Based on that information, is the claim\n{Input2} \"entailment\", or \"contradiction\"? \\n answer:{Output}\nInput1: No Weapons of Mass Destruction Found in Iraq Yet.\nInput2: Weapons of Mass Destruction Found in Iraq.\nOutput: contradiction\nMNLI\n{Input1}. Based on that information, is the claim\n{Input2} \"True\", \"False\", or \"Inconclusive\"? \\n answer:{Output}\nInput1: Good luck, my friends.\nInput2: I wish my friends luck.\nOutput: True\nMRPC\nAre the following two sentences \"equivalent\" or \"not equivalent\"?\n\\n {Input1}.\\n {Input2}. \\n answer:{Output}\nInput1: Staff writer Dave Michaels contributed to this report.\nInput2: Staff writers Frank Trejo and Robert Ingrassia contributed to this report.\nOutput: equivalent\nBoolQ\n{Input1}. Based on that information, is the claim\n{Input2} \"True\", or \"False\"? \\n answer:{Output}\nInput1: is there going to be another season of Britannia.\nInput2: In March 2018, is was announced that Sky Atlantic had renewed the show for a second season.\nOutput: True\nQNLI\n{Input1}. Based on that information, is the claim\n{Input2} \"entailment\", or \"contradiction\"? \\n answer:{Output}\nInput1: About 40,000,000 tons were produced in 1984.\nInput2: How many tons of bitumen ere produced in 1984?\nOutput: entailment\nTREC\ncontent: {Input} \\n {Output}\nInput: What films featured the character Popeye Doyle ?\nOutput: entity\nDBpedia\ntitle: {Input1}; content: {Input2} \\n {Output}\nInput1: Panay Technological College\nInput2: Panay Technological College is a higher institution in Kalibo Aklan.\nOutput: educational institution\nHellaswag\nThe topic is {Input1}. {Input2} \\n {Output}\nInput1: Hurling\nInput2: A group of lacrosse players are shown on a field. they\nOutput: run around, trying to get the ball away from each other.\nCOPA\n{Input2}. What was the {Input1} of this? \\n {Output}\nInput1: cause\nInput2: My body cast a shadow over the grass.\nOutput: The sun was rising.\nCosmosQA\n{Input1}. {Input2} \\n {Output}\nInput1: El dropped me off at B. \u2019s house. She welcomed El . and me into her home .\nInput2: Why did she welcome us into the house ?\nOutput: She liked us and enjoys our company .\nSubj\nInput: {Input}. \\n Type: {Output}\nInput: katie is a young girl who loves to climb .\nOutput: objective\nXSUM\nwrite a short summary:\\n {Input}. \\n TL;DR: {Output}\nInput: A lone hiker salutes the aptly named Wet Sleddale Reservoir in Cumbria, as it overflows down a 21 metre high dam wal\nOutput: Photograph by Jeff Overs / BBC\nNQ\nWrite an answer: {Input} \\n {Output}\nInput: who is credited with creating the gothic art movement\nOutput: Abbot Suger\ne 12: Prompt templates and corresponding examples used in each datas\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in large language models (LLMs), which traditionally relies on large-scale labeled datasets that are often impractical to obtain. The authors propose a selective annotation mechanism to optimize the selection of demonstration examples for ICL, thus reducing dependency on extensive labeled data.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of effective demonstration selection for in-context learning, particularly when available labeled data is limited.",
            "key obstacle": "The main obstacle is the reliance on large labeled support sets, which are not always feasible in practical scenarios, leading to suboptimal performance in ICL."
        },
        "idea": {
            "intuition": "The authors were inspired by the need for a more efficient way to select demonstration examples that balance uncertainty and diversity, leading to improved performance in ICL.",
            "opinion": "The proposed idea involves a novel selective annotation mechanism using the Language Model-based Determinantal Point Process (LM-DPP), which selects examples based on their uncertainty and diversity.",
            "innovation": "The innovation lies in the introduction of LM-DPP, which jointly models uncertainty and diversity, marking a significant improvement over existing methods that focus solely on one of these factors."
        },
        "method": {
            "method name": "Language Model-based Determinantal Point Process",
            "method abbreviation": "LM-DPP",
            "method definition": "LM-DPP is a method for selecting a subset of unlabeled instances for annotation that maximizes the effectiveness of in-context learning by balancing uncertainty and diversity.",
            "method description": "The core of LM-DPP is to select instances that exhibit both high diversity and low uncertainty to optimize performance in ICL.",
            "method steps": [
                "Score candidate instances using the perplexity of the LLMs.",
                "Compute vector representations for the candidate instances using a conditional kernel matrix.",
                "Perform greedy MAP inference to filter the candidate annotation set."
            ],
            "principle": "The principle behind LM-DPP's effectiveness is that it combines measures of uncertainty (via perplexity) and diversity (via a Gram matrix) to select the most useful subset of instances for annotation."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on 9 NLU and 2 Generation tasks, with annotation budgets set at 16 and 100 labeled examples. Various baseline methods were used for comparison.",
            "evaluation method": "The performance of the method was assessed by comparing the predicted outcomes against true labels, reporting accuracy for NLU tasks and exact matching scores for QA tasks."
        },
        "conclusion": "The experimental results demonstrate that LM-DPP significantly improves the performance of in-context learning across various datasets and models, showing that balancing uncertainty and diversity leads to better outcomes.",
        "discussion": {
            "advantage": "The key advantages of LM-DPP include its ability to reduce the need for large labeled datasets while still enhancing performance through effective demonstration selection.",
            "limitation": "One limitation of the method is that it may not perform as well in scenarios with high uncertainty and low diversity, which could negatively impact ICL performance.",
            "future work": "Future research will focus on applying LM-DPP to additional NLP tasks and exploring annotation-free selection methods."
        },
        "other info": {
            "info1": "The method has been validated across multiple language models including GPT-J, LlaMA, and GPT-3.",
            "info2": {
                "info2.1": "The approach shows promise in low-resource ICL scenarios.",
                "info2.2": "The authors suggest further investigation into the trade-offs between diversity and uncertainty in demonstration selection."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of in-context learning (ICL) in large language models (LLMs), which traditionally relies on large-scale labeled datasets that are often impractical to obtain."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea involves a novel selective annotation mechanism using the Language Model-based Determinantal Point Process (LM-DPP), which selects examples based on their uncertainty and diversity."
        },
        {
            "section number": "2",
            "key information": "The problem defined in this paper is the challenge of effective demonstration selection for in-context learning, particularly when available labeled data is limited."
        },
        {
            "section number": "3.3",
            "key information": "The core of LM-DPP is to select instances that exhibit both high diversity and low uncertainty to optimize performance in ICL."
        },
        {
            "section number": "4.2",
            "key information": "The key advantages of LM-DPP include its ability to reduce the need for large labeled datasets while still enhancing performance through effective demonstration selection."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the method is that it may not perform as well in scenarios with high uncertainty and low diversity, which could negatively impact ICL performance."
        },
        {
            "section number": "7",
            "key information": "The experimental results demonstrate that LM-DPP significantly improves the performance of in-context learning across various datasets and models, showing that balancing uncertainty and diversity leads to better outcomes."
        }
    ],
    "similarity_score": 0.7258239705412218,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process.json"
}