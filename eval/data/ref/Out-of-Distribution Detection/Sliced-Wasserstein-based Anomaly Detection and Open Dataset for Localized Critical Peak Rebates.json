{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.21712",
    "title": "Sliced-Wasserstein-based Anomaly Detection and Open Dataset for Localized Critical Peak Rebates",
    "abstract": "In this work, we present a new unsupervised anomaly (outlier) detection (AD) method using the sliced-Wasserstein metric. This filtering technique is conceptually interesting for MLOps pipelines deploying machine learning models in critical sectors, e.g., energy, as it offers a conservative data selection. Additionally, we open the first dataset showcasing localized critical peak rebate demand response in a northern climate. We demonstrate the capabilities of our method on synthetic datasets as well as standard AD datasets and use it in the making of a first benchmark for our open-source localized critical peak rebate dataset.",
    "bib_name": "pallage2024slicedwassersteinbasedanomalydetectionopen",
    "md_text": "# Sliced-Wasserstein-based Anomaly Detection and Open Dataset for Localized Critical Peak Rebates\nChristophe B\u00e9langer Open Data | Data and Analytics Hydro-Qu\u00e9bec\n# Abstract\nIn this work, we present a new unsupervised anomaly (outlier) detection (AD) method using the sliced-Wasserstein metric. This filtering technique is conceptually interesting for integration in MLOps pipelines deploying trustworthy machine learning models in critical sectors like energy. Additionally, we open the first dataset showcasing localized critical peak rebate demand response in a northern climate. We demonstrate the capabilities of our method on synthetic datasets as well as standard AD datasets and use it in the making of a first benchmark for our open-source localized critical peak rebate dataset.\narXiv:2410.21712v1\n# 1 Introduction\nQu\u00e9bec, Canada, stands as an outlier in the electrical grid decarbonization paradigm. As there is a global tendency to invest in intermittent renewable energy sources to decarbonize grids worldwide, Qu\u00e9bec, while not totally stranger to this trend, is generally mostly carbon-free thanks to its impressive hydroelectric power capacity. One of its main challenges comes from its northern climate, its reliance on electric baseboard heating systems for residential heating, and its unrestrictive home insulation policies [1]. During glacial winter days, as electric heaters are all running at once, and peak consumption hours hit, Qu\u00e9bec\u2019s hydropower capacity can be reached [2]. To accommodate winter peak power needs, Hydro-Qu\u00e9bec, the state-owned company in charge of electric power generation, transmission, and distribution, must operate its only on-grid thermal power plant and import electricity from neighbouring provinces and states. These imports are usually expensive and produce much more greenhouse gas emissions in comparison to local energy sources [3]. To remediate this issue without solely relying on the deployment of new generation and transmission infrastructures, demand response (DR) initiatives have flourished in the province [4]. Demand response can be defined as changes in normal electrical power consumption patterns by end-use customers in response to a signal, e.g., financial incentives or control setpoints [5]. With Qu\u00e9bec\u2019s long tradition of fixed electricity rates, one of its main DR mechanisms is critical peak rebates (CPR). Residential customers enrolled in a CPR program receive financial compensation during pre-specified time periods, referred to as challenges, for reducing their energy consumption with respect to their expected baseload [6]. CPR programs are purely voluntary and virtually penalty-free. They thus depend on consumers\u2019 goodwill, motivation, and sensitivity. Yet, as we have seen in our work, CPR events can be powerful tools for shifting power consumption before and after each event.\n\u2217Corresponding author: julien.pallage@polymtl.ca.\nTackling Climate Change with Machine Learning workshop at NeurIPS 2024\nWe work with a variation of CPR, viz., localized CPR (LCPR), in which the events are called for localized relief in the grid instead of being cast for the whole system. LCPR can diversify the types of services offered by typical CPR programs, e.g., they can alleviate stress on local equipment like substation transformers [7] or they can be paired with generation forecasts of distributed energy resources (DERs), e.g., roof solar panels and private windfarms, to balance demand and generation during peak hours. LCPR is highly underexplored in the literature and is a valuable application to benchmark trustworthy machine learning (ML) models. Indeed, the higher spatial granularity, the critical aspect of the task, the dependence on behavioural tendencies, and the lower margin for error require the deployment of forecasting models that offer performance guarantees [8], robustness to noise [9], interpretability [10], physical constraints satisfaction [11], a sense of prediction confidence [12, 13], or a combination of them [14]. Being able to predict and utilize localized peak-shaving potential in the electrical grid, through programs similar to LCPR, could accelerate the integration of DERs and, specifically for Qu\u00e9bec, phase out its dependence on fossil energy-based imports. To the best of our knowledge, no published open-source datasets are showcasing either LCPR or CPR schemes, in a northern climate. As with most smart grid applications, dataset quality can highly influence the performance of ML models when used for training. The adversarial properties stemming from the amalgam of numerical errors, noise in sensor readings, telemetry issues, meter outages, and unusual extreme events can disrupt the prediction quality of ML models. Anomaly detection (AD) and outlier filtering are thus primordial in an MLOps pipeline to enhance model robustness [15]. Unsupervised AD methods are preferable as they do not need human-made labels and their hyperparameters can be tuned simultaneously with other ML models\u2019 included in the loop. Popular unsupervised AD methods [16] include local outlier factor (LOF) [17], isolation forest [18], k-nearest neighbours (KNN) [19], connectivity-based outlier factor [20], and one-class support vector machine (SVM) [21]. These methods either use clustering or local density to assign an outlier score. We are interested in optimal transport-based (OT) metrics for AD. Reference [22] proposed a Wasserstein generative adversarial network for AD tasks and authors from [23] have designed a differentiable training loss function using the Wasserstein metric for deep classifiers. To the best of our knowledge, no unsupervised OT method has been proposed yet for AD. In this work, we address the lack of open-source datasets showcasing CPR demand response mechanisms in northern climates by releasing two years of aggregated consumption data for customers participating in an LCPR program in Montr\u00e9al, Qu\u00e9bec, Canada2. These customers are spread in three adjacent distribution substations. With it, we hope to stimulate research on trustworthy ML models for demand response applications. We also address the challenges of training ML models with unfiltered smart grid data by proposing a new simple unsupervised outlier filter leveraging the Sliced-Wasserstein distance [24]. We showcase the performance of this filter on standard anomaly detection datasets and leverage it to present a benchmark for the prediction of the localized energy consumption of LCPR participants on our open-source dataset.\n# 2 Localized critical peak rebates in Qu\u00e9bec\nQu\u00e9bec\u2019s CPR and LCPR programs have been carried out under the banner of Hilo, a division of Hydro-Qu\u00e9bec in charge of DR aggregation. Hilo, calls a DR challenge a day ahead of the event, and users choose their degree of participation for the next day. Hilo subscribers are equipped with smart thermostats and their respective heating setpoints are controlled by Hilo, according to an agreed-upon strategy, throughout the event. With the Hilo mobile application and different connected objects, customers can program the response of their house to different scenes. For example, they can choose the heating setpoint of each thermostat when there is a DR event, or when nobody is home. When notified, smart thermostats to augment user comfort during events which typically last 4 hours: either between 6 AM and 10 AM or 5 PM and 9 PM. A maximum of 30 CPR events can be called per year. LCPR is an additional program currently under testing. Testers can be asked for up to 10 extra LCPR events. Rewards are proportional to the total amount of energy shaved during the event, i.e., heating-related curtailment and others, with respect to their estimated baselines [25].\n2Will be available December 14th, 2024\n# 3 Open dataset\nDescription The dataset we share contains the aggregated hourly consumption of 197 anonymous LCPR testers located in three substations. Additional hourly weather data and LCPR information are also present. Table 1 details the features and the label. Note that we also provide cyclical encoding of temporal features, e.g., month, day of the week, and hour. We remark that outliers and anomalies are present in the dataset because of metering and telemetry issues or even blackouts, e.g., an aberrant (and impossible) 32.2 MWh energy consumption is registered at some point. We refer readers to Appendix A for additional analyses and visualizations of the dataset\u2019s features and labels. Open data initiatives at Hydro-Qu\u00e9bec Hydro-Qu\u00e9bec estimates that the yearly energy demand in Qu\u00e9bec will double by 2050. This translates to an additional production of 60 TWh by 2035 and the addition of 8 to 9 GW of peak power to Qu\u00e9bec\u2019s 38 GW capacity [26]. Qu\u00e9bec\u2019s energy landscape must evolve rapidly to meet the increasing demand and DR is a powerful tool to mitigate infrastructure growth. Hydro-Qu\u00e9bec recognizes the importance of sharing data to foster research, innovation, and informed decision-making. Through its plateform3, it provides historical and real-time data on energy production and consumption in Qu\u00e9bec as well as other datasets, e.g., geolocated hydrometeorological data from its remote weather stations. This new dataset is Hydro-Qu\u00e9bec\u2019s first dedicated to DR research. By supporting data democratization, Hydro-Qu\u00e9bec encourages the emergence of a culture of transparency and openness in the energy sector which is needed to accelerate the energy transition. This approach aims to stimulate researchers, citizens, and businesses in their desire to participate in the design of innovative applications for a more sustainable, interoperable, reliable, and safe power grid.\n# 4 Sliced-Wasserstein Filter\nThe Wasserstein distance is a metric that provides a sense of distance between two distributions. It is also called the optimal transport plan because it can be conceived as the minimal effort that it would take to displace a pile of a weighted resource (\ud835\udc511) to shape a specific second pile (\ud835\udc512) [14]. The order-\ud835\udc61Wasserstein distance between distributions U and V is defined as: \ufffd \u222b \ufffd\nwhere z1 and z2 are the marginals of U and V, respectively, P(Z) is the space of all probability distributions with support on Z, and Z is the set of all possible values of z1 and z2 [9]. In general, computing the Wasserstein distance is of high computational complexity [27] except for some special cases. For example, the Wasserstein distance for one-dimensional distributions has a closed-form solution that can be efficiently approximated. The sliced-Wasserstein (SW) distance is a metric that makes use of this property by calculating infinitely many linear projections of the high-dimensional distribution onto one-dimensional distributions and then computing the average of the Wasserstein distance between these one-dimensional representations [24, 27]. Interestingly, it possesses similar theoretical properties to the original Wasserstein distance while being computationally tractable [28]. The order-\ud835\udc61SW distance\u2019s approximation under a Monte Carlo scheme is defined as: \ufffd \ufffd\n\ud835\udc46\ud835\udc4a\u2225\u00b7\u2225,\ud835\udc61(U, V) \u2248 \ufffd 1 \ud835\udc3f \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc4a\u2225\u00b7\u2225,\ud835\udc61(RU (\u00b7, \ud835\udf03\ud835\udc59) , RV (\u00b7, \ud835\udf03\ud835\udc59))\ud835\udc61 \ufffd1/\ud835\udc61 , where RD(\u00b7) denotes the Radon transform of distribution function D(\u00b7). Interested readers are referred to references [24, 27] for in-depth mathematical explanations. We now utilize this approximation of the sliced-Wasserstein distance to formulate a simple outlier filter. Consider an empirical distribution \u02c6P\ud835\udc41= 1 \ud835\udc41 \ufffd\ud835\udc41 \ud835\udc56=1 \ud835\udeffz\ud835\udc56(z), where \ud835\udeffz\ud835\udc56(\u00b7) is a Dirac delta function assigning a probability mass of one at each known sample z\ud835\udc56\u2208R\ud835\udc51. Consider the notation \u02c6P\u2212z\ud835\udc56 \ud835\udc41\u22121 to denote a variation of \u02c6P\ud835\udc41in which we remove sample z\ud835\udc56. As the weight of each sample is identical and because the SW distance compares distributions of equal weights, we propose an outlier filter by\n3https://www.hydroquebec.com/documents-data/open-data/\nusing a simple voting system in which we compare the SW distance between the empirical distribution minus the outlier candidate and the empirical distribution minus a random sample. Let O \u2286D and O\ud835\udc50\u2286D denote the sets of outlier and inlier samples, respectively, under the perspective of the filter and let D = O \u222aO\ud835\udc50\u2282R\ud835\udc51be the available dataset. A vote is positive if the distance between the two empirical distributions exceeds the threshold \ud835\udf16> 0. A sample is labelled an outlier if the proportion of positive votes is greater than the threshold \ud835\udc5d. \uf8f1 \ufffd\ufffd \ufffd \ufffd \uf8fc\n\uf8f4\uf8f2 \uf8f4\uf8f3 \ufffd\ufffd\ufffd\ufffd \uf8f4\uf8fd \uf8f4\uf8fe where 1(\u00b7) denotes an indicator function, \ud835\udc5bis the number of points used for the vote, \ud835\udc5d\u2208[0, 1] is the voting threshold required to label a sample as an outlier, and \u27e6\ud835\udc41\u27e7\u2261{1, 2, . . . , \ud835\udc41}. This filter is interesting for several reasons. It is unsupervised, purely analytical, and uses a well-known explainable mathematical distance to filter data points that seem out-of-sample under the SW metric. The intuition is that we remove samples that are costly in the transportation plan when compared to other random samples of the same distribution. We can use the voting percentage to measure the algorithm\u2019s confidence in its labelling. It is also parallelizable, as seen in our implementation provided on our GitHub page4. Numerical results and figures are presented in Appendix B. A downside of this method is that it does not scale with large datasets: the SW distance computational burden increases as bigger distributions are compared. Splitting the dataset randomly and filtering each split separately accelerates the process, but nuances may be lost. We remark that the transportation plan between a distribution minus a sample and the same distribution minus another sample can be roughly approximated by the Euclidian distance between the two removed samples. As such, we introduce a fast Euclidian approximation for \ud835\udc4a\u2225\u00b7\u22252,1 using this idea: \uf8f1 \ufffd\ufffd \uf8fc\n\uf8f4\uf8f2 \uf8f4\uf8f3 \ufffd\ufffd\ufffd\ufffd \u2211\ufe01  \ufffd\ufffd \uf8f4\uf8fd \uf8f4\uf8fe where \ud835\udf02is the threshold of the Euclidian distance. This method is not as accurate as the first proposal to filter out-of-sample data points. But, because (SWAD) and (FEAD) use the same principle, they share similar classification patterns when \ud835\udf16and \ud835\udf02are tuned accordingly, as can be seen in Figure 7.\n# 5 Benchmark\nWe now propose a first benchmark on our LCPR dataset. Our goal is to predict the aggregated hourly consumption at each substation in winter when peak demand is critical. To follow the literature [29], and propose a simple yet meaningful benchmark, we implement a Gaussian process [30] in a rolling horizon. Samples dating before 2023-12-15 are used for hyperparameter tuning while those between 2023-12-15 and 2024-04-15, during the most recent winter DR season, are used for testing. We train one model per week and the training window size is a hyperparameter to be tuned. The SW filter is used to preprocess data in the tuning phase and increase generalization in testing. Our method is interesting because it can filter clear out-of-sample points without filtering samples from sparse LCPR events which could be considered as local outliers by other methods. We refer interested readers to our Github page for more details. Testing mean average errors (MAE) and root mean squared errors (RMSE) are presented in Table 2 for each substation. See Appendix C for additional figures.\n# 6 Closing remark\nIn this work, we present a new unsupervised outlier filter by leveraging the sliced-Wasserstein metric. This filter is interesting for MLOps integration on applications where global outliers may be adversarial to the prediction quality of trained models, e.g., smart grid data. We also hope to stimulate research on trustworthy ML models in critical sectors by releasing the first open dataset showcasing localized critical peak rebate demand response schemes in a northern climate. This dataset has a strong potential for benchmarking of such models as it opens a window to a real-world critical application where accuracy and robustness are equally important. To get the ball rolling, we provide a first benchmark by tuning simultaneously our SW filter and a Gaussian process.\n4https://github.com/jupall/swfilter\n(SWAD)\n(FEAD)\n[1] T. Gerbet, \u201cPerformance \u00e9nerg\u00e9tique des b\u00e2timents : Qu\u00e9bec manque de courage, d\u00e9plorent des experts,\u201d Radio-Canada, Sep 2023. [2] J. Whitmore and P.-O. Pineau, \u201c\u00c9tat de l\u2019\u00e9nergie au Qu\u00e9bec,\u201d 2024, pr\u00e9par\u00e9 pour le gouvernement du Qu\u00e9bec. [3] Hydro-Qu\u00e9bec, Une \u00e9nergie renouvelable pour un Qu\u00e9bec durable, Rapport sur le d\u00e9veloppement durable, 2023. [4] F. Pelletier and A. Faruqui, \u201cDoes dynamic pricing work in a winter-peaking climate? A case study of Hydro Quebec,\u201d The Electricity Journal, vol. 35, no. 2, p. 107080, 2022. [5] M. H. Albadi and E. F. El-Saadany, \u201cDemand Response in Electricity Markets: An Overview,\u201d in 2007 IEEE Power Engineering Society General Meeting, 2007, pp. 1\u20135. [6] A. Mercado, R. Mitchell, S. Earni, R. Diamond, and E. Alschuler, \u201cEnabling interoperability through a common language for building performance data,\u201d Proceedings of the 2014 ACEEE Summer Study on Energy Efficiency in Buildings, 2014. [7] F. Li, I. Kocar, and A. Lesage-Landry, \u201cA Rapid Method for Impact Analysis of Grid-Edge Technologies on Power Distribution Networks,\u201d IEEE Transactions on Power Systems, vol. 39, no. 1, pp. 1530\u20131542, 2024. [8] A. Venzke and S. Chatzivasileiadis, \u201cVerification of neural network behaviour: Formal guarantees for power system applications,\u201d IEEE Transactions on Smart Grid, vol. 12, no. 1, pp. 383\u2013397, 2021. [9] R. Chen and I. C. Paschalidis, \u201cDistributionally robust learning,\u201d Foundations and Trends\u00ae in Optimization, vol. 4, no. 1-2, pp. 1\u2013243, 2020. 10] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. A. Specter, and L. Kagal, \u201cExplaining Explanations: An Approach to Evaluating Interpretability of Machine Learning,\u201d CoRR, vol. abs/1806.00069, 2018. 11] G. S. Misyris, A. Venzke, and S. Chatzivasileiadis, \u201cPhysics-informed neural networks for power systems,\u201d in 2020 IEEE Power & Energy Society General Meeting (PESGM), 2020, pp. 1\u20135. 12] L. V. Jospin, H. Laga, F. Boussaid, W. Buntine, and M. Bennamoun, \u201cHands-on bayesian neural networks\u2014a tutorial for deep learning users,\u201d IEEE Computational Intelligence Magazine, vol. 17, no. 2, pp. 29\u201348, 2022. 13] A. G. Wilson and P. Izmailov, \u201cBayesian deep learning and a probabilistic perspective of generalization,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 4697\u20134708, 2020. 14] J. Pallage and A. Lesage-Landry, \u201cWasserstein Distributionally Robust Shallow Convex Neural Networks,\u201d arXiv preprint arXiv:2407.16800, 2024. 15] J. Stiasny, S. Chevalier, R. Nellikkath, B. S\u00e6varsson, and S. Chatzivasileiadis, \u201cClosing the loop: A framework for trustworthy machine learning in power systems,\u201d Proceedings of 11th Bulk Power Systems Dynamics and Control Symposium (IREP 2022), 2022. 16] M. Goldstein and S. Uchida, \u201cA Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data,\u201d PLOS ONE, vol. 11, no. 4, pp. 1\u201331, 04 2016. 17] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, \u201cLOF: identifying density-based local outliers,\u201d Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, p. 93\u2013104, 2000. 18] F. T. Liu, K. M. Ting, and Z.-H. Zhou, \u201cIsolation Forest,\u201d in 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. 413\u2013422.\n[19] F. Angiulli and C. Pizzuti, \u201cFast Outlier Detection in High Dimensional Spaces,\u201d Proceedings of the Sixth European Conference on the Principles of Data Mining and Knowledge Discovery, vol. 2431, pp. 15\u201326, 08 2002. [20] J. Tang, Z. Chen, A. W.-C. Fu, and D. W.-L. Cheung, \u201cEnhancing Effectiveness of Outlier Detections for Low Density Patterns,\u201d in Proceedings of the 6th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, ser. PAKDD \u201902. Berlin, Heidelberg: Springer-Verlag, 2002, p. 535\u2013548. [21] M. Amer, M. Goldstein, and S. Abdennadher, \u201cEnhancing one-class support vector machines for unsupervised anomaly detection,\u201d Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description, p. 8\u201315, 2013. [22] M. Ducoffe, I. Haloui, and J. S. Gupta, \u201cAnomaly detection on time series with Wasserstein GAN applied to PHM,\u201d International Journal of Prognostics and Health Management, vol. 10, no. 4, 2019. [23] Y. Wang, W. Sun, J. Jin, Z. Kong, and X. Yue, \u201cWOOD: Wasserstein-Based Out-of-Distribution Detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 2, pp. 944\u2013956, 2024. [24] N. Bonneel, J. Rabin, G. Peyr\u00e9, and H. Pfister, \u201cSliced and Radon Wasserstein barycenters of measures,\u201d Journal of Mathematical Imaging and Vision, vol. 51, pp. 22\u201345, 2015. [25] Hilo. (2023, 09) Residential. [Online]. Available: https://www.hiloenergie.com/en-ca/ [26] Hydro-Qu\u00e9bec, Action Plan 2035, Towards a Decarbonized and Prosperous Qu\u00e9bec, 2023. [27] S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde, \u201cGeneralized sliced wasserstein distances,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019. [28] N. Bonnotte, \u201cUnidimensional and evolution methods for optimal transportation,\u201d Ph.D. dissertation, Universit\u00e9 Paris Sud-Paris XI; Scuola normale superiore (Pise, Italie), 2013. [29] Y. Weng and R. Rajagopal, \u201cProbabilistic baseline estimation via Gaussian process,\u201d in 2015 IEEE Power & Energy Society General Meeting, 2015, pp. 1\u20135. [30] C. Williams and C. Rasmussen, \u201cGaussian processes for regression,\u201d Advances in neural information processing systems, vol. 8, 1995. [31] C. Spearman, \u201cThe Proof and Measurement of Association Between Two Things,\u201d American Journal of Psychology, vol. 15, pp. 88\u2013103, 1904. [32] T. Chen and C. Guestrin, \u201cXGBoost: A Scalable Tree Boosting System,\u201d in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201916. New York, NY, USA: Association for Computing Machinery, 2016, p. 785\u2013794. [33] S. M. Lundberg and S.-I. Lee, \u201cA unified approach to interpreting model predictions,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS\u201917. Red Hook, NY, USA: Curran Associates Inc., 2017, p. 4768\u20134777. [34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011. [35] G. O. Campos, A. Zimek, J. Sander, R. J. Campello, B. Micenkov\u00e1, E. Schubert, I. Assent, and M. E. Houle, \u201cOn the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study,\u201d Data Mining and Knowledge Discovery, vol. 30, pp. 891\u2013927, 2016.\n# A Detailed analysis of the dataset\nIn this section, we present some additional insights on the LCPR dataset. Figure 1 presents the distribution count of some key features for each substation. We observe that meteorological features are identical for each substation as they are geographically adjacent and located in dense neighborhoods.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df97/df97abfd-af9b-4d0e-a029-3158f92a4d83.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Distribution of key features for each substation</div>\nFigure 2 shows a correlation heatmap of important features and label for each substation. We observe that each substation follows the same general tendencies. We remark significant correlations between the energy consumed, the month of the year, the outside temperatures, and the temperature setpoints. This is also highlighted in Figure 3 which presents the Spearman coefficients ranking [31] between each feature and the label. A positive sign indicates that both the label and the feature grow or decrease in the same direction while a negative sign indicates an opposite direction. The coefficients are ranked in decreasing importance from left to right. To have a more nuanced analysis of the contribution of each feature to the output, we also provide in Figure 4 an analysis of Shapley values of a trained extreme gradient boosting model (XGBoost) [32] for each substation. These analyses were realized with the Python package SHAP [33]. As we see, some lower-ranked features, viz., the challenge flags, sometimes have a strong impact on the model\u2019s output even though their general impact is null.\n# B Numerical study of the Sliced-Wasserstein Filter\nWe first begin this section by showing the AD mechanism of the SW filter on a simple two-dimensional example. We generate three Gaussian distributions with different population sizes. As shown in Figure 5, the first distribution is the majority group, the second is the minority group, and the third represents clear statistical outliers. We now merge the three distributions into a single one to test our SW filter. We vary the radius of the Wasserstein ball to see how the filter behaves. The results are presented in Figure 6.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9cb9/9cb9a02f-2af6-430e-b8f3-ac1f974280de.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Substation C</div>\n<div style=\"text-align: center;\">Figure 2: Correlation heatmap of key features and label for each substati</div>\nAs we see, we can generate three filtering scenarios by modifying the value of \ud835\udf16. With \ud835\udf16= 0.1, we filter only the statistical outliers. With \ud835\udf16= 0.05, we only keep the majority group. And, with \ud835\udf16= 0.01, we only keep the samples closest to the barycenter of the majority group. In Figure 7, we compare different AD algorithms on synthetic datasets provided in scikit-learn\u2019s example collection [34]. Hyperparameters are fixed for each algorithm to see how a single hyperparameter choice influences the labelling on each dataset. As can be seen, the SW filter and its fast Euclidian approximation are better at isolating outliers when there is a clear majority group but are not as precise in identifying local outliers based on local density, e.g., one-class SVM. Finally, we run a more thorough experiment with typically used real-world benchmark datasets. We run experiments on the Lymphography, Ionosphere, Glass, Shuttle, WPBC, Arrhythmia, and Pima, datasets presented in [35] for AD benchmarking. We compare isolation forest and LOF to our method. We implement a grid search and select each model\u2019s run with the best accuracy \ud835\udc34. We then extract the precision score \ud835\udc43from this run. The performance indicators are defined as follows:\n      where \ud835\udc47p, \ud835\udc39p, \ud835\udc47n, and \ud835\udc39n stand for true positives, false positives, true negatives, and false negatives, respectively. Results are presented in Figure 8. We observe a similar performance between each model, except on the Ionosphere dataset where the SW filter lags behind the other models, and on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0dc5/0dc552a8-9526-4fd3-a95c-8243160c1ece.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Spearman coefficients of key features for each substation</div>\nArrhythmia where the fast Euclidian approximation is underperforming. The SW filter\u2019s strength is that it considers the global distributional properties of the population to guide its labelling. Every experiment is available on our GitHub page.\n# C Supplementary content to the benchmark\nThis section presents visual test predictions of the best validation runs for each substation as well as the test error of the benchmark. As can be seen in both Figure 9 and Table 2, substation C is the hardest to predict for the Gaussian process. In general, the confidence of the model is also low (standard deviation is high). This hints at the complexity of the patterns.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ed8/3ed80d89-d907-4c67-b0b4-4b40976ddce4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Substation C Figure 4: Shapley analysis of key features for each substation on trained XGBoost</div>\n<div style=\"text-align: center;\">Figure 4: Shapley analysis of key features for each substation on trained XGBoost</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4391/43915254-7fc9-4021-b828-7f0c39ade18b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Illustration of different groups</div>\n<div style=\"text-align: center;\">Table 1: Description of features and label of the dataset</div>\nTable 1: Description of features and label of the dataset\nName\nDescription\nPossible values\nsubstation\nSubstation identifier\n{'A', 'B', 'C'}\ntimestamp_local\nTimestamp in local time (UTC-5)\nand ISO 8601 format [AAAA-MM-\nDD hh:mm:ss]\n\u2212\nconnected_clients\nNumber of clients connected to the\nsubstation during the considered\nhour\n{9, 10, . . . , 104}\nconnected_smart_tstats\nNumber of smart thermostats con-\nnected to the substation during the\nconsidered hour\n{59, 60, . . . , 1278}\naverage_inside_temperature\nHourly average indoor temperature\nmeasured by smart thermostats in\nsubstation [\u25e6C]\n[16.21, 27.08]\naverage_temperature_setpoint Hourly average setpoint of smart ther-\nmostats in substation [\u25e6C]\n[9.31, 21.03]\naverage_outside_temperature\nHourly average outside temperature\nat substation [\u25e6C]\n[\u221232.0, 35.2]\naverage_solar_radiance\nHourly average solar radiance at sub-\nstation [W/m2]\n[0, 961]\naverage_relative_humidity\nHourly average relative humidity at\nsubstation [%]\n[0,100]\naverage_snow_precipitation\nHourly average amount of snow pre-\ncipitation at substation [mm]\n[0.0,306.0]\naverage_wind_speed\nHourly average wind speed at sub-\nstation [m/s]\n[0, 15.68 ]\ndate\nDate [AAAA-MM-DD]\n[2022-01-01, 2024-06-30]\nmonth\nMonth\n{1, 2, . . . , 12}\nday\nDay of the month\n{1, 2, . . . , 31}\nday_of_week\nDay of the week with Sunday and\nSaturday being 1 and 7, respectively\n{1, 2, . . . , 7}\nhour\nHour of the day\n{0, 1, . . . , 23}\nchallenge_type\nType of challenge during the given\nhour\n{'None', 'CPR', 'LCPR'}\nchallenge_flag\nFlag indicating hours in challenge\n{0, 1}\npre_post_challenge_flag\nFlag\nindicating\nhours\nin\npre-\nchallenge or post-challenge\n{0, 1}\nis_weekend\nFlag indicating weekends\n{0, 1}\nis_holiday\nFlag indicating Qu\u00e9bec holidays\n{0, 1}\nweekend_holiday\nFlag indicating whether a weekend\nor a holiday\n{0, 1}\ntotal_energy_consumed\nHourly energy consumption of the\nsubstation [kWh]\n[7.45, 32240.17]\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7805/7805a9aa-ce1f-480f-aca3-329125b654ed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1 (b) \ud835\udf16= 0.05 (c Figure 6: Labelling of the SW filter for different values of \ud835\udf16</div>\n<div style=\"text-align: center;\">Figure 6: Labelling of the SW filter for different values of \ud835\udf16</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1de/b1de79ef-29e4-4ff2-b3ad-24a2524525b3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: AD comparison on multiple synthetic datasets</div>\n<div style=\"text-align: center;\">Table 2: Absolute test errors of the best validation run for each substation</div>\nTable 2: Absolute test errors of the best validation run for each substati\nSubstation\nA\nB\nC\nMAE [kWh]\n20.49103\n17.90663\n41.21746\nRMSE [kWh]\n26.08270\n22.39355\n51.25044\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb3e/bb3e7782-5f42-45e7-9252-8865db1cbdf4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b377/b3771c3f-8324-4b6f-986f-0d64bbcc291c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f73f/f73f97d2-5412-4180-bfe0-9d7bc7888aa2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Results of the grid search for each AD model on each datase</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f5b5/f5b55fd3-f56b-4e7a-8521-c4384f32cebe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Substation C Figure 9: Test predictions of the benchmark at each substation</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of unsupervised anomaly detection in the context of localized critical peak rebates (LCPR) in Qu\u00e9bec, which is crucial for managing energy consumption during peak demand periods. Previous methods for anomaly detection, such as local outlier factor and isolation forest, have limitations that necessitate a new approach, particularly given the unique challenges posed by the northern climate and the specific characteristics of LCPR programs.",
        "problem": {
            "definition": "The problem involves detecting anomalies in energy consumption data from residential customers participating in LCPR programs, where accurate predictions are essential for effective demand response strategies.",
            "key obstacle": "The main difficulty lies in the noisy and complex nature of smart grid data, which includes various sources of error such as metering inaccuracies and environmental factors, making it challenging for existing anomaly detection methods to perform effectively."
        },
        "idea": {
            "intuition": "The idea was inspired by the need for robust anomaly detection methods that can handle the intricacies of energy consumption data, particularly in a context where traditional methods fall short.",
            "opinion": "The proposed method leverages the sliced-Wasserstein distance to create a new unsupervised outlier filter that can effectively identify anomalies in high-dimensional data without requiring labeled training samples.",
            "innovation": "This method differs from existing approaches by utilizing optimal transport principles, specifically the sliced-Wasserstein metric, which offers a computationally efficient and theoretically grounded way to measure discrepancies between distributions."
        },
        "method": {
            "method name": "Sliced-Wasserstein Filter",
            "method abbreviation": "SWAD",
            "method definition": "The Sliced-Wasserstein Filter is an unsupervised anomaly detection method that uses the sliced-Wasserstein distance to identify outliers in energy consumption data.",
            "method description": "It filters out anomalies by comparing the empirical distribution of the data with that of random samples, using a voting mechanism based on the sliced-Wasserstein distance.",
            "method steps": [
                "Calculate the empirical distribution of the dataset.",
                "Remove a candidate sample and compute the sliced-Wasserstein distance to a random sample.",
                "Use a voting system to determine if the candidate sample is an outlier based on the distance comparison.",
                "Label the sample as an outlier if it receives a sufficient proportion of positive votes."
            ],
            "principle": "The method is effective because it focuses on the transportation costs of samples within the distribution, allowing it to identify points that are costly to transport, indicating they are likely outliers."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using synthetic datasets and standard anomaly detection datasets, including Lymphography, Ionosphere, Glass, Shuttle, WPBC, Arrhythmia, and Pima, to benchmark the proposed method against existing techniques.",
            "evaluation method": "Performance was assessed using precision scores and mean average errors (MAE) and root mean squared errors (RMSE) for different models, comparing the SW filter with isolation forest and local outlier factor."
        },
        "conclusion": "The proposed Sliced-Wasserstein Filter demonstrates promising results in anomaly detection, particularly in the context of localized critical peak rebates, indicating its potential for enhancing the robustness of machine learning models in energy management applications.",
        "discussion": {
            "advantage": "The key advantages include its unsupervised nature, computational efficiency, and ability to handle high-dimensional data, making it suitable for real-world applications in smart grid systems.",
            "limitation": "A limitation of the method is its scalability; as dataset size increases, the computational burden also rises, which may necessitate additional strategies for filtering large datasets.",
            "future work": "Future research could focus on improving the scalability of the method and exploring its application in other domains with similar challenges in anomaly detection."
        },
        "other info": {
            "dataset availability": "The dataset used in the experiments will be made available on December 14th, 2024, and contains aggregated hourly consumption data for 197 anonymous LCPR testers.",
            "github_link": "https://github.com/jupall/swfilter"
        }
    },
    "mount_outline": [
        {
            "section number": "2.3",
            "key information": "The paper addresses unsupervised anomaly detection in the context of localized critical peak rebates (LCPR) in Qu\u00e9bec, which is crucial for managing energy consumption during peak demand periods."
        },
        {
            "section number": "2.1",
            "key information": "The problem involves detecting anomalies in energy consumption data from residential customers participating in LCPR programs, where accurate predictions are essential for effective demand response strategies."
        },
        {
            "section number": "3.2",
            "key information": "The proposed method, the Sliced-Wasserstein Filter (SWAD), utilizes the sliced-Wasserstein distance to identify outliers in energy consumption data."
        },
        {
            "section number": "5.1",
            "key information": "The key advantages of the Sliced-Wasserstein Filter include its unsupervised nature, computational efficiency, and ability to handle high-dimensional data, making it suitable for real-world applications in smart grid systems."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the Sliced-Wasserstein Filter is its scalability; as dataset size increases, the computational burden also rises, which may necessitate additional strategies for filtering large datasets."
        },
        {
            "section number": "7.2",
            "key information": "Future research could focus on improving the scalability of the Sliced-Wasserstein Filter and exploring its application in other domains with similar challenges in anomaly detection."
        }
    ],
    "similarity_score": 0.678042373229143,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Sliced-Wasserstein-based Anomaly Detection and Open Dataset for Localized Critical Peak Rebates.json"
}