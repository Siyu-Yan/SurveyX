{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.12375",
    "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
    "abstract": " ABSTRACT\nABSTRACT\nThe predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input\u2013label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.\n13 Mar 2\n# 1 INTRODUCTION\nBrown et al. (2020) have shown that Large Language Models (LLMs) (Radford et al., 2019; Chowdhery et al., 2022; Hoffmann et al., 2022; Zhang et al., 2022a) can perform so-called in-context learning (ICL) of supervised tasks. In contrast to standard in-weights learning, e.g. gradient-based finetuning of model parameters, ICL requires no parameter updates. Instead, examples of the input\u2013 label relationship of the downstream task are simply prepended to the query for which the LLM predicts. This is sometimes also referred to as few-shot ICL to differentiate from other ICL variants that do not use example demonstrations (Liu et al., 2023). Few-shot ICL is widely used, e.g. in all LLM publications cited above, to improve predictions across a variety of established NLP tasks, such as sentiment or document classification, question answering, or natural ",
    "bib_name": "kossen2024incontextlearninglearnslabel",
    "md_text": "# IN-CONTEXT LEARNING LEARNS LABEL RELATIONSHIPS BUT IS NOT CONVENTIONAL LEARNING\nJannik Kossen1\u2207\nYarin Gal1\u2206\n1 OATML, Department of Computer Science, University of Oxford 2 Department of Statistics, University of Oxford\n# ABSTRACT\nABSTRACT\nThe predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input\u2013label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.\n13 Mar 2\n# 1 INTRODUCTION\nBrown et al. (2020) have shown that Large Language Models (LLMs) (Radford et al., 2019; Chowdhery et al., 2022; Hoffmann et al., 2022; Zhang et al., 2022a) can perform so-called in-context learning (ICL) of supervised tasks. In contrast to standard in-weights learning, e.g. gradient-based finetuning of model parameters, ICL requires no parameter updates. Instead, examples of the input\u2013 label relationship of the downstream task are simply prepended to the query for which the LLM predicts. This is sometimes also referred to as few-shot ICL to differentiate from other ICL variants that do not use example demonstrations (Liu et al., 2023). Few-shot ICL is widely used, e.g. in all LLM publications cited above, to improve predictions across a variety of established NLP tasks, such as sentiment or document classification, question answering, or natural language inference. However, there is currently no consensus on why ICL improves predictions, with prior work presenting a large variety of often contradictory perspectives. For example, Brown et al. (2020) highlight similarities between the behavior of ICL and finetuning of LLMs, such as improvements with model size and number of examples. Since then, some have argued that ICL works because it implements general-purpose learning algorithms such as Bayesian inference or gradient descent (Xie et al., 2022; Husz\u00b4ar, 2023; Hahn & Goyal, 2023; Jiang, 2023; Zhang et al., 2023; Von Oswald et al., 2023; Aky\u00a8urek et al., 2023; Han et al., 2023). In contrast, others have highlighted practical shortcomings of ICL, suggesting ICL does not really \u2018learn\u2019 from examples in the way one expects (Liu et al., 2022; Lu et al., 2022; Zhao et al., 2021; Chen et al., 2022; Agrawal et al., 2023; Chang & Jia, 2022; Razeghi et al., 2022; Li & Qiu, 2023; Wei et al., 2023). In particular, Min et al. (2022b) claim their \u2018findings suggest that [LLMs] do not learn new tasks at test time\u2019 in the sense of \u2018capturing the input-label correspondence\u2019. Clearly, these claims, if true, are not compatible with the behavior we would expect from a general-purpose learning algorithm. In this paper, we address the pressing need for an improved understanding of how information given in-context affects ICL predictions. Concretely, we formulate a set of questions that encode our beliefs about how an idealized conventional learning algorithm should incorporate label information \u2207Correspondence to jannik.kossen@cs.ox.ac.uk. \u2206Equal advising.\n\u2207Correspondence to jannik.kossen@cs.ox.ac.uk. \u2206Equal advising.\nTom Rainforth2\u2206\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/555c/555c624d-0696-4d63-a75c-b17ee3549662.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">No Observed In-Context Examples</div>\nFigure 1: ICL predictions generally depend on the conditional label distribution of in-context examples: when in-context labels are randomized, average log likelihoods of label predictions decrease compared to ICL with default labels for LLaMa-2-70B across a variety of tasks. Results averaged over 500 in-context datasets and thin lines are 99 % confidence intervals. See \u00a75 for details.\nand then study ICL behavior in relation to this concept of a conventional learner. (1) Does ICL take the input\u2013label relationship of the in-context examples into account when predicting for test queries? (2) Is ICL powerful enough to overcome prediction preferences originating from pre-training? (3) Does ICL treat all information provided in-context equally? To study these questions rigorously, we rephrase them as null hypotheses that we study empirically. Our results yield an improved understanding of the similarities and differences between ICL and idealized conventional learners. Unlike prior work, we study in detail how ICL predictions evolve as an increasing number of examples are provided, from no examples at all up to the maximum possible, across a range of LLMs and tasks. We further show that using probabilistic metrics better highlights the resulting ICL dynamics, often revealing large changes in the confidence of ICL predictions, even when accuracy metrics barely change at all. These measures ensure we obtain a comprehensive picture of ICL behavior. In our experiments, we first examine if ICL predictions depend on the labels of in-context examples by studying how probabilistic metrics react to randomized in-context labels (\u00a75, Fig. 1). Further, we study ICL on a truly novel task the LLM cannot know from pre-training (\u00a76). Both experiments show that ICL typically considers in-context label relations. We then investigate if ICL is powerful enough to overcome prediction preferences learned from pre-training data (\u00a77). In our experiments, we find this is typically not the case as ICL performance plateaus if label relations oppose pretraining preference. Further, while additional prompting can improve ICL here, we ultimately do not find prompts that lead to the desired behavior. Finally, we study if ICL treats all information provided in-context equally (\u00a78). This is important when the context contains multiple different label relationships. By modifying label relations during ICL, we find it does not treat all in-context information equally, and, instead, ICL preferentially makes use of information closer to the query. In summary, our results suggest a new middle ground regarding the capabilities and limitations of ICL. While ICL can learn from label information, it does so differently than an idealized learner. Our findings thus contribute to a better understanding of information processing in ICL, which, in turn, is crucial to our ability to deploy LLMs safely and effectively. For example, Bai et al. (2022b) suggest to use ICL for alignment, which relies on ICL being able to sufficiently adjust LLM behavior.\n# 2 BACKGROUND\nA large and growing body of prior work studies ICL. We here highlight those studies that are most relevant to our motivation, and discuss other related work later in \u00a710. Since its introduction by Brown et al. (2020), few-shot ICL has become an integral part of LLM evaluations: e.g. many recent publications rely on few-shot ICL tasks, such as the popular HELM benchmark (Liang et al., 2022), to evaluate their LLMs (Chowdhery et al., 2022; Hoffmann et al., 2022). In the wake of ICL\u2019s success, follow up work has speculated if ICL implements a general purpose learning algorithm such as gradient descent (Von Oswald et al., 2023) or Bayesian inference (Xie et al., 2022). This line of work implies that ICL captures not just how much LLMs have learned during pre-training but, rather, how much LLMs have learned how to learn novel supervised tasks in-context. However, so far arguments have been largely theoretical, lacking solid experimental evidence in actual LLMs (Zhang et al., 2023; Husz\u00b4ar, 2023; Wies et al., 2023; Jiang, 2023).\nConversely, a variety of studies have highlighted unexpected shortcomings of ICL. For example, ICL can be sensitive to the formatting (Min et al., 2022b) or order (Lu et al., 2022) of the in-context learning examples. Further, LLMs prefer to predict labels that are common in the pre-training data (Zhao et al., 2021), they can predict drastically different for similar prompts (Chen et al., 2022), and they rely on task formulations similar to those observed in the pre-training data (Wu et al., 2023). In particular, Min et al. (2022b) claim that ICL does not learn label relationships from in-context examples and that ICL \u2018performance drops only marginally when labels in the demonstrations are replaced by random labels\u2019. Further, they suggest that, instead of learning input\u2013label relationships, ICL only works because the model learns about the general label space, the formatting of the examples, and their input distribution. They assert that ICL does \u2018not learn new tasks at test time\u2019 and that \u2018ground truth demonstrations are in fact not required\u2019 in many common scenarios. In the first part of our evaluation, we will revisit and ultimately disagree with these claims.\n# 3 NULL HYPOTHESES ON HOW ICL INCORPORATES LABEL INFORMATION\nWe wish to obtain a better understanding of how ICL uses information about the input\u2013label relationship provided in-context. We therefore study to what extent ICL behavior matches our expectations of how a machine learning algorithm should behave in an idealized world. To this end, we introduce the concept of a \u2018conventional learning algorithm\u2019 as an algorithm that conforms to our intuitions of how an idealized learner will make predictions given data. In particular, we focus on the following intuitions of an idealized learner: (1) it makes use of the labels for learning, (2) it allows the true label relationship to be learned when provided with sufficient data, and (3) it considers all information in the data equally. Note here that some existing approaches may not always conform to some or all of these intuitions, e.g. due to imperfections in training schemes, but our concept of the conventional learning algorithm reflects how we would expect them to behave if our training worked perfectly. To allow these institutions to be tested for ICL, we now introduce a series of null hypotheses that we will subsequently try to falsify. Our first hypothesis follows from the notion that conventional learners make use of the conditional distribution of labels given inputs. Null Hypothesis 1 (NH1): ICL predictions are independent of the conditional label distribution of the examples given in-context. We will investigate NH1 in multiple ways. In \u00a75, we revisit and refine the randomized in-context label experiment of Min et al. (2022b): in addition to revising their results for point predictions, we propose the use of probabilistic metrics to study if label randomization really does not affect ICL predictive beliefs. Then, in \u00a76, we study ICL on a novel task that we create and which ICL can only solve by learning a novel label relationship that it cannot know from pre-training. Next, we study how ICL incorporates label information. The pre-trained model already contains information towards many NLP tasks: even without any ICL, predictions are significantly better than random guessing. This raises the question of how information given in-context interacts with this pre-training preference. In typical applications of conventional learners, we would expect that predictions eventually follow the label relation of the training examples when provided with sufficient data. A learner that does not conform to this is inherently limited in what it can learn. With NH2, we study if ICL conforms to these intuitions and can overcome the initial pre-training preference. Null Hypothesis 2 (NH2): ICL can overcome the zero-shot prediction preferences of the pre-trained model. In \u00a77, we modify in-context label relationships and study how this changes ICL predictions. If NH2 is true, ICL should eventually predict according to any label relation given in-context. Our last hypothesis relies on the following typical property of conventional learning algorithms: they will consider all information in the examples equally. If a dataset contains multiple sources of information about a label relationship, e.g. the dataset itself is a union of multiple datasets, then all information is considered equally by the learner. NH3 investigates if this is holds true for ICL as well. Null Hypothesis 3 (NH3): ICL considers all information given in-context equally. In \u00a78, we study non-stationary input distributions for which label relations change during ICL. If NH3 is true, ICL predictions should not depend on the order in which we present label relations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/95c0/95c04bf9-6f10-42c3-bffb-dce59b955f3c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Few-shot ICL training dynamics in a default label scenario on SST-2. Accuracy (\u2191) and log likelihood (\u2191) improve with in-context dataset size, and entropies decrease appropriately. Averages over 500 random subsets, thick lines with moving average (window size 5) for clarity.</div>\nWe here detail our experimental setup for the subsequent evaluation of few-shot ICL behavior. Models & Tasks. We employ LLMs from the LLaMa-2 (Touvron et al., 2023b), LLaMa (Touvron et al., 2023a), and Falcon (TII, 2023) families due to their strong performance and open source nature. We evaluate on SST-2, Subjective (Subj.), Financial Phrasebank (FP), Hate Speech (HS), AG News (AGN), MQP, MRPC, RTE, and WNLI. We provide citations for all tasks in \u00a7D. Context Size. We always report few-shot ICL performance across all possible numbers of in-context demonstrations, i.e. from zero-shot performance up to the maximum number of examples within the LLMs\u2019 input token limit. This is in contrast to prior work, which often evaluates few-shot ICL at only a few context set sizes, and allows us to obtain a comprehensive picture of ICL \u2018training dynamics\u2019. Computationally Cheap ICL Evaluations. We propose a novel evaluation strategy that obtains ICL predictions at all possible numbers of in-context demonstrations without incurring any additional cost. Concretely, we exploit the fact that each forward pass through the model gives not just a prediction for the next token, but rather, the predicted probabilities for each input token (given all preceding tokens). By extracting those token predictions that correspond to labels of in-context examples, we obtain few-shot ICL predictions at all in-context dataset sizes with each forward pass. We refer to \u00a7B for a formalization of few-shot ICL and further description of our evaluation strategy. Evaluation Metrics. We evaluate few-shot ICL performance in terms of accuracy (\u2191) and (average) log likelihood (\u2191) of label predictions. We also report entropy, which, while not a performance metric, is useful for understanding how much predicted probabilities are spread over classes. We average metrics over sets of in-context examples drawn randomly and without replacement from the training set, and we compare to a guessing baseline that predicts with probabilities equal to class frequencies. Default Training Dynamics. Before modifying label relationships in the following sections, Fig. 2 shows standard few-shot ICL training dynamics for Falcon models on SST-2. We observe reasonable behavior for all models: as more in-context examples are observed, accuracies and log likelihoods increase, while entropies decrease. Notably, log likelihoods and entropies show in-context learning more clearly: predicted probabilities continue to improve at larger context sizes, whereas accuracies saturate quickly. Differences between models are more noticeable for probabilistic metrics, too: entropies reveal that larger or instruction-tuned Falcon models predict with higher certainty on SST-2. Similar findings also hold for LLaMa and LLaMA-2 models, for which we provide results in Fig. F.1.\n# 5 DO ICL PREDICTIONS DEPEND ON IN-CONTEXT LABELS?\nWe now study the null hypotheses (NH) formulated in \u00a73, starting with NH1, which states that ICL predictions are independent of the conditional label distribution of the in-context examples. To this end, we first revisit the experiments of Min et al. (2022b), replacing all labels of in-context examples with labels drawn randomly from the training set of the task. If NH1 is true, then accuracy, log-likelihood, and entropy should be identical for the randomized and standard label scenario. We note that, while we believe the results of this experiment are already sufficient to reject NH1, the experiments in \u00a76\u2013\u00a78 will provide additional strong evidence for this conclusion. Observations & Discussion. We evaluate NH1 across all our models, tasks, and metrics, computing full ICL training curves as introduced in \u00a74. Figure 1 shows log likelihoods for LLaMa-2-70B for\nObservations & Discussion. We evaluate NH1 across all our models, tasks, and metrics, computing ull ICL training curves as introduced in \u00a74. Figure 1 shows log likelihoods for LLaMa-2-70B for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce54/ce54ef13-5176-4ed3-8b58-62bb5980d54f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">No Observed In-Context Examples</div>\n\u2206Log Likelihood\nSST-2\nSubj\nFP\nHS\nAGN\nMQP\nMRPC\nRTE\nWNLI\nLLaMa-2 7B\n0.42\n0.39\n0.57\n0.18\n0.53\n0.03\n0.02\n0.03\n0.02\nLLaMa-2 13B\n0.41\n0.62\n0.49\n0.24\n0.81\n0.04\n0.01\n0.06\n0.02\nLLaMa-2 70B\n0.51\n0.53\n0.57\n0.34\n0.80\n0.29\n0.04\n0.22\n0.18\nFalcon 7B\n0.20\n0.19\n0.25\n0.06\n0.31\n0.01\n0.01\n\u22120.01\n0.01\nFalcon 7B Instr.\n0.13\n0.08\n0.11\n0.03\n0.15\n0.03\n0.02\n\u22120.00\n0.00\nFalcon 40B\n0.34\n0.35\n0.31\n0.18\n0.90\n0.06\n0.01\n0.01\n0.02\nFalcon 40B Instr.\n0.25\n0.37\n0.27\n0.02\n0.77\n0.06\n0.02\n0.02\n0.04\na selection of tasks, and Fig. 3 shows all metrics for all Falcon models on SST-2. We observe significant differences in ICL behavior for the default and randomized label scenario. As the context size grows, likelihoods eventually degrade significantly for randomized labels. In Fig. 3, we can further see entropies increase when randomizing labels. This is reasonable from a probabilistic learning perspective: as noisy labels are observed, estimates of uncertainty will typically increase. While differences are large for probabilistic log likelihood and entropy, they can be harder to spot for accuracy. In Fig. 2, only Falcon-40B experiences a sizeable accuracy decrease. (For LLaMa(-2) models, accuracy decreases more frequently and can approach guessing level, cf. Figs. F.3 to F.8.) We provide results for label randomization across all our models, tasks, and metrics: we invite the reader to view the full set of training curves in \u00a7F and provide a summary of the results in Table 1. It shows the average difference in log likelihoods between the default and randomized labels at the maximum number of demonstrations for each task and model. We gray out entries where ICL on default labels does not outperform the guessing baseline as we are only interested in studying label randomization when ICL works in the first place. When default label performance is better than random (black entries), differences are almost always significantly positive (bold entries), indicating ICL performs worse for randomized labels. Based on these results, we reject NH1 that ICL predictions do not depend on the conditional label distribution of in-context examples. Notably, Table 1 shows that LLaMa-2-70B, our largest and most capable model, always performs worse under label randomization. This suggests the importance of labels in ICL will increase as models become more powerful in the future. However, performance often degrades significantly even for smaller models, although they struggle to reach better than random performance on the entailment tasks MQP, MRPC, RTE, and WNLI. Occasionally, likelihoods improve despite random labels, e.g. for the small Falcon models in Fig. 3. Following Pan et al. (2023); Min et al. (2022b), we attribute this to ICL \u2018recognizing\u2019, rather than learning, the task from the random label demonstrations. However, we note that, even here, there are significant performance gaps to the default scenario. We conclude that label randomization adversely affecting ICL predictions is the rule not the exception. Discussion of Min et al. (2022b). Lastly, we discuss possible reasons for why Min et al. (2022b) arrive at the conclusion that label randomization only \u2018barely hurts\u2019 ICL performance: (1) They do\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afe7/afe7a123-87fe-472e-a4d4-97ee268fdf0c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Few-shot ICL achieves accuracies significantly better than random guessing on our novel author identification task. Thus, LLMs can learn novel label relationships entirely in-context. Averages over 500 runs, thick lines with additional moving average (window size 5) for clarity.</div>\nnot study probabilistic metrics, which are more sensitive to randomization. (2) They use a fixed ICL dataset size of 16, but effects of random labels increase with growing contexts. (3) Only one model they study has more than 20B parameters (GPT-3), but we observe that larger models react more to randomization. (Pan et al. (2023) also observe this, cf. \u00a710.) (4) On some tasks, performance for Min et al. (2022b) could be close to random guessing, where label randomization has less of an effect.\n# 6 CAN ICL LEARN TRULY NOVEL LABEL RELATIONSHIPS?\nThe results of \u00a75 show that ICL predictions do depend on the label relationship of in-context examples. Here, we explore the extent to which ICL can extract label information from the context. Concretely, we study if LLMs can learn truly novel label relationships in-context. To do this, we create a task that is guaranteed to not appear in the pre-training data. The task needs to be distinct from established NLP tasks, for which the pre-training data could be contaminated and for which, often, strong zero-shot performance shows the model has learned the task, perhaps implicitly, during pre-training. Specifically, we create an authorship identification (Stamatatos, 2009) dataset from private messages between two authors of this paper. The task is to identify the author corresponding to a given message. As messages stem from private communication, they are guaranteed to not be part of the pre-training corpus. For ICL to succeed here, it needs to learn the novel input\u2013label relationship provided in-context: while the LLM could have some general notion of authorship identification tasks, the specific input\u2013label relationship is definitely novel, as the authors\u2019 private writing styles cannot be known to the LLM. We give further details on the task in \u00a7C. Observations & Discussion. Figure 4 shows that ICL with LLaMa-2 succeeds at learning the author identification task. Accuracies and log likelihoods increase, agreeing with expectations about conventional learning. Performance improves with model size, but all models perform better than random. We show results for more LLMs in Fig. F.13: all models, except Falcon-7B-Instruct, achieve better than random performance. We conclude that LLMs can learn truly novel tasks incontext, correctly inferring the label relation from examples. These results also strongly support our previous rejection of NH1 as, clearly, ICL predictions must depend on labels to learn the novel task.\n# 7 CAN ICL OVERCOME PRE-TRAINING PREFERENCE\nWith NH2, we explore how in-context label information trades off against the LLM\u2019s pre-training preference, i.e. its zero-shot predictions based on label relationships inferred from pre-training and stored in the model parameters. Often, pre-training preference and in-context label relationships agree: e.g. in Fig. 3, performance is high zero-shot and then improves with ICL. To test NH2 if ICL can overcome pre-training preference, we create scenarios where pre-training preference and in-context observations are not aligned. We then study if ICL behavior is compatible with fully overcoming pre-training preference as we would expect from a conventional learner. Concretely, we use replacement label relationships when constructing the in-context examples. (1) We flip the default labels, e.g. (negative, positive) get mapped to (positive, negative) for SST-2. (2) We study arbitrary labels, e.g. (negative, positive) become (A, B) or (B, A)\u2014we deliberately choose arbitrary labels here such that the LLM should not have a significant preference for assigning them to positive or negative. We then evaluate ICL performance for predicting the replacement\nWith NH2, we explore how in-context label information trades off against the LLM\u2019s pre-training preference, i.e. its zero-shot predictions based on label relationships inferred from pre-training and stored in the model parameters. Often, pre-training preference and in-context label relationships agree: e.g. in Fig. 3, performance is high zero-shot and then improves with ICL. To test NH2 if ICL can overcome pre-training preference, we create scenarios where pre-training preference and in-context observations are not aligned. We then study if ICL behavior is compatible with fully overcoming pre-training preference as we would expect from a conventional learner.\nConcretely, we use replacement label relationships when constructing the in-context examples. (1) We flip the default labels, e.g. (negative, positive) get mapped to (positive, negative) for SST-2. (2) We study arbitrary labels, e.g. (negative, positive) become (A, B) or (B, A)\u2014we deliberately choose arbitrary labels here such that the LLM should not have a significant preference for assigning them to positive or negative. We then evaluate ICL performance for predicting the replacement\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7196/71969be8-e5a0-43e2-bbaf-b3c1d54b198f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">No Observed In-Context Examples</div>\nFigure 5: Few-shot ICL with replacement labels for Falcon-40B on SST-2, LLaMa-65B on Hate Speech, and LLaMa-2-70B on MQP. Table 2 and \u00a7F contain results for all other models and tasks. ICL achieves better than guessing performance for all label relations and models. However, predictions for flipped labels (dashed blue) plateau at a higher entropies and lower likelihoods than those for the default label relation (solid blue). For arbitrary labels (pink), the model performs similarly for both label directions. Averages over 100 runs and thick lines with moving average (window size 5). Table 2: Average differences between ICL entropies for default and flipped labels. Bold entries indicate differences are statistically significant. Again, we disregard entries for which default ICL performance is not significantly better than the guessing baseline (lightgray entries). When default ICL outperforms the baseline, ICL entropies are almost always significantly different between scenarios. We average 100 runs, report results at maximum context size, and show standard errors in Table F.2.\n\u2206Entropy\nSST-2\nSubj\nFP\nHS\nAGN\nMQP\nMRPC\nRTE\nWNLI\nLLaMa-2 7B\n\u22120.52\n0.01\n\u22120.40\n\u22120.10\n\u22120.75\n\u22120.00\n0.05\n\u22120.03\n\u22120.01\nLLaMa-2 13B\n\u22120.48\n\u22120.06\n\u22120.47\n\u22120.19\n\u22120.95\n\u22120.03\n\u22120.07\n\u22120.11\n\u22120.07\nLLaMa-2 70B\n\u22120.17\n\u22120.10\n\u22120.40\n\u22120.26\n\u22121.00\n\u22120.24\n\u22120.15\n\u22120.26\n\u22120.21\nFalcon 7B\n\u22120.28\n\u22120.12\n\u22120.02\n\u22120.06\n\u22120.52\n0.00\n0.00\n\u22120.00\n\u22120.01\nFalcon 7B Instr.\n\u22120.37\n\u22120.07\n\u22120.33\n\u22120.05\n\u22120.22\n\u22120.02\n0.13\n0.01\n0.00\nFalcon 40B\n\u22120.39\n\u22120.23\n\u22120.42\n\u22120.19\n\u22120.90\n\u22120.00\n\u22120.10\n\u22120.02\n\u22120.00\nFalcon 40B Instr.\n\u22120.48\n\u22120.16\n\u22120.43\n\u22120.31\n\u22120.92\n\u22120.10\n\u22120.02\n\u22120.06\n\u22120.01\nlabel relationship, e.g. the flipped labels in (1). Note that, we rely on the flipped label experiment to evaluate NH2; results on arbitrary labels serve to complete the picture, and we discuss them later\nObservations & Discussion. We evaluate NH2 across all our models, tasks, and metrics. Figure 5 shows results for a selection of large models and datasets. Evidently, the LLMs can, to some extent, learn to predict the flipped label relationships against pre-training preference. Accuracies on flipped labels reach levels significantly better than random guessing. However, in particular for entropies, there is a consistent gap between the default and flipped label scenarios: ICL predictions on flipped labels are much less certain. Importantly, given the plateauing behavior, this gap is unlikely to disappear with additional in-context observations. (Practically speaking, we cannot actually add any additional examples as input size is maximal already and will deteriorate when exceeding the LLMs\u2019 input token limit; this is itself a limitation of ICL compared to conventional learning.) It seems that label relationships inferred from pre-training have a permanent effect that cannot be overcome through in-context observations. This does not agree with conventional learning: predictions on flipped labels should continue to improve as observations continue to contradict pre-training preference. Crucially, we observe this behavior across models and tasks. We encourage the reader to view the full set of training curves in \u00a7F. Table 2 provides a summary of the results, showing differences in entropy between default and flipped label scenarios at maximum context size, highlighting statistical significance in bold and graying out entries where ICL fails on default labels. Across the board, we again observe that predictions on flipped labels plateau: a significant gap between predictions on default and flipped labels remains, even at maximum input size. For the models we study, we reject\nNH2 that ICL can overcome prediction preferences from pre-training. Again, the results here strongly support our previous rejection of NH1, as clearly, predictions change for replacement labels. Figure 5 also shows that for replacement labels (A, B) and (B, A) both directions are similarly easy for ICL to learn. This suggests the LLM has not learned a preference for them from pre-training, agreeing with our intuition. Further, learning arbitrary labels is slower than learning default labels but faster than learning flipped labels, which agrees with intuitions about inductive biases. Can Prompting Help? In \u00a7A, we further study if specific prompts, i.e. instructions that inform the LLMs of the flipped labels, can improve ICL predictions. We find that, while prompts initially can help the model predict on flipped labels, eventually, prompts no longer improve predictions.\nWith NH3, we study if ICL considers all in-context information equally. We have just seen that ICL does not treat pre-training preference equivalently to in-context label information. However, it is similarly important to understand how ICL treats different sources of purely in-context information. To test NH3, we change the label relationship during in-context learning in three different scenarios. (D \u2192F): after N observations of the default label relation we flip the label relation for all following observations, e.g. from (negative, positive) to (positive, negative) for SST-2. (F \u2192D): we now start with N flipped label observations and then expose the model to default labels. (Alternate F \u2194D): we alternate between the default and flipped labels after each observation. For all three setups, after 2N observations, the model has observed the same number of the flipped and default label examples. If NH3 is true, ICL should treat all observed label relations equally, no matter their position in the input. This means, predictions should be the same for all three scenarios after 2N observations. Observations & Discussion. Figure 6 shows accuracies for a selection of models and tasks, and we report full probabilistic metrics for all tasks and model combinations for which ICL was able to learn the flipped relationships well in Figs. F.32 to F.36. We observe that, across almost all tasks and model combinations, predictions are significantly different between the three setups after observing the same number of examples of both label relationships (after 2N total observations, red dashed line in the figure). We thus reject NH3 that ICL treats all information provided in-context equivalently. After the changepoint N, predictions immediately begin to adjust to the new label relationship. In particular, after 2N observations, the (F \u2192D) setup has a bias for predicting according to the default label relationship, while the (D \u2192F) setup has a bias for predicting the flipped label relationship.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf70/cf70e991-7cc2-482f-bca4-3553b935a622.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">No Observed In-Context Examples</div>\nFigure 6: Few-shot ICL accuracies when the label relationship changes throughout ICL. For (D \u2192F), we start with default labels and change to flipped labels at the changepoint, for (F \u2192D) we change from flipped to the default labels at the changepoint, and for (Alternating F \u2194D) we alternate between the two label relationships after every observation. For all setups, at \u20182 x Changepoint\u2019, the LLMs have observed the same number of examples for both label relations. If, according to NH3, ICL treats all in-context information equally, predictions should be equal at that point\u2014but they are not. Bootstrapped 99 % confidence intervals, moving averages (size 3), and 500 repetitions.\nIn other words, LLMs prefer to use information that is closer to the query, instead of considering all available information equally. Our finding is distinct from Zhao et al. (2021), who observe that ICL preferentially predicts labels that appear frequently near the query for a single fixed label relation. Lastly, we note once more that the results here also strongly support our previous rejection of NH1.\n# 9 DISCUSSION & LIMITATIONS\nAlignment. For alignment of LLMs, it is crucial to understand how pre-training preference and inputs trade-off, as well as how different parts of the input, such as a context string and user input, interact and influence predictions. Our results suggest prompt-based alignment (Bai et al., 2022b) may struggle to overwrite pre-training preference and could itself easily be overcome by future user input. Do Labels Always Matter? It is plausible that labels matter less for other NLP tasks such as question answering, where in-context examples may provide limited information towards the answer of the query question. However, for the randomized label experiment, capable LLMs might still identify that the provided in-context answers are random and imitate this in their predictions. Limitations. We focus on few-shot ICL tasks where evaluation is based on logits and not free-form generation. We do this mostly to avoid complications around evaluating free-form generation tasks and believe our results should transfer to this setting. Further, our experiments do not cover RLHF-finetuned LLMs (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022).\n# 10 RELATED WORK\nSome recent work has studied the effect of labels in ICL. Yoo et al. (2022) also revisit label randomization and find significant variance across tasks and models. Pan et al. (2023) further separate ICL into label-independent and -dependent learning, which they study by replacing labels with arbitrary tokens. Wei et al. (2023) find that smaller or instruction-tuned models are less capable when performing ICL with replacement labels. Similar to Min et al. (2022b), the above studies do not consider probabilistic metrics or full ICL training curves, and thus can underestimate changes in ICL predictions for modified labels. For example, Pan et al. (2023) find that the gap between random and default labels is \u2018insignificant\u2019 for small models, which our results, in particular for probabilistic metrics, contradict, cf. \u00a75. Further, Wei et al. (2023) claim that \u2018large models can override prior knowledge from pretraining [...] in-context\u2019 and \u2018small models do not change their predictions when seeing flipped labels\u2019, which is not supported by our results in \u00a77. Lastly, Gao et al. (2021) observe that replacement labels can also degrade performance when finetuning language models. More generally, ICL has been the subject of many recent studies. For example, Min et al. (2022a) fine-tune language models to improve ICL, Si et al. (2023) measure the inductive bias of ICL predictions, Chan et al. (2022b); Dasgupta et al. (2022) study differences between in-weights and incontext generalization, and Chang & Jia (2022); Liu et al. (2022); Zhang et al. (2022b) observe that the selection of examples affect ICL predictions significantly. In this paper, we emphasize a probabilistic treatment of ICL predictions. Uncertainty in LLMs has previously been studied, e.g. by Kadavath et al. (2022); Lin et al. (2023); Bai et al. (2022a); Gonen et al. (2022). On non-language tasks, Kirsch et al. (2022); Chan et al. (2022a) study properties that lead to the emergence of ICL. Also related are Garnelo et al. (2018); Kossen et al. (2021); Gordon et al. (2019), who propose deep non-parametric models on non-language tasks. Unlike ICL, they can guarantee invariance to example-order or closely approximate Bayesian predictive distributions (M\u00a8uller et al., 2022).\n# 11 CONCLUSIONS\nIn this paper, we have investigated how the conditional label distribution of in-context examples affects ICL predictions. To ensure our conclusions represent ICL behavior well, we have studied ICL across all possible in-context dataset sizes and considered probabilistic aspects of ICL predictions. In some sense, we have shown that ICL is both better and worse than expected. On the one hand, our results demonstrate that, against expectations set by prior work, ICL does incorporate in-context label information and can even learn truly novel tasks in-context. On the other hand, we have shown that analogies between ICL and conventional learning algorithms fall short in a variety of ways. In particular, label relationships inferred from pre-training have a lasting effect that cannot be surmounted by in-context observations. Additional prompting can improve but likely not overcome this deficiency. Further, ICL does not treat all information provided in-context equally and preferentially makes use of label information that appears closer to the query.\nWe discuss the details of our experimental evaluation in \u00a7E. We provide the code to reproduce ou results at the following repository: github.com/jlko/in context learning.\n# REFERENCES\nREFERENCES Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. Incontext examples selection for machine translation. In ACL, 2023. URL https://arxiv. org/abs/2212.02437. Ekin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In ICLR, 2023. URL https: //arxiv.org/abs/2211.15661. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv:2204.05862, 2022a. URL https://arxiv.org/abs/2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv:2212.08073, 2022b. URL https://arxiv.org/ abs/2212.08073. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. URL https://arxiv.org/abs/2005.14165. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. NeurIPS, 2022a. URL https://arxiv.org/abs/2205.05055. Stephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K Lampinen, and Felix Hill. Transformers generalize differently from information stored in context vs in weights. arXiv:2210.05675, 2022b. URL https://arxiv.org/abs/2210.05675. Ting-Yun Chang and Robin Jia. Careful data curation stabilizes in-context learning. In EMNLP, 2022. URL https://arxiv.org/abs/2212.10378. Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. On the relation between sensitivity and accuracy in in-context learning. arXiv:2209.07661, 2022. URL https://arxiv. org/abs/2209.07661. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv:2204.02311, 2022. URL https://arxiv. org/abs/2204.02311. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. NeurIPS, 2017. URL https://arxiv. org/abs/1706.03741. Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine learning challenges workshop, 2005. URL https://link.springer. com/chapter/10.1007/11736790_9. Ishita Dasgupta, Erin Grant, and Tom Griffiths. Distinguishing rule and exemplar-based generalization in learning systems. In ICML, 2022. URL https://arxiv.org/abs/2110.04328. Ona de Gibert, Naiara Perez, Aitor Garc\u00b4\u0131a-Pablos, and Montse Cuadros. Hate Speech Dataset from a White Supremacy Forum. In ACL Workshop on Abusive Language Online (ALW2), 2018. URL https://www.aclweb.org/anthology/W18-5102.\nBill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005. URL https:// aclanthology.org/I05-5002/. Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 2020. URL https://arxiv.org/abs/1905.08737. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In ACL, 2021. URL https://arxiv.org/abs/2012.15723. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv:1807.01622, 2018. URL https://arxiv.org/ abs/1807.01622. Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv:2212.04037, 2022. URL https: //arxiv.org/abs/2212.04037. Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Metalearning probabilistic inference for prediction. In ICLR, 2019. URL https://arxiv.org/ abs/1805.09921. Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv:2303.07971, 2023. URL https://arxiv.org/abs/2303.07971. Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression. arXiv:2305.12766, 2023. URL https://arxiv.org/abs/2305. 12766. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In NeurIPS, 2022. URL https://arxiv.org/ abs/2203.15556. Ferenc Husz\u00b4ar. Implicit bayesian inference in large language models, 2023. URL https://www. inference.vc/implicit-bayesian-inference-in-sequence-models/. [Online; accessed 10-July-2023]. Hui Jiang. A latent space theory for emergent abilities in large language models. arXiv:2304.09960, 2023. URL https://arxiv.org/abs/2304.09960. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv:2207.05221, 2022. URL https://arxiv.org/ abs/2207.05221. Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv:2212.04458, 2022. URL https://arxiv. org/abs/2212.04458. Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Tom Rainforth, and Yarin Gal. Selfattention between datapoints: Going beyond individual input-output pairs in deep learning. In NeurIPS, 2021. URL https://arxiv.org/abs/2106.02584. Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012. URL https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf. Xiaonan Li and Xipeng Qiu. Finding supporting examples for in-context learning. arXiv:2302.13539, 2023. URL https://arxiv.org/abs/2302.13539. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv:2211.09110, 2022. URL https://arxiv.org/abs/2211.09110.\nStephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. TMLR, 2023. URL https://arxiv.org/abs/2205.14334. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In ACL, 2022. URL https://arxiv.org/abs/ 2101.06804. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 2023. URL https://arxiv.org/abs/2107.13586. Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In ICLR, 2018. URL https: //arxiv.org/abs/1801.10198. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In ACL, 2022. URL https://arxiv.org/abs/2104.08786. P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 2014. URL https://arxiv.org/abs/1307.5336. Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Effective transfer learning for identifying similar questions: Matching user questions to covid-19 faqs. arXiv:2008.13546, 2020. URL https://arxiv.org/abs/2008.13546. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. In NAACL, 2022a. URL https://arxiv.org/abs/2110.15943. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In ACL, 2022b. URL https://arxiv.org/abs/2202.12837. Samuel M\u00a8uller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In ICLR, 2022. URL https://arxiv.org/abs/2112. 10510. Kevin P Murphy. Probabilistic machine learning: an introduction. MIT press, 2022. URL https: //probml.github.io/pml-book/book1.html. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022. URL https://arxiv.org/abs/2203. 02155. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \u201dlearns\u201d in-context: Disentangling task recognition and task learning. In ACL, 2023. URL https://arxiv.org/ abs/2305.09731. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. URL https://pytorch.org/. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. Technical report, OpenAI, 2018. URL https: //openai.com/research/language-unsupervised. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskeve et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. UR https://d4mucfpksywv.cloudfront.net/better-language-models/\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. In EMNLP, 2022. URL https://arxiv.org/abs/ 2202.07206. Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. Measuring inductive biases of in-context learning with underspecified demonstrations. In ACL, 2023. URL https: //arxiv.org/abs/2305.13299. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013. URL https://aclanthology.org/D13-1170/. Efstathios Stamatatos. A survey of modern authorship attribution methods. American Society for information Science and Technology, 2009. URL https://onlinelibrary.wiley.com/ doi/10.1002/asi.21001. Technology Innovation Institute TII. Falcon llm, 2023. URL https://falconllm.tii.ae/. [Online; accessed 10-July-2023]. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023a. URL https: //arxiv.org/abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023b. URL https://arxiv.org/ abs/2307.09288. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u02dcao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In ICML, 2023. URL https://arxiv.org/abs/2212.07677. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2019. URL https://arxiv.org/abs/1804.07461. Sida I Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, 2012. URL https://dl.acm.org/doi/10.5555/2390665. 2390688. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv:2303.03846, 2023. URL https://arxiv.org/abs/2303.03846. Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. arXiv:2303.07895, 2023. URL https://arxiv.org/abs/2303.07895. Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1989. URL https://ieeexplore.ieee.org/ document/6795228. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. In EMNLP: System Demonstrations, 2020. URL https://arxiv.org/abs/1910.03771. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00a8urek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv:2307.02477, 2023. URL https:// arxiv.org/abs/2307.02477.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. In EMNLP: System Demonstrations, 2020. URL https://arxiv.org/abs/1910.03771.\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00a8urek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv:2307.02477, 2023. URL https:// arxiv.org/abs/2307.02477.\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00a8urek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv:2307.02477, 2023. URL https:// arxiv.org/abs/2307.02477.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In ICLR, 2022. URL https://arxiv.org/abs/ 2111.02080. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sanggoo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In EMNLP, 2022. URL https://arxiv.org/abs/2205.12685. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv:2205.01068, 2022a. URL https://arxiv.org/abs/2205.01068. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NeurIPS, 2015. URL https://arxiv.org/abs/1509.01626. Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. In EMNLP, 2022b. URL https://arxiv.org/abs/2211.04486. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv:2305.19420, 2023. URL https://arxiv.org/abs/2305.19420. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In ICML, 2021. URL https://arxiv.org/ abs/2102.09690. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\nA CAN PROMPTS HELP ICL LEARN FLIPPED LABEL RELATIONSHIPS?\n# CAN PROMPTS HELP ICL LEARN FLIPPED LABEL RELATIONSHIPS?\nIn this section, we explore if prompts can be used to overcome the plateauing ICL performance when default labels are flipped. Before the in-context examples, we insert prompt strings that inform the LLM of the flipped label relationship in some fashion, and should thus help the LLM adjust to it during ICL. These prompts could fundamentally change few-shot ICL behavior. In fact, one can think of the in-context learner as the union of LLM and prompt, where so far the prompt was simply left empty. There could exist prompts that help ICL learn the flipped label relationship better than without them, or as well as in the default scenario. Note that, regardless of the outcome here, NH2 remains rejected as ICL should not need to rely on a prompt to correctly consider in-context observations. Nevertheless, for the \u2018prompted few-shot ICL\u2019 setup, NH2 should be reconsidered. We explore the following three prompts: \u2018In the following ...\u2019, (Instruct Prompt) \u2018...negative means positive and positive means negative\u2019 (here for SST-2 and adapted to other tasks), (Ignore Prompt) \u2018...ignore all prior knowledge\u2019, and (Invert Prompt) \u2018...flip the meaning for all answers\u2019. Observations & Discussions. Figure A.1 gives results for the prompted few-shot ICL setup for LLaMa-65B and Falcon-40B on SST-2. Figures F.38 to F.46 give results for our largest models across all tasks. However, prompting is most successful for the scenarios in Fig. A.1, making this the most interesting result to study NH2. In particular, prompting has a surprisingly weak effect for LLaMa-2-70B. In Fig. A.1 we observe that prompts, in particular the instruct and invert prompts, can help improve ICL performance. However, it seems that the positive impact from prompting is restricted to an initial boost at small in-context datasets sizes. We then sometimes observe a \u2018dip\u2019 in performance, which could indicate ICL forgetting about the prompt. At large context sizes, none of our prompts have any advantage, and flipped label performance again plateaus short of performance for the default label setup. Therefore, we reject NH2 for the prompted ICL variations that we study. It is possible that there exist prompts\u2014that we have not found\u2014for which we cannot reject NH2. However, we are sceptical these prompts exist for the models we study, as their behavior at large context sizes is strikingly similar across all prompts we investigate. For more capable LLMs, we suspect it may be possible to obtain a zero-shot performance on the flipped scenario that is equal to the zero-shot of the default scenario, i.e. the prompt leads to the LLM perfectly flipping all its zero-shot predictions. However, we are unsure if, in addition to flipping zero-shot predictions, such prompts would also improve ICL on flipped label observations to be as good as in the default scenario.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2267/226725fe-86e9-4ad9-a0b0-247e865a4f45.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">No Observed In-Context Examples</div>\nFigure A.1: Prompted few-shot ICL with flipped labels on SST-2. Some prompts are able to improve ICL on flipped labels compared to not using a prompt as before (label none in the figure). However, improvements have no lasting effect: performance at larger context sizes does not improve and still plateaus short of the default scenario (solid grey line). We average over 100 random subsets and then additionally apply moving averages (window size 5) for clarity.\nB EVALUATION APPROACH FOR CHEAP IN-CONTEXT LEARNING DYNAMICS In this section, we suggest a\u2014to the best of our knowledge\u2014novel way of evaluating ICL that gives performance metrics at all in-context dataset sizes in a single forward pass without incurring additional cost. We start by introducing the notation necessary to formalize few-shot ICL in LLMs. Dataset to Input String. The few-shot task is defined by a dataset D = {(Si, Yi)}N i=1, where Si \u2208T dSi are input sentences from which to predict the associated labels Yi \u2208T dyi , and T v are text strings of length v. A verbalizer V (S, Y ) takes a sentence\u2013label pair and maps it to an example, e.g. the sequence \u2018I am happy\u2019 and label \u2018positive\u2019 are verbalized as \u2018Sentence: I am happy\\n Label: positive\\n\u2019. We also define a query verbalizer Vq(S) that maps a test query S to a query example, e.g. \u2018I am sad\u2019 is mapped to \u2018Sentence: I am sad\\n Label:\u2019, such that the next-token prediction of an LLM will be encouraged to predict the label for the query. We apply the verbalizer to the entire dataset set and concatenate its output to obtain the context C = \u2295N i=1V (Si, Yi). Finally, we concatenate context C and verbalized query Vq(S), where S is a sentence drawn from a separate test set, to obtain the input to the language model, I = C \u2295Vq(S) \u2208T dI. Input String to Tokens. The input I is tokenized before it can be processed by the language model. The tokenizer, T(I) = (X1, . . . , XM), maps an input sequence I to a sequence of integers, or tokens, Xi \u2208(1, . . . , D), where D is the vocabulary size, i.e. the number of unique tokens. We keep track of which token positions correspond to labels, L = (l1, . . . , lN), e.g. the indices of the tokens immediately following the string \u2018Label:\u2019 in the above example. Tokens to Predictions. In the following, we use capital letters to denote random variables and lower-case letters for their realizations. Here, we describe the behavior of decoder-only language models (Liu et al., 2018; Radford et al., 2018), a popular architecture choice for LLMs. Given the observed sequence of input tokens (X1 = x1, . . . , XM = xM), a single forward pass through the language model gives an estimate of the joint probability, p(X1 = x1) \u00b7 p(X2 = x2 | X1 = x1) \u00b7 ... \u00b7 p(XM = xM | X1 = x1, . . . , XM\u22121 = xM\u22121). (1) We highglight that Eq. (1) gives the joint probability at the observed outcomes: we obtain M \u2018onestep ahead\u2019 predictions, each conditioned only on observed outcomes and not on model predictions. Equation (1) is a common objective in LLM training, where \u2018the joint probability the model assigns to the observations\u2019 is sometimes referred to as teacher forcing (Williams & Zipser, 1989). At test time, LLMs are usually iteratively conditioned on their own predictions, generating novel outputs via multiple forward passes, i.e. one first samples \u02c6xM \u223cp(XM| . . . ), and then \u02c6xM+1 \u223c p(XM+1 | . . . , XM = \u02c6xM), and so on. We here use \u2018. . . \u2019 to stand in for any additional tokens also conditioned on, e.g. (x1, . . . , xM\u22121). One usually ignores all other terms of the joint here\u2014 the predictions for (X1, . . . , XM\u22121) that are generated in each forward pass\u2014as only the last term p(XM | . . . ) is needed to sample the next token, i.e. the label in standard few-shot ICL applications. Single-Forward Pass ICL Training Dynamics. We now explain our approach for efficient evaluation of ICL training dynamics. Given input tokens (X1, . . . , XM) for the few-shot ICL setup described above, we first select those terms from Eq. (1) that correspond to label token predictions,\nB EVALUATION APPROACH FOR CHEAP IN-CONTEXT LEARNING DYNAMICS In this section, we suggest a\u2014to the best of our knowledge\u2014novel way of evaluating ICL that gives performance metrics at all in-context dataset sizes in a single forward pass without incurring additional cost. We start by introducing the notation necessary to formalize few-shot ICL in LLMs. Dataset to Input String. The few-shot task is defined by a dataset D = {(Si, Yi)}N i=1, where Si \u2208T dSi are input sentences from which to predict the associated labels Yi \u2208T dyi , and T v are text strings of length v. A verbalizer V (S, Y ) takes a sentence\u2013label pair and maps it to an example, e.g. the sequence \u2018I am happy\u2019 and label \u2018positive\u2019 are verbalized as \u2018Sentence: I am happy\\n Label: positive\\n\u2019. We also define a query verbalizer Vq(S) that maps a test query S to a query example, e.g. \u2018I am sad\u2019 is mapped to \u2018Sentence: I am sad\\n Label:\u2019, such that the next-token prediction of an LLM will be encouraged to predict the label for the query. We apply the verbalizer to the entire dataset set and concatenate its output to obtain the context C = \u2295N i=1V (Si, Yi). Finally, we concatenate context C and verbalized query Vq(S), where S is a sentence drawn from a separate test set, to obtain the input to the language model, I = C \u2295Vq(S) \u2208T dI. Input String to Tokens. The input I is tokenized before it can be processed by the language model. The tokenizer, T(I) = (X1, . . . , XM), maps an input sequence I to a sequence of integers, or tokens, Xi \u2208(1, . . . , D), where D is the vocabulary size, i.e. the number of unique tokens. We keep track of which token positions correspond to labels, L = (l1, . . . , lN), e.g. the indices of the tokens immediately following the string \u2018Label:\u2019 in the above example. Tokens to Predictions. In the following, we use capital letters to denote random variables and lower-case letters for their realizations. Here, we describe the behavior of decoder-only language models (Liu et al., 2018; Radford et al., 2018), a popular architecture choice for LLMs. Given the observed sequence of input tokens (X1 = x1, . . . , XM = xM), a single forward pass through the language model gives an estimate of the joint probability,\n \u00b7 | \u00b7 \u00b7 | We highglight that Eq. (1) gives the joint probability at the observed outcomes: we obtain M \u2018onestep ahead\u2019 predictions, each conditioned only on observed outcomes and not on model predictions. Equation (1) is a common objective in LLM training, where \u2018the joint probability the model assigns to the observations\u2019 is sometimes referred to as teacher forcing (Williams & Zipser, 1989). At test time, LLMs are usually iteratively conditioned on their own predictions, generating novel outputs via multiple forward passes, i.e. one first samples \u02c6xM \u223cp(XM| . . . ), and then \u02c6xM+1 \u223c p(XM+1 | . . . , XM = \u02c6xM), and so on. We here use \u2018. . . \u2019 to stand in for any additional tokens also conditioned on, e.g. (x1, . . . , xM\u22121). One usually ignores all other terms of the joint here\u2014 the predictions for (X1, . . . , XM\u22121) that are generated in each forward pass\u2014as only the last term p(XM | . . . ) is needed to sample the next token, i.e. the label in standard few-shot ICL applications. Single-Forward Pass ICL Training Dynamics. We now explain our approach for efficient evaluation of ICL training dynamics. Given input tokens (X1, . . . , XM) for the few-shot ICL setup described above, we first select those terms from Eq. (1) that correspond to label token predictions,\n\ufffd For each term, the model predicts a distribution over the entire token vocabulary, i.e. p(Xli | . . . ) is a categorical distribution, p = (p1, . . . , pD), which is then evaluated at the observed tokens in Eq. (2). We can transform this into a prediction over only the few-shot task label Y by selecting the indices of the categorical distribution which correspond to the tokenized labels and then renormalizing, p(Y ) = (pt1, . . . , ptC| . . . ), where C is the number of unique labels which are encoded to tokens (t1, . . . , tC). With this, we can rewrite Eq. (2) as the joint probability the model assigns to the sequence of labels given input sentences\n  Note how, because the joint is evaluated at the observations, its individual terms are always conditioned on the true labels and not previous predictions. This allow us to cheaply compute the training\n(1)\n(2)\n(3)\ndynamics of ICL as a function of increasing in-context dataset size. With each forward pass, we obtain the individual terms of Eq. (3), which are the few-shot ICL predictions at all possible context dataset sizes, i = (1, . . . , N). In contrast, in standard few-shot ICL evaluations, each forward pass only yields predictions for a single test query, neglecting the information the joint contains about the first N \u22121 label predictions. There may be interesting applications of Eq. (3) to model selection, as the quantity has links to both Bayesian evidence (Murphy, 2022) and cross-validation (Fong & Holmes, 2020), although we do not explore this any further in this paper. Multi-Token Labels. So far, we have assumed that each label string is encoded as a single token. However, our approach can also be applied if some or all labels are encoded as multiple tokens. In essence, we continue to measure only the probability the model assigns to the first token of each label, making the (fairly harmless) assumption that the first (or only) token that each label is encoded to is unique among labels. We believe this is justified, as, given the first token for a label, the model should near-deterministically predict the remaining tokens, i.e. all the predictive information is contained in the first token the model predicts for a label. For example, for the Subjectivity dataset, the label \u2018objective\u2019 is encoded by the LLaMa tokenizer as a single token but the label \u2018subjective\u2019 is encoded as two tokens, [subject, ive]. We only use the probability assigned to [subject] to assign probabilities to \u2018subjective\u2019, and ignore any predictions for [ive]. However, if the model successfully accommodates the pattern of the in-context example labels, we would expect probabilities for [ive] following [subject] to be close to 1 always. For LLaMa-7B on Subjectivity, we have investigated the above assumption empirically. After the first observation of the \u2018subjectivity\u2019 label in-context, the probability of predicting [ive] after observing [subject] are 0.9998 \u00b1 0.0003 for the following 12 instances of the \u2018subjectivity\u2019 label, with probabilities normalized over all tokens of the vocabulary here. In other words, we can safely evaluate the performance of the LLaMa model from its predictions of only the [subject] token, even though the full label is split over two tokens [subject, ive].\n# C AUTHORSHIP IDENTIFICATION TASK\nData Collection & Processing. We extract the last 151 messages sent between two authors of this paper on the Slack messaging platform. If multiple messages are sent in a row by one author, these count as multiple inputs. We filter out 42 messages that were of low quality: URLs, article summaries, missed call notifications, and meeting notes. This leaves us with 58 and 51 messages per author. We set the maximum message length to be 200 and truncate any messages longer than that. Before truncation, the longest message was 579 characters long. The median message length is 68 before and after truncation, mean and standard deviation shrink from 100 \u00b1 98 to 88 \u00b1 65. For use in ICL, we treat this dataset as we would any other and present messages in random order. Data Release. For now, we have decided to not make this dataset public for two reasons: (1) It contains genuinely private communication, and (2) releasing the data would mean that future LLMs might be trained on it, so we could no longer use it to test their ability to learn truly novel label relationships in-context. However, below, we give 6 random examples from the dataset: Author 1 Would 10.30am on Tuesday work? Author 1 Sounds good. When are you back again? Author 1 Yeah, we might have to find somewhere else depending on whether my office mates are in, but its out of term so should be plenty of free meeting rooms if needed Author 2 No problem! Author 2 I\u2019ll be in [redacted] next week, so do you think we can meet in person? Author 2 The vacation is 16 days!\n# D DATASET CITATIONS\nWe evaluate on SST-2 (Socher et al., 2013), Subjective (Wang & Manning, 2012), Financial Phrasebank (Malo et al., 2014), Hate Speech (de Gibert et al., 2018), AG News (Zhang et al., 2015), Medical Questions Pairs (MQP) (McCreery et al., 2020), as well as Microsoft Research Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), and Winograd Schema Challenge (WNLI) (Levesque et al., 2012) from GLUE (Wang et al., 2019).\nBelow we give additional details on our experimental evaluation. Guessing Baseline. In our experiments, we frequently display a \u2018guessing based on class frequencies\u2019 baseline as a grey-dashed line. This baseline presents an informed guess that relies only on knowing the class frequencies of the task and makes the exact same prediction for each input datapoint. We here explain how we compute this baseline for accuracy, entropy, and log likelihood. We are given a classification task with C classes which appear with frequencies p = [p1, . . . , pC] in the training set. The baseline always predicts p = [p1, . . . , pC], i.e. it predicts the class frequencies. For accuracy, it thus always predicts the majority class c\u2217= arg maxk pk, which leads to accuracy pc\u2217. Further, the baseline prediction leads to a log likelihood of \ufffd k pk log pk and an entropy of \u2212\ufffd k pk log pk under the training data distribution. Class Flipping. While most of our tasks are binary classification, Financial Phrasebank and AG News are not. For these datasets, when \u2018flipping\u2019 labels in \u00a77, \u00a7A, and \u00a78, we actually rotate labels instead, i.e. we reassign labels y as y \u2190(y +1) mod C, where C is the number of classes. For AG News, [\u2019world\u2019, \u2019sports\u2019, \u2019business\u2019, \u2019science and technology\u2019] get mapped to [\u2019sports\u2019, \u2019business\u2019, \u2019science and technology\u2019, \u2019world\u2019]. For Financial Phrasebank, [\u2019negative\u2019, \u2019neutral\u2019, \u2019positive\u2019] get mapped to [\u2019neutral\u2019, \u2019positive\u2019, \u2019negative\u2019]. Note that, for Financial Phrasebank, rotating the labels is harder than naively inverting the label order, as rotating does not leave the meaning of the \u2018neutral\u2019 label unchanged. In-Context Example Formatting. We use the following simple templates to format the in-context examples. For SST-2, Subjectivity, Financial Phrasebank, Hate Speech, and our author identification task, we use the following line of Python code to format each input example: f\"Sentence: \u2019{sentence}\u2019\\nAnswer: {label}\\n\\n\". For MRPC, WNLI, and RTE, we format instances with f\"Sentence 1: \u2019{sentence1}\u2019\\nSentence 2: \u2019{sentence2}\u2019\\nAnswer: {label}\\n\\n\". For MQP, we use f\"Question 1: \u2019{sentence1}\u2019\\nQuestion 2: \u2019{sentence2}\u2019\\nAnswer: {label}\\n\\n\".\n\u2019{sentence1}\u2019\\nQuestion 2: \u2019{sentence2}\u2019\\nAnswer:\nImplementation. We rely on the Hugging Face Python library (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) to implement the experiments of this paper. We use half-precision floating-point numbers for LLaMa-65B and LLaMa-2-70B, and we use 8 bit-quantization for all other models, which we have found to not affect performance notably. In Fig. E.1, we illustrate this by showing the difference between 8 bit quantization and full 32 bit precision for default ICL and ICL with label randomization for LLaMa-2-7B on the Subjectivity dataset: there is no significant loss of precision or change in behavior from 8 bit quantization. Datasets. We use Hugging Face Spaces to access all tasks considered in this paper. For Hate Speech, we select the first 1000 examples with labels 0 and 1, skipping datapoints with labels 2 and 3. We do not use custom processing for any other dataset. Whitespace Tokenization. To evaluate few-shot ICL performance as introduced in \u00a74, we need to identify the tokens that individual task labels are encoded to. We here detail how to achieve this at the example of the SST-2 label \u2018positive\u2019. In particular, we highlight the, perhaps unexpected, effects of whitespaces on input tokenization. These details are important and, if not considered correctly, can degrade performance significantly. For Falcon models, the tokenizer encodes \u2018Answer:\u2019 as [20309, 37], \u2018Answer:-\u2019 as [20309, 37, 204], and \u2018Answer:-positive\u2019 as [20309, 37, 3508]. We here use dashes \u2018-\u2019 instead of whitespaces for improved legibility. Clearly, the relevant token for the label \u2018positive\u2019 is [3508]. Note how the token [204] for the trailing whitespace disappears again after appending the label. Further, just encoding \u2018positive\u2019 without a preceding whitespace gives [28265]. However, this token does not appear in the input, where the label is preceded by a whitespace\u2014we should thus use the token [3508] to measure ICL performance. The LLaMa and LLaMa-2 tokenizer encodes \u2018Answer:\u2019 as [673, 29901], \u2018Answer:-\u2019 as [673, 29901, 29871], and \u2018Answer:-positive\u2019 as [673, 29901, 6374]. Thus,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d23/0d23ec04-ff5a-4350-b0f3-2cd6d8e02e66.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">No Observed In-Context Examples</div>\n<div style=\"text-align: center;\">Figure E.1: Few-shot ICL at 8 bit and 32 bit precision with default and randomized labels for LLaMa-2-7B on Subjectivity. There is no significant performance degradation from 8 bit quantization. We average over 500 random in-context datasets, thin lines are bootstrapped 99 % confidence intervals, and we apply moving averages (window size 5) for clarity.</div>\n<div style=\"text-align: center;\">Table E.1: Maximum number of in-context examples we consider for each model-task combination. Below, we shorten AG News as AGN, Hate Speech as HS, and Financial Phrasebank as FP.</div>\nSST-2\nSubj\nFP\nHS\nAGN\nMQP\nMRPC\nRTE\nWNLI\nLLaMa-2\n140\n79\n73\n76\n45\n47\n40\n28\n57\nLLaMa\n66\n37\n33\n26\n21\n21\n20\n13\n27\nFalcon\n67\n39\n37\n28\n25\n23\n21\n15\n30\nthe relevant token for the label \u2018positive\u2019 is [6374]. In contrast to the Falcon tokenizer, just encoding \u2018positive\u2019 without a preceding whitespace also gives [6374]. Lastly, similar caveats apply to the classic evaluation procedure for few-shot ICL, where we only evaluate the prediction for a single test query at the end of the input. Here, it is crucially important that we do not end inputs with a trailing whitespace. As we have seen above, for both LLaMa and Falcon tokenizers, the trailing whitespace leads to the generation of an extra token that is not present when encoding complete in-context examples, as the whitespace would usually be included in the label prediction itself. This change in tokenization between in-context examples and test query can adversely affect ICL performance. Statistical Significance. In Tables 1, 2, F.1, and F.2 we bold differences if they are statistically significant at a 95 % level. Concretely, we compute if the absolute average differences are larger than 1.96 times the standard error. Similarly, when deciding if default label performance is significantly better than random guessing performance, we check if mean performance plus 1.645 times the standard error is larger than the guessing baseline across accuracy and log likelihood. Maximum Context Dataset Size. For each task, we create in-context datasets by sub-sampling from the training set of the task. Falcon and LLaMa support input sizes up to 2048 tokens, and LLaMa-2 supports up to 4096 input tokens. For all models, performance will degrade if the input size exceeds this limit. This caps the number of in-context examples we can include for each task. For tasks where the individual input sentences are longer, we will be able to include fewer examples in-context. Across all in-context datasets that we sample for a task, we compute the minimum number of in-context examples needed to exceed the token limit of 2048 or 4096. This is the maximum in-context dataset size up to which we report results for that task. We list these numbers in Table E.1 Calibration. We do not calibrate predicted probabilities by first dividing them by a \u2018prior\u2019 probability and then renormalizing as suggested by Zhao et al. (2021). We have found this rarely improves, and sometimes degrades predictions, cf. Fig. E.2. We observe this happening in particular for tasks where labels are encoded as multiple tokens.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9880/9880eced-a992-4296-9d54-22da664f05d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nHate Spe d In-Con a-65B. W do not fin als and w\nSection 4 \u2013 Training Dynamics: Figure F.1 shows few-shot ICL training dynamics on SST-2 for a selection of models at different parameter counts. Section 5 \u2013 Label Randomization: Figure F.2 gives results for randomized labels for all models on SST2. Figures F.3 to F.12 give results for all tasks and models comparing ICL with randomized labels to the default label setup. Table F.1 gives full summary statistics across all models, tasks, and metrics for the label randomization experiment.",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of how in-context learning (ICL) in Large Language Models (LLMs) improves predictions on downstream tasks by including examples of the input-label relationship. Despite its success, there is no consensus on the mechanisms behind ICL, with conflicting views on whether it learns label relationships or merely recognizes patterns.",
        "problem": {
            "definition": "The problem is understanding how ICL incorporates label information from in-context examples and whether it behaves like a conventional learning algorithm.",
            "key obstacle": "The main challenge is the lack of agreement in existing literature about the nature of ICL and its effectiveness in learning label relationships."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to clarify how ICL functions, particularly in its ability to learn from examples provided in context.",
            "opinion": "The authors believe that ICL does incorporate in-context label information and can learn novel tasks, but not in the same way as conventional learners.",
            "innovation": "This work innovates by rigorously studying ICL behavior across various tasks and model sizes, using probabilistic metrics to analyze its dynamics."
        },
        "Theory": {
            "perspective": "The perspective is that ICL can be conceptualized as a form of learning that utilizes label information but does not conform entirely to the expectations of conventional learning algorithms.",
            "opinion": "The authors posit that conventional learning intuitions can guide the understanding of ICL, but the latter has unique limitations.",
            "proof": "The paper provides empirical evidence through experiments that show ICL predictions depend on in-context labels and can learn novel tasks, but also exhibit biases from pre-training."
        },
        "experiments": {
            "evaluation setting": "The experiments utilize LLMs from the LLaMa and Falcon families, evaluating on various datasets including SST-2, Subjective, and Hate Speech. The study examines ICL performance across all possible numbers of in-context examples.",
            "evaluation method": "The evaluation involves manipulating in-context labels and measuring changes in prediction accuracy, log likelihood, and entropy, using probabilistic metrics to highlight ICL dynamics."
        },
        "conclusion": "The conclusions drawn indicate that while ICL can learn from in-context label information and novel tasks, it struggles to overcome biases from pre-training data and does not treat all in-context information equally.",
        "discussion": {
            "advantage": "The advantages of this paper include a comprehensive analysis of ICL behavior and the introduction of probabilistic metrics to better understand its dynamics.",
            "limitation": "A limitation is that the findings may not generalize to all NLP tasks, especially those where in-context examples provide limited information.",
            "future work": "Future work could explore the development of prompts or other mechanisms to enhance ICL performance and further investigate its limitations in different contexts."
        },
        "other info": [
            {
                "info1": "The paper emphasizes the importance of understanding ICL for safe and effective deployment of LLMs."
            },
            {
                "info2": {
                    "info2.1": "The authors note that prior studies often lacked solid experimental evidence regarding ICL.",
                    "info2.2": "They provide code for reproducing their results, indicating a commitment to transparency in research."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses how in-context learning (ICL) in Large Language Models (LLMs) improves predictions on downstream tasks by including examples of the input-label relationship."
        },
        {
            "section number": "1.2",
            "key information": "The main challenge identified is the lack of agreement in existing literature about the nature of ICL and its effectiveness in learning label relationships."
        },
        {
            "section number": "1.3",
            "key information": "The authors believe that ICL does incorporate in-context label information and can learn novel tasks, but not in the same way as conventional learners."
        },
        {
            "section number": "3.2",
            "key information": "The perspective is that ICL can be conceptualized as a form of learning that utilizes label information but does not conform entirely to the expectations of conventional learning algorithms."
        },
        {
            "section number": "3.3",
            "key information": "The paper innovates by rigorously studying ICL behavior across various tasks and model sizes, using probabilistic metrics to analyze its dynamics."
        },
        {
            "section number": "6.1",
            "key information": "The findings indicate that while ICL can learn from in-context label information and novel tasks, it struggles to overcome biases from pre-training data."
        },
        {
            "section number": "6.4",
            "key information": "Future work could explore the development of prompts or other mechanisms to enhance ICL performance and further investigate its limitations in different contexts."
        }
    ],
    "similarity_score": 0.7337090073212497,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning Learns Label Relationships but Is Not Conventional Learning.json"
}