{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.07099",
    "title": "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such 'in-context' learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.",
    "bib_name": "yu2023explanationawaresoftensembleempowers",
    "md_text": "# E XPLANATION AWARE S OFT E NSEMBLE E MPOWER L ARGE L ANGUAGE M ODEL I N CONTEXT L EARNING\n\nYue Yu \u2660\u2217, Jiaming Shen \u2663, Tianqi Liu \u2663, Zhen Qin \u2663, Jing Nathan Yan \u2662\u2217, Jialu Liu Chao Zhang \u2660, Michael Bendersky \u2663\n\u2660 Georgia Institute of Technology \u2663 Google Research \u2662 Cornell University\n\n# A BSTRACT\n\nA BSTRACT\n\nLarge language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such \u201cin-context\u201d learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose E A SE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.\n\n# I NTRODUCTION\n\nRecent advancements in Natural Language Processing (NLP) have witnessed the remarkable capabilities of Large Language Models (LLMs) (Brown et al., 2020; Tay et al., 2023; Chowdhery et al., 2022; Anil et al., 2023; Touvron et al., 2023; OpenAI, 2023). These LLMs can rapidly adapt to new tasks by learning only on a few input-output pairs (a.k.a. demonstrations) in context, without any gradient update (Wei et al., 2022a; Xie et al., 2022). Yet, beyond those demonstrations, a significant facet of human learning revolves around explanations. These explanations 1, typically in the form of a few keywords or sentences, reveal the underlying principles connecting the input and output (Zaidan et al., 2007; Narang et al., 2020). Consequently, the integration of free-text explanations into LLM prompting holds great potentials to further enhance in-context learning performance.\nRecent studies have examined how to incorporate free-text explanations into LLM in-context learning scheme. For instance, the Predict-then-Explain pipeline (Lampinen et al., 2022) proposes to generate the explanation after making the prediction. Consequently, the predictions from LLM won\u2019t directly benefit from their corresponding explanations. In contrast, the Explain-then-Predict pipeline (also called \u201cChain-of-Thought\u201d) (Nye et al., 2021; Wei et al., 2022b) generates explanations before making predictions via greedy sampling. When the LLM-generated explanations are unreliable, predictions from this approach will be largely distracted and defective (Ye & Durrett, 2022). To mitigate this issue, Wang et al. (2023c) improves the \u201cChain-of-Thought\u201d pipeline by first generating multiple predictions with different explanations using temperature sampling and then aggregating them via majority voting. However, this approach can be sub-optimal as (1) temperature sampling increases the inconsistency between generated explanations and their associated class predictions, and (2) majority voting treats different predictions associated with explanations of varying qualities equally. As a result, how to robustly leverage natural language explanations for empowering LLM in-context learning remains an open research question.\nIn this work, we present a novel E xplanationa ware S oft E nsemble framework, named E A SE, to assist LLM in-context learning with explanations. Our technique integrates explanations into the\n\u2217 Work done during the internship at Google Research. E-mail: yueyu@gatech.edu\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/85ec/85ec7299-5c66-4404-8deb-a50e809e3775.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The overview of E A SE framework.\n</div>\nensemble procedure and employs soft probability to mitigate discrepancies between explanations and predictions. The key module of the E A SE framework hinges upon the idea of weighted ensemble: As shown in Figure 1, instead of treating all predictions equally, we assign a score to each prediction based on the contextual relevance and inherent quality of its associated explanation, which will be used as a weight during the final ensemble stage. This explanation-aware ensemble stage is also realized with an LLM \u2014 after generating explanations and predictions using temperature sampling for each test instance, we prompt the LLM to weight all class predictions based on their associated explanations in an in-context manner. While the LLM offers great promise for the weighting purpose, it is crucial to provide sufficient supervision signals as demonstrations to guide the LLM scoring, yet the primary constraint for this step lies in the absence of negative explanations from few-shot demonstrations. To construct negative examples efficiently, we first use LLM to generate explanations for few-shot demonstrations, then select explanations associated with incorrect predictions as the negative samples. In this way, the LLM scorer can be readily applied to perform explanation-aware ensembling without any additional annotation.\nBeyond explanation-aware ensembling, E A SE incorporates an additional technique named soft probability aggregation, which helps to mitigate the inconsistency  between explanations and predictions, given the sampling process may inevitably infuse noises into the final prediction. Specifically, it employs probabilities across various class-indicative verbalizers in place of the original one-hot predictions. This design, although conceptually simple, can effectively reduce the discrepancies between explanations and predictions and further improve the final predictions accuracy.\n\n# Our contributions can be summarized as follows:\n\n\u2022 We propose the E A SE framework to better facilitate in-context learning for large language models with natural language explanations.\n\nels with natural language explanations.\n\u2022 We design two techniques, namely explanation-aware ensemble and soft probability aggregation, to enable the model to focus on predictions associated with explanations of higher qualities while reducing the inconsistency between explanations and predictions.\n\u2022 We conduct experiments on seven natural language understanding (NLU) datasets spanning between natural language inference (NLI) and question answering (QA), and our method outperforms previous state-of-the-art approaches using different LLMs as the backbone. Our analysis further justifies the advantages of using LLMs for explanation weighting to support correct answer candidates and leveraging soft probability aggregation to mitigate inconsistent predictions.\n\n\u2022 We design two techniques, namely explanation-aware ensemble and soft probability aggregation to enable the model to focus on predictions associated with explanations of higher qualities while reducing the inconsistency between explanations and predictions.\n\n\u2022 We conduct experiments on seven natural language understanding (NLU) datasets spanning between natural language inference (NLI) and question answering (QA), and our method outperforms previous state-of-the-art approaches using different LLMs as the backbone. Our analysis further justifies the advantages of using LLMs for explanation weighting to support correct answer candidates and leveraging soft probability aggregation to mitigate inconsistent predictions\n\n# R ELATED W ORK\n\nTwo prevalent explanation types exist for interpreting NLP models: (1)  extraction-based explanations that highlight important segments of the original input text (Zhang et al., 2016; DeYoung et al., 2020; Paranjape et al., 2020; Zhou et al., 2020; Yin & Neubig, 2022) and (2) free-form explanations that craft prediction rationales directly using natural language text (Rajani et al., 2019; Sun et al., 2022; Wiegreffe et al., 2021; 2022; Wang et al., 2023a; Ludan et al., 2023). Beyond aiding in model\n\ninterpretation, recent studies have demonstrated that these explanations can also enhance the fewshot learning capabilities of large language models. For example, Wei et al. (2022b); Zelikman et al. (2022) propose to prepend explanations before the answers while Lampinen et al. (2022) suggest adding post-answer explanations. Given that these explanations are often derived during the LLM decoding stage and may contain noise, (Wang et al., 2023c; 2022) advocate for generating multiple candidate explanations with their respective predictions, followed by aggregating these predictions via majority voting. In our study, we focus on free-form explanations  and explore how to better aggregating these predictions with explanations in a weighted ensemble. Using a bootstrapped LLM, we subsequently evaluate each explanation to enhance in-context learning outcomes.\nAnother line of research related to our study is automated explanation quality evaluation (Sun et al., 2022; Joshi et al., 2023; Wiegreffe et al., 2021; Chen et al., 2023a; c). Ye & Durrett (2022) utilize lexical features to measure the faithfulness of explanations without considering their semantics. Chen et al. (2021); Li et al. (2023b) leverage a NLI fine-tuned model to verify the explanations reliability. (Fu et al., 2023; Liu et al., 2023; Qin et al., 2023; Chen et al., 2023b) also study how to use LLM to build a generic text quality scorers for generation and ranking tasks. These studies often rely on additional ground-truth labels and human annotations, making them less suitable when the labels for test instances are unknown. In contrast, our research diverges from the pure evaluative perspective while focusing more on effectively leveraging model-generated explanations to empower the LLM in-context learning performance. There are also several works that attempted to use LLMs to generate demonstrations (Shao et al., 2023; Kim et al., 2023; Yu et al., 2023), but they mainly focus on producing few-shot demonstrations, whereas our approach emphasizes the generation of negative examples for more robust scoring and evaluation of explanations.\n\n# 3 M ETHOD\n\nIn this section, we first give a brief introduction to the problem definition. Then, we present ou approach with two designs, namely explanation-aware ensemble and soft probability aggregation with the goal of leveraging the generated explanations to improve the final prediction performance\n\n# 3.1 P ROBLEM D EFINITION\n\nIn this task, we are given a LLM M parameterized by \u03b8, a set of few-shot demonstrations D = {(x i, e i, y i)} K i =1 on a target classification task 2, where K is the number of demonstrations, x i, y i are the input text and label for the i-th example, and e i is the corresponding ground-truth explanation. For each test example x \u2208D test, we aim to leverage M and D to predict its own label. Our primary goal is to improve the prediction accuracy for test examples.\n\nHere we give a brief introduction to the self-consistency approach (Wang et al., 2023c). For each test example x \u2208D test, it first forms the prompt for few-shot demonstrations as P = \ufffd T, shuffle(\u2225 K i =1 (x i, e i, y i) \ufffd}, where T is the prompt template, and shuffle \ufffd \u2225 K i =1 (x i, e i, y i) \ufffd is a permutation of K demonstrations. Then, it generates N  candidate explanations together with predictions (denoted as (e j, p j)) via sampling from the LLM with non-zero temperature as\n\nFinally, it aggregates these N candidates into the final prediction via majority voting as\n\n\ufffd\nSelf-consistency enhances the standard explain-then-predict pipeline by utilizing multiple predicions derived from varied explanations. Despite its strong performance, through our examination, we\u2019ve pinpointed two primary bottlenecks within the self-consistency pipeline, listed as follows:\n\n\ufffd\nSelf-consistency enhances the standard explain-then-predict pipeline by utilizing multiple predictions derived from varied explanations. Despite its strong performance, through our examination, we\u2019ve pinpointed two primary bottlenecks within the self-consistency pipeline, listed as follows:\n2 Future work would be suited to consider extending our work to generative tasks.\n\n(1)\n\n\u2022 Explanation-agnostic Ensembling: Self-consistency uniformly weights all predictions and aggregates them via simple majority voting. This approach overlooks the variance in explanation quality, which can be problematic when certain predictions stem from flawed reasoning paths evident in poor-quality explanations.\n\u2022 Explanation-Prediction Inconsistency: During its prediction phase, Self-consistency employs the temperature sampling technique to draw samples from the LLM. This sampling step can introduce noise, leading to predictions that are inconsistent with their corresponding explanations (Ye & Durrett, 2022).\n\nintroduce noise, leading to predictions that are inconsistent with their corresponding explanations (Ye & Durrett, 2022).\nThe identified limitations necessitate the need for new techniques to better harvest intermediate explanations for obtaining the final prediction. Towards this goal, we propose our framework E A SE, which is tailored to tackle the aforementioned challenges. E A SE is comprised with two techniques, explanation-aware ensemble and soft probability aggregation, to optimize the LLM\u2019s prediction accuracy when deriving final outcomes from multiple candidate explanations.\n\nThe identified limitations necessitate the need for new techniques to better harvest intermediate explanations for obtaining the final prediction. Towards this goal, we propose our framework E A SE, which is tailored to tackle the aforementioned challenges. E A SE is comprised with two techniques, explanation-aware ensemble and soft probability aggregation, to optimize the LLM\u2019s prediction accuracy when deriving final outcomes from multiple candidate explanations.\n\n# 3.3 E XPLANATION GUIDED E NSEMBLE\n\nLLMs typically produce multiple explanations along with their predictions through a sampling process. Due to the intrinsic randomness of this sampling, the quality of these predictions can fluctuate. To address the potential pitfalls where erroneous explanations results in inaccurate predictions, we introduce the explanation-aware ensemble technique. This method estimates the significance of each class prediction based on its corresponding explanation. Consequently, our explanation-aware ensemble technique ensures that predictions linked with better explanations carry greater weight during the final prediction aggregation phase.\nLLM as Explanation Scorer To evaluate various explanations, past research has either measured the lexical overlap between the explanation and the input text (Ye & Durrett, 2022) or employed models fine-tuned for NLI tasks (Chen et al., 2021; Li et al., 2023b). In contrast to these methods, which either overlook the deep semantics of explanations or require extra human-annotated data, our explanation scorer is developed based on the powerful LLM M, directly harnessing its inherent linguistic and reasoning capabilities.\nGiven the original task input x and one explanation e, we use the verbalizer v pos (v neg) to represent the class of this explanation being \u201cpositive\u201d (\u201cnegative\u201d). A \u201cpositive\u201d explanation means this explanation can help the model reach correct answer and a \u201cnegative\u201d explanation means the other way around. Then, we craft a supplementary prompt T score = \u201cCan this explanation be used to help the model answer the question?\u201d for LLM prompting. With the verbalizers and prompts, we effectively recast the problem of explanation scoring into determining the conditional probability of producing the verbalizer aligned with the positive label v pos, expressed as\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dcc4/dcc45520-c40c-4c05-b502-eef81043acbd.png\" style=\"width: 50%;\"></div>\nBootstrapped LLM Scorer Although the above approach can already produce scores for each prediction, the score generated with the LLM M can still be biased and less precise (Wang et al., 2023b), especially under the zero-shot scenario where no demonstrations are provided. To mitigate the bias and generate reliable scores, we propose to provide additional examples to serve as \u201cpositive\u201d and \u201cnegative\u201d explanations to facilitate LLM scoring using the original few-shot demonstrations in D.\n\n(3)\n\n<div style=\"text-align: center;\">Figure 2: Bootstrapped LLM Scorer.\n</div>\n\u201cpositive\u201d examples from the ground-truth explanation. Obtaining \u201cnegative\u201d examples, on the other hand, can be more challenging as they are not explicitly provided. To tackle this issue, we exploit the assumption based on the utility of explanations: an ideal explanation should guide the model towards the accurate prediction of ground-truth labels (Wiegreffe et al., 2021). Consequently, it\u2019s reasonable to classify explanations leading to erroneous predictions as \u201dnegative\u201d. In practice, for every instance (x i, y i) \u2208D, we randomly select k (8 in this work) exemplars from the training set and then use these as demonstrations and generate a set of candidate pairs C i = {(e ij, p ij)} N j =1 via sampling from the LLM. Then, if the explanation-prediction pair (e ij, p ij) from C i satisfies y i \u0338 = p ij, we select e ij to serve as the negative explanation set N i for x i as\n\n\u201cpositive\u201d examples from the ground-truth explanation. Obtaining \u201cnegative\u201d examples, on the other hand, can be more challenging as they are not explicitly provided. To tackle this issue, we exploit the assumption based on the utility of explanations: an ideal explanation should guide the model towards the accurate prediction of ground-truth labels (Wiegreffe et al., 2021). Consequently, it\u2019s reasonable to classify explanations leading to erroneous predictions as \u201dnegative\u201d. In practice, for every instance (x i, y i) \u2208D, we randomly select k (8 in this work) exemplars from the training set and then use these as demonstrations and generate a set of candidate pairs C i = {(e ij, p ij)} N j =1 via sampling from the LLM. Then, if the explanation-prediction pair (e ij, p ij) from C i satisfies y i \u0338 = p ij, we select e ij to serve as the negative explanation set N i for x i as\nN i = {(e ij, p ij) \u2208C i | y i \u0338 = p ij}. (4)\nTo finalize the demonstration set for the LLM scoring step, we balance between \u201cpositive\u201d and \u201cnegative\u201d explanations: only instances possessing negative explanations (i.e. with non-empty N i) are incorporated into the demonstrations; For every instance, a single negative explanation is chosen at random from the respective candidate set. This methodology produces a balanced demonstration set for LLM-based explanation scoring without requiring extra human annotations.\n\nsampling from the LLM. Then, if the explanation-prediction pair (e ij, p ij) from C i satisfies y i \u0338 = p ij, we select e ij to serve as the negative explanation set N i for x i as\nN i = {(e ij, p ij) \u2208C i | y i \u0338 = p ij}. (4)\nTo finalize the demonstration set for the LLM scoring step, we balance between \u201cpositive\u201d and \u201cnegative\u201d explanations: only instances possessing negative explanations (i.e. with non-empty N i) are incorporated into the demonstrations; For every instance, a single negative explanation is chosen at random from the respective candidate set. This methodology produces a balanced demonstration set for LLM-based explanation scoring without requiring extra human annotations.\n\nTo finalize the demonstration set for the LLM scoring step, we balance between \u201cpositive\u201d and \u201cnegative\u201d explanations: only instances possessing negative explanations (i.e. with non-empty N i) are incorporated into the demonstrations; For every instance, a single negative explanation is chosen at random from the respective candidate set. This methodology produces a balanced demonstration set for LLM-based explanation scoring without requiring extra human annotations.\n\nIn the preceding step, the primary objective is to assign a score to each prediction based on its associated explanation through the LLM M. This process, however, does not account for directly modeling the LLM\u2019s output predictions. To bridge this gap, we propose soft probability aggregation, a simple and intuitive approach to resolve the discrepancy between the explanations and predictions\u2014 rather than aggregating over the raw predictions, it directly computes the sum of the probabilities associated with each potential label, expressed as\n\n\ufffd\nThe soft probability aggregation addresses the noise inherited in different LLM sampling-base decoding algorithms, resulting in a more accurate and refined final prediction.\n\n3.5 S UMMARY\n\nBy plugging these two techniques together, we obtain the final prediction \ufffd y for the test instance\n\n\ufffd\nwhere e j is the intermediate explanations generated via Eq. 1, the \u03c9 e j is the weight for e j using the bootstrapped LLM scorer using Eq. 3, and p \u03b8 (y | P, x, e j) is the soft probability generated using Eq. 5. Overall, calculating the score for each explanation and the soft probability both take an additional O (N) time complexity. Fortunately, these two steps do not require additional model training and can be efficiently supported with distributed inference techniques in practice. Other than these two techniques, our framework keeps other components intact and can be plugged into most LLM backbones for empowering its in-context learning ability.\n\n# 4 E XPERIMENTS\n\n# 4.1 E XPERIMENT S ETUPS\n\nTasks We evaluate our E A SE framework on two types of tasks: natural language inference and question answering. Specifically, we use the following datasets: (1) E-SNLI (Camburu et al., 2018) is an enriched version of the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), augmented with human-annotated natural language explanations for entailment relations; (2) ANLI-R1/R2/R3 (Nie et al., 2020) is a set of three collections of adversarially generated NLI examples curated through a human-in-the-loop process; (3) ECQA (Aggarwal et al., 2021) is built upon CommonsenseQA benchmark (Talmor et al., 2019) and contains additional human-annotated question explanations; (4) OpenbookQA (Mihaylov et al., 2018) is a QA dataset that requires com\n\n(4)\n\n(5)\n\n(6)\n\nTable 1: The main experiments results, where \u201cBLS\u201d stands for bootstrapped LLM scorer and \u201cSPA\u201d stands for soft probability aggregation.\n\nBackbone\nMethods\nE-SNLI\nANLI-R1\nANLI-R2\nANLI-R3\nECQA\nStrategyQA\nOpenbookQA\nAverage\nPaLM 2-S\nICL (Brown et al., 2020)\n59.88\n54.38\n48.10\n52.66\n59.84\n66.69\n80.21\n60.25\nPE (Lampinen et al., 2022)\n71.02\n62.59\n55.18\n57.17\n74.39\n71.75\n79.70\n67.40\nEP (Wei et al., 2022b)\n64.53\n57.40\n53.00\n53.33\n72.11\n72.40\n81.38\n64.88\nSelf-consistency (Wang et al., 2023c)\n68.68\n65.40\n56.49\n59.00\n74.48\n76.94\n83.47\n69.21\nFLamE (Zhou et al., 2023)\n67.58\n60.36\n52.00\n50.15\n72.80\n75.33\n80.14\n65.48\nEASE\n75.01\n66.48\n59.66\n64.33\n75.59\n78.23\n84.10\n71.92 (\u21913.91%)\nEASE w/o BLS\n73.84\n66.84\n58.74\n62.66\n75.17\n78.40\n83.91\n71.37\nEASE w/o SPA\n69.82\n67.77\n58.50\n62.50\n75.42\n78.33\n83.68\n70.73\nPaLM 2-L\nICL (Brown et al., 2020)\n87.42\n79.00\n68.33\n65.65\n81.29\n81.13\n91.17\n79.14\nPE (Lampinen et al., 2022)\n88.84\n80.55\n71.49\n68.33\n83.13\n83.19\n92.46\n81.14\nEP (Wei et al., 2022b)\n84.59\n79.03\n67.99\n67.66\n80.51\n85.45\n89.74\n79.28\nSelf-consistency (Wang et al., 2023c)\n87.34\n81.29\n73.16\n70.16\n82.67\n87.85\n92.88\n82.19\nFLamE (Zhou et al., 2023)\n83.23\n71.85\n58.50\n56.83\n80.26\n84.79\n93.14\n75.51\nEASE\n89.42\n83.69\n76.16\n74.00\n83.65\n89.90\n93.93\n84.40 (\u21912.69%)\nEASE w/o BLS\n88.94\n82.87\n75.60\n72.66\n83.42\n89.34\n93.72\n83.79\nEASE w/o SPA\n88.21\n82.59\n73.83\n71.33\n83.42\n89.35\n93.51\n83.18\nprehensive understanding and reasoning from open-book sources. As no ground-truth explanations are given, we use the provided facts for each question as the proxy explanations. (5) StrategyQA (Geva et al., 2021) focuses on reasoning over complex, multi-hop questions that often require strategic planning and decision-making.\nBaselines We consider the following baselines: (1) Standard In-context Learning (ICL) (Brown et al., 2020): it solely uses the input-label pairs for few-shot learning without using natural language explanations. (2) Predict-then-Explain (PE) (Lampinen et al., 2022): it provides the explanation after the labels for each instance when constructing prompts for demonstrations. During the inference stage, it generates the explanation after the prediction. (3) Explain-then-Predict (EP) (Wei et al., 2022b): it is the standard chain-of-thought pipeline which provides an explanation before the label for demonstrations. During the inference stage, it first generates an explanation, then followed by the prediction. Note that for both PE and EP method, we use greedy sampling to obtain the explanation and prediction. (4) Self-consistency (Wang et al., 2022; 2023c): it improves over the standard EP pipeline by aggregating over multiple explanations from LLMs to enhance the robustness of the results. (5) FLamE (Zhou et al., 2023) is a recent LLM few-shot learning method that generates multiple label-conditioned explanations and determines the final prediction based on the label that achieves the highest logit after reviewing all explanations for the given instance 3.\nImplementation Details In our main experiments, we use PaLM2-S and PaLM2-L (Anil et al.,\n2023) as the backbone model. Results on more (open source) backbone models are reported in Section 4.3. For each dataset, we set the size of few-shot examples to 48 following (Zhou et al., 2023; Marasovic et al., 2022), and fit as many instances as possible during inference until reached the maximum length. As the LLM is often sensitive to the selection of few-shot examples (Yu et al., 2022; Ye & Durrett, 2023; Liu et al., 2022), for each dataset, we create 5 splits from the original dataset, each containing 300 test examples, and report the average performance over 5 splits. During sampling, we set the default temperate to t = 0. 7 and sample N = 9 candidate explanations for each instance.\n\nprehensive understanding and reasoning from open-book sources. As no ground-truth explanations are given, we use the provided facts for each question as the proxy explanations. (5) StrategyQA (Geva et al., 2021) focuses on reasoning over complex, multi-hop questions that often require strategic planning and decision-making.\nBaselines We consider the following baselines: (1) Standard In-context Learning (ICL) (Brown et al., 2020): it solely uses the input-label pairs for few-shot learning without using natural language explanations. (2) Predict-then-Explain (PE) (Lampinen et al., 2022): it provides the explanation after the labels for each instance when constructing prompts for demonstrations. During the inference stage, it generates the explanation after the prediction. (3) Explain-then-Predict (EP) (Wei et al., 2022b): it is the standard chain-of-thought pipeline which provides an explanation before the label for demonstrations. During the inference stage, it first generates an explanation, then followed by the prediction. Note that for both PE and EP method, we use greedy sampling to obtain the explanation and prediction. (4) Self-consistency (Wang et al., 2022; 2023c): it improves over the standard EP pipeline by aggregating over multiple explanations from LLMs to enhance the robustness of the results. (5) FLamE (Zhou et al., 2023) is a recent LLM few-shot learning method that generates multiple label-conditioned explanations and determines the final prediction based on the label that achieves the highest logit after reviewing all explanations for the given instance 3.\n\nImplementation Details In our main experiments, we use PaLM2-S and PaLM2-L (Anil et al.,\n2023) as the backbone model. Results on more (open source) backbone models are reported in Section 4.3. For each dataset, we set the size of few-shot examples to 48 following (Zhou et al., 2023; Marasovic et al., 2022), and fit as many instances as possible during inference until reached the maximum length. As the LLM is often sensitive to the selection of few-shot examples (Yu et al., 2022; Ye & Durrett, 2023; Liu et al., 2022), for each dataset, we create 5 splits from the original dataset, each containing 300 test examples, and report the average performance over 5 splits. During sampling, we set the default temperate to t = 0. 7 and sample N = 9 candidate explanations for each instance.\n\n4.2 O VERALL R ESULTS\n\n# 4.2 O VERALL R ESULTS\n\nTable 1 exhibits the performance of E A SE and baselines on seven datasets using PaLM 2-S and PaLM 2-L as the backbone. From the results, we have the following findings: First, we can see that leveraging explanations often improves LLM in-context learning. This enhancement is particularly pronounced when the final prediction is aggregated from multiple predictions sampled from the LLM. Conversely, the standard EP pipeline sometimes even hurts the performance, especially for larger models. Second, despite its complex design, the latest baseline FLamE often falls short compared to other baselines, which suggests that fine-tuning an additional classifier is particularly important for FLamE and it might be less compatible with the LLM in-context learning framework. Third, we notice that E A SE can consistently outperform all other methods across both the PaLM 2-S and PaLM 2-L backbones in nearly all datasets, which indicates that E A SE provides a reliable\n\n3 In the original FLamE paper, the RoBERTa is used for final classification. For a fair comparison, we adjusted FLamE to use the in-context LLM as the classifier.\n\n<div style=\"text-align: center;\">Table 2: The main experiments results on open-source models, where \u201cBLS\u201d stands for bootstrapped LLM scorer and \u201cSPA\u201d stands for soft probability aggregation.\n</div>\nModel (\u2192)\nFLAN-UL2 (20B)\nLlama-2 (7B)\nDataset (\u2192)\nStrategyQA\nE-SNLI\nANLI-R1\nANLI-R2\nANLI-R3\nECQA\nStrategyQA\nOpenbookQA\nAvg.\nICL (Brown et al., 2020)\n61.76\n51.14\n34.58\n36.05\n27.48\n45.48\n53.81\n47.48\n42.29\nPE (Lampinen et al., 2022)\n73.42\n54.25\n37.83\n37.50\n34.37\n52.33\n56.21\n56.48\n47.00\nEP (Wei et al., 2022b)\n75.46\n56.90\n35.41\n39.16\n36.04\n54.45\n57.17\n44.35\n46.21\nSelf-consistency (Wang et al., 2023c)\n76.01\n58.79\n40.16\n40.16\n36.16\n55.14\n57.12\n60.87\n49.77\nFLamE (Zhou et al., 2023)\n72.17\n49.32\n36.83\n35.16\n36.50\n45.11\n57.70\n46.23\n43.84\nEASE\n78.70 (\u21913.55%)\n60.80\n44.50\n41.66\n41.33\n60.45\n59.81\n64.43\n53.28 (\u21917.05%)\nEASE w/o BLS\n77.31\n59.54\n43.45\n41.33\n40.33\n60.34\n59.62\n65.06\n52.81\nEASE w/o SPA\n77.78\n58.50\n41.33\n40.16\n35.33\n54.97\n57.40\n61.71\n49.91\n<div style=\"text-align: center;\">Table 3: The study on different scoring approaches. Note that to ensure fair comparison, we do not use soft probability aggregation for our method and baselines.\n</div>\nTable 3: The study on different scoring approaches. Note that to ensure fair comparison, we do n use soft probability aggregation for our method and baselines.\n\nDataset (\u2192)\nE-SNLI\nOpenbookQA\nStrategyQA\nModel (\u2192)\nPaLM 2-S\nPaLM 2-L\nPaLM 2-S\nPaLM 2-L\nFLAN-UL2\nEASE\n69.82\n83.68\n83.68\n93.51\n78.70\nEASE w/ PE Negative\n68.90\n83.91\n83.54\n93.93\n78.06\nLLM Zero-shot Scoring (Fu et al., 2023)\n66.84\n81.77\n81.38\n88.50\n75.15\nLLM Pairwise Scoring (Qin et al., 2023)\n69.25\n82.97\n82.97\n93.14\n76.93\nLexical Scoring (Ye & Durrett, 2022)\n67.72\n83.54\n82.66\n93.72\n75.34\nNLI Scoring (Chen et al., 2021)\n64.87\n81.89\n82.21\n91.52\n76.11\nway to improve LLM in-context learning over different tasks. Finally, When comparing E A SE with its own variants (e.g. w/o BLS and SPA), it\u2019s observed that the original E A SE consistently holds an advantage, indicating the necessity of both PW and SA components in maximizing performance.\n\n# 4.3 R ESULTS ON O PEN SOURCE M ODELS\n\nIn order to demonstrate the generalizability of our E A SEframework, as well as promote reproducibility, we extend our investigations to open-source LLMs including FLAN-UL2 (Tay et al., 2023) 4 and Llama-2-7b (Touvron et al., 2023). Both models have publicly accessible weights 5. As exhibited in Table 2, we observe that these two models generally perform worse than the PaLM 2 model in the main experiments, as they have fewer parameters, and thus may not perform well on these challenging NLU benchmarks. Despite this, the experiment results still align with our prior findings, demonstrating that our proposed techniques can consistently yield performance enhancements across these open-source LLMs.\n\n4.4 S TUDY ON E XPLANATION AWARE E NSEMBLE\n\nWe perform additional experiments to further understand the benefit of the explanation-aware ensemble, and the result is shown in Table 3.\n\nPerformance w/ Different Scoring Methods We first compare our LLM-based explanation scorer with a few alternative methods including (1) lexical scoring, which estimates the reliability of explanations via the lexical gap (Ye & Durrett, 2022), and (2) NLI Scoring that uses an NLI model to verify the reliability of explanations. In this work, we use MT5-XXL (Xue et al., 2021) fine-tuned on NLI datasets as the scorer. Overall, we observe that our model outperforms these models in most of the cases, indicating that LLM has a strong capacity for estimating the quality of the explanations. In addition, we observe that pairwise scoring does not perform well for weighting the predictions. This is because it was originally proposed for text ranking tasks, while there are many differences between it and our scenarios, including input formats and relevance signals.\nPerformance w/ Different Bootstrapping Strategies To justify the design of leveraging the Explain-then-Predict (EP) pipeline to generate negative demonstrations, we also consider other ways including removing demonstrations as well as using the Predict-then-Explain (PE) pipeline. Over\n\n4 Link: https://github.com/google-research/google-research/tree/master/ ul2. We only test on StrategyQA dataset since FLAN-UL2 has been fine-tuned on labeled data from other datasets, thus violating the true few-shot setting. 5 Link: https://huggingface.co/meta-llama/Llama-2-7b.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e98a/e98ab51d-52aa-4d9b-83c7-54a969ee9c6d.png\" style=\"width: 50%;\"></div>\nall, in many cases, using the EP pipeline leads to better results, as we observe that the PE pipeline sometimes causes the false negative issue: it will first generate incorrect predictions but followed with reasonable explanations. However, when the model performs reasonably well (e.g. PaLM 2-L on OpenbookQA), then it may make less erroneous prediction during the bootstrapping step, which may lead to insufficient training signals for E A SE to perform well. In addition, no matter whether PE and EP is used, they both largely outperform the baseline where no demonstration is given, necessitating the role of demonstration for explanation-aware ensembling.\nScore Distribution of Explanations To delve deeper into the scores assigned to each explanation and justify that better scores are assigned to explanations with correct answers, we plot the score distribution for explanations with correct predictions 6 in Figure 3. Overall, we observe that explanations that lead to correct answers generally have higher scores \u2014 the score distribution is more skewed towards higher values. Besides, the score distribution using PaLM2-L on explanation with correct and incorrect predictions are more separable, indicating larger models tend to have better scoring performance.\n\nHuman Study on Explanations We conduct additional human studies to further investigate whether the scores generated by LLM are aligned with human preferences. For each instance, we sample two explanations with different predictions as {(e 1, p 1), (e 2, p 2)}, with one being correct. We compare our approach and two baselines (NLI model, lexical overlap) with human raters: for each pair of explanations, we first ask four humans to determine which explanation is better and use c i (i = 1, 2) to denote the number of raters that select e i  as the better one. Then, we use different models to estimate the score for explanations separately, denoted as (s e 1, s e 2). The final judge of \u201cWin-Tie-Lose\u201d is determined to be:\n\nr =\n\uf8f1 \uf8f4 \uf8f2\nwin, if (c 1> c 2 and s e 1> s e 2) or (c 1 < c 2 and s e 1 < s e 2); tie, c 1 = c 2; lose, if (c 1 < c 2 and s e 1> s e 2) or (c 1> c 2 and s e 1 < s e 2).\n\uf8f4 \uf8f3\n\n\uf8f4 \uf8f3\nOn two datasets, we randomly select 80 instances, and the final results are shown in Figure 4. The ohen\u2019s kappa among human raters are 0.75 (E-SNLI) and 0.64 (StrategyQA), which stands for substantial agreement\u201d. Overall, we observe that E A SE aligns with human preferences the best, ndicating its better ability to be the proxy for explanation quality estimation. We display more xamples on generated explanations and the scores in Appendix E.1.\n\n# 4.5 S TUDY ON S OFT P ROBABILITY A GGREGATIO\n\nS TUDY ON S OFT P ROBABILITY A GGREGATION\n\nThe premise behind soft probability aggregation is the potential inaccuracy in the prediction token due to temperature sampling variability. To verify this, we calculate the proportion of cases where the prediction token p i is different than the prediction p i \u0338 = argmax p (\u00b7|P, x, e i).\n\n6 To eliminate the effect of the sampling randomness, we calculate the predic ity using Eq. 5.\n\n<div style=\"text-align: center;\">(c) OBQA, PaLM2-S\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f2b/8f2bd10c-f6c6-4ac8-8f7d-a0a45a2d1c5a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Human Evaluation.\n</div>\n(7)\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8de9/8de98e3e-b150-4be4-9408-d6fe9d408faa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Prompt Format\n</div>\n<div style=\"text-align: center;\">Figure 6: Effect of different temperatures.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f5d0/f5d09ad9-b76e-4c2b-b6bc-95d5f52bd3f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b69e/b69ee7d0-50c1-458a-8e48-788b2a9828ef.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) E-SNLI\n(b) StrategyQA\nFigure 7: Effect of number of explanations.\n</div>\n<div style=\"text-align: center;\">Figure 7: Effect of number of explanations.\n</div>\nOverall, as exhibited in Table 4, we observe that such inconsistency predictions appear in 10% to 15% of the cases, which is not rare in practice. By using the soft score, we observe that it will consistently lead to performance boosts. The gain is more evident when the inconsistency issue is more severe \u2014 on E-SNLI\n\nTable 4: The study on different probability aggregation approaches. Note that we do not use explanation-aware ensemble for our method and baselines.\n\nDataset (\u2192)\nE-SNLI\nOpenbookQA\nStrategyQA\nModel (\u2192)\nPaLM 2-S\nPaLM 2-L\nPaLM 2-S\nPaLM 2-L\nFLAN-UL2\nInconsistency Ratio\n14.60%\n10.06%\n13.96%\n10.71%\n10.00%\nEASE\n73.84\n88.21\n83.91\n93.72\n78.70\nw/ argmax\n73.20\n87.90\n83.68\n93.51\n78.42\nCond. Gen (Li et al., 2023a)\n70.77\n82.20\n78.07\n84.38\n72.80\ndataset using PaLM 2-S as the backbone, there exist around 15% examples with inconsistent predictions. When incorporating soft probability aggregation, we observe a notable performance gain (from 68.68% to 73.84%). When compared to other methods for prediction correction, such as using the hard prediction (i.e. argmax p (\u00b7|P, x, e i)) or generation probability conditioned on different verbalizers, E A SE also achieves better performance. More case studies on using soft probabilities are deferred to Appendix E.2.\n\n4.6 A DDITIONAL S TUDIES\n\nAs E A SE relies on several key components such as prompts and sampling steps, in this section, we study their effect on the final prediction performance, using PaLM 2-S as the backbone model.\nEffect of the Sampling Temperatures and Prompt Templates We study the robustness E A SE to different prompt templates by choosing three different prompt formats from (Bach et al., 2022) (the details are in Appendix B.3) on two datasets. Overall, from Figure 5 we observe that E A SE is robust to them as all of the prompt formats lead to performance gains when compared to the strongest baseline self-consistency. Similarly, in Figure 6, we observe that E A SE also performs better than baseline under all temperature settings, further justify its robustness across different settings.\nEffect of the Number of Generated Explanations N In Figure 7, we examine the influence of the number of explanations. On both datasets, increasing the explanations generally improves the performance, while E A SE achieves better performance than the baselines using only 30% - 40% of the generated explanations, which can reduce the burden of sampling massive explanations while maintaining the performance.\n\nAs E A SE relies on several key components such as prompts and sampling steps, in this section, we study their effect on the final prediction performance, using PaLM 2-S as the backbone model.\n\n<div style=\"text-align: center;\">(b) OpenbookQA\n</div>\n<div style=\"text-align: center;\">(c) StrategyQA\n</div>\n<div style=\"text-align: center;\">(a) E-SNLI\n(b) StrategyQA\nFigure 8: Effect of number of demonstrations.\n</div>\n<div style=\"text-align: center;\">(a) E-SNLI\n</div>\n<div style=\"text-align: center;\">(a) E-SNLI\n(b) StrategyQA\nFigure 8: Effect of number of demonstrations.\n</div>\n<div style=\"text-align: center;\">Figure 8: Effect of number of demonstrations.\n</div>\nEffect of the Size of demonstrations K Figure 8 illustrates the performance with different size of demonstrations. By increasing the number of demonstration K, the performance gradually increases, while E A SE achieves performance gains under all value of K.\n\n# 5 C ONCLUSION AND D ISCUSSION\n\nIn this work, we empower LLM\u2019s in-context learning ability with natural language explanations. Specifically, we design explanation-aware ensemble to weight multiple predictions using their associated explanations and realize this idea using a bootstrapped LLM scorer. In addition, we leverage a soft probability aggregation scheme to mitigate the issue of inconsistent predictions for ensembling. We conduct extensive experiments on seven datasets from a diverse task set and show our proposed framework can outperform previous state-of-the-art methods using four LLMs as backbones.\nNotably, while E A SE augments in-context learning by weighting predictions through explanations, it does not refine the explanation\u2019s content. For future works, it is potential to leverage techniques such as self-refinement (Madaan et al., 2023; Ling et al., 2023) and debating (Du et al., 2023) to elevate explanation quality and strengthen the model\u2019s reasoning abilities.\n\n# L IMITATIONS\n\nIn this work, our primary goal is to identify the existing issues to better leverage explanations to empower in-context learning. While our approach has shown promise, it also comes with increased computational demands, as both explanation-aware ensemble and soft probability aggregation steps require additional computation overhead. Future work could explore designing more powerful prompts to let LLMs directly output the suffix tokens as quality score (Tian et al., 2023). Additionally, our methodology depends on the logits returned in both the explanation-aware ensemble and soft probability aggregation processes, making it less suitable to directly adopted black-box LLMs (e.g. ChatGPT, OpenAI (2023)). To approximate the soft score, one strategy is to set the temperature to non-zero value and conduct multiple sampling steps, then use the frequency of the corresponding verbalizers as the proxy of the score.\nBesides, the key assumption of E A SE is that different explanations are of diverse quality, while those explanation leads to correct predictions tend to be of higher quality. We mainly conduct empirical experiments to support this point, yet there often exists multiple facets to evaluate the quality of free-text explantions (Chen et al., 2023a; c; Sun et al., 2022). More in-depth metrics are needed to faithfully evaluate the quality of free-text explanations and reveal the true inner workings of E A SE.\n\n# R EFERENCES\n\nShourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for commonsenseqa: New dataset and models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3050\u20133065, 2021.\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\n\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93\u2013104, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Conference on Empirical Methods in Natural Language Processing, pp. 632\u2013642, 2015.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nOana-Maria Camburu, Tim Rockt\u00a8aschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations.  Advances in Neural Information Processing Systems, 31, 2018.\nHanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, and Swabha Swayamdipta. REV: Information-theoretic evaluation of free-text rationales. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2007\u2013 2030, Toronto, Canada, July 2023a.\nJifan Chen, Eunsol Choi, and Greg Durrett. Can nli models verify qa systems\u2019 predictions? In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 3841\u20133854, 2021.\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023b.\nYanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves? counterfactual simulatability of natural language explanations. arXiv preprint arXiv:2307.08678, 2023c.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. ERASER: A benchmark to evaluate rationalized NLP models. In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, Online, July 2020.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013361, 2021.\nBrihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, and Xiang Ren. Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7103\u20137128, Toronto, Canada, July 2023.\nSungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo. Aligning large language models through synthetic feedback. arXiv preprint arXiv:2305.13735, 2023.\nAndrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537\u2013563, Abu Dhabi, United Arab Emirates, December 2022.\n\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12286\u201312312, Toronto, Canada, July 2023a. Association for Computational Linguistics.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013 5333, Toronto, Canada, July 2023b. Association for Computational Linguistics.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, 2022.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\nJosh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Explanation-based finetuning makes models more robust to spurious cues. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4420\u20134441, Toronto, Canada, July 2023. Association for Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nAna Marasovic, Iz Beltagy, Doug Downey, and Matthew Peters. Few-shot self-rationalization with natural language prompts. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 410\u2013424, Seattle, United States, July 2022. Association for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2381\u20132391, 2018.\nSharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546, 2020.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4885\u20134901, 2020.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\nOpenAI. Gpt-4 technical report, 2023.\nBhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. An information bottleneck approach for controlling conciseness in rationale extraction. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1938\u20131952, Online, November 2020.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023.\n\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12286\u201312312, Toronto, Canada, July 2023a. Association for Computational Linguistics.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013 5333, Toronto, Canada, July 2023b. Association for Computational Linguistics.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, 2022.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\nJosh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Explanation-based finetuning makes models more robust to spurious cues. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4420\u20134441, Toronto, Canada, July 2023. Association for Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nAna Marasovic, Iz Beltagy, Doug Downey, and Matthew Peters. Few-shot self-rationalization with natural language prompts. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 410\u2013424, Seattle, United States, July 2022. Association for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2381\u20132391, 2018.\nSharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546, 2020.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4885\u20134901, 2020.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\nOpenAI. Gpt-4 technical report, 2023.\nBhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. An information bottleneck approach for controlling conciseness in rationale extraction. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1938\u20131952, Online, November 2020.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023.\n\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4932\u20134942, Florence, Italy, July 2019. Association for Computational Linguistics.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. In  Proceedings of the 40th International Conference on Machine Learning, pp. 30706\u201330775. PMLR, 2023.\nJiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. Investigating the benefits of freeform rationales. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5867\u20135882, Abu Dhabi, United Arab Emirates, December 2022.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2023.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nPeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. PINTO: Faithful language reasoning using prompt-generated rationales. In The Eleventh International Conference on Learning Representations, 2023a.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023c.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022b.\nJerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, et al. Symbol tuning improves in-context learning in language models. arXiv preprint arXiv:2305.08298, 2023.\nSarah Wiegreffe, Ana Marasovi\u00b4c, and Noah A. Smith. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10266\u201310284, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4932\u20134942, Florence, Italy, July 2019. Association for Computational Linguistics.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. In  Proceedings of the 40th International Conference on Machine Learning, pp. 30706\u201330775. PMLR, 2023.\nJiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. Investigating the benefits of freeform rationales. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5867\u20135882, Abu Dhabi, United Arab Emirates, December 2022.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2023.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nPeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. PINTO: Faithful language reasoning using prompt-generated rationales. In The Eleventh International Conference on Learning Representations, 2023a.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023c.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022b.\nJerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, et al. Symbol tuning improves in-context learning in language models. arXiv preprint arXiv:2305.08298, 2023.\nSarah Wiegreffe, Ana Marasovi\u00b4c, and Noah A. Smith. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10266\u201310284, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Reframing human-AI collaboration for generating free-text explanations. In  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 632\u2013658, July 2022.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483\u2013498, Online, June 2021. Association for Computational Linguistics.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning. Advances in neural information processing systems, 35:30378\u201330392, 2022.\nXi Ye and Greg Durrett. Explanation selection using unlabeled data for chain-of-thought prompting, 2023.\nKayo Yin and Graham Neubig. Interpreting language models with contrastive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 184\u2013198, 2022.\nYue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, and Chao Zhang. Cold-start data selection for few-shot language model fine-tuning: A prompt-based uncertainty propagation approach. arXiv preprint arXiv:2209.06995, 2022.\nYue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed training data generator: A tale of diversity and bias. arXiv preprint arXiv:2306.15895, 2023.\nOmar F Zaidan, Jason Eisner, and Christine D Piatko. Using \u201cannotator rationales\u201d to improve machine learning for text categorization. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 260\u2013267, 2007.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.\nYe Zhang, Iain Marshall, and Byron C Wallace. Rationale-augmented convolutional neural networks for text classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 795\u2013804, 2016.\nWangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. Towards interpretable natural language understanding with explanations as latent variables. Advances in Neural Information Processing Systems, 33:6803\u20136814, 2020.\nYangqiaoyu Zhou, Yiming Zhang, and Chenhao Tan. FLamE: Few-shot learning from natural language explanations. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6743\u20136763, Toronto, Canada, July 2023.\n\nThe seven benchmarks in our experiments are all publicly available. Below are the links to downloadable versions of these datasets.\n\u2022 E-SNLI: https://huggingface.co/datasets/esnli;\n\u2022 ANLI R1/R2/R3: https://github.com/facebookresearch/anli;\n\u2022 ECQA: https://github.com/allenai/feb;\n\u2022 OpenbookQA: https://huggingface.co/datasets/openbookqa;\n\u2022 StrategyQA: for StrategyQA we use the question-only set from the link\nhttps://github.com/google/BIG-bench/blob/main/bigbench/ benchmark_tasks/strategyqa\n\nBy default, we sample few-shot demonstrations from the train set and sample from the test split for all datasets. For OpenbookQA, as the original dataset only contains 500 test examples, in each split we use 100 examples. For ANLI, as some of the examples contain no explanations, while the explanations for some examples include task-irrelevant information such as \u2018 I think the computer was confused because so many of the words were similar to the description \u2019. To reduce the effect of such examples, we remove those examples occurs with term \u2018 the system \u2019, \u2018 the computer \u2019, \u2018 the model \u2019, \u2018 the AI \u2019, and manually checked all the few-shot demonstrations to ensure that there is no such information in explanations.\n\n# B P ROMPT F ORMATS\n\nIn this section, we list the prompts used in our experiments.\n\n# B.1 P ROMPT F ORMAT F OR I N CONTEXT L EAR\n\nIn this step, we list the prompt for generating the explanations and predictions. Many of the prompt formats are adapted from (Bach et al., 2022). Note that the blue text is instance-dependent, while the red text is the model\u2019s expected output.\n\nB.1.1 E-SNLI\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/82e0/82e03a68-2e4f-4c53-bb6f-173f8441bdf0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3e2/b3e2da3b-e09b-4188-8632-29b87189f36d.png\" style=\"width: 50%;\"></div>\nB.1.2 ANLI\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55d9/55d99757-42cb-4c83-9d9b-4e74bf17cbb5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b91d/b91dfa03-2163-4dd5-a0ba-567574b9bfaa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8e4e/8e4eeaba-15a4-46d6-82ed-9eb7a2039f37.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e99e/e99e6a7b-b038-4328-835f-495c9403eb61.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">B.1.4 S TRATEGY QA\n</div>\nListing 10: Prompt format for StrategyQA, standard in-context learning.\nIn this task, given a question, you need to answer True or False.\n# demonstrations (no more than 48)\nFor the question: \u2019[question]\u2019, do you think it is the True or\nFalse?\nAnswer: [Answer]\n# test examples\nFor the question: \u2019[question]\u2019, do you think it is the True or\nFalse?\nAnswer: [Answer]\nListing 11: Prompt format for StrategyQA, using predict-then-explain pipeline.\nIn this task, given a question, you need to answer True or False.\n# demonstrations (no more than 48)\nFor the question: \u2019[question]\u2019, do you think it is the True or\nFalse?\nAnswer: [Answer]\nExplanation: [Explanation]\n# test examples\nFor the question: \u2019[question]\u2019, do you think it is the True or\nFalse?\nAnswer: [Answer]\nExplanation:\n[Explanation]\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/527f/527f042f-e883-4d92-9096-f20a7e6a16c7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7323/73233cce-066b-455d-af46-f3a951fa1927.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/38f3/38f36d3c-a1b8-43e5-a743-56e0910a269c.png\" style=\"width: 50%;\"></div>\nListing 13: Prompt format for LLM Scoring. Note that we use the probability of the \u2018Answer\u2019 token\nas the proxy for the quality score.\nIn this task, you will be given the input for the [task_name] task\n, your job is to determine whether the explanation provided is a\ngood one for the given input. Please consider the explanation\u2019s\ncoherence, informativeness, and consistency with the prediction to\nevaluate its quality.\n# demonstrations (no more than 48)\nFor \u2019[task input]\u2019, can you determine whether the explanation is a\ngood one for the given [task]?\nExplanation: [Explanation]\nAnswer: [Answer] [Yes or No]\n# test examples\nFor \u2019[task input]\u2019, can you determine whether the explanation is a\ngood one for the given [task]?\nExplanation: [Explanation]\nAnswer: [Answer]\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd23/fd23daaa-7052-451b-9be8-7112231ee4e7.png\" style=\"width: 50%;\"></div>\nListing 15: Prompt Format 3 for E-SNLI dataset\nIn this task, given a premise and a hypothesis, your job is to\ndetermine whether the hypothesis can be inferred from the premise.\n# demonstrations (no more than 48)\nBased on the premise [premise], can we conclude the hypothesis\nthat [hypothesis]? Choose among Yes, Maybe, and No.\nAnswer: [Answer]\nExplanation: [Explanation]\n# test examples\nBased on the premise [premise], can we conclude the hypothesis\nthat [hypothesis]? Choose among Yes, Maybe, and No.\nExplanation: [Explanation]\nAnswer:\n[Answer]\nListing 16: Prompt format 2 for StrategyQA, using explain-then-predict pipeline.\nIn this task, given a question, you need to answer True or False.\n# demonstrations (no more than 48)\nAnswer the question: \u2019[question]\u2019, by True or False.\nExplanation: [Explanation]\nAnswer: [Answer]\n# test examples\nAnswer the question: \u2019[question]\u2019, by True or False.\nExplanation: [Explanation]\nAnswer:\n[Answer]\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7cc8/7cc809c4-f9ae-48fd-b33a-05b2e757147e.png\" style=\"width: 50%;\"></div>\n# C H UMAN E VALUATION\n\n<div style=\"text-align: center;\">Listing 18: Human Evaluation Guideline for E-SNLI dataset.\n</div>\nFor this explanation grading task, given the task input (e.g. the\npremise and hypothesis for the NLI task and the question for the\nQA task), ground-truth answer, as well as a pair of explanations\nfrom the LLM, you job is to determine which explantion will reach\nthe ground-truth answer for that input.\nFor the E-SNLI dataset, your task is to predict if the hypothesis\nis entailed/neutral/contradicts the premise.\nfrom the LLM, you job is to determine which explantion will reach\nthe ground-truth answer for that input.\nFor the E-SNLI dataset, your task is to predict if the hypothesis\nis entailed/neutral/contradicts the premise.\nListing 19: Human Evaluation Guideline for StrategyQA dataset.\nFor this explanation grading task, given the task input (e.g. the\npremise and hypothesis for the NLI task and the question for the\nQA task), ground-truth answer, as well as a pair of explanations\nfrom the LLM, you job is to determine which explantion will reach\nthe ground-truth answer for that input.\nFor the strategyQA dataset, your task is to answer the question\nwith \u2019True\u2019 or \u2019False\u2019.\nFor this explanation grading task, given the task input (e.g. the\npremise and hypothesis for the NLI task and the question for the\nQA task), ground-truth answer, as well as a pair of explanations\nfrom the LLM, you job is to determine which explantion will reach\nthe ground-truth answer for that input.\nFor the strategyQA dataset, your task is to answer the question\nwith \u2019True\u2019 or \u2019False\u2019.\n# D S TUDIES ON V ERBALIZERS FOR B OOTSTRAPPED LLM S CORER\n\nWe investigate the role of verbalizers for representing the \u201cpositive\u201d and \u201cnegative\u201d explanations. We consider three set of verbalizers, namely V1:\u201cYes\u201d and \u201cNo\u201d, V2: \u201cTrue\u201d and \u201cFalse\u201d, and V3: \u201cFoo\u201d and \u201cJaa\u201d using symbolic tuning (Wei et al., 2023). Using PaLM 2-S as the backbone, we observe that the original \u201cYes\u201d and \u201cNo\u201d generally perform better. Symbolic tuning does not work as well as other verbalizers with concrete semantics, indicating it may not be strong enough for the explanation scoring task.\nTable 5: Verbalizer Study for Bootstrapped LLM Scorer, using PaLM 2-S as the backbone.\n\n<div style=\"text-align: center;\">Table 5: Verbalizer Study for Bootstrapped LLM Scorer, using PaLM 2-S as the backbone.\n</div>\nTemplate\nV1\nV2\nV3\nE-SNLI\n75.01\n73.75\n74.12\nStrategyQA\n78.40\n78.23\n76.75\n<div style=\"text-align: center;\">Table 6: Case study I for explanation-aware ensemble on E-SNLI Dataset.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/30c2/30c2af42-4c30-43ef-9d07-bc29613f8d1a.png\" style=\"width: 50%;\"></div>\nPremise: A man is working on a computer while two people sit and talk in front.\nHypothesis: The two people sat and chatted for a bit while the IT guy removed the virus.\nGround-truth Label: Neutral,\nMajority Voting Prediction: Entail.\nExplanation 1: The two people cannot be sitting and talking in front while the IT guy removes the virus.\nPrediction 1: Contradict\nScore 1: 0.468\nExplanation 2: There is no proof that there is an IT Person removed the virus.\nPrediction 2: Neutral\nScore 2: 0.562\nExplanation 3: The two people were sitting and talking.\nPrediction 3: Entail\nScore 3: 0.369\nExplanation 4: There is no evidence that the man is an IT guy, or that he is removing a virus.\nPrediction 4",
    "paper_type": "method",
    "attri": {
        "background": "Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. However, existing methods often treat different class predictions equally and neglect the potential discrepancies between explanations and predictions. To fully unleash the power of explanations, a new framework is proposed.",
        "problem": {
            "definition": "The problem addressed is how to effectively leverage natural language explanations to enhance the in-context learning capabilities of LLMs, specifically in the context of natural language understanding tasks.",
            "key obstacle": "The main challenge is the inconsistency between generated explanations and their corresponding predictions, which can lead to unreliable outcomes."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that human learning often involves explanations that enhance understanding and decision-making.",
            "opinion": "The proposed framework, EASE, integrates explanations into the ensemble process to improve prediction accuracy by focusing on the quality of explanations.",
            "innovation": "EASE introduces two key techniques: explanation-aware ensemble and soft probability aggregation, which differentiate it from existing approaches by addressing the quality of explanations and the noise in predictions."
        },
        "method": {
            "method name": "Explanation-Aware Soft Ensemble",
            "method abbreviation": "EASE",
            "method definition": "EASE is a framework designed to enhance the in-context learning of LLMs by incorporating natural language explanations into the prediction process.",
            "method description": "The core of EASE involves weighting predictions based on the quality of their associated explanations and aggregating probabilities to improve final outcomes.",
            "method steps": [
                "Generate multiple candidate explanations and predictions for each test instance.",
                "Evaluate the quality of each explanation using a bootstrapped LLM scorer.",
                "Weight the predictions based on their associated explanation scores.",
                "Aggregate the weighted predictions using soft probability aggregation."
            ],
            "principle": "EASE is effective because it uses the quality of explanations to guide the prediction process, allowing the model to focus on more reliable outputs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on seven natural language understanding tasks using four different sizes of LLMs. The datasets included E-SNLI, ANLI-R1/R2/R3, ECQA, OpenbookQA, and StrategyQA.",
            "evaluation method": "Performance was measured by comparing EASE against baseline methods using various metrics across the datasets, focusing on accuracy improvements."
        },
        "conclusion": "The EASE framework demonstrates significant improvements in leveraging explanations for in-context learning in LLMs, outperforming previous state-of-the-art methods across multiple tasks.",
        "discussion": {
            "advantage": "The key advantages of EASE include its ability to enhance prediction accuracy by utilizing explanation quality and reducing inconsistencies in predictions.",
            "limitation": "The method introduces additional computational overhead due to the explanation-aware ensemble and soft probability aggregation processes.",
            "future work": "Future research could explore refining the quality of explanations and developing more efficient computational strategies for the EASE framework."
        },
        "other info": {
            "references": [
                "Brown et al., 2020; Tay et al., 2023; Chowdhery et al., 2022; Anil et al., 2023; Touvron et al., 2023; OpenAI, 2023.",
                "Lampinen et al., 2022; Nye et al., 2021; Wei et al., 2022b; Wang et al., 2023c."
            ],
            "datasets": [
                {
                    "name": "E-SNLI",
                    "link": "https://huggingface.co/datasets/esnli"
                },
                {
                    "name": "ANLI R1/R2/R3",
                    "link": "https://github.com/facebookresearch/anli"
                },
                {
                    "name": "ECQA",
                    "link": "https://github.com/allenai/feb"
                },
                {
                    "name": "OpenbookQA",
                    "link": "https://huggingface.co/datasets/openbookqa"
                },
                {
                    "name": "StrategyQA",
                    "link": "https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/strategyqa"
                }
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning enhances the capabilities of large language models (LLMs) by allowing them to adapt quickly to target tasks with minimal demonstration examples."
        },
        {
            "section number": "1.3",
            "key information": "Large language models have shown remarkable capabilities in various natural language understanding tasks, enabling them to leverage explanations to enhance in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "The Explanation-Aware Soft Ensemble (EASE) framework improves robustness in in-context learning by integrating natural language explanations into the prediction process."
        },
        {
            "section number": "3.2",
            "key information": "EASE introduces techniques such as explanation-aware ensemble and soft probability aggregation, which enhance the quality of predictions by focusing on the reliability of explanations."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts can significantly influence the outcomes of in-context learning, particularly when explanations are integrated into the ensemble process."
        },
        {
            "section number": "5.2",
            "key information": "EASE has been applied to various natural language understanding tasks, demonstrating its effectiveness in question answering and information extraction."
        },
        {
            "section number": "6.2",
            "key information": "The EASE framework introduces additional computational overhead due to the explanation-aware ensemble and soft probability aggregation processes."
        },
        {
            "section number": "7",
            "key information": "The EASE framework demonstrates significant improvements in leveraging explanations for in-context learning in LLMs, outperforming previous state-of-the-art methods across multiple tasks."
        }
    ],
    "similarity_score": 0.710282723356643,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning.json"
}