{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1705.05264",
    "title": "Extending Defensive Distillation",
    "abstract": "Machine learning is vulnerable to adversarial examples: inputs carefully modified to force misclassification. Designing defenses against such inputs remains largely an open problem. In this work, we revisit defensive distillation\u2014which is one of the mechanisms proposed to mitigate adversarial examples\u2014to address its limitations. We view our results not only as an effective way of addressing some of the recently discovered attacks but also as reinforcing the importance of improved training techniques.",
    "bib_name": "papernot2017extendingdefensivedistillation",
    "md_text": "# Extending Defensive Distillation\nNicolas Papernot and Patrick McDaniel Pennsylvania State University {ngp5056,mcdaniel}@cse.psu.edu\n# Abstract\nMachine learning is vulnerable to adversarial examples: inputs carefully modified to force misclassification. Designing defenses against such inputs remains largely an open problem. In this work, we revisit defensive distillation\u2014which is one of the mechanisms proposed to mitigate adversarial examples\u2014to address its limitations. We view our results not only as an effective way of addressing some of the recently discovered attacks but also as reinforcing the importance of improved training techniques.\n# 1 Introduction\nDeployed machine learning (ML) models are vulnerable to inputs maliciously perturbed to force them to mispredict [1, 2]. A class of such inputs, named adversarial examples, are systematically constructed through slight perturbations of otherwise correctly classified inputs [3, 4]. These perturbations are chosen to maximize the model\u2019s prediction error while leaving the semantics of the input unchanged. Although this often poses a non-tractable optimization problem for popular architectures like deep neural networks, heuristics allow the adversary to find effective perturbations\u2014typically through the evaluation of gradients of the model\u2019s output with respect to its inputs [3, 5]. To defend against adversarial examples, two classes of approaches exist. The first algorithmically improves upon the learning to make the model inherently more robust: techniques that fall in this class include adversarial training [3, 5] or defensive distillation [6]. The second is a set of detection mechanisms used to reject inputs suspected to be malicious. One approach is to analyze internal model features for anomalies [7, 8]. Another approach chooses to add an outlier class to the set of existing outputs that make up the ML task, and then train the model to map adversarial examples to this special class [9, 10]. However, most\u2014if not all\u2014of these defenses fail to adapt to novel attack strategies. Defensive distillation, which is the primary mechanism discussed in the present document, is no exception. It is successful against attacks known at the time of writing, such as the Fast Gradient Sign Method (FGSM) [5] and the Jacobian-based Saliency Map Approach (JSMA) [11]. However, as advancements found new ways to mount attacks against ML, defensive distillation can now be evaded [12, 13].\nOne of the successful strategies is to mount a black-box attack, as shown in [12]: the adversary first trains an undefended surrogate model which mimicks the defended model, and then uses the surrogate model to generate adversarial examples that transfer back to the distilled model (i.e., the same inputs are misclassified by both the undefended and defended models). This attack strategy succeeds because of the often strong transferability of adversarial examples across models trained to solve the same ML task [3, 5, 14] In addition, distillation can be evaded with optimization attacks `a la Szegedy et al. [3], which have been revisited in [13] with new objectives and optimizers. This second attack strategy is successful against defensive distillation because of a phenomenon called gradient masking [12]: the defense mechanism destroys gradients essential to the heuristics of attacks like the FGSM and JSMA\u2014 instead of reducing the model\u2019s error. Note that defensive distillation is not the only mechanism that yields to gradient masking [15] Given the two failure modes identified above, we propose in this work a variant of defensive distillation that addresses them. We demonstrate that our approach is less susceptible to transferability and gradient masking by mounting black-box attacks from both undefended and defended surrogate models. Like the original defensive distillation, the technique does not require that the defender generate adversarial examples. Its applicability is thus less likely to be limited to specific adversarial example heuristics. Unfortunately, it is currently infeasible to formally prove robustness guarantees for models like deep neural networks. Techniques for the verification of these models are also still in their infancy [16, 17]. We thus resort to experimental validation of our approach and leave a formal analysis to future work.\n# Defensive distillation\nAdapted from [18], defensive distillation successively trains two instances of the same deep neural network architecture.1 The first model is trained with the original dataset {(x, y)} where y indicates the expected correct class for the input x. Learning is performed conventionally to the exception of the softmax2 temperature, which is usually set to 1 and is here increased. In the interest of space, we refer readers to the detailed presentation in [6]. Briefly, the model outputs probabilities that are closer to a uniform distribution at high softmax temperatures. This first model f is then used to label each training point x with its probability vector (roughly indicating how likely f believes the input x to be in each class of the task). This defines a newly labeled training set {(x, f(x))}, which the second model f d is trained on. When deployed with a temperature of 1, the model f d is found to be robust to the FGSM and JSMA attacks.\n1As noted in [6], distillation is applicable to any model outputting an energy-based probability distribution for which a temperature can be defined. 2The term softmax refers here to the last layer of a neural network classifier. It transforms scores assigned by the model to each class of the task into probabilities.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ecee/ecee6111-dbe1-436c-b280-fd12815f8b0c.png\" style=\"width: 50%;\"></div>\nFigure 1: The extended defensive distillation procedure: (1) the first neural network is trained as usual on one-hot labels, (2) a new labeling vector is defined for each training point by combining the original label information together with the first model\u2019s predictive uncertainty\u2014which is measured by taking several stochastic passes through the model to infer the logits, (3) the distilled model is trained at temperature T \u22651 with the new label vectors.\n# Extending defensive distillation\nWe revisit the defensive distillation approach by modifying the labeling information used to train the distilled model f d. Our changes are motivated by two lines of previous work. The first augments the output of models with an outlier class as a means to mitigate adversarial examples [9, 10]: the model learns to classify adversarial inputs in the outlier class. The second provides uncertainty estimates in neural networks through stochastic inference [19, 20]. The variant of defensive distillation described below successively trains two models and transfers the knowledge of the first model f into the second. The mechanism enables the second (i.e., distilled) model f d to be uncertain about its prediction when the input it is asked to label is far from the training manifold. Uncertainty estimates required to train this distilled model are obtained through an analysis of the predictions made by the first of the two models.\nMeasuring uncertainty To capture the uncertainty of the first model in the labels created to train the second\u2014distilled\u2014model, we use dropout at inference. Dropout is a technique which randomly \u201cdrops\u201d (i.e., removes) some of the neurons in the neural network\u2019s architecture. It is typically applied during training as a regularizer. Instead, Gal and Ghahramani apply dropout while inferring, in order to approximate Bayesian inference [19]. We use here a similar process to quantify the predictive uncertainty of the first model f.\nWhile dropout is still activated, we take N forward passes through the neural network f and record the N logit vectors z0(x), ..., zN\u22121(x) predicted. Each logit vector zm(x) is n \u22121 dimensional, where n is the number of classes in the problem. The logits are scores assigned to each class, before they are transformed into a probability through the application of a softmax at the output of the network. For instance, zm j (x) is the score assigned by model f to class j in the m-th forward pass on input x with dropout. We then compute the mean logit vector \u00afz(x) and the variance over the N stochastic passes to measure uncertainty at the input point x considered:\n\uf8ed \ufffd \ufffd \uf8f8 Unlike what was proposed in the original defensive distillation mechanism, the softmax temperature of the first model f is maintained at T = 1 at all times. Training the distilled model with uncertainty The output of the distilled model is augmented with an outlier class, which is separate from the existing n \u201creal\u201d classes of the problem. When training the distilled model f d, each training point is relabeled with a vector that indicates (1) the correct class of the input and (2) the uncertainty \u03c3(x) of the first model f. The components j \u22080..n of this new labeling vector k(x) are defined by:\n\uf8ed \uf8f8 Unlike what was proposed in the original defensive distillation mechanism the softmax temperature of the first model f is maintained at T = 1 at all times\nTraining the distilled model with uncertainty The output of the distilled model is augmented with an outlier class, which is separate from the existing n \u201creal\u201d classes of the problem. When training the distilled model f d, each training point is relabeled with a vector that indicates (1) the correct class of the input and (2) the uncertainty \u03c3(x) of the first model f. The components j \u22080..n of this new labeling vector k(x) are defined by:\n\uf8f4 \uf8f3 Informally, the uncertainty defines the probability assigned to the outlier class, while the remaining probability is assigned exclusively to the correct class. The parameter \u03b1 weights the importance given to the uncertainty measure. When the first model f was uncertain on x, the measure \u03c3(x) is large and kn(x) as well. Hence, the distilled model is more likely to classify uncertain inputs as outliers. As a consequence, if \u03b1 is set to a too large value, all inputs are classified as outliers. The distilled model f d trained with labeling vector k(x) learns how to correctly classify x as long as kl(x) > kn(x) where l is the index of the correct label for x and n the index of the outlier class. Training with an outlier class is expected to increase the model\u2019s robustness to finite perturbations (e.g., FGM and JSMA adversarial examples), as observed by previous efforts including [9, 10]. We now make one last modification to the training procedure in order to address smaller perturbations (such as the ones produced by the AdaDelta strategy). Specifically, we augment the training loss of our distilled model to include the following penalty:\n(1)\n(2)\n(3)\nwhere l is the correct class, \u03b3 weights the relative contribution of the penalty in the training loss and \u03ba prevents unnecessary extrema during optimization. Finally, we note that unlike in the original defensive distillation mechanism, we maintain the temperature of the distilled model at all times equal to T = 1.\n# 4 Evaluation\nWe evaluate the robustness of the distilled model f d in the face of attacks mounted both in white-box and black-box threat models. In addition to corresponding to a more realistic threat model, robustness to black-box attacks is a compelling indicator of the absence of gradient masking. Below, the strengths and weaknesses of the approach are characterized by four rates:\n\u2022 Accuracy: percentage of legitimate inputs correctly classified. \u2022 False positives: percentage of legitimate inputs classified as outliers \u2022 Recovered: percentage of adversarial inputs whose class was recovered (i.e., they are assigned the label of the originally unperturbed input). \u2022 Detected: percentage of adversarial examples classified in the outlier class..\n\u2022 Ideally, the distilled model\u2019s accuracy should be comparable to the one of an undefended model. Combined, the recovered and detected adversarial examples should leave as few misclassified adversarial examples as possible. Finally, the false positive rate should be as low as possible, otherwise the model becomes unusable when presented with legitimate inputs. When evaluating attacks, we also report the mean perturbation of adversarial examples that were successful for the adversary (i.e., neither recovered or detected). This allows us to better visualize the effectiveness of the defense, because it exposes the trade-off between indistinguishibility of adversarial examples and their likelihood of being misclassified.\n# 4.1 Experimental setup\nWe experiment with MNIST [21] using the convolutional neural network provided in the cleverhans v.1 library tutorials. Dropout layers are inserted before the input layer and after the last convolution. Hence, dropout is applied to the feature representation extracted by the convolutional layers, before the fully connected layer essential to classification. The probability of dropping a neuron is set to 0.2 before the input layer and 0.5 after the convolution. We consider three attack strategies: the fast gradient method (FGM) [5], the Jacobian Saliency Map Approach (JSMA) [11] and the AdaDelta optimization strategy (AdaDelta) [13]. For the JSMA and AdaDelta attacks, we randomly select the target class among the set of \u201creal\u201d classes (i.e., we omit the outlier class since it is not a profitable output for adversaries). Adversarial examples are clipped to ensure their components are within the range of acceptable values. In our evaluation, the JSMA and AdaDelta attacks are ran on less test inputs than the FGM\u2014they are computationally expensive. Thus, the variability of the results for these two attacks is larger than for results relative to the FGM. Our source code will be open-sourced through cleverhans.\n# .2 Robustness to white-box attacks\nIn the white-box setting, the adversary has access to the defended model\u2019s architecture and parameters: the attack algorithms are directly applied to the model that is targeted (i.e., the distilled model). The distilled model was trained for 10 epochs with N = 20 dropout passes, a variance coefficient \u03b1 = 0.9, a loss penalty weight \u03b3 = 5 \u00b7 10\u22124, and a loss penalty constant \u03ba = 40. This choice of hyper-parameters is justified below, except for \u03b3 and \u03ba: they were chosen with a grid search. The model accuracy on legitimate test inputs is 97.28%, compared to a 98.41% accuracy for the same architecture trained without defensive distillation for 10 epochs. The distilled model\u2019s false positive remains below 1%.\nRobustness We run the attacks mentioned in Section 4.1 with different parameters to evaluate the percentage of misclassified adversarial examples (i.e., those that are neither recovered or detected) with respect to their mean perturbation. Figure 2a, 2b and 2c illustrate the results respectively for the FGM, JSMA and AdaDelta attacks. We break down correctly classified adversarial examples as recovered or detected, but stack the two rates to better visualize the inputs that were correctly processed by the model. The defended model is robust in a certain neighborhood of the test points. In comparison, the undefended model has a misclassification rate of 90.8%, 92.2% and 96.0% on the FGM, JSMA and AdaDelta attacks respectively. As the attacks increase the perturbation introduced, the model is less and less able to cope with adversarial input. For the FGM and JSMA attacks, the mechanism is able to correctly classify a large fraction of adversarial examples produced with a perturbation norm less than 4 according to the \u21132 norm. For the AdaDelta strategy, there remains an evasion window for the adversary using 2 \u2264\u03ba \u22642.5. A visual inspection reveals that when the \u21132 norm of adversarial examples is larger than 5, the perturbations begin to modify the correct class (e.g., the digit corresponding to the wrong class predicted by the model appears transparently in the image or the digit corresponding to the correct class is partially erased). Parameter exploration The fraction of adversarial examples whose correct class is recovered or detected increases with the value of \u03b1, as illustrated in Figure 2d. Indeed, larger values of \u03b1 assign a greater probability to the outlier class when creating the labeling vector k(x) with which the distilled model is trained. However for \u03b1 \u22651.0, this comes at the expense of a lower accuracy on legitimate inputs: there are more false positives\u2014because legitimate inputs are increasingly likely to be labeled by the model as outliers. As more forward passes with dropout are taken to estimate the uncertainty of the first model, the distilled model is able to detect and recover more adversarial examples. Yet, measurements depicted in Figure 2e show that there is a number of passes (\u224320) after which the curves exhibit a plateau. The approximation performed when taking successive stochastic passes eventually converges as the number of passes increases.\nRobustness We run the attacks mentioned in Section 4.1 with different parameters to evaluate the percentage of misclassified adversarial examples (i.e., those that are neither recovered or detected) with respect to their mean perturbation. Figure 2a, 2b and 2c illustrate the results respectively for the FGM, JSMA and AdaDelta attacks. We break down correctly classified adversarial examples as recovered or detected, but stack the two rates to better visualize the inputs that were correctly processed by the model. The defended model is robust in a certain neighborhood of the test points. In comparison, the undefended model has a misclassification rate of 90.8%, 92.2% and 96.0% on the FGM, JSMA and AdaDelta attacks respectively. As the attacks increase the perturbation introduced, the model is less and less able to cope with adversarial input. For the FGM and JSMA attacks, the mechanism is able to correctly classify a large fraction of adversarial examples produced with a perturbation norm less than 4 according to the \u21132 norm. For the AdaDelta strategy, there remains an evasion window for the adversary using 2 \u2264\u03ba \u22642.5. A visual inspection reveals that when the \u21132 norm of adversarial examples is larger than 5, the perturbations begin to modify the correct class (e.g., the digit corresponding to the wrong class predicted by the model appears transparently in the image or the digit corresponding to the correct class is partially erased).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0db7/0db77f51-c5aa-4db1-9b45-b6ffc1e83e17.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Robustness to AdaDelta</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a6ac/a6ac1bef-78a3-4c55-ad7d-0f6a1efa68da.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Exploration of parameter \u03b1.</div>\n<div style=\"text-align: center;\">Figure 2: Evaluation of defensive distillation in the white-box setting. Figure 2a, 2b and 2c report the recovered and detected rates vs. the mean perturbation of successful adversarial example produced with the three attacks from Section 4.1. For clarity of presentation, the parameter exploration performed in Figures 2d and 2e is plotted with the FGM only. 7</div>\nFigure 2: Evaluation of defensive distillation in the white-box setting. Figure 2a, 2b and 2c report the recovered and detected rates vs. the mean perturbation of successful adversarial example produced with the three attacks from Section 4.1. For clarity of presentation, the parameter exploration performed in Figures 2d and 2e is plotted with the FGM only. 7\n# 4.3 Robustness to black-box attacks\nWe mount worst-case black-box attacks through transferability from a surrogate model trained using the same architecture and data. For each attack, we consider two types of surrogate models: undefended ones (denoted by U) and ones (denoted by D) defended with the mechanism introduced in Section 3. This allows us to test for gradient masking (which would be indicated by strong transferability of adversarial examples from the surrogates to the distilled model). Our results, reported in Figure 3, are comparable with those obtained in the white-box setting (see Figure 2). Black-box adversarial examples are somewhat more likely to be detected as outliers than their white-box counterparts. Yet, the misclassification rates of each attack are comparable in the white-box and black-box settings, regardless of the fact that the model was defended or not. This comforts the design choices made to prevent gradient masking.\n# 5 Discussion\nWe addressed the numerical instabilities encountered by the original defensive distillation mechanism. In lieu of extracting uncertainty estimates from the probability vectors at high temperature, we considered the logits produced by multiple stochastic passes through the neural network. Future work may build on the ideas of transferring knowledge and uncertainty between models in order to improve ML performance on other tasks in adversarial environments. The variant proposed defends models in a comparable capacity in the face of white-box and black-box attacks. This indicates that the defense is less likely to suffer from gradient masking. While its benefits recede as perturbations become larger, defensive distillation is able to improve the robustness of a neural network in a vicinity of its test data at a reasonable price in accuracy. Although the method is generic and applicable to any neural net and input domain, our evaluation remains preliminary. The MNIST dataset is nevertheless compelling because of the strong transferability across architectures, which makes defending against black-box attacks particularly hard. Our results stress that it is key to defend against two types of adversarial examples\u2014built with infinitesimal and finite perturbations. In the black-box setting, the latter can be more challenging because they are more \u201ctransferable\u201d. However, AdaDelta adversarial examples transfer less across architectures at \u03ba \u22432. Thus, our results on undefended black-box attacks are pessimistic in the sense that they assumed the adversary has perfect knowledge about the defended model\u2019s architecture. Defensive distillation\u2019s most appealing aspect remains that it does not require that the defender generate adversarial examples. This leaves room for another line of work combining defensive distillation with other defenses. For example, adversarial training could be combined with defensive distillation to learn a mapping between adversarial examples and the correct or outlier classes. Given that adversarial training is typically effective against finite perturbations (e.g., the FGM), it is likely to complement defensive distillation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e2b0/e2b07eb1-5734-4eff-ac7f-025975efacdc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Robustness to defended FGM</div>\n<div style=\"text-align: center;\">(c) Robustness to defended JSMA</div>\n<div style=\"text-align: center;\">Figure 3: Evaluation of defensive distillation in the black-box setting. In Figures 3a, 3c and 3e, we use a defended surrogate model trained with defensive distillation. Figures 3b, 3d and 3f use an undefended surrogate model. 9</div>\n<div style=\"text-align: center;\">(b) Robustness to undefended FGM.</div>\n<div style=\"text-align: center;\">(d) Robustness to undefended JSMA.</div>\nNicolas Papernot is supported by a Google PhD Fellowship in Security. We thank NVIDIA for the donation of a Titan X Pascal. Research was supported in part by the Army Research Laboratory, under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA), and the Army Research Office under grant W911NF-13-1-0421.\n# References\n[1] D. Lowd and C. Meek, \u201cAdversarial learning,\u201d in Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 641\u2013647. [2] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar, \u201cCan machine learning be secure?\u201d in Proceedings of the 2006 ACM Symposium on Information, computer and communications security. ACM, 2006, pp. 16\u201325. [3] C. Szegedy et al., \u201cIntriguing properties of neural networks,\u201d arXiv preprint arXiv:1312.6199, 2013. [4] B. Biggio et al., \u201cEvasion attacks against machine learning at test time,\u201d in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 387\u2013402. [5] I. J. Goodfellow et al., \u201cExplaining and harnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572, 2014. [6] N. Papernot et al., \u201cDistillation as a defense to adversarial perturbations against deep neural networks,\u201d in Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016, pp. 582\u2013597. [7] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, \u201cOn detecting adversarial perturbations,\u201d arXiv preprint arXiv:1702.04267, 2017. [8] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, \u201cDetecting adversarial samples from artifacts,\u201d arXiv preprint arXiv:1703.00410, 2017. [9] K. Grosse, P. Manoharan, N. Papernot, M. Backes, and P. McDaniel, \u201cOn the (statistical) detection of adversarial examples,\u201d arXiv preprint arXiv:1702.06280, 2017. 10] H. Hosseini et al., \u201cBlocking transferability of adversarial examples in black-box learning systems,\u201d arXiv preprint arXiv:1703.04318, 2017. 11] N. Papernot et al., \u201cThe limitations of deep learning in adversarial settings,\u201d in Security and Privacy (EuroS&P), 2016 IEEE European Symposium on. IEEE, 2016, pp. 372\u2013387.\n[12] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, \u201cPractical black-box attacks against machine learning,\u201d in Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017, pp. 506\u2013519. [13] N. Carlini et al., \u201cTowards evaluating the robustness of neural networks,\u201d arXiv preprint arXiv:1608.04644, 2016. [14] F. Tram`er, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, \u201cThe space of transferable adversarial examples,\u201d arXiv preprint arXiv:1704.03453, 2017. [15] W. Brendel and M. Bethge, \u201cComment on\u201d biologically inspired protection of deep networks from adversarial attacks\u201d,\u201d arXiv preprint arXiv:1704.01547, 2017. [16] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, \u201cSafety verification of deep neural networks,\u201d arXiv preprint arXiv:1610.06940, 2016. [17] G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer, \u201cReluplex: An efficient smt solver for verifying deep neural networks,\u201d arXiv preprint arXiv:1702.01135, 2017. [18] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015. [19] Y. Gal and Z. Ghahramani, \u201cDropout as a bayesian approximation: Representing model uncertainty in deep learning,\u201d arXiv preprint arXiv:1506.02142, vol. 2, 2015. [20] Y. Gal, \u201cUncertainty in deep learning,\u201d Ph.D. dissertation, PhD thesis, University of Cambridge, 2016. [21] Y. LeCun et al., \u201cThe mnist database of handwritten digits,\u201d 1998.\n",
    "paper_type": "method",
    "attri": {
        "background": "Machine learning models are vulnerable to adversarial examples, which are inputs modified to force misclassification. Defensive distillation is a proposed mechanism to mitigate such inputs, but it has limitations that need to be addressed to improve robustness against novel attack strategies.",
        "problem": {
            "definition": "The problem is the vulnerability of machine learning models to adversarial examples that can exploit model weaknesses, leading to incorrect classifications.",
            "key obstacle": "Existing methods, including defensive distillation, fail to adapt to new attack strategies, particularly those that leverage transferability and gradient masking."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to enhance the robustness of defensive distillation against evolving adversarial attacks by incorporating uncertainty estimates into the training process.",
            "opinion": "The proposed idea involves modifying the labeling information used in defensive distillation to include uncertainty measures, thus improving the model's ability to classify inputs that are far from the training data.",
            "innovation": "This method differs from existing approaches by maintaining uncertainty estimates during training and incorporating an outlier class, which enhances robustness against both white-box and black-box attacks."
        },
        "method": {
            "method name": "Extended Defensive Distillation",
            "method abbreviation": "EDD",
            "method definition": "EDD is a variant of defensive distillation that utilizes uncertainty estimates from a first model to enhance the training of a second distilled model, improving its robustness to adversarial examples.",
            "method description": "The method involves training two models sequentially, where the first model provides uncertainty information used to label inputs for the second model.",
            "method steps": [
                "Train the first model on the original dataset with modified softmax temperature.",
                "Use dropout during inference to capture uncertainty in predictions.",
                "Relabel training points for the second model using the uncertainty estimates.",
                "Train the second model using the new labeling vectors that include an outlier class."
            ],
            "principle": "The effectiveness of this method lies in its ability to incorporate uncertainty into the training process, allowing the distilled model to better handle inputs that deviate from the training distribution."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the MNIST dataset with a convolutional neural network, testing against three attack strategies: FGSM, JSMA, and AdaDelta.",
            "evaluation method": "Performance was assessed by measuring accuracy, false positives, recovery rates of adversarial inputs, and detection rates of adversarial examples classified as outliers."
        },
        "conclusion": "The extended defensive distillation method demonstrated improved robustness against both white-box and black-box attacks, effectively reducing misclassification rates while maintaining a reasonable accuracy on legitimate inputs.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its robustness against a wide range of adversarial attacks, particularly due to the incorporation of uncertainty measures and the use of an outlier class.",
            "limitation": "A limitation of the method is that its effectiveness diminishes as the perturbation size increases, which may lead to misclassifications in highly perturbed inputs.",
            "future work": "Future research should explore combining extended defensive distillation with other defense mechanisms, such as adversarial training, to further enhance robustness against adversarial examples."
        },
        "other info": [
            {
                "info1": "The method does not require the generation of adversarial examples by the defender, which simplifies its application."
            },
            {
                "info2": {
                    "info2.1": "Nicolas Papernot is supported by a Google PhD Fellowship in Security.",
                    "info2.2": "Research was partially supported by the Army Research Laboratory and the Army Research Office."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.4",
            "key information": "Machine learning models are vulnerable to adversarial examples, which are inputs modified to force misclassification."
        },
        {
            "section number": "3.4",
            "key information": "EDD is a variant of defensive distillation that utilizes uncertainty estimates from a first model to enhance the training of a second distilled model, improving its robustness to adversarial examples."
        },
        {
            "section number": "5.2",
            "key information": "The proposed approach is robust against a wide range of adversarial attacks, particularly due to the incorporation of uncertainty measures and the use of an outlier class."
        },
        {
            "section number": "6.1",
            "key information": "The idea is inspired by the need to enhance the robustness of defensive distillation against evolving adversarial attacks by incorporating uncertainty estimates into the training process."
        },
        {
            "section number": "7.1",
            "key information": "Existing methods, including defensive distillation, fail to adapt to new attack strategies, particularly those that leverage transferability and gradient masking."
        }
    ],
    "similarity_score": 0.6145076782986973,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Extending Defensive Distillation.json"
}