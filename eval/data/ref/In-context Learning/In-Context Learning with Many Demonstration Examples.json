{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2302.04931",
    "title": "In-Context Learning with Many Demonstration Examples",
    "abstract": "Large pre-training language models (PLMs) have shown promising in-context learning abilities. However, due to the backbone transformer architecture, existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored. In this study, we propose a long-range language model EVALM based on an efficient transformer mechanism. EVALM is trained with 8k tokens per batch line and can test up to 256k-lengthed contexts with extrapolation, 128\u00d7 to the limit of existing PLMs (e.g. GPT3). Based on EVALM, we scale up the size of examples efficiently in both instruction tuning and in-context learning to explore the boundary of the benefits from more annotated data. Experimental results on a diverse set of tasks show that EVALM achieves 4.1% higher accuracy on average, and the average length of achieving the best accuracy score over tasks is around 12k. We find that in-context learning can achieve higher performance with more demonstrations under many-shot instruction tuning (8k), and further extending the length of instructions (16k) can further improve the upper bound of scaling incontext learning. Code is available on https: //github.com/Shark-NLP/EVALM.",
    "bib_name": "li2023incontextlearningdemonstrationexamples",
    "md_text": "# In-Context Learning with Many Demonstration Examples\nMukai Li 1 Shansan Gong 1 Jiangtao Feng 1 Yiheng Xu 1 2 Jun Zhang 1 Zhiyong Wu 1 Lingpeng Kong 1 2\n# Abstract\nLarge pre-training language models (PLMs) have shown promising in-context learning abilities. However, due to the backbone transformer architecture, existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored. In this study, we propose a long-range language model EVALM based on an efficient transformer mechanism. EVALM is trained with 8k tokens per batch line and can test up to 256k-lengthed contexts with extrapolation, 128\u00d7 to the limit of existing PLMs (e.g. GPT3). Based on EVALM, we scale up the size of examples efficiently in both instruction tuning and in-context learning to explore the boundary of the benefits from more annotated data. Experimental results on a diverse set of tasks show that EVALM achieves 4.1% higher accuracy on average, and the average length of achieving the best accuracy score over tasks is around 12k. We find that in-context learning can achieve higher performance with more demonstrations under many-shot instruction tuning (8k), and further extending the length of instructions (16k) can further improve the upper bound of scaling incontext learning. Code is available on https: //github.com/Shark-NLP/EVALM.\narXiv:2302.04931v1\n# 1. Introduction\nWith the increasing scale of pre-trained language models (PLMs), in-context learning (ICL) has emerged as a novel paradigm for utilizing PLMs (Brown et al., 2020b; Zhang et al., 2022c; Chowdhery et al., 2022). Unlike learning\n1Shanghai Artificial Intelligence Laboratory 2Department of Computer Science,The University of HongKong. Correspondence to: Jiangtao Feng <fengjiangtao@pjlab.org.cn>, Lingpeng Kong <lpk@cs.hku.hk>.\nmethods that require updating parameters, in-context learning allows for good model performance with a prompt that only includes natural language instructions and/or a few demonstrations (Dong et al., 2023). In addition to that, a recent line of research on instruction tuning shed new light on closing the gap between pre-training and in-context learning (Chung et al., 2022; Min et al., 2022), facilitating the usage of natural language instructions to interact with the PLMs.\nmethods that require updating parameters, in-context learning allows for good model performance with a prompt that only includes natural language instructions and/or a few demonstrations (Dong et al., 2023). In addition to that, a recent line of research on instruction tuning shed new light on closing the gap between pre-training and in-context learning (Chung et al., 2022; Min et al., 2022), facilitating the usage of natural language instructions to interact with the PLMs. However, the computational overhead of the backbone vanilla transformer architecture prevents existing PLMs from a longer context. A maximum context size (i.e., 2048) is set in the most popular pre-training models (e.g., GPT3, Brown et al. 2020b; OPT, Zhang et al. 2022c; PaLM Chowdhery et al. 2022). The direct consequence is scaling up to large numbers of samples in instruction tuning or in-context learning becomes under-explored. How effectively can we improve the in-context learning performance of the PLMs by serving more demonstration examples? To answer this question, we start from responding to the challenge of long-range language models (LRLMs). We train an LRLM named EVALM (\u00a7 3.2), which backbones on a state-of-the-art efficient transformer architecture EVA (Zheng et al., 2023), with modifications to handle the extrapolation of position embeddings (\u00a7 3.1). EVALM with many-shot instruction tuning achieves better performance in long-range language modeling with cheap memory and computational costs (\u00a7 4.4). The learned circular position embedding and incremental encoding we propose help EVALM to extrapolate to an input length of 256k tokens effectively. We then conduct a series of experiments testing the performance of EVALM when scaling up the number of demonstration examples in ICL in various tasks. We find that with more demonstration examples, EVALM is able to achieve better ICL performance than comparable PLMs with rare extra overheads. We summarize our contribution as follows:\n# 2. Related Work\nPre-trained Language Model PLMs are trained on large and general corpora and then finetuned or few-shot transferred to perform various NLU and NLG tasks. Among them, besides encoder-decoder Transformer (Vaswani et al., 2017) architecture such as T5 (Raffel et al., 2020), there are auto-regressively pre-trained models, like XLNet (Yang et al., 2019), GPT (Radford et al., 2019; Brown et al., 2020a; Black et al., 2022), OPT (Zhang et al., 2022c), PaLM (Chowdhery et al., 2022), BLOOM (Scao et al., 2022), and etc. These decoder-based causal language models soon occupy kinds of NLP leaderboards, showing excellent language modeling and in-context learning ability of them. However, the huge computing overhead (including memory and time consumption) makes nonprofits and smaller labs difficult to create or even use PLMs. Furthermore, this also prevents PLMs from encoding longer inputs. Efficient Attention A surge of efficient attention models are devised to enhance the efficiency of the original Transformer model (Vaswani et al., 2017). These models explore diverse philosophies to improve the efficiency, including sparse attention matrix (Luong et al., 2015; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Ainslie et al., 2020), memory compression (Liu et al., 2018; Lee et al., 2019; Rae et al., 2020; Wang et al., 2020) low-rank decomposition (Xiong et al., 2021; Lu et al., 2021; Chen et al., 2021), kernel-based linear attention (Choromanski et al., 2021; Peng et al., 2021; 2022; Zheng et al., 2022; 2023), state-space model (Gu et al., 2022; Gupta et al., 2022; Dao et al., 2022b), and CUDA re-implementation (Dao et al., 2022a). Thus models with efficient attention architecture are promising to handle longer input sequences when memory consumption is saved. H3 (Dao et al., 2022b) is pre-trained as an efficient language model but fails to scale up the training sequence length which remains 2048. In-Context Learning With the increasing scale and capacity of PLMs, ICL has become a new paradigm for\nIn-Context Learning With the increasing scale and capacity of PLMs, ICL has become a new paradigm for NLP (Brown et al., 2020a). The success of ICL has been\ndemonstrated on a wide range of NLP tasks, including question answering (Joshi et al., 2017), information retrieval (Tay et al., 2022), math word problem (Cobbe et al., 2021), commonsense reasoning (Geva et al., 2021), and fact checking (Rae et al., 2021) etc. Several recent studies (Liu et al., 2022; Wu et al., 2022) have observed a positive correlation between the number of in-context examples and ICL\u2019s performance: increasing the number of in-context examples can bring steady improvements. Further investigation is carried out to pack and/or distill more examples into the context through continued pre-training (Choi et al., 2022), and instruction tuning (Snell et al., 2022). However, the input length limitation of current PLMs still restricts us from directly feeding more in-context examples into the model.\n# 3. EVALM\nWe propose a long-range language model named EVALM to scale up the sequence length reached by existing pre-trained language models. The rest of this section is organized as follows: \u00a7 3.1 introduces the overall architecture of EVALM; \u00a7 3.2 focuses on learning EVALM on both of pre-training and instruction tuning; \u00a7 3.3 shows how EVALM scales up the maximum size of shots in in-context learning, with an incremental encoding technique. The overall architecture is shown in Figure 1.\n# 3.1. Architecture\nWe adopt EVA (Zheng et al., 2023), a recently introduced attention competitor, as an efficient alternative to vanilla softmax attention (Vaswani et al., 2017), for its high efficiency in long sequence modeling and strong performance. The original EVA performs both causal and noncausal attention in sequence modeling, and here we focus on its causal version for its adaption to language modeling. A general computation process of causal EVA is described as follows. Given a query qt \u2208Rd, and key-value sequences K1:t, V1:t \u2208Rt\u00d7d, where d is the dimensionality and t is the timestamp, EVA learns attentive features as: a) chunking key-value features K1:t, V1:t as Kr, Kl = C(K1:t), Vr, Vl = C(V1:t), where C(\u00b7) is a chunking function with chunk size c, and superscripts r and l denote the remote features beyond present chunk of qt and the local features within the chunk; b) compressing remote features within each chunk by another efficient attention and pooling operation M(\u00b7) as \u02c6Kr = M(Kr), \u02c6Vr = M(Vr), where the efficient attention here is LARA (Zheng et al., 2022); c) performing vanilla attention on concatenated remote and local features by EVA(qt) = softmax(qt[ \u02c6Kr; Kl]\u22a4)[ \u02c6Vr; Vl]\u22a4. It is worth noting that EVA is capable of handling long-term dependencies by performing attention on remote compressed features \u02c6Kr, \u02c6Vr. We refer interested readers to (Anonymous, 2023)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ce8/0ce82960-dab9-4a1c-82d3-5d7e76ef788c.png\" style=\"width: 50%;\"></div>\nchunk size \u2026 Figure 1. The illustration of EVALM scaling up in-context learning. The pre-training stage empowers the language modeling capacity of EVALM, and the instruction tuning explicitly aligns EVALM with instructions from different tasks. For different downstream tasks EVALM can in-context learn from the demonstrations. With the help of CPE and incremental encoding technique, k could be scaled up\nfor further details.\nApart from the advanced attention mechanism EVA, we present circular positional embedding (CPE) to enforce position information. For i-th token, its positional embedding is set to pi%M \u2208Rd, where M is the maximum size of learned positional embeddings. An intriguing characteristic of CPE is its ability on extrapolation and long-term dependency. CPE implicitly learns a position-aligned matrix P = {p\u22a4 i%Mpj%M} between each pair of tokens, which is added to attention matrices. The matrix P is close to the pattern of strided attention (Ho et al., 2019; Tay et al., 2020) with stride size M, and encourages feature interaction to distant features.\nExtrapolation Extrapolation is a vital challenge in longrange language modeling. Remind that the LRLMs are expected to scale the sequence length to tens, hundreds, or even more times to the current limitation with thousands of tokens from existing mainstream pre-trained language models such as GPT (Brown et al., 2020a) and OPT (Zhang et al., 2022b). The challenges of LRLMs lie in the two aspects. On the one hand, such length is still unaffordable for current models, even for efficient attention models, during the training stage, despite incremental decoding (Ott et al., 2019) helping reduce memory consumption in the inference stage. On the other hand, pre-trained data from long-range texts are limited. Thus a practical solution is \u201ctrain short, test long\u201d, a.k.a. extrapolation. Thus finding an architecture with extrapolation capability is important for\nLRLMs. In EVALM, we enhance its extrapolation in two aspects: a) based on the observation that locality contributes to extrapolation (Zhang et al., 2022a), we choose EVA that also models the locality; b) we use circular positional embedding that fledges vanilla learned positional embedding to extrapolate to longer contexts.\n# 3.2. Pre-training & Instruction Tuning\nWe pre-train a causal language model EVALM based on EVA transformer decoder with our preprocessed Pile (Gao et al., 2020) corpus and further tune it using Many-Shot Instruction Tuning (MSIT).\nPre-training Data processing details are in Appendix A.1, Pre-training details are in Appendix A.2. Our EVALM was trained on a widely-used corpus the Pile (Gao et al., 2020), which is a massive dataset designed for training large language models. We built a preprocessing pipeline including filtering, deduplicating, and blending to prepare the pretraining corpus to support the large-scale distributed training process. We conducted catalog and content filtering following BLOOM (Scao et al., 2022) and deduplicated the filtered data using fuzzy deduplication similar to previous work (Zhang et al., 2022b; Smith et al., 2022). The final corpus roughly contains 121B tokens. Please refer to Appendix A.1 for detailed data processing and comparison. The training process EVALM mainly follows GPT3 (Brown et al., 2020a) and OPT (Zhang et al., 2022c), optimizing the\nnegative log-likelihood of next tokens in an auto-regressive way. We scaled the training sequence length to 8192 to accommodate more in-context samples. Fully sharded data parallel (FSDP) was applied in our pre-training stage, which can reduce the memory footprint of a single GPU to accommodate longer sequences. Please refer to Appendix A.2 for a detailed pre-training setting.\nMany-Shot Instruction Tuning Instruction tuning simulates the in-context learning settings and shows the promising ability to activate the model\u2019s respective capacity during inference (Min et al., 2022), with maximum training shots to 32. Based on our long-range EVALM, we can further investigate the impact of instruction tuning after scaling up the shots. We instruction-tuned EVALM on m instruction tuning tasks DIT j = {(xj i, yj i)}Nj i=1, which m is the number of tasks and Nj is sample number for each dataset. Each input-output pair (xj i, yj i) is turned into an instruction sequence sIT i = I(xi, yi) wrapped by the instruction I(\u00b7) written in natural language. I(\u00b7) is derived from an instruction templates pool that is manually designed for different tasks j. Before feeding into EVALM, we concatenate instructions until their total length reaches the limitation of 8192, in a batch-by-token way, named many-shot instruction tuning (MSIT). Further, we introduce the plus version of MSIT, which extrapolates the total length of instructions per batch line to 2 \u00d7 8192. The pressed EVALM learns the concatenated sIT i , under the supervision of the negative log-likelihood objective as language modeling. The data used in our instruction tuning refers to Appendix B.\n# 3.3. In-Context Learning with EVALM\nConsider a downstream task with dataset D, from which we construct a demonstration exemplar set De = {(xi, yi)}k i=1. As instruction tuning, the demonstration exemplars are turned into instruction sequences and then concatenated to se = [I(xi, yi)]k i=1. For a test input text x and its corresponding candidate categories Y, we first concatenate se and I(x, y) together to form a prompt for y \u2208Y. The prompt is then fed into the pre-trained EVALM to compute the likelihood of the current answer y along with x, and we choose the most possible one as the predicted label:\n(1)\nThere are several approaches to constructing De specifically, listed in \u00a7 4.1. For instance-level ICL, the same test sample x shares the same se, and for dataset-level ICL, all test samples share the same se (Wu et al., 2022). In this situation, Eq. (1) turns into:\narg max y\u2208Y P(I(x, y)|se).\n(2)\nLimited by the maximum encoding length of current PLMs (e.g., 2048), the maximum k of ICL is generally about 32 (Min et al., 2022). The upper bound of ICL when scaling up k remains a question. Intuitively, scaling up the shot number k of ICL can further help ICL reach the capacity of finetuning. Beyond the maximum encoding length of 8192, further scaling up k in an efficient way needs the incremental encoding technique.\nIncremental Encoding Incremental decoding (Ott et al., 2019) enhances the sequence generation efficiency by caching useful historical states, namely incremental states, for future usage, which saves memory from the redundant computation. Inspired by this, we devise incremental encoding, which updates EVA cache states incrementally, for long context encoding. According to EVA architecture (\u00a7 3.1), we maintain all local and remote features as incremental states S: {Kl, Vl, \u02c6Kr, \u02c6Vr}. When encoding the incoming tokens, we first concatenate them with local features and then compress full-chunk-sized local features into remote features and update S, details in Algorithm 1.\nAlgorithm 1 Incremental Encoding\nInput: chunk size c, compression attention and pooling\noperation M(\u00b7), incoming token xt, previous incremental\nstates St\u22121 : {Kl\nt\u22121, Vl\nt\u22121, \u02c6Kr\nt\u22121, \u02c6Vr\nt\u22121}\nOutput: updated incremental states St\nqt, kt, vt = projection(xt)\nKl\nt = [Kl\nt\u22121; kt], Vl\nt = [Vl\nt\u22121; vt]\nif length(Kl\nt) is c then\n\u02c6Kr\nt = [ \u02c6Kr\nt\u22121; M(Kl\nt)], \u02c6Vr\nt = [ \u02c6Vr\nt\u22121; M(Vl\nt)]\nKl\nt := \u2205, Vl\nt := \u2205\nelse\n\u02c6Kr\nt = \u02c6Kr\nt\u22121, \u02c6Vr\nt = \u02c6Vr\nt\u22121\nend if\nreturn St : {Kl\nt, Vl\nt, \u02c6Kr\nt, \u02c6Vr\nt }\nPreviously, encoding long-range context requires quadratic memory complexity, and incremental encoding consequently scales it down to linear by caching previous S, where the memory consumption grows linearly along with the increase of S. Powered by this, EVALM further reduces the memory bottleneck and ensures the input length is scalable. In practice, the upper bound of encoding length is 32\u00d7 than training, and it is possible to encode an extremely long sequence into incremental states losslessly, with the compression rate c. Incremental encoding thus brings numerous benefits for many scenarios like ICL. For ICL, considering many test samples share the same demonstration sequence se, we can encode it once, cache the long-term incremental states, and reuse them for further possible encoding. The test samples are then fed forward, conditioned on S, to predict the result using Eq. (2). Reusing the incremental states of demonstration saves the extra overheads of scaling k.\n# 4. Experiments\nIn this section, we conduct in-context learning experiments to validate our EVALM and its instruction-tuned version on various tasks.\n# 4.1. Experimental setting\nPre-training We pre-trained EVALM (350M and 1.3B) on 32 NVIDIA A100 80G GPUs. The hyper-parameters of EVALMs are identical to GPT3 (Brown et al., 2020a) and OPT (Zhang et al., 2022c) in the same scale, where the hidden size, number of attention heads and number of layers are 1024, 16, 24 respectively for the 350M model and 2048, 32, 24 respectively for the 1.3B model.\nInstruction Tuning Following FLAN (Wei et al., 2021), we experiment ICL on the downstream tasks using EVALM instruction-tuned on FLAN datasets. FLAN dataset that belongs to the same cluster with the current test task is excluded during the instruction tuning stage, preventing the evaluation from data leakage and remaining our setting regarded as zero-shot or many-shot. There are three settings for instruction tuning in our experiments: a) IT: one-shot IT; b) MSIT: many-shot instruction tuning with a maximum of 8192 per batch line; c) MSIT+1: MSIT with a maximum of 2 \u00d7 8192 per batch line. More instruction tuning details can be seen in Appendix B.\nIn-Context Learning We mainly follow Wu et al. (2022) and Wei et al. (2021) to select several datasets from different NLP tasks. We choose SST-2 and SST-5 for sentiment classification (Socher et al., 2013), MNLI (Williams et al., 2018) for natural language inference, MultiRC (Khashabi et al., 2018) and BoolQ (Clark et al., 2019) for reading comprehension, AgNews (Zhang et al., 2015) for topic classification, WSC (Levesque et al., 2012) for coreference resolution, COPA (Roemmele et al., 2011) for commonsense reasoning and Trec (Hovy et al., 2001) along with WiC (Pilehvar & Camacho-Collados, 2019) for miscellaneous tasks.\nWe mainly adopt zero-shot and many-shot settings. The zero-shot setting directly wraps up the testing input with a task-specific template for inference. The many-shot approach randomly selects k demonstrations from the training set and uses the same demonstrations for the whole test set. This approach is universally used as dataset-level ICL. We also adopt Top-k approach following Wu et al. in \u00a7 4.4. Prompt designs are detailed in Appendix C.2.\nWe find the best shot number on the validation set and test on the test set when the label of the test set is available\n(AgNews, Trec, SST-5). For other datasets, we split 500 samples from each training set as a validation set and report our results on the test set. The demonstration number k is set from 1 to 2000, please refer to Appendix C.1 for more in-context learning details.\nBaselines We use OPT (Zhang et al., 2022c) as the main baseline due to its similar model architecture, number of parameters, training flops, training data, and training framework to our EVALM, allowing for a fair comparison. We conduct experiments using models of 350M and 1.3B parameters.\n# 4.2. Main Results\nThe overall in-context learning results are shown in Table 1. Based on this, we make the following observations.\nScaling up demonstration examples helps ICL Since EVALM pre-trained with longer sequence length and adapted for extrapolation, we can use more demonstrations when conducting in-context learning experiments. At both 350M and 1.3B scale, EVALM outperforms OPT on both zero-shot and many-shot settings, and tends to achieve the best score at higher average shot number k (about 10 times to OPT). This shows that long-range EVALM can effectively utilize the information in demonstrations to get better results. The specific best shot numbers for each dataset and model are in Appendix C.1.\nMSIT arouses the potential of many-shot ICL Table 1 shows that the model with MSIT, especially MSIT+, obtains the most growth, from zero-shot to many-shot setting, which is indicated by the relative improvement scores. This is partly because MSIT learns to align the language modeling with many-shot in-context learning scenarios, making it more suitable for testing in many-shot settings. Another reason is the relatively poor zero-shot performance with MSIT. A potential explanation is that learning too many tasks fills the capacity of small PLMs, which can be harmful to their zero-shot performance, as mentioned in FLAN (Wei et al., 2021). Thus, we speculate that combining MSIT and scaling in-context shot number k together is essential for getting the best in-context learning results.\nLarger PLMs suit many-shot ICL Both EVALM-1.3B and OPT-1.3B show more significant progress compared with the 350M model. This is also consistent with the rule of scaling law (Chung et al., 2022). Large PLM contains more knowledge and can better conduct ICL through more demonstrations. This suggests that scaling up ICL may yield greater benefits on larger models.\nTable 1. Main results of in-context learning on diverse tasks. The light grey shade refers to the ablation modules of IT. We average the\nshot number of demonstrations when the best score is achieved. The best overall results are bolded. The abbreviation avg. is for average,\nimprv. is for improvement, acc is for accuracy. The relative improvements of models in the many-shot setting are compared with\nthe same model but in the zero-shot setting respectively.\nModels\nSentiment\nNLI\nMiscellaneous\nReading\nTopic\nCoreference Commonsense Avg.\nacc Imprv. Avg.\nshot\nSST-2 SST-5 MNLI\nTrec\nWiC\nMultiRC BoolQ AgNews\nWSC\nCOPA\nzero-shot\nOPT-350M\n64.6\n29.9\n21.6\n23.0\n52.7\n46.3\n53.8\n50.9\n63.4\n65.0\n47.1\n-\n-\nEVALM-350M\n61.4\n25.8\n27.5\n21.8\n51.7\n56.9\n56.9\n46.6\n63.5\n64.0\n47.6\n-\n-\nw/ MSIT\n50.8\n28.2\n28.6\n20.4\n50.6\n43.3\n49.9\n47.5\n63.5\n65.0\n44.8\n-\n-\nw/ MSIT+\n64.0\n29.3\n28.0\n22.2\n50.4\n42.0\n53.3\n48.6\n63.5\n62.0\n46.3\n-\n-\nOPT-1.3B\n73.0\n31.3\n20.0\n22.0\n50.3\n41.7\n51.4\n56.6\n62.5\n72.0\n48.1\n-\n-\nEVALM-1.3B\n82.3\n31.3\n21.6\n22.8\n52.1\n41.7\n58.5\n55.3\n58.6\n72.0\n49.6\n-\n-\nw/ MSIT\n58.4\n33.5\n21.6\n23.0\n52.3\n52.2\n52.4\n55.2\n58.2\n71.0\n47.8\n-\n-\nmany-shot\nOPT-350M\n62.3\n31.0\n33.8\n27.6\n51.6\n57.2\n62.8\n63.8\n63.4\n64.0\n51.7\n4.6\n10\nEVALM-350M\n61.0\n32.3\n32.1\n49.6\n52.0\n55.7\n60.6\n69.7\n63.4\n63.0\n53.9\n6.3\n97\nw/ MSIT\n65.2\n31.2\n34.1\n39.4\n52.4\n53.4\n57.5\n70.1\n63.5\n72.0\n53.9\n9.1\n236\nw/ MSIT+\n70.6\n33.7\n34.5\n40.4\n50.4\n53.1\n59.2\n73.3\n63.5\n73.0\n55.2\n8.8\n208\nOPT-1.3B\n73.0\n40.1\n31.3\n45.6\n50.3\n52.5\n65.2\n60.0\n63.4\n74.0\n55.3\n7.3\n14\nEVALM-1.3B\n76.6\n40.4\n30.2\n46.8\n54.3\n58.9\n62.5\n62.5\n63.5\n74.0\n57.0\n7.3\n152\nw/ MSIT\n84.2\n45.4\n33.9\n49.4\n54.2\n60.2\n64.2\n63.2\n65.4\n74.0\n59.4\n11.6\n269\n<div style=\"text-align: center;\">Table 2. Average accuracy and input length when achieving highest scores over all datasets with different IT strategies.</div>\nModels\nVanilla\nw/ IT\nw/ MSIT\nAcc. Length Acc. Length Acc. Length\nOPT-350M\n51.7\n584.5\n50.9\n560.2\n51.5 1592.3\nEVALM-350M 53.9 3904.9 53.7\n3682\n53.9 8087.7\nOPT-1.3B\n55.3\n665.0\n54.5\n670.3\n54.8 1809.6\nEVALM-1.3B 57.0 7337.0 56.9 8140.6 59.4 12558.0\n# 4.3. Analysis on MSIT\nEfficacy of MSIT To further investigate the effectiveness of MSIT, we average the best ICL results on instructiontuned EVALM as the shot number increases. Considering the average example length of different datasets varying from each other, we also count the length of demonstrations at the peak of accuracy instead of using the shot number. All results are averaged over 10 datasets in Table 2. We also conduct the same experiment on the same size OPT model, but with 2048 tokens per batch line for MSIT. We observe that as the number of IT examples grows, the average lengths of many-shot examples increase accordingly, for both OPT and EVALM. It reflects that MSIT indeed learns the alignment with many-shot ICL. Such alignment helps EVALM\u2019s enhance its capability on many-shot ICL, but becomes helpless or even harmful to OPT. A possible reason is that EVALM is specialized in long-range language modeling with extrapolation whilst OPT is not. Thus, many-\nScaling k-shot ICL We dig into the specific accuracy curve as the demonstration length rises, taking the AgNews dataset using randomly selected many-shot ICL and the Trec dataset using Top-k many-shot ICL as examples. Please refer to \u00a7 4.4 for more analysis about the Top-k setting. We choose this setting to analyze considering the robustness of the approach and stability of the curve. As shown in Figure 2, we observe that EVALM without instruction tuning or just with one-shot instruction tuning achieves the highest accuracy within 128 shots, which corresponds to around 2k length, and further adding demonstrations makes the accuracy curve drop quickly. With many-shot instruction tuning, the best accuracy is improved and the heavy drop gets alleviated. With instruction tuning on longer range (MSIT+), the accuracy grows steadily along with increasing demonstration length and peaks at 768 shots, which corresponds to around 15k length. Similar trends can be found in Figure 3 but OPT can not. This trend indicates that MSIT encourages our language model to achieve higher accuracy, and scaling up the shot number of IT further improves the upper bound of scaling in-context learning on downstream tasks. However, the increasing trend is not endless, even for MSIT+. When the length of demonstration examples reaches 20k, the rapid drop of the accuracy curve can be seen. The possible reasons are listed as follows. On the one hand, modeling the longer input length relies on the extrapolation ability of models, and the size of models could also be the limiting factor. There are more discussions in \u00a7 4.4. On\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8ce8/8ce84c05-5cd3-4f4a-aeea-ff18061fdbb3.png\" style=\"width: 50%;\"></div>\nFigure 2. The ICL accuracy curve along with demonstration length on Trec dataset using the Top-k approach, for EVALM-350M models with different instruction tuning strategies.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6813/68130b23-923c-4f1c-9ced-aa2fcbf16356.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Length of tokens (\u00d78192)</div>\nFigure 3. The ICL accuracy curve along with demonstration length on AgNews dataset using the random in-context examples, for EVALM-350M with different instruction tuning strategies and OPT-350M.\nthe other hand, the setting of enlarging the demonstration example sizes, in both of instruction tuning and in-context learning, is under-explored, due to the lack of pre-trained LRLMs. Therefore, advanced ICL algorithms are demanded for further investigation on many-shot in-context learning.\n# 4.4. Discussion\nExtrapolation Ability To ensure the extrapolation ability of EVALM, we simply adopt CPE (\u00a7 3.1), and incremental encoding (\u00a7 3.3) is deployed to save memory consumption. With these techniques, EVALM is able to encode super-long inputs, i.e. 256k on 80G NVIDIA A100, during inference. For comparison, we also adapt the incremental encoding technique to the OPT model of the same size, whose maximum context size still lags behind EVALM\u2019s. Detailed comparison of memory consumption between OPT and EVALM can be found in Appendix C.1.\nBased on this, we further evaluate the extrapolation ability of different models using perplexity. The experiment is conducted on PG-19 dataset (Rae et al., 2019), a dataset focusing on long-range language modeling, following the setting by Zhang et al. (2022a). The perplexity curve along with the input length is addressed in Figure 4. The perplexity of OPT grows steeply once the input length is over 2048, indicating its poor extrapolation ability. The vanilla EVALM with MSIT increases the perplexity, which is expected considering that the instruction tuning will adapt PLMs from\n<div style=\"text-align: center;\">Table 3. Results of using Top-k ICL approach. The light grey shade refers to the ablation modules of IT. The best results are bolded. The abbreviation avg. is for average.</div>\nbolded. The abbreviation avg. is for average.\nModels\nSST-2 SST-5 MNLI Trec AgNews Avg.\nOPT-350M\n86.1\n44.5\n33.8\n74.8\n91.0\n66.0\nEVALM-350M\n88.2\n46.5\n29.5\n76.8\n91.8\n66.6\nw/ MSIT\n86.0\n43.6\n27.2\n78.0\n90.9\n65.1\nw/ MSIT+\n88.3\n44.9\n27.7\n83.8\n91.9\n67.3\nOPT-1.3B\n86.7\n43.2\n25.1\n77.0\n91.3\n64.7\nEVALM-1.3B\n87.7\n47.4\n30.2\n79.0\n91.8\n67.2\nw/ MSIT\n88.2\n47.0\n32.2\n76.0\n91.0\n66.9\nthe general corpus towards several specific tasks. Compared with MSIT, EVALM with MSIT+ achieves lower perplexity even lower than the vanilla model. MSIT+ also reaches the lowest perplexity at a larger input length around 16k. These observations explain why the OPT benefits less while EVALM benefits more from MSIT especially MSIT+ in Table 2, Figure 2 and Figure 3.\nTop-k ICL Following Wu et al., we also deploy Top-k approach (instance-level) which selects the k most similar samples from the training dataset based on embedding similarities (Liu et al., 2022; Gao et al., 2021) and puts the samples with higher similarity closer to the testing input. We conduct Top-k ICL in our commonly used datasets to further verify the effects of MSIT.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b32e/b32e1b55-ede2-40f2-b25f-8df7deab6e75.png\" style=\"width: 50%;\"></div>\nFigure 4. The perplexity of OPT-350M and EVALM-350M on PG19 dataset when the length of input sequence scaled up. The extrapolation of OPT starts from 2048 and others from 8192\nthis situation, MSIT+ still shows a positive effect on most of the tasks, showing the considerable effects of more shots instruction tuning.\nBesides, Top-k approach, an instance-level ICL algorithm, selects different demonstration examples for each test sample, demanding heavy computation resources in ICL inference. In contrast, the random approach, a dataset-level ICL algorithm, is much cheaper by sharing and caching incremental encoded examples for all the test samples. Thus, we believe that the random approach or advanced datasetlevel ICL algorithms are more compatible and promising to LRLMs.\nEfficiency We test the efficiency of EVALM with training FLOPs and inference times, which are considered crucial for PLMs in upstream training and downstream usage. As shown in Table 4, EVALM can achieve better in-context learning performance with OPT in the same size with even lower training costs. This is due to the efficiency of the causal EVA and our deduplicated training data. Compared with pre-training, the cost of instruction tuning is significantly lower, making it a more easily adopted way to improve the in-context learning performance of PLMs.\n<div style=\"text-align: center;\">Table 4. Training FLOPs of different models</div>\nTable 4. Training FLOPs of different models\nModels\nVanilla\nw/ MSIT\nOPT-350M\n3.84E+20 1.60E+18\nEVALM-350M 2.80E+20 1.15E+18\nOPT-1.3B\n1.42E+21 5.94E+18\nEVALM-1.3B\n1.03E+21 4.27E+18\nAs for inference efficiency, according to \u00a7 3.3, with incremental encoding and the reuse of incremental states, the additional cost of k-shot ICL when scaling up k in EVALM is relatively low in many-shot settings. Figure 5 illustrates the time consumption of EVALM-350M for each test sample along with the number of shots k, and the results are conducted on SST-5 averaged over 1000 samples. It indicates that without the reuse of incremental states, the overheads grow rapidly while reusing saves redundant computation. The consumption of first encoding long-range demonstrations is diluted by the number of test samples.\n# 5. Conclusions & Future Work\nThe under-investigated pre-trained long-range language model limits the exploration of more shots instruction tuning and in-context learning. In this work, we first pre-train a casual language model EVALM based on an efficient attention mechanism EVA, successfully enabling training with 8k tokens and extrapolating with 256k-length contexts. With techniques such as incremental encoding for efficiency and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/64d0/64d04661-a9f5-4f50-82a4-e340cf1d7217.png\" style=\"width: 50%;\"></div>\nFigure 5. Inference time for each sample on SST-5 with or without the reuse of incremental states. 2000 shots corresponds to 58k input length for this dataset\ncircular position embedding for extrapolation, we consequently inspect the effectiveness of increasing the shot number of both instruction tuning and in-context learning using EVALM. Experimental results across a variety of tasks show EVALM with many-shot instruction tuning and plus outperforms the same size OPT by 4.1% accuracy on average. Interestingly, we find that many-shot instruction tuning can help ICL achieve higher performance with larger demonstrations, and with longer instructions, this phenomenon is more obvious. Notably, such many-shot ICL, with incremental encoding and caching, demands rare extra computational overheads.\nEVALM takes the first step towards many-shot in-context learning with pre-trained long-range language models, but it still has several limitations. First, due to our limited computational resources, the experimented EVALM is relatively small in model size compared to existing large-scale language models, e.g. GPT, OPT and PaLM. We will actively work on scaling up its capacity, and it would be interesting to expect its performance on larger LRLMs. Second, although the backbone attention model EVA is efficient and competitive with vanilla attention, it still struggles to scale to longer sequence modeling, due to its quadratic complexity to sequence length in causal language modeling. We will improve LRLMs with linear attention mechanisms to further scale up the reachable length of contexts. Third, when scaling up in-context examples, EVALM is incapable of gaining performance from marginal ones, consistently. We will explore new many-shot in-context learning algorithms that consistently gain performance from the increasing number of in-context examples.\n# 6. Acknowledgements\nWe thank Lin Zheng for proposing the state-of-art efficient attention EVA and providing a well-designed codebase. This work is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100) and the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N HKU714/21.\n# References\nAinslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268\u2013284, Online, 2020. Association for Computational Linguistics.\nAnonymous. Efficient attention via control variates. In Submitted to The Eleventh International Conference on Learning Representations, 2023. under review.\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. ArXiv preprint, abs/2004.05150, 2020.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., and Weinbach, S. GPT-NeoX20B: An open-source autoregressive language model. In Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95\u2013136, virtual+Dublin, 2022. Association for Computational Linguistics.\nrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a.\nrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020b.\nChen, Y., Zeng, Q., Ji, H., and Yang, Y. Skyformer: Remodel self-attention with gaussian kernel and nystr\\\u201dom method. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021.\nChoi, E., Jo, Y., Jang, J., and Seo, M. Prompt injection: Parameterization of fixed inputs. ArXiv preprint, abs/2206.11349, 2022.\nChoromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00b4os, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416, 2022.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\nCobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022a.\nDong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., and Sui, Z. A survey for in-context learning. ArXiv preprint, abs/2301.00234, 2023.\nGupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.\nHo, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. ArXiv preprint, abs/1912.12180, 2019.\nHuman Language Technologies, Volume 1 (Long Papers), pp. 252\u2013262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\nLevesque, H. J., Davis, E., and Morgenstern, L. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR\u201912, pp. 552\u2013561. AAAI Press, 2012. ISBN 9781577355601.\nLiu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013 114, Dublin, Ireland and Online, 2022. Association for Computational Linguistics.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\ntoolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48\u201353, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 9438\u20139447. PMLR, 2020.\nTay, Y., Tran, V. Q., Dehghani, M., Ni, J., Bahri, D., Mehta, H., Qin, Z., Hui, K., Zhao, Z., Gupta, J., et al. Transformer memory as a differentiable search index. ArXiv preprint, abs/2202.06991, 2022.\nZhang, J., Jiang, S., Feng, J., Zheng, L., and Kong, L. Cab: Comprehensive attention benchmarking on long sequence modeling. ArXiv preprint, abs/2210.07661, 2022a.\nZhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.\nZheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings\n# A. Pre-training Details\nWe build the pre-training corpus based on the Pile (Gao et al., 2020), and the pipeline includes filtering, deduplicating,  blending.\nFiltering Many of our content filtering strategies were inspired by the data preparation pipeline of BLOOM (Scao et al., 2022) model.2 We filtered raw data from the Pile, including catalog and content filtering. For the catalog filtering, the pre-training corpus contains a subset of the Pile, including BookCorpus2, Books3, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, Pile-CC, USPTO, and Wikipedia. We exclude the other subsets of the Pile. On the one hand, based on this project\u2019s scope, we aim to demonstrate our model on the general natural language tasks, and the other domain-specific subsets of the Pile are unsuitable for this purpose. On the other hand, these subsets are relatively noisy, which increases the difficulty and instabilities of the pre-training process, according to the tendency to cause spikes in gradient norms (Zhang et al., 2022a). For the content filtering, we first modified the raw data by standardizing the whitespace and removing the non-ASCII characters. Then we filtered the text documents on (1) the flagged harmful words, (2) the stop word ratio, (3) the word/character repetition ratio, and (4) the specific character ratio.\nFor the content filtering, we first modified the raw data by standardizing the whitespace and removing the non-ASC characters. Then we filtered the text documents on (1) the flagged harmful words, (2) the stop word ratio, (3) th word/character repetition ratio, and (4) the specific character ratio.\nDeduplicating We opted to take the fuzzy deduplication inspired by previous works (Zhang et al., 2022b; Smith et al., 2022). In our implementation, we calculated the mini-hashes and performed LSH using datasketch3, computed the connected components using scipy4, cached the hash fingerprint using Redis5. We first whitespace-tokenized the documents into words and vectorized the documents with the 1-gram language model. Then we calculated the mini-hashes of the document vectors to obtain the document fingerprints with 100-bit hash length. We perform Locality Sensitive Hashing (LSH) through all the document fingerprints to find the neighborhoods of each document with a Jaccard similarity larger than 0.95. After that, we constructed a sparse graph with each document as a node and connected the nodes with their neighborhoods. In this way, we can find the sets of near-duplicated documents by computing the connected components of the graph. Finally, we selected the high-quality documents from each set and removed the other documents in the order of predefined priority. After the filtering and deduplication, we blended the filtered data into heterogeneous batches to obtain the final pre-training corpus. The details are shown in Table 5.\n<div style=\"text-align: center;\">Table 5. Number of tokens per dataset in the final pre-training corpus</div>\nble 5. Number of tokens per dataset in the final pre-trainin\nDatasets\nTokens (billion)\nBookCorpus2\n1.6\nGutenberg (PG-19)\n3.0\nWikipedia (en)\n12.1\nOpenWebText2\n15.7\nBooks3\n26.0\nPile-CC\n52.2\nDM Mathematics\n3.8\nHackerNews\n1.1\nOpenSubtitles\n1.6\nUSPTO Backgrounds\n4.0\nTotal\n121\nA.2. Training Details\nWe pre-trained EvaLM based on metaseq6, the pre-training hyperparameters are listed in Table 6.\n<div style=\"text-align: center;\">Table 6. Hyperparameters used for pre-training</div>\nTable 6. Hyperparameters used for pre-training\nHypermeters\nEVALM-350M\nEVALM-1.3B\nDropout\n0.1\nWeight Decay\n0.1\nClip Norm\n1.0\nClip Norm Type\nL2\nLR Schedular\nPolynomial decay\nLearning Rate\n8e-5\nGlobal Batch Size\n64\n128\nDDP Backend\nDDP\nFSDP\n# B. Instruction Tuning Details\nWe mainly follow settings in FLAN (Wei et al., 2021) to conduct ICL experiment. FLAN dataset consists of 12 dataset clusters including 9 NLU clusters and 3 NLG clusters. As we treat Agnews as a classification task, we only block out this dataset itself rather than the whole summarization cluster. The training hyperparameters are the same in the pre-training stage. We train all models for 5 epochs on selected FLAN datasets to get a fair comparison between IT, MSIT, and MSIT+ during the instruction tuning stage.\n# C. In-Context Learning Results\n# C.1. In-context learning details\nWe conduct in-context experiments with 0, 1, 3, 4, 8, 16, 32, 64, 80, 128, 192, 256, 372, 512, 640, 768, 896, 1024, 1280, 1536, 1792, 2000 shots considering the limited computing resources. We compare the memory consumption for EVALM-350M and OPT-350M on single NVIDIA 80G A100 in Figure 6.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/808b/808bdedf-b578-4c0f-b6f7-bdac138f6319.png\" style=\"width: 50%;\"></div>\nWe provide supplementary results for the many-shot setting in Table 7, which is the shot number of demonstrations when the best score is achieved for each dataset respectively.\n# C.2. Prompt Template\nFor the sake of reproduction, we list the prompt template and label mapping used in our experiments for different tasks. We refer to templates protocol used in GPT3 and other works (Wu et al., 2022).\n<div style=\"text-align: center;\">ble 7. Supplementary results for many-shot setting: the shot number of demonstrations when the best score is achieved for each datas</div>\nModels\nSentiment\nNLI\nMiscellaneous\nReading\nTopic\nCoreference CMS\nSST-2 SST-5 MNLI Trec\nWiC\nMultiRC BoolQ AgNews\nWSC\nCOPA\nOPT350M\n1\n1\n80\n1\n1\n1\n3\n8\n4\n1\nEVALM350M\n1\n4\n372\n372\n128\n8\n4\n64\n16\n1\nw/ MSIT\n16\n8\n512\n1280\n128\n16\n3\n80\n256\n64\nw/ MSIT+\n16\n4\n1280\n372\n128\n64\n3\n80\n8\n128\nOPT1.3B\n8\n16\n80\n16\n1\n1\n4\n8\n4\n1\nEVALM1.3B\n192\n16\n256\n256\n192\n8\n128\n64\n372\n16\nw/ MSIT\n192\n16\n1280\n256\n192\n16\n128\n64\n512\n16\n<div style=\"text-align: center;\">Table 8. Prompt template and label mapping in our experiment</div>\nDataset\nTemplate\nLabal Space\nSST-2\n{Label} Movie Review: {Sentence}\nNegative / Positive\nSST-5\n{Sentence} It is {Label}\nterrible / bad / okay / good / great\nMNLI\n{Premise}?{Label}, {Hypothesis}\nNo / Maybe / Yes\nTrec\n{Sentence} It is about {Label}\nabbreviation / entity / description and abstract\nconcept / human being / location / numeric value\nWIC\n{Sentence1}\\n {Sentence2}\\n\nquestion: Is the word {Word} used in the\nsame way in the two sentences above?\\n\nanswer: {Label}\nno / yes\nMultiRC\nContext: {Paragraph}\\n\\n {Questions}\\n\n{Label} answer: {Answer}\nincorrect / correct\nBoolQ\nContext:{Passage}\\n Question: {Question}?\\n\nanswer: {Label}\nno / yes\nAgNews\n{Sentence} It is about {Label}\nworld / sports / business / technology\nWSC\n{Paragraph}\\n Question: In the passage above,\nwhat does the pronoun {Span2} refer to?\\n\nAnswer:{Span1} This is a {Label} answer.\nfalse / true\nCOPA\nContext: {Premise}\\n\nCorrect Answer: {Choices}\nfalse / true\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of the limitations of existing pre-trained language models (PLMs) in handling long-range in-context learning due to their transformer architecture, which restricts context size due to memory and computational costs. The authors argue for the necessity of developing a new method that can efficiently scale up context size and improve in-context learning performance.",
        "problem": {
            "definition": "The problem is the inability of existing PLMs to effectively utilize a large number of demonstration examples in in-context learning due to their limited maximum context size (2048 tokens), which hinders their performance in various natural language processing tasks.",
            "key obstacle": "The main challenge is the computational overhead of the vanilla transformer architecture, which limits the maximum context size and prevents effective scaling when increasing the number of demonstration examples."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that increasing the number of in-context examples can improve the performance of PLMs, but existing models cannot handle the computational demands of longer contexts.",
            "opinion": "The proposed idea is to create a long-range language model named EVALM that can efficiently process longer contexts (up to 256k tokens) and utilize many demonstration examples for better in-context learning.",
            "innovation": "EVALM introduces an efficient transformer architecture (EVA) and techniques like circular positional embedding and incremental encoding, which allow it to scale context size and improve performance compared to existing PLMs."
        },
        "method": {
            "method name": "EVALM",
            "method abbreviation": "EVALM",
            "method definition": "EVALM is a long-range language model designed to efficiently handle long contexts and improve in-context learning performance by utilizing advanced attention mechanisms and encoding techniques.",
            "method description": "EVALM enables training with 8k tokens and extrapolation with contexts up to 256k tokens, significantly enhancing the capabilities of in-context learning.",
            "method steps": [
                "Pre-train EVALM using a large corpus with an efficient transformer architecture.",
                "Use circular positional embedding to enhance the model's ability to handle long-term dependencies.",
                "Implement incremental encoding to cache previous states and reduce memory consumption during inference."
            ],
            "principle": "The effectiveness of EVALM lies in its ability to efficiently compress and encode long sequences, allowing it to leverage more demonstration examples without a proportional increase in computational cost."
        },
        "experiments": {
            "evaluation setting": "EVALM was evaluated on various NLP tasks using datasets such as SST-2, MNLI, and AgNews, comparing its performance against baseline models like OPT under different instruction tuning strategies.",
            "evaluation method": "The performance of EVALM was measured by accuracy on test sets and was analyzed based on various configurations of instruction tuning and the number of demonstration examples used."
        },
        "conclusion": "The experimental results demonstrate that EVALM outperforms existing models by an average of 4.1% accuracy, particularly benefiting from many-shot instruction tuning and longer instructions, highlighting its potential for improved in-context learning performance.",
        "discussion": {
            "advantage": "EVALM's key advantages include its ability to handle significantly longer contexts and effectively utilize more demonstration examples, leading to improved performance in in-context learning tasks.",
            "limitation": "Despite its advancements, EVALM's performance gains are inconsistent with marginal increases in demonstration examples, indicating potential limitations in its learning capacity.",
            "future work": "Future research will focus on scaling up EVALM's model size, improving its attention mechanisms for longer sequences, and developing new algorithms for consistently improving performance with increasing in-context examples."
        },
        "other info": {
            "code availability": "Code is available on https://github.com/Shark-NLP/EVALM.",
            "acknowledgements": "The authors acknowledge support from the Shanghai Committee of Science and Technology and the National Natural Science Foundation of China."
        }
    },
    "mount_outline": [
        {
            "section number": "1.3",
            "key information": "The paper introduces EVALM, a long-range language model capable of processing contexts up to 256k tokens, which facilitates improved in-context learning."
        },
        {
            "section number": "3.4",
            "key information": "EVALM utilizes advanced attention mechanisms and encoding techniques, such as circular positional embedding and incremental encoding, to handle long-term dependencies and adapt to context effectively."
        },
        {
            "section number": "4.1",
            "key information": "EVALM demonstrates that increasing the number of in-context examples can enhance performance, highlighting the influence of effective prompt design on in-context learning outcomes."
        },
        {
            "section number": "6.2",
            "key information": "The paper discusses the computational overhead of the vanilla transformer architecture as a limitation, which restricts the maximum context size and affects the efficiency of in-context learning."
        },
        {
            "section number": "7",
            "key information": "The experimental results indicate that EVALM outperforms existing models by an average of 4.1% accuracy, showcasing its potential for improved in-context learning performance."
        }
    ],
    "similarity_score": 0.7209614648336314,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning with Many Demonstration Examples.json"
}