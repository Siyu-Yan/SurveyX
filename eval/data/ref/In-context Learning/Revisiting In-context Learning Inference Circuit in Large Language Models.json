{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.04468",
    "title": "Revisiting In-context Learning Inference Circuit in Large Language Models",
    "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.",
    "bib_name": "cho2024revisitingincontextlearninginference",
    "md_text": "# REVISITING IN-CONTEXT LEARNING INFERENCE CIRCUIT IN LARGE LANGUAGE MODELS\nHakaze Cho1,\u2217 Mariko Kato1,\u2020 Yoshihiro Sakai1,\u2020 1Japan Advanced Institute of Science and Technology 2RIKEN *Correspondence to: yfzhao@jaist.ac.jp \u2020Equal contribution\n# ABSTRACT\nIn-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.\narXiv:2410.04468v1\n# INTRODUCTION\nIn-Context Learning (ICL) (Radford et al., 2019; Dong et al., 2022) is an emerging few-shot learning paradigm: given the demonstrations {(xi, yi)}k i=1 consisting of [input text]-[label token] pairs and a query xq, Language Models (LMs) take the sequence [x1][s1][y1] . . . [xk][sk][yk][xq][sq]1 as input and then predicts the label for xq by causal language modeling operation. Typically, the label tokens yi are preceded by and also predicted by forerunner tokens si (e.g., the colon in \u201cLabel: \u201d). ICL has aroused widespread interest, but its underlying mechanism is still unclear. There have been theoretical or empirical trials to characterize and explain the inference process of ICL (Xie et al., 2021; Dai et al., 2023; Wang et al., 2023; Han et al., 2023a; Jeon et al., 2024; Zheng et al., 2024). However, to capture all the operating dynamics and the observed interesting phenomenon of ICL in Large Language Models2(LLMs), a more comprehensive characterization is still necessary. Therefore, this paper proposes a unified inference circuit and measures various properties in LLMs for a conformation to the observed ICL phenomenon. As shown in Fig. 1, we decompose ICL dynamics into 3 atomic operations on Transformer layers. Step 1: SUMMARIZE: LMs encode each input text xi into linear representations in the hidden state of its corresponding forerunner token si. Step 2: SEMANTICS MERGE: For demonstrations, LMs merge the encoded representations of si with the hidden state of its corresponding label tokens yi.\n1In this paper, we denote tokenization as [\u00b7]. 2Large refers to scaled LMs trained by natural language data, such as Llama 3 (AI@Meta, 2024), contrast to simplified work that uses simple models trained and test on well-embedded input in toy models.\nNaoya Inoue1,2\nStep 3: FEATURE RETRIEVAL AND COPY: LMs retrieve merged label representations y1:k from Step 2 similar to the query representation sq in a task-relevant subspace and then merge them with the query representation. Finally, LM heads predict the label for xq using the label-attached query representation sq. Steps 2 and 3 form a typical induction circuit, which is a key mechanism of ICL but only examined in synthetic scenarios (Elhage et al., 2021; Singh et al., 2024b; Reddy, 2024).\nWe empirically find evidence for the existence of each proposed step in LLMs, and conduct more fine-grained measurements to gain insights into some phenomena observed in ICL scenarios, such as (1) position bias: the prediction is more influenced by the latter demonstration (Zhao et al., 2021), (2) noise robustness: the prediction is not easy to be affected by demonstrations with noisy labels (Min et al., 2022), while larger models are less robust to noise (Wei et al., 2023), and (3) demonstration saturation: the improvements plateau when sufficient demonstrations are given (Agarwal et al., 2024; Bertsch et al., 2024), etc. (discussed in \u00a75.3). Moreover, we find multiple bypass mechanisms for ICL with the help of residual connections, while the 3-phase dynamics remains dominant. Our contributions can be summarized as:\nFigure 1: The 3-phase inference diagram of ICL. Step 1: LMs encode every input text into representations, Step 2: LMs merge the encoded text representations of demonstrations with their corresponding label semantics, Step 3: LMs retrieve merged label-text representations similar to the encoded query, and copy the retrieved representations into the query representation.\nence circuit to characterize the inference process of ICL, and find empirical evidence of their existence in LLMs. (2) We conduct careful measurements for each inference step and successfully capture a large number of interesting phenomena observed in ICL, which enhances the practicality of the proposed circuit. (3) Our ablation analysis suggests that the proposed circuit dominates, but some bypass mechanisms exist in parallel to perform ICL. We introduce some of these bypasses along with their empirical evidence.\n# 2 PREPARATION\n# 2.1 BACKGROUND & RELATED WORKS\nIn-context Learning. Discovered by Radford et al. (2019), ICL is an emerging few-shot learning paradigm with only feed-forward calculation in LMs. Given demonstrations {(xi, yi)}k i=1 composed of structured input-label pairs and a query xq, typical ICL creates a prompt [x1][s1][y1] . . . [xk][sk][yk][xq][sq], with some structural connectors (e.g. \u201cLabel: \u201d) including forerunner token si (e.g. \u201c: \u201d), as shown in Fig. 1. LMs receive such prompts and return the next token distribution, where the label token with the highest likelihood is chosen as the prediction. Explaining the principle of ICL is an unresolved research topic, although there have been some efforts on the relationship between ICL capacity and pre-training data (Li & Qiu, 2023; Singh et al., 2024b;a; Gu et al., 2023; Han et al., 2023b; Chan et al., 2022), the feature attribution of input prompt (Min et al., 2022; Yoo et al., 2022; Pan, 2023; Kossen et al., 2024), and reduction to simpler algorithms (Zhang et al., 2023; Dai et al., 2023; Xie et al., 2021; Han et al., 2023a). However, a comprehensive explanation of real-world LMs is needed to capture the operating dynamics of ICL. Induction Circuit. Introduced by Elhage et al. (2021), an induction circuit is a pair of two cooperating attention heads from two transformer layers, where the \u201cprevious token head\u201d writes information about the previous token to each token, and the \u201cinduction head\u201d uses this information to identify a token that should follow each token. Such a function is implemented by two atomic operations: (1) copy the representation of the previous token [A] to the next token [B], and (2) retrieve and copy similar representations on [A] to the current token [A\u2032]. Concisely, it performs inference in the form of [A][B] . . . [A\u2032] \u21d2[B], which is similar to ICL-styled data. Therefore, this circuit has been widely used to explain the inference dynamics of ICL (Wang et al., 2023) and the emergence of ICL during pre-training (Olsson et al., 2022; Reddy, 2024; Singh et al., 2024b). Despite their valuable insights,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dbab/dbab519e-ea8a-49b7-810b-2cdd01b54a1a.png\" style=\"width: 50%;\"></div>\ntheir experiments rely on a synthetic setting: using simplified models and well-embedded (linearly separable) inputs, which differs from the practical ICL scenario using real-world LMs with many layers and complicated inputs. We bridge the gap between the synthetic and real-world settings, providing more detailed explanations for the inference dynamics of ICL based on real-world LLMs.\n# 2.2 EXPERIMENT SETTINGS\nModels. We mainly conduct experiments on 4 modern LMs: Llama 3 (8B, 70B) (AI@Meta, 2024), and Falcon (7B, 40B) (Almazrouei et al., 2023). Unless specified, we report the results on Llama 3 70B, since its deep and narrow structure (80 layers, 64 heads) makes it easier to show hierarchical inference dynamics (discussed in \u00a75.2). The results of other models can be found in Appendix F.2. Datasets. We build ICL-formed test inputs from 6 sentence classification datasets: SST-2 (Socher et al., 2013), MR (Pang & Lee, 2005), Financial Phrasebank (Malo et al., 2014), SST-5 (Socher et al., 2013), TREC (Li & Roth, 2002; Hovy et al., 2001), AGNews (Zhang et al., 2015)specified, we report the average results on these datasets. Others. Unless specified, we use k = 4 demonstrations in ICL inputs. For each dataset, we randomly sample n = 512 test data and assign one order-fixed demonstration sequence for each test sample. About the prompt templates, etc., please refer to Appendix A.1.\nThis section mainly confirms that LMs construct task-relevant and linearly separable semantic representations for every input text (demonstrations and queries) in the hidden states. Such linear representations are an important foundation for explaining the dynamics of ICL based on induction head, since attention-based feature retrieval, a key mechanism of induction head, can be easily done on linear representations. Current successful studies on simplified models and inputs (Chan et al., 2022; Reddy, 2024; Singh et al., 2024b) also assume the existence of such linear representations. Moreover, we confirm some interesting properties of the input text representations: (1) It is based on the capacity in the model weights and can be enhanced by demonstrations in context (Fig. 2 (Middle, Right)). (2) The similarity of representations is biased towards the encoding target\u2019s position.\n# 3.1 LLMS SUMMARIZE INPUT TEXT ON FORERUNNER TOKENS IN HIDDEN STATES\n3.1 LLMS SUMMARIZE INPUT TEXT ON FORERUNNER TOKENS IN HIDDEN STATES\nWe first study the existence of input text encoding in hidden states and then explain their linear separability and task relevance in \u00a73.2. For each text-label pair (xt, yt) (encoding target) sampled from the datasets, we prepend them with k demonstrations, resulting in ICL-style inputs [x1][s1][y1] . . . [xk][sk][yk][xt][st][yt]. These inputs are then fed into an LM to extract the hidden states of a specific token in [xt][st][yt] from each layer, serving as the ICL inner representations. To assess the quality of these representations as sentence representations, we use the sentence embedding of xt encoded by BGE M3 (Chen et al., 2024), a SotA encoder-only Transformer, as a \u201creference\u201d representation and then calculate the mutual nearest-neighbor kernel alignment3 (Huh et al., 2024) between these representations. See Appendix A.1 and A.2 for details. Forerunner Tokens Encode Input Text Representations. We plot the kernel alignment using 3 types of tokens in Fig. 2 (Left). The forerunner token, while often overlooked in previous work, produces the best input text-encoding, emerging in the early phase (layers 0-28) of the inference process, and keeping a high level to the end of inference. Interestingly, hidden states of label words are not satisfactory input text representations even with a high background value (the result at layer 0, refer to Appendix A.2.1 for details), which is a critical supplement to previous work which suggests the label tokens are pivots for collecting the information of demontrations (Wang et al., 2023). Input Text Encoding is Enhanced by Demonstrations. We investigate the influence of contextual in on input text encoding by repeating the experiments with different k. As shown in Fig. 2 (Middle), when the demonstrations increase, feature alignment is enhanced, which is counterintuitive since longer preceding texts are more likely to confuse encoding targets. Such findings indicate that\n3Intuitively, kernel alignment measures similarity between two representations toward the same datasets, and according to Huh et al. (2024), a higher cross-model kernel alignment usually means a better representation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a64b/a64b0c0d-e6b5-4d41-a7bf-9caa13881ad5.png\" style=\"width: 50%;\"></div>\nFigure 2: Input text encoding magnitudes (metricized by kernel alignment against feature encoded by encoder-structured models) of hidden states in various layers in ICL scenario (The controlled experiments are results between current 6 datasets and TEE (Mohammad et al., 2018)). Left: Encoding magnitudes on hidden states from various types of token. Middle4: Encoding magnitudes with different k on the forerunner tokens. Right: Encoding magnitudes in layer 24 of Llama 3 70B against the causal language modeling loss of the input text with (upper) k = 0 and (lower) k = 8.\nLMs (1) utilize contextual information to enhance the input text encoding and (2) correctly segment different demonstrations (detailed operation discussed in Appendix C). Perplexed Texts are Encoded Worse. We investigate the correlation between kernel alignment and the perplexity of encoding targets with different k. Fig. 2 (Right, upper) shows a negative correlation for k = 0, that is, LMs generate poorer encodings for more complex input text when no demonstrations are given, which can be identified as an In-weight Learning (IWL) property of the inner text encoding. While, when demonstrations are given in context (Fig. 2 (Right, lower)), the negative correlation disappears, which suggests that LMs effectively encode more complex samples with the help of demonstrations in context. More discussion about the relationship of classification performance and perplexity is in Appendix E. The above findings suggest that the inner text encoding is a hybrid process of ICL and IWL: Basic encoding capability presents from LMs weights, and is enhanced by demonstrations in context, which can be a clue to how demonstrations help ICL. Moreover, we are about to illustrate that these encodings are sufficiently informative for ICL tasks and linearly separable, which meets the presumption of simplified models on the linear and well-embedded input features.\nInput Text Encoding is Linear Separable and Task-relevant. We train a centroid classifier on hold-out 256 input samples (Cho et al., 2024), using the hidden states of a specific token in [xt][st]5 from each layer and then predict the label yt (see Appendix A.3 for details). The results are shown in Fig. 3. The considerably high classification accuracy of the forerunner token suggests the high linear separabilities of the hidden states in the task-semanticrelevant subspaces since the centroid classifier is linear. In addition, a similar emerging trend in accuracy and kernel alignment confirms the reliability of the kernel alignment measurement.\nInput Text Encoding is Biased towards Position. Ideally, the inner representations of similar queries should be highly similar regardless of their position in ICL inputs to support attention-based operations for classification. To verify this, for each encoding\ntarget, we extract the hidden states of forerunner tokens with various numbers of preceding demonstrations. We then calculate the cosine similarity between all possible pairs of the hidden states for the same target or different targets. As shown in Fig. 4, although the overall similarities on the same target are higher than on the different targets, they are both especially higher when their positions are close to each other. As to be discussed in \u00a75.3, such positional similarity bias may lead to one\n4Experiments of Fig. 2 (Middle) on Llama 3 70B do not involve results on AGNews. 5We skip the experiments on label tokens because of the leakage of ground-truth label information.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1fb5/1fb53563-792e-4b2b-8485-05c2e2f5fd8e.png\" style=\"width: 50%;\"></div>\nFigure 3: Test results of centroid classifier trained on ICL hidden states. Solid: Centroid classification accuracy, Dotted: Kernel alignment.\nflaw: demonstrations closer to the query have stronger impacts on ICL (Zhao et al., 2021; Lu et al., 2022; Chang & Jia, 2023; Guo et al., 2024). The principle of such bias is discussed in Appendix C.\n4 INDUCTION CIRCUITS IN NATURAL LANGUAGE MODELS\nThis section mainly shows how LMs utilize the summarized linear text representations in induction circuits with a typical 2-step form (Singh et al., 2024b): Forerunner Token Heads merge the demonstration text representations in the forerunner token into their corresponding label tokens with a selectivity regarding the compatibility of demonstrations and label semantics. Induction Heads copy the information in the label representations similar to the query representations back to the query. Such operations are done on task-specific subspaces, enabling LMs to solve multiple tasks by multiplexing hidden spaces.\nThis subsection mainly examines and measures the forerunner token heads, which copy the information in the forerunner into label tokens. We investigate the interaction between the forerunner tokens and label tokens, and focus on how the representations are merged, especially when the semantics of labels and text are disjoint, towards an explanation of why ICL are robust to wrong labels. Text Representations are Copied to Label Tokens. To confirm the existence of the representation copy process, we start by calculating the kernel alignment between the hidden state of forerunner token st at layer l (the copy source) and that of label token yt at layer (l + 1) (the copy target). To suppress the high background values caused by the semantics of labels, we use abstract label tokens {\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, . . . } instead of the original label tokens. The results are shown in Fig. 5 (Left), where the kernel alignment between the hidden states of the label token and the forerunner token gradually increases and then bumps up after the encoding in the forerunner token (described in \u00a73) finished improving. It indicates that the hidden states of the input text representation encoded in the forerunner tokens are merged into their label tokens, suggesting the existence of copy processing from the forerunner token to the label token. Text Representations are Copied without Selectivity. For each attention head, we extract the attention score \u03b1yt\u2192st from a label token yt (as attention query) to the corresponding forerunner token st (as attention keys). We then mark the head with \u03b1yt\u2192st \u2a7e5/nt (nt: the length of tokens before yt) as a Forerunner Token Head and count them in each layer. The results are shown in Fig. 5 (Middle, \u201cCorrect Label\u201d), where the peak matches the copy period in Fig. 5 (Left). Moreover, to investigate the influence of the correctness of label tokens, we replace yt with a wrong label token6, where the results in Fig. 5 (Middle, \u201cWrong Label\u201d) are almost identical to the correct-label setting, suggesting that the forerunner token heads don\u2019t show selectivity toward the semantic consistency between input text and labels simply merge the input text representations into the label tokens. Hidden States of Label Tokens are Joint Representations of Text Encodings and Label Semantics. Given the findings above, we probe the content of hidden states of label tokens [yt], i.e., how the text representation interacts with the original label semantics. We first train two centroid classifiers to predict the corresponding label yt: (1) Cf trained on the hidden states of forerunner tokens [st] and (2) Cl trained on the hidden states of label tokens [yt]. To check whether the label tokens include the information of forerunner tokens, we use Cf to predict the label on the hidden state of label token [yt] in Fig. 5 (Right, solid). It shows that, during the copy processing, high classification accuracies can be achieved both on the correct label tokens and wrong label tokens, suggesting that the text features in the forerunner tokens can be partly and linearly detected in the label tokens. Moreover, results using Cl (dotted line) shows extreme results, suggesting the label information remains in the label token. So, we can conclude that: hidden states of label tokens are joint representations of label semantics and text representations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e2a/1e2ac811-e4fa-41c0-8bd1-e5024e7035af.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The similarities of ICL hidden states in different positions on layer 24 between the same queries (Left) or two different queries (Right) (on SST-2).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea9b/ea9b40be-02cc-4ce1-9b4d-1107574d6a87.png\" style=\"width: 50%;\"></div>\nFigure 5: Hidden states copy magnitude from forerunner tokens to label tokens against layers. Left: Kernel alignment between the forerunner token (the copy source) and the label token of the next layer (the copy target). Middle: Curves: The count of marked forerunner token heads with correct and wrong labels; Colored Areas: The maximum attention scores from forerunner token to query (copy magnitude) with correct and wrong labels (detailed attention head statistical data is in Appendix F.1). Right: Centroid classifier results predicted on the hidden states of correct and wrong label tokens, on SST-2 and MR. (Solid: Predicted by classifiers Cf trained on hidden states of forerunners. Dotted: Predicted by classifiers Cl trained on hidden states of label tokens.)\nLabel Denoising is Conducted on the Overlap of Label Semantics and Text Representations. Notice that in Fig. 5 (Right, solid), compared to the typical results predicted on the forerunner tokens, accuracies are improved on the correct label and suppressed on the wrong label, which suggests that information consistent with the label semantics is easier to be enhanced by the label tokens and vice versa, showing a feature selectivity on the consistency between the text representations and the label semantics. Given the observation that the information on label semantics and text features can be extracted separately and linearly, we can confirm that these two kinds of information are located in different sub-spaces of the hidden states, and linearly merged by the attention operation of forerunner heads. Moreover, given the fact that there is no selectivity is observed in the copy behavior of the forerunner token head (Fig. 5 (Middle)), it is intuitive that the feature selectivity shown in Fig. 5 (Right) comes from the arithmetical interaction of feature vectors on the overlap of sub-spaces between the label semantics and text features, making ICL stable against label noise (Min et al., 2022). Moreover, as mentioned by Wei et al. (2023), large models show poorer stability against label noise. We infer that the larger hidden dimensions in larger models lower the overlap between the sub-spaces of label semantics and text representations to reduce the interaction.\n# TEP 3, INDUCTION HEAD: FEATURE RETRIEVAL ON TASK SUBSPACE\nThis subsection examines the existence of the aforementioned induction heads, which retrieve similar label token features as the queries\u2019 forerunner feature, and copy the retrieved features back to the query. We claim the necessity of multi-head attention in this process: correct feature retrieval can only be conducted on the subspace of the hidden space, which is captured by some attention heads. Induction is Correct in Minority Subspaces. Similar to Fig. 5 (Middle), we mark (1) the attention heads with the sum of attention scores from the query\u2019s forerunner token sq (as attention query) to all the label tokens [y1], . . . , [yk] (as attention keys) in the demonstration more than 5k/nt as induction heads, and (2) attention heads with the sum of scores to all the correct label tokens more than 5k/|Y|nt as correct induction heads (Y is a label space). We show the number of both kinds of induction heads in Fig. 6 (Left, detailed head statistics in Appendix F.1), where a unimodal pattern is observed later than the copy processing of Step 2. Moreover, more than half of the induction heads are not correct ones, suggesting that task-specific feature similarity can only be caught on some induction subspaces (defined by low-rank transition matrix W h\u22a4 Q W h K of correct induction head h). We enhance this claim in Fig. 6 (Middle) (details in Appendix A.4), where both vanilla attention (without transformation and head split) and attention scores averaged among heads show low assignment on correct label tokens, while some heads show considerable correctness. Considering the average value, the majority of attention heads almost randomly copy label token information to the query, causing the prediction biased to the frequency of labels in the prompt (Zhao et al., 2021). As the reason, we infer that the hidden states are sufficient (Fig. 3) but not minimum for ICL, where redundant information interferes with the similarity calculation of attention.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33dc/33dcd3c6-9ec8-45d1-a2b5-25f7e43cc7cc.png\" style=\"width: 50%;\"></div>\nFigure 6: Measurements for induction heads. Left: The count of marked induction heads and correct induction heads against layers. Middle: The correctness of attention assignment (the sum of assignment towards correct label tokens normalized by assignment towards all the label tokens). (Vanilla Attention: attention scores directly calculated on full dimensionality. Head Average: the averaged attention scores among all the heads. Best Ind. Head: the scores of attention head with the most correctness.) Right: The correct induction heads overlap of all dataset pairs.\nSome Induction Subspaces are Task-specific. We check if different tasks share the same induction subspaces based on the overlap of the correct induction heads across different datasets. Given nD(h), the number of times h is marked as correct induction head on dataset D, the overlap rate S is defined as:\n(1)\n  \ufffd The results are shown in Fig. 6 (Right), where: (1) A significant overlap of induction heads indicates that a part of correct induction heads is inherent in the model, built by the pre-training process (Reddy, 2024; Singh et al., 2024b). (2) Such overlap is not fully observed, suggesting that some induction subspaces are task-specific: input texts evoke taskspecific attention in induction heads, enabling the\nanisotropy multiplex of different sub-spaces in the hidden spaces to transmit relevant information for various tasks. Therefore, we can restore ICL to implicit end-to-end multi-task learning methods with hidden state multiplexing since they also use various task heads on common bottom network layers and informative hidden state (Zhang & Yang, 2021).\nDemonstration Saturates on Induction Subspace. We visualize the demonstrations\u2019 label token representations mapped on the induction subspaces by the transition matrix W h\u22a4 Q W h K and principal component analysis in Fig. 7, indicating: (1) Compared to a correct induction head, a wrong induction head is easier to map label representations linearly inseparably. (2) In the early stage of demonstration (k = 1 \u21922), when a new demonstration is given, the morphology of attention assignment towards query changes significantly (shown as background color in Fig. 7; see \u00a7A.5 for details), while in the late stage (k = 15 \u219216), attention assignment morphology is stable. This can explain the demonstration saturation (Agarwal et al., 2024; Bertsch et al., 2024): the performance is submodular against the demonstrations. Intuitively, since demonstrations follow a prior distribution, representation of a new demonstration is likely to be located within the closure of existing demonstrations, making it less contributable to the attention assignment in the induction subspace.\n# 5 PUTTING THINGS TOGETHER\nSo far, we have revealed the existence of the circuit with 3 steps, organized by the sequential inference process among Transformer layers. In this section, we find that the circuit is dominant in the ICL inference, while some bypass mechanisms activated by residual connection assist ICL inference. Moreover, a series of phenomena observed in ICL is successfully explained by the circuit.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2128/2128080d-d0a4-4520-86a0-18aaacff93db.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Label representations of k demonstrations visualized on (Left 4) correct and (Right 4) wrong induction head, on one sample of SST-2 (see Appendix F.3). \u25e6: \u201cpositive\u201d label, \u00d7: \u201cnegative\u201d label, \u25b2: zero vector. Color: attention assigned to query, negative to positive (cartography: Appendix A.5).</div>\nFigure 7: Label representations of k demonstrations visualized on (Left 4) correct and (Right 4) wrong induction head, on one sample of SST-2 (see Appendix F.3). \u25e6: \u201cpositive\u201d label, \u00d7: \u201cnegative\u201d label, \u25b2: zero vector. Color: attention assigned to query, negative to positive (cartography: Appendix A.5).\nTable 1: Accuracy variation (%) with each inference step ablated on Llama 3 8B. Small numbers are controlled results of randomly ablating equivalent amounts of connections. Ablations are applied from the bottom to the top layers, and results with various ablated layers are reported.\nTo demonstrate that our 3-phase circuit dominates or at least participates in ICL process, we disconnect the related attention connection of each step in the proposed circuit, and test the accuracies without such connections as shown in Table 1. The results show that when the nontrivial connections designated by the proposed circuit are ablated, accuracies of ICL significantly decrease, supporting the existence of our circuit. However, the result doesn\u2019t fully match expectations, for example, the result without induction (line 5) should be consistent with zeroshot (line 6), since all the expected communication from demonstration to query is intercepted, but that\u2019s not the case; and the contribution of Step 1 in later layers are unexpectedly high, indicating the existence of some bypass mechanisms parallelly contributing to ICL accuracies.\n#\nAttention Disconnected\nKey \u2192Query\nAffected Layers Ratio (from layer 1)\n25%\n50%\n75%\n100%\n1\nNone (4-shot baseline)\n\u00b10 (Acc. 68.55)\n\u2013 Step1: Summarize \u2013\n2 Demo. Texts xi \u2192Forerunner si\n\u22124.98\n\u22120.85\n\u221215.82\n\u22121.33\n\u221223.43\n\u22124.65\n\u221230.60\n\u22121.50\n3 Query Texts xq \u2192Forerunner sq\n\u221213.87\n\u22120.13\n\u221221.10\n\u22120.10\n\u221224.74\n\u22120.68\n\u221228.38\n\u22120.62\n\u2013 Step2: Semantics Merge \u2013\n4 Demo. Forerunner si \u2192Label yi\n\u22122.24\n+0.07\n\u22123.45\n\u22120.20\n\u22123.39\n+0.10\n\u22123.42\n\u22120.26\n\u2013 Step3: Feature Retrieval & Copy \u2013\n5 Label yi \u2192Query Forerunner sq\n\u22125.14\n+0.07\n\u221210.03\n\u22120.07\n\u221211.36\n\u22120.03\n\u221210.22\n\u22120.03\nReference Value\n6\nZero-shot\n\u221217.90 (Acc. 50.65)\n7\nRandom Prediction\n\u221236.05 (Acc. 32.50)\n5.2 BYPASS MECHANISM\n# 5.2 BYPASS MECHANISM\nMotivated by the ablation results, we believe that several mechanisms including our circuit run parallelly for ICL, since the residual connection supports complex paths among layers and attention heads. We list some possible bypasses and plan a complete enumeration as future work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/915f/915f10c7-5a26-46cc-aaa8-0504e5676898.png\" style=\"width: 50%;\"></div>\nParallel Circuits. Multiple 3-step circuits can execute in parallel, that is, one layer can assign multiple inference functions to different heads, causing dispersion and deserialization as shown in Fig. 8, where a narrow (fewer heads) and deep (more layers) model is more likely to generate localized inference, and vice versa.\nParallel Circuits. Multiple 3-step circuits can execute in parallel, that is, one layer can assign multiple inference functions to different heads, causing dispersion and deserialization as shown in Fig. 8, where a narrow (fewer heads) and deep (more layers) model is more likely to generate localized inference, and vice versa. Direct Decoding. Residual connection to output embedding allows intermediate hidden states to be decoded directly. Intuitively, a shortcut from Step 1 encodings to the LM head enables ICL with zero-shot capacities, since we\nDirect Decoding. Residual connection to output embedding allows intermediate hidden states to be decoded directly. Intuitively, a shortcut from Step 1 encodings to the LM head enables ICL with zero-shot capacities, since we\n<div style=\"text-align: center;\">Figure 8: Dynamics and deserialization of magnitudes of proposed 3 inference steps (cartography details: Appendix A.6).</div>\nhave confirmed that encoded representations are informative for ICL tasks, while the decoding methods should be selected carefully (Cho et al., 2024) (e.g. with essential calibration). On the other hand, shortcuts from insufficiently encoded features may lead to meaningless information decoded by language model heads, causing prediction bias, i.e., even if no query is given, ICL still returns unbalanced results (Zhao et al., 2021) decoded from tokens of prompt template (see Appendix D).\nShortcut Induction. Note that a k-shot ICL input sequence always contains a (k \u22121)-shot sequence, where the k-th forerunner (once served as the query) is previously processed. So every forerunner can directly retrieve previously pro-\nAttention Disconnected\n25%\n50%\n75%\n100%\nForerunner s1:i \u2192Forerunner si:q\n\u22121.30\n\u22120.00\n\u22120.75\n\u22120.16\n\u22120.78\n\u22120.16\n\u22121.56\n\u22120.71\ncessed forerunners of demonstrations to copy their induction results directly. The non-trivial resu (Table 2) with all forerunners disconnected from each other confirms such infer.\nDifficulty-based Demonstration Selection. In \u00a73.1, we find that in the zero-shot scenario, perplexed texts are harder to be encoded, which explains the observation of PPL-ICL (Gonen et al., 2023) selecting demonstrations with lower perplexity. Moreover, while the demonstrations increase, LMs can encode more complex inputs with diverse information to update the attention assign-\n<div style=\"text-align: center;\">Table 2: Accuracy drop with shortcut ablated.</div>\nment shown in Fig. 7, making it beneficial to input harder demonstrations later, which explains the ICCL (Liu et al., 2024), which build demonstrations sequence from easy to hard.\nthe ICCL (Liu et al., 2024), which build demonstrations sequence from easy to hard. Prediction Bias. (1) Contextual Bias: As shown in \u00a75.2 and Appendix D, direct decoding insufficiently encoded information adds meanless logits into LM\u2019s output, causing a background prediction value even if no queries are given (named bias). (2) Position Bias: As shown in \u00a73.2, closer input texts are encoded more similarly, so label tokens near the query have more similar information to the query, causing more attention assignment in the induction processing, so that more influences on the prediction. (3) Frequency Bias: As shown in \u00a74.2, in the induction, some attention heads are without selectivity towards labels, causing an averaged induction from label tokens to the query, triggering a prediction bias towards the label frequency in the demonstration, even if their contribution (absolute value of attention score on label tokens) is small. All three biases are observed by Zhao et al. (2021), and can be removed by ICL calibration methods. The Roles and Saturates of Demonstrations. It is well known that demonstrations improve the performance of ICL. We decompose such performance improvement into 2 parts: (1) demonstrations help early layers encode better (\u00a73.1), and (2) more demonstrations provide larger label token closure, enabling more accurate attention assignment (\u00a74.2), while the volume of such closure is submodular to demonstrations, causing the saturates of ICL performance towards demonstrations. The Effect of Wrong Label. It is well-known that the label noise is less harmful in ICL (Min et al., 2022) than in gradient-based learning (Zhang et al., 2021). We have explained in \u00a74.1 that ICL implies labels denoise to stabilize ICL against label noise, while weakened by dimensionality.\n# 6 CONCLUSION AND DISCUSSION\nConclusion. In summary, this paper restores ICL inference into 3 basic operations and confirms their existence. Careful measurements are conducted to capture and explain various phenomena successfully. Moreover, ablation studies show the proposed inference circuit dominates and reveals the existence of bypass mechanisms. We hope this paper can bring new insight into ICL practice.\nThe Role of Early and Later Layers. Our framework and Fig. 3 show: encoding result of Step 1 can be directly used for classification with reliable decoding, and later transformer layers are not contributing to centroid classification accuracies, leading to a taxonomy of Encoding for Step 1 and Output Preparation for Step 2 and 3. LMs complete multi-task classification implicitly in early layers, and verbalize it by merging task-specific la-\nbel semantics in later layers. Therefore, we suggest removing some top layers and using a centroid classifier (Cho et al., 2024) to accelerate ICL inference as shown in Table 3 and Appendix D.\nPre-training Possibility from Natural Language Data. A large gap can be considered between such a precise circuit and gradient descent pre-training on the wild data. However, we believe the wild training target contains the ICL circuit functionally. Based on the previous works finding trainability of ICL on linear representation-label pairs (Chan et al., 2022; Reddy, 2024; Singh et al., 2024b), we speculate that in early training step, Transformers learn to extract linear representations shown in \u00a73 from wild data (Appendix B), serving as the training input of later layers to evoke the emergence of induction heads with the same mechanism shown in aforementioned previous works. Moreover, our conclusion of Step 3 highlights the input data requirements for the later layers: These data should activate the multiplex of hidden space, i.e., it should implicate multi-task classification with a wide distribution, which is consistent with the aforementioned previous works. Limitations. (1) These 3 basic operations are not functionally indivisible. Ideally, one can reduce every operation in ICL inference to the interconnection of special attention heads to ulteriorly examine how the operating subspaces interact between steps. (2) Even though we have some insights into how pre-training promotes ICL in natural language models, its detailed dynamics are still unknown. One can start by decomposing pre-training targets into implicit tasks, and examine how these tasks can evoke the occurrence of the 3-step inference operations. (3) We only focus on classification tasks, while we believe that our findings can be applied to non-classification tasks, efforts are still needed to fill the gap.\n<div style=\"text-align: center;\">Table 3: Performance of full and layerpruned ICL inference.</div>\nInference\nAcc.\nPara.#\nSpeed\nFull + LM Head\n66.19%\n70.6B\n1\u00d7\nFull + Cent.\n83.24%\n69.5B\n1.00\u00d7\nLayer34 + LM Head\n49.29%\n32.7B\n2.16\u00d7\nLayer34 + Cent.\n84.27%\n31.2B\n2.38\u00d7\nREFERENCES\nRishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Luis Rosias, Stephanie CY Chan, Biao Zhang, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning. In ICML 2024 Workshop on In-Context Learning, 2024. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397\u20132430. PMLR, 2023. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022. Ting-Yun Chang and Robin Jia. Data curation alone can stabilize in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8123\u20138144, 2023. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, and Naoya Inoue. Tokenbased decision criteria are suboptimal in in-context learning. arXiv preprint arXiv:2406.16535, 2024. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 10136\u201310148, 2023. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4849\u20134870, 2023. Qi Guo, Leiyu Wang, Yidong Wang, Wei Ye, and Shikun Zhang. What makes a good order of examples in in-context learning. In Findings of the Association for Computational Linguistics ACL 2024, pp. 14892\u201314904, 2024.\nChi Han, Ziqi Wang, Han Zhao, and Heng Ji. Explaining emergent in-context learning as kernel regression. arXiv preprint arXiv:2305.12766, 2023a. Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. Understanding in-context learning via supportive pretraining data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12660\u201312673, 2023b. Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. URL https://www.aclweb.org/ anthology/H01-1069. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. Hong Jun Jeon, Jason D Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic analysis of in-context learning. In Forty-first International Conference on Machine Learning, 2024. Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329, 2023. Jannik Kossen, Yarin Gal, and Tom Rainforth. In-context learning learns label relationships but is not conventional learning. In The Twelfth International Conference on Learning Representations, 2024. Xiaonan Li and Xipeng Qiu. Finding support examples for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 6219\u20136235, 2023. Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. URL https://www.aclweb.org/ anthology/C02-1150. Jinlong Liu, Guoqing Jiang, Yunzhi Bai, Ting Chen, and Huayan Wang. Understanding why neural networks generalize well through gsnr of parameters. arXiv preprint arXiv:2001.07384, 2020. Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. Let\u2019s learn step by step: Enhancing in-context learning ability with curriculum learning. arXiv preprint arXiv:2402.10738, 2024. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, 2022. P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65, 2014. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11048\u201311064, 2022. Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. Semeval2018 task 1: Affect in tweets. In Proceedings of the 12th international workshop on semantic evaluation, pp. 1\u201317, 2018. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Jane Pan. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. Master\u2019s thesis, Princeton University, 2023.\nBo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pp. 115\u2013124, 2005. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In The Twelfth International Conference on Learning Representations, 2024. Aaditya Singh, Stephanie Chan, Ted Moskovitz, Erin Grant, Andrew Saxe, and Felix Hill. The transient nature of emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 36, 2024a. Aaditya K Singh, Ted Moskovitz, Felix Hill, Stephanie CY Chan, and Andrew M Saxe. What needs to go right for an induction head? a mechanistic study of in-context learning circuits and their formation. arXiv preprint arXiv:2404.07129, 2024b. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 9840\u20139855, 2023. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sanggoo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2422\u20132437, 2022. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013 115, 2021. Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015. Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609, 2021. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pp. 12697\u201312706. PMLR, 2021. Bowen Zheng, Ming Ma, Zhongqiao Lin, and Tianming Yang. Distributed rule vectors is a key mechanism in large language models\u2019 in-context learning. arXiv preprint arXiv:2406.16007, 2024.\n<div style=\"text-align: center;\">Table 4: Prompt templates used in this paper.</div>\nDataset\nPrompt Template (Unit)\nLabel Tokens\nSST-2\nsentence:\n<input sentence> sentiment:\n<label token> \\n\nnegative, positive\nMR\nreview:\n<input sentence> sentiment:\n<label token> \\n\nnegative, positive\nFP\nsentence:\n<input sentence> sentiment:\n<label token> \\n\nnegative, neutral, positive\nSST-5\nsentence:\n<input sentence> sentiment:\n<label token> \\n\npoor, bad, neutral, good, great\nTREC\nquestion:\n<input sentence> target:\n<label token> \\n\nshort, entity, description, person, location, number\nAGNews\nnews:\n<input sentence> topic:\n<label token> \\n\nworld, sports, business, science\n# A EXPERIMENT DETAILS AND SETTINGS\nA.1 DETAILED OVERALL EXPERIMENTAL SETTINGS\nPrompt Template. We conduct experiments on a specific prompt template for each dataset as shown in Table 4. Moreover, similar to typical ICL practices, we reduce the label into one token to simplify the prediction decoding. The reduced label tokens are also shown in Table 4. Quantization. In our experiments, we use BitsAndBytes7 to quantize Llama 3 70B and Falcon 40B to INT4. For the other models, full-precision inference is conducted. Other. All the experiment materials (models and datasets) are loaded from huggingface. For the BGE M3, we use its pooler output as the output feature.\nIn this paper, we need to measure the similarity between features from two different models or model layers. There are many approaches (Klabunde et al., 2023), and we use mutual nearestneighbor kernel alignment (Huh et al., 2024), which is relatively efficient and accurate, calculated as follows to measure the similarity of representation from the same object set X = {xi}n i=1 in different feature spaces. Given a representation mapping \u03b4 : X \u2192Hd from the objects to a space where similarity measurement \u27e8\u00b7, \u00b7\u27e9: Hd \u00d7 Hd \u2192R is defined, we can calculate the similarity map from dataset X as S\u03b4 \u2208Rn\u00d7n, where the elements are S\u03b4|i,j = \u27e8\u03b4 (xi) , \u03b4 (xj)\u27e9, especially, we axiomatic define \u27e8x, x\u27e9= 1, so we set the diagonal element S\u03b4|i,i \u22520 since they are trivial values. Given two encoding \u03b41 and \u03b42, two similarity map can be calculated as S\u03b41 and S\u03b42 on the same object set X. For each line vector index i = 1, 2, . . . , n in S\u03b41, we select the index of top-k elements from greater to lower as topk \ufffd S\u03b41|i \ufffd . Similarly, we get topk \ufffd S\u03b42|i \ufffd from S\u03b42. Then, we calculate the kernel alignment for sample i as:\n XX Implementation. In our experiments, we choose cosine similarity as the \u27e8\u00b7, \u00b7\u27e9, and k \u225264. According to experiment settings in \u00a72.2, n \u2252512 is defined, and a randomlized matrix S have KA = 64/512 = 0.125 as the random baseline.\nGiven two specific tokens xi and xj where kernel alignment is calculated from different ICL-styled input sequences pi and pj, in a specific layer of a decoder Transformer, the representations can be written as \u03b4(x) = e(x) + \u03f5(p), where e(x) are the embedding vector of the token x, and \u03f5(p) are the residual side-flow w.r.t the context p. Intuition. As shown in Fig. 9, in the hidden states of ICL, the hidden state on the label token has a prior clustering, making it naturally similar to the representation generated by the encoder model, even if the ICL process does not encode it sufficiently. So, at layer 0, since the model is not able to\nGiven two specific tokens xi and xj where kernel alignment is calculated from different ICL-styled input sequences pi and pj, in a specific layer of a decoder Transformer, the representations can be written as \u03b4(x) = e(x) + \u03f5(p), where e(x) are the embedding vector of the token x, and \u03f5(p) are the residual side-flow w.r.t the context p.\n(2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a8a7/a8a727c5-f25a-4e25-aaef-8b5db61dddf0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Distributions (clusters) of representations generated by: Left: encoder model (BGE), clustering w.r.t. the label; Middle: ICL on the forerunner token, where representations gather into one point when no Transformer operation is conducted; Right: ICL on the label token, where representations gather into points w.r.t. label (using a 2-way example) when no Transformer operation is conducted, causing a high background value.</div>\nFigure 9: Distributions (clusters) of representations generated by: Left: encoder model (BGE), clustering w.r.t. the label; Middle: ICL on the forerunner token, where representations gather into one point when no Transformer operation is conducted; Right: ICL on the label token, where representations gather into points w.r.t. label (using a 2-way example) when no Transformer operation is conducted, causing a high background value.\nperform any encoding in this layer, the kernel alignment on the forerunner is on a random baseline value of 0.125, but the value on the label token will be greater. In the case where (1) BGE generates fully linearly separable clusters with sufficient inter-cluster distance, and (2) the number of samples for each label is not less than 64, this upper bias can be expected to be 0.125(|Y|\u22121) (proof omitted). Intuitively, such bias can propagate along the residual network to every layer, making the results of any layer unfaithful. Similarity on Forerunner Tokens. The cosine similarity between \u03b4(xi) and \u03b4(xj), where the xi and xj are forerunner tokens of the input sequence can be written as:\n  Denote Bi,j = \u27e8e(xi), \u03f5(pj)\u27e9+ \u27e8e(xj), \u03f5(pi)\u27e9, Ci,j = \u2225e(xi) + \u03f5(pi)\u22252 \u2225e(xj) + \u03f5(pj)\u22252, and notice that xi = xj since they are forerunner tokens which is kept consistent in experiments, we have:\nSimilarity on Label Tokens. Similarly, the cosine similarity between \u03b4(yi) and \u03b4(yj) on labe tokens can be written as:\nimilarity on Label Tokens. Similarly, the cosine similarity between \u03b4(yi) and \u03b4(yj) on label okens can be written as:\nEncoding on Label Tokens Enhances the Similarity with Same Labels. Given k = 1 for simplicity, the probability top1 \ufffd S\u03b4|i \ufffd selects a sample with the similar label with i-th sample on the forerunner token can be written as:\n\u27e8\u27e9 where the ryi is the label ratio of yi. Similarly, the probability on the label token can be written as: P \ufffd y = y \ufffd\n(3)\n(5)\n(6)\n(7) (8)\n(9)\nThat is, the inputs with the same labels with xi are easier to be selected into the top1 \ufffd S\u03b4|i \ufffd . Notice that we make approximations here: (1) we consider the Ej|yi=yj [Ci,j] \u2248Ej [Ci,j], i.e., the 2-norm of two encoding vectors are considered equal granted by normalization used in Transformer. (2) We consider the context term c in the label token scenario the same as the forerunner scenario since the difference is only a label token, which usually occupies quite a small part of the input sequence. Background Values of Kernel Alignment. According to the explanation above, it is intuitive to conclude that topk \ufffd S\u03b4|i \ufffd from label tokens is easier to cluster samples with the same label as yi. Moreover, a well-pre-trained encoder can catch the prior distribution determined by these labels, and also cluster samples with the same label, causing a high similarity of similarity map, so that a high but unfaithful kernel alignment as the background value from the similarity on e(y) but not \u03f5(p). An intuitive verification of such background value is shown in the Fig. 2 (Left), where the \u201cLabel Token\u201d curve has a high value in layer 0 with \u03f5(p) = 0. However, the background value also indicates that the representation generated by BGE correctly clusters the samples, confirming its reliability.\n# A.3 TRAINING AND INFERENCE OF CENTROID CLASSIFIER\nIn this paper, we follow Cho et al. (2024) to train centroid classifiers as a probe toward hidden states of LMs. In detail, given the LM\u2019s hidden states set \ufffd hl i \ufffdm i=1 of the selected tokens (according to the experimental setting, the last label token or forerunner token) in layer l from an [input prompt][query label] set Z = {(pi, yi)}m i=1, where the labels are limited in label space Y, in the training phase, we calculate the centroid of the hidden state \u00afhl y for each label respectively:\n\ufffd \ufffd In the inference phase, we extract the equitant8 hidden state hl t as the training phase from the test input, and calculate the similarity between hl t and the centroids calculated above. Then, we choose the label of the most similar centroids as the prediction:\n\ufffd \ufffd Implementation. In our experiments, we set training sample number m \u2252256, similarity functi \u27e8a, b\u27e9= \u2212\u2225a \u2212b\u22252.\nIn Fig. 6 (Middle), we define a Correct Label Assignment, here we introduce how this measuremen is calculated. Suppose we have an attention score AW,f(K, Q) calculated as:\n\ufffd \ufffd with hidden dimensionality of d, give a certain layer, K \u2208Rd\u00d7nt is the hidden state matrix of full context, Q \u2208Rd\u00d71 is the hidden state of query\u2019s forerunner token (Q\u22a4= K\u22a4 nt), f : Rnt \u2192\u2126nt is a normalization mapping from nt-dimensional real vector to nt-dimensional probability vector (usually softmax function), the W is a linear kernel, usually W h\u22a4 Q W h K for multi-head attention or I = diag (1nt) for vanilla attention. For one input sample, given the token-index set of label tokens as L, the token-index set of label tokens which is the same as the query\u2019s ground truth label as L+, we define the Correct Label Assignment (CLA) of one sample as: \ufffd\n\ufffd   Intuitively, CLA reflects the accuracy of attention computation AW,f towards label tokens on one input. For an input set built from a dataset, we calculate the averaged CLA on these inputs, and repeat in every layer to plot a curve of Averaged CLA against layer numbers. Specifically:\n8Equitant refers to hidden states from the same layer and token type. While, in experiments shown in Fig. (Right), we don\u2019t keep the token type consistent.\n8Equitant refers to hidden states from the same layer and token type. While, in experiments shown in Fig. 5 (Right), we don\u2019t keep the token type consistent.\n(10)\n(11)\n(12)\n(13)\n(1) Vanilla attention. We assign W \u2252I, f to linear normlization. (2) Best Induction Head. For each attention head h, we assign W h \u2252W h\u22a4 Q W h K, f \u2252softmax. For each input, we calculate max h CLAW h,f (K, Q, L, L+) as the result for single input. (3) Head Average. For each attention head h, we assign W h \u2252W h\u22a4 Q W h K, f \u2252softmax. For each input, we calculate \ufffd h CLAW h,f (K, Q, L, L+) /|H|, where the |H| is the amount of heads in current layer, as the result for single input. Note that we do not consider the absolute value of attention assignment on label tokens in this experiment, and most of the heads have little scores assigned to the label (Fig. 6 (Left)), therefore, although the average assignments tend to be average, this result shown in Fig. 6 (Middle) does not contradict the phenomenon that ICL can achieve high accuracy.\n# A.5 CARTOGRAPHY DETAILS OF FIG. 7\nFor Fig. 7, we input one sample from SST-2 into Llama 3 70B, take the output of layer 30 on the label tokens to span a matrix KL, and map them by W h\u22a4 Q W h K of head 32 (the best induction heads in this layer) and 9 (the worst induction heads) of layer 31 (the layer with the most correct induction heads), respectively. We visualize the distribution of these mapped W h\u22a4 Q W h KKL, we conduct principal component analysis on them, and plot them on the plane of the first two components. For each point q \u2208R2 on the principal component plane, we calculate the attention assignment as follows. Give the index set of \u201cpositive\u201d label token in KL as L+, the index set of \u201cnegative\u201d label token as L\u2212, we calculate the attention assignment, which can be an estimate of ICL prediction (Wang et al., 2023), as:\nFor each point q \u2208R2 on the principal component plane, we calculate the attention assignment as follows. Give the index set of \u201cpositive\u201d label token in KL as L+, the index set of \u201cnegative\u201d label token as L\u2212, we calculate the attention assignment, which can be an estimate of ICL prediction (Wang et al., 2023), as:\nWe map this value to the degree of blue color of each pixel. The larger the positive value, the bluer it is, and the smaller the negative value, the redder it is.\nA.6 CARTOGRAPHY DETAILS OF FIG. 8\nFor Fig. 8, we calculate the magnitude of Step 1 as the finite differences of kernel alignment in Fig. 2 (Left, Forerunner Token of Label). We directly use the head counting of Fig. 5 (Middle) and Fig. 6 (Left) as the magnitude of Steps 2 and 3. These data are regularized and converted into transparencies.\n# B LM PRE-TRAINING DYNAMICS MEASURED BY ICL CIRCUIT\nWe extend the discussion of pre-training dynamics in \u00a76 here.\nOne can divide a self-regression model into an early part and a later part, where the early part encodes the input into a hidden representation, and the later part decodes the hidden representation back to the input. So, the training object can also be divided into an encoding loss and a decoding loss. According to the discussion in \u00a76, the summarization operation (Step 1) can be classified as encoding, and the other two steps of the induction circuit can be classified as decoding. Intuitively, since the decoding operations require the encoding results as input, unless the encoding operation converges to a stable output, the decoding can not be trained since the input-output mapping is noised, causing unstable gradients to interfere with the training (Liu et al., 2020). We confirm such inference by a measurement in Pythia\n(14)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3502/3502832f-9632-4159-8982-4a05a56e3073.png\" style=\"width: 50%;\"></div>\nFigure 10: Operating magnitude (normalized) and ICL accuracy w.r.t. pre-training steps on Pythia 6.9B and SST-2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8168/8168b4f0-f04d-40c5-8f8b-d895ed7fc180.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: The 3 operating magnitudes on Pythia 6.9B with various pre-training steps. Left: Step 1, summarize; Middle: Step 2, semantics merge; Right: Step 3, feature retrieval and copy, measured by the correct induction head numbers.</div>\nThe magnitude of the 3 operations is emergent in the early phase of pre-training (less than 10k steps), and is monotonically increasing, while the encoding operation has the fastest growth rate. (2). ICL capacity appearance after all three operations reaches a high level (around 50k steps, notice that the random accuracy is 0.5), while the curve morphology of the operating magnitude against the layer number (shown in Fig. 11) is convergence to the last training step and also the main results in this paper. Such results suggest that: LMs start to produce the inner encoding in the very early steps of the pre-training, and can be an important fundamental in building the subsequent induction circuits, as explained in the previous works mentioned in \u00a72.1.\nDATA DISTRIBUTION REQUIREMENT EXPLAINED BY HIDDEN STATE \nMoreover, the hidden space multiplexing in the induction operation observed in this paper can give a prototypical and phenomenological conjecture for the data distribution requirement found in the previous works (Olsson et al., 2022; Reddy, 2024; Singh et al., 2024b), where data with a large label space and various tasks can promote ICL and suppress In-weight Learning (IWL), and vice versa. Intuitively, suppose the encoding inputted into the later layers is clustered by their labels (similar to the well-embedded input styles in the works above, confirmed in Fig. 3). In that case, we can say a cluster center is the eigen-subspace of the corresponding label. Since attention only conducts dot-multiplication operations, let us assume that these eigen-subspaces are radially distributed.\nDuring the training, (1) When the label space is small, the trained attention heads only need to extract the projected length of the query on each label eigen-subspace. For each label, such operation has a parameters\u2019 analytical solution with (encoding kernel) W \u22a4 Q WK = I and (decoding transformation) WOWV = o\u22a4 y ey, where oy is the label token\u2019s output embedding, and ey is the label\u2019s eigen-subspace. From such an operation, theoretically, one layer can handle at most |H| labels, where |H| is the head amounts. While, considering the sparsity of these eigen-subspaces, such an upper bound can be increased to d\u2032|H| by multiplex one head to decode an orthogonal group of labels with orthogonal eigen-subspaces of E = [ey1; ey2; . . . ; eyd\u2032 ] and orthogonal output embedding O = [oy1; oy2; . . . ; oyd\u2032 ], where d\u2032 is the inner dimension of attention head, with decoding transformation O\u22a4E. (2) When th\n<div style=\"text-align: center;\">Figure 12: Attention matrix visualized on layer 2, head 53 in Llama 3 70B from an input case. y1; ey2; . . . ; eyd\u2032 ] and orthogonal output g O = [oy1; oy2; . . . ; oyd\u2032 ], where d\u2032 is dimension of attention head, with decodormation WOWV = O\u22a4E. (2) When the label space expands9, the decoding transforn not distinguish all the clusters since the WOWV is low-rank. Driven by the training</div>\n9Notice that such a situation can also occur when the orthogonality between eigen-subspaces or outpu embeddings is lost. A common situation is that the variance of the cluster increases, creating confusion withi the decoding space of the attention head.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7098/70982a1f-7d70-4a70-bced-09800e6ec7dd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d415/d415e6ae-9a71-40b4-8bc6-66f0ac197c59.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Attention score visualized with the forerunner token of the ICL query as the attention query, from (Left) layer 21 and (Right) layer 2 in Llama 3 70B from an input case.</div>\nloss, the model can choose to transform the encoding kernel to focus on catching the most similar label tokens with the query, and copy the label tokens\u2019 information back to the query. As a result, one attention head can catch at most d\u2032 groups of label tokens mapped collinearly by the encoding kernel (note that this set of labels may not appear simultaneously in the context, so confusion can be avoided), and the common space of these label words become the induction subspace shown in \u00a74.2. Our other conjecture is that the ICL training endpoint is thermodynamically stable (with a lower loss), and in contrast, the IWL training endpoint is kinetically stable (with a more accessible training trajectory). Moreover, the IWL training object can be a precursor of ICL training, since the total number of labels fed into the model gradually increases with the data. So, we can hypothesize that: when the metastable state is disturbed by some condition, such as the appearance of rare or noisy labels, the training can show a phase transition toward a thermodynamically stable state. Notice that this section is our hypothesis based on the results of this paper, which should be empirically validated in future work. In addition, we still need to find implicit input-label tuples in wild data, which will also be addressed in future work.\n# C CAN LMS SEGMENT ICL PROMPT?\nThe experiments in \u00a73.1 imply that LMs conduct effective segmentation on ICL-styled inputs, enabling LMs to block the interference of preceding demonstrations in the input text encoding operation. Here, as a prototypical discussion, we confirm the existence of segmentation and then reveal that such segmentation can be done in very early layers from an attention operation focusing on some specific segmentation tokens in the inputs. As a preliminary observation, we visualize the attention scores with the last forerunner token as the attention queue at layer 21 (a layer with high encoding magnitude, refer to Fig. 2) from an input case,\nas shown in Fig. 13 (Left), where most of the attention heads are focused on the query text. The visualization suggests that encoding operations are localized into the query. Although position embedding inserts sufficient positional information to hidden states and enables attention heads to identify nearby tokens, we believe that position embedding is insufficient to accurately segment the various input parts of uncertain lengths. We hypothesize that LM focused on\nTable 5: Accuracy drop with delimiters removed / modified from prompts on SST-2.\nTemplate Modification\nAcc. (%)\nNone (Table 4)\n91.60\n- w/o \u201c\\n\u201d\n93.36\n- w/o \u201c:\u201d\n85.74\n- w/o \u201csentence:\u201d, \u201csentiment:\u201d\n79.10\n- w/o all above\n50.98\n- \u201c:\u201d \u2192\u201chello\u201d\n78.91\n- \u201c:\u201d \u2192\u201c@\u201d\n91.41\n- \u201c:\u201d \u2192\u201cpositive\u201d\n71.48\n(Random)\n50.00\nnatural delimiters (e.g. \u201c\\n\u201d, \u201c:\u201d) in the input during the early stages of inference, and visualization in Fig. 13 (Right) supports such hypothesis: in layer 2, most of the attention heads focus on the natural delimiters, and another visualization in Fig. 12 shows that all attention queries (not only the forerunner token) exhibit similar separator-focusing behavior, suggesting that: some attention heads merge all the preceding delimiters\u2019 representation into every token as delimiter-based positional encoding, making the representations of tokens with the same number of preceding delimiters similar, while differentiating the representations of tokens with different numbers of preceding delimiters. In the subsequent inference process, LM can utilize these delimiter-based positional encodings for localization operations. Such observation is also consistent with Fig. 4. Furthermore, we empirically demonstrate that delimiters have significant saliency towards ICL accuracies in Table 5 (upper), experimented by removing them from prompt templates. Interestingly, the trial to completely remove these delimiters from the prompt yielded almost random results, even though these inputs still conform to the primary form of ICL. A reliable reason can be that: The missing delimiter interferes with the encoding operation (Step 1) on both demonstrations and queries, so that completely disrupts the ICL process. The scale of such a segmentation operation can surprise one since more than half of the heads focus on the segmenting operation as shown in Fig. 13 (Right). However, as an assumption, we want to argue that dividing the input text into local segmentation is a crucial step in language modeling, so, functionally, LM has sufficient motivation to focus on segmenting during the training process. Moreover, based on the above principles, as long as the delimiter appears periodically at appropriate positions and can be captured by attention heads (only in the structured parts of the prompt template), as shown in Table 5 (lower), the delimiter can be designed to any token. While we still recommend natural delimiters without semantics in the template design.\n# D MEASUREMENT ON DIRECT DECODING\nThis section measures the direct decoding bypass, suggesting that: (1) Direct decoding on well-processed hidden states with some later layer skipped can get satisfactory accuracy even better than the full inference process. (2) Direct decoding on insufficient processed hidden states adds bias towards the predicting distribution. We examine the first claim by applying the language model head on each layer\u2019s hidden state on SST-2, Llama 3 8B, and conduct a standard ICL process on the decoded token prediction distribution. The results are shown in Fig. 14, where direct decoding accuracy emerges from random to near 1 around layer 18. Refer to Fig. 8 and results in Appendix F.2, we can confirm: accuracy emerges after all three steps are executed.\nThis section measures the direct decoding bypass, suggesting that: (1) Direct decoding on well-processed hidden states with some later layer skipped can get satisfactory accuracy even better than the full inference process. (2) Direct decoding on insufficient processed hidden states adds bias towards the predicting distribution.\ner, the accuracies on the intermediate hidden states are even higher than the last hidden which is aligned with the discussion in Table 3. So, we can conclude: direct decoding on cessed hidden states can classify well.\nMoreover, we infer that direct decoding from lower layers, where hidden states are not sufficiently processed, causes prediction bias. We investigate the influence of the direct decoding result of layer 0, by the relationship between direct decoded distribution and final prediction distribution. In detail, on SST-2 and Llama 3 8B, we use various forerunner tokens with different direct decoding distributions on the label tokens \u201cpositive\u201d and \u201cnegative\u201d, and calculate their ICL prediction probability distributions respectively, as shown in Fig. 15 (upper), where forerunner tokens with biased direct decoding distribution produce prediction biases\n#\nAttention Disconnected\nKey \u2192Query\nAffected Layers Ratio (from layer 1)\n25%\n50%\n75%\n100%\n1\nNone (4-shot baseline)\n\u00b10 (Acc. 65.27)\n\u2013 Step1: Summarize \u2013\n2 Demo. Texts xi \u2192Forerunner si\n\u22127.65\n\u22120.42\n\u221215.69\n\u22120.36\n\u221227.15\n\u22120.10\n\u221229.10\n\u22124.68\n3 Query Texts xq \u2192Forerunner sq\n\u22128.30\n\u22120.26\n\u221221.13\n\u22120.13\n\u221228.84\n+0.19\n\u221231.74\n\u22120.00\n\u2013 Step2: Semantics Merge \u2013\n4 Demo. Forerunner si \u2192Label yi\n\u22121.01\n+0.68\n\u22121.92\n\u22120.03\n\u22121.04\n\u22120.00\n\u22121.27\n\u22120.07\n\u2013 Step3: Feature Retrieval & Copy \u2013\n5 Label yi \u2192Query Forerunner sq\n+3.32\n+3.71\n\u22123.61\n\u22120.03\n\u22127.91\n\u22120.00\n\u22125.92\n+0.16\nReference Value\n6\nZero-shot\n\u22124.28 (Acc. 60.99)\n7\nRandom Prediction\n\u221232.77 (Acc. 32.50)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/acf3/acf30809-e4ec-4e7c-8845-f8e43f35ed4c.png\" style=\"width: 50%;\"></div>\nFigure 14: Direct decoding accuracies on various layers.\n<div style=\"text-align: center;\">Table 6: Results of Table 1 on Falcon 7B.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/658d/658df829-cb6a-4350-ba8a-5a29cbe22d9e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Forerunner Tokens / Direct Decoding Logits Difference (N P, Layer 0)</div>\nFigure 15: Predicting distributions on different forerunner tokens with various direct decoding logits. Inference process used: Upper: Vanilla ICL, Lower: Biased removed inference by Contextual Calibration proposed by Zhao et al. (2021).\nwith the same tendencies. While, when we apply contextual calibration (Zhao et al., 2021), which removes the background value without a query from the prediction, such similar tendencies disappear (Fig. 15 (Lower)).\nWe investigate the correlation between the queries\u2019 perplexities and the classification accuracies with and without demonstrations, as a supplement of results in Fig. 2 (Right). We divide the queries into 10 bins w.r.t. the language modeling loss, and calculate the prediction accuracy in each bin, shown in Fig. 16. In these results, although a unified correlation can not be observed, we can confirm that: compared to the 0-shot results, the 4-shot inference shows better accuracies, especially on queries with high language modeling loss. So, we can conclude that: demonstrations enhance the inference accuracy of perplexed queries, consistent with the results in Fig. 2 (Right).\n# F AUGMENTATED EXPERIMENT RESULTS\nWe count the marked count of each attention head as Forerunner Token Head (Fig. 18, 19, 20, 21) / Correct Induction Head (Fig. 22, 23, 24, 25) by every data sample from each dataset on each model. Forerunner Token Head Statistics. We plot the distributions of the marked Forerunner Token Heads towards correct and wrong labels, where: there are observable morphological differences in figures across different datasets, while the forerunner token heads marked on the correct and incorrect labels of the same dataset are almost identical. The detailed data confirms our conclusion in \u00a74.1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f561/f56137e8-fbcc-4e33-b7ee-afd05ab2c1f0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: The correlations between language modeling loss and ICL prediction accuracies. Upper: 0-shot results; Lower: 4-shot results.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e8ef/e8efa3f9-b9fa-431f-bf86-848daca92cb9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 17: Supplemental experiment result on more samples for Fig. 7</div>\nThe results of most experiments in the main text on Llama 3 8B are shown in Fig. 26, 29, 32, and 35; The results of most experiments in the main text on Falcon 40B are shown in Fig. 27, 30, 33, and 36; The results of most experiments in the main text on Falcon 7B are shown in Fig. 28, 31, 34, 37, and Table 6. From these results, we can conclude consistently with the main text. However, as discussed in \u00a75.2, inference dynamics on these models are delocalized, thus clear serialization of the 3 steps can not be observed in these results.\nThe results of most experiments in the main text on Llama 3 8B are shown in Fig. 26, 29, 32, and 35; The results of most experiments in the main text on Falcon 40B are shown in Fig. 27, 30, 33, and 36; The results of most experiments in the main text on Falcon 7B are shown in Fig. 28, 31, 34, 37, and Table 6.\nFrom these results, we can conclude consistently with the main text. However, as discussed in \u00a75.2, inference dynamics on these models are delocalized, thus clear serialization of the 3 steps can not be observed in these results.\n# F.3 MORE RESULTS OF FIG. 7\nTo enhance the persuasiveness, we additionally and randomly try 4 input samples as supplements to Fig. 7 on SST-2 and Llama 3 70B as shown in Fig. 17. From these results, we can observe similar phenomena to Fig. 17 and conclude consistently.\nACKNOWLEDGMENTS\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59ad/59ad9116-a4d1-4f6c-88c0-2ed26588264c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 18: Forerunner Token Head marked on Llama 3 70B.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/758f/758f5bed-efdd-4a87-a6a9-5cce3760b75e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05b9/05b9e4c6-3e7a-4f48-927f-d0577524f250.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 19: Forerunner Token Head marked on Llama 3 8B.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d295/d295d3c6-dc9c-4bf7-bbcf-acc699ca72b5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e172/e172c0c1-7c65-43d8-bf22-224ed1655d0c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 20: Forerunner Token Head marked on Falcon 40B.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e0ec/e0ecd8ac-9372-",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of understanding In-Context Learning (ICL) in large language models (LLMs), which has been previously explored but lacks a comprehensive explanation of the inference dynamics involved. Existing methods struggle to capture all the phenomena associated with ICL, necessitating a new approach to elucidate the underlying mechanisms.",
        "problem": {
            "definition": "The problem is the unclear mechanism of ICL in LLMs, which results in insufficient understanding of how these models perform few-shot learning effectively.",
            "key obstacle": "The main challenge is the inability of existing models to fully capture the inference dynamics and the various phenomena observed during the ICL process."
        },
        "idea": {
            "intuition": "The idea stems from the recognition that ICL processes can be decomposed into fundamental operations that can be systematically analyzed and modeled.",
            "opinion": "The proposed idea is to introduce a comprehensive inference circuit that models the dynamics of ICL, providing a clearer understanding of how LLMs process input and make predictions.",
            "innovation": "The innovation lies in the development of a three-phase circuit that captures the essential operations of ICL, differentiating it from previous methods that lacked such a detailed breakdown."
        },
        "method": {
            "method name": "Inference Circuit for In-Context Learning",
            "method abbreviation": "IC-ICL",
            "method definition": "The method defines a structured approach to analyze ICL by breaking it down into three major operations: Summarization, Semantics Merge, and Feature Retrieval and Copy.",
            "method description": "The IC-ICL method systematically decomposes the ICL process into three fundamental operations that describe how LLMs infer from demonstrations and queries.",
            "method steps": [
                "Summarize: Encode input texts into linear representations.",
                "Semantics Merge: Merge encoded representations with their corresponding labels.",
                "Feature Retrieval and Copy: Retrieve and copy relevant features from merged representations to the query."
            ],
            "principle": "The method is effective because it provides a structured explanation of how LLMs utilize contextual information to enhance predictions, thereby clarifying the underlying mechanisms of ICL."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using Llama 3 and Falcon models on six sentence classification datasets, with a focus on understanding the dynamics of ICL through empirical measurements.",
            "evaluation method": "The performance of the proposed method was assessed through ablation studies, measuring the impact of each inference step on classification accuracy."
        },
        "conclusion": "The proposed inference circuit successfully captures the dynamics of ICL, demonstrating its dominance in the inference process. The findings reveal the existence of bypass mechanisms that operate in parallel, contributing to the understanding of ICL in LLMs.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its comprehensive nature, which allows for a detailed understanding of ICL dynamics and the factors influencing performance.",
            "limitation": "One limitation is that the three operations are not functionally indivisible, and further exploration is needed to understand the interactions between them.",
            "future work": "Future research should focus on exploring the detailed dynamics of pre-training and how they relate to the observed ICL mechanisms, as well as investigating the applicability of these findings to non-classification tasks."
        },
        "other info": [
            {
                "info1": "The experiments confirmed the existence of linear representations in LLMs that are crucial for ICL.",
                "info2": {
                    "info2.1": "The study identified several bypass mechanisms that assist in ICL inference.",
                    "info2.2": "Demonstrations were shown to enhance the encoding of input texts significantly."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of understanding In-Context Learning (ICL) in large language models (LLMs) and emphasizes the need for a comprehensive explanation of the inference dynamics involved."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea introduces a comprehensive inference circuit that models the dynamics of ICL, providing a clearer understanding of how LLMs process input and make predictions."
        },
        {
            "section number": "3",
            "key information": "The method defines a structured approach to analyze ICL by breaking it down into three major operations: Summarization, Semantics Merge, and Feature Retrieval and Copy."
        },
        {
            "section number": "3.1",
            "key information": "The proposed inference circuit captures the dynamics of ICL, demonstrating its effectiveness in enhancing predictions by utilizing contextual information."
        },
        {
            "section number": "6",
            "key information": "One limitation noted in the paper is that the three operations involved in the ICL process are not functionally indivisible, indicating a need for further exploration of their interactions."
        }
    ],
    "similarity_score": 0.7260621884706824,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Revisiting In-context Learning Inference Circuit in Large Language Models.json"
}