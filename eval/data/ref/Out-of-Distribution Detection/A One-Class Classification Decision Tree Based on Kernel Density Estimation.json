{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1805.05021",
    "title": "A One-Class Classification Decision Tree Based on Kernel Density Estimation",
    "abstract": "One-class Classification (OCC) is an area of machine learning which addresses prediction based on unbalanced datasets. Basically, OCC algorithms achieve training by means of a single class sample, with potentially some additional counter-examples. The current OCC models give satisfaction in terms of performance, but there is an increasing need for the development of interpretable models. In the present work, we propose a one-class model which addresses concerns of both performance and interpretability. Our hybrid OCC method relies on density estimation as part of a tree-based learning algorithm, called One-Class decision Tree (OC-Tree). Within a greedy and recursive approach, our proposal rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated its effectiveness, through the ability to tackle interpretable diag-",
    "bib_name": "itani2020oneclassclassificationdecisiontree",
    "md_text": "# A One-Class Classification Decision Tree Based on Kernel Density Estimation\nSarah Itania,b,\u2217, Fabian Lecronc, Philippe Fortempsc\naFund for Scientific Research - FNRS (F.R.S.-FNRS), Brussels, Belgium bFaculty of Engineering, University of Mons, Department of Mathematics and Operation Research, Mons, Belgium cFaculty of Engineering, University of Mons, Department of Engineering Innovation Management, Mons, Belgium\n# Abstract\nOne-class Classification (OCC) is an area of machine learning which addresses prediction based on unbalanced datasets. Basically, OCC algorithms achieve training by means of a single class sample, with potentially some additional counter-examples. The current OCC models give satisfaction in terms of performance, but there is an increasing need for the development of interpretable models. In the present work, we propose a one-class model which addresses concerns of both performance and interpretability. Our hybrid OCC method relies on density estimation as part of a tree-based learning algorithm, called One-Class decision Tree (OC-Tree). Within a greedy and recursive approach, our proposal rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated its effectiveness, through the ability to tackle interpretable diag-\n\u2217Corresponding author. University of Mons, Department of Mathematics and Operations Research, Rue de Houdain, 9, 7000 Mons, Belgium. Email addresses: sarah.itani@umons.ac.be (Sarah Itani), fabian.lecron@umons.ac.be (Fabian Lecron), philippe.fortemps@umons.ac.be (Philippe Fortemps)\nnosis aid based on unbalanced datasets. Keywords: One-class classification, decision trees, kernel density estimation, explainable artificial intelligence\n# 1. Introduction\nAs precious assets of knowledge extraction, data are massively collected in the fields of industry and research, day by day. Though valuable, the proliferation of data requires attention upon processing. In particular, unbalanced datasets may be hardly addressed through the classical scheme of multi-class prediction. The practice of One-Class Classification (OCC) has been developed within this consideration [1, 2]. OCC is of major concern in several domains where it may be expensive and/or technically difficult to collect data on a range of behaviors or phenomenons [3]. For example, it may be quite affordable to gather data on the representatives of a given pathology in medicine, or positive operating scenarios of machines in the industry. The related complementary occurrences are, by contrast, scarce and/or expensive to raise [2]. As a matter of fact, one-class classifiers are trained on a single class sample, in the possible presence of a few counter-examples. The resulting models allow to predict target (or positive) patterns and to reject outlier (or negative) ones. Basically, OCC is pursued for outlier (or anomaly) detection. One-Class Support Vector Machine (OCSVM) and Support Vector Data Description (SVDD) are among the most common OCC methods [4, 5]. OCSVM aims at finding the hyper-plane that separates the target instances from the origin with the wider margin, while SVDD aims at enclosing these instances within a single hyper-sphere of minimal volume. Far from being contested, the effectiveness of these methods has notably been improved with the development of variants that better fit some data structures [6, 7, 8, 9, 10, 11]. Indeed, the instances of a single class may be enclosed within several groupings in the form of sub-concepts that it would be interesting to raise separately [12]. Clus-\nterSVDD [13] achieves such a purpose: this recent method may be seen as a K-means algorithm [14] ruled by the results of distinct SVDD problems. Admittedly, the current methods of OCC give satisfaction, but that is without counting on the advent of explainable artificial intelligence which opens new research horizons for machine learning in encouraging the development of interpretable models [15]. In this regard, some methods have been developed as post-hoc explainers on the predictions of classifiers [16, 17]. But a great challenge remains the development of interpretable models by nature, which provide simultaneously high levels of performance. This challenge is the major source of motivation for the present work. Originally introduced for supervised classification, decision trees [18] provide satisfaction in terms of interpretability. The extensions of the algorithm proposed to tackle OCC often rely on the generation of outliers [19, 20]. However, a decision tree is basically built under the hypothesis that the different classes cover the whole domain by their representatives. Thus, the one-class variants may associate the target class with a large subspace against the one which the class occupies in reality. In a different perspective, the work of [21] revisits the development of decision trees by orienting the training process towards the isolation of outliers rather than of target instances. The intuition behind the method, called Isolation Forest (iForest), is that outliers are scarce and easily detectable compared to target instances [21]. The outliers can thus be isolated by means of a low number of divisions. An IF is an ensemble of trees built based on a random choice of attributes and thresholds. For a given instance, if the average path skimmed in the trees is short, the instance is predicted as outlier. Kernel Density Estimation (KDE) [22] is another approach which can address OCC intuitively, in computing the non-parametric estimation of a sample distribution. Thresholded at a given level of confidence, this estimation is used to reject any instance located beyond the decision boundary thus established. However, KDE loses in performance and readability towards high dimensional samples [19]. In the present work, we tackle OCC through a hybrid method, called One-\nass decision Tree (OC-Tree), which is intended to combine the benefits of the ndard decision tree and KDE. The contributions of our work are exposed ow. \u2022 Compared to previous adaptations of the decision tree to OCC, our proposal rather focuses on the isolation of the target training instances through a density-based hierarchical process of splitting, in which subdivisions are based on closed intervals of interest. This innovative splitting mechanism is supported by KDE. \u2022 The combination of decision trees with kernel density estimators was originally proposed by Smyth et al. [23]. Such a method was intended to boost the performances of the standard decision tree, in using a kernel density estimator to compute posterior class probabilities, based exclusively on the attributes belonging to a given decision chain. Our proposal differs in some regards. Indeed, the work of [23] tackled multi-class classification, through an approach which is implemented in two phases: (A) decision tree induction and (B) kernel density estimation to compute class probability. Our proposal tackles OCC through a hybrid methodology where density estimation is considered as a part of decision tree induction. \u2022 The OC-Tree may be seen as the integration of a multi-dimensional KDE within an intuitive and structured decision scheme. Indeed, the OC-Tree encloses data within hyper-rectangles, based on a subset of training attributes selected for their discriminative power. \u2022 The method has shown favorable performances in comparison to reference methods, including ClusterSVDD [13], OCSVM [4] and iForest [21] on benchmark datasets. \u2022 We apply our algorithm for the diagnosis of Attention Deficit Hyperactiv-\n\u2022 The method has shown favorable performances in comparison to reference methods, including ClusterSVDD [13], OCSVM [4] and iForest [21] on benchmark datasets.\n\u2022 We apply our algorithm for the diagnosis of Attention Deficit Hyperactivity Disorder (ADHD), based on the ADHD-200 collection. In this regard, the classification accuracy achieved by the OC-Tree is competitive in comparison to the results reported in the recent literature. We believe that the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e636/e636bf72-cdc9-471c-adcb-4f2a826f2768.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f8a/9f8a674e-0abf-41b6-a8a8-cb4b7e24b9d6.png\" style=\"width: 50%;\"></div>\nconvenience and the interpretability of the OC-Tree make it a promising model for future clinical practice. To illustrate the objective of our OC-Tree, let us consider a toy example proposed in Fig. 1 (left). The latter is processed in two distinct ways : \u2022 with a multi-class decision tree. In this case, each Gaussian blob is associated to a distinct class (C1, in red and C2, in green). The associated space division is represented in dashed lines. \u2022 with an OC-Tree. In this case, the Gaussian blobs are all the representatives of the same Class (called C). The limits of the corresponding hyper-rectangles are represented in continuous lines. The complementary space is the one of Outliers (called O). As shown, multi- and one-class learning processes lead to different predictive models (Fig. 1, right). Indeed, in the context of a multi-class problem, the class representatives are supposed to share the whole domain in which the attributes take their values. Hence, a decision tree learned with an algorithm like C4.5 [18, 24] proposes a decomposition of the whole space in hyper-rectangles by means of one single attribute. On the opposite, aiming at solving a one-class classification problem, we propose a learning process looking for target hyper-rectangles that do not necessarily cover the whole domain in which the attributes take their values, since there may exist outliers to discard. The remainder of the paper is organized as follows. Sec. 2 describes our algorithm which was assessed in comparison to reference methods according to the experimental protocol presented in Sec. 3. We expose the related results in Sec. 4. Then, in Sec. 5, we present a medical case study whose challenging aspects can be appropriately addressed by the OC-Tree. Finally, we discuss and summarize our findings in Sec. 6, before concluding the paper in Sec. 7.\n# 2. Our proposal\nIn a divide and conquer spirit, the implementation of our one-class tree rests on successive density estimations to raise target areas as hyper-rectangles of interest. We assess the relevance of a subdivision against an information gain criterion adapted to OCC issues proposed by [20]. Let us consider \u03c7 \u2282Rd a hyper-rectangle of dimensions d including target training instances. Let us note A = {a1, a2, . . . , ad} the set of attributes and X = {x1, x2, . . . , xn} the set of instances. The goal of our proposition is the division of the initial hyper-rectangle \u03c7 in (non necessarily adjacent) sub-spaces \u03c7ti, represented by tree nodes ti, in absence of counter-examples. Let us denote as At the set of eligible attributes for division at a given node t. Thus, At \u2286A. We note At = {a\u2032 1, a\u2032 2, . . . a\u2032 lt}, lt being the number of eligible attributes at node t, with lt \u2264d accordingly. At each node t, the algorithm searches the attribute a\u2032 j \u2208At which best cuts the initial sub-space \u03c7t into one or several sub-space(s) \u03c7ti such that:\n# \u03c7ti = {x \u2208\u03c7t : Lti \u2264xa\u2032 j \u2264Rti}\nxa\u2032 j is the value of instance x for attribute a\u2032 j; Lti and Rti are respectively the left and right bounds of the closed sub-intervals raised to split the current node t in target nodes ti, based on attribute a\u2032 j. For each attribute a\u2032 j \u2208At, the algorithm achieves the following steps, at a given node t. 1. Check if the attribute is still eligible and compute the related Kernel Density Estimation (KDE), i.e., an estimation of the probability density function \u02c6fj(x) based on the available training instances (see Sec. 2.1). 2. Divide the space \u03c7t, based on the modes of \u02c6fj(x) (see Sec. 2.2). 3. The quality of the division is assessed by the computation of the impurity of the resulting nodes deriving from division (see Sec. 2.3).\n(1)\nAt each iteration, the attribute that achieves the best purity score is selected to split the current node t in child nodes. If necessary, some branches are prepruned in order to preserve the interpretability of the tree (see Sec. 2.4). The algorithm is run recursively; termination occurs under some stopping conditions (see Sec. 2.5). In the rest of this paper, what we refer to as the training accuracy corresponds to the rate of training instances included in target nodes. It follows that, in this context of OCC, the training classification error corresponds to the rate of training instances predicted as outliers by the predictive model.\n# 2.1. Density estimation\nIn order to identify concentrations of target instances, we have to estimate their distribution over the space, which can be provided by a Kernel Density Estimation (KDE). In particular, our proposal is based on the popular Gaussian kernel [22]:\nwhere \u02c6fj is the KDE related to attribute a\u2032 j, Xt = {x1, x2, . . . , xnt} is the set of nt instances available at node t, K the kernel function and ht, a parameter called bandwidth. The parameter ht influences the pace of the resulting function \u02c6fj(x) [22]. As ht tends towards zero, \u02c6fj(x) appears over-shaped while high values of ht induce a less detailed density estimation. Adaptive methods, such as a least-squares cross-validation, may help setting the bandwidth value [25, 26]. However, such iterative techniques are computationally expensive; their use may be hardly considered in this context of recursive divisions. Hence, we compute ht as [22]:\n(2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1081/108149e8-db39-44be-869d-30d48302dee5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Division mechanism</div>\nwhere \u02c6\u03c3 is the standard deviation of the sample Xt and IQR, the associated inter-quartile range. The first relation corresponds to the Silverman\u2019s rule of thumb [22]. We consider the second relation to address samples with IQR = 0, which may reveal very concentrated data, with the potential presence of some singularities that should be eliminated.\n# 2.2. Division\n# At node t, division is executed based on \u02c6fj(x), in four steps.\nAt node t, division is executed based on \u02c6fj(x), in four steps.\n# (a) Clipping KDE (\u03b3)\n\u02c6fj(x) is thresholded at the level \u03b3 \u00b7 maxx\u2208\u03c7t \u02c6fj(x). This allows to raise a set of target sub-intervals Yj t.\n(b) Revision (\u03b1) If \u02c6fj(x) is k-modal (k \u0338= 1) and 1 \u2264|Yj t| < k, revision occurs since some\n# (b) Revision (\u03b1)\nmodes were not identified. Each sub-interval of Yj t is thus analyzed: if its image by \u02c6fj(x) includes at least a significant local minimum, the interval is split in two sub-intervals around this (these) local minimum (minima). The significance of a local minimum is assessed through a parameter \u03b1 (see below).\n# (c) Assessment (\u03b2)\nThe sub-intervals of Yj t covering a number of training instances inferior to a quantity \u03b2.|T| are dropped. This ensures keeping the most significant target nodes.\n# (d) Shrinking\nThe detected sub-intervals are shrunk in closed intervals in a way to fit the domain strictly covered by the related target training instances, as defined by Eq. 1.\nActually, Yj  may be updated at the end of steps (b), (c), (d). If we consider the KDE presented by Fig. 2, (a) results in Yj t = {[A, B]; [C, D]} As the density estimation is 3-modal in this case, a revision of the interval partitioning (b) is launched. It appears there is no need to split the sub-interval [C, D] since the piecewise \u02c6fj([C, D]) includes a single maximum. By contrast, a local minimum is detected in m1, in the piecewise \u02c6fj([A, B]). The sub-interval [A,B] is thus split into three parts around the local minimum. Concretely, such a split occurs if the local minimum is significant, i.e., sufficiently deep in comparison with both nearby local maxima. In mathematical terms:\n# \u02c6fj(m1) \u2264\u03b1 \u00b7 min( \u02c6fj(M1), \u02c6fj(M2)).\nThus Yj t = {[A, m1[; [m1, C[; [C, D]}. Steps (c) and (d) are then launched. The sub-intervals are shrunk around the target training instances (represented by crosses in Fig.2), which results in:\nYj t = {[Lj t1, Rj t1]; [Lj t2, Rj t2]; [Lj t3, Rj t3]}.\nThe complement Yj t represents the set of outlier sub-spaces: it may be represented by a single branch entitled \u201delse\u201d. Except for prior knowledge that would help choosing its value more specifically, there should be no reason to set a high reject threshold \u03b2 (e.g., > 2%) since the training set is supposed to include a majority of target instances; this would be penalizing with the exclusion of real target nodes as a consequence. An appropriate value for parameter \u03b1 may be selected by cross-validation; actually, a non-zero value for \u03b1 (e.g., 0.5) will lead to revision, which appears to be interesting if we want to detect precisely target groupings. Basically, the value of the clipping threshold \u03b3 should be low (e.g., 0.05), because it aims at rejecting outliers.\n# 2.3. Impurity decrease computation\nAt this stage of the algorithm, we have to assess the quality of a division in a particular context, i.e., the absence of representatives for at least a second class. One way to achieve this task is to resort to the physical generation of n\u2032 t outliers in each node [27, 19]; as a result of the division, each child node would include a number of n\u2032 ti instances which would have to be estimated. The virtual generation of outliers is worth considering as well. In this regard, the work of [20] assumes each parent node includes uniformly distributed outliers in equal number to that of the target instances, i.e., nt = n\u2032 t. Thus, the number of outliers in each child node may be easily deduced:\nwhere \u00b5 denotes the measure of the hyper-rectangle to which it relates. Assuming nt = n\u2032 t may appear counter-intuitive. Indeed, we would naturally be inclined to assume, once and for all, n = n\u2032 in the initial root node and to deduce the number of outliers in each child node according to Eq. 3. But throughout the iterations, this would lead to increase the scarcity of the outliers, and thus to their unfair representation in each node. The latter situation\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/681b/681bd2a4-f832-4c0f-886e-05712fefe7c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Pre-pruning mechanism</div>\ncorresponds to the well-known effect of the curse of dimensionality [28]. This is why, to address this issue, the number of outliers in each node t is considered as corresponding to the number nt of target instances prior to any division [20]. Based on this predictive calculation, the work of [20] gives a proxy for the Gini impurity decrease for OCC. We adapt this result to our proposal where a division may result in more than two child nodes ti, based on sub-intervals of interest:\nwhere rt is the total number of target and outlier sub-intervals, included in Yj t \u222aYj t.\n# 2.4. Pre-pruning mechanism\nA branch of an OC-Tree is prepruned if there are no more eligible attributes for division. An attribute is not eligible if:\n\u2022 the attribute was already used previously to cut the same target node which was not split in several target nodes in the meantime; \u2022 the computed bandwidth ht is strictly inferior to the minimum of the difference between two (different) successive values in the set of available instances, i.e., data granularity. At a given node t, a division based on a non-eligible attribute makes no more sense. Fig. 3 shows a tree trained on two attributes. The nodes in dotted lines are developed in absence of a pre-pruning mechanism; the latter allows to get a shorter and readable decision tree. Note that the branches related to outliers were omitted for the sake of clarity. The user has basically the choice to keep either the tree as (1) a full predictive model which describes the development that brought to the space division, or (2) the description of the final target hyper-rectangles as a set of sub-intervals of interest regarding the attributes that were used for division.\n\u2022 the attribute was already used previously to cut the same target node which was not split in several target nodes in the meantime;\n\u2022 the computed bandwidth ht is strictly inferior to the minimum of the difference between two (different) successive values in the set of available instances, i.e., data granularity.\nAt a given node t, a division based on a non-eligible attribute makes no more sense. Fig. 3 shows a tree trained on two attributes. The nodes in dotted lines are developed in absence of a pre-pruning mechanism; the latter allows to get a shorter and readable decision tree. Note that the branches related to outliers were omitted for the sake of clarity. The user has basically the choice to keep either the tree as (1) a full predictive model which describes the development that brought to the space division, or (2) the description of the final target hyper-rectangles as a set of sub-intervals of interest regarding the attributes that were used for division.\n# 2.5. Stopping conditions\nLet us denote the training accuracy as Atr: it corresponds to the ratio of training instances included in the target nodes. The algorithm stops under some global and local conditions.\n\u2022 Globally, the algorithm is stopped:\n# \u2022 Globally, the algorithm is stopped:\n\u2013 if Atr remains stable after an iteration in which no additional target node was raised. In this case, the training process reaches a stage where the target sub-spaces are simply more precisely delimited on the basis of additional attributes, with no further multiplication. \u2013 if Atr < 1 \u2212\u03bd, where \u03bd is a parameter corresponding to the fraction of training instances which we tolerate to reject and to consider as outliers.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c306/c306d471-e77a-4654-a972-6f153ab9c898.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Experimental pipeline</div>\n\u2022 Divisions may be stopped locally if there are compelling reasons to convert a node in a leaf, i.e., when pre-pruning is necessary (see Sec. 2.4).\n# 3. Experimental protocol\nFigure 4 summarizes our experimental protocol which is explained in detail in the following sections.\n# 3.1. Reference methods\nWe compared the OC-Tree with three reference methods, namely the ClusterSVDD [13], One-Class Support Vector Machine (OCSVM) [4] and Isolation Forest (iForest) [21]. The comparison of the OC-Tree with ClusterSVDD is highly relevant since both methods pursue similar objectives, i.e., enclosing data within one or several hyper-rectangle(s) and hyper-sphere(s) respectively. ClusterSVDD requires that two parameters should be optimized on a dataset: \u03bd and k which constitute respectively, the upper bound on the fraction of instances lying outside the decision boundary and the supposed number of clusters. Table 1 exposes a theoretical comparison of the OC-Tree with ClusterSVDD. OCSVM is a standard OCC method to which a comparison is thus worth considering. We considered a gaussian kernel for this method, and we optimized \u03bd which pursues the same objective as in ClusterSVDD and OC-Tree. Thus, to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/35bb/35bb0aa2-5287-461f-8032-3b11229ba5d1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 1: Comparison of ClusterSVDD & OC-Tree</div>\nensure a fair comparison, we adjusted this parameter in the same way that we did for ClusterSVDD. Finally, a method like iForest provides a relevant benchmark since it is of the same nature than OC-Tree, i.e., a tree-based method, but built in a very different way. Indeed, this ensemble technique aims at the development of decision trees based on a random choice of attributes and thresholds. If the average path length skimmed in the trees is low (resp. high), an instance is predicted as outlier (resp. target). We used the standard parameter settings for this method, since it was shown that the performances are ensured to be quite optimal with such settings [21].\nIn absence of benchmark data for OCC, it is standard practice to convert multi-class problems into one-class ones for evaluation purposes. We thus considered a set of benchmark datasets (see Table 2), where each instance belongs to a class ci among a set of C. The relevancy of OC-Tree and of the reference methods on these datasets was assessed in two distinct ways. A. All the instances, whatever their class, were considered as the representatives of a same class. We injected in this dataset a certain percentage of additional outliers following a uniform distribution [13]. (Approach A) B. We adopted the one vs rest [19] strategy which consists of considering a class ci \u2208C as a target one and the others as outliers [29, 27, 19,\n# Classes\n# Features\n# Instances\nAustralian\n2\n14\n690\nDiabetes\n2\n8\n268\nIonosphere\n2\n34\n351\nIris\n3\n4\n150\nSatimage\n6\n36\n4435\nSegment\n7\n19\n2310\n30, 31, 32]. In this case, the outliers injected in a given data subset were randomly picked among the representatives of the outlier classes, i.e., C\\ci. (Approach B) Whether through approach A or B, the resulting dataset was split in a way that two thirds constituted a training set, while the remaining was kept as a test set. 3.3. Evaluation metrics As one-class classification deals with unbalanced datasets, we may hardly consider true positives (or true targets) and true negatives (or true outliers) as equally significant. On this regard, the couple precision-recall provides appropriate evaluation metrics [35]. Let us denote as TT (resp. TO), the number of True Targets (resp. True Outliers), i.e., the number of instances correctly detected as targets (resp. outliers); FT (resp. FO) are the number of False Targets (resp. False Outliers) [30]. Precision (P) and Recall (R) are defined as follows.\n30, 31, 32]. In this case, the outliers injected in a given data subset were randomly picked among the representatives of the outlier classes, i.e., C\\ci. (Approach B)\nWhether through approach A or B, the resulting dataset was split in a way that two thirds constituted a training set, while the remaining was kept as a test set.\n# 3.3. Evaluation metrics\nAs one-class classification deals with unbalanced datasets, we may hardly consider true positives (or true targets) and true negatives (or true outliers) as equally significant. On this regard, the couple precision-recall provides appropriate evaluation metrics [35]. Let us denote as TT (resp. TO), the number of True Targets (resp. True Outliers), i.e., the number of instances correctly detected as targets (resp. outliers); FT (resp. FO) are the number of False Targets (resp. False Outliers) [30]. Precision (P) and Recall (R) are defined as follows.\n\u2022 Precision expresses the ratio of instances that were correctly predicted as target ones to those which were predicted as such.\n\u2022 Recall expresses the ratio of instances that were correctly predicted as\n(4)\nMethod\nSettings\nOC-Tree\n\u2022 \u03b3 = 0.05\n\u2022 \u03b1 = {0.5, 0.6, 0.7, 0.8}\n\u2022 \u03b2 = 2% (min. 5 inst./node)\n\u2022 \u03bd = {0.05, 0.1, 0.15, 0.2}\nClusterSVDD\n\u2022 k (see Table 4)\n\u2022 \u03bd = {0.05, 0.1, 0.15, 0.2}\nOCSVM\n\u03bd = {0.05, 0.1, 0.15, 0.2}\niForest\nNot required\ntarget ones to those which are truly target instances.\nPrecision and recall can be embedded in a single performance indicator, namely the F1-score [35].\n# 3.4. Model selection\nThe OC-Tree and some reference methods rely on a certain number of parameters that have to be adjusted appropriately. This parameter tuning was achieved through a 10-fold Cross-Validation (10-fold CV) procedure, based on the values presented in Table 3. Note that we conducted a grid search in the case where we had to optimize two parameters. The range of values for parameter k, i.e., the number of clusters in ClusterSVDD, has been differentiated depending on the considered dataset and the approach under which the datasets were addressed, as defined in Sec. 3.2. Some\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fc2d/fc2d39bd-2d0e-4c2c-8fb7-ac9ed581c6ac.png\" style=\"width: 50%;\"></div>\nApproach A\nApproach B\nAustralian\n{1, 2}\n{1, 2, 3, 4, 5}\nDiabetes\n{1, 2, 4}\nIonosphere\n{1, 2}\nIris\n{1, 2, 3}\nSatimage\n{1, 3, 6, 9} [13]\nSegment\n{1, 5, 7, 10, 14} [13]\n<div style=\"text-align: center;\">Table 4: Selected values for parameter k (ClusterSVDD)</div>\nvalues are suggested in [13]. More particularly in regards to approach B, it appeared to us reasonable to set a range of [1, 5] as possible values for parameter k, regardless of the considered dataset. Indeed, in this case, each class of the multi-class problem is considered for OCC. Thus, intuitively, one would expect that data are concentrated within a small number of target groupings but in the same time, the presence of a single class may reveal a structure of data different from the one observed in the case of a multi-class problem. That is why k may present higher values than those considered with approach A for some datasets. Thus, except for iForest, each algorithm was tuned through a CV procedure, in quest of the model which presents the best performance at the sense of the F1score (see Eq. 6). The model selection was naturally achieved on the training set extracted from each dataset. The selected models were finally assessed against a test set.\n# 4. Results\nIn this section, we first propose to the reader a preliminary experiment on synthetic data, to better understand the scope of the advocated method. We then report the results achieved on benchmark datasets.\nWe propose a first qualitative evaluation of our OC-Tree with ClusterSVDD with respect to the detection of three Gaussian blobs enclosing altogether 1000 instances. The parameter settings are given below.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/46bc/46bcf6f7-d79f-4f6b-adad-14ee84b934f7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Detection of three Gaussian blobs with 2% of outliers included in the training </div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31d9/31d99856-cd88-47c3-af38-a575b5cc75e3.png\" style=\"width: 50%;\"></div>\n\u2022 OC-Tree : \u03b3 = 0.05, \u03b1 = 1, \u03b2 = 0%, \u03bd = 0.1.\nThe parameters of OC-Tree were established in a quite penalizing way, in the sense that setting \u03b1 at 1 means a systematic revision of any division with the risk of decomposing unnecessarily the space covered by the target instances. Moreover, setting \u03b2 at 0% means no node is dropped; this may potentially lead to small hyper-rectangles to describe the target data. Additional instances were added to the dataset in the form of uniformly distributed outliers, in proportions of 2% and 5% of the initial training set size respectively. The results are proposed in Figs. 5 and 6. Both methods detect the blobs in the form of circles and rectangles respectively. However, it seems that\nNoise level\nClusterSVDD\nOC-Tree\nPrecision\nRecall\nPrecision\nRecall\n2%\n0.998\n0.917\n0.998\n0.985\n5%\n0.995\n0.940\n0.999\n0.987\nOC-Tree is less sensitive to higher noise levels than ClusterSVDD (see Fig. 6). Table 5 compares the performances of ClusterSVDD and OC-Tree in terms of precision and recall.\n# 4.2. Experiments on benchmark datasets\nIn the present section, we compare our algorithm to ClusterSVDD, OCSVM, and iForest on benchmark datasets, according to the protocol summarized in Sec. 3. Table 6 on the one hand, and Tables 7, 8 on the other hand summarize the results for the approaches A and B respectively. We report the couple of values Precision (P) \u2013 Recall (R) for a validation achieved on the test sets. The results that are marked with an asterisk indicate that the corresponding method outperforms OC-Tree of more than 2% in terms of F1-score. The lines that are succeeded with \u2019(+)\u2019 indicates that the OC-Tree achieves a score superior or equal to the other techniques for the considered dataset.\n# 4.2.1. Based on uniformly distributed noise \u2013 Approach A\nOur first experiment was achieved on the benchmark datasets summarized in Table 2, in which uniformly distributed noise was injected in proportions of 2, 5, 10, 15 % of the initial dataset sizes. It appears that the OC-Tree performs favorably in comparison to the other reference methods. The improvements achieved against iForest may be explained by the fact that the latter method is properly intended for anomaly detection, and may thus have slightly lower performances when the proportion of outliers in the training set is low [21], especially for proportions of 2% and 5%. Moreover, compared to iForest, the OC-Tree seems globally to better handle the ionosphere and satimage datasets. Actually, the ionosphere dataset has a quite diffuse distribution of data along\n<div style=\"text-align: center;\">Dataset Noise level ClusterSVDD OCSVM iForest OC</div>\nDataset\nNoise level\nClusterSVDD\nOCSVM\niForest\nOC-Tree\nAustralian\n2%\n0.986 - 0.921\n0.995 - 0.908\n1.000 - 0.926\n1.000 - 0.965\n(+)\n5%\n0.973 - 0.926\n0.977 - 0.922\n1.000 - 0.922\n1.000 - 0.970\n(+)\n10%\n0.900 - 0.960\n0.916 - 0.960\n1.000 - 0.991*\n0.900 - 0.996\n15%\n0.877 - 0.961\n0.882 - 0.935\n0.955 - 1.000\n1.000 - 0.961\n(+)\nDiabetes\n2%\n1.000 - 0.941\n1.000 - 0.941\n1.000 - 0.874\n0.992 - 0.965\n(+)\n5%\n0.998 - 0.965*\n0.988 - 0.957\n0.996 - 0.914\n0.992 - 0.911\n10%\n0.932 - 0.972\n0.975 - 0.933\n0.996 - 0.952\n0.980 - 0.952\n15%\n0.974 - 0.897\n0.974 - 0.901\n0.965 - 0.988\n0.992 - 0.945\nIonosphere\n2%\n0.972 - 0.914\n0.972 - 0.897\n0.981 - 0.879\n1.000 - 1.000\n(+)\n5%\n0.938 - 0.913\n0.937 - 0.904\n0.937 - 0.904\n1.000 - 1.000\n(+)\n10%\n0.884 - 0.939\n0.880 - 0.904\n0.904 - 0.912\n0.983 - 1.000\n(+)\n15%\n0.828 - 0.946\n0.824 - 0.920\n0.832 - 0.929\n0.982 - 1.000\n(+)\nIris\n2%\n1.000 - 0.902\n1.000 - 0.961\n1.000 - 0.922\n1.000 - 0.941\n5%\n0.977 - 0.860\n0.980 - 0.960\n0.978 - 0.900\n0.943 - 1.000\n(+)\n10%\n0.979 - 0.920\n1.000 - 0.940*\n0.958 - 0.920\n0.978 - 0.900\n15%\n0.902 - 0.920\n0.889 - 0.960\n0.889 - 0.960\n0.862 - 1.000\n(+)\nSatimage\n2%\n0.995 - 0.945\n0.996 - 0.957\n0.996 - 0.890\n0.996 - 0.968\n(+)\n5%\n0.986 - 0.974\n0.986 - 0.971\n0.984 - 0.914\n0.979 - 0.981\n(+)\n10%\n0.991 - 0.981\n0.977 - 0.946\n0.952 - 0.922\n0.981 - 0.969\n15%\n0.990 - 0.963\n0.966 - 0.937\n0.907 - 0.934\n0.980 - 0.968\nSegment\n2%\n0.999 - 0.963\n0.999 - 0.974\n1.000 - 0.912\n1.000 - 1.000\n(+)\n5%\n0.974 - 0.970\n0.978 - 0.970\n1.000 - 0.940\n1.000 - 1.000\n(+)\n10%\n0.927 - 0.980\n0.930 - 0.979\n1.000 - 0.991\n0.993 - 1.000\n(+)\n15%\n0.898 - 0.970\n0.928 - 0.946\n0.976 - 1.000*\n0.872 - 0.996\n<div style=\"text-align: center;\">Table 6: Results - Approach A (P-R)</div>\nsome dimensions, which involves that some normal instances may lie far away from the others. As it is built on a random choice of attributes, the iForest method is likely to detect these instances as outliers. On the opposite, the OCTree is built on attributes which concentrate the instances, so the ones lying outside these concentrations may be really perceived as outliers. As regards the satimage dataset, the low proportion of outliers in such a high dimensional dataset may have disadvantaged the iForest method, with a difference in terms of F1-score that can reach 5%. As regards the performances of OCSVM, they are in some cases lower than OC-Tree, which may be explained by the fact that OCSVM encloses data within a single boundary and can thus not exactly adjust to the structure of data. Finally, as mentioned previously, ClusterSVDD may be sensitive to noise, which explains why the OC-Tree provides better results in some cases.\nDataset\nNoise level\nClusterSVDD\nOCSVM\niForest\nOC-Tree\nAustralian (-1)\n2%\n0.984 - 0.945\n0.983 - 0.914\n0.991 - 0.875\n0.992 - 0.977\n(+)\n5%\n0.936 - 0.936\n0.935 - 0.920\n0.959 - 0.928\n0.945 - 0.960\n(+)\n10%\n0.902 - 0.960\n0.919 - 0.912\n0.941 - 0.896\n0.890 - 0.968\n(+)\n15%\n0.819 - 0.934\n0.822 - 0.917\n0.886 - 0.901\n0.834 - 1.000\n(+)\nAustralian (+1)\n2%\n0.980 - 0.951\n0.980 - 0.951\n0.980 - 0.951\n0.990 - 0.980\n(+)\n5%\n0.950 - 0.950\n0.950 - 0.941\n0.949 - 0.921\n0.943 - 0.990\n(+)\n10%\n0.906 - 0.950\n0.932 - 0.950\n0.947 - 0.891\n0.901 - 0.990\n(+)\n15%\n0.881 - 0.960\n0.872 - 0.950\n0.855 - 0.940\n0.860 - 0.980\nDiabetes (-1)\n2%\n0.978 - 0.978*\n0.977 - 0.966*\n0.976 - 0.910\n0.976 - 0.910\n5%\n0.957 - 0.978*\n0.956 - 0.967*\n0.954 - 0.922*\n0.952 - 0.878\n10%\n0.944 - 0.934\n0.926 - 0.956\n0.928 - 0.846\n0.926 - 0.956\n(+)\n15%\n0.863 - 0.921\n0.876 - 0.955\n0.874 - 0.933\n0.862 - 0.910\nDiabetes (+1)\n2%\n0.981 - 0.933\n0.980 - 0.903\n0.980 - 0.879\n0.982 - 0.988\n(+)\n5%\n0.945 - 0.951\n0.956 - 0.927\n0.955 - 0.909\n0.942 - 0.982\n(+)\n10%\n0.895 - 0.962\n0.905 - 0.956\n0.909 - 0.938\n0.881 - 0.975\n15%\n0.853 - 0.938\n0.858 - 0.938\n0.871 - 0.919\n0.856 - 0.963\n(+)\nIonosphere (-1)\n2%\n0.974 - 0.881\n0.967 - 0.690\n0.971 - 0.810\n0.977 - 1.000\n(+)\n5%\n0.946 - 0.833\n0.935 - 0.690\n0.946 - 0.833\n0.955 - 1.000\n(+)\n10%\n0.872 - 0.829\n0.857 - 0.732\n0.872 - 0.829\n0.943 - 0.805\n(+)\n15%\n0.889 - 0.930\n0.861 - 0.721\n0.895 - 0.791\n0.905 - 0.884\n(+)\nIonosphere (+1)\n2%\n1.000 - 0.960\n1.000 - 0.960\n1.000 - 0.893\n0.986 - 0.973\n(+)\n5%\n0.973 - 0.973\n0.986 - 0.946\n0.986 - 0.919\n0.947 - 0.959\n10%\n0.972 - 0.932\n0.973 - 0.959\n0.956 - 0.878\n0.973 - 0.973\n(+)\n15%\n0.920 - 0.958*\n0.909 - 0.972\n0.920 - 0.958*\n0.861 - 0.944\nIris (1)\n2%\n1.000 - 1.000*\n1.000 - 1.000\n1.000 - 0.941*\n1.000 - 0.882\n5%\n0.933 - 0.824\n1.000 - 0.824\n1.000 - 0.882\n1.000 - 0.882\n(+)\n10%\n0.938 - 0.833\n0.938 - 0.833\n1.000 - 1.000*\n1.000 - 0.889\n(+)\n15%\n0.941 - 0.842\n0.941 - 0.842\n1.000 - 1.000*\n1.000 - 0.895\nIris (2)\n2%\n1.000 - 1.000\n1.000 - 1.000\n1.000 - 0.824\n1.000 - 1.000\n(+)\n5%\n0.944 - 1.000\n0.944 - 1.000\n0.944 - 1.000\n0.944 - 1.000\n(+)\n10%\n1.000 - 1.000\n0.947 - 1.000\n1.000 - 1.000\n1.000 - 1.000\n(+)\n15%\n1.000 - 1.000*\n0.947 - 0.947\n1.000 - 1.000*\n0.947 - 0.947\nIris (3)\n2%\n1.000 - 0.824\n1.000 - 0.824\n1.000 - 0.824\n1.000 - 1.000\n(+)\n5%\n0.941 - 0.941*\n0.933 - 0.824\n0.933 - 0.824\n0.938 - 0.882\n10%\n1.000 - 1.000*\n1.000 - 0.722\n1.000 - 1.000*\n1.000 - 0.833\n15%\n0.929 - 0.684\n1.000 - 0.789\n1.000 - 0.895\n1.000 - 0.895\n(+)\nTable 7: Results (P-R) - Approach B\n<div style=\"text-align: center;\">Table 7: Results (P-R) - Approach B</div>\nDataset\nNoise level\nClusterSVDD\nOCSVM\niForest\nOC-Tree\nSatimage (1)\n2%\n0.997 - 0.975\n0.991 - 0.955\n1.000 - 0.905\n0.997 - 0.958\n5%\n0.977 - 0.964\n0.980 - 0.972\n0.997 - 0.952\n0.983 - 0.972\n(+)\n10%\n0.958 - 0.992\n0.955 - 0.986\n0.986 - 0.981\n0.970 - 0.992\n(+)\n15%\n0.914 - 0.981\n0.914 - 0.978\n0.959 - 0.983\n0.918 - 0.992\nSatimage (2)\n2%\n0.980 - 0.942\n0.980 - 0.949\n0.986 - 0.872\n0.979 - 0.910\n5%\n0.994 - 0.981\n1.000 - 0.975\n1.000 - 0.898\n0.980 - 0.955\n10%\n0.910 - 0.987\n0.915 - 0.981\n0.967 - 0.942\n0.937 - 0.968\n15%\n0.925 - 0.948\n0.902 - 0.955\n0.950 - 0.987*\n0.884 - 0.981\nSatimage (3)\n2%\n0.984 - 0.984\n0.984 - 0.981\n1.000 - 0.953\n0.987 - 0.959\n5%\n0.984 - 0.972\n0.984 - 0.966\n0.994 - 0.957\n0.966 - 0.975\n10%\n0.942 - 0.985\n0.944 - 0.988\n0.979 - 0.985\n0.972 - 0.948\n15%\n0.906 - 0.981\n0.917 - 0.985\n0.944 - 0.988\n0.924 - 0.971\nSatimage (4)\n2%\n0.984 - 0.933\n0.984 - 0.933\n0.992 - 0.926\n0.984 - 0.933\n(+)\n5%\n0.969 - 0.941\n0.977 - 0.941\n0.992 - 0.889\n0.964 - 0.985\n(+)\n10%\n0.907 - 0.948\n0.920 - 0.948\n0.961 - 0.918\n0.917 - 0.985\n(+)\n15%\n0.869 - 0.940\n0.910 - 0.978\n0.928 - 0.955\n0.905 - 0.993\n(+)\nSatimage (5)\n2%\n0.980 - 0.948\n0.979 - 0.935\n0.993 - 0.869\n0.966 - 0.941\n5%\n0.967 - 0.961\n0.966 - 0.947\n0.993 - 0.882\n0.931 - 0.974\n10%\n0.937 - 0.955\n0.932 - 0.968\n0.942 - 0.929\n0.921 - 0.968\n15%\n0.876 - 0.993\n0.883 - 0.953\n0.898 - 0.940\n0.865 - 9.980\nSatimage (6)\n2%\n0.997 - 0.977\n0.994 - 0.977\n1.000 - 0.951\n0.994 - 0.986\n(+)\n5%\n0.977 - 0.968\n0.977 - 0.971\n0.988 - 0.942\n0.980 - 0.991\n(+)\n10%\n0.948 - 0.986\n0.950 - 0.986\n0.977 - 0.980\n0.961 - 0.980\n15%\n0.910 - 0.972\n0.931 - 0.952\n0.941 - 0.997\n0.966 - 0.957\nSegment (1)\n2%\n0.982 - 0.973*\n0.982 - 0.982*\n1.000 - 0.855\n1.000 - 0.873\n5%\n0.981 - 0.972\n0.972 - 0.981\n1.000 - 0.906\n1.000 - 0.934\n10%\n0.915 - 1.000\n0.938 - 0.991\n1.000 - 1.000*\n0.945 - 0.972\n15%\n0.945 - 0.963\n0.938 - 0.981\n0.973 - 1.000*\n0.919 - 0.953\nSegment (2)\n2%\n0.991 - 0.955\n0.990 - 0.936\n1.000 - 0.882\n1.000 - 0.927\n5%\n0.981 - 0.972\n0.981 - 0.962\n1.000 - 0.934\n0.955 - 0.991\n(+)\n10%\n0.910 - 0.944\n0.927 - 0.944*\n1.000 - 0.972\n0.955 - 0.991\n15%\n0.855 - 0.991\n0.851 - 0.963*\n0.964 - 1.000*\n0.990 - 0.897\nSegment (3)\n2%\n0.981 - 0.918\n0.981 - 0.936\n0.990 - 0.927\n1.000 - 0.982\n(+)\n5%\n0.950 - 0.896\n0.949 - 0.887\n0.939 - 0.877\n0.962 - 0.943\n(+)\n10%\n0.909 - 0.935\n0.925 - 0.925\n0.951 - 0.916\n0.904 - 0.972\n(+)\n15%\n0.862 - 0.935\n0.860 - 0.916\n0.907 - 0.907\n0.866 - 0.963\n(+)\nSegment (4)\n2%\n1.000 - 0.945\n1.000 - 0.955\n0.990 - 0.918\n0.991 - 1.000\n(+)\n5%\n0.952 - 0.934\n0.981 - 0.962\n0.990 - 0.925\n0.981 - 0.991\n(+)\n10%\n0.937 - 0.972\n0.945 - 0.963\n0.971 - 0.925\n0.922 - 0.991\n(+)\n15%\n0.898 - 0.991\n0.906 - 0.991\n0.927 - 0.953\n0.869 - 0.991\nSegment (5)\n2%\n0.981 - 0.945\n0.981 - 0.964\n0.990 - 0.927\n0.991 - 0.964\n(+)\n5%\n0.952 - 0.934\n0.962 - 0.962\n0.963 - 0.972\n0.955 - 0.991\n(+)\n10%\n0.917 - 0.925\n0.920 - 0.963\n0.936 - 0.963\n0.921 - 0.981\n(+)\n15%\n0.864 - 0.953\n0.858 - 0.963\n0.938 - 0.981*\n0.862 - 0.991\nSegment (6)\n2%\n0.991 - 0.964\n0.991 - 0.964\n1.000 - 0.909\n0.990 - 0.936\n5%\n0.971 - 0.943\n0.980 - 0.934\n0.990 - 0.925\n0.981 - 0.953\n(+)\n10%\n0.955 - 0.991\n0.964 - 0.991\n0.981 - 0.991*\n0.930 - 0.991\n15%\n0.946 - 0.981\n0.946 - 0.981\n0.964 - 0.991*\n0.898 - 0.991\nSegment (7)\n2%\n1.000 - 0.918\n1.000 - 0.909\n1.000 - 0.945\n1.000 - 0.982\n(+)\n5%\n0.970 - 0.925\n0.970 - 0.906\n1.000 - 0.925\n1.000 - 0.972\n(+)\n10%\n0.909 - 0.935\n0.919 - 0.953*\n1.000 - 0.981*\n0.886 - 0.944\n15%\n0.863 - 0.944\n0.871 - 0.944\n0.939 - 1.000*\n0.875 - 0.981\nTable 8: Results (P-R) - Approach B\n4.2.2. Based on the one vs rest strategy \u2013 Approach B In this case, the multi-class problems related to the considered datasets are converted to one-class problems in which the representatives of the other classes are considered as outliers, injected in proportions of 2, 5, 10, 15 % of the oneclass dataset sizes. In such a situation, we can expect that reference methods such as OCSVM and iForest perform better since they handle the data of each class separately. De facto, the OC-Tree shows overall smaller differences in performance.\n# 5. Application to the diagnosis of ADHD\nIn the previous section, we compared the OC-Tree on benchmark datasets with reference one-class methods, against which it proved to perform favorably. In the present section, we propose a real-world case study in which an algorithm such as the OC-Tree is worth considering. The application is related to the diagnosis of Attention Deficit Hyperactivity Disorder (ADHD).\n# 5.1. Problem statement\nADHD is a neurodevelopmental disorder in children which has been subject to a considerable number of studies, including those conducted on the ADHD200 collection [36]. This open and free database has been made available since 2012 in order to advance the state of knowledge about ADHD [37]. The epidemiology of ADHD depends on gender, and evidence suggests that the disorder affects more often boys than girls [38]. Such a gender-differentiated distribution poses some concerns about the development of diagnosis aid models through multi-class classification. Indeed, unbalanced distributions of ADHD and NeuroTypical (NT) subjects are often observed for each gender group in the training sets related to ADHD. This applies to the ADHD-200 collection, and more particularly to the corresponding NYU data subset. The boys\u2019 training sample includes approximately twice as many ADHD subjects as NT ones, and the reverse trend is observed in the girls\u2019 training sample. Fig. 7 presents the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a720/a72072c8-e07f-41e5-bcee-18c8fa11dc83.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Confusion matrices achieved on the NYU test set for boys (left) and girls (right), based on a multi-class decision tree [39]</div>\nconfusion matrices related to the predictions recently achieved on the NYU test set for boys and girls, based on a multi-class decision tree (according to the methodology proposed in [39]). Actually, these results show the effects of class unbalance within each gender group in the training set. Though providing an overall satisfactory predictive accuracy, the final predictive model has a high (resp. low) sensitivity and a low (resp. high) specificity in boys (resp. girls). This bias is among the reasons that explain the limited applicability of such a binary predictive model in the clinical practice setting. The OC-Tree may alleviate this issue. We thus propose to tackle ADHD diagnosis on a genderdifferentiated basis, in focusing on the description of the neuropathology with the OC-Tree.\n# 5.2. Data\nWe consider the preprocessed ADHD-200 collection [40], and focus on the NYU sample. Table 9 presents the distribution of the training and test data, based on the gender and the diagnostic labels. For each subject, the sample includes blood-oxygen-level-dependent signals [41], at resting-state, given a brain parcellation in 90 regions of interest (cf. AAL90 atlas [42]). We considered the variance of the signals as predictors, since they proved to achieve successful predictions [43, 39]. They were computed for brain regions included in two functional systems which were associated to ADHD-related abnormalities in the literature: the limbic system [39] and the Default Mode Network (DMN) [44, 45].\n# 5.3. Tuning and assessment\nIn this context, a quick visualization of the data shows that the instances are concentrated within a single grouping. Thus, there are no clusters to raise:\nTraining set\nNT\n50\n43\nADHD\n25\n92\nTest set\nNT\n4\n8\nADHD\n9\n20\nTable 9: Distribution of the NYU sample considered in our study\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6240/6240b99f-60fc-4872-8d18-e8a3199eda12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Cross-validation procedure used to tune the OC-Tree for ADHD prediction</div>\n<div style=\"text-align: center;\">Girls Boys</div>\nthe models may be reduced to a set of descriptive rules. This means that the parameter \u03b1 has no influence here. Five values were considered in order to tune parameter \u03bd = {0.05, 0.1, 0.15, 0.2, 0.25}. The parameter was tuned through a 5-fold CV procedure, which is depicted in Fig. 8. The NT subjects of the training set are fully used at each iteration as a test fold in combination to the one extracted from the partitioning of the ADHD training set into 5 folds. In OCC, performance metrics such as those presented in Sec. 3.3 are generally computed with regards to the target class, i.e. ADHD in this case. However, in the specific case of psychiatric diagnosis, there is a need for cautious predictions, even though that would imply to wrongly predict a subject as neurotypical [46]. In other terms, high specificity and a reasonable level of sensitivity are requirements that a predictive model should meet in this context. We thus propose to assess the model towards its capability to predict NT cases, and thus to compute the metrics with respect to the NT group. The models which achieve the best F1-score and precision were held as relevant for boys and girls respectively. Indeed, let us recall that our choice to assess the performance of the OC models towards the class of typical controls is motivated by the need to favor high levels of specificity. However, this is an already existing trend in girls, given that there are generally more NT girls than ADHD ones. Thus, to avoid falling into the traps of a somewhat insensitive model and to ensure that ADHD cases are predicted in a reasonable number of situations, we focus on the precision whose maximization is achieved through a minimization of the number of false NT subjects.\n# 5.4. Classification framework\nOn a gender-differentiated basis, we need to predict a diagnosis based on the activity of brain regions included in the limbic system and/or the DMN. As announced, this is achieved through the practice of OCC, in targeting the ADHD group. For such a purpose, we assessed the relevance of four distinct options presented below. \u2022 O1: train the OC-Tree model on the features related to the limbic system.\n\u2022 O4: constitute an ensemble of OC classifiers by the aggregation of two models trained on the limbic and DMN features separately.\nIn the case of the fourth option, a subject is diagnosed with ADHD once he/she tests positive with both models. In the other cases, the subject is predicted as disease-free, concerned with the need for a cautious diagnosis [46].\n# 5.5. Final models and performance\nIn boys, the ensemble strategy as defined by option O4 appeared to be the most successful, with a F1-score of 65.3% on the training set (\u03bd = 0.25). The results were most tightly contested in girls between option O1 and O3, yielding respectively precision rates of 93.6% and 93.3% (\u03bd = 0.15). We selected the latter as a final model since it provides a more detailed description of the pathology than O1, which is based only on two rules. Fig. 9 presents the final confusion matrices for boys and girls. Tables 10 and 11 present the decision rules (expressed in terms of the logarithm of the variance) related to boys and girls respectively. Note that (L) and (R) denote brain regions included in the Left and Right hemispheres respectively. Our results confirm that the resting-state activity of both the limbic system and the DMN brings some discriminative information for ADHD diagnosis. The mental condition appears to be more complex to describe in boys, and requires the combination of two distinct models. The girls\u2019 model is by contrast more minimalist. These important differences between boys and girls in terms of models strengthens our conviction that a gender-differentiated classification is definitely pertinent. In alleviating the issue of class imbalance within each gender group, we could improve the balance between the diagnostic specificity and sensitivity. If we compare with the confusion matrices presented in Figs. 7 and 9, in boys, the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f571/f5712e40-5378-4eb5-8622-78c26205c47e.png\" style=\"width: 50%;\"></div>\nPredicted as \u25b7\nNT\nADHD\nNT\n4\n0\nADHD\n1\n8\nPredicted as \u25b7\nNT\nADHD\nNT\n6\n2\nADHD\n6\n14\nPredicted as \u25b7\nNT\nADHD\nNT\n4\n0\nADHD\n1\n8\n# Figure 9: Confusion matrix achieved by the OC-Tree on the NYU test set for boys (left) and girls (right)\nimprovement made on specificity (75% against 37.5% previously), was achieved at the expense of the sensitivity (70% against 95% previously). In girls, the sensitivity was doubled without loss of specificity. The overall prediction accuracy was improved as well (78.0% against 73.2%).\n# 6. Discussion\nIn Sec. 4, we showed that the OC-Tree presents favorable performances in comparison to reference methods such as ClusterSVDD, OCSVM, and iForest, in similar conditions. Depending on the targeted objectives, the OC-Tree may be a wise choice to achieve a OCC task. The presence of noise in the data may impair the performances of a method like ClusterSVDD, while as a density-based method, the OC-Tree shows more ability to reject such outliers in the data. Moreover, the OC-Tree is developed to be as compact as possible, which constitutes a key to interpretability. Indeed, the predictive model is based on the most discriminative attributes to achieve OCC while ClusterSVDD and OCSVM do not consider such a selection; the corresponding models are computed based on the whole set of training attributes. The OC-Tree also detects automatically the number of groupings related to the class targeted by the classification. This constitutes a significant advantage compared to ClusterSVDD which requires to set the number of possible clusters as an input parameter. As compared to the iForest technique, the OC-Tree is more compact and readable while being able at the same time to perform outlier rejection. Finally, the OC-Tree better fits to the structure of the data as compared to OCSVM, since it allows the detection of sub-concepts of a single class as target groupings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c720/c720e2f3-0396-4823-badd-09d68d9704df.png\" style=\"width: 50%;\"></div>\nIn Sec. 5, we were interested in a case study related to the diagnosis of ADHD. Through this study, we could: \u2022 show the interest of considering the OC-Tree rather than a multi-class decision tree, given the effective availability of the data ; \u2022 highlight the advantageous interpretability of the OC-Tree, which is an important characteristic towards a concrete clinical applicability; \u2022 consider one-class ensembles that may help in modeling complex conditions while preserving the interpretability of the predictive framework. These promising results tend to show that our model may be transposable to medical practice as a diagnosis aid tool.\nIn Sec. 5, we were interested in a case study related to the diagnosis of ADHD. Through this study, we could: \u2022 show the interest of considering the OC-Tree rather than a multi-class decision tree, given the effective availability of the data ; \u2022 highlight the advantageous interpretability of the OC-Tree, which is an important characteristic towards a concrete clinical applicability; \u2022 consider one-class ensembles that may help in modeling complex conditions while preserving the interpretability of the predictive framework. These promising results tend to show that our model may be transposable to medical practice as a diagnosis aid tool.\nThese promising results tend to show that our model may be transposable to medical practice as a diagnosis aid tool.\n# 7. Conclusion & future work\nIn some applications, the limited availability of data has lead to look for alternatives to the traditional supervised techniques. The practice of One-Class Classification (OCC) has been considered in this context. This area of machine learning has generated a considerable interest with the development of new methods, some of which were adapted from supervised classification techniques. In this work, we proposed a one-class decision tree by completely rethinking the splitting mechanism considered to build such models. Our One-Class Tree (OC-Tree) may be actually seen as an adaptation of the Kernel Density Estimation (KDE) for the sake of interpretability, based on a subset of significant attributes for the purpose of prediction. In that respect, our method has shown favorable performances in comparison to reference methods such as ClusterSVDD, one-class SVM and isolation forest. Against these approaches, our one-class model is quite simple while being in the same time transparent and performant. Such qualities are particularly valuable for medical diagnosis, where a balanced representation of the classes is not always ensured. We could illustrate the benefits of the OC-Tree for the diagnosis of ADHD. Our\nresults show that the OC-Tree constitutes a step towards greater applicability of diagnosis aid models. This work leaves some interesting perspectives. In particular, the parametriza tion of the KDE remains an open question as regards the computation of the bandwidth h and the use of other kernels K. Indeed, on the one hand, our proposal is based on a Gaussian kernel attractive by its mathematical properties, but the pertinence of other configurations may be studied on a comparative basis. On the other hand, deduced based on the Silverman\u2019s rule of thumb, h is quite sensitive to the training set content. In our proposal, this sensitivity is controlled by setting a pre-pruning mechanism. In the future, we would like to rise to the challenge of establishing a rule able to address this issue of sensitivity.\n# 8. Acknowledgments\nSarah Itani is a research fellow of Fonds de la Recherche Scientifique - FNRS (F.R.S.- FNRS).\n# References\n[1] M. M. Moya, M. W. Koch, L. D. Hostetler, One-class classifier networks for target recognition applications, Tech. rep., Sandia National Labs., Albuquerque, NM (United States) (1993). [2] S. S. Khan, M. G. Madden, A survey of recent trends in one class classification, in: Artificial Intelligence and Cognitive Science: 20th Irish Conference (AICS 2009), Springer, 2009, pp. 188\u2013197. [3] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM computing surveys (CSUR) 41 (3) 15:1\u201315:58. [4] B. Sch\u00a8olkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, R. C. Williamson, Estimating the support of a high-dimensional distribution, Neural computation 13 (7) (2001) 1443\u20131471.\n[5] D. Tax, One-class classification: Concept learning in the absence of counter examples, Delft University of Technology (2001) 202. [6] H. Yu, Single-class classification with mapping convergence, Machine Learning 61 (1-3) (2005) 49\u201369. [7] M. Wu, J. Ye, A small sphere and large margin approach for novelty detection using training data with outliers, IEEE transactions on pattern analysis and machine intelligence 31 (11) (2009) 2088\u20132092. [8] Y. Xiao, B. Liu, L. Cao, X. Wu, C. Zhang, Z. Hao, F. Yang, J. Cao, Multi-sphere support vector data description for outliers detection on multidistribution data, in: IEEE International Conference on Data Mining Workshops, IEEE, 2009, pp. 82\u201387. [9] T. Le, D. Tran, W. Ma, D. Sharma, A theoretical framework for multisphere support vector data description, in: 17th International Conference on Neural Information Processing. Models and applications, Springer, 2010, pp. 132\u2013142. 10] P. Nguyen, D. Tran, T. Le, T. Hoang, D. Sharma, Multi-sphere support vector data description for brain-computer interface, in: 4th International Conference on Communications and Electronics (ICCE), IEEE, 2012, pp. 318\u2013321. 11] J. Yang, T. Deng, R. Sui, An adaptive weighted one-class SVM for robust outlier detection, in: 2015 Chinese Intelligent Systems Conference, Springer, 2016, pp. 475\u2013484. 12] V. Barnab\u00b4e-Lortie, C. Bellinger, N. Japkowicz, Active learning for one-class classification, in: 14th International Conference on Machine Learning and Applications (ICMLA), IEEE, 2015, pp. 390\u2013395. 13] N. G\u00a8ornitz, L. A. Lima, K.-R. M\u00a8uller, M. Kloft, S. Nakajima, Support vector data descriptions and k-means clustering: One class?, IEEE transactions on neural networks and learning systems.\n[14] J. A. Hartigan, J. Hartigan, Clustering algorithms, Vol. 209, Wiley New York, 1975. [15] A. Holzinger, C. Biemann, C. Pattichis, D. Kell, What do we need to build explainable ai systems for the medical domain?, arXiv preprint arXiv:1712.09923. [16] S. Hara, K. Hayashi, Making tree ensembles interpretable, arXiv preprint arXiv:1606.05390. [17] M. T. Ribeiro, S. Singh, C. Guestrin, Why should I trust you? Explaining the predictions of any classifier, in: 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2016, pp. 1135\u20131144. [18] J. R. Quinlan, Induction of decision trees, Machine learning 1 (1) (1986) 81\u2013106. [19] C. D\u00b4esir, S. Bernard, C. Petitjean, L. Heutte, One class random forests, Pattern Recognition 46 (12) (2013) 3490\u20133506. [20] N. Goix, N. Drougard, R. Brault, M. Chiapino, One class splitting criteria for random forests, arXiv preprint arXiv:1611.01971. [21] F. T. Liu, K. M. Ting, Z.-H. Zhou, Isolation forest, in: 8th IEEE International Conference on Data Mining, IEEE, 2008, pp. 413\u2013422. [22] B. W. Silverman, Density estimation for statistics and data analysis, Vol. 26, CRC press, 1986. [23] P. Smyth, A. Gray, U. M. Fayyad, Retrofitting decision tree classifiers using kernel density estimation, in: Machine Learning Proceedings 1995, Elsevier, 1995, pp. 506\u2013514. [24] J. R. Quinlan, C4. 5: programs for machine learning, San Mateo: Morgan Kaufmann, 1993.\n[25] M. C. Jones, J. S. Marron, S. J. Sheather, A brief survey of bandwidth selection for density estimation, Journal of the American Statistical Association 91 (433) (1996) 401\u2013407. [26] Q. Li, J. S. Racine, Nonparametric econometrics: theory and practice, Princeton University Press, 2007. [27] K. Hempstalk, E. Frank, I. H. Witten, One-class classification by combining density and class probability estimation, in: Machine Learning and Knowledge Discovery in Databases: European Conference (ECML PKDD 2008), Springer, 2008, pp. 505\u2013519. [28] R. Bellman, Dynamic Programming, Princeton University Press, Princeton, 1957. [29] D. Wang, D. S. Yeung, E. C. Tsang, Structured one-class classification, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 36 (6) (2006) 1283\u20131295. [30] D. T. Nguyen, K. J. Cios, Rule-based OneClass-DS learning algorithm, Applied Soft Computing 35 (2015) 267\u2013279. [31] V. Fragoso, W. Scheirer, J. Hespanha, M. Turk, One-class slab support vector machine, in: 23rd International Conference on Pattern Recognition (ICPR), IEEE, 2016, pp. 420\u2013425. [32] S. Wang, L. Zhao, E. Zhu, J. Yin, H. Yang, Ensemble one-class extreme learning machine based on overlapping data partition, in: International Conference on Cognitive Systems and Signal Processing, Springer, 2016, pp. 408\u2013416. [33] M. Lichman, UCI machine learning repository (2013). URL http://archive.ics.uci.edu/ml [34] C.-C. Chang, C.-J. Lin, LIBSVM: a library for support vector machines, ACM transactions on intelligent systems and technology (TIST) 2 (3) (2011) 27.\n[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research 12 (2011) 2825\u20132830. [36] P. Bellec, C. Chu, F. Chouinard-Decorte, Y. Benhajali, D. S. Margulies, R. C. Craddock, The Neuro Bureau ADHD-200 Preprocessed Repository, Neuroimage 144 (2017) 275\u2013286. [37] M. P. Milham, D. Fair, M. Mennes, S. H. Mostofsky, et al., The ADHD-200 consortium: a model to advance the translational potential of neuroimaging in clinical neuroscience, Frontiers in systems neuroscience 6 (2012) 62. [38] ADHD Institute, ADHD Epidemiology \u2014 ADHD Institute, http: //adhd-institute.com/burden-of-adhd/epidemiology/, [Online; accessed 18-06-2017] (2017). [39] S. Itani, M. Rossignol, F. Lecron, P. Fortemps, Towards interpretable machine learning models for diagnosis aid: A case study on attention deficit/hyperactivity disorder, PloS one 14 (4) (2019) e0215720. [40] The Neuro Bureau, NITRC: neurobureau:AthenaPipeline - NITRC Wiki, http://www.nitrc.org/plugins/mwiki/index.php/neurobureau: AthenaPipeline, [Online; accessed 18-06-2017] (2011). [41] G. Aguirre, E. Zarahn, M. D\u2019esposito, The variability of human, BOLD hemodynamic responses, Neuroimage 8 (4) (1998) 360\u2013369. [42] N. Tzourio-Mazoyer, B. Landeau, D. Papathanassiou, F. Crivello, O. Etard, N. Delcroix, B. Mazoyer, M. Joliot, Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain, Neuroimage 15 (1) (2002) 273\u2013289.\n41] G. Aguirre, E. Zarahn, M. D\u2019esposito, The variability of human, BOLD hemodynamic responses, Neuroimage 8 (4) (1998) 360\u2013369.\n[42] N. Tzourio-Mazoyer, B. Landeau, D. Papathanassiou, F. Crivello, O. Etard, N. Delcroix, B. Mazoyer, M. Joliot, Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain, Neuroimage 15 (1) (2002) 273\u2013289.\n[43] S. Itani, F. Lecron, P. Fortemps, A multi-level classification framework for multi-site medical data: Application to the adhd-200 collection, Expert Systems with Applications 91 (2018) 36 \u2013 45. [44] L. Tamm, M. E. Narad, T. N. Antonini, K. M. OBrien, L. W. Hawk, J. N. Epstein, Reaction time variability in adhd: a review, Neurotherapeutics 9 (3) (2012) 500\u2013508. [45] S. J. Broyd, C. Demanuele, S. Debener, S. K. Helps, C. J. James, E. J. Sonuga-Barke, Default-mode brain dysfunction in mental disorders: a systematic review, Neuroscience & biobehavioral reviews 33 (3) (2009) 279\u2013 296. [46] S. Itani, F. Lecron, P. Fortemps, Specifics of medical data mining for diagnosis aid: A survey, Expert systems with applications 118 (2019) 300 \u2013 314.\n",
    "paper_type": "method",
    "attri": {
        "background": "One-class Classification (OCC) is an area of machine learning that addresses prediction based on unbalanced datasets, where current models perform well but lack interpretability. This paper introduces a new method, the One-Class decision Tree (OC-Tree), which combines density estimation with tree-based learning to enhance both performance and interpretability.",
        "problem": {
            "definition": "The problem addressed is the challenge of effectively classifying data in one-class scenarios, particularly when dealing with unbalanced datasets where only a single class is represented.",
            "key obstacle": "Existing methods struggle with interpretability and performance, especially in high-dimensional spaces, making it difficult to understand the decision-making process of the models."
        },
        "idea": {
            "intuition": "The idea stems from the need to develop interpretable machine learning models that can effectively handle one-class classification tasks while maintaining high performance.",
            "opinion": "The proposed OC-Tree method integrates kernel density estimation within a decision tree framework, allowing for effective classification of target instances while providing clear decision rules.",
            "innovation": "Unlike traditional decision trees that may not adequately address one-class scenarios, the OC-Tree employs a density-based hierarchical splitting mechanism, enhancing its ability to isolate target instances."
        },
        "method": {
            "method name": "One-Class decision Tree",
            "method abbreviation": "OC-Tree",
            "method definition": "The OC-Tree is a one-class classification method that utilizes kernel density estimation to create a decision tree structure that encloses target instances within hyper-rectangles.",
            "method description": "The OC-Tree combines decision tree induction with kernel density estimation to achieve effective one-class classification.",
            "method steps": [
                "Estimate the probability density function of target instances using kernel density estimation.",
                "Identify significant sub-intervals based on the density estimation to split the data.",
                "Assess the quality of splits using an impurity measure adapted for one-class classification.",
                "Recursively apply the above steps until stopping conditions are met."
            ],
            "principle": "The effectiveness of the OC-Tree lies in its ability to leverage density estimation to accurately identify and isolate target instances while maintaining interpretability through a structured decision tree format."
        },
        "experiments": {
            "evaluation setting": "The OC-Tree was evaluated against benchmark datasets using reference methods such as ClusterSVDD, OCSVM, and iForest, with a focus on precision and recall metrics to assess performance.",
            "evaluation method": "Performance was measured using precision-recall metrics, with evaluations conducted through cross-validation on training sets and comparisons made against established one-class classification methods."
        },
        "conclusion": "The OC-Tree demonstrated superior performance compared to existing one-class classification methods in various experimental settings, highlighting its potential for practical applications, especially in medical diagnosis.",
        "discussion": {
            "advantage": "The OC-Tree offers enhanced interpretability and performance in one-class classification tasks, making it suitable for applications requiring clear decision-making processes.",
            "limitation": "The method's performance may still be affected by the presence of noise in the data, and further research is needed to optimize its parameters for different datasets.",
            "future work": "Future research will focus on refining the kernel density estimation process and exploring alternative kernels to improve the OC-Tree's robustness and applicability across various domains."
        },
        "other info": {
            "acknowledgments": "Sarah Itani is a research fellow of Fonds de la Recherche Scientifique - FNRS (F.R.S.-FNRS)."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem addressed is the challenge of effectively classifying data in one-class scenarios, particularly when dealing with unbalanced datasets where only a single class is represented."
        },
        {
            "section number": "2.1",
            "key information": "Existing methods struggle with interpretability and performance, especially in high-dimensional spaces, making it difficult to understand the decision-making process of the models."
        },
        {
            "section number": "3.5",
            "key information": "The proposed OC-Tree method integrates kernel density estimation within a decision tree framework, allowing for effective classification of target instances while providing clear decision rules."
        },
        {
            "section number": "3.5",
            "key information": "The OC-Tree employs a density-based hierarchical splitting mechanism, enhancing its ability to isolate target instances."
        },
        {
            "section number": "5.1",
            "key information": "The OC-Tree offers enhanced interpretability and performance in one-class classification tasks, making it suitable for applications requiring clear decision-making processes."
        },
        {
            "section number": "7.1",
            "key information": "The method's performance may still be affected by the presence of noise in the data, and further research is needed to optimize its parameters for different datasets."
        },
        {
            "section number": "7.2",
            "key information": "Future research will focus on refining the kernel density estimation process and exploring alternative kernels to improve the OC-Tree's robustness and applicability across various domains."
        }
    ],
    "similarity_score": 0.6097522074186756,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/A One-Class Classification Decision Tree Based on Kernel Density Estimation.json"
}