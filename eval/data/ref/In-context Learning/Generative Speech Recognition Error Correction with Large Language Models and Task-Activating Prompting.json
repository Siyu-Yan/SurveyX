{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.15649",
    "title": "Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting",
    "abstract": "We explore the ability of large language models (LLMs) to act as speech recognition post-processors that perform rescoring and error correction. Our first focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zeroand few-shot in-context learning, and a novel \u201ctask activation\u201d prompting method that combines causal instructions and demonstration to increase its context windows. Next, we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs, using a pretrained first-pass recognition system and rescoring output on two out-of-domain tasks (ATIS and WSJ). By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs. Index Terms: large language model, N-best rescoring, instruction prompting, few-shot learning, in-context learning.",
    "bib_name": "yang2023generativespeechrecognitionerror",
    "md_text": "# GENERATIVE SPEECH RECOGNITION ERROR CORRECTION WITH LARGE LANGUAGE MODELS AND TASK-ACTIVATING PROMPTING\nChao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, Andreas Stolcke\nAmazon, USA\n# ABSTRACT\nWe explore the ability of large language models (LLMs) to act as speech recognition post-processors that perform rescoring and error correction. Our first focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zeroand few-shot in-context learning, and a novel \u201ctask activation\u201d prompting method that combines causal instructions and demonstration to increase its context windows. Next, we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs, using a pretrained first-pass recognition system and rescoring output on two out-of-domain tasks (ATIS and WSJ). By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs. Index Terms: large language model, N-best rescoring, instruction prompting, few-shot learning, in-context learning.\n# 1. INTRODUCTION\nLarge-scale language models (LLMs) have exhibited outstanding performance on downstream tasks by conditioning on input information, including task descriptions (e.g., performing mathematical calculations) or a limited number of input-output pairs obtained from training text (e.g., goaloriented demonstrations). This new capability of task-specific inference from contextual information has been referred to as \u201cin-context learning\u201d in Brown et al. [1]. More specifically, the ability to learn in-context has been reported in previous studies [2] of pretrained LLMs with over 100B parameters trained with an unsupervised auto-regressive objective. Although recent advances in in-context learning have consistently demonstrated excellent performance on a wide range of tasks [3], there have been limited studies on the interaction or benefits of in-context learning on automatic speech recognition (ASR) tasks. As an example, contextual information [4] has been shown to play a vital role on ASR applications in complex domains, such as recognizing utterances referring to trending news. One open question in the development of robust ASR applications is how recent in-context learning frameworks can\nutilize their zero-shot learning capability to enhance ASR systems. Meanwhile, scaling ASR model sizes up to 10B parameters [5] by itself has not proven adequate for achieving high performance on challenging (e.g., conversational) speech tasks from domain-specific data. The challenge to obtain better generalization of neural ASR models has motivated proposals to incorporate external knowledge from textual data [6]. For instance, one way to improve the RNNtransducer is to incorporate an external LM [7] for domainaware adaptation in streaming-based applications. However, the external LM size is often limited to a range of 10M to 100M for on-device deployment. Given these limitations, cloud-based second-pass rescoring with LLMs may be a promising approach that leverages frozen pretrained models and leverages in-context learning. Toward this end, in this work we explore novel ASR post-processing pipelines that utilize frozen LLMs by exploiting in-context learning. We consider two ASR second-pass pipelines, as shown in Figure 1: Pipeline 1: a standard rescoring system takes in N-best output from a first ASR pass, and is trained to minimize the word error rate (MWER) by reranking the hypotheses. As illustrated in Figure 1(a), an LLM in-context learning process is inserted into the pipeline to post-process first-pass hypotheses to apply error correction. Pipeline 2: a new task-activating prompting method is used to initialize the frozen LLM with task-oriented instruc-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aab9/aab95b86-aa96-4a40-907e-ab2f70c083e8.png\" style=\"width: 50%;\"></div>\nFig. 1. Two ASR post-processing frameworks using LLMs: (a) correct errors (e.g., grammar [8]) before applying a standard rescoring model, or (b) perform zero/few-shot rescoring; with optional task-activating prompting (Section 3.2).\ntions. A list of N-best ASR hypotheses is formatted as input to the LLM, thus allowing \u201cin-context learning initialization\u201d and/or \u201cin-domain fine-tuning\u201d (e.g., using adapters for parameter-efficient model update) that results in an improved speech transcription. In the remaining sections we present a first exploration of this novel way to utilize LLMs for the ASR task, demonstrate its surprising effectiveness, and compare results with different in-context learning schemes, as well as those of standard rescoring methods.\n# 2. RELATED WORK\nLLM-based post-processing to improve hypotheses. Error correction post-processing [9, 10] aims to fix grammar or deletion errors in output sentences and has been shown to improve the first-pass hypotheses generated from end-to-end ASR. A key characteristic of correction techniques is their reliance on pretrained LLMs, which benefit from rich contextual information. Liao et al. [9] propose ASR post-processing for readability, by extracting semantic expressions and generating readable text from ASR transcriptions. N-best T5 [11] used the T5 encoder-decoder architecture for rescoring with discriminative training. Zero-shot learning for acoustic and language modeling. Prior work has demonstrated that language modeling can generalize to zero-shot multi-tasks without exemplars [3, 12, 13]. However, zero-shot and few-shot language modeling techniques often rely on the fine-tuning, which requires redeployment of pretrained models.\n# In-context learning based on information prompting.\nIn-context learning (ICL) [1, 14] induces a single model to perform domain-agnostic inference without fine-tuning by providing a single or few prompts, thus addressing the aforementioned limitations. Prior study [2] has shown the ground truth demonstrations impose smaller effect than the author expected and significant zero-shot performance improvement under ICL framework. It implies external information gain can be extracted from frozen pretrained LLMs itself, if we select correct prompting strategy. However, ICL has its own shortcomings regarding tasks of reasoning. The chainof-thought (CoT) prompting [15] decomposes the reasoning tasks by providing models with a sequence of questions or prompts that gradually guide the model to make predictions for a target task. While employment of CoT prompting is attributed to few-shot setup, LLMs has been proven to be the zero-shot reasoner given the single and specific prompt [16]. In this work, we applied the above ICL techniques to ASR rescoring for the first time and empirically evaluate the their performance individually against the baseline.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a5d/5a5de6cd-fa18-4c8a-9b5d-0c1dc8e70175.png\" style=\"width: 50%;\"></div>\n# Fig. 2. Four LLM in-context learning uses for ASR 2nd pass 3. METHOD\nWe now review some recent advances in in-context learning techniques [15, 1, 16] and describe how they can be incorporated into second-pass rescoring applications.\n# 3.1. In-Context Learning Background and Techniques\nIn-context learning [1] can emerge from modeling long-range coherence in the pretraining data. Based on a recent theoretical justification [17] by Bayesian inference, LLM would have implicitly learned to infer a latent concept during its pretraining stage. As an empirical result, in-context learning occurs if the LM can still infer the shared concept across examples (e.g., task instruction or prompts) to perform a target task. To model the in-context learning process, we can formulate its distribution over token o within the vocabulary O by sampling a latent confounding variable [18] \u03b8 from its population \u0398. The prediction over the pretraining distribution could be inferred by marginalizing over the confounding variable \u03b8:\n(1)\nUnder the in-context learning framework, prediction sequence Oi is inferred from the pretraining distribution conditioned on a prompt variable \u03b8\u2217, test-time sample (questions we would like to answer) xtest and its in-context predictor pprompt(y|x):\nFor instance, a simple prompt to empower in-context learning is to directly provide a \u201ctask-oriented question\u201d to the pretrained LLM, as shown in Figure 2(a). We further illustrate more in-context learning setups in the following subsections.\n# 3.1.1. Zero-shot domain-hint prompting\nIn the zero-shot setting, given a prompt template function r() and \u03b8\u2217as the domain-specific confounding variable (e.g., air-\nline travel), a pretrained LLM models the conditional probability of the original input x and target y, even if they were never trained, into their template function rx(x) and ry(y).\n(3)\nIn this work, we consider two general acoustic domains for making template function as a hard-coded input of \u201cairline information\u201d or \u201cfinancial market\u201d, as shown in Figure 2(b).\nZero-shot reasoning [16] employs chain-of-thought prompting [15] in a zero-shot setting with only two prompts: (i) reasoning extraction and (ii) answer extraction. Based on [16], the reasoning extraction step uses a fixed and canonical prompt: Let\u2019s think step by step, as shown in Figure 2(c). In our experiments, we noticed that the reasoning extraction prompt is essential to boost the performance of zero-shot LLM rescoring. Under this zero-shot self-reasoning setup, the LLM output will first explain the task it is working on, then produce the actual task output. In the case of zero-shot rescoring, the LLM will first define a ASR-LM rescoring tasks and then provide LM scores for each N-best hypothesis.\n3.1.3. Few-shot and one-shot in-context learning\nA standard few-shot in-context learning process uses pairs of demonstrations [1] \u201cquestions and targeted tasks,\u201d retrieved from training data, to inform frozen LLMs for performing the target output, as illustrated in Figure 2(d). One-shot incontext learning takes place by using a single demonstration as an input prompt for the frozen LLMs. Note that demonstrations (from an unseen training set) are distinct from test examples, and that the unsupervised-trained LLM has been reported to have a memory bottleneck based on term frequencies [19], which avoids potential data leakage issues for its few-shot learning evaluation reported in previous work [2, 1, 19].\n3.1.4. N-best hypotheses to transcription fine-tuning\nWe introduce a hypotheses to transcription (H2T) mapping loss function: LH2T = \ufffdN i=1 \u2212{log P(y\u2217|xi, \u0398) + \u03bb \u00b7 MSE(si, P(y\u2217|x(i), \u0398))}, where P(y\u2217|xi, \u0398) represents the probability of the true transcription (y\u2217) given the i-th hypothesis (xi) and the model parameters (\u0398). To integrate acoustic information, a regularization term using mean squared error (MSE) is applied to penalize the model when there is a significant discrepancy between the predicted probabilities and the posterior probability scores with a \u03bb coefficient of 0.01. Furthermore, we also consider parameter-efficient finetuning methods as in [6], which only update a small subset \u03b8 \u2282\u0398 of total trainable parameters to avoid potential overfitting that would hurt the generalization of the LLM [20, 21].\nWe now introduce a new in-context learning strategy that triggers the necessary sequential concepts for the ASR rescoring task, by utilizing multiple-round contextual sequences [22]. This technique is referred to as \u201ctask-activating prompting\u201d (TAP). In this configuration, the LLM is given leading questions to clarify the task it needs to perform. Following this, the model is instructed to provide an example and, ultimately, it is presented with the top-N hypotheses from which to generate the actual output for the task. In our experiments, we noted that LLMs are capable of producing lists of the top N predictions, made up of utterances with analogous pronunciations. This demonstrates that LLMs assimilate acoustic (e.g., lattice-level) information during their pretraining phase. We illustrate the queries and responses used for task-activating prompting in Figure 3. We observed that the responses from InstructGPT vary slightly as a function of the random seed. In our experiments, we utilize the API key from OpenAI, and the prompts are fixed, except for the final query that contains the sample and test N-best lists. Recent work on ICL [23] focuses on the selection strategy for in-domain demonstration samples, which clearly could affect inference results. However, we leave demonstration selection for the N-best task to future work, and in our few-shot learning experiments manually select longer demonstration utterances, following [2].\n# 4. EXPERIMENTS AND RESULTS\nWe conduct our experiments by evaluating the final word error rate (WER) of a second-pass rescoring ASR system. First, we employ pretrained ASR models to decode audio from a target domain and collect the top-10 hypotheses. Then, we compare different in-context learning-based rescoring pipelines, including P1, the LLM-based error correction to enhance a standard ASR rescoring process, and P2, a prompting scheme that enables the LLM to directly perform ASR rescoring, as shown in Figure 1.\n# 4.1. Pretrained ASR and Rescoring Model Training\nWe select a conformer-based RNN-transducer [24, 25] (760M parameters) pretrained with 960 hours of Librispeech [26], 10k hours of the GigaSpeech [27], 24k hours of English data in VoxPopuli [28], and wav2vec2-based Libri-Light [29, 30] pretraining as the first-pass model to generate the hypothesis list. We follow the conformer architecture presented in [25]. The pretrained model achieves a WER of 2.45% (test-clean) and 5.45% (test-other) on LibriSpeech without external LM. For P1, to obtain a standard neural LM for rescoring, we employ an ASR-LM training procedure similar to RescoreBERT [31], which involves a two-stage process. The first stage is adaptation using a first-stage masked language model (MLM) [32] loss, and the second stage is to use minimum\nQ: Do you know speech recognition? R: Yes, I am familiar with speech recognition. Speech recognition, also known as automatic speech recognition (ASR) or speech-to-text, is the process of converting spoken language into text. This technology involves using algorithms and machine learning models to analyze and transcribe the acoustic features of spoken words and phrases. Speech recognition has many applications, including voice-controlled assistants, automated phone systems, and transcription services. Q: Do you know language model for speech recognition? R: Yes, I can provide you with some information about language models used in speech recognition. Language models are an essential component of Automatic Speech Recognition (ASR) systems. They help to predict the likelihood of a sequence of words, given the context. This helps the ASR system to better understand and transcribe spoken language into text. This separate model is typically more complex and powerful than the initial language model used for the transcription, and it is used to re-score the transcription based on the probability of the words occurring in the given context. The rescoring process involves taking the output of the initial language model, which is usually based on statistical methods such as Hidden Markov Models, and then applying a more advanced language model, such as a neural network-based language model, to generate a more accurate transcription. This is accomplished by re-ranking the possible transcriptions based on the probabilities assigned by the more advanced language model. Language model rescoring has been shown to significantly improve the accuracy of speech recognition systems, particularly in noisy or challenging environments where the initial language model may not perform well. Q: Could you give a possible example of language model rescoring with some hypotheses? R: Sure, here is an example of language model rescoring for ASR with 5-best hypotheses: 1. recognize speech with artificial intelligence. 2. recognized speech with artificial intelligence. 3. recognize speech with artificial intelligent. 4. reckon eyes speech with artificial intelligence. 5. recognize peach with artificial intelligence.\nAfter rescoring, the model assigns a higher probability to the first hypothesis, which is the correct transcription of the input. The other hypotheses receive lower probabilities, reflecting their decreased likelihood given the improved language model. In this case, the top-ranked hypothesis after rescoring would be chosen as the final output: recognize speech with artificial intelligence Q: Nice job, I will provide some examples as a demonstration from [target domain]. The 10-best hypothesis is:[hypotheses list from training set], and I would expect your output is: [corresponding transcription]. Following this example, could you report the true transcription from the following 10-best hypotheses:? [hypotheses list for inference]\nword error rate (MWER) training [33, 34, 35]. We choose the Albert [36] base-v2 model with 4.8 million trainable parameters, and apply rescoring [31] using the error-corrected hypotheses obtained with a frozen LLM. For P2, we examine how, by suitable prompt construction, the frozen pretrained LLMs can be instructed to perform ASR rescoring. The specific LLM configurations used are described next.\n# 4.2. Pretrained LLM Configurations\nGPT-2 (1.5B): From the Generative Pretrained Transformer (GPT) family of causal models we selected this one [1, 37] as our basic LLM for in-context prompting setup. This version of GPT-21 is 100x smaller than very large teacher models, such as BLOOM [38], making it much more suitable for real-world deployment. GPT-2 is trained primarily using Wikipedia [39] and Common Crawl [40]. OpenLLaMA (13B): an open collection of transformerbased, decoder-only, causal language models ranging from 1B to 13B parameters [41]. It is trained exclusively on the RedPajama [42] dataset; we have confirmed that our Linguistic Data Consortium eval sets are not included. BLOOM (176B): the first open-source LLM trained on the public supercomputer provided by the French government [38], BLOOM is available for reproducible studies of LLMs\n1The pretrained model is publicly accessible under MIT License https: //github.com/openai/gpt-2\nover 100B. The model is pretrained using a large public collection of 498 HuggingFace datasets [43], comprising 1.61 TB of text spanning 46 natural languages and 13 programming languages (our evaluation datasets are not included.) InstructGPT (175B): an LLM created by training a GPT3 [1] model through reinforcement learning from human feedback (RLHF) [44]. InstructGPT demonstrates improved zeroshot learning performance, benefiting from human knowledge transferred through RLHF, a process similar to studentteacher learning. InstructGPT is trained using human feedback without using open-domain data for evaluation.2\n# 4.3. Target-Domain Datasets\nWe use the pretrained ASR model introduced in Section 4.1 to decode two public datasets, both in P1 (post-processing error correction) and P2 (pretrained LLM-based rescoring). Airline Travel Information System (ATIS) [45] contains 4978 training and 893 utterances. ATIS comprises spoken queries for air travel information, such as flight times and availability. Wall Street Journal (WSJ) [46] consists of transcribed audio recordings of read news articles from the Wall Street Journal, covering a wide range of topics and featuring a diverse set of speakers. We adapt the pretrained conformer\n2InstructGPT is an earlier version of ChatGPT,. We did not use ChatGPT in our experiments due to its frequent revisions and unclear technical documentation.\nTable 1. Oracle WERs for original and error-corrected Nbest output, using P1 processing as shown in Figure 1(a). The oracle error rates show the improvement in hypothesis quality as a result of post-processing using different sizes of LLMs.\nP1: correction setup\nWSJ\nATIS\nWERavg\nM-Size\n(a) N-best\n9.78\n6.43\n8.11\n-\n(a) + corrected by GPT-2\n9.91\n6.11\n8.01\n1.5B\n(a) + corrected by OpenLLaMA\n9.95\n5.73\n7.43\n13B\n(a) + corrected by BLOOM\n9.21\n5.64\n7.42\n176B\n(a) + corrected by InstructGPT\n8.41\n5.43\n6.92\n175B\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/19e1/19e198ee-8874-4ddf-be69-69c500281480.png\" style=\"width: 50%;\"></div>\nFig. 4. P1 ASR rescoring (RS) training using hypotheses corrected by LLM. The dashed red line marks the N-best WER. The WER gradually decreases in the three stages of rescoring using our P1 processing: Stage 0, N-best hypothesis with LLM correction (NC); Stage 1, fine-tuned RescoreBERT [31] using the masked language modeling (MLM) loss; and Stage 2, MWER training. model on the development set of train-si284 and test on 93dev.\n# 4.4. Pipeline 1 Results\nAs shown in Table 1, we first use a pretrained LLM for error correction, using the setup in [9] to improve hypothesis quality as measured by oracle (minimum achievable) N-best error rates. Figure 4 shows how error correction by LLMs complements existing rescoring with adapted LMs, such as RescoreBERT [31]. We observe that the existing rescoring pipeline [31] reduces WER from 11.3% to 8.7% compared to its fine-tuning-only baseline. Furthermore, Pipeline 1 employed the frozen pretrained language models to achieve an additional performance boost.\n# 4.5. Pipeline 2 Results\nCase 1: Zero-shot learning. Table 3 shows results for rescoring with in-context learning using different LLMs, as well as various baselines. Note that for in-context rescoring, we ex-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78a5/78a560e3-1403-43e8-8e5c-cec4915b8ea4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Numbers of Demos from Training Pairs</div>\nFig. 5. WER results on ATIS and WSJ with few-shot learning based on InstructGPT, for increasing numbers of demonstration samples. \u201cOne-by-one prompting\u201d resets the model history after each utterance, \u201cin-context prompting\u201d lets the history (and thus the examples provided) accumulate. tract LM scores from the model output responses, which differs from the standard approach of using the softmax outputs from the rescoring LM, for which we also report results. The best P2 rescoring setup is the one using InstructGPT (Table 3, last row), achieving 19.7% relative WER reduction compared to rescoring with a fine-tuned GPT-2. Note that the frozen GPT-2 failed to give improvements over a 4-gram baseline, showing that a certain model size is required for generative error correction. For LLMs with over 100 billion parameters, the use of prompting information showed better results compared to using the softmax scores directly. Next, we tested some popular prompting variants for incontext learning to possibly improve the performance of P2. As shown in Table 2, P2 and prompting with \u201cstep-by-step\u201d (known as zero-shot reasoning in [16]) achieved the best results for both LLMs (fourth row). It outperforms the one-shot learning variant (prompting with one input-output example; fifth row) by 1.8% relative WER difference. It is worth noting that the standard rescoring training P1 (first row) still outperforms zero-shot P2 by 14.6% relative. Case 2: Few-shot learning. We gain some insight into the effects of in-context learning by considering few-shot learning in the form of conversational prompting. We feed InstructGPT examples drawn from the training portions of the two datasets, and report the results on the unseen test sets. As shown in Figure 5, we see that frozen InstructGPT improved its rescoring performance as the number of training samples is increased from 1 to 12. It is better to let the model history accumulate (green plot) than to reset it after each utterance (red plot), thereby compounding the effect of demonstrations. Case 3: In-domain fine-tuning. We also obtained indomain fine-tuning results, where we use the training portions of all the speech dataset to fine-tune the LLMs and then evaluate performance on the test sets; note that InstructGPT, being an API-only model, could not be fine-tuned. For prompting we use TAP (Section 3.2); however, we observed that after fine-tuning the exact method of prompting makes very little difference. As shown in Table 4, fine-tuning with low-rank adapters (LoRA) outperforms full fine-tuning in the generative error correction case, as do residual adapters. One reason would be that adapters avoid modifying the parameters\n<div style=\"text-align: center;\">ble 2. WERs on ATIS and WSJ using prompting variants to enhance the P2 in-context learning pipeline. We report the ults of InstructGPT and BLOOM as LLMs over 100B; GPT-2 and OpenLLaMA do not perform consistently in this setting.</div>\nWSJ\nATIS\nIn-context learning variant\nInstructGPT\nBLOOM\nInstructGPT\nBLOOM\nP1: LLM-corrected N-best w/ RescoreBERT [31]\n10.13\n10.46\n7.13\n8.46\nP2: (c) Zero-shot scoring\n10.43\n11.23\n7.95\n8.45\nP2: (c) + zero-shot reasoning [16]\n10.20\n11.88\n7.77\n8.53\nP2: (c) + domain-hint prompting [2]\n10.98\n11.45\n7.59\n8.49\nP2: (d) Scoring with one example-pair\n9.42\n9.45\n6.34\n7.30\nP2: (d) + zero-shot reasoning [16]\n9.87\n11.46\n7.25\n8.64\nP2: (d) + domain-hint prompting [2]\n9.70\n10.99\n6.19\n7.12\nP2: (d) + task-activating prompting (TAP)\n8.84\n8.99\n5.79\n6.72\nTable 3. WERs with P2 pipeline, using LLM in-context learning (ICL) for rescoring by zero-shot prompts illustrated in Figure 2(a). Where indicated we use task-activating prompting (TAP) to condition the LLM.\nP2: zero-shot rescoring setup\nWSJ\nATIS\n(a) Oracle\n9.78\n6.43\n(b) First pass\n11.87\n8.82\n(b) + 4-gram LM\n11.21\n8.57\n(b) + frozen GPT-2\n29.56\n27.51\n(b) + frozen GPT-2 w/ TAP\n27.37\n27.59\n(b) + frozen OpenLLaMA\n13.32\n9.27\n(b) + frozen OpenLLaMA w/ TAP\n11.53\n8.61\n(b) + frozen BLOOM\n12.59\n9.21\n(b) + frozen BLOOM w/ TAP\n10.82\n8.42\n(b) + frozen InstructGPT\n9.97\n7.15\n(b) + frozen InstructGPT w/ TAP\n8.72\n6.39\nof a pretrained model (by inserting a neural module with a small number of additional trainable parameters that approximate the full parameter updates), allowing for efficient learning of the task without affecting the pretrained parameters of the LLM. LoRA-based generative error correction introduces trainable low-rank decomposition matrices into the pretrained LLM layers, enabling the model to adapt to new data while keeping the original LLMs fixed to retain the pretrained knowledge. Specifically, LoRA performs a reparameterization of each model layer expressed as a matrix multiplication by inserting low-rank decomposition matrices. As a result, the representations (generated by the LLM) are not distorted due to task-specific tuning, while the adapter module acquires the ability to perform the error correction task. Compared to previous state-of-of-the-art results with the Universal Speech Model (USM) [47] with text injection training over 2B parameters, our best fine-tuning results improve upon their WSJ results (WER of 3.2%). This is noteworthy considering that our generative error correction method is based on a smaller underlying conformer-RNN-T ASR system. Another parameter-efficient form of fine-tuning is prefix tuning [50], where a continuous prompt prefix is inserted\n<div style=\"text-align: center;\">Table 4. WERs on ATIS and WSJ, using fine-tuning (FT) and parameter-efficient adaptation to enhance the P2-TAP pipeline</div>\n P-TAP \nWSJ\nATIS\nMethod\nGPT-2\nOpen-\nLLaMA\nBLOOM\nGPT-2\nOpen-\nLLaMA\nBLOOM\nFT + ranking [11]\n9.93\n8.09\n8.42\n6.34\n3.71\n3.75\nFull fine-tune\n9.94\n7.71\n6.91\n5.98\n2.23\n2.49\nRes. adapter [48]\n7.24\n5.94\n4.57\n4.45\n2.48\n2.12\nLoRA [49]\n7.52\n2.11\n2.81\n4.57\n1.69\n1.97\nPrefix tuning [50]\n9.32\n6.99\n7.43\n5.32\n2.63\n2.74\ninto the input and tuned to optimize task performance of the LLM. However, this method gave worse results than the full or adapter-based fine-tuning methods for the larger LLMs.\n# 5. CONCLUSIONS\nWe have explored how in-context learning can be applied to pretrained large language models for improving first-pass ASR N-best output without fine-tuning. For this task setting, we introduce two post-processing pipelines utilizing incontext learning. The first one uses a pretrained LLM for error correction prior to standard rescoring with a fine-tuned LM. The second pipeline uses in-context learning by prompting, to instruct the frozen pretrained LLM to perform the rescoring task by itself. The latter method shows substantial gains over the first-pass ASR output and can be further enhanced with chain-of-thought and example prompting, as well as a new prompting scheme we call task-activating prompting. The best methods show 31% (on ATIS) to 38% (on WSJ) WER reduction over first-pass ASR, using a frozen InstructGPT, and better than with a fine-tuned GPT-2 LM. Substantial additional gains are achieved by fine-tuning the LLM for the ASR output-correction task. Post-processing with OpenLLaMA and LoRA fine-tuning achieves 86% and 80% WER reduction on ATIS and WSJ, respectively. These results are below the N-best oracle error rate, showing the LLM\u2019s ability to utilize prelearned knowledge to correct ASR output errors. Possible future work can look at how to integrate extra acoustic representations into pretrained LLMs for further enhancing generative ASR error correction.\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 1877\u2013 1901, 2020. [2] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, \u201cRethinking the role of demonstrations: What makes in-context learning work?\u201d in Proc. EMNLP, 2022, pp. 11 048\u201311 064. [3] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, \u201cFinetuned language models are zero-shot learners,\u201d in Proc. ICLR, 2022. [4] R. D. Martinez, S. Novotney, I. Bulyko, A. Rastrow, A. Stolcke, and A. Gandhe, \u201cAttention-based contextual language model adaptation for speech recognition,\u201d in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 1994\u20132003. [5] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, M. Ma, and J. Bai, \u201cScaling endto-end models for large-scale multilingual asr,\u201d in Proc. ASRU. IEEE, 2021, pp. 1011\u20131018. [6] C. Chen, Y. Hu, C.-H. H. Yang, S. M. Siniscalchi, P.-Y. Chen, and E. S. Chng, \u201cHyporadise: An open baseline for generative speech recognition with large language models,\u201d arXiv preprint, 2023. [7] D. Le, G. Keren, J. Chan, J. Mahadeokar, C. Fuegen, and M. L. Seltzer, \u201cDeep shallow fusion for RNN-T personalization,\u201d in Proc. SLT. IEEE, 2021, pp. 251\u2013257. [8] J. Guo, T. N. Sainath, and R. J. Weiss, \u201cA spelling correction model for end-to-end speech recognition,\u201d in Proc. ICASSP. IEEE, 2019, pp. 5651\u20135655. [9] J. Liao, S. E. Eskimez, L. Lu, Y. Shi, M. Gong, L. Shou, H. Qu, and M. Zeng, \u201cImproving readability for automatic speech recognition transcription,\u201d Transactions on Asian and Low-Resource Language Information Processing, 2020. 10] J. Yang, R. Li, and W. Peng, \u201cASR error correction with constrained decoding on operation prediction,\u201d Proc. Interspeech, pp. 3874\u20133878, 2022. 11] R. Ma, M. J. Gales, K. Knill, and M. Qian, \u201cN-best T5: Robust ASR error correction using multiple input hypotheses and constrained decoding space,\u201d arXiv preprint arXiv:2303.00456, 2023. 12] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., \u201cMultitask prompted training enables zero-shot task generalization,\u201d arXiv preprint arXiv:2110.08207, 2021. 13] R. Zhong, K. Lee, Z. Zhang, and D. Klein, \u201cAdapting language models for zero-shot learning by meta-tuning on dataset and prompt collections,\u201d arXiv preprint arXiv:2104.04670, 2021. 14] D. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei, \u201cWhy can GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers,\u201d in Proc. ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\n[15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou et al., \u201cChain-of-thought prompting elicits reasoning in large language models,\u201d in NeurIPS, 2022. [16] T. Kojima et al., \u201cLarge language models are zero-shot reasoners,\u201d Advances in Neural Information Processing Systems, 2022. [17] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma, \u201cAn explanation of in-context learning as implicit bayesian inference,\u201d Proc. ICLR, 2021. [18] C. Floccia, J. Goslin, F. Girard, and G. Konopczynski, \u201cDoes a regional accent perturb speech processing?\u201d Journal of Experimental Psychology: Human Perception and Performance, vol. 32, no. 5, p. 1276, 2006. [19] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh, \u201cImpact of pretraining term frequencies on few-shot reasoning,\u201d arXiv preprint arXiv:2202.07206, 2022. [20] A. Kumar, A. Raghunathan, R. Jones, T. Ma, and P. Liang, \u201cFine-tuning can distort pretrained features and underperform out-of-distribution,\u201d in Proc. ICLR, 2022. [21] C.-H. H. Yang, Y.-Y. Tsai, and P.-Y. Chen, \u201cVoice2Series: Reprogramming acoustic models for time series classification,\u201d in Proc. International Conference on Machine Learning. PMLR, 2021, pp. 11 808\u201311 819. [22] K. Hambardzumyan, H. Khachatrian, and J. May, \u201cWarp: Word-level adversarial reprogramming,\u201d in Proc. ACL, 2021, pp. 4921\u20134933. [23] X. Ye and G. Durrett, \u201cExplanation selection using unlabeled data for in-context learning,\u201d arXiv preprint arXiv:2302.04813, 2023. [24] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., \u201cState-of-the-art speech recognition with sequence-tosequence models,\u201d in Proc. ICASSP. IEEE, 2018, pp. 4774\u2013 4778. [25] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d Proc. Interspeech, pp. 5036\u20135040, 2020. [26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibriSpeech: an ASR corpus based on public domain audio books,\u201d in Proc. ICASSP. IEEE, 2015, pp. 5206\u20135210. [27] G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang et al., \u201cGigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio,\u201d arXiv preprint arXiv:2106.06909, 2021. [28] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, \u201cVoxPopuli: A largescale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d arXiv preprint arXiv:2101.00390, 2021. [29] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar\u00b4e, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen et al., \u201cLibri-Light: A benchmark for ASR with limited or no supervision,\u201d in Proc. ICASSP. IEEE, 2020, pp. 7669\u20137673.\n[30] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12 449\u201312 460, 2020. [31] L. Xu, Y. Gu, J. Kolehmainen, H. Khan, A. Gandhe, A. Rastrow, A. Stolcke, and I. Bulyko, \u201cRescoreBERT: Discriminative speech recognition rescoring with BERT,\u201d in Proc. ICASSP. IEEE, 2022, pp. 6117\u20136121. [32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proc. NAACL, 2018, p. 4171\u20134186. [33] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu, and A. Kannan, \u201cMinimum word error rate training for attention-based sequence-to-sequence models,\u201d in Proc. ICASSP. IEEE, 2018, pp. 4839\u20134843. [34] B.-H. Juang, W. Hou, and C.-H. Lee, \u201cMinimum classification error rate methods for speech recognition,\u201d IEEE Transactions on Speech and Audio Processing, vol. 5, no. 3, pp. 257\u2013265, 1997. [35] D. Povey, \u201cDiscriminative training for large vocabulary speech recognition,\u201d Ph.D. dissertation, University of Cambridge, 2005. [36] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, \u201cALBERT: A lite BERT for self-supervised learning of language representations,\u201d in Proc. ICLR, 2020. [37] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., \u201cLanguage models are unsupervised multitask learners,\u201d OpenAI blog, vol. 1, no. 8, p. 9, 2019. [38] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow, R. Castagn\u00b4e, A. S. Luccioni, F. Yvon, M. Gall\u00b4e et al., \u201cBLOOM: A 176b-parameter open-access multilingual language model,\u201d arXiv preprint arXiv:2211.05100, 2022. [39] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel mixture models,\u201d in Proc. ICLR, 2018. [40] T. H. Trinh and Q. V. Le, \u201cA simple method for commonsense reasoning,\u201d arXiv preprint arXiv:1806.02847, 2018. [41] X. Geng and H. Liu, \u201cOpenLLaMA: An open reproduction of LLaMA,\u201d https://github.com/openlm-research/open llama, May 2023. [42] T. Computer, \u201cRedPajama: An open source recipe to reproduce LLaMA training dataset,\u201d https://github.com/togethercomputer/RedPajama-Data, Apr. 2023. [43] Q. Lhoest, A. V. del Moral, Y. Jernite, A. Thakur, P. von Platen, S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall et al., \u201cDatasets: A community library for natural language processing,\u201d in Proc. EMNLP, 2021, pp. 175\u2013184. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Gray et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Advances in Neural Information Processing Systems, 2022. [45] C. T. Hemphill, J. J. Godfrey, and G. R. Doddington, \u201cThe ATIS spoken language systems pilot corpus,\u201d in Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990, 1990.\n[46] M. Marcus, B. Santorini, and M. A. Marcinkiewicz, \u201cBuilding a large annotated corpus of English: The Penn Treebank,\u201d Computational Linguistics, vol. 19, no. 2, 1993. [47] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang et al., \u201cGoogle USM: Scaling automatic speech recognition beyond 100 languages,\u201d arXiv preprint arXiv:2303.01037, 2023. [48] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, \u201cParameter-efficient transfer learning for NLP,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 2790\u20132799. [49] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLoRA: Low-rank adaptation of large language models,\u201d in Proc. ICLR, 2021. [50] X. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continuous prompts for generation,\u201d in Proc. ACL, 2021, pp. 4582\u20134597.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of automatic speech recognition (ASR) systems that struggle with error correction and rescoring, particularly in complex domains. Previous methods relied heavily on fine-tuning language models, which were often limited by their size and the need for extensive retraining. The introduction of large language models (LLMs) and their in-context learning capabilities presents a breakthrough opportunity to enhance ASR performance without the need for fine-tuning.",
        "problem": {
            "definition": "The primary challenge is the need for effective error correction in ASR outputs, particularly in scenarios where existing models struggle to generalize or adapt to new domains.",
            "key obstacle": "The main difficulty lies in the reliance on fine-tuning large language models, which can be resource-intensive and may not yield significant improvements in performance across diverse tasks."
        },
        "idea": {
            "intuition": "The idea stems from the observation that large language models can leverage their pre-trained knowledge and in-context learning capabilities to improve ASR outputs without extensive retraining.",
            "opinion": "The proposed approach involves using LLMs as post-processors for ASR, employing prompting techniques to enhance rescoring and error correction tasks.",
            "innovation": "The key innovation is the introduction of task-activating prompting, which combines causal instructions and demonstrations to maximize the LLM's performance in ASR tasks without the need for fine-tuning."
        },
        "method": {
            "method name": "Task-Activating Prompting",
            "method abbreviation": "TAP",
            "method definition": "TAP is a method that utilizes frozen large language models to perform rescoring and error correction in ASR systems by employing specific prompting techniques.",
            "method description": "TAP involves initializing LLMs with task-oriented instructions and examples to enhance their performance in post-processing ASR outputs.",
            "method steps": [
                "Collect N-best hypotheses from a first-pass ASR model.",
                "Format these hypotheses into a prompt for the LLM.",
                "Apply task-activating instructions to guide the LLM in processing the hypotheses.",
                "Generate corrected outputs based on the LLM's predictions."
            ],
            "principle": "This method is effective because it leverages the LLM's ability to understand context and perform reasoning tasks through in-context learning, allowing it to generate more accurate corrections and scores for ASR outputs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using two datasets: the Airline Travel Information System (ATIS) and the Wall Street Journal (WSJ). The performance of the proposed method was evaluated against standard rescoring techniques and various LLM configurations.",
            "evaluation method": "The evaluation involved measuring the word error rate (WER) of the ASR outputs before and after applying the TAP method, comparing results with baseline methods and analyzing the impact of different prompting strategies."
        },
        "conclusion": "The study demonstrated that task-activating prompting significantly enhances the performance of ASR systems by achieving substantial reductions in WER. The results indicate that LLMs can effectively utilize their pre-trained knowledge for error correction and rescoring tasks, outperforming traditional methods and showcasing the potential for future improvements in ASR applications.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to improve ASR performance without the need for extensive fine-tuning, making it more efficient and scalable.",
            "limitation": "A limitation of the method is that the performance may vary depending on the quality of the prompt construction and the specific characteristics of the input data.",
            "future work": "Future research could explore integrating additional acoustic representations into LLMs and further optimizing prompting strategies to enhance generative ASR error correction."
        },
        "other info": [
            {
                "info1": "The study utilized a variety of LLMs, including GPT-2, OpenLLaMA, BLOOM, and InstructGPT, to evaluate their effectiveness in ASR tasks."
            },
            {
                "info2": {
                    "info2.1": "The experiments highlighted the importance of in-context learning and prompting techniques in leveraging LLM capabilities.",
                    "info2.2": "Performance metrics were consistently reported in terms of WER, showcasing improvements across different configurations."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning is leveraged by large language models (LLMs) to improve automatic speech recognition (ASR) outputs without extensive retraining."
        },
        {
            "section number": "1.3",
            "key information": "Large language models facilitate in-context learning by utilizing pre-trained knowledge and task-activating prompting techniques to enhance ASR performance."
        },
        {
            "section number": "3.1",
            "key information": "The method 'Task-Activating Prompting' (TAP) allows LLMs to adapt to various ASR contexts and improve error correction capabilities."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical framework of in-context learning in LLMs is demonstrated through the application of TAP, which combines causal instructions and demonstrations."
        },
        {
            "section number": "4.1",
            "key information": "Effective prompt design, as illustrated by the TAP method, significantly influences the outcomes of in-context learning in ASR tasks."
        },
        {
            "section number": "6.1",
            "key information": "The reliance on prompt construction quality impacts the performance of the proposed method, highlighting issues related to model bias and context sensitivity."
        },
        {
            "section number": "6.2",
            "key information": "The computational costs associated with fine-tuning large language models are mitigated by using LLMs as post-processors for ASR, enhancing efficiency."
        },
        {
            "section number": "7",
            "key information": "The study concludes that LLMs can effectively utilize their pre-trained knowledge for error correction and rescoring tasks, showcasing potential for future ASR improvements."
        }
    ],
    "similarity_score": 0.6901304004101708,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting.json"
}