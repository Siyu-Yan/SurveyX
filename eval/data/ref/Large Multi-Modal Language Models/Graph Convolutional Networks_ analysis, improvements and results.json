{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1912.09592",
    "title": "Graph Convolutional Networks: analysis, improvements and results",
    "abstract": "In the current era of neural networks and big data, higher dimensional data is processed for automation of different application areas. Graphs represent a complex data organization in which dependencies between more than one object or activity occur. Due to the high dimensionality, this data creates challenges for machine learning algorithms. Graph convolutional networks were introduced to utilize the convolutional models concepts that shows good results. In this context, we enhanced two of the existing Graph convolutional network models by proposing four enhancements. These changes includes: hyper parameters optimization, convex combination of activation functions, topological information enrichment through clustering coefficients measure, and structural redesign of the network through addition of dense layers. We present extensive results on four state-of-art benchmark datasets. The performance is notable not only in terms of lesser computational cost compared to competitors, but also achieved competitive results for three of the datasets and state-of-the-art for the fourth dataset.",
    "bib_name": "ullah2019graphconvolutionalnetworksanalysis",
    "md_text": "# Graph Convolutional Networks: analysis, improvements and results\nIhsan Ullahb,\u2217, Mario Manzoa, Mitul Shahb, Michael Maddenb\naInformation Technology Services, University of Naples \u201dL\u2019Orientale\u201d, Naples 80121, Italy bData Mining and Machine Learning Group, School of Computer Science, National University of Ireland Galway, Galway, Ireland\non Technology Services, University of Naples \u201dL\u2019Orientale\u201d, Naples 80121, Italy arning Group, School of Computer Science, National University of Ireland Galway, Galwa\n19 Dec 2019\n19 Dec \nAbstract\nIn the current era of neural networks and big data, higher dimensional data is processed for automation of different application areas. Graphs represent a complex data organization in which dependencies between more than one object or activity occur. Due to the high dimensionality, this data creates challenges for machine learning algorithms. Graph convolutional networks were introduced to utilize the convolutional models concepts that shows good results. In this context, we enhanced two of the existing Graph convolutional network models by proposing four enhancements. These changes includes: hyper parameters optimization, convex combination of activation functions, topological information enrichment through clustering coefficients measure, and structural redesign of the network through addition of dense layers. We present extensive results on four state-of-art benchmark datasets. The performance is notable not only in terms of lesser computational cost compared to competitors, but also achieved competitive results for three of the datasets and state-of-the-art for the fourth dataset.\n[cs.LG]\narXiv:1912.09592v1\n# 1. Introduction\nIn the last few years, data is usually represented as points in a vector space. Today, structured data is omnipresent, able to include structural information between points and can be particularly important to better represent the models learned on them. For this purpose, the graphs are widely used to represent this type of information through nodes/vertices and edges, including local and spatial information derived from the data. Very often the interest concerns a prediction about the node properties in such graphs. For example, given a network that represents a human phenomenon, like a common exchange of messages in a social network, the goal is to predict the area of belonging i.e. users with common interests. Performing a forecasting process, especially in a semi-supervised environment, has been at the center of graph-based semi-supervised learning (SSL) Rozza et al. [2014]. In graph-based SSL, a small set of nodes is ini-\n\u2217Corresponding author: Tel.: +353-089-9897986; Email address: ihsan.ullah@nuigalway.ie (Ihsan Ullah )\nPreprint submitted to Elsevier\ntially labeled. Starting from this information the remaining part of the graph structure is adopted, initially without the label, to label the nodes. Notationally, the structure described by the graph is normally incorporated as an explicit regularizer which applies a sliding constraint on the labels of the nodes to estimate. Recently, Graph Convolutional Networks (GCN)Defferrard et al. [2016]; Kipf and Welling [2017] have been proposed with a purpose to work on deep neural networks and graph-structured data. In this paper, our attention is on the task of graph-based SSL using GCNs. GCN progressively estimate a transformation from graph to vector space, also called embedding, and an aggregation of neighborhood nodes, whereas a target loss function for backpropagation errors is adopted. Indeed, node embedding result represents an estimation for label scores on the nodes. Moreover, it would be appropriate to obtain an association of confidence estimation to the label scores. This confidence scores can be adopted to understand the reliability of the estimated labels on a generic node. This improvement is introduced in the model called Confidence based Graph Convolutional Networks (Con-\nDecember 23, 2019\n1. The models provide a set of parameters to be optimized. In this phase the goal is to find the optimal combination (activation function, loss function, hidden layers, number of nodes, etc) in order to obtain the best performance. It is assumed that this phase is very long and expensive. 2. In past few years, many researchers have been worked designing novel activation functions in order to help deep neural networks to converge and obtain better performance. Standard neural networks employ logistic sigmoid activation functions which is affected by saturation problem and and consequently the effectiveness and efficiency of the classification is reduced. In this paper, we introduce an efficient approach to learn, during training, combinations of base activation functions (such as Relu6); our goal is to check a search space for the activation functions through a convex combination of the base functions. 3. The models adopt only the information relating to the degree of the single nodes (matrix \u02dcD in equation 1) to process the graphs. In this regard, we introduce a measure that provides additional topological information called clustering coefficients. 4. In deep learning the goal is to improve performance by focus attention on adding new layers, modifying the activation functions or changing the regularization methods. Furthermore, current structure, layers, can be redesigned to obtain optimal results compared to existing models. To this aim, we combined GCN and Dense layers. This new model provides a mixture of both GCN and Dense layers and compared to individual GCN, resulted in better performance.\nAdditionally, we analyze the two baseline models, GCN and ConfCGN, in order to show the impact of proposed changes during training and testing phases. The paper is organized as follows: section 2 gives an overview of related work. Section 3 provides an overview of the two models GCN and ConfCGN. Section 4 explains the enhancement we proposed in this paper. Whereas, section 5 discusses the results achieved with proposed approach on GCN and confGCN. Finally, section 6 gives some future directions and concludes our paper.\n# 2. Related Work\nRecent literature provides some interesting insights about application of neural networks and data organized as graphs. In Kipf and Welling [2017] a variant of convolutional neural networks, called Graph Convolutional Networks (GCNs), which operate directly on graphs is presented. Main motivation of convolutional architecture is related to localized first-order approximation of spectral graph convolutions. The model works by scaling linearly nodes connections and adopts hidden layer representations which encode both structure and features of graphs. \u2014\u2014\u2014s on graphs, is presented. The proposed formulation does not alter the computational complexity of standard CNNs, despite being found to be processing graph structures. In Marcheggiani and Titov [2017], an enhanced version of Kipf and Welling [2017] is introduced. It is able to work with syntactic dependency graphs in form of sentence encoders and extracts words latent feature representations arranged in a sentence. Moreover, the authors showed that layers are complementary to LSTM layers. In Veli\u02c7ckovi\u00b4c et al. [2018], a neural network architecture for inductive and transductive problems, based on masked self-attentional layers, called graph attention networks (GATs), for graph-structured data is presented. In this model, nodes are able to contribute about neighboring features extraction and different weights to different nodes in a neighborhood are enabled, eliminating expensive matrix operations. In this way, several key challenges of spectral-based graph neural networks are addressed at the same time. In Vashishth et al. [2019], a modified version called Confidence-based Graph Convolutional Networks (ConfGCN) of Kipf and Welling [2017] is introduced. The improvement concerns a confidence estimation about label scores that has not been explored in GCNs. ConfGCN adopts label scores estimation to identify the influence of a node on the neighborhood during aggregation, thus acquiring anisotropic abilities. In Liao et al. [2018] a graph partition neural networks (GPNN), an extension of graph neural networks (GNNs), useful to work with large graphs are described. GPNNs combine local information between nodes in small subgraphs and global information between the subgraphs. Graphs are partitioned in efficient way through several al-\ngorithms and, additionally, a novel variant for fast processing of large scale graphs is introduced. In Yadav et al. [2019] a modified version of Kipf and Welling [2017] named Lov\u00b4asz Convolutional Networks (LCNs) is introduced. The model is able to capture global graph properties through Lov\u00b4asz orthonormal embeddings of the nodes. In Atwood and Towsley [2016], a DiffusionConvolutional Neural Networks (DCNNs) are described. Diffusion-convolution operation is useful to learn representations as an effective basis for node classification. The model includes different qualities such as latent representation for graphical data, invariance under isomorphism, polynomial-time prediction and learning. In Bruna et al. [2014], possible generalizations of Convolutional Neural Networks (CNNs) to signals is defined for more general domains. In particular, two models, one based upon a hierarchical clustering of the domain and another based on the spectrum of the graph Laplacian are described. The model is able to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures. Further, a deep architecture with small learning complexity on general non-Euclidean domains is introduced in Henaff et al. [2015]. The model is an extension of Spectral Networks which includes a graph estimation procedure. Finally, in Li et al. [2016] authors describe Gated graph sequence neural networks (GGNN), an extended versions of Graph Neural Networks (GNN) Scarselli et al. [2008], which uses modified gated recurrent units modern optimization techniques and extends output sequences. In the following section, we will explain the two baseline models i.e. GCN and ConfGCN.\n# 3. Baseline Models\nIn this section, we briefly introduce Graph Convolutional Networks (GCNs) Kipf and Welling [2017] and its enhancement, Confidence-based Graph Convolutional Networks Vashishth et al. [2019]. The two models are compared and analyzed in detail in terms of their limitations and differences. Subsequently, starting from these points a set of improvements are proposed and demonstrated experimentally.\nIn this section we define the important elements for this research. Given G = (V, E, X) an undirected graph, where V = Vl \u222aVu the set containing labeled (Vl) and unlabeled (Vu) nodes in the graph of dimension nl and nu, E is the set of edges and X \u2208R(nl+nu)\u00d7d is the input node features. The label of a node v is represented by a vector Yv \u2208Rm, belonging to m classes. In this context, the goal is to predict the labels, Y \u2208Rnl\u00d7m, of the unlabeled nodes of G. To consider the confidence, label distribution \u00b5v \u2208Rm and a diagonal co-variance matrix \u03a3v \u2208Rm\u00d7m estimations are added, \u2200v \u2208V. \u00b5v,i represents the score of label i on node v, while (\u03a3v)ii represents the variance in the estimation of \u00b5v,i. In other words, (\u03a3\u22121 v )ii is confidence in \u00b5v,i.\n# 3.2. Graph Convolutional Networks\nGraph Convolutional Networks (GCNs) Kipf and Welling [2017] works on undirected graphs. Given a graph G = (V, E, X), the node representation after a single layer of GCN can be defined as:\n(GCNs)\n(1)\nW \u2208Rd\u00d7d includes the model parameters, A represents nodes adjacency and \u02dcDii = \ufffd j(A+ I)i j. f is any activation function such as ReLU, f(x) = max(0, x). Equation 1 can be reformulated as:\n(2)\nb \u2208Rd represents bias, N(v) includes nodes neighborhood of v in graph G including v and hv is representation of node v. The goal is to acquire multi-hop dependencies between nodes, different GCN layers can be superimposed over one another. The representation of the node v after k layers can be written as\nwhere, Wk and bk represent the weight and bias parame ters of GCN layer.\nIn Vashishth et al. [2019] Confidence-based Graph Convolutional Networks (ConfGCN) is described. The authors define the influence score of node u considering its near node v during GCN process as follows:\n(4)\ndM(u, v) represents Mahalanobis distance between two nodes Orbach and Crammer [2012]. Specifically, considering nodes u and v, with label distributions \u00b5u and \u00b5v and co-variance matrices \u03a3u and \u03a3v, ruv gives more importance to spatially close nodes belonging to same class, otherwise reduces importance of nodes with low confidence scores. This results leads to inclusion of anisotropic capability during neighborhood exploration. For a node v, equation 3 can be rewritten as:\n(6)\nThe final label prediction is obtained by equation 7 with K number of layers.\n(7)\n# 3.4. GCN versus ConfGCN Models\nThe differences between the two models are shown below:\n1. The major difference in both models is that GCN implements the nodes embedding, projection from graph space to vector space, to describe the neighborhood while ConfGCN implements the confidence based prediction scheme where the neighboring nodes having higher confidence would be important parameters for the label of the unknown nodes. 2. GCN model implements Chebychev polynomial method for the computational cost reduction while ConfGCN model uses the Loss Smoothening, regularization and optimization for better efficiency. Despite having more executional time per epoch ConfGCN model has better efficiency with the similar datasets.\n3. GCN doesnt have constraints on the number of nodes that influences the representation of a given target node and each node is influenced by all the nodes in its k-hop neighborhood. Whereas, in ConfGCN the label confidences are used to ignore less confident nodes and nodes having higher confidence would be considered important. Furthermore, number of nodes influencing do not sway the prediction of the wrong labels. 4. For more number of nodes in graphs such as Cora and CoraML datasets, ConfGCN has significantly better performances than Kipf GCN as the previous model implements the Nodes entropy of neighborhood calculation.\n1. In GCN, memory requirement grows linearly in the size of the dataset. Whereas, ConfGCN requires higher memory requirement. 2. GCN is not applicable to directed graphs. It does not support edge features and is limited to undirected graphs (weighted or unweighted). 3. In GCN, locality for the nodes are assumed. 4. ConfGCN require more computational cost compared to the basic model. Cost increases as a confidence value, (equation 4), for the exploration of the neighborhood node. 5. ConfGCN require more time for execution. 6. In ConfGCN, increasing layers reduces the accuracy. This behavior is connected to the increase of influencing nodes with increasing layers, which results in average information during aggregation.\nIn the following section, we will explain the proposed enhancement we did for selecting an optimal deep model that may result in fewer executing time and enhanced performance.\n# 4. Proposed Models: Enhanced GCN and ConfGCN\nWe proposed four major enhancement for both the models. The first enhancement is changing the hyperparameters and training algorithm. The second and third are major enhancement i.e. adding more structural information to adjacency matrix and canonical optimization technique (also referred as convex). Finally, the fourth\nconcerns a combination of two base models with introduction of additional dense layers. All these enhancement are applied on both the baseline models. Following section will explain how these models are designed and implemented.\n# 4.1. Optimizing Hyper-parameters\nFirst we optimized the baseline models by fine-tuning the hyper-parameter that include activation function (AF), loss function (LF), and the number of nodes in each hidden layer. For AF we have explored it with ReLu, ReLu6, Elu, and S elu. In case of LF, we utilized simple cross entropy and cross entropy so ftmax V2. Whereas, to explore the best number of nodes, we have taken nodes in each layer as 16, 32, 48, 64, 80, 96, 100, 112 and 200. We explored to find the best combination of these parameters to provide optimal result in minimum amount of time. From now we will call the two enhanced versions Optimized Graph Convolutional Networks (OpGCN) and Optimized Confidence based Graph Convolutional Networks (OpConfGCN).\n# 4.2. Convex combination of activation functions\nA standard neural network Nd can be composed of a set of hidden layers d and a set functions Li that lead to a final mapping L related to a problem to address: Nd = L\u25e6Ld \u25e6\u00b7 \u00b7 \u00b7\u25e6L1. Specifically, each hidden layer function Li is composed of two functions, gi and \u03c3i, which include parameters within the spaces Hgi and H\u03c3i. A remapping of the layer input neurons in form of activation function can be seen as: Li = \u03c3i \u25e6gi. The learning process of Li consists in a procedure of optimization in the space Hi = H\u03c3i \u00d7 Hgi. Commonly, \u03c3i does not provide for a learning phase and H\u03c3i is a singleton. Then, Hi = {\u03c3i} \u00d7 Hgi. If we consider a fully-connected layer from Rni to Rmi which adopts Relu activation function, Hgi represents the set of all affine transformations from Rni to Rmi, then Hi = ReLu \u00d7 Lin(Rni, Rmi) \u00d7 K(Rmi), where Lin(A, B) and K(B) are respectively the sets of linear maps between A and B, and the set of translations of B. In this paper, we adopt a technique to define learnable activation functions Manessi and Rozza [2018] that can be adopted in all hidden layers of a GCN architecture. The approach consists of a hypothesis space H\u03c3i and is based on the following idea:\n\u2022 select a set of activation functions F = {f1, . . . , fN}, in which elements can be adopted as base elements;\n\u2022 select a set of activation functions F = {f1, . . . , fN}, in which elements can be adopted as base elements; \u2022 fix the activation function \u03c3i to combine in linear way as elements belonging to F set;\n\u2022 fix the activation function \u03c3i to combine in linear way as elements belonging to F set;\n\u2022 look for an optimal hypothesis space;\n\u2022 look for GCN optimization respect to Hi = H\u03c3i\u00d7Hgi.\nconv(A) := {\u03a3iciai|\u03a3ici = 1, ci \u22650, ai \u2208A};\nconv(A) is not vector subspace of V and is a generic convex subset in V reducing to a (|A| \u22121)-dimensional simplex when the elements of A are linearly independent. If we consider F := {f0, f1, . . ., fN} the set of activation functions fi, the vector space F is defined from F considering all linear combinations \ufffd i ci fi with ci \u22650, \u03a3ici = 1. Note that, despite F is a spanning set of F, it is not generally a basis; indeed |F| \u2265dim F. Based on previous definitions, we can now define the technique to build learnable activation functions as follows:\n\u2022 fix a finite set F = {f1, . . ., fN}, where each fi is a learneable activation function;\n\u2022 create an additional activation function f as a linear combination of all the fi \u2208F;\n\u2022 select as hypothesis space H f the conv(F) set; The results are obtained for the following combination for F:\nThe results are obtained for the following combination for F:\nThe results are obtained for the following combination for F:\n(10)\nRelu6 = min(max(0, x), 6)\nIn this work we have implemented two methods:\n1. Taking two input layers of the model, use the different activation for them and then applying any mathematical operations on the inputs, i.e. Summation, Subtraction, Maximum, minimum and Average values of both Input layers output.\n2. Looking at those results we got to know that summation operations are having the best results so we applied the canonical form on the outputs. In this case the convex combination becomes conv(A) := c1Relu6 + c2Relu6. The Structure of Base-Line model with optimized results is shown in Table 1:\n<div style=\"text-align: center;\">Table 1: Baseline Model structure for enhancing with convex approach Input Size L1-Nodes L1-ActivationFun OutputNodes loss function 1433 16 Relu 3 Cross Entrop</div>\nTable 1: Baseline Model structure for enhancing with convex approach\nInput Size\nL1-Nodes\nL1-ActivationFun\nOutputNodes\nloss function\n1433\n16\nRelu\n3\nCross Entropy\nWhereas its enhanced model structure is given in Table 2:\n<div style=\"text-align: center;\">Table 2: Enhanced Model structure for convex approach</div>\nTable 2: Enhanced Model structure for convex approach\nIn-Size\nL1-Nodes\nL1a-AF\nL1b-AF\nOut-Nodes\nLossFun\nc1\nc2\n1433\n16\nRelu6\nRelu6\n3\nCrossEntropy\n0.8\n0.2\nFrom now we will call the two enhanced versions Convex Graph Convolutional Networks (ConvGCN) and Convex Confidence based Graph Convolutional Networks (ConvConfGCN).\n# 4.3. Clustering coefficients\nIn equation 1 the adjacency matrix A, which describes the topology of the network, is very significant part of both models. Furthermore, the identity matrix I is added to A in order to remove zero values on the main diagonal. Our idea is to add more information about nodes by introducing a particular property called Clustering Coefficients. In graph theory, the clustering coefficient describes the degree of aggregation of nodes in a graph. The measure is based on triplets of nodes. A triplet is defined as three connected nodes. A triangle can include three closed triplets, each one centered on one of the nodes. Two possible versions can be defined: the Global Clustering Coefficients (GCCs) and the local Clustering Coefficients (CCs) Opsahl [2013]. We adopt the second defined as:\n(11)\nki is the degree of node i and \u03b4i is the number of edges between the ki neighbors of node i. The measure is in the range {0, . . ., 1}, 0 if none of the neighbors of a node is connected and 1 if all of the neighbors are connected. Topological information is provided through CCs, which\nis connected to other structural properties Strang et al. [2018], such as transitivity, density, characteristic path length, and efficiency, useful for representation in the vector space. In this work we are suggesting that there is another possibility of the matrix I which is to replace the main diagonal of the matrix I with CCs values. For a graph of n \u00d7 n nodes the identity matrix becomes:\n(12)\n\uf8ef\uf8f0 \uf8fa\uf8fb From now we will call the two enhanced versions Clustering Coefficients Graph Convolutional Networks (CCGCN) and Clustering Coefficients Confidence based Graph Convolutional Networks (CCConfGCN). The Structure of Base-Line model with optimized results is similar to Table 1. Matrix was added to the Adjacency matrix while pre-processing of the input and the combined matrix was considered as input to the neural network. The new matrix In, having the same size as Identity matrix, is added to the adjacency matrix instead of plain identity matrix.\n# 4.4. GCN and Dense Layer combination\nDeep learning models have shown that beside creating a new layer, activation function, regularization method etc., if one can redesign existing layers etc. in a proper way. It can result in optimal performance as compare to the previous models. We adopted the same GCN and Dense layers and created a model that gave the optimal results. A dense layer is commonly known as fully connected layer and it is represented as:\n# ylnu = fln \ufffd\ufffdI i=1 \ufffd\ufffd wln (i,v) . yln\u22121 (i) \ufffd + bln (1,v) \ufffd\ufffd\n(13)\n\ufffd Here, ylnu represents the neuron at layer n, wln i,v represents the weight (i, v) for that neuron multiplied with input neuron yln\u22121 i , and blnv represents that bias that is added to the weighted sum. The resultant weighted sum value is passed through an activation function fln. Table 3 shows the structure of the model. We used this model on all four datasets. After extensive experiments, the best results are shown in Table 5. This combination provides a mixture\nof both GCN and Dense layers and result in better performance compared to individual GCN or Dense layer. The training phase adopts the same Adam optimizer (similar to all other models). In each layer, we used Relu6 activation function. From now we will call the two enhanced versions Dense Graph Convolutional Networks (DGCN) and Dense Confidence based Graph Convolutional Networks (DConfGCN).\n<div style=\"text-align: center;\">Table 3: Model having both GCN and Dense Layer</div>\nTable 3: Model having both GCN and Dense Layer\nLayer\nIn-Nodes\nOut-Nodes\nAF\nDO\nInput\n1433\n-\n-\n-\nGCN\n1433\n32\nRelu6\n0.5\nDense-1\n32\n16\nRelu6\n0.5\nDense-2\n16\n32\nRelu6\n0.5\nGCN\n32\n48\nRelu6\n0.5\nGCN\n48\n7\n-\n0.5\nOutput\n7\n7\nSoftmax\n-\nIn Table 3, \u2019In-Nodes\u2019 represents the input nodes to a layer, \u2019Out-Nodes\u2019 represent the output nodes of a layer, \u2019AF\u2019 represents the activation function, whereas drop out rate is represented by \u2019DO\u2019.\n# 5. Results\nThis section describes the results obtained on public datasets with the proposed improvements. In addition, the achieved results will be compared to the state-of-the-art models in literature.\n# 5.1. Datasets\nFor performance evaluation we adopt several semisupervised classification datasets that are commonly used by other researchers. The set of dataset comprise of Cora, Citeseer, Pubmed Sen et al. [2008], and Cora-ML Bojchevski and G\u00a8unnemann [2018]. The setup is the same as being followed in Vashishth et al. [2019]. Our aim concerns to classify documents into one of the predefined classes. Datasets represent citation networks, in which each document is encoded using bag-of-words features with undirected edges between nodes. The dataset statistics is summarized in table 4. Label mismatch concerns the fraction of edges between nodes with different labels in the training data. The datasets have substantially low label mismatch rate except Cora-ML.\n<div style=\"text-align: center;\">Table 4: Dataset statistics</div>\nTable 4: Dataset statistics.\nDataset\nNodes\nEdges\nClasses\nFeatures\nLabels Mismatch\nVl\nV\nCora\n2708\n5429\n7\n1433\n0.002\n0.052\nCora-ML\n2995\n8416\n7\n2879\n0.018\n0.166\nCiteseer\n3327\n4372\n6\n3703\n0.003\n0.036\nPubmed\n19717\n44338\n3\n500\n0.0\n0.003\n# 5.2. Competitors\nOur method is compared with approaches of different nature. Competitors can be divided into four groups. First group includes approaches based on extensions of the GCN model. G-GCN Marcheggiani and Titov [2017] provides an extension adopting edge-wise gating to remove noisy edges during aggregation. GAT Veli\u02c7ckovi\u00b4c et al. [2018] provides a method based on attention which gives different weights to different nodes by allowing nodes to attend to their neighborhood. Dual-GCN Monti et al. [2018] allows to learn both vertex and edge features and generalizes the GAT model Veli\u02c7ckovi\u00b4c et al. [2018]. LGCN Gao et al. [2018] works based on a learnable graph convolutional layer (LGCL). LGCL automatically selects a fixed number of neighboring nodes for each feature based on value ranking in order to transform graph data into grid-like structures in 1-D format. Fast-GCN Liang et al. [2015] is an accelerated and optimized tool for constructing gene co-expression networks that can fully harness the parallel nature of GPU (Graphic Processing Unit) architectures. Second group includes approaches based on extensions of the GNN model Scarselli et al. [2008]. GGNN Li et al. [2016] generalizes RNN framework for graph-structured data application. GPNN Liao et al. [2018] adopts partition approach to spread the information after the subdivision of large graphs into subgraphs. Third group includes approaches based on embedding. SemiEmb Weston et al. [2012] is a framework which provides semi-supervised regularization to improve training. DeepWalk Perozzi et al. [2014] adopts random walks to learns node features. Planetoid Yang et al. [2016] adopts a transductive and inductive approach for class label prediction using neighborhood information. Fourth group includes baseline approaches. LP Zhu et al. [2003] is a label propagation algorithm which spreads labels information to neighborhoodfollowing the proximity. ManiReg Belkin et al. [2006] provides geometric regularization on data. Feat Yang et al. [2016] works based on node features ignoring the structure infor-\n# 5.3. Comparison\nWe have summarized our results by showing the best results of all the enhancements for all the datasets. Table 5 shows the accuracy of all the models mentioned in Section 4. We have been successful in getting state-of-the-art result on one dataset as well as very close to the stateof-the-art work done till now on the other three datasets as highlighted in green in Table 5. On Cora ML dataset we achieved the current best accuracy of 86.9 \u00b1 0.4 using DConfGCN model. This is the current state-of-theart based on our knowledge as the most recent papers i.e. Dual-GCN, LGCN, and Fast-GCN did not reported their results on Cora ML dataset. In case of Citeseer dataset, the best result which we achieved is 73.26%, this is more than Dual GCN and 0.3% less from LGCN. This makes our accuracy with ConvConfGCN the second best till date. However, just to highlight that LGCN Gao et al. [2018] report only the best result whereas our result are based on 100 run which are more stronger compare to reporting one highest performance. We have got the 3rd best accuracy for Pubmed dataset i.e. 79.8 \u00b1 0.4. Finally, on Cora dataset, we achieved 82.1\u00b11.2 accuracy with DGCN that is better than baseline GCN and ConfGCn by slight margin, but at 4th position overall in the list. One of the reason for not having the best result for Citeseer, Cora, and Pubmed could be that the best reported results in the papers Gao et al. [2018]; Monti et al. [2018]; Liang et al. [2015] are not having the mean performance over multiple runs. Another reason is that, our model can not be directly compared with model like LGCN as it uses regular convolutional kernel in their model. Rather designing new kernels to work on graph data, in LGCN the authors organized the graph data in a way that normal convolutional kernel can operate over it and learn feature from them. These enhancement and results are reported to provide baseline for future works to be done in the field of SSL for the Graphs. In table 6 execution time for PubMed dataset is shown. As the size of the features in each dataset varies, that is why the time (in seconds) per epoch varies for each dataset. GCN and its enhancements are faster compared to confGCN and its enhancements. While optimizing based on hyper-parameters, we found that the major reduction in computational cost was due to usage of the\n<div style=\"text-align: center;\">Table 5: Performance comparison of different methods on described</div>\ndatasets.\nMethod\nCiteseer\nCora\nPubmed\nCora ML\nLP Zhu et al. [2003]\n45.3\n68.0\n63.0\n-\nManiReg Belkin et al. [2006]\n60.1\n59.5\n70.7\n-\nSemiEmb Weston et al. [2012]\n59.6\n59.0\n71.1\n-\nFeat Yang et al. [2016]\n57.2\n57.4\n69.8\n-\nDeepWalk Perozzi et al. [2014]\n43.2\n67.2\n65.3\n-\nGGNN Li et al. [2016]\n68.1\n77.9\n77.2\n-\nPlanetoid Yang et al. [2016]\n64.9\n75.7\n75.7\n-\nG-GCN Marcheggiani and Titov [2017]\n69.6 \u00b1 0.5\n81.2 \u00b1 0.4\n77.0 \u00b1 0.3\n86.0 \u00b1 0.2\nGPNN Liao et al. [2018]\n68.1 \u00b1 1.8\n79.0 \u00b1 1.7\n73.6 \u00b1 0.5\n69.4 \u00b1 2.3\nGAT Veli\u02c7ckovi\u00b4c et al. [2018]\n72.5 \u00b1 0.7\n83.0 \u00b1 0.7\n79.0 \u00b1 0.3\n83.0 \u00b1 0.8\nGCN Kipf and Welling [2017]\n69.4 \u00b1 0.4\n80.9 \u00b1 0.4\n76.8 \u00b1 0.2\n85.7 \u00b1 0.3\nOpGCN\n70.1\u00b1 0.7\n80.3\u00b1 0.4\n79.1\u00b1 0.3\n85.3\u00b1 0.4\nConvGCN\n70.1 \u00b1 0.3\n80.1\u00b1 0.2\n79.0\u00b1 0.2\n84.3\u00b1 0.3\nCCGCN\n53.1 \u00b1 0.6\n55.3 \u00b1 2.4\n71.1 \u00b1 0.7\n63.3 \u00b1 0.4\nDGCN\n70.9 \u00b10.7\n82.1 \u00b1 1.2\n79.10 \u00b1 0.4\n86.3 \u00b1 0.3\nConfGCN Vashishth et al. [2019]\n72.7 \u00b1 0.8\n82.0 \u00b1 0.3\n79.5 \u00b1 0.5\n86.5 \u00b1 0.3\nOpConfGCN\n70.1 \u00b1 1.4\n80.9 \u00b1 0.8\n79.8 \u00b1 0.4\n84.6 \u00b1 0.5\nConvConfGCN\n73.1\u00b1 0.2\n82.1\u00b1 0.6\n79.8\u00b1 0.4\n86.4\u00b1 0.3\nCCConfGCN\n70.8 \u00b1 0.3\n82.1 \u00b1 0.6\n78.2 \u00b1 0.4\n83.4 \u00b1 0.5\nDConfGCN\n58.03 \u00b1 0.9\n81.0 \u00b1 1.4\n78.8 \u00b1 0.6\n86.9 \u00b1 0.4\nDual-GCN Monti et al. [2018]\n72.6\n83.5\n80.0\n-\nLGCN Gao et al. [2018]\n73.4\n83.3\n79.7\n-\nFast-GCN Liang et al. [2015]\n-\n86\n88\n-\ncross-entropy softmax V2 function rather than simple cross-entropy. Therefore, in all our later experiments we used this loss function. ConfGCN based models took more time compare to GCN based models. The optimal models interms of execution time is OPGCN.\n<div style=\"text-align: center;\">Table 6: Execution time on Pubmed dataset</div>\nTable 6: Execution time on Pubmed dataset\nMethod\nTime (sec)\nGCN Kipf and Welling [2017]\n0.8\nOpGCN\n0.415\nConvGCN\n0.585\nCCGCN\n0.417\nDGCN\n0.662\nConfGCN Vashishth et al. [2019]\n1.344\nOpConfGCN\n1.93\nConvConfGCN\n1.96\nCCConfGCN\n1.93\nDConfGCN\n1.99\n# 6. Conclusions\nWe present enhanced models of GCN and ConfGCN for the Graph Convolution with Semi-supervised learning. The main focus among all the enhancements was on four changes: parametric configuration, adding more structural information to adjacency matrix for graph representation, convex optimization related to activation functions\nand combination of base models and dense layers. As this work is related to the Graph convolutions and with these enhanced models we have been able to show that the addition of the layers can be helpful for the increment of the accuracy, so this process has opened a path where addition of layer means accuracy reduction limitation of SSL has been removed. Also currently all the Graph Convolutional Layers are using 1D convolutions to operate the model, there can be 2D or 3D dimensional weighing schemes can be implemented on the concurrent models. The GCN was a new approach for SSL and in that the layer-wise propagation rule was implemented while ConfGCN is a model which estimates label scores with labels confidence. We have prepared six different models with different configurations and we have validated our models with four benchmark datasets. In majority of the enhancements, we have been successful in increasing the accuracy as well as the execution time for all the best possible configurations in all four data-sets.\n# Acknowledgments\nThe first two authors acknowledge the guidance and supervision of their late Prof. Alfredo Petrosino. May he rest in peace.\n# References\nJames Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1993\u20132001, 2016.\nMikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research, 7(Nov):2399\u20132434, 2006.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally connected networks on graphs. In International Conference on\nLearning Representations (ICLR2014), CBLS, April 2014, 2014.\nMicha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pages 3844\u2013 3852, 2016.\nHongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1416\u20131424. ACM, 2018.\nMikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. CoRR, abs/1506.05163, 2015.\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.\nMeimei Liang, Futao Zhang, Gulei Jin, and Jun Zhu. Fastgcn: a gpu accelerated tool for fast gene co-expression networks. PloS one, 10(1):e0116776, 2015.\nRenjie Liao, Marc Brockschmidt, Daniel Tarlow, Alexander L. Gaunt, Raquel Urtasun, and Richard S. Zemel. Graph partition neural networks for semi-supervised classification. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018.\nFranco Manessi and Alessandro Rozza. Learning combinations of activation functions. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 61\u201366. IEEE, 2018.\nDiego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506\u20131515, 2017.\nFederico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan G\u00a8unnemann, and Michael M Bronstein. Dual-primal graph convolutional networks. arXiv preprint arXiv:1806.00770, 2018.\nTore Opsahl. Triadic closure in two-mode networks: Redefining the global and local clustering coefficients. Social Networks, 35(2):159\u2013167, 2013.\nMatan Orbach and Koby Crammer. Graph-based transduction with confidence. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 323\u2013338. Springer, 2012.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM, 2014.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29 (3):93\u201393, 2008.\nAlexander Strang, Oliver Haynes, Nathan D Cahill, and Darren A Narayan. Generalized relationships between characteristic path length, efficiency, clustering coefficients, and density. Social Network Analysis and Mining, 8(1):14, 2018.\nShikhar Vashishth, Prateek Yadav, Manik Bhandari, and Partha Talukdar. Confidence-based graph convolutional networks for semi-supervised learning. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 1792\u20131801, 2019. Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. Jason Weston, Fr\u00b4ed\u00b4eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade, pages 639\u2013655. Springer, 2012. Prateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth, Arun Rajkumar, and Partha Talukdar. Lov\u00b4asz convolutional networks. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1978\u20131987, 2019. Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48, pages 40\u201348. JMLR. org, 2016. Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912\u2013919, 2003.\nShikhar Vashishth, Prateek Yadav, Manik Bhandari, and Partha Talukdar. Confidence-based graph convolutional networks for semi-supervised learning. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 1792\u20131801, 2019.\nPetar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.\nJason Weston, Fr\u00b4ed\u00b4eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade, pages 639\u2013655. Springer, 2012.\nPrateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth, Arun Rajkumar, and Partha Talukdar. Lov\u00b4asz convolutional networks. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1978\u20131987, 2019.\n",
    "paper_type": "method",
    "attri": {
        "background": "In the current era of neural networks and big data, higher dimensional data is processed for automation of different application areas. Graphs represent a complex data organization in which dependencies between more than one object or activity occur. Due to the high dimensionality, this data creates challenges for machine learning algorithms. Graph convolutional networks were introduced to utilize the convolutional models concepts that shows good results. In this context, we enhanced two of the existing Graph convolutional network models by proposing four enhancements.",
        "problem": {
            "definition": "The paper aims to solve the issue of effectively processing graph-structured data for semi-supervised learning, particularly focusing on improving the performance of Graph Convolutional Networks (GCNs).",
            "key obstacle": "The main challenge is the high dimensionality of graph data, which complicates the learning process and leads to suboptimal performance of existing methods."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to improve the performance and efficiency of GCNs in processing graph data, particularly through enhancements that address their limitations.",
            "opinion": "The proposed idea involves enhancing existing GCN models by optimizing hyperparameters, enriching topological information, and redesigning the network structure.",
            "innovation": "The key innovation lies in the combination of hyperparameter optimization, convex activation functions, incorporation of clustering coefficients, and the addition of dense layers to the GCN architecture."
        },
        "method": {
            "method name": "Enhanced Graph Convolutional Networks",
            "method abbreviation": "EGCN",
            "method definition": "EGCN refers to the enhanced versions of Graph Convolutional Networks that integrate various improvements for better performance in semi-supervised learning tasks.",
            "method description": "The method enhances GCNs by optimizing hyperparameters, enriching adjacency matrices with clustering coefficients, and integrating dense layers.",
            "method steps": [
                "Optimize hyperparameters such as activation functions and loss functions.",
                "Introduce clustering coefficients into the adjacency matrix.",
                "Combine GCN and dense layers to form a new model architecture.",
                "Evaluate the performance on benchmark datasets."
            ],
            "principle": "The method is effective due to its comprehensive approach that combines multiple enhancements, allowing for better representation of graph structures and improved learning efficiency."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on four benchmark datasets: Cora, Citeseer, Pubmed, and Cora-ML, which are commonly used for semi-supervised graph classification tasks.",
            "evaluation method": "Performance was assessed by comparing the accuracy of the enhanced models against state-of-the-art methods across the datasets, using metrics such as accuracy and execution time."
        },
        "conclusion": "The enhanced GCN and ConfGCN models demonstrated significant improvements in accuracy and execution time across multiple benchmark datasets, validating the effectiveness of the proposed enhancements.",
        "discussion": {
            "advantage": "The proposed approach stands out due to its comprehensive enhancements that lead to improved accuracy and reduced computational costs compared to existing methods.",
            "limitation": "Despite the advancements, the method may still face limitations in scalability for very large graphs and the increased complexity of hyperparameter tuning.",
            "future work": "Future research should focus on further optimizing the model for larger datasets and exploring alternative architectures that could leverage 2D or 3D convolutions."
        },
        "other info": {
            "acknowledgments": "The first two authors acknowledge the guidance and supervision of their late Prof. Alfredo Petrosino."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper introduces Enhanced Graph Convolutional Networks (EGCN) as a method to improve performance in semi-supervised learning tasks involving graph-structured data."
        },
        {
            "section number": "2.2",
            "key information": "The paper discusses the historical development of Graph Convolutional Networks (GCNs) and their challenges, particularly the high dimensionality of graph data that complicates the learning process."
        },
        {
            "section number": "3.1",
            "key information": "The architecture of EGCN combines GCN and dense layers, optimizing hyperparameters and integrating clustering coefficients into the adjacency matrix."
        },
        {
            "section number": "5.1",
            "key information": "The proposed method enhances GCNs by optimizing hyperparameters, enriching topological information, and redesigning the network structure for improved model efficiency."
        },
        {
            "section number": "7.1",
            "key information": "The paper identifies scalability as a challenge for the proposed method when applied to very large graphs and notes the increased complexity of hyperparameter tuning."
        },
        {
            "section number": "7.3",
            "key information": "Future research directions include optimizing the model for larger datasets and exploring alternative architectures that could leverage 2D or 3D convolutions."
        }
    ],
    "similarity_score": 0.5962022036496674,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Graph Convolutional Networks_ analysis, improvements and results.json"
}