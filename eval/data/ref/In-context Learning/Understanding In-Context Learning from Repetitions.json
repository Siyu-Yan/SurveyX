{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.00297",
    "title": "Understanding In-Context Learning from Repetitions",
    "abstract": "This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of token co-occurrence reinforcement, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.",
    "bib_name": "yan2024understandingincontextlearningrepetitions",
    "md_text": "# UNDERSTANDING IN-CONTEXT LEARNING FROM REPETITIONS\nJianhao Yan1,2 Jin Xu4 Chiyu Song1,2 Chenming Wu5 Yafu Li1,2 Yue Zhang2,3,\u2217 1Zhejiang University 2School of Engineering, Westlake University 3 Institute of Advanced Technology, Westlake Institute for Advanced Study 4 Tsinghua University 5 Baidu Research elliottyan37@gmail.com\n# ABSTRACT\nThis paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of token co-occurrence reinforcement, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.\n# INTRODUCTION\nThe impressive ability of Large Language Models (LLMs; Touvron et al. 2023a; Chowdhery et al. 2022; OpenAI 2023) to execute in-context learning (ICL) is a standout characteristic. This behavior mirrors human learning and reasoning from analogy (Winston, 1980), enabling LLMs to rapidly adapt to a range of downstream tasks. Without being explicitly pretrained to learn from demonstrations, LLMs can predict responses to unseen test queries from a few demonstrations and without any instruction given (Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022). An example of in-context learning can be found in Figure 1(a), where a pre-trained LLaMA model is given demonstrations for a binary classification task, and learns to make predictions correctly. Despite the success in applications, the working mechanism of in-context learning is still an open question. Existing work has investigated input-label mapping (Min et al., 2022; Yoo et al., 2022; Wei et al., 2023) and demonstration construction (An et al., 2023; Lu et al., 2022; Liu et al., 2022) as underlying factors for ICL. However, little research has focused on the correlation between ICL and textual features. Intuitively, the behavior of ICL depends on the context and can be fragile to its variations. As Figure 1(b) shows, the same LLaMA model makes the incorrect prediction \u2018True\u2019 given the input \u201cCirculation revenue has decreased by 5% in Finland.\u201d, which is likely because of the repeated pattern \u201cAnswer:\u201d -> \u201cTrue\u201d from the demonstrations. In the same perspective, the success case in Figure 1(a) can be attributed to learning desired patterns such as \u201cAnswer:\u201d -> \u201cTrue|False\u201d in the demonstrations. Such patterns are apparently used as features in the autoregressive inference process by the model. We take a feature-centric view to understand ICL, analyzing the key patterns in the input context that correlate with ICL behavior. The patterns we discussed above can be viewed as generalizations to repetition patterns (Holtzman et al., 2019; Fu et al., 2020) and self-reinforced patterns (Xu et al., 2022) which have been discussed in the literature. The \u2018self-reinforcement effect\u2019 describes a phenomenon where the model tends to perpetuate the generation of sentences that have frequently appeared in its context. These effects are regarded as harmful to text generation and previous work puts efforts\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e617/e61743d3-8e2c-467e-bcc2-8245239727c1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) A correct prediction of in-context learning.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e38e/e38e859a-aedd-413a-a131-7ad9fee406ae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) An in-correct prediction of in-context learning.</div>\nFigure 1: We showcase correct and incorrect predictions of in-context learning of LLaMA-65B. The shown task is to identify whether the given sentence presents a positive sentiment. We involve the token reinforced connections from demonstrations. In both cases, LLMs learn connections from the demonstrations and make decisions based on these connections. In the case of in-context learning, the model learns reliable connections and hopefully several of these connections result in the function of sentiment analysis. On the other hand, in repetitive demonstrations, the model gets stuck to spurious connections and misses the key information \u2018decreased\u2019, leading to a wrong prediction.\nto mitigate it. However, they could give a unique view of the ICL behaviors from the angle of text generation\nWe quantitatively investigate in-context learning from the perspective of surface patterns, illustrating the inherent correlation among surface patterns, self-reinforcement, and ICL. First, we study the roles of self-reinforce patterns as surface features that guide text generation. We empirically establish the existence of the token co-occurrence reinforcement, where the connection between any two tokens gets reinforced with the number of contextual co-occurrences, a primary principle in learning surfacelevel patterns. We further delve into the reasons and inner-workings causing token reinforcement, showing it as an evitable result out of model\u2019s efforts on maximizing likelihood on data. Given the existence and reasons behind such patterns, we scrutinize the beneficial and detrimental effects of these surface patterns on in-context learning. On the one hand, experiments on MMLU and GSM8K show that the reinforcement helps constrain the output space and format outputs to follow demonstrations like outputting \u2018Let\u2019s think step by step.\u2019. On the other hand, experiments with non-informative connections and reordered answers in MMLU demonstrate that intentionally constructed connections make LLMs lean towards specific answers, revealing the risk of unintended, spurious connections. This not only reveals the intrinsic workings of in-context learning to some extent, providing a perspective not analyzed in the literature but also explains the underlying reasons for the failure of in-context learning1.\n//github.com/ElliottYan/understand-icl-from-repetition\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/acca/accaa677-ca8f-4e50-8706-d89e5e39546d.png\" style=\"width: 50%;\"></div>\nFigure 2: Left: An example of the self-reinforcement effect. We choose a normal sentence (\u2018Answer is A\u2019), repeat it several times, and present the probability of the token \u2018A\u2019. The model used is LLaMA-7B. Right: Sentence-level self-reinforcement over LLMs. We plot all sizes of OPT and LLaMA with colors from light to dark. All sizes of LLaMA and OPT models demonstrate strong sentence-level self-reinforcement effects.\n<div style=\"text-align: center;\">Figure 2: Left: An example of the self-reinforcement effect. We choose a normal sentence (\u2018Answer is A\u2019), repeat it several times, and present the probability of the token \u2018A\u2019. The model used is LLaMA-7B. Right: Sentence-level self-reinforcement over LLMs. We plot all sizes of OPT and LLaMA with colors from light to dark. All sizes of LLaMA and OPT models demonstrate strong sentence-level self-reinforcement effects.</div>\n\u2022 We propose a novel perspective to understand ICL with repetitive text generations. \u2022 We perform systematic analyses and empirically establish the existence of token reinforcement across various LLMs, alongside with the reason behind. \u2022 We show that token reinforcement constrains output space and enables desired patterns for ICL, but is also responsible for spurious connections and possible failure of ICL.\n# 2 ICL AND REPTITIONS\nIn ICL, given a desired task f, we feed an LLM with K demonstrations {(xk, yk), n \u2208[1, K]}, where yk = f(xk). Here, yk can be a label word or a free text phrase. Each demonstration dk = (FI, xk, FO, yk) can be divided into four parts. FI and FO denote formatting tokens for inputs and outputs, e.g., \u2018Input:\u2019 and \u2018Answer:\u2019. Note that xk and yk can consist of several tokens. A pretrained language model M predicts the output y conditioned on the concatenation of both demonstrations and the test query x,\nPICL(y|x, k) := M(y|(FI, x1, FO, y1, \u00b7 \u00b7 \u00b7 , FI, xk, FO, yk, FI, x, FO))\nTo further understand the influence of surface repetitions from Figure 1(b), we show a case of sentence repetition and plots the probability of the sentence as the number of repetition in the previous context increases (Figure 2). When we manually repeat the sentence \u2018Answer is A\u2019, the probability of generating \u2018A\u2019 after \u2018Answer is\u2019 gets boosted from 0.03 to almost 1.0. The right part of Figure 2 demonstrates our preliminary study with two families of large language models quantitatively, namely OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, Zhang et al. 2022) and LLaMA (7B, 13B, 30B, 65B, Touvron et al. 2023a). We plot the average sentence probability after 10 repeats for LLMs of varying sizes, arranged by light to dark colors. We use 1,000 sentences from each of the three different datasets \u2013 Wikitext-103 (Merity et al., 2016), BookCorpus (Zhu et al., 2015), and sequences of random words. More experimental details can be found in Appendix B. With 10 repeats, the probability of generating the sentence is significantly increased across all tested LLMs. Current LLMs amplify the occurrence of previously presented sentences, even sequences of random tokens.2 The above observations are related to the study of self-reinforcement effect (Xu et al.,\nTo further understand the influence of surface repetitions from Figure 1(b), we show a case of sentence repetition and plots the probability of the sentence as the number of repetition in the previous context increases (Figure 2). When we manually repeat the sentence \u2018Answer is A\u2019, the probability of generating \u2018A\u2019 after \u2018Answer is\u2019 gets boosted from 0.03 to almost 1.0.\n2Xu et al. (2022) reported that sentences with a low initial probability \u2014 such as sentences composed of random tokens \u2014 have a smaller self-reinforcement effect. In our experiments, even sentences with random tokens (which initially have a near-zero probability) become reinforced to a probability nearing one. The difference may come from different model sizes (150M vs. maximum 65B) and pretrained corpus size (hundred millions of tokens vs. trillions of tokens).\n2Xu et al. (2022) reported that sentences with a low initial probability \u2014 such as sentences composed of random tokens \u2014 have a smaller self-reinforcement effect. In our experiments, even sentences with random tokens (which initially have a near-zero probability) become reinforced to a probability nearing one. The difference may come from different model sizes (150M vs. maximum 65B) and pretrained corpus size (hundred millions of tokens vs. trillions of tokens).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c824/c824941e-cb6b-4b08-ae61-f08b6a757f95.png\" style=\"width: 50%;\"></div>\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a1ab/a1ab6dcc-6f99-4577-9d6e-75b8a45e809b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Token co-occurrence reinforcement. Even if only one token repeats in context, the self-reinforcement loop triggers. \u201c..X..Y..\u201d denotes 2 tokens are kept unchanged. The mean and variance are computed over 1,000 randomly generated samples.</div>\nM(w|[s1; s2; \u00b7 \u00b7 \u00b7 ; sn\u22121; w1 \u00b7 \u00b7 \u00b7 wi\u22121]), where s is the repeating sentence, n denotes the number of occurrences, and wi is the i-th token in the sentence s. Previous research finds the self-reinforcement effect, where the probability of generating the sentence s of length Ls occurred N times in context TPN = 1 Ls \ufffd i PREP(w|w = wi; n = N), almost monotonically increases with the number of N While the generation of repetitive sentences above can be understood as the influence of a single surface feature, we investigate more sophisticated surface patterns, which can be causes to behaviors of in-context learning.\n# 3 SELF-REINFORCED SURAFACE FEATURES FOR IN-CONTEXT LEARNING\n3 SELF-REINFORCED SURAFACE FEATURES FOR IN-CONTEXT LEARNING\nIn accordance with Figure 1, we set s = [FI; x1; FO; y1] and have,\nPREP(w|n = K) = M(y|( K times \ufffd \ufffd\ufffd \ufffd FI, x1, FO, y1, \u00b7 \u00b7 \u00b7 , FI, x1, FO, y1, FI, x1, FO)). PICL(y|x, k = K) = M(y|(FI, x1, FO, y1, \u00b7 \u00b7 \u00b7 , FI, xK, FO, yK, FI, x, FO)).\nComparing PREP(w) to PICL(y), we find: (1) FI and FO are both repeated across demonstrations; (2) In repetitive generation, x1 and y1 are repeated, while in ICL, x and y are changing. To investigate the correlation between surface patterns and the resulting answer y, we gradually expand self-reinforcement patterns toward in-context learning. We achieve this by introducing random perturbations to each demonstration, imitating the role of changing x while leaving certain components, such as FO and y, unchanged. The experiments in this section are conducted over the dataset of randomly generated sentences as in Section 2 and with the four LLaMA models. The results on Wikitext-103 and BookCorpus, and results with OPT and various other LLMs can be found in Appendix D. For each experiment, we repeat the pattern 20 times and report the mean and standard deviation of the probabilities for the kept tokens.\n# 3.1 ANY TWO TOKENS FORM A TOKEN REINFORCED LOOP\nWe assume that tokens are the base unit of self-reinforcement. By assessing the self-reinforcement effect among tokens, we understand the self-reinforcement effect from first principles. Formally, given a sentence s = (w1, \u00b7 \u00b7 \u00b7 , wLs) from a corpus D, we construct a binary mask sequence \u02c6m = ( \u02c6m1, \u02c6m2, \u00b7 \u00b7 \u00b7 , \u02c6mLs), and we define a replacement operation R(w, \u02c6m) = \ufffdwr, if \u02c6m = 0 w, if \u02c6m = 1 that replaces w with a randomly sampled token wr from the vocabulary if \u02c6m = 0 and keep it unchanged when \u02c6m = 1. Note that wr is independently sampled for each sentence and each position. As for mask sequence \u02c6m, we randomly sample positions to put the 0s and 1s of the mask sequence \u02c6m and control the number of kept tokens. In this way, a sentence s is transformed into \u02c6sn = (R(w1, \u02c6m1), \u00b7 \u00b7 \u00b7 , R(wLs, \u02c6mLs)), where \ufffd l\u2208[1,Ls] \u02c6ml = Lt. Then, we report the average token probability \u02c6 TPN as in the previous section. Suppose we have a sentence s = (Apple, there, is, red)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7dcf/7dcfa60d-67e2-4bc2-bb76-76cd7aa0002a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Successive and distant reinforcement. The self-reinforcement effect is the strongest when two tokens are successive, i.e., distance=0. Otherwise, the reinforcement is smaller and appears insensitive to the distance. \u201c..X.Y..\u201d denotes the distance between two tokens is 1. and m = (1, 0, 1, 1), and the target token w = red. Then, the demonstrations in this section will be like \u2018Apple there is red // Apple logo is red // Apple juice is red\u2019.</div>\nNumber of Tokens As depicted in Figure 3, even only one single token shared across demonstrations elicits self-reinforcement. We are particularly interested in the scenario with two tokens kept unchanged, as it reveals a fundamental rule of one token triggers the generation of the other one. We find that the connection between any two tokens gets reinforced and the probability increases monotonically with the number of their contextual co-occurrences. We refer to this base effect as the token co-occurrence reinforcement. When we increase the number of preserved tokens from 2 to 4, we observe a strengthening of the reinforcement effect. This is because each former token forms a reinforced connection with all the latter ones.\nDistance In-Between We further examine the role of distance in token reinforcement. This analysis is confined to two tokens. Figure 4 distinctly differentiates between successive tokens (distance= 0) and distant tokens (distance>= 1), which we term as successive reinforcement and distant reinforcement, respectively. The successive reinforcement significantly elevates the probability, from 0 to 0.4, with only several repetitions. Conversely, the distant reinforcement provides a moderate boost to the probability, from 0 to 0.2, and appears to be indifferent to the distance between tokens. Across all experiments, we observe a marked increase in reinforcement as model size scales, especially in the distant token reinforcement. This consistent escalation suggests that larger LLMs are more capable of following complex patterns in in-context demonstrations, which is consistent with results from Wei et al. (2023). We provide more supporting evidence of token reinforcement in Appendix D. The link we found between any two tokens forms the foundation of sentence-level self-reinforcement. Each token is strengthened by the ones before it. In ICL, common elements in demonstrations, such as pattern words, form connections with label words like \"A, B, C, D\".\n# 3.2 REASON BEHIND TOKEN REINFORCEMENT\nToken reinforcement is inherently embedded within the pre-training corpus. We find that token reinforcement could be a result of the model\u2019s effort to maximize the likelihood of the training data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf20/cf20f1ac-1c1d-4c01-bf27-cc89320cb7df.png\" style=\"width: 50%;\"></div>\nGiven a pre-training corpus Dtrain, the language modeling task over a sequence of tokens {w1, w2, \u00b7 \u00b7 \u00b7 , wT } with length T is defined by max \u2212log P(wi|w<i). We compute the following empirical probabilities over the corpus:\nwhere Count(\u00b7, \u00b7) denotes the function to count the number of a word w in the partial sequence w<i, and V is the vocabulary. Intuitively, these probabilities reveal whether a particular word w is likely to recur if there have already been n instances of w\nFigure 5: The probabilities of next occurrence after several occurrence observed in context.\nreinforcement.\nWe compute these probabilities over a commonly used pretraining corpus, wikipedia-english-20223, encompassing over 5.7B tokens, using the LLaMA tokenizer to first tokenize corpus and preprocess each context window with T = 1024 tokens. Figure 5 demonstrates our results. We find that the trend of word probabilities accords to our results in Section 3.1. The more instances seen, the greater the probability of the same word recurring. Therefore, we infer that the token co-occurrence reinforcement stems from the LLMs\u2019 optimisation of the training likelihood. The LLMs manage to learn this inherent feature from the training data and generalize it to longer phrases and distant connections. This also elucidates the scaling with reinforcement, where larger models more effectively capture this feature from the training corpus.\n# 3.3 UNDERSTANDING ICL VIA REINFORCEMENTS\nToken reinforcement effect discussed in previous sections provides a new perspective to understand in-context learning. Consider the following example with three demonstrations [A,B,C,D ; A,b,C,D ; a,B,C,D ; A,b,C,(?)]. Our target is \u2018D\u2019. In this example, several reinforced connections exist concurrently. \u2018A->D\u2019 is reinforced twice; \u2018b->D\u2019 is reinforced once; \u2018C->D\u2019 is reinforced three times. Here, all three reinforcements reach a consensus and predict \u2018D\u2019 in cooperation. However, this ideal scenario is not always the case. Consider another example [A,B,C,D ; A,b,C,E ; a,B,C,F ; A,b,C,(?)]. At this time, \u2018A->D\u2019 is reinforced once; \u2018b->E\u2019 is reinforced once; \u2018a->F\u2019 is reinforced once and et cetera. These reinforcements create conflicts and compete against each other. Thus, instead of the traditional view of ICL as a mapping from input to output, we view ICL from a feature angle, as a combination of connection of tokens, even though some of them are highly reinforced and some of them are not. The next section shows how the reinforcements play a crucial role in in-context learning.\n# 4 THE EFFECTS OF SURFACE PATTERNS TO IN-CONTEXT LEARNING\nWe quantitatively study how self-reinforced surface patterns lead to both beneficial functions and detrimental effects in ICL. The experiments in this section are conducted over MMLU (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). Due to limit of computing resources, we randomly sampled 20 samples for each of the 57 tasks of MMLU, resulting in a collection of 1140 test samples. The demonstrations are independently drawn for each of the test samples. All experiments are conducted across three random seeds. For further experimental details, see Appendix B.\n# 4.1 BENEFICIAL EFFECTS\nConstraining Output Space. An important advantage brought by the reinforced effect is that it helps constrain the output space \u2014 with several demonstrations, connections between formatting tokens (\u2018Input:\u2019 and \u2018Answer:\u2019) and label words in each demonstration (\u2018ABCD\u2019) are reinforced, through distant and successive reinforcement, respectively. In this way, the LLM learns to predict either one of \u2018ABCD\u2019 as the final answer, instead of continuation sequences such as \u2018Oh, an interesting question. I hope I know the answer.\u2019. We verify this advantage with the MMLU dataset, which is widely used to evaluate the language understanding of real-world large language models. To isolate the effect of self-reinforcement, we construct masked demonstrations for analyses. An example of how we mask demonstrations is shown in the left part of Figure 6. Particularly, a demonstration in the MMLU dataset can be divided into five parts: question content, option name (e.g., \u2018A.\u2019), option content (e.g., \u2018114, 27.35\u2019), answer indicator (e.g., \u2018Answer:\u2019) and final answer (e.g., \u2018D\u2019). Based on token reinforcement, we hypothesize that the option names, i.e., \"A.\", \"B.\", reinforce outputting \"A,B,C,D\" via distant reinforcement. The answer indicator, i.e., \u201cAnswer: \u201d, reinforces outputting within label space via successive reinforcement. To validate our hypothesis, we first mask all the questions and option contents in all demonstrations and keep the formatting words, final answer, and test query unchanged. Then, we\ns://huggingface.co/datasets/olm/olm-wikipedia-20221220\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/80c5/80c532c8-f378-4ef2-a3b3-4d7e9a7bccf1.png\" style=\"width: 50%;\"></div>\nFigure 6: Left: An example demonstration from MMLU\u2019s high school statistics dataset. Colors indicate the parts to be masked. Right: Probability of MMLU\u2019s label space. We find that: (1) Masking question contents and answer contents of the demonstration does not influence directing the label space. (2) Both replacing the option names and the answer indicator significantly hurt the ability to constrain the label space. The gray shadow denotes the standard deviation across three runs.\nLearning to Follow Patterns. Another distinctive feature of in-context learning is to follow the patterns of demonstrations. This is exemplified in techniques such as the Few-shot Chain-of-thought (CoT) prompting (Wei et al., 2022), frequently employed in reasoning tasks of LLMs like the GSM8K (Cobbe et al., 2021). Here, we illustrate how the reinforced features in Section 3.1 affect the pattern following of ICL, by showing how LLMs follow the chain-of-thought demonstrations in the GSM8K high school math dataset. Each demonstration in the dataset follows the form \u201cQuestion: [Question] // Let\u2019s think step by step. // [CoT Answer]\u201d. We demonstrate how models learn to say the CoT pattern, i.e., \u201cLet\u2019s think step by step.\u201d. We further discuss the connection between surface patterns and [CoT Answer] in Appendix E.1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e4ff/e4fffe35-e368-44d1-8a31-1809ab5a8e16.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Ability to follow patterns from demonstrations.</div>\nBased on the findings in previous sections, we hypothesize that the common parts in the demonstrations teach the LLM to generate the CoT pattern. More specifically, \u2018Question:\u2019 builds a distant reinforcement, and the new liner \u2018//\u2019 builds a successive reinforcement with the CoT pattern. We mask out each part in each demonstration progressively with random tokens to ablate the influences. The probability of the CoT pattern is shown in Figure 7. After masking out \u201c//\", the probability gains obtained from demonstrations almost diminish, verifying our hypothesis of successive reinforcement. Another interesting finding is masking [Question] reduces the probability of generating the CoT pattern, indicating the [Question] part to some extent builds a connection with the CoT pattern. Since the questions in demonstrations are different but lie in the same group of grad math problems, there might be some common patterns among these questions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7abc/7abca971-5aae-4b9a-ae76-16ee85153442.png\" style=\"width: 50%;\"></div>\nTable 1: Top: Effect of Non-Informative Connections (NC). The accuracy of D increases with the cost of A, B and C. Bottom: Effect of Reordered Answers. With more reordered demonstrations, the outputs are more leaned toward D. The delta values on the superscript denote the improvement compared with zero-shot scenarios. \u2018Avg. [A,B,C]\u2019 denotes the average accuracy of samples whose golden answer is A, B or C. \u2018D\u2019 denotes the accuracy of samples whose golden answer is D. \u201c\u2020\": significantly different compared to its corresponding ICL baseline (p < 0.05).\nNon-informative Connections\n# Demos\n0\n1\n2\n3\n4\n5\nAvg. [A,B,C]\n63.39\n63.74+0.35\n64.56+1.17\n64.17+0.78\n64.44+1.05\n64.37+0.97\nAvg. [A,B,C] w/ NC\n63.04\n61.21-1.83\u2020\n62.57-0.47\u2020\n63.27+0.23\u2020\n63.00-0.04\u2020\n63.47+0.43\nD\n52.28\n59.65+7.37\n60.00+7.72\n59.30+7.02\n59.88+7.60\n59.53+7.25\nD w/ NC\n49.47\n63.63+14.15\u2020\n64.09+14.62\u2020\n63.51+14.04\u2020\n62.57+13.10\u2020\n61.64+12.16\nReordered Answers\nAvg. [A,B,C]\n63.39\n63.74+0.35\n64.56+1.17\n64.17+0.78\n64.44+1.05\n64.37+0.97\nAvg. [A,B,C] w/ 50% RA\n63.39\n64.56+1.17\n64.44+1.05\n64.05+0.66\n63.94+0.55\n63.86+0.47\nAvg. [A,B,C] w/ 75% RA\n63.39\n64.52+1.13\n62.65-0.74\u2020\n62.53-0.86\u2020\n61.95-1.44\u2020\n62.34-1.05\u2020\nAvg. [A,B,C] w/ 100% RA\n63.39\n64.80+1.40\u2020\n62.03-1.36\u2020\n61.17-2.22\u2020\n59.10-4.29\u2020\n58.05-5.34\u2020\nD\n52.28\n59.65+7.37\n60.00+7.72\n59.30+7.02\n59.88+7.60\n59.53+7.25\nD w/ 50% RA\n52.28\n59.30+7.02\n60.82+8.54\n61.40+9.12\n61.75+9.47\n61.64+9.36\nD w/ 75% RA\n52.28\n59.06+6.78\n62.81+10.53\n65.50+13.22\u2020\n67.02+14.74\u2020\n66.90+14.62\u2020\nD w/ 100% RA\n52.28\n58.71+6.43\n66.20+13.92\u2020\n71.35+19.06\u2020\n75.67+23.39\u2020\n77.19+24.91\u2020\n4.2 DETRIMENTAL EFFECTS\nToken reinforcement is not always helpful. In this section, we explore the detrimental consequences that arise from it. As shown in Section 3.1, distant and successive reinforcement are activated with even two random tokens. This could potentially lead to spurious patterns across demonstrations, which might be completely unforeseen by end users. We illustrate this point using two experiments where we manually construct spurious patterns in the MMLU dataset.\nNon-informative Connections. Our first approach is adding connections between a phrase and a certain choice. We append a reasonable but non-informative phrase such as \u2018Please kindly provide your answer.\u2019 or \u2018Looking forward to your choice.\u2019 right before the template \u2018Answer:\u2019 each time the question\u2019s answer is \u2018D\u2019. In testing, we also append the same phrase and check whether the outputs are navigated toward \u2018D\u2019. By doing so, we construct a distant reinforcement loop from the non-informative phrase to the answer \u2018D\u2019. We ensure the test set is balanced with equal numbers of questions having golden answers \"A,B,C,D\", and we report the accuracies at the top of Table 1. We first see a gap even without demonstrations, where adding non-informative phrases lowers the accuracy of choices D. We further discuss the selection bias (Zheng et al., 2023) of different choices in the Appendix. Then, we see that the non-informative connection overcomes the selection bias and significantly elevates the accuracy choice D with a noticeable gap, in the cost of accuracy of A, B, and C. These results show the potential risk of manually injecting spurious connections and directing in-context learning toward unintended outcomes.\nAnswer Indicator Connections. In our second experiment, we show that reinforcing the connection between \u2018Answer:\u2019 and a certain choice, e.g., D, navigates the outputs. To this end, we randomly replace r percent of demonstrated answers with D. Simultaneously, we exchange the option contents of the original golden answer and D, to keep the demonstrations valid. In this way, we gradually reinforce the connection between \u2018Answer:\u2019 and \u2018D\u2019, with successive reinforcement. The results are presented at the bottom of Table 1. Our baseline is 25%, where the demonstrations are balanced. With the increased ratio of answer D in demonstrations, the accuracy of D is largely improved, from 0.52 to 0.78, while the accuracy of A, B, and C decreases from 0.63 to 0.58. Our findings corroborate with An et al. (2023), where they show how diversity affects the performance of in-context learning. Our findings demonstrate how unbalanced demonstrations bring an unfair advantage for certain outputs.\nDiscussion (1) Reinforcements can be the underlying reason for ICL, but also causes its vulnerability. (2) Our observations guide how to build demonstrations to maximize ICL effectiveness! The demonstrations should be both balanced for all possible output labels, and kept concise enough without introducing any unnecessary reinforcement.\n# 5 RELATED WORK\nExplaining In-Context Learning. A range of contributions has deepened our understanding of In-Context Learning (ICL). Chan et al. (2022) and Xie et al. (2022) explore the emergence of ICL from the perspective of training data and Bayesian inference, respectively. Implicit learning of ICL over demonstrations is further highlighted by Garg et al. (2023), Li et al. (2023), and Hahn & Goyal (2023) theoretically show that performance of ICL can be represented by a complexity that repetition structures can be represented by a small PCFG tree. The similarity between gradient descent learner and in-context learner is demonstrated by Aky\u00fcrek et al. (2023); von Oswald et al. (2023), while Dai et al. (2023) explain language models as meta-optimizers and likens ICL to implicit finetuning. Our work differs from this line of work with a novel perspective via repetitions and reinforced features, and our findings could potentially explain the mechanism of how LLMs achieve implicit learning. For instance, the step by step reinforcement across demonstrations intuitively resembles the gradient descent process of ICL described in previous work. Olsson et al. (2022) introduce induction heads of copying patterns and provide evidence for their relationship with ICL. Differently, our work investigates the LLM as a whole (Anderson, 1972), studies sophisticated patterns, views ICL as a combination of reinforcements, and scrutinizes both the benefits and drawbacks of reinforcements. Analyzing In-Context Learning. Several studies have analyzed ICL properties. Min et al. (2022); Yoo et al. (2022) identify and discuss key factors that influence ICL capability such as input-label mappings. Wei et al. (2023) proposes that learning input-label mapping is an emergent ability. Factors such as structural similarity, diversity, simplicity (An et al., 2023) , and order or embedding distribution (Lu et al., 2022; Liu et al., 2022) are also investigated. Pan et al. (2023) partitions ICL ability into task recognition and task learning, observing different phenomena with varying model sizes. Lastly, Si et al. (2023) unveils the presence of inductive biases in ICL by designing underspecified demonstrations. Our findings corroborate with multiple previous analyses of in-context learning. For example, the scaling for distant reinforcement echoes Wei et al. (2023); Pan et al. (2023)\u2019s findings of different phenomena when varying model sizes. The importance of demonstration ordering and diversity in An et al. (2023); Lu et al. (2022) can be explained by avoiding spurious connections. Repetitive Generation and Self-Reinforcement Effect. Repetition is a notorious issue in neural text generation, affecting tasks like open-ended and directed generation (Holtzman et al., 2019; Welleck et al., 2019; Lin et al., 2021; See et al., 2017; Liu & Lapata, 2019). Maximization-based decoding strategies lead to bland, consecutive repetitions at word, phrase, and sentence levels (Holtzman et al., 2019; Welleck et al., 2019; Li et al., 2016; Karpathy & Fei-Fei, 2015; Guan et al., 2021). Despite advancements in large-scale pre-training with Transformer architecture (Vaswani et al., 2017; Radford et al., 2019; Lewis et al., 2020), unexpected sentence-level repetitions persist (Radford et al., 2019; Brown et al., 2020; Fu et al., 2020). The repetition issue is puzzling given the lack of repetitive sentences in the training data. A series of studies investigate the cause, with both from the theoretical perspective (Fu et al., 2020) and empirical findings (Holtzman et al., 2019). Recently, Xu et al. (2022) proposes the self-reinforcement effect, suggesting a repetitive loop when combined with maximization decoding. Our study extends the effect to large language models and token reinforcement, explains the reasons, and bridges the excellent ability of in-context learning to this notorious issue.\n# 6 CONCLUSION\nWe have taken a novel feature-centric approach to understanding in-context learning, by exploring its relationship with repetitive generation. We have identified a key mechanism, the token reinforcement loop, where any two tokens can form a strong connection through multiple co-occurrences. We delve into the reasons and inner-working of token reinforcement, demonstrating it to be an inevitable result of maximizing likelihood. Based on our findings, we view in-context learning as a combination of token reinforcements with different level of strength instead of input-label mapping. Furthermore,\nwe conduct experiments to demonstrate that token reinforcement plays a crucial role in shaping the output space and following patterns in in-context learning. We also illustrate through various studies how token reinforcement leads to spurious connections in in-context learning, highlighting the role of in-context learning as a double-edged sword, where informed demonstrations can maximize ICL effect.\nThis publication has emanated from research conducted with the financial support of both the Pioneer and \u201cLeading Goose\" R&D Program of Zhejiang under Grant Number 2022SDXHDX0003 and the National Natural Science Foundation of China Key Program under Grant Number 6233000066.\nREFERENCES\n# REFERENCES\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, 2023. Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, and Dongmei Zhang. How do in-context examples affect compositional generalization?, 2023. Philip W Anderson. More is different: Broken symmetry and the nature of the hierarchical structure of science. Science, 177(4047):393\u2013396, 1972. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent incontext learning in transformers, 2022. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers, 2023. Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition problem in text generation. arXiv preprint arXiv:2012.14660, 2020. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes, 2023. Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, and Minlie Huang. Long text generation by modeling sentence-level and discourse-level coherence. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6379\u20136393, 2021. Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. hiyouga. Llama factory. https://github.com/hiyouga/LLaMA-Factory, 2023.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, 2023. Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, and Dongmei Zhang. How do in-context examples affect compositional generalization?, 2023. Philip W Anderson. More is different: Broken symmetry and the nature of the hierarchical structure of science. Science, 177(4047):393\u2013396, 1972. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent incontext learning in transformers, 2022. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers, 2023. Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition problem in text generation. arXiv preprint arXiv:2012.14660, 2020. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes, 2023. Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, and Minlie Huang. Long text generation by modeling sentence-level and discourse-level coherence. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6379\u20136393, 2021. Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. hiyouga. Llama factory. https://github.com/hiyouga/LLaMA-Factory, 2023.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2019. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128\u20133137, 2015. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, 2020. Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562, 2016. Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023. Xiang Lin, Simeng Han, and Shafiq Joty. Straight to the gradient: Learning to use novel tokens for neural text generation. In International Conference on Machine Learning, pp. 6642\u20136653. PMLR, 2021. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022. deelio-1.10. Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3730\u20133740, 2019. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long. 556. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. OpenAI. Gpt-4 technical report, 2023. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \"learns\" in-context: Disentangling task recognition and task learning, 2023. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2023. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In International Conference on Learning Representations, 2019. Patrick H Winston. Learning and reasoning by analogy. Communications of the ACM, 23(12): 689\u2013703, 1980. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI. Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation, 2022. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2023. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In International Conference on Learning Representations, 2019. Patrick H Winston. Learning and reasoning by analogy. Communications of the ACM, 23(12): 689\u2013703, 1980. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI. Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation, 2022. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations, 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models\u2019 selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.\n# Appendix\nA Limitations\nB Experimental Details 14 B.1 Dataset Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.2 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n# C Further Investigation of Reasons and Inner-workings for Token Co-occurrence Reinforcement 17\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/13d3/13d35292-e92a-461a-a045-2c64fccb64fc.png\" style=\"width: 50%;\"></div>\nD Supporting Experiments of Token Co-occurrence Reinforcement D.1 Phrase Co-occurrence Reinforcement . . . . . . . . . . . . . . . . . . . . . . . D.2 Token Reinforcement of OPT Models . . . . . . . . . . . . . . . . . . . . . . . D.3 Token Reinforcement of Other LLMs . . . . . . . . . . . . . . . . . . . . . . . D.4 Token Reinforcement on Other Datasets . . . . . . . . . . . . . . . . . . . . . . D.5 Semantic Relationships of Token Reinforcement . . . . . . . . . . . . . . . . . D.6 Improved Ratio of Token Reinforcement . . . . . . . . . . . . . . . . . . . . .\nE Other Experiments on Detrimental Effects 21 E.1 Generation of Cot Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Selection Bias of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n# A LIMITATIONS\nWhile this study provides some insightful findings in the field of in-context learning, there are some limitations that should be noted. First, the experiments in this work constrain themselves to repeating surface patterns. More sophisticated patterns are not discussed. Second, our work mainly focuses on revealing the token co-occurrence reinforcement and understanding its influence on in-context learning. Its detrimental influences on in-context learning suggest that resolving the spurious connections would be helpful to either chain-of-thought or in-context learning.\n# B EXPERIMENTAL DETAILS\n# B.1 DATASET DESCRIPTIONS\nIn this paper, we mainly use the following five datasets, and we introduce each of them and describe our preprocess of these datasets individually. We present cases from each dataset to demonstrate their characteristics in Table 2.\nWikitext-103 The Wikitext-103 dataset, introduced by Merity et al. (2016)4, is a language modeling dataset that contains a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. We use a randomly sampled collection of 1000 sentences provided by Xu et al. (2022)5. The provided version is pre-tokenized to words and we use the moses detokenizer6 to restore the untokenized version for compatibility with the tokenizer of transformers.\n4https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-langu 5https://github.com/Jxu-Thu/DITTO/blob/main/data/wiki_sentences.txt 6https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ tokenizer/detokenizer.perl\n14 14 16\nDataset\nNumber of Cases\nExamples\nWikitext-103\n1000\nThe Bintulu Government Secondary School was built in\n1964.\nBookCorpus\n1000\n\u201cA little, \u201dhe admitted.\nRandom\n1000\nGreen Ou incarcer hijab ura na Edmonton regardless iken\nMayor\nGSM8K\n1000\nQuestion: Janet\u2019s ducks lay 16 eggs per day. She eats\nthree for breakfast every morning and bakes muffins for\nher friends every day with four. She sells the remainder at\nthe farmers\u2019 market daily for $2 per fresh duck egg. How\nmuch in dollars does she make every day at the farmers\u2019\nmarket?\nLet\u2019s think step by step.\nJanet sells 16 - 3 - 4 = \u00ab16-3-4=9\u00bb9 duck eggs a day. She\nmakes 9 * 2 = $\u00ab9*2=18\u00bb18 every day at the farmer\u2019s\nmarket. ### 18\nMMLU\n1140\nAs more lamps are connected in parallel in a circuit, the\ncurrent in the power source\nA. increases\nB. decreases\nC. remains the same\nD. Not enough information to say\nAnswer: A\n<div style=\"text-align: center;\">Table 3: Semantically equivalent substitutes.</div>\nCategory\nOriginal\nSubstitutes\nOption Names\nA.; B.; C.; D.\nI.; II.; III.; IV.\nE.; F.; G.; H.\n1.; 2.; 3.; 4.\n(a).; (b).; (c).; (d).\nAnswer Indicator\nAnswer:\nSolution\nReply\nResponse\nResult\nChoice\nBookCorpus BookCorpus (Zhu et al., 2015) is originally introduced to align the books to their movie releases in order to provide rich descriptive explanations for visual content. It contains 74M sentences from various sources of books, and we randomly sample 1000 sentences and also detokenize with moses.\nRandom Generated Sentences Here, we follow Xu et al. (2022) and construct 1000 randomly generated sentences. For each sentence, we first sample a random length from 5 tokens to 10 tokens, and then sample the tokens from the whole vocabulary. All random sentences are generated by uniformly sampled from the vocabulary. All the symbols are equally likely to appear, except the special tokens.\nGSM8K GSM8K (Cobbe et al., 2021) is a dataset of high-quality grade school math problems created by human problem writers. The dataset contains 7500 training problems and 1000 test problems. All the problems are answered with between 2 and 8 steps to solve. It is a frequently used benchmark to evaluate the reasoning ability of large language models (Touvron et al., 2023a; Chowdhery et al., 2022). To analyze the chain-of-thought (Wei et al., 2022) effect of LLMs, we add \u201cLet\u2019s think step by step\" followed by the question and right before the answer.\nMMLU The MMLU (Hendrycks et al., 2021) dataset is another commonly used benchmark to evaluate the knowledge of large language models. It covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem-solving ability. We randomly sample 20 test cases from each task, resulting in 1140 test queries in total. In the \u2018undesired\u2019 section, we uniformly redistribute the answers and options to isolate the selection bias of LLMs.\n# B.2 EXPERIMENTAL DETAILS\nRandom Substitutes Throughout the paper, we adopt random substitutes to isolate the effects of tokens, specific formats, and other components. The random substitutions are conducted in the following manner. To avoid the effect of different sequence lengths, we first tokenize the original sentence or demonstration using the Transformers tokenizer. Then, we replace the part to substitute with random tokens from the corresponding vocabulary. With the substitutes, we exclude the special tokens to ensure that all random tokens are valid.\nSelf-Reinforcement For each experiment, we report the averaged results across three runs with different seeds. We mainly conduct our experiments over two model families, LLaMA and OPT. For LLaMA models, we use 4 models sized in [7b, 13b, 30b, 65b]. For OPT models, we use 7 models with sizes ranging in [125m, 350m, 1.3b, 2.7b, 6.7b, 13b, and 30b]. Each sentence is repeated 20 times in our experiments. Following Xu et al. (2022), we randomly concatenate a prefix before all repeated sentences.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dad5/dad51b06-b818-4a0e-bed2-16445ecdb477.png\" style=\"width: 50%;\"></div>\nIn-Context Learning In the MMLU experiments, we replace option names and answer indicators to study the importance of token reinforcement in directing output space. Specifically, for each replacement, we choose semantically equivalent substitutes from a pool and randomly replace the original option name/answer indicator with the chosen substitute. In this way, we break the token reinforcement from the demonstrations. We put the pool of our replacement in Table 3.\nSignificance test We conduct the significance test using paired t-tests, where we randomly split the test set into 5 folds and compute the significance over accuracies on these 5 subsets. In Table 1, we compute the significance levels for [A, B, C] and D separately.\n# C FURTHER INVESTIGATION OF REASONS AND INNER-WORKINGS FOR TOKEN CO-OCCURRENCE REINFORCEMENT\nIn Section 3.2, we investigate the reason of token reinforcement as it is inherently embedded within the pretraining corpus. Here, we further investigate token reinforcement with two more experiments, from the learning process and attention mechanism, respectively. Leveraging repetitive features helps maximizing likelihood. Our next experiment shows that the utlization of repetitive features is a natural choice of LLMs as it helps in the pre-training stage. To illustrate this, we focus on a conditional language modeling task that predicts tokens based on a given prompt. Formally, we model the probability P(wTp:T |w<Tp), where T and Tp are the total context length and the number of tokens to condition on, respectively. We then consider three types of prompts. The first one serves as our baseline prompt with w<Tp. In the second type, we mask the tokens in the prompt that appear in wTp:T . For the third type, we mask the same quantity of tokens as in the second type, but we select the tokens randomly. For each type of prompts, we pretrain a language model. The base architecture of our model is the same as LLaMA. Due to limitation of computational cost, we make several changes in hyperparameters for a smaller size. We set the hidden size to 1024 and the FFN size to 4096, and we incorporate 12 layers. The tokenizer is the same as LLaMA and the vocabulary size is 32000. This configuration results in a model with 267M trainable parameters. The dataset we use is the wikipedia-english-2022, and the code base we use is LLaMA-Factory (hiyouga, 2023). We pretrain each model for 50k steps, with a total batch size of 160 sentences per step. Each sentence contains 1024 tokens. Prompt length Tp is 0.75 * 1024 = 768 tokens. We run our experiments on 4 NVIDIA A100 GPUs, and each experiment takes about 30 hours to finish. Our results are shown in Figure 8. Masking repetitive features lead to a worse converged loss compared with masking random tokens and no masking. It indicates that repetitive feature is favorable in optimizing training loss, and thus making it natural for language models to use repetitive features.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bf54/bf54d671-3a75-41e5-8cf3-31e4ad393d64.png\" style=\"width: 50%;\"></div>\nFigure 9: Left: Attention example of successive token reinforcement. \u2018_fif\u2019 and \u2018ute\u2019 are the two kept tokens randomly choosed. Right: Ablation study of attending preceding token. The results are based on LLaMA-65B.\n<div style=\"text-align: center;\">Figure 9: Left: Attention example of successive token reinforcement. \u2018_fif\u2019 and \u2018ute\u2019 are the two kept tokens randomly choosed. Right: Ablation study of attending preceding token. The results are based on LLaMA-65B.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/908e/908eef7a-354d-463b-8dd8-afdcec669fc6.png\" style=\"width: 50%;\"></div>\nFigure 10: Phrase co-occurrence reinforcement. When disrupting the reinforcement loop with random prefixs, the self-reinforcement effect still retains. (A)-(C) plot the scaling of LLaMA models with a certain phrase length (e.g., \u201cXY\u201d denotes length of 2). (D) plots LLaMA-65B\u2019s probability varying phrase length. The gray shadow denotes the standard deviation across 1,000 samples.\nAttending preceding token is responsible for reinforcement. Another important question is what inner-working of LLMs is responsible for token reinforcement? We start our investigation with a visualization of attention weights. An example is show n in the left of Figure 9. In this example, each token wi attends its preceding tokens wi\u22121 with a relatively high attention weights. Thus, our intuitive is that attending preceding token is the key for reinforcing the connection. To validate this hypothesis, we reproduce the successive reinforcement experiment in Section 3.1, with an attention mask preventing each token by attending its preceding token. Our results are shown in the right of Figure 9. We observe that upon masking the adjacent token, the strength of reinforcement is reduced by a factor of ten. Hence, the reinforcement is not simply realized by attending to similar tokens. Rather, the information is propagated through a process wherein each token iteratively attends to its adjacent token.\n# D SUPPORTING EXPERIMENTS OF TOKEN CO-OCCURRENCE REINFORCEMENT\nIn this section, we provide more experimental evidence related to the token co-occurrence reinforcement. The experimental settings in this section follow the same setting as in Section 3.1, except for models to use, datasets to use, and the choices of tokens.\n# D.1 PHRASE CO-OCCURRENCE REINFORCEMENT\n# We first construct consecutive tokens to create phrase reinforcement in this section. Specifically, we Without loss of generality, we place the phrase at the end of each demonstration.\nWe construct a binary mask sequence m = ( Ls\u2212Lp \ufffd\ufffd\ufffd\ufffd 0, \u00b7 \u00b7 \u00b7 , 0, Lp \ufffd\ufffd\ufffd\ufffd 1, \u00b7 \u00b7 \u00b7 , 1), where Lp is the length of the kept phrase. Then, we can define a randomly perturbed sentence \u02dcs =\nWe construct a binary mask sequence m = ( \ufffd\ufffd\ufffd\ufffd 0, \u00b7 \u00b7 \u00b7 , 0, \ufffd\ufffd\ufffd\ufffd 1, \u00b7 \u00b7 \u00b7 , 1), where Lp is the length of the kept phrase. Then, we can define a randomly perturbed sentence \u02dcs =\n(R(w1, m1), \u00b7 \u00b7 \u00b7 , R(wLs, mLs)). Effectively, we keep a phrase of length Lp unchanged for each sentence, and replace other tokens with random tokens. We compute PREP-P(w) = M(w|[\u02dcs1; \u02dcs2; \u00b7 \u00b7 \u00b7 ; \u02dcsn\u22121; R(w1, m1), \u00b7 \u00b7 \u00b7 , R(wi\u22121, mi\u22121)]) and the average token probability that only considers the kept tokens \u02dc TPN = 1 Lp \ufffdLs i=Ls\u2212Lp+1 PREP-P(w|w = wi, n = N) \u00d7 mi. As a concrete example, we take a sentence s = (Apple, is, red) and m = (0, 1, 1), and the target token w = red. Then, the demonstrations discussed will be like \u2018Apple is red // Orange is red // Banana is red\u2019. The pattern we kept unchanged is in color red. In the context of in-context learning, this pattern corresponds to demonstrations like \u2018...Answer: D\u2019 or \u2018...Response: Positive\u2019. Figure 10 depicts our experimental results for repeated and discontinuous phrases. We use randomly generated sentences here. We see that disrupting the sentence-level loop with random prefixes does not break the self-reinforcement effect. The effect persists across various LLaMA models. In addition, we observe a stronger reinforcement when increasing the phrase length. More tokens are kept unchanged, higher probability the model assigns to the phrase. Particularly, Figure 10(D) serves as an ablation study, as each line progressively adds one more unchanged token. A repeating phrase of \u2018...XYZ\u2019 gets reinforced more compared to the phrase \u2018...XY\u2019, indicating that each repetitive token bolsters its subsequent tokens.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/decb/decb3171-2a69-4c57-b36e-2cac19e54a79.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Token co-occurrence reinforcement of OPT models.</div>\n# D.2 TOKEN REINFORCEMENT OF OPT MODELS\nIn Figure 11, we plot the token reinforcement of all 7 OPT models. The results are consistent with our results of LLaMA in the main context, validating the generality of token reinforcement across different families of large language models.\n# D.3 TOKEN REINFORCEMENT OF OTHER LLMS\nWe present token reinforcement on LLaMA-2 (Touvron et al., 2023b)(7B, 13B, 70B), Mistral (Jiang et al., 2023)(7B), GPT-J (Wang & Komatsuzaki, 2021)(6B). Figure 12 demonstrates that all thes LLMs exhibit similar token reinforcement effect, with various strength. These results consolidate ou findings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de4e/de4edcea-c3c6-4ea5-9b85-795b92f3033e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Token reinforcement on Various LLMs.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9a5/e9a5fc63-257e-4636-b77a-bcecfd4bc23e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Token co-occurrence reinforcement on Wikitext-103.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c957/c957bb37-7d14-40ca-ae63-7f4372128dd5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Token co-occurrence reinforcement on BookCorpus.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff8b/ff8b7050-aac8-48ee-91d1-b3c75fce2d03.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: Token reinforcement against semantic relationships</div>\nHere, we plot the token reinforcement on Wikitext-103 and BookCorpus. As we can see, the probability of reinforced tokens is quite different in the two datasets. In BookCorpus, with 4 kept tokens, the probability can be boosted to about 0.8, whereas in Wikitext-103, the probability can only reach about 0.5. Note that compared to the results on randomly generated sentences, tokens in these two datasets are more likely to co-occur in the pre-training data. This indicates the semantic relationship between tokens affects how token reinforcement performs.\n# D.5 SEMANTIC RELATIONSHIPS OF TOKEN REINFORCEMENT\nWe further conduct experiments to investigate how the semantic relationship between two tokens affects the token co-occurrence reinforcement. We choose three relationships: (1) Random two tokens. (2) The same two tokens. (3) Two tokens that are similar in the embedding space. Figure 15 plots our results for different LLaMA models. We observe clear gaps among different semantic relationships. Two tokens that are the same can build a strong connection stronger than two tokens that are similar in the embedding space. Further investigating the reasons behind this is interesting and may unravel the internal biases of large language models. We leave it as the future work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb41/eb41a4b4-cb29-4866-8acf-137ac262bcda.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: Improved Ratio on three datasets.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0610/06102287-a187-4fa6-a021-672fc4c05321.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 17: Left: The probability of generating the CoT answer. Right: The probability  generating the first token in CoT answer.</div>\nD.6 IMPROVED RATIO OF TOKEN REINFORCEMENT\nIn this section, we solidify our findings with the ratio of improved token probability. Formally, the improved ratio (IR; Xu et al. (2022)) is defined as follows:\nOur metric of improved ratio is defined over the setting in Section 3.1. Figure 16 plots our results of IR on three datasets. We only consider the case of two tokens.\nOur metric of improved ratio is defined over the setting in Section 3.1. Figure 16 plots our results of IR on three datasets. We only consider the case of two tokens. As we can see, the improved ratios for all three datasets quickly reach almost 1.0 with only several repetitions, indicating token reinforcement exists in most of the cases. In addition, we observe that IRs in Wikitext-103 and BookCorpus have larger variances than those in randomly generated sentences, because tokens in Wikitext-103 and BookCorpus are more likely to co-occur in the same context and thus get larger probabilities without reinforcement.\nAs we can see, the improved ratios for all three datasets quickly reach almost 1.0 with only several repetitions, indicating token reinforcement exists in most of the cases. In addition, we observe that IRs in Wikitext-103 and BookCorpus have larger variances than those in randomly generated sentences, because tokens in Wikitext-103 and BookCorpus are more likely to co-occur in the same context and thus get larger probabilities without reinforcement.\n# E OTHER EXPERIMENTS ON DETRIMENTAL EFFECTS\n# E.1 GENERATION OF COT ANSWER\nIn this section, we study how the CoT answer is generated with respect to our discovered surface patterns. We study the problem on the GSM8K dataset. Recall the demonstrations in GMS8K, i.e., \u201cQuestion: [Question] // Let\u2019s think step by step. // [CoT Answer]\u201d. We plot the probability of [CoT Answer] in Figure 17. We would expect the [Question] to have a huge effect on generatin the CoT answer. Thus, we involve the probability with random substitution of the [Question] part. Interestingly, we find that even without the [Question] part, the probability of [CoT Answer] still increases with demonstrations, suggesting the patterns play a crucial role in learning how to generate the CoT answers. So what does the [Question] part do in in-context learning of GSM8K? We further plot the probability of the first token of [CoT Answer]. The probability of the first token significantly decreases when we mask out [Question]. Hence, even though the [Question] part does not affect much the probability of [CoT Answer], it substantiates the generation of [CoT Answer] at the very beginning.\n(2)\n# E.2 SELECTION BIAS OF LLMS\nIn experiments illustrated in table 1, we observe a selection bias of LLaMA-65B. We show the results for each class in Table 4. Note that we randomly permute the test set to make sure all questions are balanced with [A,B,C,D]. First, we see that the zero-shot performances in different class are quite different. Class \u2018A\u2019 gets an accuracy as high as 71.58% and class \u2018D\u2019 only gets 52.28%. Second, with more demonstrations, the overall accuracy is improved, from 60.61% to 63.16%, demonstrating the effectiveness of ICL. However, the accuracy of class \u2018A\u2019 largely decreases, while the accuracies for \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019 all increase. The above findings indicate that LLaMA-65B has a selection bias in both zero-shot and few-shot scenarios.\n<div style=\"text-align: center;\">Table 4: Accuracies for each class on our balanced MMLU. We use the LLaMA-65B and vary th number of demonstrations from 0 to 5.</div>\nClass\n0\n1\n2\n3\n4\n5\nA\n71.58\n57.31\n58.25\n57.31\n56.96\n54.74\nB\n59.65\n66.90\n69.94\n70.41\n69.59\n70.76\nC\n58.95\n67.02\n65.50\n64.80\n66.78\n67.60\nD\n52.28\n59.65\n60.00\n59.30\n59.88\n59.53\nAvg.\n60.61\n62.72\n63.42\n62.95\n63.30\n63.16\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). It highlights the importance of understanding in-context learning, which allows LLMs to adapt to various tasks without explicit training.",
        "problem": {
            "definition": "The working mechanism of in-context learning in LLMs remains an open question, particularly how textual features influence model behavior.",
            "key obstacle": "The main challenge is the fragility of in-context learning, which can lead to incorrect predictions due to reliance on repetitive patterns."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that LLMs learn from surface features and repetitions in their context.",
            "opinion": "The central idea posits that surface repetitions significantly impact in-context learning, reinforcing connections between tokens.",
            "innovation": "The primary improvement over previous methods is the introduction of the token co-occurrence reinforcement principle, which quantitatively analyzes the effects of surface features."
        },
        "Theory": {
            "perspective": "The theory perspective views in-context learning through the lens of surface patterns and their reinforcement effects.",
            "opinion": "The authors assume that the connection between tokens is strengthened through their contextual co-occurrences.",
            "proof": "The paper provides empirical evidence for token reinforcement and its role in guiding text generation through experiments."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using datasets such as MMLU and GSM8K, with baseline comparisons to understand the effects of surface patterns.",
            "evaluation method": "Evaluation involved analyzing the impact of self-reinforced patterns on model predictions, including controlled experiments with varying demonstration structures."
        },
        "conclusion": "The findings conclude that token reinforcement plays a crucial role in shaping the output space of in-context learning, revealing both beneficial and detrimental effects.",
        "discussion": {
            "advantage": "The paper highlights advantages such as constraining the output space and enhancing pattern-following capabilities in in-context learning.",
            "limitation": "A limitation is the potential for spurious connections arising from token reinforcement, which can mislead model predictions.",
            "future work": "Future improvements could focus on mitigating the risks of spurious connections and exploring more complex patterns in in-context learning."
        },
        "other info": [
            {
                "info1": "The paper emphasizes the need for balanced demonstrations to optimize in-context learning effectiveness."
            },
            {
                "info2": {
                    "info2.1": "The research was supported by grants from Zhejiang and the National Natural Science Foundation of China.",
                    "info2.2": "Further experimental details are provided in the appendix, including dataset descriptions and experimental methodologies."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs), highlighting the importance of understanding in-context learning, which allows LLMs to adapt to various tasks without explicit training."
        },
        {
            "section number": "1.3",
            "key information": "The central idea posits that surface repetitions significantly impact in-context learning, reinforcing connections between tokens, which is facilitated by LLMs."
        },
        {
            "section number": "3.2",
            "key information": "The theory perspective views in-context learning through the lens of surface patterns and their reinforcement effects, assuming that the connection between tokens is strengthened through their contextual co-occurrences."
        },
        {
            "section number": "3.4",
            "key information": "The findings conclude that token reinforcement plays a crucial role in shaping the output space of in-context learning, revealing both beneficial and detrimental effects."
        },
        {
            "section number": "4.1",
            "key information": "The paper emphasizes the need for balanced demonstrations to optimize in-context learning effectiveness, which can be influenced by effective prompt design."
        },
        {
            "section number": "6",
            "key information": "A limitation discussed in the paper is the potential for spurious connections arising from token reinforcement, which can mislead model predictions."
        }
    ],
    "similarity_score": 0.7468449148945175,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Understanding In-Context Learning from Repetitions.json"
}