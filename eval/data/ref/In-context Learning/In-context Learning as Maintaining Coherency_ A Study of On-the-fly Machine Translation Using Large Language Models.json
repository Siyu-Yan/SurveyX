{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.03573",
    "title": "In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models",
    "abstract": "The phenomena of in-context learning has typically been thought of as \"learning from examples\". In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain setting, which uses prompt examples from a moving window. We study this with respect to other factors that have previously been identified in the literature such as length, surface similarity and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B), and three translation directions (\\texttt{en}$\\rightarrow$\\{\\texttt{pt, de, fr}\\}) suggest that the long-term coherency of the prompts and the test sentence is a good indicator of downstream translation performance. In doing so, we demonstrate the efficacy of In-context Machine Translation for on-the-fly adaptation.",
    "bib_name": "sia2023incontextlearningmaintainingcoherency",
    "md_text": "# In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models\nSuzanna Sia Johns Hopkins University ssia1@jhu.edu Kevin Duh Johns Hopkins University kevinduh@cs.jhu.edu\nAbstract\nThe phenomena of in-context learning has typically been thought of as \"learning from examples\". In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain setting, which uses prompt examples from a moving window. We study this with respect to other factors that have previously been identified in the literature such as length, surface similarity and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B), and three translation directions (en\u2192{pt, de, fr}) suggest that the long-term coherency of the prompts and the test sentence is a good indicator of downstream translation performance. In doing so, we demonstrate the efficacy of In-context Machine Translation for on-the-fly adaptation.\narXiv:2305.03573v1\n# 1 Introduction\nIn-context Machine Translation is a relatively new paradigm that uses large autoregressive Language Models to carry out the task of Machine Translation (MT) by being shown translation pairs in the prefix. From a practitioner\u2019s viewpoint, In-context learning presents itself as an attractive approach for rapidly adapting a Translation model on-the-fly. Previous strategies for adapting a pre-trained MT model still require additional engineering or training of the model, e.g fine-tuning with in-domain data using adaptor layers (Philip et al., 2020). Instead, simply changing the inputs to the model might be an effective way to adapt on-the-fly without any model modification. The in-context learning paradigm describes a phenomena where large autoregressive language\nKevin Duh Johns Hopkins University kevinduh@cs.jhu.edu\nmodels perform a task when shown examples (known as prompts) in the prefix (Brown et al., 2020; Bommasani et al., 2021). Previous work approaches the role of the prompt context as allowing the model to \"learn by examples\". This intuitive approach to formulating the task of prompt selection has led to the suggestion of selecting examples that are similar to the source sentence being translated. Semantic similarity based on sentence embeddings (Liu et al., 2021) and BM25 have been proposed to select examples to present as \u201cdemonstrations\" (Rubin et al., 2021). This approach was further expanded by Agrawal et al. (2022) who show that BM25 and a heuristic version optimizing for word coverage, is effective for selecting examples. We focus on Machine Translation as a complex conditional generation task and offer an alternate perspective: the in-context paradigm depends on maintaining coherency. Coherence is an aspect of natural language that reflects the overall semantic and syntatic consistency in a body of text (Flowerdew and Mahlberg, 2009). We investigate this by first exploring the model\u2019s behavior when showing matching and mismatching domains in the context and the test sentence. Next we consider a stricter notion of coherency using a moving window of previous gold translations directly preceding the test source sentence to be next translated. Our experiments compare the coherence factor with similarity based factors for prompt selection, additionally controlling for length (Xie et al., 2021) which is typically overlooked but is important to consider for performance and available labeling (translation) budget. The contributions of this work are \u2022 We identify coherency of prompt examples with respect to test sentence as a critical factor for translation performance. Experiments across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B) and 4 domains (Medical, Social Media, Wikipedia, and TED Talks) suggest that mod-\nTranslate English to French.\nEnglish: A discomfort which lasts ..\nFrench:\nUn malaise qui dure\nEnglish: HTML is a language for formatting\nFrench:\nHTML est un langage de formatage\n...\n...\nEnglish: After you become comfortable with formatting ..\nFrench:\nels perform better when prompts are randomly drawn from the same domain.\n Within the TED talks domain, we investigate local coherence using document-level translation experiments, by adopting a moving window directly preceding the test source sentence to be translated. Overall, our results across the 3 models and three translation directions (en\u2192{pt, de, fr}) suggest that the coherency of the prompts with regard to the test sentence is a good indicator of translation performance.\n# 2 Preliminaries\n# 2.1 In-context Machine Translation\nIn an in-context learning setup, several formatting decisions need to be made on how to present the prompt examples to the model. We adopt the following commonly used prompt format where the instructions are straightforwardly provided as in the following (Table 1).1 In this work, we consider both sentence level translation (Section 5.1) and an on-the-fly document-level setting (Section 5.3).\n# 2.2 Coherence in Natural Language Text\nThe computational linguistics literature holds many competing definitions of coherence in text (Wang and Guo, 2014). We consider two aspects of coherence, first from a more global level where we investigate domain effects, and also from a local sentence level, where we consider a coherent context as a moving window of previous (gold) translations which directly precede a test sentence. A similar working definition of coherence has been used in discrimination tasks that require a model to identifying the right order of (shuffled) sentences (Elsner et al., 2007; Barzilay and Lapata, 2008; Laban et al., 2021).\n# 3 Factors which affect In-context MT\nWe outline several factors studied in this paper related to example selection for In-context MT. While we emphasise the notion of Coherence (Section 2.2), by studying the domain factor (Section 3.4) and local coherence (Section 3.5), our experiments seek to compare this against other factors that have been highlighted in previous literature. Namely, length (Section 3.1), surface similarity (Section 3.2) and semantic similarity (Section 3.3). To demonstrate, in Table 1, the first sentence is semantically similar and the second sentence has surface similarity with the test sentence.\n# 3.1 Length (Translation Budget)\nOne previously overlooked factor for in-context MT is the length (number of words) in the prompt examples. The perspective of In-context Learning as implicit Bayesian Inference argues that longer examples provide more evidence to the model on the desired task pattern (Xie et al., 2021). Longer examples are also more likely to contain nontrivial translation exemplars, although it is not clear whether this affects downstream performance. We find example length to be correlated with the domain (Figure 2), and it may thus be a confounding factor for in-context MT.\nControlling for Length We adopt the notion of a \u201cTranslation Budget\" which is the total word count of all the prompt examples provided (excluding the test sentence). Examples can be selected as long as they satisfy the budget constraint. A generalized algorithm is provided in Section 4.3. From a resource perspective, this reflects the work of the human annotator in providing example translations.\n# 3.2 Surface Similarity\n# 3.2 Surface Similarity 3.2.1 BM25\nBM25 (Robertson et al., 2009) is a bag-of-words unsupervised retrieval function that ranks a set of documents based on the query terms appearing in the documents. Agrawal et al. (2022) report that using BM25 to retrieve similar prompt examples\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f72/9f7258c9-3f9e-4005-bf79-efbc0cde2183.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Factors identified and studied in this paper. Selecting from matching Domain increases coherence (Appendix C) and each domain has different length distributions (Section 5.2). Surface similarity and embedding similarity are associated (Table 4) Surface similarity selection also results in longer sentences (Section 5.4) Rectangle boxes next to the node are measures of these factors. We describe and quantify positive and negative interference of the model for translation performance in Section 6.3.</div>\noutperforms random selection. They also advocate for a variant of BM25 with increased coverage of test sentence source words although with marginal gains (<1 BLEU point) increase. Following Agrawal et al. (2022), we order the examples according to their similarity to the source, with the most similar examples on the left in all our experiments.\n# 3.2.2 Maximising Surface Similarity Coverage\nTo maximise word overlap across all prompts and the source sentence, we adopt Submodular optimisation by Maximal Marginal Relevance (Carbonell and Goldstein, 1998; Lin and Bilmes, 2010). Formally we are given a finite size set of objects U (the size of the prompt bank). A valuation function f : 2U \u2192R+ returns a non-negative real value for any subset X \u2282U. The function f is said to be submodular if it satisfies the property of \u201cdiminishing returns\", namely, for all X \u2282Z and Z /\u2208U, we have f(X \u222au) \u2212f(X) \u2265f(Z \u222au) \u2212f(Z). The algorithm optimises for sentences with maximal word overlap weighted by the BM25 score.\n# 3.3 Semantic Similarity (Nearest Neighbors)\nThe semantic similarity of prompts based on their sentence embeddings has also been advocated for selecting good in-context examples. Liu et al. (2021) apply a pre-trained Roberta-large sentence encoder to the test sentence, and query for its nearest neighbors to use as in-context demonstrations. In our experiments we apply a similar strategy using MPNet base (Song et al., 2020) which achieved highest scores on HuggingFace sentence embed-\nding and semantic search benchmarks.2 We do not consider training a prompt retriever (Rubin et al., 2021) or fine-tuning the sentence encoder (Liu et al., 2021) in this study, as these are no longer \"light-weight\" retrieval methods that are comparable with the other unsupervised strategies.\n# 3.4 Domain Coherence\nGPT is able to do style transfer just from instructions or from being shown surface prompt examples (Reif et al., 2022). Simply providing demonstrations from the same domain may induce the large language model (LLM) to generate a similar style which is coherent with the target text. Another possibility is that particular lexical translation exemplars which match the source sentence may be present. However, due to the very high dimensionality of the raw vocabulary, this is less likely if translation examples are randomly sampled. Domain may also present spurious correlations which are confounded by the training data of LLMs. For instance, there may be certain domains which are better at eliciting Translation behavior from the model, regardless of what the test domain is.\n# 3.5 Local Coherence (Moving Window)\nWe hypothesise that the local coherence (Section 2.2) of the context to the test sentence to be translated may be an important factor for performance. To test this, we adopt a moving context window of the previously translated gold sentence pairs as the prompt examples. To our knowledge, Section 3.4 and Section 3.5 are previously unexplored for In-context Machine Translation.\n# 4 Experiments\n# 4.1 Data\nDomain Coherence We organise our experiments investigating four en\u2192fr domains, WMT19 Biomedical (MED) (Bawden et al., 2019), a social media dataset, MTNT (Michel and Neubig, 2018), multilingual TED Talks, and Wikipediabased FLORES (Goyal et al., 2021). Except for MED, all other datasets have a wide range of topics in the train (prompt bank) and test set which are shuffled in random sampling, and thus the domain experiments are more focused on the writing style of the text. We use standard train-test splits, with the trainset being used as the prompt bank. Scores are reported using SacreBLEU (Post, 2018).3\nLocal Coherence (document level) We use the Multitarget TED Talks dataset from Duh (2018). The original dataset has 30 documents in the test set, where each document corresponds to a 10-20 minute TED talk. To increase the size of the test set, we partition the \"original\" trainset into a train (prompt bank) and test split, where talks with a minimum of 100 lines were used as the test and talks with less than 100 lines were used as the \"out-of-document\" prompt bank. We used 120 test documents that had a minimum of 100 lines, and we evaluated each up to 120 lines, where each TED talk is a document. The document level BLEU scores are reported for three language directions en\u2192{fr, pt, de}. We do not use a dev set as there is no training or any tuning of any hyperparameters. Since this is a non-standardised data split, we provide the numbers in the following table.\nTalks\n(Docs)\nLines\nper\ndoc\nTotal\nLines\n\"Outside-doc\"\nPrompt Bank\n450\n<100\n26000+\n\"Within-doc\"\nPrompt Bank\n1\n100-120\n120\nTest\n120\n100-120\n12000+\n# 4.2 Models\nWe use three models, GPTNeo2.7B (Black et al., 2021), XGLM2.9B (Lin et al., 2021), and Bloom3B (Scao et al., 2022) which are open access LLMs\navailable on HuggingFace (Wolf et al., 2020). The later two have been advertised as \"Multilingual Language Models\". GPTNeo2.7B is a GPT3 replicate pretrained on The Pile (Gao et al., 2020), while XGLM adopts a similar architecture trained on a multilingual corpus (CC100-XL). Bloom3B has been trained on the ROOTS Corpus (Lauren\u00e7on et al., 2022), a collection of huggingface datasets of 1.6 TB of text. To our knowledge, there has not been any reports of sentence level parallel corpora in the training datasets of these models.\n# 4.3 Algorithm for Greedy selection with Length Constraint\nIn our experiments, we investigate BM25 (Section 3.2.1), BM25 with submodular optimisation (BM25-s; Section 3.2.2), and semantic similarity (nn; Section 3.3). To control for length effects, we employ an algorithm for selection with length constraints (algorithm 1) which closely follows greedy submodular algorithms (Krause and Guestrin, 2008). Retrieval methods adopts a utility function: f, which is used to retrieve highest scoring sentences. For BM25 and BM25-s, fis BM25, while ui is selected by f({u}), and f({u}|Xi) respectively. While for nn, f is the L2 embedding similarity between prompt sentence and test query.\nAlgorithm 1: Generalised greedy (submod-\nular) algorithm with length budget\n1 Input: (Submodular) function\nf : 2U \u2192R+, cost function m, budget b,\nfinite prompt bank U\n2 Output: Xk where k is the number of\niterations/prompts.\n3 Set X0 \u2190; i \u21900;\n4 while m(Xi) < b do\n5\nui = argmaxu\u2208U\\Xif({u} | Xi)\n6\nXi+1 \u2190Xi \u222aui;\n7\ni \u2190i + 1\n# 5 Analysis of Factors 5.1 Domain Coherence [Table 2]\nDoes coherence of domain allow models to adapt on the fly? If models are adapting to the domain shown in the context, sampling and testing within the same domain should result in the highest translation performance, as compared to being shown examples out of domain. For example, if we are\nGPTNeo2.7B\nBloom3B\nXGLM2.9B\nPrompt / Test\nFLORES\nMED\nMTNT\nTED\nFLORES\nMED\nMTNT\nTED\nFLORES\nMED\nMTNT\nTED\nFLORES\n24.6\n19.7\n23.1\n24.6\n36.7\n28.5\n28.5\n31.1\n29.3\n20.9\n24.7\n25.7\nMED\n23.0\n19.2\n21.1\n23.2\n34.5\n28.7\n26.2\n29.5\n27.5\n21.4\n22.9\n24.4\nMTNT\n23.7\n18.6\n22.4\n23.7\n35.5\n27.7\n29.1\n30.6\n27.9\n21.2\n25.0\n25.4\nTED\n23.2\n18.6\n22.1\n23.6\n36.1\n27.9\n29.1\n31.2\n27.8\n21.1\n24.2\n24.8\nTable 2: Crosstable of BLEU scores from sampling and testing in different domains. We present the average BLEU scor across 5 randomly sampled prompt sets. The size of the prompt sets (number of translation pair examples) is 5. We bold th largest value column-wise.\ntesting on the TED domain, is it important that the prompt be also drawn from TED or is it sufficient to have sentence pairs from any domain illustrating the translation task? To account for prompt selection and ordering effects, all inference runs were repeated with 5 randomly sampled prompt sets from the training data. We focus on en \u2192fr which is common across datasets.\n# Results and Discussion\n Models are able to perform some form of domain adaptation on-the-fly. There appears to be evidence of domain adaptation in Bloom3B and XGLM, as sampling and testing within the same domain (e.g., sample from MED test with MED) mostly results in the highest performance column-wise. We also observe that matching domains result in lower conditional source sentence perplexity (Appendix C).\n For GPTNeo, sampling from FLORES results in the best translation performance across all test sentences even with domain mismatch. This suggests that translation performance in GPTNeo is best induced using FLORES and is less adaptive to the domain. Note that the second best column wise result for GPTNeo tends to occur when there is matching prompt and test domain.\n# 5.2 Domain controlling for Length\nHow does length of prompts affect translation across different domains? In Figure 2, we randomly sample 1000 sentences from each domain\u2019s training set. Randomly sampled sentences from different domains show distinct length effects. We study the impact of these length effects by selecting either a 5-10 word or 15-20 word long sentences for translation examples, and compare the differences in scores for the non-filtered scenario (Table 3).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/efb2/efb2d11c-9825-4548-bde2-d83f2be48194.png\" style=\"width: 50%;\"></div>\nFigure 2: Histograms of sentence lengths (word counts) randomly sampled from different domains, which has implications for the total prompt length when sampling from these domains. FLORES sentences tend to be nearly twice as long as MTNT and TED sentences.\n<div style=\"text-align: center;\">Figure 2: Histograms of sentence lengths (word counts) randomly sampled from different domains, which has implications for the total prompt length when sampling from these domains. FLORES sentences tend to be nearly twice as long as MTNT and TED sentences.</div>\nPrompt / Test\nFLORES\nMED\nMTNT\nTED\nFLORES\n-\n-\n-\n-\nMED\n\u21d322.4 (0.3)\n\u21d318.5 (0.3)\n\u219320.8 (0.8)\n\u21d322.5 (0.8)\nMTNT\n\u21d323.2 (0.4)\n\u219318.3 (0.5)\n\u21d321.9 (1.2)\n\u219323.5 (0.5)\nTED\n\u21d321.7 (1.4)\n\u21d317.6 (0.6)\n\u21d320.1 (1.8)\n\u21d322.3 (1.5)\n<div style=\"text-align: center;\">5-10 words long sentences; GPTNeo 2.7B</div>\nPrompt / Test\nFLORES\nMED\nMTNT\nTED\nFLORES\n24.2 (0.2)\u2193\n19.6 (0.3)\n22.7 (0.8)\u2193\n24.3 (0.5)\u2193\nMED\n22.9 (0.6)\n19.3 (0.1)\n21.1 (0.9)\n22.8 (0.7) \u2193\nMTNT\n24.0 (0.4) \u2191\n18.9 (0.6)\u2191\n22.5 (0.0)\n24.3 (0.3)\u21d1\nTED\n23.8 (0.4)\u21d1\n19.0 (0.4)\u2191\n22.9 (0.2)\u21d1\n23.8 (0.4)\nTable 3: Selecting for short source sentences (5-10 words) vs longer source sentences (15-20 words) as translation examples. \u2193and \u2191refers to differences > 0.3, and \u21d3and \u21d1refers to differences > 0.5 when compared to the no-length filter scenario in Table 2.\n\u2022 When source prompt sentences are 5-10 words, all BLEU scores decrease. For 15-20 words sentences which is \"long\" for MTNT and TED, but \"short\" for FLORES, the BLEU score of the former increases while the latter decreases. BLEU scores are similar for MED as 15-20 words is close to the mean of MED length distribution.\n\u2022 We inspect the length of generation under different prompt lengths, and find that average differences in generation length are marginal (only 1-2 words difference) indicating that poorer per-\nformance is not simply due to a difference in generation lengths.\n# 5.3 Local Coherence [Table 4]\nHow important is a coherent context (as compared to other prompt selection methods?) Section 5.1 showed that models are able to adapt when shown prompts from a matching domain. We hypothesise that coherence of the prompts with respect to the test source sentence (Section 2.2) is an important factor for performance. We use the TED talks dataset (data preparation described in Section 4.1), and consider a moving window of previous gold translations (window) as a coherent context for the model.4 We compare this against the baselines of (BM25; Section 3.2.1), (BM25-s; Section 3.2.2), and Nearest Neighbor retrieval of sentence embeddings (nn; Section 3.3) from a large prompt bank outside the document. We use a prompt set of 5 examples for all experiments, and randomly sample from outside of the document if the available window is smaller than 5. Document level BLEU scores are averaged across 120 documents and reported in Table 4.\nQuantifying Similarity We report the ROUGE1-precision (coverage; Lin (2004)) and the L2 Euclidean distance (L2) of the source sentences in the prompt set, with the test source sentence to be translated. If translation performance is due to word overlap or embedding similarity, then we expect that having a higher coverage or lower L2 would have better performance than window. Note that all similarity based retrieval methods depend only on the source sentences, and is model and target language independent. i.e., the single coverage and L2 value applies for all results columns in Table 4.\n# Results and Discussion\n The moving window (window) outperforms all other baselines across the 3 models and 3 language directions, with the exception of Bloom3B on en\u2192de direction. The gains are from 0.5 to 2.6 BLEU points from the next best performing retrieval method. Importantly, coverage and L2 shows that the performance is not due to similarity or word overlap.\n\u2022 Interestingly, randomly sampling sentences from within the document (talk) performs well compared to other similarity based retrieval methods from outside of the document. This further highlights that coherence is a critical factor for In-context Machine Translation.\n Similarity based retrieval mostly does better than randomly sampled prompt sets, which is consistent with existing literature which did not consider the factor of coherence. A notable exception is XGLM en\u2192fr results, where similarity based methods are doing poorly compared to that reported by (Agrawal et al., 2022). We find that the similarity based retrieval methods does better for XGLM when the number of prompts is increased from 5 to 15. The same trend is observed at 15 prompts, window continues to outperform the other methods (results in Appendix B).\nCrucially, this set of experiments show that similarity based methods are not as critical for translation as compared to coherency, a new factor that we identify in this work.\n# 5.4 Similarity based Retrieval within the\n# 5.4 Similarity based Retrieval within the Document\nHow well do similarity based retrieval methods perform for previous on-the-fly translations? In Section 5.3, we established that using a moving window (local coherence) outperforms retrieval from outside the document with similarity-based retrieval methods. Here we apply bm25, bm25-s, nn for retrieval within the document. We consider the more realistic \"on-the-fly\" or computer-aided translation scenario, where the human translator works with MT systems, and translation examples in the document can only be selected prior to the test sentence (Alabau et al., 2014).\nControlling for Length When doing retrieval based methods within the document for an \"on-thefly\" setting, length factors in and longer sentences are retrieved on average. We thus investigate budgeting for the length constraint to be same as the moving window (window). For every test sentence, we compute the budget used by it\u2019s own moving window, and apply it as a length constraint to for the other retrieval based methods as described in Section 4.3. Results are presented in Figure 3.\nResults and Discussion\n<div style=\"text-align: center;\">GPTNeo2.7B(BLEU)</div>\n<div style=\"text-align: center;\">LEU) Bloom3B(BLEU)</div>\n<div style=\"text-align: center;\">Bloom3B(BLEU)</div>\nGPTNeo2.7B(BLEU)\nBloom3B(BLEU)\nXGLM2.9B(BLEU)\nL2\ncoverage\nIn/outdoc\nen\u2192fr\nen-pt\nen-de\nen-fr\nen-pt\nen-de\nen-fr\nen-pt\nen-de\n-\n-\nrandom\nout\n26.3\n27.1\n16.6\n35.2\n35.5\n7.9\n27.1\n26.7\n18.9\n1.35\n0.31\nnn\nout\n26.8\n26.9\n16.9\n35.1\n35.1\n8.2\n25.6\n26.6\n18.3\n0.98\n0.49\nbm25\nout\n27.1\n27.4\n17.3\n35.1\n35.3\n9.4\n25.3\n27.0\n18.4\n1.21\n0.75\nbm25-s\nout\n27.2\n27.5\n17.4\n34.8\n34.9\n9.1\n25.6\n27.4\n18.7\n1.25\n0.80\nrandom\nwithin\n27.4\n27.3\n17.3\n35.9\n35.8\n7.8\n26.6\n28.8\n19.6\n1.28\n0.34\nwindow\nwithin\n28.1\n28.3\n17.9\n36.9\n37.0\n8.8\n28.6\n31.6\n21.2\n1.22\n0.40\nTable 4: BLEU score comparison of similarity-based retrieval methods from out of document, and moving window (window) from within the document. Coverage (Rouge1-precision) refers to the word overlap between prompt source sentences and test source sentence. L2 refers to the average L2 Euclidean distance between source prompt sentence embeddings and the test sentence embedding.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d4b3/d4b3ceb9-a3be-4aea-a284-c697b4770a45.png\" style=\"width: 50%;\"></div>\nFigure 3: Comparison of Retrieval methods controling for budget: No budget or same budget as moving window. Model is GPTNeo2.7B on en->fr. random is sampled within the document.\n<div style=\"text-align: center;\">Figure 3: Comparison of Retrieval methods controling for budget: No budget or same budget as moving window. Model is GPTNeo2.7B on en->fr. random is sampled within the document.</div>\n\u2022 We observe similar performance for all retrieval methods, with bm25-s doing slightly better than bm25 and nearest neighbors (nn).\n Without any budget restriction, performance of retrieval methods outperforms window. However when restricted to the same budget as window, we find that the performance is within 0.1-0.5 BLEU score difference. Furthermore, the coverage is only 0.01-0.03 less if not using similarity based retrieval, indicating that most of the differences in contributions could be coming from the length effect and not because of similarity.\n# 6 Further Analysis and Discussion\nIn this section, we focus on GPTNeo2.7B and in the en\u2192fr direction.\n# 6.1 Perplexity and Coverage\nOne natural question that arises is the relationship between Coverage, Coherence, and translation performance. Although there is no widely accepted measure of general coherence, we can formulate this with respect to the particular model being studied. We consider the model\u2019s conditional perplexity\nretrieval\nbleu\nL2\nCoverage\nppl_s\nstatic\n26.6\n1.22\n0.41\n16.8\nrandom\n27.4\n1.28\n0.31\n14.9\nwindow\n28.1\n1.22\n0.40\n11.1\nshuffle\n28.3\n1.22\n0.40\n12.0\nof the test sentence given the context. Perplexity is a widely used measure of suprisal in text and has also been used as a measure in topic coherence (Newman et al., 2010). Concurrent work by Gonen et al. (2022) argue that total perplexity of the input sequence is related to In-context performance. In Figure 4, we produce scatterplots of Sentence BLEU scores, source perplexity and Coverage (word overlap). We observe that there is a negative relationship between source perplexity and Sentence BLEU (-0.22 Pearson\u2019s r), but very noisy relationship between Sentence BLEU and word overlap, and word overlap and source perplexity.\n# 6.2 Studying Local Coherence [Table 5]\nWe compare the window with other baselines which may give some indication of what is important in the document in terms of local coherence.\n\u2022 Shuffle simulates whether the model is affected by the the local coherence by shuffling sentences within window.\n\u2022 Static refers to the first k (window size) translation sentences of the document which is then held fix throughout when translating the rest of the document.\nInterestingly, shuffling the set of prompts within the moving window which breaks the natural ordering of the document \"coherence\" does not deteriorate in-context translation performance. This\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f35/7f351602-4359-4220-80c3-e715ba565916.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">: Scatterplots of Sentence BLEU Scores, with Source Perplexity and Word Ov</div>\nrandom\nbm25\nbm25-s\nnn\nwindow\nPositive\n0.56\n0.62\n0.61\n0.6\n0.62\nNegative\n0.32\n0.31\n0.31\n0.32\n0.29\nNo Change\n0.12\n0.07\n0.08\n0.08\n0.09\nTable 6: Positive, Negative and No change (proportions) in BLEU scores across different prompt selection methods. For positive row, higher is better. For negative row, lower is better.\nfinding is consistent across several models and languages Appendix D. The ordering of the document does affect source perplexity, with perplexity increasing from 11.1 \u2192to 12.0, however this does not negatively affect translation performance. This suggests that the relationship between coherence and translation is indirect or non-linear, and the way models use context might be counter-intuitive; a view increasingly advocated by recent research (Webson and Pavlick, 2021; Min et al., 2022). Overall this suggests we may benefit from methods which perform selection from within the document which we leave to future work.\n# 6.3 Do we need Translation examples at all?\nGiven the rise of instruction-following GPT (Ouyang et al., 2022) a reasonable question is whether prompt example selection will still be relevant in future models. For a large language model, merely providing the instruction \"Translate English to French\" without any prompt examples (zeroshot) can still elicit a translation. In spite of zeroshot success, a common finding (for MT as well as other NLP tasks) is that providing more prompt examples typically results in better performance albeit with diminishing effect. Since examples are not strictly necessary for translation but can enhance the model\u2019s downstream translation ability, what is the role of prompt examples?\nPositive vs Negative Task Interference One curiosity that we observe across all of our experiments, is that prompt sets do better on-average\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ec1/2ec19d19-d39a-4117-9c73-4b7ba56d56d6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Histograms of sentence BLEU scores for zeroshot (no prompts), random, and window.</div>\nrather than across all examples, relative to the Zero-shot, instructions only setting. This suggests a notion of interference; examples may guide generation towards a poorer translation (negative interference) or better translation (positive interference). A closely related concept is task location (Reynolds and McDonell, 2021). Table 6 quantifies this across different methods corresponding to the results in Table 4 for GPTNeo2.7B en\u2192fr direction. window has both the highest positive interference and lowest negative interference. From Figure 5, the major role of prompting methods compared to the zero-shot scenario is to have greater positive interference chiefly over sentence BLEU of 20-60 and for some extreme cases of 100 BLEU, although a large proportion of sentences still lie in the low-scoring region.\n# 7 Conclusion\nIn-context Learning has typically been thought of as learning from examples. In this work, we introduce a different perspective of coherency of the context with the test sentence. We first showed that models are mostly able to adapt to different writing styles when the prompt bank and test set are matching/consistent in domain. Experiments across 3 models and 3 languages show that a moving win-\ndow is up to 2.6 BLEU points better than previously reported similarity based retrieval methods from outside the document. From this perspective, the problem of prompt selection for in-context MT is one of maintaining a coherency for text generation. Preliminary analysis on local coherence effects, and the presence of negative interference compared to the zero-shot setting, suggests avenues for future work on investigating more careful mechanisms for controlling in-context Machine Translation.\n# 8 Limitations\nThis section details several limitations and ethical concerns associated with this work.\n\u2022 While we have identified coherency of domain and document as a factor for in-context MT, we expect there should be other factors that could be more predictive of downstream performance, such as activation of attention patterns from source to target sentence during generation.\n\u2022 We studied GPTNeo, Bloom and XGLM which have different training data but similar sizes. Due to GPU memory limitations we did not study larger models and it is not clear whether findings generalise to even larger models.\n Although the TED talks dataset is a good overall testbed because it covers many topics and combines formal language and informal text, we did not quantify whether coherence is more likely to affect formal or informal language and might be studied with other datasets.\n\u2022 This paper focuses heavily on MT as a complex generative task to study coherence of context, and it is not immediately clear whether the findings would also generalise to other longer-context generation tasks such as document summarization or how this would affect simple classification.\n# 9 Ethical Concerns\nLarge LMs are known to hallucinate text content, potentially produce toxic speech or misinformation. While we did not observe this frequently in our experiments, we did not quantify the extent of this across various methods.\n# References\nRegina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1\u201334.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\nHui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 912\u2013920.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021. Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations.\n# A Resources\n# Software\n\u2022 Implementation of Nearest Neighbor Retrieval with FAISS library (Johnson et al., 2019) \u2022 Huggingface library was used for LLMs, model weights and calculation of perplexity.\n# Hardware\n\u2022 All experiments can be run with a single NVIDIA-TITAN RTX GPU (24GB).\nDatasets All datasets for the experiments are open-source.\n# B Local Coherence (nprompts=15)\nAblation experiments for Section 5.3 prompt set size of 15 shown in Table 7. The same trend is observed for 5 and 15 prompts.\n# C Domain vs Perplexity\nWe report the perplexity of the source sentences when randomly sampling and testing from different domains. Although there is no widely accepted measure of general coherence, we can formulate this with respect to the particular model being studied. We consider the model\u2019s conditional perplexity of the test sentence given the context. Perplexity is a widely used measure of suprisal in text and has also been used as a measure in topic coherence (reference). Concurrent work by Gonen et al. (2022) argue that total perplexity of the input sequence is related to In-context performance. We report the conditional perplexity from sampling and testing in different domains for GPTNeo2.7B in Table 8 and Bloom3B in Table 9. We did not report XGLM2.9B because the model log likelihood is very poorly calibrated.\n# D Local Coherence Shuffle Effects\nBLEU scores for comparing window with other baselines, accompanying appendix section to Section 6.2 which reports BLEU scores for GPTNeo2.7B en\u2192fr. we find that results generalise across several models and languages that we further investigated.\n<div style=\"text-align: center;\">GPTNeo2.7B(BLEU) Bloom3B(BLEU)</div>\n<div style=\"text-align: center;\">GPTNeo2.7B(BLEU)</div>\nGPTNeo2.7B(BLEU)\nBloom3B(BLEU)\nXGLM2.9B(BLEU)\nIn/outdoc\nen-fr\nen-pt\nen-de\nen-fr\nen-pt\nen-de\nen-fr\nen-pt\nen-de\nrandom\nout\n27.2\n27.3\n16.9\n35.3\n35.4\n8.0\n29.2\n31.2\n20.7\nnn\nout\n27.3\n28.2\n17.1\n35.6\n35.9\n9.1\n30.0\n32.0\n21.6\nbm25\nout\n27.9\n29.0\n17.4\n36.1\n36.4\n10.8\n31.2\n33.0\n22.2\nbm25-s\nout\n27.7\n29.1\n17.3\n35.2\n36.0\n9.1\n29.8\n32.0\n21.6\nrandom\nwithin\n28.1\n29.2\n17.6\n36.8\n37.3\n8.9\n30.9\n33.3\n22.3\nwindow\nwithin\n28.9\n29.8\n18.2\n37.8\n38.1\n9.6\n31.7\n34.4\n23.0\nPrompt / Test\nFLORES\nMED\nMTNT\nTED\nFLORES\n21.3\n24.5\n54.9\n25.5\nMED\n24.1\n16.2\n62.4\n27.0\nMTNT\n25.4\n26.9\n40.8\n23.3\nTED\n24.0\n24.9\n52.2\n19.6\nPrompt / Test\nFLORES\nMED\nMTNT\nTED\nFLORES\n21.5\n23.4\n61.8\n28.5\nMED\n25.1\n16.3\n74.8\n33.4\nMTNT\n25.2\n25.7\n47.2\n26.1\nTED\n24.1\n23.5\n60.1\n22.1\nGPTNeo\n(en-pt)\nGPTNeo\n(en-de)\nXGLM\n(en-fr)\nBloom\n(en-fr)\nstatic\n27.1\n16.8\n27.7\n34.9\nrandom\n27.3\n17.3\n26.6\n35.9\nwindow\n28.3\n17.9\n28.5\n36.9\nshuffle 28.5\n17.9\n28.7\n36.9\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context Machine Translation is a relatively new paradigm that uses large autoregressive Language Models to carry out the task of Machine Translation (MT) by being shown translation pairs in the prefix. Previous strategies for adapting a pre-trained MT model still require additional engineering or training of the model, e.g fine-tuning with in-domain data using adaptor layers. Instead, simply changing the inputs to the model might be an effective way to adapt on-the-fly without any model modification. This paper introduces a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples.",
        "problem": {
            "definition": "The problem addressed in this paper is the effectiveness of prompt selection in in-context Machine Translation and how coherency between prompts and the test sentence affects translation performance.",
            "key obstacle": "The main challenge is that existing methods for prompt selection often overlook the importance of maintaining coherency, leading to suboptimal translation results."
        },
        "idea": {
            "intuition": "The idea stems from observing that translation performance improves when prompts are selected based on their coherency with the test sentence rather than just semantic or surface similarity.",
            "opinion": "The proposed idea is to utilize a moving window of previously translated sentences as prompts to maintain coherency, which is hypothesized to enhance translation quality.",
            "innovation": "This method differs from existing approaches by emphasizing the role of coherency in prompt selection, demonstrating that models perform better when prompts are contextually relevant to the task at hand."
        },
        "method": {
            "method name": "In-context Machine Translation with Coherency",
            "method abbreviation": "ICMT-C",
            "method definition": "ICMT-C is a method that adapts translation models on-the-fly by selecting prompts that maintain coherency with the test sentence, rather than relying solely on semantic similarity.",
            "method description": "The method involves using a moving window of previous gold translations as prompts to enhance the coherence of the translation context.",
            "method steps": [
                "Select a test sentence to be translated.",
                "Identify a moving window of previously translated sentences.",
                "Use this window as prompts for the translation model.",
                "Evaluate the translation performance based on BLEU scores."
            ],
            "principle": "The effectiveness of this method is rooted in the idea that coherent prompts provide a more relevant context for the model, leading to improved translation accuracy."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted across three models (GPTNeo2.7B, Bloom3B, XGLM2.9B) and four domains (Medical, Social Media, Wikipedia, and TED Talks), utilizing standard train-test splits and measuring performance using BLEU scores.",
            "evaluation method": "The evaluation involved comparing translation performance using the proposed method with baseline methods, focusing on the impact of coherency on translation quality."
        },
        "conclusion": "The study concludes that maintaining coherency in prompt selection significantly enhances translation performance, with a moving window approach yielding better results than traditional similarity-based methods.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to adapt translation models on-the-fly without requiring extensive retraining, leading to more efficient and effective translations.",
            "limitation": "A limitation of this work is that it does not explore the potential influence of other factors that may affect translation performance, such as attention patterns during generation.",
            "future work": "Future research should investigate additional factors that could enhance coherency in prompt selection and explore the applicability of the findings to other natural language processing tasks."
        },
        "other info": [
            {
                "info1": "The study highlights the importance of coherency in natural language tasks and suggests that further exploration could lead to improved methodologies in machine translation."
            },
            {
                "info2": {
                    "info2.1": "The experiments utilized open-source datasets, ensuring reproducibility of results.",
                    "info2.2": "The models used are publicly available, allowing for further research and validation of findings."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context Machine Translation is a relatively new paradigm that uses large autoregressive Language Models to carry out the task of Machine Translation (MT) by being shown translation pairs in the prefix."
        },
        {
            "section number": "1.2",
            "key information": "The effectiveness of prompt selection in in-context Machine Translation and how coherency between prompts and the test sentence affects translation performance is a significant aspect of in-context learning."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, In-context Machine Translation with Coherency (ICMT-C), adapts translation models on-the-fly by selecting prompts that maintain coherency with the test sentence."
        },
        {
            "section number": "1.4",
            "key information": "The study emphasizes the role of coherency in prompt selection, demonstrating that models perform better when prompts are contextually relevant to the task at hand."
        },
        {
            "section number": "3.1",
            "key information": "The method involves using a moving window of previous gold translations as prompts to enhance the coherence of the translation context."
        },
        {
            "section number": "3.2",
            "key information": "The effectiveness of the proposed method is rooted in the idea that coherent prompts provide a more relevant context for the model, leading to improved translation accuracy."
        },
        {
            "section number": "5.2",
            "key information": "Experiments were conducted across multiple domains (Medical, Social Media, Wikipedia, and TED Talks) to evaluate the impact of coherency on translation quality."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of this work is that it does not explore the potential influence of other factors that may affect translation performance, such as attention patterns during generation."
        },
        {
            "section number": "7",
            "key information": "The study concludes that maintaining coherency in prompt selection significantly enhances translation performance, with a moving window approach yielding better results than traditional similarity-based methods."
        }
    ],
    "similarity_score": 0.7225069088690433,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-context Learning as Maintaining Coherency_ A Study of On-the-fly Machine Translation Using Large Language Models.json"
}