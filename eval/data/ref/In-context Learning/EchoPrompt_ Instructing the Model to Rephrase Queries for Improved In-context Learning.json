{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.10687",
    "title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning",
    "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, which reveal that both the original query and the model-generated rephrased version are instrumental in its performance gains. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance. We recommend incorporating EchoPrompt into various baseline prompting strategies to achieve performance boosts.",
    "bib_name": "mekala2024echopromptinstructingmodelrephrase",
    "md_text": "# EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning\n\nRajasekhar Reddy Mekala \u2217\nrmekala@uci.edu Yasaman Razeghi \u2217\nyrazeghi@uci.edu\n\n# Abstract\n\nLanguage models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt\u2019s effectiveness through ablation studies, which reveal that both the original query and the model-generated rephrased version are instrumental in its performance gains. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance. We recommend incorporating EchoPrompt into various baseline prompting strategies to achieve performance boosts.\n\n# Introduction\n\nLarge language models have revolutionized natural language task-solving through prompting (Brown et al., 2020). This technique involves conditioning the language model with an instruction (zero-shot) or augmenting the prompt with a small set of taskspecific examples (few-shot), resulting in the model to generalize and respond effectively to tasks. A rapidly advancing body of research has introduced techniques to enhance these prompting\n\u2217 First two authors contributed equally.\n\nSameer Singh sameer@uci.edu\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b091/b091f026-3ca7-460a-8e66-998c6ae54acc.png\" style=\"width: 50%;\"></div>\nFigure 1: Comparison of prompts in Zero-shot-CoT with and without EchoPrompt, highlighting the modification in prompts. Zero-shot-CoT with EchoPrompt uses the prompt \u201cLet\u2019s repeat the question and also think step by step\u201d to aid the model in recalling the query before solving it.\n\nmethodologies. Notably, chain-of-thought prompting (Wei et al., 2023; Kojima et al., 2022) has emerged as a powerful method for enhancing language model performance in reasoning tasks. Least-to-most prompting (Zhou et al., 2022a) and Tree of Thoughts (Yao et al., 2023) further support chain-of-thought by breaking down complex problems into simpler subproblems.\nWhile both standard prompting and chain-ofthought prompting exhibit impressive capabilities and find applications across various domains, they can sometimes lead to inaccurate responses due to logical errors, symbol mapping issues, and omission of intermediate steps (Kojima et al., 2022), indicating potential oversights in adequately addressing various facets of the queries.\nIn this paper, we propose EchoPrompt, a prompting strategy that builds upon existing prompting\n\napproaches by incorporating Query-Rephrasing as a preliminary task in the in-context learning process. EchoPrompt draws inspiration from the innate cognitive strategies employed by humans, precisely the act of self-questioning, when answering queries. By verbalizing queries before answering them, humans establish a cognitive checkpoint to refine their thoughts, uncovering misconceptions that might have otherwise gone unnoticed (Joseph and Ross, 2018; Joseph et al., 2019). Figure 1\nprovides an illustrative example of EchoPrompting in Zero-shot-CoT settings. While the approach proposed by (Kojima et al., 2022) uses the prompt \u201cLet\u2019s think step by step.\" to elicit chain-of-thought reasoning and then extracts the answer using the prompt \u201cTherefore, the answer is\", we modify the first prompt to \u201cLet\u2019s repeat the question and also think step by step.\"  or similar texts. This modification guides the model to generate a version of the original query before solving it. We empirically evaluate our approach against various prompting baselines using a wide variety of model families with different sizes, including code-davinci-002, GPT-3.5-Turbo 1, Starcoder15B, Llama-13B, and GPT-J-6B. Our results show that EchoPrompt significantly improves the performance of language models on arithmetic, reading comprehension, and logical reasoning tasks. We observe substantial performance gains with both standard and chain-of-thought prompting, particularly in zero-shot scenarios for large language models (code-davinci-002, GPT-3.5-turbo) and with standard prompting on smaller models (Starcoder15B, Llama-13B, and GPT-J-6B). For example, EchoPrompt increases the Zero-shot-CoT performance from 56.8% to 67.3% on DROP (Census) and from 75.1% to 82.6% on GSM8K with chainof-thought prompting on GPT-3.5(gpt-3.5-turbo). We conduct a series of ablation studies to gain deeper insights into the effectiveness of the EchoPrompt technique. First, we examine whether the accuracy gains attributed to EchoPrompt resulted solely from rephrased queries. Our findings demonstrate that both the original query and the rephrased query are essential in achieving performance improvements. Next, we investigate whether EchoPrompt can be seen as a query augmentation technique by considering the alternative approach of directly augmenting the original\n\n1 https://openai.com/blog/chatgpt/. We use gpt-3.5turbo-0301 snapshot from March 2023\n\nquery with a rephrased version. We observe comparable results between these two approaches, indicating that EchoPrompt serves as a query augmentation technique. Additionally, we explore whether instructing EchoPrompt to generate multiple rephrases can further enhance performance. Interestingly, we observe a slight performance drop as the number of rephrases increases. This suggests that the improvements achieved with EchoPrompt cannot be solely attributed to generating more tokens. Finally, we assess the performance of EchoPrompt in the presence of irrelevant text within the queries and find that it maintains improvements despite replicating irrelevant text in the rephrases. Our study indicates that EchoPrompt fundamentally improves in-context learning performance and finds broad applicability as a building block in emerging complex techniques that leverage prompting in multiple stages.\n\n# 2 EchoPrompt\n\nEchoPrompt teaches language models to generate a version of the query before solving it. The finegrained details of this technique are explained in the following two subsections, with examples.\n\n# 2.1 Zero-shot EchoPrompt\n\nIn zero-shot prompting, the standard approach relies on a single prompt \u201cTherefore, the answer is\" to directly extract the answer. In contrast, Zero-shot EchoPrompt introduces a two-stage prompting process. The language model is initially instructed to rephrase the query using a task-agnostic prompt, \u201cLet\u2019s repeat the question. \u201c\" and then the answer is extracted using the same prompt as in zero-shot prompting. Similarly, in Zero-shot-CoT, as proposed by (Kojima et al., 2022), the conventional approach involves using the prompt \u201cLet\u2019s think step by step.\" to guide the model in generating its reasoning steps before producing the final answer. However, in Zero-shot-CoT with EchoPrompt, we introduce a query-rephrasing subtask by employing prompts like \u201cLet\u2019s repeat the question and also think step by step.\". This modification encourages the model to generate the query in its own words and then engage in multi-hop reasoning. The prompt used for answer extraction remains consistent in both zero-shot and Zero-shot-CoT scenarios. Figure1 shows an example, highlighting the key differences between the two approaches. Tables1, 11 gives a\n\nOriginal Question: If Pam is currently twice as young as Rena is, and in 10 years Rena will be 5 years older than her, how old is Pam now? Compound Sentence Rephrase Given that Pam is currently twice as young as Rena and that in 10 years Rena will be 5 years older than Pam, how old is Pam now? Question First Rephrase What is Pam's current age if Rena is twice as old as Pam and in 10 years Rena will be 5 years older than Pam? Simple Sentence Rephrase Currently, Pam is twice as young as Rena. In 10 years, Rena will be 5 years older than Pam. So, how old is Pam now? Repeatition If Pam is currently twice as young as Rena is, and in 10 years Rena will be 5 years older than her, how old is Pam now?\n\nFigure 2: Example of rephrases used for the proposed rephrase structures in EchoPrompt in few-shot prompting exemplars. The Rephrases of exemplars are generated using ChatGPT based on prompts in Table10.\n\ncomprehensive overview of the prompts we experimented with in this approach 2.\n\n# 2.2 Few-shot EchoPrompt\n\nIn few-shot learning, we teach the language model to rephrase the test query in a particular structure before answering the query. We do this by providing exemplars demonstrating the rephrase structure and corresponding responses to example queries. We examine three distinct rephrasing structures in addition to teaching the model to repeat the exact query in the following formats:\n\n\u2022 Rephrased to Compound Sentences: Queries are formulated using compound sentences incorporating multiple clauses or phrases.\n\n# Rephrased to putting the question First:\n\nQueries are structured to present the final question at the beginning, followed by contextual information.\n\n\u2022 Rephrased to Short and Simple Sentences: Queries are constructed by breaking down the original problem\u2019s context into simpler and shorter sentences.\n\n\u2022 Repetition: Repeating the original query itself can serve as a fundamental form of rephrasing, and we consider it one of the rephrase structures.\n\n2 In zero-shot prompting, EchoPrompt only focuses on repeating the exact query, whereas in Zero-shot-CoT, we explore both query-repetition and rephrasing. This is because we can easily identify the end of query repetition by using quotations. However, there is no clear way to detect when the rephrase is complete.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4abd/4abd9989-4966-4582-86f0-a12fb0d6a22e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Q: Kelly has 5 quarters and 2 dimes. If she buys a can of pop for 55 cents, how many cents will she have left? A: Rewriting in simple words, the question is: \"Given that Kelly has 5 quarters and 2 dimes, and she buys a can of pop for 55 cents, how many cents will she have left?\" Now, to answer the rewritten question, the answer is 90. \u2714\n</div>\nQ: Kelly has 5 quarters and 2 dimes. If she buys a can of pop for 55 cents, how many cents will she have left? A: Rewriting in simple words, the question is: \"Given that Kelly has 5 quarters and 2 dimes, and she buys a can of pop for 55 cents, how many cents will she have left?\" Now, to answer the rewritten question, the answer is 90. \u2714\n\nFigure 3: Example of EchoPrompt with Compound Sentences. Standard Prompting approach showcases exemplars with queries and corresponding answering formats. In contrast, the EchoPrompt incorporates a Query-Rephrase step, where the exemplars showcase a rephrased version of the query along with the answering format.\n\nFigure2 shows an example of these rephrasing formats for a query. We use ChatGPT(OpenAI, 2021) to generate the rephrases for the exemplars in these structures. This way, even our exemplars are generated automatically and with the minimum human effort, which makes EchoPrompt simple to use. The prompts used for generating the rephrases for the exemplars are shown in Table10. In Figure3, we present an illustrative example of the proposed compound sentences  rephrasing. The exemplars in the standard prompting approach (highlighted in blue) demonstrate a sample query and the corresponding answering format. Consequently, when the model is presented with a test query, it responds similarly. However, with the introduction of EchoPrompt, the exemplars now showcase an additional step: query-rephrasing. Consequently, when the model encounters a test query, it produces a rephrased variant and answers it using the original and generated query reformulation.\n\n# 3 Evaluation Setup\n\n# 3.1 Benchmarks\n\nWe evaluate EchoPrompt across a range of natural language processing tasks, specifically focusing on four types, including fourteen widely recognized benchmarks. We experiment with four categories\n\nof causal language models to ensure a broad and thorough evaluation. In this section, we delve into the details of our evaluation setup.\n\nNumerical Reasoning We evaluate numerical reasoning tasks from (Wei et al., 2023) for a fair comparison between the methods including, GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), AQUA-RAT (Ling et al., 2017),  SingleOp and MultiArith subsets from (Roy and Roth, 2016) . Additionally, we examine the performance of EchoPrompt on the high school mathematics subset of the MMLU dataset (Hendrycks et al., 2021a, b) and the GSMIC-4k dataset (Shi et al., 2023), which focuses explicitly on queries containing perturbations.\n\nLogical Reasoning For logical reasoning, we assess the Date Understanding,  Shuffled Objects (tracking three objects) tasks from bigBench (Ghazal et al., 2013), LogiQA (Liu et al., 2020) and generate 1000 random samples with two trials of flipping for Coin Flipping task (Wei et al., 2023).\n\n# Reading Comprehension\n\nReading Comprehension While we evaluate multiple numerical subsets of DROP (Dua et al., 2019), (including Football, Non-football, Census, and Break(Wolfson et al., 2020) from the QDMR dev subset) and could also be included in the arithmetic benchmarks, we group it with SQuAD (Rajpurkar et al., 2016) based on the query style. We evaluate EchoPrompt on DROP (Dua et al., 2019) and SQuAD (Rajpurkar et al., 2016) as two standard reading comprehension benchmarks. The Football subset of the DROP dataset was curated by applying keyword-based filtering with the keyword \u201cyard\" (Zhou et al., 2022a), and the Census subset was created by selectively filtering passages that contained the terms \u201cpopulation\" and \u201ccensus.\"\n\nCommonsense Reasoning For commonsense reasoning, we use StrategyQA (Geva et al., 2021), Winogrande (ai2, 2019) datasets to assess the performance of EchoPrompt on tasks that involve simpler queries but require factual knowledge.\n\n# 3.2 Language models\n\nFor our experiments, we use code-davinci-002 (Chen et al., 2021) as the primary model for all tasks since this model is free to evaluate and has a strong in-context learning ability. Additionally, we present the results on a subset of datasets on\n\nGPT-3.5-Turbo, a model comparable to the size of code-davinci-002. We also experiment with the smaller and publicly available models such as StarCoder-15B (Li et al., 2023), Llama-13B (Touvron et al., 2023), and GPT-J-6B (Wang and Komatsuzaki, 2021) specifically on synthetic and simpler tasks.\n\n# 3.3 Prompts\n\nFew-shot Exemplars For a fair comparison of methods, we use the same exemplars introduced in (Wei et al., 2023) for the GSM8K, SVAMP, SingleOp, MultiArith, Date Understanding, and Coin-Flipping tasks across all models. Additionally, we evaluate with the prompts suggested by (Zhou et al., 2022a) for GSM8K, SVAMP, MultiArith, and DROP subsets. Furthermore, we provide a new set of prompts specifically for the DROP Census subset since no prior proposals exist.\nZero-shot-CoT Prompts As proposed in (Kojima et al., 2022), we employ the prompt \u201cLet\u2019s think step by step.\" in stage 1. In stage 2, we extract the answer using different prompts depending on the type of task. For multiple-choice tasks, we utilize prompts like \u201cFrom (a) through (e), the answer is.\" For other tasks, we use the phrase \u201cTherefore, the answer is.\"\n\n# 4 Results\n\nWe conduct an extensive comparison of our approach against zero-shot, Zero-shot-CoT, few-shot, and few-shot-CoT prompting strategies. Figure4 (and Table9  in Appendix) provides the overall results of EchoPrompt while the extended results on code-davinci-002 and other models are presented in AppendixA. The findings on individual models are summarized below.\nCode-davinci-002 Overall, We observe that EchoPrompt performs well regardless of the baseline prompting strategy. Notably, EchoPrompt shows significant improvements in zero-shot prompting scenarios, especially for tasks with longer query contexts, such as different DROP and SQuAD subsets containing extraneous information. For example, we observed an 18.5% improvement in accuracy on the DROP(Census subset) dataset for zero-shot prompting. Similarly, EchoPrompt with Zero-shot-CoT on SVAMP achieves (7.4%) improvement in accuracy, which makes the overall accuracy comparable to few-shot-CoT prompting.\n\nWe conduct an extensive comparison of our approach against zero-shot, Zero-shot-CoT, few-shot, and few-shot-CoT prompting strategies. Figure4 (and Table9  in Appendix) provides the overall results of EchoPrompt while the extended results on code-davinci-002 and other models are presented in AppendixA. The findings on individual models are summarized below.\n\nFigure 4: Performance summary of EchoPrompt with repetition in zero-shot and compound sentence rephrasing in few-shot settings. Darker colored bars show EchoPrompt augmented with the baseline method. EchoPrompt consistently achieves performance gains across different prompting strategies, particularly in zero-shot scenarios. For details, see Table9 in Appendix.\n\nHowever, it is worth noting that EchoPrompt does not yield improvements in cases where the baseline method cannot solve the task. For example, in the Shuffled Objects task involving three objects, EchoPrompt shows a slight drop in zero-shot performance (36.4% to 35.2%), which is close to random choice (33.3%). Nevertheless, it considerably improves the accuracy in Zero-shot-CoT (42.4% to 58.2%), where the model can partially solve the task. We also do not observe any consistent improvements in tasks involving multiplechoice questions, such as AQuA-RAT, MMLU, and LogiQA, where the model must select one option among several rather than explicitly generating the answer.\n\nGPT-3.5-Turbo To assess the performance of the EchoPrompt technique on a non-code-trained model of similar size to Code-davinci-002, we experiment with GPT-3.5-Turbo on a subset of tasks. Detailed results are in Table9 in Appendix. Overall, these results align with our previous experiments on code-davinci-002. For example, the EchoPrompt technique significantly improves accuracy on GSM8K, from 75.1% to 83.5% in fewshot-CoT. However, we observe a drop in performance on reading comprehension tasks (DROP, and SQuAD) in zero-shot scenarios. After manual qualitative analysis, we observe that the model generates descriptive rather than instruction-based extractable answers, which explains some of the\n\ndrop in performance.\n\nStarCoder-15B, Llama-13B, GPT-J-6B Similarly, we evaluate the performance of EchoPrompt on smaller and publicly available models: StarCoder-15B, Llama-13B, and GPT-J-6B. Our evaluation includes tasks such as coin-flipping, SingleOp, SVAMP, and date-understanding since these smaller models are less capable of challenging reasoning tasks. This set encompasses a toy task and two relatively simpler datasets, while date understanding is considered a challenging task on Bigbench. Detailed results are in Table9 in Appendix. EchoPrompt improves the performance with standard prompting, although we observe inconsistent results with chain-of-thought reasoning. This finding is not entirely surprising, as chain-of-thought is considered an emergent phenomenon in larger language models (Wei et al., 2023).\nComparision with least-to-most prompting Table2 shows a comparison of EchoPrompt in few-shot-CoT against least-to-most prompting 3, which is considered to be state-of-the-art for numerical reasoning tasks. While EchoPrompt utilizes rephrased queries, least to most prompting breaks down the problem into subproblems and solves these subproblems sequentially using chainof-thought. For a fair comparison, we evaluate\n\nStarCoder-15B, Llama-13B, GPT-J-6B Similarly, we evaluate the performance of EchoPrompt on smaller and publicly available models: StarCoder-15B, Llama-13B, and GPT-J-6B. Our evaluation includes tasks such as coin-flipping, SingleOp, SVAMP, and date-understanding since these smaller models are less capable of challenging reasoning tasks. This set encompasses a toy task and two relatively simpler datasets, while date understanding is considered a challenging task on Bigbench. Detailed results are in Table9 in Appendix. EchoPrompt improves the performance with standard prompting, although we observe inconsistent results with chain-of-thought reasoning. This finding is not entirely surprising, as chain-of-thought is considered an emergent phenomenon in larger language models (Wei et al., 2023).\n\n3 In all our evaluations, we employ the condensed variant of least-to-most prompting, where both decomposition and problem-solving are accomplished within a single step.\n\n<div style=\"text-align: center;\">EchoPrompt?\n</div>\nEchoPrompt?\nStage-1 Prompt\nGSM8K\nSVAMP\nMultiArith SingleOp\nZero-shot\n\u2717\n-\n16.4\n66.8\n31.0\n91.6\n\u2713\nLet\u2019s repeat the question. \u201c\n20.7(+4.3) 74.7(+7.9) 48.5(+17.5) 91.8(+0.2)\n\u2713\nLet\u2019s reiterate the question. \u201c\n19.7(+3.3) 73.4(+6.6) 51.0(+20.0) 93.0(+1.4)\n\u2713\nLet\u2019s restate the question. \u201c\n19.2(+2.8) 74.6(+7.8) 47.7(+16.7) 89.6(\u22122.0)\n\u2713\nLet\u2019s summarize the question. \u201c\n20.6(+4.2) 73.2(+6.4) 48.8(+17.8) 93.7(+2.1)\nZero-shot-CoT\n\u2717\nLet\u2019s think step by step.\n49.3\n66.5\n76.0\n82.9\n\u2713\nLet\u2019s repeat the question and also think step by step.\n44.6(\u22124.7) 74.7(+8.2) 70.9(\u22125.1) 92.3(+9.4)\n\u2713\nLet\u2019s reiterate the question and also think step by step.\n51.1(+1.8) 73.9(+7.4) 78.7(+2.7) 92.4(+9.5)\n\u2713\nLet\u2019s repeat the question and also think step by step. \u201c\n42.0(\u22127.3) 60.4(\u22126.1) 78.1(+2.1) 88.3(+5.4)\n\u2713\nLet\u2019s restate the question and also think step by step.\n47.0(\u22122.3) 73.9(+7.4) 79.3(+3.3) 90.2(+7.3)\n\u2713\nLet\u2019s summarize the question and also think step by step.\n49.9(+0.6) 74.2(+7.7) 75.8(\u22120.2) 90.9(+8.0)\nTable 1: Code-davinci-002: Arithematic reasoning Evaluation of EchoPrompt on various prompt templates. All\nGSM8K SVAMP Multi-\nArith\nDROP\n(Census)\nDROP\n(Break)\nDROP\n(Football)\nCoT\n61.1\n75.2\n96.1\n70.0\n65.3\n67.3\nCoT+Compound\n65.9\n79.0\n97.8\n75.4\n69.6\n70.8\nLTM\n63.2\n82.2\n93.7\n73.8\n61.2\n66.2\nTable 2: code-davinci-002 Table show a comparison of\nTable 2: code-davinci-002 Table show a comparison of EchoPrompt with CoT against least-to-most prompting. EchoPrompt outperforms least-to-most prompting on most of the benchmarks.\n\nboth numerical (GSM8K, SVAMP, Multiarith) and reading comprehension (DROP) tasks using the prompts proposed (Wei et al., 2023; Zhou et al., 2022a). Although EchoPrompt is a relatively simpler approach, it outperforms least-to-most prompting on two of the three arithmetic reasoning tasks and all reading comprehension subsets.\n\n# 5 Analysis\n\nTo gain a deeper understanding of the factors that contribute to the success of EchoPrompt, we perform a series of ablation studies in the following sections:\n\nEffect of prompts on zero-shot EchoPrompt To investigate the impact of prompts used to instruct the language model in rephrasing queries in zeroshot settings, we conducted experiments using a variety of prompts on arithmetic tasks, including both standard and chain-of-thought prompting. The results shown in Table1 indicate that EchoPrompt consistently enhances performance when compared to the baseline method, regardless of the chosen prompt. However, we observe a difference in per\n\nformance with various prompt selections in the Zero-shot-CoT setting. The prompt \u201cLet\u2019s reiterate the question and also think step by step.\" achieves the best results.\nEffect of rephrases on few-shot EchoPrompt In the few-shot setting, we assess the performance of the proposed rephrase structures compared to baseline techniques, focusing on arithmetic and reading comprehension tasks that require explicit answer generation. The results, as shown in Table3, reveal that although there is variance among the performance, all the rephrase structures outperform the standard and chain-of-thought prompting, highlighting the effectiveness of EchoPrompt. Notably, no single rephrase structure consistently outperforms the others.\n\n# Are rephrased queries self-sufficient?\n\nsess whether the EchoPrompt performance gains are solely due to the rephrased queries or if both the original and rephrased queries are essential, We isolate the LM generated rephrases. This process involves two steps. First, through in-context learning, we generate the rephrased query using the same method as before and with the same exemplars. Then, we prompt the language model with the revised exemplars that match the rephrased query structure. We only provide the rephrased queries for the model to answer. The results in Table4  show that standalone rephrases consistently yield lower accuracies than EchoPrompt. Although rephrased queries can improve accuracy compared to baseline prompting (compound sentence rephrases), the improvements are still considerably lower than those achieved with EchoPrompt. This suggests that the primary source of improve\n\nEchoPrompt\nGSM8K\nSVAMP\nMultiArith\nDROP\n(Census)\nDROP\n(Break)\nDROP\n(Football)\nSQuAD(F1)\nStandard\n-\n19.2\n69.8\n44.0\n56.8\n55.5\n63.7\n88.7\nRepeat\n21.4(+2.2)\n75.8(+6.6)\n53.8(+9.8)\n65.9(+9.1)\n63.1(+7.6)\n69.2(+5.5)\n91.3(+2.6)\nCompound\n20.8(+1.6)\n75.1(+5.3)\n54.0(+10.0)\n67.3(+10.5)\n62.7(+6.9)\n67.7(+4.0)\n90.6(+1.9)\nQuestion First\n20.9(+1.7)\n75.0(+5.2)\n53.6(+9.6)\n65.2(+8.4)\n59.7(+3.9)\n63.1(\u22120.6)\n92.2(+3.5)\nSimple\n21.5(+2.3)\n76.6(+6.8)\n55.6(+11.6)\n65.1(+8.3)\n63.1(+7.6)\n67.1(+3.4)\n90.9(+2.2)\nCoT\n-\n61.1\n75.2\n96.1\n70.0\n65.3\n67.3\n90.5\nRepeat\n63.5(+2.4)\n77.6(+2.4)\n98.8(+2.7)\n71.6(+1.6)\n70.0(+4.7)\n71.3(+4.0)\n-\nCompound\n65.9(+4.8)\n79.0(+3.8)\n97.8(+1.7)\n75.4(+5.4)\n69.6(+4.3)\n70.8(+3.5)\n90.8(+0.3)\nQuestion First\n64.4(+3.3)\n77.0(+1.8)\n98.3(+2.2)\n75.3(+5.3)\n68.1(+2.8)\n72.0(+4.7)\n-\nSimple\n63.6(+2.5)\n76.9(+1.7)\n99.0(+2.9)\n73.5(+3.5)\n67.7(+2.4)\n71.2(+3.9)\n-\nTable 3: code-davinci-002 Evaluation of EchoPrompt using the proposed rephrase structures and query-repetition.\nTable 3: code-davinci-002 Evaluation of EchoPrompt using the proposed rephrase structures and query-repetition. We compare these approaches with baseline methods in arithmetic and reading comprehension tasks. The results showcase improvements across all rephrase structures, with no single structure consistently outperforming the others.\n\nment in EchoPrompt lies in the provision of two query versions.\n\nComparing the rephrase and the original queries We compare the BLEU scores for the rephrased queries alongside the original ones (refer to Table16 in the Appendix). Additionally, we compute the fraction of tokens retained in the rephrased queries (see Table15 in the Appendix). In numerical tasks, the rephrases retain most of the information from the original queries. However, we observe considerable differences in scores in the standalone rephrases in reading comprehension tasks, particularly in the DROP Football and Break subsets. In these datasets, the original queries exhibit a huge variance in the token count distribution, leading to low-quality rephrase generation, which may be why we observe a significant drop in accuracy.\n\nTo\n\n# Generating vs Augmenting the rephrases\n\nstudy whether EchoPrompt can be considered as a query augmentation technique, we compare the performance of EchoPrompt with directly augmenting the original question using a rephrase (generated in Section5). In EchoPrompt, the model generates both the rephrase and the answer simultaneously, while in query augmentation, the query is provided to the language model beforehand, and the model only generates the answer. Table18 (in Appendix) shows an example highlighting the distinction between the two settings. The result of this experiment is summarized in Table5, demonstrating that both approaches yield comparable improvements in accuracy. This result indicates that although we introduce EchoPrompt as a subtask within incontext learning, it can also be considered a query\n\naugmentation technique. This is because the language model utilizes the same rephrased query and the original query to solve the query in both cases.\n\nStacking multiple rephrases for EchoPrompt The benefits observed with query-rephrasing in EchoPrompt naturally prompted us to investigate the effects of having the language model generate multiple rephrases. The summarized results in Table6 show a drop in performance as the number of rephrases increases. When manually examining the generated answers, we observe a tendency towards repetition in the chain-of-thought reasoning despite successfully generating the desired number of rephrases. This repetition phenomenon becomes particularly prominent when the question requires longer multi-hop reasoning. The Appendix shows Examples illustrating this finding in Table\n17. This observation aligns with expectations since the task\u2019s focus shifts from chain-of-thought reasoning to rephrase generation when the number of rephrases is increased in EchoPrompt. Consequently, the model prioritizes generating the requested number of rephrases rather than the reasoning process.\n\n# Robustness to irrelevant text\n\net al., 2023) has shed light on the sensitivity of large language models (LLMs) to irrelevant information using various prompting methods, including the CoT reasoning. Intuitively, EchoPrompt could be particularly prone to such distractions, given that it rephrases or regenerates the query, including the distractions. To evaluate if EchoPrompt technique works even in the presence of such perturbations, we study the performance of EchoPrompt on GSMIC-4k dataset (Shi et al., 2023). The evalua\n\nQuery Structure\nGSM8K\nSVAMP\nDROP\n(Census)\nDROP\n(Break)\nDROP\n(Football)\nStandard\nOriginal\n19.2\n69.8\n56.8\n55.5\n63.7\nCompound\n19.9(+0.7)\n71.8(+2.0)\n59.1(+2.3)\n54.1(\u22121.4)\n65.1(+1.4)\nQuestion First\n14.6(\u22124.6)\n58.5(\u221211.3)\n28.2(\u221228.6)\n36.2(\u221219.3)\n48.8(\u221214.9)\nSimple\n19.7(+0.5)\n70.9(+1.1)\n56.5(\u22120.3)\n55.5(+0.0)\n62.7(\u22121.0)\nStandard+ Repeat\n-\n21.5\n76.6\n65.1\n63.1\n67.1\nCoT\nOriginal\n61.1\n75.2\n69.6\n65.3\n67.3\nCompound\n62.1(+1.0)\n78.0(+2.8)\n71.9(+2.3)\n66.7(+1.4)\n68.2(+0.9)\nQuestion First\n55.1(\u22126.0)\n66.6(\u22128.6)\n48.1(\u221221.5)\n64.5(\u22120.8)\n57.8(\u22129.5)\nSimple\n61.3(+0.2)\n75.8(+0.6)\n70.3(+0.7)\n67.3(+2.0)\n67.1(\u22120.2)\nCoT+ Compound\n-\n65.9\n79.0\n74.3\n69.6\n70.8\n Standalone Rephrases: code-davinci-002\nTable 4: Standalone Rephrases: code-davinci-002 Compound Sentence rephrasing performs original queries, while question-first rephrasing performs worse. We observe information loss in certain tasks (see Table15), indicating that the performance gains of EchoPrompt are due to the rephrasing and having multiple versions.\n\nGSM8K SVAMP DROP\nRepeat\nSubTask\n63.5\n77.6\n70.0\nAugment\n63.4\n76.3\n69.3\nCompound\nSubTask\n65.9\n79.0\n69.6\nAugment\n64.2\n77.2\n69.7\n code-davinci-002\n<div style=\"text-align: center;\">Table 5: code-davinci-002 A comparison between EchoPrompt and query augmentation, indicating similar performance improvements for both approaches.\n</div>\ntimes\nGSM8K\nSVAMP\nDROP\nRepeat\n1\n63.5\n77.6\n70.3\n2\n61.7\n77.6\n68.5\n3\n59.8\n77.8\n69.3\n5\n59.9\n76.9\n67.5\nCompound\n1\n65.9\n79.0\n69.6\n2\n63.7\n77.9\n68.8\n3\n63.2\n78.9\n67.9\nable 6: code-davinci-002 The accuracies drop as the\nTable 6: code-davinci-002 The accuracies drop as the number of rephrases/repetitions increases when generating multiple rephrases with EchoPrompt.\n\nTable 6: code-davinci-002 The accuracies drop as the number of rephrases/repetitions increases when generating multiple rephrases with EchoPrompt.\n\ntion results in Table 7 demonstrate that EchoPrompt maintains improvements across all prompting techniques, even in the presence of perturbations.\n\n# 6 Related Work\n\nPrompting Large language models\u2019 success has sparked interest in improving task performances through prompting techniques (Brown et al., 2020). While the recent studies focus on task-based instruction tuning, either by fine-tuning the entire model (Raffel et al., 2020; Wei et al., 2021; Sanh et al., 2021; Wang et al., 2022b; Huang et al., 2022) or maintaining task-specific parameters (Li and Liang, 2021; Lester et al., 2021), our work is a general prompting approach that improves the incontext learning abilities and does not require any fine-tuning.\n\nStandard\nCoT\nLTM\nEchoPrompt?\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\nZero-shot\n23.7\n30.1\n(+6.4)\n46.7\n52.8\n(+6.1)\nN/A\nN/A\n1-shot\n27.1\n29.1\n(+2.0)\n72.6\n77.2\n(+4.6)\n73.8\n81.3\n(+7.5)\n4-shot\n25.2\n31.0\n(+5.8)\n77.4\n81.8\n(+4.4)\n84.3\n85.4\n(+1.1)\nTable 7: code-davinci-002 Performance of EchoPrompt\nTable 7: code-davinci-002 Performance of EchoPrompt on GSMIC-4k(which contains irrelevant context in queries). EchoPrompt improves performance on both chain-of-thought and least-to-most prompting, even though it repeats the perturbation sentence in the rephrase.\n\nIntermediate steps The concept of employing language models to generate intermediate steps for process supervision has been extensively examined in the context of solving reasoning tasks, whether through training (Nye et al., 2021;  Zelikman et al., 2022), zero-shot (Kojima et al., 2022), few-shot prompting (Wei et al., 2022) or action planning(Yao et al., 2022). Recent works focus on problem decomposition and teaching the language model to answer the subtasks, to eventually answer complex problems (Zhou et al., 2022a; Dua et al., 2022; Wang et al., 2022a; Zhou et al., 2022b). EchoPrompt is orthogonal to these approaches, augmenting the input query rather than rationale generation. Consequently, it can be easily extended with any of these prompting strategies.\n\nInterpretability, Consistency and Outcome correction Another related research direction involves exploring interpretability and consistency in the rationale generated by large-scale models. Recent works (Imani et al., 2023; Miao et al.,\n\nEchoPrompt?\nWinoGrande\nStrategyQA\n-\n71.9\n74.8\nRepeat\n71.9\n75.3\nCompound\n70.8\n74.5\nTable 8: code-davinci-002 EchoPrompt with Standard\n2023; Madaan and Yazdanbakhsh, 2022) help improve the interpretability in arithmetic and reasoning tasks through validation. Although these approaches are not directly tied to the EchoPrompt technique, they utilize chain-of-thought prompting, where we have shown that EchoPrompt exhibits promising results, particularly in zero-shot scenarios. In the domain of outcome correction, approaches such as (Jung et al., 2022; Wang et al., 2023; Yao et al., 2023; Miao et al., 2021; Xie et al., 2023) leverage consistency among multiple generated rationales while (Weng et al., 2023; Khalifa et al., 2023; Yang and Klein, 2021; Ni et al., 2023; Chen et al., 2022) prioritize the ranking of plausible generations to enhance performance across arithmetic, reasoning, and code-generation tasks. Building upon these foundations, self-correction methodologies like (Madaan et al., 2023; Jiang et al., 2023; Hao et al., 2023; Shinn et al., 2023), which employ feedback loops for refinement and multi-agent debating strategies (Du et al., 2023; Cohen et al., 2023; Fu et al., 2023) have evolved. EchoPrompt distinguishes itself from these approaches by focusing on single rationale generation rather than considering multiple generated responses.\n\n# 7 Limitations\n\nWhile the EchoPrompt subtask presents notable advantages, several limitations exist. Although we provide several ablation studies and qualitative examples, answering the question of when EchoPrompt works better, we could not explain why EchoPrompt results in performance gains, particularly in standard prompting. Additionally, it is worth noting that our approach involves regenerating the entire query before solving the tasks. Consequently, the model must generate many tokens when dealing with long queries, leading to increased compute requirements and time delays.\n\n# 8 Conclusion\n\nWe have proposed EchoPrompt, a simple yet effective approach that builds upon existing prompting approaches and integrates query-rephrasing as a\n\nsubtask in the in-context learning process inspired by how humans think. It enables the language model to recall the query before attempting to solve it. EchoPrompt offers a direct approach to enhance in-context learning in pre-trained language models without fine-tuning, making it a simple and powerful approach to achieve performance boosts.\n\n# 9 Reproducibility Statement\n\nOur primary results are on Code-davinci-002 and GPT-3.5-Turbo, which are publicly accessible OpenAI models. To increase reproducibility, we have included prompts used for all the tasks in the Appendix. We also plan to release the code soon.\n\n# References\n\n2019. Winogrande: An adversarial winograd schema challenge at scale.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Zieglxer, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.  Training verifiers to solve math word problems.\nRoi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. Lm vs lm: Detecting factual errors via cross examination.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Zieglxer, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.  Training verifiers to solve math word problems.\nRoi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. Lm vs lm: Detecting factual errors via cross examination.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate.\n\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate.\n\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomposing complex questions. arXiv preprint arXiv:2212.04092.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving language model negotiation with self-play and in-context learning from ai feedback.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013 361.\nAhmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, and Hans-Arno Jacobsen. 2013. Bigbench: Towards an industry standard benchmark for big data analytics. In  Proceedings of the 2013 ACM SIGMOD international conference on Management of data, pages 1197\u20131208.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared human values.  Proceedings of the International Conference on Learning Representations (ICLR).\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding.  Proceedings of the International Conference on Learning Representations (ICLR).\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398.\nShuyang Jiang, Yuhao Wang, and Yu Wang. 2023.  Selfevolve: A code evolution framework via large language models.\nLaurice M Joseph, Sheila Alber-Morgan, Leigh Ann Amspaugh, Kelsey Ross, Maria Helton, Moira Konrad, and Carrie Davenport. 2019. Stop to ask and respond: Effects of a small-group self-questioning intervention on reading comprehension performance. Research and Practice in the Schools: The Official Journal of the Texas Association of School Psychologists, 6(1):27.\n\nLaurice M Joseph and Kelsey M Ross. 2018. Teaching middle school students with learning disabilities to comprehend text using self-questioning. Intervention in School and Clinic, 53(5):276\u2013282.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. 2023.\nDiscriminator-guided multi-step reasoning with language models.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you!\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u2013 4597, Online. Association for Computational Linguistics.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun\n\nLaurice M Joseph and Kelsey M Ross. 2018. Teaching middle school students with learning disabilities to comprehend text using self-questioning. Intervention in School and Clinic, 53(5):276\u2013282.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. 2023.\nDiscriminator-guided multi-step reasoning with language models.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you!\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u2013 4597, Online. Association for Computational Linguistics.\n\nproblems. In  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, Vancouver, Canada. Association for Computational Linguistics.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686.\nNing Miao, Yee Whye Teh, and Tom Rainforth. 2023.\nSelfcheck: Using llms to zero-shot check their own step-by-step reasoning.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. A diverse corpus for evaluating and developing english math word problem solvers.\nAnsong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen tau Yih, Sida I. Wang, and Xi Victoria Lin. 2023. Lever: Learning to verify language-to-code generation with execution.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\nOpenAI. 2021. Chatgpt. [ChatGPT].\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems?\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.\nSubhro Roy and Dan Roth. 2016.  Solving general arithmetic word problems.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.\n\nproblems. In  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, Vancouver, Canada. Association for Computational Linguistics.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686.\nNing Miao, Yee Whye Teh, and Tom Rainforth. 2023.\nSelfcheck: Using llms to zero-shot check their own step-by-step reasoning.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. A diverse corpus for evaluating and developing english math word problem solvers.\nAnsong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen tau Yih, Sida I. Wang, and Xi Victoria Lin. 2023. Lever: Learning to verify language-to-code generation with execution.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\nOpenAI. 2021. Chatgpt. [ChatGPT].\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems?\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.\nSubhro Roy and Dan Roth. 2016.  Solving general arithmetic word problems.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.  Reflexion: Language agents with verbal reinforcement learning.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022a. Iteratively prompt pre-trained language models for chain of thought. arXiv preprint arXiv:2203.08383.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.  Chain-of-thought prompting elicits reasoning in large language models.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. 2023. Large language models are better reasoners with self-verification.\n\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 8:183\u2013198.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, MinYen Kan, Junxian He, and Qizhe Xie. 2023.  Decomposition enhances reasoning via self-evaluation guided decoding.\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In  Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3511\u20133535, Online. Association for Computational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via incontext learning. arXiv preprint arXiv:2211.09066.\n\nModel\nDataset\nzero-shot\nfew-shot\nStandard\nCoT\nStandard\nCoT\nEchoPrompt ?\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\nCode-davinci-002\nGSM8K\n16.4 20.7(+4.3)\n49.3 51.1(+1.8)\n19.2\n21.4(+2.2)\n61.1\n65.9(+4.8)\nSVAMP\n66.8 74.7(+7.9)\n66.5 73.9(+7.4)\n69.8\n75.8(+6.0)\n75.2\n79.0(+3.8)\nMultiArith\n31.0 48.5(+17.5)\n76.0 78.7(+2.7)\n44.0\n53.8(+9.8)\n96.1\n97.8(+1.7)\nSingleOp\n91.6 91.8(+0.2)\n82.9 92.4(+9.5)\n93.2\n94.2(+1.0)\n92.8\n94.7(+1.9)\nShuffled Objects\n36.4 35.2(\u22121.2)\n42.4 58.2(+15.8)\n34.8\n36.7(+1.9)\n66.0\n68.9(+2.9)\nCoin Flip\n47.7 47.2(\u22120.5)\n58.5 60.1(+1.6)\n99.6\n100.0(+0.4)\n100.0 100.0(+0.0)\nDate\n44.2 43.8(\u22120.4)\n39.0 46.8(+7.8)\n49.3\n50.4(+1.1)\n65.6\n68.1(+2.5)\nDROP(Football)\n50.8 58.3(+7.5)\n44.1 58.0(+13.9)\n63.7\n69.2(+5.5)\n67.3\n70.8(+3.5)\nDROP(Nonfootball)\n43.2 57.1(+13.9)\n39.7 52.6(+12.9)\n57.1\n63.3(+6.2)\n69.2\n72.2(+3.0)\nDROP(Census)\n45.9 66.3(+20.4)\n30.0 53.3(+23.3)\n56.8\n65.9(+9.1)\n69.6\n75.4(+5.8)\nDROP(Break)\n43.7 55.8(+12.1)\n38.2 51.2(+13.0)\n55.5\n63.1(+7.6)\n65.3\n69.6(+4.3)\nSQuAD(F1)\n65.7 69.8(+4.1)\n52.6 54.4(+1.8)\n88.7\n91.3(+2.6)\n90.5\n90.8(+0.3)\nAQUA-RAT\n21.2 23.3(+2.1)\n37.0 35.4(\u22121.6)\n30.3\n29.9(\u22120.4)\n43.7\n41.3(+2.4)\nMMLU-h\n31.8 36.7(+4.9)\n42.5 41.7(\u22120.8)\n36.7\n39.3(+2.6)\n44.1\n42.1(\u22122.0)\nlogiQA\n42.5 41.6(\u22120.9)\n37.0 40.9(+3.9)\n45.3\n46.6(+1.3)\n40.9\n41.0(+0.1)\nGPT-3.5\n(Turbo)\nGSM8K\n5.6\n24.8(+19.2)\n75.7 76.4(+0.7)\n31.3\n32.1(+0.8)\n75.1\n83.5(+8.4)\nSVAMP\n51.9 76.0(+24.1)\n80.5 83.5(+3.0)\n76.1\n78.4(+2.3)\n77.4\n81.9(+5.5)\nMultiArith\n76.5 83.7(+7.2)\n93.4 96.3(+2.9)\n83.4\n90.5(+7.1)\n97.8\n98.5(+0.7)\nSingleOp\n92.6 96.8(+4.2)\n91.4 94.8(+3.4)\n93.9\n96.2(+2.3)\n95.7\n96.5(+0.8)\nShuffled Objects\n26.9 21.6(\u22125.3)\n79.5 82.2(+2.7)\n30.6\n34.6(+4.0)\n68.8\n74.3(+5.5)\nCoin Flip\n76.7 86.8(+10.1)\n99.8 98.6(\u22121.2)\n90.0\n95.6(+5.6)\n100.0 100.0(+0.0)\nDate\n45.7 44.1(\u22121.6)\n46.6 45.8(\u22120.6)\n50.4\n49.3(\u22121.1)\n64.5\n66.2(+1.7)\nDROP(Break)\n47.1 52.9(+5.8)\n51.9 51.0(\u22120.9)\n59.9\n62.7(+2.8)\n61.6\n66.5(+4.9)\nSQuAD(F1)\n79.1 80.6(+1.5)\n62.1 58.3(\u22123.8)\n76.4\n83.2(+6.8)\n85.3\n86.1(+0.8)\nAQUA-RAT\n27.9 28.4(+0.5)\n51.1 50.8(\u22120.3)\n33.4\n35.8(+2.4)\n39.7\n57.1(+17.4)\nMMLU-h\n25.6 31.1(+5.5)\n51.1 52.9(+1.8)\n34.1\n34.8(+0.7)\n28.9\n41.1(+12.2)\nlogiQA\n36.2 38.2(+2.0)\n37.6 39.0(+1.4)\n45.1\n43.3(\u22121.8)\n32.5\n32.3(\u22120.2)\nStarcoder\n(15B)\nSingleOp\n63.1 66.9(+3.8)\n53.5 66.5(+13.0)\n64.0\n70.1(+6.1)\n68.8\n73.6(+4.8)\nSVAMP\n35.6 37.9(+2.3)\n30.9 36.7(+5.8)\n32.4\n37.2(+4.8)\n30.2\n36.2(+6.0)\nCoin Flip\n55.4 54.3(\u22121.1)\n51.6 51.0(\u22120.6)\n98.6\n99.8(+1.2)\n100.0 100.0(+0.0)\nDate\n15.9 19.2(+3.3)\n20.6 19.9(\u22120.7)\n24.4\n26.6(+2.2)\n38.4\n33.8(\u22124.6)\nLlama\n(13B)\nSingleOp\n78.4 81.1(+2.7)\n64.9 73.0(+8.1)\n81.1\n83.3(+2.2)\n81.3\n80.6(\u22120.7)\nSVAMP\n36.4 46.3(+9.9)\n30.7 34.0(+3.3)\n39.2\n43.0(+3.8)\n38.7\n41.3(+2.6)\nCoin Flip\n53.2 51.3(\u22121.8)\n51.0 51.0(+0.0)\n89.8\n92.7(+2.9)\n100.0 100.0(+0.0)\nDate\n24.9 26.6(+1.7)\n22.5 23.0(+0.5)\n32.8\n30.1(\u22121.7)\n42.3\n40.9(\u22121.4)\nGPT-J\n(6B)\nSingleOp\n-\n-\n-\n-\n37.2\n39.9(+2.7)\n45.3\n44.5(\u22120.8)\nSVAMP\n-\n-\n-\n-\n8.9\n10.1(+1.2)\n21.1\n19.8(\u22121.3)\nCoin Flip\n-\n-\n-\n-\n81.3\n81.3(+0.0)\n80.6\n96.4(+15.8)\nDate\n-\n-\n-\n-\n13.2\n13.6(+0.4)\n11.1\n15.8(+4.7)\nTable 9: Performance Summary of EchoPrompt on all models. EchoPrompt consistently improves performance\nRephrase\nPrompt\nCompound\nRephrase the following query using compound sentences without loss of details,\nstarting with \u201cGiven that\" and ending with the question in the query:\n<Question>\nQuestion First\nRephrase the following query by asking the question in the query first, without loss\nof details:\n<Question>\nShort and simple sentences\nRephrase the following query using short and simple sentences, without loss of\ndetails:\n<Question>\nTable 10: Prompts used to create rephrases for exemplars, using ChatGPT\nEchoPrompt?\nStage-1 Prompt\nDROP\n(Football)\nDROP\n(Nonfootball)\nDROP\n(Census)\nDROP\n(Break)\nzero-shot\n\u2717\n-\n50.8\n43.2\n46.4\n43.7\n\u2713\nLet\u2019s repeat the complete question. \u201c\n58.3(+7.5)\n57.1(+13.9)\n66.3(+19.9)\n55.8(+12.1)\n\u2713\nLet\u2019s reiterate the complete question. \u201c\n57.0(+6.2)\n56.9(+13.7)\n66.3(+19.9)\n54.1(+10.4)\n\u2713\nLet\u2019s restate the complete question. \u201c\n60.5(+9.7)\n57.1(+13.9)\n66.7(+20.3)\n56.2(+12.5)\n\u2713\nLet\u2019s summarize the complete question. \u201c\n59.6(+8.8)\n55.6(+12.4)\n63.9(+17.5)\n54.2(+10.5)\nZero-shot-CoT\n\u2717\nLet\u2019s think step by step.\n44.1\n39.7\n30.0\n38.2\n\u2713\nLet\u2019s repeat the complete question and also think step\nby step.\n58.0(+13.9)\n52.6(+12.9)\n53.3(+23.3)\n51.2(+13.0)\n\u2713\nLet\u2019s reiterate the complete question and also think\nstep by step.\n53.1(+9.0)\n53.6(+13.9)\n53.1(+23.1)\n50.8(+12.6)\n\u2713\nLet\u2019s repeat the complete question and also think step\nby step. \u201c\n51.4(+7.3)\n51.7(+12.0)\n46.3(+16.3)\n48.0(+9.8)\n\u2713\nLet\u2019s restate the complete question and also think\nstep by step.\n51.6(+7.5)\n51.7(+12.0)\n48.1(+18.1)\n50.0(+11.8)\n\u2713\nLet\u2019s summarize the complete question and also think\nstep by step.\n51.4(+7.3)\n52.4(+12.7)\n51.5(+21.5)\n48.3(+10.1)\nTable 11: Code-davinci-002: Reading Comprehension This table compares the performance of the propose\nEchoPrompt?\n\nAQuA-RAT\nDate\nDROP\n(Non-football)\nStandard\n30.3\n49.3\n57.1\nStandard+ Repeat\n29.0\n50.4\n63.3\nCoT\n43.7\n65.6\n69.2\nCoT+ Repeat\n40.9\n67.8\n71.9\nCoT + Compound\n41.3\n68.0\n72.2\nLTM\n-\n-\n66.2\nTable 12: code-davinci-002: EchoPrompt extended resul\n\nDataset\nApproach\n2\n3\n4\n5\n>=6\n2\n3\n4\n5\n>=6\nGSM8K\nCOT\n77.3\n67.6\n56.5\n51.2\n29.1\n81.3\n71.1\n62.2\n61.5\n39.7\nEchoPrompt+CoT\n84.3\n71.8\n60.8\n55.2\n36.4\n84.3\n72.4\n67.2\n64.9\n39.8\nLTM\n78.8\n68.3\n57.8\n55.7\n30.4\n81.9\n74.1\n60.2\n62.6\n41.1\nDROP\nCOT\n-\n66.7\n56.9\n60.5\n76.3\n-\n74.2\n63.2\n69.4\n77.3\n(break)\nEchoPrompt+CoT\n-\n70.4\n59.8\n67.2\n79.3\n-\n71.1\n66.7\n71.6\n78.8\nLTM\n-\n61.7\n63.2\n60.5\n71.8\n-\n62.9\n63.2\n60.5\n72.7\nTable 13: code-davinci-002: EchoPrompt extended results\nable 13: code-davinci-002: EchoPrompt extended results\n\nGSM8k\nSVAMP\nMultiArith\nDROP\n(Non-football)\nDROP\n(Break)\nStandard\n-\n17.0\n67.6\n39.5\n60.5\n56.1\nRepeat\n16.7(\u22120.3)\n72.0(+4.4)\n46.0(+6.5)\n65.9(+4.4)\n63.3(+7.2)\nCompound\n19.2(+2.2)\n72.2(+4.6)\n51.3(+11.8)\n65.7(+4.2)\n61.3(+5.2)\nQuestion First\n18.4(+1.4)\n71.1(+3.5)\n53.1(+12.6)\n60.9(+0.4)\n56.9(+0.8)\nSimple\n18.2(+1.2)\n72.4(+4.8)\n51.6(+12.1)\n64.8(+4.3)\n59.9(+3.8)\nCoT\n-\n66.9\n74.7\n92.8\n75.9\n70.6\nRepeat\n68.2(+1.3)\n75.4(+0.7)\n96.8(+4.0)\n78.1(+2.2)\n72.0 (+1.4)\nCompound\n69.3(+2.4)\n76.4(+1.7)\n95.0(+2.2)\n74.1 (\u22121.8)\n67.9(\u22122.7)\nQuestion First\n68.4 (+1.5)\n76.2(+1.5)\n95.6(+2.8)\n76.9 (+1.0)\n72.2(+1.6)\nSimple\n68.2 (+1.3)\n75.3(+0.6)\n95.3(+2.5)\n77.3(+1.4)\n71.6 (+1.0)\nble 14: Code-davinci-002: Effect of exemplar selection: While Table3 utilized exemplars proposed in (W al., 2023; Zhou et al., 2022a) that showcase simpler reasoning, this table employs exemplars demonstrating long asoning-chains. The results indicate that although EchoPrompt provides higher gains with simpler exempla oosing better exemplars achieves higher overall accuracies, highlighting the significance of exemplar selection\n\nQuery Structure\nGSM8K\nSVAMP\nDROP(Census)\nDROP(Break)\nDROP(football)\nOriginal\n56.8\n37.4\n215.4\n271.8\n319.3\nCompound\n52.1\n37.1\n218.6\n236.9\n294.3\nQuestion First\n47.9\n32.9\n212.2\n212.6\n154.1\nSimple\n56.7\n37.3\n192.2\n237.1\n217.0\nTable 15: Rephrases - Token counts The table presents a comparison of token counts in the model-generated\nble 15: Rephrases - Token counts The table presents a comparison of token counts in the model-genera phrases and the original query. We observe a significant token loss in DROP subsets.\n\nQuery Structure\nGSM8K\nSVAMP\nDROP(Census)\nDROP(Break)\nDROP(football)\nCompound\n70.5\n64.5\n76.6\n70.1\n76.7\nQuestion First\n63.2\n62.8\n92.9\n50.6\n20.8\nSimple\n99.3\n98.9\n74.1\n78.2\n40.3\nTable 16: Rephrases - BLEU Scores The table compares BLEU scores between the model-generated rephrases and\nQuery Structure\nGSM8K\nSVAMP\nDROP(Census)\nDROP(Break)\nDROP(football)\nCompound\n70.5\n64.5\n76.6\n70.1\n76.7\nQuestion First\n63.2\n62.8\n92.9\n50.6\n20.8\nSimple\n99.3\n98.9\n74.1\n78.2\n40.3\nTable 16: Rephrases - BLEU Scores The table compares BLEU scores between the model-generated rephrases and\nthe original query. The BLEU scores for numerical tasks are high, indicating good similarity between the rephrases\nand the original query. However, for reading comprehension tasks, the BLEU scores of the rephrases experience a\nsignificant drop.\nble 16: Rephrases - BLEU Scores The table compares BLEU scores between the model-generated rephrases a e original query. The BLEU scores for numerical tasks are high, indicating good similarity between the rephras d the original query. However, for reading comprehension tasks, the BLEU scores of the rephrases experience\n\n<div style=\"text-align: center;\">mples of queries that lead to repetition, when the language model is prompted to generate 2 repetitions\n</div>\nQuestion\nResponse\nQ: Shiloh is 44 years old today. In 7 years, he will be three\ntimes as old as his nephew. How old is his nephew today?\nA: Rewriting in simple words, the question is: \u201cShiloh is 44 years old\ntoday. In 7 years, he will be three times as old as his nephew. How old\nis his nephew today?\"\nRepeating the question again, the question is: \u201cShiloh is 44 years old\ntoday. In 7 years, he will be three times as old as his nephew. How old\nis his nephew today?\"\nNow, to answer the rewritten question, Shiloh is 44 years old today. In\n7 years, he will be three times as old as his nephew. How old is his\nnephew today?\nShiloh is 44 years old today. In 7 years, he will be three times as old as\nhis nephew. How old is his nephew today?\nShiloh is 44 years old today. In 7 years, he will be three times as old as\nhis nephew. How old is his nephew today?...\nQ: The cell-phone recharges at the rate of 1 percentage-\npoint of charge per 3 minutes. Now, the phone is at 60%\ncharged. How long will it take to fully charge, in hours?\nA: Rewriting in simple words, the question is: \u201cThe cell-phone\nrecharges at the rate of 1 percentage-point of charge per 3 minutes.\nNow, the phone is at 60% charged. How long will it take to fully charge,\nin hours?\"\nRepeating the question again, the question is:\n\u201cThe cell-phone\nrecharges at the rate of 1 percentage-point of charge per 3 minutes.\nNow, the phone is at 60% charged. How long will it take to fully charge,\nin hours?\"\nNow, to answer the rewritten question, The phone is at 60% charged. It\ntakes 3 minutes to charge 1 percentage-point. So it will take 3 minutes\nto charge 1 percentage-point. So it will take 3 minutes to charge 1\npercentage-point...\nQ: Ian has a board that is 40 feet long. He decides to make\na cut so he can have two pieces. The longer piece is 4\ntimes longer than the shorter piece. How long is the longer\npiece?\nA: Rewriting in simple words, the question is: \u201cIan has a board that is\n40 feet long. He decides to make a cut so he can have two pieces. The\nlonger piece is 4 times longer than the shorter piece. How long is the\nlonger piece?\"\nRepeating the question again, the question is: \u201cIan has a board that is\n40 feet long. He decides to make a cut so he can have two pieces. The\nlonger piece is 4 times longer than the shorter piece. How long is the\nlonger piece?\"\nNow, to answer the rewritten question, Ian has a board that is 40 feet\nlong. He decides to make a cut so he can have two pieces. The longer\npiece is 4 times longer than the shorter piece. How long is the longer\npiece?\nThe longer piece is 4 times longer than the shorter piece. So the longer\npiece is 4 x the shorter piece. The shorter piece is the board minus\nthe longer piece. So the shorter piece is 40 - the longer piece. So the\nlonger piece is 4 x (40 - the longer piece).\nSo the longer piece is 4 x (40 - the longer piece).\nSo the longer piece is 4 x (40 - the longer piece).\nSo the longer piece is 4 x (40 - the longer piece).\nQ: Shiloh is 44 years old today. In 7 years, he will be three times as old as his nephew. How old is his nephew today?\n\nQ: The cell-phone recharges at the rate of 1 percentagepoint of charge per 3 minutes. Now, the phone is at 60% charged. How long will it take to fully charge, in hours?\n\nQ: The cell-phone recharges at the rate of 1 percentagepoint of charge per 3 minutes. Now, the phone is at 60% charged. How long will it take to fully charge, in hours?\n\nQ: Ian has a board that is 40 feet long. He decides to make a cut",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving performance in natural language tasks through prompting techniques, focusing on the limitations of existing methods and the necessity for a new approach to enhance in-context learning.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of achieving accurate responses from language models when using prompting techniques, particularly in complex reasoning tasks.",
            "key obstacle": "The main difficulty lies in the tendency of existing prompting methods to produce inaccurate or incomplete answers due to logical errors and the omission of intermediate reasoning steps."
        },
        "idea": {
            "intuition": "The idea of EchoPrompt is inspired by human cognitive strategies, specifically the practice of self-questioning to refine thoughts before arriving at an answer.",
            "opinion": "EchoPrompt entails prompting the model to rephrase its queries before answering them, thereby enhancing the model's ability to process and solve questions more effectively.",
            "innovation": "The primary innovation of EchoPrompt is its two-stage prompting process, which integrates query-rephrasing as a preliminary task to improve the accuracy of language model responses."
        },
        "method": {
            "method name": "EchoPrompt",
            "method abbreviation": "EP",
            "method definition": "EchoPrompt is a prompting strategy that instructs language models to generate a rephrased version of the query before providing an answer, enhancing their reasoning capabilities.",
            "method description": "EchoPrompt improves in-context learning by incorporating a query-rephrasing step that allows models to clarify their understanding of the task.",
            "method steps": [
                "Step 1: Instruct the model to rephrase the query.",
                "Step 2: Use the rephrased query to extract the answer."
            ],
            "principle": "The effectiveness of EchoPrompt stems from its ability to reinforce the model's understanding of the question, leading to improved reasoning and accuracy in responses."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using various datasets for numerical reasoning, reading comprehension, and logical reasoning across multiple language models, including code-davinci-002 and GPT-3.5-Turbo.",
            "evaluation method": "Performance was assessed by comparing the accuracy of EchoPrompt against baseline prompting techniques across different tasks and datasets."
        },
        "conclusion": "The results indicate that EchoPrompt significantly enhances the performance of language models in various reasoning tasks, demonstrating its effectiveness as a new prompting strategy.",
        "discussion": {
            "advantage": "EchoPrompt offers notable advantages over traditional prompting methods by improving accuracy and reasoning capabilities, particularly in complex tasks.",
            "limitation": "One limitation of EchoPrompt is its increased computational requirements due to generating additional tokens during the rephrasing step.",
            "future work": "Future research should explore optimizing the rephrasing process and investigating the applicability of EchoPrompt in other language tasks."
        },
        "other info": {
            "info1": "The paper includes detailed ablation studies to analyze the contributions of different components of EchoPrompt.",
            "info2": {
                "info2.1": "EchoPrompt is adaptable for both zero-shot and few-shot learning scenarios.",
                "info2.2": "The method is designed to be easily integrated into existing prompting frameworks."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of improving performance in natural language tasks through prompting techniques, focusing on the limitations of existing methods and the necessity for a new approach to enhance in-context learning."
        },
        {
            "section number": "1.3",
            "key information": "The primary innovation of EchoPrompt is its two-stage prompting process, which integrates query-rephrasing as a preliminary task to improve the accuracy of language model responses."
        },
        {
            "section number": "3.1",
            "key information": "EchoPrompt improves in-context learning by incorporating a query-rephrasing step that allows models to clarify their understanding of the task."
        },
        {
            "section number": "4.1",
            "key information": "EchoPrompt offers notable advantages over traditional prompting methods by improving accuracy and reasoning capabilities, particularly in complex tasks."
        },
        {
            "section number": "6.2",
            "key information": "One limitation of EchoPrompt is its increased computational requirements due to generating additional tokens during the rephrasing step."
        },
        {
            "section number": "7",
            "key information": "The results indicate that EchoPrompt significantly enhances the performance of language models in various reasoning tasks, demonstrating its effectiveness as a new prompting strategy."
        }
    ],
    "similarity_score": 0.7138460989191959,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/EchoPrompt_ Instructing the Model to Rephrase Queries for Improved In-context Learning.json"
}