{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.02028",
    "title": "Why does in-context learning fail sometimes? Evaluating in-context learning on open and closed questions",
    "abstract": "We measure the performance of in-context learning as a function of task novelty and difficulty for open and closed questions. For that purpose, we created a novel benchmark consisting of hard scientific questions, each paired with a context of various relevancy. We show that counter-intuitively, a context that is more aligned with the topic does not always help more than a less relevant context. This effect is especially visible for open questions and questions of high difficulty or novelty. This result reveals a fundamental difference between the treatment of closeform and open-form questions by large-language models and shows a need for a more robust evaluation of in-context learning on the variety of different types of questions. It also poses a new question of how to optimally select a context for large language models, especially in the context of Retrieval Augmented Generation (RAG) systems. Our results suggest that the answer to this question can be highly application-dependent and might be contingent on factors including the format of the question, the perceived difficulty level of the questions, and the novelty or popularity of the information we seek.",
    "bib_name": "li2024doesincontextlearningfail",
    "md_text": "# Why does in-context learning fail sometimes? Evaluating in-context learning on open and closed questions.\nXiang Li1,2 Haoran Tang2 Siyu Chen2 Ziwei Wang2 Ryan Chen2 Marcin Abram1,3\nDepartment of Physics and Astronomy, University of Southern California, Los Angeles, CA, USA 2Department of Computer Science, University of Southern California, Los Angeles, CA, USA 3Information Sciences Institute, University of Southern California, Los Angeles, CA, USA {limike,haoranta,schen809,zwang476,rchen259,mjabram}@usc.edu\n# Abstract\nWe measure the performance of in-context learning as a function of task novelty and difficulty for open and closed questions. For that purpose, we created a novel benchmark consisting of hard scientific questions, each paired with a context of various relevancy. We show that counter-intuitively, a context that is more aligned with the topic does not always help more than a less relevant context. This effect is especially visible for open questions and questions of high difficulty or novelty. This result reveals a fundamental difference between the treatment of closeform and open-form questions by large-language models and shows a need for a more robust evaluation of in-context learning on the variety of different types of questions. It also poses a new question of how to optimally select a context for large language models, especially in the context of Retrieval Augmented Generation (RAG) systems. Our results suggest that the answer to this question can be highly application-dependent and might be contingent on factors including the format of the question, the perceived difficulty level of the questions, and the novelty or popularity of the information we seek.\narXiv:2407.02028v1\n# 1 Introduction\nDespite their indisputable successes [Bommasani et al., 2021, Drori et al., 2022, Chang et al., 2024] Large Language Models (LLMs) often struggle to answer challenging questions [Rawte et al., 2023] While they can achieve superhuman accuracy on many benchmarks [Luo et al., 2024], they also suffer from hallucinations [Ye et al., 2023, Azamfirei et al., 2023], lack of coherence [Xie et al., 2023b] and are prone to cognitive errors [Jones and Steinhardt, 2022, Hagendorff and Fabi, 2023]. To make the difficult situation even worse, it is not always easy to detect mistakes committed by LLMs since their responses are often presented in a way that emulates correct and coherent answers [Bender et al., 2021, Scheurer et al., 2023]. For practical reasons, many existing benchmarks only test the ability to answer either closed [Chang et al., 2024] or easy-to-verify questions, e.g., regarding common knowledge [Bisk et al., 2020, Clark et al., 2018] or questions that can be algorithmically verified [Srivastava et al., 2024].\nTo make the difficult situation even worse, it is not always easy to detect mistakes committed by LLMs since their responses are often presented in a way that emulates correct and coherent answers [Bender et al., 2021, Scheurer et al., 2023]. For practical reasons, many existing benchmarks only test the ability to answer either closed [Chang et al., 2024] or easy-to-verify questions, e.g., regarding common knowledge [Bisk et al., 2020, Clark et al., 2018] or questions that can be algorithmically verified [Srivastava et al., 2024].\nAnother challenge concerns domain generalization and domain shift problems, resulting in the need to constantly update your machine learning models to account for the evolution of various trends in your data [Zhou et al., 2022]. However, improving the performance of pre-trained LLMs for specific tasks by fine-tuning is both expensive [Bender et al., 2021, Luccioni et al., 2023] and technically\nPreprint. Last modification on July 3, 2024.\nchallenging [Kandpal et al., 2023, Gaspers et al., 2022]. While some techniques like Low-Rank Adaptation (LoRa) can reduce the cost of training [Hu et al., 2021], it does not solve the main issue, namely, how to allow LLMs to leverage new pieces of information that were not a part of the initial training corpus [Liu, 2017]. One approach to the issue might be in-context learning [Brown et al., 2020], where LLMs effectively learn to solve a given problem leveraging a limited number of examples without updating the model parameters. Namely, in-context learning incorporates question-solution pairs in the input prompt, allowing LLMs to detect the logic and patterns of those examples, subsequently improving the LLMs output accuracy. It enables LLMs to acquire new knowledge in the inference time and utilize it in subsequent responses. This technique significantly reduces the complexity of improving the LLMs performance compared to alternative approaches such as fine-tuning [Min et al., 2022b]. It should also be noted that the effectiveness of the popular Retrieval-Augmented Generation (RAG) techniques relies heavily on the strength of in-context learning [Gao et al., 2024], as discussed later. In this paper, we focused on the question of how various types of context improve the effectiveness of in-context learning when answering challenging questions. We noticed a surprising behavior. Namely, depending on the difficulty and novelty of the question, and depending on the fact whether the question is of the open or closed type, the relation of the measured performance of the model to both the perceived and quantified relevancy of the context varies. Notably, the measured in-context learning performance of GPT-4 was positively correlated to context relevancy in two benchmarks with closed-form questions but negatively correlated in our benchmark with open-form questions, indicating different utilization of context depending on the form of the received questions. In the next sections, we introduce our novel dataset, which comprises 160 unique question-response pairs from the fields of physics and computer science with varying levels of difficulty. For the purpose of evaluation, each question is accompanied by one of four types of context (including no context to serve as a control group) and paired with a generated answer from GPT-4. In the subsequent sections, we detail our grading scheme and present the results aggregated from each of our graders. Next, we compare our findings with the existing work by Min et al. [2022b], highlighting a notable discrepancy in the measured effectiveness of the context. To elucidate this difference, we delve deeper into the nature of the problem, discovering that the main impact comes from the open or closed form of the questions, with additional effects related to the difficulty or novelty of those queries. To further strengthen our analyses, we then compare the performance improvement associated with in-context learning across a range of context relevancy using two additional close-ended question datasets, MetaICL [Min et al., 2022a] and NephSAP [Wu et al., 2023b] and we contrast the results with our findings harvested with the help of our open-ended question dataset. Following this, in the discussion section, we discuss the impact of our work, especially in the context of the RAG systems, future research directions, and other methods that enhance LLM performance\nLarge Language Models. LLMs have shown remarkable capabilities in various tasks, including code generation [Kojima et al., 2022, Siddiq and Santos, 2023], text summarization [Sahu et al., 2023], and database query optimization [Li et al., 2023]. They demonstrate a surprising ability to perform in-context learning, where an LLM \u201clearns\u201d to do a task simply by conditioning on a prompt containing input-output examples, achieving state-of-the-art (SOTA) results on various benchmarks. However, there has been little understanding of how the model leverages the context and what makes in-context learning work. In addition, their performance significantly depends on the contextual information provided and, as discussed in this paper, on the form and type of the queries.\nIn-Context Learning. In-context learning has been a focal point in recent research. Unlike traditional fine-tuning methods, in-context learning adapts models to unseen tasks by incorporating examples directly into the input context, as highlighted by Brown et al. [2020]. Xie et al. [2022] discussed how in-context learning can be understood as implicit Bayesian inference, where models infer latent concepts to generate coherent responses. Techniques such as chain-of-thought prompting [Wei et al., 2022, Press et al., 2023, Wang et al., 2022, Zhou et al., 2023, Imani et al., 2023, Besta et al., 2023] have shown significant improvements in reasoning tasks. Recent frameworks like OpenICL\n[Wu et al., 2023b] have further streamlined the implementation of in-context learning by providing unified and flexible tools for integrating various retrieval and inference methods. Many recent research focuses on the example selection strategies of in-context learning. One of the most common strategies is to select examples for demonstration based on similarity in the embedding space [Liu et al., 2022, Qin et al., 2023, Gao et al., 2021]. In-context learning seems robust to label-noise, as indicated by work of Min et al. [2022b], in which authors show that demonstrations, even one with randomly shuffled labels, can still significantly improve LLM\u2019s performance in the MetaICL dataset.\nEvaluation Benchmarks. Benchmarking is essential for understanding LLM performance across different domains. Existing benchmarks like AGIEval [Zhong et al., 2023], ChenLLMBench [Guo et al., 2023], SCIEval [Sun et al., 2023], PIXIU [Xie et al., 2023a], and MME [Fu et al., 2024] provide comprehensive datasets for evaluating LLMs. While these benchmarks are useful for understanding the general capabilities of LLMs, they do not capture the complexity of more open-ended and contextsensitive queries. Here, the added value of our work, as we believe the novel open-question validation set we created, fills that gap.\n# Originality and general impact of the work assessment\nOriginality. In this paper, we argue that closed questions, such as multiple-choice or fill-inthe-blank formats, do not adequately reflect the challenges posed by open questions that require deep understanding and synthesis of information from diverse contexts. While Min et al. [2022b] have shown that context significantly affects LLM performance, they have not quantified how different levels of context relevancy impact responses to different types of questions. Our research addresses this gap by creating a novel benchmark that focuses on open, challenging questions. These questions are paired with various types of contexts to systematically evaluate how context affects LLM performance.\nImpact of the paper. Furthermore, our work suggests areas for improving the performance of Retrieval-Augmented Generation (RAG). Current RAG studies focus on providing context during model inference. Given our observation of the inconsistent relationship between the relevance of context and model performance for different question types (open-form and closed-form), we believe that the context retrieved by comparing vector similarity using RAG may not always correlate with the most useful context for enhancing LLM inference performance and does not mitigate issues such as hallucinations and logic errors. We propose that the type of context selected should be tailored to the attributes of the type of questions with several practical propositions of the retrieval regions outlined in the discussion.\n# 4 Is more relevant context always better?\n# 4.1 Novel question bank and evaluation methodology\nTo investigate the relationship between the relevance of context and the performance of large language models (LLMs), we created an open-form questions dataset comprising physics and computer science questions of varying difficulty levels and originality. Next, we prepared contexts with four different levels of relevancy for each question in our dataset.\nmodels (LLMs), we created an open-form questions dataset comprising physics and computer science questions of varying difficulty levels and originality. Next, we prepared contexts with four different levels of relevancy for each question in our dataset. The selected questions cover the following areas: quantum mechanics, physics for life science, electromagnetism, classical mechanics, and computer science. Solutions usually involve a combination of complex calculations and the application of conceptual knowledge. Each question is categorized under one of the three different difficulty levels: easy, medium, and hard. The difficulty of the question is defined by the grader according to their perceived complexity of the question. Additionally, each question is also categorized under one of three originality categories: known, paraphrased, and original. Known questions can be found online or in textbooks, paraphrased questions are modified versions of known questions, and original questions were handcrafted by the authors of this paper. For each question, we created a ground truth answer for scoring reference and four context types with different levels of relevance. The four context types are: (1) \u201cno context\u201d to serve as a control\ngroup, (2) \u201cirrelevant context\u201d, which consists of text on topics that do not match the subject of the question, (3) \u201cvague context\u201d, which incorporates some topics or keywords related to the question, and (4) \u201crelevant context\u201d, which provides reasoning context for the question, or answer to a highly related question. Next, for each unique pair of question-context, we generated a response employing the OpenAI\u2019s gpt-4-1106-preview model. After retrieving the responses, we constructed 160 question-response pairs, each accompanied by the corresponding ground truth. Aware that human grading can be subjective, we decided that each question would be evaluated by six independent graders using a pre-defined scoring sheet. This gave us 960 evaluation responses in total. The Supplementary Material includes examples of the questions and context types, as well as the evaluation sheets.\n# 4.2 Evaluation\nOur evaluation system comprised three main categories, Completeness and Relevancy (5 points), Logic and Reasoning (5 points), and Truthfulness (understood as lack of hallucination) (5 points).\nIn addition, graders had the option to identify specific problems in the responses, such as hallucinations, omission, irrelevant, calculation error, and logic error. They could also highlight portions of the responses as incorrect, correct, or irrelevant. An open response section was provided for graders to give comments and feedback about the generated responses. Finally, graders were asked to rate their confidence in their own grading. These options allowed us to gain deeper insights into the grading process and to assess the quality of the generated responses in detail. A screenshot of the scoring interface can be found in the Supplementary Material. Each grader may have different biases and varying levels of expertise. To enhance the accuracy and reliability of our evaluation, we ensured that all graders assessed all 160 questions. This approach was essential for obtaining consistent and accurate results. By having multiple graders evaluate each response, we mitigated individual biases and ensured a more comprehensive assessment. This method captured a broader range of perspectives and expertise, leading to a more robust and reliable evaluation of the generated responses. As demonstrated later, this comprehensive grading significantly improved the accuracy and consistency of our findings.\n# 4.3 Results\n# 4.3.1 Context Relevance\nTo illustrate the correlations between the context types and the quality of the corresponding generated responses, in Fig. 1 panel A, we show the raw average scores of each context type for each grader. Notably, the results are rather noisy, with each grader having an individual tolerance for different types of errors, resulting in different reference levels for each of them. By design, each question was evaluated by each grader. This additional redundancy allows us to standardize the scores for each grader and then average them, resulting in reduced variance in the final results. This aggregation procedure is depicted in Fig. 1 panel B. As a result, although the raw scores displayed differences in trends and values across all three grading rubrics, a clear trend appeared after we applied the aggregation procedure, as depicted in Fig. 1 panel C. Counter-intuitively, a higher standardized average score was associated with no context, and the lowest score with the relevant context.\n# 4.3.2 Difficulty Levels and Originality Types\nTo investigate how the difficulty of questions affects the quality of generated responses, we compared the results across three difficulty levels (easy, medium, and hard) for each of the four context types as shown in Figure 2, panel A. We can observe a clear trend of decreasing scores as the difficulty of the questions increased from medium to hard, indicating that GPT-4\u2019s performance declines with higher question difficulty. This also indicates that human-perceived difficulty of the question was in fact, correlated with the factual difficulty experienced by GPT-4, a result interesting on its own. For\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/64d4/64d4735d-458f-48c7-8448-cb1c104b2afc.png\" style=\"width: 50%;\"></div>\nFigure 1: (A) Raw average scores of generated responses for each context type (no context, irrelevant context, vague context, and relevant context) evaluated for Completeness and Relevancy (Correctness), Logic and Reasoning (Logic Score), and Truthfulness (lack of hallucination), assessed by six different graders. (B) The process of standardizing raw scores from each grader to calculate the overall standardized average scores. The raw scores are converted to Z-scores, which are then averaged to obtain standardized average scores. (C) Standardized average scores of generated responses for each context type aggregated across all graders.\neasy and medium-difficulty problems, GPT-4 generated responses with similar scores, indicating that the alignment between the human-perceived and machine-perceived difficulty has its own limits. In Figure 2, panel B, we show the comparison between the aggregated standardized average score for the different levels of originality types for each context type. It is evident that GPT-4 scores highest for known questions, likely because these questions were part of its training data, and therefore GPT-4 has a higher chance to answer them correctly. Interestingly, the score for known questions given irrelevant context is twice as high as that for relevant context. This suggests that irrelevant context might be more helpful than relevant context for known questions, at least for the open type of question, as measured here.\n# 4.3.3 Result comparison\nIn this section, we combined the standardized scores from all graders and compared them across different context types. Our results indicate that, on average, the responses generated with no additional context or with the help of irrelevant context are of higher quality than the responses generated for queries incorporating highly relevant context. This result is in striking difference to results of Min et al. [2022b]. To further understand this discrepancy, in the next section, we replicate the key findings of Min et al. [2022b], and we discuss what might cause the difference in the behavior.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/974d/974d1de7-ebb5-4ac0-92ec-117d2084c123.png\" style=\"width: 50%;\"></div>\nFigure 2: (A): Standardized average scores of generated responses for each context type (no context, irrelevant context, vague context, and relevant context), categorized by three levels of question difficulty (easy, medium, and hard) for correctness, logic errors, and lack of hallucination. (B): Standardized average scores of generated answers for each context type, subdivided into known, paraphrased, and original categories, evaluated for correctness, logic score, and lack of hallucination.\n<div style=\"text-align: center;\">Figure 2: (A): Standardized average scores of generated responses for each context type (no context, irrelevant context, vague context, and relevant context), categorized by three levels of question difficulty (easy, medium, and hard) for correctness, logic errors, and lack of hallucination. (B): Standardized average scores of generated answers for each context type, subdivided into known, paraphrased, and original categories, evaluated for correctness, logic score, and lack of hallucination.</div>\n# 5 Citical comparison with existing study\n# 5.1 Intro\nMin et al. [2022b] demonstrates that in-context learning allows us to achieve significantly better results compared to the \u201cno context\u201d case. In addition, the authors show that in-context learning is robust to label noise. Namely, the authors show that context with randomly shuffled labels and \u201cgolden\u201d context (with correct labels) have similar effects in enhancing the quality of generated responses for closed questions, such as multiple choice and true/false questions. However, to investigate the striking difference in the observed trends and to eliminate the effect of different versions of ChatGPT playing a potential role here, we decided to replicate the key results from Min et al. [2022b] using precisely the same framework as above and using the same version of the LLM, namely gpt-4-1106. For the replication, we decided to use two different existing benchmarks, MetaICL [Min et al., 2022a] and a dataset from NephSAP [Wu et al., 2023a]. The only significant element, differentiating this study from our previous evaluations, is that both of these datasets contain close-form questions.\n# 5.2 Data and Methodology\nOur evaluation of in-context learning of closed-form questions involves two datasets. For the MetaICL dataset, we take a subset of 10 different tasks, each containing multiple-choice questions. For the NephSAP dataset, we take multiple-choice questions within 20 different subjects. Details about tasks, subjects, and sample questions can be found in the Supplementary Materials. We conduct an 80-20 train test split for both the MetalCL dataset and the NephSAP dataset. For each multiple-choice question in the test set, we generate a response using the gpt-4-1106-preview model. We do it three times: once without any context, once with a randomly sampled demonstration with a different task or subject from the training set of the dataset, and once with a randomly sampled demonstration with the same subject or task from the training set. We also compute the embedding of the questions and the demonstrations. We bin the embedding similarity of each demonstration/response pair into separate bins. Treating the no-context response as a benchmark, we record the general score improvement of the response within each embedding similarity bin compared to the raw benchmark.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b25/9b254d5e-ba8a-449a-b0da-16c36bff4d47.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n(a) The MetaICL dataset contains close-form questions. The last bin is insignificant as it contains only 7 samples of data. The relationship between similarity and score improvement is positively correlated.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ea5/4ea5d26b-780a-4165-9884-7db619418983.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) The dataset contains open-form physics questions. The relationship between similarity and score improvement is anti-correlated.</div>\nFigure 3: Comparison of results for different datasets. (a) Results for the MetalCL dataset. (b) Results for the NephSAP dataset. (c) Results for the Open dataset.\nIn Fig. 3, we show the score improvement as a result of different contexts, using the no-context answer as the baseline. Note how context similarity is positively correlated with the mean score improvement in both of the closed-question datasets (MetalICL and NephSAP). This result is consistent with the arguments made by Liu et al. [2021] and Rubin et al. [2022]. Note also that in both closed-question datasets, the context with the lowest levels of similarity scores has a tendency to have a negative mean improvement (meaning, adding context hurts the results). As contexts with low levels of similarities are more likely to be contexts with a different subject or task, this result is consistent with the findings in [Liu et al., 2021], where irrelevant demonstrations can hurt the performance of LLM. This contrasts the results for the closed-form questions, as depicted in Fig. 3, panel C. Our open-form question results display a negative correlation between context similarity and mean improvement. The results suggest that, in this case, context with a lower level of similarity can be more helpful in improving the quality of the response, whereas context with a higher level of similarity can hurt the quality of the response.\nIn Fig. 3, we show the score improvement as a result of different contexts, using the no-context answer as the baseline.\n# 6 Discussion\n# 6.1 Impact of our work and the future directions\nOur results have suggested a significant difference between open-form question evaluation and close-form question evaluation, as the relationship between context-similarity and performance improvement is completely reversed in those two cases. The implications of this result are twofold. First, the difference between open-question evaluation and close-question evaluation invokes a new\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/81ff/81ff40fb-07df-4eba-8028-9732116e752a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n(b) The NephSAP dataset contains close-form questions. The first bin and the last bin are insignificant as they contain only 1 sample each. The relationship between similarity and score improvement is positively correlated.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25b6/25b6729f-712b-4650-8ff8-3f6a4323bd0a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: (A) A typical hypershpere, from which we sample documents in RAG applications. (B) An alternative approach, where we either exclude or at least diminish the impact of contexts that are too close to the point representing the querry.</div>\ndiscussion on their different applicability in the context of in-context learning. Second, those mixed results suggest that similarity score might not be the best indicator for context selection in in-context learning, especially in cases that involve open-form questions. This has profound implications, especially in the context of Retrieval Augmented Generation (RAG) applications. For example, instead of selecting all points that lie in the vicinity of a certain point in the embedding space representing a query (cf. Fig.4, panel A), a better choice could be to either exclude or at least diminish the impact of contexts that are too close to that point (cf. Fig.4, panel B). This would lead to more interesting topologies. Instead of sampling the context from a hypersphere, we could sample from shells of various thicknesses.\n# 6.2 How should we evaluate in-context-learning? Open vs Close\nThe different behaviors exhibited in open-form question evaluation and closed-form question evaluation stem from a different treatment of context in those two cases. We provide a hypothetical interpretation of that mechanism. In closed-form multiple-choice questions, the evaluated language model is treated as a classification model. A relevant demonstration provided as a context can improve the LLM\u2019s performance by aligning it with the correct choice. In open-form questions, the evaluated language model is treated as a generative model, and the response is open-form. Instead of being either correct or incorrect, an open-form response can be anywhere in between. A relevant context provides alignment with one way of approaching the question, but it can also introduce bias, leading to performance degradation instead of improvement.\n# 6.3 How should we select context with respect to RAG\nThe difference between the relationship between context relevancy and performance in open-form and closed-form questions suggests that the RAG is highly application-dependent. For example, the strategy for context retrieval for open-form applications should be different from the strategy used in closed-form applications. It is also important to be mindful when evaluating RAG, as common closedform benchmarks might not be good indicators of RAG\u2019s performance in open-form applications. When designing an RAG, especially in open-form applications, it is important to include some other factors than pure embedding distance or relevancy. Sometimes including a piece of context that is not as close in embedding distance to the question might be helpful as it does not reinforce the hidden bias inside the question.\n# References\nRazvan Azamfirei, Sapna R. Kudchadkar, and James Fackler. Large language models and the perils of their hallucinations. Critical Care, 27(1), March 2023. ISSN 1364-8535. doi: 10.1186/ s13054-023-04393-x. URL http://dx.doi.org/10.1186/s13054-023-04393-x.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432\u20137439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6239. URL https://doi.org/10.1609/aaai. v34i05.6239.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the Opportunities and Risks of Foundation Models, 2021. URL https://arxiv.org/abs/2108.07258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1\u201345, March 2024. ISSN 2157-6912. doi: 10.1145/3641289. URL http://dx.doi.org/10.1145/3641289. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, 2018. URL https://arxiv.org/abs/1803.05457. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti,\nJayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32), August 2022. ISSN 1091-6490. doi: 10.1073/pnas.2123433119. URL http://dx.doi.org/10.1073/pnas. 2123433119. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models, 2024. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 2021. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024. URL https://arxiv.org/abs/2312.10997. Judith Gaspers, Anoop Kumar, Greg Ver Steeg, and Aram Galstyan. Temporal generalization for spoken language understanding. In NAACL 2022, 2022. URL https://www.amazon.science/ publications/temporal-generalization-for-spoken-language-understanding. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. What Can Large Language Models Do in Chemistry? A Comprehensive Benchmark on Eight Tasks, 2023. URL https://arxiv.org/abs/2305.18365. Thilo Hagendorff and Sarah Fabi. Human-like intuitive behavior and reasoning biases emerged in language models \u2013 and disappeared in gpt-4. 2023. doi: 10.1038/s43588-023-00527-x. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical Reasoning using Large Language Models. In Sunayana Sitaram, Beata Beigman Klebanov, and Jason D. Williams, editors, Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track, ACL 2023, Toronto, Canada, July 9-14, 2023, pages 37\u201342. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-INDUSTRY.4. URL https: //doi.org/10.18653/v1/2023.acl-industry.4. Erik Jones and Jacob Steinhardt. Capturing failures of large language models via human cognitive biases. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 11785\u201311799. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/4d13b2d99519c5415661dad44ab7edcd-Paper-Conference.pdf. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 15696\u201315707. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr. press/v202/kandpal23a.html. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs, 2023. URL https:\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs, 2023. URL https: //arxiv.org/abs/2305.03111.\nScience, 11(3):359\u2013361, June 2017. ISSN 2095-2236. doi: 10.1007/s11704-016-6903-6. URL http://dx.doi.org/10.1007/s11704-016-6903-6. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? CoRR, abs/2101.06804, 2021. URL https://arxiv. org/abs/2101.06804. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good incontext examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 2022. Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 24(253): 1\u201315, 2023. URL http://jmlr.org/papers/v24/23-0069.html. Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y\u00e1\u00f1ez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Leyla Loued-Khenissi, Anna Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, and Bradley C. Love. Large language models surpass human experts in predicting neuroscience results, 2024. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In NAACL-HLT, 2022a. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022b. Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, and David Jurgens. Potato: The portable text annotation tool. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2022. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023. Chengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. In-context learning with iterative demonstration selection. CoRR, abs/2310.09881, 2023. Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, and Amitava Das. The Troubling Emergence of Hallucination in Large Language Models \u2013 An Extensive Definition, Quantification, and Prescriptive Remediations, 2023. URL https://arxiv.org/abs/2310.04988. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 191. URL https://aclanthology.org/2022.naacl-main.191. Gaurav Sahu, Olga Vechtomova, and Issam H. Laradji. Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler, 2023. URL https://arxiv.org/ abs/2311.09559. J\u00e9r\u00e9my Scheurer, Mikita Balesni, and Marius Hobbhahn. Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure, 2023. URL https: //arxiv.org/abs/2311.07590.\nScience, 11(3):359\u2013361, June 2017. ISSN 2095-2236. doi: 10.1007/s11704-016-6903-6. URL http://dx.doi.org/10.1007/s11704-016-6903-6. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? CoRR, abs/2101.06804, 2021. URL https://arxiv. org/abs/2101.06804. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good incontext examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 2022. Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 24(253): 1\u201315, 2023. URL http://jmlr.org/papers/v24/23-0069.html. Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y\u00e1\u00f1ez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Leyla Loued-Khenissi, Anna Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, and Bradley C. Love. Large language models surpass human experts in predicting neuroscience results, 2024. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In NAACL-HLT, 2022a. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022b. Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, and David Jurgens. Potato: The portable text annotation tool. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2022. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023. Chengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. In-context learning with iterative demonstration selection. CoRR, abs/2310.09881, 2023. Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, and Amitava Das. The Troubling Emergence of Hallucination in Large Language Models \u2013 An Extensive Definition, Quantification, and Prescriptive Remediations, 2023. URL https://arxiv.org/abs/2310.04988. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 191. URL https://aclanthology.org/2022.naacl-main.191. Gaurav Sahu, Olga Vechtomova, and Issam H. Laradji. Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler, 2023. URL https://arxiv.org/ abs/2311.09559. J\u00e9r\u00e9my Scheurer, Mikita Balesni, and Marius Hobbhahn. Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure, 2023. URL https: //arxiv.org/abs/2311.07590.\nMohammed Latif Siddiq and Joanna C. S. Santos. Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. CoRR, abs/2311.00889, 2023. doi: 10.48550/ARXIV.2311. 00889. URL https://doi.org/10.48550/arXiv.2311.00889. Saurabh Srivastava, Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, and Sooraj Thomas. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap, 2024. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research, 2023. URL https://arxiv.org/abs/2308.13149. Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt pre-trained language models for chain of thought, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo Kao, Fabien Scalzo, and Ira Kurtz. A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology, 08 2023a. Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, and Zhiyong Wu. Openicl: An open-source framework for in-context learning, 2023b. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu: A large language model, instruction data and evaluation benchmark for finance, 2023a. Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia. Ask again, then fail: Large language models\u2019 vacillations in judgment, 2023b. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2022. Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive mirage: A review of hallucinations in large language models, 2023. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models, 2023. URL https://arxiv.org/abs/2304.06364. Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023. Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 1\u201320, 2022. ISSN 1939-3539. doi: 10.1109/tpami.2022.3195549. URL http://dx.doi.org/10.1109/TPAMI. 2022.3195549.\n# Data and Code Availability\nData and code can be found in the following GitHub repository: https://github.com/mikelixiang88/context-matters.git\n# Acknowledgements\nWe would like to take this opportunity to thank Professor Stephan Haas for helpful discussions at the early stage of this project and Anurag Maravi for his engagement during the preliminary stage of the work.\n# Author Contributions\nX.L., H.T., and M.A. contributed to the conceptual design, X.L. and H.T. developed the Python code and conducted the experiments, X.L., H.T., S.C., and M.A. analyzed and interpreted the results. All authors equally contributed to the creation of the novel dataset, M.A. provided supervision and proposed the experiment measuring the impact of the context. All the authors contributed to writing the article.\n# Competing Interests\n# Supplementary Material\n# A Sample question\n# A.1 Sample Question for Open Dataset\nQuestion: Given the wavelength of an electron is 0.364 \u00b7 10\u22129 m, calculate the speed of the electron. Ground Truth for Grading: \u03bb = 0.364 \u00d7 10\u22129 m Mass of electron, m = 9.1 \u00d7 10\u221231 kg Planck\u2019s Constant, h = 6.62607015 \u00d7 10\u221234 Js The de Broglie wavelength is given by \u03bb = h mv Velocity of the electron, v = 2 \u00d7 106 ms\u22121\n# A.2 Sample question for MetaICL dataset\nBird feet can also vary greatly among different birds. Some birds, such as gulls and terns and other waterfowl, have webbed feet used for swimming or floating (Figure below). Other birds, such as herons, gallinules, and rails, have four long spreading toes, which are adapted for walking delicately in the wetlands (Figure below). You can predict how the beaks and feet of birds will look depending on where they live and what type of food they eat. Flightless birds also have long legs that are adapted for running. Flightless birds include the ostrich and kiwi. Some birds, such as gulls and terns and other waterfowl, have what type of feet used for swimming or floating?\n# Test Output: webbed\n# Test Options:\n1. lobed 2. quad toed 3. bipedal 4. webbed\nFor our task selections from the MetaICL dataset, please visit our GitHub repository, where the tas category selections and code are presented.\n# A.3 Sample question for NephSAP dataset\nA 54-year-old man with ESRD is admitted for management of presumed catheter\u2013related bacteremia He had no pre\u2013ESRD nephrology care and recently started maintenance hemodialysis on an urgent basis for symptomatic uremia. Two days ago, he developed acute onset of fever to 40 C, chills and rigors during dialysis. After obtaining blood cultures, he received intravenous vancomycin and ceftazidime at the outpatient dialysis center before admission. His tunneled dialysis catheter is removed because of erythema and purulent exudate at the exit site. His fever is now resolved\nHe does not have back pain, cough, dental complaints, or rash. On physical examination, he is now afebrile. The BP is 136/88 mmHg. There is a 1-cm area of erythema at the tunneled dialysis catheter removal site. He has no stigmata of emboli. He has no obvious dental issues on gross inspection of the oropharynx. The cardiac examination shows a grade 2 early peaking systolic crescendo-decrescendo murmur along the right sternal border without radiation. The remainder of the examination is unremarkable. The blood cultures drawn before implementation of antibiotic therapy grew S. aureus within 12 hours. The results of sensitivity testing and repeat blood cultures are pending. Laboratory studies show sodium 138 mEq/L, potassium 3.8 mEq/L, chloride 102, total CO2 25 mmol/L, BUN 45 mg/dl, and creatinine 6.4 mg/dl. In addition to continued antibiotic therapy, which ONE of the following is the next BEST step in management?\nA. Arrange for placement of a tunneled dialysis catheter today B. Transesophageal echocardiography C. Magnetic resonance imaging of the spine D. Bone scintigraphy (bone scan) E. Examination for an occult dental source of bacteremia\n# B Context types\n# C Generated Answer\n# C.1 Sample Response for Question in A.1\nResponse with No Context: To calculate the speed of the electron, we can utilize de Broglie\u2019s equation that relates the momentum of a particle to its wavelength. Here is the formula:\nWhere:\n\u2022 \u03bb is the wavelength \u2022 h is Planck\u2019s constant \u2022 p is the momentum.\ne momentum of a particle can also be described as its mass times its velo\nWe know that the mass of an electron (m) is 9.11 \u00d7 10\u221231 kg. And Planck\u2019s constant (h) is 6.626 \u00d7 10\u221234 Js. So the velocity (v) is given as:\nSubstituting the known values, we get:\nResponse with Irrelevant Context: The speed of an electron given its wavelength can be calculated using the de Broglie\u2019s equation:\nResponse with Irrelevant Context: The speed of an electron given its wavelength can be calculated using the de Broglie\u2019s equation:\nwhere:\n\u2022 \u03bb is the wavelength, \u2022 h is Planck\u2019s constant, which is roughly equals to 6.626 \u00d7 10\u221234 Js, \u2022 m is the mass of the electron, which is roughly equals to 9.109 \u00d7 10\u221231 kg, \u2022 v is the velocity of the electron.\nWe can rearrange the equation to solve for the velocity:\nSubstitute the given values into the equation:\nand calculate the velocity.\nResponse with Vague Context: The wavelength of a matter wave, such as an electron, can be found using De Broglie\u2019s equation, which is given by:\nwhere:\n\u2022 \u03bb is the wavelength \u2022 h is Planck\u2019s constant (6.626 \u00d7 10\u221234 Js) \u2022 p is the momentum of the particle, which can be expressed as p = mv\n\u2022 \u03bb is the wavelength \u2022 h is Planck\u2019s constant (6.626 \u00d7 10\u221234 Js) \u2022 p is the momentum of the particle, which ca\nSince we want to find the speed of the electron (v), we can rearrange this equation in terms of v\nGiven that \u03bb = 0.364 \u00d7 10\u22129 m and the mass of an electron is m = 9.109 \u00d7 10\u221231 kg, we ca substitute these values into the equation:\n \u00d7 \u00d7 Evaluating this will give the speed of the electron in m/s.\nResponse with Relevant Context: The De Broglie equation relates the wavelength of a particle to its momentum. According to De Broglie\u2019s principle, every object in motion has a wave character. The De Broglie wavelength is given by the formula:\nwhere:\n\u2022 \u03bb is the wavelength, \u2022 h is Planck\u2019s constant (6.626 \u00d7 10\u221234 m2 \u00b7 kg/s), \u2022 m is the mass of the particle,\nWe are given that \u03bb = 0.364 \u00d7 10\u22129 m, and the mass m of an electron is 9.11 \u00d7 10\u221231 kg. We can find the speed v by rearranging the above formula:\nSubstitute the known values into the rearranged equation to find v, the speed of the electron:\nSolving the above expression will give us the speed of the electron.\n# D Grading Interface\nTo better evaluate the open response to our physics questions, we modified the potato annotation system [Pei et al., 2022] and applied it as our evaluation system. Our evaluation system not only allows users to select numeric grades for each response but also enables the user to highlight parts of the response, apply labels, and write descriptions to justify their grading. In addition, the system randomly shuffles the order of the responses for each grader to mitigate any potential bias in grading as a result of the ordering of responses. A short video tutorial is provided at the beginning page to provide guidance and alignment in grading. A screenshot of the interface of the evaluation system is shown in Fig. 5. The system is also accessible via the link: http://quantumgpt.science:8080/?PROLIFIC_PID=testuser.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/831f/831f5a3f-61f2-4c07-b20c-c184507354f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: The potato grading interface used in evaluation</div>\nTo check whether our context relevancy is well defined, we compute the embedding of the questions and their respective contexts for both our open-form question dataset and the two closed-form question datasets we use. We then calculate the cosine distance between the embedding of each question and the different contexts associated with them. We show the results for the open question dataset in Fig. 6. We computed the embedding of each question and each context using OpenAI\u2019s \u201ctext-embedding-3large\u201d model. For the no-context part, we used a space as a placeholder instead of an empty string. As expected, the results show that more relevant contexts, as perceived by us when designing the dataset, receive a higher mean similarity score with their respective questions. Different question types can result in a large standard deviation in similarity scores in different contexts. We show the details breakdown of those results in Fig. 7. All question types except hard paraphrased questions display the same trend, confirming the relationship between context types and embedding similarities.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed34/ed341b18-1cd3-4e6f-8734-fdfbad585bf0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Mean context similarity by context type.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fb3e/fb3ea0b6-029d-49d5-9b84-edfa777771d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">: Mean context similarity by context type across different originality and</div>\nFor the closed datasets, the similarity score between context and question is shown in Table 1. For both datasets, the same task/subject demonstrations possess a higher mean similarity score than the different task/subject demonstrations. To further verify this relationship, we have also plotted the similarity score of the same task demonstrations and different task demonstrations for each task in the MetaICL dataset in Fig. 8. The results confirm that the same task demonstration displays higher mean similarity than the different task demonstration in every task in the dataset.\n<div style=\"text-align: center;\">Table 1: Mean context similarity for closed datasets</div>\nDataset\nAverage Different Task Similarity\nAverage Same Task Similarity\nMetaICL\n0.719\n0.787\nNephSAP\n0.443\n0.557\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6658/66581efc-ebf9-4d48-8419-82809419ae85.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Mean context similarity by demonstration type across different MetaICL tasks.</div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Large Language Models (LLMs) often struggle to answer challenging questions, especially in open-ended formats, and existing benchmarks primarily focus on closed or easily verifiable questions. This creates a gap in understanding how LLMs perform in more complex, context-sensitive scenarios.",
            "purpose of benchmark": "The benchmark is intended to evaluate the performance of in-context learning in answering hard scientific questions, particularly focusing on the effects of context relevancy and question format (open vs closed)."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of evaluating LLMs on open-form questions that require deep understanding and synthesis of information from diverse contexts.",
            "key obstacle": "Existing benchmarks do not adequately capture the complexity of open-ended questions, leading to a lack of understanding of LLM performance in these scenarios."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the need to investigate the relationship between context relevancy and LLM performance, particularly for open-form questions.",
            "opinion": "The authors believe that this benchmark is crucial for advancing the understanding of in-context learning and improving LLM performance in real-world applications.",
            "innovation": "This benchmark differs from previous ones by focusing specifically on open-form questions and systematically varying the relevancy of context provided to the LLM.",
            "benchmark abbreviation": "ICL-Benchmark"
        },
        "dataset": {
            "source": "The dataset was created by compiling 160 unique question-response pairs from the fields of physics and computer science, each paired with varying levels of context relevancy.",
            "desc": "The dataset includes questions categorized by difficulty (easy, medium, hard) and originality (known, paraphrased, original).",
            "content": "The dataset contains scientific questions that require complex calculations and conceptual knowledge, along with the corresponding answers generated by GPT-4.",
            "size": "160",
            "domain": "Physics",
            "task format": "Open Question"
        },
        "metrics": {
            "metric name": "Completeness and Relevancy, Logic and Reasoning",
            "aspect": "Accuracy and coherence of responses generated by LLMs.",
            "principle": "The metrics were chosen to evaluate the depth of understanding and reasoning capabilities of LLMs when responding to complex questions.",
            "procedure": "Responses were graded by independent evaluators using a predefined scoring scheme that assessed various aspects of the generated answers."
        },
        "experiments": {
            "model": "GPT-4",
            "procedure": "The models were tested by generating responses to the dataset questions under different context relevancy conditions, evaluated by multiple graders.",
            "result": "Responses with no context or irrelevant context scored higher on average than those with highly relevant context, indicating counterintuitive performance trends.",
            "variability": "Variability in results was accounted for by having multiple graders evaluate each response, which mitigated individual biases."
        },
        "conclusion": "The benchmark revealed that the relationship between context relevancy and model performance is complex and varies significantly between open and closed questions, suggesting that context selection strategies need to be reconsidered.",
        "discussion": {
            "advantage": "The benchmark provides a novel approach to understanding LLM performance in open-ended contexts, contributing valuable insights to the field.",
            "limitation": "The reliance on human grading may introduce subjectivity, and the benchmark's applicability to other domains remains to be explored.",
            "future work": "Future research should focus on developing context selection strategies tailored to different question types and further investigating the implications of context relevancy in LLM applications."
        },
        "other info": {
            "data availability": "Data and code can be found in the GitHub repository: https://github.com/mikelixiang88/context-matters.git"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning is evaluated through a benchmark focusing on answering hard scientific questions, highlighting the necessity of understanding complex, context-sensitive scenarios."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark aims to improve the understanding of in-context learning by evaluating LLMs on open-form questions that require deep synthesis of information."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark reveals that LLMs exhibit counterintuitive performance trends, where responses with no context or irrelevant context scored higher than those with highly relevant context."
        },
        {
            "section number": "3.2",
            "key information": "The relationship between context relevancy and model performance is complex and varies significantly between open and closed questions."
        },
        {
            "section number": "5.1",
            "key information": "The dataset includes 160 unique question-response pairs in the fields of physics and computer science, categorized by difficulty and originality, showcasing applications in scientific domains."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark's reliance on human grading may introduce subjectivity, presenting a challenge in evaluating LLM performance."
        },
        {
            "section number": "6.4",
            "key information": "Future research should focus on developing context selection strategies tailored to different question types, addressing scalability and applicability challenges."
        }
    ],
    "similarity_score": 0.7197538184085641,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Why does in-context learning fail sometimes_ Evaluating in-context learning on open and closed questions.json"
}