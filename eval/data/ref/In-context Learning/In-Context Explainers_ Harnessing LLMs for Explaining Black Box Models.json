{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.05797",
    "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models",
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in complex tasks like machine translation, commonsense reasoning, and language understanding. One of the primary reasons for the adaptability of LLMs in such diverse tasks is their in-context learning (ICL) capability, which allows them to perform well on new tasks by simply using a few task samples in the prompt. Despite their effectiveness in enhancing the performance of LLMs on diverse language and tabular tasks, these methods have not been thoroughly explored for their potential to generate post hoc explanations. In this work, we carry out one of the first explorations to analyze the effectiveness of LLMs in explaining other complex predictive models using ICL. To this end, we propose a novel framework, In-Context Explainers, comprising of three novel approaches that exploit the ICL capabilities of LLMs to explain the predictions made by other predictive models. We conduct extensive analysis with these approaches on real-world tabular and text datasets and demonstrate that LLMs are capable of explaining other predictive models similar to state-of-the-art post hoc explainers, opening up promising avenues for future research into LLM-based post hoc explanations of complex predictive models.",
    "bib_name": "kroeger2024incontextexplainersharnessingllms",
    "md_text": "# In-Context Explainers: Harnessing LLMs for Explaining Black Box Models\nNicholas Kroeger1*, Dan Ley2*, Satyapriya Krishna2, Chirag Agarwal2, and Himabin Lakkaraju2\n# Nicholas Kroeger1*, Dan Ley2*, Satyapriya Krishna2, Chirag Agarwal2, and Himabindu Lakkaraju2\n1 University of Florida 2 Harvard University\n*Equal contribution. Corresponding author: Nick Kroeger (nkroeger@ufl.edu)\nAbstract. Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in complex tasks like machine translation, commonsense reasoning, and language understanding. One of the primary reasons for the adaptability of LLMs in such diverse tasks is their in-context learning (ICL) capability, which allows them to perform well on new tasks by simply using a few task samples in the prompt. Despite their effectiveness in enhancing the performance of LLMs on diverse language and tabular tasks, these methods have not been thoroughly explored for their potential to generate post hoc explanations. In this work, we carry out one of the first explorations to analyze the effectiveness of LLMs in explaining other complex predictive models using ICL. To this end, we propose a novel framework, In-Context Explainers, comprising of three novel approaches that exploit the ICL capabilities of LLMs to explain the predictions made by other predictive models. We conduct extensive analysis with these approaches on real-world tabular and text datasets and demonstrate that LLMs are capable of explaining other predictive models similar to state-of-the-art post hoc explainers, opening up promising avenues for future research into LLM-based post hoc explanations of complex predictive models.\narXiv:2310.05797v4\n# 1 Introduction\nLarge Language Models (LLMs) have become ubiquitous across various industries and are being increasingly employed for a large range of applications, including language understanding [1], genomics [2], tabular medical data records [3, 4], and drug discovery [5]. While LLMs attain high performance and generalization capabilities for numerous tasks [6], they have also increased their parameter sizes and the computational costs for additional fine-tuning on new downstream tasks. To alleviate this, recent works have shown that LLMs can learn new tasks using in-context learning (ICL), which allows them to perform well on new tasks by simply using a few task samples in the prompt [7]. In-context learning allows language models to dynamically understand, adapt, and generate responses based on the immediate context provided in the prompt, eliminating the need for expensive retraining or fine-tuning. This capability of LLMs enables them to adapt to diverse applications. By generalizing from patterns within the input, ICL supports problem solving and complex reasoning, offering a powerful tool for LLMs [8]. Despite its effectiveness in enhancing the performance of LLMs, the ability of in-context learning for data interpretation and analysis remains underexplored. In particular, there is very little work on systematically analyzing the potential of using in-context learning to explain the behavior of other complex predictive models. Thus, the potential of LLMs to serve as reliable explainers and enhance the understanding of predictive models remains an open question. Present work. In this work, we investigate whether in-context learning can help LLMs explain the behavior of other complex predictive models? (see Fig. 1). To answer this question, we propose a novel framework called In-Context Explainers. This framework comprises two broad in-context learning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/07ea/07ea9cf0-3f00-4586-acf9-e030b23a2496.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: Overview of the in-context explanation generation and evaluation process. Given a dataset and a model to explain, we introduce novel ICL strategies to generate explanations of model predictions using LLMs. The resulting LLM-based explanations are then parsed, and their faithfulness is evaluated using diverse metrics.</div>\nstrategies: perturbation-based and explanation-based ICL, which provide the foundation for exploring the explanation-generating capabilities of state-of-the-art LLMs. Perturbation-based ICL builds on the ideas from explainable AI literature which involve constructing local linear approximations [9] to explain the predictions made by a given ML model. To this end, we provide instance perturbations and their corresponding model outputs in the prompt to the LLM and ask the LLM to explain (i.e., output the top features driving) these predictions. We consider a couple of variants under this strategy, namely, Perturb ICL (P-ICL) and Perturb+Guide ICL (PG-ICL). For P-ICL, we prompt the LLM to use its chain-of-thought reasoning. For PG-ICL, we add detailed guidelines in the prompt for generating explanations. In our Explain ICL strategy, we prompt the LLM with a small, random selection of model input-output pairs and their respective explanations (generated using a state-of-the-art post hoc explanation method) to generate explanations for new samples. We analyze the effectiveness of LLMs in explaining other predictive models on eight real-world datasets, four black box models, and three Gpt models. Our extensive investigation reveals the following key findings: 1 ICL enables LLMs to generate faithful explanations that are on par with several state-of-the-art post hoc explanation methods (despite some of these methods having access to the underlying black box model); 2 our proposed Explain ICL prompting strategy allows LLMs to mimic the behavior of six state-of-the-art explanation methods; 3 On average across five tabular datasets, we find that providing the LLM with detailed guidelines shows more consistent and higher faithfulness scores than simply providing generic instructions; 4 Our results with three text datasets show that ICL capabilities aid LLMs to identify most important words in sentiment classification tasks. Finally, our exploration highlights LLMs\u2019 effectiveness as post hoc explainers, paving the way for future research on LLM-generated explanations.\n# 2 Related Work\nOur work lies at the intersection of post hoc explanations and LLMs, which we discuss below. Post Hoc Explanations. The task of understanding model predictions has become increasingly intricate with the growing popularity of complex ML models [10] due to their inherent black box nature, which makes it difficult to interpret their internal reasoning. To this end, a plethora of feature attribution methods (commonly referred to as post hoc explanation methods) have been proposed to provide explanations for these models\u2019 predictions. These explanations are predominantly presented in the form of feature attributions, which highlight the importance of each input feature on the model\u2019s prediction. Broadly, post hoc explainers can be divided into perturbation-based and gradient-based methods. While perturbation-based methods [9, 11, 12] leverage perturbations of the given instance to construct an interpretable approximation of the black box model behavior, gradient-based methods [13, 14] leverage gradients w.r.t. the given instance to explain model predictions. In this work, we primarily focus on state-of-the-art local post hoc explainers, i.e., methods explaining individual feature importance for model predictions of individual instances. Large Language Models. LLMs have seen exponential growth in recent years, both in terms of their size and the complexity of tasks they can perform [15]. Recent advances in LLMs are changing\n# Using the Perturbed Sample x\u2032 Input: A: 0.430, B: -0.001, C: -0.196, D: -0.139, E: 0.050, F: 0.987 Output: 0 # Using the Raw Perturbation \ud835\udeff Change in Input: A: 0.316, B: -0.112, C: -0.200, D: -0.139, E: 0.050, F: -0.013 Change in Output: 1\nFig. 2: Sample serialization template for the Recidivism dataset with six features.\nthe paradigm of NLP research and have led to their widespread use across applications spanning machine translation [16], question-answering [1], text generation [15], and medical data records [3, 4]. In this work, we, for the first time, explore their use in explaining other predictive models.\n# 3 Our Framework: In-Context Explainers\nHere, we describe our proposed prompting strategies to generate natural language explanations from LLMs that can explain the behavior of predictive models. We will first discuss the notation used to describe the prompting templates and proceed to detail the prompts in Secs. 3.1-3.2. Notation. Let \ud835\udc53: R\ud835\udc51\u2192[0, 1] denote a black box ML model trained on a tabular data that takes an input x \u2208R\ud835\udc51and returns the probability of x belonging to a class \ud835\udc50\u2208C and the predicted label \ud835\udc66. Following previous works in XAI [9, 13], we randomly sample points from the local neighborhood N\ud835\udc65of the given input x to generate explanations, where N\ud835\udc65= N (0, \ud835\udf0e2) denotes the neighborhood of perturbations around x using a Normal distribution with mean 0 and variance \ud835\udf0e2.\n# 3.1 Perturb ICL\nExisting post hoc explainers, such as those relying on a large number of neighborhood samples [9, 13], often encounter computational bottlenecks. In contrast, we explore the utility of our proposed perturbation-based ICL strategy that leverages the power of LLMs in explaining the behavior of predictive models efficiently. In addition, unlike post hoc explainers like LIME [9], SmoothGrad [13], and SHAP [11], LLMs have the potential to produce natural language explanations that are more plausible and coherent to human practitioners. We aim to explore this by utilizing LLMs to articulate the top-\ud835\udc58most important features in determining the output of a given model \ud835\udc53in a rank-ordered manner. In particular, we sample input-output pairs from the neighborhood N\ud835\udc65of x and generate their respective strings following a serialization template. For instance, let a neighborhood sample for x = [0.114, 0.111, 0.004, 0] be x\u2032 = [0.430, \u22120.001, \u22120.196, \u22120.139], where x\u2032 = x + \ud835\udeffand \ud835\udeff= [0.316, \u22120.112, \u22120.200, \u22120.139], and assume the model predictions for x and x\u2032 are 1 and 0, respectively. Considering a binary classification problem, the corresponding change in the model predictions then belongs to {\u22121, 0, 1}. Next, we provide examples of transforming the perturbed sample (x\u2032) and the perturbation (\ud835\udeff) into a natural-language string for our prompt templates. For brevity, in the following sections, we will use the perturbed sample in our template for describing our prompting strategies but explore both in our experiments. Motivated by the local neighborhood approximation works in XAI, the Perturb ICL prompting strategy presumes that the local behavior of model \ud835\udc53is a simple linear decision boundary, contrasting with the often globally exhibited complex non-linear decision boundary. Hence, assuming a sufficient number of perturbations in N\ud835\udc65, the LLM is expected to accurately approximate the black box model\u2019s behavior and utilize this information to identify the top-\ud835\udc58most important features. In addition, to alleviate the computational problems of post hoc explainers, we explore using a small number of \ud835\udc5bICL samples (16 in our experiments) from N\ud835\udc65in our prompting templates for LLMs to generate\nFig. 3: A sample prompt generated using our proposed Perturb ICL (P-ICL) prompting strategy.\nexplanations. For samples in N\ud835\udc65, we select those with the highest predictive confidence by the underlying ML model, helping the LLM produce explanations centered on model certainty. In our study, we explore two paradigms of generating the explanations: i) Perturb ICL ( PICL), and ii) Perturb+Guide ICL ( PG-ICL), which we will describe next using the local neighborhood perturbations discussed above. Both of our strategies use four distinct steps in their template: Context, Dataset, Question, and Instructions. In Context, we provide a general background of the underlying ML model, the number of features in the dataset, the number of classes, and the model predictions. In Dataset, we leverage the success of ICL and provide a list of inputs and their respective model outputs using randomly sampled points from the local neighborhood of a given test input. In Question, we specify the task we want the underlying LLM to perform. Finally, in the Instructions, we enumerate the guidelines we want the LLM to follow while generating the output explanations. For P-ICL, we only prompt the LLM to leverage its chain-of-thought (CoT) reasoning abilities using \u201cThink about the question\u201d before generating the response, but for PG-ICL, we prompt the model with a detailed guideline of how to think about the given problem. P-ICL. Given the test sample x to be explained, we combine the Context of the predictive model, the Dataset of \ud835\udc5bICL input-output pairs from N\ud835\udc65, the Question, and the general Instructions in our prompt to explore the effectiveness of the LLM in generating explanations. In Figure 3, we provide a sample prompting template for the P-ICL strategy. PG-ICL. The PG-ICL prompting strategy transitions from specifying general instructions in the prompt to providing detailed guidance on the strategy for task execution. Rather than solely instructing the LLM to \u201cThink about the question\u201d of what the task entails, this strategy delineates how to conduct the given task. The objective remains to explore the effectiveness of LLMs in explaining the behavior of other predictive models by identifying the top-\ud835\udc58most important features. However, with step-by-step guidelines, we aim to induce a more structured and consistent analytical process within the LLM to generate more faithful explanations. We follow a similar prompting template as in Method 1, including the four components, viz., Context, Dataset, Question, and Instructions, but we modify the Instructions component. The PG-ICL prompt template is provided in Figure 4.\n# PG-ICL prompt template Context: \u201cWe have . . . outputs.\u201d Dataset: Input: . . .\nQuestion: \u201cBased on . . . output?\u201d Instructions: \u201cFor each feature, starting with \u2018A\u2019 and continuing to \u2018F\u2019: 1. Analyze the feature in question. Rate the importance of the feature in determining the output on a scale of 0-100, considering both positive and negative correlations. Ensure to give equal emphasis to both positive and negative correlations and avoid focusing only on absolute values. 2. After analyzing the feature, position it in a running rank compared to the features already analyzed. For instance, after analyzing feature \u2018B\u2019, determine its relative importance compared to \u2018A\u2019 and position it accordingly in the rank (e.g., BA or AB). Continue this process until all features from \u2018A\u2019 to \u2018F\u2019 are ranked. After explaining your reasoning, provide your answer as the final rank of features from \u2018A\u2019 to \u2018F\u2019 from most important to least important, in descending order, separated by commas. Only provide the feature names on the last line. Do not provide any further details on the last line.\u201d # LLM Response: To determine the most important features, we need to . . . . . .\n# LLM Response: To determine the most important features, we need to . . . . . . B, A, C, F, D, E\nFig. 4: A sample prompt generated using the Perturb+Guide ICL (PG-ICL) prompting strategy. Note that the Context, Dataset, and Question are the same as in Fig. 3.\nHere, we provide some detailed guidelines to the LLM for understanding the notion of important features and how to analyze them through the lens of correlation analysis. To achieve this, we instruct LLMs to study each feature sequentially and ensure that positive and negative correlations are equally emphasized. The LLM assigns an importance score to each feature in the given dataset and then positions it in a running rank. This rank encourages the LLM to differentiate features and avoid ties in its evaluation of feature importance. The final line in the template ensures that the LLM\u2019s responses are strictly analytical, minimizing non-responsiveness or digressions.\n# 3.2 Explain ICL\nRecent studies show that LLMs can learn new tasks through ICL, enabling them to excel in new downstream tasks by merely observing a few instances of the task in the prompt. Unlike Perturb ICL (Sec. 3.1), which samples input-output pairs from the neighborhood N\ud835\udc65of a test sample x for ICL prompts, the Explain ICL (E-ICL) strategy uses random input-output-explanation triplets generated from a held-out dataset for prompting. Here, the explanations used in the ICL are generated by any post hoc explanation method as a ground truth. Intuitively, we explore whether an LLM can learn the behavior of a given post hoc explanation method by looking at some of its generated explanations. For constructing the ICL set, we randomly select \ud835\udc5bICL input instances XICL from the ICL split of the dataset and generate their predicted labels yICL using the given predictive model \ud835\udc53. Next, we generate explanations EICL for samples (XICL, yICL) using any post hoc explainer. Using the above input-output-explanation triplets, we construct a prompt by concatenating them to form the prompt in Figure 5. The E-ICL prompting strategy explores how LLMs can produce faithful explanations by analyzing the \ud835\udc5bICL input-output-explanations generated by state-of-the-art post hoc explainer.\n# Explain ICL Prompt Template Input: A: 0.172, B: 0.000, C: 0.000, D: 1.000, E: 0.000, F: 0.000 Output: 1 Explanation: A,C,B,F,D,E . . . Input: A: 0.052, B: 0.053, C: 0.073, D: 0.000, E: 0.000, F: 1.000 Output: 0 Explanation: A,B,C,E,F,D Input: A: 0.180, B: 0.222, C: 0.002, D: 0.000, E: 0.000, F: 1.000 Output: 0 Explanation:\n# 4 Experimental Evaluation\nNext, we evaluate the effectiveness of LLMs as post hoc explainers, focusing on three key questions: Q1) Can LLMs generate post hoc explanations for predictive models trained on tabular datasets? Q2) How well can LLMs generate explanations for sentiment classifiers? Q3) What impact do variations in the LLM\u2019s prompting strategy have on the faithfulness of explanations?\n# 4.1 Datasets and Experimental Setup\nWe first describe the datasets and models used to study the reliability of LLMs as post hoc explainers and then outline the experimental setup. Datasets. For tabular datasets, we follow previous LLM works [17] and perform analysis on five realworld tabular datasets: Blood [18], Recidivism [19], Adult [20], Credit [21], and HELOC [22]. For text datasets, we use three sentiment datasets: Amazon reviews, IMDb, and Yelp [23]. The datasets come with a random train-test split, and we further divide the train set, allocating 80% for training and the remaining 20% for ICL sample selection, as detailed in Sec. 3.2. See Appendix A.1 for more details. Predictive Models. For the tabular dataset classifiers, we consider three ML models with varying non-linearity in our experiments: Logistic Regression (LR), a three-layer Artificial Neural Network (ANN), and a six-layer ANN. For the sentiment classifier, we train a single layer transformer model for the text datasets to show the generalization capability of our analysis in generating explanations for diverse predictive models. We use PyTorch [24] to implement the LR, ANN, and transformer model. Please refer to Appendix A.1 for architecture details and Tables 2 and 3 for predictive performances of these models. Large Language Models. We consider Gpt-3.5, Gpt-4, and Gpt-4-0125-preview as LLMs for all experiments. All experiments default to using Gpt-4, unless stated otherwise. Baseline Methods. For the tabular dataset classifiers, we use six post hoc explainers as baselines to investigate the effectiveness of explanations generated using LLMs: LIME [9], SHAP [11], Vanilla Gradients [12], SmoothGrad [13], Integrated Gradients [14], and Gradient x Input (ITG) [25]. For text classifiers, we cannot directly retrieve gradient-based attributions at the input layer since the model uses an embedding layer with discrete vocabulary inputs. Hence, we compute attributions at the embedding layer of each token, where the core idea here is to use a layer-wise attribution method to compute how much each dimension of the embedding vector contributes to the final prediction. Finally, we sum and normalize attributions across all dimensions of each token\u2019s embedding vector, resulting in a single scalar value per token. The six gradient-based post hoc explainers for the text classifiers are Layer Integrated Gradients (LIG), Layer Gradient SHAP (LGS), Layer Deep\nLift (LDL), Layer Gradient x Activation (LGxA), Layer Activation (LA), and Layer Conductance (LC) [26] from the Captum python library [27]. Performance Metrics. We employ four distinct metrics to measure the faithfulness of an explanation. We use the Feature Agreement (FA) and Rank Agreement (RA) metrics introduced in [28] that compares the LLM\u2019s top-\ud835\udc58directly with the LR model\u2019s coefficient (see Appendix A.1 for details). The FA and RA metrics range from [0, 1], where 0 means no agreement and 1 means full agreement. In the absence of a top-\ud835\udc58model coefficient (as is the case with ANNs), we use the Prediction Gap on Important feature perturbation (PGI) and the Prediction Gap on Unimportant feature perturbation (PGU) metrics from OpenXAI [29]. While PGI measures the change in prediction probability that results from perturbing the features deemed as influential, PGU examines the impact of perturbing unimportant features. Here, the perturbations are generated using Gaussian noise N (0, \ud835\udf0e2). For text datasets, we adapt the PGI and PGU metrics to evaluate the faithfulness of explanations by measuring the impact of perturbing the top-\ud835\udc58important and unimportant words respectively, as identified by the explanation model. The perturbations are implemented by masking the specified words rather than using Gaussian noise, which is more suited for continuous data-types. These modified metrics for text provide insights into how specific words influence the model\u2019s predictions. All reported values use the area under the curve (AUC) of the faithfulness score evaluated from \ud835\udc58= 1 to \ud835\udc58= 3, as a default setting, unless otherwise stated. Implementation Details. To generate perturbations for each tabular dataset\u2019s ICL prompt, we use a neighborhood size of \ud835\udf0e=0.1 and generate local perturbation neighborhoods N\ud835\udc65for each test sample x. We sample n\ud835\udc65=10, 000 points for each neighborhood, where the values for \ud835\udf0eand n\ud835\udc65were chosen to give an equal number of samples for each class, whenever possible. For the text dataset\u2019s ICL prompt, we generate neighborhood sentences by randomly selecting a subset of words to omit, aiming to simulate a range of linguistic variations that enable sensitivity analysis of word-level changes in input sentences. As shown in Figure 2, we present the ICL set in two main formats: 1) as the perturbed sample (x\u2032) and its corresponding predicted output, or 2) as the perturbation (\ud835\udeff) around a sample (x) and the corresponding change in output. Note that the perturbation (\ud835\udeff) format for the ICL set is used as a default setting in all experiments unless otherwise stated. Additionally, both of these formats are absent from Sec. 3.2, which uses test samples directly and does not compute perturbations. For the LLMs, we use OpenAI\u2019s text generation API with a temperature of \ud835\udf0f= 0 for our main experiments. To evaluate the LLM explanations, we extract and process its answers to identify the top-\ud835\udc58most important features. We first save each LLM query\u2019s reply to a text file and use a script to extract the listed features. We added explicit instructions like \u201c. . . provide your answer as a feature name on the last line. Do not provide any further details on the last line.\u201d to ensure reliable parsing of LLM outputs. In rare cases, the LLM won\u2019t follow our requested response format or it replies with \u201cI don\u2019t have enough information to determine the most important features.\u201d See Appendix A.1 for further details.\n# 4.2 Results\nNext, we discuss our results that answer key questions highlighted at the beginning of this section about LLMs as post hoc explainers (Q1-Q3). 1) ICL strategies can identify important tabular features. We compare the proposed prompting based LLM explanation strategies, namely P-ICL, PG-ICL, and E-ICL to existing post hoc explainers on the task of identifying important features for understanding different predictive models for tabular and text classification datasets. For the ANN model, the LLM-based explanations perform on par with post hoc explainers (despite having white-box access to the underlying predictive model or training surrogate linear model). We observe that LLM explanations, on average, achieve 50.13% lower PGU and 144.82% higher PGI than ITG, SHAP, and random baselines for larger datasets (more number of features) like Adult and Credit compared to 27.49% lower PGU and 20.67% higher\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6997/69970437-f500-41fb-a780-bc9ac44d9d6c.png\" style=\"width: 50%;\"></div>\nFig. 6: Top: FA (left) and RA (right) scores of explanations generated using post hoc explainers and GPT-4 (P-ICL, PG-ICL, and E-ICL strategies) for an LR model. Center: PGU (left) and PGI (right) scores for a three-layer ANN model. Bottom: PGU (left) and PGI (right) scores for a six-layer ANN. On average, across four datasets and three predictive models, ICL strategies demonstrate non-trivial post hoc explanation capabilities: E-ICL explanations (with in-context examples selected from LIME) match the faithfulness of gradient-based/LIME methods; P-ICL and PG-ICL explanations achieve more faithful scores than ITG and SHAP methods, while demonstrating similar faithfulness to LIME16 (i.e., given the same number of input perturbations).\nPGI for Blood and Recidivism datasets. The improved performance of LLM explanations in datasets with more features suggests that LLMs may be better suited for handling the complexities of larger datasets. While the LLM prompting strategies achieve competitive PGU and PGI scores across different datasets for ANN models, the PG-ICL strategy, on average across four datasets, achieves higher FA and RA scores than P-ICL for the LR model (Fig. 6). Moreover, on average, PG-ICL achieves 6.89% higher FA and 16.43% higher RA across datasets compared to P-ICL. We find that gradientbased methods and LIME achieve almost perfect FA and RA scores as they can get accurate model gradients and approximate the model behavior with high precision. Interestingly, the LLM-based explanations perform better than ITG, SHAP, and Random baseline methods, even for a linear model. Additionally, for the E-ICL prompting strategy, LLM-augmented explainers achieve similar faithfulness to their vanilla counterparts. The results show that LLMs generate explanations that achieve faithfulness performance on par with those generated using post hoc explanation methods for LR and ANN predictive models across all five datasets (Fig. 12; see Table 5 for complete results) and four evaluation metrics. We demonstrate that very few in-context examples (\ud835\udc5bICL = 4) are sufficient to make the LLM mimic the behavior of any post hoc explainer and generate faithful explanations, suggesting the effectiveness of LLMs as an explanation method, indicating that LLMs can effectively utilize their inherent capabilities to maintain the faithfulness of explanations in line with traditional methods. 2) ICL strategies identify important words for sentiment classifiers. Natural language is one domain of particular interest when assessing the capabilities of LLMs as explainers of other predictive models. When providing examples of removed words and corresponding changes in sentiment classification, we find that GPT-4 is able to consistently identify a top-3 most important wordset that achieves higher faithfulness than gradient-based layer-wise attributions (LGxA, LIG,\nFig. 7: PGU-text (left) and PGI-text (right) faithfulness scores of the sentiment classifier\u2019s explanations generated using post hoc explainers and LLMs (P-ICL and PG-ICL strategies) for Yelp, IMDb, and Amazon review datasets. Across three datasets, ICL strategies consistently demonstrate non-trivial post hoc explanation capabilities, achieving higher faithfulness than gradient-based layer-wise attributions, and approaching the performance of current state-of-the-art (LIME).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f7f/6f7f4510-e1e9-4908-bd9d-e2bfbcdc0897.png\" style=\"width: 50%;\"></div>\nFig. 8: ICL set representation: Faithfulness explanation performance of P-ICL and PG-ICL using the perturbed samples (x\u2032) and the raw perturbations (\ud835\udeff) as shown in Fig. 2 in the prompt for LR (left) and ANN (right) models. On average, across both prompting strategies and Blood and Adult datasets, we find that generating ICL samples using the raw perturbation format results in significantly better faithfulness performance across all four metrics.\n<div style=\"text-align: center;\">Fig. 8: ICL set representation: Faithfulness explanation performance of P-ICL and PG-ICL using the perturbed samples (x\u2032) and the raw perturbations (\ud835\udeff) as shown in Fig. 2 in the prompt for LR (left) and ANN (right) models. On average, across both prompting strategies and Blood and Adult datasets, we find that generating ICL samples using the raw perturbation format results in significantly better faithfulness performance across all four metrics.</div>\nLA, LGS, LDL, and LC), according to the modified PGU/PGI metrics described in Sec. 4.1. While LIME yields more faithful top-3 wordsets on average, we do observe cases where GPT-4 outperforms LIME-16, e.g., a 35% vs 30% increase in PGI-text over the random baseline on the Amazon dataset, using the P-ICL prompting strategy. This approaches the gold standard of LIME at around 41%, indicating promising capabilities given that a) only 16 examples are provided to the LLM, b) our strategies use solely the prompt in order to derive explanations (i.e., ICL rather than finetuning). PG-ICL achieves higher faithfulness than P-ICL for the Yelp dataset (roughly a 30% vs 10% increase over random), whereas P-ICL, on average, yields better increases on the Amazon and IMDb datasets (around 14% vs 9%, and 35% vs 27%, respectively). 3) Ablation study. Here, we show how prompt modifications and choice of LLMs affect explanation faithfulness. a) ICL set representation. Does the choice between the raw perturbation (\ud835\udeff) and the perturbed sample (x\u2032) affect faithfulness? On average, across the Blood and Adult datasets for both LR and ANN (Fig. 8), our results show that using raw perturbation in the prompts significantly aids LLMs in discerning the most important features. We find that providing only the raw perturbation bypasses the LLM\u2019s need to internally compute the difference w.r.t. the test sample and this relational perspective allows the LLM to focus directly on variations in input and output. b) Choice of LLMs. How do different LLMs impact faithfulness? Here, we perform an ablation using different models from the Gpt family, viz. Gpt-3.5, Gpt-4, and Gpt-4-0125-preview. Our results in Fig. 9 show that faithfulness performance, on average, improves with LLM\u2019s capabilities. In particular, we observe that Gpt-4-0125-preview achieves better FA, RA, and PGI scores across both datasets and prompting strategies. We attribute this improvement to the LLM\u2019s updated knowledge and ability to reduce the \u201claziness\u201d cases, where the model doesn\u2019t complete a task. Please refer to Appendix A.2 for additional ablation results on the impact of enforcing CoT, using context in the prompting template, different ICL set sizes (\ud835\udc5bICL = {4, 8, 12, 16, 32}), and other\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b269/b269159b-26ec-4a2d-b9d2-6ffda54d16e2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9: Choice of LLMs: Faithfulness explanation performance of P-ICL and PG-ICL prompting strategies on Blood and Adult datasets for different Gpt models. On average, across the LR (left) and ANN (right) models and both datasets, the Gpt-4-0125-preview explanations outperform the Gpt-3.5 and Gpt-4 models.</div>\nopen-sourced LLMs like Llama and Mixtral. We detail this ablation in the Appendix due to space constraints.\nopen-sourced LLMs like Llama and Mixtral. We detail this ablation in the A constraints.\n# 5 Conclusion\nWe introduce a novel framework, In-Context Explainers, and explore the potential of using LLMs as post hoc explainers. To this end, we propose three prompting strategies \u2014 Perturb ICL, Perturb+Guide ICL, and Explain ICL\u2014 leveraging the context, dataset, and varying levels of instructions to generate explanations using LLMs for other predictive models. We conducted many experiments to evaluate LLM-generated explanations using eight datasets. Our results across different prompting strategies highlight that LLMs can generate faithful explanations, similar to post hoc explainers. Our work paves the way for several exciting future directions in explainable artificial intelligence (XAI) to explore LLM-based explanations.\n# References\n1. Brown T, Mann B, Ryder N, et al. Language models are few-shot learners. NeurIPS 2020. 2. Zvyagin M, Brace A, Hippe K, et al. GenSLMs: Genome-scale language models reveal SARSCoV-2 evolutionary dynamics. The International Journal of High Performance Computing Applications 2023. 3. Lee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 2020. 4. Alsentzer E, Murphy JR, Boag W, et al. Publicly available clinical BERT embeddings. arXiv 2019. 5. Li T, Shetty S, Kamath A, et al. CancerGPT for few shot drug pair synergy prediction using large pretrained language models. NPJ Digital Medicine 2024. 6. Wei J, Tay Y, Bommasani R, et al. Emergent abilities of large language models. arXiv 2022. 7. Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, and Neubig G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys 2023. 8. Aky\u00a8urek E, Schuurmans D, Andreas J, Ma T, and Zhou D. What learning algorithm is incontext learning? investigations with linear models. arXiv preprint arXiv:2211.15661 2022. 9. Ribeiro MT, Singh S, and Guestrin C. \u201cWhy should I trust you?\u201d Explaining the predictions of any classifier. In: KDD. 2016. 10. Doshi-Velez F and Kim B. Towards a rigorous science of interpretable machine learning. arXiv 2017.\n11. Lundberg SM and Lee SI. A Unified Approach to Interpreting Model Predictions. In: NeurIPS. 2017. 12. Zeiler MD and Fergus R. Visualizing and understanding convolutional networks. In: ECCV. 2014. 13. Smilkov D, Thorat N, Kim B, Vi\u00b4egas F, and Wattenberg M. Smoothgrad: Removing noise by adding noise. arXiv 2017. 14. Sundararajan M, Taly A, and Yan Q. Axiomatic Attribution for Deep Networks. In: ICML. 2017. 15. Radford A, Jozefowicz R, and Sutskever I. Learning to Generate Reviews and Discovering Sentiment. 2017. arXiv: 1704.01444 [cs.LG]. 16. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. NeurIPS 2017. 17. Hegselmann S, Buendia A, Lang H, Agrawal M, Jiang X, and Sontag D. Tabllm: Few-shot classification of tabular data with large language models. In: AISTATS. PMLR. 2023. 18. Yeh IC, Yang KJ, and Ting TM. Knowledge discovery on RFM model using Bernoulli sequence. Expert Systems with applications 2009. 19. ProPublica. How We Analyzed the COMPAS Recidivism Algorithm. https://www.propublica. org/article/how-we-analyzed-the-compas-recidivism-algorithm. Accessed: 2024-0614. 20. Kaggle. Adult Income Dataset. https : / / www . kaggle . com / wenruliu / adult - income dataset. Accessed: 2020-01-01. 21. UCI. Default of Credit Card Clients Data Set. https://archive.ics.uci.edu/ml/datasets/ default+of+credit+card+clients. Accessed: 2020-01-01. 22. FICO. Explainable Machine Learning Challenge. 2019. url: https://community.fico.com/ s/explainable-machine-learning-challenge?tabset-158d9=3 (visited on 06/13/2024). 23. Kotzias D. Sentiment Labelled Sentences. UCI Machine Learning Repository. 2015. 24. Paszke A, Gross S, Massa F, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: NeurIPS. 2019. arXiv: 1912.01703 [cs.LG]. 25. Shrikumar A, Greenside P, and Kundaje A. Learning important features through propagating activation differences. In: ICML. 2017. 26. Dhamdhere K, Sundararajan M, and Yan Q. How Important Is a Neuron? arXiv:1805.12233 [cs, stat]. 2018. doi: 10.48550/arXiv.1805.12233. url: http://arxiv.org/abs/1805.12233 (visited on 06/14/2024). 27. Kokhlikyan N, Miglani V, Martin M, et al. Captum: A unified and generic model interpretability library for PyTorch. 2020. arXiv: 2009.07896 [cs.LG]. 28. Krishna S, Han T, Gu A, et al. The Disagreement Problem in Explainable Machine Learning: A Practitioner\u2019s Perspective. arXiv 2022. 29. Agarwal C, Krishna S, Saxena E, et al. Openxai: Towards a transparent evaluation of model explanations. NeurIPS 2022.\n# A Appendix: Additional results and Experimental details\n# A.1 Additional Experimental Details\nBad replies. The total number of occurrences in which either the LLM didn\u2019t follow our requested response format or it replied with \u201cI don\u2019t have enough information to determine the most importan features\u201d is detailed in Table 1.\n<div style=\"text-align: center;\">Table 1: Percentage of \u2018bad replies\u2019 produced by Gpt-4 across all datasets, models, and prompting strategies (P-ICL, PG-ICL, and E-ICL under default prompt settings). Bad replies include refusal to answer or failure to return answers in a parseable format. Here, Rec. stands for Recidivism.</div>\nMethod\nLR\nANN\nBlood Rec. Credit Adult\nBlood Reci. Credit Adult\nMean percentage\nGPT-3.5\nP-ICL\n0%\n1%\n2%\n1%\n0%\n0%\n0%\n1%\n0.62%\nPG-ICL\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0.00%\nE-ICL\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0.00%\nGPT-4\nP-ICL\n0%\n0%\n3%\n18%\n0%\n0%\n3%\n14%\n4.75%\nPG-ICL\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0.00%\nE-ICL\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0.00%\nGPT-4-Preview\nP-ICL\n0%\n0%\n3%\n2%\n0%\n0%\n3%\n2%\n1.25%\nPG-ICL\n0%\n0%\n0%\n4%\n0%\n0%\n0%\n1%\n0.62%\nE-ICL\n0%\n0%\n4%\n18%\n0%\n0%\n0%\n1%\n2.88%\nDatasets. The Blood dataset [18] comprises of 4 attributes of 748 donors to a blood transfusion service from Taiwan. The task is to determine whether patients return for another donation. The Recidivism dataset [19] has criminal records and demographics features for 6,172 defendants released on bail at U.S state courts during 1990-2009. The task is to classify defendants into bail (unlikely to commit a violent crime if released) vs. no bail (likely to commit one). The Credit dataset [21] includes financial and demographic data from credit card users at a bank, such as age, gender, education, marital status, credit limit, payment history, and bill amounts for several months. The primary task is to predict whether an individual will default on their payment. The Adult Income dataset [20] contains demographic (e.g., age, race, and gender), education (degree), employment (occupation, hours-per week), personal (marital status, relationship), and financial (capital gain/loss) features for 45,222 individuals. The task is to predict whether an individual\u2019s income exceeds $50K per year vs. not. The HELOC dataset [22] comprises of financial (e.g., total number of trades, average credit months in file) attributes from anonymized applications submitted by 9,871 real homeowners. A HELOC (Home Equity Line of Credit) is a line of credit typically offered by a bank as a percentage of home equity. The task is to predict whether applicants will repay their HELOC within 2 years. The Amazon reviews dataset contains textual reviews from users describing their experiences with products. Each review includes a rating provided by the user. The task involves predicting the sentiment (positive or negative) of each review based on its content. The IMDb dataset consists of movie reviews taken from the Internet Movie Database website. Each entry includes a textual review along with an associated sentiment label. The task involves predicting the sentiment (positive or negative) of each review based on its content. The Yelp dataset comprises user-generated reviews from Yelp, an online platform where individuals review various businesses and services. It contains reviews along with their corresponding\nstar ratings, ranging from 1 to 5. The task involves predicting the sentiment (positive or negative) of each review based on its content. Architecture details of ANNs. For the tabular dataset classsifiers, we used two different neural network models in our experiments: ANN-L, which has three hidden layers of size 64, 32, and 16, using ReLU for the hidden layers and Softmax for the output, and ANN-XL, which has six hidden layers of size 512, 256, 128, 64, 32, and 16 using ReLU for the hidden layers and Softmax for the output. Architecture details of the Transformer. For the sentiment classification task, we use a transformer model with a token embedding layer to transform a tokenized sentence into real-dimensional vectors. The embeddings are added with sinusoidal positional encodings and then passed through a transformer encoder, configured with a single layer and four attention heads. The transformer also was trained with a dropout layer with \ud835\udc5d= 0.5, followed by a linear binary-classification output layer and a softmax activation. LLM perturbation hyperparameters. We use the LLM\u2019s top-\ud835\udc58features to calculate explanation faithfulness using four evaluation metrics. For calculating PGU and PGI metrics, we use perturbation mean \ud835\udf07\ud835\udc43\ud835\udc3a=0, standard deviation \ud835\udf0e\ud835\udc43\ud835\udc3a=0.1, and the number of perturbed samples \ud835\udc5a\ud835\udc43\ud835\udc3a=10, 000. We follow the default hyperparameters from OpenXAI for generating explanations from standard post hoc explainers. Metrics. We follow [29] and used their evaluation metrics in our work. Below, we provide their respective definitions. a) Feature Agreement (FA) metric computes the fraction of top-\ud835\udc3efeatures that are common between a given post hoc explanation and the corresponding ground truth explanation. b) Rank Agreement (RA) metric measures the fraction of top-\ud835\udc3efeatures that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also have the same position in the respective rank orders. c) Prediction Gap on Important feature perturbation (PGI) metric measures the difference in prediction probability that results from perturbing the features deemed as influential by a given post hoc explanation. d) Prediction Gap on Unimportant feature perturbation (PGU) which measures the difference in prediction probability that results from perturbing the features deemed as unimportant by a given post hoc explanation. For a given instance x, we first obtain the prediction probability \u02c6\ud835\udc66output by the underlying model \ud835\udc53, i.e., \u02c6\ud835\udc66= \ud835\udc53(x). Let \ud835\udc52x be an explanation for the model prediction of x. In the case of PGU, we then generate a perturbed instance x\u2032 in the local neighborhood of x by holding the top-\ud835\udc58features constant, and slightly perturbing the values of all the other features by adding a small amount of Gaussian noise. In the case of PGI, we generate a perturbed instance x\u2032 in the local neighborhood of x by slightly perturbing the values of the top-\ud835\udc58features by adding a small amount of Gaussian noise and holding all the other features constant. Finally, we compute the expected value of the prediction difference between the original and perturbed instances as:\nPGI(x, \ud835\udc53, \ud835\udc52x, \ud835\udc58) = Ex\u2032\u223cperturb(x, \ud835\udc52x, top-\ud835\udc3e)[|\u02c6\ud835\udc66\u2212\ud835\udc53(x\u2032)|],\nPGU(x, \ud835\udc53, \ud835\udc52x, \ud835\udc58) = Ex\u2032\u223cperturb(x, \ud835\udc52x, non top-\ud835\udc3e)[|\u02c6\ud835\udc66\u2212\ud835\udc53(x\u2032)|],\nwhere perturb(\u00b7) returns the noisy versions of x as described above. For text datasets, we adapt the prediction gap metrics to assess explanation faithfulness more appropriately for natural language inputs: e) Prediction Gap on Important word perturbation (PGI-text) and f) Prediction Gap on Unimportant word perturbation (PGU-text) measure the impact of the presence or absence of specific words identified as mportant or unimportant in the text classifier\u2019s decision. Unlike tabular datasets\n(1)\n(2)\nwhere perturbations involve adding noise, in text datasets, perturbations involves selectively removing words based on the post hoc explainer\u2019s importance ranking. For a given text instance x represented by a sentence, we first obtain the classifier\u2019s prediction probability \u02c6\ud835\udc66= \ud835\udc53(x). Let \ud835\udc52x denote an explanation that identifies the importance of words within x. In the case of PGU-text, we generate a perturbed version x\u2032 by removing words not highlighted as top-\ud835\udc3eimportant, simulating the omission of supposedly unimportant words. Conversely, for PGItext, x\u2032 is formed by removing the top-\ud835\udc3ewords deemed important, to observe how their absence affects the prediction:\nThe Area Under the Curve (AUC) of these metrics across varying \ud835\udc58captures the overall faithfulness of the model\u2019s explanations. Hyperparameters for XAI methods. Below, we provide the values for all hyperparameters of the explanation methods used in our experiments. a) LIME. kernel width = 0.75; std LIME = 0.1; mode = \u2018tabular\u2019; sample around instance = True; n samples LIME = 1000 or 16; discretize continuous = False b) Grad. absolute value = True c) Smooth grad. n samples SG = 100; std SG = 0.005 d) Integrated gradients. method = \u2018gausslegendre\u2019; multiply by inputs = False; n steps = 50 e) SHAP. n samples = 500 f) Layer Integrated Gradients. method = \u2018gausslegendre\u2019; n steps = 500 g) Layer Gradient SHAP. n samples = 5; stdevs = 0.0 h) Layer Conductance. method = \u2018gausslegendre\u2019; n steps = 50\n# A.2 Additional Results\nHere, we include additional and detailed results of the experiments discussed in Sec. 4. Identifying top-k=1 feature. To demonstrate the LLM\u2019s capability in identifying the most important feature, we show the faithfulness performance of generated explanations across four datasets. In particular, for the LR model (in Table 7 and Fig. 13), we find significant variations in the feature agreement scores for the most important feature (top-\ud835\udc58= 1). Gradient-based methods like Grad, SG, and IG consistently achieved perfect FA scores across all datasets, demonstrating their reliability in identifying key features. In contrast, ITG and SHAP showed considerable variability in FA scores, particularly in the Recidivism and Adult datasets, with ITG recording as low as 0.190\u00b10.039 and 0.020\u00b10.014, respectively. Among the LLM methods, E-ICL performed notably well, achieving 0.490\u00b10.050 in Recidivism and 0.926\u00b10.027 in Credit, outperforming P-ICL and PG-ICL. However, P-ICL showed a significant disparity, with a low FA score of 0.011\u00b10.011 in Recidivism but high scores of 1.000\u00b10.000 in Credit and 0.988\u00b10.012 in Blood dataset. This lower performance in Recidivism is attributed to the LLM\u2019s approach to similarly important features, defaulting to alphabetical order in cases of near-equal importance. PG-ICL generally showed more consistent and higher FA scores than P-ICL, particularly in Recidivism and Adult, indicating its effectiveness in certain contexts by adding detailed guidance to assist in task execution. Our experiments for the ANN model (in Table 8 and Fig. 13) show that the top-\ud835\udc58=1 features identified by our proposed prompting strategies achieve similar PGU and PGI scores to gradientbased and LIME post hoc explainers, showing the utility of LLMs when explaining non-linear ML models. In particular, we observe that for Recidivism, Credit, and Blood datasets, the PGI scores obtained by P-ICL, PG-ICL, and E-ICL are on par with that of all gradient-based and LIME explainers. Overall, our results across four datasets and two ML models highlight\n(3)\n(4)\nthe reliability of gradient-based methods and highlight the varying effectiveness of our prompting strategies. Ablation: Open-Sourced LLMs. We conduct additional experiments using two open-source LLMs, namely Llama-2 70B and Mixtral 8x7B. Our results show that both models achieve comparable performance on the evaluation metrics. As expected, however, they do not exhibit capabilities as strong as GPT-4 (Fig. 10). This mirrors the trends shown in Fig. 9, demonstrating increasing post hoc explanation capabilities with increasing model size. Ablation: Impact of Chain-of-Thought (CoT). Does incorporating a CoT approach before answering affect faithfulness? We leverage the CoT capability of LLMs by including the phrase \u201cThink about the question\u201d in the instructions. Our results show that for the Blood dataset, CoT\u2019s influence varies and does not consistently improve performance across models and prompting strategies (P-ICL and PG-ICL). Conversely, for the Adult dataset, omitting CoT generally enhances performance across both models and prompting strategies (Fig. 11), highlighting the significance of providing additional guidelines to the prompt in certain contexts. Ablation: Impact of setting the Context. How does the inclusion or exclusion of the prompt Context impact faithfulness? Here, we take the original prompts in Figs. 3-4 and remove the Context from the template. On average, across two datasets and models, adding context doesn\u2019t have a significant impact on the faithfulness scores (Fig. 14), suggesting that the LLM can generate explanations using only the ICL set, question, and instructions. This could imply that the LLMs are either inherently capable of generating faithful explanations without needing extra contextual clues, or that the specific type of context provided in these experiments does not contribute meaningfully to the explanation process.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b8d2/b8d21c9f-73f7-47d7-821a-8da7d5c3d213.png\" style=\"width: 50%;\"></div>\nFig. 10: Faithfulness explanation performance of P-ICL and PG-ICL prompting strategies on Blood and Adult datasets for Llama-2 70B and Mixtral 8x7B models. Both achieve comparable performance on the evaluation metrics. However, GPT-4, on average across both datasets and metrics, outperforms the two open-sourced LLMs.\n<div style=\"text-align: center;\">Fig. 10: Faithfulness explanation performance of P-ICL and PG-ICL prompting strategies on Blood and Adult datasets for Llama-2 70B and Mixtral 8x7B models. Both achieve comparable performance on the evaluation metrics. However, GPT-4, on average across both datasets and metrics, outperforms the two open-sourced LLMs.</div>\nAblation: ICL set size. What impact do different ICL set sizes (\ud835\udc5bICL = 4, 8, 12, 16, and 64) have on faithfulness? Our ablation on the number of ICL samples (Fig. 15) shows that fewer and larger numbers of ICL samples are not beneficial for LLMs to generate post hoc explanations. While fewer ICL samples provide insufficient information to the LLM to approximate the predictive behavior of the underlying ML model, a large number of ICL samples increases the input context, where the LLM struggles to retrieve relevant information from longer prompts, resulting in a decrease in the faithfulness of the explanations generated by LLMs. In contrast to LIME, the faithfulness of LLM explanations deteriorates upon increasing the number of ICL samples (analogous to the neighborhood of a given test sample).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b13/0b1306da-8b9e-4fbe-83c9-ca92b3fce296.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 11: Faithfulness explanation performance of P-ICL and PG-ICL prompting strategies on Blood and Adult datasets w/ and w/o CoT (i.e., \u201cThink about the question\u201d) in the prompt template. On average, across the LR (left) and ANN (right) models, both datasets, and prompting strategies, the effectiveness of CoT varies but consistently enhances explanation faithfulness for the Adult dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c120/c12081c8-5c4b-4fcd-b2f0-aa2f94b465e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">LLM-Augmented Explainer</div>\nFig. 12: Faithfulness metrics for E-ICL on the Recidivism dataset for six post hoc explainers and their LLM-augmented counterparts for a given LR (left) and ANN (right) model. LLM-augmented explanations achieve on-par performance w.r.t. post hoc methods across all four metrics (see Table 5 for complete results on all other datasets). Faithfulness metrics were computed for the top-\ud835\udc58, \ud835\udc58 being the number of features in each respective dataset.\n# A.3 LLM Replies\nWe provide five example prompts below on the Blood, Adult, and IMDb datasets for P-ICL. Se Fig. 16 for a correct reply and Fig. 17 for an incorrect reply on the Blood dataset. See Fig. 18 fo a partially correct reply and Fig. 19 for an incorrect reply on the Adult dataset. See Fig. 20 for a example reply on the IMDb sentiment classification task.\n<div style=\"text-align: center;\">Base Post Hoc Explainer</div>\nLogistic Regression\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aaa8/aaa88b41-a766-4a04-91da-aa35eef03b4e.png\" style=\"width: 50%;\"></div>\nFig. 14: Faithfulness explanation performance of P-ICL and PG-ICL prompting strategies on Blood and Adult datasets w/ and w/o Context in the prompt template. On average, across LR (left) and ANN (right) models and both datasets, we find that Context doesn\u2019t have a significant impact on the faithfulness scores.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d50c/d50cc61c-6c75-4b9c-a905-c35c841be12e.png\" style=\"width: 50%;\"></div>\nFig. 15: FA and RA performance of P-ICL and PG-ICL prompting strategies and LIME as we increase the number of ICL samples (analogous to neighborhood samples in LIME) for the LR model. In contrast to LIME, the faithfulness of LLM explanations across different metrics decreases for a higher number of ICL samples. This is likely due to limited capabilities of the LLM in tackling longer prompts, and/or its reluctance to analyze them (the number of successfully parsed replies decreases as we increase the number of samples).\nDataset\nLR\nANN\nBlood\nRecidivism\nCredit\nAdult\n70.59%\n76.90%\n87.37%\n77.37%\n64.71%\n76.90%\n88.34%\n80.11%\nNeural Network\nDataset\nTransformer\nAmazon reviews\nIMDb\nYelp\n72.50%\n76.25%\n71.25%\nTable 4: Average faithfulness metric values and their standard errors for explanations across 100 test instances. It compares explanations from Perturb ICL and Perturb+Guide ICL using Gpt-4, six post hoc explanation methods, and a random baseline for LR and ANN predictions on four datasets. For the LLM methods, we queried the LLM for the top-\ud835\udc58= 5 (\ud835\udc58= 4 for Blood) most important features and calculated each metric\u2019s area under the curve (AUC) for \ud835\udc58= 3 (where the AUC is calculated from \ud835\udc58= 1 to \ud835\udc58= 3). Arrows (\u2191, \u2193) indicate the direction of better performance.\n\u2191 \u2193\nLR\nANN\nDataset\nMethod\nFA (\u2191)\nRA (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nGrad\n1.000\u00b10.000 1.000\u00b10.000 a 0.010\u00b10.000 0.042\u00b10.000 0.060\u00b10.009 0.115\u00b10.013\nSG\n1.000\u00b10.000 1.000\u00b10.000 0.010\u00b10.000 0.042\u00b10.000 0.060\u00b10.009 0.115\u00b10.013\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.010\u00b10.000 0.042\u00b10.000 0.061\u00b10.009 0.116\u00b10.013\nITG\n0.722\u00b10.019 0.563\u00b10.037 0.019\u00b10.001 0.037\u00b10.001 0.081\u00b10.010 0.100\u00b10.012\nSHAP\n0.723\u00b10.020 0.556\u00b10.037 0.019\u00b10.001 0.036\u00b10.001 0.085\u00b10.011 0.098\u00b10.012\nLIME\n1.000\u00b10.000 1.000\u00b10.000 0.010\u00b10.000 0.042\u00b10.000 0.061\u00b10.009 0.116\u00b10.013\nRandom\n0.502\u00b10.022 0.232\u00b10.032 0.029\u00b10.001 0.026\u00b10.001 0.091\u00b10.011 0.090\u00b10.012\nPrediction-based ICL 0.834\u00b10.013 0.724\u00b10.023 0.013\u00b10.000 0.041\u00b10.000 0.061\u00b10.010 0.105\u00b10.013\nBlood\nPerturb+Guide ICL 0.875\u00b10.016 0.742\u00b10.039 0.013\u00b10.000 0.041\u00b10.001 0.064\u00b10.010 0.104\u00b10.013\nGrad\n1.000\u00b10.000 1.000\u00b10.000 0.059\u00b10.003 0.106\u00b10.005 0.095\u00b10.008 0.149\u00b10.011\nSG\n1.000\u00b10.000 1.000\u00b10.000 0.059\u00b10.003 0.106\u00b10.005 0.095\u00b10.008 0.149\u00b10.011\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.059\u00b10.003 0.106\u00b10.005 0.096\u00b10.008 0.149\u00b10.011\nITG\n0.493\u00b10.021 0.214\u00b10.030 0.090\u00b10.005 0.078\u00b10.004 0.129\u00b10.011 0.122\u00b10.010\nSHAP\n0.473\u00b10.023 0.217\u00b10.032 0.092\u00b10.005 0.076\u00b10.004 0.130\u00b10.011 0.122\u00b10.010\nLIME\n1.000\u00b10.000 1.000\u00b10.000 0.059\u00b10.003 0.106\u00b10.005 0.096\u00b10.008 0.149\u00b10.011\nRandom\n0.308\u00b10.023 0.127\u00b10.024 0.101\u00b10.005 0.063\u00b10.005 0.146\u00b10.011 0.092\u00b10.009\nPrediction-based ICL 0.746\u00b10.003 0.088\u00b10.005 0.061\u00b10.003 0.105\u00b10.005 0.096\u00b10.008 0.147\u00b10.012\nRecidivism\nPerturb+Guide ICL 0.756\u00b10.013 0.275\u00b10.037 0.065\u00b10.004 0.103\u00b10.005 0.099\u00b10.009 0.146\u00b10.012\nGrad\n1.000\u00b10.000 1.000\u00b10.000 0.065\u00b10.005 0.195\u00b10.009 0.072\u00b10.008 0.173\u00b10.011\nSG\n1.000\u00b10.000 1.000\u00b10.000 0.065\u00b10.005 0.195\u00b10.009 0.072\u00b10.008 0.172\u00b10.011\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.065\u00b10.005 0.195\u00b10.009 0.074\u00b10.008 0.172\u00b10.010\nITG\n0.211\u00b10.026 0.157\u00b10.026 0.150\u00b10.006 0.106\u00b10.012 0.155\u00b10.009 0.089\u00b10.011\nSHAP\n0.212\u00b10.026 0.161\u00b10.026 0.150\u00b10.006 0.107\u00b10.012 0.150\u00b10.008 0.098\u00b10.012\nLIME\n0.988\u00b10.005 0.985\u00b10.007 0.065\u00b10.005 0.195\u00b10.009 0.071\u00b10.008 0.173\u00b10.010\nRandom\n0.173\u00b10.020 0.095\u00b10.020 0.185\u00b10.010 0.054\u00b10.006 0.176\u00b10.011 0.053\u00b10.007\nPrediction-based ICL 0.622\u00b10.008 0.604\u00b10.008 0.077\u00b10.006 0.192\u00b10.009 0.080\u00b10.009 0.171\u00b10.011\nCredit\nPerturb+Guide ICL 0.646\u00b10.014 0.594\u00b10.020 0.081\u00b10.007 0.186\u00b10.009 0.084\u00b10.009 0.165\u00b10.011\nGrad\n0.999\u00b10.001 0.999\u00b10.001 0.056\u00b10.006 0.221\u00b10.011 0.081\u00b10.011 0.228\u00b10.014\nSG\n0.999\u00b10.001 0.999\u00b10.001 0.056\u00b10.006 0.221\u00b10.011 0.080\u00b10.011 0.227\u00b10.014\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.056\u00b10.006 0.221\u00b10.011 0.082\u00b10.011 0.228\u00b10.014\nITG\n0.385\u00b10.012 0.099\u00b10.019 0.215\u00b10.011 0.061\u00b10.007 0.227\u00b10.014 0.075\u00b10.010\nSHAP\n0.387\u00b10.012 0.150\u00b10.020 0.215\u00b10.011 0.061\u00b10.007 0.225\u00b10.014 0.075\u00b10.010\nLIME\n0.963\u00b10.012 0.953\u00b10.015 0.056\u00b10.006 0.221\u00b10.011 0.078\u00b10.011 0.229\u00b10.014\nRandom\n0.130\u00b10.017 0.053\u00b10.015 0.198\u00b10.012 0.054\u00b10.008 0.213\u00b10.014 0.064\u00b10.010\nPrediction-based ICL 0.541\u00b10.022 0.450\u00b10.033 0.086\u00b10.009 0.197\u00b10.012 0.110\u00b10.013 0.197\u00b10.015\nAdult\nPerturb+Guide ICL 0.669\u00b10.021 0.622\u00b10.028 0.075\u00b10.007 0.210\u00b10.011 0.107\u00b10.013 0.208\u00b10.014\n<div style=\"text-align: center;\">Table 5: Average faithfulness metric values and their standard errors for explanations across 100 test instances. It compares explanations from Explain ICL using Gpt-4 and six post hoc methods for LR and ANN predictions on four datasets. Metrics were calculated for the top-\ud835\udc58features, with \ud835\udc58matching the dataset feature count. Arrows (\u2191, \u2193) denote the direction of improved performance.</div>\natching the dataset feature count. Arrows (\u2191, \u2193) denote the direction of improved performa\nLR\nANN\nDataset\nMethod\nFA (\u2191)\nRA (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nLLM-Lime 1.000\u00b10.000 0.978\u00b10.011 0.000\u00b10.000 0.041\u00b10.001 0.074\u00b10.009 0.099\u00b10.012\nLime\n1.000\u00b10.000 1.000\u00b10.000 0.008\u00b10.000 0.043\u00b10.000 0.044\u00b10.006 0.121\u00b10.013\nLLM-Grad 0.997\u00b10.003 0.996\u00b10.004 0.008\u00b10.000 0.043\u00b10.000 0.058\u00b10.009 0.116\u00b10.012\nGrad\n1.000\u00b10.000 1.000\u00b10.000 0.008\u00b10.000 0.043\u00b10.000 0.044\u00b10.006 0.120\u00b10.013\nLLM-SG\n0.990\u00b10.006 0.983\u00b10.011 0.008\u00b10.000 0.043\u00b10.000 0.055\u00b10.008 0.116\u00b10.012\nSG\n1.000\u00b10.000 1.000\u00b10.000 0.008\u00b10.000 0.043\u00b10.000 0.044\u00b10.006 0.120\u00b10.013\nLLM-IG\n0.989\u00b10.005 0.982\u00b10.009 0.008\u00b10.000 0.043\u00b10.000 0.046\u00b10.007 0.120\u00b10.013\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.008\u00b10.000 0.043\u00b10.000 0.044\u00b10.006 0.120\u00b10.013\nLLM-Shap 0.684\u00b10.013 0.401\u00b10.025 0.020\u00b10.001 0.034\u00b10.001 0.069\u00b10.009 0.102\u00b10.012\nShap\n0.773\u00b10.014 0.516\u00b10.033 0.015\u00b10.001 0.038\u00b10.001 0.066\u00b10.009 0.107\u00b10.012\nLLM-ITG\n0.702\u00b10.013 0.387\u00b10.029 0.017\u00b10.001 0.036\u00b10.001 0.069\u00b10.010 0.105\u00b10.012\nBlood\nITG\n0.774\u00b10.014 0.532\u00b10.034 0.014\u00b10.001 0.038\u00b10.001 0.063\u00b10.008 0.108\u00b10.012\nLLM-Lime 0.990\u00b10.001 0.958\u00b10.005 0.029\u00b10.001 0.115\u00b10.002 0.048\u00b10.001 0.165\u00b10.004\nLime\n1.000\u00b10.000 1.000\u00b10.000 0.029\u00b10.002 0.116\u00b10.006 0.044\u00b10.004 0.164\u00b10.012\nLLM-Grad 0.997\u00b10.001 0.990\u00b10.003 0.029\u00b10.001 0.115\u00b10.002 0.048\u00b10.001 0.165\u00b10.004\nGrad\n1.000\u00b10.000 1.000\u00b10.000 0.029\u00b10.002 0.116\u00b10.006 0.043\u00b10.004 0.165\u00b10.012\nLLM-SG\n0.997\u00b10.001 0.990\u00b10.003 0.029\u00b10.001 0.115\u00b10.002 0.047\u00b10.001 0.165\u00b10.004\nSG\n1.000\u00b10.000 1.000\u00b10.000 0.029\u00b10.002 0.116\u00b10.006 0.043\u00b10.004 0.165\u00b10.012\nLLM-IG\n0.996\u00b10.001 0.988\u00b10.003 0.029\u00b10.001 0.115\u00b10.002 0.048\u00b10.001 0.166\u00b10.004\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.029\u00b10.002 0.116\u00b10.006 0.044\u00b10.004 0.165\u00b10.012\nLLM-Shap 0.666\u00b10.004 0.216\u00b10.008 0.057\u00b10.001 0.098\u00b10.002 0.082\u00b10.002 0.151\u00b10.004\nShap\n0.670\u00b10.012 0.200\u00b10.024 0.058\u00b10.003 0.099\u00b10.005 0.087\u00b10.008 0.146\u00b10.011\nLLM-ITG\n0.690\u00b10.004 0.247\u00b10.008 0.056\u00b10.001 0.099\u00b10.002 0.085\u00b10.002 0.148\u00b10.004\nRecidivism\nITG\n0.689\u00b10.011 0.195\u00b10.022 0.056\u00b10.003 0.100\u00b10.005 0.078\u00b10.007 0.149\u00b10.011\nLLM-Lime 0.909\u00b10.001 0.632\u00b10.005 0.023\u00b10.001 0.222\u00b10.003 0.035\u00b10.002 0.230\u00b10.004\nLime\n0.907\u00b10.005 0.743\u00b10.017 0.018\u00b10.002 0.224\u00b10.011 0.029\u00b10.005 0.235\u00b10.014\nLLM-Grad 0.938\u00b10.000 0.801\u00b10.001 0.022\u00b10.001 0.223\u00b10.003 0.035\u00b10.002 0.230\u00b10.004\nGrad\n0.999\u00b10.001 0.997\u00b10.003 0.018\u00b10.002 0.224\u00b10.011 0.029\u00b10.004 0.234\u00b10.014\nLLM-SG\n0.938\u00b10.000 0.802\u00b10.001 0.022\u00b10.001 0.223\u00b10.003 0.035\u00b10.002 0.230\u00b10.004\nSG\n0.999\u00b10.001 0.997\u00b10.003 0.018\u00b10.002 0.224\u00b10.011 0.029\u00b10.004 0.234\u00b10.014\nLLM-IG\n0.938\u00b10.000 0.804\u00b10.000 0.022\u00b10.001 0.223\u00b10.003 0.033\u00b10.002 0.231\u00b10.004\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.018\u00b10.002 0.224\u00b10.011 0.031\u00b10.005 0.235\u00b10.014\nLLM-Shap 0.676\u00b10.002 0.069\u00b10.003 0.109\u00b10.002 0.148\u00b10.003 0.123\u00b10.003 0.153\u00b10.004\nShap\n0.662\u00b10.007 0.107\u00b10.012 0.139\u00b10.009 0.127\u00b10.009 0.144\u00b10.011 0.149\u00b10.013\nLLM-ITG\n0.665\u00b10.002 0.039\u00b10.002 0.107\u00b10.002 0.150\u00b10.003 0.132\u00b10.003 0.146\u00b10.004\nAdult\nITG\n0.627\u00b10.006 0.068\u00b10.010 0.175\u00b10.010 0.099\u00b10.009 0.170\u00b10.011 0.130\u00b10.013\nLLM-Lime 0.954\u00b10.001 0.787\u00b10.003 0.030\u00b10.001 0.189\u00b10.003 0.042\u00b10.002 0.178\u00b10.003\nLime\n0.977\u00b10.004 0.878\u00b10.015 0.030\u00b10.003 0.201\u00b10.009 0.037\u00b10.004 0.186\u00b10.010\nLLM-Grad 0.984\u00b10.000 0.896\u00b10.001 0.029\u00b10.001 0.189\u00b10.003 0.042\u00b10.002 0.178\u00b10.003\nGrad\n1.000\u00b10.000 1.000\u00b10.000 0.030\u00b10.003 0.201\u00b10.009 0.038\u00b10.005 0.185\u00b10.011\nLLM-SG\n0.984\u00b10.000 0.897\u00b10.000 0.029\u00b10.001 0.189\u00b10.003 0.072\u00b10.003 0.165\u00b10.003\nSG\n1.000\u00b10.000 1.000\u00b10.000 0.030\u00b10.003 0.201\u00b10.009 0.037\u00b10.004 0.185\u00b10.011\nLLM-IG\n0.984\u00b10.000 0.896\u00b10.001 0.029\u00b10.001 0.189\u00b10.003 0.041\u00b10.002 0.179\u00b10.003\nIG\n1.000\u00b10.000 1.000\u00b10.000 0.030\u00b10.003 0.201\u00b10.009 0.041\u00b10.005 0.185\u00b10.010\nLLM-Shap 0.543\u00b10.003 0.067\u00b10.004 0.088\u00b10.002 0.140\u00b10.003 0.094\u00b10.003 0.126\u00b10.003\nShap\n0.525\u00b10.009 0.086\u00b10.012 0.088\u00b10.005 0.163\u00b10.010 0.091\u00b10.006 0.146\u00b10.011\nLLM-ITG\n0.526\u00b10.003 0.052\u00b10.003 0.088\u00b10.002 0.139\u00b10.003 0.091\u00b10.002 0.129\u00b10.003\nCredit\nITG\n0.516\u00b10.010 0.076\u00b10.012 0.086\u00b10.005 0.165\u00b10.010 0.084\u00b10.006 0.152\u00b10.010\nTable 6: Average faithfulness metric values (PGI-text and PGU-text) and their standard errors for explanations across 100 test instances. It compares explanations from Perturb ICL and Perturb+Guide ICL using Gpt-4 and Gpt-4-0125-preview for the top-\ud835\udc58= 3 most important words on the Yelp, IMDb, and Amazon datasets. We calculated each metric\u2019s area under the curve (AUC) for \ud835\udc58= 3 (where the AUC is calculated from \ud835\udc58= 1 to \ud835\udc58= 3). Arrows (\u2191, \u2193) denote the direction of improved performance.\nimproved performance.\nYelp\nIMDb\nAmazon\nMethod\nPGI-text (\u2191)PGU-text (\u2193)PGI-text (\u2191)PGU-text (\u2193)PGI-text (\u2191)PGU-text (\u2193)\nPG-ICL (Gpt-4-0125-preview) 0.244\u00b10.035\n0.260\u00b10.034\n0.329\u00b10.045\n0.214\u00b10.039\n0.371\u00b10.042\n0.205\u00b10.035\nP-ICL (Gpt-4-0125-preview)\n0.300\u00b10.038\n0.231\u00b10.033\n0.295\u00b10.040\n0.263\u00b10.039\n0.390\u00b10.041\n0.252\u00b10.037\nPG-ICL (Gpt-4)\n0.349\u00b10.039\n0.243\u00b10.033\n0.318\u00b10.043\n0.251\u00b10.038\n0.386\u00b10.038\n0.238\u00b10.036\nP-ICL (Gpt-4)\n0.300\u00b10.036\n0.293\u00b10.037\n0.332\u00b10.038\n0.205\u00b10.031\n0.409\u00b10.038\n0.222\u00b10.036\nLA\n0.212\u00b10.035\n0.371\u00b10.041\n0.140\u00b10.028\n0.418\u00b10.048\n0.218\u00b10.034\n0.290\u00b10.038\nLC\n0.170\u00b10.030\n0.355\u00b10.042\n0.178\u00b10.033\n0.339\u00b10.042\n0.216\u00b10.035\n0.303\u00b10.037\nLDL\n0.301\u00b10.038\n0.305\u00b10.038\n0.208\u00b10.034\n0.367\u00b10.046\n0.297\u00b10.040\n0.265\u00b10.038\nLGS\n0.291\u00b10.039\n0.355\u00b10.038\n0.186\u00b10.033\n0.375\u00b10.046\n0.240\u00b10.035\n0.313\u00b10.040\nLGxA\n0.280\u00b10.038\n0.331\u00b10.042\n0.229\u00b10.037\n0.373\u00b10.045\n0.327\u00b10.042\n0.272\u00b10.039\nLIG\n0.291\u00b10.040\n0.346\u00b10.040\n0.217\u00b10.035\n0.394\u00b10.047\n0.263\u00b10.040\n0.358\u00b10.042\nLIME\n0.382\u00b10.038\n0.239\u00b10.033\n0.486\u00b10.039\n0.163\u00b10.031\n0.429\u00b10.040\n0.181\u00b10.031\nLIME-16\n0.380\u00b10.041\n0.258\u00b10.033\n0.359\u00b10.044\n0.201\u00b10.034\n0.396\u00b10.041\n0.171\u00b10.032\nRandom\n0.271\u00b10.029\n0.251\u00b10.027\n0.291\u00b10.032\n0.284\u00b10.031\n0.304\u00b10.032\n0.281\u00b10.034\nTable 7: Faithfulness scores for the most important feature value, top-\ud835\udc58= 1, identified by existing post hoc explanation methods as well as the three LLM methods which generated explanations from Gpt-4 across four datasets and the LR model. (Since FA = RA for top-\ud835\udc58= 1, we omit RA to avoid\nredundancy).\nRecidivism\nAdult\nCredit\nBlood\nMethod\nFA (\u2191)\nPGU (\u2193)\nFA (\u2191)\nPGU (\u2193)\nFA (\u2191)\nPGU (\u2193)\nFA (\u2191)\nPGU (\u2193)\nGrad\n1.000\u00b10.000 0.096\u00b10.005 1.000\u00b10.000 0.073\u00b10.007 1.000\u00b10.000 0.081\u00b10.006 1.000\u00b10.000 0.020\u00b10.000\nSG\n1.000\u00b10.000 0.095\u00b10.005 1.000\u00b10.000 0.073\u00b10.007 1.000\u00b10.000 0.081\u00b10.006 1.000\u00b10.000 0.020\u00b10.000\nIG\n1.000\u00b10.000 0.096\u00b10.005 1.000\u00b10.000 0.073\u00b10.007 1.000\u00b10.000 0.081\u00b10.006 1.000\u00b10.000 0.020\u00b10.000\nITG\n0.190\u00b10.039 0.108\u00b10.006 0.020\u00b10.014 0.221\u00b10.011 0.270\u00b10.044 0.163\u00b10.007 0.700\u00b10.046 0.026\u00b10.001\nSHAP\n0.210\u00b10.041 0.108\u00b10.006 0.020\u00b10.014 0.221\u00b10.011 0.270\u00b10.044 0.163\u00b10.007 0.700\u00b10.046 0.026\u00b10.001\nLIME\n1.000\u00b10.000 0.096\u00b10.005 0.990\u00b10.010 0.221\u00b10.011 1.000\u00b10.000 0.081\u00b10.006 1.000\u00b10.000 0.020\u00b10.000\nRandom 0.130\u00b10.034 0.113\u00b10.006 0.060\u00b10.024 0.214\u00b10.011 0.070\u00b10.026 0.195\u00b10.010 0.190\u00b10.039 0.038\u00b10.001\nP-ICL\n0.011\u00b10.011 0.102\u00b10.005 0.716\u00b10.050 0.116\u00b10.012 1.000\u00b10.000 0.081\u00b10.007 0.988\u00b10.012 0.020\u00b10.000\nPG-ICL 0.269\u00b10.046 0.101\u00b10.005 0.869\u00b10.034 0.094\u00b10.009 0.918\u00b10.028 0.092\u00b10.008 0.845\u00b10.039 0.023\u00b10.001\nE-ICL\n0.490\u00b10.050 0.098\u00b10.005 0.919\u00b10.027 0.086\u00b10.009 0.926\u00b10.027 0.090\u00b10.007 0.758\u00b10.045 0.025\u00b10.001\n<div style=\"text-align: center;\">Table 8: Faithfulness scores for the most important feature value, top-\ud835\udc58= 1, identified by existing post hoc explanation methods as well as the three LLM methods which generated explanations from Gpt-4 across four datasets and the ANN model.</div>\nGpt-4 across four datasets and the ANN model.\nRecidivism\nAdult\nCredit\nBlood\nMethod\nPGU (\u2193)\nPGI (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nPGU (\u2193)\nPGI (\u2191)\nGrad\n0.147\u00b10.011 0.117\u00b10.010 0.103\u00b10.013 0.224\u00b10.014 0.085\u00b10.009 0.166\u00b10.010 0.087\u00b10.012 0.103\u00b10.012\nSG\n0.146\u00b10.011 0.117\u00b10.010 0.103\u00b10.013 0.224\u00b10.014 0.084\u00b10.009 0.167\u00b10.010 0.087\u00b10.012 0.102\u00b10.012\nIG\n0.147\u00b10.011 0.116\u00b10.010 0.103\u00b10.013 0.225\u00b10.014 0.085\u00b10.009 0.167\u00b10.010 0.087\u00b10.012 0.103\u00b10.012\nITG\n0.154\u00b10.012 0.084\u00b10.009 0.232\u00b10.014 0.056\u00b10.009 0.181\u00b10.010 0.057\u00b10.009 0.103\u00b10.012 0.083\u00b10.012\nSHAP\n0.152\u00b10.012 0.092\u00b10.009 0.231\u00b10.014 0.047\u00b10.008 0.169\u00b10.009 0.076\u00b10.011 0.104\u00b10.012 0",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of effectively explaining the predictions of complex predictive models using Large Language Models (LLMs) through in-context learning (ICL). Previous methods have struggled to provide satisfactory post hoc explanations, necessitating a new approach that leverages LLM capabilities.",
        "problem": {
            "definition": "The problem being addressed is the lack of effective methods for generating post hoc explanations of complex predictive models, particularly in the context of LLMs.",
            "key obstacle": "Existing methods often fail to provide clear and interpretable explanations due to the black box nature of many predictive models."
        },
        "idea": {
            "intuition": "The idea stems from the observation that LLMs can learn to generate explanations based on examples provided in context, utilizing their in-context learning capabilities.",
            "opinion": "The proposed framework, In-Context Explainers, aims to harness LLMs to explain the predictions of other models by generating natural language explanations based on provided examples.",
            "innovation": "The primary innovation is the introduction of three distinct prompting strategies (Perturb ICL, Perturb+Guide ICL, and Explain ICL) that effectively utilize ICL to generate explanations that are comparable to state-of-the-art post hoc explainers."
        },
        "method": {
            "method name": "In-Context Explainers",
            "method abbreviation": "ICE",
            "method definition": "A framework that utilizes in-context learning capabilities of LLMs to generate explanations for predictions made by other predictive models.",
            "method description": "The framework consists of three prompting strategies that guide LLMs in generating explanations based on input-output pairs and contextual information.",
            "method steps": [
                "Select a predictive model and a dataset.",
                "Use ICL strategies to create prompts that include examples of input-output pairs.",
                "Generate explanations using the LLM based on the prompts."
            ],
            "principle": "The method is effective because it leverages the LLM's ability to understand and generalize from examples, allowing it to produce coherent and contextually relevant explanations."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on eight real-world datasets, including tabular data and text datasets, using various predictive models such as Logistic Regression and neural networks.",
            "evaluation method": "Performance was assessed through metrics that measure the faithfulness of the generated explanations, comparing them to those produced by established post hoc explanation methods."
        },
        "conclusion": "The proposed In-Context Explainers framework demonstrates that LLMs can generate faithful explanations comparable to traditional post hoc methods, paving the way for future research in explainable AI.",
        "discussion": {
            "advantage": "The key advantages include the ability to generate coherent explanations without the need for retraining and the flexibility of LLMs to adapt to different types of predictive models.",
            "limitation": "A limitation of the approach is that the effectiveness of explanations can vary depending on the quality of the input examples and the specific prompting strategy used.",
            "future work": "Future research could explore optimizing prompting strategies and extending the framework to other types of models and datasets."
        },
        "other info": {
            "authors": "Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju",
            "affiliations": "University of Florida, Harvard University",
            "corresponding author": "Nick Kroeger (nkroeger@ufl.edu)",
            "arXiv": "arXiv:2310.05797v4"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is leveraged to explain predictions of complex predictive models using Large Language Models (LLMs)."
        },
        {
            "section number": "1.2",
            "key information": "The significance of ICL lies in its ability to generate natural language explanations that enhance interpretability in the context of LLMs."
        },
        {
            "section number": "1.3",
            "key information": "LLMs facilitate in-context learning by utilizing their capabilities to learn from examples provided in context."
        },
        {
            "section number": "3.1",
            "key information": "The In-Context Explainers framework demonstrates how LLMs adapt to generate coherent explanations based on input-output pairs."
        },
        {
            "section number": "3.2",
            "key information": "The framework introduces three prompting strategies (Perturb ICL, Perturb+Guide ICL, and Explain ICL) that enhance the theoretical understanding of ICL in generating explanations."
        },
        {
            "section number": "4.1",
            "key information": "Effective prompt design is crucial as it guides LLMs in generating faithful and coherent explanations for predictions made by other models."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the proposed approach is that the effectiveness of explanations can vary depending on the quality of the input examples and the specific prompting strategy used."
        },
        {
            "section number": "7",
            "key information": "The conclusion emphasizes that the In-Context Explainers framework paves the way for future research in explainable AI, highlighting the role of LLMs in generating faithful explanations."
        }
    ],
    "similarity_score": 0.7697899137737265,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Explainers_ Harnessing LLMs for Explaining Black Box Models.json"
}