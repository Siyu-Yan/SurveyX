{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.04874",
    "title": "Approximate Bayesian Computation with Deep Learning and Conformal prediction",
    "abstract": "Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these \"user-choices\". In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).\n  Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature.",
    "bib_name": "baragatti2024approximatebayesiancomputationdeep",
    "md_text": "# Approximate Bayesian Computation with Deep Learning and Conformal prediction\nMe\u00efli Baragatti1, Bertrand Cloez2, David M\u00e9tivier2, and Isabelle Sanchez2\nMISTEA, Universit\u00e9 de Montpellier, INRAE, Institut Agro, Montpellier, France 2MISTEA, Universit\u00e9 de Montpellier, INRAE, Montpellier, France\nJuly 10, 2024\nApproximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these \u201cuser-choices\u201d. In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance, and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates). Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or other moment type functionals), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature. Keywords: Likelihood-free inference \u00b7 Approximate Bayesian computation \u00b7 Convolutional neural networks \u00b7 Dropout \u00b7 Conformal prediction \u00b7 probability matching criterion.\n[stat.ME]\narXiv:2406.04874v2 \n# 1 Introduction\nIn many situations, the likelihood function plays a key role in both frequentist or Bayesian inference. However, it is not unusual that such likelihood is difficult to work with either because no closed form expression exists, e.g., the corresponding statistical model is too complex, or because its computational cost is prohibitive. For example, the first situation occurs with max-stable processes [de Haan, 1984] or Gibbs random fields [Besag, 1974; Grelaud et al., 2009], while the second case frequently occurs with mixture models [Simola et al., 2021]. In this paper, we will restrict our attention to parameter inference in the Bayesian framework and in particular to Approximate Bayesian Computation (ABC), which is one of the most popular likelihood-free parameter inference method. Indeed, ABC can be an efficient alternative to overcome the situation of intractable likelihood functions [Marin et al., 2012]. Within the ABC framework, the need for computing the likelihood function is substituted by the need to be able to simulate from the statistical model under consideration. The first proposed ABC methods enabled to obtain an approximation of the posterior distribution of interest, from simulations [Pritchard et al., 1999]. For these early methods, the degree of approximation depends strongly on the choices of the tolerance threshold, the distance and of the so-called summary statistics used during the ABC estimation. Unfortunately, identifying the most relevant summary statistics is problem dependent and in many situations the statistics are chosen by the practitioner, even if several\nstudies provide guidance in this choice, see for instance Joyce and Marjoram [2008], McKinley et al. [2009], Blum et al. [2012], Fearnhead and Prangle [2012] among others. Recently, several approaches managed to mitigate the need of identifying relevant summary statistics. These approaches combine the ABC framework with machine learning methods, such as random forests or neural networks. For instance, Raynal et al. [2018] propose an automatic selection of relevant summary statistics. They propose to consider a large set of potentially relevant summary statistics and use the random forest framework as a black box to get some point estimates of the posterior distribution given these summary statistics. Another approach of interest is the one of Akesson et al. [2022], which uses the estimated posterior mean as a summary statistic, this posterior mean being estimated through a Convolutional Neural Network (CNN). Consequently, since the emergence of ABC methods, a wide variety of methods have been proposed, from the standard \u201cbasic\u201d ABC to methods using random forests or neural networks, and including those using MCMC or filtering algorithms (see Marjoram et al. [2003], Sisson et al. [2007], Baragatti and Pudlo [2014] and references therein). To quantify the uncertainties, Bayesian methods provide posterior credible intervals from the posterior distribution, through posterior quantiles or highest posterior density regions. This can be justified because, in many simple parametric models, the Bernstein-von Mises theorem ensures that posterior credible intervals are asymptotically as well, reconciling the frequentist and Bayesian approaches. However, these credible intervals are not necessarily relevant in practice. Indeed, note for instance that in nonparametric and highdimensional models, research is still ongoing to understand better the link between the coverage of credible intervals and the coverage of frequentist, see Rousseau and Szabo [2016]; Datta et al. [2000]; Hoff [2023] among others. Indeed, the posterior coverage probability of a Bayesian credible interval does not always match with the corresponding frequentist coverage probability. Quite often, they can even diverge in nontrivial ways, see for instance Wasserman [2011]. In this case, we say that the probability matching criterion is not verified. This criterion can be hard to check when the exact posterior distribution is known, and even more so when the posterior is approximated [Frazier et al., 2018], which is the cases of most applications requiring an ABC method. Another drawback of using posterior credible intervals in ABC methods is that they only take into account the inherent randomness of the model when estimating the prediction uncertainty. Nevertheless, by definition of the method, the a posteriori distribution is approximated, so there is an error due to the method to take into account. Our contribution. In this paper, we propose a new ABC implementation that does not require summary statistics, tolerance threshold and distance. Our method produce point estimates of the posterior distribution e.g., mean, variance, quantiles, without aiming to approximate the whole posterior distribution. It also provides associated confidence sets of the estimates. The estimates are obtained using neural networks with Monte Carlo Dropout, while the confidence sets are obtained through conformal prediction. Our ABC estimation method only requires a 1) sampling function, 2) a neural network (NN) whose architecture is tailored for the problem of interest e.g., CNN for images, GNN for data with a graph structure, RNN for sequential data, and 3) a confidence level. While 1) and 3) are common to all ABC methods, requirement 2) is more original, but still easy to implement because well known architectures can be used with minimal changes, i.e., the inclusion of Dropout (or concrete Dropout) layers. Another originality of this work is that it produces directly confidence sets with proper marginal coverage. We test the proposed ABCD-Conformal algorithm in three different problems. We compare the results against the standard ABC, the ABC Random Forest (ABC-RF) of Raynal et al. [2018] and the ABC-CNN of Akesson et al. [2022]. We measure both the accuracy of the estimation and the frequentist coverage of confidence sets. The paper is organized as follows. In section 2 we briefly present some existing ABC methods: standard ABC, ABC-CNN and ABC-RF. In section 3, we propose the ABCD-Conformal methodology, combining neural networks and conformal prediction in a likelihood-free framework. Then, we illustrate and compare all these methods on three examples in section 4, before a discussion in section 5.\nstudies provide guidance in this choice, see for instance Joyce and Marjoram [2008], McKinley et al. [2009], Blum et al. [2012], Fearnhead and Prangle [2012] among others. Recently, several approaches managed to mitigate the need of identifying relevant summary statistics. These approaches combine the ABC framework with machine learning methods, such as random forests or neural networks. For instance, Raynal et al. [2018] propose an automatic selection of relevant summary statistics. They propose to consider a large set of potentially relevant summary statistics and use the random forest framework as a black box to get some point estimates of the posterior distribution given these summary statistics. Another approach of interest is the one of Akesson et al. [2022], which uses the estimated posterior mean as a summary statistic, this posterior mean being estimated through a Convolutional Neural Network (CNN). Consequently, since the emergence of ABC methods, a wide variety of methods have been proposed, from the standard \u201cbasic\u201d ABC to methods using random forests or neural networks, and including those using MCMC or filtering algorithms (see Marjoram et al. [2003], Sisson et al. [2007], Baragatti and Pudlo [2014] and references therein). To quantify the uncertainties, Bayesian methods provide posterior credible intervals from the posterior distribution, through posterior quantiles or highest posterior density regions. This can be justified because, in many simple parametric models, the Bernstein-von Mises theorem ensures that posterior credible intervals are asymptotically as well, reconciling the frequentist and Bayesian approaches. However, these credible intervals are not necessarily relevant in practice. Indeed, note for instance that in nonparametric and highdimensional models, research is still ongoing to understand better the link between the coverage of credible intervals and the coverage of frequentist, see Rousseau and Szabo [2016]; Datta et al. [2000]; Hoff [2023] among others. Indeed, the posterior coverage probability of a Bayesian credible interval does not always match with the corresponding frequentist coverage probability. Quite often, they can even diverge in nontrivial ways, see for instance Wasserman [2011]. In this case, we say that the probability matching criterion is not verified. This criterion can be hard to check when the exact posterior distribution is known, and even more so when the posterior is approximated [Frazier et al., 2018], which is the cases of most applications requiring an ABC method. Another drawback of using posterior credible intervals in ABC methods is that they only take into account the inherent randomness of the model when estimating the prediction uncertainty. Nevertheless, by definition of the method, the a posteriori distribution is approximated, so there is an error due to the method to take into account.\nOur contribution. In this paper, we propose a new ABC implementation that does not require summary statistics, tolerance threshold and distance. Our method produce point estimates of the posterior distribution e.g., mean, variance, quantiles, without aiming to approximate the whole posterior distribution. It also provides associated confidence sets of the estimates. The estimates are obtained using neural networks with Monte Carlo Dropout, while the confidence sets are obtained through conformal prediction. Our ABC estimation method only requires a 1) sampling function, 2) a neural network (NN) whose architecture is tailored for the problem of interest e.g., CNN for images, GNN for data with a graph structure, RNN for sequential data, and 3) a confidence level. While 1) and 3) are common to all ABC methods, requirement 2) is more original, but still easy to implement because well known architectures can be used with minimal changes, i.e., the inclusion of Dropout (or concrete Dropout) layers. Another originality of this work is that it produces directly confidence sets with proper marginal coverage. We test the proposed ABCD-Conformal algorithm in three different problems. We compare the results against the standard ABC, the ABC Random Forest (ABC-RF) of Raynal et al. [2018] and the ABC-CNN of Akesson et al. [2022]. We measure both the accuracy of the estimation and the frequentist coverage of confidence sets. The paper is organized as follows. In section 2 we briefly present some existing ABC methods: standard ABC, ABC-CNN and ABC-RF. In section 3, we propose the ABCD-Conformal methodology, combining neural networks and conformal prediction in a likelihood-free framework. Then, we illustrate and compare all these methods on three examples in section 4, before a discussion in section 5.\n#  few approaches of Approximate Bayesian\n# 2.1 Standard ABC: assets and drawbacks\n2.1 Standard ABC: assets and drawbacks\nConsider a parametric statistical model {f(x | \u03b8): x \u2208Rd, \u03b8 \u2208\u0398} on which we assume a prior distribution \u03c0 on the parameter \u03b8 leading to the Bayesian parametric model {f(\u00b7 | \u03b8), \u03c0}. Based on the sample x = (x1, . . . , xn), the cornerstone of any Bayesian inference is the posterior distribution,\n\u03c0(\u03b8 | x) = f(x | \u03b8)\u03c0(\u03b8) \ufffd \u0398 f(x | \u03b8)\u03c0(\u03b8)d\u03b8 \u221df(x | \u03c0)\u03c0(\u03b8),\n\ufffd where f(x | \u03b8) denotes the likelihood function for the sample x and parameter \u03b8. In a setting where we assume that the likelihood is not tractable, the simple, standard ABC methodology was the first powerfu algorithm to bypass this hurdle (see Pritchard et al. [1999] or Marin et al. [2012]) and, in one of its simplest form, is described in Algorithm 1.\nAlgorithm 1: Pseudocode for the standard ABC sampler\nInput\n: A Bayesian parametric model {f(\u00b7 | \u03b8), \u03c0}, a data sample x, the size of the training set\nNtrain, summary statistics \u03b7 : Rd \u2192Rl, a (pseudo)-distance d on Rl, and a tolerance\nthreshold \u03b1 \u2208(0, 1].\nOutput: A set \u03b81, . . . , \u03b8[Ntrain\u03b1] whose distribution is approximately \u03c0(\u03b8 | x)\n1 Generation of a reference table :\n2 for j \u21901 to Ntrain do\n3\nDraw \u03b8j \u223c\u03c0;\n4\nDraw synthetic sample xj = (x1,j, . . . , xd,j)\u22a4from the model f(\u00b7 | \u03b8j)\n5 end\n6 Summary statistics and distances :\n7 for j \u21901 to Ntrain do\n8\nCompute the vector of summary statistics \u03b7(xj);\n9\nCompute the (pseudo)-distance dj = d(\u03b7(x), \u03b7(xj))\n10 end\n11 Order these distances, i.e., d(1) < d(2) < \u00b7 \u00b7 \u00b7 < d(Ntrain);\n12 Keep the \u03b8j corresponding to the \u230aNtrain\u03b1\u230bsmallest distances.\nd: Rl \u00d7 Rl \u2212\u2192R+ (\u03b7(x), \u03b7(x)) \ufffd\u2212\u2192\u2225\u03b7(x) \u2212\u03b7(x)\u2225,\nwhere \u03b7 summarizes the data and is called the vector of summary statistics. \u2225\u00b7 \u2225is typically the usu Euclidean norm. This algorithm samples \u03b8 and x from the joint posterior density\n\u03c0\u03b1,d(\u03b8, x | \u03b7(x)) = \u03c0(\u03b8)f(x | \u03b8)1\u03b1,d(x) \ufffd \u03c0(\u03b8)f(x | \u03b8)1\u03b1,d(x)dxd\u03b8,\n\ufffd where 1\u03b1,d(x) = 1 \ufffd d \ufffd \u03b7(x), \u03b7(x) \ufffd < d([\u03b1N]+1) \ufffd . The approximate Bayesian computation posterior density is defined as: \ufffd\n\ufffd \u03c0\u03b1,d(\u03b8 | \u03b7(x)) = \ufffd \u03c0\u03b1,d(\u03b8, x | \u03b7(x))dx\n\ufffd The basic idea behind ABC is that using a representative (enough) summary statistic \u03b7 coupled wit a small (enough) tolerance \u03b1 should produce a good (enough) approximation to the posterior distribution\n(1)\nthat is \u03c0\u03b1,d(\u03b8 | \u03b7(x)) \u2248\u03c0(\u03b8 | x). Different theoretical results have validated this approximation. Let us cite the work of Frazier et al. [2018] which prove that the posterior distribution concentrates on sets containing the true parameter under general assumptions and with some rates of convergence. Cite also Fearnhead and Prangle [2012] which describe the best summary statistics to perform this approximation. Nevertheless, for the algorithm to give a valid approximation of the true posterior distribution, not only \u03c0\u03b1,d has to be close to \u03c0, but also the Monte Carlo approximation error :\nneeds to be considered. In Eq. (2), dj and d(.) are defined in Algorithm 1. Here it only corresponds to the rate of convergence of the empirical measure in the classical law of large number. This question has recently attracted a great deal of attention Fournier and Guillin [2015]; Dereich et al. [2013]; Weed and Bach [2019]. There is an unavoidable curse of dimensionality : the empirical measure over a n-sample (here n = \u230aN\u03b1\u230b) is at distance n\u22121/d to the target measure. This explains why the standard ABC lacks efficiency when the dimension of x is large or when the dimension of the summaries is large. This also induces high computation times. Using a small number of relevant summaries is then crucial, but, when the size of the data tends to infinity, these summaries should converge toward an injective mapping from parameters to summaries. This injectivity depends on both the true structural model and the particular choice of \u03b7(\u00b7), hence there is no general method for finding such statistics \u03b7(\u00b7). This can restrict the (computational) use or the validity of the standard ABC algorithm, because without this condition, posterior concentration is not guaranteed.\n# 2.2 Towards a preprocessing summary statistics free ABC\nIn this section, we briefly review two attempts in the ABC literature to obtain methods depending less on the summary statistics choice (and distance) than the original ABC method.\n2.2.1 ABC-Convolutional Neural Network (Akesson et al. [2022])\n# 2.2.1 ABC-Convolutional Neural Network (Akesson et al. [2022])\nThe earliest work to bypass the choice of summary statistics in the ABC method is maybe the one of Fearnhead and Prangle [2012]. Indeed, they showed that the optimal summary statistic is the true posterior mean of the parameter of interest. Although it cannot be calculated analytically, they proposed an extra stage of simulation to estimate it. This estimate is then used as a summary statistic within a standard ABC algorithm. In practice, they used linear regression to estimate the posterior mean, with appropriate functions of the data as predictors. This approach has inspired Jiang et al. [2017] who proposed to estimate the posterior means using deep Neural Networks (NN) instead of linear regression. Akesson et al. [2022] goes a bit further by proposing to estimate them using Convolutional Neural Networks (CNN). Indeed, CNNs are often more effective than classical dense NNs for tasks involving structured grid data, such as images, audio, video and time-series data (see for example LeCun et al. [1999] or Boston et al. [2022]). The approach of Akesson et al. [2022] is then a slight modification of the standard ABC: CNN are used to estimate the posterior mean of a parameter, which is then used as a summary statistic. Hence, to have asymptotic consistency and valid frequentist coverages, the same kind of conditions are needed than for Algorithm 1. For sake of completeness, this algorithm is briefly described and commented in appendix, see Algorithm 3.\n# 2.2.2 ABC-Random Forest [Raynal et al., 2018]\nAnother interesting approach is the one of Raynal et al. [2018]. Their goal is slightly different from the goal of standard ABC, as they do not seek to approximate the full posterior distribution. Instead, they focused on functional of the posterior distribution, like posterior mean, posterior variance or posterior quantiles. Indeed, they are easier to interpret and report than the whole posterior distribution, and are the main interest of practitioners. Formally, they predict a scalar transform T(\u03b8) of a multidimensional parameter \u03b8. Without\n(2)\nwhere c(x) is a normalizing constant. From this functional, we easily recover posterior mean, posterior variance or posterior quantiles using respectively T(\u03b8) = \u03b8, T(\u03b8) = \u03b82 and T(\u03b8) = \u03b8, and T(\u03b8) = 1\u03b8\u2264q. To mitigate the curse of dimensionality, the dimension of (3) can be reduced by using summary statistics, i.e., the focus is now on\nwhich is hoped to be a good approximation of \u03c8(x) but much simpler to estimate. Although many studies focus on how to calculate efficiently an average from a sample see for example Gobet et al. [2022] and references therein, the idea of directly approximating the functional of interest in Eq. (3) (or similarly in Eq. (4)) bypasses the costly step of approximating the empirical measure Eq. (2), especially when the dimension is large. Raynal et al. [2018] then proposed approximations of \u03c8\u03b7(x), of the posterior variance V\u03c0[T(\u03b8) | \u03b7(x)] and of posterior quantiles, using (regression) random forests [Breiman, 2001]. Their algorithm is briefly described and commented in Algorithm 4. A drawback of this method (as presented in Raynal et al. [2018] ) is that it only works on unidimensional parameter inference. When the parameter \u03b8 of interest is multidimensional, Raynal et al. [2018] recommends constructing one random forest (RF) for each unidimensional component of \u03b8, and covariance between components is not taken into account. Additional RFs might be constructed if one is interested in estimating posterior covariance between pairs of components. Piccioni et al. [2022] used, for instance, this approach with an additional sensitivity analysis. To quantify the uncertainty of their estimation, Raynal et al. [2018] proposes, as usual, to use their approximated estimation of quantiles and variances (in Raynal et al. [2018, Section 2.3.3]) but also to use the so-called out-of-bag simulations inherent from RF algorithms (in Raynal et al. [2018, Section 2.3.4]). This latter gives a first hint to quantify uncertainty without giving nevertheless precise confidence intervals.\n# 3 The ABCD-Conformal algorithm\nIn this section, we propose a new ABC method based on Deep learning with Dropout and conformal predic tion, entitled ABCD-Conformal.\n# 3.1 Motivation and principle\nLike Raynal et al. [2018], instead of approximating the whole posterior \u03c0(\u03b8 | x), we are interested in estimating functional of the form \u03c8(x) = E\u03c0[T(\u03b8) | x] (see equation Eq. (3)). The ABCD-Conformal method will both output a prediction and an associated exact confidence interval. Our method uses Neural Networks (NN), similarly to Akesson et al. [2022]. However, in contrast to their approach, the NN directly outputs the functional of interest and an associated uncertainty, enabling us to bypass the nearest neighbors step detailed in Section 2.1 to overcome the shortcomings of standard ABC. To use conformal prediction, we need both a prediction and an associated measure of uncertainty that we will name heuristic uncertainty and denote \ufffdV(x). To this end, we will use neural networks with Dropout layers. Dropout layers are classically used to prevent overfitting for neural networks, see Hinton et al. [2012] and Srivastava et al. [2014a]. However, they can also be used to associate uncertainties e.g., variance with predictions from neural networks, see the Bayesian Neural Network literature [Gal, 2016]. We can then obtain valid confidence sets using these uncertainties through conformal prediction [Angelopoulos and Bates, 2023]. These two steps are described in Sections 3.2 and 3.3.\n(3)\n(4)\n# d heuristic uncertainty using Monte Carlo Dropou\nFor an introduction to Deep Learning, we refer the reader to Courville et al. [2016]; Gu\u00e9ron [2019] and references therein. Neural networks have a strong predictive power, however they are often regarded as black boxes making their predictions not explainable and without uncertainty. This is an issue in many areas like medicine or autonomous vehicles, where false predictions can have big consequences Filos et al. [2020]. A lot of efforts have been made recently either to explain predictions, see the Explainable AI [Xu et al., 2019; Dantas et al., 2023] literature or to associate an uncertainty with the prediction, see Izmailov et al. [2021] or Kompa et al. [2021] among many others approaches. In this paper we focus on the Dropout method proposed in Gal [2016] and Gal and Ghahramani [2016] even if more recent uncertainty methods might be more powerful as for example Variational BNN methods Folgoc et al. [2021]; Jospin et al. [2022]. We choose the classical Dropout method as it is generic, simple to implement and to integrate into an already existing NN architecture. As explained in Section 3.3, using conformal prediction will enable us to compensate the shortcomings of Dropout. At every training step, Dropout layers randomly drop (set to zero) some neurons (elements of the weights matrices) [Hinton et al., 2012; Srivastava et al., 2014b] according to some user-choice parameters (such as the Dropout probability). This mechanism prevents complex co-adaptations on the training data: an input or hidden unit cannot rely on other hidden units being present, this can also be seen as a regularization method during the training, to prevent overfitting. Standard Dropout layers are deactivated during test/prediction phase. In their seminal paper Gal and Ghahramani [2016] proposed to activate Dropout during test, introducing randomness in the NN, hence in the output. Repeating the same prediction task several times produces a distribution and is sometimes called Monte Carlo Dropout (MC Dropout). We briefly describe the MC Dropout procedure. Let D = {(xj, \u03b8j), j = 1, . . . , N} be the training set, containing inputs x and outputs \u03b8, where x corresponds in the ABC framework to the sampled data and \u03b8 (or a transformation T(\u03b8)) to the unknown parameter of interest. The vector of parameters of the network (weights and bias) is denoted \u03c9. During training, the goal is to find parameters of the network \u03c9 that are likely to have generated the outputs (\u03b8j, j = 1, . . . , N), given the inputs (xj, j = 1, . . . , N). For this goal, we use another Bayesian inference model to infer such \u03c9 and we then aim to estimate the distribution \u03c0(\u03c9 | D). After training, the goal is then to predict the parameter of interest \u03b8 associated to a new x. The posterior distribution of interest is then:\nThe density \u03c0(\u03c9 | D) is quite complex and cannot be evaluated analytically. It is approximated using a variational approach by q(\u03c9). The approximate posterior distribution of interest is then given by \ufffd\n\ufffd The first two moments of this distribution can be estimated empirically following Monte Carlo integration with K samples. For each Monte Carlo sample, different units of the network are randomly dropped out Hinton et al. [2012], we note \ufffd \u03c9k the estimated weights of the associated network with dropped units. Hence, these weights \ufffd \u03c9k are different for each Monte Carlo iteration. Denote by f \u03c9(x) the model\u2019s stochastic output for input x and parameters \u03c9. Assuming that \u03b8 | x, \u03c9 \u223cN(f \u03c9(x), \u03c4 \u22121I) and \ufffd \u03c9k \u223cq(\u03c9), t = 1, . . . , K; an unbiased (consistent) estimator for Eq(\u03b8|x)[\u03b8 | x] is given by:\n\ufffd \ufffd   which corresponds to the average of K stochastic forward passes through the network with Dropout. An unbiased (consistent) estimator for the second moment Eq(\u03b8|x)[\u03b8T \u03b8] is given by:\n(5)\n(6)\n(7)\n\ufffdV[\u03b8 | x] = \u03c4 \u22121I \ufffd\ufffd\ufffd\ufffd \ufffdVa[\u03b8|x] + 1 K K \ufffd k=1 f \ufffd \u03c9k(x)T f \ufffd \u03c9k(x) \u2212\ufffd\u03b8(x)T \ufffd\u03b8(x) \ufffd \ufffd\ufffd \ufffd ,\n\ufffd \ufffd\ufffd \ufffd \ufffd which corresponds to the inverse model precision plus the sample variance of K stochastic forward passes through the network with Dropout. Hence, using Dropout and its associated randomness, we can obtain an estimate of Eq(\u03b8|x)[\u03b8], as well as associated uncertainty. This estimate can be considered as an approximation of E\u03c0[\u03b8] because q(\u03b8 | x) is an approximation of the posterior of interest \u03c0(\u03b8 | x, D). Moreover, Gal [2016] interprets the uncertainty Eq. (8) as the sum of an aleatoric uncertainty \ufffdVa[\u03b8 | x] and an epistemic uncertainty \ufffdVe[\u03b8 | x]. The aleatoric uncertainty \ufffdVa[\u03b8 | x] is interpreted as the noise in the data, it is the result of measurement imprecision (it is often modelled as part of the likelihood, and this is often a Gaussian corrupting noise). The epistemic uncertainty \ufffdVe[\u03b8 | x] is interpreted as the model uncertainty: uncertainty in the model parameters or in the structure of the model. The total predictive uncertainty \ufffdV[\u03b8 | x] combines these both types of uncertainties and will be used as our heuristic uncertainty \ufffdV(x). In practice, for each of the K Monte Carlo iterations, the weights \ufffd \u03c9k are different, and the NN with input x gives as outputs f \ufffd \u03c9k(x) and \u03c4 \u22121 \ufffd \u03c9k . The aleatoric uncertainty \u03c4 \u22121 is estimated by the mean of the \u03c4 \u22121 \ufffd \u03c9k , and the epistemic uncertainty by the sample variance of the f \ufffd \u03c9k(x). The main drawback of Dropout layers is that the Dropout probability is a new model hyperparameter. To circumvent this issue, Gal et al. [2017] propose a Dropout variant, called Concrete Dropout, that allows for automatic tuning of this probability in large models, using gradient methods. It improves performance and tunes automatically the Dropout rate, producing better uncertainties compared to classical Dropout. This is the Dropout version we will be using in the following. Remark 1. Gal [2016] and Gal and Ghahramani [2016] showed that optimizing any neural network with Dropout is equivalent to a form of approximate inference in a probabilistic interpretation of the model. In\n\ufffd \ufffd\ufffd \ufffd \ufffd which corresponds to the inverse model precision plus the sample variance of K stochastic forward passes through the network with Dropout. Hence, using Dropout and its associated randomness, we can obtain an estimate of Eq(\u03b8|x)[\u03b8], as well as associated uncertainty. This estimate can be considered as an approximation of E\u03c0[\u03b8] because q(\u03b8 | x) is an approximation of the posterior of interest \u03c0(\u03b8 | x, D). Moreover, Gal [2016] interprets the uncertainty Eq. (8) as the sum of an aleatoric uncertainty \ufffdVa[\u03b8 | x] and an epistemic uncertainty \ufffdVe[\u03b8 | x]. The aleatoric uncertainty \ufffdVa[\u03b8 | x] is interpreted as the noise in the data, it is the result of measurement imprecision (it is often modelled as part of the likelihood, and this is often a Gaussian corrupting noise). The epistemic uncertainty \ufffdVe[\u03b8 | x] is interpreted as the model uncertainty: uncertainty in the model parameters or in the structure of the model. The total predictive uncertainty \ufffdV[\u03b8 | x] combines these both types of uncertainties and will be used as our heuristic uncertainty \ufffdV(x). In practice, for each of the K Monte Carlo iterations, the weights \ufffd \u03c9k are different, and the NN with input x gives as outputs f \ufffd \u03c9k(x) and \u03c4 \u22121 \ufffd \u03c9k . The aleatoric uncertainty \u03c4 \u22121 is estimated by the mean of the \u03c4 \u22121 \ufffd \u03c9k , and the epistemic uncertainty by the sample variance of the f \ufffd \u03c9k(x). The main drawback of Dropout layers is that the Dropout probability is a new model hyperparameter. To circumvent this issue, Gal et al. [2017] propose a Dropout variant, called Concrete Dropout, that allows for automatic tuning of this probability in large models, using gradient methods. It improves performance and tunes automatically the Dropout rate, producing better uncertainties compared to classical Dropout. This is the Dropout version we will be using in the following. Remark 1. Gal [2016] and Gal and Ghahramani [2016] showed that optimizing any neural network with Dropout is equivalent to a form of approximate inference in a probabilistic interpretation of the model. In other words, the optimal parameters found through the optimization of a Dropout neural network are the same as the optimal variational parameters in a probabilistic Bayesian neural network with the same structure (the parameters are found through a variational inference approach). It means that a network trained with Dropout is equivalent to a Bayesian Neural Network and possesses all the properties of such a network (see MacKay [1992] or Neal [2012] concerning Bayesian Neural Networks). For this equivalence to be true, only one condition should be verified in the variational inference approach, which is about the Kullback-Leibler divergence between the prior distribution of the parameters \u03c9 and an approximating distribution for \u03c9. We will not need to verify such assumption because the theoretical guaranty of our confidence sets will be assured through the conformal prediction.\n# 3.3 Conformal prediction\nFor a test input x, the NN with concrete Dropout method enables us to obtain an approximation \ufffd\u03b8(x) of E\u03c0[\u03b8 | x], as well as associated heuristic uncertainties \ufffdV(x). However, it does not give valid confidence sets. In order to obtain such confidence sets, we propose to apply a conformal procedure, as explained in Angelopoulos and Bates [2023]. Conformal prediction is indeed a straightforward way to generate valid confidence sets for a wide variety of models from a proxy of the uncertainty. The better this proxy will be, the smaller the confidence interval will be. It requires a small amount of additional calibration data \ufffd (\u03b8j, xj), j = 1, . . . , Ncal \ufffd , hundreds of calibration data are theoretically enough (see Eq. (12) below). We propose to use as heuristic uncertainties \ufffdV(x) one of the variances obtained from the NN with Dropout in Section 3.2, see Eq. (8). This heuristic uncertainty can be transformed into a rigorous confidence interval C(x) through the following conformal procedure.\n(8)\nUsing the heuristic uncertainty \ufffdV(x), we compute a score s(\u03b8, x) for each data sample in the calibration set, from these scores we can define \ufffdq the conformal quantile and then a confidence set. More precisely, as in Messoudi et al. [2022], the score function will be\n\ufffd This corresponds to an ellipsoid which center is \ufffd\u03b8(x) and covariance matrix is \ufffdV(x)\u22121/\ufffdq2. In one dimension, we have s(\u03b8, x) = |\u03b8 \u2212\ufffd\u03b8(x)|/ \ufffd \ufffdV(x), and\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd nfidence set satisfies the marginal coverage property, i.e., for a chosen \u03b4 \u2208[0, 1] :\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd This confidence set satisfies the marginal coverage property, i.e., for a chosen \u03b4 \u2208[0, 1] :\n1 \u2212\u03b4 \u2264P[\u03b8 \u2208C(x)] \u22641 \u2212\u03b4 + 1 Ncal + 1.\nIndeed, see [Angelopoulos and Bates, 2023, Appendix D] (in particular the right-hand side inequality needs that random variables s(\u03b8j, xj) to be continuous). This probability is marginal over the randomness in the calibration and test points [Angelopoulos and Bates, 2023], see Algorithm 2. One great advantage of this procedure is that it is easy to implement, fast and generic. Moreover, the obtained prediction intervals are non-asymptotic, with no distributional or model assumptions. The only assumption needed is that the calibration data and the test data are i.i.d [Vovk et al., 1999] which is naturally the case in our setting. Finally, this procedure provides a frequentist perspective on a Bayesian approach, reconciling the two points of view. The performance of this approach in terms of intervals length depends only on the quality of the uncertainty measure used. A \u201cgood\u201d uncertainty measure should reflect the magnitude of model error: it should be smaller for easy input, and larger for hard ones. If the uncertainty measure is \u201cbad\u201d, then the intervals obtained by the conformal procedure will be quite large and their lengths will not differ a lot between easy and hard inputs. From the previous MC Dropout method, three types of uncertainties were obtained: epistemic, aleatoric and overall (sum of aleatoric and epistemic). In section 4, we show that depending on the examples, one, or the other of these uncertainties will produce the best results.\nark 2. Note that a conditional coverage would be preferable, that is\nP[\u03b8 \u2208C(x) | x] \u22651 \u2212\u03b4.\nIndeed, marginal coverage does not guarantee the coverage of C(x) for any specific x, only the average coverage over the whole domain. However, this conditional property is not guaranteed by the conformal procedure in general, but some metrics can be used to check how close we are to this stronger property, see Angelopoulos and Bates [2023].\n# 3.4 Implementation of ABCD-Conformal\nThe training step of the proposed method is as follows. a) Generate the training dataset i.e., reference table in ABC vocabulary and the calibration dataset, of sizes Ntrain and Ncal respectively. b) The NN with Dropout is trained using the reference table. In principle, one could select the network architecture using an extra validation dataset. c) Monte Carlo Dropout prediction is performed, that is, each sample of the calibration dataset is passed through the trained network with Dropout, K times. For each of these samples,\n(9)\n(10)\n(11)\n(12)\n(13)\nan approximation of E\u03c0[\u03b8 | x] is obtained as the average of the K predictions (see Eq. (7)), associated with an heuristic uncertainty which is here the variance, see Eq. (8). d) Using this approximation and associated uncertainty, the score of each sample of the calibration dataset is calculated. The conformal quantile is then the \u2308(Ncal+1)(1\u2212\u03b4)\u2309 Ncal quantile of the calibration scores. Then, for a new data sample x, to approximate E\u03c0[\u03b8 | x] and obtain an associated confidence interval: e) Monte Carlo Dropout prediction is performed to obtain an approximation of E\u03c0[\u03b8 | x] (average of the predictions) and the associated heuristic uncertainty (variance of the predictions). A confidence interval is then calculated using the conformal quantile calculated in d) Eq. (11). The pseudocode of the algorithm proposed is detailed in Algorithm 2.\n# 4 Applications\nIn this section, we will illustrate our model on three examples: The Moving Average 2 model (Section 4.1), a two-dimensional Gaussian random field (Section 4.2) and a Lokta-Volterra model (Section 4.3). For all these examples, we compare the ABCD-Conformal algorithm with the standard ABC, the ABCCNN of Akesson et al. [2022] and the ABC-RF of Raynal et al. [2018] (only for the two-dimensional Gaussian random field of Section 4.2 because it is not designed for multidimensional output). We simulate a reference table (training set) of size Ntrain. For the ABC-CNN and the ABCD-Conformal we need a validation set of size Nval, plus a calibration set of size Ncal for the ABCD-Conformal. We will test these methods on a test set of size Ntest. The sizes used in the different examples are given in Table 1). These datasets are the same for the different methods studied. To approximate the posterior, the standard ABC and the ABC-CNN are used with an \u03b1 acceptance ratio, hence \u03b1 \u00d7 Ntrain generated samples are kept to approximate the posterior distribution. Also, the type of results obtained with the different methods are not exactly the same. Using the standard ABC approach and the ABC-CNN, we obtain approximations of the whole posterior distributions. Using ABC-RF or ABCD-Conformal approach, we obtain directly estimates of some posterior functional estimate of the parameter of interest \u03b8, in particular the posterior expected values E\u03c0[\u03b8 | x] which can be used to estimate the true parameters \u03b8. To compare the methods, we then compute estimates of the posterior expected values from the approximated posterior distributions obtained with standard ABC and ABC-CNN. Performing that for the Ntest datasets of the test set, we can compare these estimates with the true values of the parameters used to generate these datasets. We compute NMAE as well as standard deviations of the absolute differences between the estimates and the true values of the parameters. More precisely, for a parameter of interest \u03b8, we compute\n\ufffd where \u03b8i is the true value of \u03b8 for the ith dataset, and \ufffd\u03b8i the estimate of \u03b8 for the ith dataset. Note that this NMAE is defined for unidimensional parameters. Hence, in case of parameters of interest that are multidimensional, we calculate it for each marginal of \u03b8 separately. For standard ABC and ABC-CNN, \ufffd\u03b8i corresponds to the empirical mean of the \u03b8 corresponding to the \u03b1Ntrain samples kept to approximate the posterior distribution of \u03b8 given the ith dataset. For ABCD-Conformal, \ufffd\u03b8i is the empirical mean of Nval stochastic forward passes through the network with Dropout, given the ith dataset as input (see Eq. (7)). To have an idea of the variability of the absolute errors, we compute sd(|\u03b8 \u2212\ufffd\u03b8|) where\n\ufffd \ufffd \ufffd Finally, as we are interested by the uncertainty associated with an estimate, we compare credible and onfidence sets for the three methods. For standard ABC and ABC-CNN, we used the \u03b4/2 and 1 \u2212\u03b4/2\nAlgorithm 2: Pseudocode for ABCD-Conformal Input : A Bayesian parametric model {f(\u00b7 | \u03b8), \u03c0}, a data sample x, integers Ntrain, Ncal and K representing sizes of training and calibration sets, and the number of stochastic passes for the Dropout. \u03b4 between 0 and 1 to obtain (1 \u2212\u03b4)% confidence sets. Output: An approximation of the posterior expected value E\u03c0[\u03b8 | x] and a confidence interval for \u03b8. 1 a) Generation of a reference table (training dataset) and a calibration dataset : 2 These datasets are of sizes Ntrain and Ncal respectively. For the reference table for instance: 3 for j \u21901 to Ntrain do 4 Draw \u03b8j \u223c\u03c0; 5 Draw synthetic sample xj = (x1,j, . . . , xd,j)\u22a4from the model f(\u00b7 | \u03b8j); 6 end 7 b) Train a CNN with concrete Dropout on the reference table: inputs are the {xj, j = 1, . . . , Ntrain}, and the outputs are the {\u03b8j, j = 1, . . . , Ntrain}. 8 c) Monte Carlo Dropout prediction on the calibration set: 9 for j \u21901 to Ncal do 10 for k \u21901 to K do 11 xj is given as input to the trained network with Dropout to obtain outputs f \ufffd \u03c9k(xj) and \u03c4 \u22121 \ufffd \u03c9k ; 12 end 13 Obtain \ufffd\u03b8j by averaging the f \ufffd \u03c9k(xj) outputs (see eq. (7))), \u03c4 \u22121 by averaging the \u03c4 \u22121 \ufffd \u03c9k ; 14 and an associated uncertainty \ufffdV(xj) that can be \ufffdVa[\u03b8j | xj], \ufffdVe[\u03b8j | xj] or \ufffdV[\u03b8j | xj] (see eq. (8)). 15 end 16 d) Computation of the conformal quantile on the calibration set : 17 for j \u21901 to Ncal do 18 Compute the calibration score sj using the score function, see eq. (9): sj = \ufffd (\u03b8j \u2212\ufffd\u03b8j)t\ufffdV(xj)\u22121(\u03b8j \u2212\ufffd\u03b8j), 19 end 20 Compute the conformal quantile \ufffdq as the \u2308(Ncal+1)(1\u2212\u03b4)\u2309 Ncal quantile of the calibration scores s1, . . . , sNcal. 21 e) For the new data sample x, approximation of E\u03c0[\u03b8 | x] and confidence set for \u03b8 : 22 for k \u21901 to K do 23 x is given as input to the trained network, to obtain an output f \ufffd \u03c9k(x); 24 end 25 Obtain \ufffd\u03b8(x) an approximation of E\u03c0[\u03b8 | x] by averaging these outputs (see Eq. (7)), and an associated uncertainty \ufffdV(x) (using Eq. (8)); 26 The confidence set for \u03b8 is an ellipsoid which center is \ufffd\u03b8(x) and covariance matrix is \ufffdV(x)\u22121/\ufffdq2, see Eq. (10).\nAlgorithm 2: Pseudocode for ABCD-Conformal\nInput\n: A Bayesian parametric model {f(\u00b7 | \u03b8), \u03c0}, a data sample x, integers Ntrain, Ncal and K\nrepresenting sizes of training and calibration sets, and the number of stochastic passes for\nthe Dropout. \u03b4 between 0 and 1 to obtain (1 \u2212\u03b4)% confidence sets.\nOutput: An approximation of the posterior expected value E\u03c0[\u03b8 | x] and a confidence interval for \u03b8.\n1 a) Generation of a reference table (training dataset) and a calibration dataset :\n2 These datasets are of sizes Ntrain and Ncal respectively. For the reference table for instance:\n3 for j \u21901 to Ntrain do\n4\nDraw \u03b8j \u223c\u03c0;\n5\nDraw synthetic sample xj = (x1,j, . . . , xd,j)\u22a4from the model f(\u00b7 | \u03b8j);\n6 end\n7 b) Train a CNN with concrete Dropout on the reference table: inputs are the\n{xj, j = 1, . . . , Ntrain}, and the outputs are the {\u03b8j, j = 1, . . . , Ntrain}.\n8 c) Monte Carlo Dropout prediction on the calibration set:\n9 for j \u21901 to Ncal do\n10\nfor k \u21901 to K do\n11\nxj is given as input to the trained network with Dropout to obtain outputs f \ufffd\n\u03c9k(xj) and \u03c4 \u22121\n\ufffd\n\u03c9k ;\n12\nend\n13\nObtain \ufffd\u03b8j by averaging the f \ufffd\n\u03c9k(xj) outputs (see eq. (7))), \u03c4 \u22121 by averaging the \u03c4 \u22121\n\ufffd\n\u03c9k ;\n14\nand an associated uncertainty \ufffdV(xj) that can be \ufffdVa[\u03b8j | xj], \ufffdVe[\u03b8j | xj] or \ufffdV[\u03b8j | xj] (see eq. (8)).\n15 end\n16 d) Computation of the conformal quantile on the calibration set :\n17 for j \u21901 to Ncal do\n18\nCompute the calibration score sj using the score function, see eq. (9):\nsj =\n\ufffd\n(\u03b8j \u2212\ufffd\u03b8j)t\ufffdV(xj)\u22121(\u03b8j \u2212\ufffd\u03b8j),\n19 end\n20 Compute the conformal quantile \ufffdq as the \u2308(Ncal+1)(1\u2212\u03b4)\u2309\nNcal\nquantile of the calibration scores\ns1, . . . , sNcal.\n21 e) For the new data sample x, approximation of E\u03c0[\u03b8 | x] and confidence set for \u03b8 :\n22 for k \u21901 to K do\n23\nx is given as input to the trained network, to obtain an output f \ufffd\n\u03c9k(x);\n24 end\n25 Obtain \ufffd\u03b8(x) an approximation of E\u03c0[\u03b8 | x] by averaging these outputs (see Eq. (7)), and an\nassociated uncertainty \ufffdV(x) (using Eq. (8));\n26 The confidence set for \u03b8 is an ellipsoid which center is \ufffd\u03b8(x) and covariance matrix is \ufffdV(x)\u22121/\ufffdq2, see\nEq. (10).\nquantiles of the marginals of the approximated posterior distributions to obtain credible intervals, for each component of \u03b8. The confidence sets are directly given by the ABC-RF and ABCD-Conformal methods. We set \u03b4 = 0.05 (only for unidimensional parameters for ABC-RF). Two uncertainty measures were used for comparison in ABCD-Conformal: the overall uncertainty and the epistemic uncertainty returned by the Dropout procedure. ABCD-Conformal is the only method to give confidence ellipses or ellipsoids for multidimensional parameters. To be able to compare its results with the other methods, we also computed confidence intervals for each component of \u03b8, by using the diagonal terms of the uncertainty measure \ufffdV(x), to be able to compute confidence intervals (by analogy with a variance-covariance matrix, this corresponds to using only the variance terms). For all methods, the coverage is estimated by the number of true values of \u03b8 or marginals of \u03b8 in the obtained sets, for the Ntest simulations from the test set. We then compare the coverage of these credible and confidence sets for these methods, as well as the mean lengths of credible and confidence intervals for marginals of \u03b8. All the parameters used in the different examples are synthesized in Table 1.\n<div style=\"text-align: center;\">ers used in the different examples are synthesized in Table 1.</div>\nExample\nNtrain\nNval\nNcal\nNtest\n\u03b1\nSection 4.1\n104\n103\n103\n103\n0.01\nSection 4.2\n7 \u00d7 103\n1900\n103\n102\n0.01\nSection 4.3\n105\n103\n103\n103\n0.005\nTable 1: Sizes of datasets used for the different methods: Ntrain for the training set, Nval for the validation set, Ncal for the calibration set and Ntest for the test set, and \u03b1 the tolerance threshold used for the ABC standard and ABC-CNN. The source codes to run the following examples are available at https://forgemia.inra.fr/mistea/ codes_articles/abcdconformal. For the Moving Average 2 and the Lokta Volterra examples, all the datasets and objects needed to find our exact results are provided.\nTable 1: Sizes of datasets used for the different methods: Ntrain for the training set, Nval for the validatio set, Ncal for the calibration set and Ntest for the test set, and \u03b1 the tolerance threshold used for the ABC standard and ABC-CNN.\nThe source codes to run the following examples are available at https://forgemia.inra.fr/mistea/ codes_articles/abcdconformal. For the Moving Average 2 and the Lokta Volterra examples, all the datasets and objects needed to find our exact results are provided.\n# 4.1 The Moving Average 2 model\n# 4.1.1 The model\nThe Moving Average 2 model (MA(2)) is a simple benchmark example used in Bayesian and ABC literature (Marin and Robert [2007], Marin et al. [2012], Jiang et al. [2017], Wiqvist et al. [2019] or Akesson et al [2022] for instance). It is defined for observations X1, . . . , Xp;\nwhere (Zj)\u22122<j\u2264p is an i.i.d. sequence of standard Gaussian N(0, 1). This model is identifiable in the following triangular region D: \u22122 < \u03b81 < 2, \u03b81 + \u03b82 > \u22121, \u03b81 \u2212\u03b82 < 1; See for instance Marin and Robert [2007][Chapter 5] or [Marin et al., 2012, Section 2]. We assume that the prior distribution on (\u03b81, \u03b82) is the uniform distribution over this triangular region. This model allows for exact calculation of the posterior (\u03b81, \u03b82) | X. The goal is to obtain posterior estimates of the bi-dimensional vector of parameters (\u03b81, \u03b82) from an observed dataset of length p, (X1, . . . , Xp). In the following, we will use p = 100.\n# 4.1.2 Algorithm parametrization\nNumber of used samples are resumed in Table 1. For the standard ABC, the summary statistics chosen ar the first two autocovariances:\n\u03c41 = p \ufffd j=2 xjxj\u22121 and \u03c42 = p \ufffd j=3 xjxj\u22122.\nTo compare two samples, we then use the L2 distance between the two associated vector of summary statistics. In ABC-CNN method, the distance function between a sample from the test set and a sample from the training set is the quadratic distances L2 distance between the parameters predicted by the CNN for the test sample, and true parameters used to simulate the training sample. Regarding ABC-CNN and ABCD-Conformal, architectures of neural networks are the same for the two methods: 3 convolutional 1D layers with 64 neurons and a kernel size of 3 followed by max-pooling for the first two layers and by a flatten layer for the third one. Then, 3 dense layers of 100 neurons. Each of the layers uses the relu activation function. The raw samples of length p = 100 are the inputs of the neural networks, and the outputs are the bi-dimensional associated vectors of parameters (\u03b81, \u03b82).\n# 4.1.3 Results\nAll indicators detailed in the beginning of Section 4 are presented in Table 2. For three test samples, Figure 1 shows predicted parameters \u03b8 associated with confidence ellipses, and true values. On the 1000 test samples the coverage is of 92.6% using the epistemic uncertainty, and 96.3% using the overall uncertainty. To highlight the sharpness of the coverage from the ABCD-Conformal algorithm, we have repeated the experiment with 10 different calibration and test sets, and we obtain (in mean) coverages of 95.00% using the overall uncertainty. To compare ABCD-Conformal with the other methods, we computed confidence intervals for each component of \u03b8. Figure 2 shows the predicted parameters \ufffd\u03b81 and \ufffd\u03b82 estimated by the ABCD-Conformal method against the true values for the 1000 test samples, and confidence intervals in gray.\n \ufffd \ufffd\nStandard ABC\nABC-CNN\nABCD-Conformal\nABCD-Conformal\noverall\nepistemic\nNMAE1\n0.1852\n0.1963\n0.1973\n0.1973\nNMAE2\n0.2644\n0.2709\n0.2703\n0.2703\nsd(|\u03b81 \u2212\ufffd\u03b81|)\n0.0953\n0.1001\n0.1004\n0.1004\nsd(|\u03b82 \u2212\ufffd\u03b82|)\n0.1061\n0.1098\n0.1018\n0.1018\nmean length conf. intervals \u03b81\n0.6003\n0.2889\n0.7340\n0.6413\nmean length conf. intervals \u03b82\n0.6385\n0.2891\n0.9173\n0.6930\ncoverage conf. intervals \u03b81\n94.8%\n62.1%\n95.2%\n92.7%\ncoverage conf. intervals \u03b82\n93.7%\n61.2%\n95.4%\n94.8%\ncoverage conf. ellipses \u03b8 (2D)\nNA\nNA\n96.3%\n92.6%\nTable 2: Moving Average 2 example: comparison of confidence intervals for each component of \u03b8, obtaine with standard ABC, ABC-CNN and ABCD-Conformal (using overall or epistemic uncertainty as uncertaint heuristic for the conformal procedure). These indicators are computed on a test set of size 103.\nWe can see in Table 2 that the three methods obtain quite similar results concerning NMAE and standard deviation of the absolute error, with a slight advantage for standard ABC. Some differences can be noted for the coverages, and the mean lengths of the confidence intervals. The coverage is sharp for standard ABC and ABCD-Conformal. However, it is too small for ABC-CNN. This is because the approximated posteriors were too peaked using this method, the dispersions around the mean values were too small, leading confidence intervals with a small coverage (and hence useless). Concerning the mean lengths, ABC-CNN has the smallest ones, but as the confidence intervals are too narrow we can not say that this method outperforms the others. The standard ABC gives the best results in terms of mean lengths, followed closely by ABCD-Conformal using the epistemic uncertainty, then by ABCD-Conformal using the overall uncertainty. It is not surprising that the standard ABC performs really well, as we used good summary statistics. Moreover, as showed by Frazier et al. [2018], conditions for good asymptotic properties are verified on this example, which is quite rare. To sum up, ABC-CNN does not provide sufficient guarantees for these confidence intervals, and ABCDConformal using epistemic uncertainty performs almost as well as the standard ABC, which is here in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f30d/f30d9a1a-2748-4e87-a5b8-41299939b4cd.png\" style=\"width: 50%;\"></div>\nFigure 1: Moving Average 2 example: predicted parameters \u03b8 (in red) and true values (in blue) estimated by the ABCD-Conformal, with associated confidence ellipses obtained by the conformal procedure (in purple, for the epistemic uncertainty). A: a case in which the true value is outside the confidence ellipse, B: a case in which the true value is outside the confidence ellipse but quite close, C: a case in which the true value is inside the confidence ellipse.\nfavourable estimation conditions. Moreover, ABCD-Conformal is the only method that gives confidence sets directly in dimension two (confidence ellipses).\n# 4.2 Two-dimensional Gaussian random field\n4.2.1 The model\n# 4.2.1 The model\nWe study stationary isotropic Gaussian random fields on the domain [0, 5] \u00d7 [0, 5] with a regular grid of size 100\u00d7100, with exponential covariance functions [Wood and Chan, 1994]. This covariance between two points and is given by:\nWe study stationary isotropic Gaussian random fields on the domain [0, 5] \u00d7 [0, 5] with a regular grid of size 100\u00d7100, with exponential covariance functions [Wood and Chan, 1994]. This covariance between two points z1 and z2 is given by: \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd\n\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd with \u03b8 the range (or scale) parameter, which is the unknown parameter. We assume that the prior distribution on \u03b8 is the uniform distribution between 0 and 1.\n# 4.2.2 Algorithm parametrization\nNumber of used samples are resumed in Table 1. For the standard ABC and the ABC-RF, we use two summary statistics: the Moran\u2019s I statistics from lag 1 to 5 (that is the Moran\u2019s correlogram from lags 1 to 5), and the semi-variogram up to a distance of 20 (15 values kept per variogram) (see Cliff and Ord [1981] for these notions). We assume the spatial weights matrix to be row-standardized, and the neighbors of a pixel being the 4-nearest pixels. As all Gaussian fields are simulated on the same grid, the variograms of the different fields are calculated at exactly the same distances, so they are comparable. For the standard ABC, the distance used to compare two Gaussian random fields is the sum of the quadratic distance between their Moran\u2019s correlograms and of the quadratic distance between their semi-variograms. Concerning the architectures of neural networks for ABC-CNN and ABC-Dropout, we used 3 convolutional layers having 32, 64 and 64 neurons and the relu activation function, followed by 2 dense layers having both 64 neurons and the relu activation function. The raw samples given as inputs of the neural networks are the 2D Gaussian random fields, and the associated outputs are the estimations of the parameter \u03b8.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd67/cd670d31-db0e-4239-987c-c4c4bc53d369.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">True value of \u03b81</div>\n<div style=\"text-align: center;\">Figure 2: Moving Average 2 example: predicted parameters \ufffd\u03b81 and \ufffd\u03b82 estimated by the ABCD-Conformal against the true values for the 1000 test samples, and confidence sets obtained by the conformal procedure (in gray intervals,</div>\n<div style=\"text-align: center;\">Figure 2: Moving Average 2 example: predicted parameters \ufffd\u03b81 and \ufffd\u03b82 estimated by the ABCD-Conformal against the true values for the 1000 test samples, and confidence sets obtained by the conformal procedure (in gray intervals, for the epistemic uncertainty).</div>\n# 4.2.3 Results\nAll indicators detailed in the beginning of Section 4 are presented in Table 3. Figure 3 shows the predicted parameters \ufffd\u03b8 estimated by the ABCD-Conformal method against the true values for the 100 test samples, and using gray intervals.\n \ufffd\nStandard\nABC-CNN\nABC-RF\nABCD-Conformal\nABCD-Conformal\nABC\noverall\nepistemic\nNMAE\n0.0436\n0.0223\n0.0154\n0.0303\n0.0303\nsd(|\u03b8 \u2212\ufffd\u03b8|)\n0.0229\n0.0106\n0.0077\n0.0141\n0.0141\ncoverage confidence sets \u03b8\n100%\n88%\n99%\n93%\n95%\nmean length confidence sets \u03b8\n0.1313\n0.0307\n0.0414\n0.0684\n0.0718\nTable 3: Comparison of standard ABC, ABC-CNN, ABC-RF and ABCD-Conformal on the two-dimensiona Gaussian random field 2 example. These indicators are computed on a test set of size 102.\nIn Table 3, we see that in this example, ABC-RF outperforms the others methods, with the smallest NMAE and sd(|\u03b8 \u2212\ufffd\u03b8|), while having a very good coverage, 99%, which is better than the expected coverage of 95%. The standard ABC and ABC-CNN do not give satisfactory results. Indeed, while having a coverage of 100%, the standard ABC has the largest NMAE and sd(|\u03b8\u2212\ufffd\u03b8|), and the good coverage is due to too large. In the case of the ABC-CNN, like in the previous example, the NMAE, sd(|\u03b8 \u2212\ufffd\u03b8|) and the mean lengths of the confidence sets are quite small, but it is counterbalanced by a too small coverage (88%). Finally, the results given by the ABCD-Conformal approach are sharp, with coverages of exactly 95% (for the epistemic uncertainty). The overall and the epistemic uncertainty give quite similar results in the conformal procedure. To sum up, the ABC-RF is efficient here on all criteria. ABCD-Conformal and ABC-CNN give good predictions, but ABCD-Conformal is much better on coverages. Standard ABC is unsatisfactory on all\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9979/9979bbd4-ba3e-48f4-bcfe-605f4aaff8e9.png\" style=\"width: 50%;\"></div>\nFigure 3: Predicted parameters \ufffd\u03b8 estimated by the ABCD-Conformal against the t and (in gray intervals, for the overall uncertainty).\n<div style=\"text-align: center;\">Figure 3: Predicted parameters \ufffd\u03b8 estimated by the ABCD-Conformal against the true values for the 100 test sampl and (in gray intervals, for the overall uncertainty).</div>\ncriteria.\n# 4.3 Lokta-Volterra model\n# 4.3 Lokta-Volterra model 4.3.1 The model\n# 4.3.1 The model\nThe Lokta-Volterra model describes the dynamics of biological systems in which two species interact, one as a predator and the other as a prey. Here we consider a stochastic Markov jump process version of this model with state (X1, X2) \u2208Z2 representing prey and predator population sizes, and three parameters c1, c2 and c3. Three transitions are possible, with hazard rates c1X1, c2X1X2 and c3X2 respectively:\n(X1, X2) c1X1 \u2212\u2192(X1 + 1, X2) (prey growth) (X1, X2) c2X1X2 \u2212\u2192 (X1 \u22121, X2 + 1) (predation interaction) (X1, X2) c3X2 \u2212\u2192(X1, X2 \u22121) (predator death)\nThe estimation of the parameters of this Lokta Volterra model has been studied by several authors in an ABC framework, see for instance Prangle [2017]. The initial conditions are X1(0) = 50 and X2(0) = 100 and a dataset corresponds to observations of state (X1, X2) at times 0, 2, 4, . . . , 36. As usual, all simulations with an extinction of either the preys or the predators were discarded : we are interested in the conditional law of survival. The prior distributions are independent uniforms U[\u22126, 2] for the transformed parameters log(c1), log(c2) and log(c3). We are interested here in the estimation of \u03b8 = (c1, c2, c3).\n# 4.3.2 Algorithm parametrization\nNumber of used samples are resumed in Table 1. The Gillespie\u2019s stochastic simulation algorithm is used to generate all the samples, using the Explicit tau-leap method from the package GillespieSSA2 [Cannoodt\net al., 2021]. To improve performances of the algorithm, once simulations have been done, we used standardized versions of the parameters. The goal is then to obtain posterior estimates of the tri-dimensional vector of normalized parameters (c1, c2, c3), from an observed sample consisting of two time series of sizes 19. Concerning the standard ABC, there is no obvious summary statistics. Instead, the distance function d \ufffd (x1, x2), (x1d, x2d) \ufffd between a sample from the test set \ufffd {x1d[i], x2d[i]}, i = 1, . . . , 19 \ufffd and a sample from the training set \ufffd {x1[i], x2[i]}, i = 1, . . . , 19 \ufffd , is given by the sum of squared differences:\n\ufffd d \ufffd (x1, x2), (x1d, x2d) \ufffd = 19 \ufffd i=1 \ufffd\ufffd x1[i] \u2212x1d[i] \ufffd2 + \ufffd x2[i] \u2212x2d[i] \ufffd2\ufffd\nConcerning ABC-CNN, the distance function between a sample from the test set and a sample from the training set is the quadratic distances between the parameters predicted by the CNN for the test sample, and the true parameters used to simulate the training sample. The architectures used for the neural networks are the same for the ABC-CNN and the ABCD-Conformal: 3 convolutional 1D layers with 128 neurons and a kernel size of 2, followed by max-pooling for the first two layers, and by a flatten layer for the third one. Then, 3 dense layers of 100 neurons. Each of the layers uses the tanh activation function. The raw samples consisting of two time series of lengths 19 are the inputs of the neural networks, and the outputs are the tri-dimensional associated vectors of parameters (c1, c2, c3).\n# 4.3.3 Results\nAll indicators detailed in the beginning of Section 4 are presented in Table 4. For one test sample, Figure 4 shows a predicted parameter c associated with a confidence ellipsoid, and true value. On the 1000 test samples the coverage is of 94.4% using the epistemic uncertainty, and 90.9% using the overall uncertainty. To compare ABCD-Conformal with the other methods, we computed confidence intervals for each component of c. Figure 5 shows the predicted normalized parameters \ufffdc1, \ufffdc2 and \ufffdc3 estimated by the standard ABC, ABC-CNN and the ABCD-Conformal methods, against the true values for the 1000 test samples, as well as associated confidence intervals.\nStandard ABC\nABC-CNN\nABCD-Conformal\nABCD-Conformal\noverall\nepistemic\nNMAE1\n0.2647\n0.1516\n0.1222\n0.1222\nNMAE2\n0.3974\n0.2421\n0.2415\n0.2415\nNMAE2\n0.2770\n0.1400\n0.0998\n0.0998\nsd(|c1 \u2212\ufffdc1|)\n0.3735\n0.1319\n0.1043\n0.1043\nsd(|c2 \u2212\ufffdc2|)\n0.4865\n0.1940\n0.2191\n0.2191\nsd(|c3 \u2212\ufffdc3|)\n0.4368\n0.1485\n0.1466\n0.1466\nmean length conf. intervals c1\n0.999\n0.6847\n0.4227\n0.4635\nmean length conf. intervals c2\n1.447\n0.8447\n0.6975\n0.8671\nmean length conf. intervals c3\n1.012\n0.6218\n0.3901\n0.4345\ncoverage conf. intervals c1\n91.7%\n91.7%\n90.0%\n92.0%\ncoverage conf. intervals c2\n94.2%\n93.3%\n91.3%\n93.3%\ncoverage conf. intervals c3\n95.9%\n97.2%\n95.4%\n95.4%\ncoverage conf. ellipsoids c (3D)\nNA\nNA\n93.6%\n91.7%\nTable 4: Lokta Volterra example: comparison of standard ABC, ABC-CNN and ABCD-Conformal (using overall or epistemic uncertainty as uncertainty heuristic for the conformal procedure). These indicators are computed on a test set of size 103. We can see in table 4 that ABCD-Conformal outperforms standard ABC and obtains quite better results than ABC-CNN, when looking at NMAE and standard deviations of the absolute errors. Predictions of\nTable 4: Lokta Volterra example: comparison of standard ABC, ABC-CNN and ABCD-Conformal (using overall or epistemic uncertainty as uncertainty heuristic for the conformal procedure). These indicators are computed on a test set of size 103.\nWe can see in table 4 that ABCD-Conformal outperforms standard ABC and obtains quite better results than ABC-CNN, when looking at NMAE and standard deviations of the absolute errors. Predictions of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66fb/66fb7ae2-5c10-45d7-8752-93c35143d791.png\" style=\"width: 50%;\"></div>\nFigure 4: Lokta Volterra example: predicted normalized parameter c (in red) and true values (in blue) estimate by the ABCD-Conformal, with associated confidence ellipsoid obtained by the conformal procedure (in gray, for th overall uncertainty).\nABC-CNN and ABCD-Conformal are better than those of the standard ABC, thanks to the power of CNN, and because we do not have relevant summary statistics for the standard ABC. Thanks to Dropout, the predictions are even better for ABCD-Conformal than for ABC-CNN. When looking marginally at each component of c, the confidence intervals coverages appear similar for the three methods. But the mean lengths of confidence intervals are different depending on the method and on the parameter. Using this criterion, ABCD-Conformal is the best, followed by ABC-CNN then by standard ABC, for the three parameters. In this example, there is also an impact of the heuristic uncertainty measure used for the conformal procedure: the overall variance gives slightly better results than the epistemic variance for c1 and c3, and this impact is bigger for c2. We also note that the lengths of the confidence intervals are quite different depending on the regions of the normalized parameters, for all methods. To understand in more detail what\u2019s happening, we compute the mean lengths of confidence intervals, as well as coverages, for different regions of the normalized parameter c3. The results are summarized in Table 5, and similar results were obtained for the two other parameters c1 and c2. We can understand that the performances of the different methods can vary a lot depending on the region of the parameter. For a given component of c, the coverages on the whole domain are quite different from the coverages on some specific regions. For instance, the coverage of standard ABC (resp. ABCD-Conformal) on the whole domain is 95.9% (resp 91.8%), while it is only 53.3% for c3 > 3 (resp 80.0%). Hence, even if in theory ABCD-Conformal gives marginal coverage and standard ABC gives conditional coverages, in this example standard ABC also does not give similar coverages depending on the region. To sum up, ABCD-Conformal is better than the other methods in this example, with better predictions and narrower confidences intervals, for similar coverages. We noted that the coverages are quite different in different regions of the parameters, illustrating the marginality of the coverage, but this is the case for ALL methods. Moreover, ABCD-Conformal is the only method that gives confidence sets directly in dimension three (confidence ellipsoids).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/881b/881b6151-f226-4319-a012-863a07a14816.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">method ABCstandard ABC_CNN ABCD_conformal</div>\nFigure 5: Lokta Volterra example: predicted normalized parameters \ufffdc1, \ufffdc2 and \ufffdc3 estimated by the standard ABC, the ABC-CNN and the ABCD-Conformal methods, against the true values for the 1000 test samples. Associated confidence intervals are also represented, for the overall uncertainty.\nStandard ABC\nABC-CNN\nABCD-Conformal overall\nc3 < 1\nmean length conf. intervals\n0.7271\n0.4970\n0.2908\n860 datasets\ncoverage\n96.3%\n96.7%\n95.6%\n1 < c3 < 3\nmean length conf. intervals\n2.5856\n1.2039\n0.8446\n125 datasets\ncoverage\n98.4%\n100%\n96.8%\n3 < c3\nmean length conf. intervals\n4.2436\n2.9231\n2.2955\n15 datasets\ncoverage\n53.3%\n100%\n73.3%\nTable 5: Lokta Volterra example: comparison of standard ABC, ABC-CNN and ABCD-Conformal (using overall uncertainty as uncertainty heuristic for the conformal procedure), in different regions for the normalized parameter c3. These indicators are computed on a test set of size 103.\n# 5 Discussion\nIn this article we propose a new ABC method that combines several approaches: the ABC framework, Neural Networks with Monte Carlo Dropout and a conformal procedure. This method is free of any summary statistic, distance, or tolerance threshold. It is able to deal with multidimensional parameters, and gives exact non-asymptotic confidences sets. In practice, this method is computationally efficient, and obtains good results. We test the method on three examples and compare its performances with other approaches: standard ABC, ABC-RF and ABCCNN. We observe that depending on the problem at hand, standard ABC or ABC-RF can have slightly better accuracy. However, we also see in the Lokta-Volterra example that ABCD-Conformal can also outperform them. Regarding ABC-CNN, on all examples it never outperforms the ABCD-Conformal, either because of bad coverages of confidence sets, or because of larger mean lengths of these intervals. In contrast with\nthe other methods, ABCD-Conformal is the only one with guaranteed coverages for confidence sets. A big advantage of ABCD-Conformal in our practice, is that it always gives both a good estimation accuracy and a good marginal frequentist coverage, which is not always the case for the other methods. It is an alternative to other methods when there is no obvious summary statistic. Moreover, the computing time is comparable with ABC-CNN and ABC-RF. The choice of the summary statistics is replaced by the choice of a network architecture. This choice can be guided by common Deep Learning architectures (imaging, time series, ...) and the massive associated literature. A drawback of our proposed method is that the coverage of the confidence sets is valid marginally, and not conditionally to some values of the parameters of interest. Indeed, on the Lokta-Volterra example we have seen that the performances of the methods can vary a lot depending on the region of the parameter. However, note that this was the case for ALL methods studied. For a more detailed comparison between the four ABC methods used, the reader can refer to Appendix B. The ABCD-Conformal algorithm proposed is promising. However, several improvements and modifications could be considered mostly at the level of the uncertainty proxy outputted by the neural network and the conformal procedure. Concerning the neural network, we can imagine using the Dropconnect technique instead of Dropout. These two techniques prevent \u201cco-adaptation\u201d of units in a neural network. Dropout randomly drops hidden nodes, and Dropconnect drops connections (but all nodes can remain partially active). Dropconnect is a generalization of Dropout since there are more possible connections than nodes in a neural network. Another possibility to be explored, could be to use ensembles of neural networks (or deep ensembles) to obtain a random estimation of the parameter of interest and associated uncertainties, see Lakshminarayanan et al. [2017]. As explained by Srivastava et al. [2014c], Dropout can even be interpreted as ensemble model combination. Regarding the conformal procedure, in this article we focused on conformalizing a scalar uncertainty estimate, because the parameter of interest \u03b8 is a vector of scalars. But we can also use conformalized quantile regression (see Angelopoulos and Bates [2023] for a presentation of this procedure). Finally, in the method proposed, we have considered a split conformal procedure. This is computationally attractive, as the model needs to be fitted only one time. But it requires having a calibration set, in addition to the training and validation ones (even if in general this set is quite small compared to the training set). A full conformal procedure could avoid these extra simulations, at the cost of many more model fits, see Angelopoulos and Bates [2023]. Hence, choosing between split or full conformal procedure could depend on the problem at hand.\n# References\nAkesson, M., Singh, P., Wrede, F., and Hellander, A. (2022). Convolutional neural networks as summary statistics for approximate bayesian computation. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(06):1\u20131. Angelopoulos, A. N. and Bates, S. (2023). Conformal prediction: A gentle introduction. Foundations and Trends\u00ae in Machine Learning, 16(4):494\u2013591. Baragatti, M. and Pudlo, P. (2014). An overview on approximate bayesian computation*. ESAIM: Proc., 44:291\u2013299. Besag, J. (1974). Spatial interactions and the statistical analysis of lattice systems. Journal of the Royal Statistical Society. Series B. Statistical methodology, 148:1\u201336. Blum, M., Nunes, M., Prangle, D., and Sisson, S. (2012). A comparative review of dimension reduction methods in approximate bayesian computation. stat sci 28: 189-208. Statistical Science, 28. Boston, T., Dijk, A., Larraondo, P., and Thackway, R. (2022). Comparing cnns and random forests for landsat image segmentation trained on a large proxy land cover dataset. Remote Sensing, 14:3396.\nBreiman, L. (2001). Random forests. Machine Learning, 45:5\u201332. Cannoodt, R., Saelens, W., Deconinck, L., and Saeys, Y. (2021). Spearheading future omics analyses using dyngen, a multi-modal simulator of single cells. Nature Communications. Cliff, A. D. and Ord, J. K. (1981). Spatial processes: models and applications. Pion Ltd, London, UK. Courville, A., Goodfellow, I., and Bengio, Y. (2016). Deep Learning. Adaptive computation and machine learning series. MIT Press. Dantas, C. F., Drumond, T. F., Marcos, D., and Ienco, D. (2023). Counterfactual explanations for remote sensing time series data: An application to land cover classification. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 20\u201336. Springer. Datta, G. S., Ghosh, M., Mukerjee, R., and Sweeting, T. J. (2000). Bayesian prediction with approximate frequentist validity. The Annals of Statistics, 28(5):1414 \u2013 1426. de Haan, L. (1984). A spectral representation for max-stable processes. The Annals of Probability, 12(4):1194\u20131204. Dereich, S., Scheutzow, M., and Schottstedt, R. (2013). Constructive quantization: Approximation by empirical measures. In Annales de l\u2019IHP Probabilit\u00e9s et statistiques, volume 49, pages 1183\u20131203. Fearnhead, P. and Prangle, D. (2012). Constructing summary statistics for approximate bayesian computation: semi-automatic approximate bayesian computation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):419\u2013474. Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine, S., and Gal, Y. (2020). Can autonomous vehicles identify, recover from, and adapt to distribution shifts? In International Conference on Machine Learning, pages 3145\u20133153. PMLR. Folgoc, L. L., Baltatzis, V., Desai, S., Devaraj, A., Ellis, S., Manzanera, O. E. M., Nair, A., Qiu, H., Schnabel, J., and Glocker, B. (2021). Is mc dropout bayesian? arXiv preprint arXiv:2110.04286. Fournier, N. and Guillin, A. (2015). On the rate of convergence in wasserstein distance of the empirical measure. Probability theory and related fields, 162(3):707\u2013738. Frazier, D. T., Martin, G. M., Robert, C. P., and Rousseau, J. (2018). Asymptotic properties of approximate Bayesian computation. Biometrika, 105(3):593\u2013607. Gal, Y. (2016). Uncertainty in deep learning. PhD thesis, University of Cambridge. Gal, Y. and Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u20131059. Gal, Y., Hron, J., and Kendall, A. (2017). Concrete dropout. 31st Conference on Neural Information Processing System. Gobet, E., Lerasle, M., and M\u00e9tivier, D. (2022). Mean estimation for randomized quasi monte carlo method. Hal preprint hal-03631879v2. Grelaud, A., Marin, J.-M., Robert, C. P., Rodolphe, F., and Taly, J.-F. (2009). ABC likelihood-free methods for model choice in gibbs random fields. Bayesian Analysis, 4. Gu\u00e9ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. O\u2019Reilly Media, Inc. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving neural\nHoff, P. (2023). Bayes-optimal prediction with frequentist coverage control. Bernoulli, 29(2):901 \u2013 928.\nIzmailov, P., Vikram, S., Hoffman, M. D., and Wilson, A. G. G. (2021). What are bayesian neural network\nposteriors really like? In International conference on machine learning, pages 4629\u20134640. PMLR.\nJiang, B., Wu, T.-Y., Zheng, C., and Wong, W. H. (2017). Learning summary statistic for approximate\nbayesian computation via deep neural network. Statistica Sinica, 27:1595\u20131618.\nJospin, L. V., Laga, H., Boussaid, F., Buntine, W., and Bennamoun, M. (2022). Hands-On Bayesian Neural\nNetworks\u2014A Tutorial for Deep Learning Users. IEEE Computational Intelligence Magazine, 17(2):29\u201348.\nJoyce, P. and Marjoram, P. (2008). Approximately sufficient statistics and bayesian computation. Statistical\napplications in genetics and molecular biology, 7:Article26.\nKompa, B., Snoek, J., and Beam, A. L. (2021). Empirical frequentist coverage of deep learning uncertainty\nquantification procedures. Entropy, 23(12).\nLakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty esti-\nmation using deep ensembles. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems, NIPS\u201917, page 6405\u20136416, Red Hook, NY, USA. Curran Associates Inc.\nLeCun, Y., Haffner, P., Bottou, L., and Bengio, Y. (1999). Object Recognition with Gradient-Based Learning,\npages 319\u2013345. Springer Berlin Heidelberg, Berlin, Heidelberg.\nMacKay, D. J. (1992). A practical bayesian framework for backpropagation networks. Neural computation,\n4(3):448\u2013472.\nMarin, J.-M., Pudlo, P., Robert, C. P., and Ryder, R. J. (2012).\nApproximate bayesian computational\nmethods. Statistics and computing, 22(6):1167\u20131180.\nMarin, J.-M. and Robert, C. (2007). Bayesian Core: A Practical Approach to Computational Bayesian\nStatistics. Springer New York, NY.\nMarjoram, P., Molitor, J., Plagnol, V., and Tavar\u00e9, S. (2003). Markov chain Monte Carlo without likelihoods.\nProceedings of the National Academy of Sciences of the USA, 100(26):15324\u201315328.\nMcKinley, T., Cook, A. R., and Deardon, R. (2009). Inference in epidemic models without likelihoods. The\nInternational Journal of Biostatistics, 5(1).\nMeinshausen, N. (2006). Quantile regression forests. Journal of Machine Learning Research, 7(35):983\u2013999.\nMessoudi, S., Destercke, S., and Rousseau, S. (2022). Ellipsoidal conformal inference for multi-target regres-\nsion. In Conformal and Probabilistic Prediction with Applications, pages 294\u2013306. PMLR.\nNeal, R. M. (2012). Bayesian learning for neural networks, volume 118. Springer Science & Business Media.\nPiccioni, F., Casenave, C., Baragatti, M., Cloez, B., and Vin\u00e7on-Leite, B. (2022). Calibration of a complex\nhydro-ecological model through approximate bayesian computation and random forest combined with\nsensitivity analysis. Ecological Informatics, 71:101764.\nPrangle, D. (2017). Adapting the ABC Distance Function. Bayesian Analysis, 12(1):289 \u2013 309.\nPritchard, J., Seielstad, M., Perez-Lezaun, A., and Feldman, M. (1999). Population growth of human Y\nchromosomes: a study of Y chromosome microsatellites. Molecular Biology and Evolution, 16:1791\u20131798.\nRaynal, L., Marin, J.-M., Pudlo, P., Ribatet, M., Robert, C. P., and Estoup, A. (2018). ABC random forests\nfor Bayesian parameter inference. Bioinformatics, 35(10):1720\u20131728.\nRousseau, J. and Szabo, B. (2016). Asymptotic frequentist coverage properties of bayesian credible sets for\nsieve priors in general settings. Annals of Statistics, 48.\nSimola, U., Cisewski-Kehe, J., and Wolpert, R. L. (2021). Approximate bayesian computation for finite\nmixture models. Journal of Statistical Computation and Simulation, 91(6):1155\u20131174.\nSisson, S. A., Fan, Y., and Tanaka, M. M. (2007). Sequential monte carlo without likelihoods. Proceedings\nof the National Academy of Sciences, 104(6):1760\u20131765.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014a). Dropout: a simple\nway to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929\u2013\n1958.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014b). Dropout: A simple\nway to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):1929\u20131958.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014c). Dropout: a simple\nway to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):1929\u20131958.\nVovk, V., Gammerman, A., and Saunders, C. (1999). Machine-learning applications of algorithmic ran-\ndomness. In Sixteenth International Conference on Machine Learning (ICML-1999) (01/01/99), pages\n444\u2013453.\nWasserman, L. (2011). Frasian inference. Statistical Science, 26(3):322\u2013325.\nWeed, J. and Bach, F. (2019). Sharp asymptotic and finite-sample rates of convergence of empirical measures\nin Wasserstein distance. Bernoulli, 25(4A):2620 \u2013 2648.\nWiqvist, S., Mattei, P.-A., Picchini, U., and Frellsen, J. (2019). Partially exchangeable networks and ar-\nchitectures for learning summary statistics in approximate Bayesian computation. In Chaudhuri, K. and\nSalakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning, vol-\nume 97 of Proceedings of Machine Learning Research, pages 6798\u20136807. PMLR.\nWood, A. T. A. and Chan, G. (1994). Simulation of stationary gaussian processes in [0, 1] d. Journal of\nComputational and Graphical Statistics, 3:409\u2013432.\nXu, F., Uszkoreit, H., Du, Y., Fan,",
    "paper_type": "method",
    "attri": {
        "background": "Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods rely on user-defined summary statistics and tolerance thresholds. Recent methods incorporate machine learning to reduce the burden of these choices. This paper introduces ABCD-Conformal, an ABC method that eliminates the need for summary statistics, distance metrics, and tolerance thresholds, while providing confidence intervals with proper frequentist coverage.",
        "problem": {
            "definition": "The problem addressed is the challenge of performing Bayesian inference in complex models where likelihood functions are not tractable, necessitating the development of methods that can effectively approximate posterior distributions without relying on traditional summary statistics.",
            "key obstacle": "Existing ABC methods are heavily dependent on the selection of appropriate summary statistics and distance measures, which can be problem-specific and may not yield effective approximations in high-dimensional settings."
        },
        "idea": {
            "intuition": "The intuition behind ABCD-Conformal stems from the observation that traditional ABC methods often fail due to their reliance on user-defined parameters. By leveraging deep learning and conformal prediction, the authors aim to create a more flexible and robust approach to Bayesian inference.",
            "opinion": "The proposed ABCD-Conformal method utilizes neural networks with Monte Carlo Dropout to estimate posterior means and applies conformal prediction to generate associated confidence sets, thereby addressing the limitations of traditional ABC methods.",
            "innovation": "ABCD-Conformal distinguishes itself by not requiring summary statistics, distances, or tolerance thresholds, directly providing confidence intervals with guaranteed marginal coverage, which is a significant advancement over existing methods."
        },
        "method": {
            "method name": "ABCD-Conformal",
            "method abbreviation": "ABCD-C",
            "method definition": "ABCD-Conformal is an ABC method that combines deep learning techniques with conformal prediction to provide point estimates and confidence intervals for posterior distributions without the need for summary statistics.",
            "method description": "The method employs neural networks with Monte Carlo Dropout to estimate posterior functionals and utilizes conformal prediction to derive valid confidence sets.",
            "method steps": [
                "Generate a training dataset and a calibration dataset.",
                "Train a neural network with Dropout on the training dataset.",
                "Perform Monte Carlo Dropout predictions on the calibration dataset to estimate posterior means and associated uncertainties.",
                "Calculate calibration scores and determine conformal quantiles.",
                "For new data, perform predictions and compute confidence sets based on the trained model."
            ],
            "principle": "The effectiveness of ABCD-Conformal is rooted in its ability to model uncertainty through Dropout and to derive valid confidence intervals using conformal prediction, allowing it to adapt to various data structures."
        },
        "experiments": {
            "evaluation setting": "The method was tested on three applications: the Moving Average 2 model, a two-dimensional Gaussian random field, and the Lokta-Volterra model, comparing results against standard ABC, ABC-CNN, and ABC-RF methods.",
            "evaluation method": "Performance was assessed based on the accuracy of posterior estimates, coverage of confidence sets, and mean lengths of these intervals, using metrics like NMAE and standard deviations of absolute errors."
        },
        "conclusion": "The ABCD-Conformal method demonstrates strong performance across various applications, providing accurate posterior estimates and reliable confidence sets. It represents a significant step forward in likelihood-free Bayesian inference, particularly in high-dimensional settings, and shows promise as a practical alternative to traditional ABC methods.",
        "discussion": {
            "advantage": "ABCD-Conformal offers the advantage of not requiring summary statistics or user-defined parameters, leading to a more flexible and robust Bayesian inference process.",
            "limitation": "A limitation of the method is that it provides marginal coverage rather than conditional coverage, which may lead to varying performance across different regions of the parameter space.",
            "future work": "Future research could explore enhancements to the uncertainty estimation process, such as using Dropconnect techniques or ensembles of neural networks, and investigating the application of full conformal procedures."
        },
        "other info": {
            "keywords": [
                "Approximate Bayesian Computation",
                "Conformal prediction",
                "Neural networks",
                "Monte Carlo Dropout",
                "Likelihood-free inference"
            ],
            "source code": "The source codes to run the examples are available at https://forgemia.inra.fr/mistea/codes_articles/abcdconformal."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "ABCD-Conformal is a method that eliminates the need for summary statistics, distance metrics, and tolerance thresholds, addressing critical challenges in Bayesian inference."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed is the challenge of performing Bayesian inference in complex models where likelihood functions are not tractable, necessitating the development of methods that can effectively approximate posterior distributions."
        },
        {
            "section number": "3.5",
            "key information": "ABCD-Conformal distinguishes itself by providing confidence intervals with guaranteed marginal coverage, which is a significant advancement over existing ABC methods."
        },
        {
            "section number": "6.1",
            "key information": "The effectiveness of ABCD-Conformal is rooted in its ability to model uncertainty through Dropout and to derive valid confidence intervals using conformal prediction."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the ABCD-Conformal method is that it provides marginal coverage rather than conditional coverage, which may lead to varying performance across different regions of the parameter space."
        }
    ],
    "similarity_score": 0.6299781042905787,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Approximate Bayesian Computation with Deep Learning and Conformal prediction.json"
}