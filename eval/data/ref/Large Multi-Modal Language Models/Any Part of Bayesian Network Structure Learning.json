{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2103.13810",
    "title": "Any Part of Bayesian Network Structure Learning",
    "abstract": "We study an interesting and challenging problem, learning any part of a Bayesian network (BN) structure. In this challenge, it will be computationally inefficient using existing global BN structure learning algorithms to find an entire BN structure to achieve the part of a BN structure in which we are interested. And local BN structure learning algorithms encounter the false edge orientation problem when they are directly used to tackle this challenging problem. In this paper, we first present a new concept of Expand-Backtracking to explain why local BN structure learning methods have the false edge orientation problem, then propose APSL, an efficient and accurate Any Part of BN Structure Learning algorithm. Specifically, APSL divides the V-structures in a Markov blanket (MB) into two types: collider V-structure and non-collider V-structure, then it starts from a node of interest and recursively finds both collider V-structures and non-collider V-structures in the found MBs, until the part of a BN structure in which we are interested are oriented. To improve the efficiency of APSL, we further design the APSL-FS algorithm using Feature Selection, APSL-FS. Using six benchmark BNs, the extensive experiments have validated the efficiency and accuracy of our methods.",
    "bib_name": "ling2021bayesiannetworkstructurelearning",
    "md_text": "IEEE TRANSACTIONS ON CYBERNETICS, VOL. 14, NO. 8, AUGUST 2020\n# Any Part of Bayesian Network Structure Learning\nZhaolong Ling, Kui Yu, Hao Wang, Lin Liu, and Jiuyong Li\nAbstract\u2014We study an interesting and challenging problem, learning any part of a Bayesian network (BN) structure. In this challenge, it will be computationally inefficient using existing global BN structure learning algorithms to find an entire BN structure to achieve the part of a BN structure in which we are interested. And local BN structure learning algorithms encounter the false edge orientation problem when they are directly used to tackle this challenging problem. In this paper, we first present a new concept of Expand-Backtracking to explain why local BN structure learning methods have the false edge orientation problem, then propose APSL, an efficient and accurate Any Part of BN Structure Learning algorithm. Specifically, APSL divides the V-structures in a Markov blanket (MB) into two types: collider V-structure and non-collider Vstructure, then it starts from a node of interest and recursively finds both collider V-structures and non-collider V-structures in the found MBs, until the part of a BN structure in which we are interested are oriented. To improve the efficiency of APSL, we further design the APSL-FS algorithm using Feature Selection, APSL-FS. Using six benchmark BNs, the extensive experiments have validated the efficiency and accuracy of our methods. Index Terms\u2014Bayesian network, Local structure learning, Global structure learning, Feature selection.\n 23 Mar 2021\n[cs.LG]\nI. INTRODUCTION\n# I. INTRODUCTION\nB AYESIAN networks (BNs) are graphical models for representing multivariate probability distributions [1], [2], [3]. The structure of a BN takes the form of a directed acyclic graph (DAG) that captures the probabilistic relationships between variables. Learning a BN plays a vital part in various applications, such as classification [4], [5], feature selection [6], [7], [8], and knowledge discovery [9], [10]. However, in the era of big data, a BN may easily have more than 1,000 nodes. For instance, Munin1 is a wellknown BN for diagnosis of neuromuscular disorders [11], which has four subnetworks, and three of them have more than 1,000 nodes. When we are only interested in one of subnetwork structures, if we can start from any one of nodes of this subnetwork and then gradually expands to learn only this subnetwork structure, it will be much more efficient than learning the entire BN structure.\nThis work is partly supported by the National Key Research and Development Program of China (under grant 2019YFB1704101), and the National Science Foundation of China (under grant 61876206 and 61872002). Z. Ling is with the School of Computer Science and Technology, Anhui University, Hefei, Anhui, 230601, China. E-mail: zlling@ahu.edu.cn. K. Yu and H. Wang are with Key Laboratory of Knowledge Engineering with Big Data of Ministry of Education (Hefei University of Technology), and the School of Computer and Information, Hefei University of Technology, Hefei, Anhui, 230009, China. E-mail: yukui@hfut.edu.cn, jsjxwangh@hfut.edu.cn. L. Liu and J. Li are with the School of Information Technology and Mathematical Sciences, University of South Australia, Adelaide, SA, 5095, Australia. E-mail: lin.liu@unisa.edu.au, jiuyong.li@unisa.edu.au. 1http://www.bnlearn.com/bnrepository/discrete-massive.html#munin4\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6fad/6fadc8b7-cf8b-4ba3-b186-e34e882dd306.png\" style=\"width: 50%;\"></div>\nFig. 1. An illustrative example of learning a part of a BN structure around node T to any depth from 1 to 4, which achieves a local BN structure around T when learning to a depth of 1, and achieves a global BN structure when learning to a depth of 4 (the maximum depth).\nThus in this paper, we focus on learning any part of a BN structure, that is, learning a part of a BN structure around any one node to any depth. For example in Fig. 1, given a target variable, structure learning to a depth of 1 means to discover and distinguish the parents and children (PC) of the target variable, structure learning to a depth of 2 means to discover and distinguish the PC of each node in the target\u2019s PC on the basis of structure learning to a depth of 1, and so on. Clearly, it is trivial to obtain any part of a BN structure if we can learn a global BN structure using a global BN structure learning algorithm [12], [13], [14]. However, learning a global BN structure is known as NP-complete [15], [16], and easily becomes non-tractable in large scale applications where thousands of attributes are involved [17], [18]. Furthermore, it is not necessary and wasteful to find a global BN structure when we are only interested in a part of a BN structure. Recently, Gao et al. [19] proposed a new global BN structure learning algorithm, called Graph Growing Structure Learning (GGSL). Instead of finding the global structure directly, GGSL starts from a target node and learns the local structure around the node using score-based local learning algorithm [20], then iteratively applies the local learning algorithm to the node\u2019s PC for gradually expanding the learned local BN structure until a global BN structure is achieved. However, if we directly apply GGSL to tackle any part of BN structure learning problem, first, GGSL is still a global BN structure learning algorithm, and second, it is time-consuming or infeasible when the BN is large because the scored-based local learning algorithm [20] used by GGSL needs to learn a BN structure involving all nodes selected currently at each iteration [7].\nIEEE TRANSACTIONS ON CYBERNETICS, VOL. 14, NO. 8, AUGUST 2020\nFig. 2. A simple Bayesian network. T is a target node in black. Existing local BN structure learning algorithms cannot orient the edge F \u2212T when they only find the local structure of T. Then, they recursively find the local structure of the nodes F, D, and C for expanding the local structure of T. Finally, since the V-structure A \u2192C \u2190B can be oriented in the local structure of C, the local algorithms backtrack the edges C \u2192D \u2192F \u2192 T, and thus F is a parent of T.\nDue to the limitation of the score-based local learning algorithm on large-sized BNs, existing local BN structure learning algorithms are constraint-based. Such as, PCDby-PCD (PCD means Parents, Children and some Descendants) [21] and Causal Markov Blanket (CMB) [22]. Local BN structure learning focus on discovering and distinguishing the parents and children of a target node [22], and thus PCD-by-PCD and CMB only learn a part of a BN structure around any one node to a depth of 1. More specifically, both of PCD-by-PCD and CMB first find a local structure of a target node. If the parents and children of the target node cannot be distinguished in the local structure, these algorithms recursively find the local structure of the nodes in the target\u2019s PC for gradually expanding the learned local structure (Expanding phase), and then backtrack the edges in the learned expansive structure to distinguish the parents and children of the target (Backtracking phase). As illustrated in Fig. 2, we call this learning process Expand-Backtracking. However, if we directly apply the local BN structure learning algorithms to tackle any part of BN structure learning problem, this will lead to that many V-structures cannot be correctly found (i.e., V-structures missed) during the Expanding phase. Missing V-structures will generate many potential cascade errors in edge orientations during the Backtracking phase. Moreover, PCD-by-PCD uses symmetry constraint (see Theorem 3 in Section III) to generate undirected edges, so it takes time to find more unnecessary PCs. CMB spends time tracking conditional independence changes after Markov blanket (MB, see Definition 6 in Section III) discovery, and the accuracy of CMB is inferior on small-sized data sets because it uses entire MB set as the conditioning set for tracking conditional independence changes. Thus, even if the existing local BN structure learning algorithms do not miss the V-structures, they still cannot learn a part of a BN structure efficiently and accurately. In this paper, we formally present any part of BN structure learning, to learn a part of a BN structure around any one node to any depth efficiently and accurately. As illustrated in Fig. 1, any part of BN structure learning can learn a local BN structure with a depth of 1, and achieve a global BN structure with a depth of the maximum depth. And hence, any part of BN structure learning has strong scalability. The main contributions of the paper are summarized as follows.\n1) We present a new concept of Expand-Backtracking, to describe the learning process of the existing local BN structure learning algorithms. And we divide the Vstructures included in an MB into collider V-structures and non-collider V-structures to analyze the missing V-structures in Expand-Backtracking. 2) Based on the analysis, we propose APSL, an efficient and accurate Any Part of BN Structure Learning algorithm. Specifically, APSL starts from any one node of interest and recursively finds both of the collider Vstructures and non-collider V-structures in MBs, until all edges in the part of a BN structure are oriented. 3) We further design APSL-FS, an any part of BN structure learning algorithm using Feature Selection. Specifically, APSL-FS employs feature selection for finding a local skeleton of a node without searching for conditioning sets to speed up local skeleton discovery, leading to improve the efficiency of APSL. 4) We conduct a series of experiments on six BN data sets, to validate the efficiency and accuracy of the proposed algorithms against 2 state-of-the-art local BN structure learning algorithms and 5 state-of-theart global BN structure learning algorithms.\nThe rest of this paper is organized as follows: Section II discusses related work. Section III provides notations and definitions. Section IV analyzes the missing V-structures in Expand-Backtracking. Section V presents the proposed algorithms APSL and APSL-FS. Section VI discusses the experimental results, and Section VII concludes the paper.\n# II. RELATED WORK\nMany algorithms for BN structure learning have been proposed and can be divided into two main types: local methods and global methods. However, there are some issues with these methods when we apply them to tackle the any part of BN structure learning problem. Local BN structure learning algorithms State-of-theart local methods apply standard MB or PC discovery algorithms to recursively find V-structures in the local BN structure for edge orientations, until the parents and children of the target node are distinguished, and thus they learn a part of a BN structure around any one node to a depth of 1. PCD-by-PCD (PCD means Parents, Children and some Descendants) [21] applies Max-Min Parents and Children (MMPC) [23] to recursively search for PC and separating sets, then uses them for local skeleton construction and finding V-structures, respectively, and finally uses the Vstructures and Meek rules [24] for edge orientations. However, at each iteration of any part of BN structure learning, since PCD-by-PCD only finds the V-structures connecting a node with its spouses V-structures, the V-structures included in the PC of the node are sometimes missed, then using the Meek-rules leads to false edge orientations in the part of a BN structure. Moreover, PCD-by-PCD uses symmetry constraint to generate undirected edges, so it needs to find the PC of each node in the target\u2019s PC to generate the\nundirected edges between the target and target\u2019s PC, which is time-consuming. Causal Markov Blanket (CMB) [22] first uses HITON-MB [25] to find the MB of the target, then orients edges by tracking the conditional independence changes in MB of the target. However, at each iteration of any part of a BN structure learning, since CMB only find V-structures included in the PC of a node, the V-structures connecting the node with its spouses are sometimes missed, then tracking conditional independence changes leads to false edge orientations in the part of a BN structure. In addition, CMB uses entire MB set as the conditioning set and needs to spend time for conditional independence tests after MB discovery, which deteriorates the performance of CMB in accuracy and efficiency, respectively. Global BN structure learning algorithms State-of-theart global methods first identify each variable\u2019s MB/PC using the existing MB/PC methods, then construct a global BN skeleton (i.e., an undirected graph) using the found MBs/PCs, and finally orient the edge directions of the skeleton using constraint-based or score-based BN learning methods. Grow-Shrink (GS) [12] first applies constraint-based MB method, Grow-Shrink Markov blanket (GSMB) [12] to find MB of each node to construct global BN skeleton, then uses conditional independence test to find all V-structures, and finally orients undirect edges by using Meek-rules [24]. Since then, many structure learning algorithms have been proposed. Max-Min Hill-Climbing (MMHC) [13] first applies constraint-based PC method, MMPC [23] to find PC of each node to construct global BN skeleton, then uses scorebased method to orient edges. Both of Score-based Local Learning+Constraint (SLL+C) [26] and Score-based Local Learning+Greedy (SLL+G) [26] uses the score-based MB method, SLL [26] to find MB/PC of each node to construct global BN skeleton, then orient edges by using constraintbased and score-based methods, respectively. However, when we apply these global methods to any part of BN structure learning, it is time-consuming to learn an entire BN structure to achieve a part of a BN structure. Recently, Gao et al. [19] proposed graph growing structure learning (GGSL) to learn a global BN structure. Instead of finding the MB/PC of each variable in advance, GGSL starts from any one node and learns the local structure around the node using the score-based MB discovery algorithm, S2TMB [20], then iteratively applies S2TMB to the node\u2019s neighbors for gradually expanding the learned local BN structure until an entire BN structure is achieved. However, GGSL still needs to learn an entire BN structure to achieve a part of a BN structure. In addition, although the score-based MB method can directly find the local BN structure without expanding outward, it is computationally expensive [7], because it needs to learn a BN structure involving all nodes selected currently at each iteration. And hence, GGSL is time-consuming or infeasible when the size of a BN is large. In summary, when we apply existing local and global BN structure learning algorithms to any part of BN structure learning, local methods are inaccurate and global methods\nSymbol\nMeaning\nU\na variable set\nX, Y, T\na variable\nx, y\na value of a variable\nQ\na regular queue (first in, first out)\nZ, S\na conditioning set within U\nX \u22a5\u22a5Y |Z\nX is conditionally independent of Y given Z\nX \u0338\u22a5\u22a5Y |Z\nX is conditionally dependent on Y given Z\nPCT\nparents and children of T\nSPT\nspouses of T\nSPT (X)\na subset of spouses of T, and each node in SPT (X)\nhas a common child X with T\nV\na queried variable set of variables\nSepT [X]\na set that d-separates X from T\n|.|\nthe size of a set\nSU(X; Y )\nthe correlation between X and Y\nare inefficient. Thus in this paper, we attempt to solve the problem of any part of BN structure learning.\n# are inefficient. Thus in this paper, we attempt to solve the problem of any part of BN structure learning.\nIII. NOTATIONS AND DEFINITIONS\nIn the following, we will introduce the relevant definitions and theorems. Table I provides a summary of the notations used in this paper. Definition 1 (Conditional Independence) [27] Two variables X and Y are conditionally independent given Z, iff P(X = x, Y = y|Z = z) = P(X = x|Z = z)P(Y = y|Z = z). Definition 2 (Bayesian Network) [27] Let P be a discrete joint probability distribution of a set of random variables U via a directed acyclic graph (DAG) G. We call the triplet < U, G, P > a Bayesian Network (BN) if < U, G, P > satisfies the Markov Condition: every variable in U is conditionally independent of its non-descendant variables given its parents. Markov condition enables us to recover a distribution P from a known DAG G in terms of conditional independence relationships. Definition 3 (D-Separation) [27]. A path p between X and Y given Z \u2286U\\{X \u222aY } is open, iff (1) every collider on p is in Z or has a descendant in Z, and (2) no other noncollider variables on p are in Z. If the path p is not open, then p is blocked. Two variables X and Y are d-separated given Z, iff every path from X to Y is blocked by Z. If two variables X and Y are d-separated relative to a set of variables Z in a BN, such a set Z would be called a separating set of X from Y , then they are conditionally independent given Z in all probability distributions where this BN can represent. Definition 4 (Faithfulness) [9]. A Bayesian network is presented by a DAG G and a joint probability distribution P over a variable set U. G is faithful to P iff every conditional independence present in P is entailed by G and the Markov condition. P is faithful iff there exists a DAG G such that G is faithful to P.\nThe faithfulness condition enables us to recover a DAG G from a distribution P to completely characterize P. Definition 5 (V-Structure) [27]. The triplet of variables X, Y , and Z forms a V-structure if node Z has two incoming edges from X and Y , forming X \u2192Z \u2190Y , and X is not adjacent to Y . Z is a collider if Z has two incoming edges from X and Y in a path, respectively. Definition 6 (Markov Blanket) [27] Under the faithfulness assumption, given a target variable T, the Markov blanket of T is unique and consists of parents, children, and spouses (other parents of the children) of T. Theorem 1 [9] Under the faithfulness assumption, X \u2208U and Y \u2208U. If X and Y are adjacent, then X \u0338\u22a5\u22a5Y |S, \u2200S \u2286U \\ {X \u222aY }. Theorem 2 [9] Under the faithfulness assumption, X \u2208 U, Y \u2208U, and Z \u2208U. If X, Y , and Z forms the V-structure X \u2192Z \u2190Y , then X \u22a5\u22a5Y |S and X \u0338\u22a5\u22a5Y |{S \u222aZ}, \u2200S \u2286U \\ {X \u222aY \u222aZ}. X is a spouse of Y . Under the faithfulness assumption, Theorem 1 presents the property of PC, and Theorem 2 presents the property of spouses in an MB. Theorem 3 Symmetry constraint. [28] Under the faithfulness assumption, if X \u2208PCY exists, then Y \u2208PCX holds.\nThe faithfulness condition enables us to recover a DAG G from a distribution P to completely characterize P. Definition 5 (V-Structure) [27]. The triplet of variables X, Y , and Z forms a V-structure if node Z has two incoming edges from X and Y , forming X \u2192Z \u2190Y , and X is not adjacent to Y . Z is a collider if Z has two incoming edges from X and Y in a path, respectively. Definition 6 (Markov Blanket) [27] Under the faithfulness assumption, given a target variable T, the Markov blanket of T is unique and consists of parents, children, and spouses (other parents of the children) of T. Theorem 1 [9] Under the faithfulness assumption, X \u2208U and Y \u2208U. If X and Y are adjacent, then X \u0338\u22a5\u22a5Y |S, \u2200S \u2286U \\ {X \u222aY }. Theorem 2 [9] Under the faithfulness assumption, X \u2208 U, Y \u2208U, and Z \u2208U. If X, Y , and Z forms the V-structure X \u2192Z \u2190Y , then X \u22a5\u22a5Y |S and X \u0338\u22a5\u22a5Y |{S \u222aZ}, \u2200S \u2286U \\ {X \u222aY \u222aZ}. X is a spouse of Y . Under the faithfulness assumption, Theorem 1 presents the property of PC, and Theorem 2 presents the property of spouses in an MB. Theorem 3 Symmetry constraint. [28] Under the faithfulness assumption, if X \u2208PCY exists, then Y \u2208PCX holds.\n# IV. MISSING V-STRUCTURES IN EXPAND-BACKTRACKING\nIV. MISSING V-STRUCTURES IN EXPAND-BACKTRACKING\nIn this section, we first give the definition of ExpandBacktracking in Section IV-A, and then use two examples to analyze the missing V-structures in Expand-Backtracking in Section IV-B.\n# A. Definition of Expand-Backtracking\nIn this subsection, we first summarize the main ideas of local BN structure learning algorithms, then give the definition of the Expand-Backtracking. Local BN structure learning aims to discover and distinguish the parents and children of a target variable, and thus the local BN structure learning algorithms are only able to learn a part of a BN structure around the target to a depth of 1. Moreover, existing local algorithms are constraintbased, because score-based local methods need to learn a BN structure involving all nodes selected currently at each iteration, which is time-consuming. As constraint-based algorithms, local BN structure learning algorithms first find a local structure of a target node using the following three steps. Then, since the parents and children of the target sometimes cannot be distinguished in the learned local structure, the local algorithms recursively apply these three steps to the target\u2019s neighbors for gradually expanding the learned local structure, until the parents and children of the target node are distinguished. 1) Skeleton identification. Use standard local discovery algorithm to construct the local BN skeleton of a target node.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4779/4779f4c3-5cc5-4fd3-86e7-b41100301fd2.png\" style=\"width: 50%;\"></div>\nFig. 3. The Markov blanket (in blue) of node T comprises A and B (parents), D and F (children), and C (spouse). (a) Collider Vstructure (T is a collider in the V-structure), and (b) noncollider V-structure (T is not a collider in the V-structure).\n2) V-structure discovery. Discover V-structures in the local BN skeleton. 3) Edge orientation. Orient as many edges as possible given the V-structures in the learned part of BN skeleton, to get a part of BN structure around the target node. Specifically, in the edge orientation step, given the discovered V-structures, local BN structure learning algorithms orient the edges not only in the local skeleton of a target node, but also the skeleton outside the local skeleton, to backtrack the edges into the parents and children of the target node for distinguishing them. To facilitate the next step in presentation and analysis, we give the definition of the learning process of the local BN structure learning algorithms as follows. Definition 7 (Expand-Backtracking) Under the faithfulness assumption, existing local BN structure learning algorithms first learn a local structure of a target node, then expand the learned local structure and backtrack the edges to distinguish parents and children of the target node. We call this learning process Expand-Backtracking. Thus, V-structure discovery plays a crucial role in ExpandBacktracking. However, when the local BN structure learning algorithms are Expand-Backtracking, they ignore the correctness of the V-structures found (i.e., V-structures missed). Since the edge orientation step is based on the V-structure discovery step, missing V-structures in ExpandBacktracking will cause a cascade of false edge orientations in the obtained structure.\n# B. Analysis of missing V-structures in Expand-Backtracking\nIn this subsection, we first define two types of V-structures in an MB, then give the examples to demonstrate which type of V-structures cannot be correctly identified when the local BN structure learning algorithms are Expand-Backtracking. Definition 8 (Collider V-structure and Non-collider Vstructure) Under the faithfulness assumption, there are two types of the V-structure included in the MB of T, 1) collider\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/023b/023b6b57-3c39-4039-b9d0-cb01703b6fbf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4. (a) The ALARM Bayesian network; (b) an example of using PCD-by-PCD to find a part of an Alarm Bayesian network structure around node 10 to a depth of 2; (c) an example of using CMB to find a part of an Alarm Bayesian network structure around node 26 to a depth of 2. The red \u2019X\u2019 symbol denotes the falsely oriented edges, the blue node is the node that needs to find local structure at each iteration, the number in parentheses represents the level of iterations of an algorithm, and \u2019\u00b7 \u00b7 \u00b7 \u2019 means omitted correctly oriented iterations.</div>\nV-structure: T is a collider in the V-structure, and 2) noncollider V-structure: T is not a collider in the V-structure. Definition 8 gives two types of the V-structures included in an MB, as illustrated in Fig. 3. Thus, whether collider V-structures or non-collider V-structures cannot be correctly identified in the V-structure discovery step, it will cause the false edge orientations in the obtained structure. Below, we give the examples of the missing V-structures in ExpandBacktracking using two representative local BN structure learning algorithms. 1) Missing collider V-structures: PCD-by-PCD [21] is a state-of-the-art local BN structure learning algorithm, which recursively uses standard PC algorithm to find PCs and V-structures. However, PCD-by-PCD only finds the Vstructures connecting the node with its spouses at each iteration, and hence, PCD-by-PCD only finds non-collider V-structures leading to missing some collider V-structures at each iteration. In the following, under the faithfulness and correct independence tests assumption, we use PCD-by-PCD to find a part of an ALARM [29] BN structure around node 10 to a depth of 2, as illustrated in Fig. 4 (b). Before giving the example step by step, to make the process easier for readers to understand, as shown in Fig. 5, we first give a detailed description of the three Meek-rules [24] used by PCD-byPCD in edge orientation step as follows: R1 No new V-structure. Orient Y \u2212Z into Y \u2192Z whenever there is a directed edge X \u2192Y such that X and Z are not adjacent; R2 Preserve acyclicity. Orient X \u2212Z into X \u2192 Z whenever there is a chain X \u2192Y \u2192Z; R3 Enforce 3-fork V-structure. Orient X \u2212Y into X \u2192 Y whenever there are two chains X \u2212Z \u2192Y and X \u2212W \u2192Y such that Z and W are not adjacent. 1st iteration: PCD-by-PCD finds PC of 10. PCD-by-PCD uses symmetry constraint to generate undirected edges, for example, PCD-by-PCD generates undirected edge A \u2212B only if A belongs to the PC of B and B also belongs to the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f84/8f8407f5-6488-4ab7-b8e9-cd5f951e3c71.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. Three Meek-rules for edge orientations.</div>\nPC of A. Since PC of 10 is {11, 35}, but PCs of 11 and 35 are initialized as empty sets and can only be discovered in the next iterations, then 10 does not belong to the PCs of 11 and 35, and there are no undirected edges generated in this iteration. 2nd iteration: PCD-by-PCD finds PC of 11. Since PC of 10 is {11, 35} and PC of 11 is {10, 12}, then 10 belongs to the PC of 11 and 11 also belongs to the PC of 10, and PCD-by-PCD generates undirected edge 10-11. There are no V-structures generated in this iteration, so PCD-by-PCD does not need to orient edges. 3rd iteration: PCD-by-PCD finds PC of 35, then generates undirected edge 10-35. Since the non-collider V-structure 11 \u219210 \u219035 is discovered, PCD-by-PCD orient the noncollider V-structure, and there are no other undirected edges can be oriented by using Meek-rules. 4th iteration: PCD-by-PCD finds PC of 12, then generates undirected edges 12-11 and 12-35. Since PCD-by-PCD only discovers non-collider V-structure at each iteration, it misses the collider V-structure 11 \u219212 \u219035. And there are no other undirected edges can be oriented by using Meek-rules. 5th iteration: PCD-by-PCD finds PC of 9, and generates undirected edge 9-35. Then there are no new V-structures generated and no other undirected edges can be oriented by using Meek-rules. 6th-9th iterations: PCD-by-PCD iteratively finds PCs of 34, 36, 8, and 13, and PCD-by-PCD correctly orients edges in these iterations, so we omit them. 10th iteration: PCD-by-PCD finds PC of 15, then generates undirected edge 15-34, and discovers non-collider V-\nstructure 15 \u219234 \u219013. Finally, according to the R1 of Meek-rules, PCD-by-PCD backtracks the edges 34 \u219235, 35 \u219212, 35 \u219236, and 12 \u219211. Thus, the edge 12 \u219211 is falsely oriented. 2) Missing non-collider V-structures: CMB [22] is another state-of-the-art local BN structure learning algorithm, which recursively uses standard MB algorithm to find MBs and tracks the conditional independence changes to find V-structures. However, CMB only finds the V-structures included in the PC of the target at each iteration. Thus, CMB only finds collider V-structures and then misses some noncollider V-structures at each iteration. In the following, under the faithfulness and correct independence tests assumption, we use CMB to find a part of an ALARM BN structure around node 26 to a depth of 2, as illustrated in Fig. 4 (c). Moreover, CMB tracks the conditional independence changes in edge orientation step, which is similar to the three Meek-rules [22]. 1st iteration: CMB finds MB of 26 and generates undirected edges using PC of 26. Then CMB discovers the collider V-structures 25 \u219226 \u219030, 30 \u219226 \u219017, and 25 \u219226 \u219017, and there are no other undirected edges can be oriented by tracking the conditional independence changes. 2nd iteration: CMB finds MB of 17, then generates undirected edge 17-31, and there are no other undirected edges can be oriented by tracking the conditional independence changes. 3rd iteration: CMB finds MB of 25 and generates undirected edges using PC of 25. Since CMB only finds collider V-structures at each iteration, it misses the non-collider V-structure 25 \u219231 \u219017. Then there are no other undirected edges can be oriented by tracking the conditional independence changes. 4th iteration: CMB finds MB of 30 and generates undirected edges using PC of 30. Since CMB discovers collider V-structure 27 \u219230 \u219029, CMB orients the collider Vstructure. Then according to the conditional independence changes, CMB executes the same way as the R1 of the Meekrules to backtrack the edges 30 \u219231, 31 \u219225, 31 \u219217, 25 \u219218, 25 \u219224, and 25 \u219232. Thus, the edges 31 \u219225 and 31 \u219217 are falsely oriented. Summary: Local BN structure learning algorithms miss V-structures in Expand-Backtracking, and thus they encounter the false edge orientation problem when learning any part of a BN structure. If we do not tackle the missing V-structures in Expand-Backtracking, many edges may be falsely oriented during the edge orientation step, leading to low accuracy of any part of BN structure learning. Clearly, to tackle the missing V-structures in ExpandBacktracking when learning any part of a BN structure, we need to correctly identify both of non-collider V-structures and collider V-structures in the current part of a BN skeleton at each iteration.\nV. THE PROPOSED APSL AND APSL-FS ALGORITHMS This section presents the proposed any part of BN structure learning algorithms, APSL in Section V-A and APSLFS in Section V-B.\n# A. APSL: Any Part of BN Structure Learning\nWith the analysis of missing V-structures in ExpandBacktracking in Section IV, we present the proposed APSL (Any Part of BN Structure Learning) algorithm, as described in Algorithm 1. APSL recursively finds both of non-collider V-structures (Step 1: Lines 9-26) and collider V-structures (Step 2: Lines 28-36) in MBs, until all edges in the part of a BN structure around the target node are oriented (Step 3: Lines 38-58).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ccee/ccee1798-34a4-4ca5-945a-80befe35e594.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. Four types of relationships between two variables.</div>\nAPSL defines an adjacency matrix G of a DAG, to detect the relationship among all the variables. In G, the four types of the relationship between any two variables A and B are shown in Fig. 6, as follows: (a) A and B are not adjacent \u21d2G(A, B) = 0 and G(B, A) = 0. (b) A and B are adjacent but cannot determine their edge direction \u21d2G(A, B) = 1 and G(B, A) = 1. (c) A and B are adjacent and A \u2192B \u21d2G(A, B) = \u22121 and G(B, A) = 0. (d) A and B are adjacent and A \u2190B \u21d2G(A, B) = 0 and G(B, A) = \u22121. APSL first initializes the queried variable set V to an empty set and initializes the queue Q, pre-storing the target variable T. Then, the next three steps will be repeated until all edges in the part of a BN structure around T to a depth of K are oriented, or the size of V equals to that of the entire variable set U, or Q is empty. Step 1: Find non-collider V-structures (Lines 9-26). APSL first pops the first variable A from the queue Q, and then uses MB discovery algorithms to find the MB (i.e., PC and spouse) of A. APSL will first find the PC and spouse of T since T is pre-stored in Q. Then, APSL pushes the PC of A into Q to recursively find the MB of each node in the PC of A in the next iterations, and stores A in V to prevent repeated learning. Finally, APSL generates undirected edges by using the PC of A (Lines 16-20), and orients the noncollider V-structures by using the spouses of A (Lines 2126). At Line 13, the MB discovery algorithm, we use is a constraint-based MB method, such as MMMB [23] or HITON-MB [25], because this type of MB methods do not require a lot of memory. Moreover, these MB methods can save the discovered PCs to avoid repeatedly learning PC\norithm 1: APSL put: D: Data, T: Target, K: a given depth; tput: G: a part of a BN structure around T; = \u2205; = {T}; = zeros(|U|, |U|); yer num = 1; er nodes(layer num) = T; = 1; peat /*Step 1 : Find non-colliderV -structures*/ A = Q.pop; if A \u2208V then continue; end [PCA, SPA] = GetMB(D, A) V = V \u222a{A}; Q.push(PCA); for each B \u2208PCA do if G(A, B) = 0&G(B, A) = 0 then G(A, B) = 1, G(B, A) = 1; end end for each B \u2208PCA do for each C \u2208SPA(B) do G(A, B) = \u22121, G(B, A) = 0; G(C, B) = \u22121, G(B, C) = 0; end end /*Step 2 : Find collider V -structures*/ for every X, Y \u2208PCA do if X \u22a5\u22a5Y |Z for some Z \u2286PCX then SepX[Y ] = Z; if X \u0338\u22a5\u22a5Y |SepX[Y ] \u222a{A} then G(X, A) = \u22121, G(A, X) = 0; G(Y, A) = \u22121, G(A, Y ) = 0; end end end /*Step 3 : Orient edges */ update G by using Meek rules ; i = i \u22121; if i = 0 then layer num = layer num + 1; for each X \u2208layer nodes(layer num \u22121) do layer nodes(layer num) = layer nodes(layer num) \u222aPCX; end i = |layer nodes(layer num) \\ V|; end if layer num > K then break flag=1; for each X \u2208layer nodes(K) do if can find G(PCX, X) = 1 then break flag=0; break; end end if break flag then break; end end til |V| = |U|, or Q = \u2205; turn G;\nsets during any part of BN structure learning, since they find spouses from the PC of each variable in the target\u2019s PC. Line 17 aims to prevent the already oriented edges from being re-initialized as undirected edges. layer num represents the number of layers, starting from 1. Thus, the number of layers is one more than the corresponding number of depths, for example, when the number of depths is 2, the corresponding number of layers is 3. layer nodes stores the nodes of each layer. Step 2: Find collider V-structures (Lines 28-36). APSL finds collider V-structures in the PC of A. If two variables X and Y in the PC of A are conditionally independent, that is, they are not adjacent owing to Theorem 1. But these two variables are conditionally dependent given the union of the collider A and their separating set, then the triple of nodes X, Y , and A can form collider V-structure of A owing to Theorem 2, X \u2192A \u2190Y . Step 3: Orient edges (Lines 38-58). Based on the oriented non-collider V-structures and collider V-structures, APSL uses Meek-rules to orient the remaining undirected edges (Line 38). The purpose of Lines 40-46 is to control the number of layers of recursion. Specifically, i reduced by 1 at each iteration, and i = 0 means that all the nodes in this layer have been traversed, then ASPL begins to traverse the nodes at the next layer in the next iterations. From Lines 4758, APSL determines whether all edges in the part of a BN structure around T are oriented. When the edges between the layer of K and K+1 of a part of a BN structure around T are all oriented, APSL terminates and outputs the part of a BN structure around T. Some edges with a number of layers less than K are not oriented because these edges can never be oriented due to the existence of Markov equivalence structures [30]. Theorem 4 Correctness of APSL Under the faithfulness and correct independence tests assumption, APSL finds a correct part of a BN structure. Proof Under the faithfulness and correct independence tests assumption, we will prove the correctness of APSL in three steps. 1) Step 1 finds all and only the non-collider V-structures. A standard MB discovery algorithm finds all and only the PC and spouses of a target node. APSL uses the MB method to find PC and spouses of the nodes that need to be found. Then, using the found PCs, APSL constructs a part of a BN skeleton with no missing edges and no extra edges. Using the found spouses, APSL finds all and only the non-collider V-structures. 2) Step 2 finds all and only the collider V-structures. APSL finds collider V-structures in PCs. First, APSL uses Theorem 1 to confirm that there is no edge between two nodes X and Y in the PC of A (the target node at each iteration). Then, owing to Theorem 2, if the collider A makes X and Y conditionally dependent, X \u0338\u22a5\u22a5Y |SepX[Y ] \u222a{A}, then X and Y are each other\u2019s spouses with the common child A, and forms a collider V-structure X \u2192A \u2190Y . Since APSL considers any two nodes in the PCs and their common child,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65e8/65e82e59-e847-4b23-a688-ccaed2bcd8d6.png\" style=\"width: 50%;\"></div>\nFig. 7. (a) An example of using APSL to find a part of an Alarm Bayesian network structure around node 10 to a depth of 2; (b) an example of using APSL to find a part of an Alarm Bayesian network structure around node 26 to a depth of 2. The red \u2019\u2713\u2019 symbol denotes the edges that local BN structure learning algorithm falsely orients but APSL correctly orients, the blue node is the target node during each iteration, the number in parentheses represents the level of iterations, and \u2019\u00b7 \u00b7 \u00b7 \u2019 means omitted iterations.\n<div style=\"text-align: center;\">Fig. 7. (a) An example of using APSL to find a part of an Alarm Bayesian network structure around node 10 to a depth of 2; (b) an example of using APSL to find a part of an Alarm Bayesian network structure around node 26 to a depth of 2. The red \u2019\u2713\u2019 symbol denotes the edges that local BN structure learning algorithm falsely orients but APSL correctly orients, the blue node is the target node during each iteration, the number in parentheses represents the level of iterations, and \u2019\u00b7 \u00b7 \u00b7 \u2019 means omitted iterations.</div>\nAPSL finds all and only the collider V-structures. 3) Step 3 finds a correct part of a BN structure. Based on the part of a BN skeleton with all non-collider V-structures and collider V-structures, APSL uses Meek-rules to recover the part of a skeleton to a correct part of a structure, some edges cannot be oriented due to the existence of Markov equivalence structures. Finally, APSL terminates when the part of a structure expands to a given depth, and thus APSL finds a correct part of a BN structure. \u25a0 Tracing APSL To further validate that our algorithm can tackle missing V-structures in Expand-Backtracking, we use the same examples in Fig. 4 to trace the execution of APSL. Case 1: As shown in Fig. 7 (a), APSL finds the collider V-structure of 10 at the 1st iteration, 11 \u219210 \u219035. Then, at the 2nd iteration, APSL finds the non-collider V-structure of 11, 11 \u219212 \u219035, which is missed by PCD-by-PCD. Case 2: As shown in Fig. 7 (b), At the 1st iteration, APSL finds the collider V-structures of 26. And at the 2nd iteration, APSL finds the non-collider V-structure of 17, 25 \u219226 \u2190 17, which is missed by CMB.\n# B. APSL-FS: APSL using Feature Selection\nIn this section, we will propose an efficient version of APSL by using feature selection. APSL uses a standard MB discovery algorithm, MMMB or HITON-MB, for MB discovery. However, the standard PC discovery algorithms, MMPC [23] and HITON-PC [25] (used by MMMB and HITON-MB, respectively), need to perform an exhaustive subset search within the currently selected variables as conditioning sets for PC discovery, and thus they are computationally expensive or even prohibitive when the size of the PC set of the target becomes large.\nFeature selection is a common dimensionality reduction technique and plays an essential role in data analytics [31], [32], [10]. Existing feature selection methods can be broadly categorized into embedded methods, wrapper methods, and filter methods [33]. Since filter feature selection methods are fast and independent of any classifiers, they have attracted more attentions. It has been proven in our previous work [34] that some filter feature selection methods based on mutual information prefer the PC of the target variable. Furthermore, these methods use pairwise comparisons [35] (i.e., unconditional independence tests) to remove false positives with less correlations, they can find the potential PC of the target variable without searching for conditioning set, and thus improving the efficiency of PC discovery. Thus, to address the problem exists in APSL for PC discovery, we use a filter feature selection method based on mutual information instead of the standard PC discovery algorithm. However, the feature selection method we use cannot find spouses for edge orientations. Because the feature selection method uses pairwise comparisons rather than conditional independence tests [35], it cannot find the separating sets which is the key to finding spouses [6]. Standard PC discovery algorithms find separating sets to make a target variable and the other variables conditionally independent, only the variables in the PC of the target are always conditionally dependent on the target [6]. Thus, standard PC discovery algorithms find PC and separating sets simultaneously. However, these algorithms are computationally expensive in finding separating sets since they need to find the separating sets of all variables independent of the target. Instead, it is only necessary to find the separating sets of the variables in the PC of each variable in the target\u2019s PC set, as spouses of the target variable exist only there. Thus in this subsection, based on using feature selection for PC discovery, we propose an efficient Markov blanket discovery algorithm for spouses discovery, called MB-FS (Markov Blanket discovery by Feature Selection). Moreover, we use MB-FS instead of the standard MB discovery algorithm for MB discovery in APSL to improve the efficiency, and we call this new any part of BN structure learning algorithm APSL-FS (APSL using Feature Selection), an efficient version of APSL using feature selection. In the following, we will go into details about using feature selection for PC discovery and MB discovery, respectively. (1) PC discovery: We choose a well-established feature selection method, Fast Correlation-Based Filter (FCBF) [35], for PC discovery because the size of the PC of each variable in a BN is not fixed. FCBF specifies a threshold \u03b4 to control the number of potential PC of the target variable, instead of specifying the number of the PC in advance. As illustrated in Algorithm 2, FCBF first finds a potential PC of the target variable from the entire variable set whose correlations with the target are higher than the threshold (Lines 1-6). Then, FCBF uses pairwise comparisons to remove false positives in the potential PC to get the true\nAlgorithm 2: FCBF\nInput: D: Data, T: Target, \u03b4: Threshold;\nOutput: PCT : PC of T ;\n1 S = \u2205;\n2 for each X \u2208U \\ {T} do\n3\nif SU(X; T) > \u03b4 then\n4\nS = S \u222aX;\n5\nend\n6 end\n7 Order S in descending SU(X; T) value;\n8 i = 1;\n9 while i <= |S| do\n10\nj = i + 1;\n11\nwhile j <= |S| do\n12\nif SU(S(i); S(j)) > SU(S(j); T) then\n13\nS(j) = \u2205;\n14\nelse\n15\nj = j + 1;\n16\nend\n17\nend\n18\ni = i + 1;\n19 end\n20 PCT = S;\n21 Return PCT ;\nAlgorithm 3: MB-FS\nInput: D: Data, T: Target, \u03b4: Threshold;\nOutput: [PCT , SPT ]: MB of T ;\n1 PCT = FCBF(D, T, \u03b4);\n2 for each X \u2208PCT do\n3\nPCX = FCBF(D, X, \u03b4);\n4\nfor each Y \u2208PCX do\n5\nif T \u22a5\u22a5Y |Z for some Z \u2286PCT then\n6\nSepT [Y ] = Z;\n7\nif T \u0338\u22a5\u22a5Y |SepT [Y ] \u222a{X} then\n8\nSPT (X) = SPT (X) \u222a{Y };\n9\nend\n10\nend\n11\nend\n12 end\n13 Return [PCT , SPT ];\nPC (Lines 7-20). (2) MB discovery: As illustrated in Algorithm 3, MB-FS first uses FCBF to find the PC of the target variable T, and uses FCBF to find the PC of each variable in the T\u2019s PC as the candidate spouses of T. Then, MB-FS finds the separating set from the subsets of the PC of T, to make T and the variable Y in the candidate spouses are conditionally independent. Finally, if T and Y are conditionally dependent given the union of the separating set and their common child X, Y is a spouse of T owing to Theorem 2.\n# VI. EXPERIMENTS\nIn this section, we will systematically evaluate our presented algorithms. In Section VI-A, we describe the data sets, comparison methods, and evaluation metrics in the experiments. Then in Section VI-B and VI-C, we evaluate our algorithms with local BN structure learning algorithms and global BN structure learning algorithms, respectively.\n<div style=\"text-align: center;\">TABLE II SUMMARY OF BENCHMARK BNS</div>\nNum.\nNum.\nMax In/out-\nMin/Max\nNetwork\nVars\nEdges\nDegree\n|PCset|\nChild\n20\n25\n2/7\n1/8\nInsurance\n27\n52\n3/7\n1/9\nAlarm\n37\n46\n4/5\n1/6\nChild10\n200\n257\n2/7\n1/8\nInsurance10\n270\n556\n5/8\n1/11\nAlarm10\n370\n570\n4/7\n1/9\n# A. Experiment setting\nTo evaluate the APSL and APSL-FS algorithms, we use two groups of data generated from the six benchmark BNs as shown in Table II2. One group includes 10 data sets each with 500 data instances, and the other group also contains 10 data sets each with 1,000 data instances. We compare the APSL and APSL-FS algorithms with 7 other algorithms, including 2 local BN structure learning algorithms, PCD-by-PCD [21] and CMB [22], and 5 global BN structure learning algorithms, GS [12], MMHC [13], SLL+C [26], SLL+G [26], and GGSL [19]. The implementation details and parameter settings of all the algorithms are as follows: 1) PCD-by-PCD, CMB, GS, MMHC3, APSL, and APSLFS are implemented in MATLAB, SLL+C/G4 and GGSL are implemented in C++. 2) The conditional independence tests are G2 tests with the statistical significance level of 0.01, the constrained MB algorithm used by APSL is HITON-MB [25], and the threshold of the feature selection method FCBF [35] used by APSL-FS is 0.05. 3) In all Tables in Section VI, the experimental results are shown in the format of A\u00b1B, where A represents the average results, and B is the standard deviation. The best results are highlighted in boldface. 4) All experiments are conducted on a computer with an Intel Core i7-8700 3.20 GHz with 8GB RAM. Using the BN data sets, we evaluate the algorithms using the following metrics: \u2022 Accuracy. We evaluate the accuracy of the learned structure using Ar Precision, Ar Recall, and Ar Distance. The Ar Precision metric denotes the number of correctly predicted edges in the output divided by the number of true edges in a test DAG, while the Ar Recall metric represents the number of correctly predicted edges in the output divided by the number of predicted edges in the output of an algorithm. The 2The public data sets are available at http : //pages.mtu.edu/ \u223c lebrown/supplements/mmhc paper/mmhc index.html. 3The codes of MMHC in MATLAB are available at http : //mensxmachina.org/en/software/probabilistic \u2212graphical \u2212 model \u2212toolbox/. 4The codes of SLL+C and SLL+G in C++ are available at https : //www.cs.helsinki.fi/u/tzniinim/uai2012/.\n2The public data sets are available at http : //pages.mtu.edu/ \u223c lebrown/supplements/mmhc paper/mmhc index.html. 3The codes of MMHC in MATLAB are available at http : //mensxmachina.org/en/software/probabilistic \u2212graphical \u2212 model \u2212toolbox/. 4The codes of SLL+C and SLL+G in C++ are available at https : //www.cs.helsinki.fi/u/tzniinim/uai2012/.\n<div style=\"text-align: center;\">TABLE III AR DISTANCE, AR PRECISION, AR RECALL, AND RUNTIME (IN SECONDS) ON LEARNING A PART OF BN STRUCTURES TO A DEPTH OF 1 USING DIFFERENT DATA SIZES (A \u00b1 B: A REPRESENTS THE AVERAGE RESULTS WHILE B IS THE STANDARD DEVIATION. THE BEST RESULTS ARE HIGHLIGHTED IN BOLDFACE.)</div>\nSize=500\nSize=1,000\nNetwork\nAlgorithm\nAr Distance\nAr Precision\nAr Recall\nRuntime\nAr Distance\nAr Precision\nAr Recall\nRuntime\nChild\nPCD-by-PCD\n0.57\u00b10.55\n0.66\u00b10.42\n0.56\u00b10.39\n0.17\u00b10.08\n0.46\u00b10.47\n0.69\u00b10.35\n0.67\u00b10.34\n0.25\u00b10.14\nCMB\n0.54\u00b10.52\n0.61\u00b10.38\n0.65\u00b10.39\n0.63\u00b10.24\n0.39\u00b10.50\n0.73\u00b10.36\n0.73\u00b10.36\n0.67\u00b10.38\nAPSL\n0.63\u00b10.48\n0.55\u00b10.34\n0.57\u00b10.37\n0.12\u00b10.06\n0.45\u00b10.50\n0.70\u00b10.36\n0.68\u00b10.36\n0.18\u00b10.06\nASPL-FS\n0.37\u00b10.46\n0.80\u00b10.32\n0.70\u00b10.35\n0.03\u00b10.02\n0.37\u00b10.46\n0.83\u00b10.32\n0.70\u00b10.35\n0.03\u00b10.03\nInsurance\nPCD-by-PCD\n0.90\u00b10.38\n0.45\u00b10.35\n0.33\u00b10.28\n0.16\u00b10.15\n0.70\u00b10.46\n0.64\u00b10.38\n0.43\u00b10.32\n0.23\u00b10.20\nCMB\n0.81\u00b10.44\n0.49\u00b10.36\n0.40\u00b10.33\n0.36\u00b10.22\n0.77\u00b10.43\n0.52\u00b10.34\n0.42\u00b10.31\n0.56\u00b10.37\nAPSL\n0.83\u00b10.38\n0.48\u00b10.34\n0.38\u00b10.25\n0.10\u00b10.09\n0.51\u00b10.36\n0.76\u00b10.30\n0.59\u00b10.27\n0.14\u00b10.10\nASPL-FS\n0.66\u00b10.50\n0.67\u00b10.43\n0.48\u00b10.33\n0.05\u00b10.06\n0.85\u00b10.48\n0.49\u00b10.42\n0.36\u00b10.32\n0.07\u00b10.06\nAlarm\nPCD-by-PCD\n0.61\u00b10.57\n0.60\u00b10.42\n0.56\u00b10.41\n0.21\u00b10.22\n0.50\u00b10.55\n0.69\u00b10.40\n0.63\u00b10.40\n0.29\u00b10.27\nCMB\n0.65\u00b10.59\n0.58\u00b10.44\n0.53\u00b10.42\n0.32\u00b10.33\n0.58\u00b10.58\n0.62\u00b10.42\n0.58\u00b10.41\n0.24\u00b10.19\nAPSL\n0.53\u00b10.57\n0.66\u00b10.42\n0.63\u00b10.40\n0.17\u00b10.16\n0.41\u00b10.49\n0.75\u00b10.36\n0.70\u00b10.36\n0.23\u00b10.20\nASPL-FS\n0.61\u00b10.55\n0.61\u00b10.41\n0.55\u00b10.40\n0.07\u00b10.09\n0.51\u00b10.55\n0.69\u00b10.41\n0.62\u00b10.39\n0.07\u00b10.09\nChild10\nPCD-by-PCD\n0.91\u00b10.49\n0.38\u00b10.38\n0.35\u00b10.35\n2.03\u00b13.29\n0.73\u00b10.55\n0.51\u00b10.41\n0.48\u00b10.40\n2.46\u00b13.73\nCMB\n0.60\u00b10.47\n0.60\u00b10.36\n0.60\u00b10.36\n2.41\u00b12.61\n0.55\u00b10.49\n0.62\u00b10.36\n0.63\u00b10.36\n1.98\u00b12.06\nAPSL\n0.68\u00b10.49\n0.52\u00b10.37\n0.56\u00b10.38\n0.83\u00b11.62\n0.48\u00b10.45\n0.66\u00b10.33\n0.69\u00b10.34\n1.05\u00b12.16\nASPL-FS\n0.53\u00b10.53\n0.70\u00b10.40\n0.59\u00b10.38\n0.08\u00b10.06\n0.47\u00b10.52\n0.75\u00b10.39\n0.63\u00b10.37\n0.15\u00b10.11\nInsurance10\nPCD-by-PCD\n0.85\u00b10.41\n0.45\u00b10.33\n0.38\u00b10.31\n1.33\u00b14.15\n0.72\u00b10.43\n0.58\u00b10.36\n0.44\u00b10.30\n1.22\u00b13.93\nCMB\n0.82\u00b10.39\n0.46\u00b10.31\n0.42\u00b10.31\n1.49\u00b11.30\n0.68\u00b10.41\n0.57\u00b10.33\n0.50\u00b10.31\n2.06\u00b11.84\nAPSL\n0.71\u00b10.38\n0.54\u00b10.31\n0.50\u00b10.30\n0.50\u00b11.23\n0.54\u00b10.40\n0.67\u00b10.31\n0.61\u00b10.31\n0.84\u00b12.56\nASPL-FS\n0.71\u00b10.40\n0.66\u00b10.37\n0.42\u00b10.29\n1.09\u00b11.86\n0.60\u00b10.40\n0.79\u00b10.35\n0.48\u00b10.30\n0.34\u00b10.32\nAlarm10\nPCD-by-PCD\n0.83\u00b10.51\n0.50\u00b10.43\n0.38\u00b10.36\n5.55\u00b19.42\n0.72\u00b10.53\n0.59\u00b10.42\n0.45\u00b10.37\n4.91\u00b19.64\nCMB\n0.81\u00b10.53\n0.47\u00b10.42\n0.42\u00b10.39\n1.42\u00b11.48\n0.70\u00b10.55\n0.57\u00b10.42\n0.49\u00b10.40\n0.92\u00b10.87\nAPSL\n0.70\u00b10.48\n0.56\u00b10.39\n0.49\u00b10.36\n3.16\u00b16.21\n0.55\u00b10.48\n0.69\u00b10.37\n0.59\u00b10.36\n5.19\u00b18.97\nASPL-FS\n0.68\u00b10.48\n0.61\u00b10.39\n0.49\u00b10.35\n0.82\u00b11.86\n0.65\u00b10.51\n0.65\u00b10.41\n0.49\u00b10.36\n1.48\u00b12.64\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d82c/d82c144d-ba7a-404a-afc6-d43573281262.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gram of the Nemenyi test of Ar Distance and Runtime on learning a part of BN structu</div>\n<div style=\"text-align: center;\">Fig. 8. Crucial difference diagram of the Nemenyi test of Ar Distance and Runtime o</div>\n<div style=\"text-align: center;\">menyi test of Ar Distance and Runtime on learning a part of BN structures (Depth=1).</div>\nAr Distance metric is the harmonic average of the Ar Precision and Ar Recall, Ar Distance = \ufffd (1 \u2212Ar Precision)2 + (1 \u2212Ar Recall)2, where the lower Ar Distance is better. \u2022 Efficiency. We report running time (in seconds) as the efficiency measure of different algorithms. The running time of feature selection is included in the total running time of our method.\n# B. Comparison of our methods with local methods\nIn this subsection, using six BNs, we compare our methods with the local methods on learning a part of a BN structure around each node to a depth of 1, Tables III summarizes the detailed results. In efficiency. PCD-by-PCD uses symmetry constraint to generate undirected edges, then it finds more PCs than APSL, and thus it is slower than APSL. CMB spends\ntime tracking conditional independence changes after MB discovery, so it is inferior to APSL in efficiency. APSL-FS does not need to perform an exhaustive subset search within conditioning sets for PC discovery, then it is much faster than APSL. In accuracy. The symmetry constraint used by PCDby-PCD may remove more true nodes, leading to a low accuracy of PCD-by-PCD. CMB uses entire MB set as the conditioning set for tracking conditional independence changes, so it is also inferior to APSL in accuracy. APSL-FS does not use conditioning set for independence tests, then it reduces the requirement of data samples, and more accurate than APSL on samll-sized sample data sets. To further evaluate the accuracy and efficiency of our methods against local methods, we conduct the Friedman test at a 5% significance level under the null hypothesis, which states that whether the accuracy and efficiency of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a630/a630f0c8-f513-449d-949d-b0b26f908b49.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">of BN structures to a depth of 3 using different data sizes (the labels of the x-axis from  d10. 5: Insurance10. 6: Alarm10, and all figures use the same legend).</div>\n<div style=\"text-align: center;\">Fig. 9. The experimental results of learning a part of BN structures to a depth of 3 using different data sizes (the labels of the x-axis from 1 to 6 denote the BNs. 1: Child. 2: Insurance. 3: Alarm. 4: Child10. 5: Insurance10. 6: Alarm10, and all figures use the same legend).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/72e1/72e180a8-8fb5-4ea6-8662-e7ed204fe93a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">The experimental results of learning a part of BN structures to a depth of 5 using differe as those in Fig. 10, and all figures use the same legend).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/10c3/10c34a27-9840-4c60-8335-b2ff6d8dbe45.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"> experimental results of learning a part of BN structures to a depth of the maximum dept are the same as those in Fig. 10, and all figures use the same legend).</div>\n<div style=\"text-align: center;\">Fig. 11. The experimental results of learning a part of BN structures to a depth of the maximum depth using different data sizes (the labels of the x-axi rom 1 to 6 are the same as those in Fig. 10, and all figures use the same legend).</div>\nAPSL and APSL-FS and that of PCD-by-PCD and CMB have no significant difference. Both of the null hypotheses of Ar Distance and Runtime are rejected, the average ranks of Ar Distance for PCD-by-PCD, CMB, APSL, and APSLFS are 1.54, 2.17, 3.04, and 3.25, respectively (the higher the average rank, the better the performance in accuracy), and the average ranks of Runtime for PCD-by-PCD, CMB, APSL, and APSL-FS are 1.75, 1.58, 2.83, 3.83, respectively (the higher the average rank, the better the performance in efficiency). Then, we proceed with the Nemenyi test as a posthoc test. With the Nemenyi test, the performance of two methods is significantly different if the corresponding average ranks differ by at least the critical difference. With the Nemenyi test, both of the critical differences of Ar Distance and Runtime are up to 1.35. Thus, we can observe that APSL-FS is significantly more accurate than PCD-by-PCD, and APSLFS is significantly more efficient than both of PCD-by-PCD CMB on learning a part of a BN structure to a depth of 1. We plot the crucial difference diagram of the Nemenyi test in Fig. 8.\n<div style=\"text-align: center;\">TABLE IV FIVE NODES WITH THE LARGEST PC SET ON EACH BN</div>\nNetwork\nSelected five nodes\nChild\n2, 6, 7, 9, 11\nInsurance\n2, 3, 4, 5, 8\nAlarm\n13, 14, 21, 22, 29\nChild10\n12, 52, 92, 132, 172\nInsurance10\n164, 166, 191, 193, 245\nAlarm10\n13, 23, 66, 103, 140\n# C. Comparison of our methods with global methods\nIn this subsection, we compare our methods with the global methods on learning a part of a BN structure to a depth of 3, 5, and the maximum depth, respectively. In Fig. 9-11, we plot the results of Ar Distance and Runtime of APSL and APSL-FS against global methods on learning part of BN structures around the five nodes with the largest PC set on each BN to a depth of 3, 5, and the maximum, respectively. The selected five nodes of each BN\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d0c/3d0c6544-f78e-41e7-a0f8-1c34b37e8913.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d54d/d54dd2c3-c737-4a6e-9c64-6e86bdfdc2e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/92da/92daa383-80dd-4037-8f5a-8e6cf36130bf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7da/b7da136e-61a3-40fe-aa76-b4ce92bbd20d.png\" style=\"width: 50%;\"></div>\nare shown in Table IV. Since SLL+C, SLL+G, and GGSL cannot generate any results on Child10, Insurance10, and Alarm10 due to memory limitation, we only plot the results of them on Child, Insurance, and Alarm. In efficiency. When learning a part of BN structures to depths of 3 and 5, since APSL and APSL-FS do not need to find the entire structures, they are faster than the global BN structure learning algorithms. When learning a part of BN structures to a depth of the maximum depth, both of our methods and global methods need to find the entire structure. However, 1) Although GS uses GSMB, an efficient MB discovery algorithm without searching for conditioning set, to find MB of each node, it still takes extra time to search for conditioning set during V-structure discovery. So GS is slightly inferior to APSL in efficiency. 2) May be using conditional independence tests is faster than using\nscore functions for edge orientations, then MMHC is slower than APSL. 3) As for SLL+C, SLL+G, and GGSL, the scorebased MB/PC methods used by them need to learn a local BN structure involving all nodes selected currently at each iteration, so they are time-consuming on small-sized BNs, and infeasible on large-sized BNs. 4) Clearly, APSL-FS is more efficient than APSL. In accuracy. When learning a part of BN structures to depths of 3 and 5, since global methods consider the global information of the structures, the accuracy of our methods is lower that of global methods except for GS. Because the GSMB (used by GS) require a large number of data samples, and its heuristic function also leads to a low MB discovery accuracy. When learning a part of BN structures to a depth of the maximum depth, 1) since the same reason of GS when learning to a depth of 3 and 5, GS is inferior\nto our methods in accuracy. 2) MMHC uses score functions for edge orientations, it can also remove false edges in the learned skeleton, while APSL can only orient edges in the learned skeleton using conditional independence tests, then MMHC is more accurate than APSL. 3) As for SLL+C, SLL+G, and GGSL, since they involve all nodes selected currently at each iteration, they are slightly more accurate than other methods on small-sized BNs, but cannot generate any results on large-sized BNs. 4) Similarly, APSL-FS is more accurate than APSL. To further evaluate the accuracy and efficiency of our methods against global methods, we conduct the Friedman test at a 5% significance level under the null hypothesis. Since SLL+C, SLL+G, and GGSL fail on the large-sized BN data sets, we do not compare our methods with them using the Friedman test. 1) Depth=3. Both of the null hypotheses of Ar Distance and Runtime are rejected, the average ranks of Ar Distance for GS, MMHC, APSL, and APSL-FS are 1.08, 3.42, 2.71, and 2.79, respectively, and the average ranks of Runtime for GS, MMHC, APSL, and APSL-FS are 2.08, 1.08, 2.83, and 4.00, respectively. Then, With the Nemenyi test, both of the critical differences of Ar Distance and Runtime are up to 1.35. Thus, we can observe that APSL and APSL-FS are significantly more accurate than GS and significantly more efficient than MMHC, and APSL-FS is significantly more efficient than GS on learning a part of a BN structure to a depth of 3. We plot the crucial difference diagram of the Nemenyi test in Fig. 12. 2) Depth=5. Similar to the results in Depth=3, the average ranks of Ar Distance for GS, MMHC, APSL, and APSL-FS are 1.08, 3.46, 2.50, and 2.96, respectively, and the average ranks of Runtime for GS, MMHC, APSL, and APSL-FS are 2.25, 1.08, 2.67, and 4.00, respectively. With the critical differences of Ar Distance and Runtime are up to 1.35, we can observe that APSL and APSL-FS are significantly more accurate than GS and significantly more efficient than MMHC, and APSL-FS is significantly more efficient than GS on learning a part of a BN structure to a depth of 5. We plot the crucial difference diagram of the Nemenyi test in Fig. 13. 3) Depth=max. Similarly, the average ranks of Ar Distance for GS, MMHC, APSL, and APSL-FS are 1.04, 3.13, 2.92, and 2.92, respectively, and the average ranks of Runtime for GS, MMHC, APSL, and APSL-FS are 2.38, 1.08, 2.54, and 4.00, respectively. With the critical differences of Ar Distance and Runtime are up to 1.35, we can observe that APSL and APSL-FS are significantly more accurate than GS and significantly more efficient than MMHC, and APSL-FS is significantly more efficient than GS on learning a part of a BN structure to a depth of the maximum. We plot the crucial difference diagram of the Nemenyi test in Fig. 14. VII. CONCLUSION In this paper, we present a new concept of Expand-\n# VII. CONCLUSION\nIn this paper, we present a new concept of ExpandBacktracking to describe the learning process of the exsiting\nlocal BN structure learning algorithms, and analyze the missing V-structures in Expand-Backtracking. Then we propose an efficient and accurate any part of BN structure learning algorithm, APSL. APSL learns a part of a BN structure around any one node to any depth, and tackles missing V-structures in Expand-Backtracking by finding both of collider V-structures and non-collider V-structures in MBs at each iteration. In addition, we design an any part of BN structure learning algorithm using feature selection, APSLFS, to improve the efficiency of APSL by finding PC without searching for conditioning sets. The extensive experimental results have shown that our algorithms achieve higher efficiency and better accuracy than state-of-the-art local BN structure learning algorithms on learning any part of a BN structure to a depth of 1, and achieve higher efficiency than state-of-the-art global BN structure learning algorithms on learning any part of a BN structure to a depth of 3, 5, and the maximum depth. Future research direction could focus on using mutual information-based feature selection methods for V-structure discovery without searching for conditioning sets, because performing an exhaustive subset search within PC for finding V-structures is time-consuming.\n[1] J. Pearl, \u201cMorgan kaufmann series in representation and reasoning,\u201d Probabilistic reasoning in intelligent systems: Networks of plausible inference. San Mateo, CA, US: Morgan Kaufmann, 1988. [2] G. F. Cooper, \u201cA simple constraint-based algorithm for efficiently mining observational databases for causal relationships,\u201d Data Mining and Knowledge Discovery, vol. 1, no. 2, pp. 203\u2013224, 1997. [3] I. Guyon, C. Aliferis et al., \u201cCausal feature selection,\u201d in Computational methods of feature selection. Chapman and Hall/CRC, 2007, pp. 75\u201397. [4] C. F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. D. Koutsoukos, \u201cLocal causal and markov blanket induction for causal discovery and feature selection for classification part ii: Analysis and extensions,\u201d Journal of Machine Learning Research, vol. 11, no. Jan, pp. 235\u2013284, 2010. [5] I. Tsamardinos and C. F. Aliferis, \u201cTowards principled feature selection: relevancy, filters and wrappers.\u201d in AISTATS, 2003. [6] C. F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. D. Koutsoukos, \u201cLocal causal and markov blanket induction for causal discovery and feature selection for classification part i: Algorithms and empirical evaluation,\u201d Journal of Machine Learning Research, vol. 11, no. Jan, pp. 171\u2013234, 2010. [7] Z. Ling, K. Yu, H. Wang, L. Liu, W. Ding, and X. Wu, \u201cBamb: A balanced markov blanket discovery approach to feature selection,\u201d ACM Transactions on Intelligent Systems and Technology (TIST), vol. 10, no. 5, pp. 1\u201325, 2019. [8] H. Wang, Z. Ling, K. Yu, and X. Wu, \u201cTowards efficient and effective discovery of markov blankets for feature selection,\u201d Information Sciences, vol. 509, pp. 227\u2013242, 2020. [9] P. Spirtes, C. N. Glymour, and R. Scheines, Causation, prediction, and search. MIT press, 2000. 10] K. Yu, L. Liu, J. Li, W. Ding, and T. Le, \u201cMulti-source causal feature selection,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. DOI: 10.1109/TPAMI.2019.2908373, 2019. 11] S. Andreassen, A. Rosenfalck, B. Falck, K. G. Olesen, and S. K. Andersen, \u201cEvaluation of the diagnostic performance of the expert emg assistant munin,\u201d Electroencephalography and Clinical Neurophysiology/Electromyography and Motor Control, vol. 101, no. 2, pp. 129\u2013144, 1996. 12] D. Margaritis and S. Thrun, \u201cBayesian network induction via local neighborhoods,\u201d in Advances in neural information processing systems, 2000, pp. 505\u2013511.\n[13] I. Tsamardinos, L. E. Brown, and C. F. Aliferis, \u201cThe max-min hillclimbing bayesian network structure learning algorithm,\u201d Machine learning, vol. 65, no. 1, pp. 31\u201378, 2006. [14] J.-P. Pellet and A. Elisseeff, \u201cUsing markov blankets for causal structure learning,\u201d Journal of Machine Learning Research, vol. 9, no. Jul, pp. 1295\u20131342, 2008. [15] J. Pearl, \u201cFusion, propagation, and structuring in belief networks,\u201d Artificial intelligence, vol. 29, no. 3, pp. 241\u2013288, 1986. [16] D. M. Chickering, D. Heckerman, and C. Meek, \u201cLarge-sample learning of bayesian networks is np-hard,\u201d Journal of Machine Learning Research, vol. 5, no. Oct, pp. 1287\u20131330, 2004. [17] M. Scanagatta, G. Corani, C. P. de Campos, and M. Zaffalon, \u201cLearning treewidth-bounded bayesian networks with thousands of variables,\u201d in Advances in neural information processing systems, 2016, pp. 1462\u20131470. [18] D. Vidaurre, C. Bielza, and P. Larra\u02dcnaga, \u201cLearning an l1-regularized gaussian bayesian network in the equivalence class space,\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, no. 5, pp. 1231\u20131242, 2010. [19] T. Gao, K. Fadnis, and M. Campbell, \u201cLocal-to-global bayesian network structure learning,\u201d in International Conference on Machine Learning, 2017, pp. 1193\u20131202. [20] T. Gao and Q. Ji, \u201cEfficient score-based markov blanket discovery,\u201d International Journal of Approximate Reasoning, vol. 80, pp. 277\u2013 293, 2017. [21] J. Yin, Y. Zhou, C. Wang, P. He, C. Zheng, and Z. Geng, \u201cPartial orientation and local structural learning of causal networks for prediction,\u201d in Causation and Prediction Challenge, 2008, pp. 93\u2013105. [22] T. Gao and Q. Ji, \u201cLocal causal discovery of direct causes and effects,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 2512\u20132520. [23] I. Tsamardinos, C. F. Aliferis, and A. Statnikov, \u201cTime and sample efficient discovery of markov blankets and direct causal relations,\u201d in Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2003, pp. 673\u2013678. [24] C. Meek, \u201cCausal inference and causal explanation with background knowledge,\u201d in Proceedings of the Eleventh conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1995, pp. 403\u2013410. [25] C. F. Aliferis, I. Tsamardinos, and A. Statnikov, \u201cHiton: a novel markov blanket algorithm for optimal variable selection,\u201d in AMIA annual symposium proceedings, vol. 2003. American Medical Informatics Association, 2003, pp. 21\u201325. [26] T. Niinimki and P. Parviainen, \u201cLocal structure discovery in bayesian networks,\u201d in 28th Conference on Uncertainty in Artificial Intelligence, UAI 2012; Catalina Island, CA; United States; 15 August 2012 through 17 August 2012, 2012, pp. 634\u2013643. [27] J. Pearl, Probabilistic reasoning in intelligent systems: networks of plausible inference. Elsevier, 2014. [28] T. Gao and Q. Ji, \u201cEfficient markov blanket discovery and its application,\u201d IEEE transactions on cybernetics, vol. 47, no. 5, pp. 1169\u20131179, 2017. [29] I. A. Beinlich, H. J. Suermondt, R. M. Chavez, and G. F. Cooper, \u201cThe alarm monitoring system: A case study with two probabilistic inference techniques for belief networks,\u201d in AIME 89. Springer, 1989, pp. 247\u2013256. [30] X. Xie, Z. Geng, and Q. Zhao, \u201cDecomposition of structural learning about directed acyclic graphs,\u201d Artificial Intelligence, vol. 170, no. 4-5, pp. 422\u2013439, 2006. [31] I. Guyon and A. Elisseeff, \u201cAn introduction to variable and feature selection,\u201d Journal of machine learning research, vol. 3, no. Mar, pp. 1157\u20131182, 2003. [32] K. Yu, X. Wu, W. Ding, and J. Pei, \u201cScalable and accurate online feature selection for big data,\u201d ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 11, no. 2, pp. 1\u201339, 2016. [33] X. Wu, K. Yu, W. Ding, H. Wang, and X. Zhu, \u201cOnline feature selection with streaming features,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 5, pp. 1178\u20131192, 2013. [34] K. Yu, L. Liu, and J. Li, \u201cA unified view of causal and non-causal feature selection,\u201d arXiv preprint arXiv:1802.05844, 2018. [35] L. Yu and H. Liu, \u201cEfficient feature selection via analysis of relevance and redundancy,\u201d Journal of machine learning research, vol. 5, no. Oct, pp. 1205\u20131224, 2004.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of learning any part of a Bayesian network (BN) structure, which is computationally inefficient using existing global BN structure learning algorithms. Local BN structure learning methods face challenges, such as the false edge orientation problem, necessitating a new approach.",
        "problem": {
            "definition": "The problem focuses on the inefficiencies of existing methods in learning a part of a BN structure, particularly when only interested in a subnetwork rather than the entire structure.",
            "key obstacle": "The main challenge is that existing methods either require learning the entire BN structure, which is NP-complete for large networks, or they fail to accurately orient edges due to missing V-structures."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for an efficient method to learn specific parts of a BN structure without the overhead of global learning algorithms.",
            "opinion": "The proposed method, APSL, aims to efficiently learn any part of a BN structure by focusing on the Markov blanket of a node of interest.",
            "innovation": "APSL introduces the concept of Expand-Backtracking and distinguishes between collider and non-collider V-structures to improve the accuracy and efficiency of learning."
        },
        "method": {
            "method name": "Any Part of BN Structure Learning",
            "method abbreviation": "APSL",
            "method definition": "APSL is designed to learn a part of a BN structure around a target node to any specified depth efficiently and accurately.",
            "method description": "APSL recursively identifies and orients both collider and non-collider V-structures in the Markov blanket of a target node.",
            "method steps": [
                "Identify the Markov blanket of the target node.",
                "Find non-collider V-structures and orient edges.",
                "Find collider V-structures and orient edges using conditional independence tests."
            ],
            "principle": "The method is effective due to its recursive approach to identifying V-structures and its reliance on the properties of conditional independence to ensure accurate edge orientation."
        },
        "experiments": {
            "evaluation setting": "Extensive experiments were conducted using six benchmark BNs with varying sizes and structures to validate the efficiency and accuracy of APSL compared to state-of-the-art algorithms.",
            "evaluation method": "Performance was assessed using metrics such as accuracy (Ar Precision, Ar Recall, Ar Distance) and runtime, comparing APSL with local and global BN structure learning algorithms."
        },
        "conclusion": "The experiments demonstrated that APSL achieves higher efficiency and better accuracy than existing local BN structure learning algorithms when learning any part of a BN structure to a depth of 1, and it outperforms global methods in efficiency for depths of 3, 5, and maximum depth.",
        "discussion": {
            "advantage": "APSL effectively tackles the missing V-structure problem that plagues existing local methods, leading to improved accuracy in edge orientation.",
            "limitation": "While APSL improves upon local methods, it may still face challenges in very large networks where even local learning can be computationally intensive.",
            "future work": "Future research could explore using mutual information-based feature selection methods for V-structure discovery to enhance efficiency further."
        },
        "other info": {
            "Funding": "This work is supported by the National Key Research and Development Program of China and the National Science Foundation of China.",
            "Authors": [
                {
                    "name": "Zhaolong Ling",
                    "affiliation": "Anhui University, China"
                },
                {
                    "name": "Kui Yu",
                    "affiliation": "Hefei University of Technology, China"
                },
                {
                    "name": "Hao Wang",
                    "affiliation": "Hefei University of Technology, China"
                },
                {
                    "name": "Lin Liu",
                    "affiliation": "University of South Australia, Australia"
                },
                {
                    "name": "Jiuyong Li",
                    "affiliation": "University of South Australia, Australia"
                }
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem focuses on the inefficiencies of existing methods in learning a part of a Bayesian network (BN) structure, particularly when only interested in a subnetwork rather than the entire structure."
        },
        {
            "section number": "2.1",
            "key information": "Local BN structure learning methods face challenges, such as the false edge orientation problem, necessitating a new approach."
        },
        {
            "section number": "2.1",
            "key information": "APSL is designed to learn a part of a BN structure around a target node to any specified depth efficiently and accurately."
        },
        {
            "section number": "5.1",
            "key information": "APSL introduces the concept of Expand-Backtracking and distinguishes between collider and non-collider V-structures to improve the accuracy and efficiency of learning."
        },
        {
            "section number": "5.4",
            "key information": "While APSL improves upon local methods, it may still face challenges in very large networks where even local learning can be computationally intensive."
        },
        {
            "section number": "7.1",
            "key information": "APSL effectively tackles the missing V-structure problem that plagues existing local methods, leading to improved accuracy in edge orientation."
        },
        {
            "section number": "7.3",
            "key information": "Future research could explore using mutual information-based feature selection methods for V-structure discovery to enhance efficiency further."
        }
    ],
    "similarity_score": 0.5776573639400571,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Any Part of Bayesian Network Structure Learning.json"
}