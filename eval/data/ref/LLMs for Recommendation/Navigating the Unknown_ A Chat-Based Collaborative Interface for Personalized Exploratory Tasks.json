{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.24032",
    "title": "Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks",
    "abstract": "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE's interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE's potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.",
    "bib_name": "peng2024navigatingunknownchatbasedcollaborative",
    "md_text": "# Navigating the Unknown: A Chat-Based Collaborative Interface for Personaliz Exploratory Tasks\nYINGZHE PENG\u2217, Southeast University, China XIAOTING QIN, Microsoft, China ZHIYANG ZHANG, State Key Laboratory for Novel Software Technology, Nanjing University, China JUE ZHANG\u2020, Microsoft, China QINGWEI LIN, Microsoft, China XU YANG, Southeast University, China DONGMEI ZHANG, Microsoft, China SARAVAN RAJMOHAN, Microsoft, USA QI ZHANG, Microsoft, China\nQI ZHANG, Microsoft, China\nThe rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE\u2019s interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE\u2019s potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.\n# 1 Introduction\nRecent advancements in large language models (LLMs) [1, 10, 32] have transformed user interactions with knowledgebased systems by enabling chatbots to synthesize vast amounts of information, surpassing human cognitive limits. While traditional search engines like Bing [4] and Google [16] efficiently address straightforward queries like fact-checking or retrieving specific information, LLM-based chatbots truly shine in guiding users through more open-ended and exploratory tasks [19, 23, 28, 41, 46]. For instance, they can support users in researching emerging scientific fields [26], or planning intricate projects that require synthesizing information [40] from diverse sources and close user engagement. This capability has made LLM-based chatbots invaluable in helping users navigate unfamiliar areas. Despite their extensive capabilities, LLM-based chatbots face challenges in delivering personalized assistance during exploratory tasks [20, 24], particularly when they lack access to user-specific data, such as past interactions and personal preferences. This limitation forces them to rely heavily on user-provided inputs for personalization. In exploratory\n\u2217Work is done during an internship at Microsoft. \u2020Corresponding author.\nAuthors\u2019 Contact Information: Yingzhe Peng, yingzhe.peng@seu.edu.cn, Southeast University, China; Xiaoting Qin, xiaotingqin@microsoft.com, Microsoft China; Zhiyang Zhang, State Key Laboratory for Novel Software Technology, Nanjing University, China; Jue Zhang, jue.zhang@microsoft.com, Microsoft China; Qingwei Lin, Microsoft, China; Xu Yang, Southeast University, China; Dongmei Zhang, Microsoft, China; Saravan Rajmohan, Microsoft, USA; Qi Zhang, Microsoft, China. Manuscript submitted to ACM 1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5711/5711c67f-41b0-4ae8-9648-1fd6aba4b628.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Baseline (Conventional Chatbot)</div>\n<div style=\"text-align: center;\">Fig. 1. Comparison of the UI and interaction styles between the CARE System and Baseline System. At the top is the CARE System, displaying the Conversation Panel, Solution Panel, and Needs Panel. The CARE System actively prompts the user, gathering their needs before creating a tailored plan. In contrast, the Baseline System, shown at the bottom right, features only a Chat Panel and tends to provide direct answers to the user\u2019s queries.</div>\ntasks, where users often begin with vague queries, generating answers based solely on limited information typically results in generic and impractical recommendations. For example, in travel planning, a query like \u201cPlan a 5-day trip to Hawaii\u201d without additional details (e.g., budget, preferred activities, or accommodation preferences) leads to an overly generic response, as shown in the bottom part of Figure 1. This lack of personalization arises either because users may not be aware of the essential details to include or because providing detailed information imposes cognitive load on users. Consequently, the travel plan generated from such a vague query would be insufficiently tailored to the user\u2019s specific needs. While users can refine their requests after receiving an immediate solution from the initial vague query, limitations remain, such as bias toward existing solutions and reduced motivation to explore alternative dimensions, and dissatisfaction is not always resolved despite using specification tactics [24]. In this study, we introduce the Collaborative Assistant for Personalized Exploration (CARE) system to tackle the challenges associated with generating personalized solutions in exploratory tasks. CARE facilitates tailored exploration Manuscript submitted to ACM\nthrough a user interface composed of three key elements (see Figure 1): the Chat Panel, Solution Panel, and Needs Panel, enabling dynamic user interaction and iterative solution refinement. The system operates on a multi-agent collaboration framework featuring serveral LLM-driven agents responsible for managing the entire process, from user needs identification to solution generation. These agents collaborate to extract both explicit and implicit user needs, prioritize inquiries, and craft customized solutions. The system also ensures transparency, flexibility, and usability, allowing users to refine their queries iteratively and receive personalized, actionable results. To evaluate the effectiveness of the proposed CARE system, we performed a within-subject user study with 22 participants. Each participant completed two exploratory tasks using both the CARE system and a baseline LLM-based chatbot. The results revealed a clear preference for CARE, with 16 out of 22 participants favoring it over the baseline. Participants praised CARE for reducing cognitive load, inspiring creative exploration, and providing more personalized solutions. Many appreciated how CARE\u2019s structured interface helped them organize complex tasks systematically and encouraged them to think about aspects they had not considered. Overall, CARE was seen as more engaging and effective in supporting exploratory tasks compared to the baseline system. Our contribution can be summarized as follows: \u2022 We introduce a collaborative, chat-based interface designed to facilitate users in accomplishing exploratory tasks, particularly when they initially present vague or ambiguous queries due to insufficient knowledge. \u2022 The interface is powered by an LLM-based multi-agent collaboration system, demonstrating its functionality and effectiveness in aiding users. \u2022 We conduct a comprehensive user study to evaluate the effectiveness of the proposed system, providing valuable insights that can inform the design of future user-centric exploratory systems.\n# 2.1 Large Language Models for Exploratory Tasks\nLLM-based human-computer interaction (HCI) systems, such as ChatGPT [31], Copilot [29], and Gemini [15], have expanded the role of AI in supporting users with exploratory tasks. Unlike traditional search engines that excel at handling well-defined queries, LLMs demonstrate notable strengths in assisting with open-ended, complex tasks. Recent work in HCI has explored the integration of LLMs into various domains, including creative workflows [41, 43, 46, 47], help-seeking [21, 36], planning [28, 50], and data exploration [23], revealing the potential of LLMs to facilitate adaptive interactions in complex contexts. Much of this research focuses on advancing prompt-based methods to enhance LLM performance, unlocking reasoning, problem-solving, and planning capabilities [6, 44]. While effective, prompt-based techniques encounter challenges such as inconsistent instruction-following [17, 48, 49], leading to reliability issues in chatbot behavior. These limitations underscore the need for systems that can more effectively manage the complexity and ambiguity inherent in exploratory tasks. Our work builds on this body of research by introducing a multi-agent collaboration system specifically designed for exploratory tasks. Task decomposition, a method effective in managing complexity, plays a central role in our approach. While some systems attempt to address exploratory tasks by prompting a single LLM agent, we found that a multi-agent architecture offers more rigorous task management and better alignment with user needs. Single-agent system often struggle to maintain focus across multiple responsibilities or lose track of ongoing tasks [24, 46], which can disrupt user progress. By assigning distinct roles to each agent, our system maintains greater control over task decomposition, milestone tracking, and user intent discovery, resulting in a more structured and personalized exploratory experience. Manuscript submitted to ACM\n# 2.2 Personalization and Inspiration in LLM-Based Systems\nPersonalization is a critical factor in enhancing user experience across AI-driven systems [25, 42]. Many interactive platforms, such as recommendation engines [2, 38] and adaptive learning systems [27, 30], achieve personalization by leveraging user data, including past behavior, preferences, and explicit feedback. However, in the context of LLM-based systems, personalization presents greater challenges, as these models typically lack access to user-specific data unless explicitly provided within the conversation (e.g., via prompts) [20]. Consequently, LLMs often generate responses based only on the limited context of the current interaction, which can result in generic or irrelevant outputs, particularly in exploratory tasks. This passive approach to personalization\u2014where LLMs react solely to the user\u2019s inputs and user follows up with new prompt\u2014often leads to incomplete articulations of the user\u2019s needs, resulting in less comprehensive and relevant responses [22]. This is especially problematic in open-ended, complex tasks where users may not fully express or even realize their own requirements. It is important to note that while adding a memory module to LLM-based chatbots can improve personalization by enabling the system to reference prior interactions, it may take a significant number of exchanges to gather enough data to meaningfully enhance this personalization. In the early stages of interaction, the chatbot may lack relevant insights, limiting its ability to provide tailored responses. Additionally, for new or unfamiliar tasks, previously stored information may not be useful, as exploratory tasks often require fresh context that past exchanges cannot fully provide. Inspiration is another critical component, particularly in open-ended tasks. Prior research has shown that LLMs can stimulate creativity, assisting users in generating new ideas or perspectives [12, 33]. Unlike studies focused on sparking creativity or employing Socratic questioning in educational contexts [7, 9], our work emphasizes the discovery of users\u2019 implicit needs and requirements. Rather than merely responding to explicit queries, we employ proactive inquiry techniques that encourage users to think expansively. By actively probing for underlying goals and unarticulated needs, our system helps users consider aspects they may not have initially thought of, ultimately leading to more comprehensive and personalized outcomes.\nTraditional chatbots and most LLM-based conversational systems rely on linear, dialogue-based interfaces, generating responses in a text stream without exploiting graphical user interfaces (GUIs). However, recent work has begun to explore the potential of graphical interfaces to better support users in complex tasks. For example, Sensecape integrates LLMs into an interactive system to facilitate information foraging and sensemaking [40], while Graphologue converts text responses into interactive diagrams to enhance information-seeking and question-answering tasks [19]. ExploreLLM employs a card-based schema to help users structure their thoughts and explore alternative solutions in a more organized manner [28]. A key challenge with linear text interfaces is the difficulty in locating and recalling relevant information, as important details can become buried within lengthy dialogue streams [14, 24, 34]. Our work contributes to this area by introducing a novel conversational interface, which separates the solution display from the dialogue interactions, while incorporating a dedicated section for managing user requirements. This structured design reduces cognitive load, enabling users to focus more easily on their tasks and better manage information flow during extended interactions.\nManuscript submitted to ACM\n# Motivating Insights from Empirical Studies on Chat-Based Interfaces\nOur proposed system is motivated by challenges users face when interacting with conventional LLM-based chatbots. A review of existing research on systems like ChatGPT [3, 5, 11, 13, 17, 24, 37], highlights key issues that persist in chatbased systems, especially regarding personalization [37], relevance [17], and sustained contextual understanding [3] in complex, open-ended scenarios. Several studies [8, 20, 24, 35] have consistently identified user dissatisfaction stemming from LLMs\u2019 frequent inability to capture the nuanced intent behind vague or incomplete queries. Users often encounter responses that are generic, irrelevant, or redundant, which diminishes trust in the system, particularly in tasks requiring deeper exploration or personalization. For example, research shows that users seeking localized information about a service may struggle when systems fail to interpret location-specific terms or understand personal preferences [50], leading to frustration and ineffective recommendations. This problem is further exacerbated in exploratory tasks, where users themselves may not have well-formed goals and rely on the system to guide them through ambiguity. Moreover, existing work [18, 24, 46, 49] has revealed that many users feel ill-equipped to manage the iterative, multi-turn interactions necessary to refine system outputs in such scenarios. The cognitive load imposed by these systems can be high, particularly when users are required to repeatedly clarify or reframe their inputs. Even with prompt engineering and optimization techniques designed to enhance LLM performance, users still frequently experience dissatisfaction when systems cannot offer personalized assistance based on minimal input or evolving needs. From these insights, several design goals emerge as essential for creating more collaborative chat-based interfaces: \u2022 Improved Contextual Understanding: System must be capable of maintaining and interpreting user intent over multiple turns, adapting to ambiguous or evolving user inputs to provide more relevant guidance. \u2022 Enhanced Personalization: System should be designed to tailor responses dynamically, accommodating user preferences and contextual needs, even when the initial input lacks specificity. \u2022 Facilitation of Inspired Exploration: In addition to providing personalized solutions, the system should inspire users to think implicit needs or aspects, promoting comprehensiveness in open-ended tasks. \u2022 Reduction of Cognitive Load: To mitigate the frustration associated with managing complex tasks, the interface should provide structure by organizing information effectively and helping users refine their queries. These design goals are informed by extensive literature on user interaction with LLMs, providing a clear direction for improving the CARE system\u2019s capabilities in supporting personalized, exploratory tasks.\n# 4 CARE: Collaborative Assistant for Personalized Exploration\nIn this section, we describe the proposed system, Collaborative Assistant for Personalized Exploration (CARE), which aims to achieve the design goals derived in the previous section. The overall system architecture of CARE is illustrated in Figure 2, and the subsequent sections elaborate on the user interface and back-end components.\n# 4.1 User Interface\nAs illustrated in Figure 2, the CARE\u2019s user interface consists of three main panels: the Chat Panel, Solution Panel, and Needs Panel. Next, we illustrate their functions using a travel planning scenario. The Chat Panel functions as the primary interface where users interact with CARE, entering initial queries such as \u201cPlan a 5-day trip to Hawaii\u201d. Through iterative feedback and conversation with the chatbot, users refine their requests, resulting in a dynamic multi-turn dialogue.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/53d4/53d49d0b-cbaf-4c72-8078-2787a8542190.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Overview of the CARE system. The gray area represents the User Interface, where users interact through the Chat, Solution, and Needs Panels. At the bottom, CARE\u2019s back-end consists of several agents, including the Inquiry Agent, Needs Discovery Agent, Solution Craft Agent, Milestone Agent, and Ranking Agent, which collaborate to process user inputs and generate personalized solutions. \u2192represents user interactions, such as chatting or updating needs. \u2192represents the internal data flow between agents. \u2192represents that the agents write data to the interface. \ufffdrepresents that the agents retrieve data from the interface.</div>\nAs the conversation progresses, CARE generates or updates a solution tailored to the user\u2019s needs, which is displayed in the Solution Panel. To enhance user experience, the solution is presented in Markdown format with rich text features (e.g., tables, emojis) for improved readability. In the travel planning case, the solution might include recommendations for accommodation, transportation, and other essential details, ensuring clarity and engagement. The Needs Panel complements this by explicitly displaying the collected user needs and identifying any points requiring clarification. This allows the system to \u201cground\u201d the solution shown in the Solution Panel by linking specific parts of the solution to particular user needs. For example, as shown in Figure 2, the green text of \u201cNeed ID: 001\u201d in the Solution Panel highlights how parts of the travel plan address needs listed in the Needs Panel, ensuring traceability and Manuscript submitted to ACM\ntransparency between user preferences and the generated plan. Additionally, users can modify, add, or delete needs directly in the Needs Panel for greater flexibility. In summary, the CARE interface introduces a novel design that separates the solution display in the Solution Panel from the conversational interactions in the Chat Panel, while incorporating a dedicated Needs Panel to manage and cross-reference user needs within the solution.\n# 4.2 LLM-Powered Multi-Agent Collaboration System\nThe back-end of the CARE system is powered by an LLM-driven multi-agent collaboration framework, as depicted in the bottom part of Figure 2. It comprises five specialized LLM-based agents: the Inquiry Agent, Milestone Agent, Needs Discovery Agent, Ranking Agent, and Solution Craft Agent. These agents collaborate with the user to progressively manage the solution development across three panels in the UI, discovering implicit needs and generating personalized solutions. While similar systems can be created by prompting a single LLM-based agent, we found that a multi-agent approach ensures more rigorous task management and better alignment with our design goals. Single-agent models can lose track of tasks or become overwhelmed by managing too many responsibilities. By assigning distinct roles to each agent, we maintain better control over individual components, ensuring consistency in task decomposition, milestone tracking, and user need discovery. Next, we introduce the overall interaction workflow of this multi-agent system, using the same example case of \u201cPlan a 5-day trip to Hawaii.\u201d 4.2.1 Interaction Workflow among Agents. The workflow, shown in Figure 2, is triggered by the query initiated by the user 1 in the Chat Panel. Subsequently, the Inquiry Agent receives the user\u2019s request 2 and passes it to the Milestone Agent 3 , who assesses if enough user needs are collected to proceed. If more details are needed, a new milestone is set 4 , such as \u201cunderstand user\u2019s hotel preferences.\u201d The Needs Discovery Agent then extracts needs from the user\u2019s input and generates follow-up questions to uncover additional needs that not mentioned by user 5 . The Ranking Agent organizes these questions, grouping and prioritizing them 6 before they are presented to the user. These questions are then passed back to the Inquiry Agent, who presents them to the user in an intuitive manner 7 . As the user responds, their answers are populated into the Needs Panel. This process continues iteratively until the Milestone Agent confirms that the collected needs are sufficient, prompting the Solution Craft Agent to generate a personalized solution 8 , which is then presented in the Solution Panel. Upon receiving the solution, the user can choose to refine the solution in two ways. They can directly communicate new preferences to the Inquiry Agent, which may trigger additional needs discovery steps, or if the preference is straightforward, the Milestone Agent will pass the new feedback directly to the Solution Craft Agent for immediate integration into a revised solution. Alternatively, the user can manually modify, add, or delete items in the Needs Panel. Any such updates will prompt the Solution Craft Agent to generate a new solutions based on the latest needs. The above workflow illustrates how the agents collaborate to deliver solutions tailored to users\u2019 preferences while facilitating a comprehensive needs discovery process. The following sections describe each agent in detail. 4.2.2 Inquiry Agent. The Inquiry Agent is designed to facilitate smooth user interactions by gathering and clarifying user needs in an intuitive manner. One of its key responsibilities is refining the questions generated by the Needs Discovery Agent before they are presented to the user. This process involves simplifying the language of questions and providing default options, which significantly reduces the user\u2019s cognitive load. For instance, instead of presenting a broad question like, \u201cWhat kind of accommodation do you prefer?\u201d, it offers more focused choices, such as \u201cHotel or Airbnb?\u201d, thereby streamlining the decision-making process for users. Manuscript submitted to ACM\nIn addition to refining questions, the Inquiry Agent actively assists users who may struggle with specific questions or express uncertainty. It provides pertinent explanations or definitions, such as explaining \u201cAirbnb\u201d, enabling users to make well-informed decisions. Moreover, once the user responds, the Inquiry Agent meticulously records the answers to the Needs Panel, ensuring that all user preferences are accurately captured for future stages of the process.\n4.2.3 Milestone Agent. The Milestone Agent plays the strategic coordinator role in CARE by determining the next critical steps (i.e., milestones) based on the existing user\u2019s input and collected needs, ensuring that all necessary user needs are gathered before moving forward with solution generation. The Milestone Agent has two primary responsibilities. First, if it determines that more specific user needs are required or the user requests an improvement on the existing solution, it sets the next milestone. This milestone typically involves collecting missing or incomplete information necessary for building a comprehensive solution. The Milestone Agent uses the current Needs Panel and input from the user to establish these milestones, ensuring that they are specific, actionable, and contribute directly to solving the user\u2019s problem. It references previously established milestones to avoid redundancy and ensures each new milestone is unique and focused on specific, measurable outcomes. Additionally, the agent carefully considers dependencies between tasks, breaking down complex problems into manageable milestones that the CARE system can address step by step. Second, if the recorded user needs are sufficient, the Milestone Agent notifies the Solution Craft Agent to begin generating a solution. At this stage, no new milestones are created, and the process transitions to solution generation. This decision is based on whether the current Needs Panel fully addresses the user\u2019s query. In cases where the user manually updates their needs via the user interface, the Milestone Agent immediately passes this information to the Solution Craft Agent, triggering a plan update without setting a new milestone. This ensures flexibility in how the CARE system adapts to user input and allows for rapid response to direct feedback.\n4.2.4 Needs Discovery Agent. The Needs Discovery Agent serves a vital function in identifying and documenting user needs, both explicit and implicit, throughout the interaction with the system. This process involves two critical tasks that ensure comprehensive coverage of user Needs. First, the agent is responsible for extracting explicit needs directly from the user\u2019s input. Explicit needs are those that the user clearly articulates. For instance, if a user expresses a desire to \u201cPlan a 5-day trip to Hawaii,\u201d the agent will identify and extract specific needs, such as \u201cthe destination is Hawaii\u201d and \u201cthe trip duration is 5 days\u201d. These explicit needs are systematically recorded in the Needs Panel to ensure they are fully documented, making it simple for users to review their own needs, while also allowing the CARE system to further develop personalized solutions. Second, the agent plays an essential role in identifying implicit needs, which are often unspoken but critical to achieving the current milestone. Implicit needs are inferred based on the chat history and the specific demands of the milestone. For example, when the task involves selecting a hotel, the agent may infer preferences such as whether the hotel needs to be close to popular attractions or if the user has specific needs for certain amenities, such as a gym, swimming pool, or free Wi-Fi. By anticipating these needs, the agent ensures a more complete understanding of the user\u2019s needs. Once the agent has completed its inference, it generates clarification questions to further refine the user\u2019s preferences. These questions may include, for example, \u201cwhether the user has specific requirements for the accommodation\u2019s location\u201d or \u201cany preferences regarding hotel amenities.\u201d The generated questions are then added to the Needs Panel, although invisible on the UI to avoid user distraction. Manuscript submitted to ACM\n4.2.5 Ranking Agent. The Ranking Agent brings structure and order to the system by organizing and prioritizing the clarification questions identified during the user needs discovery process. Instead of overwhelming users with a disjointed list of queries, the agent groups related questions thematically, such as accommodation preferences or budget constraints, allowing the user to focus on one topic at a time. This approach not only minimizes cognitive overload but also keeps the interaction flowing smoothly. Once the questions are grouped, the Ranking Agent further arranges them in a logical sequence. It starts with simpler, more general inquiries, then gradually progresses to more detailed, specific questions. By controlling the pacing and order of the interaction, the agent ensures that users can answer with confidence and clarity, without being rushed or overwhelmed. This results in a seamless experience where the user is led through the interaction with ease, while all necessary information is gathered efficiently.\n4.2.5 Ranking Agent. The Ranking Agent brings structure and order to the system by organizing and prioritizing the clarification questions identified during the user needs discovery process. Instead of overwhelming users with a disjointed list of queries, the agent groups related questions thematically, such as accommodation preferences or budget constraints, allowing the user to focus on one topic at a time. This approach not only minimizes cognitive overload but also keeps the interaction flowing smoothly. Once the questions are grouped, the Ranking Agent further arranges them in a logical sequence. It starts with simpler, more general inquiries, then gradually progresses to more detailed, specific questions. By controlling the pacing and order of the interaction, the agent ensures that users can answer with confidence and clarity, without being rushed or overwhelmed. This results in a seamless experience where the user is led through the interaction with ease, while all necessary information is gathered efficiently. 4.2.6 Solution Craft Agent. The Solution Craft Agent is central to transforming user needs into concrete, personalized solutions. This agent excels at turning complex sets of needs into clear, actionable outcomes. After analyzing the user\u2019s preferences and constraints, it organizes the solution in a way that is both transparent and user-friendly. Each solution is neatly structured, with headings and concise explanations, making it easy for users to follow and understand how their specific needs are being addressed. By linking every recommendation to a particular need, the agent ensures that users can see exactly how their input shapes the proposed solution. Personalization is another key strength of the Solution Craft Agent. Rather than offering generic solutions, it tailors each recommendation to align with the user\u2019s context, preferences, and limitations. Whether suggesting accommodations that fit within a specific budget or transportation options that align with the user\u2019s schedule, the agent ensures that the solution is relevant and practical. This emphasis on customization guarantees that each user receives a solution that is uniquely suited to their situation, making the final outcome both useful and highly personalized.\n4.2.6 Solution Craft Agent. The Solution Craft Agent is central to transforming user needs into concrete, personalized solutions. This agent excels at turning complex sets of needs into clear, actionable outcomes. After analyzing the user\u2019s preferences and constraints, it organizes the solution in a way that is both transparent and user-friendly. Each solution is neatly structured, with headings and concise explanations, making it easy for users to follow and understand how their specific needs are being addressed. By linking every recommendation to a particular need, the agent ensures that users can see exactly how their input shapes the proposed solution. Personalization is another key strength of the Solution Craft Agent. Rather than offering generic solutions, it tailors each recommendation to align with the user\u2019s context, preferences, and limitations. Whether suggesting accommodations that fit within a specific budget or transportation options that align with the user\u2019s schedule, the agent ensures that the solution is relevant and practical. This emphasis on customization guarantees that each user receives a solution that is uniquely suited to their situation, making the final outcome both useful and highly personalized.\n# 4.3 Implementation Details\nThe CARE system is a web-based application featuring a UI developed with Streamlit [39] and a back-end multi-agent system constructed using AutoGen [45]. Additionally, we implemented a conventional LLM-based chat interface by directly prompting the base LLM, which serves as the baseline system for our user study. Both CARE and the baseline systems utilize GPT-4o as the underlying LLM, with all corresponding prompts detailed in Supplementary Material.\n# 5 User Study\nTo minimize the effects of participant variability, we conducted a within-subject user study with 22 participants, comparing CARE to a baseline LLM-based chatbot as described above. Each participant completed two exploratory tasks, travel planning and skill learning, using both systems. To control for order effects, we counterbalanced the order of the systems and tasks, resulting in four settings. The study aimed to address the following research questions (RQs): \u2022 RQ1. How does the CARE system compare to the baseline system in supporting users in exploratory tasks in terms of interaction experience and user satisfaction? \u2022 RQ2. How does the CARE system provide personalized and inspired solutions compared to the baseline system? \u2022 RQ3. How does the CARE system\u2019s thoughtful interface design help reduce cognitive load during complex, open-ended tasks, compared to the baseline system?\nTo minimize the effects of participant variability, we conducted a within-subject user study with 22 participants comparing CARE to a baseline LLM-based chatbot as described above. Each participant completed two exploratory tasks, travel planning and skill learning, using both systems. To control for order effects, we counterbalanced the order of the systems and tasks, resulting in four settings. The study aimed to address the following research questions (RQs)\n\u2022 RQ1. How does the CARE system compare to the baseline system in supporting users in exploratory tasks in terms of interaction experience and user satisfaction? \u2022 RQ2. How does the CARE system provide personalized and inspired solutions compared to the baseline system? \u2022 RQ3. How does the CARE system\u2019s thoughtful interface design help reduce cognitive load during complex, open-ended tasks, compared to the baseline system?\n# 5.1 Procedure\nEach study session began with an introduction to both systems, during which participants received basic instructions on how to interact with the systems (e.g., explanation on UI layout, reminder on no web search function). Participants then completed a demographic questionnaire (Section A.2) detailing their background and prior experience with LLM-based chatbots. Each participant completed two exploratory tasks using both systems. We encouraged participants to spend 5-10 minutes on each task. After interacting with each system, participants filled out a post-task questionnaire (Table 1, questionnaire format in Section A.3) to evaluate their experience. Throughout the session, participants were encouraged to think aloud. Follow-up interviews (Section A.1) were conducted at the end of each session to gather additional qualitative insights, particularly focusing on any notable highlights or frustrations with each system. Each session lasted approximately one hour. The study was conducted in person, and participants were compensated with cash equivalent to two hours of the local minimum wage in appreciation of their time.\n# 5.2 Data Collection and Analysis\nTo evaluate how effectively CARE supports users in exploratory tasks, we designed a comprehensive questionnaire (Table 1) focusing on several key aspects of the user experience: Interaction, Cognitive Load, Inspiration, Comprehensiveness, and Personalization. During each study session, we recorded participants\u2019 screen activities and audio to capture their think-aloud verbalizations, allowing us to gather qualitative insights alongside quantitative data. For the quantitative analysis, we employed a combination of statistical methods, including paired t-tests and Pearson\u2019s Chi-Square tests, to assess the significance of our findings. The paired t-test allowed us to determine whether there was a statistically significant difference in user ratings between CARE and the baseline system. The Chi-Square test was used to examine categorical differences in participant responses across various dimensions of the user experience. To provide further context for the quantitative results, we analyzed the think-aloud data and post-task interviews using an inductive qualitative approach. This allowed us to probe into the reasons behind the participants\u2019 ratings and explore their subjective experiences with the two systems. The qualitative feedback was instrumental in corroborating the experimental findings and providing deeper insight into how CARE facilitated user exploration, reduced cognitive load, and supported creativity.\nMeasure\nStatement (5-Point Likert Scale)\nInteraction\nQ1. I enjoy the way I interact with the system.\nCognitive Load\nQ2. The system\u2019s UI helps me organize complex tasks and reduces my cognitive load.\nInspiration\nQ3. Interacting with the system inspires me to consider new aspects of exploratory tasks.\nComprehensiveness\nQ4. The answers provided by the system feel comprehensive and sufficient to me.\nPersonalization\nQ5. The answers provided by the system match my personal needs.\nTable 1. Post-task questionnaire filled out by participants after they interacted with two systems, one with CARE and the other with\nTable 1. Post-task questionnaire filled out by participants after they interacted with two systems, one with CARE and the other w the Baseline. Each statement was rated on a 5-point Likert scale (the larger the better).\n# 5.3 Participants\nWe recruited 22 participants (4 female, 18 male) through a call for participation distributed via the university\u2019s mailing list, as well as through word-of-mouth referrals. Participants represented a diverse range of academic disciplines, Manuscript submitted to ACM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dbe5/dbe5ef59-d5f2-4d59-85d7-8e5ae1d6ec11.png\" style=\"width: 50%;\"></div>\nincluding Computer Science (9), Engineering (8), Natural Sciences (2), Social Sciences (2), and Arts (1). Their educational backgrounds varied, with 4 undergraduates, 17 master\u2019s students, and 1 PhD student. All participants had prior experience with conversational LLM-based systems, such as ChatGPT, but their frequency of use varied. Seven participants reported using such systems less than once per week, five used them several times per week but not daily, and ten were frequent users who interacted with LLM-based chatbots multiple times per day.\n# 6 Quantitative Results\nThe binary choice results, collected through post-task interviews, reveal a clear preference for CARE over the baseline system, with 16 out of 22 participants favoring CARE. This finding aligns with the quantitative data from the five survey questions listed in Table 1, as depicted in Figure 3. When averaging scores across all five questions, CARE was rated significantly higher (\ud835\udc61= 3.07, \ud835\udc5d= 0.0058), with an average score of 4.20 (\ud835\udf0e= 0.78) compared to the baseline\u2019s 3.25 (\ud835\udf0e= 0.91). Cohen\u2019s \ud835\udc51= 1.12 also illustrates the considerable difference in perceived effectiveness, highlighting the participants\u2019 clear preference for CARE. We now turn to the detailed statistical analysis of each survey question. Overall Interaction Experience. The Pearson Chi-Square analysis (\ud835\udf122(4) = 5.17, \ud835\udc5d= 0.27) indicates no significant difference in overall satisfaction between the two systems in Q1. However, a larger proportion of participants rated CARE favorably in the \u201cAgree\u201d and \u201cStrongly Agree\u201d categories, with 17 out of 22 participants finding CARE engaging, compared to only 10 for the baseline. This pattern, though not statistically significant, suggests that CARE might provide a better interaction experience for the majority of users. Reducing Cognitive Load. According to the results in Q2, users perceived that CARE significantly reduced cognitive load in organizing and managing complex tasks compared to the baseline (\ud835\udf122(4) = 19.04, \ud835\udc5d= 0.001). Notably, 13 participants strongly agreed that CARE was more systematic in presenting and organizing information, compared to only one participant in the baseline system.\nManuscript submitted to ACM\nInspiring New Ideas. The Chi-Square result for Q3 (\ud835\udf122(4) = 14.21, \ud835\udc5d= 0.007) demonstrates a statistically significant difference between the two systems. Notably, 14 participants strongly agreed that CARE inspired them to generate new ideas or explore alternative perspectives during the interaction, compared to only 5 participants in the baseline system. This demonstrates that CARE encourages a more exploratory and creative interaction compared to the baseline. Solution Comprehensiveness. The Chi-Square analysis of Q4 (\ud835\udf122(4, \ud835\udc41= 22) = 5.72, \ud835\udc5d= 0.221) reveals no significant difference between the two systems in terms of solution comprehensiveness. This outcome is encouraging, as it implies that CARE offers tailored solutions without compromising the comprehensiveness of the responses. Solution Personalization. CARE showed a notable advantage (\ud835\udf122(4) = 8.27, \ud835\udc5d= 0.082) in terms of solution personalization (Q5), with 19 participants agreeing that CARE\u2019s plans were more personalized, in contrast to 11 for the baseline system. While the \ud835\udc5dvalue did not meet the conventional threshold of 0.05, it suggests a trend toward significance, indicating potential for future enhancements to the CARE system. The above quantitative analysis highlights a clear preference for CARE over the baseline system, with most participants favoring CARE in terms of effectiveness and user experience. While not all survey questions showed statistically significant differences, CARE consistently outperformed the baseline in reducing cognitive load, inspiring new ideas, and providing personalized solutions. These findings set the stage for a deeper exploration of qualitative insights from the post-task interviews, where participants\u2019 nuanced perspectives on their interaction with CARE will be discussed.\n# 7 Findings\nThe findings from our user study are organized around addressing the three research questions listed in Section 5, providing insights into the strengths of the CARE system compared to the baseline. First, we explore how CARE\u2019s inquiry process supports exploratory tasks by fostering deeper exploration, ensuring comprehensiveness, and delivering personalized outputs, aligning with RQ1 and RQ2. Next, we examine how CARE\u2019s collaborative workspace effectively organizes user needs, generated solutions, and chat interactions, contributing to reduced cognitive load and improved task performance, directly addressing RQ3. These findings collectively highlight the key benefits of CARE\u2019s design in enhancing user experience and productivity.\n# 7.1 CARE\u2019s Inquiry Process Inspires Exploration, Ensures Comprehensiveness, and Personalizes Output\n7.1.1 CARE\u2019s Inquiry Process Inspires Users by Uncovering Implicit Needs. Through our investigation, we found that CARE effectively uncovers users\u2019 implicit needs by encouraging them to think broadly and respond to proactive inquiries. During the interviews, we further identified that users\u2019 implicit needs can be divided into two distinct categories: hidden needs and latent needs. Hidden needs refer to those needs that users are aware of but do not explicitly express, while latent needs are those that users do not recognize until they are prompted by CARE\u2019s exploratory questions. CARE effectively inspires users to consider both hidden and latent needs. Specifically, most participants (17/22) indicated that CARE\u2019s questioning strategy encouraged them to think about aspects they would not have otherwise considered. For instance, P7 highlighted how CARE helped him uncover hidden needs while planning a trip: \u201cFor example, when planning a trip, it (CARE) would ask me if I prioritize sightseeing, food, or shopping, and if I have a preference, it can better tailor the itinerary for me.\u201d This kind of questioning reveals needs that the user might be aware of but did not communicate explicitly. On the other hand, P19 emphasized how CARE brought to light latent needs during the process of creating a baseball training plan: \u201cI think System A (CARE) inspired me a lot. For example, when I wanted to start playing baseball, it asked about the coach\u2019s style and how much time I wanted to train\u2014especially questions about the Manuscript submitted to ACM\ncoach\u2019s style, which I had never thought about before.\u201d These examples demonstrate how CARE\u2019s inquiry process guides users to explore crucial factors they initially overlooked but which are critical to making more informed decisions. In contrast, participants found that the baseline system reacted passively, only responding to explicit inputs without offering additional considerations. P11 pointed out: \u201cB (Baseline) only answered what I asked, but A (CARE) asked about things like my budget and dietary preferences, which I hadn\u2019t thought about.\u201d This passive approach often led to incomplete articulation of the user\u2019s needs, resulting in less comprehensive and relevant output. 7.1.2 CARE\u2019s Milestone-Driven Approach Improves User Focus And Personalizes Task Progression. To ensure that CARE continuously focuses on the user\u2019s current needs, tasks are organized into sequential milestones by Milestone Agent, guiding other agents in the system step by step through the planning process. Each milestone represents a distinct phase of progress, enabling agents to focus on one task at a time while maintaining a coherent long-term strategy. As users provide feedback or refine their goals based on the current milestone, the system dynamically updates the plan or generates the next milestone, ensuring continuous alignment with the user\u2019s evolving needs. This milestone-driven approach was noticed and appreciated by participants (10/22). P20 described this as, \u201cIt (CARE) asked questions that helped me break down a big problem into smaller, more systematic tasks. This way, I could understand my needs better, and the system could offer more personalized settings\u201d. Similarly, P8 mentioned, \u201cThe system A (CARE) would update after each step, and I could see how my decisions affected the plan, which helped me stay on track.\u201d The baseline system, however, lacked this granular task breakdown. Without the ability to iteratively refine and adjust based on continuous feedback, users found it difficult to maintain focus on evolving goals. P15 remarked, \u201cWith the another system (Baseline), I often felt lost trying to manage everything at once.\u201d 7.1.3 Comprehensive Inquiry Process Ensures More Detailed, Structured, and Complete Solution. CARE ensures that users\u2019 needs are comprehensively covered through a systematic inquiry process during interactions. This comprehensiveness is reflected in two dimensions: the comprehensiveness of the inquiry process and the comprehensiveness of the generated solution. The former refers to the system\u2019s ability to cover multiple aspects of the user\u2019s needs through its multidimensional questions, while the latter pertains to the final solution being more structured and complete. Some participants (13/22) expressed a preference for this proactive, multi-dimensional inquiry-driven interaction style. For example, P12 noted: \u201cA (CARE) asks follow-up questions based on your answers and helps fill in things you might not have thought about. I think it\u2019s very comprehensive.\u201d Similarly, P4 highlighted CARE\u2019s ability to expand a single question into multiple dimensions, noting: \u201cA (CARE) is suitable for general users because many times you only have one question in your mind, but it can help you break it down into various aspects that need to be considered.\u201d Moreover, CARE incorporates a Ranking Agent, which prioritizes questions to allow users to first answer simpler, foundational questions, thereby reducing cognitive load during interactions. This design ensures that questions are presented in a structured sequence, avoiding users being overwhelmed by excessive information at once. As P8 stated: \u201cIt\u2019s (CARE\u2019s) interactive, and it asks simple questions, like multiple-choice or short questions. I felt relaxed while interacting with it, and it was really good at recognizing my intent and adding that to the list of needs.\u201d This level of comprehensiveness during interaction was lacking in the baseline system. Users had to think through the task and provide as much information as possible upfront by themselves, and the system did not proactively ask about additional dimensions of the task. Some participants (8/22) expressed frustration with this limitation. P21 explained: \u201cWith chatbot B (Baseline), I had to ask more follow-up questions myself to complete the plan. But sometimes, I might forget to ask something important or miss a key detail.\u201d\ncoach\u2019s style, which I had never thought about before.\u201d These examples demonstrate how CARE\u2019s inquiry process guides users to explore crucial factors they initially overlooked but which are critical to making more informed decisions. In contrast, participants found that the baseline system reacted passively, only responding to explicit inputs without offering additional considerations. P11 pointed out: \u201cB (Baseline) only answered what I asked, but A (CARE) asked about things like my budget and dietary preferences, which I hadn\u2019t thought about.\u201d This passive approach often led to incomplete articulation of the user\u2019s needs, resulting in less comprehensive and relevant output. 7.1.2 CARE\u2019s Milestone-Driven Approach Improves User Focus And Personalizes Task Progression. To ensure that CARE continuously focuses on the user\u2019s current needs, tasks are organized into sequential milestones by Milestone Agent, guiding other agents in the system step by step through the planning process. Each milestone represents a distinct phase of progress, enabling agents to focus on one task at a time while maintaining a coherent long-term strategy. As users provide feedback or refine their goals based on the current milestone, the system dynamically updates the plan or generates the next milestone, ensuring continuous alignment with the user\u2019s evolving needs. This milestone-driven approach was noticed and appreciated by participants (10/22). P20 described this as, \u201cIt (CARE) asked questions that helped me break down a big problem into smaller, more systematic tasks. This way, I could understand my needs better, and the system could offer more personalized settings\u201d. Similarly, P8 mentioned, \u201cThe system A (CARE) would update after each step, and I could see how my decisions affected the plan, which helped me stay on track.\u201d The baseline system, however, lacked this granular task breakdown. Without the ability to iteratively refine and adjust based on continuous feedback, users found it difficult to maintain focus on evolving goals. P15 remarked, \u201cWith the another system (Baseline), I often felt lost trying to manage everything at once.\u201d\n7.1.3 Comprehensive Inquiry Process Ensures More Detailed, Structured, and Complete Solution. CARE ensures that users\u2019 needs are comprehensively covered through a systematic inquiry process during interactions. This comprehensiveness is reflected in two dimensions: the comprehensiveness of the inquiry process and the comprehensiveness of the generated solution. The former refers to the system\u2019s ability to cover multiple aspects of the user\u2019s needs through its multidimensional questions, while the latter pertains to the final solution being more structured and complete. Some participants (13/22) expressed a preference for this proactive, multi-dimensional inquiry-driven interaction style. For example, P12 noted: \u201cA (CARE) asks follow-up questions based on your answers and helps fill in things you might not have thought about. I think it\u2019s very comprehensive.\u201d Similarly, P4 highlighted CARE\u2019s ability to expand a single question into multiple dimensions, noting: \u201cA (CARE) is suitable for general users because many times you only have one question in your mind, but it can help you break it down into various aspects that need to be considered.\u201d Moreover, CARE incorporates a Ranking Agent, which prioritizes questions to allow users to first answer simpler, foundational questions, thereby reducing cognitive load during interactions. This design ensures that questions are presented in a structured sequence, avoiding users being overwhelmed by excessive information at once. As P8 stated: \u201cIt\u2019s (CARE\u2019s) interactive, and it asks simple questions, like multiple-choice or short questions. I felt relaxed while interacting with it, and it was really good at recognizing my intent and adding that to the list of needs.\u201d This level of comprehensiveness during interaction was lacking in the baseline system. Users had to think through the task and provide as much information as possible upfront by themselves, and the system did not proactively ask about additional dimensions of the task. Some participants (8/22) expressed frustration with this limitation. P21 explained: \u201cWith chatbot B (Baseline), I had to ask more follow-up questions myself to complete the plan. But sometimes, I might forget to ask something important or miss a key detail.\u201d\nManuscript submitted to ACM\nAs a result of the comprehensiveness of the inquiry process, most participants (16/22) found that solutions generated by CARE were more comprehensive, although not prominent in the quantitative analysis. A significant advantage of CARE, as noted by participants, was its ability to guide users through various dimensions of their task, ensuring that no critical aspects were overlooked. For instance, P16 shared: \u201cA (CARE) really helped me think through every aspect of my travel plans. It wasn\u2019t just a basic itinerary; it considered things like how much free time I would have, what activities were nearby, and even suggestions for restaurants based on my budget.\u201d Moreover, the structured organization of CARE\u2019s generated solutions helped users follow and trust the planning process more easily, further enhancing their perception of the solution\u2019s thoroughness. P19 noted that CARE\u2019s step-by-step breakdown of tasks made the final solution more coherent: \u201cA\u2019s (CARE\u2019s) plan was laid out in clear steps, and it felt more complete because I could see how each decision built on the previous one. It wasn\u2019t just a list of things to do; it was a logical progression that covered everything.\u201d In contrast, participants (7/22) noted that the baseline system often delivered less detailed plans, particularly due to its reactive nature. P12 described an instance where the baseline plan failed to include specific logistical considerations: \u201cWith B (Baseline), it felt like I was getting a quick summary rather than a full plan. It didn\u2019t take into account things like transportation between activities, so I had to figure that out on my own.\u201d 7.1.4 Personalization Through Tailored Inquiry Results in More User-Specific Solutions. CARE generates highly personalized plans by continuously adjusting its output based on users\u2019 specific needs through dynamic questioning. Specifically, CARE\u2019s inquiries focus on clarifying vague requests by asking about personalized preferences or needs, such as preferred activity types or priorities. This not only helps users articulate their preferences more clearly but also ensures that the system tailors its output accordingly. The flexibility and specificity of the generated solution made users feel that the system was responsive to their input, enhancing their satisfaction. P10 highlighted this by explaining, \u201cA (CARE) kept asking questions that I hadn\u2019t even thought about, like whether I wanted to prioritize certain skills or what time of day I preferred for training. This really helped make the final plan feel like it was made just for me.\u201d The baseline system often fell short in this area as it relied on users to provide all necessary input upfront without prompting further clarification. As a result, the solutions generated by the baseline system were often less personalized. P9 commented, \u201cB (Baseline) didn\u2019t ask any follow-up questions after I gave my preferences. It just generated a plan that was okay, but it didn\u2019t really reflect everything I wanted.\u201d This feedback underscores how baseline\u2019s limited engagement led to less tailored solutions, resulting in solutions felt generic. Even though occasionally the baseline system encouraged users to follow up with additional inputs, long-form answers could still lead to missing crucial personalization elements. For instance, P13 noted: \u201cFor my fitness training task, the plan from B (Baseline) didn\u2019t match what I wanted at first. I had to follow up and ask for revisions, but even after updating, it still missed some details that A (CARE) had caught because A asked very specific questions from the start.\u201d This illustrates that even when users try to supplement their preferences with follow-up inquiries in the baseline system, the lack of thorough questioning from the outset could result in oversights. CARE\u2019s adaptability during the interaction process further enhanced its ability to personalize plans. P8 appreciated how the system dynamically adjusted based on real-time inputs: \u201cWith A (CARE), I could tweak my needs as we went along, and it would adjust the plan. That made me feel like I had control over the outcome and that the plan was really made for me.\u201d This iterative refinement process ensured that the final plan was more accurately tailored to the user\u2019s evolving preferences, providing a clear advantage over the more static baseline system. 7.1.5 CARE\u2019s Inquiry Process May Occasionally Cause Friction for Some Users. While some participants (9/22) explicitly expressed that the CARE system\u2019s structure made it better suited for guiding users through complex tasks, with P20 Manuscript submitted to ACM\n7.1.4 Personalization Through Tailored Inquiry Results in More User-Specific Solutions. CARE generates highly personalized plans by continuously adjusting its output based on users\u2019 specific needs through dynamic questioning Specifically, CARE\u2019s inquiries focus on clarifying vague requests by asking about personalized preferences or needs, such as preferred activity types or priorities. This not only helps users articulate their preferences more clearly but also ensures that the system tailors its output accordingly. The flexibility and specificity of the generated solution made users feel that the system was responsive to their input, enhancing their satisfaction. P10 highlighted this by explaining, \u201cA (CARE) kept asking questions that I hadn\u2019t even thought about, like whether I wanted to prioritize certain skills or what time of day I preferred for training. This really helped make the final plan feel like it was made just for me.\u201d The baseline system often fell short in this area as it relied on users to provide all necessary input upfront without prompting further clarification. As a result, the solutions generated by the baseline system were often less personalized P9 commented, \u201cB (Baseline) didn\u2019t ask any follow-up questions after I gave my preferences. It just generated a plan that was okay, but it didn\u2019t really reflect everything I wanted.\u201d This feedback underscores how baseline\u2019s limited engagement led to less tailored solutions, resulting in solutions felt generic. Even though occasionally the baseline system encouraged users to follow up with additional inputs, long-form answers could still lead to missing crucial personalization elements For instance, P13 noted: \u201cFor my fitness training task, the plan from B (Baseline) didn\u2019t match what I wanted at first. I had to follow up and ask for revisions, but even after updating, it still missed some details that A (CARE) had caught because A asked very specific questions from the start.\u201d This illustrates that even when users try to supplement their preferences with follow-up inquiries in the baseline system, the lack of thorough questioning from the outset could result in oversights. CARE\u2019s adaptability during the interaction process further enhanced its ability to personalize plans. P8 appreciated how the system dynamically adjusted based on real-time inputs: \u201cWith A (CARE), I could tweak my needs as we went along, and it would adjust the plan. That made me feel like I had control over the outcome and that the plan was really made for me.\u201d This iterative refinement process ensured that the final plan was more accurately tailored to the user\u2019s evolving preferences, providing a clear advantage over the more static baseline system. 7.1.5 CARE\u2019s Inquiry Process May Occasionally Cause Friction for Some Users. While some participants (9/22) explicitly expressed that the CARE system\u2019s structure made it better suited for guiding users through complex tasks, with P20 Manuscript submitted to ACM\nnoting: \u201cA (CARE) helps break down my larger tasks into smaller, more manageable parts, making it easier to navigate and ensure all details are covered\u201d, some participants (4/22) commented that the number of questions felt excessive at times. For instance, P13 noted: \u201cA (CARE) sometimes provided new information, and while some of it was helpful, I also felt that when it kept asking about things I didn\u2019t really care about, I wasn\u2019t sure how to answer.\u201d This feedback suggests that users who prefer to resolve issues quickly may find the step-by-step inquiry process to be a source of mild friction.\n# 7.2 CARE\u2019s Collaborative Workspace Effectively Organizes User Needs, Generated Solutions, and Cha\nInteractions, thereby Reducing Cognitive Load\nOne of the most frequently mentioned advantages of the CARE system during interviews was its well-organized UI. Many participants (14/22) noted that compared to the baseline, CARE\u2019s UI significantly improved their overall experience by reducing cognitive load and making interactions more intuitive. Specifically, users appreciated the structured layout, which clearly separated their needs from generated solutions and system interactions. This organization helped them stay focused on their tasks and reduced the effort needed to manage the flow of information during extended interactions. In the following sections, we will delve deeper into how specific features of CARE\u2019s UI, such as the Needs Panel and Solution Panel, contributed to these advantages.\n7.2.1 Panel Separation Improved Clarity and Reduced Cognitive Load. The division of the Chat Panel, Solution Panel, and Needs Panel within CARE significantly enhances clarity and minimizes users\u2019 cognitive load. Isolating the generated solution from the chat eliminates the need to sift through previous messages, while the distinct presentation of needs keeps them readily visible and adjustable. This design allows users to keep track of their evolving objectives without being overwhelmed by unrelated content. Several participants appreciated how the clear separation of chat and solution output streamlined their interaction with the system. For example, P14 remarked: \u201cIt (CARE) asks questions on the right side and outputs what I need on the left, so I don\u2019t need to search through the conversation to find the result.\u201d This clear division between the chat interactions and the solution outputs allowed users to locate relevant information effortlessly, minimizing the cognitive effort associated with scrolling back through chat histories. The Needs Panel, in particular, proved to be a critical feature in reducing mental load. It enabled users to track and update their requirements in real time without revisiting previous conversations. P8 noted, \u201cA\u2019s (CARE\u2019s) needs list is clearly displayed, making it easy to see and update my requirements.\u201d This visualization of needs provided users with a constant, structured overview, reducing the effort required to recall or modify their requests. Similarly, P21 highlighted how the separation of the Needs Panel from the chat log made it easier to manage goals: \u201cHaving the needs separate from the chat makes it easier to verify what I\u2019ve said and what I want.\u201d This dedicated space for evolving goals ensured that critical information was not overlooked, providing users with confidence that their inputs were being considered. Moreover, this clear organization reduced the cognitive burden associated with planning tasks. P19 shared: \u201cA\u2019s (CARE\u2019s) table makes everything much clearer, so I don\u2019t have to keep everything in my head or scroll back to find important details.\u201d By visually organizing task details in the Needs Panel, CARE allows users to focus on decision-making rather than on recalling details, making the interaction less mentally demanding. In contrast, the unstructured chat format of the baseline system led to frequent cognitive strain for some participants (6/22). The unstructured chat format made it difficult to locate and recall information, requiring constant scrolling. As P16 described: \u201cWith B (Baseline), I have to scroll up and down constantly to find the information.\u201d This lack of clear\norganization increased users\u2019 mental load, making the process more tedious and hindering their ability to maintain an overview of their plans.\n7.2.2 User Needs Panel Simplified Interaction and Built Trust Through Transparent Solution Mapping. The CARE system\u2019s Needs Panel emerged as a key feature that simplified interaction and enhanced user trust by providing a clear, real-time display of user requirements and linking those needs transparently to the generated solutions. The Needs Panel was recognized by several participants (10/22) for providing a real-time display of user requirements, enabling them to manage preferences efficiently throughout the interaction. For instance, P14 explained: \u201cThere is a Needs Panel where I can directly input my needs, and I don\u2019t have to worry about the AI missing any of them.\u201d This dynamic interaction was further supported by the real-time updating of needs, which allowed users to adapt their goals on the fly, improving the personalization of the outcomes. P17 highlighted this benefit: \u201cA\u2019s (CARE\u2019s) ability to update my needs in real-time led to more accurate and personalized plans. In contrast, B (Baseline) would often forget or shift focus after multiple conversations, making it harder to track everything.\u201d Moreover, CARE\u2019s user needs reference mechanism further reinforced user trust by linking specific parts of the generated solutions back to the corresponding user needs, as shown by the linkage of the Needs Panel and Solution Panel in Figure 2. Several participants (7/22) appreciated how this transparency reassured them that their input was being accurately incorporated. P21 commented, \u201cThe need references in A (CARE) are very useful because they show me which parts of the plan were based on my input, so I know exactly how my needs were considered.\u201d This clear mapping made it easier for users to understand and adjust the solutions, fostering a more engaged and confident interaction. As P14 noted, \u201cWith A (CARE), I could always see which parts of the generated solution were directly related to what I had asked for, which made it much easier to make adjustments.\u201d Participants consistently highlighted that this transparent connection between user needs and solution mapping increased their trust and sense of ownership. P8 observed, \u201cSeeing how my requirements were implemented step-by-step in A (CARE) gave me a lot of confidence that the system was understanding and working with my needs accurately.\u201d In contrast, the baseline system, lacking a similar mechanism, often left users uncertain about how their input was utilized, leading to frustration. P22 expressed: \u201cIn B (Baseline), I wasn\u2019t always sure if the system really understood my needs, since there was no way to see how my input was reflected in the final output.\u201d Overall, the combination of CARE\u2019s Needs Panel and user needs reference features simplified the user experience and built trust by providing a transparent and tangible link between user input and system output. This transparency not only improved user confidence but also empowered them to make more informed and precise adjustments, resulting in a more reliable and user-centric planning process. 7.2.3 Enhanced Readability of the Solution Panel through Structured and Rich-Text Formatting. The structured presentation in CARE\u2019s Solution Panel, using Markdown elements such as tables, rich text, and icons, was highlighted by participants as a key factor in enhancing readability. Several users (6/22) appreciated the structured format, particularly in contrast to the unstructured chat format of the Baseline system. P2 noted, \u201cA\u2019s (CARE\u2019s) tables, icons, and emojis made everything much more organized and readable. If it were only text, I might not continue the conversation after a few exchanges.\u201d This organized layout helped users understand and follow the system\u2019s generated plans with less cognitive effort. P16 similarly highlighted the benefits of the table-based layout: \u201cA (CARE) puts everything into tables, which makes my schedule much clearer. With B (Baseline), I constantly have to scroll to find the information.\u201d This layout made navigating complex information more straightforward, reducing the need for excessive scrolling and minimizing cognitive overload.\nparticipants as a key factor in enhancing readability. Several users (6/22) appreciated the structured format, particularly in contrast to the unstructured chat format of the Baseline system. P2 noted, \u201cA\u2019s (CARE\u2019s) tables, icons, and emojis made everything much more organized and readable. If it were only text, I might not continue the conversation after a few exchanges.\u201d This organized layout helped users understand and follow the system\u2019s generated plans with less cognitive effort. P16 similarly highlighted the benefits of the table-based layout: \u201cA (CARE) puts everything into tables, which makes my schedule much clearer. With B (Baseline), I constantly have to scroll to find the information.\u201d This layout made navigating complex information more straightforward, reducing the need for excessive scrolling and minimizing cognitive overload. Manuscript submitted to ACM\nIn summary, the CARE system\u2019s organized UI reduces cognitive load by clearly separating user needs, solutions, and chats. Features like the Needs Panel and Solution Panel provide real-time clarity, helping users manage tasks efficiently and with less effort. This structured design enhances ease of use, trust, and reduces the strain of managing information compared to the baseline system.\n# 8 Discussion\n# 8.1 Broader Implications for Collaborative Chat-Based Interfaces\nThe findings from our study provide several important insights into designing more effective, collaborative chat-based systems, particularly for exploratory tasks. A key takeaway is the need to shift from reactive, dialogue-based interactions to a more proactive and structured approach, as demonstrated by the CARE system. CARE\u2019s ability to reduce cognitive load and support more meaningful engagement with users stems from its structured interface and adaptive collaboration, setting it apart from traditional, free-flowing conversational agents. One of the broader design principles emerging from our study is the separation of conversational inputs from task management. Many LLM-based systems rely heavily on an undifferentiated, continuous dialogue format, which can overwhelm users and make it difficult to organize and track complex tasks. In contrast, CARE\u2019s dual-panel approach\u2014distinguishing between conversational inputs (Chat Panel) and structured outputs (Solution Panel)\u2014proves highly effective in managing information. By separating these components, users can focus on task progression without being bogged down by unnecessary dialogue or clutter. A dedicated Needs Panel, designed to ensure traceability and transparency between user inputs and system outputs, enhances user confidence and trust in the system. This suggests that future chat-based systems could benefit from integrating multimodal interaction spaces that visually differentiate between ongoing conversation, system outputs, and task organization. Such interfaces could vastly improve user experience by mitigating information overload and making it easier to manage evolving, multi-step processes. Furthermore, CARE\u2019s design prioritizes adaptive collaboration. Unlike conventional systems that passively respond to user inputs, CARE actively inquires about user needs, offers suggestions, and encourages exploration. This interaction model fosters creativity and supports personalization, particularly in open-ended tasks where users might not have well-defined goals. By dynamically adjusting to users\u2019 evolving needs and preferences, CARE transforms the interaction from one of mere conversational efficiency to an ongoing, exploratory dialogue. This agent-driven approach could be particularly valuable in domains such as complex decision-making, creative processes, or long-term project management. For the broader HCI community, the CARE system exemplifies how conversational agents can evolve from static responders into active collaborators, promoting more exploratory and creative problem-solving. Additionally, our findings reveal that the CARE system helps uncover implicit user needs by inspiring users to think beyond their initial queries. Rather than just providing direct answers, CARE prompts users to consider alternative perspectives and explore unanticipated paths, creating a more enriching interaction. This design feature could inform future work on intelligent agents, which need to balance guidance and autonomy. Systems that empower users to extend their thinking without overwhelming them represent a significant opportunity for future conversational interfaces, particularly in areas like education, content creation, and collaborative work environments. The broader implication for HCI is a shift towards systems that don\u2019t just provide information but actively support users in navigating uncertainty, promoting discovery, and enhancing creativity.\nManuscript submitted to ACM\nManuscript submitted to ACM\n# 8.2 Limitations and Future Work\nWhile our study demonstrates CARE\u2019s potential in improving exploratory tasks, it has limitations that future research should address. CARE\u2019s response latency, inherent to its multi-agent design, poses a challenge. While the system excelled in reducing cognitive load and enhancing personalization, the added response time could affect user satisfaction. Advancements in LLM technology will help reduce these delays. Another limitation is the relatively small and homogeneous participant pool, which may affect the generalizability of our findings. Future studies should recruit more diverse participants to capture a broader range of user needs, preferences, and interaction patterns. Lastly, our study used GPT-4o, one of the most advanced LLMs at the time. Future research should investigate whether the findings generalize across other LLM implementations, especially as new, more efficient models emerge. Exploring alternative interaction modalities like voice or gesture could also further enhance the system\u2019s capabilities.\n# 9 Conclusion\nIn this paper, we introduced CARE, a collaborative chat-based interface designed to enhance personalized exploratory tasks by addressing key limitations in existing LLM-based systems. Motivated by challenges such as the lack of sustained contextual understanding and personalization in current LLM-based chatbots, we developed a system that reimagines user-LLM interaction through structured and adaptive collaboration. Our design goals focused on improving contextual understanding, facilitating personalization, inspiring user exploration, and reducing cognitive load. Through a user study comparing CARE with a baseline LLM chatbot with 22 participants, the results demonstrated that CARE outperformed the baseline system in several key areas, notably in inspiring user exploration, personalizing responses, and organizing information effectively. These findings have broader implications for the design of future chat-based interfaces. By empowering users to navigate ambiguity and supporting them in complex tasks, systems like CARE have the potential to transform how users engage with conversational agents, making them not just reactive tools, but proactive partners in problem-solving and discovery.\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, and Sujay Kumar Jauhar. 2024. Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW \u201924). Association for Computing Machinery, New York, NY, USA, 3355\u20133366. https://doi.org/10.1145/3589334.3645404 [3] Morteza Behrooz, William Ngan, Joshua Lane, Giuliano Morse, Benjamin Babcock, Kurt Shuster, Mojtaba Komeili, Moya Chen, Melanie Kambadur, Y-Lan Boureau, and Jason Weston. 2023. The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges. arXiv preprint arXiv:2306.04765 (2023). [4] Bing. 2024. Bing. https://www.bing.com/. Accessed: 2024-10-10. [5] Ali Borji. 2023. A Categorical Archive of ChatGPT Failures. arXiv preprint arXiv:2302.03494 (2023). [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS \u201920). Curran Associates Inc., Red Hook, NY, USA, Article 159, 25 pages. [7] Edward Y. Chang. 2023. Prompting Large Language Models With the Socratic Method. In 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC). 0351\u20130360. https://doi.org/10.1109/CCWC57344.2023.10099179 [8] Yang Deng, Wenqiang Lei, Wai Lam, and Tat-Seng Chua. 2023. A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, Edith Elkind (Ed.). International Joint Conferences on Artificial Intelligence Organization, 6583\u20136591. https://doi.org/10.24963/ijcai.2023/738 Survey Track. Manuscript submitted to ACM\n[9] Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, and Liang He. 2024. Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching. arXiv preprint arXiv:2407.17349 (2024). [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [11] Luciano Floridi. 2023. AI as Agency Without Intelligence: on ChatGPT, Large Language Models, and Other Generative Models. Philos. Technol. 36 (2023), 15. https://doi.org/10.1007/s13347-023-00621-y [12] Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks: Inspiration for Science Writing using Language Models. In Proceedings of the 2022 ACM Designing Interactive Systems Conference (Virtual Event, Australia) (DIS \u201922). Association for Computing Machinery, New York, NY, USA, 1002\u20131019. https://doi.org/10.1145/3532106.3533533 [13] Sukhpal Singh Gill and Rupinder Kaur. 2023. ChatGPT: Vision and challenges. Internet of Things and Cyber-Physical Systems 3 (2023), 262\u2013271. https://doi.org/10.1016/j.iotcps.2023.05.004 [14] Charles Goodwin. 2015. Professional Vision. Springer Fachmedien Wiesbaden, Wiesbaden, 387\u2013425. https://doi.org/10.1007/978-3-531-19381-6_20 [15] Google. 2023. Google Gemini. https://gemini.google.com. Accessed: 2024-10-10. [16] Google. 2024. Google. https://www.google.com/. Accessed: 2024-10-10. [17] Myeongjun Jang and Thomas Lukasiewicz. 2023. Consistency Analysis of ChatGPT. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 15970\u201315985. https://doi.org/10.18653/v1/2023.emnlp-main.991 [18] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-based Prototyping with Large&nbsp;Language&nbsp;Models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922). Association for Computing Machinery, New York, NY, USA, Article 35, 8 pages. https://doi.org/10.1145/ 3491101.3503564 [19] Peiling Jiang, Jude Rayan, Steven P. Dow, and Haijun Xia. 2023. Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST \u201923). Association for Computing Machinery, New York, NY, USA, Article 3, 20 pages. https://doi.org/10.1145/3586183.3606737 [20] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and Applications of Large Language Models. arXiv preprint arXiv:2307.10169 (2023). [21] Anjali Khurana, Hariharan Subramonyam, and Parmit K Chilana. 2024. Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 288\u2013303. https://doi.org/10.1145/3640543.3645200 [22] Hyunwoo Kim, Yoonseo Choi, Taehyun Yang, Honggu Lee, Chaneon Park, Yongju Lee, Jin Young Kim, and Juho Kim. 2024. Using LLMs to Investigate Correlations of Conversational Follow-up Queries with User Satisfaction. arXiv preprint arXiv:2407.13166 (2024). [23] Hyunwoo Kim, Khanh Duy Le, Gionnieve Lim, Dae Hyun Kim, Yoo Jin Hong, and Juho Kim. 2024. DataDive: Supporting Readers\u2019 Contextualization of Statistical Statements with Data Exploration. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 623\u2013639. https://doi.org/10.1145/3640543.3645155 [24] Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, and Juho Kim. 2024. Understanding Users\u2019 Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 385\u2013404. https://doi.org/10.1145/3640543.3645148 [25] Guy Laban and Theo Araujo. 2020. The Effect of Personalization Techniques in Users\u2019 Perceptions of Conversational Recommender Systems. In Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents (Virtual Event, Scotland, UK) (IVA \u201920). Association for Computing Machinery, New York, NY, USA, Article 34, 3 pages. https://doi.org/10.1145/3383652.3423890 [26] Steven A. Lehr, Aylin Caliskan, Suneragiri Liyanage, and Mahzarin R. Banaji. 2024. ChatGPT as Research Scientist: Probing GPT\u2019s capabilities as a Research Librarian, Research Ethicist, Data Generator, and Data Predictor. Proceedings of the National Academy of Sciences 121, 35 (2024), e2404328121. https://doi.org/10.1073/pnas.2404328121 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2404328121 [27] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. 2024. Learning to Rewrite Prompts for Personalized Text Generation. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW \u201924). Association for Computing Machinery, New York, NY, USA, 3367\u20133378. https://doi.org/10.1145/3589334.3645408 [28] Xiao Ma, Swaroop Mishra, Ariel Liu, Sophie Ying Su, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng, Quoc Le, and Ed Chi. 2024. Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems (CHI EA \u201924). Association for Computing Machinery, New York, NY, USA, Article 56, 12 pages. https://doi.org/10.1145/3613905. 3651093 [29] Microsoft. 2023. Microsoft Copilot. https://copilot.microsoft.com. Accessed: 2024-10-10. [30] Ahmadreza Nazari, Lorans Alabood, Kaylyn B Feeley, Vikram K. Jaswal, and Diwakar Krishnamurthy. 2024. Personalizing an AR-based Communication System for Nonspeaking Autistic Users. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 731\u2013741. https://doi.org/10.1145/3640543.3645153 [31] OpenAI. 2023. OpenAI ChatGPT. https://chatgpt.com/. Accessed: 2024-10-10.\nManuscript submitted to ACM\n[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730\u201327744. [33] Savvas Petridis, Michael Terry, and Carrie J Cai. 2024. PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers\u2019 Workflows. In Proceedings of the 2024 ACM Designing Interactive Systems Conference (Copenhagen, Denmark) (DIS \u201924). Association for Computing Machinery, New York, NY, USA, 743\u2013756. https://doi.org/10.1145/3643834.3661613 [34] Dimitri Popolov, Michael Callaghan, and Paul Luker. 2000. Conversation space: visualising multi-threaded conversation. In Proceedings of the Working Conference on Advanced Visual Interfaces (Palermo, Italy) (AVI \u201900). Association for Computing Machinery, New York, NY, USA, 246\u2013249. https://doi.org/10.1145/345513.345330 [35] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1088\u20131113. https://doi.org/10.18653/v1/2024.acl-long.61 [36] Crystal Qian and James Wexler. 2024. Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 370\u2013384. https://doi.org/10.1145/3640543.3645198 [37] Partha Pratim Ray. 2023. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems 3 (2023), 121\u2013154. https://doi.org/10.1016/j.iotcps.2023.04.003 [38] \u00cdtallo Silva, Leandro Marinho, Alan Said, and Martijn C. Willemsen. 2024. Leveraging ChatGPT for Automated Human-centered Explanations in Recommender Systems. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 597\u2013608. https://doi.org/10.1145/3640543.3645171 [39] Streamlit. 2024. Streamlit: A faster way to build and share data apps. https://streamlit.io/. Accessed: 2024-10-10. [40] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST \u201923). Association for Computing Machinery, New York, NY, USA, Article 1, 18 pages. https://doi.org/10.1145/3586183.3606756 [41] Yi Tang, Chia-Ming Chang, and Xi Yang. 2024. PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 419\u2013430. https://doi.org/10.1145/3640543.3645174 [42] Wout Vossen, Maxwell Szymanski, and Katrien Verbert. 2024. The effect of personalizing a psychotherapy conversational agent on therapeutic bond and usage intentions. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 761\u2013771. https://doi.org/10.1145/3640543.3645195 [43] Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, and Raj Sodhi. 2024. LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 699\u2013714. https://doi.org/10.1145/3640543.3645143 [44] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv preprint arXiv:2302.11382 (2023). [45] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155 (2023). [46] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI \u201922). Association for Computing Machinery, New York, NY, USA, Article 385, 22 pages. https://doi.org/10.1145/3491102.3517582 [47] Xiaotong (Tone) Xu, Jiayu Yin, Catherine Gu, Jenny Mar, Sydney Zhang, Jane L. E, and Steven P. Dow. 2024. Jamplate: Exploring LLM-Enhanced Templates for Idea Reflection. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI \u201924). Association for Computing Machinery, New York, NY, USA, 907\u2013921. https://doi.org/10.1145/3640543.3645196 [48] J.D. Zamfirescu-Pereira, Heather Wei, Amy Xiao, Kitty Gu, Grace Jung, Matthew G Lee, Bjoern Hartmann, and Qian Yang. 2023. Herding AI Cats: Lessons from Designing a Chatbot by Prompting GPT-3. In Proceedings of the 2023 ACM Designing Interactive Systems Conference (Pittsburgh, PA, USA) (DIS \u201923). Association for Computing Machinery, New York, NY, USA, 2206\u20132220. https://doi.org/10.1145/3563657.3596138 [49] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Can\u2019t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/3544548.3581388 [50] Xuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng, and Tat-Seng Chua. 2024. Ask-before-Plan: Proactive Language Agents for Real-World Planning. arXiv preprint arXiv:2406.12639 (2024).\nManuscript submitted to ACM\n# A Appendix: User Study Materials A.1 Follow-up Interview\n# A Appendix: User Study Materials\nDuring the follow-up interview, we asked following questions to the participants to gather additional qualitative nsights, particularly focusing on any notable highlights or frustrations with each system. \u2022 Overall, which system do you prefer, Chatbot A or Chatbot B? \u2022 Which interaction style do you prefer, Chatbot A or Chatbot B? Please describe in detail. \u2022 Can you describe in detail which features or functions of the system were most helpful to you? Please provide examples. \u2022 How well do you think the system\u2019s responses matched your needs? If they didn\u2019t match, please describe what information or personalization you think the system overlooked. \u2022 Did the system provide you with new inspiration or directions for your thinking during the exploratory tasks? Please explain in detail how you were inspired. \u2022 What was your experience with the system? Were there any aspects that impressed you, or areas where you felt improvement is needed? \u2022 Did you encounter any confusion or inconvenience while using the system? Please describe in detail. \u2022 What do you think of the UI design?\n# A.2 User Experiment Registration Questionnaire\nThis questionnaire was used to collect basic demographic information from participants to assist in analyzing the data from this experiment. We greatly appreciate the participants\u2019 involvement in this study. (1) Your ID (e.g., P1):\n(2) Your gender:\n(Please select one)\n\u2022 Male\n\u2022 Female\n\u2022 Other (please specify)\n\u2022 Prefer not to disclose\n(3) Your field of study:\n(Please select one)\n\u2022 Computer Science\n\u2022 Engineering (e.g., Electrical, Mechanical)\n\u2022 Natural Sciences (e.g., Physics, Chemistry)\n\u2022 Social Sciences (e.g., Psychology, Economics)\n\u2022 Arts\n\u2022 Other (please specify)\n(4) Your education level:\n(Please select one)\n\u2022 Undergraduate student\n\u2022 Master\u2019s student\n\u2022 PhD student \u2022 PhD or above (5) How familiar are you with conversational AI systems (e.g., ChatGPT, Kimi, Wenxin Yiyan, Tongyi Qianwen, etc.)? (Please select one) \u2022 Heard about it, but never used \u2022 Used it, less than once per week \u2022 Used it, 1-7 times per week \u2022 Used it, multiple times per day\n\u2022 PhD student \u2022 PhD or above (5) How familiar are you with conversational AI systems (e.g., ChatGPT, Kimi, Wenxin Yiyan, Tongyi Qianwen, etc.)? (Please select one) \u2022 Heard about it, but never used \u2022 Used it, less than once per week \u2022 Used it, 1-7 times per week \u2022 Used it, multiple times per day\nThis questionnaire was used to gather feedback on participants\u2019 experience interacting with a conversational AI system. The feedback helps to improve our understanding of user experience with the system. We highly appreciate the participants\u2019 engagement in this experiment.\n(1) Your ID and the chatbot system you used (e.g., P1_A):\n(2) I enjoyed the way I interacted with the system. (Please rate on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) \u2022 1 (strongly disagree) \u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree) (3) The system\u2019s UI presentation helped me better organize complex tasks and reduced my cognitiv (Please rate on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) \u2022 1 (strongly disagree) \u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree) (4) The interaction with the system inspired me to consider new aspects of exploratory tasks. (Please rate on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) \u2022 1 (strongly disagree) \u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree) (5) The answers provided by the system felt comprehensive and sufficient. (Please rate on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) \u2022 1 (strongly disagree) anuscript submitted to ACM\n\u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree) (6) The answers provided by the system matched my personal needs. (Please rate on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) \u2022 1 (strongly disagree) \u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree)\n\u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree) (6) The answers provided by the system matched my personal needs. (Please rate on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) \u2022 1 (strongly disagree) \u2022 2 (disagree) \u2022 3 (neutral) \u2022 4 (agree) \u2022 5 (strongly agree)\n# B Agent Prompts\nIn this appendix, we provide an overview of the prompts used by the five LLM-based agents in our system, along with a summary of the user survey employed for evaluation",
    "paper_type": "method",
    "attri": {
        "background": "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface.",
        "problem": {
            "definition": "LLM-based chatbots face challenges in delivering personalized assistance during exploratory tasks, particularly when they lack access to user-specific data, such as past interactions and personal preferences.",
            "key obstacle": "Existing methods rely heavily on user-provided inputs for personalization, which can lead to generic and impractical recommendations when users start with vague queries."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that users often have implicit needs that are not articulated during interactions with chatbots, leading to unsatisfactory responses.",
            "opinion": "The proposed idea involves a collaborative assistant that actively engages users in refining their queries and exploring their needs to generate personalized solutions.",
            "innovation": "CARE differentiates itself by employing a multi-agent collaboration framework that systematically discovers and addresses both explicit and implicit user needs, enhancing the personalization of responses."
        },
        "method": {
            "method name": "Collaborative Assistant for Personalized Exploration",
            "method abbreviation": "CARE",
            "method definition": "CARE is a chat-based system that utilizes a multi-agent framework to assist users in exploratory tasks by actively engaging them to refine their needs and generate personalized solutions.",
            "method description": "The method involves a structured interface consisting of a Chat Panel, Solution Panel, and Needs Panel, allowing for iterative query refinement and dynamic solution generation.",
            "method steps": [
                "User initiates a query in the Chat Panel.",
                "The Inquiry Agent processes the query and identifies milestones.",
                "The Needs Discovery Agent extracts explicit and implicit needs.",
                "The Ranking Agent organizes follow-up questions for the user.",
                "The Solution Craft Agent generates a personalized solution based on the refined needs."
            ],
            "principle": "The effectiveness of CARE lies in its ability to systematically engage users in a structured manner, ensuring that their evolving needs are continuously addressed throughout the interaction."
        },
        "experiments": {
            "evaluation setting": "A within-subject user study was conducted with 22 participants comparing CARE to a baseline LLM-based chatbot, focusing on tasks such as travel planning and skill learning.",
            "evaluation method": "Participants completed two exploratory tasks using both systems and rated their experiences through a post-task questionnaire assessing interaction, cognitive load, inspiration, comprehensiveness, and personalization."
        },
        "conclusion": "CARE demonstrated significant advantages over the baseline system in terms of inspiring user exploration, personalizing responses, and effectively organizing information, highlighting its potential to transform user engagement with conversational agents.",
        "discussion": {
            "advantage": "CARE's structured interface and proactive inquiry significantly reduce cognitive load and enhance user satisfaction compared to traditional chatbots.",
            "limitation": "The multi-agent design may introduce response latency, potentially affecting user satisfaction, and the participant pool was relatively small and homogeneous.",
            "future work": "Future research should explore enhancing response times, diversifying participant recruitment, and investigating the generalizability of findings across different LLM implementations."
        },
        "other info": {
            "additional details": {
                "user study duration": "Approximately one hour per session.",
                "participant demographics": {
                    "gender": {
                        "male": 18,
                        "female": 4
                    },
                    "educational background": {
                        "undergraduate": 4,
                        "master's": 17,
                        "PhD": 1
                    }
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks."
        },
        {
            "section number": "1.3",
            "key information": "CARE demonstrates significant advantages over the baseline system in terms of inspiring user exploration, personalizing responses, and effectively organizing information, highlighting its potential to transform user engagement with conversational agents."
        },
        {
            "section number": "2.1",
            "key information": "LLM-based chatbots face challenges in delivering personalized assistance during exploratory tasks, particularly when they lack access to user-specific data, such as past interactions and personal preferences."
        },
        {
            "section number": "3.2",
            "key information": "The proposed idea involves a collaborative assistant that actively engages users in refining their queries and exploring their needs to generate personalized solutions."
        },
        {
            "section number": "4.1",
            "key information": "CARE is a chat-based system that utilizes a multi-agent framework to assist users in exploratory tasks by actively engaging them to refine their needs and generate personalized solutions."
        },
        {
            "section number": "4.2",
            "key information": "The effectiveness of CARE lies in its ability to systematically engage users in a structured manner, ensuring that their evolving needs are continuously addressed throughout the interaction."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore enhancing response times, diversifying participant recruitment, and investigating the generalizability of findings across different LLM implementations."
        }
    ],
    "similarity_score": 0.7407984988929279,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Navigating the Unknown_ A Chat-Based Collaborative Interface for Personalized Exploratory Tasks.json"
}