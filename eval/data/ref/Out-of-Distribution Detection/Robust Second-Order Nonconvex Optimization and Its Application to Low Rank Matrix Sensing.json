{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.10547",
    "title": "Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing",
    "abstract": "Finding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings. In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with dimension-independent accuracy guarantees, using \ufffdO(D2/\u01eb) samples where D is the ambient dimension and \u01eb is the fraction of corrupted datapoints. As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both the sensing matrices and the measurements. In addition, we establish a Statistical Query lower bound providing evidence that the quadratic dependence on D in the sample complexity is necessary for computationally efficient algorithms.",
    "bib_name": "li2024robustsecondordernonconvexoptimization",
    "md_text": "# Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing\nShuyao Li\nUniversity of Wisconsin-Madison\nshuyao.li@wisc.edu\nYu Cheng\nBrown University\nyu_cheng@brown.edu\nIlias Diakonikolas\nUniversity of Wisconsin-Madison\nilias@cs.wisc.edu\nJelena Diakonikolas\nUniversity of Wisconsin-Madison\njelena@cs.wisc.edu\nRong Ge\nDuke University\nrongge@cs.duke.edu\nStephen Wright\nUniversity of Wisconsin-Madison\nswright@cs.wisc.edu\n]  12 Mar 2024\n# Abstract\nFinding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings. In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with dimension-independent accuracy guarantees, using \ufffdO(D2/\u01eb) samples where D is the ambient dimension and \u01eb is the fraction of corrupted datapoints. As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both the sensing matrices and the measurements. In addition, we establish a Statistical Query lower bound providing evidence that the quadratic dependence on D in the sample complexity is necessary for computationally efficient algorithms.\n# 1 Introduction\nLearning in the presence of corrupted data is a significant challenge in machine learning (ML) with many applications, including ML security [Bar+10; BNL12; SKL17; Dia+19] and exploratory data analysis of real datasets, e.g., in biological settings [Ros+02; Pas+10; Li+08; Dia+17]. The goal in such scenarios is to design efficient learning algorithms that can tolerate a small constant fraction of outliers and achieve error guarantees independent of the dimensionality of the data. Early work in robust statistics [Ham+86; HR09] gave sample-efficient robust estimators for various tasks (e.g., the Tukey median [Tuk75] for robust mean estimation), alas with runtimes exponential in the dimension. A recent line of work in computer science, starting with [Dia+16; LRV16], developed the first computationally efficient robust algorithms for several fundamental high-dimensional tasks. Since these early works, there has been significant progress in algorithmic aspects of robust high-dimensional statistics (see [DK19] and [DK23] for a comprehensive overview).\nIn this paper, we study the general problem of smooth (with Lipschitz gradient and Hessian) stochastic nonconvex optimization minx \u00aff(x) in the outlier-robust setting, where \u00aff(x) := EA\u223cG f(x, A) and G is a possibly unknown distribution of the random parameter A. We will focus on the following standard adversarial contamination model (see, e.g., [Dia+16]). Definition 1.1 (Strong Contamination Model). Given a parameter 0 < \u01eb < 1/2 and an inlier distribution G, an algorithm receives samples from G with \u01eb-contamination as follows: The algorithm first specifies the number of samples n, and n samples are drawn independently from G. An adversary is then allowed to inspect these samples, and replace an \u01eb-fraction of the samples with arbitrary points. This modified set of n points is said to be \u01eb-corrupted, which is then given to the algorithm. The stochastic optimization problem we consider is computationally intractable in full generality \u2014 even without corruption \u2014 if the goal is to obtain globally optimal solutions. At a high level, an achievable goal is to design sample and computationally efficient robust algorithms for finding locally optimal solutions. Prior work [Pra+20; Dia+19] studied outlier-robust stochastic optimization and obtained efficient algorithms for finding approximate first-order stationary points. While first-order guarantees suffice for convex problems, it is known that in many tractable non-convex problems, first-order stationary points may be bad solutions, but all second-order stationary points (SOSPs) are globally optimal. This motivates us to study the following questions:\nCan we obtain sample and computationally efficient algorithms for outlier-robust versions of tractable nonconvex problems using this framework?\nIn this work, we answer both questions affirmatively. We introduce a framework for efficiently finding an approximate SOSP when \u01eb-fraction of the functions are corrupted and then use our framework to solve the problem of outlier-robust low rank matrix sensing.\ning an approximate SOSP when \u01eb-fraction of the functions are corrupted and then use our framework to solve the problem of outlier-robust low rank matrix sensing. In addition to the gradient being zero, a SOSP requires the Hessian matrix to not have negative eigenvalues. The second-order optimality condition is important because it rules out suboptimal solutions such as strict saddle points. It is known that all SOSPs are globally optimal in nonconvex formulations of many important machine learning problems, such as matrix completion [GLM16], matrix sensing [BNS16], phase retrieval [SQW16], phase synchronization [BBV16], dictionary learning [SQW17], and tensor decomposition [Ge+15] (see also [WM22, Chapter 7]). However, the properties of SOSPs are highly sensitive to perturbation in the input data. For example, it is possible to create spurious SOSPs for nonconvex formulations of low rank matrix recovery problems, even for a semi-random adversary that can add additional sensing matrices but cannot corrupt the measurements in matrix sensing [GC23] or an adversary who can only reveal more entries of the ground-truth matrix in matrix completion [CG18]. Those spurious SOSPs correspond to highly suboptimal solutions. Finding SOSPs in stochastic nonconvex optimization problems in the presence of arbitrary outliers was largely unaddressed prior to our work. Prior works [Pra+20; Dia+19] obtained efficient and robust algorithms for finding first-order stationary points with dimension-independent accuracy guarantees. These works relied on the following simple idea: Under certain smoothness assumptions, projected gradient descent with an approximate gradient oracle efficiently converges to an approximate first-order stationary point. Moreover, in the outlier-robust setting, approximating the gradient at a specific point amounts to a robust mean estimation problem (for the underlying distribution of the gradients), which can be solved by leveraging existing algorithms for robust mean estimation. Our work is the first to find approximate SOSPs with dimension-independent errors in outlier-robust settings. Note that in standard non-robust settings, approximate SOSPs can be computed using first-order methods such as perturbed gradient descent [Jin+17; Jin+21]. This strategy might seem extendable to outlier-robust settings through perturbed approximate gradient descent, utilizing robust mean estimation algorithms to approximate gradients. The approach in [Yin+19] follows this idea, but unfortunately their second-order guarantees scale polynomially with dimension, even under very strong distributional assumptions (e.g., subgaussianity). Our lower bound result provides evidence that approximating SOSPs with dimension-independent error is as hard as approximating full Hessian, suggesting that solely approximating the gradients is not sufficient. On a different note, [IPL23] recently employed robust estimators for both gradient and Hessian in solving certain convex\nstochastic optimization problems, which has a different focus than ours and does not provide SOSPs with the guarantees that we achieve.\n# 1.1 Our Results and Contributions\nThe notation we use in this section is defined in Section 2. To state our results, we first formally define our generic nonconvex optimization problem. Suppose there is a true distribution over functions f : RD \u00d7 A \u2192R, where f(x, A) takes an argument x \u2208RD and is parameterized by a random variable A \u2208A drawn from a distribution G. Our goal is to find an (\u01ebg, \u01ebH)-approximate SOSP of the function \u00aff(x) := EA\u223cG f(x, A). Definition 1.2 (\u01eb-Corrupted Stochastic Optimization). The algorithm has access to n functions (fi)n i=1 generated as follows. First n random variables (Ai)n i=1 are drawn independently from G. Then an adversary arbitrarily corrupts an \u01eb fraction of the Ai\u2019s. Finally, the \u01eb-corrupted version of fi(\u00b7) = f(\u00b7, Ai) is sent to the algorithm as input. The task is to find an approximate SOSP of the ground-truth average function \u00aff(\u00b7) := EA\u223cG f(\u00b7, A). Definition 1.3 (Approximate SOSPs). A point x is an (\u01ebg, \u01ebH)-approximate second-order stationary point (SOSP) of \u00aff if \ufffd\ufffd\u2207\u00aff(x) \ufffd\ufffd\u2264\u01ebg and \u03bbmin \ufffd \u22072 \u00aff(x) \ufffd \u2265\u2212\u01ebH. We make the following additional assumptions on f and G. Assumption 1.4. There exists a bounded region B such that the following conditions hold: (i) There exists a lower bound f \u2217> \u2212\u221esuch that for all x \u2208B, f(x, A) \u2265f \u2217with probability 1. (ii) There exist parameters LDg, LDH, BDg, and BDH such that, with high probability over the randomness in A \u223cG, letting g(x) = f(x, A), we have that g(x) is LDg-gradient Lipschitz and LDH-Hessian Lipschitz over B, and \u2225\u2207g(x)\u2225\u2264BDg and \ufffd\ufffd\u22072g(x) \ufffd\ufffd F \u2264BDH for all x \u2208B. (iii) There exist parameters \u03c3g, \u03c3H > 0 such that for all x \u2208B, \u2225CovA\u223cG(\u2207f(x, A))\u2225op \u2264\u03c32 g and \ufffd\ufffdCovA\u223cG(vec(\u22072f(x, A))) \ufffd\ufffd op \u2264\u03c32 H. Note that the radius of B and the parameters LDg, LDH, BDg, BDH are all allowed to depend polynomially on D and \u01eb (but not on x and A). Our main algorithmic result for \u01eb-corrupted stochastic optimization is summarized in the following theorem. A formal version of this theorem is stated as Theorem 3.1 in Section 3. Theorem 1.5 (Finding an Outlier-Robust SOSP, informal). Suppose f satisfies Assumption 1.4 in a region B with parameters \u03c3g and \u03c3H. Given an arbitrary initial point x0 \u2208B and an \u01eb-corrupted set of n = \ufffd\u2126 \ufffd D2/\u01eb \ufffd functions where D is the ambient dimension, there exists a polynomial-time algorithm that with high probability outputs an (O(\u03c3g \u221a\u01eb) , O(\u03c3H \u221a\u01eb))-approximate SOSP of \u00aff, provided that all iterates of the algorithm stay inside B. Although the bounded iterate condition in Theorem 1.5 appears restrictive, this assumption holds if the objective function satisfies a \u201cdissipativity\u201d property, which is a fairly general phenomenon [Hal10]. Moreover, adding an \u21132-regularization term enables any Lipschitz function to satisfy the dissipativity property [RRT17, Section 4]. As an illustrating example, a simple problemspecific analysis shows that this bounded iterate condition holds for outlier-robust matrix sensing by exploiting the fact that the matrix sensing objective satisfies the dissipativity property. In this paper, we consider the problem of outlier-robust symmetric low rank matrix sensing, which we formally define below. We focus on the setting with Gaussian design. Definition 1.6 (Outlier-Robust Matrix Sensing). There is an unknown rank-r ground-truth matrix M \u2217\u2208Rd\u00d7d that can be factored into U \u2217U \u2217\u22a4where U \u2217\u2208Rd\u00d7r. The (clean) sensing matrices {Ai}i\u2208[n] have i.i.d. standard Gaussian entries. The (clean) measurements yi are obtained as yi = \u27e8Ai, M \u2217\u27e9+\u03b6i, where the noise \u03b6i \u223cN(0, \u03c32) is independent from all other randomness. We denote the (clean) data generation process by (Ai, yi) \u223cG\u03c3. When \u03c3 = 0, we have \u03b6i = 0 and we write G := G0 for this noiseless (measurement) setting. In outlier robust matrix sensing, an adversary can\narbitrarily change any \u01eb-fraction of the sensing matrices and the corresponding measurements. This corrupted set of (Ai, yi)\u2019s is then given to the algorithm as input, where the goal is to recover M \u2217. We highlight that in our setting, both the sensing matrices Ai \u2208Rd\u00d7d and the measurements yi \u2208R can be corrupted, presenting a substantially more challenging problem compared to prior works (e.g., [Li+20b; Li+20a]) that only allow corruption in yi. Let \u03c3\u22c6 1 and \u03c3\u22c6 r denote the largest and the smallest nonzero singular value of M \u2217respectively. We assume \u03c3\u22c6 r and the rank r are given to the algorithm, and we assume that the algorithm knows a multiplicative upper bound \u0393 of \u03c3\u22c6 1 such that \u0393 \u226536\u03c3\u22c6 1 (a standard assumption in matrix sensing even for non-robust settings [GJZ17; Jin+17]). Let \u03ba = \u0393/\u03c3\u22c6 r. Our main algorithmic result for the low rank matrix sensing problem is summarized in the following theorem. For a more detailed statement, see Theorems 3.2 and 3.3 in Section 3. Theorem 1.7 (Our Algorithm for Outlier-Robust Matrix Sensing). Let M \u2217\u2208Rd\u00d7d be the rank r ground-truth matrix with smallest nonzero singular value \u03c3\u22c6 r. Let \u0393 \u226536 \u2225M \u2217\u2225op and let \u03ba = \u0393/\u03c3\u22c6 r. There exists an algorithm for outlier-robust matrix sensing, where an \u01eb = O(1/(\u03ba3r3)) fraction of samples from G\u03c3 as in Definition 1.6 gets arbitrarily corrupted, that can output a rank-r matrix \ufffd M such that \u2225\ufffd M \u2212M \u2217\u2225F \u2264\u03b9 with probability at least 1 \u2212\u03be, where \u03b9 > 0 is the error parameter: 1) If \u03c3 \u2265r\u0393, then \u03b9 = O(\u03c3\u221a\u01eb); 2) If \u03c3 \u2264r\u0393, then \u03b9 = O(\u03ba\u03c3\u221a\u01eb); 3) If \u03c3 = 0 (noiseless), then \u03b9 can be made arbitrarily small, achieving exact recovery. \ufffd \ufffd\n(noiseless), then \u03b9 can be made arbitrarily small, achieving exact recover\n \ufffd \ufffd \ufffd Finally, we complement our algorithmic results for outlier-robust matrix sensing with a Statistical Query (SQ) lower bound, which provides strong evidence that quadratic dependence on d in the sample complexity is unavoidable for efficient algorithms. A detailed statement of this result is provided in Section 4.\n# 1.2 Our Techniques\nOutlier-robust nonconvex optimization. To obtain our algorithmic result in the general nonconvex setting, we leverage existing results on robust mean estimation [DKP20], which we use as a black box to robustly estimate the gradient and the (vectorized) Hessian. We use these robust estimates as a subroutine in a randomized nonconvex optimization algorithm (described in Appendix A.2), which can tolerate inexactness in both the gradient and the Hessian. With high probability, this algorithm outputs an (\u01ebg, \u01ebH)-approximate SOSP, where \u01ebg and \u01ebH depend on the inexactness of the gradient and Hessian oracles. We remark that robust estimation of the Hessian is crucial to obtaining our dimension-independent approximation error result and is what causes D2 dependence in the sample complexity (which is unavoidable for SQ algorithms as discussed below). Notably, the only prior work on approximating SOSPs in the outlier-robust setting [Yin+19] used robust mean estimation only on the gradients and had sample complexity scaling linearly with D; however, they can only output an order (\u221a\u01eb, (\u01ebD)1/5)-SOSP, which is uninformative for many problems of interest, including the matrix sensing problem considered in this paper, due to the dimensional dependence in the approximation error.\nApplication to low rank matrix sensing. Our main contribution on the algorithmic side is showing that our outlier-robust nonconvex optimization framework can be applied to solve outlier-robust matrix sensing with dimension-independent approximation error, even achieving exact recovery when the measurements are noiseless. We obtain this result using the following geometric insights about the problem: We show that the norm of the covariance of the gradient and the Hessian can both be upper bounded by the sum of \u03c32 and a function of the distance to the closest optimal solution. We further prove that all iterates stay inside a nice region using a \u201cdissipativity\u201d property [Hal10]), which says that the iterate aligns with the direction of the gradient when the iterate\u2019s norm is large.\nWe show that this approximate SOSP must be close to a global optimal solution. Additionally, we establish a local regularity condition in a small region around globally optimal solutions (which is similar to strong convexity but holds only locally). This local regularity condition bounds below a measure of stationarity, which allows us to prove that gradient descent-type updates contract the distance to the closest global optimum under appropriate stepsize. We leverage this local regularity condition to prove that the iterates of the algorithm stay near a global optimum, so that the regularity condition continues to hold, and moreover, the distance between the current solution and the closest global optimum contracts, as long as it is larger than a function of \u03c3. Consequently, the distancedependent component of the gradient and Hessian covariance bound contracts as well, which allows us to obtain more accurate gradient and Hessian estimates. While such a statement may seem evident to readers familiar with linear convergence arguments, we note that proving it is quite challenging, due to the circular dependence between the distance from the current solution to global optima, the inexactness in the gradient and Hessian estimates, and the progress made by our algorithm. The described distance-contracting argument allows us to control the covariance of the gradient and Hessian, which we utilize to recover M \u2217exactly when \u03c3 = 0, and recover M \u2217with error roughly O(\u03c3\u221a\u01eb) when 0 \u0338= \u03c3 \u2264r\u0393. We note that the \u03c3\u221a\u01eb appears unavoidable in the \u03c3 \u0338= 0 case, due to known limits of robust mean estimation algorithms [DKS17].\nSQ lower bound. We exhibit a hard instance of low rank matrix sensing problem to show that quadratic dependence on the dimension in sample complexity is unavoidable for computationally efficient algorithms. Our SQ lower bound proceeds by constructing a family of distributions, corresponding to corruptions of low rank matrix sensing, that are nearly uncorrelated in a well-defined technical sense [Fel+17]. To achieve this, we follow the framework of [DKS17] which considered a family of distributions that are rotations of a carefully constructed one-dimensional distribution. The proof builds on [DKS19; Dia+21], using a new univariate moment-matching construction which yields a family of corrupted conditional distributions. These induce a family of joint distributions that are SQ-hard to learn.\n# 1.3 Roadmap\nSection 2 defines the necessary notation and discusses relevant building blocks of our algorithm and analysis. Section 3 introduces our framework for finding SOSPs in the outlier-robust setting. Section 3.1 presents how to extend and apply our framework to solve outlier-robust low rank matrix sensing. Section 4 proves that our sample complexity has optimal dimensional dependence for SQ algorithms. Most proofs are deferred to the supplementary material due to space limitations.\n# 2 Preliminaries\nFor an integer n, we use [n] to denote the ordered set {1, 2, . . ., n}. We use [ai]i\u2208I to denote the matrix whose columns are vectors ai, where I is an ordered set. We use 1E(x) to denote the indicator function that is equal to 1 if x \u2208E and 0 otherwise. For two functions f and g, we say f = \ufffdO(g) if f = O(g logk(g)) for some constant k, and we similarly define \ufffd\u2126. For vectors x and y, we let \u27e8x, y\u27e9denote the inner product x\u22a4y and \u2225x\u2225denote the \u21132 norm of x. For d \u2208Z+, we use Id to denote the identity matrix of size d \u00d7 d. For matrices A and B, we use \u2225A\u2225op and \u2225A\u2225F to denote the spectral norm and Frobenius norm of A respectively. We use \u03bbmax(A) and \u03bbmin(A) to denote the maximum and minimum eigenvalue of A respectively. We use tr(A) to denote the trace of a matrix A. We use \u27e8A, B\u27e9= tr(A\u22a4B) to denote the entry-wise inner product of two matrices of the same dimension. We use vec(A) = [a\u22a4 1 , a\u22a4 2 , . . . , a\u22a4 d ]\u22a4to denote the canonical flattening of A into a vector, where a1, a2, . . . , ad are columns of A. Definition 2.1 (Lipschitz Continuity). Let X and Y be normed vector spaces. A function h : X \u2192Y is \u2113-Lipschitz if \u2225h(x1) \u2212h(x2)\u2225Y \u2264\u2113\u2225x1 \u2212x2\u2225X , \u2200x1, x2. In this paper, when Y is a space of matrices, we take \u2225\u00b7\u2225Y to be the spectral norm \u2225\u00b7\u2225op. When X is a space of matrices, we take \u2225\u00b7\u2225X to be the Frobenius norm \u2225\u00b7\u2225F ; this essentially views the function\nh as operating on the vectorized matrices endowed with the usual \u21132 norm. When X or Y is the Euclidean space, we take the corresponding norm to be the \u21132 norm. A Randomized Algorithm with Inexact Gradients and Hessians. We now discuss how to solve the unconstrained nonconvex optimization problem minx\u2208RD f(x), where f(\u00b7) is a smooth function with Lipschitz gradients and Lipschitz Hessians. The goal of this section is to find an approximate SOSP as defined in Definition 1.3. Proposition 2.2 ([LW23]). Suppose a function f is bounded below by f \u2217> \u2212\u221e, has Lg-Lipschitz gradient and LH-Lipschitz Hessian, and its inexact gradient and Hessian computations \ufffdgt and \ufffdHt satisfy \u2225\ufffdgt \u2212\u2207f(xt)\u2225\u2264 1 3\u01ebg and \u2225\ufffdHt \u2212\u22072f(xt)\u2225op \u2264 2 9\u01ebH. Then there exists an algorithm (Algorithm A.1) with the following guarantees: 1. (Correctness) If Algorithm A.1 terminates and outputs xn, then xn is a ( 4 3\u01ebg, 4 3\u01ebH)-approximate SOSP. 2. (Runtime) Algorithm A.1 terminates with probability 1. Let C\u01eb := min \ufffd\u01eb2 g 6Lg , 2\u01eb3 H 9L2 H \ufffd . With probability at least 1 \u2212\u03b4, Algorithm A.1 terminates after k iterations for\n2. (Runtime) Algorithm A.1 terminates with probability 1. Let C\u01eb := min \ufffd\u01eb g 6Lg , 2\u01eb3 H 9L2 H \ufffd . With probability at least 1 \u2212\u03b4, Algorithm A.1 terminates after k iterations for\nThe constants 1/3 and 2/9 are chosen for ease of presentation. For all constructions of Hessian oracles in this paper, we take the straightforward relaxation \u2225\ufffdHt \u2212\u22072f(xt)\u2225op \u2264\u2225\ufffdHt \u2212\u22072f(xt)\u2225F and upper bound Hessian inexactness using Frobenius norm. Proof of a simplified version of Proposition 2.2 with a weaker high probability bound that is sufficient for our purposes is provided in Appendix A.2 for completeness.\nRobust Mean Estimation. Recent algorithms in robust mean estimation give dimensionindependent error in the presence of outliers under strong contamination model.\nWe use the following results, see, e.g., [DKP20], where the upper bound \u03c3 on the spectral norm of covariance matrix is unknown to the algorithm. Proposition 2.3 (Robust Mean Estimation). Fix any 0 < \u03be < 1. Let S be a multiset of n = O((k log k + log(1/\u03be))/\u01eb) i.i.d. samples from a distribution on Rk with mean \u00b5S and covariance \u03a3. Let T \u2282Rk be an \u01eb-corrupted version of S as in Definition 1.1. There exists an algorithm (Algorithm A.2) such that, with probability at least 1 \u2212\u03be, on input \u01eb and T (but not \u2225\u03a3\u2225op) returns \ufffd\n \ufffd\ufffd \ufffd Algorithm A.2 is given in Appendix A.3. Proposition 2.3 states that for Algorithm A.2 to succeed with high probability, \ufffdO(k/\u01eb) i.i.d. samples need to be drawn from a k-dimensional distribution of bounded covariance. State of the art algorithms for robust mean estimation can be implemented in near-linear time, requiring only a logarithmic number of passes on the data, see, e.g, [Dia+22; CDG19; DHL19]. Any of these faster algorithms could be used for our purposes. With the above results, the remaining technical component for applying the robust estimation subroutine (Algorithm A.2) in this paper is handling the dependence across iterations. Because we will run RobustMeanEstimation in each iteration of our optimization algorithm, the gradients {\u2207fi(xt)}n i=1 and Hessians {\u22072fi(xt)}n i=1 can no longer be considered as independently drawn from a distribution after the first iteration. Although they are i.i.d. for fixed x, the dependence on previous iterations through x will break the independence assumption. Therefore, we will need a union bound over all xt to handle dependence across different iterations t. We deal with this technicality in Appendix B.\n# 3 General Robust Nonconvex Optimization\nIn this section, we establish a general result that uses Algorithm A.1 to obtain approximate SOSPs in the presence of outliers under strong contamination. The inexact gradient and inexact Hessian\noracles are constructed with the robust mean estimation subroutine (Algorithm A.2). We consider stochastic optimization tasks in Definition 1.2 satisfying Assumption 1.4. We construct the inexact gradient and Hessian oracle required by Algorithm A.1 as follows:\n# oracles are constructed with the robust mean estimation subroutine (Algorithm A.2). We consider stochastic optimization tasks in Definition 1.2 satisfying Assumption 1.4. We construct the inexact gradient and Hessian oracle required by Algorithm A.1 as follows:\n# \ufffdgt \u2190RobustMeanEstimation({\u2207fi(xt)}n i=1, 4\u01eb) \ufffdHt \u2190RobustMeanEstimation({\u22072fi(xt)}n i=1, 4\u01eb)\n\ufffdgt \u2190RobustMeanEstimation({\u2207fi(xt)}n i=1, 4\u01eb) \ufffdHt \u2190RobustMeanEstimation({\u22072fi(xt)}n i=1, 4\u01eb)\n\ufffd Then we have the following guarantee:\n\ufffd Then we have the following guarantee: Theorem 3.1. Suppose we are given \u01eb-corrupted set of functions {fi}n i=1 for sample size n, generated according to Definition 1.2. Suppose Assumption 1.4 holds in a bounded region B \u2282RD of diameter \u03b3 with gradient and Hessian covariance bound \u03c3g and \u03c3H respectively, and we have an arbitrary initialization x0 \u2208B. Algorithm A.1 initialized at x0 outputs an (\u01ebg, \u01ebH)-approximate SOSP for a sufficiently large sample with probability at least 1 \u2212\u03be if the following conditions hold:\n(I) All iterates xt in Algorithm A.1 stay inside the bounded region B.\n(I) All iterates xt in Algorithm A.1 stay inside the bounded region B. (II) For an absolute constant c > 0, it holds that \u03c3g \u221a\u01eb \u2264c\u01ebg and \u03c3H \u221a\u01eb \u2264c\u01ebH.\nII) For an absolute constant c > 0, it holds that \u03c3g \u221a\u01eb \u2264c\u01ebg and \u03c3H \u221a\u01eb \u2264c\u01ebH.\nThe algorithm uses n = \ufffdO(D2/\u01eb) samples, where \ufffdO(\u00b7) hides logarithmic dependence on D, \u01eb, LDg, LDH, BDg, BDH, \u03b3/\u03c3H, \u03b3/\u03c3g, and 1/\u03be. The algorithm runs in time polynomial in the above parameters.\nThe algorithm uses n = \ufffdO(D2/\u01eb) samples, where \ufffdO(\u00b7) hides logarithmic dependence on D, \u01eb, LDg, LDH, BDg, BDH, \u03b3/\u03c3H, \u03b3/\u03c3g, and 1/\u03be. The algorithm runs in time polynomial in the above parameters. Note that we are able to obtain dimension-independent errors \u01ebg and \u01ebH, provided that \u03c3g and \u03c3H are dimension-independent.\nNote that we are able to obtain dimension-independent errors \u01ebg and \u01ebH, provided that \u03c3g and \u03c3H are dimension-independent.\n# 3.1 Low Rank Matrix Sensing Problems\nIn this section, we study the problem of outlier-robust low rank matrix sensing as formally defined in Definition 1.6. We first apply the above framework to obtain an approximate SOSP in Section 3.1.2. Then we make use of the approximate SOSP to obtain a solution that is close to the ground-truth matrix M \u2217in Section 3.1.3; this demonstrates the usefulness of approximate SOSPs.\n# 3.1.1 Main results for Robust Low Rank Matrix Sensing\nThe following are the main results that we obtain in this section: Theorem 3.2 (Main Theorem Under Noiseless Measurements). Consider the noiseless setting as in Theorem 1.7 with \u03c3 = 0. For some sample size n = \ufffdO((d2r2 + dr log(\u0393/\u03be))/\u01eb) and with probability at least 1 \u2212\u03be, there exists an algorithm that outputs a solution that is \u03b9-close to M \u2217in Frobenius norm in O(r2\u03ba3 log(1/\u03be) + \u03ba log(\u03c3\u22c6 r/\u03b9)) calls to the robust mean estimation subroutine (Algorithm A.2).\nThis result achieves exact recovery of M \u2217, despite the strong contamination of samples. Each iteration involves a subroutine call to robust mean estimation. Algorithm A.2 presented here is one simple example of robust mean estimation; there are refinements [Dia+22; DHL19] that run in nearly linear time, so the total computation utilizing those more efficient algorithms indeed requires \ufffdO \ufffd r2\u03ba3\ufffd passes of data (computed gradients and Hessians). Theorem 3.3 (Main Theorem Under Noisy Measurements). Consider the same setting as in Theorem 1.7 with \u03c3 \u0338= 0. There exists a sample size n = \ufffdO((d2r2 + dr log(\u0393/\u03be))/\u01eb) such that \u2022 if \u03c3 \u2264r\u0393, then with probability at least 1 \u2212\u03be, there exists an algorithm that outputs a solution \ufffd M in \ufffdO(r2\u03ba3) calls to robust mean estimation routine A.2, with error \u2225\ufffd M \u2212M \u2217\u2225F = O(\u03ba\u03c3\u221a\u01eb); \u2022 if \u03c3 \u2265r\u0393, then with probability at least 1 \u2212\u03be, there exists a (different) algorithm that outputs a solution \ufffd M in one call to robust mean estimation routine A.2, with error \u2225\ufffd M \u2212M \u2217\u2225F = O(\u03c3\u221a\u01eb). We prove Theorem 3.3 in Appendix C.4, and instead focus on the noiseless measurements with \u03c3 = 0 when we develop our algorithms in this section; the two share many common techniques. In the remaining part of Section 3.1, we use G0 in Definition 1.6 for the data generation process.\nWe now describe how we obtain the solution via nonconvex optimization. Consider the following objective function for (uncorrupted) matrix sensing:\nWe can write M = UU \u22a4for some U \u2208Rd\u00d7r to reparameterize the objective function. Let\nWe can compute\n\ufffd\ufffd\ufffd\ufffd We seek to solve the following optimization problem under the corruption model in Definition 1.2:\nThe gradient Lipschitz constant and Hessian Lipschitz constant of \u00aff are given by the following result. Fact 3.4 ([Jin+17], Lemma 6). For any \u0393 > \u03c3\u22c6 1, \u00aff(U) has gradient Lipschitz constant Lg = 16\u0393 and Hessian Lipschitz constant LH = 24\u0393 1 2 inside the region {U : \u2225U\u22252 op < \u0393}.\nThe gradient Lipschitz constant and Hessian Lipschitz constant of \u00aff are given by the following result. Fact 3.4 ([Jin+17], Lemma 6). For any \u0393 > \u03c3\u22c6 , \u00aff(U) has gradient Lipschitz constant L = 16\u0393\nFact 3.4 ([Jin+17], Lemma 6). For any \u0393 > \u03c3\u22c6 1, \u00aff(U) has gradient Lipschitz constant Lg = 16\u0393 and Hessian Lipschitz constant LH = 24\u0393 1 2 inside the region {U : \u2225U\u22252 op < \u0393}.\n# Global Convergence to an Approximate SOSP\nIn this section, we apply our framework Theorem 3.1 to obtain global convergence from an arbitrary initialization to an approximate SOSP, by providing problem-specific analysis to guarantee that both Assumption 1.4 and algorithmic assumptions (I) and (II) required by Theorem 3.1 are satisfied. Theorem 3.5 (Global Convergence to a SOSP). Consider the noiseless setting as in Theorem 1.7 with \u03c3 = 0 and \u01eb = O(1/(\u03ba3r3)). Assume we have an arbitrary initialization U0 inside {U : \u2225U\u22252 op \u2264\u0393}. There exists a sample size n = \ufffdO \ufffd (d2r2 + dr log(\u0393/\u03be))/\u01eb \ufffd such that with probability at least 1 \u2212\u03be, Algorithm A.1 initialized at U0 outputs a ( 1 24\u03c3\u22c6 r 3/2, 1 3\u03c3\u22c6 r)-approximate SOSP using at most O \ufffd r2\u03ba3 log(1/\u03be) \ufffd calls to robust mean estimation subroutine (Algorithm A.2).\nIn this section, we apply our framework Theorem 3.1 to obtain global convergence from an arbitrary initialization to an approximate SOSP, by providing problem-specific analysis to guarantee that both Assumption 1.4 and algorithmic assumptions (I) and (II) required by Theorem 3.1 are satisfied.\nTheorem 3.5 (Global Convergence to a SOSP). Consider the noiseless setting as in Theorem 1.7 with \u03c3 = 0 and \u01eb = O(1/(\u03ba3r3)). Assume we have an arbitrary initialization U0 inside {U : \u2225U\u22252 op \u2264\u0393}. There exists a sample size n = \ufffdO \ufffd (d2r2 + dr log(\u0393/\u03be))/\u01eb \ufffd such that with probability at least 1 \u2212\u03be, Algorithm A.1 initialized at U0 outputs a ( 1 24\u03c3\u22c6 r 3/2, 1 3\u03c3\u22c6 r)-approximate SOSP using at most O \ufffd r2\u03ba3 log(1/\u03be) \ufffd calls to robust mean estimation subroutine (Algorithm A.2).\n\ufffd \ufffd Proof of Theorem 3.5. To apply Theorem 3.1, we verify Assumption 1.4 first. To verify (i), for all U and Ai, fi(U) = 1 2 \ufffd \u27e8UU \u22a4, Ai\u27e9\u2212yi \ufffd2 \u22650 , so f \u2217= 0 is a uniform lower bound. We verify (ii) in Appendix C.2: conceptually, by Fact 3.4, \u00aff is gradient and Hessian Lipschitz; both gradient and Hessian of fi are sub-exponential and concentrate around those of \u00aff. To check (iii), we calculate the gradients and Hessians of fi in Appendix C.1.1 and bound their covariances from above in Appendix C.1.2 and C.1.3. The result is summarized in the following lemma. Note that the domain of the target function in Algorithm A.1 and Theorem 3.1 is the Euclidean space RD, so we vectorize U and let D = dr. The gradient becomes a vector in Rdr and the Hessian becomes a matrix in Rdr\u00d7dr. Rd\u00d7r 2\n# Lemma 3.6 (Gradient and Hessian Covariance Bounds). For all U \u2208Rd\u00d7r with \u2225U\u22252 op \u2264\u0393 and fi defined in Equation (3), it holds\n\u2225Cov(vec(\u2207fi(U)))\u2225op \u22648 \ufffd\ufffdUU \u22a4\u2212M \u2217\ufffd\ufffd2 F \u2225U\u22252 op \u226432r2\u03933 \u2225Cov(vec(Hi))\u2225op \u226416r \ufffd\ufffdUU \u22a4\u2212M \u2217\ufffd\ufffd2 F + 128 \u2225U\u22254 op \u2264192r3\u03932\n\ufffd\ufffd\ufffd\ufffd We proceed to verify the algorithmic assumptions in Theorem 3.1. For the assumption (I), we prove the following Lemma in Appendix C.2 to show that all iterates stay inside the bounded region in which we compute the covariance bounds. Lemma 3.7. All iterates of Algorithm A.1 stay inside the region {U : \u2225U\u22252 op \u2264\u0393}.\n(2)\n(3)\n(4)\n(5)\n(6) (7)\nTo verify Theorem 3.1 (II), we let \u01ebg = 1 32\u03c3\u22c6 r 3/2, \u01ebH = 1 4\u03c3\u22c6 r and \u03c3g = 8r\u03931.5, \u03c3H = 16r1.5\u0393. S we assume \u01eb = O(1/(\u03ba3r3)), then for the absolute constant c in Theorem 3.1 it holds that\nHence, Theorem 3.1 applies and Algorithm A.1 outputs an (\u01ebg, \u01ebH)-approximate SOSP with high probability in polynomial time. To bound the runtime, since \u00aff(U0) = 1/2 \ufffd\ufffdU0U \u22a4 0 \u2212M \u2217\ufffd\ufffd2 F = O(r2\u03932) for an arbitrary initialization U0 with \u2225U0\u22252 op < \u0393, the initial distance can be bounded by O(r2\u03932). Setting Lg = 16\u0393, LH = 24\u03931/2, \u00aff(U0) = O(r2\u03932), f \u2217= 0 and thus C\u01eb = O(\u03c3\u22c6 r 3/\u0393), Proposition 2.2 implies that Algorithm A.1 outputs a ( 1 24\u03c3\u22c6 r 3/2, 1 3\u03c3\u22c6 r)-approximate second order stationary point USOSP in O(r2\u03ba3 log(1/\u03be)) steps with high probability.\n# 3.1.3 Local Linear Convergence\nIn this section, we describe a local search algorithm that takes a ( 1 24\u03c3\u22c6 r 3/2, 1 3\u03c3\u22c6 r)-approximate secondorder stationary point as its initialization and achieves exact recovery even in the presence of outliers.\nAlgorithm 3.1: Local Inexact Gradient Descent\nData: The initialization USOSP is a ( 1\n24\u03c3\u22c6\nr\n3/2, 1\n3\u03c3\u22c6\nr)-approximate SOSP, corruption fraction is\n\u01eb, corrupted samples are {(Ai, yi)}n\ni=1, target distance to optima is \u03b9\nResult: U that is \u03b9-close in Frobenius norm to some global minimum\n1 \u03b7 = 1/\u0393, U0 = USOSP\n2 for t = 0, 1, ... do\n3\n\ufffdgt := RobustMeanEstimation({\u2207fi(Ut)}n\ni=1, 4\u01eb)\n4\nUt+1 \u2190Ut \u2212\u03b7\ufffdgt\n\ufffd Theorem 3.8 (Local Linear Convergence). Consider the same noiseless setting as in Theorem 1.7. Assume we already found a ( 1 24\u03c3\u22c6 r 3/2, 1 3\u03c3\u22c6 r)-approximate SOSP USOSP of \u00aff. Then there exists a sample size n = \ufffdO (dr log(1/\u03be)/\u01eb) such that with probability at least 1 \u2212\u03be, Algorithm 3.1 initialized at USOSP outputs a solution that is \u03b9-close to some global minimum in Frobenius norm after O(\u03ba log(\u03c3\u22c6 r/\u03b9)) calls to robust mean estimation subroutine (Algorithm A.2). Moreover, all iterates Ut are 1 3\u03c3\u22c6 r 1/2-close to some global minimum in Frobenius norm.\nProof Sketch. First we use known properties of \u00aff = E(Ai,yi)\u223cG0 fi from the literature [Jin+17] to show approximate SOSPs of \u00aff \u2014 in particular our initialization USOSP \u2014 are in a small neighborhood of the global minima of \u00aff. In that neighborhood, it was also known that \u00aff satisfies some local regularity conditions that enable gradient descent\u2019s linear convergence.\nProof Sketch. First we use known properties of \u00aff = E(Ai,yi)\u223cG0 fi from the literature [Jin+17] to show approximate SOSPs of \u00aff \u2014 in particular our initialization USOSP \u2014 are in a small neighborhood of the global minima of \u00aff. In that neighborhood, it was also known that \u00aff satisfies some local regularity conditions that enable gradient descent\u2019s linear convergence. However, the algorithm only has access to the inexact gradient from the robust mean estimation subroutine (line 3 in Algorithm 3.1) and, therefore, we need to establish linear convergence of inexact gradient descent. We achieve this with the following iterative argument: As the iterate gets closer to global optima, the covariance bound of sample gradient in Equation (6) gets closer to 0. Because the accuracy of the robust mean estimation scales with the covariance bound (see Proposition 2.3), a more accurate estimate of population gradient \u2207\u00aff can be obtained via the robust estimation subroutine (Algorithm A.2). This, in turn, allows for an improved inexact gradient descent step, driving the algorithm towards an iterate that is even closer to global optima. See Appendix C.3 for the complete proof.\nHowever, the algorithm only has access to the inexact gradient from the robust mean estimation subroutine (line 3 in Algorithm 3.1) and, therefore, we need to establish linear convergence of inexact gradient descent. We achieve this with the following iterative argument: As the iterate gets closer to global optima, the covariance bound of sample gradient in Equation (6) gets closer to 0. Because the accuracy of the robust mean estimation scales with the covariance bound (see Proposition 2.3), a more accurate estimate of population gradient \u2207\u00aff can be obtained via the robust estimation subroutine (Algorithm A.2). This, in turn, allows for an improved inexact gradient descent step, driving the algorithm towards an iterate that is even closer to global optima. See Appendix C.3 for the complete proof.\n# 4 Statistical Query Lower Bound for Low Rank Matrix Sensing\nOur general algorithm (Theorem 3.1) leads to an efficient algorithm for robust low rank matrix sensing with sample complexity O(d2r2/\u01eb) (Theorem 3.3). Interestingly, the sample complexity of the underlying robust estimation problem \u2014 ignoring computational considerations \u2014 is \u0398(dr/\u01eb). The information-theoretic upper bound of O(dr/\u01eb) can be achieved by an exponential (in the dimension)\ntime algorithm (generalizing the Tukey median to our regression setting); see, e.g., Theorem 3.5 in [Gao20]. Given this discrepancy, it is natural to ask whether the sample complexity achieved by our algorithm can be improved via a different computationally efficient method. In this section, we provide evidence that this may not be possible. In more detail, we establish a near-optimal informationcomputation tradeoff for the problem, within the class of Statistical Query (SQ) algorithms. To formally state our lower bound, we require basic background on SQ algorithms. Basics on SQ Model. SQ algorithms are a class of algorithms that, instead of access to samples from some distribution P, are allowed to query expectations of bounded functions over P. Definition 4.1 (SQ Algorithms and STAT Oracle [Kea98]). Let P be a distribution on Rd2+1. A Statistical Query (SQ) is a bounded function q : Rd2+1 \u2192[\u22121, 1]. For \u03c4 > 0, the STAT(\u03c4) oracle responds to the query q with a value v such that |v \u2212EX\u223cP[q(X)]| \u2264\u03c4. An SQ algorithm is an algorithm whose objective is to learn some information about an unknown distribution P by making adaptive calls to the corresponding STAT(\u03c4) oracle. In this section, we consider P as the unknown corrupted distribution where (Ai, yi) are drawn. The SQ algorithm tries to learn the ground truth matrix M \u2217from this corrupted distribution; the goal of the lower bound result is to show that this is hard. The SQ model has the capability to implement a diverse set of algorithmic techniques in machine learning such as spectral techniques, moment and tensor methods, local search (e.g., Expectation Maximization), and several others [Fel+17]. A lower bound on the SQ complexity of a problem provides evidence of hardness for the problem. [Bre+21] established that (under certain assumptions) an SQ lower bound also implies a qualitatively similar lower bound in the low-degree polynomial testing model. This connection can be used to show a similar lower bound for low-degree polynomials. Our main result here is a near-optimal SQ lower bound for robust low rank matrix sensing that applies even for rank r = 1, i.e., when the ground truth matrix is M \u2217= uu\u22a4for some u \u2208Rd. The choice of rank r = 1 yields the strongest possible lower bound in our setting because it is the easiest parameter regime: Recall that the sample complexity of our algorithm is \ufffdO(d2r2) as in Theorems 3.2 and 3.3, and the main message of our SQ lower bound is to provide evidence that the d2 factor is necessary for computationally efficient algorithms even if r = 1. Theorem 4.2 (SQ Lower Bound for Robust Rank-One Matrix Sensing). Let \u01eb \u2208(0, 1/2) be the fraction of corruptions and let c \u2208(0, 1/2). Assume the dimension d \u2208N is sufficiently large. Consider the \u01eb-corrupted rank-one matrix sensing problem with ground-truth matrix M \u2217= uu\u22a4 and noise \u03c32 = O(1). Any SQ algorithm that outputs \ufffdu with \u2225\ufffdu \u2212u\u2225= O(\u01eb1/4) either requires 2\u2126(dc)/d2\u22124c queries or makes at least one query to STAT \ufffd eO(1/\u221a\u01eb)/O \ufffd d1\u22122c\ufffd\ufffd . In other words, we show that, when provided with SQ access to an \u01eb-corrupted distribution, approximating u is impossible unless employing a statistical query of higher precision than what can be achieved with a strictly sub-quadratic number (e.g., d1.99) of samples. Note that the SQ oracle STAT(eO(1/\u221a\u01eb)/O(d1\u22122c)) can be simulated with O(d2\u22124c)/eO(1/\u01eb) samples, and this bound is tight in general. Informally speaking, this theorem implies that improving the sample complexity from d2 to d2\u22124c requires exponentially many queries. This result can be viewed as a near-optimal information-computation tradeoff for the problem, within the class of SQ algorithms. The proof follows a similar analysis as in [DKS19; Dia+21], using one-dimensional moment matching to construct a family of corrupted conditional distributions, which induce a family of corrupted joint distributions that are SQ-hard to learn. We provide the details of the proof in Appendix D. Apart from the formal proof, in Appendix E we also informally discuss the intuition for why some simple algorithms that require O(d) samples do not provide dimension-independent error guarantees.\n# Acknowledgements\nShuyao Li was supported in part by NSF Awards DMS-2023239, NSF Award CCF-2007757 and the U. S. Office of Naval Research under award number N00014-22-1-2348. Yu Cheng was supported in\npart by NSF Award CCF-2307106. Ilias Diakonikolas was supported in part by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), a Sloan Research Fellowship, and a DARPA Learning with Less Labels (LwLL) grant. Jelena Diakonikolas was supported in part by NSF Award CCF-2007757 and by the U. S. Office of Naval Research under award number N00014-22-1-2348. Rong Ge was supported in part by NSF Award DMS-2031849, CCF-1845171 (CAREER) and a Sloan Research Fellowship. Stephen Wright was supported in part by NSF Awards DMS-2023239 and CCF-2224213 and AFOSR via subcontract UTA20-001224 from UT-Austin.\n# References\n[Bar+10] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. \u201cThe security of machine learning\u201d. In: Machine Learning 81.2 (2010), pp. 121\u2013148. [BBV16] A. S. Bandeira, N. Boumal, and V. Voroninski. \u201cOn the low-rank approach for semidefinite programs arising in synchronization and community detection\u201d. In: Conference on learning theory. PMLR. 2016, pp. 361\u2013382. [Ber+22] E. H. Bergou, Y. Diouane, V. Kunc, V. Kungurtsev, and C. W. Royer. \u201cA subsampling line-search method with second-order results\u201d. In: INFORMS Journal on Optimization 4.4 (2022), pp. 403\u2013425. [BNL12] B. Biggio, B. Nelson, and P. Laskov. \u201cPoisoning Attacks against Support Vector Machines\u201d. In: Proceedings of the 29th International Coference on International Conference on Machine Learning. Omnipress, 2012, pp. 1467\u20131474. [BNS16] S. Bhojanapalli, B. Neyshabur, and N. Srebro. \u201cGlobal optimality of local search for low rank matrix recovery\u201d. In: Advances in Neural Information Processing Systems. 2016, pp. 3873\u20133881. [Bre+21] M. S. Brennan, G. Bresler, S. Hopkins, J. Li, and T. Schramm. \u201cStatistical Query Algorithms and Low Degree Tests Are Almost Equivalent\u201d. In: Proceedings of Thirty Fourth Conference on Learning Theory. Ed. by M. Belkin and S. Kpotufe. Vol. 134. Proceedings of Machine Learning Research. PMLR, Aug. 2021, pp. 774\u2013774. [CDG19] Y. Cheng, I. Diakonikolas, and R. Ge. \u201cHigh-Dimensional Robust Mean Estimation in Nearly-Linear Time\u201d. In: Proceedings of the 30th ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 2019, pp. 2755\u20132771. [CG18] Y. Cheng and R. Ge. \u201cNon-convex matrix completion against a semi-random adversary\u201d. In: Conference On Learning Theory. PMLR. 2018, pp. 1362\u20131394. [DHL19] Y. Dong, S. Hopkins, and J. Li. \u201cQuantum entropy scoring for fast robust mean estimation and improved outlier detection\u201d. In: Advances in Neural Information Processing Systems 32 (2019). [Dia+16] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. \u201cRobust estimators in high dimensions without the computational intractability\u201d. In: 57th Annual IEEE Symposium on Foundations of Computer Science\u2014FOCS 2016. IEEE Computer Soc., Los Alamitos, CA, 2016, pp. 655\u2013664. [Dia+17] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. \u201cBeing Robust (in High Dimensions) Can Be Practical\u201d. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by D. Precup and Y. W. Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, Aug. 2017, pp. 999\u20131008. [Dia+19] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart. \u201cSever: A Robust Meta-Algorithm for Stochastic Optimization\u201d. In: Proceedings of the 36th International Conference on Machine Learning. Ed. by K. Chaudhuri and R. Salakhutdinov. Vol. 97. Proceedings of Machine Learning Research. PMLR, June 2019, pp. 1596\u2013 1606. [Dia+21] I. Diakonikolas, D. Kane, A. Pensia, T. Pittas, and A. Stewart. \u201cStatistical Query Lower Bounds for List-Decodable Linear Regression\u201d. In: Advances in Neural Information Processing Systems. Ed. by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan. Vol. 34. Curran Associates, Inc., 2021, pp. 3191\u20133204. [Dia+22] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. \u201cStreaming Algorithms for HighDimensional Robust Statistics\u201d. In: Proceedings of the 39th International Conference on Machine Learning. Ed. by K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato. Vol. 162. Proceedings of Machine Learning Research. PMLR, July 2022, pp. 5061\u20135117. [DK19] I. Diakonikolas and D. M. Kane. \u201cRecent advances in algorithmic high-dimensional robust statistics\u201d. In: arXiv preprint arXiv:1911.05911 (2019). [DK23] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust Statistics. Cambridge University Press, 2023.\n[DKP20] I. Diakonikolas, D. M. Kane, and A. Pensia. \u201cOutlier Robust Mean Estimation with Subgaussian Rates via Stability\u201d. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 1830\u20131840. [DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. \u201cStatistical Query Lower Bounds for Robust Estimation of High-Dimensional Gaussians and Gaussian Mixtures\u201d. In: 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS). 2017, pp. 73\u201384. [DKS19] I. Diakonikolas, W. Kong, and A. Stewart. \u201cEfficient algorithms and lower bounds for robust linear regression\u201d. In: Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, Philadelphia, PA, 2019, pp. 2745\u20132754. [Dur19] R. Durrett. Probability\u2014theory and examples. Vol. 49. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 2019. [Fel+17] V. Feldman, E. Grigorescu, L. Reyzin, S. Vempala, and Y. Xiao. \u201cStatistical Algorithms and a Lower Bound for Detecting Planted Cliques\u201d. In: J. ACM 64.2 (2017), 8:1\u20138:37. [Gao20] C. Gao. \u201cRobust regression via mutivariate regression depth\u201d. In: Bernoulli 26.2 (2020), pp. 1139\u20131170. [GC23] X. Gao and Y. Cheng. \u201cRobust Matrix Sensing in the Semi-Random Model\u201d. In: Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS) (2023). [Ge+15] R. Ge, F. Huang, C. Jin, and Y. Yuan. \u201cEscaping from saddle points\u2014online stochastic gradient for tensor decomposition\u201d. In: Conference on Learning Theory. 2015, pp. 797\u2013 842. [GJZ17] R. Ge, C. Jin, and Y. Zheng. \u201cNo Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis\u201d. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by D. Precup and Y. W. Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, Aug. 2017, pp. 1233\u20131242. [GLM16] R. Ge, J. D. Lee, and T. Ma. \u201cMatrix completion has no spurious local minimum\u201d. In: Advances in Neural Information Processing Systems. 2016, pp. 2973\u20132981. [Hal10] J. K. Hale. Asymptotic behavior of dissipative systems. 25. American Mathematical Soc., 2010. [Ham+86] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust statistics. The approach based on influence functions. Wiley New York, 1986. [HR09] P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley New York, 2009. [Hub64] P. J. Huber. \u201cRobust estimation of a location parameter\u201d. In: Annals of Mathematical Statistics 35 (1964), pp. 73\u2013101. [IPL23] E. Ioannou, M. S. Pydi, and P.-L. Loh. \u201cRobust empirical risk minimization via Newton\u2019s method\u201d. In: arXiv preprint arXiv:2301.13192 (2023). [Jin+17] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. \u201cHow to Escape Saddle Points Efficiently\u201d. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by D. Precup and Y. W. Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, Aug. 2017, pp. 1724\u20131732. [Jin+21] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. \u201cOn nonconvex optimization for machine learning: gradients, stochasticity, and saddle points\u201d. In: Journal of the ACM 68.2 (2021), Art. 11, 29. [Kea98] M. J. Kearns. \u201cEfficient noise-tolerant Learning from Statistical Queries\u201d. In: Journal of the ACM 45.6 (1998), pp. 983\u20131006. [Li+08] J. Li, D. Absher, H. Tang, A. Southwick, A. Casto, S. Ramachandran, H. Cann, G. Barsh, M. Feldman, L. Cavalli-Sforza, and R. Myers. \u201cWorldwide human relationships inferred from genome-wide patterns of variation\u201d. In: Science 319 (2008), pp. 1100\u2013 1104. [Li+20a] X. Li, Z. Zhu, A. Man-Cho So, and R. Vidal. \u201cNonconvex robust low-rank matrix recovery\u201d. In: SIAM Journal on Optimization 30.1 (2020), pp. 660\u2013686. [Li+20b] Y. Li, Y. Chi, H. Zhang, and Y. Liang. \u201cNon-convex low-rank matrix recovery with arbitrary outliers via median-truncated gradient descent\u201d. In: Information and Inference: A Journal of the IMA 9.2 (2020), pp. 289\u2013325.\n[LRV16] K. A. Lai, A. B. Rao, and S. Vempala. \u201cAgnostic Estimation of Mean and Covariance\u201d. In: focs2016. 2016, pp. 665\u2013674. [LW23] S. Li and S. J. Wright. \u201cA randomized algorithm for nonconvex minimization with inexact evaluations and complexity guarantees\u201d. In: arXiv preprint arXiv:2310.18841 (2023). [Pas+10] P. Paschou, J. Lewis, A. Javed, and P. Drineas. \u201cAncestry Informative Markers for FineScale Individual Assignment to Worldwide Populations\u201d. In: Journal of Medical Genetics 47 (2010), pp. 835\u2013847. [Pra+20] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. \u201cRobust estimation via robust gradient estimation\u201d. In: Journal of the Royal Statistical Society. Series B. Statistical Methodology 82.3 (2020), pp. 601\u2013627. [Ros+02] N. Rosenberg, J. Pritchard, J. Weber, H. Cann, K. Kidd, L. Zhivotovsky, and M. Feldman. \u201cGenetic structure of human populations\u201d. In: Science 298 (2002), pp. 2381\u2013 2385. [RRT17] M. Raginsky, A. Rakhlin, and M. Telgarsky. \u201cNon-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis\u201d. In: Proceedings of the 2017 Conference on Learning Theory. Ed. by S. Kale and O. Shamir. Vol. 65. Proceedings of Machine Learning Research. PMLR, July 2017, pp. 1674\u20131703. [SKL17] J. Steinhardt, P. W. Koh, and P. S. Liang. \u201cCertified Defenses for Data Poisoning Attacks\u201d. In: Advances in Neural Information Processing Systems 30. 2017, pp. 3520\u2013 3532. [SQW16] J. Sun, Q. Qu, and J. Wright. \u201cA geometric analysis of phase retrieval\u201d. In: Information Theory (ISIT), 2016 IEEE International Symposium on. IEEE. 2016, pp. 2379\u20132383. [SQW17] J. Sun, Q. Qu, and J. Wright. \u201cComplete Dictionary Recovery Over the Sphere I: Overview and the Geometric Picture\u201d. In: IEEE Trans. Inf. Theor. 63.2 (Feb. 2017), pp. 853\u2013884. [Tuk75] J. Tukey. \u201cMathematics and picturing of data\u201d. In: Proceedings of ICM. Vol. 6. 1975, pp. 523\u2013531. [WM22] J. Wright and Y. Ma. High-dimensional data analysis with low-dimensional models: Principles, computation, and applications. Cambridge University Press, 2022. [Yin+19] D. Yin, Y. Chen, R. Kannan, and P. Bartlett. \u201cDefending Against Saddle Point Attack in Byzantine-Robust Distributed Learning\u201d. In: Proceedings of the 36th International Conference on Machine Learning. Ed. by K. Chaudhuri and R. Salakhutdinov. Vol. 97. Proceedings of Machine Learning Research. PMLR, June 2019, pp. 7074\u20137084.\n# Supplementary Material\nSupplementary material is organized as follows. In Appendix A, we provide useful auxiliary facts and relevant technical results from previous works. Appendix B proves our result for general robust nonconvex optimization (Theorem 3.1). Appendix C provides omitted computation and proofs for robust low rank matrix sensing (Section 3.1). Appendix D proves our SQ lower bound (Section 4) for the sample complexity of efficient algorithms for the outlier-robust low rank matrix sensing problem, and Appendix E discusses the intuition why some simple algorithms that violate our SQ lower bound fail.\n# A Technical Preliminaries\n# A.1 Notation and Auxiliary Facts\nIn the appendix, we use \ufffdO(\u00b7) notation to suppress, for conciseness, logarithmic dependences on all defined quantities, even if they do not appear inside \ufffdO(\u00b7). More precise statements can be found in the corresponding main parts of the paper. For matrix space Rd\u00d7r and a set X \u22c6\u2282Rd\u00d7r, we use PX \u22c6(\u00b7) to denote the Frobenius projection onto X \u22c6, i.e., PX \u22c6(U) = arg minZ\u2208X \u22c6\u2225U \u2212Z\u22252 F . We use dist(U, X \u22c6) to denote \u2225U \u2212PX \u22c6(U)\u2225F . Fact A.1. For matrices A, B with compatible dimensions,\n\u2225AB\u2225F \u2264\u2225A\u2225op \u2225B\u2225F \u2225AB\u2225F \u2264\u2225A\u2225F \u2225B\u2225op\nFor two matrices A and B, let A \u2297B denote the Kronecker product of A and B. Fact A.2. For matrices A, B, C with compatible dimensions, vec(ABC) = \ufffd C\u22a4\u2297A \ufffd vec(B). Fact A.3. \u2225A \u2297B\u2225op = \u2225A\u2225op \u2225B\u2225op\nWe will frequently use the following fact about the mean and the variance of the quadratic form for zero-mean multivariate normal distributions. Fact A.4. Let X \u223cN(0, \u03a3) be a k-dimensional random variable and let G be a k \u00d7 k real matrix; then\n<div style=\"text-align: center;\">A.2 Simplified Proof for the Optimization Algorithm with Inexact Gradients and Hessians</div>\nAlgorithm A.1: Nonconvex minimization with inexact gradients and Hessians [LW23]\n1 for t = 1, 2, . . . do\n2\nObtain \ufffdgt from the inexact gradient oracle\n3\nif \u2225\ufffdgt\u2225> \u01ebg then\n4\nxt+1 = xt \u22121\nL\ufffdgt\n5\nelse\n6\nObtain \ufffdHt from the inexact Hessian oracle\n7\nCompute smallest eigenvalue and its corresponding eigenvector (\ufffd\u03bbt, \ufffdpt)\n8\nif \ufffd\u03bbt < \u2212\u01ebH then\n9\n\u03c3t = \u00b11 with probability 1\n2\n10\nxt+1 = xt + 2\u01ebH\nLH \u03c3t\ufffdpt\n11\nelse\n12\nreturn xt\nWe stated the following guarantee (Proposition 2.2) in Section 2:\nProposition A.5. Suppose a function f is bounded below by f \u2217> \u2212\u221e, has Lg-Lipschitz gradient and LH-Lipschitz Hessian, and its inexact gradient and Hessian computation \ufffdgt and \ufffdHt satisfy \u2225\ufffdgt \u2212\u2207f(xt)\u2225\u2264 1 3\u01ebg and \u2225\ufffdHt \u2212\u22072f(xt)\u2225op \u2264 2 9\u01ebH. Then there exists an algorithm (Algorithm A.1) with the following guarantees: 1. (Correctness) If Algorithm A.1 terminates and outputs xn, then xn is a ( 4 3\u01ebg, 4 3\u01ebH)-approximate second-order stationary point. 2. (Runtime) Algorithm A.1 terminates with probability 1. Let C\u01eb := min \ufffd\u01eb2 g 6Lg , 2\u01eb3 H 9L2 H \ufffd . With probability at least 1 \u2212\u03b4, Algorithm A.1 terminates after k iterations for\nProposition A.5. Suppose a function f is bounded below by f \u2217> \u2212\u221e, has Lg-Lipschitz gradient and LH-Lipschitz Hessian, and its inexact gradient and Hessian computation \ufffdgt and \ufffdHt satisfy \u2225\ufffdgt \u2212\u2207f(xt)\u2225\u2264 1 3\u01ebg and \u2225\ufffdHt \u2212\u22072f(xt)\u2225op \u2264 2 9\u01ebH. Then there exists an algorithm (Algorithm A.1) with the following guarantees:\n\ufffd \ufffd 1. (Correctness) If Algorithm A.1 terminates and outputs xn, then xn is a ( 4 3\u01ebg, 4 3\u01ebH)-approximat second-order stationary point.\n2. (Runtime) Algorithm A.1 terminates with probability 1. Let C\u01eb := min \ufffd\u01eb g 6Lg , 2\u01eb H 9L2 H \ufffd . With proba bility at least 1 \u2212\u03b4, Algorithm A.1 terminates after k iterations for\nThis section proves the correctness and a slightly weaker runtime guarantee in terms of the dependence on \u03b4. We prove a O(1/\u03b4) dependence instead of O(log(1/\u03b4)), i.e., with probability at least 1 \u2212\u03b4, Algorithm A.1 terminates after k iterations for\nProof of correctness and a weaker runtime [LW23]. We prove the correctness first. From the stopping criteria, we have \u2225\ufffdgk\u2225\u2264\u01ebg and \ufffd\u03bbk \u2265\u2212\u01ebH. Thus,\nTo bound \u03bbmin(\u22072f(xk)), write Uk = \ufffdHk \u2212\u22072f(xk). By Hessian inexactness condition, \u03bbmin(\u2212Uk) \u2265\u2212\u2225\u2212Uk\u2225op \u2265\u22122 9\u01ebH.\n  We use Weyl\u2019s theorem to conclude that\nas required.\nNow we analyze the runtime. We proceed by first establishing an expectation bound and then use Markov\u2019s inequality. Write vt := \ufffdgt \u2212\u2207f(xt). When the algorithm takes a gradient step, we have \u2225\ufffdgt\u2225> \u01ebg. By gradient inexactness condition, we have \u2225vt\u2225\u22641 3\u2225\ufffdgt\u2225, so that \u2225\u2207f(xt)\u2225\u22652 3\u2225\ufffdgt\u2225. From Taylor\u2019s Theorem, we have \ufffd \ufffd\n16\n(8)\n(9)\nFor the negative curvature step, recall that \u2225\ufffdHt \u2212\u22072f(xt)\u2225op \u2264 2 9\u01ebH. It follows from Taylor\u2019 theorem that\n\ufffd \ufffd \ufffd When the algorithm takes a negative curvature step, we have \ufffd\u03bbt < \u2212\u01ebH < 0, so by Hessian inexactness condition, we have \u2225\ufffdHt \u2212\u22072f(xt)\u2225\u22642 9\u01ebH \u22642 9|\ufffd\u03bbt|. It follows from the definition of operator norm and Cauchy-Schwarz that |\ufffdp\u22a4 t \ufffdHt\ufffdpt \u2212\ufffdp\u22a4 t \u22072f(xt)\ufffdpt|\u2264\u2225\ufffdHt\ufffdpt \u2212\u22072f(xt)\ufffdpt\u2225\u22642|\ufffd\u03bbt| 9 , so\nWe thus have\n\ufffd We have the following result for expected stopping time of Algorithm A.1. Here the expectation is taken with respect to the random variables \u03c3t used at the negative curvature iterations. For purposes of this and later results, we define\nAssuming Lemma A.6, the runtime guarantee in Proposition A.5 follows from Markov inequality. Lemma A.6. Consider the same setting as in Proposition A.5. Let T denote the iteration at which Algorithm A.1 terminates. Then T < \u221ealmost surely and\nAssuming Lemma A.6, the runtime guarantee in Proposition A.5 follows from Markov inequality.\nLemma A.6. Consider the same setting as in Proposition A.5. Let T denote the iteration at which Algorithm A.1 terminates. Then T < \u221ealmost surely and\nwhere C\u01eb is defined in (11).\nThe proof of this result is given below. It constructs a supermartingale1 based on the function value and uses a supermartingale convergence theorem and optional stopping theorem to obtain the final result. A similar proof technique is used in [Ber+22] but for a line-search algorithm. We collect several relevant facts about supermartingales before proving the result. First, we need to ensure the relevant supermartingale is well defined even after the algorithm terminates, so that it is possible to let the index t of the supermartingale go to \u221e. Fact A.7 ([Dur19, Theorem 4.2.9]). If T is a stopping time and Xt is a supermartingale, then Xmin(T,t) is a supermartingale. The following supermatingale convergence theorem will be used to ensure the function value converges, so that the algorithm terminates with probability 1.\n1A supermartingale with respect to filtration {G1, G2, . . . } is a sequence of random variables {Y1, Y2, . . . } such that for all k \u2208Z+, (i) E |Yt| < \u221e, (ii) Yt is Gt-measurable, and (iii) E(Yt+1 |Gt) \u2264Yt.\n(10)\n(11)\n(12)\nFact A.8 (Supermartingale Convergence Theorem, [Dur19, Theorem 4.2.12]). If Xt \u22650 is a supermartingale, then as t \u2192\u221e, there exists a random variable X such that Xt \u2192X a.s. and E X \u2264E X0. Finally, we will use the optional stopping theorem to derive the expected iteration complexity. Note that we use a version of the optional stopping theorem specific to nonnegative supermartingales that does not require uniform integrability. Fact A.9 (Optional Stopping Theorem, [Dur19, Theorem 4.8.4]). If Xn is a nonnegative supermartingale and N \u2264\u221eis a stopping time, then E X0 \u2265E XN. Proof of Lemma A.6. We first construct a supermartingale based on function values. Since E [\u03c3t] = 0, linearity of expectation implies that E \ufffd 2 \u03b1t M \u2207f(xt)\u22a4\u03c3t\ufffdpt \ufffd\ufffdxt \ufffd = 0. We therefore have from (10) that\nE [f(xt+1)|xt] \u2264f(xt) \u2212min \ufffd \u01eb2 g 6Lg , 2\u01eb3 H 9L2 H \ufffd = f(xt) \u2212C\u01eb.\nConsider the stochastic process Mt := f(xt) + tC\u01eb. We have E [Mt+1|xt] = E [f(xt+1) + (t + 1)C\u01eb|xt] \u2264E [f(xt) \u2212C\u01eb + (t + 1)C\u01eb|xt] = E [f(xt) + tC\u01eb|xt] = Mt.\nWe need to select a filtration to define the supermartingale Mt. We view xt as random variables defined with respect to \u03c3i, i \u2264k. Since Mt is expressed as a function of xt only, we define the filtration {Gt} to be the filtration generated by xt, and it naturally holds that Mt is Gt-measurable for all k and E [Mt+1|Gt] = E [Mt+1|xt]. Hence, {Mt} is a supermartingale with respect to filtration {Gt}. Let T denote the iteration at which our algorithm stops. Since the decision to stop at iteration t depends only on xt, we have {T = t} \u2208Gt, which implies T is a stopping time. We will use the supermartingale convergence theorem (Fact A.8) to show that T < +\u221ealmost surely, since the function value cannot decrease indefinitely as it is bounded by f \u2217from below. To apply Fact A.8, we need to let t \u2192\u221e, so we need to transform {Mt} to obtain a supermartingale {Yt}that is well defined even after the algorithm terminates. It follows from Fact A.7 that Yt := Mmin(t,T ) is also a supermartingale. Since Yt \u2265f \u2217, it follows from the supermartingale convergence theorem (Fact A.8) applied to Yt \u2212f \u2217that Yt \u2192Y\u221ealmost surely for some random variable Y\u221ewith E Y\u221e\u2264E Y0 = E M0 = f(x0) < \u221e. Hence P[Y\u221e= +\u221e] = 0. On the other hand, as t \u2192\u221e, we have tC\u01eb \u2192\u221e, so T = +\u221e =\u21d2 Yt = Mt \u2265 f \u2217+ tC\u01eb \u2192\u221e=\u21d2Y\u221e= +\u221e. Therefore we have P[T < +\u221e] = 1. We can then apply the optional stopping theorem (Fact A.9) to Yt \u2212f \u2217. It follows that f \u2217+ E T \u00b7 C\u01eb \u2264E f(xT ) + E T \u00b7 C\u01eb = E [MT ] = E [YT ] \u2264E[Y0] = E [M0] = f(x0), where the first equality uses T < +\u221ealmost surely and the last inequality is Fact A.9. By reorganizing this bound, we obtain E T \u2264f(x0)\u2212f \u2217 C\u01eb , as desired.\nA class of algorithms that robustly estimate quantities in the presence of outliers under strong contamination model (Definition 1.1) are based on the stability of samples: Definition A.10 (Definition of Stability [Dia+17]). Fix 0 < \u01eb < 1/2 and \u03b4 > \u01eb. A finite set S \u2282Rk is (\u01eb, \u03b4)-stable with respect to mean \u00b5 \u2208Rk and \u03c32 if for every S\u2032 \u2282S with |S\u2032|\u2265(1 \u2212\u01eb)|S|, the following conditions hold: (i) \u2225\u00b5S\u2032 \u2212\u00b5\u2225\u2264\u03c3\u03b4, and (ii) \ufffd\ufffd\u00af\u03a3S\u2032 \u2212\u03c32I \ufffd\ufffd op \u2264\u03c32\u03b42\u01eb, where \u00b5S\u2032 and \u00af\u03a3S\u2032 denote the empirical mean and empirical covariance over the set S\u2032 respectively.\n\nAlgorithm A.2: RobustMeanEstimation with unknown covariance bound\nData: 0 < \u01eb < 1/2 and T is an \u01eb-corrupted set\nResult: \ufffd\u00b5 with \u2225\ufffd\u00b5 \u2212\u00b5S\u2225= O(\n\ufffd\n\u2225\u03a3\u2225op \u01eb)\n1 Initialize a weight function w : T \u2192R\u22650 with w(x) = 1/|T | for all x \u2208T ;\n2 while \u2225w\u22251 \u22651 \u22122\u01eb do\n3\n\u00b5(w) :=\n1\n\u2225w\u22251\n\ufffd\nx\u2208T w(x)x;\n4\n\u03a3(w) :=\n1\n\u2225w\u22251\n\ufffd\nx\u2208T w(x)(x \u2212\u00b5(w))(x \u2212\u00b5(w))\u22a4;\n5\nCompute the largest eigenvector v of \u03a3(w) ;\n6\ng(x) := |v\u22a4(x \u2212\u00b5(w))|2;\n7\nFind the largest threshold t such that \u03a3x\u2208T :g(x)\u2265tw(x) \u2265\u01eb;\n8\nf(x) := g(x)1{g(x) \u2265t} ;\n9\nw(x) \u2190w(x)\n\ufffd\n1 \u2212\nf(x)\nmaxy\u2208T :w(y)\u0338=0 f(y)\n\ufffd\n;\n10 return \u00b5(w)\nOn input the corrupted version of a stable set, Algorithm A.2 has the following guarantee. Proposition A.11 (Robust Mean Estimation with Stability [DKP20, Theorem A.3]). Let T \u2282Rk be an \u01eb-corrupted version of a set S, where S is (C\u01eb, \u03b4)-stable with respect to \u00b5S and \u03c32, and where C > 0 is a sufficiently large constant. Algorithm A.2 on input \u01eb and T (but not \u03c3 or \u03b4) returns (deterministically) a vector \ufffd\u00b5 in polynomial time so that \u2225\u00b5S \u2212\ufffd\u00b5\u2225= O(\u03c3\u03b4). Proposition A.11 requires the uncorrupted samples to be stable. With a large enough sample size, most independent and identically distributed samples from a bounded covariance distribution are stable. The remaining samples can be treated as corruptions. Proposition A.12 (Sample Complexity for Stability [DKP20, Theorem 1.4]). Fix any 0 < \u03be < 1. Let S be a multiset of n independent and identically distributed samples from a distribution on Rk with mean \u00b5 and covariance \u03a3. Then, with probability at least 1 \u2212\u03be, there exists a sample size n = O \ufffd k log k+log(1/\u03be) \u01eb \ufffd and a subset S\u2032 \u2286S such that |S\u2032| \u2265(1 \u2212\u01eb) n and S\u2032 is (2\u01eb, \u03b4)-stable with respect to \u00b5 and \u2225\u03a3\u2225op, where \u03b4 = O(\u221a\u01eb).\nOn input the corrupted version of a stable set, Algorithm A.2 has the following guarantee. Proposition A.11 (Robust Mean Estimation with Stability [DKP20, Theorem A.3]). Let T \u2282Rk be an \u01eb-corrupted version of a set S, where S is (C\u01eb, \u03b4)-stable with respect to \u00b5S and \u03c32, and where C > 0 is a sufficiently large constant. Algorithm A.2 on input \u01eb and T (but not \u03c3 or \u03b4) returns (deterministically) a vector \ufffd\u00b5 in polynomial time so that \u2225\u00b5S \u2212\ufffd\u00b5\u2225= O(\u03c3\u03b4). Proposition A.11 requires the uncorrupted samples to be stable. With a large enough sample size, most independent and identically distributed samples from a bounded covariance distribution are stable. The remaining samples can be treated as corruptions. Proposition A.12 (Sample Complexity for Stability [DKP20, Theorem 1.4]). Fix any 0 < \u03be < 1. Let S be a multiset of n independent and identically distributed samples from a distribution on Rk with mean \u00b5 and covariance \u03a3. Then, with probability at least 1 \u2212\u03be, there exists a sample size n = O \ufffd k log k+log(1/\u03be) \u01eb \ufffd and a subset S\u2032 \u2286S such that |S\u2032| \u2265(1 \u2212\u01eb) n and S\u2032 is (2\u01eb, \u03b4)-stable with respect to \u00b5 and \u2225\u03a3\u2225op, where \u03b4 = O(\u221a\u01eb). Proof of Proposition 2.3. Proposition A.12 implies that for i.i.d. samples from a k-dimensional bounded covariance distribution to contain a stable set of more than (1 \u2212\u01eb)-fraction of samples with high probability, we need \ufffdO \ufffdk \u01eb \ufffd samples. We refer to the remaining \u01eb-fraction of samples as unstable samples. Since the adversary corrupts an \u01eb-fraction of clean samples S, the input set T can be considered as a 2\u01eb-corrupted version of a stable set, if we view the unstable samples as corruptions. Therefore, Proposition A.11 applies and gives the desired error guarantee.\nProof of Proposition 2.3. Proposition A.12 implies that for i.i.d. samples from a k-dimensional bounded covariance distribution to contain a stable set of more than (1 \u2212\u01eb)-fraction of samples with high probability, we need \ufffdO \ufffdk \u01eb \ufffd samples. We refer to the remaining \u01eb-fraction of samples as unstable samples. Since the adversary corrupts an \u01eb-fraction of clean samples S, the input set T can be considered as a 2\u01eb-corrupted version of a stable set, if we view the unstable samples as corruptions. Therefore, Proposition A.11 applies and gives the desired error guarantee.\n# B Omitted Proofs in General Robust Nonconvex Optimization\nSample Size for Successful Robust Mean Estimation For each iteration t in Algorithm A.1 and Algorithm 3.1, we would like to robustly estimate the mean of a set of corrupted points {vec (\u2207f(xt))}n i=1 and/or {vec (\u22072f(xt))}n i=1 where the inliers are drawn from a distribution with bounded covariance \u03a3 \u2aaf\u03c32I. For fixed xt, inliers are drawn independently and identically distributed (i.i.d.), but the dependence across all iterations t is allowed. Theorem B.1. Let X \u2282Rl be a closed and bounded set with radius at most \u03b3 (i.e., \u2225x\u2225\u2264\u03b3, \u2200x \u2208 X). Let p\u2217be a distribution over functions h : X \u2192Rk. Let \u03be \u2208(0, 1) be the failure probability. Suppose h is L-Lipschitz and uniformly bounded, i.e., there exists B > 0 such that \u2225h(x)\u2225\u2264 B \u2200x \u2208X almost surely. Assume further that for each x \u2208X we have \u2225Covh\u223cp\u2217(h(x))\u2225op \u2264\u03c32. Let S be a multiset of n i.i.d. samples {hi}n i=1 from p\u2217. Then there exists a sample size \ufffd \ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4b82/4b82e2a4-a972-4e7a-b1d1-9947792dcc95.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">19</div>\nsuch that with probability 1 \u2212\u03be, it holds that for each x \u2208X, there exists a subset S\u2032 \u2282S with |S\u2032|\u2265(1 \u22122\u01eb)n (potentially different subsets S\u2032 for different x) such that {hi(x)}i\u2208S\u2032 is (2\u01eb, \u03b4)stable with respect to \u00b5 and \u03c32, where \u03b4 = O(\u221a\u01eb). To prove this theorem, we work with an easier version of stability condition specialized to samples from a bounded covariance distribution. Claim B.2 (Claim 2.1 in [DKP20]). (Stability for bounded covariance) Let R \u2282Rk be a finite multiset such that \u2225\u00b5R \u2212\u00b5\u2225\u2264\u03c3\u03b4, and \ufffd\ufffd\u00af\u03a3R \u2212\u03c32I \ufffd\ufffd op \u2264\u03c32\u03b42/\u01eb for some 0 \u2264\u01eb \u2264\u03b4. Then R is (\u0398(\u01eb), \u03b4\u2032) stable with respect to \u00b5 and \u03c32, where \u03b4\u2032 = O(\u03b4 + \u221a\u01eb). Proof of Theorem B.1. Given Claim B.2, it suffices to show that with probability 1 \u2212\u03be, for each x \u2208X there exists a subset S\u2032 \u2282S with |S\u2032|\u2265(1 \u22122\u01eb)n such that \ufffd \ufffd\nProof of Theorem B.1. Given Claim B.2, it suffices to show that with probability 1 \u2212\u03be, for each x \u2208X there exists a subset S\u2032 \u2282S with |S\u2032|\u2265(1 \u22122\u01eb)n such that \ufffd \ufffd\n\ufffd\ufffd \ufffd\ufffd By Proposition A.12, for each x \u2208X, with probability 1 \u2212 \ufffd \u03be\u03b3BL \u03c3\u221a\u01eb \ufffdl \u22651 \u2212\u03be/ \ufffd \u03b3BL \u03c3\u221a\u01eb \ufffdl , there exists a subset S\u2032 \u2282S with |S\u2032|\u2265(1 \u22122\u01eb)n such that (13) and (14) hold. We proceed with a net argument. Up to a multiplicative error that can be suppressed by O(\u00b7), if Equation (13) holds for some x \u2208X, it also holds for all other x\u2032 in a ball of radius \u03c3\u03b4/L because h(\u00b7) is L-Lipschitz. Similarly, (14) is equivalent to\n\ufffd \ufffd Since h(\u00b7) is L-Lipschitz and uniformly bounded by B, if (14) holds for some x \u2208X, it also holds for all x\u2032 in a ball of radius \u03c32\u03b42 \u01ebBL . Therefore, it suffices for Equation (13) and (14) to hold for a \u03c4-net of X, where for \u03b4 = O(\u221a\u01eb) we have \u03c4 = min \ufffd \u03c3\u03b4 L , \u03c32\u03b42 \u01ebBL \ufffd = \u2126 \ufffd \u03c3\u221a\u01eb BL \ufffd . An \u2126 \ufffd \u03c3\u221a\u01eb BL \ufffd -ne\n(II) For an absolute constant c > 0, it holds that \u03c3g \u221a\u01eb \u2264c\u01ebg and \u03c3H\nMoreover, there exists an absolute constant Cest such that for each iteration t, the gradient oracle \ufffdgt and Hessian oracle \ufffdHt satisfy \u2225\ufffdgt \u2212\u2207f(xt)\u2225\u2264Cest\u03c3g \u221a\u01eb and \u2225\ufffdHt \u2212\u22072f(xt)\u2225F \u2264Cest\u03c3H \u221a\u01eb. And we recall that the we construct the gradient and Hessian oracles in Algorithm A.1 in the following way:\n\ufffd If we ignore that the dependence between {\u2207fi(xt)} introduced via xt, this theorem follows directl from Proposition 2.3. Here we fix this technicality via a union bound over all xt with Theorem B.1\n(13) (14)\n(14)\nProof. By Proposition 2.2, it suffices to check that gradient and Hessian inexactness condition is satisfied. That is, for all iterations t, it holds that\n \ufffd Here we use the fact that \u2225vec( \ufffdHt) \u2212vec(\u22072f(xt))\u2225= \u2225\ufffdHt \u2212\u22072f",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of finding approximate second-order stationary points (SOSP) in stochastic nonconvex optimization, particularly in the presence of outliers, which limits the effectiveness of existing algorithms.",
        "problem": {
            "definition": "The problem involves finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted.",
            "key obstacle": "Existing nonconvex algorithms struggle to provide guarantees in adversarial settings, particularly when outliers are present."
        },
        "idea": {
            "intuition": "The idea stems from the observation that traditional methods fail to handle the presence of outliers effectively, necessitating a new approach.",
            "opinion": "The proposed idea involves developing a framework that can efficiently find an approximate SOSP with dimension-independent accuracy guarantees in the outlier-robust setting.",
            "innovation": "This method differs from existing approaches by providing dimension-independent error bounds for SOSPs in the presence of outliers."
        },
        "method": {
            "method name": "Outlier-Robust Second-Order Optimization",
            "method abbreviation": "ORSO",
            "method definition": "This method aims to find an approximate SOSP in a stochastic optimization problem where a fraction of the data is corrupted.",
            "method description": "The core of the method involves robustly estimating gradients and Hessians using existing robust mean estimation techniques.",
            "method steps": [
                "Define the optimization problem under the strong contamination model.",
                "Use robust mean estimation to calculate the gradient and Hessian.",
                "Apply a randomized nonconvex optimization algorithm to find the approximate SOSP."
            ],
            "principle": "The method is effective because it leverages robust estimators to mitigate the influence of outliers on the optimization process."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using synthetic datasets with controlled levels of corruption, comparing the proposed method against baseline algorithms.",
            "evaluation method": "Performance was assessed based on the accuracy of the SOSP found and the robustness of the algorithm against varying levels of data corruption."
        },
        "conclusion": "The proposed method successfully finds approximate SOSPs with dimension-independent guarantees, demonstrating significant improvements over traditional approaches in the presence of outliers.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to maintain performance regardless of the dimensionality of the data and the presence of outliers.",
            "limitation": "One limitation is that the method may require careful tuning of parameters to achieve optimal performance in practice.",
            "future work": "Future research could explore further refinements to the algorithm and its application to more complex nonconvex optimization problems."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge support from various NSF awards and other funding sources.",
            "references": [
                "Bar+10, BNL12, SKL17, Dia+19"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of finding approximate second-order stationary points (SOSP) in stochastic nonconvex optimization, particularly in the presence of outliers, which limits the effectiveness of existing algorithms."
        },
        {
            "section number": "2.1",
            "key information": "The problem involves finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted."
        },
        {
            "section number": "3.5",
            "key information": "The proposed method, Outlier-Robust Second-Order Optimization (ORSO), aims to find an approximate SOSP in a stochastic optimization problem where a fraction of the data is corrupted."
        },
        {
            "section number": "6.1",
            "key information": "The method leverages robust estimators to mitigate the influence of outliers on the optimization process."
        },
        {
            "section number": "7.1",
            "key information": "A key obstacle is that existing nonconvex algorithms struggle to provide guarantees in adversarial settings, particularly when outliers are present."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore further refinements to the algorithm and its application to more complex nonconvex optimization problems."
        }
    ],
    "similarity_score": 0.6246688961377134,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing.json"
}