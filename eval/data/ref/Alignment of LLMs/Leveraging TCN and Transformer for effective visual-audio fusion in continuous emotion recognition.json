{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2303.08356",
    "title": "Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition",
    "abstract": "Human emotion recognition plays an important role in human-computer interaction. In this paper, we present our approach to the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel multi-modal fusion model that leverages Temporal Convolutional Networks (TCN) and Transformer to enhance the performance of continuous emotion recognition. Our model aims to effectively integrate visual and audio information for improved accuracy in recognizing emotions. Our model outperforms the baseline and ranks 3 in the Expression Classification challenge.",
    "bib_name": "zhou2023leveragingtcntransformereffective",
    "md_text": "# Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition\nWeiwei Zhou*, Jiada Lu*, Zhaolong Xiong, Weifeng Wang Chinatelecom Cloud\nHuman emotion recognition plays an important role in human-computer interaction. In this paper, we present our approach to the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective Behavior Analysis in-thewild (ABAW). Specifically, we propose a novel multi-modal fusion model that leverages Temporal Convolutional Networks (TCN) and Transformer to enhance the performance of continuous emotion recognition. Our model aims to effectively integrate visual and audio information for improved accuracy in recognizing emotions. Our model outperforms the baseline and ranks 3 in the Expression Classification challenge.\n# 1. Introduction\nFacial Expression Recognition (FER) can be used in a variety of applications, such as emotion recognition in videos, facial recognition for security purposes, and even in virtual reality applications. Many facial-related tasks have achieved high accuracies, such as face recognition and face attribute recognition. Despite this, the capacity to comprehend the emotions of a person is still not adequate. The subtle distinctions between emotional expressions can lead to ambiguity or uncertainty in the perception of emotions, which makes it harder to assess the emotion of a person. Therefore, the scale of most of the FER datasets are not sufficient to build a robust model. The appearance of AffWild and AffWild2 dataset and the corresponding challenges [5\u201312, 12\u201314, 30] boost the development of affective recognition study. The Aff-Wild2 dataset contains about 600 videos with around 3M frames. The dataset is annotated with three different affect attributes: a) dimensional affect with valence and arousal; b) six basic categorical affect; c) action units of facial mus-\n*These authors contributed equally to this work.\ncles. To facilitate the utilization of the Aff-Wild2 dataset, the ABAW5 2023 competition was organized for affective behavior analysis in the wild. Multi-modal emotion recognition has been proven to be a more effective approach than single-modality emotion recognition, as it can utilize the complementary information between modalities to capture a more complete emotional state while being less susceptible to various noises. This improved recognition ability and generalization ability of the model can lead to more accurate and reliable results. Considering the fact that visual and audio information contains much emotional information, we propose to use multi-modal features for continuous facial emotion recognition and design a network structure based on TCN and Transformer for feature fusion. Visual and audio features are first fed into their respective TCN modules, then the features are concatenated and fed into the Transformer encoder for learning, and finally, an MLP is used for prediction. Our approach can unify visual and audio features into a temporal model, designing an efficient emotion recognition network with Transformer, thereby improving the evaluation accuracy of Valence-Arousal Estimation, Action Unit Detection, and Expression Classification. The remaining parts of the paper are presented as follows: Sec 2 describe the study of facial emotion recognition and multi-modal fusion technique. Sec 3 describes our methodology; Sec 4 describes the experiment details and the result; Sec 5 is the conclusion of the paper.\n# 2. Related Work\nMany previous studies were focusing on the fusion of visual and audio features for emotion recognition. Juan et al. [19] presented a network that used traditional audio features and visual features extracted with a pre-trained CNN model. Vu et al. [28] built a multi-task model for valencearousal estimation and facial expressions prediction. The authors applied the distillation knowledge architecture for training and prediction because the dataset does not include labels for all two tasks. One of the approaches using the\nmulti-modal mechanism for facial emotion recognition was proposed by Tzirakis et al. [26], where the visual and audio features are extracted with the CNN module and are concatenated to feed into the LSTM network. Nguyen et al. [18] proposed a network consisting of a two-stream autoencoder and an LSTM to integrate visual and audio signals for emotion recognition. Zhang et al. [31] proposed a multi-modal multi-feature approach that extracts visual features from 3D-CNN and audio features from a bidirectional recurrent neural network. Srinivas et al. [21] propose a transformer architecture with encoder layers to integrate audio-visual features for expression tracking. Tzirakis et al. [25] use attention-based methods to fuse the visual and audio features. Previous studies have proposed some useful networks on the Aff-wild2 dataset. Kuhnke et al. [15] combine vision and audio information in the video and construct a twostream network for emotion recognition and achieving high performance. Yue Jin et al. [4] propose a transformer-based model to merge audio and visual feature. Temporal Convolutional Network (TCN) was proposed by Colin Lea et al. [16], which hierarchically captures relationships at low-, intermediate-, and high-level time scales. Jin Fan et al. [3] proposed a model with a spatial-temporal attention mechanism to catch dynamic internal correlations with stacked TCN backbones to extract features from different window sizes. The Transformer mechanism proposed by Vaswani et al. [27] has achieved high performance in many tasks, so many researchers exploit Transfomer for affective behavior studies. Zhao et al. [32] proposed a model with spatial and temporal Transformer for facial expression analysis. Jacob et al. [17] proposed a network to learn the relationship between action units with transformer correlation module. Inspired by the previous work, in this paper we proposed a multi-modal fusion model with TCN and Transformer to enhance the performance of emotion recognition.\n# 3. Methodology\nIn this section, we describe in detail our proposed method for tackling the three challenging tasks of affective behavior analysis in the wild that are addressed by the 5th ABAW Competition: Valence-Arousal Estimation, EXPR Classification, and AU Detection. We explain how we design our model architecture, data processing, and training strategy for each task and how we leverage multi-modal to improve our performance.\n# 3.1. Preprocessing\nWe extract the audio stream from the video and preprocess it by converting it to a mono channel with a sample rate of 16, 000 Hz. This allows us to reduce the noise and complexity of the audio signal. Some of the video frames\ndo not contain valid faces, either due to missing or not detected by the face detector. To handle this issue, we replace these frames with the closest frame that has valid face detection. This ensures that we have a consistent sequence of facial images for each video.\n# 3.2. Audio Features\nWe use Wav2Vec2-emotion [22] to extract the audio features that capture the emotional content of speech. Wav2Vec2-emotion is a model based on Wav2Vec2Large-Robust, which is pre-trained on 960 hours of LibriSpeech audio with a sampling rate of 16kHz. The model is then fine-tuned on 284 instances of MSP-Podcast data, which contains emotional speech from different speakers and scenarios. The feature vector dimension is 512, which represents a high-level representation of the acoustic signal. To align the audio features with the video frames, we resize the features to match the length of each frame using interpolation. This ensures that we have a consistent temporal resolution for both modalities.\n# 3.3. Visual Features\nWe extract four visual feature vectors using different models that capture various aspects of facial appearance and expression. The first feature vector is extracted using ArcFace [2] from insightface, which has been pre-trained on the Glint360K dataset [1] for face recognition. This vector encodes the identity and pose of the face with a dimension of 512. The second feature vector is extracted using EfficientNet-b2 [23, 24], which has been pre-trained on the VGGFace2 dataset [20] for face identification and fine-tuned on the AffectNet8 dataset. This vector captures the facial attributes and expressions with a dimension of 1280. The third and fourth feature vectors are extracted using a model from DAN [29], pre-trained on MSCeleb, and finetuned on RAF-DB and AffectNet8. These vectors represent the global and local features of the face with a dimension of 512 each.\n# 3.4. Split Videos\nVideos are first split into segments with a window size w and stride s. Given the segment window w and stride s, a video with n frames would be split into [n/s] + 1 segments, where the i-th segment contains frames \ufffd F(i\u22121)\u2217s+1, . . . , F(i\u22121)\u2217s+w \ufffd . In other words, videos are cut into some overlapping chunks, each with a fixed number of frames. The purpose of doing this is to break down the video into smaller parts that are easier to process and analyze. Each chunk has some\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a6a9/a6a940f2-edce-4e51-bb2d-d7e2f4af61b6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. The architecture of our proposed model. The model consists of four components: pre-trained feature extractors for audio and visual features, TCN with three temporal blocks, Transformer encoder, and MLP for final prediction.</div>\ndegree of overlap with the previous and next ones so that no information in the video is missed.\n# 3.5. Modeling\nWe denote audio features as f a i and visual features as f v i corresponding to the i-th segment.\n# 3.5.1 Temporal Convolutional Network\nEach feature is fed into a dedicated Temporal Convolutional Network (TCN) for temporal encoding, which can be formulated as follows:\ngv i = TCN (f v i )\nga i = TCN (f a i )\nwhere gv i denotes visual features, ga i denotes audio features. Then, visual features and audio features are concatenated, denotes as gc i .\nThis means that we use a special type of neural network that can capture the temporal patterns and dependencies of the features over time. The TCN takes the input feature vector and applies a series of convolutional layers with different kernel sizes and dilation rates to produce an output feature vector. The output feature vector has the same length as the input feature vector but contains more information about the temporal context. For example, the TCN can learn how the sound and image change over time in each segment of the video. The output feature vectors for both sound and image are then combined together by concatenating them along a dimension. This creates a new feature vector that contains both audio and visual information for each segment of the video.\n# 3.5.2 Temporal Encoder\nWe utilize a transformer encoder to model the temporal information in the video segment as well, which can be formulated as follows:\nhi = TransformerEncoder (gc i ) .\nThe Transformer encoder only models the context within a single segment, thereby ignoring the dependencies between frames across segments. To account for the context of different frames, overlapping between consecutive segments can be employed, thus enabling the capture of the dependencies between frames across segments, which means s \u2264w. We use another type of neural network that can learn the relationships and interactions among the features within each segment. The transformer encoder takes the input feature vector that contains both audio and visual information and applies a series of self-attention layers and feed-forward layers to produce an output feature vector. The output feature vector has more semantic meaning and representation power than the input feature vector. For example, the transformer encoder can learn how different parts of the sound and image relate to each other in each segment of the video. However, the transformer encoder does not consider how different segments of the video are connected or influenced by each other. To solve this problem, we can make some segments overlap with each other so that some frames are shared by two or more segments. This way, we can capture some information about how different segments affect each other. The degree of overlap is controlled by two parameters: s is the length of a segment and w is the sliding window size. If s is smaller than or equal to w, then there will be some overlap between consecutive segments.\n# 3.5.3 Prediction\nAfter the temporal encoder, the features hi are finally fed into MLP for regression, which can be formulated as fol-\n# yi = MLP(hi)\nwhere yi are the predictions of i-th segment. For VA challenge, yi \u2208Rl\u00d72. For EXPR challenge, yi \u2208Rl\u00d78. For AU challenge, yi \u2208Rl\u00d712 . The prediction vector contains the values that we want to estimate for each segment. The MLP consists of several layers of neurons that can learn non-linear transformations of the input. The MLP can be trained to minimize the error between the prediction vector and the ground truth vector. The ground truth vector is the actual values that we want to predict for each segment. Depending on what kind of challenge we are solving, we have different types of ground truth vectors and prediction vectors. For the VA challenge, we want to predict two values: valence and arousal. Valence measures how positive or negative an emotion is. Arousal measures how active or passive an emotion is. For the EXPR challenge, we want to predict eight values: one for each basic expression (anger, disgust, fear, happiness, sadness, and surprise) plus neutral and other expressions. For the AU challenge, we want to predict twelve values: one for each action unit (AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, AU26).\n# 3.6. Loss Functions\nVA challenge: We use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the measure, which is defined as in Eq 1. It measures the correlation between two sequences x and y and ranges between -1 and 1, where -1 means perfect anticorrelation, 0 means no correlation, and 1 means perfect correlation. The loss is calculated as Eq 2.\n(1)\n(2)\nEXPR challenge: We use the cross-entropy loss as the loss function, which is defined as in Eq 3.\n(3)\nwhere yic is a binary indicator (0 or 1) if class c is the correct classification for observation i. pic is the predicted probability of observation i being in class c, M is the number of classes. The multiclass cross entropy loss function measures how well a model predicts the true probabilities of each class for a given observation. It penalizes wrong\npredictions by taking the logarithm of the predicted probabilities. The lower the loss, the better the model. AU challenge: We employ BCEWithLogitsLoss as the loss function, which integrates a sigmoid layer and binary cross-entropy, which is defined as in Eq 4.\n(4)\nwhere N is the number of samples, yi is the target label for sample i, xi is the input logits for sample i, \u03c3 is the sigmoid function The advantage of using BCEWithLogitsLoss over BCELoss with sigmoid is that it can avoid numerical instability and improve performance.\n# 4. Experiments and Results 4.1. Experiments Settings\n# 4. Experiments and Results\n# 4.1. Experiments Settings\nAll models are trained on an Nvidia GeForce GTX 3090 GPU which has 24GB of memory. We use AdamW optimizer and cosine learning rate schedule with the first epoch warmup. The learning rate is 3e \u22125, the weight decay is 1e \u22125, the dropout prob is 0.3, and the batch size is 32. For VA Challenge, we use Wav2Vec2-emotion, Eff, RAF-DB, and AffectNet8 as the input features. For EXPR Challenge, we use two types of input features: Eff and AffectNet8 as described above. For AU Challenge, we use three types of input features: Eff, RAF-DB, and AffectNet8 as described above. For all three challenges, we split videos using a segment window w = 300 and a stride s = 200. This means we divide each video into segments of 300 frames with an overlap of 100 frames between consecutive segments. This helps us capture the temporal dynamics of facial expressions and emotions.\n# 4.2. Overall Results\nTable 1 displays the experimental results of our proposed method on the validation set of the VA, EXPR, and AU Challenge, where the Concordance Correlation Coefficient (CCC) is utilized as the evaluation metric for both valence and arousal prediction, and F1-score is used to evaluate the result of EXPR and AU challenge. As demonstrated in the table, our proposed method outperforms the baseline significantly. These results show that our proposed approach using TCN and Transformer-based model effectively integrates visual and audio information for improved accuracy in recognizing emotions on this dataset. Table 2, Table 3, and Table 4 display the overall test results on the three challenges. Notably, Netease Fuxi and SituTech achieved the first and second highest scores in all three challenges, surpassing other teams significantly, indicating their exceptional performance in these challenges.\nExperiment\nFeature\nValence\nArousal\nF1-score\nVA\nEff, AffectNet8, RAF-DB, Wav2Vec2-emotion\n0.5505\n0.6809\n-\nEXPR\nEff, AffectNet8\n-\n-\n0.4138\nAU\nEff, AffectNet8, RAF-DB\n-\n-\n0.5248\nTable 1. Performance of our method on the validation dataset of three experiments\nTeams\nTotal Score\nCCC-V\nCCC-A\nSituTech\n0.6414\n0.6193\n0.6634\nNetease Fuxi\n0.6372\n0.6486\n0.6258\nCBCR\n0.5913\n0.5526\n0.6299\nOurs\n0.5666\n0.5008\n0.6325\nHFUT-MAC\n0.5342\n0.5234\n0.5451\nHSE-NN-SberAI\n0.5048\n0.4818\n0.5279\nACCC\n0.4842\n0.4622\n0.5062\nPRL\n0.4661\n0.5043\n0.4279\nSCLAB CNU\n0.4640\n0.4578\n0.4703\nUSTC-AC\n0.2783\n0.3245\n0.2321\nbaseline\n0.201\n0.211\n0.191\nTable 2. The overall test results on VA challenge. The bold fonts indicate the best results.\nTable 2. The overall test results on VA challenge. The bold fonts indicate the best results.\nTeams\nF1\nNetease Fuxi\n0.4121\nSituTech\n0.4072\nOurs\n0.3532\nHFUT-MAC\n0.3337\nHSE-NN-SberAI\n0.3292\nAlphaAff\n0.3218\nUSTC-IAT-United\n0.3075\nSSSIHL DMACS\n0.3047\nSCLAB CNU\n0.2949\nWall Lab\n0.2913\nACCC\n0.2846\nRT IAI\n0.2834\nDGU-IPL\n0.2278\nbaseline\n0.2050\nTable 3. The overall test results on EXPR challenge. The bold fonts indicate the best results.\nTable 3. The overall test results on EXPR challenge. The bold fonts indicate the best results.\nOur team ranks fourth in the VA challenge, third in the EXPR challenge, and sixth in the AU challenge. Our team\u2019s performance demonstrates our competitive standing in the challenges, with notable achievements in the VA, EXPR, and AU challenges.\nTeams\nF1\nNetease Fuxi\n0.5549\nSituTech\n0.5422\nUSTC-IAT-United\n0.5144\nSZFaceU\n0.5128\nPRL\n0.5101\nOurs\n0.4887\nHSE-NN-SberAI\n0.4878\nUSTC-AC\n0.4811\nHFUT-MAC\n0.4752\nSCLAB CNU\n0.4563\nUSC IHP\n0.4292\nACCC\n0.3776\nbaseline\n0.365\nTable 4. The overall test results on AU challenge. The bold fonts indicate the best results.\n# 4.3. Ablation Study\nIn this section, we perform several ablation studies on these three experiments to compare the contribution of different features. From Table 6, it can be seen that almost every feature contributes to the VA prediction task, and the combination of 4 visual features: Eff, ArcFace, AffectNet8, RAF-DB, and the audio features: Wav2Vec2-emotion reach the highest CCC score on VA experiment. Table 7 shows that the use of Eff and AffectNet8 can reach the highest F1score in the EXPR experiments. Table 8 shows that Eff, AffectNet8, and RAF-DB can reach the highest F1-score in the EXPR and AU experiments. The cross-validation result of the VA, EXPR, and AU experiments are reported in Table 5. Fold 0 is exactly the original data from the ABAW dataset.\n# 5. Conclusion\nOur proposed approach utilizes a combination of a Temporal Convolutional Network (TCN) and a Transformerbased model to integrate visual and audio information for improved accuracy in recognizing emotions. The TCN captures relationships at low-, intermediate-, and high-level time scales, while the Transformer mechanism merges audio and visual features. We conducted our experiment on the Aff-Wild2 dataset, which is a widely used benchmark\nTask\nEvaluation Metric\nPartition\nMethod\nFold 0\nFold 1\nFold 2\nFold 3\nFold 4\nValence\nCCC\nValidation\nOurs\n0.5505\n0.6455\n0.5889\n0.5394\n0.5406\nBaseline\n0.24\n-\n-\n-\n-\nTest\nOurs\n0.5504\n0.4979\n0.5008\n0.4979\n0.4875\nBaseline\n0.211\n-\n-\n-\n-\nArousal\nValidation\nOurs\n0.6809\n0.6259\n0.6539\n0.6468\n0.6591\nBaseline\n0.20\n-\n-\n-\n-\nTest\nOurs\n0.5805\n0.5396\n0.6325\n0.5037\n0.5569\nBaseline\n0.191\n-\n-\n-\n-\nEXPR\nF1-score\nValidation\nOurs\n0.4138\n0.4350\n0.3614\n0.3959\n0.4234\nBaseline\n0.23\n-\n-\n-\n-\nTest\nOurs\n0.3406\n0.2979\n0.3532\n0.3293\n0.3427\nBaseline\n0.2050\n-\n-\n-\n-\nAU\nF1-score\nValidation\nOurs\n0.5248\n0.5524\n0.5000\n0.5060\n0.5393\nBaseline\n0.39\n-\n-\n-\n-\nTest\nOurs\n0.4735\n0.4822\n0.4887\n0.4818\n0.4720\nBaseline\n0.365\n-\n-\n-\n-\n<div style=\"text-align: center;\">Table 5. Results for the five folds of three tasks</div>\nVisual Features\nAudio Features\nValence\nArousal\nArcFace\nNone\n0.5013\n0.6054\nAffectNet8\nNone\n0.5392\n0.6629\nRAF-DB\nNone\n0.5109\n0.6579\nEff\nNone\n0.5208\n0.6467\nEff, ArcFace\nNone\n0.5216\n0.6519\nEff, ArcFace, AffectNet8\nNone\n0.5345\n0.6532\nEff, ArcFace, AffectNet8, RAF-DB\nNone\n0.5429\n0.6613\nEff, ArcFace, AffectNet8, RAF-DB\nWav2Vec2-emotion\n0.5505\n0.6809\nTable 6. Ablation study of features on the validation dataset of VA experiment.\n<div style=\"text-align: center;\">Table 6. Ablation study of features on the validation dataset of VA experiment.</div>\nVisual Features\nAudio Features\nF1-score\nArcFace\nNone\n0.3512\nAffectNet8\nNone\n0.3937\nRAF-DB\nNone\n0.3928\nEff\nNone\n0.4018\nEff, ArcFace\nNone\n0.4015\nEff, AffectNet8\nNone\n0.4138\nEff, RAF-DB\nNone\n0.4012\nEff, AffectNet8, ArcFace\nNone\n0.4093\nEff, AffectNet8, RAF-DB\nNone\n0.4087\nEff, AffectNet8\nWav2Vec2-emotion\n0.4028\nVisual Features\nAudio Features\nF1-score\nArcFace\nNone\n0.4598\nAffectNet8\nNone\n0.4894\nRAF-DB\nNone\n0.4915\nEff\nNone\n0.5118\nEff, ArcFace\nNone\n0.5042\nEff, AffectNet8\nNone\n0.5215\nEff, RAF-DB\nNone\n0.5155\nEff, AffectNet8, ArcFace\nNone\n0.5109\nEff, AffectNet8, RAF-DB\nNone\n0.5248\nEff, AffectNet8, RAF-DB\nWav2Vec2-emotion\n0.5134\nTable 7. Ablation study of features on the validation dataset of EXPR experiment.\ndataset for emotion recognition. Our results show that our method significantly outperforms the baseline. Finally, our\nTable 8. Ablation study of features on the validation dataset of AU experiment.\nteam ranks fourth in the VA challenge, third in the EXPR challenge, and sixth in the AU challenge.\n[1] Xiang An, Jiangkang Deng, Jia Guo, Ziyong Feng, Xuhan Zhu, Yang Jing, and Liu Tongliang. Killing two birds with one stone: Efficient and robust training of face recognition cnns by partial fc. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2 [2] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019. 2 [3] Jin Fan, Ke Zhang, Yipan Huang, Yifei Zhu, and Baiping Chen. Parallel spatio-temporal attention-based tcn for multivariate time series prediction. Neural Computing and Applications, pages 1\u201310, 2021. 2 [4] Yue Jin, Tianqing Zheng, Chao Gao, and Guoqiang Xu. A multi-modal and multi-task learning method for action unit and expression recognition. arXiv preprint arXiv:2107.04187, 2021. 2 [5] Dimitrios Kollias. Abaw: Learning from synthetic data & multi-task learning challenges. arXiv preprint arXiv:2207.01138, 2022. 1 [6] Dimitrios Kollias. Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2328\u20132336, 2022. [7] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing affective behavior in the first abaw 2020 competition. In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG), pages 794\u2013 800, 2020. [8] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou. Face behavior a la carte: Expressions, affect and action units in a single network. arXiv preprint arXiv:1910.11111, 2019. [9] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou. Distribution matching for heterogeneous multitask learning: a large-scale face study. arXiv preprint arXiv:2105.03790, 2021. 10] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges, 2023. 11] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou, Athanasios Papaioannou, Guoying Zhao, Bj\u00a8orn Schuller, Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond. International Journal of Computer Vision, pages 1\u201323, 2019. 12] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface. arXiv preprint arXiv:1910.04855, 2019. 1 13] Dimitrios Kollias and Stefanos Zafeiriou. Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework. arXiv preprint arXiv:2103.15792, 2021. 14] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affective behavior in the second abaw2 competition. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision, pages 3652\u20133660, 2021. 1 [15] Felix Kuhnke, Lars Rumberg, and J\u00a8orn Ostermann. Twostream aural-visual affect analysis in the wild. In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020), pages 600\u2013605. IEEE, 2020. 2 [16] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks: A unified approach to action segmentation. In Computer Vision\u2013ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 1516, 2016, Proceedings, Part III 14, pages 47\u201354. Springer, 2016. 2 [17] Geethu Miriam Jacob and Bj\u00a8orn Stenger. Facial action unit detection with transformers. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7676\u20137685, 2021. 2 [18] Dung Nguyen, Duc Thanh Nguyen, Rui Zeng, Thanh Thi Nguyen, Son N Tran, Thin Nguyen, Sridha Sridharan, and Clinton Fookes. Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition. IEEE Transactions on Multimedia, 24:1313\u20131324, 2021. 2 [19] Juan DS Ortega, Patrick Cardinal, and Alessandro L Koerich. Emotion recognition using fusion of audio and video features. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pages 3847\u20133852. IEEE, 2019. 1 [20] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In British Machine Vision Conference, 2015. 2 [21] Srinivas Parthasarathy and Shiva Sundaram. Detecting expressions with multimodal transformers. In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 636\u2013643. IEEE, 2021. 2 [22] Leonardo Pepino, Pablo Riera, and Luciana Ferrer. Emotion recognition from speech using wav2vec 2.0 embeddings. arXiv preprint arXiv:2104.03502, 2021. 2 [23] Andrey V. Savchenko. Video-based frame-level facial analysis of affective behavior on mobile devices using efficientnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 2359\u20132366, June 2022. 2 [24] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International conference on machine learning, pages 10096\u201310106. PMLR, 2021. 2 [25] Panagiotis Tzirakis, Jiaxin Chen, Stefanos Zafeiriou, and Bj\u00a8orn Schuller. End-to-end multimodal affect recognition in real-world environments. Information Fusion, 68:46\u201353, 2021. 2 [26] Panagiotis Tzirakis, George Trigeorgis, Mihalis A Nicolaou, Bj\u00a8orn W Schuller, and Stefanos Zafeiriou. End-toend multimodal emotion recognition using deep neural networks. IEEE Journal of selected topics in signal processing, 11(8):1301\u20131309, 2017. 2 [27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [28] Manh Tu Vu, Marie Beurton-Aimar, and Serge Marchand. Multitask multi-database emotion recognition. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision, pages 3637\u20133644, 2021. 1 [29] Zhengyao Wen, Wenzhong Lin, Tao Wang, and Ge Xu. Distract your attention: Multi-head cross attention network for facial expression recognition. arXiv preprint arXiv:2109.07270, 2021. 2 [30] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou, Athanasios Papaioannou, Guoying Zhao, and Irene Kotsia. Aff-wild: Valence and arousal \u2018in-the-wild\u2019challenge. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pages 1980\u20131987. IEEE, 2017. 1 [31] Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, and Shiguang Shan. M 3 f: Multi-modal continuous valence-arousal estimation in the wild. In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020), pages 632\u2013636. IEEE, 2020. 2 [32] Zengqun Zhao and Qingshan Liu. Former-dfer: Dynamic facial expression recognition transformer. In Proceedings of the 29th ACM International Conference on Multimedia, pages 1553\u20131561, 2021. 2\n",
    "paper_type": "method",
    "attri": {
        "background": "Human emotion recognition plays an important role in human-computer interaction. Despite advancements in facial expression recognition (FER), the ability to comprehend subtle emotional distinctions remains inadequate, necessitating new methods to improve accuracy. The emergence of large datasets, such as AffWild2, has spurred developments in affective recognition, highlighting the need for effective multi-modal approaches that leverage both visual and audio data.",
        "problem": {
            "definition": "The paper addresses the challenge of accurately recognizing continuous emotions through facial expressions by integrating visual and audio information.",
            "key obstacle": "The core challenge lies in the ambiguity of emotional expressions and the limitations of existing single-modality recognition methods, which struggle to capture the full emotional context."
        },
        "idea": {
            "intuition": "The idea is inspired by the complementary nature of visual and audio features in conveying emotional information, which can enhance recognition accuracy when combined.",
            "opinion": "The proposed multi-modal fusion model utilizes Temporal Convolutional Networks (TCN) and Transformer architectures to effectively integrate visual and audio data for continuous emotion recognition.",
            "innovation": "The primary innovation of this method is the combination of TCN for temporal feature extraction and Transformer for contextual learning, which sets it apart from existing approaches that often rely on simpler fusion techniques."
        },
        "method": {
            "method name": "Multi-modal Emotion Recognition Model",
            "method abbreviation": "MMERM",
            "method definition": "A novel approach that integrates visual and audio features through a combination of Temporal Convolutional Networks and Transformer architectures to enhance continuous emotion recognition.",
            "method description": "The method processes audio and visual features separately using TCNs, concatenates them, and then applies a Transformer encoder for contextual learning before making predictions with an MLP.",
            "method steps": [
                "Extract audio features from video using Wav2Vec2-emotion.",
                "Extract visual features using pre-trained models like ArcFace and EfficientNet.",
                "Preprocess audio and visual data to ensure consistency.",
                "Feed features into TCNs for temporal encoding.",
                "Concatenate the outputs of TCNs and pass them through a Transformer encoder.",
                "Use an MLP for final predictions based on the encoded features."
            ],
            "principle": "The method is effective because it captures temporal dependencies in the data and leverages the strengths of both audio and visual modalities, leading to a more robust understanding of emotional states."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the Aff-Wild2 dataset, with models trained on Nvidia GeForce GTX 3090 GPUs. Various input features were utilized across three challenges: Valence-Arousal Estimation, Expression Classification, and Action Unit Detection.",
            "evaluation method": "Performance was assessed using metrics such as Concordance Correlation Coefficient (CCC) for valence and arousal, and F1-score for expression and action unit detection. Results were compared against baseline methods."
        },
        "conclusion": "The proposed approach demonstrates significant improvements over baseline methods in recognizing emotions by effectively integrating visual and audio information. The results validate the model's efficacy across multiple challenges, showcasing its competitive performance in emotion recognition tasks.",
        "discussion": {
            "advantage": "The key advantages include improved accuracy due to multi-modal fusion and the ability to capture complex emotional states through temporal modeling.",
            "limitation": "A limitation of the method is its dependency on high-quality input data; inaccuracies in audio or visual features can adversely affect performance.",
            "future work": "Future research could explore enhancing the model's robustness against noisy data and expanding its applicability to more diverse emotional contexts."
        },
        "other info": {
            "team ranking": {
                "VA challenge": "4th",
                "EXPR challenge": "3rd",
                "AU challenge": "6th"
            },
            "dataset": "Aff-Wild2"
        }
    },
    "mount_outline": [
        {
            "section number": "4",
            "key information": "Human emotion recognition plays an important role in human-computer interaction. Despite advancements in facial expression recognition (FER), the ability to comprehend subtle emotional distinctions remains inadequate, necessitating new methods to improve accuracy."
        },
        {
            "section number": "4.1",
            "key information": "The proposed multi-modal fusion model utilizes Temporal Convolutional Networks (TCN) and Transformer architectures to effectively integrate visual and audio data for continuous emotion recognition."
        },
        {
            "section number": "4.4",
            "key information": "A limitation of the method is its dependency on high-quality input data; inaccuracies in audio or visual features can adversely affect performance."
        },
        {
            "section number": "5",
            "key information": "The idea is inspired by the complementary nature of visual and audio features in conveying emotional information, which can enhance recognition accuracy when combined."
        },
        {
            "section number": "8.1",
            "key information": "The core challenge lies in the ambiguity of emotional expressions and the limitations of existing single-modality recognition methods, which struggle to capture the full emotional context."
        },
        {
            "section number": "8.3",
            "key information": "Future research could explore enhancing the model's robustness against noisy data and expanding its applicability to more diverse emotional contexts."
        }
    ],
    "similarity_score": 0.5203162234268431,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1730_natur/papers/Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition.json"
}