{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1911.03362",
    "title": "Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation",
    "abstract": "Neural machine translation benefits from synthetic parallel training data. Source-side monolingual data can be (forward-)translated into the target language for self-training; target-side monolingual data can be backtranslated. It has been widely reported that back-translation delivers superior results, but could this be due to artefacts in standard test sets? We perform a case study on a FrenchEnglish news translation task and separate test sets based on their original languages. We show that forward translation can deliver superior BLEU in most cases on sentences that were originally in the source language, complementing previous studies which show large improvements with back-translation on sentences that were originally in the target language. However, humans mostly prefer the output of a backtranslated system, regardless of the input direction. We explore differences between forward and back-translation and find evidence for subtle domain effects, and show that the relative success of the two methods is affected by the quality of the systems used to produce synthetic data, with forward translation being more sensitive to this quality.",
    "bib_name": "bogoychev2020domaintranslationesenoisesynthetic",
    "md_text": "# Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation\nNikolay Bogoychev1 Rico Sennrich2,1\n1School of Informatics, University of Edinburgh 2Institute of Computational Linguistics, University of Zurich n.bogoych@ed.ac.uk, sennrich@cl.uzh.ch\n# Abstract\nNeural machine translation benefits from synthetic parallel training data. Source-side monolingual data can be (forward-)translated into the target language for self-training; target-side monolingual data can be backtranslated. It has been widely reported that back-translation delivers superior results, but could this be due to artefacts in standard test sets? We perform a case study on a FrenchEnglish news translation task and separate test sets based on their original languages. We show that forward translation can deliver superior BLEU in most cases on sentences that were originally in the source language, complementing previous studies which show large improvements with back-translation on sentences that were originally in the target language. However, humans mostly prefer the output of a backtranslated system, regardless of the input direction. We explore differences between forward and back-translation and find evidence for subtle domain effects, and show that the relative success of the two methods is affected by the quality of the systems used to produce synthetic data, with forward translation being more sensitive to this quality.\n# 1 Introduction\nThe quality of neural machine translation can be improved by leveraging additional monolingual resources in various different ways (Sennrich et al., 2016b; Zhang and Zong, 2016; Gulcehre et al., 2017; Ramachandran et al., 2017; Freitag et al., 2019). Among these, back-translation is the most widely used technique in shared translation tasks (Barrault et al., 2019, p. 15), and it has been reported that it outperforms self-training with forward translation (Burlot and Yvon, 2018). However, in the past year, attention was drawn to the fact that standard test sets are often shared between translation directions and thus contain both por-\ntions where the original text is on the source side (original), as well as portions where the original text is used as the reference translation, with the source text being a human translation (reverse) (See Figure 1). This use of \u201coriginal\u201c and \u201creverse\u201d test sets heavily affects empirical results for back-translation. When augmenting the model with back-translation, improvements in BLEU (Papineni et al., 2002) are a lot more evident if the sentence was translated in the reverse direction, that is to say with naturally produced reference and a human translation on the source side (Edunov et al., 2019). Freitag et al. (2019) explore automatic post editing (APE), which heavily relies on synthetic training data, and find that there is a loss in BLEU score on the original portion, despite humans perceiving improvement in the translation quality. Zhang and Toral (2019) show that the ranking of submissions to the news translation task changes when evaluating only the portion with original sources, or only that with translationese sources. Interestingly, systems that rely heavily on large-scale backtranslation, such as that by Edunov et al. (2019), are more dominant on the reverse portion. We focus on three factors that we hypothesise play a large role in explaining the observed differences in effectiveness between forward and backtranslation, and between performance on the original and reverse portion of standard test sets: differences in language style between naturally produced text and translationese text, differences in the domains between source-side and target-side monolingual texts,1 and differences in how noise in the synthetic data, specifically translation errors due to the varying quality of MT systems producing it, will affect the final system, depending on whether it is on the source-side (back-translation) or target side (forward translation). We perform the 1here, we use domain in a broad sense to refer to various\n We show that when the test sets are split according to original language, forward translation is generally better than backtranslation in terms of BLEU on the original portion, complementing the findings of (Edunov et al., 2019), who find that back-translation is better at improving BLEU on the reverse portion.\n\u2022 We perform human evaluation on a subset of our translation, comparing a baseline system, one augmented with back-translation and one augmented with forward translation. We see that despite the huge discrepancies in BLEU, humans measure adequacy to be pretty similar across all systems, especially on the original portion of the dataset. Humans, however tend to prefer the backtranslation\u2019s system fluency a lot more than that of the other two systems. \u2022 We perform language model experiments where we contrast language style and language domain and evaluate on the test sets.\n\u2022 We show that the language between original and translationese French is sufficiently different to be reliably detected by a neural network on a document level.\n We explore the effectiveness of forward and back-translation in a scenario where the quality of the synthetic data produced is poor, and find that forward translation is more sensitive to the quality of the initial translation system than back-translation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b95b/b95bd2ab-faa9-4846-aa4f-bd29ca39df03.png\" style=\"width: 50%;\"></div>\nFigure 1: Original and reverse portions of French\u2192English test set, differences in domain and differences in BLEU evaluation.\nStatistical machine translation relies on the noisy channel model, which makes large-scale language models, and hence extensive monolingual targetlanguage data, very valuable (e.g. Brants et al., 2007). In neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017) however, it is not immediately clear how to make use of monolingual target-language resources. This led to the development of different methods such as language model fusion (Gulcehre et al., 2017), language model pretraining (Ramachandran et al., 2017), back-translation (Sennrich et al., 2016b), but also the exploration of methods to incorporate sourcelanguage data via forward translation (Zhang and Zong, 2016). Out of these, back-translation is the most widely used (see Barrault et al., 2019), and has been reported to work better than forward translation in particular (Burlot and Yvon, 2018).\n# 2.1 Back-translation\nGiven a translation task L1 \u2192L2, where largescale monolingual L2 data is available, backtranslation refers to training a translation model L2 \u2192L1 and using it to translate the L2 data into L1, creating a synthetic parallel corpus that can be added to the true bilingual data for the purpose of training a L1 \u2192L2 model. While this technique was first explored for statistical machine translation (Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011), it has a different effect on training, and was found to be much more effective, in neural machine translation, particularly in low resource scenarios (Sennrich et al., 2016b,a). However, it is not entirely clear what causes the large improvement in translation quality. Previous work has analysed increases in fluency when training on back-translated data (e.g. Sennrich et al., 2016b; Edunov et al., 2019), and domain adaptation effects (e.g. Sennrich et al., 2016b; Chinea-R\u00edos et al., 2017a), which can be attributed to the target-side data, but the properties of synthetic source sentences have also been investigated. Burlot and Yvon (2018) have found that automatic translations tend to be more monotonic and simpler than natural parallel data, which could make learning easier, but these biases also make the training distribution less similar to natural input. While there is some evidence that the quality of the back-translation system matters (Burlot and Yvon, 2018), models are relatively robust to noise,\nand Edunov et al. (2018) even find that they obtain better models when using sampling rather than standard beam search for back-translation, or explicitly add noise, even if this reduces the quality of back-translations. Caswell et al. (2019) argue that if the model is given means to distinguish real from synthetic parallel data, either via noise or more simply a special tag, it can avoid learning detrimental biases from synthetic training data.\n# 2.2 Forward translation\nGiven a translation task L1 \u2192L2, where largescale monolingual L1 data is present, forward translation refers to training a translation model L1 \u2192L2 and using it to translate the L1 data into L2, creating a synthetic parallel corpus that can be added to the true bilingual data for the purpose of training an improved L1 \u2192L2 model. Self-training with forward translation was also pioneered in statistical machine translation (Ueffing et al., 2007), but attracted new interest in neural machine translation, where improvements in BLEU were demonstrated (Zhang and Zong, 2016; Chinea-R\u00edos et al., 2017b). Compared to backtranslation, biases and errors in synthetic data are intuitively more problematic in forward translation since they directly affect the gold labels. Also, there is no clear theoretical link between forwardtranslated synthetic training data and a model\u2019s fluency, but other effects, such as domain adaptation and improved learnability of translation from synthetic data remain plausible.2 Burlot and Yvon (2018) perform a systematic study which shows that forward translation leads to some improvements in translation quality, but not nearly as much as back-translation. In very recent work, Wu et al. (2019) show large-scale experiments where a combination of synthetic data produced by both forward and backward translation delivers superior results to just using one or the other. The amount of research on forward translation is however significantly smaller than that on back-translation.\n# 3 Domains and Translationese\nBased on these studies, we consider how the original and reverse portion of standard test sets differ,\nand how this can partially explain the observed differences between forward and back-translation.\n# 3.1 Domains\nIt has previously been shown that back-translation can be used for domain adaptation (Sennrich et al., 2016b; Chinea-R\u00edos et al., 2017a), and the effectiveness of back-translation and forward translation heavily depends on the availability of relevant, indomain monolingual data. Even if we have both source-side and target-side data from the same general domain, we believe that there can be subtle differences between them. Even in restricted domain tasks, such WMT news translation (Barrault et al., 2019), newspaper articles in different languages talk about different topics.3 For example, French news article cover subjects of local interest, such as the Quebec local elections. On the other hand, English language news in WMT test sets talk about mostly American or international topics. Therefore when performing back-translation, which is based on target-side data, this implicitly adapts systems to this target-side news domain, while forward translation would adapt systems to the source-side news domain.\nA second important distinction between the original and reverse portion of test sets comes from their creation, i.e. the process of translation. Human translations show systematic differences to natural text, and this dialect has been termed translationese. Translationese has been extensively studied in the context of natural language processing (Baroni and Bernardini, 2005; He et al., 2016). Translationese texts tend to have different word distribution than naturally produced text due to interference from the source language (Koppel and Ordan, 2011), and other translation strategies such as simplification and explicitation. While translationese is hard to spot for humans, machine learning methods can reliably identify it (Ilisei et al., 2010; Koppel and Ordan, 2011; Rabinovich and Wintner, 2015). Translationese and its effect have been studied in the context of statistical machine translation: Kurokawa et al. (2009); Lembersky et al. (2012) observe that systems reach higher BLEU on test sets if the direction of the test set is the same as\nthe direction of the training set, Stymne (2017) show how systems can be tuned specifically to translationese and Riley et al. (2020) even show how BLEU can be gamed by specifically producing translationese. Due to the directional nature of the WMT19 test sets (Barrault et al., 2019), research on translationese in the context of neural machine translation has been revitalized (Freitag et al., 2019; Edunov et al., 2019; Zhang and Toral, 2019; Graham et al., 2019; Bizzoni et al., 2020). One of our goals in this paper is to isolate domain effects and translationese effects in the analysis of synthetic training corpora.\n# 4 Experimental setup\nWe used the WMT 15 English-French news translation task dataset (Bojar et al., 2015), consisting of 35.8M parallel sentences. For back and forward translation we used 49.8M English monolingual sentences and 46.1M French monolingual sentences from the respective News Crawl corpora. For training the back-translation and forward translation systems we used a both a shallow RNN (Bahdanau et al., 2015), equivalent to the one used by (Sennrich et al., 2016a), as well as a transformer base system (Vaswani et al., 2017). Our shallow RNN was about 1 BLEU better than the transformer on fr-en (used for forward translation), and about 3 BLEU worse on en-fr (used for backtranslation) than the transformer. For producing the synthetic data we used sampling from the softmax distribution (Edunov et al., 2018). Byte pair encoding (BPE) (Sennrich et al., 2016c) was used to produce a shared vocabulary of 88k tokens. For training the baseline model, as well as the ones augmented with synthetic data, we used the transformer base architecture. The models denoted as BT and FWD are trained by augmenting the parallel dataset with back-translation and forward translation respectively and a subscript transformer or rnn denotes what system was used to produce the synthetic data. All model hyperparameters are the same as those of the baseline. All training and decoding was done using the Marian machine translation toolkit (Junczys-Dowmunt et al., 2018). All models were trained with early stopping on a dev set (newstest2014) with patience 10.\n# 4.1 Directional test sets\nWe used all available datasets from the news translation task and split them by direction, based on\nthe source language, equivalent to the way done by Post (2018), and we evaluated each dataset with all of our models.\n# 5 Translation experiments\nSystem\n2008\n2009\n2010\n2011\n2012\n2013\nOriginal (French source)\nBaseline\n29.4\n44.2\n32.9\n32.3\n37.3\n47.4\nBTtransformer\n28.0\n41.8\n30.0\n30.3\n34.0\n45.8\nBTrnn\n29.3\n42.2\n31.7\n31.5\n34.6\n46.9\nFWDtransformer\n29.0\n43.8\n32.3\n32.4\n36.4\n49.0\nFWDrnn\n30.9\n45.1\n32.0\n33.1\n38.3\n48.3\nReverse (Translationese French source)\nBaseline\n29.1\n29.6\n37.3\n45.3\n34.5\n35.4\nBTtransformer\n31.6\n32.9\n42.6\n50.8\n39.3\n39.5\nBTrnn\n32.1\n33.4\n43.3\n50.5\n39.0\n38.4\nFWDtransformer\n28.0\n28.7\n36.7\n44.5\n33.7\n35.2\nFWDrnn\n27.5\n28.1\n36.0\n43.0\n33.0\n33.9\nFull test set*\nBaseline\n29.2\n37.3\n35.2\n38.8\n35.9\n41.6\nBTtransformer\n30.0\n37.6\n36.1\n40.7\n36.8\n42.9\nBTrnn\n30.9\n38.1\n37.3\n40.5\n36.9\n42.5\nFWDtransformer\n28.5\n36.7\n34.4\n38.5\n35.0\n42.2\nFWDrnn\n29.0\n37.1\n33.9\n38.0\n35.6\n41.5\nTable 1: French\u2192English BLEU scores on newstest. BT and FWD denote baseline system augmented with transformer or RNN produced back-translation and forward translation respectively. *For some test sets, some of the sentences are originally in neither French nor English, so we removed them from the test set.\nWe present our experimental results on Table 1. On the original portion, the systems augmented with FWDrnn translated data performs the best on most test sets. The back-translation system is worse than the baseline on all test sets. Furthermore it is interesting to observe that the system trained on transformer-produced synthetic data is worse than that trained on RNN-produced synthetic data. We observe the opposite on the reverse portion: a back-translation system (either BTrnn or BTtransformer, with no clear winner between the two, despite a 3 BLEU difference between the quality of the baseline RNN and Transformer) is always the best, and the forward translation systems shows no improvement over the baseline. On the full datasets, the overall trend is that forward translation does not improve the overall translation quality, which is not consistent with previous work (Burlot and Yvon, 2018). We note that RNN produced synthetic data mostly outperforms their transformer counterparts. We note that overall, the backtranslation augmented system produces\nthe best BLEU, which is consistent with Burlot and Yvon (2018). It is tempting to conclude that forward translation works better for texts in the original translation direction, but we can\u2019t do that without conducting human evaluation, as BLEU is known to not correspond directly to translation quality, especially for high quality systems (Ma et al., 2019; Freitag et al., 2020; Mathur et al., 2020). It does seem that forward translation is more sensitive to the quality of the system used to produce the synthetic data.\n# 6 Human Evaluation\nTable 1 shows big discrepancies in the BLEU scores based on the type of synthetic data and directionality of the datasets, but BLEU does not tell the full story. In order to get further insight on the effects of forward and backward translated data, we sampled uniformly 1008 sentences from all the newstest datasets, 504 in the forward direction and 504 in the reverse direction. We recruited 4 native English speakers to evaluate the translations of those sentences with three distinct systems (the baseline, BTrnn, and FWDrnn). We followed the evaluation scheme of Callison-Burch et al. (2007) where we request our annotators to rate translations in terms of fluency and adequacy on a scale from 1 to 5. Annotators are only shown the three translations for the fluency evaluation; for the adequacy evaluation, they are additionally provided with a reference translation. Rating scales and instructions are shown in the Appendix. Translations are blinded and given in random order to prevent biases. Each annotator was asked to annotate 377 sentences for fluency and adequacy each and the sets for fluency and adequacy are distinct. Among those, 50 sentences appear twice in order to measure intra-annotator agreement, and 100 sentences are common across all annotators in order to measure inter-annotator agreement. We report Kohen\u2019s Kappa scores (Landis and Koch, 1977) for annotator agreement on Table 3. We test statistical significance with three-way p-values computed using the ANOVA test (Heiberger and Neuwirth, 2009). We also report results of the ttest, comparing the FWD and BT systems. Our human evaluation results are presented in Table 2. In terms of adequacy on the original portion of the dataset, we see that all systems perform very similarly, with no significant differences between systems. On the reverse portion of the dataset, back-\ntranslation has a slight edge over the baseline, and a more notable edge against the forward translation system which is consistent with related work. In terms of fluency the results are more clear: The backtranslation system clearly produces more fluent output than its competitors, regardless of the translation direction. This finding is consistent with the findings of Edunov et al. (2020) who also show that humans have a preference for backtranslation augmented systems due to their more fluent output.\n<div style=\"text-align: center;\">Adequacy</div>\n<div style=\"text-align: center;\">Fluency</div>\nSystem\nOriginal Reverse Original\nReverse\nBaseline\n4.18\n4.05\n3.87\n3.96\nBT\n4.15\n4.14\n4.05\n4.10\nFWD\n4.10\n3.98\n3.60\n3.7\np-value\n0.244\n<0.011 <0.00001 <0.00001\nt-test on FWD and BT\np-value\n0.33\n0.003 <0.00001 <0.00001\nAnnotator\nFluency\nAdequacy\n0\n0.28\n0.50\n1\n0.38\n0.67\n2\n0.47\n0.61\n3\n0.38\n0.69\n{0 1 2 3}\n0.16\n0.26\n# 7 Language model experiments\nBLEU scores are insufficient to draw conclusions about the nature of the improvements both data augmentation methods bring. We previously touched upon two hypotheses:\n translationese effects: the references in the reverse portion are native-produced text, those in the original portion may contain translationese artifacts. Training on backtranslations may improve language modelling and favour the production of more native-like text, while training on forward translations may bias the MT system towards producing more translationese text.\n\u2022 domain effects: there may be subtle domain differences in the synthetic data sets, mirror-\ning differences between the two portions of the test set.\nWe designed a language modelling experiment in order to distinguish between the two explanations. Specifically, we measure the similarity between training and test sets by training language models on our training data, and measuring perplexity to variants of the test sets. We used a transformer language model architecture with 8 layers and 8 heads, similar to the transformer-base machine translation systems. We used the same preprocessing and BPE settings as our translation experiments. We trained four language models using the data that we had prepared for forward and back-translation: two native English and French language models and two English and French translationese models (we denote the latter two ENMT and FRMT, respectively). The language models computed on the machine translated data exhibit specific features: They are trained on sampled data so we expect below average fluency, but good adaptation to the domain (source-side news or target-side news). Therefore we expect that the native French language model will perform better (i.e. have lower perplexity) on native French text compared to a translationese French language model, as the style and the domain of the native text match with those of the native language model. We expect that we will observe the same effect when evaluating native vs translationese English language model on native English text. When considering translated test sets, we will expect them to be closer to the translationese language models \u2013 this is both compatible with the interpretation that the two types of texts are similar because they are both translationese, as well as the interpretation that they are similar because they are from the same source-language domain. But what if we have native English data that has been human translated into French and then automatically translated into English? In this case it will share the domain with native English, but after the intermediate human translation, we expect the style to be closer to the language model trained on the translationese text. This variant of the test set gives us the most direct answer as to what extent translationese or domain effects affect the similarity between training and test data. Table 4 shows the language model performance of the native French language model and the translationese French language model. We observe that\nLM\ntest set\nFRnative\nFRMT\nnative FR\n99.22\n118.83\nHTEN\u2192FR\n113.98\n117.97\nTable 4: Language model perplexity on the French side of the combined directional datasets, normalised by number of sentences. We distinguish whether test sets are native French or human translation (HT).\nLM\ntest set\nENnative\nENMT\nnative EN\n101.90\n118.71\nHTEN\u2192FR, MTFR\u2192EN\n98.01\n99.71\nMTFR\u2192EN\n102.28\n94.43\nHTFR\u2192EN\n113.99\n111.90\nTable 5: Language model perplexity on the English side of the combined directional datasets, normalised by number of sentences. We distinguish whether test sets are native English, human translation (HT), machine translation (MT), or roundtrip translation with multiple translation steps.\nunsurprisingly, the language model trained on original French data shows lower perplexity on the original French data than the one trained on MT translated French. Somewhat surprisingly the trend is maintained in the translationese French dataset, even if the two perplexity scores are closer to each other. This is unlike the results on the English language models on Table 5, where the language model that performs better is always the one trained on the same original language as the original language of the dataset. Of most interest are the result for HTEN\u2192FR, MTFR\u2192EN, i.e. the roundtrip translation of native English text. Based on our hypothesis that sourcelanguage and target-language domains are slightly different, we expect the ENnative LM to perform better than ENMT. Based on the more established explanation that the main distinguishing feature of translated text are translationese artefacts, we would expect ENMT to perform better than ENnative. In fact, perplexities are very close to each other, suggesting that domain effects and translationese effects both come into play, and roughly balance each other out.\n# 8 Domain identification experiments\nInspired by the work of Caswell et al. (2019); Marie et al. (2020), who tag back-translated data on the\nsource side to distinguish it from parallel data, we explore if translation models can learn whether training instances come from the source-language or target-language \u201cdomain\u201d. To this end, we train a French\u2192English translation model only using synthetic training data (both forward translations and back-translations), and we add a tag at the beginning of the target sentence indicating the original language. The resulting model correctly identifies the original language in 83% of training set sentences. When evaluating it on test sets, the model has a marked preference to identify the original language as French. On the originally French portion, the model found 89.4% of the sentences be native French, whereas on the human translated French portion, the model predicts 51% of the sentences to be native French. Caswell et al. (2019) motivate source-side tags as a way to help the system distinguish backtranslations from parallel text in lieu of noise. While we did not test the effectiveness of sourceside tags, our experiment shows that even a model without them can predict the provenance of source sentences relatively well. The fact that prediction accuracy remains far above chance level (69%) on human translations shows that the high classification accuracy cannot be simply explained by the model learning to identify MT noise; the signal the model uses to correctly classify the test sets are either domain effects, or translationese effects shared between human translation and MT.\n# 9 Other Language Pairs\nTo see if our findings generalise to other language pairs, we trained Estonian\u2192English and Finnish\u2192English translation models, following the procedure described in Section 4. In order to better control for domain and style, we only use the parallel news crawl data from the WMT18 (Bojar et al., 2018) translation task, which resulted in 3.1M sentence pairs for Finnish\u2013English and 0.9M sentence pairs for Estonian\u2013English. For data augmentation, we use all the available news-crawl on the Estonian/Finnish side for forward translation and the equivalent amount of English newscrawl for back-translation. This resulted in 14.5M monolingual sentences for FinnishEnglish back/forward translation and 2.9M sentences for Estonian-English back/forward translation. We again produced an RNN and a transformer variant of the synthetic data.\nOriginal (Estonian source)\nBaseline\n18.0\n19.4\nBTrnn\n17.1\n17.9\nBTtransformer\n20.8\n21.5\nFWDrnn\n16.2\n17.4\nFWDtransformer\n19.6\n20.8\nReverse (Translationese Estonian source)\nBaseline\n20.2\n20.6\nBTrnn\n23.2\n22.8\nBTtransformer\n29.4\n28.0\nFWDrnn\n17.9\n18.3\nFWDtransformer\n20.5\n20.8\nFull test set\nBaseline\n19.1\n20.0\nBTrnn\n20.1\n20.5\nBTtransformer\n25.1\n24.9\nFWDrnn\n17.1\n17.8\nFWDtransformer\n20.6\n20.8\nOriginal (Estonian source)\nBaseline\n18.0\n19.4\nBTrnn\n17.1\n17.9\nBTtransformer\n20.8\n21.5\nFWDrnn\n16.2\n17.4\nFWDtransformer\n19.6\n20.8\nTable 6: BLEU scores on Estonian\u2192English. The RNN and Transformer subscripts refer to the system used for producing backtranslation.\nWe present our results in Tables 6 and 7. In the case of Estonian (Table 6), we have a scenario which produced particularly poor synthetic data: The RNN English\u2013Estonian, reaches just 12 BLEU on the dev set, while the transformer\u201418 BLEU. On Estonian-English the RNN reaches 15 BLEU, while the transformer\u201417. We see that when BLEU is low, the quality of the synthetic data is much more important: The systems augmented with transformer back-translation gained 4.7 BLEU points on average against the RNN back-translation. Relatively, the forward translation system has improved significantly more: Just 2 BLEU points of difference between the RNN and transformer models used to create the synthetic data resulted in 3.2 points increase in BLEU. This suggests that data augmentation via forward translation is substantially more sensitive to the translation quality of the initial translation system than back-translation. Our observations are confirmed in the slightly higher-resource experiment on Finnish\u2192English (Table 7). The quality of the translation model used for back-translation was improved by 9 BLEU (from 17 to 26) when using a transformer instead of an RNN, but on the final system, this yielded just 1.1 BLEU increase on average. In contrast, the quality of the translation system used for forward translation was improved from 17 to 23 BLEU, improving the final system by 2 BLEU on average.\nBaseline\n22.6\n24.2\n24.2\n19.4\nBTrnn\n20.9\n21.7\n21.7\n19.4\nBTtransformer\n21.5\n22.9\n22.2\n20.0\nFWDrnn\n19.6\n22.2\n22.4\n13.9\nFWDtransformer\n22.4\n24.3\n24.2\n15.0\n<div style=\"text-align: center;\">Reverse (Translationese Finnish source)</div>\nBaseline\n18.9\n22.7\n26.1\n22.1\nBTrnn\n22.6\n28.8\n31.5\n20.3\nBTtransformer\n24.1\n30.4\n33.3\n21.4\nFWDrnn\n16.8\n20.4\n23.4\n20.1\nFWDtransformer\n18.3\n22.5\n26.0\n22.2\nFull test set\nBaseline\n20.6\n23.4\n25.2\n18.3\nBTrnn\n21.9\n25.7\n27.2\n19.8\nBTtransformer\n23.0\n27.1\n28.4\n20.6\nFWDrnn\n18.1\n21.2\n22.9\n16.5\nFWDtransformer\n20.2\n23.3\n25.2\n18.1\nTable 7: BLEU scores on Finnish\u2192English. The RNN and Transformer subscripts refer to the system used for producing backtranslation.\n# 10 Conclusions\nIn this paper we reviewed the effect of directionality on machine translation results, focusing both on the direction of data augmentation (forward and back-translation), and the original language of test sets, focusing on French\u2192English as a case study, with additional experiments on Estonian\u2192English and Finnish\u2192English. We confirm that the original language of parallel test sets affects BLEU scores, particularly when data augmentation approaches are compared. We find that back-translation is more effective than forward translation in the artifical setting where the input to the translation system is itself a human translation, and the original text is used as reference. In the natural setting where the input is native text, and the reference a human translation, forward translation can perform better in terms of BLEU, although it still trails behind back-translation if the forward translations in the synthetic data sets are very poor, indicating that forward translation is more sensitive to the quality of the system that produced it compared to backtranslation. However, manual evaluation shows that better BLEU scores do not necessarily correspond to better translation quality according to human judgements. Despite wildly differing BLEU results depending on the original language of the test sets, humans evaluators prefer our backtranslated sys-\ntems over our other systems in terms of fluency. Despite achieving higher BLEU on the original portion of the test set, our forward-translation system was rated worst in the human evaluation. To better understand the differences between forward and back-translation, we consider both translationese effects and subtle domain differences between source-language and target-language monolingual data. Language model experiments indicate that both of these play a role, and partially explain why back-translation is so suitable for reverse test sets. Experiments with translation systems trained on only synthetic data (forward and back-translation) also show that the provenance of test set sentences is predictable with 69% accuracy. Our findings are agree with concurrent and independent work by Shen et al. (2019), who perform low-resource translation experiments with backtranslation and self-learning, an iterative form of forward translation. They also find that the original language of parallel test data determines whether back-translation or forward translation is a more effective strategy for data augmentation. Based on our findings, we can make several recommendations for the use of forward translation and back-translation to augment neural machine translation. Firstly, while BLEU is very sensitive to the choice of data augmentation, with up to 6 BLEU difference between the two choices in our French\u2192English experiments, depending on the make-up of the test set, human annotators were less sensitive to test set directionality. Human annotators favoured backtranslation over forward translation, mostly in terms of fluency, while adequacy was largely the same across all of them, especially on the original translation direction. Our results should serve as a warning to not over-rely on automatic evaluation when data augmentation is involved: The results of the WMT19 news translation task (Barrault et al., 2019), also show negative correlation between BLEU and human evaluation. Secondly, we observe subtle domain differences between corpora in different languages, even if they cover the same general domain (news) and were collected with the same methods. Following the general heuristic to use training data that matches the test domain as closely as possible, this may be an argument for using forward translations in settings where (only) in-domain source data is available, but further study of this setting is necessary. Of course, the use of forward and back-translation\nis not mutually exclusive, and in settings with access to suitable monolingual corpora in both the source and target language, combining the two is another viable strategy (Wu et al., 2019).\n# References\ngium, Brussels. Association for Computational Linguistics.\nOnd\u02c7rej Bojar and Ale\u0161 Tamchyna. 2011. Improving translation model by monolingual data. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 330\u2013336, Edinburgh, Scotland. Association for Computational Linguistics.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858\u2013867, Prague, Czech Republic. Association for Computational Linguistics.\nSergey Edunov, Myle Ott, Marc\u2019Aurelio Ranzato, and Michael Auli. 2019. On the evaluation of machine translation systems trained with back-translation.\nIustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov. 2010. Identification of translationese: A machine learning approach. In Computational Linguistics and Intelligent Text Processing, pages 503\u2013511, Berlin, Heidelberg. Springer Berlin Heidelberg.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, Andr\u00e9 F. T. Martins, and Alexandra Birch. 2018. Marian: Fast neural machine translation in C++. In Proceedings of ACL 2018, System Demonstrations, pages 116\u2013 121, Melbourne, Australia. Association for Computational Linguistics.\nMoshe Koppel and Noam Ordan. 2011. Translationese and its dialects. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1318\u20131326, Portland, Oregon, USA. Association for Computational Linguistics.\nDavid Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009. Automatic detection of translated text and its impact on machine translation. In In Proceedings of MT-Summit XII, pages 81\u201388.\nPatrik Lambert, Holger Schwenk, Christophe Servan, and Sadaf Abdul-Rauf. 2011. Investigations on Translation Model Adaptation Using Monolingual Data. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 284\u2013293, Edinburgh, Scotland. Association for Computational Linguistics.\nJ. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1).\nGennadi Lembersky, Noam Ordan, and Shuly Wintner. 2012. Adapting translation models to translationese improves SMT. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 255\u2013265, Avignon, France. Association for Computational Linguistics.\nBenjamin Marie, Raphael Rubino, and Atsushi Fujita. 2020. Tagged back-translation revisited: Why does it really work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5990\u20135997, Online. Association for Computational Linguistics.\nNitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984\u20134997, Online. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh neural machine translation systems for WMT 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 371\u2013376, Berlin, Germany. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86\u201396, Berlin, Germany. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016c. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013 1725, Berlin, Germany. Association for Computational Linguistics.\n# A Human Evaluation Protocol\nThe annotators were given the following instructions: Fluency is simply how natural a sentence sounds. You will have to rate the sentences produced by three different systems in terms of how good the English is. Use the following scale: 1 Incomprehensible 2 Disfluent English 3 Non-native English 4 Good English 5 Flawless English Adequacy on the other hand, tries to judge how good the meaning is conveyed, compared to a reference translation. For this task, you will be given a reference translation, and the translations produced by three different systems. You are required to rate each of them using the following scale: 1 None 2 Little 3 Much 4 Most 5 All Please pay special attention to sentences that are \"almost\" correct but a crucial word is missed or one with reverse meaning is used (eg \"convicted\" vs \"acquitted\"). Note that it is possible to have all systems produce equally good (or equally bad), yet different results For this task, you should ignore how fluent (or disfluent) the English is and focus just on the meaning You should not penalise a system for having a bad language, as long as the meaning is conveyed.\n# B Manual analysis\nIn this section we present manual analysis of sentences produced by all of our systems on Original French source (Figure 2) and on Translationese English source and (Figure 3). We noticed that in the case of original French source, sometimes the backtranslation system gets unfavourably penalised for producing a correct translation, that is more fluent than the reference (Figure 2, example 1). When the backtranslation is presented with an input that is confusing to it, it tends to just copy, instead of producing a translations (Figure 2, examples 2 and 3), whereas the forward translation system never suffers from this issue on original French source. On Translationese French source, we can see that the forward translation struggles with undertranslation on some out of domain sequences (Figure 3, example 7). It is interesting to note that, rare named entities such as \"Xindu\" on example 5, get translated as \"Xinhua\" by the backtranslation system, because this token is much more common there than in the training set (and completely absent in the forward translated data). Strangely enough, the backtranslation system tends to hallucinate \"he said\" when it translates quotes (Figure 3, example 6).\nsource\nEsp\u00e9rons que cela ne sera pas le cas.\nreference\nLet us hope that this is not the case.\nbaseline\nLet us hope that will not be the case.\nFWD\nLet us hope that this will not be the case.\nBT\nHopefully that won\u2019t happen.\nsource\nMais \u00e7a n\u2019est pas l\u2019entr\u00e9e en lice de S\u00e9gol\u00e8ne Royal qui pousse Nicolas Sarkozy \u00e0 se d\u00e9voiler.\nreference\nBut it is not the entry into the lists of Segolene Royal, which pushes Nicolas Sarkozy to unmask himself.\nbaseline\nBut it was not the entry of S\u00e9gol\u00e8ne Royal that led Nicolas Sarkozy to discover himself.\nFWD\nBut it is not the entry into the contest of S\u00e9gol\u00e8ne Royal, which pushes Nicolas Sarkozy to unveil himself.\nBT\nMais \u00e7a n\u2019est pas l\u2019entr\u00e9e en lice de S\u00e9gol\u00e8ne Royal qui pousse Nicolas Sarkozy \u00e0 se prononcer.\nsource\nL\u2019Eure, l\u2019Eure-et-Loir, les Deux-S\u00e8vres et la Vienne, se sont ajout\u00e9s aux 20 d\u00e9partements d\u00e9j\u00e0 en vigilance\norange, 18 depuis mercredi apr\u00e8s-midi et deux, la Dordogne et la Gironde, plus tard dans la nuit.\nreference\nEure, Eure-et-Loir, Deux-S\u00e8vres ad Vienne have joined the 20 d\u00e9partements already on orange alert, 18 since\nWednesday afternoon and two, Dordogne and Gironde, later during the night.\nbaseline\nthe Eure , the Eure and Loir , the Two Shares and Vienna have joined the 20 departments already under orange\nvigilance , 18 since Wednesday afternoon and two , the Dordogne and the Gironde , later in the night .\nFWD\nThe Eure, Eure-et-Loir, the Two-Shares and Vienna have been added to the 20 departments already in orange\nvigilance, 18 since Wednesday afternoon and two, Dordogne and Gironde later in the night.\nBT\nL\u2019Eure, l\u2019Eure-et-Loir, les Deux-S\u00e8vre et la Vienne, se sont ajout\u00e9 aux 20 d\u00e9partements d\u00e9j\u00e0 en vigilance orange,\n18 depuis mercredi apr\u00e8s-midi et deux, la Dordogne et la Gironde, plus tard dans la nuit.\nsource\nLe but des futurs acqu\u00e9reurs est de rouvrir les trois salles le 27 f\u00e9vrier prochain en projetant le tr\u00e8s beau film de\nXavier Beauvois Des hommes et des dieux.\nreference\nThe aim of future purchasers is that of reopening the three halls on 27 February, showing the very beautiful film\nby Xavier Beauvois entitled \"Men and Gods.\"\nbaseline\nThe goal of future buyers is to reopen the three rooms on February 27 by screening Xavier Beauvus\u2019s very\nbeautiful film Men and gods.\nFWD\nThe purpose of future buyers is to reopen the three rooms on February 27 by projecting Xavier Beauvoir\u2019s\nbeautiful film Les hommes et des gods.\nBT\nThe goal of future buyers is to re-open the three rooms on February 27 next year with Xavier Beauvoir\u2019s beautiful\nfilm Men and Gods.\nsource\n\"Ce qui est int\u00e9ressant, c\u2019est que j\u2019ai progress\u00e9 \u00e0 chaque comp\u00e9tition.\"\nreference\n\"What is interesting is that I did better at each competition.\"\nbaseline\n\"What\u2019s interesting is that I made progress in every competition.\"\nFWD\n\"What is interesting is that I have made progress at each competition.\"\nBT\n\"What\u2019s interesting is that I\u2019ve progressed to every competition,\" he said.\nsource\nLe nombre de ch\u00f4meurs s\u2019\u00e9tablit \u00e0 2,631 millions en m\u00e9tropole.\nreference\nThe number of unemployed is 2.631 million in metropolitan France.\nbaseline\nThe number of unemployed in metropolitan areas is 2,631 million.\nFWD\nThe number of unemployed is 2,631 million in metropolitan France.\nBT\nThe number of unemployed is 2.631 million in metropolitan areas.\n<div style=\"text-align: center;\">Lundi soir, Peek et Rickard ont v\u00e9rifi\u00e9 dans le motel. On Monday night, Peek and Rickard checked into the motel.</div>\nreference\nOn Monday night, Peek and Rickard checked into the motel.\nbaseline\nHe checked in the motel on Monday evening.\nFWD\nOn Monday evening, Peek and Rickard checked in the motel.\nBT\nOn Monday night, Peek and Rickard checked into the motel.\nsource\nIl a m\u00eame ordonn\u00e9 \u00e0 M. Su, un pragmatique, de ne pas briguer la pr\u00e9sidence.\nreference\nHe even ordered Mr Su, a pragmatist, not to run for the presidency.\nbaseline\nHe even ordered Mr President, a pragmatic one, not to run for the presidency.\nFWD\nHe has even ordered a pragmatic President to not run the presidency.\nBT\nHe even ordered Mr Su, a pragmatic, not to run for the presidency.\nsource\nC\u2019est bien.\"\nreference\nThat is good.\"\nbaseline\nIt is good.\"\nFWD\nThat is good.\"\nBT\nIt\u2019s good.\"\"\nsource\nDans un jugement de 1966 qui a fait date, la Cour supr\u00eame des \u00c9tats-Unis a annul\u00e9 la condamnation d\u2019un\nost\u00e9opathe de Cleveland, le Dr Sam Sheppard, pour l\u2019assassinat de sa femme, soutenant dans sa d\u00e9cision qu\u2019il y\navait une \"atmosph\u00e8re de carnaval au proc\u00e8s \" \u00e0 cause des m\u00e9dias.\nreference\nIn a landmark 1966 ruling, the United States Supreme Court overturned the conviction of a Cleveland osteopath,\nDr. Sam Sheppard, for the murder of his wife, saying in its decision, Sheppard v. Maxwell, that there was a\n\"carnival atmosphere at trial\" because of the news media.\nbaseline\nIn a landmark 1966 judgement, the Supreme Court of the United States set aside the conviction of an osteopcom-\nmitted by him for the murder of his wife. In its decision, the Supreme Court of the United States ruled that there\nwas a \"carnaval atmosphere at trial\" because of the media.\nFWD\nIn a 1966 judgement, the Supreme Court of the United States set aside the conviction of a Cleveland osteopath,\nDr. Sam\nBT\nIn a 1966 ruling, the U.S. Supreme Court overturned the conviction of a Cleveland osteopath, Dr. Sam, for the\nmurder of his wife, arguing in his decision that there was a \"carnival atmosphere at trial\" because of the media.\nsource\nXindu a jusqu\u2019ici \u00e9chapp\u00e9 \u00e0 une telle violence.\nreference\nXindu has so far escaped such violence.\nbaseline\nSo far, such violence has been avoided.\nFWD\nXunfair has so far escaped such violence.\nBT\nXinhua has so far escaped such violence.\nsource\n\"Luzhkov est au sommet.\"\nreference\n\"Luzhkov is at the top.\"\nbaseline\n\"Luzhkov is at the top.\"\nFWD\n\"Luzhkov is at the top.\"\nBT\n\"Luzhkov is at the top, he said.\"\nsource\nLes quatre chiens d\u2019exposition - deux akitas et deux corgis pembroke gallois - ont disparu mardi lorsque\nquelqu\u2019un a vol\u00e9 la camionnette Chevrolet Express dans laquelle ils avaient pass\u00e9 la nuit \u00e0 l\u2019ext\u00e9rieur d\u2019un motel\n6 de Bellflower, ont annonc\u00e9 les autorit\u00e9s.\nreference\nThe four show dogs - two Akitas and two Pembroke Welsh corgis - vanished Tuesday when someone stole the\n2006 Chevrolet Express van they had spent the night in outside a Bellflower Motel 6, authorities said.\nbaseline\nThe four display dogs - two ak\nFWD\nThe four exposure dogs - two akitas and two corgis\nBT\nThe four showbiz dogs - two akitas and two Welsh Corgis - disappeared Tuesday when someone stole the\nChevrolet Express van in which they had spent the night outside a Bellflower motel 6, authorities said.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The quality of neural machine translation can be improved by leveraging additional monolingual resources in various ways. Back-translation has been widely used in shared translation tasks, but concerns have arisen regarding the artifacts in standard test sets that may affect performance evaluations.",
            "purpose of benchmark": "The benchmark aims to compare the effectiveness of forward and back-translation in neural machine translation, focusing on their impact on translation quality and human evaluations."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of evaluating translation quality in neural machine translation systems, particularly in the context of forward and back-translation methods.",
            "key obstacle": "Existing benchmarks often do not account for the directionality of test sets, which can skew results and obscure the true effectiveness of translation methods."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need to understand how the original language of test sets influences translation quality, particularly in differentiating between forward and back-translation outcomes.",
            "opinion": "The authors believe that this benchmark is crucial for providing clearer insights into the effectiveness of different data augmentation strategies in neural machine translation.",
            "innovation": "This benchmark introduces a systematic approach to evaluating translation quality by considering the original language of test sets, which has not been adequately addressed in previous benchmarks.",
            "benchmark abbreviation": "NMT-BT-FWD"
        },
        "dataset": {
            "source": "The dataset was created using the WMT 15 English-French news translation task dataset, along with monolingual data from News Crawl corpora.",
            "desc": "The dataset consists of 35.8M parallel sentences, supplemented by 49.8M English and 46.1M French monolingual sentences.",
            "content": "The dataset includes parallel sentences in French and English, as well as monolingual sentences for both languages.",
            "size": "35,800,000",
            "domain": "Translation",
            "task format": "Machine Translation"
        },
        "metrics": {
            "metric name": "BLEU",
            "aspect": "Translation quality",
            "principle": "BLEU was chosen as it is a widely accepted metric for evaluating machine translation performance, despite its limitations.",
            "procedure": "Model performance is evaluated using BLEU scores calculated on both the original and reverse portions of the test sets."
        },
        "experiments": {
            "model": "The models tested include baseline systems and those augmented with back-translation and forward translation, utilizing both RNN and transformer architectures.",
            "procedure": "The models were trained using both back-translation and forward translation techniques, with early stopping based on a development set.",
            "result": "The experiments showed that back-translation generally outperformed forward translation, especially in terms of BLEU scores on the reverse portion of the test set.",
            "variability": "Variability was accounted for by conducting multiple trials and using different subsets of the dataset for evaluation."
        },
        "conclusion": "The benchmark reveals that while back-translation often yields higher BLEU scores, human evaluations indicate a preference for the fluency of back-translated systems, highlighting the discrepancy between automatic and human assessments.",
        "discussion": {
            "advantage": "The benchmark provides a clearer understanding of the effects of translation directionality on machine translation performance, contributing valuable insights to the field.",
            "limitation": "One limitation is that BLEU scores may not fully capture translation quality, particularly in cases where human evaluations differ significantly.",
            "future work": "Future research could explore combining forward and back-translation techniques to further enhance translation quality in various contexts."
        },
        "other info": {
            "additional details": {
                "human evaluation protocol": "Human evaluators rated translations based on fluency and adequacy, revealing significant insights into the performance of different systems."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The quality of neural machine translation can be improved by leveraging additional monolingual resources in various ways."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark aims to compare the effectiveness of forward and back-translation in neural machine translation, focusing on their impact on translation quality and human evaluations."
        },
        {
            "section number": "2.1",
            "key information": "The models tested include baseline systems and those augmented with back-translation and forward translation, utilizing both RNN and transformer architectures."
        },
        {
            "section number": "2.2",
            "key information": "Back-translation generally outperformed forward translation, especially in terms of BLEU scores on the reverse portion of the test set."
        },
        {
            "section number": "3.1",
            "key information": "This benchmark introduces a systematic approach to evaluating translation quality by considering the original language of test sets."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that BLEU scores may not fully capture translation quality, particularly in cases where human evaluations differ significantly."
        },
        {
            "section number": "6.3",
            "key information": "Future research could explore combining forward and back-translation techniques to further enhance translation quality in various contexts."
        }
    ],
    "similarity_score": 0.6667338142582816,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0239_large/papers/Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation.json"
}