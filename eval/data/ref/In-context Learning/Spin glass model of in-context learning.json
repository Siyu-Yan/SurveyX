{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.02288",
    "title": "Spin glass model of in-context learning",
    "abstract": "Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and further clarifies why an unseen function can be predicted by providing only a prompt yet without further training. Our theory reveals that for single-instance learning, increasing the task diversity leads to the emergence of in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed analytically tractable model thus offers a promising avenue for thinking about how to interpret many intriguing but puzzling properties of large language models.",
    "bib_name": "li2024spinglassmodelincontext",
    "md_text": "# Spin glass model of in-context learning\nYuhao Li1, Ruoran Bai1, and Haiping Huang1,2\u2217 1PMI Lab, School of Physics, Sun Yat-sen University, Guangzhou 510275, People\u2019s Republic of China and 2Guangdong Provincial Key Laboratory of Magnetoelectric Physics and Devices, Sun Yat-sen University, Guangzhou 510275, People\u2019s Republic of China (Dated: November 14, 2024)\nLarge language models show a surprising in-context learning ability\u2014being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and further clarifies why an unseen function can be predicted by providing only a prompt yet without further training. Our theory reveals that for single-instance learning, increasing the task diversity leads to the emergence of in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed analytically tractable model thus offers a promising avenue for thinking about how to interpret many intriguing but puzzling properties of large language models.\nIntroduction.\u2014 Thanks to earlier breakthroughs in processing natural languages (e.g., translation), vector representation and attention concepts were introduced into machine learning [1\u20134], which further inspired a recent breakthrough of implementing the self-attention as a feedforward model of information flow, namely transformer [5]. The self-attention captures dependencies between different parts of the input (e.g., image or text), coupled with a simple cost of next-token prediction [6, 7], leading to a revolution in the field of natural language processing [8], so-called large language model (LLM). One of the astonishing abilities of the transformer is the in-context learning [9], i.e., the pre-trained transformer is able to accomplish previously-unseen complicated tasks by showing a short prompt in the form of instructions and a handful of demonstrations, especially without a need for updating the model parameters. LLMs thus develop a wide range of abilities and skills (e.g., question answering, code generation) [10], which are not explicitly contained in the training dataset and are not specially designed to optimize. This remarkable property is achieved only by training for forecasting the next tokens and only if corpus and model sizes are scaled up to a huge number [11, 12]. The above characteristics of transformer and the in-context learning (ICL) are in stark contrast to perceptron models in the standard supervised learning context, presenting a formidable challenge for a mechanistic interpretation [13, 14]. To achieve a scientific theory of ICL, previous works focused on optimization via gradient descent dynamics [15, 16], representation capacity [17], Bayesian inference [18, 19], and in particular the pre-training task diversity [19\u201322]. The theoretical efforts were commonly based on a single-layer linear attention [15, 21, 23, 24], which revealed that a sufficient pre-training task diver-\nsity guarantees the emergence of ICL, i.e., the model can generalize beyond the scope of pre-training tasks. However, rare connections are established to physics models, which makes a physics model of ICL lacking so far, preventing us from a deep understanding of how ICL emerges from pre-trained model parameters. Here, we treat the transformer learning as a statistical inference problem, and then rephrase the inference problem as a spin glass model, where the transformer parameters are turned into real-valued spins, and the input sequences act as a quenched disorder, which makes the spins strongly interact with each other to lower down the ICL error. A unique spin solution exists in the model, guaranteeing that the transformer can predict the unknown function embedded in test prompts. The derived formulas specify the intelligence boundary of ICL and how this can be achieved. Transformer with linear attention.\u2014 We consider a simple transformer structure\u2014a single-layer selfattention transforming an input sequence to an output one. Given an input sequence X \u2208RD\u00d7N, where D is the embedding dimension and N is the context length, the self-attention matrix is a softmax function Softmax(Q\u22a4K/ \u221a D), where Q = WQX, K = WKX. WQ and WK are the query and key matrices (\u2208RD\u00d7D), respectively. \u221a D inside the softmax function makes its argument order of unity. The self-attention refers to the attention matrix generated from the input sequence itself and allows each element (query) to attend to all other elements in one input sequence, being learnable through pre-training. The softmax function is thus calculated independently for each row. Taking an additional transformation V = WVX, where WV \u2208RD\u00d7D is the value matrix, one can generate the output Y = V \u00b7 Softmax(Q\u22a4K/ \u221a D). Hence, this simple transformer\nimplements a function \u03c6TF(X) : RD\u00d7N \u2192RD\u00d7N. For simplicity, we replace the computationally expensive softmax by linear attention, which is still expressive [25]. Defining W \u2261W\u22a4 QWK, and choosing WV = 1D (1D indicates a D \u00d7 D identity matrix) for our focus on the query and key matrices, we re-express the linear transformer as Y = 1 DN XX\u22a4WX, where X contains prompts and the query (to be predicted by the transformer), and W \u2208RD\u00d7D is the equivalent weight matrix to be trained, and 1 DN is a normalization coefficient. We next design the training task as a high-dimensional linear regression. Each example consists of the data x \u223cN(0, 1D) and the corresponding label y = w\u22a4x, where the latent task weight w \u223cN(0, 1D). To construct the \u00b5-th input matrix X\u00b5, we use N samples as prompts using the same w\u00b5 yet different x within the input sequence. An additional sample \u02dcx\u00b5 is regarded as the query whose true label \u02dcy is masked yet to be predicted by the transformer. The structure of each input matrix X\u00b5 is thus represented as\n(1)\nDue to this form of the input matrix, the number of trainable elements in W becomes (D+1)2. The last element of Y corresponds to the predicted label of the query \u02dcx\u00b5, i.e., \u02c6y\u00b5 = Y\u00b5 D+1,N+1. The goal of ICL is to use the prompt to form a prediction for the query, and the true function governing the linear relationship for the testing prompt is hidden during pre-training, because each \u00b5 is generated by an independently drawn w during both training and test phases. We consider an ensemble of P sequences, and P is thus called the task diversity. This setting is a bit different from that in recent works [19, 21]. The pre-training is carried out by minimizing the mean squared error function, and the total training loss is given by\n(2)\nwhere \u03bb controls the weight-decay strength. The generalization error on unseen tasks is written as \u03f5g = E\u02dcx,x,w(\u02dcy \u2212\u02c6y)2, where the ensemble average over all disorders is considered. Spin-glass model mapping.\u2014 Equation (2) can be treated as a Hamiltonian in statistical physics. The linear attention structure makes the spin-model mapping possible. This proceeds as follows. The prediction to the \u00b5-th input matrix can be recast as \u02c6y\u00b5 = (DN)\u22121 \ufffd m,n C\u00b5 D+1,mWm,nX\u00b5 n,N+1, where C\u00b5 \u2261 X\u00b5X\u00b5\u22a4. Then we define an index mapping \u0393 : (m, n) \u2192 i to flatten a matrix into a vector. Therefore, one can write \u03c3i = \u0393Wm,n, and s\u00b5 i = (DN)\u22121\u0393C\u00b5 D+1,mX\u00b5 n,N+1, where i = (D + 1)(m \u22121) + n, and finally the prediction\nas \u02c6y\u00b5 = \ufffd i s\u00b5 i \u03c3i. Consequently, the mean-squared error for each input matrix reads\n(3)\nwhere we omit the constant term (\u02dcy\u00b5)2/2. Upon defining J\u00b5 ij \u2261\u2212s\u00b5 i s\u00b5 j , h\u00b5 i \u2261\u02dcy\u00b5s\u00b5 i , and \u03bb\u00b5 i \u2261 \u03bb\u2212J\u00b5 ii, one can rewrite the total loss L = (1/P) \ufffd \u00b5 \u2113\u00b5 + 1 2\u03bb\u2225W\u22252 as\n(4)\nBy moving the elements for i = j in the first term to the regularization term, and defining an anisotropic regularization coefficient \u03bb\u00b5 i \u2261\u03bb \u2212J\u00b5 ii, we can formally define the effective interaction Jij \u2261(1/P) \ufffd \u00b5 J\u00b5 ij, the external field hi \u2261(1/P) \ufffd \u00b5 h\u00b5 i and the regularization factor \u03bbi \u2261(1/P) \ufffd \u00b5 \u03bb\u00b5 i . Finally, we obtain a spin glass model with the following Hamiltonian\n(5)\nwhere the total number of spins is given by (D + 1)2. The disorder in the pre-training dataset including the diversity in the latent task vectors w is now encoded into the interactions between spins, and random fields the spins feel. In fact, this is a densely connected spin glass model, while the coupling and field statistics do not have an analytic expression [26], as they bear a fat tail (Fig. 1). This reminds us of the two-body spherical spin model studied in spin glass theory [27, 28], but the current glass model of ICL seems much more complex than the spherical model. By construction, this model reflects the nature of associative memory [29], yet the spin variable is now the underlying parameter of the transformer. To conclude, we derive a spin glass model of ICL, opening a physically appealing route towards the mechanistic interpretation of ICL and even more complex transformers (see extra experiments in [30]). Statistical mechanics analysis.\u2014 The probability of each weight configuration is given by the GibbsBoltzmann distribution P(\u03c3) = e\u2212\u03b2H(\u03c3)/Z, where Z is the partition function, and \u03b2 is an inverse temperature tuning the energy level. Because the statistics of couplings and fields have no analytic form, one has to use the cavity method widely used in spin glass theory [31]. The cavity method is also known as the belief propagation algorithm, working by iteratively solving a closed equation of cavity quantity, i.e., a spin is virtually removed [29]. The cavity method can thus be used on single instances of ICL. Different weight components are likely strongly correlated, but the cavity marginal \u03b7i\u2192j(\u03c3i) becomes conditionally independent in the absence of spin j, which\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a68f/a68f9994-dd98-4678-9741-f10e4a51a369.png\" style=\"width: 50%;\"></div>\n(a)\nFIG. 1: Statistical properties of the interaction matrix and the external field. (a) The H matrix corresponding to the external field h has three blocks with different properties: A \u2208 RD\u00d7D, B \u2208RD, and the all-zero vector 0 \u2208RD+1 (the rightmost column). (b) The symmetric J matrix also has three different blocks: C \u2208RD(D+1)\u00d7D(D+1), D \u2208RD(D+1)\u00d7(D+1) and E \u2208R(D+1)\u00d7(D+1). The statistics of different blocks are shown in the right panel. For some (i, j), Jij = 0, but this ratio is (2D + 1)/(D + 1)2 since the last column of S is an all-zero vector. We count the non-zero values for the distribution. All results are plotted based on an average over 100 000 ensembles of (P, D, N) = (1 000, 5, 100).\nfacilitates our derivation of the following self-consistent iteration (namely mean-field equation, see [30] for more details):\n(6)\nwhere zi\u2192j is a normalization constant, and \u03b7i\u2192j is defined as the cavity probability of spin \u03c3i in the absence of the interaction between spins i and j. After the iteration reaches a fixed point, the marginal probability \u03b7i(\u03c3i) of each spin can be calculated by\n(7)\nBecause of the continuous nature of spin and weak but dense interactions among spins, we can further simplify the mean-field equation [Eq. (6)], and derive the approximate message passing (AMP) by assuming \u03b7i(\u03c3i) \u223c N(mi, vi), where (mi, vi) is the fixed point of the follow-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b052/b052871e-bcd7-41db-acd3-79b62834f0c7.png\" style=\"width: 50%;\"></div>\nFIG. 2: Test error and the optimal weight matrix of ICL. (a) The test error of the linear attention trained by the stochastic gradient descent (SGD) method for different task diversities. (b) The weight matrix for P = 10 at the end of training. (c) The weight matrix for P = 1 000 at the end of training. (d) The weight matrix retrieved from the solution {mi} of AMP. (e) The variance matrix retrieved from the solution {vi} of AMP. The color bars are the same with (a). (D, N) = (10, 100), \u03bb = 0 for SGD, P = 5 000, (\u03bb, \u03b2) = (0.01, 100) for AMP.\ning iterative equation:\n(8a)\n(8b)\n \ufffd which is also rooted in the Thouless\u2013Anderson\u2013Palmer equation in glass physics [29, 32]. Technical details of deriving the AMP and the self-consistent justification of the Gaussian approximation are given in the appendix (an expanded one is given in [30]). Results.\u2014 The iteration of the cavity method depends on the specific form of Jij and hi, which rely on Sm,n defined as Sm,n = (DN)\u22121CD+1,mXn,N+1. The index \u00b5 is omitted here. The matrix S is divided into three blocks: the last column is an all-zero vector [due to the masked label in Eq. (1)], while the other two blocks are labeled as A (m < D+1, n \u0338= D+1) and B (m = D+1, n \u0338= D+1). As hi = 1 P \ufffd \u00b5 \u02dcy\u00b5s\u00b5 i , the field matrix has the same block structure with S [Fig. 1 (a)]. In addition the interaction matrix J, generated by the outer product of the flattened S with itself, has three main blocks, labeled as C, D, and E respectively in Fig. 1 (b). In contrast to traditional spin glass models [27], P(J) [or P(h)] does not have an analytic form [26], as the coupling or field can be expressed as a complex function of a sum of products of two i.i.d. standard Gaussian random variables (see details in [30]). Therefore, we provide the numerical estimation of the coupling distribution, which all bear a fat tail [Fig. 1 (b)]. We thus define a new type of spin glass model corresponding to ICL, or a metaphor of transformer in large language models. To see whether our spin glass model captures the correct structure of the weight matrix in the simple trans-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c2e5/c2e521c0-cb01-4535-8e5c-f40a417edb4c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98d1/98d17fc4-311e-4dc1-b4c8-030d73dc6353.png\" style=\"width: 50%;\"></div>\nFIG. 3: The energy landscape of the spin glass model for P = 1 000 (a) and P = 10 (b). To draw this landscape, we randomly sample 2 000 points from the whole weight space when D = 10, N = 50, \u03bb = 0.01, and calculate their energies by Eq. (12). Then we reduce the weight space with (D + 1)2 dimension to the two-dimensional plane by t-distributed stochastic neighbor embedding (t-SNE) [33]. The color deep blue indicates lowest energies. The flag on the bottom plane indicates the target weight matrix.\nformer, we first divide the weight matrix W into blocks in the same way as we do for the input matrix, i.e.,\n(9)\nwhere W11 \u2208RD\u00d7D, W12 \u2208RD\u00d71, W21 \u2208R1\u00d7D, and W22 \u2208R. In our linear regression task, the actual prediction of the transformer to the test query \u02dcx can be written as\n(10)\nTo derive the above prediction, we have used 2 \u00d7 2 block matrix form of X, the task y = w\u22a4x, and the fact of i.i.d. {x\u2113} (see technical details in [30]). Hence, the weight matrix of a well-trained transformer must satisfy\n(11)\nIt is clear that, in the case of P > 1, the weights have a unique optimal solution W11 = 1D and W21 = 0. In the standard stochastic gradient descent (SGD) training process minimizing Eq. (2), a large value of P is needed to make the weight matrix converge to the unique solution. In Fig. 2, we show the learning curves and the weight matrix after the training when the amount of training data P = 10 and P = 1 000 respectively. By iterating the AMP equations [Eq. (8a) and Eq. (8b)], we get a fixed point of {mi} and {vi}, transformed back into the matrix form by inverting \u0393. We find that the m matrix exhibits the same property as the weight matrix that is well-trained by the SGD. Since the last column of W is initialized as N(0, 1) and does not participate in the training, the solution of AMP retains the structure of m = 0 and v = 1. This result shows that our spin glass model captures the properties of practical SGD training [24, 34, 35]. In Fig. 3, we show the energy landscape of our spin glass model. When the task diversity P is large enough,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d48b/d48bdd72-6f5d-4d3a-8223-c11ffbc7d574.png\" style=\"width: 50%;\"></div>\nFIG. 4: Results obtained from running the AMP algorithm. (a) The heat map of the contrast C with D = 40, \u03bb = 10, and \u03b2 = 100. In the left-bottom corner, AMP does not converge. (b) The test error decreases with the context (prompt) length N, with D = 20, \u03bb = 0.01, P = 10 000, and \u03b2 = 100. All the results of AMP and SGD are averaged over 100 trials.\nthere is only one global minimum in the energy landscape, and the learning can easily reach the lowest energy. When P is small, there emerge multiple local minima in the energy landscape, and the learning gets easily trapped by metastable states, which prevents the transformer from accurate in-context inference. To get the phase diagram for single instance pretraining of the transformer, we first define a contrast ratio C = (\u27e8m2 ii\u27e9\u2212\u27e8m2 ij\u27e9)/\u27e8m2 ii\u27e9, i \u0338= j to measure whether the model is well trained according to the transformed m matrix. C = 1 means that the model converges to the unique solution, while C = 0 indicates that the model does not learn the features at all. We show a heat map of the contrast ratio with the rescaled number of the data P/D2 and the rescaled prompt length N/D in Fig. 4 (a). The heat map suggests that when the task diversity increases, a smooth transition to perfect generalization occurs, while keeping a large value of task diversity, increasing the prompt length further lowers the generalization error, which is consistent with recent empirical works [19] and theoretical works [21] based on random matrix theory (despite a slightly different setting). In addition, the AMP result coincides perfectly with the SGD [Fig. 4 (b)], which verifies once again that our spinglass model of ICL is able to predict an unseen embedded function in the test prompts, which is determined by the ground states of H(\u03c3). We finally remark that our theory carries over to more complex situations of ICL (see further experiments in [30]). Conclusion.\u2014 A fundamental question in large language models is what contributes to the emergence ability of ICL, i.e., why simple next-token prediction-based pretraining leads to in-context learning of previously unseen tasks, especially without further tuning the model parameters. Here, we turn the ICL into a spin glass model and verify the equivalence between the standard SGD training and our statistical mechanic inference. We ob-\nserve the fat tail distribution of coupling that determines how the model parameters of the transformer interact with each other. The transformer parameters are akin to an ensemble of real-valued spins in physics whose ground state suggests that the model can infer an unknown function from the shown test prompts after a pre-training of input sequences of sufficient task diversity. The phase diagram for single instance learning is also derived by our method, suggesting a continuous ICL transition. The spin-glass model mapping of the linear transformer establishes a toy model of understanding emergent abilities such as ICL of large language models. The ground state determines the intelligence boundary, while the task diversity guarantees the accessibility of the ground state. Without a clear understanding of this toy model, it is hard to imagine what are really computed inside the black box of a general transformer in more complex tasks. Future exciting directions include explaining the chain-of-thought prompting [12], i.e., decomposition of a complex task into intermediate steps, and more challenging case of hallucination [36], i.e., the\n# Derivation of cavity method and approximate message passing equation\nH = \u2212 \ufffd i<j Jij\u03c3i\u03c3j \u2212 \ufffd i hi\u03c3i \u2212\u03bb 2 \ufffd i \u03c32 i ,\nH = \u2212 \ufffd i<j Jij\u03c3i\u03c3j \u2212 \ufffd i hi\u03c3i \u2212\u03bb 2 \ufffd i \u03c32 i ,\nP(\u03c3) = 1 Z e\u2212\u03b2H(\u03c3) = 1 Z \ufffd i e\u03b2hi\u03c3i\u2212\u03b2\u03bb 2 \u03c32 i \ufffd i<j e\u03b2Jij\u03c3i\u03c3j,\n  where Z serves as the partition function. Taking each interaction pair as a factor node and each site term as an external field in Eq. (13), one can write the following cavity iteration in an explicit form according to the standard format of the cavity method (e.g., see the textbook [29]).\nwhich is also called the belief propagation equation. The notation ij denotes an interaction (or factor node in a factor graph representation of the model). Since our model only involves two-body interactions, which means that \u2202ij\\i contains only one element j, the product notation \ufffd j\u2208\u2202ij\\i can be omitted, and we can simplify the notation i \u2192ij by i \u2192j and ij \u2192i by j \u2192i. Therefore, we can combine Eq. (14) and Eq. (15) together into\n  After the iteration reaches a fixed point, the marginal probability \u03b7i(\u03c3i) of each spin can be calculated based on the converged cavity marginals as follows\nmodel could not distinguish the generated outputs from factual knowledge, or it could not understand what they generate [14]. We speculate that this hallucination may be intimately related to the solution space of the spin glass model given a fixed complexity of training dataset, e.g., spurious states in a standard associative memory model, as implied by Eq. (12). These open questions are expected to be addressed in the near future by considering this intriguing physics link, thereby enhancing the robustness and trustworthiness of AI systems.\n# ACKNOWLEDGMENTS\nThis research was supported by the National Natural Science Foundation of China for Grant number 12122515, and Guangdong Provincial Key Laboratory of Magnetoelectric Physics and Devices (No. 2022B1212010008), and Guangdong Basic and Applied Basic Research Foundation (Grant No. 2023B1515040023).\n(12)\n(13)\n(14)\n(15)\n(16)\n(17)\nThe space complexity to run Eq. (16) is of O(D4), since i = 1, \u00b7 \u00b7 \u00b7 , (D + 1)2. To reduce the space complexi intuitive approach is to approximate the cavity marginal by a Gaussian distribution N(mi\u2192j, vi\u2192j), and then  the iterative equations for the first two moments. To validate this intuition, we start from Eq. (16) and app Fourier transform and Taylor expansion. To proceed, we first define \u03bei\u2192j = \ufffd k\u0338=i,j \u03b2Jik\u03c3k, and G(\u03bei\u2192j) = e\u03c3i\u03bei\u2192j, and then Eq. (16) can be written a\nIn Eq. (18b), we insert the Fourier transform of G(\u03be), i.e., \u02c6G(\u02c6\u03bei\u2192j), and absorb irrelevant constants into zi\u2192j. Let  calculate the last integral in Eq. (18c):\nIn Eq. (18b), we insert the Fourier transform of G(\u03be), i.e., \u02c6G(\u02c6\u03bei\u2192j), and absorb irrelevant constants into zi\u2192j. Let u calculate the last integral in Eq. (18c):\nIn Eq. (19b), we use the Taylor expansion of an exponential function, and in Eq. (19c), we define the mean mk\u2192i an the variance vk\u2192i of the message (the cavity marginal probability) as \ufffd\n\ufffd In Eq. (19e), we use the fact that the high order power of Jij are negligible and thus recover the exponential form Then, we substitute Eq. (19) back to Eq. (18) and obtain\nThen, we re-express \u02c6G(\u02c6\u03bei\u2192j) by G(\u03be) and obtain\n(18b)\n(18c)\n(19a)\n(20)\n(20) (21)\n(21)\n(22)\n(23a)\n(23b)\n(23c)\nThen, the marginal probability \u03b7i\u2192j(\u03c3i) can be further calculated as\nEquation (26d) implies that \u03b7i\u2192j(\u03c3i) follows a Gaussian distribution with the following mean and variance\nNow, we have obtained the so-called relaxed belief propagation (r-BP) equation:\n\uf8f4 \uf8f4 \uf8f4 \uf8f3 \uf8f4 \uf8f4 \uf8f3 Compared to the original belief propagation equation, the r-BP equations do not require a numerical integration thereby improving the computational efficiency. Moreover, it provides a basis for deriving the AMP equation shown in the main text. After the relaxed belief propagation equation converges, we get further the marginal probability as \u03b7i(\u03c3i) = 1 zi e\u03b2hi\u03c3i\u22121 2 \u03b2\u03bbi\u03c32 i \ufffd j\u0338=i \ufffd\ufffd d\u03c3j exp \ufffd \u22121 2 (\u03b2\u03bbj \u2212Vj\u2192i) \u03c32 j + (\u03b2hj + \u03b2Jij\u03c3i + Mj\u2192i) \u03c3j \ufffd\ufffd (30a)\n\uf8f4 \uf8f4 \uf8f3 \uf8f4 \uf8f3 ompared to the original belief propagation equation, the r-BP equations do not require a numerical integration, ereby improving the computational efficiency. Moreover, it provides a basis for deriving the AMP equation shown the main text. After the relaxed belief propagation equation converges, we get further the marginal probability as\nAs we expected, Eq. (30c) suggests that \u03b7i(\u03c3i) indeed follows a Gaussian distribution N(mi, vi), with the following mean and variance\nAs we expected, Eq. (30c) suggests that \u03b7i(\u03c3i) indeed follows a Gaussian distrib mean and variance\n \ufffd  \ufffd where mj\u2192i and vj\u2192i are the fixed points of the r-BP iterative equations [Eq. (29)]. To conclude, the intuition of Gaussian approximation is theoretically justified.\n7\n(24)\n(25)\n(26a)\n(26b)\n(26c)\n(26d)\n(27)\n(28)\n(29)\n(30a)\n(30b)\n(30c)\n(31)\nIf we can remove the directed arrows (e.g., i \u2192j) in the r-BP, the space complexity of the algorithm can be greatly reduced, reducing from O(D4) to O(D2). We next show how this can be possible. By using the following identities Mi = Mi\u2192j + \u03b2Jijmj\u2192i and Vi = Vi\u2192j + \u03b22J2 ijvj\u2192i, we further find that\nand\narge D limit (or even only D is not too small), we can further simplify the relaxed belief propagation equation AMP equation  \ufffd\nIn the large D limit (or even only D is not too small), we can further simplify the relaxed belief propagation equation\nIn the large D limit (or even only D is not too small), we can further simplify th to the AMP equation\nt too small), we can further simplify the relaxed belief propagation equation  \ufffd\n \ufffd In this case, we only need to iterate the above two equations of (mi, vi) to obtain the fixed points. This AMP equation saves (D + 1)2-fold space complexity for running the algorithm if we record (D + 1)2 as the number of spins in the system. In different forms, the AMP equation was first discovered in a statistical mechanics analysis of signal transmission problem [37], and is also rooted in the Thouless\u2013Anderson\u2013Palmer equation in glass physics [29, 31, 32].\n# Derivation of the unique optimal solution to the linear attention transformer\nIn this section, we provide a detailed derivation of the unique optimal solution to the linear attention transformer [i.e., Eq. (10) and Eq. (11) in the main text]. The input matrix X and the weight matrix W can be represented as 2 \u00d7 2 block matrices\nwhere X0 = {x1, x2, \u00b7 \u00b7 \u00b7 , xN} \u2208RD\u00d7N denotes the data used for the prompts, y0 = X\u22a4 0 w is the corresponding label vector, and \u02dcx is the testing prompt whose label needs to be predicted by the transformer. The blocks of the weight matrix W have the same size as the corresponding blocks of X, with W11 \u2208RD\u00d7D, W12 \u2208RD\u00d71, W21 \u2208R1\u00d7D, and W22 \u2208R. Thus, we can calculate\nand\nConsidering the prediction of the transformer \u02c6y = YD+1,N+1 = 1 DN (XX\u22a4WX)D+1,N+1, using 2 \u00d7 2 block matrix multiplication, one obtains the following result: \u02c6y = \u03b3y\u22a4 0 X\u22a4 0 W11\u02dcx + \u03b3y\u22a4 0 y0W21\u02dcx (38a) = \u03b3 \ufffd X\u22a4 0 w \ufffd\u22a4X\u22a4 0 W11\u02dcx + \u03b3 \ufffd X\u22a4 0 w \ufffd\u22a4\ufffd X\u22a4 0 w \ufffd W21\u02dcx (38b) = \u03b3w\u22a4X0X\u22a4 0 W11\u02dcx + \u03b3w\u22a4X0X\u22a4 0 wW21\u02dcx (38c) = w\u22a4(W11 + wW21) \u02dcx, (38d)\nConsidering the prediction of the transformer \u02c6y = YD+1,N+1 = 1 DN (XX\u22a4WX)D+1,N+1, using 2 \u00d7 2 block matri multiplication, one obtains the following result:\n\u02c6y = \u03b3y\u22a4 0 X\u22a4 0 W11\u02dcx + \u03b3y\u22a4 0 y0W21\u02dcx = \u03b3 \ufffd X\u22a4 0 w \ufffd\u22a4X\u22a4 0 W11\u02dcx + \u03b3 \ufffd X\u22a4 0 w \ufffd\u22a4\ufffd X\u22a4 0 w \ufffd W21\u02dcx = \u03b3w\u22a4X0X\u22a4 0 W11\u02dcx + \u03b3w\u22a4X0X\u22a4 0 wW21\u02dcx = w\u22a4(W11 + wW21) \u02dcx,\n(32)\n(33)\n(34a)\n(34b)\n(35)\n(36)\n(37)\n(38a) (38b) (38c) (38d)\nwhere \u03b3 = (DN)\u22121, the definition y0 = X\u22a4 0 w is used to obtain Eq. (38b), and the approximation 1 N X0X\u22a4 0 = I for the large N is used to obtain Eq. (38d). Note that the prefactor \u03b3 in the actual output is implied in the above derivation to keep the output being of the order unity. Comparing the actual prediction with the true label \u02dcy = w\u22a4\u02dcx corresponding to \u02dcx, one finds that the optimal solution for the weight matrix must satisfy the following condition:\nW\u22c6 11 + wW\u22c6 21 = 1D.\nEquation (39) is actually a set of linear equations, which can be written in the form of Ax = b where {xi} indicates the weight components to be determined. The number of variables in the equations is D2 + D. Given an input matrix X generated by a specific w, we can write down D2 equations to solve for the optimal weight. Therefore, the mathematics of linear equations tells us that when the number of input matrices P = 1, the weight matrix W has infinitely many solutions. However, as long as P > 1, the set of equations has a unique solution, which can be readily deduced as W\u22c6 11 = 1D and W\u22c6 21 = 0. In practice, the transformer or our spin glass model needs a larger value of P to identify this optimal matrix.\n# s of elements in different blocks of the S matrix and the Intera\nThe block structure of the S matrix looks the same with that of field matrix H in Fig. 1 (a) of the main text. By definition, the matrix S is divided into three blocks: the last column is an all-zero vector (due to the masked label in the input matrix), while the other two blocks are labeled as A (m < D +1, n \u0338= D +1) and B (m = D +1, n \u0338= D +1). We present here the explicit formulation of the elements in each block. We denote the distribution of elements in the block A as P(A), and similar notations for other blocks.\n\u2022 P(A) is equivalent to P(z), where z = u \ufffd i vi \ufffd j wjxij; \u2022 P(B) is equivalent to P(\u03b8), where \u03b8 = u \ufffd i(\ufffd j wjxij)2; \u2022 P(C) is equivalent to P(\u03be1), where \u03be1 = z1z2, and z1, z2 \u223cP(z); \u2022 P(D) is equivalent to P(\u03be2), where \u03be2 = z\u03b8, and z \u223cP(z), \u03b8 \u223cP(\u03b8); \u2022 P(E) is equivalent to P(\u03be3), where \u03be3 = \u03b81\u03b82, and \u03b81, \u03b82 \u223cP(\u03b8).\n\u2022 P(A) is equivalent to P(z), where z = u \ufffd i vi \ufffd j wjxij; \u2022 P(B) is equivalent to P(\u03b8), where \u03b8 = u \ufffd i(\ufffd j wjxij)2; \u2022 P(C) is equivalent to P(\u03be1), where \u03be1 = z1z2, and z1, z2 \u223cP(z); \u2022 P(D) is equivalent to P(\u03be2), where \u03be2 = z\u03b8, and z \u223cP(z), \u03b8 \u223cP(\u03b8); \u2022 P(E) is equivalent to P(\u03be3), where \u03be3 = \u03b81\u03b82, and \u03b81, \u03b82 \u223cP(\u03b8).\nAll the variables u, v, w, x used above are i.i.d. standard Gaussian random variables. A surprising observation is that the distributions of blocks A, B and E before and after increasing the prompt length are almost the same, while the blocks C and D seem sensitive to the change of the prompt length (see Fig. 5, but a weaker effect for D). However, once P is sufficiently large, the correct weight solution will be learned, and thus the transformer achieves a nearly perfect in-context inference, which captures the essence of ICL explained in the main text.\n# Generalization to more complex attention structures\nIn this section, we verify whether our spin glass model is robust to more complex attention structures in transformers by carrying out extensive experimental simulations. The linear attention setting used in the main text is calle simplified linear attention (SLA) in this section. The explicit formulation is given below.\nwhere W \u2261W\u22a4 QWK. This SLA will be compared with the case of removing the softmax function yet keeping the value matrix trainable, which we call the full linear attention (FLA) as follows.\nThe single-head full softmax attention (SA) defined below is also compared. YSA = WVX \u00b7 Softmax \ufffd(WQX)\u22a4WKX \u221a D \ufffd\n(39)\n(40)\n(41)\n(42)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d904/d904934f-ba9b-425f-ac56-8471b693e4ef.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">FIG. 5: The KL divergence DKL[PN\u2225PN+\u2206N] between the original distribution of the elements in blocks shown in Fig.1 of main text and that with the prompt length N prolonged by \u2206N. (P, D, N) = (1 000, 20, 50). The results are averaged  1 000 examples.</div>\n<div style=\"text-align: center;\">FIG. 5: The KL divergence DKL[PN\u2225PN+\u2206N] between the original distribution of the elements in blocks shown in Fig.1 of th main text and that with the prompt length N prolonged by \u2206N. (P, D, N) = (1 000, 20, 50). The results are averaged ove  000 examples.</div>\nIt is important to note that defining W \u2261W\u22a4 QWK in the linear attention case is not a simplification. A full transformer network consists of an embedding layer, encoder blocks, and decoder blocks [5]. For our linear regression task, we do not need an embedding layer. An encoder block includes two parts. The first part is the self-attention mechanism, aiming to evaluate the correlations among tokens in the input block X. To this end, we introduce three trainable matrices, namely, query (Q), key (K), and value (V ). Then, a linear transformation of the input is applied as follows.\nwhere WQ, WK \u2208RH\u00d7(D+1) and WV \u2208RH\u00d7(D+1) are Q, K, V matrices respectively, and H is the internal size of the attention operation. Therefore, we define Xt as the t-th column of X, and then we can define three vectors namely kt = WKXt, vt = WV Xt, and qt = WQXt. Then, the t-th column of the self-attention matrix SA(X) is given by\ngiven by\n\ufffd here \u03b1i(t) is a softmax operation containing information about the pairwise interactions between tokens. T ormalization factor \u221a H is required to retain relevant quantities in the exponential function being of the order on he second part is two feed-forward layers with skip connection, i.e.,\nwhere W1, W2 and b1, b2 are weights and biases of the two feed-forward layers. We call this complex layered transformer structure as TF. In addition, the transformer usually employs the multi-head attention structure. Therefore, for comparison, we further consider a multi-head linear attention model (MHLA), where the multiple outputs of Eq. (41) (one output\n(43)\n(44)\n(45)\ncorresponds to one head) are concatenated and then linearly read through a learnable readout matrix WO. We denote the number of attention heads, i.e., the dimension of the linear readout, as M. We leave exploration of more complex tasks beyond linear regression and more complex transformers such repeated transformer blocks together with multi-headed structures to future works. The current experiments are sufficient to justify our theory of spin-glass mapping. The experimental results of the above different models are shown in Fig. 6. Figure 6 shows the robustness of our theory against different transformer settings, although SA and TF display a higher test error. This can be understood as the linear attention structure is more computable for the linear regression task compared to the non-linear softmax attention. For the softmax attention and more advanced structures, it is hard to derive such concise results as in Sec , and an Ising spin glass model specified in the main text. However, the ground state interpretation of the learning is not a specific picture, and the underlying mechanism for the ICL in the simple but non-trivial setting we consider can be analytically clarified, thereby offering a promising avenue for thinking about how to model many intriguing but puzzling properties of large language models.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/67df/67dfbbdc-e55c-4790-bb07-54c6a8d82eac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">G. 6: Comparison of generalization errors for different transformer settings. \u03bb = 0.01, P = 10 000, D = 20, and \u03b2 = 100 f MP. For SA and TF, H = 32. M = 10 for MHLA. Results are averaged over 100 independent trials.</div>\n\u2217Electronic address: huanghp7@mail.sysu.edu.cn [1] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137\u20131155, 2003. [2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201913, pages 3111\u20133119, Red Hook, NY, USA, 2013. Curran Associates Inc. [3] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734. Association for Computational Linguistics, 2014. [4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR 2015 : International Conference on Learning Representations 2015, 2015. [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \ufffdLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pages 6000\u20136010, Red Hook, NY, USA, 2017. Curran Associates Inc. [6] Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv:2309.06979, 2023. [7] Chan Li, Junbin Qiu, and Haiping Huang. Meta predictive learning model of languages in neural circuits. Phys. Rev. E, 109:044309, 2024. [8] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv:2303.12712, 2023. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\n\u2217Electronic address: huanghp7@mail.sysu.edu.cn [1] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137\u20131155, 2003. [2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201913, pages 3111\u20133119, Red Hook, NY, USA, 2013. Curran Associates Inc. [3] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734. Association for Computational Linguistics, 2014. [4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR 2015 : International Conference on Learning Representations 2015, 2015. [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \ufffdLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pages 6000\u20136010, Red Hook, NY, USA, 2017. Curran Associates Inc. [6] Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv:2309.06979, 2023. [7] Chan Li, Junbin Qiu, and Haiping Huang. Meta predictive learning model of languages in neural circuits. Phys. Rev. E, 109:044309, 2024. [8] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv:2303.12712, 2023. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. [10] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1:9, 2019. [11] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020. [12] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. [13] Micah Goldblum, Anima Anandkumar, Richard Baraniuk, Tom Goldstein, Kyunghyun Cho, Zachary C Lipton, Melanie Mitchell, Preetum Nakkiran, Max Welling, and Andrew Gordon Wilson. Perspectives on the state and future of deep learning - 2023. arXiv:2312.09323, 2023. [14] Haiping Huang. Eight challenges in developing theory of intelligence. Front. Comput. Neurosci, 18:1388166, 2024. [15] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u02dcao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174, 2023. [16] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. arXiv:2306.00802, 2023. [17] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. [18] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv:2111.02080, 2021. [19] Allan Ravent\u00b4os, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of nonbayesian in-context learning for regression. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 14228\u201314246. Curran Associates, Inc., 2023. [20] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594, 2023. [21] Yue M. Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic theory of in-context learning by linear attention. arXiv:2405.11751, 2024. [22] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L. Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? arXiv:2310.08391, 2024. in ICLR 2024. [23] Ekin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv:2211.15661, 2022. [24] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1\u201355, 2024. [25] Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). arXiv:2310.01082, in ICLR, 2024. [26] M. D. Springer and W. E. Thompson. The distribution of products of beta, gamma and gaussian random variables. SIAM Journal on Applied Mathematics, 18(4):721\u2013737, 1970. [27] A. Crisanti and H. J. Sommers. The sphericalp-spin interaction spin glass model: the statics. Zeitschrift f\u00a8ur Physik B Condensed Matter, 87(3):341\u2013354, 1992. [28] Giacomo Gradenigo, Maria Chiara Angelini, Luca Leuzzi, and Federico Ricci-Tersenghi. Solving the spherical p-spin model with the cavity method: equivalence with the replica results. Journal of Statistical Mechanics: Theory and Experiment, 2020(11):113302, 2020. [29] Haiping Huang. Statistical Mechanics of Neural Networks. Springer, Singapore, 2022. [30] See the supplemental material at http://... for technical and experimental details, which includes Ref. [21, 29, 31, 32, 37]. [31] M. M\u00b4ezard, G. Parisi, and M. A. Virasoro. Spin Glass Theory and Beyond. World Scientific, Singapore, 1987. [32] P. W. Anderson D. J. Thouless and R. G. Palmer. Solution of \u2019solvable model of a spin glass\u2019. Phil. Mag., 35(3):593\u2013601, 1977. [33] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008. [34] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv:2402.19442, 2024. [35] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv:2310.05249, 2023. [36] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625\u2013630, 2024. [37] Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief propagation. Journal of Physics A: Mathematical and General, 36(43):11111, 2003.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in large language models (LLMs), highlighting its significance in achieving complex tasks without additional training. The mechanistic interpretation of ICL remains challenging, as it contrasts with traditional supervised learning methods.",
        "problem": {
            "definition": "The problem focuses on understanding why pre-trained transformers can perform unseen tasks using prompts without further training.",
            "key obstacle": "The main challenge lies in establishing a clear connection between the empirical phenomenon of ICL and theoretical frameworks, particularly relating to the parameters of transformers."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to interpret the emergent properties of large language models through a physical lens, specifically using a spin glass model.",
            "opinion": "The authors propose that the interactions among weight parameters in transformers can explain the ICL phenomenon.",
            "innovation": "The primary improvement lies in mapping transformer structures to a spin glass model, providing a novel framework for understanding ICL compared to previous methods that lacked such a connection."
        },
        "Theory": {
            "perspective": "The paper treats transformer learning as a statistical inference problem, reformulating it into a spin glass model where parameters act as real-valued spins.",
            "opinion": "The authors assume that the interactions among model parameters are crucial for enabling ICL.",
            "proof": "The derivation involves establishing a Hamiltonian that describes the interactions and proving the existence of a unique solution for weight parameters under certain conditions."
        },
        "experiments": {
            "evaluation setting": "The evaluation involves a simple transformer structure with linear attention, using datasets generated through high-dimensional linear regression tasks.",
            "evaluation method": "Experiments were conducted by minimizing the mean squared error function and analyzing the generalization error across different task diversities."
        },
        "conclusion": "The conclusion drawn is that the spin glass model successfully captures the mechanisms behind ICL, demonstrating that sufficient task diversity leads to the emergence of ICL capabilities in transformers.",
        "discussion": {
            "advantage": "The paper provides a novel theoretical framework that links ICL to statistical mechanics, enhancing the understanding of transformer behavior.",
            "limitation": "The complexity of the spin glass model may hinder its applicability to more intricate transformer architectures.",
            "future work": "Future research could explore the implications of this model on more complex transformers and investigate other emergent properties like chain-of-thought prompting."
        },
        "other info": [
            {
                "acknowledgments": "Supported by the National Natural Science Foundation of China and other local research foundations."
            },
            {
                "additional experiments": "Further experiments are referenced to validate the theoretical claims and extend the findings to more complex models."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning (ICL) in large language models (LLMs), highlighting its significance in achieving complex tasks without additional training."
        },
        {
            "section number": "1.3",
            "key information": "The problem focuses on understanding why pre-trained transformers can perform unseen tasks using prompts without further training."
        },
        {
            "section number": "3.2",
            "key information": "The paper treats transformer learning as a statistical inference problem, reformulating it into a spin glass model where parameters act as real-valued spins."
        },
        {
            "section number": "3.4",
            "key information": "The conclusion drawn is that the spin glass model successfully captures the mechanisms behind ICL, demonstrating that sufficient task diversity leads to the emergence of ICL capabilities in transformers."
        },
        {
            "section number": "6.1",
            "key information": "The authors assume that the interactions among model parameters are crucial for enabling ICL, which relates to model bias and context sensitivity."
        },
        {
            "section number": "6.4",
            "key information": "The complexity of the spin glass model may hinder its applicability to more intricate transformer architectures, presenting a scalability challenge."
        },
        {
            "section number": "7",
            "key information": "Future research could explore the implications of the spin glass model on more complex transformers and investigate other emergent properties like chain-of-thought prompting."
        }
    ],
    "similarity_score": 0.6937873236225527,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Spin glass model of in-context learning.json"
}