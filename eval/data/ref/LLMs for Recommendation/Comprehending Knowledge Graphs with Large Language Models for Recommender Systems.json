{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.12229",
    "title": "Comprehending Knowledge Graphs with Large Language Models for Recommender Systems",
    "abstract": "In recent years, the introduction of knowledge graphs (KGs) has significantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing methods still face several limitations. First, most KGs suffer from missing facts or limited scopes. This can lead to biased knowledge representations, thereby constraining the model\u2019s performance. Second, existing methods cannot effectively utilize the semantic information of textual entities as they typically convert textual information into IDs, resulting in the loss of natural semantic connections between different items. Third, existing methods struggle to capture high-order relationships in global KGs due to their inefficient layer-by-layer information propagation mechanisms, which are prone to introducing significant noise. To address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) for knowledge-aware recommendation. The extensive world knowledge and remarkable reasoning capabilities of LLMs enable them to supplement KGs. Additionally, the strong text comprehension abilities of LLMs allow for a better understanding of semantic information. Based on this, we first extract subgraphs centered on each item from the KG and convert them into textual inputs for the LLM. The LLM then outputs its comprehension of these item-centered subgraphs, which are subsequently transformed into semantic embeddings. Furthermore, to utilize the global information of the KG, we construct an item-item graph using these semantic embeddings, which can directly capture higher-order associations between items. Both the semantic embeddings and the structural information from the item-item graph are effectively integrated into the recommendation model through our designed representation alignment and neighbor augmentation modules. Extensive experiments on four real-world datasets demonstrate the superiority of our method.",
    "bib_name": "cui2024comprehendingknowledgegraphslarge",
    "md_text": "# Comprehending Knowledge Graphs with Large Language Models for Recommender Systems\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9507/95078487-59bc-4795-a0af-cdeadcf97f7d.png\" style=\"width: 50%;\"></div>\n# Abstract\nIn recent years, the introduction of knowledge graphs (KGs) has significantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing methods still face several limitations. First, most KGs suffer from missing facts or limited scopes. This can lead to biased knowledge representations, thereby constraining the model\u2019s performance. Second, existing methods cannot effectively utilize the semantic information of textual entities as they typically convert textual information into IDs, resulting in the loss of natural semantic connections between different items. Third, existing methods struggle to capture high-order relationships in global KGs due to their inefficient layer-by-layer information propagation mechanisms, which are prone to introducing significant noise. To address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) for knowledge-aware recommendation. The extensive world knowledge and remarkable reasoning capabilities of LLMs enable them to supplement KGs. Additionally, the strong text comprehension abilities of LLMs allow for a better understanding of semantic information. Based on this, we first extract subgraphs centered on each item from the KG and convert them into textual inputs for the LLM. The LLM then outputs its comprehension of these item-centered subgraphs, which are subsequently transformed into semantic embeddings. Furthermore, to utilize the global information of the KG, we construct an item-item graph using these semantic embeddings, which can directly capture higher-order associations between items. Both the semantic embeddings and the structural information from the item-item graph are effectively integrated into the recommendation model through our designed representation alignment and neighbor augmentation modules. Extensive experiments on four real-world datasets demonstrate the superiority of our method.\n\u2217Work done as an intern in FiT, Tencent \u2020Corresponding author.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4328/4328507b-46f6-43aa-8eb0-e08a497845f3.png\" style=\"width: 50%;\"></div>\n# Figure 1: An illustrative diagram demonstrating the potential issues of existing KG-based recommendation methods.\n<div style=\"text-align: center;\">Figure 1: An illustrative diagram demonstrating the potential issues of existing KG-based recommendation methods.</div>\nKeywords Knowledge Graphs, Large Language Models, Recommendation\nKeywords Knowledge Graphs, Large Language Models, Recommendation\nKnowledge Graphs, Large Language Models, Recommendation\n# 1 Introduction\nThe rapid advancement of web applications has led to an increasingly critical issue of information overload. Recommender systems address this problem by modeling user preferences based on historical data and providing personalized recommendations. Collaborative filtering (CF) [9, 10, 22], as one of the most classic and efficient methods, has been extensively employed in existing recommender systems. However, CF-based methods exclusively rely on user-item collaborative signals, often suffering from the data sparsity issue for users who have not interacted with a sufficient number of items [25]. To address such data sparsity issue, recent studies [32, 41, 43] have incorporated knowledge graphs (KGs) as external knowledge sources into recommendation models, achieving significant progress. Typically, these methods capture diverse and higher-order semantic relationships between items by\nmodeling the structural and semantic information in KGs, thereby generating enhanced user and item representations to improve the recommendation model [41]. Despite the effectiveness of existing KG-enhanced recommendation methods, they still have several limitations. First, many KGs suffer from missing facts and limited scopes [8], as constructing KGs often requires significant manual effort and domain expertise. The absence of key attributes, such as the genres of a movie, can cause items that originally share the same attribute to lose their semantic connections. As illustrated in Figure 1, item A and item B should have a connecting path (A-P-B) in the KG. However, due to item A missing the P attribute, they are not associated with each other. In this situation, the recommendation model can only learn from biased knowledge, leading to suboptimal performance. Second, textual entities and relations are not effectively utilized. Existing methods [31\u201333, 40, 41] typically convert textual entities and relations into IDs, failing to leverage the semantic information inherent in the text. Moreover, this can result in the loss of natural semantic connections between different items. For instance, in Figure 1, \u201chorror\u201d and \u201cthriller\u201d are two semantically related attribute nodes of Item F and Item G, respectively. However, similar semantics are not reflected in different entity IDs, which further results in the disconnection between item F and item G. Third, existing methods [24, 31\u201333, 40, 41] struggle to capture high-order relationships in global KGs. Most of them propagate and aggregate information by stacking multiple layers of graph neural networks (GNNs). The layer-by-layer propagation is not only inefficient but also accumulates a large amount of irrelevant node information, leading to the over-smoothing issue [8, 31]. This problem becomes more severe as the order increases. For instance, let us assume that points A and H in Figure 1 have a strong semantic connection. However, the considerable distance between them in the KG presents big challenges for existing KG-based recommendation methods in capturing this semantic relation. Due to these limitations, recommender systems are unable to effectively capture the semantic relationships between target items and users\u2019 historically interacted items through the KG, resulting in suboptimal predictions, as illustrated in the lower left corner of Figure 1. Empowered by extensive knowledge and remarkable reasoning abilities, large language models (LLMs) have demonstrated significant promise in semantic understanding and knowledge extraction. Consequently, LLMs have the potential to address the aforementioned issues. Recently, efforts have been made to leverage LLMs to improve recommendation models. Some studies utilize LLMs to supplement missing attributes of items and generate semantic representations of item profiles [21]. Other studies employ LLMs to determine whether a complementary relationship exists between two items, thereby recommending complementary products based on users\u2019 historical behaviors [44]. However, these methods do not fully exploit the semantic and structural information of KGs. As one of the most common and important sources of knowledge, KGs contain a wealth of semantic associations among entities and relations, which are often overlooked by existing methods that typically consider only item profiles. Additionally, KGs serve as task-relevant knowledge repositories, effectively aiding LLMs in acquiring task-specific knowledge and mitigating the issue of hallucinations caused by excessive divergence. Nevertheless, effectively\neveraging LLMs to model the diverse semantic relationships in KGs and enhance recommendation performance remains an open uestion. To bridge this gap, in this paper, we propose a novel method amed Comprehending Knowledge Graphs with Large Language Models for Recommendation (CoLaKG). The core idea is to leverage LMs for understanding the semantic and structural information f KGs to enhance the representation learning of users and items. Our method comprises two stages: \u2022 Comprehending KGs with LLMs. As LLMs cannot directly process graph-structured data, we first transform the KG into the text format. Given the impracticality and redundancy of inputting the entire KG into LLMs, we propose extracting subgraphs corresponding to each item and converting these item-centered subgraphs into text. Next, we carefully design prompts to leverage the extensive knowledge and reasoning capabilities of LLMs to fully understand, complete, and refine the local KG, thereby generating a comprehensive understanding of these subgraphs. Then, a pre-trained text embedding model is used to obtain semantic embeddings of these generated texts. In addition to local KG information, we construct an item-item graph based on these semantic embeddings, where the relationship between two items corresponds to their semantic similarity. This approach effectively leverages global KG information, facilitating the capture of high-order semantic associations between items. \u2022 Incorporating semantic embeddings into the recommendation model. Our objective is to integrate semantic embeddings with the ID embeddings of the recommendation model, thereby leveraging both collaborative signals and semantic information from the KG. Since semantic and ID embeddings originate from different modalities and typically have different dimensions, we design an adapter to map the semantic embeddings to align with the item ID embedding space and then employ a simple method to fuse the embeddings from both modalities. Additionally, to capture high-order semantic associations of items within the KG, we enhance the representations by aggregating the representations of semantically related items. Finally, the learned representations are used for prediction. t is important to note that these two stages are decoupled, meanng that our model does not involve LLM inference during the ecommendation process. This decoupling allows our model to be ffectively applied in real-world recommendation scenarios. Our contributions are summarized as follows: \u2022 We propose a novel method that utilizes LLMs to comprehend and transform the semantic and structural information of KGs. This approach addresses the issues of missing facts and the inability to leverage semantic information from text in current KG-based recommendation methods. \u2022 We construct an item-item graph based on the semantic relationships of items and introduce a KG semantic-based neighbor augmentation method to enhance item representations. This approach effectively captures higher-order relations in the KG and leverages global KG information for better recommendations. \u2022 Extensive experiments are conducted on four real-world datasets to validate the superiority of our method. Further analysis demonstrates the rationale behind our approach.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/34ea/34ea764a-0f73-473c-b78a-6972936d5268.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The framework of our proposed CoLaKG for knowledge-aware recommendation.</div>\n# 2 Related Work\n# 2.1 Knowledge-aware Recommendation\nExisting knowledge-aware recommendation methods can be categorized into three types [8]: embedding-based methods, pathbased methods, and GNN-based methods. Embedding-based methods [3, 28, 43] enhance the representations of users and items by leveraging the relations and entities within the KGs. Notable examples include CKE [43], which integrates various types of side information into a collaborative filtering framework using TransR [19] for structural knowledge. Another example is DKN [28], which improves news representations by combining textual embeddings of sentences and knowledge-level embeddings of entities. Pathbased methods leverage KGs to explore long-range connectivity [13, 34, 42]. For example, Personalized Entity Recommendation (PER) [42] treats a Knowledge Graph (KG) as a heterogeneous information network and extracts meta-path-based latent features to represent the connectivity between users and items along various types of relational paths. MCRec [13] constructs meta-paths and learns the explicit representations of meta-paths to depict the interaction context of user-item pairs. Despite their effectiveness, these approaches heavily rely on domain knowledge and human effort for meta-path design. Recently, GNN-based methods have been proposed, which enhance entity and relation representations by aggregating embeddings from multi-hop neighbors [29, 32, 33]. For instance, KGAT [32] employs graph attention mechanisms to propagate embeddings and utilizes multi-layer perceptrons to generate final recommendation scores in an end-to-end manner. Similarly, KGIN [33] adopts an adaptive aggregation method to capture finegrained user intentions. Additionally, some methods [31, 40, 41, 47]\nemploy contrastive learning to mitigate potential knowledge noise and identify informative knowledge connections.\n# 2.2 LLMs for Recommendation\nIn light of the emergence of large language models and their remarkable achievements in the field of NLP, scholars have begun to explore the potential application of LLMs in recommender systems [4, 6, 37, 45]. Due to the powerful reasoning capabilities and extensive world knowledge of LLMs, they have been already naturally applied to zero-shot [11, 12, 30] and few-shot recommendation scenarios [2, 18]. In these studies, LLMs are directly used as a recommendation model [17, 46], where the output of LLMs is expected to offer a reasonable recommendation result [38]. However, when the dataset is sufficiently large, their performance often falls short of that achieved by traditional recommendation models. Another line of research involves leveraging LLMs as feature extractors. These methods [1, 14, 15, 21, 21, 35, 36, 48] generate intermediate decision results or semantic embeddings of users and items, which are then input into traditional recommendation models to produce the final recommendations. Unlike existing methods, our approach aims to leverage the extensive knowledge base and reasoning capabilities of LLMs to understand KGs and transform them into semantic embeddings, thereby addressing existing issues in KG-based recommender systems and enhancing recommendation performance.\n# 3 Preliminaries\nUser-Item Interaction Graph. Let U and V denote the user set and item set, respectively, in a recommender system. We construct a user-item bipartite graph G = {(\ud835\udc62,\ud835\udc66\ud835\udc62\ud835\udc63, \ud835\udc63)|\ud835\udc62\u2208U, \ud835\udc63\u2208V} to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e043/e0432dad-3f20-481d-9aaa-a894e42fe18e.png\" style=\"width: 50%;\"></div>\nSecond-order info: The movies with the same director (James Cameron) also include: Aliens, \u2026   The movies with the same actor (Leonardo DiCaprio) also include: Inception, \u2026   First-order info:  (Titanic, Directed by, James Cameron), (Titanic, Acted by, Leonardo DiCaprio), \u2026 Assume you are an expert in movie recommendation. You will be given a certain movie with its  first-order information (in the form of triples) and some second-order relationships (movies related  to this movie). Please complete the missing knowledge, summarize the movie and analyze what  kind of users would like it. \u2026\n# Figure 3: An example of the prompt for KG comprehension.\nrepresent the collaborative signals between users and items. Here, \ud835\udc66\ud835\udc62\ud835\udc63= 1 if user \ud835\udc62interacted with item \ud835\udc63, and vice versa. Knowledge Graph. We capture real-world knowledge about items using a heterogeneous graph composed of triplets, represented as G\ud835\udc58= {(\u210e,\ud835\udc5f,\ud835\udc61)}. In this context, \u210eand \ud835\udc61are knowledge entities belonging to the set E, while \ud835\udc5fis a semantic relation from the set R that links them, as exemplified by the triplet (James Cameron, directed, Titanic). Notably, the item set is a subset of the entity set, denoted as V \u2282E. This form of knowledge graph enables us to model the intricate relationships between items and entities. Task Formulation. Following the task format of most KG-aware recommendation models, we formulate the task as follows: Given the user-item interaction graph G and the corresponding knowledge graph G\ud835\udc58, our objective is to learn a recommendation model that predicts the probability of user \ud835\udc62interacting with item \ud835\udc63.\n# 4 Methodology\nIn this section, we introduce our proposed method CoLaKG in detail. An overview of our method is illustrated in Figure 2. For each item, we extract a subgraph centered on it from the KG. The LLM then comprehends this subgraph and converts it into a semantic embedding, thereby fully utilizing the local information of the KG. Additionally, we enhance the representation of each item by aggregating the most semantically similar neighbors based on the semantic similarity between items, leveraging the global information of the KG. Furthermore, we generate semantic embeddings for users based on their preferences derived from the KG. Finally, these semantic embeddings are integrated with the ID representations of the recommendation model, resulting in enhanced representations of both items and users, thereby improving the performance of the recommendation model.\n# 1 KG Comprehension with LLMs\nKGs have been widely utilized in recommender systems to provide semantic information and model latent associations between items. However, KGs are predominantly manually curated, leading to missing facts and limited knowledge scopes. Additionally, the highly structured nature of KGs poses challenges for effectively utilizing textual information. Existing methods often transform textual entities and relations into IDs, resulting in a significant loss of semantic information. Recently, the rapid emergence of LLMs has provided a promising approach to addressing these issues. Their extensive knowledge and reasoning abilities enable them to complete missing entities and expand the knowledge scopes of KGs. Furthermore, their inherent text comprehension abilities facilitate the effective utilization of textual entities. Leveraging these advantages, we propose the use of LLMs to enhance the understanding and refinement of KGs for improved recommendations.\n4.1.1 Item-Centered KG Subgraph Comprehension. Equipping LLMs to comprehend KGs presents certain challenges, as LLMs cannot directly interpret non-textual graph data. Consequently, KGs must be converted into a textual format. However, due to the vast number of entities in a KG, inputting the entire KG into an LLM is impractical. To address this, we initially extract the subgraph of the KG centered on each item. This approach enables the effective utilization of local KG information for each item. Note that we also consider the utilization of global KG information, which will be discussed in Section 4.1.2. First, we represent the first-order KG subgraph centered on each item (i.e., ego network [20]) using triples. Specifically, given an item \ud835\udc63\u2208V, we use T\ud835\udc63= {(\ud835\udc63,\ud835\udc5f,\ud835\udc52)|(\ud835\udc63,\ud835\udc5f,\ud835\udc52) \u2208G\ud835\udc58} to denote the set of triplets where\ud835\udc63is the head entity. In the context of recommendation, the first-order neighboring entities of an item in a KG are usually attributes. Therefore, we use \ud835\udc52to represent these attribute entities to distinguish them from those item entities \ud835\udc63. During generating triples, in cases where the attribute or relation is absent, the term \u201cmissing\u201d is employed as a placeholder. Next, we consider the secondorder relations in KGs. The number of entities in an ego network centered on a single entity increases exponentially with the growth of the radius. However, the input length of an LLM is strictly limited. Consequently, including all second-order neighbors associated with the central item in the prompt becomes impractical. To address this issue, we adopt a simple but effective strategy, random sampling, to explore second-order connections of \ud835\udc63. Let E\ud835\udc63= {\ud835\udc52| (\ud835\udc63,\ud835\udc5f,\ud835\udc52) \u2208T\ud835\udc63} denote the set of first-order connected neighbors of \ud835\udc63. For each \ud835\udc52\u2208E\ud835\udc63, we randomly sample \ud835\udc5atriples from the set T\ud835\udc52to construct the triples of second-order connections, denoted as T\ud835\udc5a \ud835\udc52. Here, T\ud835\udc52= {(\ud835\udc52,\ud835\udc5f, \ud835\udc63\u2032) | (\ud835\udc52,\ud835\udc5f, \ud835\udc63\u2032) \u2208G\ud835\udc58, \ud835\udc63\u2032 \u2260\ud835\udc63} represents the set of triples where \ud835\udc52\u2208E\ud835\udc63is the head entity. We fix \ud835\udc5ato 10 in our paper and do not perform hyperparameter exploration due to cost considerations associated with the LLM. After converting first-order and second-order relationships into triples, we transform these triples into textual form. For first-order relations, we concatenate all the first-order triples in T\ud835\udc63to form a single text, denoted as D\ud835\udc63. For second-order relations, we use a template to transform the second-order triples T\ud835\udc5a \ud835\udc52 into coherent sentences D\u2032\ud835\udc63, facilitating the understanding of the LLM. In addition to D\ud835\udc63and D\u2032\ud835\udc63, we have carefully designed a system prompt I\ud835\udc63as the instruction to guide the generation. By combining I\ud835\udc63, D\ud835\udc63, and D\u2032\ud835\udc63, we obtain the prompt, which is shown in Figure 3. The prompt enables the LLM to fully understand, complete, and refine the KG, thereby generating the final comprehension for the \ud835\udc63-centered KG subgraph. This process can be formulated as follows:\n(1)\nOnce we have obtained the LLM\u2019s comprehension of the KG subgraphs, we need to convert these textual answers into continuous vectors for utilization in downstream recommendation models. Here, we employ a pre-trained text embedding model P to transform C\ud835\udc63into embedding vectors s\ud835\udc63, which can be formulated as:\n(2)\n4.1.2 Semantic-Relational Item-Item Graph Construction. This section introduces the utilization of global KG information. Items that are distant in the KG can still have close semantic associations.\nHowever, existing KG-based recommendation methods propagate information by stacking multiple GNN layers. Due to the low propagation efficiency of this approach and the introduction of irrelevant neighbor noise, most methods reach saturation with relatively few layers. To address this issue, we take a novel perspective by utilizing the generated semantic embeddings of KG subgraphs to directly and efficiently model semantic relations between items from the global KG. For each item, we have obtained the semantic embedding corresponding to its local KG graph. Based on this, we can directly compute the semantic relationships between any two items. Specifically, we employ the cosine similarity as the metric to quantify the relations between items. Given two different items \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57, their semantic relation \ud835\udc5f(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57) is computed as:\nwhere sim denotes the cosine similarity function. Once we obtain the semantic associations between any two items in the entire KG, we treat the semantic similarity between the two items as the edge weight between them, allowing us to construct an item-item graph:\n(4)\nFrom a high-level perspective, we transform higher-order associations between items in the KG into semantic relationships on the constructed item-item graph G\ud835\udc63. Based on this foundation, it is essential to identify items that are semantically strongly related to the given item, as items with lower semantic relevance may introduce noise. Specifically, given an item \ud835\udc63\ud835\udc56, we rank all other items \ud835\udc63\ud835\udc57\u2208V where \ud835\udc63\ud835\udc57\u2260\ud835\udc63\ud835\udc56in descending order based on the semantic similarity \ud835\udc5f(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57). Subsequently, we select the top-\ud835\udc58items with the highest similarity scores, forming the neighbor set of \ud835\udc63\ud835\udc56: N\ud835\udc58(\ud835\udc63\ud835\udc56), where 0 < \ud835\udc58< |V| is an adjustable hyperparameter, representing the number of selected neighbors. In this manner, we explicitly filter out items with low semantic associations to the current item while retaining those with relatively strong associations. Traditional KG-based recommendation methods aggregate related items through layer-by-layer information propagation on the KG. Items with the same attributes are 2-hop neighbors in the KG, requiring at least two layers of GNN to capture their relation. Higher-order item associations necessitate even more propagation and aggregation steps to be captured. In contrast, our method, by constructing an item-item graph based on KG subgraph semantic embeddings, can recall strongly semantically related neighbors of any order across the entire graph through a single semantic similarity calculation. The purpose of identifying semantic-related neighbors is to leverage them to enhance the item representations, thereby improving the effectiveness of the recommendation model. We design a fine-grained approach to enhance the representation of item \ud835\udc63\ud835\udc56with its neighbors N\ud835\udc58(\ud835\udc63\ud835\udc56). Details on this approach will be covered in Section 4.3.2.\n# 4.2 User Preference Comprehension\nThe introduction of KGs allows for the expansion of user-item bipartite graphs and enables us to understand user preferences from a knowledge-driven perspective. Given a user \ud835\udc62, we first extract the subgraph corresponding to user \ud835\udc62from the user-item bipartite graph, denoted as B\ud835\udc62. For each item \ud835\udc63\u2208B\ud835\udc62, we extract its firstorder KG subgraph and represent it as a set of triples, denoted\nas T\ud835\udc63. We then concatenate all triples in T\ud835\udc63to form a single text, denoted as D\ud835\udc63. The detailed approach is the same as described in Section 4.1.1. Subsequently, we represent user \ud835\udc62with all items the user has interacted with in the training set and the corresponding knowledge triples D\ud835\udc63:\n(5)\nwhere \u2295denotes concatenation operation, and name\ud835\udc63denotes the text name of item \ud835\udc63. Additionally, we have meticulously designed a system prompt, denoted as I\ud835\udc62, to serve as an instruction for guiding the generation of user preferences. By combining D\ud835\udc62and I\ud835\udc62, we enable the LLM to comprehend the user preference for \ud835\udc62, which can be formulated as:\n(6)\nFurthermore, we also utilize the text embedding function P to transform the textual answers C\ud835\udc62into embedding vectors s\ud835\udc62, which can be expressed as:\n(7)\n# 4.3 Representation Alignment and Neighbor Augmentation\n4.3.1 Cross-Modal Representation Alignment. In a traditional recommendation model, each item and user is associated with an ID embedding. Let e\ud835\udc63\u2208R\ud835\udc51represent the ID embedding of item \ud835\udc63 and e\ud835\udc62\u2208R\ud835\udc51represent the ID embedding of user \ud835\udc62. In addition to these ID embeddings, we also obtain the semantic embedding s\ud835\udc63\u2208R\ud835\udc51\ud835\udc60w.r.t. the comprehension of \ud835\udc63-centric KG subgraph, and the semantic embedding s\ud835\udc62\u2208R\ud835\udc51\ud835\udc60w.r.t. the comprehension of user \ud835\udc62\u2019s preference. Since ID embeddings and semantic embeddings belong to two different modalities and typically possess different embedding dimensions, we employ an adapter network to align the semantic embeddings with the ID embedding space. Specifically, the adapter networks consist of a linear map and a non-linear activation function, which are formulated as:\n(8)\nwhere both W1 \u2208R\ud835\udc51\u00d7\ud835\udc51\ud835\udc60and W2 \u2208R\ud835\udc51\u00d7\ud835\udc51\ud835\udc60are are weight ma \ud835\udf0erepresents the non-linear activation function ELU [5].\nwhere both W1 \u2208R\ud835\udc51\u00d7\ud835\udc51\ud835\udc60and W2 \u2208R\ud835\udc51\u00d7\ud835\udc51\ud835\udc60are are weight matrices, \ud835\udf0erepresents the non-linear activation function ELU [5]. Note that during the training process, we fix s\ud835\udc63and s\ud835\udc62, training solely the corresponding projection parameters W1 and W2, and the parameters of the recommendation model. The benefits of this method are two-fold. Firstly, by preserving s\ud835\udc63and s\ud835\udc62, we can utilize the rich semantic information they already contain, which can guide the recommendation model to converge more effectively during the initial stage of training. Secondly, the number of parameters in s\ud835\udc63 and s\ud835\udc62is typically much greater than those in the recommendation model\u2019s ID embeddings due to their large dimensions. Consequently, altering these parameters would significantly affect the updates to the ID embeddings and slow down the gradient update process, leading to an unstable training procedure. After mapping the representations to the same space, we need to fuse the representations of the two modalities, leveraging both the collaborative signals and the semantic information to form a complementary representation. To achieve this, we employ a straightforward mean pooling technique to fuse their embeddings,\nthereby integrating the them into a unified representation:\n(9)\n    where h\ud835\udc63\u2208R\ud835\udc51and h\ud835\udc62\u2208R\ud835\udc51represent the merged embeddings of item \ud835\udc63and user \ud835\udc62, respectively.\n4.3.2 Item Representation Augmentation with Semantic-related Neighbors. For each item, we have obtained its semantic-related items from the constructed item-item graph in Section 4.1.2. To fully utilize these neighbors, we propose to aggregate their information to enhance the representations. Considering the varying contributions of different neighbors to the central item, we employ the attention mechanism for weighted aggregation of representations. Specifically, for item \ud835\udc63\ud835\udc56and its top-\ud835\udc58neighbor set N\ud835\udc58(\ud835\udc63\ud835\udc56), we first compute attention coefficients that indicate the importance of item \ud835\udc63\ud835\udc57\u2208N\ud835\udc58(\ud835\udc63\ud835\udc56) to item \ud835\udc63\ud835\udc56as follows:\n(10)\nHere, W \u2208R\ud835\udc51\ud835\udc4e\u00d7\ud835\udc51is a learnable weight matrix to capture higherlevel features of s\ud835\udc63\ud835\udc56and s\ud835\udc63\ud835\udc57, \u2225is the concatenation operation, \ud835\udc4e denotes the attention function: R\ud835\udc51\ud835\udc4e\u00d7 R\ud835\udc51\ud835\udc4e\u2192R, where we adopt a single-layer neural network and apply the LeakyReLU activation function following [26]. Note that the computation of attention weights is exclusively dependent on the semantic representation of items, as our objective is to calculate the semantic associations between items, rather than the associations present in collaborative signals. In addition, we employ the softmax function for easy comparison of coefficients across different items:\n(11)\nThe attention scores \ud835\udefc\ud835\udc56\ud835\udc57are then utilized to compute a linear combination of the corresponding neighbor embeddings. Finally, the weighted average of neighbor embeddings and the embedding of item \ud835\udc63\ud835\udc56itself are combined to form the final output representation for item \ud835\udc63\ud835\udc56: \ufffd \ufffd \u2211\ufe01 \ufffd\ufffd\n(12)\n\ufffd \u2211\ufe01 \ufffd where \ud835\udf0edenotes the non-linear activation function.\n# 4.4 User-Item Modeling\nHaving successfully integrated the semantic information from the KG into both user and item representations, we can use them as inputs for traditional recommendation models to generate prediction results. This process can be formulated as follows:\n(13)\nwhere \u02c6\ud835\udc66\ud835\udc62\ud835\udc63is the predicted probability of user \ud835\udc62interacting with item \ud835\udc63, h\ud835\udc62is the representation for user \ud835\udc62, h\ud835\udc63is the augmented representation for item \ud835\udc63, and F denotes the function of the recommendation model. Specifically, we select the classic model, LightGCN [10], as the architecture for our recommendation method due to its simplicity and effectiveness. The trainable parameters of original LightGCN are only the embeddings of users and items, similar to standard\nmatrix factorization. First, we adopt the simple weighted sum aggregator to learn the user-item interaction graph, which is defined as:\n(14)\n\u221a\ufe01 \u221a\ufe01 where h(\ud835\udc59) \ud835\udc62 and h(\ud835\udc59) \ud835\udc63 represent the embeddings of user \ud835\udc62and item \ud835\udc63 after \ud835\udc59layers of propagation, respectively. The initial embeddings h(0) \ud835\udc62 = h\ud835\udc62and h(0) \ud835\udc63 = h\u2032\ud835\udc63are obtained in Section 4.3. M\ud835\udc62denotes the set of items with which user\ud835\udc62has interacted, while M\ud835\udc63signifies the set of users who have interacted with item \ud835\udc63. The symmetric normalization term is given by 1/ \u221a\ufe01 |M\ud835\udc62||M\ud835\udc63|. Subsequently, the embeddings acquired at each layer are combined to construct the final representation:\n(15)\nwhere \ud835\udc3frepresents the number of hidden layers. Ultimately, the model prediction is determined by the inner product of the final user and item representations:\n(16)\n# 4.5 Model Training\nOur approach can be divided into two stages. In the first stage, we employ the LLM to comprehend the KGs, generating corresponding semantic embeddings for each item and user, denoted as s\ud835\udc63and s\ud835\udc62, respectively. In the second stage, these semantic embeddings are integrated into the recommendation model through an adapter network to enhance its performance. Only the second stage necessitates supervised training, where we adopt the widely-used Bayesian Personalized Ranking (BPR) loss: \u2211\ufe01\n(17)\nHere, O = {(\ud835\udc62, \ud835\udc63+, \ud835\udc63\u2212)|(\ud835\udc62, \ud835\udc63+) \u2208R+, (\ud835\udc62, \ud835\udc63\u2212) \u2208R\u2212} represents the training set, R+ denotes the observed (positive) interactions between user \ud835\udc62and item \ud835\udc63, while R\u2212indicates the sampled unobserved (negative) interaction set. \ud835\udf0e(\u00b7) is the sigmoid function. \ud835\udf06\u2225\u0398\u22252 2 is the regularization term, where \ud835\udf06serves as the weight coefficient and \u0398 constitutes the model parameter set.\n# 5 Experiments\n# 5 Experiments 5.1 Experimental Settings\n# 5.1 Experimental Settings\n5.1.1 Datasets. We conducted experiments on four real-world datasets, including three public datasets (MovieLens1, MIND2, LastFM3), and one industrial dataset (Fund). The statistics for these datasets are presented in Table 1. These datasets cover a wide range of application scenarios. Specifically, MovieLens is a wellestablished benchmark that collects movie ratings provided by users. MIND is a large-scale news recommendation dataset constructed from user click logs on Microsoft News. Last-FM is a well-known\n1https://grouplens.org/datasets/movielens/ 2https://msnews.github.io/ 3https://grouplens.org/datasets/hetrec-2011/\n<div style=\"text-align: center;\">Table 1: Dataset statistics.</div>\nTable 1: Dataset statistics.\nStatistics\nMovieLens\nLast-FM\nMIND\nFunds\n# Users\n6,040\n1,859\n44,603\n209,999\n# Items\n3,260\n2,813\n15,174\n5,701\n# Interactions\n998,539\n86,608\n1,285,064\n1,225,318\nKnowledge Graph\n# Entities\n12,068\n9,614\n32,810\n8,111\n# Relations\n12\n2\n14\n12\n# Triples\n62,958\n118,500\n307,140\n65,697\nmusic recommendation dataset that includes user listening history and artist tags. The Fund dataset is sampled from the data of a large-scale online financial platform aiming to recommend funds for users. We adopt the similar setting as numerous previous studies [10, 39], filtering out items and users with fewer than five interaction records. For each dataset, we randomly select 80% of each user\u2019s historical interactions to form the training set, while the remaining 20% constitute the test set, following [10]. From the training set, we further randomly select 10% of the interactions to create a validation set for tuning hyperparameters. Each observed user-item interaction is considered a positive instance, and we apply a negative sampling strategy by pairing it with one negative item that the user has not interacted with.\n5.1.2 Evaluation Metrics. To evaluate the performance of the models, we employ widely recognized evaluation metrics: Recall and Normalized Discounted Cumulative Gain (NDCG), and report values of Recall@k and NDCG@k for k=10 and 20, following [10, 32]. To ensure unbiased evaluation, we adopt the all-ranking protocol. All items that are not interacted by a user are the candidates.\n5.1.3 Baseline Methods. To ensure a comprehensive assessment, we compare our method with ten baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL, KGRec), and LLM-based methods (RLMRec). BPR-MF [22] employs matrix factorization to model users and items, and uses the pairwise Bayesian Personalized Ranking (BPR) loss to optimize the model. NFM [9] is an advanced factorization model that subsumes FM [23] under neural networks. LightGCN [10] facilitates message propagation between users and items by simplifying GCN [16]. CKE [43] is an embedding-based method that uses TransR to guide entity representation in KGs to enhance recommendation performance. RippleNet [27] automatically discovers users\u2019 hierarchical interests by iteratively propagating users\u2019 preferences in the KG. KGAT [32] designs an attentive message passing scheme over the knowledge-aware collaborative graph for node embedding fusion. KGIN [33] adopts an adaptive aggregation method to capture finegrained user intentions.\n5.1.3 Baseline Methods. To ensure a comprehensive assessment, we compare our method with ten baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL, KGRec), and LLM-based methods (RLMRec). BPR-MF [22] employs matrix factorization to model users and items, and uses the pairwise Bayesian Personalized Ranking (BPR) loss to optimize the model. NFM [9] is an advanced factorization model that subsumes FM [23] under neural networks. LightGCN [10] facilitates message propagation between users and items by simplifying GCN [16]. CKE [43] is an embedding-based method that uses TransR to guide entity representation in KGs to enhance recommendation performance. RippleNet [27] automatically discovers users\u2019 hierarchical interests by iteratively propagating users\u2019 preferences in the KG. KGAT [32] designs an attentive message passing scheme over the knowledge-aware collaborative graph for node embedding fusion. KGIN [33] adopts an adaptive aggregation method to capture finegrained user intentions.\nKGCL [41] uses contrastive learning for knowledge graphs to reduce potential noise and guide user preference learning. KGRec [40] is a state-of-the-art KG-based recommendation model which devises a self-supervised rationalization method to identify informative knowledge connections. RLMRec [21] is a state-of-the-art LLM-based model. It directly utilizes LLMs to generate text profiles and combine them with recommendation models through contrastive learning. Since their method is model-agnostic, to ensure a fair comparison, we chose LightGCN as its backbone model, consistent with our method.\n5.1.4 Implementation Details. We implement all baseline methods according to their released code. The embedding size \ud835\udc51for all recommendation methods is set to 64 for a fair comparison. All experiments are conducted with a single V100 GPU. We set the batch size to 1024 for the Last-FM dataset and 4096 for the other datasets to expedite training. The Dropout rate is chosen from the set {0.2, 0.4, 0.6, 0.8} for both the embedding layer and the hidden layers. We employ the Adam optimizer with a learning rate of 0.001. The maximum number of epochs is set to 2000. The number of hidden layers for the recommendation model \ud835\udc3fis set to 3. For the LLM, we select DeepSeek-V2, a robust large language model that demonstrates exceptional performance on both standard benchmarks and open-ended generation evaluations. For more detailed information about DeepSeek, please refer to their official website4. Specifically, we utilize DeepSeek-V2 by invoking its API5. To reduce text randomness of the LLM, we set the temperature \ud835\udf0fto 0 and the top-\ud835\udc5dto 0.001. In addition, We fix the sampled number \ud835\udc5ato 10 and do not perform hyperparameter exploration due to cost considerations. For the text embedding model P, we use the pre-trained sup-simcse-roberta-large6 [7]. We use identical settings for the baselines that also involve LLMs and text embeddings to ensure fairness in comparison.\n# 5.2 Comparison Results\nWe compare 10 baseline methods across four datasets and run each experiment five times. The average results are reported in Table 2. Based on the results, we make the following observations: \u2022 Our method consistently outperforms all the baseline models across all four datasets. The performance ceiling of traditional methods (BPR-MF, NFM, LightGCN) is generally lower than that of KG-based methods, as the former rely solely on collaborative signals without incorporating semantic knowledge. However, some KG-based methods do not perform as well as LightGCN, indicating that effectively leveraging KG is a challenging task. \u2022 Among the KG-based baselines, KGCL and KGRec stand out the most. Both models incorporate self-supervised learning on top of general KG-based recommendation frameworks. During training, they jointly optimize the recommendation task and KG-based self-supervised tasks. However, they face challenges such as missing facts and difficulty in understanding semantic information. Additionally, they are unable to model higher-order associations of items within the KG. In contrast, our method does\n4https://github.com/deepseek-ai/DeepSeek-V2 5https://api-docs.deepseek.com/ 6https://huggingface.co/princeton-nlp/sup-simcse-roberta-large\n<div style=\"text-align: center;\">Table 2: Performance comparison of different methods, where R denotes Recall and N denotes NDCG. The best results ar bolded, and the second best results are underlined. Our improvement is statistically significant with a significance level of 0.01</div>\nModel\nMovieLens\nLast-FM\nMIND\nFunds\nR@10\nN@10\nR@20\nN@20\nR@10\nN@10\nR@20\nN@20\nR@10\nN@10\nR@20\nN@20\nR@10\nN@10\nR@20\nN@20\nBPR-MF\n0.1257\n0.3100\n0.2048\n0.3062\n0.1307\n0.1352\n0.1971\n0.1685\n0.0315\n0.0238\n0.0537\n0.0310\n0.4514\n0.3402\n0.5806\n0.3809\nNFM\n0.1346\n0.3558\n0.2129\n0.3379\n0.2246\n0.2327\n0.3273\n0.2830\n0.0495\n0.0356\n0.0802\n0.0458\n0.4388\n0.3187\n0.5756\n0.3651\nLightGCN 0.1598\n0.3901\n0.2512\n0.3769\n0.2589\n0.2799\n0.3642\n0.3321\n0.0624\n0.0492\n0.0998\n0.0609\n0.4992\n0.3778\n0.6353\n0.4204\nCKE\n0.1524\n0.3783\n0.2373\n0.3609\n0.2342\n0.2545\n0.3266\n0.3001\n0.0526\n0.0417\n0.0822\n0.0510\n0.4926\n0.3702\n0.6294\n0.4130\nRippleNet\n0.1415\n0.3669\n0.2201\n0.3423\n0.2267\n0.2341\n0.3248\n0.2861\n0.0472\n0.0364\n0.0785\n0.0451\n0.4764\n0.3591\n0.6124\n0.4003\nKGAT\n0.1536\n0.3782\n0.2451\n0.3661\n0.2470\n0.2595\n0.3433\n0.3075\n0.0594\n0.0456\n0.0955\n0.0571\n0.5037\n0.3751\n0.6418\n0.4182\nKGIN\n0.1631\n0.3959\n0.2562\n0.3831\n0.2562\n0.2742\n0.3611\n0.3215\n0.0640\n0.0518\n0.1022\n0.0639\n0.5079\n0.3857\n0.6428\n0.4259\nKGCL\n0.1554\n0.3797\n0.2465\n0.3677\n0.2599\n0.2763\n0.3652\n0.3284\n0.0671\n0.0543\n0.1059\n0.0670\n0.5071\n0.3877\n0.6355\n0.4273\nKGRec\n0.1640\n0.3968\n0.2571\n0.3842\n0.2571\n0.2748\n0.3617\n0.3251\n0.0627\n0.0506\n0.1003\n0.0625\n0.5104\n0.3913\n0.6467\n0.4304\nRLMRec\n0.1613\n0.3920\n0.2524\n0.3787\n0.2597\n0.2812\n0.3651\n0.3335\n0.0619\n0.0486\n0.0990\n0.0602\n0.4988\n0.3784\n0.6351\n0.4210\nCoLaKG\n0.1699 0.4130 0.2642 0.3974 0.2738 0.2948 0.3803 0.3471 0.0698 0.0562 0.1087 0.0684 0.5273 0.4012 0.6524 0.4392\n<div style=\"text-align: center;\">Table 3: Ablation study on all four datasets.</div>\nMetric\nw/o s\ud835\udc63\nw/o s\ud835\udc62\nw/o N\ud835\udc58(\ud835\udc63)\nw/o D\u2032\ud835\udc63\nCoLaKG\nML\nR@20\n0.2553\n0.2613\n0.2603\n0.2628\n0.2642\nN@20\n0.3811\n0.3948\n0.3902\n0.3960\n0.3974\nLast-FM\nR@20\n0.3628\n0.3785\n0.3725\n0.3789\n0.3803\nN@20\n0.3278\n0.3465\n0.3403\n0.3459\n0.3471\nMIND\nR@20\n0.1043\n0.1048\n0.1064\n0.1076\n0.1087\nN@20\n0.0640\n0.0658\n0.0662\n0.0671\n0.0684\nFunds\nR@20\n0.6382\n0.6481\n0.6455\n0.6499\n0.6524\nN@20\n0.4247\n0.4351\n0.4305\n0.4378\n0.4392\nnot require the introduction of self-supervised tasks. Instead, we leverage LLMs to address these existing challenges, resulting in significant improvements across all datasets and metrics. \u2022 For LLM-based recommendation methods, considering the cost, we select a recent and representative method closely related to our work: RLMRec. It can be observed that RLMRec only shows a slight improvement over its backbone model, LightGCN. In contrast, under the same backbone settings, our method significantly outperforms RLMRec, further validating the superiority of our approach. RLMRec only utilizes LLMs to capture textual profiles, neglecting the structural information of the KG and the higher-order semantic associations between items. Conversely, our method leverages LLMs to understand the KG and construct a semantic relational item-item graph, fully exploiting the semantic associations between items. This results in better item and user representations and improved recommendation performance.\n# 5.3 Ablation Study\nIn this section, we demonstrate the effectiveness of our model by comparing its performance with four different versions across all four datasets. The results are shown in Table 3, where \u201cw/o s\ud835\udc63\u201d denotes removing the semantic embeddings of items, \u201cw/o s\ud835\udc62\u201d denotes removing the semantic embeddings of users, \u201cw/o N\ud835\udc58(\ud835\udc63)\u201d\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d28f/d28f54cb-5f1a-4e01-adea-0322fecdc640.png\" style=\"width: 50%;\"></div>\n# Figure 4: Hyperparameter study of \ud835\udc58on four datasets.\n<div style=\"text-align: center;\">Figure 4: Hyperparameter study of \ud835\udc58on four datasets.</div>\nmeans removing the neighbor augmentation of items based on the constructed item-item graph, and \u201cw/o D\u2032\ud835\udc63\u201d means removing the second-order triples from the LLM\u2019s prompts. When the semantic embeddings of items are removed, the model\u2019s performance significantly decreases across all datasets, underscoring the critical role of semantic information captured by LLMs from the KG. Similarly, the removal of user semantic embeddings also results in a performance decline, affirming that LLMs can effectively infer user preferences from the KG. Furthermore, removing N\ud835\udc58(\ud835\udc63) leads to a performance drop across all datasets, highlighting the significance of the item representation augmentation module based on the constructed semantic-relational item-item graph. Without this module, the model can only capture local KG information from item-centered subgraphs and cannot leverage the semantic relations present in the global KG. The inclusion of this module facilitates the effective integration of both local and global KG information. Lastly, removing second-order KG triples from the prompts causes a slight performance decline. This finding suggests that incorporating second-order information from the KG allows the LLMs to produce a higher-quality comprehension of the local KG.\n# 5.4 Hyperparameter Study\nIn this section, we investigate the impact of the hyperparameter \ud835\udc58 of N\ud835\udc58(\ud835\udc63) on Recall@20 and NDCG@20 across four datasets. Here, \ud835\udc58 represents the number of semantically related neighbors, as defined\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7b98/7b98aba2-6719-421f-a1ec-d6f1f34d2bf5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Performance comparison on different user groups, where a smaller group ID indicates fewer interaction records.</div>\nin Section 4.1.2. Our experiments evaluate the model\u2019s performance as \ud835\udc58varies from 0 to 100. The results are presented in Figure 4. We observe that as \ud835\udc58increases, both Recall@20 and NDCG@20 initially rise and then slightly decline across all datasets. The performance is worst when \ud835\udc58= 0 and best when \ud835\udc58is between 10 and 30. This can be explained as follows: when \ud835\udc58= 0, no neighbors are used, which is equivalent to the ablation study without N\ud835\udc58(\ud835\udc63), thereby not incorporating any global semantic associations from the KG. When \ud835\udc58> 0, the introduction of semantically related items enhances the item\u2019s representations, leading to a noticeable improvement. However, as \ud835\udc58continues to increase, some noise may be introduced because the relevance of neighbors decreases with their ranking. Consequently, items with lower relevance may interfere with the recommendation performance. Our findings suggest that a range of 10-30 neighbors is optimal.\n# 5.5 Robustness to Varying Degrees of Sparsity\nOne of the key functions of KGs is to alleviate the issue of data sparsity. To further examine the robustness of our model against users with varying levels of activity, particularly its performance with less active users, we sort users based on their interaction frequency and divide them into four equal groups. A lower group ID indicates lower user activity (01 being the lowest, 04 the highest). We analyze the evaluation results on two relatively sparse datasets, Last-FM and MIND, as shown in Figure 5. By comparing our model with three representative and strong baseline models, we observe that our model consistently outperforms the baselines in each user group. Notably, the improvement ratio of our model in the sparser groups (01 and 02) is higher compared to the denser groups (03 and 04). For the group with the most limited data (Group 01), our model achieves the most significant lead. This indicates that the average improvement of our model is primarily driven by enhancements in the sparser groups, demonstrating the positive impact of CoLaKG in addressing data sparsity.\n# 5.6 Case Study\nIn this section, we conduct an in-depth analysis of the rationality of our method through two real cases. In the first case, we present the movie \u201cApollo 13\u201d and its five semantically related neighbor items in the item-item graph identified by our method. The first three movies belong to the same genre as \u201cApollo 13\u201d, making them 2-hop neighbors in the KG. In contrast, the other two movies, \u201cTop Gun\u201d and \u201cStar Trek\u201d, do not share any genre or other attributes\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c58f/c58fd0ca-9bcb-45f8-9d80-df50d8e31ef6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Case study.</div>\nwith \u201cApollo 13\u201d, indicating they are distant or unconnected in the KG. However, \u201cTop Gun\u201d and \u201cStar Trek\u201d are semantically related to \u201cApollo 13\u201d as they all highlight themes of human resilience, courage, and the spirit of adventure. Traditional KG-based recommendation methods, which rely on layer-by-layer information propagation, struggle to capture such high-order neighbors. In contrast, our method leverages similarity calculations based on item-centered KG semantic embeddings, successfully identifying these two strongly related movies. This demonstrates that our approach can effectively and efficiently capture semantically relevant information from the global KG. In the second case, we examine the movie \u201cA Little Princess\u201d and its related neighbors. Among the five related movies identified, \u201cThe Story of Cinderella\u201d and \u201cThe Princess Bride\u201d should share the same genre as \u201cA Little Princess\u201d. However, due to missing genres in the KG, these movies lack a path to \u201cA Little Princess\u201d within the KG. Despite this, our method successfully identifies these two movies. This demonstrates that our approach, by leveraging LLMs to complete and interpret the KG, can effectively address challenges posed by missing key attributes.\n# 6 Conclusion\nIn this paper, we analyze the limitations of existing KG-based recommendation methods and propose a novel approach, CoLaKG, to address these issues. CoLaKG comprehends item-centered KG subgraphs to obtain semantic embeddings for both items and users. These semantic embeddings are then used to construct a semantic relational item-item graph, effectively leveraging global KG information. We conducted extensive experiments on four datasets to validate the effectiveness and robustness of our method. The results demonstrate that our approach significantly enhances the performance of recommendation models.\n# References\n[1] Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe. 2023. Llm based generation of item-description for recommendation system. In Proceedings of the 17th ACM Conference on Recommender Systems. 1204\u20131207. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 1007\u20131014.\n[3] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019. Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences. In The world wide web conference. 151\u2013161. [4] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023. When large language models meet personalization: Perspectives of challenges and opportunities. arXiv preprint arXiv:2307.16376 (2023). [5] Djork-Arn\u00e9 Clevert. 2015. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289 (2015). [6] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023). [7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Empirical Methods in Natural Language Processing (EMNLP). [8] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering 34, 8 (2020), 3549\u20133568. [9] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. 355\u2013364. [10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639\u2013648. [11] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large language models as zero-shot conversational recommenders. In Proceedings of the 32nd ACM international conference on information and knowledge management. 720\u2013730. [12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval. Springer, 364\u2013381. [13] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging metapath based context for top-n recommendation with a neural co-attention model. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1531\u20131540. [14] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation via llm-based semantic embedding learning. In Companion Proceedings of the ACM on Web Conference 2024. 103\u2013111. [15] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2024. Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1395\u20131406. [16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [17] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large language models for generative recommendation: A survey and visionary discussions. arXiv preprint arXiv:2309.01157 (2023). [18] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web Conference 2024. 3497\u20133508. [19] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the AAAI conference on artificial intelligence, Vol. 29. [20] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. 2018. Deepinf: Social influence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2110\u20132119. [21] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464\u2013 3475. [22] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [23] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 635\u2013644. [24] Yu Tian, Yuhao Yang, Xudong Ren, Pengfei Wang, Fangzhao Wu, Qian Wang, and Chenliang Li. 2021. Joint knowledge pruning and recurrent graph convolution for news recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 51\u201360.\n[25] Riku Togashi, Mayu Otani, and Shin\u2019ichi Satoh. 2021. Alleviating cold-start problems in recommendation through pseudo-labelling over knowledge graph. In Proceedings of the 14th ACM international conference on web search and data mining. 931\u2013939. [26] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [27] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In Proceedings of the 27th ACM international conference on information and knowledge management. 417\u2013426. [28] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep knowledge-aware network for news recommendation. In Proceedings of the 2018 world wide web conference. 1835\u20131844. [29] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge graph convolutional networks for recommender systems. In The world wide web conference. 3307\u20133313. [30] Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153 (2023). [31] Shuyao Wang, Yongduo Sui, Chao Wang, and Hui Xiong. 2024. Unleashing the Power of Knowledge Graph for Recommendation via Invariant Learning. In Proceedings of the ACM on Web Conference 2024. 3745\u20133755. [32] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 950\u2013958. [33] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactions with knowledge graph for recommendation. In Proceedings of the web conference 2021. 878\u2013887. [34] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. 2019. Explainable reasoning over knowledge graphs for recommendation. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5329\u20135336. [35] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 806\u2013815. [36] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2024. Exploring large language model for graph data understanding in online job recommendations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 9178\u20139186. [37] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A survey on large language models for recommendation. arXiv preprint arXiv:2305.19860 (2023). [38] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024. A survey on large language models for recommendation. World Wide Web 27, 5 (2024), 60. [39] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In 2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 1259\u2013 1273. [40] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang. 2023. Knowledge graph self-supervised rationalization for recommendation. In Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining. 3046\u20133056. [41] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge graph contrastive learning for recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval. 1434\u20131443. [42] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM international conference on Web search and data mining. 283\u2013292. [43] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 353\u2013362. [44] Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu. 2024. Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph. arXiv preprint arXiv:2402.13750 (2024). [45] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [46] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).\n47] Xinjun Zhu, Yuntao Du, Yuren Mao, Lu Chen, Yujia Hu, and Yunjun Gao. 2023. Knowledge-refined Denoising Network for Robust Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 362\u2013371.\n[48] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2024. Collaborative large language model for recommender systems. In Proceedings of the ACM on Web Conference 2024. 3162\u20133172.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of existing knowledge graph (KG)-based recommendation methods, which suffer from missing facts, ineffective utilization of semantic information from textual entities, and challenges in capturing high-order relationships in global KGs. A new method is proposed to leverage large language models (LLMs) to enhance recommendation performance by comprehending KGs.",
        "problem": {
            "definition": "The problem involves the inability of current KG-based recommendation methods to effectively capture semantic relationships between items and users due to missing facts, loss of semantic information through ID conversion, and inefficient high-order relationship propagation.",
            "key obstacle": "The core obstacle preventing effective solutions is the reliance on incomplete KGs and the ineffectiveness of existing methods in utilizing the rich semantic information available in textual entities."
        },
        "idea": {
            "intuition": "The intuition behind the proposed method is that LLMs possess extensive world knowledge and reasoning capabilities that can be harnessed to understand and complete KGs, thereby improving recommendation performance.",
            "opinion": "The proposed idea, CoLaKG, involves using LLMs to comprehend item-centered KG subgraphs and generate semantic embeddings that enhance user and item representations in recommendation models.",
            "innovation": "The primary innovation of CoLaKG lies in its ability to transform KGs into a format suitable for LLMs, allowing for the effective extraction of semantic embeddings and the construction of a semantic relational item-item graph, which captures higher-order associations."
        },
        "method": {
            "method name": "Comprehending Knowledge Graphs with Large Language Models for Recommendation",
            "method abbreviation": "CoLaKG",
            "method definition": "CoLaKG is a method that utilizes LLMs to comprehend and transform the semantic and structural information of KGs into semantic embeddings for enhancing recommendations.",
            "method description": "CoLaKG extracts item-centered subgraphs from KGs, utilizes LLMs to generate semantic embeddings, and constructs an item-item graph to capture higher-order associations.",
            "method steps": [
                "Extract item-centered subgraphs from the KG.",
                "Convert these subgraphs into textual inputs for the LLM.",
                "Generate semantic embeddings from the LLM's comprehension of the subgraphs.",
                "Construct an item-item graph based on the semantic embeddings.",
                "Integrate semantic embeddings with ID embeddings in the recommendation model."
            ],
            "principle": "The effectiveness of CoLaKG stems from its ability to leverage LLMs for understanding KGs, thereby enriching the representation of items and users with semantic information that enhances recommendation accuracy."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on four real-world datasets: MovieLens, MIND, LastFM, and Funds, which vary in user interaction and item characteristics.",
            "evaluation method": "The performance of CoLaKG was assessed using Recall and Normalized Discounted Cumulative Gain (NDCG) metrics, comparing results against ten baseline methods across multiple datasets."
        },
        "conclusion": "The results demonstrate that CoLaKG significantly improves the performance of recommendation models by effectively leveraging semantic embeddings derived from KGs, thereby addressing the limitations of existing methods.",
        "discussion": {
            "advantage": "CoLaKG's key advantages include its ability to capture high-order relationships and effectively utilize both local and global KG information, resulting in superior recommendation performance.",
            "limitation": "One limitation of CoLaKG is the dependency on the quality of the underlying KG, as missing data can still impact the comprehensiveness of the embeddings generated.",
            "future work": "Future research could explore further enhancements in LLM integration and methods to improve the construction of KGs, focusing on reducing missing data and enhancing the semantic richness of the graphs."
        },
        "other info": {
            "info1": "The method was validated through extensive experiments demonstrating its robustness across various datasets.",
            "info2": {
                "info2.1": "The proposed approach is designed to be decoupled from LLM inference during the recommendation process.",
                "info2.2": "CoLaKG can be applied in real-world recommendation scenarios without requiring continuous LLM interaction."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The proposed idea, CoLaKG, involves using LLMs to comprehend item-centered KG subgraphs and generate semantic embeddings that enhance user and item representations in recommendation models."
        },
        {
            "section number": "3.2",
            "key information": "CoLaKG extracts item-centered subgraphs from KGs, utilizes LLMs to generate semantic embeddings, and constructs an item-item graph to capture higher-order associations."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of CoLaKG stems from its ability to leverage LLMs for understanding KGs, thereby enriching the representation of items and users with semantic information that enhances recommendation accuracy."
        },
        {
            "section number": "8",
            "key information": "CoLaKG is a method that utilizes LLMs to comprehend and transform the semantic and structural information of KGs into semantic embeddings for enhancing recommendations."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore further enhancements in LLM integration and methods to improve the construction of KGs, focusing on reducing missing data and enhancing the semantic richness of the graphs."
        }
    ],
    "similarity_score": 0.7374664068044944,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Comprehending Knowledge Graphs with Large Language Models for Recommender Systems.json"
}