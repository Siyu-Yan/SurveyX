{
    "from": "google",
    "scholar_id": "KrxWo1oxMY4J",
    "detail_id": null,
    "title": "Once: Boosting content-based recommendation with both open-and closed-source large language models",
    "abstract": "\n\nABSTRACT\n\n# ABSTRACT\n\nPersonalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However, existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tasks. In this study, we explore the potential of leveraging both open- and closed-source LLMs to enhance content-based recommendation. With open-source LLMs, we utilize their deep layers as content encoders, enriching the representation of content at the embedding level. For closed-source LLMs, we employ prompting techniques to enrich the training data at the token level. Through comprehensive experiments, we demonstrate the high effectiveness of both types of LLMs and show the synergistic relationship between them. Notably, we observed a significant relative improvement of up to 19.32% compared to existing state-of-the-art recommendation models. These findings highlight the immense potential of both openand closed-source of LLMs in enhancing content-based recommendation systems. We will make our code and LLM-generated data available 1 for other researchers to reproduce our results.\n\n\n\u2022 Information systems \u2192 Personalization; Data mining;  Recommender systems.\n\n# KEYWORDS\n\n\u2217 Corresponding author. 1 https://github.com/Jyonn/ONCE\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, t",
    "bib_name": "liu2024once",
    "md_text": "# CE: Boosting Content-based Recommendation with Bot Open- and Closed-source Large Language Models\n\nQijiong Liu liu@qijiong.work The Hong Kong Polytechnic University Hong Kong, China\n\nTetsuya Sakai tetsuyasakai@acm.org Waseda University Tokyo, Japan\n\nABSTRACT\n\n# ABSTRACT\n\nPersonalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However, existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tasks. In this study, we explore the potential of leveraging both open- and closed-source LLMs to enhance content-based recommendation. With open-source LLMs, we utilize their deep layers as content encoders, enriching the representation of content at the embedding level. For closed-source LLMs, we employ prompting techniques to enrich the training data at the token level. Through comprehensive experiments, we demonstrate the high effectiveness of both types of LLMs and show the synergistic relationship between them. Notably, we observed a significant relative improvement of up to 19.32% compared to existing state-of-the-art recommendation models. These findings highlight the immense potential of both openand closed-source of LLMs in enhancing content-based recommendation systems. We will make our code and LLM-generated data available 1 for other researchers to reproduce our results.\n\n\n\u2022 Information systems \u2192 Personalization; Data mining;  Recommender systems.\n\n# KEYWORDS\n\n\u2217 Corresponding author. 1 https://github.com/Jyonn/ONCE\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX\n\nNuo Chen pleviumtan@toki.waseda.jp Waseda University Tokyo, Japan\n\nxiao-ming.wu@polyu.edu.hk The Hong Kong Polytechnic University Hong Kong, China\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfb0/dfb022d7-8ea3-4865-8d8b-5226641686f7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Employing two different types of LLMs for contentbased recommendations.\n</div>\nACM Reference Format: Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2018. ONCE: Boosting Content-based Recommendation with Both Open- and Closedsource Large Language Models. In  Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym \u2019XX). ACM, New York, NY, USA, 14 pages. https://doi.org/XXXXXXX.XXXXXX\n\n# 1 INTRODUCTION\n\nContent-based recommender systems analyze the content and properties of items (e.g., articles, movies, books, or products) to deliver relevant and personalized recommendations to users. Some instances of such systems are Google News 2, which offers recommendations for news articles, and Goodreads 3, which provides recommendations for books. With the rapid expansion of digital content, it becomes increasingly essential to improve content-based recommendation techniques in order to meet users\u2019 expectations for precise and pertinent recommendations. The core component of content-based recommender systems is the content encoder, which is used for encoding the textual information of items in order to capture semantic features. In the past, recommendation models [1, 43, 45] commonly utilized convolutional neural networks (CNNs) as content encoders, typically initialized with pre-trained word representations such as GloVe [27]. In recent years, recommendation methods [46] have made use of pretrained language models (PLMs) based on the Transformer architecture [36] to extract more comprehensive semantic information.\n\n2 https://news.google.com/ 3 https://www.goodreads.com/\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7808/7808e564-cde4-40bc-8521-b5990d43299c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">te Summer Tree\nBook ID: 104086\n</div>\ngure 2: Comparison of content encoders used for content-based recommendation. To illustrate the similarity among the ree books, we employ relative distance to align the three different embedding spaces. First, we compute the cosine similarities \ud835\udc57 between each pair of books. Then, we calculate their relative distances using \ud835\udc51 \ud835\udc56,\ud835\udc57 = (1 \u2212 \ud835\udc60 \ud835\udc56,\ud835\udc57)/(1 \u2212 \ud835\udc60 \ud835\udc4f\ud835\udc59\ud835\udc62\ud835\udc52,\ud835\udc5c\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc52), i.e., by fixing e similarity between \u201cThe Lion King\u201d and \u201cThe Lions of Al-Rassan\u201d as 1. This approach allows for a direct comparison of eir similarities across the three distinct embedding spaces. It\u2019s important to note that a shorter distance indicates a greater milarity.\n\nDespite these advancements, existing methods still struggle to fully comprehend the content of items. To illustrate the limitations of previous content encoders, we present an example in Figure 2. We chose three books from the Goodreads dataset: \u201cThe Lion King\u201d, a novel adapted from a Disney animated movie, and the historical fantasy novels \u201cThe Lions of Al-Rassan\u201d and \u201cThe Summer Tree\u201d, both authored by Guy Gavriel Kay, belonging to a distinct category. We use different encoders to encoder the titles of these books and visualize their relative similarities in the embedding space. The results reveal that early content encoders relying on pretrained word embeddings struggle at the word level, failing to recognize crucial terms like \u201cAl-Rassan\u201d and resulting in erroneously high similarity between \u201cThe Lion King\u201d and \u201cThe Lions of Al-Rassan\u201d. Similarly, small-scale pretrained language models (PLMs) face challenges at the content level. Constrained by their pretraining data lacking relevant knowledge and their limited representation dimensions (e.g., 768), they are unable to fully grasp the content of \u201cThe Lions of Al-Rassan\u201d and \u201cThe Summer Tree\u201d and accurately perceive their similarity, leading to outcomes similar to the early content encoders. Such limitations can be overcome by the emerging large language models (LLMs), with the likes of closed-source ChatGPT 4\nand open-source LLaMA [32] leading the way. These models possess billions of parameters and are trained on datasets containing trillions of tokens. With each token represented in thousands of dimensions, they can store an extensive amount of information. In contrast to small PLMs like BERT, these LLMs demonstrate remarkable \u201cemergent abilities\u201d [42] in terms of advanced language comprehension and generation capabilities, making it possible to\n\n4 https://chat.openai.com\n\ndeliver more contextually relevant and personalized recommendations. When we use ChatGPT to inquire about a book, it showcases its enriched knowledge at the content level by providing detailed information such as the author, publication date, and subject matter. In Figure 2, we initially prompted LLaMA to generate concise descriptions of the three books solely based on the title information, and then employed it to encode these descriptions and obtain the corresponding representations 5. The results clearly indicate that the representations generated by LLaMA accurately reflect the similarity in content between the three books: the similarity between \u201cThe Lions of Al-Rassan\u201d and \u201cThe Summer Tree\u201d is higher than their similarity with \u201cThe Lion King\u201d. In this paper, we investigate the possibility of enhancing contentbased recommendation by leveraging both O pe N- and C los E dsource (ONCE) LLMs. As depicted in Figure 1, our approach ONCE adopts different strategies for each type of LLMs. For open-source LLMs like LLaMA, we employ a di scriminative re commendation approach named DIRE, reminiscent of the PLM-NR [46] method, by replacing the original content encoder with the LLM. This enables us to extract content representations and fine-tune the model specifically for recommendation tasks, ultimately enhancing user modeling and content understanding. Conversely, for closed-source LLMs like GPT-3.5, where we only have access to token outputs, we propose a gen erative re commendation approach named GENRE. By devising various prompting strategies, we enrich the available training data and acquire more informative textual and user features, which contribute to improved performance in downstream recommendation tasks.\n\n5 It is important to note that in our experiments in Section 5, we used LLaMA as a content encoder without any prompting.\n\n5 It is important to note that in our experiments in Section 5, we used LLaMA as a content encoder without any prompting.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2317/2317c662-ff27-45c7-8b19-601a26271611.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Prompting closed-source LLMs\nGENRE\n</div>\n<div style=\"text-align: center;\">Figure 3: An overview of our proposed ONCE framework, designed to enhance content-based re open-source LLMs (DIRE) and employing prompts for closed-source LLMs (GENRE).\n</div>\nosed ONCE framework, designed to enhance content-based recommendations by finetuning ploying prompts for closed-source LLMs (GENRE).\n\n<div style=\"text-align: center;\">verview of our proposed ONCE framework, designed to enhance content-based recommendations by finetuning LMs (DIRE) and employing prompts for closed-source LLMs (GENRE).\n</div>\nWe conducted extensive evaluations using two well-established content recommendation benchmarks: MIND [49] and Goodreads [38]. Our main objective was to thoroughly assess the impact of both open-source and closed-source LLMs on content-based recommendation models, focusing on recommendation quality and training efficiency. The results of our study demonstrate that both openand closed-source LLMs are highly effective, especially the former. Through the process of finetuning LLaMA, we consistently observed enhancements of more than 10 percentage points compared to existing state-of-the-art recommendation models. Additionally, we discovered a complementary relationship between open- and closed-source LLMs. Specifically, the enriched data generated by ChatGPT substantially accelerated the efficiency of finetuning LLaMA while simultaneously enhancing recommendation quality. Our findings highlight the immense potential of both types of LLMs in enhancing content-based recommendation systems.\n\n# 2 OVERVIEW\n\nBefore delving into the details of our proposed method, we first introduce basic notations and formally define the content-based recommendation task. Let N represent the set of contents, where each content \ud835\udc5b \u2208N is characterized by a diverse feature set, such as title, category, or description, in various recommendation scenarios. Similarly, let U denote the set of users, where each user \ud835\udc62 \u2208U maintains a history of browsed content denoted as \u210e (\ud835\udc62). Additionally, D corresponds to the set of click data, with each click \ud835\udc51 \u2208D represented as a tuple (\ud835\udc62,\ud835\udc5b,\ud835\udc66), indicating whether user \ud835\udc62 clicked on content \ud835\udc5b with label \ud835\udc66 \u2208 0, 1. The objective of contentbased recommendation is to infer the user\u2019s interest in a candidate content. A content-based recommendation model typically consists of three core modules: a content encoder, a history encoder, and an interaction module. The content encoder is responsible for encoding the multiple features of each content, consolidating them into a unified \ud835\udc51-dimensional content vector v \ud835\udc5b. On top of the content\n\n<div style=\"text-align: center;\">Finetuning open-source LLMs\nDIRE\n</div>\nencoder, the history encoder generates a unified \ud835\udc51-dimensional user vector v \ud835\udc62 based on the sequence of browsed content vectors. Finally, the interaction module aims to identify the positive sample that best aligns with the user vector v \ud835\udc62 among multiple candidate content vectors V \ud835\udc50 = [v (1) \ud835\udc50, ..., v (\ud835\udc58 + 1) \ud835\udc50], where \ud835\udc58 represents the number of negative samples. This process can be viewed as a classification problem.\n\n# 2.2 Enhancing Content-based Recommendatio with Open- and Closed-source LLMs (ONC\n\nLarge language models, endowed with deep semantic understanding and comprehensive knowledge acquired from pretraining, have exhibited proficiency across a multitude of natural language processing tasks. In this paper, we introduce the ONCE framework, which leverages both open-source and closed-source LLMs to enhance content-based recommendations. As highlighted in Touvron et al. [33], there remains a discernible gap between open-source models, encompassing approximately 10 billion parameters, and the closed-source GPT-3.5, an expansive entity boasting over 175 billion parameters. Our ONCE framework capitalizes on the strengths of both types and constructs a more robust recommendation system. We initiate the process by utilizing the closed-source LLM through prompting, enhancing the dataset from various perspectives, following our designed generative recomendation framework (GENRE). This infusion of external knowledge, a facet not readily accessible to open-source models, ensues. Subsequently, we propose a discriminative recommendation framework (DIRE) to harness the deep layers of the open-source LLM as content encoders, thereby amplifying content representations.\n\n# 3 DIRE: FINETUNING OPEN-SOURCE LLMS\n\nIntegrating open-source language models as content encoders is a straightforward and widely adopted method in content-based recommendation [26, 46]. Notably, PLM-NR [46] employs smallscale pretrained language models (PLMs, e.g., BERT [10]) to replace original news encoders and finetunes on the recommendation task.\n\nThe success of this approach relies on two factors: 1) the knowledge inherent in the pretrained language models (including model size and pretraining data quality), and 2) the finetuning strategy. As discussed earlier, we have already highlighted the advantages of large language models in content understanding and user modeling, addressing the first factor. In this section, we propose discriminative recommendation framework, namely DIRE, and explore how to leverage open-source large language models to further enhance recommendation performance by considering the second factor.\n\n# 3.1 Network Architecture\n\nAs depicted in Figure 3, we seamlessly incorporate the open-source large language model and an attention fusion layer into the contentbased recommendation framework. Embedding Layer.  In contrast to the approach taken by smallerscale PLMs like BERT, which utilize specific tokens (e.g., \u27e8 cls \u27e9, \u27e8 sep \u27e9) to segment distinct fields, we adopt natural language templates for concatenation. For instance, consider a news content \ud835\udc5b containing attributes such as title, abstract, and category features. As illustrated in Figure 3, We introduce the label \u201cnews article:\u201d at the outset of the sequence, while each feature is prefixed with \u201c\u27e8 feature \u27e9\u201d. This procedure transforms the multi-field content into a cohesive individual sequence s of length \ud835\udc59. We refer to this technique as the \u201cNatural Concator\u201d. Following this, we make use of pretrained token embeddings provided by the LLM to map discrete text sequence into a continuous embedding space of dimensionality \ud835\udc51 \ud835\udc5b, denoted as:\n\n# E 0 = \ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f (s) \u2208 R \ud835\udc59 \u00d7 \ud835\udc51 \ud835\udc5b.\n\n(1)\n\nTransformer Decoder. The design of the LLM (or LLaMA) is based on the Transformer architecture [36], incorporating multiple tiers of Transformer Layers. This configuration is intricately interconnected, with the output hidden state from each layer feeding into the input of the next layer, denoted as:\n\ufffd \ufffd\n\nE i = \ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f \ufffd E i \u2212 1 \ufffd \u2208 R \ud835\udc59 \u00d7 \ud835\udc51 \ud835\udc5b,\ud835\udc56 \u2208{1, ..., \ud835\udc3b},\n\n(2)\n\n\ufffd \ufffd\nwhere \ud835\udc3b represents the number of Transformer Layers. Attention Fusion Layer. To combine the sequential hidden states from the last layer into a single cohesive content representation, we employ the attention fusion layer, following a similar approach as used in PLM-NR [46]. Specifically, we begin by mapping the high-dimensional hidden states from a large space of dimensionality \ud835\udc51 \ud835\udc5b to a smaller \ud835\udc51-dimensional space (where \ud835\udc51 \ud835\udc5b \u226b \ud835\udc51), defined by:\n\n# Z = E i W + b \u2208 R \ud835\udc59 \u00d7 \ud835\udc51,\n\n(3)\n\nwhere W \u2208 R \ud835\udc51 \ud835\udc5b \u00d7 \ud835\udc51 and b \u2208 R \ud835\udc51 are the learnable parameters of the linear transformation. Next, we utilize the additive attention mechanism [2] to further condense the reduced representation into a unified representation z, defined by:\n\nwhich will be fed into the user modeling module or interaction module for further personalized recommendation.\n\n# 2 Finetuning Strategy\n\nPartial Freezing and Caching. Running large language models incurs significant computational demands due to their expansive collection of transformer layers and associated parameters. Given that the lower layers of the LLM tend to possess a more generalized and less task-specific nature, we opt to keep these layer parameters fixed. Instead, we exclusively fine-tune the uppermost \ud835\udc58 layers, where \ud835\udc3b \u226b \ud835\udc58. Furthermore, we adopt a caching strategy wherein we precompute and store the hidden states from the lower layers for all contents within the dataset (potentially numbering in the thousands) prior to fine-tuning. For instance, in the case of the LLaMA-7B model comprising 32 layers, only the top 2 layers are subjected to fine-tuning. This caching process substantially mitigates computational costs, reducing the LLM\u2019s computation load to a mere 2 / 32 \u2248 6% of the original cost. Parameter-Efficient Tuning. Low-Rank Adaptation (LoRA) [14] introduces trainable rank decomposition matrices into pretrained model layers, notably slashing the necessary trainable parameters for downstream tasks. This outperforms traditional fine-tuning, drastically reducing parameters, sometimes by a factor of 10,000. Here, we apply LoRA to the unfrozen Transformer layers, which are the most parameter-intensive components of the model. We also test finetuning without LoRA, employing distinct learning rates for the pretrained Transformer layers and other model components. Further elaboration is available in the Experiments section.\n\n# 4 GENRE: PROMPTING CLOSED-SOURCE LLMS\n\n# 4 GENRE: PROMPTING CLOSED-SOURCE\n\nLarge language models differ significantly from previous models like BERT [10] in terms of their emergent abilities [42] such as strong text comprehension and language generation capabilities, having resulted in a paradigm shift from the traditional pretrainfinetune approach to the prompting-based approach. Previous studies [21] have found that using closed-source LLMs directly as recommenders without finetuning (completely bypassing conventional recommendation systems), using methods like prompts [25, 39] or in-context learning [7], only matches the performance of basic matrix factorization [17] methods or even random recommendations. This falls short when compared to modern attention-based approaches. To overcome this, we propose a generative recommendation framework, namely GENRE, as shown in Figure 4a: leveraging closed-source LLMs (specifically, GPT-3.5) to augment data, aiming to enhance their performance on downstream conventional recommendation models. More precisely, the workflow consists of the following four steps. 1) Prompting: create prompts or instructions to harness the capability of a LLM for data generation for diverse objectives. 2) Generating: the LLM generates new knowledge and data based on the designed prompts. 3) Updating (optional): use the generated data to update the current data for the next round of prompting and generation. 4) Training: leverage the generated data to train news recommendation models. If the updating step is performed, we name it as \u201cChain-based Generation\u201d, otherwise, we name it as \u201cOne-pass Generation\u201d.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/07f6/07f68f93-6cc6-4085-be0f-84ea8b73ee4e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6098/6098caae-7798-4423-878a-88629b9be421.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)\n</div>\n# .1 LLMs as Content Summarizer\n\nLarge language models are capable of summarizing text content into concise phrases or sentences, due to their training on vast amounts of natural language data and summarization tasks. Moreover, entities like the names of individuals and locations may have appeared infrequently in the original dataset, making it challenging to learn their representations with traditional methods. However, large language models can associate them more effectively with knowledge learned during pretraining. By providing the content title, abstract, and category as input, the large language model produces a more informative title as output, as illustrated in Figure 4b. During downstream training, the enhanced content title will replace the original one and be used as one of the input features for the content encoder (Figure 3).\n\n# 4.2 LLMs as User Profiler\n\nThe user profile generally refers to their preferences and characteristics, such as age, gender, topics of interest, and geographic location. They are usually not provided in the anonymized dataset due to privacy policies. Large language models are capable of understanding the browsing history and analyze an outline of the user profile. As depicted in Figure 4b, the large language model produces topics and regions of interest when given the user browsing history. In this example, GPT-3.5 infers that the user may be interested in the region of \u201cFlorida\u201d, based on the word \u201cMiami\u201d in the news. While \u201cMiami\u201d may have a low occurrence in the dataset, \u201cFlorida\u201d is more frequently represented and therefore more likely to be connected to other news or users for collaborative filtering. To incorporate the inferred user profile into the recommendation model, we first fuse the topics and regions of interest into an interest vector v \ud835\udc56, defined by:\n\n(5)\n\nwhere POOL is the average pooling operation, E topics and E regions are the embedding matrices of the interested topics and regions, and [;] is the vector concatenation operation. Then, the interest vector v \ud835\udc56 will be combined with the user vector v \ud835\udc62 learned from the history encoder (shown in Figure 3) to form the interest-aware user vector v \ud835\udc56\ud835\udc62 as follows:\n\nv \ud835\udc56\ud835\udc62 = MLP ([v \ud835\udc62; v \ud835\udc56]) \u2208 R \ud835\udc51,\n\nwhere MLP is a multi-layer perceptron with ReLU activation. Finally the interest-aware user vector will replace the original user vector to participate in the click probability prediction.\n\n# 4.3 LLMs as Personalized Content Generator\n\nRecent studies [6, 34] have shown that large language models possess exceptional capabilities to learn from few examples. Hence, we propose to use GPT-3.5 to model the distribution of user-interested content given very limited browsing history data. Specifically, we use it as a personalized content generator to generate synthetic content that may be of interest to new users 6 have limited interaction data, making it difficult for the user encoder to capture their characteristics and ultimately weakening its ability to model warm users 7, enhancing their historical interactions and allowing the history encoder to learn effective user representations.\n\n# 4.4 Chain-based Generation\n\nWhile we have shown several examples of \u201cone-pass generation\u201d, it is worth noting that large language models allow iterative generation and updating. The data generated by the large language models can be leveraged to enhance the quality of current data, which can subsequently be utilized in the next round of prompting and generation in an iterative fashion. We design a chain-based personalized content generator by combining the one-pass user profiler and personalized content generator. Specifically, we first use the GPT-3.5 to generate the interested topics and regions of a user, which are then combined with the user history to prompt the large language model to generate synthetic content pieces. The user profile helps the large language models to engage in chain thinking, resulting in synthetic content that better matches the user\u2019s interests than the one-pass prompting.\n\n# 5 EXPERIMENTS\n\n# 5.1 Experimental Setup\n\nDatasets.  We conduct experiments on two real-world contentbased recommendation dataset, i.e., news recommendation dataset MIND [49] and book recommendation dataset Goodreads [38]. In Table 1, we present the statistics of both the original dataset and the augmented versions. We use LLaMA-7B and LLaMA-13B models [32] as our open-source large language models, and GPT-3.5 8\n\n6 Following [13], we use \u201cnew users\u201d to refer to users with no more than five contents in browsing history. 7 We use \u201cwarm user\u201d to represent the user who has browsed more than five contents. 8 https://platform.openai.com/docs/guides/chat\n\nTable 1: Data statistics. We use \u201cuser \ud835\udc5b\u201d to denote new users. Green numbers signify improvements over the original dataset, while blue numbers indicate the values of newly introduced features.\n\nDataset\nMIND\nGoodreads\nDataset MIND Goodreads\nOriginal\nContent Summarizer (CS)\n# content\n65,238\n16,833\ntokens/title\n+3.17\n-\ntokens/title\n13.56\n6.10\ntokens/desc\n-\n29.28\n# users\n94,057\n23,089\nUser Profiler (UP)\n# new user\n20,110\n2,306\ntopics/user\n4.82\n4.55\ncontent/user\n14.98\n7.81\nregions/user\n0.29\n-\ncontent/user\ud835\udc5b\n3.19\n3.03\nPersonalized Content Generator (CG)\n# pos\n347,727\n273,888\n#content +40,220\n+4,612\n# neg 8,236,715\n485,233\ncontent/user\ud835\udc5b\n+2.00\n+2.00\nas our closed-source model. For the augmented datasets, only the attributes that are different than the original datasets are shown in Table 1. For the Goodreads dataset, the content summarizer is used for the book description generation, given only the book title.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2fd7/2fd73f25-f202-4763-b9a6-21c7d3df01ee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) NRMS\n</div>\n<div style=\"text-align: center;\">(b) Fastformer\n</div>\nFigure 5: Training curves for open-source LLMs and ONCE. The y-axis AUC value is evaulated on the validation set.\n\n<div style=\"text-align: center;\">Figure 5: Training curves for open-source LLMs and ONCE. The y-axis AUC value is evaulated on the validation set.\n</div>\nRecommendation Models. We evaluate the effectiveness of proposed ONCE method with three popular content-based recommendation models, namely NAML [43], NRMS [45], and Fastformer [47]. We also compare with PLM-NR [46] method, which replaces the original content encoder with small-scale pretrained language models such as BERT [10]. Evaluation Metrics. We follow the common practice [26, 45, 46] to evaluate the effectiveness of news recommendation models with the widely used metrics, i.e., AUC [11], MRR [37] and nDCG [15]. In this work, we use nDCG@1 and nDCG@5 for evaluation on the Goodreads dataset, and nDCG@5 and nDCG@10 for evaluation on the MIND dataset shortly denoted as N@1, N@5 and N@10, respectively. Implementation Details. During training, we employ Adam [16] optimizer with a learning rate of 1e-3 for the MIND dataset and 1e-4 for the Goodreads dataset. If the large language models are not tuned with LoRA [14], their learning rates are set to 1e-5. For all models, the embedding dimension of non-LLM modules is set to 64,\n\nand the negative sampling ratio is set to 4. We tune the hyperparameters of all base models to attain optimal performance. We average the results of five independent runs for each model and observe the p-value smaller than 0.01. All LLaMA-based experiments are conducted on a single NVIDIA A100 device with 80GB memory, and others on a single NVIDIA GeForce RTX 3090 device. We release all our code and datasets 9 for other researches to reproduce our work.\n\n# 5.2 Performance Comparison\n\nTable 2 provides an overview of the performance enhancements observed across four base models on two datasets, boosted by opensource LLM, closed-source LLM, and dual LLM (i.e., ONCE) approaches. Drawing from the results, we can derive the following observations: Firstly, the open-source LLM group exhibits substantial improvements in the base models. The pretraining of LLaMA endows it with robust semantic understanding and a wealth of content-level knowledge, including elements like book titles and geographic locations. Additionally, its high-dimensional representation space ensures efficient encoding of extensive information within hidden states. Secondly, the closed-source LLM group also demonstrates impressive performance, highlighting the efficacy of data enrichment in introducing enhanced semantic features to the dataset. The fusion of diverse prompt techniques (i.e., \u201cALL\u201d) further amplifies model effectiveness. Thirdly, our dual LLM-based ONCE method showcases additional performance gains compared to employing a single LLM, albeit the improvement is relatively modest compared to the open-source finetuning. GPT-3.5 offers LLaMA with supplementary semantic insights, elevating its content comprehension capabilities. However, the closed-source LLM contributes tokenlevel discrete features, which bear less influence when juxtaposed with the continuous embedding-level representations delivered by open-source LLMs. In addition,Figure 5 presents the training curves for open-source LLMs and ONCE. Notably, ONCE (using LLaMA-13B as the backbone), leveraging closed-source LLM information, demonstrates both a stronger initial performance and quicker training efficiency. Specifically, on the NRMS model, ONCE reaches performance equivalent to LLaMA-13B\u2019s 8th epoch by its 6th epoch, a substantial 25% improvement. On the Fastformer model, ONCE surpasses LLaMA13B\u2019s 15th epoch performance by its 9th epoch, showcasing an impressive 40% enhancement.\n\n# 5.3 Ablation Study on Open-source LLMs\n\nHere, we study the impact of the finetuning layers and low-rank adaptation (LoRA) on the performance of open-source LLMs. Table 3 presents a comparison of finetuning effects on the top 0 \u223c 2 layers of transformers across different open-source LLMs. Key findings from the results include: Firstly, In most instances, substantial enhancements in recommendation models are evident even without finetuning (T=0) the LLMs. Notably, the BERT model on the Goodreads dataset is an exception due to the unique challenge posed by book titles as content, which lacks the enriched knowledge of LLaMA, resulting in less effective representations primarily focused on literal meanings.\n\n9 https://github.com/Jyonn/ONCE\n\nTable 2: Performance comparison among original recommenders, recommenders enhanced by open-source LLMs (i.e., DIRE), those enhanced by closed-source LLMs (i.e., GENRE), and those boosted by both types of LLMs (i.e., ONCE). In the open-source LLM category, we reference the BERT 12 \ud835\udc3f approach as detailed by PLM-NR [46]. Within the closed-source LLM category, the abbreviations \u201cCS\u201d, \u201cUP\u201d, \u201cCG\u201d, and \u201cUP \u2192 CG\u201d represent datasets augmented by the one-pass content summarizer, one-pass user profiler, one-pass personalized content generator, and chain-based personalized content generator, respectively. Furthermore, \u201cALL\u201d denotes a dataset that incorporates enhancements from CS, and UP \u2192 CG. The top-performing results are emphasized in bold.\n\nNAML (2019a)\nNRMS (2019c)\nFastformer (2021b)\nMINER (2022)\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nMIND dataset\nOriginal\n61.75\n30.60\n31.35\n37.85\n61.71\n30.20\n30.98\n37.42\n62.26\n31.14\n31.90\n38.32\n63.88\n32.19\n33.04\n39.45\nDIRE\nBERT12L [46]\n65.32\n33.16\n34.29\n40.35\n64.08\n31.24\n32.35\n38.66\n65.48\n32.47\n33.41\n39.75\n65.82\n32.77\n34.02\n40.19\nLLaMA7B (Ours)\n68.34\n35.80\n37.60\n43.48\n68.50\n36.21\n38.11\n43.91\n68.55\n36.59\n38.38\n44.06\n68.70\n36.58\n38.49\n44.18\nLLaMA13B (Ours)\n68.23\n35.99\n37.93\n43.77\n68.45\n36.15\n38.02\n43.88\n68.51\n36.37\n38.20\n44.02\n68.59\n36.46\n38.38\n44.05\nGENRE\nCS (Ours)\n63.73\n31.83\n32.94\n39.24\n63.85\n31.57\n32.35\n38.80\n64.73\n32.81\n33.68\n40.06\n65.71\n33.59\n34.90\n40.96\nUP (Ours)\n62.19\n30.90\n31.78\n38.26\n61.90\n30.60\n31.54\n37.66\n63.40\n31.94\n32.76\n39.15\n64.45\n32.09\n33.14\n39.54\nCG (Ours)\n62.93\n30.83\n32.10\n38.34\n63.04\n31.00\n31.84\n38.22\n64.69\n32.28\n33.31\n39.76\n64.21\n32.30\n33.57\n39.91\nUP\u2192CG (Ours)\n63.61\n31.58\n32.63\n39.07\n62.95\n32.00\n32.80\n39.00\n64.82\n32.44\n33.51\n39.93\n64.73\n33.09\n34.10\n40.32\nALL (Ours)\n63.88\n32.17\n33.14\n39.37\n63.71\n32.14\n33.11\n39.43\n66.70\n34.20\n35.81\n41.78\n66.46\n34.20\n35.47\n41.48\nONCE (ours)\n68.62\n36.50\n38.31\n44.05\n68.74\n36.66\n38.60\n44.37\n68.83\n36.68\n38.56\n44.35\n68.92\n36.74\n38.72\n44.48\nImprovement (%) over Original 11.13% 19.28% 22.20% 16.38% 11.39% 21.39% 24.60% 18.57% 10.55% 17.79% 20.88% 15.74% 7.89% 14.13% 17.19% 12.75%\nImprovement (%) over BERT12\ud835\udc3f\n5.05%\n10.07% 11.72%\n9.17%\n7.27%\n17.35% 19.32% 14.77%\n5.12%\n12.97% 15.41% 11.57% 4.71% 12.11% 13.82% 10.67%\nGoodreads dataset\nOriginal\n66.47\n75.75\n58.49\n82.20\n68.95\n77.05\n60.62\n83.16\n70.85\n78.37\n62.90\n84.15\n71.03\n78.46\n63.09\n84.20\nDIRE\nBERT12L [46]\n70.68\n78.17\n62.26\n83.99\n71.80\n78.87\n63.62\n84.51\n72.47\n79.29\n64.45\n84.82\n73.36\n80.08\n65.19\n85.25\nLLaMA7B (Ours)\n77.01\n82.74\n71.09\n89.39\n75.90\n81.75\n69.13\n86.65\n76.52\n82.31\n70.48\n87.03\n76.45\n82.46\n70.31\n86.92\nLLaMA13B(Ours)\n77.43\n83.05\n71.56\n87.61\n77.57\n82.96\n71.41\n87.55\n77.46\n83.00\n71.36\n87.58\n77.50\n83.07\n71.44\n87.64\nGENRE\nCS (Ours)\n67.68\n76.41\n59.64\n82.69\n69.77\n77.57\n61.54\n83.35\n71.41\n78.77\n63.70\n84.43\n71.96\n79.09\n64.30\n84.72\nUP (Ours)\n68.45\n76.91\n60.70\n83.08\n69.45\n77.58\n61.89\n83.57\n71.15\n78.68\n63.86\n84.39\n71.67\n78.85\n63.94\n84.50\nCG (Ours)\n66.94\n76.10\n59.26\n82.47\n70.09\n77.95\n62.34\n83.83\n71.08\n78.53\n63.38\n84.26\n71.81\n78.89\n63.99\n84.53\nUP\u2192CG (Ours)\n67.98\n76.78\n60.56\n82.96\n69.95\n77.79\n62.07\n83.71\n71.88\n79.02\n64.10\n84.63\n71.79\n78.93\n63.97\n84.56\nALL (Ours)\n68.95\n77.25\n61.19\n83.32\n72.07\n79.13\n64.46\n84.72\n73.23\n79.97\n66.07\n85.33\n73.21\n79.91\n65.73\n85.29\nONCE (ours)\n77.63\n83.13\n71.65\n87.66\n77.89\n83.31\n71.89\n87.79\n78.03\n83.52\n72.52\n87.96\n77.82\n83.35\n71.96\n87.85\nImprovement (%) over Original 16.79%\n9.74%\n22.50%\n6.64%\n12.97%\n8.12%\n18.59%\n5.57%\n10.13%\n6.57%\n15.29%\n4.53%\n9.56%\n6.23%\n14.06%\n4.33%\nImprovement (%) over BERT12\ud835\udc3f\n9.83%\n6.35%\n15.08%\n4.37%\n8.48%\n5.63%\n13.00%\n3.88%\n7.67%\n5.33%\n12.52%\n3.70%\n6.08%\n4.08%\n10.39%\n3.05%\nSecondly, within the MIND dataset, LLaMA-7B generally outperforms LLaMA-13B with finetuning 1 \u223c 2 layers. This might stem from the relative difficulty in fine-tuning LLaMA-13B, while the 7B model sufficiently captures the semantic richness of news headlines. Conversely, for the Goodreads dataset, LLaMA-13B demonstrates the most promising outcomes. Thirdly, overall, a greater number of tuned layers correlates with improved performance, though this also entails increased training costs. Table 4 presents the influence of LoRA during the finetuning process of open-source LLMs. Our findings indicate that, for the MIND dataset, LoRA leads to improved performance, while a different pattern emerges for the Goodreads dataset. This divergence might be attributed to differences in the nature of the input textual data. Goodreads employs book titles with relatively limited informative content, whereas MIND\u2019s input news headlines inherently encapsulate the core essence of the content. Constructing a robust\n\nrepresentation from book titles requires more nuanced adjustments of the network parameters.\n\n# .4 Ablation Study on Closed-source LLMs\n\nHere, we investigate the impact of the synthetic content data on two user groups, i.e., new user group and warm user group. From the results in Table 5, it can be seen that the personalized content generator improves the performance of both the new and warm user groups in most cases. This is because the history encoder struggles to capture the interests of new users due to their limited history, which also affects its ability to model warm users. With the generated content pieces added to the history of new users, the history encoder can better capture their interests, leading to a performance improvement on both groups.\n\n<div style=\"text-align: center;\">Table 3: Influence of the number of frozen layers on three open-source LLMs. Best results are highli inferior to the respective base models are indicated in red. \u201cF/T\u201d denotes the number of frozen and\n</div>\n<div style=\"text-align: center;\">fluence of the number of frozen layers on three open-source LLMs. Best results are highlighted in bold, while result the respective base models are indicated in red. \u201cF/T\u201d denotes the number of frozen and tuning layers, respectively\n</div>\nNAML (2019a)\nNRMS (2019c)\nFastformer (2021b)\nMINER (2022)\nEncoder\nF/T\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nMIND dataset\nOriginal\n-\n61.75\n30.60\n31.35\n37.85\n61.71\n30.20\n30.98\n37.42\n62.26\n31.14\n31.90\n38.32\n63.88\n32.19\n33.04\n39.45\nBERT12\ud835\udc3f\n12/0\n65.32\n33.16\n34.29\n40.35\n64.08\n31.24\n32.35\n38.66\n64.25\n32.05\n32.88\n39.17\n64.75\n32.44\n33.60\n39.87\n11/1\n65.10\n32.86\n33.99\n40.19\n62.59\n31.46\n32.09\n38.61\n65.48\n32.47\n33.41\n39.75\n65.82\n32.77\n34.02\n40.19\n10/2\n63.79\n32.27\n32.95\n39.40\n62.68\n30.95\n31.61\n37.89\n63.41\n31.57\n32.56\n38.92\n64.01\n31.69\n32.82\n39.17\nLLaMA7\ud835\udc35\n32/0\n67.78\n35.17\n36.84\n42.78\n68.10\n35.33\n36.91\n43.04\n67.83\n35.19\n36.57\n42.59\n67.96\n35.28\n36.72\n42.80\n31/1\n68.34\n35.80\n37.60\n43.48\n68.33\n35.81\n37.43\n43.37\n68.51\n36.56\n38.46\n44.15\n68.45\n36.41\n38.25\n43.93\n30/2\n68.18\n36.09\n37.76\n43.65\n68.50\n36.21\n38.11\n43.91\n68.55\n36.59\n38.38\n44.06\n68.70\n36.58\n38.49\n44.18\nLLaMA13\ud835\udc35\n40/0\n68.23\n35.99\n37.93\n43.77\n68.45\n36.15\n38.02\n43.88\n68.51\n36.37\n38.20\n44.02\n68.59\n36.46\n38.38\n44.05\n39/1\n67.66\n35.73\n37.59\n43.35\n68.23\n36.05\n37.97\n43.72\n68.60\n36.45\n38.27\n43.96\n68.53\n36.37\n38.21\n44.00\n38/2\n68.19\n36.07\n37.89\n43.68\n68.30\n36.13\n37.95\n43.74\n68.19\n35.96\n37.72\n43.50\n67.83\n35.88\n37.64\n43.45\nGoodreads dataset\nOriginal\n-\n66.47\n75.75\n58.49\n82.20\n68.95\n77.05\n60.62\n83.16\n70.85\n78.37\n62.90\n84.15\n71.03\n78.46\n63.09\n84.20\nBERT12\ud835\udc3f\n12/0\n62.05\n72.82\n53.37\n80.06\n64.49\n74.38\n56.47\n81.22\n66.83\n75.85\n58.68\n82.35\n67.11\n76.09\n58.88\n82.48\n11/1\n62.32\n73.03\n53.82\n80.22\n65.94\n75.35\n58.12\n81.94\n66.23\n75.53\n58.12\n82.05\n66.72\n75.93\n58.60\n82.28\n10/2\n65.22\n74.90\n57.07\n81.58\n63.77\n73.94\n55.53\n80.88\n67.66\n76.49\n60.03\n82.76\n67.94\n76.72\n60.22\n82.89\n0/12\n70.68\n78.17\n62.26\n83.99\n71.80\n78.87\n63.62\n84.51\n72.47\n79.29\n64.45\n84.82\n73.36\n80.08\n65.19\n85.25\nLLaMA7\ud835\udc35\n32/0\n69.29\n77.32\n60.89\n83.37\n71.96\n79.19\n64.73\n84.77\n72.25\n79.16\n64.35\n84.73\n71.14\n78.53\n63.44\n84.27\n31/1\n73.82\n80.34\n66.51\n85.61\n75.18\n81.23\n68.04\n86.27\n75.80\n81.70\n68.69\n86.58\n75.33\n81.35\n68.26\n86.33\n30/2\n77.01\n82.74\n71.09\n89.39\n75.90\n81.75\n69.13\n86.65\n76.52\n82.31\n70.48\n87.03\n76.45\n82.46\n70.31\n86.92\nLLaMA13\ud835\udc35\n40/0\n70.31\n78.00\n62.36\n83.88\n72.82\n79.79\n65.79\n85.21\n71.66\n78.81\n63.52\n84.48\n73.28\n80.13\n66.09\n85.23\n39/1\n77.43\n83.05\n71.56\n87.61\n76.55\n82.32\n70.08\n87.07\n76.42\n82.36\n70.51\n87.10\n77.18\n82.60\n71.17\n87.43\n38/2\n76.25\n82.18\n69.98\n86.98\n77.57\n82.96\n71.41\n87.55\n77.46\n83.00\n71.36\n87.58\n77.50\n83.07\n71.44\n87.64\n<div style=\"text-align: center;\">Table 4: Influence of the use of low-rank adaption (LoRA). The experiments are conducted over the NAML model.\n</div>\nLoRa\nw/o LoRa\nEncoder\nAUC\nMRR\nN@5\nN@10\nAUC\nMRR\nN@5\nN@10\nMIND dataset\nBERT12l\n65.10\n32.86\n33.99\n40.19\n62.94\n31.32\n32.20\n38.52\nLLaMA7B\n68.34\n35.80\n37.60\n43.48\n67.25\n34.28\n36.00\n42.12\nGoodreads dataset\nBERT12L\n63.18\n76.80\n55.37\n80.69\n70.68\n78.17\n62.26\n83.99\nLLaMA7B\n75.00\n81.23\n68.44\n86.29\n77.01\n82.74\n71.09\n89.39\n# 6 RELATED WORKS\n\n# 6.1 LLMs for Recommendation\n\nThe recent advancement of Large Language Models (LLMs) like ChatGPT and LLaMa [32], has triggered a new wave of interest,\n\nTable 5: Effectiveness of the personalized content generator (CG) for both new user and warm user groups, assessed on the MIND dataset. ORI: training with the original data. Imp.: denotes the improvement realized through the personalized content generator.\n\nNew User\nWarm User\nAUC MRR N@5 N@10 AUC MRR N@5 N@10\nNAML\nORI 59.24 32.82 34.24 40.34 62.21 30.20 30.83\n37.40\nCG 60.21 32.69 34.67 40.33 63.43 30.49 31.64 37.98\nImp. 0.97\n-\n0.43\n-\n1.22\n0.29\n0.81\n0.58\nNRMS\nORI 59.49 32.75 33.99\n40.09\n62.12 29.74 30.43\n36.93\nCG 59.88 32.90 34.42 40.16 63.61 30.65 31.37 37.87\nImp. 0.39\n0.25\n0.43\n0.07\n1.49\n0.91\n0.94\n0.94\nresulting in the development of diverse applications across multiple domains [6, 29, 50]. Using self-supervised learning on large\n\ndatasets, these models excel in text representation and, with transfer techniques such as fine-tuning and prompt tuning, they hold the potential to enhance recommendation systems, gaining notable attention in the RS domain. According to the categorization proposed by Lin et al. [22], the application of LLMs in recommendation systems can be segmented into five categories based on their position in the pipeline: User data collection, Feature engineering (e.g., [3]), Feature encoder (e.g., [52]), Scoring/Ranking function (e.g., [19, 23]), and Pipeline controller (e.g., [42]); alternatively, they can also be grouped into four types, considering two dimensions: (1) whether they are a tune LLM and (2) whether they infer in conjunction with conventional recommendation models (CRMs). In our study, we employed LLMs for dataset enhancement (feature engineering) and encoding content features, which were subsequently integrated into CRMs. To the best of our knowledge, we are the first to combine the openand closed-source LLMs in recommendation.\n\n# 6.2 Content-based Recommendation\n\nContent-based recommendations encompass a diverse range of domains, including but not limited to music [4, 35, 41], news [12, 24], and videos [8, 9, 18]. In this paper, our primary focus is on the news and book recommendation. To better capture textual knowledge and user preferences in news recommendation, in the past few years, several models based on deep neural networks have been proposed [1, 43\u2013 45]. Despite their effectiveness, these end-to-end models have limited semantic comprehension abilities. In recent years, there has been a surge of interest in using pretrained language models (PLMs) such as BERT [10] and GPT [30] in news recommendation systems [26, 46, 48, 51], owing to the powerful transformer-based architectures and the availability of large-scale pretraining data. The emergence of LLMs has further offered potential to enhance recommender systems using its rich general knowledge. In the latest developments, LLMs have been applied to personalization [31] and product recommendation [19]. Nevertheless, [25] points out that directly employing LLMs as a recommender system has shown negative results, indicating that the use of LLMs for news recommendation remains understudied.\n\n# 7 CONCLUSION\n\nOur work addresses the limitations of content-based recommendation systems and offers a new approach that leverages both openand closed-source LLMs to enhance their performance. Our findings indicate that combining the finetuning on the open-source LLMs and the prompting on the closed-source LLMs into recommendation systems can lead to substantial improvements, which has important implications for online content platforms. Our ONCE framework can be applied to other content-based domains beyond news and book recommendation. We hope our work will encourage further research and contribute to the development of more effective recommendation systems based on large language models.\n\n# REFERENCES\n\n1] Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu, and Xing Xie. 2019. Neural News Recommendation with Long- and Short-term User Representations. In Proceedings of the 57th Annual Meeting of the Association for\n\n] Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu, and Xing Xie. 2019. Neural News Recommendation with Long- and Short-term User Representations. In Proceedings of the 57th Annual Meeting of the Association for\n\nComputational Linguistics. Association for Computational Linguistics, Florence, Italy, 336\u2013345.\n[2]  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).\n[3] Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. 2022. Language Models are Realistic Tabular Data Generators. In The Eleventh International Conference on Learning Representations.\n[4] Jiajun Bu, Shulong Tan, Chun Chen, Can Wang, Hao Wu, Lijun Zhang, and Xiaofei He. 2010. Music Recommendation by Unified Hypergraph: Combining Social Media Information and Music Content (MM \u201910). Association for Computing Machinery, New York, NY, USA, 391\u2013400. https://doi.org/10.1145/1873951.1874005\n[5] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data. 1\u20134.\n[6] Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, and Xiang Li. 2023. AugGPT: Leveraging ChatGPT for Text Data Augmentation. arXiv:2302.13007 [cs.CL]\n[7]  Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. arXiv:2305.02182 [cs.IR]\n[8] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https: //doi.org/10.1145/1864708.1864770\n[9] Yashar Deldjoo, Mehdi Elahi, Paolo Cremonesi, Franca Garzotto, Pietro Piazzolla, and Massimo Quadrana. 2016. Content-based video recommendation system based on stylistic visual features. Journal on Data Semantics 5 (2016), 99\u2013113.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT.\n[11] Tom Fawcett. 2006. An introduction to ROC analysis. Pattern recognition letters 27, 8 (2006), 861\u2013874.\n[12] Florent Garcin, Christos Dimitrakakis, and Boi Faltings. 2013. Personalized News Recommendation with Context Trees. In Proceedings of the 7th ACM Conference on Recommender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing Machinery, New York, NY, USA, 105\u2013112. https: //doi.org/10.1145/2507157.2507166\n[13] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web. 507\u2013517.\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\n[15] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422\u2013446.\n[16]  Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. International Conference on Learning Representations (2015).\n[17]  Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30\u201337.\n[18] Joonseok Lee and Sami Abu-El-Haija. 2017. Large-Scale Content-Only Video Recommendation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops.\n[19] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation. arXiv:2304.03879 [cs.IR]\n[20] Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin Jiang, and Qun Liu. 2022. MINER: Multi-Interest Matching Network for News Recommendation. In Findings of the Association for Computational Linguistics: ACL 2022. 343\u2013352.\n[21] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023).\n[22] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv:2306.05817 [cs.IR]\n[23] Guang Liu, Jie Yang, and Ledell Wu. 2022. PTab: Using the Pre-trained Language Model for Modeling Tabular Data. arXiv:2209.08060 [cs.LG]\n[24] Jiahui Liu, Peter Dolan, and Elin R\u00f8nby Pedersen. 2010. Personalized News Recommendation Based on Click Behavior. In Proceedings of the 15th International\n\nConference on Intelligent User Interfaces (Hong Kong, China) (IUI \u201910). Association for Computing Machinery, New York, NY, USA, 31\u201340. https://doi.org/10.1145/ 1719970.1719976\n[25] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv preprint arXiv:2304.10149 (2023).\n[26] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiaoming Wu. 2022. Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation. In Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 2823\u20132833. https://aclanthology.org/2022.coling-1.249\n[27] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532\u20131543.\n[28] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th international conference on data mining (ICDM). IEEE, 1149\u20131154.\n[29] Basit Qureshi. 2023. Exploring the Use of ChatGPT as a Tool for Learning and Assessment in Undergraduate Computer Science Curriculum: Opportunities and Challenges. arXiv:2304.11214 [cs.CY]\n[30] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).\n[31]  Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023. LaMP: When Large Language Models Meet Personalization. arXiv:2304.11406 [cs.CL]\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\n[33]  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[34] Solomon Ubani, Suleyman Olcay Polat, and Rodney Nielsen. 2023. ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT. arXiv:2304.14334 [cs.AI]\n[35] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In  Advances in Neural Information Processing Systems, C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (Eds.), Vol. 26. Curran Associates, Inc. https://proceedings.neurips. cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[37] Ellen M Voorhees et al. 1999. The trec-8 question answering track report.. In Trec, Vol. 99. 77\u201382.\n[38] Mengting Wan and Julian McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM conference on recommender systems. 86\u201394.\n[39] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153 (2023).\n[40] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD\u201917 (Halifax, NS, Canada) (ADKDD\u201917). Association for Computing Machinery, New York, NY, USA, Article 12, 7 pages.\n[41]  Xinxi Wang and Ye Wang. 2014. Improving Content-Based and Hybrid Music Recommendation Using Deep Learning. In Proceedings of the 22nd ACM International Conference on Multimedia (Orlando, Florida, USA) (MM \u201914). Association for Computing Machinery, New York, NY, USA, 627\u2013636. https: //doi.org/10.1145/2647868.2654940\n[42] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. Trans. Mach. Learn. Res. (2022).\n[43] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, et al. 2019. Neural news recommendation with attentive multi-view learning. In International Joint Conferences on Artificial Intelligence.\n[44] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. NPA: Neural News Recommendation with Personalized Attention. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). Association for Computing Machinery, New York, NY, USA, 2576\u20132584. https://doi.org/10. 1145/3292500.3330665\n[45] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019. Neural news recommendation with multi-head self-attention. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 6389\u20136394.\n\n[46] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1652\u20131656.\n[47]  Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2021. Fastformer: Additive attention can be all you need. arXiv preprint arXiv:2108.09084 (2021).\n[48] Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, and Qi Liu. 2021. NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application. In Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics, Punta Cana, Dominican Republic, 3285\u20133295. https://doi.org/10.18653/v1/2021.findings-emnlp.280\n[49] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 3597\u20133606.\n[50] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. BloombergGPT: A Large Language Model for Finance. arXiv:2303.17564 [cs.LG]\n[51] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, et al. 2021. UNBERT: User-News Matching BERT for News Recommendation. In International Joint Conferences on Artificial Intelligence.\n[52] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang, and Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recommendation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, Zhi-Hua Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization, 3356\u20133362. https://doi.org/10. 24963/ijcai.2021/462 Main Track.\n[53] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1059\u20131068.\n\n# A PROMPTS FOR CLOSED-SOURCE LLMS\n\nHere, we demonstrate prompts of one-pass content summarizer (Figure 6), user profiler (Figure 8), personalized content generator (Figure 10), and chain-based personalized content generator (Figure 11) introduced in section 4. Blue, green, and brown texts represent system role, prompt, and one-time reply, respectively. All prompts for two datasets are available at https://github.com/Jyonn/ONCE.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ccb/4ccbd9fc-345d-4502-8a77-9f35aa48a500.png\" style=\"width: 50%;\"></div>\nFigure 6: Prompt and example for content summarizer on the MIND dataset.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/27ea/27eaa60c-3b51-4d45-b319-2f766a473524.png\" style=\"width: 50%;\"></div>\nFigure 7: Influence of news features. The MIND dataset employs the original title, image, and category as inputs. The MIND-NS dataset uses the enhanced title, image, and category as inputs. The asterisk (*) represents using additional abstract and subcategory information as inputs.\n\n<div style=\"text-align: center;\">Figure 7: Influence of news features. The MIND dataset employs the original title, image, and category as inputs. The MIND-NS dataset uses the enhanced title, image, and category as inputs. The asterisk (*) represents using additional abstract and subcategory information as inputs.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a05/5a05e5a6-a3ac-4364-b4aa-67ad23d51950.png\" style=\"width: 50%;\"></div>\nFigure 8: Prompt and example for user profiler on the MIND dataset.\n\n# B MORE EXPERIMENTS FOR CLOSED-SOURCE LLMS\n\nSince the experiments on open-source LLM are extensively conducted. Here, we present additional experiments for prompt-based closed-source LLMs (i.e., OpenAI GPT-3.5).\n\n# B.1 More base models\n\nWe evaluate the effectiveness of GPT-generated data with popular content-based recommendation models, including four matchingbased models, namely NAML [43], LSTUR [1], NRMS [45], and PLMNR [46], and four ranking-based deep CTR models, namely BST [5], DCN [40], PNN [28], and DIN [53].\n\n# Performance Comparison\n\nTable 6 presents the performance comparison for 1) one-pass content summarizer (CS), 2) one-pass user profiler (UP), and 3) onepass personalized content generator (CG), 4) chain-based content generator (UP \u2192 CG) describe in the previous section, and 5) the combination of CS and UP \u2192 CG (ALL). The results in Table 6 show that: Firstly, the combination of the three generative schemes (i.e.,\n\nTable 6: Performance comparison on the MIND dataset, among the one-pass content summarizer (CS), one-pass user profiler (UP), one-pass personalized content generator (CG), chain-based personalized content generator (UP \u2192 CG), and ALL that combines the content title generated by the one-pass content summarizer and synthetic content generated by the chain-based personalized content generator. ORI: training with the original data.\n\nMatching\nNAML\nLSTUR\nNRMS\nPLMNR\nAUC MRR N@5 N@10 AUC MRR N@5 N@10 AUC MRR N@5 N@10 AUC MRR N@5 N@10\nORI\n61.75\n30.60\n31.35\n37.85\n61.27\n29.64\n30.28\n36.76\n61.71\n30.20\n30.98\n37.42\n62.53\n30.74\n31.31\n38.03\nCS\n63.73\n31.83\n32.94\n39.24\n62.16\n30.52\n31.27\n37.85\n63.85 31.57\n32.35\n38.80\n64.80 33.08 34.25\n40.35\nUP\n62.19\n30.90\n31.78\n38.26\n61.81\n30.39\n31.00\n37.46\n61.90\n30.60\n31.54\n37.66\n63.31\n31.58\n32.65\n38.87\nCG\n62.93\n30.83\n32.10\n38.34\n63.88\n31.76\n32.92\n39.16\n63.04\n31.00\n31.84\n38.22\n63.11\n30.90\n32.02\n38.37\nUP\u2192CG\n63.61\n31.58\n32.63\n39.07\n63.57\n31.43\n32.62\n39.01\n62.95\n32.00\n32.80\n39.00\n64.02\n31.98\n33.25\n39.40\nALL\n63.88 32.17 33.14\n39.37\n64.04 32.40 33.30\n39.47\n63.71 32.14 33.11\n39.43\n65.13 32.98 34.30\n40.49\nRanking\nBST\nDCN\nPNN\nDIN\nAUC MRR N@5 N@10 AUC MRR N@5 N@10 AUC MRR N@5 N@10 AUC MRR N@5 N@10\nORI\n61.73\n29.84\n30.55\n37.22\n62.63\n29.73\n30.52\n37.12\n61.75\n29.45\n29.99\n36.67\n60.95\n28.13\n28.77\n35.42\nCS\n62.85\n31.51\n32.16\n38.78\n64.19\n31.96\n32.67\n39.16\n63.85\n31.54\n32.38\n38.78\n61.26\n29.72\n30.38\n36.76\nUP\n62.67\n30.75\n31.63\n38.01\n63.47\n29.92\n30.66\n37.47\n62.34\n29.67\n30.46\n37.07\n62.65\n30.74\n31.50\n38.05\nCG\n62.86\n30.54\n31.32\n37.93\n62.67\n29.81\n30.63\n37.18\n62.24\n29.34\n30.05\n36.73\n62.18\n29.33\n29.88\n36.79\nUP\u2192CG\n63.28\n31.49\n32.45\n38.84\n63.05\n29.79\n30.61\n37.23\n63.63\n30.85\n31.14\n38.69\n63.53\n30.76\n31.21\n38.13\nALL\n63.94 32.05 33.09\n39.41\n65.77 32.86 34.10\n40.48\n65.49 32.78 33.81\n40.19\n63.80 31.68 32.57\n39.08\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8f3/c8f311e6-9278-48f5-b1c8-09d602026206.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">#generated news articles per new user\n</div>\n<div style=\"text-align: center;\">Figure 9: Influence of the number of generated news articles on the AUC metric over four base models.\n</div>\nFigure 9: Influence of the number of generated news articles on the AUC metric over four base models.\n\n\u201cALL\u201d) achieve the best performance for all recommendation models in most cases, significantly outperforming training with the original data (ORI). Secondly, chain-based personalized content generator performers better than one-pass variants, which indicates the effectiveness of such chain-of-thought prompt.\n\nTable 7: Comparison of the cost and cost conversion rate (CCR) of different generative schemes. Imp.: the average improvement in AUC compared with the original dataset. CCR: the ratio of improvement to cost. Note that the cost of UP \u2192 NG is calculated by 120 \u00d7 0. 21 + 60, where 120 is the cost of UP, 0. 21 is the new user ratio, and 60 is the cost of chain-based NG.\n\nMatching\nRanking\nCost (USD) Imp. CCR (%) Imp. CCR (%)\nNS\n60\n1.82\n3.03\n1.27\n2.11\nUP\n120\n0.49\n0.41\n1.02\n0.85\nNG\n40\n1.42\n3.55\n0.72\n1.80\nNG\n40\n1.47\n3.68\n0.95\n2.38\nUP\u2192NG\n85\n2.43\n2.86\n1.57\n1.85\n# B.3 Content Summarizer\n\nThe text feature in the Goodreads dataset is only the book title, while in the MIND dataset, there are news title, abstract, and category. In the above experiments, we only utilize (enhanced) news title and category as inputs to the content encoder. Here, we assess the impact of combining more news features. From Figure 7, the\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a22/2a22e188-0461-4719-a9d6-b2d5b215ae0b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Prompt and example for personalized content generator on the MIND dataset (different two replies).\n</div>\nfollowing can be summarized. Firstly, the inclusion of additional news features such as abstract and subcategory does lead to an improved model performance, although they are usually excluded from existing models out of efficiency concerns. Secondly, while MIND* has included all available news features, MIND-NS* still outperform MIND*, indicating the effectiveness of the news titles generated by GPT-3.5.\n\n# B.4 Personalized Content Generator\n\nHere, we study how the number of generated content affects the recommendation performance. As depicted in Figure 9 conducted on the MIND dataset, we evaluate the effectiveness of utilizing 0, 1, and 2 generated news articles per new user for four base models. It can be seen that for each model, the performance improves as the number of generated content increases.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b600/b6004f08-3ab5-4dee-9885-056234e43ef3.png\" style=\"width: 50%;\"></div>\nFigure 11: Prompt and example for chain-based personalized content generator on the MIND dataset (different two replies).\n\n# Figure 11: Prompt and example for chain-based personalized content generator on the MIND dataset (different two replies).\n\nB.5 Cost Conversion Rate\n\n# Cost Conversion Rate\n\nFinally, we investigate the cost and cost conversion rate (CCR) of different generative schemes under our ONCE framework, as presented in Table 7. We compute the average improvement in AUC compared with the original dataset for both matching and ranking models based on the results from Table 6, as well as the cost conversion rate (ratio of improvement in AUC to cost of employing the\n\nGPT-3.5 API). Based on the results, we can conclude the following. Firstly, with the full dataset, the personalized content generator (CG) has the best CCR for matching-based models, and the content summarizer (CS) has the best CCR for ranking-based models.  Secondly, the user profiler (UP) has the worst CCR, since the extensive\n\nlength of a user\u2019s browsing history results in a high token count per request, leading to increased cost for the user profiler. Thirdly, chain-based generation achieves a higher improvement compared to one-pass generation, but its CCR decreases due to the use of the expensive user profiler.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Personalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However, existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tasks. This study explores the potential of leveraging both open- and closed-source LLMs to enhance content-based recommendation.",
        "problem": {
            "definition": "Existing content-based recommendation systems struggle to fully comprehend the content of items, leading to suboptimal recommendations.",
            "key obstacle": "The main difficulty lies in the limitations of previous content encoders, which fail to recognize crucial semantic features and struggle with content-level understanding."
        },
        "idea": {
            "intuition": "The idea is inspired by the advanced capabilities of large language models, which can provide deeper semantic understanding and richer representations of content.",
            "opinion": "The proposed idea involves using both open-source and closed-source LLMs to create a more effective recommendation system by enhancing content representation and user modeling.",
            "innovation": "The key innovation is the ONCE framework, which integrates open-source LLMs as content encoders and closed-source LLMs through prompting to improve recommendation performance."
        },
        "method": {
            "method name": "ONCE",
            "method abbreviation": "ONCE",
            "method definition": "The ONCE framework combines open-source and closed-source large language models to enhance content-based recommendation systems.",
            "method description": "ONCE utilizes open-source LLMs for content encoding and closed-source LLMs for data enrichment through prompting.",
            "method steps": "1. Use closed-source LLMs for data generation through prompting; 2. Enhance the dataset; 3. Fine-tune open-source LLMs for content encoding; 4. Implement the recommendation model.",
            "principle": "This method is effective due to the synergistic relationship between open-source and closed-source LLMs, allowing for improved content representation and user profiling."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using two benchmarks: MIND and Goodreads datasets, comparing the ONCE method against existing state-of-the-art recommendation models.",
            "evaluation method": "Performance was assessed through metrics such as AUC, MRR, and nDCG, with results averaged over five independent runs."
        },
        "conclusion": "The ONCE framework significantly enhances the performance of content-based recommendation systems by effectively leveraging both types of LLMs, achieving improvements of up to 19.32% over existing models.",
        "discussion": {
            "advantage": "The main advantages include improved semantic understanding and enhanced recommendation quality through the combined use of open- and closed-source LLMs.",
            "limitation": "One limitation is the computational cost associated with using large language models, which can be resource-intensive.",
            "future work": "Future research could focus on optimizing the efficiency of the ONCE framework and exploring its application in other content-based domains."
        },
        "other info": {
            "code availability": "The code and LLM-generated data will be made available for reproducibility.",
            "datasets used": {
                "dataset1": "MIND",
                "dataset2": "Goodreads"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Personalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services."
        },
        {
            "section number": "1.2",
            "key information": "Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tasks."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea involves using both open-source and closed-source LLMs to create a more effective recommendation system by enhancing content representation and user modeling."
        },
        {
            "section number": "2.1",
            "key information": "Existing content-based recommendation systems struggle to fully comprehend the content of items, leading to suboptimal recommendations."
        },
        {
            "section number": "2.3",
            "key information": "The ONCE framework combines open-source and closed-source large language models to enhance content-based recommendation systems."
        },
        {
            "section number": "3.2",
            "key information": "The key innovation is the ONCE framework, which integrates open-source LLMs as content encoders and closed-source LLMs through prompting to improve recommendation performance."
        },
        {
            "section number": "4.2",
            "key information": "ONCE utilizes open-source LLMs for content encoding and closed-source LLMs for data enrichment through prompting."
        },
        {
            "section number": "10.1",
            "key information": "One limitation is the computational cost associated with using large language models, which can be resource-intensive."
        },
        {
            "section number": "11",
            "key information": "The ONCE framework significantly enhances the performance of content-based recommendation systems by effectively leveraging both types of LLMs, achieving improvements of up to 19.32% over existing models."
        }
    ],
    "similarity_score": 0.7931832769025573,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfb0/dfb022d7-8ea3-4865-8d8b-5226641686f7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7808/7808e564-cde4-40bc-8521-b5990d43299c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2317/2317c662-ff27-45c7-8b19-601a26271611.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/07f6/07f68f93-6cc6-4085-be0f-84ea8b73ee4e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6098/6098caae-7798-4423-878a-88629b9be421.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2fd7/2fd73f25-f202-4763-b9a6-21c7d3df01ee.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ccb/4ccbd9fc-345d-4502-8a77-9f35aa48a500.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/27ea/27eaa60c-3b51-4d45-b319-2f766a473524.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a05/5a05e5a6-a3ac-4364-b4aa-67ad23d51950.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8f3/c8f311e6-9278-48f5-b1c8-09d602026206.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a22/2a22e188-0461-4719-a9d6-b2d5b215ae0b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b600/b6004f08-3ab5-4dee-9885-056234e43ef3.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Once_ Boosting content-based recommendation with both open-and closed-source large language models.json"
}