{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.07811",
    "title": "In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",
    "abstract": "In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax\u2014a requirement for robust language understanding. We further investigate whether out-ofdistribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.",
    "bib_name": "mueller2024incontextlearninggeneralizesrobustly",
    "md_text": "# In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax\nAaron Mueller1,2* Albert Webson3 Jackson Petty4\u2020 Tal Linzen5 1Northeastern University 2Technion \u2013 Israel Institute of Technology 3Google DeepMind 4New York University 5Google Research\n# Abstract\nIn-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax\u2014a requirement for robust language understanding. We further investigate whether out-ofdistribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.\n# 1 Introduction\nLanguage models (LMs) have become increasingly important subjects of study due to their expressive power and performance at scale. When training large language models (LLMs) on massive amounts of text, surprisingly sophisticated linguistic behaviors, such as in-context learning (ICL), emerge (Brown et al., 2020; Min et al., 2022a): given only a small number of labeled training examples in the input context, LLMs can generalize to new instances of the task without weight updates. Thus, even without access to the model\u2019s weights, we\n* Parts of this work completed when A.M. was a longterm visitor at New York University. \u2020 Work completed while J.P. was a student researcher at Google Research.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59c8/59c8c9ef-792e-40b6-be11-f23cedf74a5a.png\" style=\"width: 50%;\"></div>\nFigure 1: The syntactic transformations paradigm. We prompt language models with labeled examples px, yqtrain that can be explained using either robust syntactic/hierarchical features or spurious positional/linear features. We also include the input from a test example xtest. We ensure the models have learned the task by evaluating on in-distribution examples px, yqtest-id. Then, we observe whether models generalize syntactically or linearly on out-of-distribution examples px, yqtest-ood.\ncan teach an LLM to perform new tasks with significantly higher-than-chance performance. This raises questions as to whether context is sufficient for LLMs to learn the underlying structure of a task, as opposed to superficial heuristics that do not generalize well. Indeed, LLMs have demonstrated counterintuitive biases in ICL settings (Pan et al., 2023; Min et al., 2022b; Webson and Pavlick, 2022), giving reason for skepticism. In this study, we ask: How robust is ICL to distribution shifts between in-context exemplars and test examples? We investigate these questions using the test case of syntactic generalization. Accurate syntactic representations are necessary for robust language understanding. In LMs, syntax acquisition is causally associated with significant and abrupt drops in loss and improved performance on NLP tasks (Chen et al., 2024). Language is structured hierarchically, but the structure of sentences is not provided to models as part of their input. Therefore, a model could incorrectly assume that sentences have lin-\near rather than hierarchical structure\u2014e.g., that syntactic dependencies consist of adjacent words\u2014 and thus give an incorrect or unrelated answer to questions such as the following:\n# (1) Is it true that children who don\u2019t like sweets are rare?\nThe main subject is \u201cchildren\u201d, but a model that assumes sentences are structured linearly could instead analyze \u201csweets\u201d as the subject, as it is the positionally closest noun to the main verb. In downstream tasks, such assumptions about sentence structure would lead to incorrect responses to inputs that humans find simple (McCoy et al., 2019). Studies using syntactic transformations tasks (Frank and Mathis, 2007) have found that pre-training on large corpora imparts syntactic inductive biases to LMs (Mueller et al., 2022), and the strength of syntactic preferences increases with model depth (Mueller and Linzen, 2023). These studies relied on fine-tuning, where the LM\u2019s weights are updated based on examples of the task; we extend these analyses to the ICL setup. We investigate LLMs\u2019 generalization in syntactic transformations and natural language inference tasks. In each task, we evaluate in two settings: one where models are provided only with a handful of instances of the task, and another in which they are additionally provided with chain-of-thought (CoT) traces (Wei et al., 2022b). CoT refers to the finding that LMs\u2019 task performance can be improved if they are instructed to predict the intermediate computations required for the task. We examine whether giving LMs access to syntactic reasoning traces and meta-linguistic information increases their reliance on robust features. We evaluate not only the accuracy of the final answer, but also the correctness of the model\u2019s reasoning and the faithfulness of the model\u2019s final answer to its own reasoning. We find that, while all models perform well on in-distribution examples, even very large LMs are prone to relying on surface heuristics. Further, chain-of-thought results can be misleading: CoT improves in-distribution performance, but often decreases out-of-distribution performance. This underscores the importance of out-of-distribution evaluation. Finally, we present evidence that exposure to code during pre-training assists models in overcoming these limitations.1\n1Our code and data are available at https://github. com/aaronmueller/syntax-icl.\n# 2 Experimental Setup\nWe first examine syntactic transformation tasks, where the training and test instances are drawn from distributions that are distinct in controlled ways (\u00a72.1). We test the syntactic generalization of multiple families of LMs (\u00a72.2) when prompted using in-context learning (\u00a72.3).\n# 2.1 Syntactic Transformations\nQuestion formation. Here, a model is given a declarative sentence and must transform it into a yes/no question by moving the main auxiliary verb to the start of the sentence. For instance, given this training example:\n# (2) The yaks near my salamanders have amused your unicorn.\n(2) The yaks near my salamanders have amused your unicorn.\nThe model should move the main auxiliary verb \u201chave\u201d to the start of the sentence to form the question, \u201cHave the yaks near my salamanders amused your unicorn?\u201d. The model should rely on hierarchical syntactic information s: it should detect the main auxiliary in the sentence and move it to the front (the MOVE-MAIN hypothesis). However, the model could instead learn the positional heuristic p and move the linearly first auxiliary in the sentence (the MOVE-FIRST hypothesis). Both s and p produce correct outputs for the training examples, but only s generalizes correctly to out-of-distribution inputs where, crucially, the main auxiliary is not linearly first in the sentence:\nMy zebras that have admired the newt haven\u2019t observed the peacocks. a. \u00ebMOVE-MAIN: Haven\u2019t my zebras that have admired the newt observed the peacocks? b. \u00ebMOVE-FIRST: *Have my zebras that admired the newt haven\u2019t observed the peacocks?\nTense reinflection. Here, the task is to convert past-tense verbs into present-tense verbs. We want the model to detect the subject of each verb and reinflect it based on its subject\u2019s number s (the AGREE-SUBJECT hypothesis). Here, the training distribution consists of examples where all nouns have the same grammatical number:\n(4) Her newt around your unicorn confused some quail.\nModel\nParams (est.)\nPre-train Tokens (est.)\n% Code (est.)\nAdditional Training\nGPT-3 text-davinci-001\n175B\n400B\n?\nFine-tuned on human demonstrations and highly\nrated model outputs\nGPT-3.5 code-davinci-002\n?\n500B\n20%\n\u2013\nGPT-3.5 (Turbo) gpt-3.5-turbo-0301\n?\n500B+\n?\n?\nGPT-3.5 text-davinci-002\n?\n500B+\n?\nFine-tuned on human demonstrations and highly\nrated model outputs\nGPT-3.5 text-davinci-003\n?\n500B+\n?\nReinforcement learning on human feedback\nGPT-4 gpt-4-0314\n?\n?\n?\n?\nPaLM\n540B\n780B\n5%\n\u2013\nFlan-PaLM\n540B\n782B\n5%\nFine-tuned on human demonstrations\nLlama 2\n70B\n2T\n4%\n\u2013\nCodeLlama\n34B\n2.5T\n20%\nFine-tuned on human and model-generated\ndemonstrations\nTable 1: Models used in this study, their estimated number of parameters and pre-training tokens, and the proportion of the pre-training corpus estimated to be source code. We use the largest sizes of each PaLM and (Code)Llama model, as larger models are typically better able to leverage in-context guidance (Wei et al., 2022a).\nBecause all nouns have the same number, a model could learn to correctly convert \u201cconfused\u201d to \u201cconfuses\u201d by simply agreeing the verbs with the closest noun p (the AGREE-RECENT hypothesis). To evaluate which hypothesis the model has learned, we evaluate on examples where only AGREE-SUBJECT produces correct outputs:\n(5) The yak upon my ravens entertained her zebras. a. \u00ebAGREE-SUBJECT: The yak upon my ravens entertains her zebras. b. \u00ebAGREE-RECENT: *The yak upon my ravens entertain her zebras.\nTask formulation. Each syntactic transformation example (x, y) consists of input sentence x and output sentence y, where x and y are semantically and lexically nearly identical but differ in the syntactic arrangement of the words (as described above). There is a training set Strain and two test sets: an in-distribution (ID) test set Stest-id used to determine whether the model has learned to perform the task, and one out-of-distribution (OOD) test set Stest-ood used to determine whether the model generalizes in a manner consistent with the latent hierarchical structure of language. The distributions of Strain and Stest-ood differ in controlled ways: the training examples Strain could be correctly transformed using either syntactic feature s or positional feature p, whereas the test examples require reliance on s for correct answers. In other words:\nprompt models with up to 8 exemplars tpx1, y1qtrain, . . . , px8, y8qtrainu, followed by the input of an example from one of the test sets xtest. A model that has learned to rely only on the spurious feature p will obtain 0% accuracy on Stest-ood, but 100% on Strain and Stest-id; only reliance on the syntactic feature s will yield 100% accuracy on Stest-ood.\n# 2.2 Models\nWe use a series of Transformer-based (Vaswani et al., 2017) decoder-only autoregressive language models that are known to support in-context learning. Estimates of model sizes and pre-training corpus sizes are presented in Table 1. Estimates are derived from OpenAI2 and prior work, including Kim and Schuster (2023) and Ye et al. (2023). As OpenAI does not provide official parameter counts or training set descriptions, the true numbers for GPT models may differ. The OpenAI GPT models we use include GPT-3 text-davinci-001 (Brown et al., 2020; Ouyang et al., 2022), GPT-3.5,3 and GPT-4.4 The GPT-3.5 variants we use include code-davinci-002 (Chen et al., 2021), which is pre-trained on natural language and a large amount of source code; text-davinci-002, a code-davinci-002 model which is fine-tuned on instructions and humanlabeled examples from many tasks (Wei et al., 2022a); and text-davinci-003, which is pre2The information OpenAI released about their models was found here: https://platform.openai.com/docs/ model-index-for-researchers. This information has been taken down, but is largely replicated in Ye et al. (2023). 3https://platform.openai.com/docs/models/ gpt-3-5 4https://platform.openai.com/docs/models/ gpt-4\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d52b/d52b26d7-6194-4d83-8945-53bf00316d6f.png\" style=\"width: 50%;\"></div>\ntrained similarly to text-davinci-002 and then trained via reinforcement learning on human feedback (RLHF; Ouyang et al., 2022). GPT-3.5 (Turbo) builds on text-davinci-003\u2019s methods, optimizing it for chat. Each of these models is estimated to be pre-trained on 400B or more tokens of text, as each is based on the 300B tokens of Brown et al. (2020) plus 100B for instruction tuning as described in Ouyang et al. (2022); models based on code-davinci-002 are additionally estimated to be pre-trained on at least 100B tokens of code (Chen et al., 2021). Few details of GPT-4\u2019s architecture or pre-training have been made public. PaLM models (Chowdhery et al., 2023) are pretrained on large amounts of English text and code; 5% of the pre-training corpus for PaLM is source code. The variants we test have 540B parameters. Flan-PaLM (Chung et al., 2022) is further trained using instruction-finetuning (Wei et al., 2022a). Finally, we evaluate Llama 2 models (Touvron et al., 2023), which are among the few opensource LMs capable of in-context learning. We use the 70B-parameter model,5 which is trained\n5For Llama 2 (70B), we use 4-bit quantization so that the model will fit on a single GPU.\non 2T tokens of mostly natural language and an unspecified amount of code. We also use CodeLlama (Rozi\u00e8re et al., 2023), which is Llama 2 trained on an additional 500B tokens of code. In preliminary experiments, we found that the 34Bparameter instruction-tuned variant of CodeLlama6 performed best, so we use that variant here. For OpenAI and PaLM models, we use greedy decoding and set the temperature to 0 to reduce variance. For Llama models, we use settings from Touvron et al. (2023): nucleus sampling with p \u201c 0.9 and temperature 0.1.7\n# 2.3 Prompt Formats\nIn ICL, a set of labeled examples, concatenated into a \u201cprompt\u201d, is provided in the model\u2019s context. We use 8 labeled examples (henceforth, exemplars).8\n# In ICL, a set of labeled examples, concatenated into a \u201cprompt\u201d, is provided in the model\u2019s context. We use 8 labeled examples (henceforth, exemplars).8\n6codellama/CodeLlama-34b-Instruct-hf 7We use different decoding hyperparameters because replicability is more crucial for models that are more difficult and costly to access. For Llama models, we consider slightly higher variance to be more acceptable in exchange for higher expected performance, for replication and trying other settings are both more accessible. 8We attempted zero-shot evaluations to avoid confounds from the exemplar set. These did not score well: the models would typically drop the auxiliaries and change the past par-\nQuestion formation exemplars are drawn from the training portion of Mueller et al. (2022). Tense reinflection exemplars are drawn from the training portion of McCoy et al. (2020). We also experiment with two types of manually written CoT traces, which explicitly provide cues to the syntactic structure of the exemplars; these are provided between the input sequence and the answer. The Verbal CoT prompt includes verbal descriptions of the syntactic structure of the sentence, where the model must describe which words compose the main subject, the main verb phrase, and other components. The Code CoT prompt likewise describes which words correspond to which syntactic components, but in a Python code format where each component is represented as a variable, and then the model is explicitly instructed (in code) how to combine these components to form the final answer. The format of the inputs is shown in Figure 2. We search over prompt formats by tuning over training accuracy on 100 examples that were not part of the exemplar set. For the Code CoT prompt, the 8 exemplars exceed the maximum sequence length of GPT-3 and (Flan-)PaLM; in such cases, we use 4 Code CoT exemplars, which is the maximum that will fit while still allowing the model to generate the full reasoning trace and answer.9\n# 2.4 Evaluation\nWe evaluate on a uniform subsample of 100 examples from the question formation generalization set of Mueller et al. (2022) and 100 examples from the tense reinflection generalization set of McCoy et al. (2020). To verify whether models have learned the task, we also present scores on 100-example uniform subsamples from the in-distribution test sets, where each example can be correctly transformed using either syntactic or positional features. All examples are generated from a probabilistic context-free grammar; as such, the distribution of syntactic structures is highly constrained, and the variation across examples is primarily lexical. We therefore reason that 100 examples should provide a representative sample of the syntactic structures\nticiple into simple past form; for example, \u201cThe newts saw the yak.\u201d Or they would simply generate labels that we did not prompt for, such as \u201cTrue\u201d or \u201cNo\u201d. 9We observe in our results that (Flan-)PaLM ID and OOD performance are still high for Code CoT despite using only 4 exemplars, and that GPT-3 does not achieve high ID or OOD performance given any prompt (regardless of the number of exemplars).\nticiple into simple past form; for example, \u201cThe newts saw the yak.\u201d Or they would simply generate labels that we did not prompt for, such as \u201cTrue\u201d or \u201cNo\u201d. 9We observe in our results that (Flan-)PaLM ID and OOD performance are still high for Code CoT despite using only 4 exemplars, and that GPT-3 does not achieve high ID or OOD performance given any prompt (regardless of the number of exemplars).\nin the evaluation sets. To evaluate question formation examples, we use main auxiliary accuracy, which measures whether the model has moved the correct main auxiliary to the front of the sentence. This is measured by observing whether the first word is correct. For tense reinflection, we use verb accuracy, which measures whether each generated verb is correctly inflected and in the correct relative position.\n# 3 Results\nAll models except GPT-3 and GPT-3.5 (Turbo) learn to perform the transformations tasks, as indicated by high in-distribution accuracies. Among models that learn the task well, there is large variance with respect to out-of-distribution accuracies. Scale does not fully explain performance. GPT-3, GPT-3.5 text-davinci-002 and GPT-3.5 text-davinci-003 are estimated to be the same size, but the GPT-3.5 models generalize better. Llama 2 and CodeLlama outperform GPT-3 and (Flan-)PaLM despite being far smaller, and CodeLlama significantly outperforms Llama 2 on question formation despite being smaller. What explains differences between models, then? Pre-training on code improves OOD generalization. On average, scores are higher given any prompt for models pre-trained on code: CodeLlama significantly outperforms Llama 2 on question formation (and performs comparably on tense reinflection), while GPT-3.5 code-davinci-002 outperforms all other GPT-3 and GPT-3.5 models. This agrees with and extends the finding of Mueller and Linzen (2023) that the domain of the pre-training corpus significantly affects syntactic generalization. This also agrees with recent findings that code pre-training improves other fundamental linguistic abilities (e.g., entity tracking; Kim and Schuster, 2023). Conversely, RLHF may harm OOD generalization. GPT-3.5 text-davinci-003 performs at least as well as other GPT-3.5 models on in-distribution examples, but it generalizes consistently worse than other models which are fine-tuned on human demonstrations (including text-davinci-002 and CodeLlama). Thus, RLHF is effective when testing on in-distribution test sets, but it may actively harm a model\u2019s ability to reason beyond its given exemplars. This finding is preliminary, however, and should be confirmed in future work via ablations over RLHF in a variety\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b667/b667200d-08a9-414a-b356-adaa5c3e549d.png\" style=\"width: 50%;\"></div>\nFigure 3: Main auxiliary accuracy on question formation and verb accuracy on tense reinflection. In-distribution accuracies reveal whether the models have learned the task, and out-of-distribution accuracies reveal whether models generalize robustly. Unfilled shapes (GPT-3, Llama 2) were trained on less than 5% code. We interpret the dashed line as a ceiling on OOD accuracy given ID accuracy.\n<div style=\"text-align: center;\">Figure 3: Main auxiliary accuracy on question formation and verb accuracy on tense reinflection. In-distribution accuracies reveal whether the models have learned the task, and out-of-distribution accuracies reveal whether models generalize robustly. Unfilled shapes (GPT-3, Llama 2) were trained on less than 5% code. We interpret the dashed line as a ceiling on OOD accuracy given ID accuracy.</div>\nof architectures and model sizes. Chain-of-thought prompting has different impacts on in-distribution and out-of-distribution accuracies. For question formation, CoT prompting tends to increase ID accuracy while not improving OOD generalization; this is especially apparent for verbal CoT\u2014and, for models trained on code, the code CoT prompt as well. This reveals the importance of testing OOD generalization when designing CoT prompts: when chain-of-thought prompting improves performance on ID examples, improvements will not necessarily generalize to OOD examples. For tense reinflection, verbal CoT equally harms both ID and OOD accuracy relative to No CoT prompts; conversely, Code CoT maintains ID accuracy while harming OOD accuracy. This generalizes the above point: depending on the task and prompt format, CoT can have different impacts on ID and OOD performance. We qualitatively analyze model errors to investigate why models achieve imperfect OOD generalization (App. B). We find that code-davinci-002 is sensitive to variable names, and that most errors are because models move affirmative verbs instead of syntactically correct verbs.\n# 4 Syntactic Generalization in Text Classification: The Case of Natural Language Inference\nSo far, our analyses indicate that LLMs leverage syntactic information to varying extents when taught a task via ICL, and that code pre-training im-\nproves OOD generalization. In this section, we test whether these trends extend to classification tasks. We leverage the Heuristic Analysis for NLI Systems (HANS; McCoy et al., 2019) dataset, which contains natural language inference (NLI) examples designed to diagnose reliance on syntactic heuristics. One typically trains the model on the training set of another NLI dataset, such as MNLI (Williams et al., 2018) or RTE (Dagan et al., 2006), and then uses HANS as an evaluation set to measure whether models rely on syntactic heuristics. Si et al. (2023b) evaluate RoBERTa and GPT-3 on HANS after training on MNLI; they find that both models perform similarly, suggesting that scale may not be enough to overcome syntactic heuristics. We extend this analysis to a wider variety of LLMs and to chain-of-thought prompting. HANS diagnoses reliance on lexical overlap, subsequence, or constituent heuristics. Models that systematically rely on any of these heuristics are expected to obtain 100% accuracy on examples where the label is \u201centailment\u201d because the heuristics happen to make the correct prediction; however, models that rely on these heuristics will also obtain 0% accuracy on examples where the label is \u201cnonentailment\u201d. We average scores across heuristics here; see App. D for full results. We evaluate LLMs on HANS in the ICL setting. The 8 ICL exemplars are drawn from MNLI, and the test examples are from HANS. Our results are based on a uniform subsample of 100 examples per heuristic and per label. We test across 3 syntactic\nEntailment\nNon-entailment\nModel\nNo CoT\nVerbal CoT\nNo CoT\nVerbal CoT\nGPT-3 text-davinci-001\n56\n99\n82\n2\nGPT-3.5 code-davinci-002\n92\n97\n81\n59\nGPT-3.5 text-davinci-002\n72\n97\n91\n57\nGPT-3.5 text-davinci-003\n97\n100\n70\n48\nGPT-3.5 gpt-3.5-turbo-0301\n82\n8\n39\n9\nGPT-4 gpt-4-0314\n97\n97\n64\n58\nPaLM\n98\n99\n60\n55\nFlan-PaLM\n100\n100\n39\n46\nLlama 2\n94\n98\n68\n51\nCodeLlama\n89\n68\n69\n79\nTable 2: Accuracies on HANS. Scores are split by label, and then by prompt format; we aggregate across syntactic heuristics (full table in App. D). High scores on entailment coupled with low scores on non-entailment signify that the model relies on the syntactic heuristic to predict the label. Chain-of-thought can increase reliance on heuristics: Compared to No CoT, Verbal CoT often demonstrates higher scores on entailment, but significantly lower scores on non-entailment.\nheuristics, and there are 2 labels (entailment and non-entailment); thus, we have 600 test examples in total. We use No CoT prompts and Verbal CoT prompts similar to those we depict in Figure 2; see App. D for examples of our prompt formats. Our results (Table 3) suggest that LLMs are susceptible to syntactic heuristics, but to a lesser extent than smaller-scale LMs fine-tuned on MNLI (McCoy et al., 2019). Most models achieve near 100% accuracy on examples where the gold label is \u201centailment\u201d, as expected. However, on \u201cnonentailment\u201d examples, models perform much more poorly. Unlike in the transformation tasks, code pre-training does not lend a significant advantage. We also observe that chain-of-thought can make models significantly more prone to relying on syntactic heuristics. With Verbal CoT, scores on entailment examples generally increase (except for GPT-3.5 (Turbo)). On non-entailment examples, however, scores reduce to near-randomchance\u2014and, for some models, to near-zero. This pattern suggests that models rely more extensively on the heuristic given Verbal CoT prompts. This reinforces the importance of detailed evaluation when tuning one\u2019s prompts: results from prompt tuning experiments can often be misleading if one only observes in-distribution accuracies, or overall (as opposed to label-specific) accuracies.\n# 5 Do LLMs Generalize Faithfully?\nLLMs have been found to generate answers which are not faithful to their own chain-of-thought reasoning (Lyu et al., 2023; Turpin et al., 2023). In\nthis section, we test the relationship between models\u2019 CoT reasoning and their answers. There are at least two possibilities that could lead to low accuracy: First, the models could produce incorrect CoT reasoning traces, but answer consistently with those traces. Second, they could produce correct reasoning traces, but ignore those traces when producing the output. Here, we evaluate the accuracy of models\u2019 reasoning, as well as the faithfulness of models\u2019 final answers to their reasoning traces.\n# 5.1 Method\nThe templatic format of the Code CoT prompt allows us to easily extract each component of the model\u2019s reasoning from the variable values. We observe whether the reasoning matches what the ground-truth values should be (reasoning accuracy) and whether the model\u2019s prediction aligns with the generated reasoning (faithfulness). For question formation, we measure how often the model extracts the main subject, the main verb phrase (VP), and the object, and how often its output follows its analysis of each component: (6) The salamanders near my yaks that haven\u2019t entertained your unicorn have amused the newt. \u00f1 Have the salamanders near my yaks that haven\u2019t entertained your unicorn amused the newt? For tense reinflection reasoning, we measure whether each noun is detected and whether each verb is correctly associated with its subject (subjectverb association). When evaluating faithfulness, we also measure whether the verbs in the output agree with the subjects they were associated with in the CoT reasoning (verb number). For example, given the following prompt and answer: (7) Your peacocks that admired the raven remembered your vulture. \u00f1 Your peacocks that admire the raven remember your vulture.10 We present a subset of our analysis, focusing on components that are essential for achieving the correct answer. See App. C for the full analysis and details on how we extract each component from a model\u2019s generated reasoning and answer.\n(6) The salamanders near my yaks that haven\u2019t entertained your unicorn have amused the newt. \u00f1 Have the salamanders near my yaks that haven\u2019t entertained your unicorn amused the newt?\nFor tense reinflection reasoning, we measure whether each noun is detected and whether each verb is correctly associated with its subject (subjectverb association). When evaluating faithfulness, we also measure whether the verbs in the output agree with the subjects they were associated with in the CoT reasoning (verb number). For example, given the following prompt and answer:\n# (7) Your peacocks that admired the raven remembered your vulture. \u00f1 Your peacocks that admire the raven remember your vulture.10\nWe present a subset of our analysis, focusing on components that are essential for achieving the correct answer. See App. C for the full analysis and details on how we extract each component from a model\u2019s generated reasoning and answer.\n10Alternating colors mean that a word is used in evaluati multiple components.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/02b7/02b7ed54-858d-47cd-bf72-38236185d7a5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Question formation</div>\nFigure 4: Reasoning accuracies and faithfulness scores for question formation (left) and tense reinflection (right) using the Code CoT prompt. Reasoning accuracies and faithfulness are highest for code-davinci-002, GPT-4, (Flan-)PaLM, and CodeLlama.\n# 5.2 Results\nReasoning accuracies and faithfulness (Figure 4) are highest for code-davinci-002, GPT-4, (Flan-)PaLM, and CodeLlama. These are the same models which had the highest OOD accuracies; indeed, OOD accuracies correlate strongly with reasoning accuracies (\u03c1Spearman = 0.71, p \u0103 .01) and faithfulness scores (\u03c1Spearman = 0.71, p \u0103 .01). This is unsurprising, but it still aids our understanding of why certain models generalize better. For question formation, models pre-trained on large amounts of code\u2014notably, GPT-3.5 code-davinci-002, text-davinci-002, GPT-4, Flan-(PaLM), and CodeLlama\u2014show high reasoning accuracies and faithfulness. GPT-3.5 text-davinci-003 reasons less accurately, perhaps due to RLHF. Trends are similar for tense reinflection, except that Llama 2 and CodeLlama perform more similarly to each other and PaLM/Flan-PaLM perform worse than GPT-3.5 models. These results raise two hypotheses: (1) code pre-training induces better OOD generalization because it induces better (more accurate and faithful) reasoning, whereas (2) RLHF induces worse generalization because it optimizes features that are orthogonal to linguistic reasoning. These hypotheses are discussed in \u00a76. We also observe that reasoning accuracy correlates strongly with faithfulness (\u03c1Spearman = 0.87, p \u0103 .01). Our hypothesis was that low performance could be explained by models either reasoning well but ignoring their reasoning, or reasoning poorly and answering accordingly. Instead, we find that a model\u2019s ability to follow its own reasoning is linked to how well it reasons. Future work should investigate to what extent accuracy, reasoning accuracy, and faithfulness are causally interdependent.\nReasoning accuracies and faithfulness (Figure 4) are highest for code-davinci-002, GPT-4, (Flan-)PaLM, and CodeLlama. These are the same models which had the highest OOD accuracies; indeed, OOD accuracies correlate strongly with reasoning accuracies (\u03c1Spearman = 0.71, p \u0103 .01) and faithfulness scores (\u03c1Spearman = 0.71, p \u0103 .01). This is unsurprising, but it still aids our understanding of why certain models generalize better. For question formation, models pre-trained on large amounts of code\u2014notably, GPT-3.5 code-davinci-002, text-davinci-002, GPT-4, Flan-(PaLM), and CodeLlama\u2014show high reasoning accuracies and faithfulness. GPT-3.5 text-davinci-003 reasons less accurately, perhaps due to RLHF. Trends are similar for tense reinflection, except that Llama 2 and CodeLlama perform more similarly to each other and PaLM/Flan-PaLM perform worse than GPT-3.5 models. These results raise two hypotheses: (1) code pre-training induces better OOD generalization because it induces better (more accurate and faithful) reasoning, whereas (2) RLHF induces worse generalization because it optimizes features that are orthogonal to linguistic reasoning. These hypotheses are discussed in \u00a76.\nlates strongly with faithfulness (\u03c1Spearman = 0.87, p \u0103 .01). Our hypothesis was that low performance could be explained by models either reasoning well but ignoring their reasoning, or reasoning poorly and answering accordingly. Instead, we find that a model\u2019s ability to follow its own reasoning is linked to how well it reasons. Future work should investigate to what extent accuracy, reasoning accuracy, and faithfulness are causally interdependent.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4b5a/4b5a4f56-3574-4c29-a36c-ea212dcf3b1b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Tense reinflection</div>\n# 6 Discussion\nLLMs guided via ICL often explain the structure of input examples and tasks using spurious positional and word-level features (\u00a73), as well as spurious syntactic heuristics (\u00a74). It is surprising that LLMs generalize in a manner not consistent with English grammar, given that language models are directly optimized over a large quantity of long contexts to produce probable sequences. Larger and deeper models generally behave in a manner more consistent with syntactic structure when fine-tuned (Mueller and Linzen, 2023), but this trend does not generalize to ICL. Thus, despite impressive performance on downstream tasks, our findings provide evidence that LLMs do not consistently leverage the latent structure of language when processing or generating language. LLMs will therefore struggle to generalize well outside of their exemplars\u2019 distribution. Chain-of-thought has significantly different impacts on ID vs. OOD examples. In transformations tasks, it sometimes improved ID performance while maintaining or decreasing OOD performance; on HANS, it significantly increased reliance on syntactic heuristics. This reveals an actionable takeaway: one should perform thorough evaluations when prompt tuning. Results from overall (rather than label-specific) accuracies and indistribution examples can be misleading. A caveat is that one could always tune these prompts further to obtain better performance, just as one could always tune hyperparameters to obtain better models. Code pre-training improves LLM generalization. Various studies observe that LMs trained on large amounts of code perform better on various NLP tasks and linguistic evaluations (Kim and Schuster, 2023; Madaan et al., 2022; Sap et al.,\n2022). Why is code such an effective signal when mixed with natural language? Some have speculated that code provides additional grounding (Potts, 2020; Merrill et al., 2021, inter alia). Perhaps more importantly, code contains frequent instances of long-range state tracking, as well as hierarchically structured classes and function stacks; these may impart inductive biases that are helpful for learning hierarchical linguistic structure. It is unclear why GPT-3.5 text-davinci-003\u2014 the only model in the GPT-3.5 family trained with RLHF\u2014performs significantly worse than comparable GPT-3.5 models. This goes against conventional wisdom that RLHF generally improves performance on downstream NLP tasks (Ye et al., 2023),11 but corroborates preliminary evidence that RLHF degrades certain aspects of performance, like certainty calibration.12 Further work is needed to fully understand RLHF\u2019s impact on generalization and whether these findings are causally linked. Perhaps the reinforcement learning procedure optimizes models to generate language that semantically aligns with human quality judgments, but at the cost of causing the model to assign lower importance to structural features. Meanwhile, providing human feedback via fine-tuning may be less optimal for aligning outputs to human judgments, but better for preserving sensitivity to syntax. Larger models often perform better on many NLP tasks. However, our findings reveal that scale is not a panacea for robust generalization: rather, other factors like training objectives, the type of pretraining data, and the supervision method(s) make a significant difference. This agrees with and extends findings from Mueller and Linzen (2023) that the domain of the pre-training data significantly influences how models generalize.\n# 7 Related Work\nSince the discovery that LMs are capable of ICL, studies have explored ICL\u2019s limits (e.g., Aky\u00fcrek et al. 2023; Chan et al. 2022). Analyses of ICL have revealed counterintuitive biases: for example, correct labels are not necessary for strong performance (Min et al., 2022b), and models can perform very well even given misleading/irrelevant prompts (Webson and Pavlick, 2022) or flipped/semantically misleading label names (Wei et al., 2023; Pan et al.,\n11OpenAI also officially states that text-davinci-003 is more capable than text-davinci-002 in multiple locations on their website, including here and here. 12https://openai.com/research/gpt-4\n2023). In contemporaneous work, Si et al. (2023a) analyze which semantic features (e.g., sentiment vs. topic) LLMs prefer by designing underspecified exemplars. Their method is similar to ours in that they use ambiguous exemplars and disambiguating test examples; however, we apply this approach to syntactic processes to analyze the fundamental linguistic structural abilities of LLMs. Any finite training set is consistent with multiple generalizations. Crucially, most benchmarking tasks rely on data where the training and test sets are drawn from the same distribution, which limits our understanding of how well models truly generalize (Linzen, 2020). Saparov and He (2023) find that LLMs are prone to relying on spurious correlations that hinder robust generalization, while Drozdov et al. (2023) find that a series of chainof-thought prompts can yield more robust generalization. With respect to syntax, LSTM- and Transformer-based encoder-decoder models trained from scratch on syntactic transformations do not generalize in a syntax-sensitive manner (McCoy et al., 2018, 2020; Petty and Frank, 2021), but pretrained encoder-decoder models do generalize syntactically (Mueller et al., 2022). Similar positive results have been reported for RoBERTa, but only after large-scale pre-training (Warstadt and Bowman, 2020; Warstadt et al., 2020). Hu and Levy (2023) prompt LLMs on metalinguistic judgments; they find that prompting underestimates syntactic awareness compared to probability measurements. Our findings extend this conclusion: this issue is more pronounced when test examples are not identically structured to the exemplars. When fine-tuning, larger models generally have inductive biases that align more strongly with syntactic structure (Mueller and Linzen, 2023), so it is plausible that greater scale should lead to more syntax-sensitive behavior when using ICL. Nonetheless, this is not what we find.\n# 8 Conclusions\nWe have investigated how well LLMs generalize out-of-distribution on tasks requiring syntax for robust performance. Our findings reveal significant variance across models that is not fully explained by scale. Models trained on code are better at leveraging in-context examples to generalize more robustly, and at reasoning accurately and faithfully\u2014 even at smaller scales.\n# Acknowledgments\nWe thank Jacob Eisenstein for helpful comments on a previous draft of this paper. We also thank Microsoft for supporting experimentation with OpenAI models via Azure under the Accelerate Foundation Models Research program. Aaron Mueller was supported by a National Science Foundation Graduate Research Fellowship (Grant #1746891). This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.\n# Limitations\nUsing closed-source language models presents many scientific challenges. There are no public resources that contain exact information about the models we test, such as their parameter counts, training distributions, and corpus sizes, among other important details. We also cannot definitively rule out that the transformations data is contained in the pre-training corpus. When asking the GPT models directly if they have seen the dataset or its GitHub repository, they did not state that they have seen them. However, Bard (not used in this study) directly admits to having seen syntax-aware evaluation datasets like HANS. OpenAI support for the Codex model was discontinued while this project was in progress. This reveals another set of challenges of working with closed-sourced models in scientific contexts: longterm replicability becomes difficult, and access is dependent on the support of non-accountable entities. While we were able to attain access again by applying to a research program, access is not guaranteed indefinitely. Regarding our experimental design, it is difficult to disentangle whether model errors are due to subhuman syntactic reasoning abilities, faulty inductive biases, or simply poor ICL abilities (App. B). We partially controlled for this by evaluating on indistribution transformations, which evaluates how well models can leverage ICL on in-distribution examples. However, the transformations task does not in itself tell us what specific mechanism leads to incorrect results: perhaps LLMs do robustly represent the syntactic structure of input sentences, but preferentially rely on superficial features. Or, perhaps they do not robustly represent sentence structure, instead relying on a series of heuristics that allow certain models to approximate syntactic generalization in specific circumstances. Future\nwork should employ mechanistic interpretability methods to uncover what, precisely, causes errors in transformations tasks.\n# Ethics Statement\nRegardless of how well large language models generalize or to what extent they demonstrate language understanding, they are still eminently capable of misuse and harm. LLMs excel at generating convincing arguments for false conclusions, or for generating false information and confidently presenting it as fact\u2014for example, when generating fake news. Our findings may erroneously suggest to some readers that addressing these concerns can wait, as LMs are still incapable of robustly understanding sentence structure. We disagree with this takeaway: research that addresses the harms of LLMs should occur in parallel with research that investigates (and improves) their decision-making mechanisms.\n# References\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations.\nEdwin L Battistella. 1996. The Logic of Markedness. Oxford University Press.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Computing Research Repository, arXiv:2107.03374.\nakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113.\nyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. Computing Research Repository, arXiv:2210.11416.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognizing textual entailment challenge. In Machine Learning Challenges Workshop, pages 177\u2013190. Springer.\nAndrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2023. Compositional semantic parsing with large language models. In The Eleventh International Conference on Learning Representations. Robert Frank and Donald Mathis. 2007. Transformational networks. In Proceedings of the Workshop on Psychocomputational Models of Human Language Acquisition. Cognitive Science Society. Joseph H Greenberg. 1966. Language Universals: With Special Reference to Feature Hierarchies. De Gruyter Mouton. Jennifer Hu and Roger Levy. 2023. Prompting is not a substitute for probability measurements in large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040\u20135060, Singapore. Association for Computational Linguistics. Najoung Kim and Sebastian Schuster. 2023. Entity tracking in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3835\u20133855, Toronto, Canada. Association for Computational Linguistics. Henry Kucera. 1982. Markedness and frequency: A computational analysis. In Coling 1982: Proceedings of the Ninth International Conference on Computational Linguistics. Tal Linzen. 2020. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210\u2013 5217, Online. Association for Computational Linguistics. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 305\u2013329, Nusa Dua, Bali. Association for Computational Linguistics. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384\u20131403, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. R. Thomas McCoy, Robert Frank, and Tal Linzen. 2018. Revisiting the poverty of the stimulus: Hierarchical generalization without a hierarchical bias in recurrent neural networks. In Proceedings of the 40th Annual Meeting of the Cognitive Science Society, pages\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.\nugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Computing Research Repository, arXiv:2307.09288.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting. In Thirty-seventh Conference on Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Alex Warstadt and Samuel R. Bowman. 2020. Can neural networks acquire a structural bias from raw linguistic data? In Proceedings of the 42nd Annual Meeting of the Cognitive Science Society, Online. Cognitive Science Society. Alex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, and Samuel R. Bowman. 2020. Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually). In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 217\u2013235, Online. Association for Computational Linguistics. Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300\u20132344, Seattle, United States. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Curran Associates, Inc. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma.\n2023. Larger language models do in-context learning differently. Computing Research Repository, arXiv:2303.03846. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics. Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. Computing Research Repository, arXiv:2303.10420.\n2023. Larger language models do in-context learning differently. Computing Research Repository, arXiv:2303.03846. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics. Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. Computing Research Repository, arXiv:2303.10420.\n2023. Larger language models do in-context learning differently. Computing Research Repository, arXiv:2303.03846.\n# A Tense Reinflection Prompt Formats\nIn Figure 5, we present the prompt formats we use for the tense reinflection task.\n# B Ablations and Error Analysis\nIn this appendix, we analyze which aspects of the prompt affect OOD generalization and investigate the type of errors that LMs make.\nWhen reasoning with code, GPT-3.5 code-davinci-002 is sensitive to variable names, but GPT-4 is not. GPT-4 and code-davinci-002 perform best on both transformations, and seem to improve with Code CoT prompting. We note that in the prompts we used in the main text, the variables had semantically meaningful names, such as main_subject; here, we ask how crucial this factor is for the models\u2019 ability to benefit from the Code CoT prompt. We present the prompt formats we use for this analysis in Figure 6. We first replace variable names (like main_auxiliary and main_subject) with nonce words (like wug and dax). We ensure that the same variable name is used for the same syntactic component in each exemplar, such that the model can form meaningful associations with the nonce (or swapped, as in the following analysis) variable names. For code-davinci-002, this reduces 8-shot performance from 97% to 76%. For GPT-4, this only reduces 8-shot performance from 97% to 96%. In a second experiment, we assign variable names adversarially, such that seemingly meaningful variable names are no longer associated with their conventional linguistic category; for example, the main verb phrase is associated with the variable main_object (e.g., main_object = \"has entertained\"), or the main subject is associated with the variable main_vp (e.g., main_vp = \"The newt\"). For code-davinci-002, this reduces performance from 98% to 83%, but for GPT-4, the accuracy stays the same at 97%. These patterns indicate that code-davinci-002 is sensitive to\u2014and may usably understand and rely on\u2014terminology like \u201csubject\u201d and \u201cverb\u201d. Meanwhile, GPT-4 may or may not understand the \u201csubject\u201d and \u201cverb\u201d names, but it does not crucially rely on them to perform the task; it instead shows an ability to adapt to arbitrary variable names.\nGPT-3 and GPT-3.5 (Turbo) rely heavily on spurious lexical features. What kinds of errors are causing such low performance in GPT-3 and GPT-3.5 (Turbo)? In a qualitative analysis, we find that when these models do not produce the correct answer, it is typically because they move the affirmative (non-negated) auxiliary, regardless of whether it is the main auxiliary. We call this the MOVE-AFFIRMATIVE heuristic. For example, in Ex. (8-a) and (8-b) below, these models would likely move the affirmative auxiliary, even though it results in the incorrect output in Ex. (8-b).\n) a. The unicorn that hasn\u2019t entertained the newts has observed the yak. \u00f1 Has the unicorn that hasn\u2019t entertained the newts observed the yak? b. The unicorn that has entertained the newts hasn\u2019t observed the yak. \u00f1 Has the unicorn that has entertained the newts observed the yak?\nIn fact, for all models we test, errors in question formation are typically due to reliance on MOVE-AFFIRMATIVE. The fact that models adopt this heuristic is puzzling, as it is not consistent with the in-context exemplars (half of the verbs in the exemplars are negative verbs), and therefore reflects an inductive bias. We hypothesize that the bias in question favors generating linguistically unmarked forms in ambiguous contexts. Such an unmarkedness bias would be easy to learn given that unmarked forms are generally more frequent (Greenberg, 1966). In this case, affirmative verb forms are likely more frequent than negative ones, especially at the beginning of a question. 13 More broadly, a systematic preference for linguistically unmarked forms could be a form of overfitting to the training data. Future work could assess whether this type of overfitting is more likely with scale, or whether it could be overcome with some form of intervention to the data or model. In tense reinflection, most errors are due to models relying on the linear AGREE-RECENT hypothesis. Thus, errors in this case reflect reliance on word position and relative word ordering in generating sentences. All models we test generalize in a syntax-sensitive manner more often than not,\n13The relationship between markedness and frequency is disputed (Battistella, 1996). Frequency and markedness often correlate, but not always; see Kucera (1982) for an empirical case study in English.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d1e/2d1e89e6-9252-4b1f-a72b-63ce5cb1e83b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: The prompt formats we use for tense reinflection. We give the model up to 8 exemplars, followed by a tes example. The model must generate the chain-of-thought reasoning and the answer. We highlight the answers with  blue background. In the code prompt, we put code (except comments) in blue text.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f47c/f47c8c76-f8e7-488b-abe0-1ca54d7d9d0a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Prompt formats for our Code CoT analyses. For each exemplar, we ensure that the same variable name is used for the same syntactic component, such that the model can form meaningful associations with the nonce/swapped variable names.</div>\nFigure 6: Prompt formats for our Code CoT analyses. For each exemplar, we ensure th name is used for the same syntactic component, such that the model can form meaningful nonce/swapped variable names.\nbut this error pattern reflects that models are not entirely dependent on syntactic inductive biases: rather, given in-context examples, models can generalize using either inductive bias.\n# C Faithfulness of Chain-of-Thought Reasoning Traces: Full Analysis\n# C.1 Extracting Components from Model Reasoning\nQuestion formation. The model must generate variables corresponding to the main subject, main object, and main verb phrase. For example, given Ex. (6), the model must predict main_subject = \"The salamanders\" for its reasoning to count as correctly identifying the main subject. For faithfulness, if the model predicts main_subject = \"The salamanders\" in its reasoning and then begins its answer with \u201cHave the salamanders...\u201d, this would be considered faithful to its own reasoning on the main subject; conversely, if it predicts main_subject = \"The salamanders\" in its reasoning but then begins its final answer with \u201cHave the yaks. . . \u201d, this would not be considered faithful. Tense reinflection. The model must generate a Python dictionary of subjects (keys) and verbs (values) as part of its CoT prompt. In this example, the correct dictionary is subjects_verbs = {\"peacocks\": [\"admired\", \"remembered\"], \"raven\": [], \"vulture\": []}. The noun reasoning accuracy is the proportion of nouns from the sentence that are present in the dictionary, while the subject-verb association accuracy is the proportion of verbs associated with the correct subject key. For faithfulness, we measure how many nouns from the dictionary are present in the answer, and whether the verbs\u2019 subjects in the answer are the same as they are in the dictionary (i.e., whether the sentence structure is compatible with the reasoning). For verb number faithfulness, we measure the proportion of verbs in the dictionary whose grammatical number in the final answer is the same as the grammatical number of their subject in the dictionary.\n# C.2 Full Results\nHere, we present reasoning accuracies and faithfulness scores for all reasoning components, models, and tasks. See Figure 7 for results on question formation, and Figure 8 for results on tense reinflection.\n# D Syntactic Generalization in Text Classification: Prompts and Full Results\nThe analyses reported in the main text found that LLMs leverage syntactic information to varying extents when taught a task via ICL, and that code pre-training improves OOD generalization. In this section, In this section, we test if these trends are specific to tasks where models must generate transformed versions of their inputs, or if they also extend to classification tasks. We leverage the Heuristic Analysis for NLI Systems (HANS; McCoy et al., 2019) dataset, which consists of Natural Language Inference (NLI) examples designed to diagnose reliance on syntactic heuristics. One typically trains the model on the training set of another NLI dataset, such as MNLI (Williams et al., 2018) or RTE (Dagan et al., 2006), and then uses HANS as an evaluation set to measure whether models rely on syntactic heuristics. Si et al. (2023b) evaluate RoBERTa and GPT-3 on HANS after fine-tuning on MNLI; they find that both models perform similarly, suggesting that scale may not be enough for models to overcome syntactic heuristics. We extend this analysis to a wider variety of LLMs and to chain-of-thought prompting, and further break down performance by label. HANS disagnoses reliance on three particular heuristics: lexical overlap, subsequence, and constituent heuristics. A model relying on lexical overlap heuristics assumes that two sentences with high word overlap entail each other. For example, although \u201cThe actor paid the doctor\u201d means the opposite of \u201cThe actor was paid by the doctor\u201d, a model could assume these are entailed because of the high amount of word overlap between the two sentences. A model relying on subsequence heuristics assumes that a hypothesis is entailed if it is a subsequence in the premise. For example, although \u201cThe actor near the doctor danced\u201d does not imply that \u201cThe doctor danced\u201d, a model may assume they are entailed because the latter sentence is fully contained in the first. Finally, a model relying on constituent heuristics assumes that a hypothesis is entailed if it is a subtree of the premise. For example, \u201cIf the artist slept, the doctor ran\u201d does not imply that \u201cThe artist slept\u201d, but a model relying on this heuristic would assume that it is entailed. Models that systematically rely on any of these heuristics are expected to obtain 100% accuracy on examples where the label is \u201centailment\u201d because\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b2f6/b2f65d5f-dc6a-4d3b-9f83-2a47aecf361f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Reasoning accuracies and faithfulness scores for various models and syntactic components using the Code CoT prompt on question formation. \u201cAll\u201d refers to the proportion of examples where models correctly label each syntactic component, or where outputs are entirely faithful to the model\u2019s generated reasoning.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f655/f655c15c-a89a-4739-be66-77bc28c8efaf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Reasoning accuracies and faithfulness scores for various models and syntactic components using the Code CoT prompt on tense reinflection. \u201cAll\u201d refers to the proportion of examples where models correctly label each syntactic component, or where outputs are entirely faithful to the model\u2019s generated reasoning.</div>\nthe heuristics happen to make the correct prediction; however, models that rely on these heuristics will also obtain 0% accuracy on examples where the label is \u201cnon-entailment\u201d. We evaluate our LLMs on HANS in the ICL setting. The 8 ICL exemplars are drawn from MNLI, and the test examples are from HANS. Our results are based on a uniform subsample of 100 examples per heuristic and per label. We test across 3 syntactic heuristics, and there are 2 labels (entailment and non-entailment); thus, we have 600 test examples in total. We use No CoT prompts and Verbal CoT prompts similar to those we depict in Figure 2. For example:\n# (9) No CoT\na. Input: Q: There are many regulations in place that will reduce air emissions from electric power generation. Is it definitely true that There are a lot of regulations in place that reduce emissions?\n...(up to 7 more exemplars from MNLI) Q: The lawyer was advised by the actor. Is it definitely true that The lawyer advised the actor? A: b. Output: Yes\nLexical Overlap\nSubsequence\nConstituent\nModel\nNo CoT\nVerbal CoT\nNo CoT\nVerbal CoT\nNo CoT\nVerbal CoT\nGPT-3 text-davinci-001\n59\n100\n55\n97\n59\n99\nGPT-3.5 code-davinci-002\n81\n99\n99\n96\n96\n96\nGPT-3.5 text-davinci-002\n84\n98\n67\n99\n64\n95\nGPT-3.5 text-davinci-003\n96\n99\n100\n100\n96\n100\nGPT-3.5 gpt-3.5-turbo-0301\n81\n16\n69\n3\n96\n6\nGPT-4 gpt-4-0314\n94\n98\n100\n96\n98\n98\nPaLM\n94\n98\n99\n100\n100\n100\nFlan-PaLM\n99\n100\n100\n100\n100\n100\nLlama 2\n84\n93\n98\n100\n99\n100\nCodeLlama\n80\n65\n95\n50\n91\n89\nLexical Overlap\nSubsequence\nConstituent\nModel\nNo CoT\nVerbal CoT\nNo CoT\nVerbal CoT\nNo CoT\nVerbal CoT\nGPT-3 text-davinci-001\n86\n0\n76\n0\n85\n6\nGPT-3.5 code-davinci-002\n99\n94\n70\n48\n74\n35\nGPT-3.5 text-davinci-002\n98\n84\n86\n48\n88\n38\nGPT-3.5 text-davinci-003\n99\n99\n50\n34\n61\n11\nGPT-3.5 gpt-3.5-turbo-0301\n82\n5\n27\n11\n9\n12\nGPT-4 gpt-4-0314\n94\n92\n48\n46\n49\n35\nPaLM\n97\n94\n57\n57\n27\n13\nFlan-PaLM\n78\n96\n36\n41\n3\n2\nLlama 2\n99\n91\n78\n51\n27\n11\nCodeLlama\n96\n100\n56\n89\n55\n47\n(b) Non-entailment\nTable 3: Accuracies on HANS. Scores are split by label: test examples whose label is \u201centailment\u201d are presented  the top table (a), while examples whose label is \u201cnon-entailment\u201d are presented in the bottom table (b). High scor on entailment coupled with low scores on non-entailment signify that the model relies on the syntactic heuristic  predict the label.\n# (10) Verbal CoT\n(10) Verbal CoT\nQ: The lawyer was advised by the actor. Is it definitely true that The lawyer advised the actor? A: b. Output:\nThe lawyer is the subject of the first sentence. The actor is the subject of the second sentence. Advised is the verb in the first sentence. Was advised is the verb in the second sentence. The answer is Yes.\nOur results (Table 3) suggest that LLMs are susceptible to syntactic heuristics, but to a lesser extent than smaller-scale LMs fine-tuned on MNLI (McCoy et al., 2019). Most models achieve close to 100% accuracy on examples where the gold label is \u201centailment\u201d, as expected. However, on \u201cnonentailment\u201d examples, variance is very high: all models (except GPT-3.5 (Turbo)) achieve high accuracies on lexical overlap examples using both prompts, suggesting that they do not succumb to the lexical overlap heuristic when generalizing. For other syntactic heuristics, however, non-entailment scores tend toward 0\u201350%. This suggests that models rely on subsequence and constituent heuristics when guided via ICL, though the extent to which\nthey rely on the heuristic depends on the prompt format. Unlike in the transformation tasks, here code pre-training does not seem to lend a significant advantage: there is no consistent significant gain (nor loss) on entailment or non-entailment examples in models pre-trained on code. We also find that chain-of-thought reasoning makes models more prone to relying on syntactic heuristics. With Verbal CoT, scores on entailment examples tend to increase across models and across syntactic heuristic types. On non-entailment examples, however, it reduces scores to near-randomchance or near-zero. This pattern suggests that Verbal CoT pushes models to prefer the syntactic heuristics more strongly.\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) is a prevalent method for training large language models (LLMs) to perform new tasks by providing labeled examples in the input context, raising questions about the models' understanding of task structure versus reliance on superficial heuristics. This study investigates LLMs' robustness in generalizing syntactic structures and explores the impact of chain-of-thought prompting on out-of-distribution performance.",
        "problem": {
            "definition": "The paper addresses the challenge of determining whether LLMs can generalize syntactically when faced with distribution shifts between training and test examples, particularly in the context of syntactic transformations and natural language inference tasks.",
            "key obstacle": "Existing methods often lead models to rely on surface-level heuristics rather than learning the underlying syntactic structures, resulting in poor generalization to out-of-distribution examples."
        },
        "idea": {
            "intuition": "The idea stems from observing that while LLMs exhibit impressive performance on in-distribution tasks, they often struggle with out-of-distribution examples, suggesting a need for improved understanding of syntax.",
            "opinion": "The proposed idea involves evaluating the effectiveness of chain-of-thought prompting to enhance LLMs' reliance on robust syntactic features rather than superficial cues.",
            "innovation": "This method differs from existing approaches by explicitly testing the impact of chain-of-thought reasoning on both in-distribution and out-of-distribution performance, revealing that improvements in one do not guarantee improvements in the other."
        },
        "method": {
            "method name": "Syntactic Transformations and Natural Language Inference Evaluation",
            "method abbreviation": "ST-NLI",
            "method definition": "The method involves prompting LLMs with syntactic transformation tasks and natural language inference tasks, assessing their ability to generalize syntactically across in-distribution and out-of-distribution examples.",
            "method description": "This method evaluates the syntactic understanding of LLMs through transformations and inference tasks, measuring their performance based on their reliance on syntactic versus positional features.",
            "method steps": [
                "Define syntactic transformation tasks with clear input-output relationships.",
                "Select LLMs from various families for testing.",
                "Prompt models with labeled examples in the context.",
                "Evaluate performance on both in-distribution and out-of-distribution test sets."
            ],
            "principle": "The method is effective because it directly assesses the models' understanding of hierarchical syntactic structures, which are crucial for robust language comprehension."
        },
        "experiments": {
            "evaluation setting": "The experimental setup includes syntactic transformation tasks and the HANS dataset for natural language inference, with models evaluated on in-distribution and out-of-distribution examples to measure generalization.",
            "evaluation method": "Performance is assessed through accuracy measures on the transformation tasks and the HANS dataset, focusing on the models' ability to utilize syntactic information correctly."
        },
        "conclusion": "The study concludes that LLMs show significant variance in their ability to generalize syntactically, with models trained on code demonstrating better robustness. Chain-of-thought prompting can improve in-distribution performance but may not enhance out-of-distribution generalization, highlighting the need for careful evaluation in prompt tuning.",
        "discussion": {
            "advantage": "The proposed approach allows for a deeper understanding of LLMs' syntactic capabilities and the effects of different training methodologies, particularly the benefits of code pre-training.",
            "limitation": "The reliance on closed-source models presents challenges in replicability and transparency, and the findings may not generalize across all tasks or model architectures.",
            "future work": "Future research should explore alternative training methods, refine evaluation techniques, and investigate the mechanisms underlying LLMs' reliance on syntactic versus superficial features."
        },
        "other info": {
            "acknowledgments": "The authors thank Jacob Eisenstein for comments and Microsoft for support via Azure. Aaron Mueller was supported by a National Science Foundation Graduate Research Fellowship.",
            "limitations": {
                "info1": "Closed-source models limit access to detailed training data and parameter information.",
                "info2": {
                    "info2.1": "The transformations data may overlap with the models' pre-training corpus, complicating evaluation.",
                    "info2.2": "Variability in model performance may arise from different training methodologies and data distributions."
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is a prevalent method for training large language models (LLMs) to perform new tasks by providing labeled examples in the input context."
        },
        {
            "section number": "1.3",
            "key information": "This study investigates LLMs' robustness in generalizing syntactic structures and explores the impact of chain-of-thought prompting on out-of-distribution performance."
        },
        {
            "section number": "2",
            "key information": "The paper addresses the challenge of determining whether LLMs can generalize syntactically when faced with distribution shifts between training and test examples, particularly in the context of syntactic transformations and natural language inference tasks."
        },
        {
            "section number": "3.1",
            "key information": "The method evaluates the syntactic understanding of LLMs through transformations and inference tasks, measuring their performance based on their reliance on syntactic versus positional features."
        },
        {
            "section number": "4.1",
            "key information": "Chain-of-thought prompting can improve in-distribution performance but may not enhance out-of-distribution generalization, highlighting the need for careful evaluation in prompt tuning."
        },
        {
            "section number": "6.1",
            "key information": "The reliance on closed-source models presents challenges in replicability and transparency, and the findings may not generalize across all tasks or model architectures."
        }
    ],
    "similarity_score": 0.7596163797340121,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-context Learning Generalizes, But Not Always Robustly_ The Case of Syntax.json"
}