{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1710.10766",
    "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
    "abstract": "Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.",
    "bib_name": "song2018pixeldefendleveraginggenerativemodels",
    "md_text": "PIXELDEFEND: LEVERAGING GENERATIVE MODELS TO UNDERSTAND AND DEFEND AGAINST ADVERSARIAL EXAMPLES\n# PIXELDEFEND: LEVERAGING GENERATIVE MODELS TO UNDERSTAND AND DEFEND AGAINST ADVERSARIAL EXAMPLES\nYang Song Stanford University yangsong@cs.stanford.edu\n# Sebastian Nowozin Microsoft Research nowozin@microsoft.com\nSebastian Nowozin Microsoft Research nowozin@microsoft.com\nNate Kushman Microsoft Research nkushman@microsoft.com\nMicrosoft Research nkushman@microsoft.com\nABSTRACT\n# ABSTRACT\nAdversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.\n# INTRODUCTION\nRecent work has shown that small, carefully chosen modifications to the inputs of a neural network classifier can cause the model to give incorrect labels (Szegedy et al., 2013; Goodfellow et al., 2014). This weakness of neural network models is particularly surprising because the modifications required are often imperceptible, or barely perceptible, to humans. As deep neural networks are being deployed in safety-critical applications such as self-driving cars (Amodei et al., 2016), it becomes increasingly important to develop techniques to handle these kinds of inputs. Rethinking adversarial examples The existence of such adversarial examples seems quite surprising. A neural network classifier can get super-human performance (He et al., 2015) on clean test images, but will give embarrassingly wrong predictions on the same set of images if some imperceptible noise is added. What makes this noise so special to deep neural networks? In this paper, we propose and empirically evaluate the following hypothesis: Even though they have very small deviations from clean images, adversarial examples largely lie in the low probability regions of the distribution that generated the data used to train the model. Therefore, they fool\nTaesup Kim Universit\u00e9 de Montr\u00e9al taesup.kim@umontreal.ca Stefano Ermon Stanford University ermon@cs.stanford.edu\nStefano Ermon Stanford University ermon@cs.stanford.edu\nclassifiers mainly due to covariate shift. This is analogous to training models on MNIST (LeCun et al., 1998) but testing them on Street View House Numbers (Netzer et al., 2011). To study this hypothesis, we first need to estimate the probability density of the underlying training distribution. To this end, we leverage recent developments in generative models. Specifically, we choose a PixelCNN (van den Oord et al., 2016b) model for its state-of-the-art performance in modeling image distributions (van den Oord et al., 2016a; Salimans et al., 2017) and tractability of evaluating the data likelihood. In the first part of the paper, we show that a well-trained PixelCNN generative model is very sensitive to adversarial inputs, typically giving them several orders of magnitude lower likelihoods compared to those of training and test images. Detecting adversarial examples An important step towards handling adversarial images is the ability to detect them. In order to catch any kind of threat, existing work has utilized confidence estimates from Bayesian neural networks (BNNs) or dropout (Li & Gal, 2017; Feinman et al., 2017). However, if their model is misspecified, the uncertainty estimates can be affected by covariate shift (Shimodaira, 2000). This is problematic in an adversarial setting, since the attacker might be able to make use of the inductive bias from the misspecified classifier to bypass the detection. Protection against the strongest adversary requires a pessimistic perspective\u2014our assumption is that the classifier cannot give reliable predictions for any input outside of the training distribution. Therefore, instead of relying on label uncertainties given by the classifier, we leverage statistical hypothesis testing to detect any input not drawn from the same distribution as training images. Specifically, we first compute the probabilities of all training images under the generative model. Afterwards, for a novel input we compute the probability density at the input and evaluate its rank (in ascending order) among the density values of all training examples. Next, the rank can be used as a test statistic and gives us a p-value for whether or not the image was drawn from the training distribution. This method is general and practical and we show that the p-value enables us to detect adversarial images across a large number of different attacking methods with high probability, even when they differ from clean images by only a few pixel values.\nmagnitude lower likelihoods compared to those of training and test images. Detecting adversarial examples An important step towards handling adversarial images is the ability to detect them. In order to catch any kind of threat, existing work has utilized confidence estimates from Bayesian neural networks (BNNs) or dropout (Li & Gal, 2017; Feinman et al., 2017). However, if their model is misspecified, the uncertainty estimates can be affected by covariate shift (Shimodaira, 2000). This is problematic in an adversarial setting, since the attacker might be able to make use of the inductive bias from the misspecified classifier to bypass the detection. Protection against the strongest adversary requires a pessimistic perspective\u2014our assumption is that the classifier cannot give reliable predictions for any input outside of the training distribution. Therefore, instead of relying on label uncertainties given by the classifier, we leverage statistical hypothesis testing to detect any input not drawn from the same distribution as training images. Specifically, we first compute the probabilities of all training images under the generative model. Afterwards, for a novel input we compute the probability density at the input and evaluate its rank (in ascending order) among the density values of all training examples. Next, the rank can be used as a test statistic and gives us a p-value for whether or not the image was drawn from the training distribution. This method is general and practical and we show that the p-value enables us to detect adversarial images across a large number of different attacking methods with high probability, even when they differ from clean images by only a few pixel values. Purifying adversarial examples Since adversarial examples are generated from clean images by adding imperceptible perturbations, it is possible to decontaminate them by searching for more probable images within a small distance of the original ones. By limiting the L\u221edistance1, this image purification procedure generates only imperceptible modifications to the original input, so that the true labels of the purified images remain the same. The resulting purified images have higher probability under the training distribution, so we can expect that a classifier trained on the clean images will have more reliable predictions on the purified images. Moreover, for inputs which are not corrupted by adversarial perturbations the purified results remain in a high density region. We use this intuition to build PixelDefend, an image purification procedure which requires no knowledge of the attack nor the targeted classifier. PixelDefend approximates the training distribution using a PixelCNN model. The constrained optimization problem of finding the highest probability image within an \u03f5-ball of the original is computationally intractable, however, so we approximate it using a greedy decoding procedure. Since PixelDefend does not change the classification model, it can be combined with other adversarial defense techniques, including adversarial training (Goodfellow et al., 2014), to provide synergistic improvements. We show experimentally that PixelDefend performs exceptionally well in practice, leading to state-of-the art results against a large number of attacks, especially when combined with adversarial training.\nPurifying adversarial examples Since adversarial examples are generated from clean images by adding imperceptible perturbations, it is possible to decontaminate them by searching for more probable images within a small distance of the original ones. By limiting the L\u221edistance1, this image purification procedure generates only imperceptible modifications to the original input, so that the true labels of the purified images remain the same. The resulting purified images have higher probability under the training distribution, so we can expect that a classifier trained on the clean images will have more reliable predictions on the purified images. Moreover, for inputs which are not corrupted by adversarial perturbations the purified results remain in a high density region. We use this intuition to build PixelDefend, an image purification procedure which requires no knowledge of the attack nor the targeted classifier. PixelDefend approximates the training distribution using a PixelCNN model. The constrained optimization problem of finding the highest probability image within an \u03f5-ball of the original is computationally intractable, however, so we approximate it using a greedy decoding procedure. Since PixelDefend does not change the classification model, it can be combined with other adversarial defense techniques, including adversarial training (Goodfellow et al., 2014), to provide synergistic improvements. We show experimentally that PixelDefend performs exceptionally well in practice, leading to state-of-the art results against a large number of attacks, especially when combined with adversarial training.\n# Contributions Our main contributions are as follows:\n\u2022 We show that generative models can be used for detecting adversarially perturbed images and observe that most adversarial examples lie in low probability regions. \u2022 We introduce a novel family of methods for defending against adversarial attacks based on the idea of purification. \u2022 We show that a defensive technique from this family, PixelDefend, can achieve state-of-theart results on a large number of attacking techniques, improving the accuracy against the strongest adversary on the CIFAR-10 dataset from 32% to 70%.\n# 2 BACKGROUND\n# 2.1 ATTACKING METHODS\n2.1 ATTACKING METHODS\nGiven a test image X, an attacking method tries to find a small perturbation \u2206with \u2225\u2206\u2225\u221e\u2264\u03f5attack, such that a classifier f gives different predictions on Xadv \u225cX + \u2206and X. Here colors in the image are represented by integers from 0 to 255. Each attack method is controlled by a configurable \u03f5attack parameter which sets the maximum perturbation allowed for each pixel in integer increments on the color scale. We only consider white-box attacks in this paper, i.e., the attack methods can get access to weights of the classifier. In the following, we give an introduction to all the attacking methods used in our experiments.\n# Random perturbation (RAND) Random perturbation is arguably the weakest attacking method, and we include it as the simplest baseline. Formally, the randomly perturbed image is given by\nXadv = X + U(\u2212\u230a\u03f5attack\u230b, \u230a\u03f5attack\u230b),\nwhere U(a, b) denotes an element-wise uniform distribution of integers from [a, b].\nFast gradient sign method (FGSM) Goodfellow et al. (2014) proposed the generation of malicious perturbations in the direction of the loss gradient \u2207XL(X, y), where L(X, y) is the loss function used to train the model. The adversarial examples are computed by\nXadv = X + \u03f5attack sign(\u2207XL(X, y)).\nBasic iterative method (BIM) Kurakin et al. (2016) tested a simple variant of the fast gradient sign method by applying it multiple times with a smaller step size. Formally, the adversarial examples are computed as\nBasic iterative method (BIM) Kurakin et al. (2016) tested a simple variant of the fast gradient sign method by applying it multiple times with a smaller step size. Formally, the adversarial examples are computed as\nXadv 0 = X, Xadv n+1 = Clip\u03f5attack X \ufffd Xadv n + \u03b1 sign(\u2207XL(Xadv n , y)) \ufffd ,\n\ufffd \ufffd where Clip\u03f5attack X means we clip the resulting image to be within the \u03f5attack-ball of X. Following Kurakin et al. (2016), we set \u03b1 = 1 and the number of iterations to be \u230amin(\u03f5attack + 4, 1.25\u03f5attack)\u230b. This method is also called Projected Gradient Descent (PGD) in Madry et al. (2017).\nDeepFool DeepFool (Moosavi-Dezfooli et al., 2016) works by iteratively linearizing the decision boundary and finding the closest adversarial examples with geometric formulas. However, compared to FGSM and BIM, this method is much slower in practice. We clip the resulting image so that its perturbation is no larger than \u03f5attack.\nCarlini-Wagner (CW) Carlini & Wagner (2017b) proposed an efficient optimization objective for iteratively finding the adversarial examples with the smallest perturbations. As with DeepFool, we clip the output image to make sure the perturbations are limited by \u03f5attack.\n# 2.2 DEFENSE METHODS\nCurrent defense methods generally fall into two classes. They either (1) change the network architecture or training procedure to make it more robust, or (2) modify adversarial examples to reduce their harm. In this paper, we take the following defense methods into comparison.\nAdversarial training This defense works by generating adversarial examples on-the-fly during training and including them into the training set. FGSM adversarial examples are the most commonly used ones for adversarial training, since they are fast to generate and easy to train. Although training with higher-order adversarial examples (e.g., BIM) has witnessed some success in small datasets (Madry et al., 2017), other work has reported failure in larger ones (Kurakin et al., 2016). We consider both variants in our work.\nLabel smoothing In contrast to adversarial training, label smoothing (Warde-Farley & Goodfellow, 2016) is agnostic to the attack method. It converts one-hot labels to soft targets, where the correct class has value 1 \u2212\u03f5 while the other (wrong) classes have value \u03f5/(N \u22121). Here \u03f5 is a small constant and N is the number of classes. When the classifier is re-trained on these soft targets rather than the one-hot labels it is significantly more robust to adversarial examples. This method was originally devised to achieve a similar effect as defensive distillation (Papernot et al., 2016c), and their performance is comparable. We didn\u2019t compare to defensive distillation since it is more computationally expensive. Feature squeezing Feature squeezing (Xu et al., 2017a) is both attack-agnostic and model-agnostic. Given any input image, it first reduces the color range from [0, 255] to a smaller value, and then smooths the image with a median filter. The resulting image is then passed to a classifier for predictions. Since this technique does not depend on attacking methods and classifiers, it can be combined with other defensive methods such as adversarial training, similar to PixelDefend.\nLabel smoothing In contrast to adversarial training, label smoothing (Warde-Farley & Goodfellow, 2016) is agnostic to the attack method. It converts one-hot labels to soft targets, where the correct class has value 1 \u2212\u03f5 while the other (wrong) classes have value \u03f5/(N \u22121). Here \u03f5 is a small constant and N is the number of classes. When the classifier is re-trained on these soft targets rather than the one-hot labels it is significantly more robust to adversarial examples. This method was originally devised to achieve a similar effect as defensive distillation (Papernot et al., 2016c), and their performance is comparable. We didn\u2019t compare to defensive distillation since it is more computationally expensive. Feature squeezing Feature squeezing (Xu et al., 2017a) is both attack-agnostic and model-agnostic.\n# 2.3 EXPERIMENT METHODOLOGIES\nDatasets Two datasets are used in our experiments: Fashion MNIST (Xiao et al., 2017) and CIFAR10 (Krizhevsky et al.). Fashion MNIST was designed as a more difficult, but drop-in replacement for MNIST (LeCun et al., 1998). Thus it shares all of MNIST\u2019s characteristics, i.e., 60, 000 training examples and 10, 000 test examples where each example is a 28 \u00d7 28 gray-scale image associated with a label from 1 of 10 classes. CIFAR-10 is another dataset that is also broadly used for image classification tasks. It consists of 60, 000 examples, where 50, 000 are used for training and 10, 000 for testing, and each sample is a 32 \u00d7 32 color image associated with 1 of 10 classes. Models We examine two state-of-the-art deep neural network image classifiers: ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2014). The architectures are described in Appendix C. PixelCNN The PixelCNN (van den Oord et al., 2016b; Salimans et al., 2017) is a generative model with tractable likelihood especially designed for images. The model defines the joint distribution over all pixels by factorizing it into a product of conditional distributions. \ufffd\nDatasets Two datasets are used in our experiments: Fashion MNIST (Xiao et al., 2017) and CIFAR10 (Krizhevsky et al.). Fashion MNIST was designed as a more difficult, but drop-in replacement for MNIST (LeCun et al., 1998). Thus it shares all of MNIST\u2019s characteristics, i.e., 60, 000 training examples and 10, 000 test examples where each example is a 28 \u00d7 28 gray-scale image associated with a label from 1 of 10 classes. CIFAR-10 is another dataset that is also broadly used for image classification tasks. It consists of 60, 000 examples, where 50, 000 are used for training and 10, 000 for testing, and each sample is a 32 \u00d7 32 color image associated with 1 of 10 classes.\nModels We examine two state-of-the-art deep neural network image classifiers: ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2014). The architectures are described in Appendix C. PixelCNN The PixelCNN (van den Oord et al., 2016b; Salimans et al., 2017) is a generative model with tractable likelihood especially designed for images. The model defines the joint distribution over all pixels by factorizing it into a product of conditional distributions.\nPixelCNN The PixelCNN (van den Oord et al., 2016b; Salimans et al., 2017) is a generative model with tractable likelihood especially designed for images. The model defines the joint distribution over all pixels by factorizing it into a product of conditional distributions.\n\ufffd The pixel dependencies are in raster scan order (row by row and column by column within each row). We train the PixelCNN model for each dataset using only clean (not perturbed) image samples. In Appendix D, we provide clean sample images from the datasets as well as generated image samples from PixelCNN (see Figure 8 and Figure 9). As a convenient representation of pCNN(X) for images, we also use the concept of bits per dimension, which is defined as BPD(X) \u225c\u2212log pCNN(X)/(I \u00d7 J \u00d7 K \u00d7 log 2) for an image of resolution I \u00d7 J and K channels.\n# 3 DETECTING ADVERSARIAL EXAMPLES\nAdversarial images are defined with respect to a specific classifier. Intuitively, a maliciously perturbed image that causes one network to give a highly confident incorrect prediction might not fool another network. However, recent work (Papernot et al., 2016a; Liu et al., 2016; Tram\u00e8r et al., 2017) has shown that adversarial images can transfer across different classifiers. This indicates that there are some intrinsic properties of adversarial examples that are independent of classifiers. One possibility is that, compared to normal training and test images, adversarial examples have much lower probability densities under the image distribution. As a result, classifiers do not have enough training instances to get familiarized with this part of the input space. The resulting prediction task suffers from covariate shift, and since all of the classifiers are trained on the same dataset, this covariate shift will affect all of them similarly and will likely lead to misclassifications. To empirically verify this hypothesis, we train a PixelCNN model on the CIFAR-10 (Krizhevsky & Hinton, 2009) dataset and use its log-likelihood as an approximation to the true underlying probability\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/682e/682efdbd-e634-4e0a-aac3-b8871113a5bb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: An image sampled from the CIFAR-10 test dataset and various adversarial examples generated from it. The text above shows the attacking method while the text below shows the predicted label of the ResNet.</div>\nFigure 1: An image sampled from the CIFAR-10 test dataset and various adversarial examples generated from it. The text above shows the attacking method while the text below shows the predicted label of the ResNet.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/203d/203d29ee-3631-4028-91ed-a68502d878f4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: (a) Likelihoods of different perturbed images with \u03f5attack = 8. (b) Test errors of a ResNet on different adversarial examples.</div>\ndensity. The adversarial examples are generated with respect to a ResNet (He et al., 2016), which gets 92% accuracy on the test images. We generate adversarial examples from RAND, FGSM, BIM, DeepFool and CW methods with \u03f5attack = 8. Note that as shown in Figure 1, the resulting adversarial perterbations are barely perceptible to humans. However, the distribution of log-likelihoods show considerable difference between perturbed images and clean images. As summarized in Figure 2, even a 3% perturbation can lead to systematic decrease of log-likelihoods. Note that the PixelCNN model has no information about the attacking methods for producing those adversarial examples, and no information about the ResNet model either. We can see from Figure 3(b) that random perturbations also push the images outside of the training distribution, even though they do not have the same adverse effect on accuracy. We believe this is due to an inductive bias that is shared by many neural network models but not inherent to all models, as discussed further in Appendix A. Besides qualitative analysis, the log-likelihoods from PixelCNN also provide a quantitative measure for detecting adversarial examples. Combined with permutation test (Efron & Tibshirani, 1994), we can provide a uncertainty value for each input about whether it comes from the training distribution or not. Specifically, let the input X\u2032 i.i.d. \u223cq(X) and training images X1, \u00b7 \u00b7 \u00b7 , XN i.i.d. \u223cp(X). The null hypothesis is H0 : p(X) = q(X) while the alternative is H1 : p(X) \u0338= q(X). We first compute the probabilities give by a PixelCNN for X\u2032 and X1, \u00b7 \u00b7 \u00b7 , XN, then use the rank of pCNN(X\u2032) in {pCNN(X1), \u00b7 \u00b7 \u00b7 , pCNN(XN)} as our test statistic:\n\ufffd Here I[\u00b7] is the indicator function, which equals 1 when the condition inside brackets is true and other wise equals 0. Let Ti = T(Xi; X1, \u00b7 \u00b7 \u00b7 , Xi\u22121, X\u2032, Xi+1, \u00b7 \u00b7 \u00b7 , XN). According to the permutation principle, Ti has the same distribution as T under the null hypothesis H0. We can therefore compute the p-value exactly by p = 1 N + 1 \ufffdN \ufffd i=1 I[Ti \u2264T] + 1 \ufffd = T + 1 N + 1 = 1 N + 1 \ufffdN \ufffd i=1 I[pCNN(Xi) \u2264pCNN(X\u2032)] + 1 \ufffd .\n\ufffd Here I[\u00b7] is the indicator function, which equals 1 when the condition inside brackets is true and other wise equals 0. Let Ti = T(Xi; X1, \u00b7 \u00b7 \u00b7 , Xi\u22121, X\u2032, Xi+1, \u00b7 \u00b7 \u00b7 , XN). According to the permutation principle, Ti has the same distribution as T under the null hypothesis H0. We can therefore comput the p-value exactly by \ufffd \ufffd \ufffd \ufffd\n<div style=\"text-align: center;\">(b)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0e42/0e42155a-4530-4549-b678-4956bc469062.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The distribution of p-values under the PixelCNN generative model. The inputs are more outside of the training distribution if their p-value distribution has a larger deviation from uniform. Here \u201cclean\u201d means clean test images. From definition, the p-values of clean training images have a uniform distribution.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6c3c/6c3cd970-ecc1-4911-b563-740b22943155.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: An example of how purification works. The above row shows an image from CIFAR-10 test set and various attacking images generated from it. The bottom row shows corresponding purified images. The text below each image is the predicted label given by our ResNet.</div>\nFor CIFAR-10, we provide histograms of p-values for different adversarial examples in Figure 3 and ROC curves of using p-values for detection in Figure 6(a). Note that in the ideal case, the p-value distribution of clean test images should be uniform. The method works especially well for attacks producing larger perturbations, such as RAND, FGSM, and BIM. For DeepFool and CW adversarial examples, we can also observe significant deviations from uniform. As shown in Figure 3(a), the p-value distribution of test images are almost uniform, indicating good generalization of the PixelCNN model.\n# 4 PURIFYING IMAGES WITH PIXELDEFEND\nIn many circumstances, simply detecting adversarial images is not sufficient. It is often critical to be able to correctly classify images despite such adversarial modifications. In this section we introduce PixelDefend, a specific instance of a new family of defense methods that significantly improves the state-of-the-art performance on advanced attacks, while simultaneously performing well against all other attacks.\nAlgorithm 1 PixelDefend\nInput: Image X, Defense parameter \u03f5defend, Pre-trained PixelCNN model pCNN\nOutput: Purified Image X\u2217\n1: X\u2217\u2190X\n2: for each row i do\n3:\nfor each column j do\n4:\nfor each channel k do\n5:\nx \u2190X[i, j, k]\n6:\nSet feasible range R \u2190[max(x \u2212\u03f5defend, 0), min(x + \u03f5defend, 255)]\n7:\nCompute the 256-way softmax pCNN(X\u2217).\n8:\nUpdate X\u2217[i, j, k] \u2190arg maxz\u2208R pCNN[i, j, k, z]\n9:\nend for\n10:\nend for\n11: end for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e66e/e66e0db0-9382-42e8-b093-f0da28107dbc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2271/22712cf4-40e4-40f1-91f1-3586044b8e01.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 5: The bits-per-dimension distributions of purified images from FGSM adversarial examples We tested two purification methods, L-BFGS-B and greedy decoding, the latter of which is used in PixelDefend. A good purification method should give images that have lower bits per dimension compared to FGSM images and ideally similar bits per dimension compared to clean ones.\n<div style=\"text-align: center;\">RETURNING IMAGES TO THE TRAINING DISTRIBUTION</div>\nThe basic idea behind PixelDefend is to purify input images, by making small changes to them in order to move them back towards the training distribution, i.e., move the images towards a high-probability region. We then classify the purified image using any existing classifier. As the example in Figure 4 shows, the purified images can usually be classified correctly. Formally, we have training image distribution p(X), and input image X of resolution I \u00d7 J with X[i, j, k] the pixel at location (i, j) and channel k \u2208{1, \u00b7 \u00b7 \u00b7 , C}. We wish to find an image X\u2217that maximizes p(X) subject to the constraint that X\u2217is within the \u03f5defend-ball of X:\nHere \u03f5defend reflects a trade-off, since large \u03f5defend may change the meaning of X while small \u03f5defend may not be sufficient for returning X to the correct distribution. In practice, we choose \u03f5defend to be some value that overestimates \u03f5attack but still keeps high accuracies on clean images. As in Section 3, we approximate p(X) with the PixelCNN distribution pCNN(X), which is trained on the same training set as the classifier. However, exact constrained optimization of pCNN(X) is computationally intractable. Surprisingly, even gradient-based optimization faces great difficulty on that problem. We found that one advanced methods in gradient-based constrained optimization, L-BFGS-B (Byrd et al., 1995) (we use the scipy implementation based on Zhu et al. (1997)), actually decreases pCNN(X) for most random initializations within the \u03f5defend-ball.\nHere \u03f5defend reflects a trade-off, since large \u03f5defend may change the meaning of X while small \u03f5defend may not be sufficient for returning X to the correct distribution. In practice, we choose \u03f5defend to be some value that overestimates \u03f5attack but still keeps high accuracies on clean images. As in Section 3 we approximate p(X) with the PixelCNN distribution pCNN(X), which is trained on the same training set as the classifier.\nFor efficient optimization, we instead use a greedy technique described in Algorithm 1, which is similar to the greedy decoding process typically used in sequence-to-sequence models (Sutskever et al., 2014). The method is similar to generating images from PixelCNN, with the additional constraint that the generated image should be within an \u03f5defend-ball of a perturbed image. As an autoregressive model, PixelCNN is slow in image generation. Nonetheless, by caching redundant calculation, Ramachandran et al. (2017) proposes a very fast generation algorithm for PixelCNN. In our experiments, adoption of Ramachandran et al. (2017)\u2019s method greatly increases the speed of PixelDefend. For CIFAR-10 images, PixelDefend on average processes 3.6 images per second on one NVIDIA TITAN Xp GPU.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a20/2a2002fe-266c-4164-ba9a-c01670f87e67.png\" style=\"width: 50%;\"></div>\nFigure 6: ROC curves showing the efficacy of using p-values as scores to detect adversarial example For computing the ROC, we assign negative labels to training images and positive labels to adversari images (or clean test images). (a) Original adversarial examples. (b) Purified adversarial example after PixelDefend.\n<div style=\"text-align: center;\">Figure 6: ROC curves showing the efficacy of using p-values as scores to detect adversarial examples For computing the ROC, we assign negative labels to training images and positive labels to adversarial images (or clean test images). (a) Original adversarial examples. (b) Purified adversarial examples after PixelDefend.</div>\nTo show the effectiveness of this greedy method compared to L-BFGS-B, we take the first 10 images from CIFAR-10 test set, attack them by FGSM with \u03f5attack = 8, and purify them with L-BFGS-B and PixelDefend respectively. We used random start points for L-BFGS-B and repeated 100 times for each image. As depicted in Figure 5, most L-BFGS-B attempts failed at minimizing the bits per dimension of FGSM adversarial examples. Because of the rugged gradient landscape of PixelCNN, L-BFGS-B even results in images that have lower probabilities. In contrast, PixelDefend works much better in increasing the probabilities of purified images, although their probabilities are still lower compared to clean ones. In Figure 6 and Figure 7, we empirically show that after PixelDefend, purified images are more likely to be drawn from the training distribution. Specifically, Figure 6 shows that the detecting power of p-values greatly decreases for purified images. For DeepFool and CW examples, purification makes them barely distinguishable from normal samples of the data distribution. This is also manifested by Figure 7, as the p-value distributions of purified examples are closer to uniform. Visually, purified images indeed look much cleaner than adversarially perturbed ones. In Appendix E, we provide sampled purified images from Fashion MNIST and CIFAR-10.\n# 4.2 ADAPTIVE PIXELDEFEND\nOne concern with the approach of purifying images is what happens when we purify a clean image. More generally, we will never know \u03f5attack and if we set \u03f5defend too large for a given attack, then we will modify all images to become the mode image, which would mostly result in misclassifications. One way to avoid this problem is to tune \u03f5defend adaptively based on the probability of the input image under the generative model. In this way, images that already have high probability under the training distribution would have a very low \u03f5defend preventing significant modification, while low probability images would have a high \u03f5defend thus allowing significant modifications. We implemented a very simple thresholding version of this, which sets \u03f5defend to zero if the input image probability is below a threshold value, and otherwise leaves it fixed at a manually chosen setting. In practice, we set this threshold based on knowledge of the set of possible attacks, so strictly speaking, the adaptive version of our technique is no longer attack-agnostic.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/43a1/43a120c7-39de-460a-96ca-6acfad20eaa0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: The distributions of p-values under the PixelCNN model after PixelDefend purification</div>\n<div style=\"text-align: center;\">Table 1: Fashion MNIST (\u03f5attack = 8/25, \u03f5defend = 32)</div>\nNETWORK\nTRAINING\nTECHNIQUE\nCLEAN\nRAND\nFGSM\nBIM\nDEEP\nFOOL\nCW\nSTRONGEST\nATTACK\nResNet\nNormal\n93/93\n89/71\n38/24\n00/00\n06/06\n20/01\n00/00\nVGG\nNormal\n92/92\n91/87\n73/58\n36/08\n49/14\n43/23\n36/08\nResNet\nAdversarial FGSM\n93/93\n92/89\n85/85\n51/00\n63/07\n67/21\n51/00\nAdversarial BIM\n92/91\n92/91\n84/79\n76/63\n82/72\n81/70\n76/63\nLabel Smoothing\n93/93\n91/76\n73/45\n16/00\n29/06\n33/14\n16/00\nFeature Squeezing\n84/84\n84/70\n70/28\n56/25\n83/83\n83/83\n56/25\nAdversarial FGSM\n+ Feature Squeezing\n88/88\n87/82\n80/77\n70/46\n86/82\n84/85\n70/46\nResNet\nNormal + PixelDefend\n88/88\n88/89\n85/74\n83/76\n87/87\n87/87\n83/74\nVGG\nNormal + PixelDefend\n89/89\n89/89\n87/82\n85/83\n88/88\n88/88\n85/82\nResNet\nAdversarial FGSM\n+ PixelDefend\n90/89\n91/90\n88/82\n85/76\n90/88\n89/88\n85/76\nAdversarial FGSM\n+Adaptive PixelDefend\n91/91\n91/91\n88/88\n85/84\n89/90\n89/84\n85/84\n# 4.3 PIXELDEFEND RESULTS\nWe carried out a comprehensive set of experiments to test various defenses versus attacks. Detailed information on experimental settings is provided in Appendix B. All experimental results are summarized in Tab. 1 and Tab. 2. In the upper part of the tables, we show how the various baseline defenses fare against each of the attacks, while in the lower part of the tables we show how our PixelDefend technique works. Each table cell contains accuracies on adversarial examples generated with different \u03f5attack. More specifically, for Fashion MNIST (Tab. 1), we tried \u03f5attack = 8 and 25. The cells in Tab. 1 is formated as x/y, where x denotes the accuracy (%) on images attacked with \u03f5attack = 8, while y denotes the accuracy when \u03f5attack = 25. For CIFAR-10 (Tab. 2), we tried \u03f5attack = 2, 8, and 16, and the cells are formated in a similar way. We use the same \u03f5defend for different \u03f5attack\u2019s to show that PixelDefend is insensitive to \u03f5attack. From the tables we observe that adversarial training successfully defends against the basic FGSM attack, but cannot defend against the more advanced ones. This is expected, as training on simple adversarial examples does not guarantee robustness to more complicated attacking techniques. Consistent with Madry et al. (2017), adversarial training with BIM examples is more successful at preventing a wider spectrum of attacks. For example, it improves the accuracy on strongest attack from 2% to 32% on CIFAR-10 when \u03f5attack = 8. But the numbers are still not ideal even with respect\n<div style=\"text-align: center;\">Table 2: CIFAR-10 (\u03f5attack = 2/8/16, \u03f5defend = 16)</div>\nNETWORK\nTRAINING\nTECHNIQUE\nCLEAN\nRAND\nFGSM\nBIM\nDEEP\nFOOL\nCW\nSTRONGEST\nATTACK\nResNet\nNormal\n92/92/92\n92/87/76\n33/15/11\n10/00/00\n12/06/06\n07/00/00\n07/00/00\nVGG\nNormal\n89/89/89\n89/88/80\n60/46/30\n44/02/00\n57/25/11\n37/00/00\n37/00/00\nResNet\nAdversarial FGSM\n91/91/91\n90/88/84\n88/91/91\n24/07/00\n45/00/00\n20/00/07\n20/00/00\nAdversarial BIM\n87/87/87\n87/87/86\n80/52/34\n74/32/06\n79/48/25\n76/42/08\n74/32/06\nLabel Smoothing\n92/92/92\n91/88/77\n73/54/28\n59/08/01\n56/20/10\n30/02/02\n30/02/01\nFeature Squeezing\n84/84/84\n83/82/76\n31/20/18\n13/00/00\n75/75/75\n78/78/78\n13/00/00\nAdversarial FGSM\n+ Feature Squeezing\n86/86/86\n85/84/81\n73/67/55\n55/02/00\n85/85/85\n83/83/83\n55/02/00\nResNet\nNormal + PixelDefend\n85/85/88\n82/83/84\n73/46/24\n71/46/25\n80/80/80\n78/78/78\n71/46/24\nVGG\nNormal + PixelDefend\n82/82/82\n82/82/84\n80/62/52\n80/61/48\n81/76/76\n81/79/79\n80/61/48\nResNet\nAdversarial FGSM\n+ PixelDefend\n88/88/86\n86/86/87\n81/68/67\n81/69/56\n85/85/85\n84/84/84\n81/69/56\nAdversarial FGSM\n+ Adaptive PixelDefend\n90/90/90\n86/87/87\n81/70/67\n81/70/56\n82/81/82\n81/80/81\n81/70/56\nto BIM attack itself. As in Tab. 2, it only gets 6% on BIM and 8% on CW when \u03f5attack = 16. We also observe that label smoothing, which learns smoothed predictions so that the gradient \u2207XL(X, y) becomes very small, is only effective against simple FGSM attack. Model-agnostic methods, such as feature squeezing, can be combined with other defenses for strengthened performance. We observe that combining it with adversarial training indeed makes it more robust. Actually, Tab. 1 and Tab. 2 show that feature squeezing combined with adversarial training dominates using feature squeezing along in all settings. It also gets good performance on DeepFool and CW attacks. However, for iterative attacks with larger perturbations, i.e., BIM, feature squeezing performs poorly. On CIFAR-10, it only gets 2% and 0% accuracy on BIM with \u03f5attack = 8 and 16 respectively. PixelDefend, our model-agnostic and attack-agnostic method, performs well on different classifiers (ResNet and VGG) and different attacks without modification. In addition, we can see that augmenting basic adversarial training with PixelDefend can sometimes double the accuracies. We hypothesize that the purified images from PixelDefend are still not perfect, and adversarially trained networks have more toleration for perturbations. This also corroborates the plausibility and benefit of combining PixelDefend with other defenses. Furthermore, PixelDefend can simultaneously obtain accuracy above 70% for all other attacking techniques, while ensuring that performance on clean images only declines slightly. Models with PixelDefend consistently outperform other methods with respect to the strongest attack. On Fashion MNIST, PixelDefend methods improve the accuracy on strongest attack from 76% to 85% and 63% to 84%. On CIFAR-10, the improvements are even more significant, i.e., from 74% to 81%, 32% to 70% and 6% to 56%, for \u03f5attack = 2, 8, and 16 respectively. In a security-critical scenario, the weakest part of a system determines the overall reliability. Therefore, the outstanding performance of PixelDefend on the strongest attack makes it a valuable and useful addition for improving AI security.\n# 4.4 END-TO-END ATTACK OF PIXELDEFEND\nA natural question that arises is whether we can generate a new class of adversarial examples targeted specifically at the combined PixelDefend architecture of first purifying the image and then using an existing classifier to predict the label of the purified image. We have three pieces of empirical evidence to believe that such adversarial examples are hard to find in general. First, we attempted to apply the iterative BIM attack to an end-to-end differentiable version of PixelDefend generated by unrolling the PixelCNN purification process. However we found the resulting network was too deep and led to problems with vanishing gradients (Bengio et al., 1994), resulting in adversarial images that were identical to the original images. Moreover, attacking the whole system is very time consuming. Empirically, it took about 10 hours to generate 100 attacking images with one TITAN Xp GPU which failed to fool PixelDefend. Secondly, we found the optimization problem in Eq. (4.1) was not amenable to gradient descent, as indicated in Figure 5. This makes gradient-based attacks especially difficult. Last but not least, the generative model and classifier are trained separately and\nhave independent parameters. Therefore, the perturbation direction that leads to higher probability images has a smaller correlation with the perturbation direction that results in misclassification Accordingly, it is harder to find adversarial examples that can fool both of them together. However, we will open source our codes and look forward to any possible attack from the community.\n# 5 RELATED WORK\nMost recent work on detecting adversarial examples focuses on adding an outlier class detection module to the classifier, such as Grosse et al. (2017), Gong et al. (2017) and Metzen et al. (2017). Those methods require the classification model to be changed, and are thus not model-agnostic. Feinman et al. (2017) also presents a detection method based on kernel density estimation and Bayesian neural network uncertainty. However, Carlini & Wagner (2017a) shows that all those methods can be bypassed. Grosse et al. (2017) also studied the distribution of adversarial examples from a statistical testing perspective. They reported the same discovery that adversarial examples are outside of the training distribution. However, our work is different from theirs in several important aspects. First, the kernel-based two-sample test used in their paper needs a large number of suspicious inputs, while our method only requires one data point. Second, they mainly tested on first-order methods such as FGSM and JSMA (Papernot et al., 2016b). We show the efficacy of PixelCNN on a wider range of attacking methods (see Figure 3), including both first-order and iterative methods. Third, we further demonstrate that random perturbed inputs are also outside of the training distribution. Some other work has focused on modifying the classifier architecture to increase its robustness, e.g., Gu & Rigazio (2014), Cisse et al. (2017) and Nayebi & Ganguli (2017). Although they have witnessed some success, such modifications of models might limit their representative power and are also not model-agnostic. Our basic idea of moving points to higher-density regions is also present in other machine learning methods not specifically designed for handling adversarial data; for example, the manifold denoising method of Hein & Maier (2007), the direct density gradient estimation of Sasaki et al. (2014), and the denoising autoencoders of Vincent et al. (2008) all move data points from low to high-density regions. In the future some of these methods could be adapted to amortize the purification process directly, that is, to learn a purification network.\n# 6 CONCLUSION\nIn this work, we discovered that state-of-the-art neural density models, e.g., PixelCNN, can detect small perturbations with high sensitivity. This sensitivity broadly exists for a large number of perturbations generated with different methods. An interesting fact is that PixelCNN is only sensitive in one direction\u2014it is relatively easy to detect perturbations that lead to lower probabilities rather than higher probabilities. Based on the sensitivity of PixelCNN, we utilized statistical hypothesis testing to verify that adversarial examples lie outside of the training distribution. With the permutation test, we give exact p-values which can be used as a uncertainty measure for detecting outlier perturbations. Furthermore, we make use of the sensitivity of generative models to explore the idea of purifying adversarial examples. We propose the PixelDefend algorithm, and experimentally show that returning adversarial examples to high probability regions of the training distribution can significantly decrease their damage to classifiers. Different from many other defensive techniques, PixelDefend is modelagnostic and attack-agnostic, which means it can be combined with other defenses to improve robustness without modifying the classification model. As a result PixelDefend is a practical and effective defense against adversarial inputs.\n# ACKNOWLEDGMENTS\nThe authors would like to thank Katja Hofmann, Ryota Tomioka, Alexander Gaunt, and Tom Minka for helpful discussions. We also thank Neal Jean, Aditi Raghunathan, Guy Katz, Aditya Grover, Jiaming Song, and Shengjia Zhao for valuable feedback on early drafts of this paper. This research\nwas partly supported by Intel Corporation, TRI, NSF (#1651565, #1522054, #1733686 ) and FL (#2017-158687).\n(#2017-158687). REFERENCES Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994. Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190\u20131208, 1995. Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. arXiv preprint arXiv:1705.07263, 2017a. Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017b. Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pp. 854\u2013863, 2017. Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994. Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017. Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adversarial and clean data are not twins. arXiv preprint arXiv:1704.04960, 2017. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017. Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016. Matthias Hein and Markus Maier. Manifold denoising. In Advances in neural information processing systems, pp. 561\u2013568, 2007. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016. Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences. arXiv preprint arXiv:1703.02914, 2017.\n# REFERENCES\nREFERENCES\nYanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574\u20132582, 2016. Aran Nayebi and Surya Ganguli. Biologically inspired protection of deep networks from adversarial attacks. arXiv preprint arXiv:1703.09202, 2017. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011. Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016a. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372\u2013387. IEEE, 2016b. Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582\u2013597. IEEE, 2016c. Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast generation for convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. Hiroaki Sasaki, Aapo Hyv\u00e4rinen, and Masashi Sugiyama. Clustering via mode seeking by direct estimation of the gradient of a log-density. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 19\u201334. Springer, 2014. Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227\u2013244, 2000. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104\u20133112, 2014. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Florian Tram\u00e8r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017. Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pp. 4790\u20134798, 2016a.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016b. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096\u20131103. ACM, 2008. David Warde-Farley and Ian Goodfellow. 11 adversarial perturbations of deep neural networks. Perturbations, Optimization, and Statistics, pp. 311, 2016. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017a. Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing mitigates and detects carlini/wagner adversarial examples. arXiv preprint arXiv:1705.10686, 2017b. Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4):550\u2013560, 1997.\n# APPENDIX A ON RANDOM PERTURBATIONS\nOne may observe from Figure 3(b) that random perturbations have very low p-values, and thus also live outside of the high density area. Although many classifiers are robust to random noise, it is not a property granted by the dataset. The fact is that robustness to random noise could be from model inductive bias, and there exist classifiers which have high generalization performance on clean images, but can be attacked by small random perturbations. It is easy to construct a concrete classifier that are susceptible to random perturbations. Our ResNet on CIFAR-10 gets 92.0% accuracy on the test set and 87.3% on randomly perturbed test images with \u03f5attack = 8. According to our PixelCNN, 175 of 10000 test images have a bits per dimension (BPD) larger than 4.5, while the number for random images is 9874. Therefore, we can define a new classifier\nwhich will get roughly 92% \u00d7 9825/10000 + 10% \u00d7 175/10000 \u224890.6% accuracy on the test se while only 87.3%\u00d7126/10000+10%\u00d79874/10000 \u224811.0% accuracy on the randomly perturbe images. This classifier has comparable generalization performance to the original ResNet, but wi give incorrect labels to most randomly perturbed images.\n# APPENDIX B EXPERIMENTAL SETTINGS\nAdversarial Training We have tested adversarial training with both FGSM and BIM examples. During training, we take special care of the label leaking problem as noted in Kurakin et al. (2016)\u2014 we use the predicted labels of the model to generate adversarial examples, instead of using the true labels. This prevents the adversarially trained network to perform better on adversarial examples than clean images by simply retrieving ground-truth labels. Following Kurakin et al. (2016), we also sample \u03f5attack from a truncated Gaussian distribution for generating FGSM or BIM adversarial examples, so that the adversarially trained network won\u2019t overfit to any specific \u03f5attack. This is different from Madry et al. (2017), where the authors train and test with the same \u03f5attack. For Fashion MNIST experiments, we randomly sample \u03f5attack from N(0, \u03b4), take the absolute value and truncate it to [0, 2\u03b4], where \u03b4 = 8 or 25. For CIFAR-10 experiments, we follow the same procedure but fix \u03b4 = 8. Feature Squeezing For implementing the feature squeezing defense, we reduce the number of colors to 8 on Fashion MNIST, and use 32 colors for CIFAR-10. The numbers are chosen to make sure color reduction will not lead to significant deterioration of image quality. After color depth reduction, we apply a 2 \u00d7 2 median filter with reflective paddings, since it is reported in Xu et al. (2017b) to be most effective for preventing CW attacks. Models We use ResNet (62-layer) and VGG (16-layer) as classifiers. In our experiments, normally trained networks have the same architectures as adversarially trained networks. Since the images of Fashion MNIST contain roughly one quarter values of those of CIFAR-10, we use a smaller network for classifying Fashion MNIST. More specifically, we reduce the number of feature maps for Fashion MNIST to 1/4 while keeping the same depths. In practive, VGG is more robust than ResNet due to using of dropout layers. The network architecture details are described in Appendix C. For the PixelCNN generative model, we adopted the implementation of PixelCNN++ (Salimans et al., 2017), but modified the output from mixture of logistic distributions to softmax. The feature maps are also reduced to 1/4 for training PixelCNN on Fashion MNIST. Adaptive Threshold We chose the adaptive threshold discussed in Section 4.2 using validation data. We set the threshold at the lowest value which did not decrease the performance of the strongest adversary. For Fashion MNIST, the threshold of bits per dimension was set to 1.8, and for CIFAR-10 the number was 3.2. As a reference, the mean value of bits per dimension for Fashion MNIST test images is 2.7 and for CIFAR-10 is 3.0. However, we admit that using a validation set to choose the best threshold makes the adaptive version of PixelDefend not strictly attack-agnostic.\nAdversarial Training We have tested adversarial training with both FGSM and BIM examples. During training, we take special care of the label leaking problem as noted in Kurakin et al. (2016)\u2014 we use the predicted labels of the model to generate adversarial examples, instead of using the true labels. This prevents the adversarially trained network to perform better on adversarial examples than clean images by simply retrieving ground-truth labels. Following Kurakin et al. (2016), we also sample \u03f5attack from a truncated Gaussian distribution for generating FGSM or BIM adversarial examples, so that the adversarially trained network won\u2019t overfit to any specific \u03f5attack. This is different from Madry et al. (2017), where the authors train and test with the same \u03f5attack. For Fashion MNIST experiments, we randomly sample \u03f5attack from N(0, \u03b4), take the absolute value and truncate it to [0, 2\u03b4], where \u03b4 = 8 or 25. For CIFAR-10 experiments, we follow the same procedure but fix \u03b4 = 8. Feature Squeezing For implementing the feature squeezing defense, we reduce the number of colors to 8 on Fashion MNIST, and use 32 colors for CIFAR-10. The numbers are chosen to make sure color reduction will not lead to significant deterioration of image quality. After color depth reduction, we apply a 2 \u00d7 2 median filter with reflective paddings, since it is reported in Xu et al. (2017b) to be most effective for preventing CW attacks. Models We use ResNet (62-layer) and VGG (16-layer) as classifiers. In our experiments, normally\nFeature Squeezing For implementing the feature squeezing defense, we reduce the number of colors to 8 on Fashion MNIST, and use 32 colors for CIFAR-10. The numbers are chosen to make sure color reduction will not lead to significant deterioration of image quality. After color depth reduction, we apply a 2 \u00d7 2 median filter with reflective paddings, since it is reported in Xu et al. (2017b) to be most effective for preventing CW attacks.\nModels We use ResNet (62-layer) and VGG (16-layer) as classifiers. In our experiments, normally trained networks have the same architectures as adversarially trained networks. Since the images of Fashion MNIST contain roughly one quarter values of those of CIFAR-10, we use a smaller network for classifying Fashion MNIST. More specifically, we reduce the number of feature maps for Fashion MNIST to 1/4 while keeping the same depths. In practive, VGG is more robust than ResNet due to using of dropout layers. The network architecture details are described in Appendix C. For the PixelCNN generative model, we adopted the implementation of PixelCNN++ (Salimans et al., 2017), but modified the output from mixture of logistic distributions to softmax. The feature maps are also reduced to 1/4 for training PixelCNN on Fashion MNIST.\nAdaptive Threshold We chose the adaptive threshold discussed in Section 4.2 using validation data. We set the threshold at the lowest value which did not decrease the performance of the strongest adversary. For Fashion MNIST, the threshold of bits per dimension was set to 1.8, and for CIFAR-10 the number was 3.2. As a reference, the mean value of bits per dimension for Fashion MNIST test images is 2.7 and for CIFAR-10 is 3.0. However, we admit that using a validation set to choose the best threshold makes the adaptive version of PixelDefend not strictly attack-agnostic.\nAPPENDIX C IMAGE CLASSIFIER ARCHITECTURES\u2217\n<div style=\"text-align: center;\">C.1 RESNET CLASSIFIER FOR CIFAR-10 & FASHION MNIST</div>\nNAME\nCONFIGURATION\nInitial Layer\nconv (filter size: 3 \u00d7 3, feature maps: 16 (4), stride size: 1 \u00d7 1)\nResidual Block 1\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 16 (4), stride size: 1 \u00d7 1)\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 16 (4), stride size: 1 \u00d7 1)\nresidual addition\n\u00d710 times\nResidual Block 2\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 32 (8), stride size: 2 \u00d7 2)\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 32 (8), stride size: 1 \u00d7 1)\naverage pooling & padding & residual addition\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 32 (8), stride size: 1 \u00d7 1)\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 32 (8), stride size: 1 \u00d7 1)\nresidual addition\n\u00d79 times\nResidual Block 3\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 64 (16), stride size: 2 \u00d7 2)\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 64 (16), stride size: 1 \u00d7 1)\naverage pooling & padding & residual addition\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 64 (16), stride size: 1 \u00d7 1)\nbatch normalization & leaky relu\nconv (filter size: 3 \u00d7 3, feature maps: 64 (16), stride size: 1 \u00d7 1)\nresidual addition\n\u00d79 times\nPooling Layer\nbatch normalization & leaky relu & average pooling\nOutput Layer\nfc_10 & softmax\nC.2 VGG CLASSIFIER FOR CIFAR-10 & FASHION MNIST\n<div style=\"text-align: center;\">C.2 VGG CLASSIFIER FOR CIFAR-10 & FASHION MNIST</div>\n# C.2 VGG CLASSIFIER FOR CIFAR-10 & FASHION MNIST\nNAME\nCONFIGURATION\nconv (filter size: 3 \u00d7 3, feature maps: 16 (4), stride size: 1 \u00d7 1)\nbatch normalization & relu\n\u00d72 times\nFeature Block 1\nmax pooling (stride size: 2 \u00d7 2)\nconv (filter size: 3 \u00d7 3, feature maps: 128 (32), stride size: 1 \u00d7 1)\nbatch normalization & relu\n\u00d72 times\nFeature Block 2\nmax pooling (stride size: 2 \u00d7 2)\nconv (filter size: 3 \u00d7 3, feature maps: 512 (128), stride size: 1 \u00d7 1)\nbatch normalization & relu\n\u00d73 times\nFeature Block 3\nmax pooling (stride size: 2 \u00d7 2)\nconv (filter size: 3 \u00d7 3, feature maps: 512 (128), stride size: 1 \u00d7 1)\nbatch normalization & relu\n\u00d73 times\nFeature Block 4\nmax pooling (stride size: 2 \u00d7 2) & flatten\nClassifier Block\ndropout & fc_512 (128) & relu\ndropout & fc_10 & softmax\n\u2217The same architecture is used for both CIFAR-10 and Fashion MNIST, but different numbers of feature maps are used. The number of feature maps in parentheses is for Fashion MNIST.\n\u2217The same architecture is used for both CIFAR-10 and Fashion MNIST, but different numbers of feature maps are used. The number of feature maps in parentheses is for Fashion MNIST.\n<div style=\"text-align: center;\">CONFIGURATION</div>\n<div style=\"text-align: center;\">CONFIGURATION</div>\nAPPENDIX D SAMPLED IMAGES FROM PIXELCNN\nAPPENDIX D SAMPLED IMAGES FROM PIXELCNN\nD.1 FASHION MNIST\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/266f/266febe0-4756-4925-97fb-4e3ff2d8d343.png\" style=\"width: 50%;\"></div>\nFigure 8: True and generated images from Fashion MNIST. The upper part shows true images sampled from the dataset while the bottom shows generated images from PixelCNN.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1d3/b1d3ea6c-c93c-4ce2-9e26-2e454fa1e169.png\" style=\"width: 50%;\"></div>\nFigure 9: True and generated images from CIFAR-10. The upper part shows true images sampled from the dataset while the bottom part shows generated images from PixelCNN.\nAPPENDIX E SAMPLED PURIFIED IMAGES FROM PIXELDEFEND\nE.1 FASHION MNIST\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dd8e/dd8ebce3-a928-462d-855c-6783642b211f.png\" style=\"width: 50%;\"></div>\nFigure 10: The upper part shows adversarial images generated from FGSM attack while the bottom part shows corresponding purified images after PixelDefend. Here \u03f5attack = 25 and \u03f5defend = 32.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eaae/eaae8f8f-64d2-42a4-880b-52ae14b9b6c6.png\" style=\"width: 50%;\"></div>\nFigure 11: The upper part shows adversarial images generated from FGSM attack while the bottom part shows corresponding purified images by PixelDefend. Here \u03f5attack = 8 and \u03f5defend = 16.\n",
    "paper_type": "method",
    "attri": {
        "background": "Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. This paper shows that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. This discovery necessitates a new method to purify maliciously perturbed images by moving them back towards the training distribution.",
        "problem": {
            "definition": "The problem addressed is the vulnerability of machine learning models to adversarial examples, which are small, imperceptible perturbations that can mislead classifiers.",
            "key obstacle": "The main challenge is that existing methods fail to effectively handle inputs that lie outside the training distribution, leading to misclassifications."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that adversarial examples are often generated from clean images by adding small perturbations, which can be detected and corrected.",
            "opinion": "The proposed method, PixelDefend, aims to purify adversarially perturbed images by returning them to high-probability regions of the training distribution.",
            "innovation": "PixelDefend differs from existing approaches by leveraging generative models to detect and purify adversarial examples without requiring knowledge of the attack or classifier."
        },
        "method": {
            "method name": "PixelDefend",
            "method abbreviation": "PD",
            "method definition": "PixelDefend is an image purification method that modifies adversarial images to make them more likely under the training distribution.",
            "method description": "The method purifies an input image by making small adjustments to return it to the training distribution.",
            "method steps": [
                "Input an image and set a defense parameter.",
                "For each pixel, determine its feasible range based on the defense parameter.",
                "Compute the softmax probabilities for the current image using the PixelCNN model.",
                "Update each pixel to the value that maximizes the probability under the feasible range."
            ],
            "principle": "This method is effective because it shifts adversarial inputs towards regions of higher likelihood, thereby increasing the chances of correct classification."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on two datasets: Fashion MNIST and CIFAR-10, comparing PixelDefend with various adversarial attacks.",
            "evaluation method": "The performance of PixelDefend was assessed by measuring the accuracy of classifiers on adversarial examples before and after purification."
        },
        "conclusion": "PixelDefend significantly improves the resilience of classifiers against a variety of adversarial attacks, demonstrating state-of-the-art performance while being model-agnostic and attack-agnostic.",
        "discussion": {
            "advantage": "The key advantages of PixelDefend include its ability to effectively purify adversarial examples, improving classification accuracy without modifying the underlying model.",
            "limitation": "One limitation is that the method may not be optimal for all types of attacks, and its effectiveness can vary based on the choice of defense parameters.",
            "future work": "Future research could explore enhancing the adaptive thresholding approach to better tune the defense parameter based on input characteristics."
        },
        "other info": {
            "authors": [
                "Yang Song (Stanford University)",
                "Sebastian Nowozin (Microsoft Research)",
                "Nate Kushman (Microsoft Research)",
                "Taesup Kim (Universit\u00e9 de Montr\u00e9al)",
                "Stefano Ermon (Stanford University)"
            ],
            "acknowledgments": "The authors thank several individuals for helpful discussions and feedback, and acknowledge support from various institutions."
        }
    },
    "mount_outline": [
        {
            "section number": "2.4",
            "key information": "Adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models."
        },
        {
            "section number": "3.4",
            "key information": "PixelDefend is an image purification method that modifies adversarial images to make them more likely under the training distribution."
        },
        {
            "section number": "5.2",
            "key information": "The main challenge is that existing methods fail to effectively handle inputs that lie outside the training distribution, leading to misclassifications."
        },
        {
            "section number": "6.1",
            "key information": "PixelDefend significantly improves the resilience of classifiers against a variety of adversarial attacks, demonstrating state-of-the-art performance while being model-agnostic and attack-agnostic."
        },
        {
            "section number": "7.1",
            "key information": "One limitation is that the method may not be optimal for all types of attacks, and its effectiveness can vary based on the choice of defense parameters."
        }
    ],
    "similarity_score": 0.6320886486404369,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/PixelDefend_ Leveraging Generative Models to Understand and Defend against Adversarial Examples.json"
}