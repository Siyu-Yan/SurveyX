{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2205.12522",
    "title": "Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset",
    "abstract": "Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with humangenerated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show strong correlation results with human evaluations when using XM3600 as golden references for automatic metrics.",
    "bib_name": "thapliyal2022crossmodal3600massivelymultilingualmultimodal",
    "md_text": "# Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, Radu Soricut Google Research\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, Radu Soricut Google Research\n{asht,jponttuset,chillxichen,rsoricut}@google.co\n# Abstract\nResearch in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with humangenerated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show strong correlation results with human evaluations when using XM3600 as golden references for automatic metrics.\narXiv:2205.12522v2\n# 1 Introduction\nImage captioning is the task of automatically generating a fluent natural language description for a given image. This task is important for enabling accessibility for visually impaired users, and is a core task in multimodal research encompassing both vision and language modeling. However, datasets for this task are primarily available in English (Young et al., 2014; Chen et al., 2015; Krishna et al., 2017; Sharma et al., 2018; Pont-Tuset et al., 2020). Beyond English, there are a few datasets such as Multi30K with captions in German (Elliott et al., 2016), French (Elliott et al., 2017) and Czech (Barrault et al., 2018), but they are limited to only a few languages that cover a small fraction of the world\u2019s population, while featuring images that severely under-represent the richness and diversity of cultures from across the globe. These aspects have hindered research on image captioning for a wide variety of languages, and directly hamper deploying accessibility solutions for a large potential audience around the world.\nCreating large training and evaluation datasets in multiple languages is a resource-intensive endeavor. Recent works (Thapliyal and Soricut, 2020) have shown that it is feasible to build multilingual image captioning models trained on machine-translated data (with English captions as the starting point). This work also shows that the effectiveness of some of the most reliable automatic metrics for image captioning, such as CIDEr1 (Vedantam et al., 2015) is severely diminished when applied to translated evaluation sets, resulting in poorer agreement with human evaluations compared to the English case. As such, the current situation is that trustworthy model evaluation can only be based on extensive and expensive human evaluations. However, such evaluations cannot usually be replicated across different research efforts, and therefore do not offer a fast and robust mechanism for model hill-climbing and comparison of multiple lines of research. The proposed XM3600 image captioning evaluation dataset provides a robust benchmark for multilingual image captioning, and can be reliably used to compare research contributions in this emerging field. Our contributions are as follows: (i) for human caption annotations, we have devised a protocol that allows annotators for a specific target language to produce image captions in a style that is consistent across languages; this protocol results in image-caption annotations that are free of direct translation artefacts, an issue that has plagued Machine Translation research for many years and is now well understood (Freitag et al., 2020); (ii) for image selection, we have devised an algorithmic approach to sample a set of 3600 geographically-diverse images from the Open Images Dataset (Kuznetsova et al., 2020), aimed at creating a representative set of images from across the world; (iii) for the resulting XM3600 bench-\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2618/261858c8-1f1d-420e-8d61-018b36e32474.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Source: Porsche Museum, Stuttgart by Brian Solis.</div>\nFigure 1: Sample captions in three different languages (out of 36 \u2013 see full list of captions in Appendix A), showcasing the creation of annotations that are consistent in style across languages, while being free of directtranslation artefacts (e.g. the Spanish \u201cnumber 42\u201d or the Thai \u201cconvertibles\u201d would not be possible when directly translating from the English versions).\nmark, we empirically measure its ability to rank image captioning model variations, and show that it provides high levels of agreement with human judgements, therefore validating its usefulness as a benchmark and alleviating the need for human judgement in the future. Fig. 1 shows a few sample captions for an image in XM3600 that exemplify point (i) above, and Fig. 2 shows the variety of cultural aspects captured by the image sampling approach from point (ii). We provide detailed explanations and results for each of the points above in the rest of the paper. We have released XM3600 under a CC-BY4.0 license at https://google.github.io/crossmodal-3600/.\n# 2 The XM3600 Dataset\nIn this section, we describe the heuristics used for language and image selection, the design of the caption annotation process, caption statistics including quality, and annotator details.\n# 2.1 Language Selection\nIn this section, we describe the heuristic used for selecting the languages. As a first step, we take a quantitative stance and choose 30 languages (L30) roughly based on their percent of web content2. As a second step, we consider an additional five languages (L5) 3 to cover low-resource languages with\nmany native speakers, or major native languages from continents that would not be covered otherwise. The protocol for caption annotation (Sec. 2.3) has been applied to the resulting union of languages plus English, for a total of 36 languages.\n# 2.2 Image Selection\nIn this section, we consider the heuristics used for selecting a geographically diverse set of images. For each of the 36 languages, we select 100 images that, as far as it is possible for us to identify, are taken in an area where the given language is spoken. The images are selected among those in the Open Images Dataset (Kuznetsova et al., 2020) that have GPS coordinates stored in their EXIF metadata. Since there are many regions where more than one language is spoken, and given that some areas are not well covered by Open Images, we design an algorithm that maximizes the percentage of selected images taken in an area in which the assigned language is spoken. This is a greedy algorithm that starts the selection of images by the languages for which we have the smallest pool (e.g. Persian) and processes them in increasing order of their candidate image pool size. Whenever there are not enough images in the area where a language is spoken, we have several back-off levels: (i) selecting from a country where the language is spoken; (ii) a continent where the language is spoken, and, as last resort, (iii) from anywhere in the world. This strategy succeeds in providing our target number of 100 images from an appropriate region for most of the 36 languages except for Persian (where 14 continent-level images are used) and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/985b/985bf167-d494-4d4a-baf7-800f68a6ee9a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: A sample of images in the XM3600 dataset, together with the language for which they have been selected. Overall, the images span regions over 36 different languages and 6 different continents.</div>\nHindi (where all 100 images are at the global level because the in-region images are assigned to Bengali and Telugu). We keep the region each image is selected from as part of our data annotation, so that future evaluations can choose to either evaluate on images relevant to particular regions of interest or on the entire dataset.\n# 2.3 Caption Annotation\nIn this section we detail the design of the caption annotation process. For a massively multilingual benchmark such as XM3600, consistency in the style of the description language is critical, since language can serve multiple communication goals. For a more in-depth discussion on these issues as they relate to image captions, we refer the reader to (Alikhani et al., 2020). We borrow from their terminology, as it identifies coherence relations between image and captions such as VISIBLE, META, SUBJECTIVE, and STORY. The goal for our caption annotation is to generate VISIBLE image captions, i.e., use the target language to formulate a sentence that is intended to recognizably characterize what is visually depicted in the image. One possible approach to generating such captions is to generate them as such in English, and have them translated (automatically, semiautomatically, or manually) into all the other languages. However, this approach results in an English-language bias, as well as other problems\nthat have been already identified in the literature. For instance, translations are often less fluent compared to natural target sentences, due to word order and lexical choices influenced by the source language. The impact of this phenomenon on metrics and modeling has recently received increased attention in the evaluation literature (Toral et al., 2018; Zhang and Toral, 2019; Freitag et al., 2020), and references created in this style are thought to cause overlap-based metrics to favor model outputs that use such unnatural language. We have designed our caption annotation process to achieve two main goals: (i) produce caption annotations in a VISIBLE relation with respect to the image content, and, strongly, create consistency in the description style across languages; (ii) be free of translation artefacts. To achieve this, we use bi-lingual annotators with a requirement to be reading-proficient in English and fluent/native in the target language. As a preliminary step, we train an image-captioning model on English-annotated data, which results in captions in the VISIBLE style of COCO-CAP (Chen et al., 2015). The annotation process proceeds as follows. Each annotation session is done over batches of N = 15 images, using the images selected as described in Sec. 2.2. The first screen shows the N images with their captions in English as generated by the captioning model, and asks the annotators if the captions are EXCELLENT, GOOD, MEDIUM,\nThe annotation process proceeds as follows. Each annotation session is done over batches of N = 15 images, using the images selected as described in Sec. 2.2. The first screen shows the N images with their captions in English as generated by the captioning model, and asks the annotators if the captions are EXCELLENT, GOOD, MEDIUM,\nBAD, or there is NOT-ENOUGH-INFO. We refer to this rating scale as the 5-level quality scale in the subsequent text. We provide the annotators with clear guidelines about what constitutes an EXCELLENT caption, and how to evaluate degradations from that quality. This step forces the annotators to carefully assess caption quality and it primes them into internalizing the style of the captions without the need for complicated and lengthy annotation instructions. The second round shows the same N images again, but one image at a time without the English captions, and the annotators are asked to produce descriptive captions in the target language for each image. In the absence of the English captions, the annotators rely on the internalized caption style, and generate their annotations mostly based on the image content \u2013 with no support from the text modality, other than potentially from memory. Note, however, that we have designed the system to support N annotations simultaneously, and we have empirically selected the value of N as to be large enough to \u201coverwrite\u201d the memory of the annotators with respect to the exact textual formulation of the English captions. As a result, we observe that the produced annotations are free of translation artefacts: See the example in Fig. 1 for Spanish mentioning \u201cnumber 42\u201d, and for Thai mentioning \u201cconvertibles\u201d. We also provide the annotators with an annotation protocol to use when creating the captions, which provides useful guidance in achieving consistent annotations across all the targeted languages. We provide the annotation guidelines in Appendices B and C. For each language, we annotate all 3600 images with captions using replication 2 (two different annotators working independently)4, except Bengali (bn) with replication 1 and Maori (mi) with roughly 1 for 2/3 and 2 for 1/3 of the images, see Table 1.\n# 2.4 Caption Statistics\nIn this section, we take a look at the the basic statistics of the captions in the dataset. Table 1 provides detailed caption statistics, including the number of captions per image and the average number of words and characters per caption. There are a total of 261,375 captions across 36 languages, each image having in the vast majority of cases at least 2\nLan.\nNum.\nReplication\nNum.\nNum.\nId.\nCap.\n1\n2\n3+\nWords\nChars\nar\n7367\n0\n3434\n166\n7.7\n42.2\nbn\n3600\n3600\n0\n0\n11.3\n62.1\ncs\n7207\n15\n3573\n12\n6.5\n39.1\nda\n7264\n0\n3542\n58\n8.7\n48.3\nde\n8643\n0\n2240\n1360\n11.2\n76.5\nel\n7204\n0\n3596\n4\n7.7\n51.4\nen\n7200\n0\n3600\n0\n9.4\n49.5\nes\n8614\n0\n2201\n1399\n9.8\n56.3\nfa\n7245\n0\n3555\n45\n12.7\n59.4\nfi\n7127\n90\n3500\n10\n7.5\n65.2\nfil\n7109\n91\n3509\n0\n12.2\n67.6\nfr\n8562\n0\n2253\n1347\n12.3\n69.6\nhe\n7200\n0\n3600\n0\n11.9\n63.6\nhi\n8503\n0\n2297\n1303\n13.4\n59.9\nhr\n7280\n0\n3553\n47\n9.0\n57.8\nhu\n7216\n0\n3586\n14\n8.5\n60.5\nid\n7126\n74\n3526\n0\n14.3\n93.5\nit\n8471\n0\n2329\n1271\n12.1\n71.8\nja\n7185\n15\n3585\n0\n1.0\n26.0\nko\n7650\n15\n3315\n270\n7.0\n24.7\nmi\n4732\n2483\n1102\n15\n11.7\n55.5\nnl\n8059\n0\n2771\n829\n8.0\n45.9\nno\n7213\n0\n3591\n9\n9.6\n54.3\npl\n7141\n59\n3541\n0\n8.3\n57.6\npt\n7243\n0\n3562\n38\n10.8\n61.7\nquz\n7200\n0\n3600\n0\n5.0\n38.6\nro\n7123\n77\n3523\n0\n15.6\n88.4\nru\n7200\n0\n3600\n0\n9.9\n66.3\nsv\n7273\n1\n3536\n63\n8.1\n46.7\nsw\n7046\n154\n3446\n0\n10.7\n63.0\nte\n7200\n0\n3600\n0\n7.1\n47.4\nth\n7200\n0\n3600\n0\n1.2\n47.9\ntr\n7233\n15\n3538\n47\n9.4\n63.4\nuk\n7215\n0\n3585\n15\n10.0\n65.7\nvi\n7350\n0\n3450\n150\n18.0\n79.3\nzh\n7174\n60\n3508\n32\n1.0\n23.0\nTable 1: Caption statistics: A total of 261,375 captions across 36 languages. We provide the replication stats per language, as well as average number of words (where applicable) and characters.\ncaptions per language. For languages with natural space tokenization, the number of words per caption can be as low as 5 or 6 for some agglutinative languages like Cusco Quechua (quz) and Czech (cs), and as high as 18 for an analytic language like Vietnamese (vi). The number of characters per caption also varies drastically \u2013 from mid-20s for Korean (ko) to mid90s for Indonesian (id) \u2013 depending on the alphabet and the script of the language.\n# 2.5 Caption Quality\nIn this section, we describe the process for ensuring the creation of high quality annotations, and present\nLanguage\nId\n%GOOD+\n%MED+\n%BAD\nArabic\nar\n97.5\n99.3\n0.7\nBengali\nbn\n100.0\n100.0\n0.0\nCzech\ncs\n96.8\n99.0\n1.0\nDanish\nda\n94.0\n99.2\n0.8\nGerman\nde\n98.2\n99.3\n0.7\nGreek\nel\n77.3\n96.0\n3.7\nEnglish\nen\n96.5\n100.0\n0.0\nSpanish\nes\n97.0\n98.3\n1.7\nFarsi\nfa\n94.0\n99.3\n0.7\nFinnish\nfi\n91.5\n98.8\n1.2\nFilipino\nfil\n79.7\n95.3\n4.5\nFrench\nfr\n92.7\n99.2\n0.8\nHebrew\nhe\n82.7\n96.7\n3.0\nHindi\nhi\n92.7\n98.7\n1.3\nCroatian\nhr\n80.7\n98.2\n1.8\nHungarian\nhu\n91.3\n94.8\n5.0\nIndonesian\nid\n90.7\n98.5\n1.5\nItalian\nit\n88.8\n97.7\n2.3\nJapanese\nja\n84.3\n96.3\n3.5\nKorean\nko\n85.2\n99.5\n0.3\nMaori\nmi\n93.5\n98.8\n1.2\nDutch\nnl\n92.8\n98.7\n1.3\nNorwegian\nno\n87.7\n96.7\n3.3\nPolish\npl\n92.2\n97.3\n2.7\nPortuguese\npt\n87.8\n99.5\n0.3\nCusco Quechua\nquz\n83.8\n98.3\n1.7\nRomanian\nro\n90.2\n98.3\n1.7\nRussian\nru\n93.8\n99.5\n0.3\nSwedish\nsv\n92.0\n99.2\n0.8\nSwahili\nsw\n70.0\n98.7\n1.3\nTelugu\nte\n98.7\n99.8\n0.2\nThai\nth\n95.2\n99.2\n0.8\nTurkish\ntr\n97.8\n98.0\n1.2\nUkrainian\nuk\n91.2\n99.2\n0.8\nVietnamese\nvi\n94.3\n97.8\n2.0\nChinese-Simpl.\nzh\n90.2\n97.8\n2.2\nTable 2: Caption quality statistics for the 36 languages. We use the median of three ratings as the aggregated rating for an image-caption pair.\nquality statistics of the annotations produced. In order to ensure quality, the annotation process is initially started with pilot runs on 150 images. The caption ratings are spot checked by the authors to verify that the raters have a good understanding of the rating scale. Further, the generated captions go through a verification round where they are rated by the human annotators on the 5-level quality scale described in Sec.2.3. If the annotations are below the desired quality, we clarify the guidelines and add more examples to provide feedback to the human annotators and then conduct another pilot. This process is repeated until very few low-quality captions are being produced5. After this, for every\nlanguage, we run the main annotation and finally a verification round where we select one caption for 600 randomly selected images and have the annotator pool (per language) rate them on the 5-level quality scale mentioned in Sec. 2.3. The quality scores are presented in Table 2.\nWe use an in-house annotation platform with professional (paid) annotators and quality assurance. Annotators are chosen to be native in the target language whenever possible, and fluent otherwise (for low-resource languages, they are usually linguists that have advanced-level knowledge of that language). All annotators are required to be proficient in English since the instructions and guidelines are given in English.\n# 3 Model Comparison using XM3600\nIn this section, we detail our experiments for comparing several models using human evaluations, and also using XM3600 annotations as gold6 references for automated metrics. For model comparison, we train several multilingual image captioning models with different sizes over different datasets, and compare them on XM3600. As our main result, we show a high level of correlation between model rankings based on human-evaluation scores and the scores obtained using CIDEr (Vedantam et al., 2015) with XM3600 annotations as gold references.\n# 3.1 Datasets\nWe build two multilingual datasets for training, CC3M-35L and COCO-35L, by translating Conceptual Captions 3M (Sharma et al., 2018) and COCO Captions (Chen et al., 2015) to the other 34 languages using Google\u2019s machine translation API7. The remaining language, Cusco Quechua (quz), is not supported by the API8. We use the standard train and validation splits for CC3M9. For COCO, we use the Karpathy split (Karpathy and Fei-Fei, 2014)10.\nModel Name\nDetails\nParameters\nBB+CC\nmT5-base + ViT-B/16 model pretrained on CC3M-35L and finetuned on COCO-35L\nlr=3e\u22124, cp=10k\nBB\nmT5-base + ViT-B/16 model trained on COCO-35L\nlr=1e\u22124, cp=10k\nBg\nmT5-base + ViT-g/14 model trained on COCO-35L\nlr=1e\u22124, cp=10k\nLg\nmT5-large + ViT-g/14 model trained on COCO-35L\nlr=1e\u22124, cp=10k\nTable 3: Model details for all model variants used in our experiments: lr denotes  number of steps in the constant period where the learning rate is constant.\n<div style=\"text-align: center;\">Table 3: Model details for all model variants used in our experiments: lr denotes the learning rate; cp denotes the number of steps in the constant period where the learning rate is constant.</div>\nModel\nLang.\nCIDEr\nCIDEr\nXM3600\nCOCO-DEV\nBB+CC\nen\n0.584\n0.980\nBB\nen\n0.297\n0.856\nBg\nen\n0.337\n0.851\nLg\nen\n0.343\n0.875\nBB+CC\nes\n0.425\n0.962\nBB\nes\n0.194\n0.844\nBg\nes\n0.232\n0.835\nLg\nes\n0.220\n0.859\nBB+CC\nhi\n0.197\n0.759\nBB\nhi\n0.098\n0.671\nBg\nhi\n0.112\n0.718\nLg\nhi\n0.111\n0.624\nBB+CC\nzh\n0.202\n0.748\nBB\nzh\n0.087\n0.659\nBg\nzh\n0.110\n0.695\nLg\nzh\n0.099\n0.656\nTable 4: CIDEr on XM3600 and COCO-DEV for the models over the four languages LCORE (COCO-DEV computed using machine-translated references). Tables 8-11 in the appendix show all the CIDEr values for all the models.\n# 3.2 Models\nIn this section we detail the model architecture we used for the experiments. Our Transformer-based (Vaswani et al., 2017) model architecture for image captioning is shown in Figure 3. On the vision side, each input image is modeled by a Vision Transformer (ViT) (Dosovitskiy et al., 2020; Zhai et al., 2021). The visual features produced by ViT for every patch of the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9072/9072f4a3-adde-48e1-b75d-65223238fca3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The architecture for the family of multilingual image captioning models used in the experiments.</div>\nimage are pooled into a single dense feature vector. On the text side, a Language Identifier (LangId) string is used to specify the language. The LangId string is tokenized and embedded into dense token embeddings, which are merged with the dense visual embeddings as the input to a multi-layer Transformer Image and Text Encoder, followed by a multi-layer Transformer Image and Text Decoder to generate the predicted captions.\nWe take advantage of existing pretrained models to initialize different parts of our model: ViT (Zhai et al., 2021) (green in Fig. 3) and mT5 (Xue et al., 2021) (orange in Fig. 3). We consider different model sizes: mT5-base, mT-large, ViT-B/16, and ViT-g/14, where 16 and 14 are the corresponding patch sizes. We choose three combinations resulting in three different model architectures: mT5base + ViT-B/16, mT5-base + ViT-g/14 and mT5large + ViT-g/14.\n# 3.3 Human Evaluation\nman evaluations comparing the performance of two models. Our main goal in creating XM3600 is to automate the evaluation of massively multilingual image captioning models, by eliminating expensive and timeconsuming human evaluations. Our results indicate that they can be substituted by using the XM3600 annotations as gold references for automated metrics such as CIDEr (Vedantam et al., 2015). To quantify the correlation between the two methods, we train four different models (Tab. 3) and conduct side-by-side human evaluations using the outputs of these models in several languages. We observe strong correlations (Sec. 3.4) between the human evaluations and the CIDEr scores using the XM3600 references. Specifically, we use a randomly selected subset of 600 images from XM3600 for human evaluations, which we call XM600. Image captions generated by a given pairing of models (m1 vs m2, where m1 is considered as the base condition and m2 as the test condition) are compared and rated side-by-side, using a similar pool of annotators as described in Sec. 2.6. Each side-by-side pair (shown in a random per-example left-vs-right order) is rated using a 7-point scale: MUCH-BETTER, BETTER, SLIGHTLYBETTER, SIMILAR, SLIGHTLY-WORSE, WORSE, MUCHWORSE, with a replication factor of 3 (three annotators rate each pair). We denote by WINS the percentage of images where the majority of raters (i.e. 2 out of 3) mark m2\u2019s captions as better, and by LOSSES the percentage of images where the majority of raters mark m2\u2019s captions as worse. We then define the overall side-by-side gain of m2 over m1 as \u2206S\u00d7S = WINS - LOSSES. Conducting the full set of six side-by-side evaluations for each pair of models over the 35 languages would require 210 human evaluation sessions. This is prohibitively expensive and time consuming. Thus, we conduct the full set of six side-by-side evaluations of the pairs of models, on a core set of four languages called LCORE11. We call this set of 24 evaluation sessions OCORE. Furthermore, we also conduct a sparser set of side-by-side evaluations over languages where the CIDEr differences on XM3600 and on COCO-DEV12 indicate 11Chinese-Simplified (zh), English (en), Hindi (hi), Span-\ndisagreement or ambiguity (e.g., opposite sign of the CIDEr differences, and/or small CIDEr differences); this gives us a set of 28 languages called LEXT13. We call the resulting set of 41 evaluation sessions OEXT. The set of all evaluations is called OALL =OCORE + OEXT, which are conducted over the languages LALL = LCORE + LEXT. The choice of which model is called m1 and which model is called m2 is arbitrary in the sideby-side evaluations, since we randomly flip left vs right before presenting the captions to the raters. Hence a single side-by-side evaluation gives two points for the correlation calculations: one with the m1 and m2 assigned as per the actual evaluation conducted, and one more with the m1 and m2 assignment flipped and the \u2206S\u00d7S sign flipped correspondingly.\nm2\nm1\nL.\n\u2206S\u00d7S\n\u2206CIDEr\n\u2206CIDEr\n\u2206CIDEr\nXM600\nXM3600\nCOCO-DEV\nBB+CC\nBB\nen\n\u221238.9\n\u22120.277\n\u22120.287\n\u22120.124\nBB+CC\nBg\nen\n\u221221.5\n\u22120.230\n\u22120.247\n\u22120.129\nBB+CC\nLg\nen\n\u221234.8\n\u22120.246\n\u22120.240\n\u22120.105\nBg\nLg\nen\n\u22123.9\n\u22120.016\n0.007\n0.024\nBg\nBB\nen\n\u221214.0\n\u22120.047\n\u22120.039\n0.005\nLg\nBB\nen\n\u221210.3\n\u22120.031\n\u22120.046\n\u22120.018\nBg\nLg\nes\n\u22121.3\n0.002\n\u22120.012\n0.024\nBg\nBB\nes\n\u22128.4\n\u22120.044\n\u22120.037\n0.008\nLg\nBB\nes\n\u22124.4\n\u22120.045\n\u22120.026\n\u22120.016\nBB+CC\nLg\nes\n\u221229.6\n\u22120.201\n\u22120.205\n\u22120.103\nBB+CC\nBg\nes\n\u221228.8\n\u22120.203\n\u22120.193\n\u22120.127\nBB+CC\nBB\nes\n\u221236.5\n\u22120.246\n\u22120.231\n\u22120.118\nBg\nLg\nhi\n\u22123.0\n\u22120.001\n\u22120.001\n\u22120.094\nBB+CC\nBg\nhi\n\u221229.3\n\u22120.095\n\u22120.084\n\u22120.040\nLg\nBB\nhi\n\u22122.0\n\u22120.012\n\u22120.013\n0.047\nBB+CC\nBB\nhi\n\u221236.5\n\u22120.108\n\u22120.099\n\u22120.088\nBg\nBB\nhi\n\u22125.2\n\u22120.013\n\u22120.015\n\u22120.047\nBB+CC\nLg\nhi\n\u221232.3\n\u22120.096\n\u22120.086\n\u22120.135\nBg\nLg\nzh\n\u22128.9\n\u22120.018\n\u22120.012\n\u22120.039\nBB+CC\nLg\nzh\n\u221230.0\n\u22120.104\n\u22120.103\n\u22120.092\nBB+CC\nBg\nzh\n\u221230.1\n\u22120.086\n\u22120.092\n\u22120.053\nBB+CC\nBB\nzh\n\u221241.2\n\u22120.102\n\u22120.115\n\u22120.089\nBg\nBB\nzh\n\u221216.0\n\u22120.016\n\u22120.023\n\u22120.036\nLg\nBB\nzh\n\u221211.8\n0.002\n\u22120.011\n0.003\nTable 5: Model comparisons over LCORE languages (m2 vs m1). L denotes the target language; \u2206CIDEr XM600 is CIDEr(m2)-CIDEr(m1) on the XM600 dataset, \u2206CIDEr XM3600 on the XM3600 dataset, and \u2206CIDEr COCO-DEV on the COCO validation split with machine-translated references. Table 7 in the appendix shows model comparisons over the LEXT languages.\n13Arabic (ar), Bengali (bn), Croatian (hr), Czech (cs), Danish (da), Dutch (nl), Filipino (fil), Finnish (fi), French (fr), German (de), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Norwegian (no), Persian (fa), Polish (pl), Portuguese (pt), Romanian (ro), Swahili (sw), Swedish (sv), Telugu (te), Thai (th), Turkish (tr), Vietnamese (vi)\nCorrelation\nLang.\nN\n\u2206CIDEr\n\u2206CIDEr\n\u2206CIDEr\nCoefficient\nXM600\nXM3600\nCOCO-DEV\nPearson\nLALL\n130\n0.88\n0.88\n0.68\nSpearman\nLALL\n130\n0.87\n0.92\n0.30\nKendall\nLALL\n130\n0.69\n0.76\n0.21\nPearson\nLEXT\n82\n0.72\n0.84\n\u22120.44\nSpearman\nLEXT\n82\n0.76\n0.84\n\u22120.52\nKendall\nLEXT\n82\n0.54\n0.65\n\u22120.32\nPearson\nLCORE\n48\n0.90\n0.90\n0.89\nSpearman\nLCORE\n48\n0.95\n0.96\n0.86\nKendall\nLCORE\n48\n0.80\n0.81\n0.67\nTable 6: Correlations between side-by-side human evaluations (\u2206S\u00d7S) and CIDEr difference on XM600, XM3600 and the translated COCO validation set. Here N represents the number of points used to compute the correlation coefficient. As noted in Sec. 3.3, each evaluation gives us two points for the correlation calculation.\n# 3.4 Results\nWe present results that show that it is feasible to use the XM3600 annotations as gold references with automated metrics such as CIDEr to compare models in lieu of human evaluations, and that this option is superior to using silver references created via automated translation. Table 5 presents the results for the OCORE set of evaluations on XM600 on the LCORE languages, while Table 7 in the appendix shows the results on the LEXT languages. The reference for the relative strength of each pairing is given by \u2206S\u00d7S, with positive numbers indicating the superiority of m2, and negative numbers indicating a superiority of m1. As can be seen from the table, the model comparisons span a range of model differences, from low \u2206S\u00d7S to high \u2206S\u00d7S. \u2206CIDEr XM600and \u2206CIDEr XM3600 capture similar information, except these numbers are based on CIDEr scores using as references XM600 and XM3600, respectively, while \u2206CIDEr COCO-DEV is based on machine-translated references from the validation split of COCO. We use the results from Table 5 (and Table 7) to compute the correlation between human judgements of the relative quality of the captioning models and the ability of the CIDEr14 metric \u2013 or, rather, of the underlying references used by the metric \u2013 to perform an equivalent task. Table 6 presents the correlation results using three correlation metrics:\nPearson, Spearman, and Kendall. The first section shows the correlations over all the side-by-side evaluations (i.e. OCORE and OEXT); These cover the LCORE and the LEXT languages. The second section shows the correlations for the OEXT covering the LEXT languages. The third section shows the correlations for the OCORE evaluations covering the LCORE languages. We observe that \u2206CIDEr XM3600 is highly correlated with human judgement according to all the correlation metrics (Bonett and Wright, 2000), over all the evaluations OALL, over the OCORE evaluations, and also the OEXT evaluations. Furthermore, for the OEXT evaluations, where most of the instances have opposite signs for \u2206CIDEr COCO-DEV and \u2206CIDEr XM3600, we find that the former is strongly anti-correlated with the human evaluation results while the latter is highly correlated with the human evaluation results. Overall, these results indicate that: (i) we can reliably substitute \u2206CIDEr XM3600 for human evaluations on XM600 when comparing models similar to the ones we used; (ii) the gold XM3600 references are preferable over the silver references obtained from translating COCO captions, in terms of approximating the judgements of the human evaluators15. Based on the results from Table 6, we recommend the use of the XM3600 references as a means to achieve high-quality automatic comparisons between multilingual image captioning models. We have provided the CIDEr scores for XM3600 in 35 languages for all the models, in Tables 8-11 in the Appendix. These can be used as baselines in future work.\n15However, it is unclear whether machine translated references for one particular language in XM3600 translated to all others, are worse than using the human generated references. In particular, we studied the correlations of CIDEr computed using XM3600-en-MT (i.e. the XM3600 English references, machine translated to all the other languages), with the human evaluations. We found that even though the translations have artifacts and disfluencies, CIDEr differences calculated using them show comparable correlations with human judgement observations. We also studied such correlations for machine translated references from German, Greek, Hebrew, Hungarian and Swahili. We found that the correlations are similar and sometimes even a bit higher than using the human generated references. We believe this happens because the rater guidelines weigh informativeness over fluency and the CIDEr metric is also not as sensitive to fluency. Further work is needed to understand the use of translated references as compared to human generated references. We believe that using the human generated references along with the set of machine translated references from all the other languages may provide even stronger correlations and will show greater diversity in the coverage of the image constituents.\nWe introduce the XM3600 dataset as a benchmark for evaluating the performance of multilingual image captioning models. The images in the dataset are geographically diverse, covering all inhabited continents and a large fraction of the world population. We believe this benchmark has the potential to positively impact both the research and the applications of this technology, and enable (among other things) better accessibility for visually-impaired users across the world, including speakers of lowresource languages. The main appeal of this benchmark is that it alleviates the need for extensive human evaluation, which is difficult to achieve across multiple languages and hinders direct comparison between different research ideas and results. We show significant improvements in correlation with human judgements when using the XM3600 dataset as references for automatic metrics, and therefore hope that the adoption of this dataset as a standard benchmark will facilitate faster progress and better comparisons among competing ideas. Our empirical observations are primarily on the full set of side-by-side comparisons over English and three other languages (Spanish, Hindi, Chinese). Due to the similarity in the data collection and the quality control process, we expect similar results to hold for all the other languages as well; we validated this expectation with additional empirical observations covering an additional 28 languages.\n# 5 Limitations\nDue to the high volume of work required and the cost associated with it, we have only targeted 36 languages for our annotation effort; while this number is significantly higher than what is available with previous annotations, it still falls short of including many other languages spoken and written around the world. Additionally, since the L30 languages were selected based on their internet presence, one unintended consequence is that the dataset over-represents European languages. While this is somewhat mitigated by including the L5 low resource languages, building and sharing this dataset can have the unintended effect of perpetuating the issue where computational linguistics and AI work is often unintentionally Eurocentric. Due to the cost and logistical constraints, we have sampled only 100 images for each of the tar-\ngeted languages, which limits the amount of natural and cultural phenomena that these images capture. While the resulting 3600 images have significantly more variety compared to previous datasets, it may still fall short of including important aspects of natural and cultural life from around the globe. Further, there is the possibility of bias in the dataset due to the uneven access to photographic equipment and internet connectivity (For example, several of the images in Fig. 4 seem be shared by people with non-native names in the context of the locales. Thus, these images may have been taken by tourists rather than natives. Further exploration into this aspect of the dataset is important as well). Another limitation is around the absence of translation artifacts in the annotations. We primarily rely on the caption generation process outlined in Sec. 2.3 and on rater quality controls for avoiding translation artifacts. Further, we have performed spot checks on captions in several languages and have not found indications of translation artifacts. Additionally, we have also compared the translations of annotations from another language such as English with generated annotations and verified that the translations show peculiar artifacts and disfluencies which are not seen in the generated annotations. We would also like to emphasize that, while this dataset aims to ameliorate the need for human evaluations for multilingual image captioning, automated evaluation may be less sensitive to small changes, e.g. when comparing highly tuned methods submitted to competitions. This was one of our motivations for comparing models that range from very different (CC+Bg vs Bg/Lg/BB)) to moderately different (BB vs Bg/Lg) and quite similar (Bg vs Lg), and the results from Table 5 show that our approach works well over this range of model differences over LCORE. We also stresstested our approach by focusing the OEXT evaluations on cases where \u2206CIDEr XM3600 or \u2206CIDEr COCO-DEV were quite small or of opposite signs, and the results from Table 7 in the appendix show that \u2206CIDEr XM3600 correlated well with human evaluations even for this harder set of evaluations. However, we caution the reader that there will be cases where human judgement will still be needed. Further, automated evaluations may be biased to methods that explicitly optimize the evaluated metric, e.g. via approaches such as Self-Critical Sequence Training(Rennie et al., 2017). We also note that the model outputs and human\njudgements data used for calculating the correlations would be useful for constructing new automated metrics and validating existing automated metrics for model comparisons. Releasing this data would also allow independent calculation of CIDEr and \u2206S\u00d7S shown in Table 5 and Table 7. However, due to the timelines involved and approvals required, we are not able to release this data with the paper. This may hamper the reproducibility of these computations. The approach to data collection and annotation of COCO-CAP (Chen et al., 2015) and CC3M (Sharma et al., 2018) upholds rigorous privacy and ethics standards, such as the avoidance of offensive content and exposure of personal identification data. This significantly mitigates but does not completely eliminate the risks that the captioning models we train would produce such information. Similarly, the XM3600 dataset mitigates such risks by adopting a defense-in-depth approach: 1) The annotations have been produced in-house and have been quality controlled, while the images used have been vetted to be appropriate for the intended use. 2) Further, the machine translations of the annotations have been scanned with an automated tool to detect personally identifiable information. 3) The machine translations of the annotations have been spot-checked by the authors. Overall, in spite of the above limitations, we believe that this dataset is a significant step toward ameliorating language and geographic bias, and that it should be used for advancing image captioning research over a wider variety of images and languages.\n# 6 Acknowledgements\nWe would like to thank the anonymous reviewers for providing feedback which led to several improvements such as: 1) A discussion about correlations of human judgement with the machine translations of the XM3600 references; 2) A discussion about the possibility of releasing model outputs and human evaluation data which may help with reproducibility and also help evaluation of existing and new automated metrics over LALL.\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. 2020. Connecting vision and language with localized narratives.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In CVPR.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2021. Scaling vision transformers.\nMike Zhang and Antonio Toral. 2019. The effect of translationese in machine translation test sets. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 73\u2013 81, Florence, Italy. Association for Computational Linguistics.\n# A Additional Caption Examples\nFigure 4 displays the captions in the 36 languages covered in XM3600 for the same image as in Figure 1.\n# B Instructions for Rating Captions\nThe following instructions are provided to annotators for rating captions:\nThis task involves rating captions. To guide your ratings, imagine that you are describing the image to a visually impaired friend, then consider: how well does the caption describe the image to this friend? Use the following scale for judging the quality of the captions (for borderline cases, use the lower rating): \u2022 BAD: The caption has one or more of the following issues: a). Caption misses the main topic of the image. b). Caption has major grammatical errors (such as being incomplete, words in wrong order, etc). Please ignore capitalization of words and punctuation. c). Caption violates the \u2018No Hallucination\u2019 rule by mentioning objects, activities, or relationships that are definitely not in the image. Note: Apply the \u2018No-Hallucination\u2019 rule only when you are certain that an object/activity/relationship is definitely not implied by the image (see the examples below). \u2022 MEDIOCRE: The caption may capture some objects and activities but misses crucial information (related to activity, important objects/persons in the scene, important modifiers, etc.) \u2022 GOOD: The caption explains most of the main objects, activities, and their relationships in the image. \u2022 EXCELLENT: The caption covers well the whole image, including all the main objects, activities, and their relationships. \u2022 NOT ENOUGH INFORMATION: Not enough information to evaluate the caption quality. Please try to use one of the four categories above as much as possible. Assume that any missing information is favorable to the caption rather than against it.\n# C Instructions for Generating Captions\n# The following instructions are provided to annotators for generating captions:\nTo guide your caption generation, imagine that you are describing the image to a visually impaired friend. The caption should explain the whole image, including all the main objects, activities, and their relationships. The objects should be named as specifically as practical: For example when describing a young boy in a picture, \u201cyoung boy\u201d is preferred over \u201cyoung child\u201d, which in turn is preferred over \u201cperson\u201d. Note: the goal is to generate captions that would be labeled as \u201cExcellent\u201d under the Rating guidelines above, but raters should not copy captions from the first phase. We want the raters to generate the captions on their own.\n\ufffd\ufffd\ufffd\u0631\u0627\u062a \ufffd\ufffd\ufffd\u0636 \ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\u0631 \u0631\ufffd\ufffd\u062f\u064a \ufffd\ufffd\ufffd\u0631\u0629\n\ufffd\ufffd\ufffd\u0631\u0627\u062a \u064b\ufffd\ufffd\ufffd\u0623 \u0648\ufffd\ufffd\ufffd\u0643 \u0627\ufffd\ufffd\ufffd\ufffd\u0631\u0627\u062a \ufffd\ufffd\ufffd\u0631\u0636 \ufffd\ufffd\u0623 \ufffd\ufffd \ufffd\ufffd \ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\u0648 \ufffd\ufffd\ufffd\ufffd\u0629 \ufffd\ufffd\ufffd\u0631\u0629\n\ufffd\ufffd\ufffd\ufffd\ufffd\u0648 \ufffd\ufffd\ufffd\ufffd\ufffd \u0623\ufffd\ufffd\u0649\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0647 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0647 \ufffd\ufffd \u062f\u0631 \u0627\ufffd\ufffd\ufffd\u062a \u0627\u06cc \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0627 \ufffd\ufffd\ufffd\u0627\u062f\u06cc\ufffd\ufffd\u0627 \ufffd\ufffd\u0647 \ufffd\ufffd\u0631\u06a9 \ufffd\ufffd\u0632\u0647 \ufffd\ufffd \u062f\u0631 \ufffd\ufffd \ufffd\ufffd\ufffd\u0631 \u062f\u0631 \ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\u0631 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \u0627\ufffd\ufffd\ufffd\u0631\u062a \ufffd\ufffd\u062f\u0631\u0648\u06cc \ufffd\ufffd\ufffd\ufffd\ufffd\n\u05e8\u05db\u05d1 \u05e4\u05d5\u05e8\u05e9\u05d4 \u05d5\u05d9\u05e0\u05d8\u05d2' \u05e2\u05dd \u05d2\u05d2 \u05e0\u05e4\u05ea\u05d7 \u05d1\u05e6\u05d1\u05e2 \u05db\u05e1\u05d5\u05e3 \u05e2\u05d5\u05de\u05d3 \u05d1\u05ea\u05e6\u05d5\u05d2\u05d4 \u05dc\u05d9\u05d3\n.\u05e8\u05db\u05d1\u05d9\u05dd \u05e0\u05d5\u05e1\u05e4\u05d9\u05dd\n\u05de\u05db\u05d5\u05e0\u05d9\u05d5\u05ea \u05e1\u05e4\u05d5\u05e8\u05d8 \u05d7\u05d3\u05e9\u05e0\u05d9\u05d5\u05ea \u05e2\u05d5\u05de\u05d3\u05d5\u05ea \u05d1\u05ea\u05e6\u05d5\u05d2\u05d4\nLanguage Name\nLanguage ID\nCaption 1\nCaption 2\nArabic\nBengali\nChinese-Simplified\nCroatian\nCusco Quechua\nCzech\nDanish\nDutch\nEnglish\nPersian\nFilipino\nFinnish\nFrench\nGerman\nGreek\nHebrew\nHindi\nHungarian\nIndonesian\nItalian\nJapanese\nKorean\nMaori\nNorwegian\nPolish\nPortuguese\nRomanian\nRussian\nSpanish\nSwahili\nSwedish\nTelugu\nThai\nTurkish\nUkrainian\nVietnamese\nar\nbn\n\u098f\u0995\ufffd \u09b9\u09c7\u09b2\u09b0 \u09ae\u09c7\u09a7l \u09b8\u09be\u09a6\u09be \u0993 \u09bf\u09b8\u09b2\u09ad\u09be\u09b0 \u09b0\u09c7\u0999\u09b0 \ufffd\ufffd\u09be\u099fj \u09b8 \u0995\u09be\u09b0 \u09b0\u09be\u0996\u09be \u0986\u09c7\u099b\nzh\n\u5728\u5c55\u5385\u2fa5\u505c\u9760\u7740\u2f00\u6392\u2f7c\u7237\u2ecb\u6b63\u5728\u5c55\u51fa\uff0c\u79bb\u5f97\u6700\u8fd1\u7684\u662f\u8fd9\u2f00\u8f86\u7070\n\u2f8a\u7684\n\u2ecb\u5c55\u4e2d\u90fd\u662f\u4fdd\u65f6\u6377\u655e\u7bf7\u8dd1\u2ecb\nhr\nspo\ufffdski automobil sive metalik boje tipa kabriolet izlo\u017een u\nmuzeju\ntrka\u010di auti na izlo\u017ebi, sivi Porsche s brojem 42\nquz\nhuk uqi karru mana tichuyuq\n\u00d1awpaq phawana carrukuna musuqllana k'anchasqa\nqhawakushan\ncs\nhistorick\u00e1 z\u00e1vodn\u00ed auta\nretro modely Porsche na v\u00fdstav\u011b aut\nda\nEn \u00e6ldre lavere s\u00f8lvfarvet racerbil p\u00e5 en udstilling med andre\nracerbiler\nUdstilling med veteranbiler\nnl\nklassieke raceauto's op een rij een museum\nKlasieke race auto's op een rij in een showroom\nen\nThe branded classic cars in a row at display.\nA vintage spo\ufffds car in a showroom with many other vintage\nspo\ufffds cars.\nfa\n\ufffdl\nmga klasikong sasakyan na nakadisplay sa tindahan ng kotse\nmagkakatabing mga lumang kotse\n\ufffd\nAntiikkisia urheiluautoja n\u00e4y\ufffdelyss\u00e4\nfr\nUne s\u00e9rie de voitures de course vintage expos\u00e9 dans un\nmus\u00e9e\nvoiture ancienne de course grise num\u00e9rot\u00e9 42 expos\u00e9 au c\u00f4t\u00e9\nd'autres voiture en int\u00e9rieur\nde\nVerschiedenfarbige Rennwagen mit Nummern auf einer\nAutoausstellung\nEin silber-metallic Cabrio Oldtimer Porsche 718 steht auf\neinem Flachen Podest mit anderen Oldtimer dahinter im\nPorsche Museum in Stu\ufffdga\ufffd\nel\n\u03ad\u03ba\u03b8\u03b5\u03c3\u03b7 \u03c1\u03b5\u03c4\u03c1\u03cc \u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u03b1\u03c5\u03c4\u03bf\u03ba\u03b9\u03bd\u03ae\u03c4\u03c9\u03bd\n\u039a\u03bb\u03b1\u03c3\u03b9\u03ba\u03cc \u03b3\u03ba\u03c1\u03b9 \u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03c4\u03b9\u03ba\u03cc \u03ba\u03b1\u03bc\u03c0\u03c1\u03b9\u03bf\u03bb\u03ad \u03b1\u03c5\u03c4\u03bf\u03ba\u03af\u03bd\u03b7\u03c4\u03bf \u03c3\u03b5 \u03ad\u03ba\u03b8\u03b5\u03c3\u03b7\n\u03b4\u03af\u03c0\u03bb\u03b1 \u03c3\u03b5 \u03ac\u03bb\u03bb\u03b1.\nhe\nhi\n\u0938\u095e\u0947\u0926 \u092b\u0936\ufffd \u092a\u0930 \u0920\u0948\u0930\u0940 \u0915\u094b\u0930\u0947 \u0930\u0902\u0917 \u0915\ufffd \u0917\u093e\u0921\u0940 \u0914\u0930 \u0909\u0938\u0915\u0947 \u092a\u0940\u091b\u0947 \u0914\u0930 \u092d\u0940 \u0917\u093e\ufffd\u095c\u092f\u093e\u0902\n\u0939\u0949\u0932 \u092e\ufffd \u0932\u0917\u0940 \u0928\u090f \u095b\u092e\u093e\u0928\u0947 \u0915\ufffd \u0917\u093e\ufffd\u095c\u092f\u093e\ufffd \u0915\u093e \ufffd\ufffd\u092f .\nhu\nVeter\u00e1n spo\ufffdaut\u00f3k egym\u00e1s melle\ufffd \u00e1llnak egy\nki\u00e1ll\u00edt\u00f3teremben\nKlasszikus spo\ufffdaut\u00f3k ki\u00e1ll\u00edtva egy m\u00fazeumban\nid\nSebuah pameran mobil konve\ufffdibel klasik di mana terdapat\nderetan mobil yang diparkir di dalam ruangan\nderetan beberapa mobil balap konve\ufffdibel klasik Porche\ndipajang di museum mobil klasik\nit\nauto spo\ufffdive d'epoca in esposizione in un salone dalle mura\nbianche\nauto da competizione di qualche decennio fa esposte a salone\ndell'auto\nja\n\u9280\u2f8a\u306e\u30e1\u30bf\u30ea\u30c3\u30af\u30fb\u30b9\u30dd\u30fc\u30c4\u30ab\u30fc\u3068\u3001\u4ed6\u306e\u5c55\u793a\u3055\u308c\u3066\u3044\u308b\u2f9e\n\u30dd\u30eb\u30b7\u30a7\u30df\u30e5\u30fc\u30b8\u30a2\u30e0\u306b\u5c55\u793a\u3055\u308c\u3066\u3044\u308b\u8907\u6570\u306e\u30aa\u30fc\u30d7\u30f3\u30ab\u30fc\nko\n\ub0b4\ubd80\uc5d0 \ud3ec\ub974\uc250 \uc790\ub3d9\ucc28\uac00 \uc9c4\uc5f4\ub418\uc5b4 \uc788\ub2e4\n\ud3ec\ub974\uc250 \uc2a4\ud3ec\uce20\uce74 \uc804\uc2dc\uc7a5\uc5d0 \uc9c4\uc5f4\ub41c \ubc88\ud638\uac00 \ub2ec\ub9b0 \ub2e4\uc591\ud55c \ucc28\ub4e4\nmi\nEtahi motuka tawhito kei roto i tetahi whare\nno\nklassiske spo\ufffdsbiler p\u00e5 rad presente\ufffd i et galleri\nLav klassisk spo\ufffdsbil i s\u00f8lv ved siden av andre biler i\nutstillingshall\npl\nRetro samochody spo\ufffdowe na wystawie\nspo\ufffdowe samochody rajdowe na wystawie\npt\nexposi\u00e7\u00e3o de carros porshe com o modelo 718 a frente\nCarros modernos en\ufffdleirados em uma exposi\u00e7\u00e3o\nro\nma\u0219ini de juc\u0103rie spo\ufffd colorate diferit aranjate la expozi\u021bie\npe ra\ufffdul alb\nmasina clasica gri decapotabila porsche cu alte masini similare\nparcate in spatele ei in interiorul unei camere cu pareti si\npodea albe\nru\n\u0441\u0435\u0440\u0435\u0431\u0440\u0438\u0441\u0442\u044b\u0439 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u044b\u0439 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c \u043c\u0430\u0440\u043a\u0438 \u041f\u043e\u0440\u0448\u0435 \u0441\n\u043a\u0440\u0430\u0441\u043d\u044b\u043c \u0441\u0430\u043b\u043e\u043d\u043e\u043c \u043d\u0430 \u0432\u044b\u0441\u0442\u0430\u0432\u043a\n\u0421\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u044b\u0439 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c \u0446\u0432\u0435\u0442\u0430 \u043c\u0435\u0442\u0430\u043b\u043b\u0438\u043a \u0441 \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u043c\n\u0432\u0435\u0440\u0445\u043e\u043c \u0432 \u0432\u044b\u0441\u0442\u0430\u0432\u043e\u0447\u043d\u043e\u043c \u0437\u0430\u043b\u0435 \u043d\u0430 \u0444\u043e\u043d\u0435 \u0434\u0440\u0443\u0433\u0438\u0445, \u0441\u0442\u043e\u044f\u0449\u0438\u0445 \u0432 \u0440\u044f\u0434,\n\u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u044b\u0445 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u0439\nes\nAutom\u00f3vil cl\u00e1sico depo\ufffdivo en exhibici\u00f3n de autom\u00f3viles de\ngaler\u00eda\nCoche peque\u00f1o de carreras color plateado con el n\u00famero 42\nen una exhibici\u00f3n de coches\nsw\nmagari matatu ya klasiki yaliyopangwa kwa mfululizo\nMagari wa muundo wa zamani wa kipekee yakiwa\nyamepangwa mfululizo kwa kando kando yakiwa kwenye\nmaonyesho\nsv\nFlera klassiska spo\ufffdbilar i rad inomhus\nTre porsche spo\ufffdmodell utan tak i ljus utst\u00e4llningslokal med\n\ufffder bilar i bakgrunden\nte\n\u0c35B\u0c38\" \u0c09\u0c28\ufffd \ufffd\u0c32\ufffdC \u0c064(\ufffd \ufffdB\ufffd\n\u0c05\u0c02\u0c17\ufffd\" \u0c2a\ufffd\u0c26\u0c30\ufffd\u0c28\" \u0c35B\u0c38\ufffd \ufffd\ufffdv \u0c28 `\u0c24\u0c28 \ufffd\u0c30\ufffd \ufffd\u0c15l \ufffd\u0c24\ufffd\u0c02\nth\n\u0e23\u0e16\u0e40\u0e1b\ufffd\u0e14\u0e1b\u0e23\u0e30\u0e17\u0e38\u0e19\u0e2b\u0e25\u0e32\u0e22\u0e2a\u0e35\u0e08\u0e2d\u0e14\u0e40\u0e23\ufffd\u0e22\u0e07\u0e01\u0e31\u0e19\u0e43\u0e19\u0e17\u0e35\ufffd\u0e08\u0e31\u0e14\u0e41\u0e2a\u0e14\u0e07\n\u0e23\u0e16\u0e41\u0e02\u0e48\u0e07\u0e27\ufffd\u0e19\u0e40\u0e17\u0e08\u0e08\u0e2d\u0e14\u0e40\u0e23\ufffd\u0e22\u0e07\u0e01\u0e31\u0e19\u0e2b\u0e25\u0e32\u0e22\u0e04\u0e31\u0e19\u0e43\u0e19\u0e07\u0e32\u0e19\u0e08\u0e31\u0e14\u0e41\u0e2a\u0e14\u0e07\ntr\nGalerideki eski Porsche yar\u0131\u015f arabalar\u0131\nKapal\u0131 alanda duran k\u0131rm\u0131z\u0131 koltuklu gri renkli \u00fczeri a\u00e7\u0131k bir spor\naraba ve arkas\u0131nda duran ona benzer ba\u015fka spor arabalar\nuk\n\u0441\u0456\u0440\u0438\u0439 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0438\u0439 \u0440\u0435\u0442\u0440\u043e \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0456\u043b\u044c \u0432 \u0430\u0432\u0442\u043e\u0441\u0430\u043b\u043e\u043d\u0456 \u043d\u0430 \u0442\u043b\u0456\n\u0442\u0430\u043a\u0438\u0445 \u0456\u043d\u0448\u043e\u0433\u043e \u043a\u043e\u043b\u044c\u043e\u0440\u0443\n\u0421\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0456 \u0440\u0435\u0442\u0440\u043e \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0456\u043b\u0456 \u041f\u043e\u0440\u0448\u0435 \u0432 \u043c\u0443\u0437\u0435\u0457\nvi\nnh\u1eefng chi\u1ebfc xe mui tr\u1ea7n sang tr\u1ecdng \u0111\u01b0\u1ee3c tr\u01b0ng b\u00e0y c\u1ea1nh\nnhau trong m\u1ed9t khu tri\u1ec3n l\u00e3m\nm\u00f4 h\u00ecnh xe h\u01a1i mua tr\u1ea7n nhi\u1ec1u m\u00e0u tr\u00ean n\u1ec1n g\u1ea1ch tr\u1eafng ph\u00eda\ntr\u01b0\u1edbc c\u00f3 t\u1ea5m b\u1ea3ng \u0111en ch\u1eef tr\u1eafng\nWe outline here a procedure that you should try and follow when writing your image caption. Note that not all these steps may be applicable for all images, but they should give you a pretty good idea of how to organize your caption. We will make use of the first image in the table below (the one with the young girl smiling) Note: It is acceptable to make assumptions that are reasonable as long as they don\u2019t contradict the information in the image (eg: in the second image below, we use \u201cfamilies\u201d in captions 1 and 3 because there seems to be a mix of children and adults though it is not perfectly clear. So it is a reasonable assumption to make and nothing in the image contradicts it. However it is also ok to use \u201cpeople\u201d.) 1. Identify the most salient objects(s)/person(s) in the image; use the most informative level to refer to something (i.e., \u201cgirl\u201d rather than \u201cchild\u201d or \u201cperson\u201d); in the example image: \u201cgirl\u201d 2. Identify the most salient relation between the main objects; example \u201cgirl standing in front of the whiteboard\u201d 3. Identify the main activity depicted; in the example image: \u201csmiling\u201d as an activity (note that this can also be an attribute of the girl), or \u201cstanding\u201d as an activity 4. Identify the most salient attributes of the main object(s)/person(s)/activity(es); in the example image: \u201csmiling\u201d and \u201cyoung\u201d as attributes for the girl 5. Identify the background/context/environment in which the scene is placed; in the example image: \u201cclassroom\u201d 6. Put everything together from steps 1-5 above; for the example image: \u201ca smiling girl standing in a classroom\u201d, or \u201ca young girl smiling in a classroom\u201d.\n# D Detailed Results for Model Comparison\nm2\nm1\nL.\n\u2206S\u00d7S\n\u2206CIDEr\n\u2206CIDEr\n\u2206CIDEr\nXM600\nXM3600\nCOCO-DEV\nLg\nBB\nar\n\u22121.2\n\u22120.003\n\u22120.003\n0.055\nLg\nBB\nbn\n\u22123.5\n\u22120.025\n\u22120.026\n0.039\nBg\nBB\ncs\n\u22125.2\n\u22120.031\n\u22120.016\n0.013\nLg\nBB\ncs\n\u22122.7\n\u22120.012\n0.002\n0.029\nBg\nBB\nda\n\u22126.9\n\u22120.021\n\u22120.029\n0.048\nBg\nLg\nda\n2.7\n\u22120.009\n\u22120.003\n0.029\nLg\nBB\nda\n\u221213.3\n\u22120.012\n\u22120.026\n0.018\nBg\nBB\nde\n\u221212.6\n\u22120.014\n\u22120.026\n0.030\nBg\nLg\nde\n\u22121.5\n\u22120.007\n\u22120.008\n0.037\nLg\nBB\nde\n\u22129.6\n\u22120.007\n\u22120.018\n\u22120.006\nLg\nBB\nel\n\u22125.1\n\u22120.005\n0.002\n0.063\nLg\nBB\nfa\n\u221211.1\n\u22120.011\n\u22120.003\n0.027\nLg\nBB\nfi\n\u22120.3\n\u22120.007\n\u22120.006\n0.008\nLg\nBB\nfil\n\u22123.2\n\u22120.024\n\u22120.004\n0.020\nLg\nBB\nfr\n\u22122.0\n\u22120.030\n\u22120.015\n0.011\nBg\nBB\nfr\n\u22123.0\n\u22120.022\n\u22120.024\n0.001\nLg\nBB\nhe\n1.7\n0.005\n0.001\n0.025\nLg\nBB\nhr\n\u22124.5\n\u22120.007\n0.002\n0.030\nBg\nBB\nhr\n\u22128.4\n\u22120.019\n\u22120.023\n0.014\nLg\nBB\nhu\n\u22124.7\n\u22120.005\n\u22120.009\n0.027\nLg\nBB\nid\n\u22124.7\n\u22120.031\n\u22120.018\n0.004\nLg\nBB\nit\n\u22126.2\n\u22120.011\n\u22120.015\n0.010\nLg\nBB\nja\n\u221210.8\n\u22120.013\n\u22120.021\n\u22120.006\nLg\nBB\nko\n\u22122.0\n\u22120.025\n\u22120.018\n0.045\nBg\nBB\nnl\n\u22127.7\n0.003\n\u22120.016\n0.009\nBg\nLg\nnl\n0.2\n0.001\n\u22120.003\n0.007\nLg\nBB\nnl\n\u22126.2\n0.002\n\u22120.013\n0.002\nLg\nBB\nno\n\u221210.3\n\u22120.043\n\u22120.033\n0.007\nBg\nBB\npl\n\u22123.5\n\u22120.022\n\u22120.023\n0.015\nLg\nBB\npl\n\u22124.4\n0.002\n\u22120.007\n0.019\nLg\nBB\npt\n\u221210.8\n\u22120.027\n\u22120.026\n\u22120.011\nLg\nBB\nro\n\u22125.6\n\u22120.027\n\u22120.017\n\u22120.001\nLg\nBB\nsv\n\u22128.1\n\u22120.028\n\u22120.035\n0.003\nBg\nLg\nsv\n\u22125.4\n0.004\n0.004\n0.031\nBg\nBB\nsv\n\u22129.3\n\u22120.024\n\u22120.032\n0.035\nLg\nBB\nsw\n\u22122.2\n\u22120.016\n0.007\n0.040\nLg\nBB\nte\n\u22121.3\n0.008\n\u22120.002\n0.037\nLg\nBB\nth\n\u22126.7\n\u22120.031\n\u22120.016\n0.028\nLg\nBB\ntr\n\u22125.1\n\u22120.026\n\u22120.016\n0.029\nBg\nBB\nvi\n\u22124.5\n0.000\n0.000\n0.058\nLg\nBB\nvi\n3.2\n0.015\n0.008\n0.082\nTable 7: Model comparison over the LEXT languages (m2 vs m1). L denotes the target language; \u2206CIDEr XM600 is CIDEr(m2)-CIDEr(m1) on the XM600 dataset, \u2206CIDEr XM3600 on the XM3600 dataset, and \u2206CIDEr COCO-DEV on the COCO validation split with machine-translated references.\nLang.\nCIDEr\nCIDEr\nXM3600\nCOCO-DEV\nar\n0.227\n0.649\nbn\n0.200\n0.682\ncs\n0.313\n0.575\nda\n0.329\n0.877\nde\n0.224\n0.735\nel\n0.199\n0.830\nen\n0.584\n0.980\nes\n0.425\n0.962\nfa\n0.311\n0.898\nfi\n0.177\n0.487\nfil\n0.353\n1.007\nfr\n0.410\n0.957\nhe\n0.230\n0.650\nhi\n0.197\n0.759\nhr\n0.224\n0.607\nhu\n0.175\n0.551\nid\n0.307\n1.088\nit\n0.321\n0.902\nja\n0.254\n0.963\nko\n0.288\n0.862\nmi\n0.405\n1.175\nnl\n0.441\n0.796\nno\n0.385\n0.856\npl\n0.236\n0.578\npt\n0.380\n0.964\nro\n0.188\n0.832\nru\n0.194\n0.675\nsv\n0.370\n0.848\nsw\n0.319\n0.796\nte\n0.196\n0.520\nth\n0.418\n0.929\ntr\n0.232\n0.668\nuk\n0.189\n0.653\nvi\n0.336\n1.150\nzh\n0.202\n0.748\nTable 8: CIDEr on XM3600 and COCO-DEV for the best performing model BB+CC on all 35 languages. (COCO-DEV computed using machine-translated references).\nLang.\nCIDEr\nCIDEr\nXM3600\nCOCO-DEV\nar\n0.121\n0.573\nbn\n0.139\n0.623\ncs\n0.157\n0.500\nda\n0.195\n0.736\nde\n0.138\n0.612\nel\n0.119\n0.777\nen\n0.337\n0.851\nes\n0.232\n0.835\nfa\n0.180\n0.816\nfi\n0.098\n0.442\nfil\n0.215\n0.951\nfr\n0.226\n0.835\nhe\n0.121\n0.584\nhi\n0.112\n0.718\nhr\n0.111\n0.509\nhu\n0.107\n0.491\nid\n0.187\n1.000\nit\n0.184\n0.783\nja\n0.154\n0.900\nko\n0.169\n0.787\nmi\n0.261\n1.121\nnl\n0.235\n0.697\nno\n0.242\n0.768\npl\n0.125\n0.499\npt\n0.222\n0.852\nro\n0.105\n0.737\nru\n0.111\n0.588\nsv\n0.221\n0.717\nsw\n0.191\n0.702\nte\n0.112\n0.497\nth\n0.253\n0.856\ntr\n0.141\n0.636\nuk\n0.091\n0.631\nvi\n0.190\n0.964\nzh\n0.110\n0.695\nTable 9: CIDEr on XM3600 and COCO-DEV for the model Bg on all 35 languages. (COCO-DEV computed using machine-translated references).\nLang.\nCIDEr\nCIDEr\nXM3600\nCOCO-DEV\nar\n0.106\n0.513\nbn\n0.133\n0.555\ncs\n0.139\n0.485\nda\n0.192\n0.765\nde\n0.130\n0.649\nel\n0.101\n0.680\nen\n0.343\n0.875\nes\n0.220\n0.859\nfa\n0.155\n0.766\nfi\n0.089\n0.419\nfil\n0.185\n0.858\nfr\n0.217\n0.825\nhe\n0.098\n0.548\nhi\n0.111\n0.624\nhr\n0.085\n0.493\nhu\n0.096\n0.451\nid\n0.167\n0.943\nit\n0.168\n0.770\nja\n0.141\n0.850\nko\n0.152\n0.716\nmi\n0.243\n0.942\nnl\n0.232\n0.704\nno\n0.230\n0.736\npl\n0.108\n0.495\npt\n0.202\n0.843\nro\n0.100\n0.709\nru\n0.089\n0.581\nsv\n0.225\n0.748\nsw\n0.151\n0.640\nte\n0.099\n0.426\nth\n0.226\n0.802\ntr\n0.122\n0.584\nuk\n0.081\n0.560\nvi\n0.182\n0.940\nzh\n0.099\n0.656\nTable 10: CIDEr on XM3600 and COCO-DEV for the model Lg on all 35 languages. (COCO-DEV computed using machine-translated references).\nLang.\nCIDEr\nCIDEr\nXM3600\nCOCO-DEV\nar\n0.103\n0.568\nbn\n0.107\n0.594\ncs\n0.141\n0.514\nda\n0.166\n0.783\nde\n0.112\n0.643\nel\n0.103\n0.743\nen\n0.297\n0.856\nes\n0.194\n0.844\nfa\n0.152\n0.793\nfi\n0.083\n0.427\nfil\n0.181\n0.878\nfr\n0.202\n0.836\nhe\n0.099\n0.573\nhi\n0.098\n0.671\nhr\n0.087\n0.523\nhu\n0.087\n0.478\nid\n0.149\n0.947\nit\n0.153\n0.780\nja\n0.119\n0.844\nko\n0.134\n0.760\nmi\n0.239\n1.049\nnl\n0.219\n0.705\nno\n0.197\n0.743\npl\n0.101\n0.515\npt\n0.176\n0.832\nro\n0.084\n0.709\nru\n0.077\n0.586\nsv\n0.189\n0.752\nsw\n0.158\n0.681\nte\n0.097\n0.463\nth\n0.210\n0.830\ntr\n0.106\n0.613\nuk\n0.081\n0.580\nvi\n0.190\n1.022\nzh\n0.087\n0.659\nTable 11: CIDEr on XM3600 and COCO-DEV for the model BB on all 35 languages. (COCO-DEV computed using machine-translated references).\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. Existing datasets are primarily available in English and cover only a small fraction of the world\u2019s languages, limiting the accessibility of image captioning technology for diverse populations.",
            "purpose of benchmark": "The XM3600 benchmark is intended for model selection in massively multilingual image captioning, providing a robust evaluation method that correlates well with human evaluations."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of evaluating multilingual image captioning models effectively and consistently across multiple languages.",
            "key obstacle": "Existing benchmarks often rely on machine-translated datasets, which do not accurately reflect human evaluations and lead to poor model comparisons."
        },
        "idea": {
            "intuition": "The creation of XM3600 was inspired by the need for a high-quality, multilingual dataset that avoids the pitfalls of translation artifacts and provides consistent annotations across languages.",
            "opinion": "The authors believe that XM3600 will significantly advance the field of multilingual image captioning by providing a reliable evaluation framework.",
            "innovation": "XM3600 offers novel features such as a geographically diverse image selection and a unique annotation process that ensures consistent style across languages, free from translation artifacts.",
            "benchmark abbreviation": "XM3600"
        },
        "dataset": {
            "source": "The dataset was created by selecting 3600 images from the Open Images Dataset, annotated with human-generated captions in 36 languages.",
            "desc": "The XM3600 dataset consists of 3600 images, each annotated with captions in multiple languages, ensuring geographical and cultural diversity.",
            "content": "The dataset includes image data with corresponding captions in various languages, focusing on natural descriptions that reflect the visual content accurately.",
            "size": "261,375",
            "domain": "Image Captioning",
            "task format": "Image Captioning"
        },
        "metrics": {
            "metric name": "CIDEr",
            "aspect": "Model performance in generating accurate and contextually relevant captions.",
            "principle": "CIDEr was chosen for its effectiveness in measuring the agreement between generated captions and human evaluations, particularly in multilingual settings.",
            "procedure": "Model performance is evaluated by comparing generated captions against human-annotated captions using CIDEr as the scoring metric."
        },
        "experiments": {
            "model": "Various multilingual image captioning models, including mT5 and ViT architectures.",
            "procedure": "Models were trained on multilingual datasets and evaluated using the XM3600 benchmark to assess their performance against human evaluations.",
            "result": "The models showed strong correlations with human evaluations, validating the XM3600 benchmark as a reliable evaluation tool.",
            "variability": "Variability in results was accounted for through multiple trials and comparisons across different model configurations."
        },
        "conclusion": "The XM3600 benchmark enables effective evaluation and comparison of multilingual image captioning models, showing significant improvements in correlation with human judgments over existing methods.",
        "discussion": {
            "advantage": "The benchmark provides a robust framework for evaluating multilingual image captioning, facilitating faster progress in research and applications.",
            "limitation": "The dataset only covers 36 languages, which may perpetuate biases towards more widely spoken languages and limit the representation of low-resource languages.",
            "future work": "Future research should explore expanding the dataset to include more languages and investigate the impact of cultural diversity on image captioning."
        },
        "other info": [
            {
                "info1": "The XM3600 dataset is released under a CC-BY4.0 license."
            },
            {
                "info2": {
                    "info2.1": "The dataset aims to improve accessibility for visually impaired users.",
                    "info2.2": "The authors conducted extensive quality control during the annotation process."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Transformer models have transformative impacts on sequential data processing, which is relevant to evaluating multilingual image captioning models effectively."
        },
        {
            "section number": "2.1",
            "key information": "The XM3600 benchmark addresses the challenge of evaluating multilingual image captioning models effectively and consistently across multiple languages."
        },
        {
            "section number": "2.3",
            "key information": "The XM3600 dataset consists of 3600 images annotated with human-generated captions in 36 languages, focusing on natural descriptions that reflect the visual content accurately."
        },
        {
            "section number": "3.1",
            "key information": "Various multilingual image captioning models, including mT5 and ViT architectures, were trained on multilingual datasets and evaluated using the XM3600 benchmark."
        },
        {
            "section number": "4.3",
            "key information": "The XM3600 benchmark provides a robust framework for evaluating multilingual image captioning, facilitating faster progress in research and applications."
        },
        {
            "section number": "5.1",
            "key information": "The CIDEr metric was chosen for its effectiveness in measuring the agreement between generated captions and human evaluations, particularly in multilingual settings."
        },
        {
            "section number": "7.1",
            "key information": "The XM3600 dataset only covers 36 languages, which may perpetuate biases towards more widely spoken languages and limit the representation of low-resource languages."
        },
        {
            "section number": "8",
            "key information": "The XM3600 benchmark enables effective evaluation and comparison of multilingual image captioning models, showing significant improvements in correlation with human judgments over existing methods."
        }
    ],
    "similarity_score": 0.5874798738071271,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Crossmodal-3600_ A Massively Multilingual Multimodal Evaluation Dataset.json"
}