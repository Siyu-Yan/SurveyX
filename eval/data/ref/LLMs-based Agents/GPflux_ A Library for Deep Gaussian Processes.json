{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2104.05674",
    "title": "GPflux: A Library for Deep Gaussian Processes",
    "abstract": "We introduce GPflux, a Python library for Bayesian deep learning with a strong emphasis on deep Gaussian processes (DGPs). Implementing DGPs is a challenging endeavour due to the various mathematical subtleties that arise when dealing with multivariate Gaussian distributions and the complex bookkeeping of indices. To date, there are no actively maintained, open-sourced and extendable libraries available that support research activities in this area. GPflux aims to fill this gap by providing a library with state-of-the-art DGP algorithms, as well as building blocks for implementing novel Bayesian and GP-based hierarchical models and inference schemes. GPflux is compatible with and built on top of the Keras deep learning eco-system. This enables practitioners to leverage tools from the deep learning community for building and training customised Bayesian models, and create hierarchical models that consist of Bayesian and standard neural network layers in a single coherent framework. GPflux relies on GPflow for most of its GP objects and operations, which makes it an efficient, modular and extensible library, while having a lean codebase.",
    "bib_name": "dutordoir2021gpfluxlibrarydeepgaussian",
    "md_text": "# GPflux: A Library for Deep Gaussian Processes\n# Abstract\nWe introduce GPflux, a Python library for Bayesian deep learning with a strong emphasis on deep Gaussian processes (DGPs). Implementing DGPs is a challenging endeavour due to the various mathematical subtleties that arise when dealing with multivariate Gaussian distributions and the complex bookkeeping of indices. To date, there are no actively maintained, open-sourced and extendable libraries available that support research activities in this area. GPflux aims to fill this gap by providing a library with state-of-the-art DGP algorithms, as well as building blocks for implementing novel Bayesian and GP-based hierarchical models and inference schemes. GPflux is compatible with and built on top of the Keras deep learning eco-system. This enables practitioners to leverage tools from the deep learning community for building and training customised Bayesian models, and create hierarchical models that consist of Bayesian and standard neural network layers in a single coherent framework. GPflux relies on GPflow for most of its GP objects and operations, which makes it an efficient, modular and extensible library, while having a lean codebase.\nKeywords: Bayesian deep learning, deep Gaussian processes, TensorFlow and GPflow\n\u00a92021 Dutordoir et al. \u2217Work done while at Secondmind..\nhugh.salimbeni@gmail.com eric.hambro@gmail.com johnangusmcleod@gmail.com felix.leibfried@gmail.com a.artemev20@imperial.ac.uk m.vdwilk@imperial.ac.uk james.hensman@gmail.com m.deisenroth@ucl.ac.uk st@secondmind.ai\nst@secondmind.ai\n# 1. Introduction\nDeep neural networks (DNNs) are flexible parametric function approximators that can be used for supervised and unsupervised learning, especially in applications where there is an abundance of data, such as in computer vision [Krizhevsky et al., 2012], natural language processing [Vaswani et al., 2017], and planning [Schrittwieser et al., 2020]. However, in small-data and noisy settings, DNNs can overfit [Goodfellow et al., 2016], or simply make overconfident predictions, which prevents their use in safety-critical applications [Yuan et al., 2019]. To address this, Bayesian learning algorithms propose to replace the usual point estimates of parameters with probability distributions that quantify the uncertainty that remains due to a lack of data. The resulting Bayesian neural networks (BNNs) can be more robust to overfitting, and make predictions together with a measure of their reliability. Alternatively, instead of representing probability distributions in weight space, Gaussian processes (GPs) can be used to represent uncertainty directly in function space [Rasmussen and Williams, 2006]. Damianou and Lawrence [2013] first used Gaussian processes as layers to create the Deep Gaussian Process (DGP), which enables learning of more powerful representations through compositional features. Recent work has shown that DGPs are particularly promising because function-space Bayesian approximations seem to be of higher quality than those of weight-space BNNs [Foong et al., 2020; Damianou, 2015].\n# 2. Motivation\nDespite their advantages, DGPs have not been adopted nor studied as widely as (Bayesian) DNNs. A possible explanation for this is the non-existence of well-designed and extendable software libraries that underpin research activities. This is crucial, especially for DGPs, as implementing them is a challenging endeavour, even when relying on toolboxes that support automatic differentiation. This is due to, among other things, the implicit (and infinite) basis functions in GPs, the difficulty of keeping track of indices in the multi-layered, multioutput setting and the numerous numerical implementation subtleties when conditioning Gaussian distributions. To date, there are no actively maintained, open-sourced and extendable DGP libraries available. Some packages (see Table 1) exist, but they are written with a single use-case in mind, and only implement one variation of a model or inference scheme. None of them address the fact that much of the code in DGPs can be factored out and reused for building novel DGP models \u2014 these are exactly the abstractions GPflux is providing. Building on top of GPflow, GPflux is also designed to be efficient and modular. That is, the library allows new variants of models and approximations to be implemented without modifying the core GPflux source [Matthews et al., 2017; van der Wilk et al., 2020]. The aim of GPflux is twofold. First, it aims to provide researchers with reusable components to develop new DGP models and inference schemes. Second, it aims to provide practitioners with existing state-of-the-art (deep) GP algorithms and layers, such as latent variables, convolutional and multi-output models. GPflux is built on top of TensorFlow[Abadi et al., 2016; Dillon et al., 2017] and is compatible with Keras [Chollet et al., 2015]. This makes it possible to leverage on a plethora of tools developed by the deep learning community for building, training and deploying deep learning models.\n# 3. Deep Gaussian Processes: Brief Overview of Model and Inference\nGiven a dataset {(xi, yi)}N i=1, a Deep Gaussian process [Damianou and Lawrence, 2013, DGP] is built by composing several GPs, where the output of one layer is fed as input to the next. For each layer f\u2113(\u00b7), we assume that it is a-priori distributed according to a GP with a kernel k\u2113(\u00b7, \u00b7). The DGP is then defined as the composition F(\u00b7) = fL(. . . f2(f1(\u00b7))). We refer to the latent function evaluation of a datapoint xi at the \u2113th GP as hi,\u2113= f\u2113(hi,\u2113\u22121) with hi,0 = xi. We further assume a general likelihood yi | F, xi \u223cp(yi | F(xi)). Salimbeni and Deisenroth [2017] introduced an elegant and scalable inference scheme for DGPs based on the work of Sparse Variational GPs [Hensman et al., 2015]. It defines L independent approximate posterior GPs q(f\u2113(\u00b7)) of the form\nq(f\u2113(\u00b7)) = GP\n\ufffd\nku\u2113(\u00b7)K\u22121\nu\u2113u\u2113m\u2113\n\ufffd\n\ufffd\ufffd\n\ufffd\n=:\u00b5\u2113(\u00b7)\n;\nk\u2113(\u00b7, \u00b7) + k\u22a4\nu\u2113(\u00b7)K\u22121\nu\u2113u\u2113(S\u2113\u2212Ku\u2113u\u2113)K\u22121\nu\u2113u\u2113ku\u2113(\u00b7)\n\ufffd\n\ufffd\ufffd\n\ufffd\n=:\u03a3\u2113(\u00b7)\n\ufffd\n,\nwhere {m\u2113, S\u2113}L \u2113=1 parameterises the variational approximate posterior q(u\u2113) over inducin variables. Ku\u2113u\u2113and ku\u2113(\u00b7) are covariance matrices computed using the kernel k\u2113(\u00b7, \u00b7). Th model is trained by optimising a lower bound (ELBO) on the log marginal likelihood\nThe complexity term in the ELBO can be computed in closed form because all of the distributions are Gaussian. An end-to-end differentiable and unbiased Monte-Carlo estimate of the data-fit term can be computed with samples from q(hi,L), which can be obtained by iteratively propagating datapoints through the layers using the reparametrisation trick: h\u2113= \u00b5\u2113(h\u2113\u22121) + \ufffd \u03a3(h\u2113\u22121) \u01eb with \u01eb \u223cN(0, I). We refer the interested reader to van der Wilk et al. [2020] and Leibfried et al. [2020] for in-depth discussion of this method.\n# 4. Key Features and Design\nGPflux is designed as a deep learning library where functionality is packed into layers, and layers can be stacked on top of each other to form a hierarchical (i.e. deep) model. Next, we focus on the layers and subsequently show how to create models using these layers. We briefly highlight some useful tooling provided by Keras for training these Bayesian models.\n# 4.1 Layers\nThe key building block in GPflux is the GPlayer, which represents the prior and posterior of a single (multi-output) GP, f\u2113(\u00b7). It can be seen as the analogue of a standard fullyconnected (dense) layer in a DNN, but with an infinite number of basis functions. It is defined by a Kernel, InducingVariables, and MeanFunction, which are all GPflow objects. Adhering to the Keras design, a layer has to implement a call method which usually maps a Tensor to another Tensor. A GPLayer\u2019s call is slightly different in that it takes the output of the previous layer, say h\u2113\u22121, but returns an object that represents the complete Gaussian distribution of f\u2113(h\u2113\u22121) as given by eq. (1). If a subsequent layer\n(1)\n(2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a5cc/a5ccebfa-2982-49e0-890a-acd5a42cb67c.png\" style=\"width: 50%;\"></div>\nis not able to use the previous layer\u2019s distributional output, a sample will be taken usin the reparametrisation trick. This functionality is provided by TensorFlow Probability DistributionLambda layer [Dillon et al., 2017].\nGPflux also provides other Bayesian and GP-based layers. A LatentVariableLayer implements a layer which augments its inputs hi,\u2113\u22121 with latent variables wi, usually through concatenation. This leads to more flexible DGPs that can model complex, non-Gaussian densities [Dutordoir et al., 2018; Salimbeni et al., 2019]. Convolutional layers can be used for temporal or spatially structured data [van der Wilk et al., 2017; Dutordoir et al., 2020]. GPflux also provides non-GP-specific layers, such as BayesianDenseLayer which implements a dense layer for variational Bayesian neural networks. Moreover, thanks to the compatibility of GPflux with the Keras eco-system, it is possible to naturally combine GPflux layers with standard DNN components, such as convolutional or fully-connected layers, as shown in Listing 1. This variety of different building blocks provided by GPflux in a single unified framework paves the way for systematic evaluation of these methods.\n# 4.2 Models and Fitting\nAs shown in the second line of Listing 1, we can in most cases directly make use of Keras\u2019 Sequential to combine the different GPflux and Keras layers into a hierarchical model. This can be convenient because we limit the number of wrappers around our core layer functionality. However, certain GPflux layers (e.g., LatentVariableLayer) require both features {xi} and the target {yi} in training, which is a functionality that Keras does not provide directly. For these use-cases GPflux provides the specialised DeepGP class. Deep neural networks (DNNs) are trained by minimising the prediction discrepancy for examples in a training set. This is a similar to the data-fit term in the ELBO (eq. (2)), which is passed to the framework using LikelihoodLoss(Gaussian()) in the listing. The KL complexity terms of the ELBO are added to the loss by the GPLayer calls. GPflux enables DGP models to reuse much of the tooling developed by the deep learning community. E.g., during training it can be advantageous to use Keras\u2019 ReduceLROnPlateau to lower the learning rate when the ELBO converges. Other callbacks make it possible to monitor the optimisation trace using TensorBoard or save the optimal weights to disk \u2014 many of these features have not been leveraged in (deep) GP libraries before. Finally, adhering to the Keras interface also gives GPflux models a battle-tested interface (e.g., fit, predict, evaluate) which should ease its adoption in downstream applications.\n# 5. Final Remarks\nGPflux is a toolbox dedicated to Bayesian deep learning and Deep Gaussian processes. GPflux uses the mathematical building blocks from GPflow and combines these with the powerful layered deep learning API provided by Keras. This combination leads to a framework that can be used for: (i) researching new models, and (ii) building, training and deploying Bayesian models in a modern way. A number of steps have been taken to ensure the quality and usability of the project. All GPflux source code is available at http://github.com/secondmind-labs/GPflux/. We use continuous integration and have a test code coverage of over 97%. To learn more or get involved we encourage the reader to have a look at our documentation which contains tutorials and a thorough API reference.\n# Acknowledgments\nWe want to thank Nicolas Durrande, Carl Rasmussen, Dongho Kim, and everybody else a Secondmind who was involved in the open-sourcing effort.\nAppendix A. Existing open-source efforts\n<div style=\"text-align: center;\">Appendix A. Existing open-source efforts</div>\nPackage\nImplements\nLast Commit\nCode Tests\nSheffieldML/PyDeepGP\nDamianou and Lawrence, 2013; Dai et al., 2015\nNov 2018\n\u2717\nFelixOpolka/Deep-Gaussian-Process\nSalimbeni et al., 2019\nMar 2021\n\u2717\nICL-SML/Doubly-Stochastic-DGP\nSalimbeni and Deisenroth, 2017; Salimbeni et al., 2018\nFeb 2019\n\u2713\nhughsalimbeni/DGPs with IWVI\nSalimbeni et al., 2019\nMay 2019\n\u2713\ncambridge-mlg/sghmc dgp\nHavasi et al., 2018\nFeb 2019\n\u2717\nkekeblom/DeepCGP\nBlomqvist et al., 2019\nSep 2019\n\u2713\nGPyTorch/DeepGP module\nSalimbeni and Deisenroth, 2017, adapted to Conj. Gr.\nJul 2020\n\u2713\nTable 1: A summary of existing Python Deep GP libraries at the time of writing.\n# References\nReferences\nM. Abadi et al. (2016). \u201cTensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\u201d. In: arXiv preprint arXiv:1603.04467. Kenneth Blomqvist, Samuel Kaski, and Markus Heinonen (2019). \u201cDeep convolutional Gaussian processes\u201d. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer. Fran\u00b8cois Chollet et al. (2015). Keras. https://keras.io. Zhenwen Dai, Andreas Damianou, Javier Gonz\u00b4alez, and Neil D. Lawrence (2015). \u201cVariational auto-encoded deep Gaussian processes\u201d. In: arXiv preprint arXiv:1511.06455. Andreas Damianou (2015). \u201cDeep Gaussian Processes and Variational Propagation of Uncertainty\u201d. PhD thesis. University of Sheffield.\nAndreas Damianou and Neil D. Lawrence (2013). \u201cDeep Gaussian processes\u201d. In: Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AISTATS). Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A. Saurous (2017). \u201cTensorflow distributions\u201d. In: arXiv preprint arXiv:1711.10604. Vincent Dutordoir, Hugh Salimbeni, James Hensman, and Marc P. Deisenroth (2018). \u201cGaussian Process Conditional Density Estimation\u201d. In: Advances in Neural Information Processing Systems 31 (NeurIPS). Vincent Dutordoir, Mark van der Wilk, Artem Artemev, and James Hensman (2020). \u201cBayesian image classification with deep convolutional Gaussian processes\u201d. In: Proceedings of the 23th International Conference on Artificial Intelligence and Statistics (AISTATS). Andrew Y. K. Foong, David R. Burt, Yingzhen Li, and Richard E. Turner (2020). \u201cOn the expressiveness of approximate inference in Bayesian neural networks\u201d. In: Advances in Neural Information Processing Systems 33 (NeurIPS). Ian Goodfellow, Yoshua Bengio, and Aaron Courville (2016). Deep Learning. The MIT Press. Marton Havasi, Jos\u00b4e Miguel Hern\u00b4andez-Lobato, and Juan Jos\u00b4e Murillo-Fuentes (2018). \u201cInference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo\u201d. In: Advances in Neural Information Processing Systems 31 (NeurIPS). James Hensman, Alexander G. de G. Matthews, and Zoubin Ghahramani (2015). \u201cScalable variational Gaussian process classification\u201d. In: Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS). Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton (2012). \u201cImageNet classification with deep convolutional neural networks\u201d. In: Advances in Neural Information Processing Systems 25 (NIPS). Felix Leibfried, Vincent Dutordoir, S. T. John, and Nicolas Durrande (2020). \u201cA tutorial on sparse Gaussian processes and variational inference\u201d. In: arXiv preprint arXiv:2012.13962 Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo L\u2019eon-Villagr\u2019a, Zoubin Ghahramani, and James Hensman (2017). \u201cGPflow: A Gaussian process library using TensorFlow\u201d. In: Journal of Machine Learning Research. Carl E. Rasmussen and Christopher K. I. Williams (2006). Gaussian Processes for Machine Learning. MIT Press. Hugh Salimbeni and Marc P. Deisenroth (2017). \u201cDoubly stochastic variational inference for deep Gaussian processes\u201d. In: Advances in Neural Information Processing Systems 30 (NIPS).\nHugh Salimbeni, Vincent Dutordoir, James Hensman, and Marc P. Deisenroth (2019). \u201cDeep Gaussian processes with importance-weighted variational inference\u201d. In: Proceedings of the 36th International Conference on Machine Learning (ICML). Hugh Salimbeni, Stefanos Eleftheriadis, and James Hensman (2018). \u201cNatural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models\u201d. In: Proceedings of the 21th International Conference on Artificial Intelligence and Statistics (AISTATS). Julian Schrittwieser et al. (2020). \u201cMastering Atari, Go, Chess and Shogi by planning with a learned model\u201d. In: Nature. Mark van der Wilk, Vincent Dutordoir, S. T. John, Artem Artemev, Vincent Adam, and James Hensman (2020). \u201cA framework for interdomain and multioutput Gaussian processes\u201d. In: arXiv preprint arXiv:2003.01115. Mark van der Wilk, Carl E. Rasmussen, and James Hensman (2017). \u201cConvolutional Gaussian processes\u201d. In: Advances in Neural Information Processing Systems 30 (NIPS). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \ufffdLukasz Kaiser, and Illia Polosukhin (2017). \u201cAttention is all you need\u201d. In: Advances in Neural Information Processing Systems 30 (NIPS). Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li (2019). \u201cAdversarial examples: attacks and defenses for deep learning\u201d. In: IEEE transactions on neural networks and learning systems.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of implementing Deep Gaussian Processes (DGPs) in a practical manner, highlighting the lack of well-designed, extendable software libraries that support research activities in this area. Previous methods have struggled with the mathematical complexities and implementation subtleties involved in DGPs.",
        "problem": {
            "definition": "The problem this paper aims to solve is the absence of actively maintained, open-sourced libraries for Deep Gaussian Processes, which hinders research and application in Bayesian deep learning.",
            "key obstacle": "The main difficulty lies in the complex bookkeeping of indices and the mathematical subtleties associated with multivariate Gaussian distributions, making the implementation of DGPs challenging."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for a robust framework that simplifies the implementation of DGPs while leveraging existing tools from the deep learning community.",
            "opinion": "The proposed idea is to create GPflux, a Python library that provides state-of-the-art DGP algorithms and building blocks for novel Bayesian and GP-based hierarchical models.",
            "innovation": "The key innovation of GPflux is its modular design built on top of GPflow, allowing for efficient and flexible model creation without modifying the core library."
        },
        "method": {
            "method name": "GPflux",
            "method abbreviation": "GPflux",
            "method definition": "GPflux is defined as a library that facilitates Bayesian deep learning with a focus on Deep Gaussian Processes, providing reusable components for model development.",
            "method description": "GPflux enables the construction and training of deep Gaussian processes using Keras-compatible layers.",
            "method steps": "The method involves defining layers using GPflow objects, stacking them to form deep models, and utilizing Keras functionalities for training and evaluation.",
            "principle": "The effectiveness of GPflux in solving the problem is based on its ability to modularly combine GP objects and layers, leveraging the deep learning ecosystem for enhanced model training and flexibility."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involves using various datasets and comparing GPflux with existing DGP libraries to assess its performance and usability.",
            "evaluation method": "The evaluation method includes measuring the performance of GPflux against baseline methods, utilizing metrics such as log marginal likelihood and predictive accuracy."
        },
        "conclusion": "The experiments demonstrate that GPflux provides a robust framework for Bayesian deep learning and DGPs, significantly advancing the usability and accessibility of these complex models.",
        "discussion": {
            "advantage": "The key advantages of GPflux include its modularity, compatibility with Keras, and the ability to leverage existing deep learning tools, making it easier for practitioners to implement and experiment with DGPs.",
            "limitation": "A limitation of GPflux is that certain layers require both features and targets during training, which may complicate its use compared to standard deep learning models.",
            "future work": "Future work could focus on expanding the library's capabilities, improving documentation, and exploring additional applications of GPflux in various domains."
        },
        "other info": {
            "repository": "http://github.com/secondmind-labs/GPflux/",
            "test coverage": "Over 97%"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the implementation of Deep Gaussian Processes (DGPs), highlighting their significance in Bayesian deep learning."
        },
        {
            "section number": "1.2",
            "key information": "The main objective of the paper is to solve the absence of actively maintained, open-sourced libraries for Deep Gaussian Processes, which hinders research and application."
        },
        {
            "section number": "1.3",
            "key information": "Understanding DGPs and the proposed GPflux library is crucial for advancing research in Bayesian deep learning and improving the usability of complex models."
        },
        {
            "section number": "2.3",
            "key information": "The paper discusses the evolution of generative models, particularly focusing on the complexities of implementing DGPs and the need for robust frameworks."
        },
        {
            "section number": "3.1",
            "key information": "Deep Gaussian Processes (DGPs) are defined as a class of models that extend Gaussian Processes to deep learning, facilitating complex data modeling."
        },
        {
            "section number": "4.1",
            "key information": "The paper outlines the primary goals of GPflux, which include simplifying the implementation of DGPs and providing reusable components for model development."
        },
        {
            "section number": "6.1",
            "key information": "GPflux represents a type of generative model that focuses on Deep Gaussian Processes, providing a modular approach to model construction."
        },
        {
            "section number": "8.3",
            "key information": "The paper identifies algorithmic challenges associated with DGPs, such as the complex bookkeeping of indices and mathematical subtleties of multivariate Gaussian distributions."
        }
    ],
    "similarity_score": 0.5078583484153031,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0141_,arti/papers/GPflux_ A Library for Deep Gaussian Processes.json"
}