{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2210.10264",
    "title": "SignReLU neural network and its approximation ability",
    "abstract": "Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.",
    "bib_name": "li2023signreluneuralnetworkapproximation",
    "md_text": "# SIGNRELU NEURAL NETWORK AND ITS APPROXIMATION ABILITY\nJianfei Li\u02da Han Feng: Ding-Xuan Zhou;\n# Jianfei Li\u02da Han Feng: Ding-Xuan Zhou;\nAugust 31, 2023\n# ABSTRACT\nDeep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.\nkeywords: Deep neural networks, Activation function, Approximation power, SignReLU activation\n# 1 Introduction\nDeep learning has become a critical method in developing AI for handling complicated real-world tasks that appeare in human societies. Wide applications of deep learning including those in image processing [32, 22] and speec recognition [43, 50] have received great successes in recent years. Theoretical explanations for the success of dee learning have been recently studied from the point of view of approximation theory.\n\u03a6 \u201c AL \u02dd \u03c3 \u02dd AL\u00b41 \u02dd \u03c3 \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03c3 \u02dd A2 \u02dd \u03c3 \u02dd A1,\nwhere Aipxq :\u201c Aix ` bi are affine transforms with weight matrices Ai P Rdi\u02c6di\u00b41 and bias vectors bi P Rdi and \u03c3 : R \u00d1 R is an activation function acting on each element of input vectors. We call \u03c3 \u02dd Ai in \u03a6 the i-th layer (hidden layer) with width di and AL the output layer. We say a neural network has width W if the maximum width max1\u010fi\u010fLtdiu is no more than W. We call the number of nonzero elements of all Ai and bi in the neural network the number of weights (size) of the neural network \u03a6, denoted by N. It is apparent that activations are a key ingredient of the nonlinearity of deep neural networks.\n(1)\nELUpx; \u03b1q :\u201c \" x, if x P r0, 8q, \u03b1pex \u00b4 1q, if x P p\u00b48, 0q, LeakyReLUpx; \u03b1q :\u201c \"\nThese modified activation functions show promising improvements on several tasks compared with ReLU [44]. Except for these monotonic activation functions, nonmonotonic activations, for example, Swish [48], Mish [39] and Logish [66] were proposed and shown great performances in various tasks. PDELU [8] is one of the most recently proposed activation functions, defined as,\n!\u201c \u2030 ) with \u03b1 and t controlling the slope and the degree of deformation, respectively. It is verified to have many desired properties, for example, speeding up the training process and possessing geometric flexibility. Its effectiveness is observed on many datasets and well-known neural network architectures (including NIN, ResNet, DenseNet) [8, 42, 13] SignReLU function [30] is inspired by the softsign function, defined as:\nIt is easy to verify that SignReLU (4) corresponds to the special case of PDELU (3) when t \u201c 2. In some experiments, SignReLU improves the convergence rate and alleviates the gradient vanishing problem in image classification tasks [30]. There exist lots of different explanations for choosing a proper activation function. In learning theory, the learning ability of a neural network is closely related to approximation error [34, 62]. In deep learning, learning tasks aim to find a proper model \u03a6px; wq parameterized by w P RN which approximates target function fpxq well. The performance of a learned model \u03a6px; \u02c6wq ( \u02c6w is learned through a learning algorithm) over a sampled dataset Z \u201c tpxi, fpxiqquN i\u201c1 can be measured by a loss function Lpx, yq. The generalization error and optimization error of model \u03a6 with parameter w is characterized by Epwq :\u201c Ex \u201c L ` \u03a6px; wq, fpxq \u02d8\u2030 and EZpwq :\u201c 1 N \u0159N i\u201c1 L ` \u03a6pxi; wq, fpxiq \u02d8 , respectively. Then, the performance of \u03a6px; \u02c6wq in learning theory can be upper-bounded by ! )\n! ) where w\u02da :\u201c arg minwPRW Epwq is the parameter with the best generalization error and wN \u201c arg minwPRW EZpwq is the parameter with the smallest optimization error. The first, second, and third term in (5) are called approximation error, optimization error, and generalization error, respectively. Obviously, controling approximation error Epw\u02daq is of great importance to control Ep \u02c6wq. See more details in [34, 62]. In practical experiments, to find a model suitable for learning tasks, one also needs to balance performance and efficiency. A lot of work for classification, object detection, and video coding attempts to compress the model while keeping the performance in order to improve inference time and lower memory usage [47, 31, 21, 49]. The inference time and memory usage of a deep neural network depend heavily on the expression of the activation functions and the total number of parameters of deep neural networks. This kind of problem can be stated as characterizing the approximation error with the total number of parameters of deep neural networks. Therefore, it is worth focusing more on the approximation properties of valuable activation functions.\n! ) where w\u02da :\u201c arg minwPRW Epwq is the parameter with the best generalization error and wN \u201c arg minwPRW EZpwq is the parameter with the smallest optimization error. The first, second, and third term in (5) are called approximation error, optimization error, and generalization error, respectively. Obviously, controling approximation error Epw\u02daq is of great importance to control Ep \u02c6wq. See more details in [34, 62].\n Epq In practical experiments, to find a model suitable for learning tasks, one also needs to balance performance and efficiency. A lot of work for classification, object detection, and video coding attempts to compress the model while keeping the performance in order to improve inference time and lower memory usage [47, 31, 21, 49]. The inference time and memory usage of a deep neural network depend heavily on the expression of the activation functions and the total number of parameters of deep neural networks. This kind of problem can be stated as characterizing the approximation error with the total number of parameters of deep neural networks. Therefore, it is worth focusing more on the approximation properties of valuable activation functions.\n# 1.1 Related work\nTheoretical studies on the approximation ability of deep neural networks with various activation functions have been developed in a large literature [64, 1, 19, 9]. Although neural networks are of great success in practical applications, existing theoretical results mainly focused on sigmoid type and ReLU activations. When the activation function \u03c3 is a C8 sigmoid type function, which means limx\u00d18 \u03c3pxq \u201c 1 and limx\u00d1\u00b48 \u03c3pxq \u201c 0, the approximation rates were given by Barron [3] for functions f P L2pRdq whose Fourier transforms \u02c6f satisfy a decay condition \u015f Rd |w|| \u02c6fpwq|dw \u0103 8. Another remarkable result (e.g. Mhaskar [38]) based on localized Taylor expansions asserts rates of approximation for functions from Sobolev spaces. These results were developed by the localized Taylor expansion approach under the assumption that \u03c3 satisfies \u03c3pkqp\u00b5q \u2030 0 for some \u00b5 P R and every k P Z`. This condition is not satisfied by ReLU-type activations. Until recent years, approximation properties were established in [24, 36] for shallow ReLU nets and in [59, 5, 45] for deep ReLU nets for target functions from Sobolev spaces and in [52] for general continuous functions.\n(2)\n(3)\n(5)\nBesides, related analysis has been also investigated for some other kinds of activation functions. In [62], the authors proposed Elementary Universal Activation Function (EUAF). They proved that all functions represented by a EUAF fully connected neural network (FNN) with a fixed structure are dense in the space of continuous functions, which was proved impossible with ReLU FNNs [52]. Unfortunately, EUAF is partly a nonsmooth periodic function which makes it not applicable in practice. Another investigation about rational activations has been developed in [6]. It was shown that rational neural networks learn smooth functions more efficiently than ReLU neural networks. Meanwhile, some numerical experiments illustrated their potential power for solving PDEs and GAN. Due to the essential impact of activations on neural networks\u2019 performance, investigating the properties of activation functions is still an essential topic in deep learning research today. In this paper, we study the approximation ability of SignReLU (4), and for simplicity, we fix \u03b1 \u201c 1. Precisely, we investigate the activation \u03c1 : R \u00d1 R, given by\nIt is easy to see that \u03c1 is monotonically increasing on R and \u03c1pxq \u00d1 \u00b41 as x \u00d1 \u00b48. Thus, it can be categorized as a ReLU-type activation function. Particularly, it enjoys a similar shape to ELU. The design on the negative part is the same as that of EUAF [62]. It is important since it allows SignReLU to represent the division gate as well as the product gate, which cannot be produced by ReLU neural networks with finite parameters.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c5f7/c5f752e1-be09-461d-9b1f-c0522896fed4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Activation functions</div>\nFigure 1: Visualization of activation functions (ReLU, LeakyReLU [\u03b1 \u201c 0.2], ELU [\u03b1 \u201c 1] and SignReLU [\u03b1 \u201c 1]) and their gradients. Our main results in the following form quantify the structure (i.e., depth, layer, size) of SignReLU neural networks that guarantee certain approximation accuracy for a given target function. In this paper, all proofs are given in Appendix. Form of approximation by SignReLU nets. Let d P N. Let \u2126\u0102 Rd and f P F be a function from function class F defined on \u2126. For any \u03b5 \u0105 0, there exists a function \u03a6 implemented by a SignReLU neural network with (depth) L\u03a6 \u201c L\u03a6p\u03b5, fq, (width) W\u03a6 \u201c W\u03a6p\u03b5, fq and (number of weights) N\u03a6 \u201c N\u03a6p\u03b5, fq such that\nOur main results in the following form quantify the structure (i.e., depth, layer, size) of SignReLU neural networks that guarantee certain approximation accuracy for a given target function. In this paper, all proofs are given in Appendix. Form of approximation by SignReLU nets. Let d P N. Let \u2126\u0102 Rd and f P F be a function from function class F defined on \u2126. For any \u03b5 \u0105 0, there exists a function \u03a6 implemented by a SignReLU neural network with (depth) L\u03a6 \u201c L\u03a6p\u03b5, fq, (width) W\u03a6 \u201c W\u03a6p\u03b5, fq and (number of weights) N\u03a6 \u201c N\u03a6p\u03b5, fq such that\nWhen there exists a SignReLU neural network that equals f on \u2126, then L\u03a6 \u201c L\u03a6pfq, W\u03a6 \u201c W\u03a6pfq and N\u03a6 \u201c Npfq only depends on the properties of f. Contributions. First, we characterize some basic properties that SignReLU neural networks possess, for example, realizing product gate and division gate, realizing rational functions, and approximating ReLU and exponential functions effectively. Then we show that given tolerance \u03b5 \u0105 0, SignReLU nets can approximate Sobolev functions W r p with non-zero parameters increasing at a rate of \u03b5\u00b4d{r. Moreover, the optimal approximation error for piecewise smooth functions is obtained. We also investigate when SignReLU neural networks overcome the curse of dimensionality, which is a crucial topic in machine learning. Our results show that when \u03b5-approximating Korobov functions or BV functions, the dominant term in the size of neural networks is \u03b5\u00b41{r, which is independent of the input dimension d. Finally, some numerical experiments are conducted on classification and image denoising tasks, illustrating competitive\nWhen there exists a SignReLU neural network that equals f on \u2126, then L\u03a6 \u201c L\u03a6pfq, W\u03a6 \u201c W\u03a6pfq and N\u03a6 \u201c Npfq only depends on the properties of f. Contributions. First, we characterize some basic properties that SignReLU neural networks possess, for example realizing product gate and division gate, realizing rational functions, and approximating ReLU and exponential functions effectively. Then we show that given tolerance \u03b5 \u0105 0, SignReLU nets can approximate Sobolev functions W r p with\nperformances compared with the existing activations\u2013ReLU, Leaky ReLU, and ELU. For the sake of convenience, w shall denote ReLU by \u03c3pxq \u201c maxtx, 0u in the rest of this paper.\n# 1.2 Notations\nLet us first clarify the basic notations to be used throughout the paper. Let R denote all the real numbers, N denote natural numbers, N` denote nonzero positive integers and Z denote the set of integers. Usually, we use d or di fo some i P N to denote the dimension of a vector. We use boldface lowercase letters to denote a d-dimensional vector for example, x P Rd with each element written as xi, which means that x \u201c px1, x2, . . . , xdqJ. For some x P Rd and n P Zd, we define xn as the operation \u015bd i\u201c1 xni i . We denote the measure of a set \u2126in Rd as |\u2126|.\n#  \u015b 2 Theoretical results for classifier functions\nPiecewise smooth functions are closely related to classification problems [45]. In image classification tasks, one needs to approximate a label for any given image x. If labels are from an integer set t1, 2, . . . , Ku, then the problem is to find the best piecewise constant function fpxq \u201c \u0159K i\u201c1 i\u03c7\u2126ipxq, which output i if x P \u2126i and zero otherwise. Function \u03c7\u2126is defined as \u03c7\u2126pxq \u201c 1 if x P \u2126, and zero otherwise. However, it is not easy to produce integer labels with neural networks. The most commonly used setting is to learn a distribution. If two images are close to each other, then it is reasonable to expect the probabilities of the labels to be close to each other. It means that one can assume that distributions are some Sobolev functions. Then the classification problems reduce to learn piecewise smooth functions \u0159L i\u201c1 fipxq\u03c7\u2126ipxq, where fi are Sobolev functions and \u2126i \u0102 Rd are disjoint. Besides, smoothness also helps neural networks yield robust results since it is unlikely to be sensitive with respect to noisy images.\n# 2.1 Basic properties of SignReLU neural networks\nThis subsection is devoted to some useful and basic properties of SignReLU neural networks, which will be applied frequently in our approximation theory. The first lemma shows that SignReLU neural networks are able to produce product/division gates. Lemma 1. Let 0 \u0103 a \u0103 M be constants. (i) There exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c 4, W\u03a6 \u010f 9 and N\u03a6 \u010f 63 such that pq \u201c @ P r\u00b4s\nBased on the above results, we are able to construct SignReLU networks to implement polynomials and rational functions. Lemma 2. Let M \u0105 0 and m, n P N`. Let p, q be polynomials with degrees at most n and m, respectively. If q has no roots on r\u00b4M, Ms, then there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Opmaxtn, muq, W\u03a6 \u201c Op1q and N\u03a6 \u201c Opmaxtn, muq such that\nfunctions. Lemma 2. Let M \u0105 0 and m, n P N`. Let p, q be polynomials with degrees at most n and m, respectively. If q has no roots on r\u00b4M, Ms, then there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Opmaxtn, muq W\u03a6 \u201c Op1q and N\u03a6 \u201c Opmaxtn, muq such that\n\u03a6pxq \u201c ppxq qpxq, @x P r\u00b4M, Ms.\nLemma 2 shows the superiority of SignReLU for realizing polynomials and rational functions, compared with ReLU Results in [29] show that at least Oplnp\u03b5\u00b41qq parameters are needed when using a ReLU neural network to \u03b5-approximate x2 on r\u00b41, 1s . The following lemma shows how ReLU can be approximated by SignReLU neural networks. Lemma 3. Let m, n P N`. (i) For any \u03b5 \u0105 0, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c 1 and W\u03a6 \u201c 1 such that |pq \u00b4pq| \u010f @ P R\nLemma 1 in [6] says that to approximate ReLU with tolerance \u03b5, the size of rational neural networks needed is of order at least C ln ln ` \u03b5\u00b41\u02d8 for some C \u0105 0. In comparison, SignReLU can approximate ReLU uniformly on R instead of r\u00b41, 1s with fixed size (Lemma 3 (i)). The exponential function is widely utilized in approximation and regression problems with Gaussian reproducing kernel Hilbert space [53, 57]. In the following proposition, we particularly give the power of SignReLU networks for approximating exponential functions. Proposition 1. Let M \u0105 0 and d P N`. (i) For any \u03b5 \u0105 0, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c O ` lnp\u03b5\u00b41q \u02d8 , W\u03a6 \u201c Op1q and N\u03a6 \u201c O ` lnp\u03b5\u00b41q \u02d8 such that\nwhere }x}1 \u201c \u0159d j\u201c1 |xj|.\n \u0159   (iii) For any \u03b5 \u0105 0, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c O ` lnp\u03b5\u00b41q W\u03a6 \u201c Op1q and N\u03a6 \u201c O ` lnp\u03b5\u00b41q \u02d8 such that\nb\u0159   Remark 1. Applying the same argument, for arbitrary real constants a, b and non-zero c, a Gaussian function fpxq \u201c ae\u00b4px\u00b4bq2{c can be approximated by an L-layer SignReLU network with accuracy e\u00b4L which is exponentially decreasing as well. Let us recall the approximation results of ReLU neural networks and rational neural networks and compare them with approximation properties of SignReLU neural networks. Telgarsky discussed approximation relationships between ReLU neural networks and rational functions [56]. To approximate a rational function with accuracy \u03b5 by a ReLU neural network, the size needed is of order O ` plnp\u03b5\u00b41qq3\u02d8 . Lemma 1 and Lemma 2 show that using a SignReLU neural network instead, the size for approximating a given rational function is independent of \u03b5, which only depends on the degree of the rational function. Rational neural networks, activated by rational functions, could be more powerful than ReLU neural networks due to the following results obtained in [6],\n\u201c \u2030 \u201c \u2030 where we use the expression Rational \u010f ReLU rNp\u03b5qs to represent when approximating a rational neural network by a ReLU neural network within the tolerance of \u03b5, the needed size of ReLU neural networks is at most CNp\u03b5q for some constant C. Conversely, the expression ReLU rNp\u03b5qs \u010f Rational represents that any ReLU neural network with the size less than CNp\u03b5q for some constant C cannot approximate the given rational neural network within tolerance \u03b5. Based on Lemma 2, Lemma 3, we are able to obtain the following improved approximation results by using SignReLU neural networks\nThe above comparison suggests that SignReLU could be more powerful than ReLU and rational activation functions. A key difference between SignReLU and other mentioned activations is its strong ability to approximate ReLU (Lemma 3) and product/division gates. Other activations do not possess these properties simultaneously. Theorem 1 theorectically verifies the claimed properties in (8). Theorem 1. Let d P N`. Denote \u03a6R / \u03a6\u03c3 a rational neural network / ReLU neural network with depth L\u03a6R / L\u03a6\u03c3, width W\u03a6R / W\u03a6\u03c3 and number of weights N\u03a6R /N\u03a6\u03c3, respectively.\n(ii) For any \u03b5 \u0105 0, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c L\u03a6\u03c3, W\u03a6 \u201c W\u03a6\u03c3 and N\u03a6 \u201c N\u03a6\u03c3 such that |\u03a6pxq \u00b4 \u03a6pxq| \u010f \u03b5, @x P Rd.\n2.2 Approximation of weighted Sobolev smooth functions For k P Nd and x P r\u00b41, 1sd, let\nFor k P Nd and x P r\u00b41, 1sd, let\nWe consider the weighted Sobolev space W r p pr\u00b41, 1sd, wq with r \u0105 0 and 1 \u010f p \u0103 8 defined by locally integrab functions on r\u00b41, 1sd with norm\nWe consider the weighted Sobolev space W r p pr\u00b41, 1sd, wq with r \u0105 0 and 1 \u010f p \u0103 8 defined by locally integrable functions on r\u00b41, 1sd with norm\nconsider the weighted Sobolev space W r p pr\u00b41, 1sd, wq with r \u0105 0 and 1 \u010f p \u0103 8 defined by locally integrable tions on with norm\nand W r 8 ` r0, 1sd\u02d8 by functions f P C ` r0, 1sd\u02d8 with norm\n` \u02d8 }f}W r 8 :\u201c max k:|k|1\u010fr ess sup xPr0,1sd \u02c7\u02c7Dkfpxq \u02c7\u02c7 \u0103 8,\nwhere |k|1 \u201c k1 ` . . . ` kd.\nr simplicity, we denote } \u00a8 }p,w,\u2126:\u201c } \u00a8 }p,w or } \u00a8 }p,w,\u2126:\u201c } \u00a8 }p, if wpxq \u201d 1 or \u2126is clear from the context. Notice at } \u00a8 } is the classical L norm.\nFor simplicity, we denote } \u00a8 }p,w,\u2126:\u201c } \u00a8 }p,w or } \u00a8 }p,w,\u2126:\u201c } \u00a8 }p, if wpxq \u201d 1 or \u2126is clear from the context. Notice that } \u00a8 }p is the classical Lp norm. The motivation for introducing weighted spaces is from a technical perspective, which allows us to apply approximation analysis by trigonometric polynomials, instead of localized Taylor expansions for developing approximation results. During changing variables, the weight function wpxq will appear. Compared to techniques used in [6, 59], our results extend p \u201c 8 to p P r1, 8q and can be directly applied to all other activation functions that are able to realize polynomial/rational functions. Besides, the constant of the obtained size Op\u03b5\u00b4d{rq in our result is CCrd, for some constants C, Cr \u0105 0 where the constant Cr depends only on r. This is better than those that appeared in [6, 59]. In the following result, we shall show the approximation ability of SignReLU neural networks to functions in the weighted Sobolev spaces. It shows that with SignReLU activation an improved rate can be achieved. Theorem 2. Let d, r P N` and 1 \u010f p \u0103 8. (i) Let f from the unit ball of W r p pr\u00b41, 1sd, wq. For any \u03b5 \u0105 0, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Op\u03b5\u00b4 1 r q, W\u03a6 \u201c Op\u03b5\u00b4 d r q and N\u03a6 \u201c Op\u03b5\u00b4 d r q, such that }f \u00b4 \u03a6}p,w \u010f \u03b5. Moreover, the constant factor of N\u03a6 only depends on d, r and can be at most CCrd, where the constant C \u0105 0 and the constant Cr only depends on r. (ii) Let f from the unit ball of W r 8pr0, 1sdq. For any \u03b5 \u0105 0, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Op1q and N\u03a6 \u201c Op\u03b5\u00b4 d r q, such that }f \u00b4 \u03a6}8 \u010f \u03b5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4959/49599589-49fe-4647-b3f1-4306cf338890.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Visualization of f1, f2, \u03c7\u2126and f \u201c f1 ` f2\u03c7\u2126with the set \u2126defined as \u2126 \u201c \u2423 px, yq P r0, 1sd : px \u00b4 0.5q2 ` py \u00b4 0.5q2 \u0103 0.25 ( . The orange/green/purple/blue region represents the value o the function f1/f2/1/f1 ` f2 on the corresponding region.</div>\nThe improvement on the constant factor Crd is significant in the Lp,w case. In fact, to achieve the same accuracy \u03b5 P p0, 1q under L8 norm, Theorem 1 of [59] asserts that f P W r 8pr0, 1sdq can be approximated by a ReLU deep net with at most cplnp\u03b5\u00b41q ` 1q layers and at most cp\u03b5\u00b4 d r lnp\u03b5\u00b41q ` 1q computation units with a constant c :\u201c cd,r. However, the constant c here increases much faster as d becomes large. More specifically, as pointed out in [65, 55], the main approach in [59] is to approximate f by a localized Taylor polynomial, which leads to the constant c at least 2d when d is large. The following theorem shows the rate obtained by using SignReLU for approximating Sobolev functions is optimal. We denote B1pFq the unit ball of any given function class F centered at 0 P F. Theorem 3 ([12]). Let d, r P N`. Let N \u0105 0 be an integer and \u03a8 : RN \u00d1 Cpr0, 1sdq be an arbitrary mapping. Assume that there is a continuous map w : B1pW r 8q \u00d1 RN such that }f \u00b4 \u03a8pwpfqq}8 \u010f \u03b5 for all f P B1pW r 8pr0, 1sdqq. Then N \u011b cr\u03b5\u00b4 d r with cr be a constant only depends on r. When we fix r and d, Theorem 2 and Theorem 3 show that the obtained bounds N\u03a6 of SignReLU neural networks that achieve tolerance \u03b5 can not be improved under the hypothesis of continuous weight selection. 2.3 Approximation of piecewise smooth functions In this subsection, we consider piecewise functions. Given a function space F on r\u00b41, 1sd and a collection A of subsets of r\u00b41, 1sd, the collection of piecewise functions is defined as\nWe consider the collection A as a collection of level sets by\n\u2126\u201c \u2423 x P r\u00b41, 1sd : hpxq \u0103 gpxq ( ,\n\u2423 ( where h, g are SignReLU neural networks that can \u03b5-approximate some functions in F. Results in Theorem 2 show that using networks to define A will not influence its generality. If the function space F is defined on r0, 1sd, then in SpF, Aq, the collection A will be modified accordingly.\nFor example, we choose F \u201c W r 8 ` r0, 1s2\u02d8 , gpx, yq \u201c \u00b4px \u00b4 0.5q2 \u00b4 py \u00b4 0.5q2, and hpx, yq \u201d \u00b40.25. By Lemma 2, functions h and g can be realized by some SignReLU neural networks. Hence, we have \u2126\u201c \u2423 px, yq P r0, 1sd : px \u00b4 0.5q2 ` py \u00b4 0.5q2 \u0103 0.25 ( , which is a region bounded by a circle. Let f1px, yq \u201c ex`y\u00b41 and f2px, yq \u201c 2px \u00b4 0.5q2 ` 2py \u00b4 0.5q2 \u00b4 3 and define f :\u201c f1 ` f2\u03c7\u2126. Then obviously, f{3 P SpF, Aq. See Figure 2 for illustration. In fact, any choice of f1, f2 P F makes f P SpF, Aq. Since the collection A depends on the function class F, we use the notation SpFq :\u201c SpF, Aq for short, and when functions in F have some smooth properties, we call functions in SpFq piecewise smooth functions induced by F. Theorem 4. Let d, r P N` and p \u011b 1. (i) Let f P S ` B1 ` W r p pr\u00b41, 1sd, wq \u02d8\u02d8 . For any \u03b5 \u0105 0, there exist a set \u2126\u03b5 \u0102 r\u00b41, 1sd and a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Op\u03b5\u00b4 1 r q, W\u03a6 \u201c Op\u03b5\u00b4 d r q and N\u03a6 \u201c Op\u03b5\u00b4 d r q such that\nFor example, we choose F \u201c W r 8 ` r0, 1s2\u02d8 , gpx, yq \u201c \u00b4px \u00b4 0.5q2 \u00b4 py \u00b4 0.5q2, and hpx, yq \u201d \u00b40.25. By Lemma 2, functions h and g can be realized by some SignReLU neural networks. Hence, we have \u2126\u201c \u2423 px, yq P r0, 1sd : px \u00b4 0.5q2 ` py \u00b4 0.5q2 \u0103 0.25 ( , which is a region bounded by a circle. Let f1px, yq \u201c ex`y\u00b41 and f2px, yq \u201c 2px \u00b4 0.5q2 ` 2py \u00b4 0.5q2 \u00b4 3 and define f :\u201c f1 ` f2\u03c7\u2126. Then obviously, f{3 P SpF, Aq. See Figure 2 for illustration. In fact, any choice of f1, f2 P F makes f P SpF, Aq. Since the collection A depends on the function class F, we use the notation SpFq :\u201c SpF, Aq for short, and when functions in F have some smooth properties, we call functions in SpFq piecewise smooth functions induced by F. Theorem 4. Let d, r P N` and p \u011b 1. (i) Let f P S ` B1 ` W r p pr\u00b41, 1sd, wq \u02d8\u02d8 . For any \u03b5 \u0105 0, there exist a set \u2126\u03b5 \u0102 r\u00b41, 1sd and a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Op\u03b5\u00b4 1 r q, W\u03a6 \u201c Op\u03b5\u00b4 d r q and N\u03a6 \u201c Op\u03b5\u00b4 d r q such that }f \u00b4 \u03a6}p,w,r\u00b41,1sdz\u2126\u03b5 \u010f \u03b5, and |\u2126\u03b5| \u00d1 0, as \u03b5 \u00d1 0. (ii) Let f P S ` B1 ` W r 8pr0, 1sdq \u02d8\u02d8 . For any \u0105, there exist a set \u0102 rsd and a function realized by a SignReLU neural network with\n  }f \u00b4 \u03a6}p,w,r\u00b41,1sdz\u2126\u03b5 \u010f \u03b5,\n` ` \u02d8\u02d8 For any \u03b5 \u0105 0, there exist a set \u2126\u03b5 \u0102 r0, 1sd and a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Op1q and N\u03a6 \u201c Op\u03b5\u00b4 d r q such that\nTheorem 4 is easy to be extended from a binary classification setting pf1 ` f2q\u03c7\u2126` f1\u03c7\u2126c to multi-classification setting \u0159K i\u201c1 fi\u03c7\u2126i, with width and the total number of weights growing at a rate of several times, which only depends on K. Since we have B1 ` W r 8pr0, 1sdq \u02d8 \u0102 S ` B1 ` W r 8pr0, 1sdq \u02d8\u02d8 , Theorem 4 cannot be improve under the hypothesis of Theorem 3.\n# 2.4 Approximation with milder dependence on dimensionality\nFor approximating Sobolev functions with regularity r and a pre-assigned accuracy \u03b5, the required size of SignReLU nets is Op\u03b5\u00b4 d r q, which increases exponentially with respect to input dimension d. In real-world image applications, the dimension d of an image is usually larger than 100 \u02c6 100 \u02c6 3. To achieve tolerance \u03b5 \u201c 0.1, the size of neural networks is almost 1030000{r, which is only applicable when r is very large. In this subsection, we attempt to break the curse of dimensionality in approximating multivariate functions by employing a \u201ctensor-friendly\u201d structure.\n \u02c6 \u02c6 \u201c is almost 1030000{r, which is only applicable when r is very large. In this subsection, we attempt to break the curse of dimensionality in approximating multivariate functions by employing a \u201ctensor-friendly\u201d structure. Here breaking the curse of dimensionality means the approximation rate or complexity rate of a model can be merely impacted by the dimension of inputs. To achieve it, we will introduce a space of functions with finite mixed derivative norms, which is sometimes referred to as a Korobov space. Related investigations with ReLU activation have been achieved in [40, 37]. The ability of using Korobov space to overcome the curse of dimensionality is followed from mixed derivatives and hyperbolic approximations [15]. When using polynomials for approximating Sobolev functions, there will be #tn P Nd : }n}8 \u010f Nu \u00ab N d multi-dimensional monomials required. Denote }n}\u03c0 \u201c \u015bd j\u201c1 maxt1, nju. Then only #tn P Nd : }n}\u03c0 \u010f Nu \u00ab Npln Nqd\u00b41 monomials are involved for approximating Korobov functions, which increase almost of order N, instead of N d. The weighted Korobov space Kr ppr\u00b41, 1sd, wq with r P N and 1 \u010f p \u0103 8 is defined by locally integrable functions on r\u00b41, 1sd with norm \u00ab ff\nThe approximation theory related to Korobov space was also considered in [40, 37, 15]. The following theorem shows that the dominant term of complexity rate is free of the input dimension. Theorem 5. Let d, r P N`, p \u011b 1 and \u03b2pr, dq \u201c p2dr ` d ` rq{r.\n) Theorem 6. Let d, r P N`. For any \u03b5 \u0105 0 and f P Vr d, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c Op\u03b5\u00b4 1 r`1 q, W\u03a6 \u201c Op1q and N\u03a6 \u201c Op\u03b5\u00b4 1 r`1 q such that }f \u00b4 \u03a6}8 \u0103 \u03b5.\n! ) which can be bounded by the classical modulus wfptq :\u201c supt|fpxq \u00b4 fpyq| : x, y P r0, 1sd, }x \u00b4 y}2 \u010f tu. Theorem 7. Let d, N P N`. For any continuous function f on r0, 1sd, there exists a function \u03a6 realized by a SignReLU neural network with L\u03a6 \u201c OpNq, W\u03a6 \u201c OpN dq and N\u03a6 \u201c OpN dq such that ||\u03a6 \u00b4 f||L8pr0,1sdq \u010f 5 4 \u0159d i\u201c1 wi fp 1 N q\n  eorem 7. Let d, N P N`. For any continuous function f on r0, 1sd, there exists a function \u03a6 realized by a SignReLU ral network with L\u03a6 \u201c OpNq, W\u03a6 \u201c OpN dq and N\u03a6 \u201c OpN dq such that ||\u03a6 \u00b4 f||L8pr0,1sdq \u010f 5 4 \u0159d i\u201c1 wi fp 1 N q.\n# 3 Experiments\nIn this section, we conduct some numerical experiments to test the ability of the SignReLU activation function for various learning tasks (regression, classification, image denoising). Codes are available at https://github.com/ JFLi11/Experiments-on-SignReLUnets.git. In each experiment, we utilize a neural network with the same architecture activated by different nonlinear activation functions, ReLU, LeakyReLU (\u03b1 \u201c 0.01), ELU (\u03b1 \u201c 1), and SignReLU. The datasets we take include MNIST [26] images with 60000 images for training and 10000 for testing, CIFAR10 [25] images with 50000 for training and 10000 for testing, and Caltech101 [27] images with 7677 for training and 1000 for testing.\n# 3.1 Regression\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d0ba/d0ba8236-bfaa-4354-9db5-de80cfdc0ac9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8901/89013441-dbe7-4d0c-aae5-c6bc2f1fe518.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Noise regression results. Figures (a-c), (d-f) show MSEs and variances on the test sets of a three-layer fully connected neural network with different activation functions. X-axis represents the size of the training set and Y-axis gives mean MSE and variance over 10 independent trails.</div>\nWe consider the following regression model on r\u00b410, 10sd\nwhere z \u201c \u0159d i\u201c1 xi and \u03be is the Gaussian noise with mean 0 and variance 0.25. See an illustration of the proposed regression model in Figure 5. We draw data with x uniformly sampled with dimension d varying in t50, 100, 1000u and the size of the training set varying in\nt2000, 2500, 3000, 4000, . . . , 10000u\ntu for each d. We randomly generate 2000 samples for the test set in the same way as the training set without noise for all experiments in this subsection. For the network structure, we choose three hidden layers with widths all equal to 100. During the training, we use the ADAM algorithm and a minibatch of size 100. The learning rate is set to be 0.0001 in 50 epochs. Figure 3 depicts the results of 10 independent trials in terms of MSE. We observe that ELU and SignReLU outperform ReLU and LeakyReLU and that ELU and SignReLU have similar performances for this regression model. When more data are used for training, all activations can help the neural network learn well. If no noise is added in the training sets, the performance of neural networks, see Figure 4, can be improved and other conclusion is similar to noise cases.\n# 3.2 Classification\n<div style=\"text-align: center;\">Table 1: Test accuracy.</div>\nTest acc(%) ReLU SignReLU\nELU\nLeakyReLU\nMNIST\n97.82\n97.92\n97.12\n97.43\nCIFAR10\n75.28\n76.57\n76.07\n74.62\nIn the classification task [16, 67], we evaluate activation functions on MNIST and CIFAR10. For MNIST, we use a fully connected neural network which is the same as that used in the previous subsection on regression. During the training, we use the ADAM algorithm and a minibatch of size 128. The learning rate is set to be 0.001 in 20 epochs. For CIFAR10, we use a \"small\" Resnet18 with output channels for each convolutional layer to be 16, and max-pooling is applied after each residual block. Since the image is small, we also remove the first 7 \u02c6 7 kernel of Resnet18. The\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0653/06537a8a-738d-45b3-91aa-10d3c15221b4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Noiseless regression results. Figures (a-c), (d-f) show MSEs and variances on the test sets of a three-layer fully connected neural network with different activation functions. X-axis represents the size of the training set and Y-axis gives mean MSE and variance over 10 independent trails.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/89c0/89c001da-1445-4033-837d-bf9cdfded938.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: 2D gray images and their corresponding spherical samples [28</div>\nclassification layer we employed is a fully connected layer with the input dimension 64. This neural network has no more than 40 thousand parameters. During the training, we use the ADAM algorithm and a minibatch of size 128. The learning rate decays exponentially from the beginning value 0.001 with the multiplicative factor of 0.9 every four epochs in 30 epochs. Weight decay is set to be 0.001. Test accuracy in Table 1 proves the superiority of the classification algorithm induced by SignReLU neural networks.\n# 3.3 Spherical image denosing\nOne more experiment we conduct follows that in [28] for spherical image denoising with convolutional neural network activated by ReLU. Spherical images, whose domain are 2-sphere, similar to graph-structured data, are not typical images on Euclidean space but arise in various situations, such as astrophysics [54] and medical imaging [60]. One\nHere we employ the neural network with the same architecture activated by various functions including LeakyReLU, ELU, and SignReLU. During the training, we use the ADAM algorithm and a mini-batch size of 20. Learning rate decay exponentially from the beginning value 0.005 with a multiplicative factor 0.9 in 20 epochs. For any image f, we add Gaussian noise with varying standard deviation \u03c3 \u201c rate \u02c6 fmax where fmax is the maximal absolute value of f. See Figure 7 for some noisy images. The peak signal-to-noise ratio (PSNR \u201c 10 log10pf 2 max{MSEq) is employed to evaluate the performance of each denoising model, where MSE represents the mean square error between noised signal and ground truth.\nThe generalization performance is evaluated by recovered PSNR over four typical images (sampled to the 2D sphere, see Figure 6 for illustration and [28] for more details), while the neural network is trained on Caltech101, see Table 2 for results. We find that the SignReLU activation function can give the best-denoised image when rate \u201c 0.3 while ReLU and LeakyReLU activations perform best when given noise rate 0.2 and 0.5 respectively. With noise rate increases, all activations have a decreasing performance. Overall, under the above settings, all these four activations give comparable denoised images. Hence, the SignReLU activation function can be valuable for image-denoising tasks. Figure 7 shows some denoising results of neural networks. As the noise rate increases, neural networks cannot recover image textures well. For example, facial parts are not able to be seen at noise rate 0.5. Developing neural networks that improve PSNR and structure information simultaneously would be an interesting topic for future work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6dec/6decc5d6-046f-4486-8473-986e96074e4f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Spherical Barbara with Gaussian noise and the corresonding denoised results.</div>\nImage\nBarbara\nBoat\nHill\nMan\nrate\n0.2\n0.3\n0.5\n0.2\n0.3\n0.5\n0.2\n0.3\n0.5\n0.2\n0.3\n0.5\nReLU\n23.823 22.533 21.309 26.104 24.515 22.777 26.034 24.703 23.116 26.367 24.901 23.243\nLeakyReLU 23.760 22.470 21.324 25.870 24.569 22.942 26.013 24.709 23.216 26.262 24.953 23.403\nELU\n23.626 22.504 21.319 25.750 24.582 22.746 25.762 24.725 23.163 26.037 25.000 23.333\nSignReLU\n23.805 22.589 21.152 25.967 24.657 22.554 26.001 24.730 23.058 26.343 25.010 23.134\n# 4 Conclusion and further remarks\nIn this work, we investigate the approximation ability of neural networks activated by SignReLU. We remark that (a) SignReLU is able to produce rational functions and approximate ReLU efficiently, (b) an improved approximation power is achieved by using SignReLU neural networks, compared with those activated by ReLU and rational activation functions and we extend the measure of error from L8 to Lp,w, p P r1, 8q, and (c) approximation results on Korobov space and tendered BV functions solves the curse of dimensionality. We would like to mention that most of our techniques are also suitable for activation functions that are able to realize polynomial/rational functions. Several experiments are conducted and show the ability of SignReLU in deep learning tasks. There are many research directions for further work. Based on Proposition 1, we are able to discuss the relationship between neural networks and reproducing kernel Hilbert spaces. Since ReLU neural networks can efficiently produce piecewise linear functions and SignReLU neural networks are able to realize rational functions, it could be more powerful to combine these two activations to enhance the learning ability in applications.\n# Acknowledgement\nThe second and last authors are supported partially by the Laboratory for AI-Powered Financial Technologies, the Research Grants Council of Hong Kong [Projects # C1013-21GF, #11306220 and #11308121], the Germany/Hong Kong Joint Research Scheme [Project No. G-CityU101/20], the CityU Strategic Interdisciplinary Research Grant [Project No. 7020010], National Science Foundation of China [Project No. 12061160462], and Hong Kong Institute for Data Science.\n# A Basics properties of SignReLU nets\nIn the following, for simplicity, we only use L, W, and N to denote the depth, width, and number of weights of a given neural network \u03a6, if it is clear from the context.\nIn the following, for simplicity, we only use L, W, and N to denote the depth, width, and number of weights of a given neural network \u03a6, if it is clear from the context. Before proving the main results, we introduce several basic operations used frequently in our proofs. Lemma 4 (Composition). Let d, d1, d2 P N` and M \u0105 0. Assume that there are two SignReLU neural networks \u03a61 : r\u00b4M, Msd \u00d1 r\u00b4M, Msd1 with depth L1, width W1 and number of weighs N1 and \u03a62 : Rd1 \u00d1 Rd2 with depth L2, width W2 and number of weighs N2. Then there exists a SignReLU neural network \u03a6 with depth L \u201c L1 ` L2 ` 1, width W \u010f maxtW1, W2u and number of weights N \u010f N1 `N2 such that \u03a6pxq \u201c \u03a62 \u02dd\u03a61pxq for all x P r\u00b4M, Msd. Proof. We denote 1m P Rm as the vector with all elements to be 1. By definition (1), we assume that\nBefore proving the main results, we introduce several basic operations used frequently in our proofs. Lemma 4 (Composition). Let d, d1, d2 P N` and M \u0105 0. Assume that there are two SignReLU neural networks \u03a61 : r\u00b4M, Msd \u00d1 r\u00b4M, Msd1 with depth L1, width W1 and number of weighs N1 and \u03a62 : Rd1 \u00d1 Rd2 with depth L2, width W2 and number of weighs N2. Then there exists a SignReLU neural network \u03a6 with depth L \u201c L1 ` L2 ` 1, width W \u010f maxtW1, W2u and number of weights N \u010f N1 `N2 such that \u03a6pxq \u201c \u03a62 \u02dd\u03a61pxq for all x P r\u00b4M, Msd. Proof. We denote 1m P Rm as the vector with all elements to be 1. By definition (1), we assume that\n  where Ai j \u201c Ai jy ` bi j are affine transforms for some matrices Ai j P Rdi j\u02c6di j\u00b41 and bi j P Rdi j, i \u201c 1, 2, j  maxtL1, L2u. Notice that d2 0 \u201c d1 L1`1 \u201c d1.\n        where \u02dc A1 L1`1pyq :\u201c A1 L1`1y ` b1 L1`1 ` M1d1 and \u02dc A2 1pyq :\u201c A2 1y ` b2 1 \u00b4 MA2 11d1. Obviously, when A1 L1`1pyq  r\u00b4M, Msd1, then \u03c1 \u00b4 \u02dc A1 L1`1pyq \u00af \u201c \u02dc A1 L1`1pyq. Hence, for any y that satisfies A1 L1`1pyq P r\u00b4M, Msd1, we have\n\u201c A  \u02dd A L`pq Combining (9), (10) and the assumption that \u03a61 : r\u00b4M, Msd \u00d1 r\u00b4M, Msd1, we conclude that \u03a6 \u201c \u03a62 \u02dd \u03a61 and it is a SignReLU neural network with L \u201c L1 ` L2 ` 1, W \u010f maxtW1, W2u and N \u010f N1 ` N2. Lemma 5 (Summation). Let d1, d2, m P N`. Denote \u03a6 \u201c \u0159m i\u201c1 \u03b1i\u03a6i, where \u03a6i : Rd1 \u00d1 Rd2 are SignReLU neural networks with depth L, width Wi and number of weights Ni. Then \u03a6 : Rd1 \u00d1 Rd2 can be represented as a SignReLU neural network with depth L, width \u0159m i\u201c1 Wi, and number of weights \u0159m i\u201c1 Ni.\nwhere Ai jpyq \u201c Ai jy ` bi j are affine transforms with matrices Ai j P Rdi j\u02c6di j\u00b41 and bi j P Rdi j. Define \u03a6 :\u201c AL`1 \u02dd \u03c1 \u02dd AL \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03c1 \u02dd A1,\nwhere Ajpyq \u201c Ajy ` bj with\nfor j \u201c 1,\nfor 2 \u010f j \u010f L, and\nAL`1 \u201c ` \u03b11A1 L`1 \u00a8 \u00a8 \u00a8 \u03b1mAm L`1 \u02d8 , bL`1 \u201c \u00ff \u201c \u03b1ibi L`1,\n(9)\n(10)\n(11)\n(12)\n(13)\n(14)\n \u0159 Proof. By definition (1), we denote\nwhere Ai jpyq \u201c Ai jy ` bi j are affine transforms with matrices Ai j P Rdi j\u02c6di j\u00b41 and bi j P Rdi j. Define \u03a6 :\u201c AL`1 \u02dd \u03c1 \u02dd AL \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03c1 \u02dd A1,\nwhere Ajpyq \u201c Ajy ` bj with\nand when 2 \u010f j \u010f L ` 1,\n\u02dd \u201a \u02dd \u201a It is easy to see that \u03a6pxq \u201c ` \u03a61pxqJ, \u03a62pxqJ, . . . , \u03a6mpxqJ\u02d8J for any x P Rd. Since compared with \u03a6i, no other nonzero elements are introduced in Aj P Rdj\u02c6dj\u00b41 and dj \u010f \u0159m i\u201c1 di j , the SignReLU neural network \u03a6 is of depth L, width \u0159m i\u201c1 Wi, and number of weights \u0159m i\u201c1 Ni. Since in the following, we will frequently use Lemma 4, Lemma 5 and Lemma 6, when we handle the composition/summation/concatenation between neural networks, we will simply define the resulting neural network as the composition/summation/concatenation and recompute the depth, width, and number of weights accordingly. We first give the proof of Lemma 1, which follows some ideas of [62][Lemma 17]. Proof of Lemma 1. Since the product gate x \u00a8 y \u201c 1 2px ` yq2 \u00b4 x2 \u00b4 y2 relies on squaring function, we first construct a neural network that can realize x2. For any x P r\u00b41, 1s, we have \u00b4x \u00b4 1 \u010f 0, \u00b4x \u00b4 2 \u010f 0 and thereby\nProof of Lemma 1. Since the product gate x \u00a8 y \u201c 1 2px ` yq2 \u00b4 x2 \u00b4 y2 relies on squaring function, we first construct a neural network that can realize x2. For any x P r\u00b41, 1s, we have \u00b4x \u00b4 1 \u010f 0, \u00b4x \u00b4 2 \u010f 0 and thereby\nHence, combining (18) and (6) with the fact that 0 \u0103 px ` 2qpx ` 3q \u010f 12 for x P r\u00b41, 1s, we can get\n(15)\n(16)\n(17)\n(18)\nHence, adding (19) and (20), we find the following SignReLU neural network \u03a61 that realize x2\n\u00b4 \u00af The above formula corresponds to a realization of x2 by a SignReLU neural network with L \u201c 2, W \u201c 3, and N \u201c Then for x, y P r\u00b4M, Ms, substituting (21) into the following expression, the neural network\nis a realization of the product function x \u00a8 y on r\u00b4M, Ms \u02c6 r\u00b4M, Ms by a SignReLU neural network. Combining Lemma 4, Lemma 5 and (21), the network \u03a6 is of L \u201c 4, W \u010f 9 and N \u010f 63. This proves the statement in (i). To see (ii), noticing that 1 \u00b4 x{a \u0103 0 for x \u0105 a \u0105 0, we have that\nThus\nis able to be realized by a neural network with L \u201c 1, width W \u201c 1 and N \u201c 4. Based on (23), we define \u03a62px, yq as\nThen, by (22) and Lemma 4, the neural network \u03a63 :\u201c \u03a6 \u02dd \u03a62 is of L \u201c 6, W \u201c 9 and N \u010f 71 and satisfie \u03a63px, yq \u201c y x, for any x P ra, Ms and y P r\u00b4M, Ms.\nThen, by (22) and Lemma 4, the neural network \u03a63 :\u201c \u03a6 \u02dd \u03a62 is of L \u201c 6, W \u201c 9 and N \u010f 71 and satisfies \u03a63px, yq \u201c y x, for any x P ra, Ms and y P r\u00b4M, Ms. In (22), we can see all the quantities like the one in the second equality is able to be represented as a fully connected neural network with depth, width and number of weights only increase in terms of the input dimension. In the following, for simplicity, we will use this observation without explanation. Proof of Lemma 2. Without loss of generality, we consider the construction on domain r\u00b41, 1s. We start by proving that any polynomial on r\u00b41, 1s of degree at most n can be achieved by a SignReLU neural network. Then rational functions are shown by combining polynomial results and product & division gates in Lemma 1. Let us first consider how to realize a neural network \u03d5 that is fed px, w, y, zqJ P r\u00b41, 1s \u02c6 r\u00b4M, Ms3 and output px, axw ` by, w, z ` cwqJ P r\u00b41, 1s \u02c6 r\u00b4M, Ms3 for some given constants a, b, c. By Lemma 1, there exists a SignReLU neural network \u03c81px, yq such that \u03c81px, yq \u201c xy for any x, y P r\u00b4M, Ms. The following neural network \u03c82 obviously realizes an identity map thanks to the linear part of SignReLU (3)\nNotice that \u03c82 \u02dd \u03c82 \u201c \u03c82 on r\u00b4M, Ms. We will abuse \u03c82 to be any neural network that may have arbitrary depth and realizes the indentity map on r\u00b4M, Ms, for the sake of the conditions needed in Lemma 5 and Lemma 6. If \u03c82 has depth L, then the number of weights is no more than 4L.\n(21)\n(22)\n(23)\n(24)\n(25)\nsatisfy our needs and by Lemma 1, Lemma 5 and Lemma 6, it is easy to see that it is a SignReLU neural network wit L, W and N are some constants. Assume that the polynomial Pn has the expansion Pnpxq \u201c \u0159n i\u201c0 dipipxq, where pi are Legendre polynomials. Reca that Legendre polynomials pjpxq satisfies a three-term recurrence relationship\nwhere p0pxq \u201d 1, p1pxq \u201c x. Define\nlooooooooomooooooooon and \u03a61pxq :\u201c px, x, 1, d0qJ. Then according to (26), (27), letting a \u201c 2`1 1`1, b \u201c \u00b4 1 1`1 and c \u201c d1, we have \u03a62pxq \u201c \u03d5 \u02dd \u03a61pxq \u201c px, a\u03c81px, xq ` b\u03c82p1q, \u03c82pxq, \u03c82pd0q ` d1\u03c82pxqqT \u201c px, p2pxq, p1pxq, d0 ` d1xqT .  Assume that the following equality holds\nThen we have\nIf we choose a \u201c 2j`1 j`1 , b \u201c \u00b4 j j`1 and c \u201c dj for (31), any polynomial Pnpxq \u201c \u0159n i\u201c0 dipipxq can be realized by the following neural network\nwhere we choose M \u201c supxPr\u00b41,1s,j\u201c0,1,...,nt|x|, |pjpxq|, |Pjpxq|u. Combining (26), (31), (32) and Lemma 4, \u03a6 is a SignReLU neural network with L \u201c Opnq, W \u201c Op1q and N \u201c Opnq. Let Rpxq :\u201c ppxq{qpxq where ppxq and qpxq are polynomials with degrees to be n, m, respectively. Then there exist SignReLU neural networks \u03a6npxq \u201c ppxq and \u03a6mpxq \u201c qpxq. If m \u0103 n, then use a similar idea to combine (25) and (32), \u03a6mpxq can be easily extended to a SignReLU neural network with the same depth as \u03a6npxq. Hence, combining Lemma 1, Lemma 6 with the polynomial result, Rpxq can be realized by a SignReLU neural network \u03c81 p\u03a6npxq, \u03a6mpxqq with L \u201c Opmaxtn, muq, W \u201c Op1q and N \u201c Opmaxtn, muq.\nLet Rpxq :\u201c ppxq{qpxq where ppxq and qpxq are polynomials with degrees to be n, m, respectively. Then there exist SignReLU neural networks \u03a6npxq \u201c ppxq and \u03a6mpxq \u201c qpxq. If m \u0103 n, then use a similar idea to combine (25) and (32), \u03a6mpxq can be easily extended to a SignReLU neural network with the same depth as \u03a6npxq. Hence, combining Lemma 1, Lemma 6 with the polynomial result, Rpxq can be realized by a SignReLU neural network \u03c81 p\u03a6npxq, \u03a6mpxqq with L \u201c Opmaxtn, muq, W \u201c Op1q and N \u201c Opmaxtn, muq.\n# The following lemma shows how SignReLU nets can approximate Re\nProof of Lemma 3. Define \u03a6pxq :\u201c \u03c1pnxq n , it is easy to check that for any x P R, we have |\u03c3pxq \u00b4 \u03a6pxq| \u010f 1 n. Then the first statement follows by taking n \u011b 1 \u03b5. To prove (ii), we define \u03a6mpxq :\u201c \u03c1\u02dd\u03c1\u02dd\u00a8 \u00a8 \u00a8\u02dd\u03c1pxq with m compositions which is equal to x when x is nonnegative and x 1\u00b4mx otherwise. Then \u03a6mpxq \u201c \u03c3pxq for x \u011b 0 and |\u03a6mpxq \u00b4 \u03c3pxq| \u201c \u00b4x 1\u00b4mx \u010f 1 m for x \u0103 0. Hence the statement in (ii) follows. For the last statement, we choose the network \u03a6pxq :\u201c \u03a6n \u02dd \u03a6n \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03a6npxq \u201c x 1\u00b4mnx with m compositions. It is easy to see the conclusion.\n(27)\n(28)\n(30)\n(32)\nProof of Proposition 1. The idea of the proof is to construct a SignReLU neural network \u03d50 that approximates e\u00b4|x|, and then combine it with previous results for the product gate (Lemma 1) and rational functions (Lemma 2) to obtain approximation rates for target functions. Step 1: Constructing SignReLU net \u03d50 that approximates e\u00b4x. Let \u03c8pxq :\u201c 1 \u03bb p\u03c1p\u03bbxq ` \u03c1p\u00b4\u03bbxqq for x P R and \u03bb \u0105 1. It is easy to see that \" \n\" \u00b4 ich implies that 0 \u010f \u03c8pxq \u010f |x| and \u02c7\u02c7\u03c8pxq \u00b4 |x| \u02c7\u02c7 \u010f 1 \u03bb for any x P R. Furthermore, since 1 \u00b4 e\u00b4x \u010f x and \u03c8pxq \u010f 1 for any x P R, we have\n\" which implies that 0 \u010f \u03c8pxq \u010f |x| and \u02c7\u02c7\u03c8pxq \u00b4 |x| \u02c7\u02c7 \u010f 1 \u03bb for any x P R. Furthermore, since 1 \u00b4 e\u00b4x \u010f x an e\u00b4\u03c8pxq \u010f 1 for any x P R, we have\n\u02c7\u02c7\u02c7\u02c7 \u02c7\u02c7\u02c7\u02c7\u02c7\u02c7 \u02c7\u02c7\u02c7\u02c7\u02c7\u02c7 \u02c7\u02c7\u02c7\u02c7\u02c7\u02c7 On the other hand, by a classical result on rational approximation to e\u00b4x, x \u011b 0 (see, e.g.[33]), for any n P N, there exists a polynomial qpxq of degree at most n such that\nCombining (33) and (34), we have\nFinally, taking \u03bb :\u201c 3n, by Lemma 2, we can construct a SignReLU neural network \u03d50pxq with depth L \u201c Opnq, width W \u201c Op1q and number of weights N \u201c Opnq such that \u03d50pxq \u201c Q \u02dd \u03c1 \u02dd \u03c8pxq \u201c 1 qp\u03c8pxqq and \u02c7\u02c7\u03d50pxq \u00b4 e\u00b4|x|\u02c7\u02c7 \u010f 31\u00b4n where Qpxq is a SignReLU neural network and satisfies Qpxq \u201c 1 qpxq and \u03c1 \u02dd \u03c8pxq \u201c \u03c8pxq since \u03c8pxq \u011b 0, @x P R. Step 2: Approximating e\u00b4}x}1. Let \u03c8dpxq \u201c \u0159d j\u201c1 \u03c8pxjq and \u03a6dpxq \u201c Q \u02dd \u03c1 \u02dd \u03c8dpxq. Applying the same arguments as in (35), we have that\n\u02c7\u02c7 \u02c7\u02c7 Then by taking \u03bb \u201c 3d, we can get the desired result in d-dimensional case. Step 3: Approximating e\u00b4}x}2 2. By Lemma 1, there exist SignReLU neural networks \u03d5ipxq \u201c x2, i \u201c 1, . . . , d such that }x}2 2 \u201c \u0159d i\u201c1 \u03d5ipxiq. Define \u03a6dpxq :\u201c Q \u02dd \u03c1 \u00b4\u0159d i\u201c1 \u03d5ipxiq \u00af . Then combining (34), we have\nStep 3: Approximating e\u00b4}x}2 2. By Lemma 1, there exist SignReLU neural networks \u03d5ipxq \u201c x2, i \u201c 1, . . . , d such that }x}2 2 \u201c \u0159d i\u201c1 \u03d5ipxiq. Define \u03a6dpxq :\u201c Q \u02dd \u03c1 \u00b4\u0159d i\u201c1 \u03d5ipxiq \u00af . Then combining (34), we have\n\u02c7\u02c7\u02c7 \u02c7\u02c7\u02c7 etting n \u201c lnp\u03b5\u00b41q ` 1 and combining (36) with Lemma 5 and Lemma 4, we can get the desired result.\nProof of Theorem 1. Given any fixed rational activation function Rpxq [6], it can be produced by a SignReLU network with fixed size (only depends on the degree of Rpxq, Lemma 2), and thus the first statement holds. Let tA\u2113uL \u2113\u201c1 be a collection of linear transforms. For any A\u2113pyq \u201c A\u2113y ` b\u2113for some matrix A\u2113and vector b\u2113, we denote a\u2113:\u201c maxt}A\u2113}8,8, }b\u2113}8u, where }A}8,8 :\u201c maxijtAiju. Without loss of generality, we assume that a\u2113\u010f 1 for all \u2113. Define a ReLU neural network f pLq \u03c3 with L \u201c L \u00b4 1, W \u201c W and N \u201c N as\n(33)\n(34)\n(35)\n(36)\n(36)\n(37)\nfor some constant \u03b4 \u0105 0. In the following, we denote f p\u2113q \u03c3 pxqj the j-th element of f p\u2113q \u03c3 pxq. Since a\u2113\u010f 1 an dL\u00b41 \u010f W, we have \u02c7 \u02c7\n\u02c7\u02c7\u02c7 If L \u201c 1, then by (40), we can get\n\u02c7 where in the last inequality, we used |\u03c3pxq \u00b4 \u03b4\u03c1px{\u03b4q| \u010f \u03b4, @x P R. Assume that for L \u201c \u2113\u00b4 1, the following inequality holds\n\u02c7\u02c7\u02c7 \u02c7\u02c7\u02c7 for some constant C\u2113\u00b41 \u0105 0. Combining (40) with (42) and a\u2113\u010f 1, we obtain \u02c7 \u02c7\n\u02c7 \u02c7 where the last inequality follows from |\u03c3pxq \u00b4 \u03c3pyq| \u010f |x \u00b4 y| for any x, y and |\u03c3p\u03b4xq \u00b4 \u03b4\u03c1pxq| \u010f \u03b4 for any x. Hence, combining (43), (39) and choosing \u03b4 which satisfies WCL\u00b41\u03b4 \u010f \u03b5, we conclude \u02c7\u02c7\u02c7f pLq \u03c3 pxq \u00b4 f pLq \u03c1 pxq \u02c7\u02c7\u02c7 \u010f \u03b5 for any x P Rd. The proof is completed.\n(39)\n(40)\n(41)\n(42)\n(43)\n# B Proof of Theorem 2, Theorem 4, and Theorem 5\nWe first derive a lemma of orthogonal expansions, which will play a key role in sequential proofs Lemma 7. Let r, d P N`, p \u011b 1 and n P Nd. 1. For any f P W r p pr\u00b41, 1sd, wq and N P N, there exists cn P R, }n}8 \u010f N, such that\nwhere }n}\u03c0 \u201c \u015bd j\u201c1 maxt1, nju, } \u00a8 }p,w is the weighted Lp norm with w.\nNoting that, in case d \u201c 1,\nWe apply the change of variable x \u201c cosp\u03b8q to (46). Note that the Lebesgue measure \u00b5 of zero \u00b5p0q \u201c 0, sinp\u03b8q \u201c \u02d8 a 1 \u00b4 cos2p\u03b8q. Since d\u03b8 \u201c \u00b4 dx sinp\u03b8q, sinp\u03b8q \u0105 0, \u03b8 P p0, \u03c0s and sinp\u03b8q \u0103 0, \u03b8 P r\u00b4\u03c0, 0q, we get\nHence\nA similar approach shows the following results for d \u0105 1\nhere recall that wpxq \u201c 2d \u015bd j\u201c1p1 \u00b4 x2 jq\u00b41{2. We can see from (49) that if f P Lp ` r\u00b41, 1sd, w \u02d8 , then Gf P Lp ` r\u00b4\u03c0, \u03c0sd\u02d8 . Since Gf is even, we obtain the following Fourier expansion of Gf in the sense of Lp\npq \u201c \u015b \u201cp \u00b4 q P ` r\u00b4s \u02d8  P Lp ` r\u00b4\u03c0, \u03c0sd\u02d8 . Since Gf is even, we obtain the following Fourier expansion of Gf in the sense of Lp\nwhere the Fourier coefficients\n(45)\n(47)\n(48)\n(49)\n(50)\nor any N P N, setting \u039b\u2113,N \u201c tn P Nd : |n\u2113| \u011b Nu, \u2113\u201c 1, . . . , d, then \u02c7\nwhere the first step we use the fact y f prqpnq \u201c pinqr \u02c6fpnq, the second and last step follows from the Littlewood-Pale inequalities with the function\nwhere the first step we use the fact y f prqpnq \u201c pinqr \u02c6fpnq, the second and last step follows from the Littlewood-Paley nequalities with the function\n\u00ff z \u017a and g2p\u03b8q \u201c \u0159 nP\u039b\u2113,N z Br \u2113Gfpnq \u015bd j\u201c1 cospnj\u03b8jq. Here recall the Littlewood-Paley inequalities \u203a \u203a\n\u203a\u203a\u203a \u203a\u203a\u203a \u203a\u203a\u203a \u203a\u203a\u203a where D is an index set and t\u03c8IuIPD is an orthogonal system, A \u201e B means c1B \u010f A \u010f c2B for some positiv constants c1, c2. By substitution \u03b8j \u201c arccospxjq into (51) and using (49), we have \u203a \u203a\nby making\n \u015b (ii) there exists a function \u03a6 realized by a SignReLU neural network with L \u010f 5rlog2 ns ` 5rlog2 ds, W \u010f 10n and N \u010f 400nd such that \u03a6pxq \u201c x\u03b2, for any x P r\u00b41, 1sd.\n(51)\nwhere \u03d50 is a SignReLU neural network that satisfies \u03d50py, zq \u201c yz for any y, z P r\u00b41, 1s. Iteratively, it is easy to verify the output of each \u03d5\u2113in (52) satisfies\nWe observe from (53) that \u03a6pxq :\u201c \u03d5Kpxq satisfies \u03a6pxq \u201c \u015b i\u010fd xi for any x P r\u00b41, 1sd. Since each \u03d5\u2113is the concatenation of 2K\u00b4\u2113product gate \u03d50, by Lemma 1 and Lemma 6, \u03d5\u2113is of L \u201c 4, W \u010f 9 \u00a8 2K\u00b4\u2113and N \u010f 63 \u00a8 2K\u00b4\u2113. Combining (52) with Lemma 4, the SignReLU neural network \u03a6 is of L \u201c 5K \u00b4 1 \u201c 5 log2 d \u00b4 1, W \u201c max\u2113\u201c1,...,K 9 \u00a8 2K\u00b4\u2113\u010f 10d and N \u201c 63 \u0159K \u2113\u201c1 2K\u00b4\u2113\u010f 130d. If 2K\u00b41 \u0103 d \u0103 2K for some integer K, we can set some xi \u201d 1 for i \u0105 d. To prove the second statement, we first construct a SignReLU neural network \u03a6\u03b2pxq that realizes x\u03b2, x P r\u00b41, 1s for some integer \u03b2 \u011b 1. Denote \u03c8pxq \u201c \u03c1p1x ` 1q \u00b4 1 where 1 P R\u03b2 is the all-one vector. Since x P r\u00b41, 1s, \u03c8pxq \u201c px, x, . . . , xqT P r\u00b41, 1s\u03b2, the SignReLU neural network \u03d5\u03b2pxq :\u201c \u03a6\u03b2 \u02dd \u03c8pxq with \u03a6\u03b2pxq \u201c \u015b\u03b2 i\u201c1 xi equals x\u03b2 on r\u00b41, 1s and by Lemma 4, \u03d5\u03b2 is of L \u201c 5rlog2 \u03b2s, W \u010f 10\u03b2 and N \u010f 130\u03b2. Denote \u03a6\u03b2pxq :\u201c \u03a6 p\u03a6\u03b21px1q, . . . , \u03a6\u03b2dpxdqq where \u03a6pxq \u201c \u015bd i\u201c1 xi for any x P r\u00b41, 1sd. Obviously, \u03a6\u03b2pxq \u201c x\u03b2. Since \u03b2i \u010f n and we can expend \u03a6\u03b2ipxq of L \u201c 5rlog2 \u03b2is, W \u010f 10\u03b2i and N \u010f 130\u03b2i, with (25) and Lemma 4, to a SignReLU neural network \u02dc\u03a6\u03b2i which is of L \u201c 5rlog2 ns, W \u010f 10n and N \u010f 130n ` 20rlog2 ns \u010f 200n and satisfies \u02dc\u03a6\u03b2ipxiq \u201c \u03a6\u03b2ipxiq for any xi P r\u00b41, 1s. Hence, by Lemma 6 and Lemma 4, the SignReLU neural network \u02dc\u03a6\u03b2pxq :\u201c \u03a6 \u00b4 \u02dc\u03a6\u03b21px1q, . . . , \u02dc\u03a6\u03b21px1q \u00af is of L \u010f 5rlog2 ns ` 5rlog2 ds, W \u010f 10nd and N \u010f 130d ` 200nd \u010f 400nd and satisfies \u02dc\u03a6\u03b2pxq \u201c x\u03b2, @x P r\u00b41, 1sd.\nProof of Theorem 2 and Theorem 5 (i) . The key idea is to employ Lemma 7 to construct a SignReLU network tha outputs a polynomial on r\u00b41, 1sd that approximates f.\n# Estimation for approximating weighted Sobolev functions.\nAccording to Lemma 2, there exist SignReLU neural networks \u03d5ipxiq with L \u201c CN, W \u010f C and N \u010f CN for som constant C such that \u03d5ipxiq \u201c xN i , @xi P r\u00b41, 1s. Here for depth, width and number of weights, the constant C ma be different, we use a single C for simplicity. We denote \u03a61pxq :\u201c p\u03d51px1q, . . . , \u03d5dpxdqq, which, using Lemma 6, is o L \u201c CN, W \u201c Cd and N \u201c CNd.\nAccording to Lemma 2, there exist SignReLU neural networks \u03d5ipxiq with L \u201c CN, W \u010f C and N \u010f CN for some constant C such that \u03d5ipxiq \u201c xN i , @xi P r\u00b41, 1s. Here for depth, width and number of weights, the constant C may be different, we use a single C for simplicity. We denote \u03a61pxq :\u201c p\u03d51px1q, . . . , \u03d5dpxdqq, which, using Lemma 6, is of L \u201c CN, W \u201c Cd and N \u201c CNd. Since (31) shows intermediate layers of \u03d5ipxiq output xni i , ni \u201c 1, . . . , N \u00b4 1. Based on \u03a61pxq, We can add at most d \u0159N\u00b41 i\u201c1 i \u010f N 2d identity mappings (25) to intermediate layers of \u03a61pxq (those identity mappings keep all xni i to the last output layer), and by Lemma 6 and Lemma 4 the resulting new network \u02dc\u03a61pxq outputs all xni i , i \u201c 1, . . . , d, ni \u201c 1, . . . , N, which is of L \u010f CN, W \u010f Cd ` N 2d and N \u010f CNd ` 4N 2d. Let \u03a62pxq be the SignReLU neural network that takes all xni i , i \u201c 1, . . . , d, ni \u201c 1, . . . , N as input and outputs xn for all }n}8 \u010f N. Obviously, by Lemma 6 and Lemma 8, \u03a62 can be constructed by the concatenation of neural networks \u015bd i\u201c1 xni i which take pxn1 1 , . . . , xnd d q as inputs, for all pn1, . . . , ndq P Nd. Since #tn : }n}8 \u010f Nu \u201c pN ` 1qd, using Lemma 6 and Lemma 8, the network \u03a62pxq is of L \u010f 5 log2 d, W \u010f 10dpN ` 1qd and N \u010f 130dpN ` 1qd. Denote \u03a63pxq \u201c \u0159 }n}8\u010fN cn\u03a62pxqn, where \u03a62pxqn \u201c xn. Then combining \u02dc\u03a61pxq, \u03a62pxq and \u03a63pxq and using Lemma 4, we can get a SignReLU neural network \u03a6pxq :\u201c \u03a63 \u02dd \u03a62 \u02dd \u02dc\u03a61pxq \u201c \u0159 }n}8\u010fN cnxn for any x P r\u00b41, 1sd and is of L \u010f CN ` 5 log2 d ` 2, W \u010f Cd ` N 2d ` 10dpN ` 1qd and N \u010f 150dpN ` 1qd ` CNd ` 4N 2d. According to Lemma 7, given any f with }f}W r p \u010f 1, there exists a polynomial Ppxq \u201c \u0159 }n}8\u010fN cnxn such that }f \u00b4 P}p,w \u010f CN \u00b4r. Setting N \u201c C 1 r \u03b5\u00b4 1 r and choosing \u03a6pxq \u201c Ppxq for any x P r\u00b41, 1sd, we conclude that \u03a6 is of L \u201c O \u00b4 \u03b5\u00b4 1 r \u00af , W \u201c O \u00b4 \u03b5\u00b4 d r \u00af and N \u201c O \u00b4 \u03b5\u00b4 d r \u00af and approximate f with error }f \u00b4 \u03a6}p,w \u010f \u03f5.\n(53)\n# Estimation for approximating weighted Korobov functions.\nGiven any function f in the Koborov space with unit norm, we use a similar idea to give the bound. Since there exixts a rational neural network that produce xn with L \u201c Oplog2 nq and N \u201c O ` plog2 nq2\u02d8 (Proposition 10, [6]), combining Lemma 8 (i) and Lemma 2, there exists a SignReLU neural network that produce xn, }n}8 \u010f N, on r\u00b41, 1sd with L \u201c Oplog2 Nq and N \u201c O ` plog2 Nq2\u02d8 . Notice that #tn : }n}\u03c0 \u010f Nu \u201c OpNpln Nqd\u00b41q. Hence, by Lemma 7 and Lemma 5, there exists a SignReLU neural network that produce \u0159 }n}\u03c0\u010fN cnxn with L \u201c Oplog2 Nq and N \u201c O ` Nplog2 Nqd`1\u02d8 that approximates the given Koborov function f with error CN \u00b4r plog2 Nqpd\u00b41qpr`1q. Choosing N \u201c \u03b5\u00b4 1 r ` log2p\u03b5\u00b41q \u02d8dpr`1q{r and applying Lemma 8, we have a neural network \u03a6 \u201c \u0159 }n}\u03c0\u010fN cnxn, x P r\u00b41, 1sd with L \u201c O ` log2p\u03b5\u00b41q \u02d8 and N \u201c O \u00b4 \u03b5\u00b4 1 r ` log2p\u03b5\u00b41q \u02d8p2dr`d`rq{r\u00af such that }f \u00b4 \u03a6}p,w \u010f \u03b5.\n! and \u2126c n \u201c r\u00b41, 1sd \u00b4 \u2126n. Then (55) implies\n! \u201c r\u00b41, 1sd \u00b4 \u2126n. Then (55) implies\n\u201c where the constant Cw only depends on w and constant Cp,w only depends on p, w. In the following, we denote \u02dc\u03c7\u2126pxq \u201c Rn \u02dd pg \u00b4 hq pxq. Let functions \u02dcf1pxq and \u02dcf2pxq be SignReLU neural networks that satisfy } \u02dcf1 \u00b4 f1}p,w \u010f \u03b5 and } \u02dcf2 \u00b4 f2}p,w \u010f \u03b5 (Theorem 2). Denote the product gate \u03c8px, yq \u201c xy the SignReLU neural network in Lemma 1. Then we have \u203a \u00b4 \u00af\u203a\n\u203a\u203a\u203af1 ` f2\u03c7\u2126\u00b4 \u00b4 \u02dcf1 ` \u03c8 ` \u02dcf2, \u02dc\u03c7\u2126 \u02d8\u00af\u203a\u203a\u203a p,w,\u2126cn \u010f }f1 \u00b4 \u02dcf1}p,w,\u2126cn ` }f2\u03c7\u2126\u00b4 f2 \u02dc\u03c7\u2126}p,w,\u2126cn ` }f2 \u02dc\u03c7\u2126\u00b4 \u02dcf2 \u02dc\u03c7\u2126}p,w,\u2126c n \u010f }f1 \u00b4 \u02dcf1}p,w,\u2126cn ` }\u03c7\u2126\u00b4 \u02dc\u03c7\u2126pxq}p,w,\u2126c n ` }f2 \u00b4 \u02dcf2}p,w,\u2126c n \u010f 2\u03b5 ` Cp,we\u00b4?n,\n\u203a\u203a\u203a \u00b4 ` \u02d8\u00af\u203a\u203a\u203a \u010f }f1 \u00b4 \u02dcf1}p,w,\u2126cn ` }f2\u03c7\u2126\u00b4 f2 \u02dc\u03c7\u2126}p,w,\u2126cn ` }f2 \u02dc\u03c7\u2126\u00b4 \u02dcf2 \u02dc\u03c7\u2126}p,w,\u2126c n \u010f }f1 \u00b4 \u02dcf1}p,w,\u2126cn ` }\u03c7\u2126\u00b4 \u02dc\u03c7\u2126pxq}p,w,\u2126c n ` }f2 \u00b4 \u02dcf2}p,w,\u2126c n \u010f 2\u03b5 ` Cp,we\u00b4?n,\n(54)\n(55)\n(57)\nwhere in the second step we use the property (55) and in the third step we use the bound of f2 on r\u00b41, 1s and (56). Denote \u02dcf :\u201c \u02dcf1 ` \u03c8 ` \u02dcf2, \u02dc\u03c7\u2126 \u02d8 \u201c \u02dcf1 ` \u03c8 ` \u02dcf2, Rn \u02dd pg \u00b4 hq \u02d8 and choose n \u201c ` lnp\u03b5\u00b41q \u02d82. Then by Lemma 2, Rn can be produced by a SignReLU neural network of L \u201c O \u00b4` lnp\u03b5\u00b41q \u02d82\u00af , W \u201c O p1q and N \u201c O \u00b4` lnp\u03b5\u00b41q \u02d82\u00af . Combining Lemma 4, Lemma 5 and Lemma 6, the function \u02dcf is able to be implemented by a SignReLU neural network with L \u201c O \u00b4 \u03b5\u00b4 1 r \u00af , W \u201c O \u00b4 \u03b5\u00b4 d r \u00af and N \u201c O \u00b4 \u03b5\u00b4 d r \u00af and satisfies }f \u00b4 \u02dcf}p,w,\u2126cn \u010f 3\u03b5. Notice that (55) guarantees the statement for functions from W r 8 by using a similar step as (56) and (57). Combining Theorem 5 with the above constructions, we can show the estimation of approximating piecewise Korobov functions similarly.\n# C Proof of Theorem 6\nProof of Theorem 6. We call a rational function Rpxq \u201c ppxq{qpxq a type pm, nq rational function if the degree of polynomials ppxq and qpxq is m and n, respectively. Set n \u011b r. Notice that Theorem 7.2 [Chapter 7, [33]] shows that given f P V rr0, 1s, there exists a rational function R of type pn, nq, such that\nfor some constant Cr depending only on r.\nLet f P Vr d with expression fpxq \u201c \u015bd i\u201c1 fpxiq. Denote Ri the rational function that approximates fi with error (58 i \u201c 1, . . . , d. Then we can get\nwhere Cr,d is a constant and\nBy Lemma 8, we denote the SignReLU neural network \u03d5pxq \u201c \u015bd i\u201c1 xi. Define \u03a6pxq :\u201c \u03d5 pRpx1q, . . . , Rdpxdqq and let n \u201c pCr,d{\u03b5q1{pr`1q. Combining Lemma 4, Lemma 6, Lemma 2 and Lemma 8 with (59), we can see \u03a6 is able to be represented by a SignReLU neural network with L \u201c O \u00b4 \u03b5\u00b4 1 r`1 \u00af , W \u201c Op1q and N \u201c Op\u03b5\u00b4 1 r`1 q.\n(58)\n# D Proof of Theorem 7\nProof of Theorem 7. Theorem 3.1 [51] shows that for any f P C ` r0, 1sd\u02d8 , there exists a multivariate polynomial Ppxq \u201c \u0159 }n}8\u010fNd cnxn such that }f\u00b4P}L8pr0,1sq \u010f 5 4 \u0159d i\u201c1 wi fp1{Nq. A similar proof as in the proof of Theorem 2 verifies the statement.\n",
    "paper_type": "method",
    "attri": {
        "background": "Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance.",
        "problem": {
            "definition": "The problem this paper addresses is the need for improved activation functions in deep neural networks to enhance their approximation capabilities.",
            "key obstacle": "Existing methods primarily utilize ReLU-type activations, which have limitations in approximation performance, particularly for complex functions."
        },
        "idea": {
            "intuition": "The idea for SignReLU was inspired by the need for an activation function that can efficiently approximate a broader class of functions, including rational and piecewise smooth functions.",
            "opinion": "SignReLU is proposed as a new activation function that combines properties of existing functions while addressing their shortcomings in approximation tasks.",
            "innovation": "The key innovation of SignReLU lies in its ability to produce rational functions and effectively approximate ReLU functions, which is not achievable by traditional activation functions."
        },
        "method": {
            "method name": "SignReLU Neural Network",
            "method abbreviation": "SignReLU",
            "method definition": "SignReLU is an activation function designed to improve the approximation capabilities of deep neural networks by enabling the realization of rational functions and enhancing performance in various tasks.",
            "method description": "The SignReLU activation function allows neural networks to approximate complex functions more effectively than traditional ReLU or rational activations.",
            "method steps": [
                "Define the SignReLU activation function.",
                "Construct a neural network architecture incorporating SignReLU.",
                "Train the network on relevant datasets.",
                "Evaluate the approximation performance against baseline methods."
            ],
            "principle": "The effectiveness of SignReLU in solving the problem is supported by its mathematical properties that allow for better representation of complex functions and reduction in approximation error."
        },
        "experiments": {
            "evaluation setting": "Numerical experiments were conducted comparing SignReLU with existing activations such as ReLU, Leaky ReLU, and ELU, using datasets like MNIST, CIFAR10, and Caltech101.",
            "evaluation method": "The performance of each activation function was assessed based on mean squared error (MSE) and classification accuracy across various tasks, including regression, classification, and image denoising."
        },
        "conclusion": "The experiments demonstrated that SignReLU significantly improves approximation performance compared to traditional activation functions, highlighting its potential for enhancing deep learning applications.",
        "discussion": {
            "advantage": "SignReLU provides a strong ability to approximate a wider range of functions, including rational functions, and improves convergence rates in training.",
            "limitation": "The method may still encounter challenges with extremely high-dimensional data or specific types of functions that are not well-represented by the activation.",
            "future work": "Future research could explore further optimizations of the SignReLU function and investigate its applicability in other areas of machine learning and neural network design."
        },
        "other info": {
            "acknowledgments": "The authors acknowledge support from various research grants and institutions.",
            "keywords": [
                "Deep neural networks",
                "Activation function",
                "Approximation power",
                "SignReLU activation"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "3",
            "key information": "Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers."
        },
        {
            "section number": "3.1",
            "key information": "The key innovation of SignReLU lies in its ability to produce rational functions and effectively approximate ReLU functions, which is not achievable by traditional activation functions."
        },
        {
            "section number": "3.2",
            "key information": "The performance of each activation function was assessed based on mean squared error (MSE) and classification accuracy across various tasks, including regression, classification, and image denoising."
        },
        {
            "section number": "8.3",
            "key information": "Future research could explore further optimizations of the SignReLU function and investigate its applicability in other areas of machine learning and neural network design."
        }
    ],
    "similarity_score": 0.5386884976185301,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1730_natur/papers/SignReLU neural network and its approximation ability.json"
}