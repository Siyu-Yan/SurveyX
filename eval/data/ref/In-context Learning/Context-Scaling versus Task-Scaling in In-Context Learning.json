{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.12783",
    "title": "Context-Scaling versus Task-Scaling in In-Context Learning",
    "abstract": "Transformers exhibit In-Context Learning (ICL), where these models solve new tasks by using examples in the prompt without additional training. In our work, we identify and analyze two key components of ICL: (1) context-scaling, where model performance improves as the number of in-context examples increases and (2) task-scaling, where model performance improves as the number of pre-training tasks increases. While transformers are capable of both context-scaling and task-scaling, we empirically show that standard Multi-Layer Perceptrons (MLPs) with vectorized input are only capable of task-scaling. To understand how transformers are capable of context-scaling, we first propose a significantly simplified transformer architecture without key, query, value weights. We show that it performs ICL comparably to the original GPT-2 model in various statistical learning tasks including linear regression, teacher-student settings. Furthermore, a single block of our simplified transformer can be viewed as data dependent \u201cfeature map\u201d followed by an MLP. This feature map on its own is a powerful predictor that is capable of context-scaling but is not capable of task-scaling. We show empirically that concatenating the output of this feature map with vectorized data as an input to MLPs enables both context-scaling and task-scaling. This finding provides a simple setting to study context and task-scaling for ICL.",
    "bib_name": "abedsoltan2024contextscalingversustaskscalingincontext",
    "md_text": "# Context-Scaling versus Task-Scaling in In-Context Learning\nAmirhesam Abedsoltan1 Adityanarayanan Radhakrishnan2,3 Jingfeng Wu5 Mikhail Belkin1,4\n# Amirhesam Abedsoltan1 Adityanarayanan Radhakrishnan2,3 Jingfeng Wu5 Mikhail Belkin1,4\n1Department of Computer Science and Engineering, UC San Diego 2Eric and Wendy Schmidt Center, Broad Institute of MIT and Harvard 3School of Engineering and Applied Sciences, Harvard University 4Halicioglu Data Science Institute, UC San Diego 5Simons Institute, UC Berkeley\nAbstract\n# Abstract\nTransformers exhibit In-Context Learning (ICL), where these models solve new tasks by using examples in the prompt without additional training. In our work, we identify and analyze two key components of ICL: (1) context-scaling, where model performance improves as the number of in-context examples increases and (2) task-scaling, where model performance improves as the number of pre-training tasks increases. While transformers are capable of both context-scaling and task-scaling, we empirically show that standard Multi-Layer Perceptrons (MLPs) with vectorized input are only capable of task-scaling. To understand how transformers are capable of context-scaling, we first propose a significantly simplified transformer architecture without key, query, value weights. We show that it performs ICL comparably to the original GPT-2 model in various statistical learning tasks including linear regression, teacher-student settings. Furthermore, a single block of our simplified transformer can be viewed as data dependent \u201cfeature map\u201d followed by an MLP. This feature map on its own is a powerful predictor that is capable of context-scaling but is not capable of task-scaling. We show empirically that concatenating the output of this feature map with vectorized data as an input to MLPs enables both context-scaling and task-scaling. This finding provides a simple setting to study context and task-scaling for ICL.\narXiv:2410.12783v1\n# 1 Introduction\n<div style=\"text-align: center;\">Pre-trained large language models exhibit In-Context Learning (ICL) capabilities, allowing them to ada to new tasks based exclusively on input without updating the underlying model parameters [5].</div>\nInput (Prompt)\nOutput\nTask (Pattern)\n(1, 2, 3), (4, 5, 9), (10, \u22129, 1), (5, 6, ?)\n11\nIn each triplet (a, b, c): c = a + b\n(4, 3, 1), (9, 0, 9), (10, 8, 2), (17, 17, ?)\n0\nIn each triplet (a, b, c): c = a \u2212b\n(1, 2, 5), (2, 3, 8), (3, 4, 11), (5, 6, ?)\n17\nIn each triplet (a, b, c): c = a + 2b\n(2, 1, 7), (3, 4, 18), (5, 2, 16), (4, 3, ?)\n17\nIn each triplet (a, b, c): c = 2a + 3b\nIn the table above, we provide an example of ICL. We prompt the Claude language model [3] using sequences of N triples of numbers (N = 4). In each row of the table, the first N \u22121 triples follow a given pattern. The model was able to infer the pattern and to fill in the missing number denoted by the question mark correctly. What makes this in-context learning possible? Recent research analyzed various ICL problems where, for example, the task data were generated using linear regression, student-teacher neural networks, and decision trees including [1, 2, 4, 10, 20, 28, 30]. In these problems, a transformer was first pre-trained on T tasks where the data in each task was generated from a given family of functions. For example, each task may involve predicting the last element in a tuple\nas a linear combination of the other elements, as was shown in the table above. The pre-trained transformer was then tested on N samples from a new task that is drawn from the same general family but was not seen during pretraining. Such a setup allows for understanding the effect of various factors including model architecture, the number of pre-training tasks T, and context length N on ICL. The ability of models to learn in context has been broadly defined as their ability to generalize to unseen tasks based on context examples without updating the model parameters. We observe that there are two distinct aspects of generalization in ICL. The first, which we call context-scaling, refers to the ability of the model to improve as the context length N increases while the number of pre-training tasks T is fixed. The second, task-scaling refers to the ability of a model to improve as T increases while N is fixed.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/faf9/faf96aa9-11f4-470e-86d4-7bd4fb5f59bb.png\" style=\"width: 50%;\"></div>\n# (A) Task-scaling\nFigure 1: Task-scaling and context-scaling of GPT-2 architecture transformers versus MLPs for ICL with linear regression tasks. (A) Task-scaling abilities of these models with 10 in-context examples. (B) Contextscaling abilities of these models with 105 (left) and 106 (right) pre-training tasks. Experimental details are provided in Appendix A.\nFigure 1: Task-scaling and context-scaling of GPT-2 architecture transformers versus MLPs for ICL with linear regression tasks. (A) Task-scaling abilities of these models with 10 in-context examples. (B) Contextscaling abilities of these models with 105 (left) and 106 (right) pre-training tasks. Experimental details are provided in Appendix A. It is a priori unclear whether a model capable of context-scaling is also capable of task-scaling and viceversa. For example, as we show in Figure 1A, both transformers and standard Multi-Layer Perceptrons (MLPs) are capable of task-scaling on ICL of linear regression tasks. In contrast, only transformers benefit from an increasing number of context examples as shown in Figure 1B. This raises the question:\nIt is a priori unclear whether a model capable of context-scaling is also capable of task-scaling and vice versa. For example, as we show in Figure 1A, both transformers and standard Multi-Layer Perceptron (MLPs) are capable of task-scaling on ICL of linear regression tasks. In contrast, only transformers benef from an increasing number of context examples as shown in Figure 1B. This raises the question:\nWhat mechanism enables models such as transformers, but not ML\nTo identify such a mechanism, we begin by constructing a bare-bones transformer with all key, query, and value weight matrices fixed to be the identity matrix. We refer to our simplified model as Simplified GPT (SGPT). Despite its simplicity, we find that SGPT is competitive with GPT-2 architecture transformers [19] for a variety of ICL problems considered in [10] including linear regression, student-teacher networks, decision trees, and sparse linear regression. Furthermore, we find that one block of SGPT applies a data dependent feature map that enables contextscaling. To illustrate how such a feature map can be effective for context-scaling, consider the following input data for ICL:\nwhere xi \u2208Rd and our goal is to predict yN. SGPT first applies a feature map \u03c8 : RN\u00d7(d+1) \u2192RN\u00d7(d+1) to A and then trains an MLP on the last row of \u03c8(A), denoted \u03c8(A)N,:. By varying \u03c8, we show that the scalar \u03c8(A)N,d+1 itself is an effective estimate for yN. For example, when \u03c8(A) = (AA\u22a4)A, \u03c8(A)N,d+1 implements one-step of Gradient Descent (GD) for linear regression (using context examples (xi, yi)N\u22121 i=1 ) [26]. When the data follow an isotropic Gaussian distribution, this estimator is consistent (as both the numbers of pre-training tasks and context examples grow) and nearly matches the optimally tuned ridge regression [17, 28]. Furthermore, we show that \u03c8(A)N,d+1 can implement the Hilbert estimate [9], which provides a statistically consistent estimate for general families of tasks beyond linear regression as the context length N approaches infinity. As such, our results provably establish that one attention layer alone is capable of context-scaling for any family of tasks.\n(1)\n# Further, we empirically show that these features can enhance the capabilities of MLPs for context-scaling We concatenate features from \u03c8(A)N,: with the vectorized input,\nFurther, we empirically show that these features can enhance the capabilities of MLPs for context-scaling. We concatenate features from \u03c8(A)N,: with the vectorized input,\nAv := \ufffd xT 1 y1 xT 2 y2 . . . xT N 0 \ufffdT ,\n\ufffd \ufffd and provide the combined input [AT v , \u03c8(A)T N,:] to an MLP. We show that the resulting model exhibits both context-scaling and task-scaling. We summarize our findings as follows:\n\u2022 We identify two regimes for characterizing generalization in ICL: context-scaling and task-scaling. We observe that MLPs task-scale but do not context-scale in contrast to GPT2 architecture transformers, which exhibit both context and task-scaling. \u2022 We propose a simplified transformer, SGPT, with all key, query, and value weight matrices set to the identity matrix, exhibits both context and task-scaling and is competitive with GPT-2 on a range of ICL tasks. \u2022 We analyze a one-layer version of SGPT, demonstrating that this model is capable of context-scaling solely through the use of a feature map, \u03c8, applied to input data. We show that \u03c8 can be selected to perform kernel smoothing to impute the missing element in each task using the other in-context examples. Choosing \u03c8 based on the Hilbert estimate [9] results in a statistically optimal (consistent) estimate as the context length approaches infinity. \u2022 We show empirically that concatenating the output of \u03c8 with vectorized data as an input to MLPs enables both context-scaling and task-scaling.\n\u2022 We identify two regimes for characterizing generalization in ICL: context-scaling and task-scaling. We observe that MLPs task-scale but do not context-scale in contrast to GPT2 architecture transformers, which exhibit both context and task-scaling.\n# 2 Prior work\nICL in controlled settings. The work by [10] initiated the study of ICL in statistical learning tasks, such as linear regression, decision tree learning, and teacher-student neural network learning. They showed that transformers such as GPT-2 [19], when pre-trained with a large number of independent tasks (around 3.2 \u00d7 107 independent tasks, each presented once), learn in-context. Specifically, during inference time, the performance of pre-trained transformers empirically matches that of standard, specialized algorithms for these tasks. These results were later extended to various other settings see, e.g., [2, 4, 14, 20, 23]. In particular, [20] empirically showed transformers can achieve nearly optimal ICL when pre-trained with multiple passes over a smaller, fixed training set of independent tasks. [4] constructed transformers that implement optimal algorithms in context even if the tasks are generated from a mixture of distributions. In a recent work, [23] empirically showed that pre-trained MLPs can achieve ICL when the context length is fixed during pre-training and inference. However, it was not known whether MLPs can achieve context-scaling, and our work empirically gives a negative answer. Linear attention. Most theoretical analyses of ICL has been in the setting of linear regression for fixed context lengths. Specifically, [26] showed by construction that single linear attention can implement one-step gradient descent (GD) in context. [1, 17, 30, 31] proved that the predictor given by pre-trained transformers\nICL in controlled settings. The work by [10] initiated the study of ICL in statistical learning tasks, such as linear regression, decision tree learning, and teacher-student neural network learning. They showed that transformers such as GPT-2 [19], when pre-trained with a large number of independent tasks (around 3.2 \u00d7 107 independent tasks, each presented once), learn in-context. Specifically, during inference time, the performance of pre-trained transformers empirically matches that of standard, specialized algorithms for these tasks. These results were later extended to various other settings see, e.g., [2, 4, 14, 20, 23]. In particular, [20] empirically showed transformers can achieve nearly optimal ICL when pre-trained with multiple passes over a smaller, fixed training set of independent tasks. [4] constructed transformers that implement optimal algorithms in context even if the tasks are generated from a mixture of distributions. In a recent work, [23] empirically showed that pre-trained MLPs can achieve ICL when the context length is fixed during pre-training and inference. However, it was not known whether MLPs can achieve context-scaling, and our work empirically gives a negative answer. Linear attention. Most theoretical analyses of ICL has been in the setting of linear regression for fixed context lengths. Specifically, [26] showed by construction that single linear attention can implement one-step gradient descent (GD) in context. [1, 17, 30, 31] proved that the predictor given by pre-trained transformers with single linear layer of attention is equivalent to that given by one-step GD. [28] proved the predictor from one-step of GD achieves the rate of optimally-tuned ridge regression. In addition, they show that the predictor given by one step of GD can be pre-trained to achieve this rate with finite independent tasks. Later works such as [7] connected nonlinear attention to functional one-step GD in feature space. These works together have substantially furthered our understanding of single-layer linear attention transformers for ICL of linear regression with a fixed context length. However, these results are primarily concerned with fixed context length settings and do not address the context-scaling abilities of pre-trained transformers. Recent work [16] provided an asymptotic theory of ICL for linear attention transformers. Unlike our analyses, they focused on settings in which the number of unique pre-training tasks (referred to as \u201cdiversity\u201d) scaled proportionally with the data dimension.\nLinear attention. Most theoretical analyses of ICL has been in the setting of linear regression for fixed context lengths. Specifically, [26] showed by construction that single linear attention can implement one-step gradient descent (GD) in context. [1, 17, 30, 31] proved that the predictor given by pre-trained transformers with single linear layer of attention is equivalent to that given by one-step GD. [28] proved the predictor from one-step of GD achieves the rate of optimally-tuned ridge regression. In addition, they show that the predictor given by one step of GD can be pre-trained to achieve this rate with finite independent tasks. Later works such as [7] connected nonlinear attention to functional one-step GD in feature space. These works together have substantially furthered our understanding of single-layer linear attention transformers for ICL of linear regression with a fixed context length. However, these results are primarily concerned with fixed context length settings and do not address the context-scaling abilities of pre-trained transformers. Recent work [16] provided an asymptotic theory of ICL for linear attention transformers. Unlike our analyses, they focused on settings in which the number of unique pre-training tasks (referred to as \u201cdiversity\u201d) scaled proportionally with the data dimension.\nSoftmax attention and kernel smoothers. The connection between softmax attention and kernel smoothers was first pointed out by [24]. Specifically, by setting the query and key matrices to be the same, an attention head can be viewed as a kernel-smoother with a learnable positive semi-definite kernel. Empirical evidence suggests that using a shared matrix for query and key matrices does not significantly impact the performance of transformers [24, 29]. Later, theoretical works [6, 8] utilized this connection to study the ICL of softmax attention in both linear [6] and nonlinear regression tasks [8]. In these settings, softmax attention can perform ICL by implementing a kernel smoother with a bandwidth parameter obtained by training query and key matrices. Compared with these works, we demonstrate that transformers can perform ICL using an attention component that does not contain trainable parameters (that is, setting query, key, and value matrices to identity matrices). Our results suggest that the transformer architecture is capable of performing ICL without needing to explicitly learn any hyperparameters in the attention head. We explain this by connecting to a hyper-parameter-free yet statistically consistent kernel smoother given by the Hilbert estimate [9]. Approximation ability of transformers. The transformer is known to be a versatile architecture that can implement efficient algorithms (by forwarding passing an input prompt) in many scenarios [2, 4, 11, 12, 15]. These results make use of query, key, and value weight matrices for constructing ICL algorithms. In this work, we show that these elements of transformers are not needed for building and training models that are competitive with GPT-2 architecture transformers for ICL tasks considered in prior works [10].\nSoftmax attention and kernel smoothers. The connection between softmax attention and kernel smoothers was first pointed out by [24]. Specifically, by setting the query and key matrices to be the same, an attention head can be viewed as a kernel-smoother with a learnable positive semi-definite kernel. Empirical evidence suggests that using a shared matrix for query and key matrices does not significantly impact the performance of transformers [24, 29]. Later, theoretical works [6, 8] utilized this connection to study the ICL of softmax attention in both linear [6] and nonlinear regression tasks [8]. In these settings, softmax attention can perform ICL by implementing a kernel smoother with a bandwidth parameter obtained by training query and key matrices. Compared with these works, we demonstrate that transformers can perform ICL using an attention component that does not contain trainable parameters (that is, setting query, key, and value matrices to identity matrices). Our results suggest that the transformer architecture is capable of performing ICL without needing to explicitly learn any hyperparameters in the attention head. We explain this by connecting to a hyper-parameter-free yet statistically consistent kernel smoother given by the Hilbert estimate [9].\nApproximation ability of transformers. The transformer is known to be a versatile architecture that can implement efficient algorithms (by forwarding passing an input prompt) in many scenarios [2, 4, 11, 12, 15]. These results make use of query, key, and value weight matrices for constructing ICL algorithms. In this work, we show that these elements of transformers are not needed for building and training models that are competitive with GPT-2 architecture transformers for ICL tasks considered in prior works [10].\n# 3 Preliminaries\nProblem formulation. For all ICL tasks studied in our work, we consider T pre-training tasks, each with input data of the form:\nwhere t = 1, . . . , T indexes the tasks, N denotes the maximum context length, and d denotes the input dat dimension. We define the loss function as:\nwhere\n\ufffd \u00b7 \u00b7 \u00b7 \ufffd and M\u03b8(\u00b7) denotes the model with trainable parameters \u03b8. The tasks At are uniformly sampled from the family of tasks F (e.g., linear regression with a Gaussian prior), representing the distribution of tasks relevant to the in-context learning problem. Attention. Given three matrices A1, A2, A3 \u2208RN\u00d7m, attention layers implement functions g : RN\u00d7m \u00d7 RN\u00d7m \u00d7 RN\u00d7m \u2192RN\u00d7m defined as follows,\nand M\u03b8(\u00b7) denotes the model with trainable parameters \u03b8. The tasks At are uniformly sampled from the family of tasks F (e.g., linear regression with a Gaussian prior), representing the distribution of tasks relevan to the in-context learning problem.\nAttention. Given three matrices A1, A2, A3 \u2208RN\u00d7m, attention layers implement functions g : RN\u00d7m \u00d7 RN\u00d7m \u00d7 RN\u00d7m \u2192RN\u00d7m defined as follows,\nwhere \u03d5 : RN\u00d7N \u2192RN\u00d7N is a generic function that could be a row-wise softmax function [25], an entry-wise activation function such as ReLU, or just an identify map. For self-attention layers, we are typically given one input matrix A \u2208RN\u00d7m and three weight matrices WQ, WK, WV \u2208Rm\u00d7m. In this case, the matrices A1, A2, A3 are computed respectively as AWQ, AWK, and AWV .\n(2)\nKernel functions. Kernel functions are positive-semidefinite functions that map pairs of inputs to real values [21]. Formally, given x, y \u2208Rd, a kernel K : Rd \u00d7 Rd \u2192R is a function of the form K(x, y) = \u27e8\u03c8(x), \u03c8(y)\u27e9H, where \u03c8 : Rd \u2192H is referred to as a feature map from Rd to a Hilbert space H. For matrix inputs A \u2208Rm\u00d7d and B \u2208Rn\u00d7d, we let K(A, B) \u2208Rm\u00d7n such that K(A, B)ij = K(Ai, Bj) where Ai and Bj denote the ith and jth rows of A and B respectively. Kernel smoother. Given a kernel K : Rd \u00d7 Rd \u2192R and a set of points (xi, yi)n i=1 where xi \u2208Rd and yi \u2208R, the kernel smoother estimate at point x is a function of the form\nKernel functions. Kernel functions are positive-semidefinite functions that map pairs of inputs to real values [21]. Formally, given x, y \u2208Rd, a kernel K : Rd \u00d7 Rd \u2192R is a function of the form K(x, y) = \u27e8\u03c8(x), \u03c8(y)\u27e9H, where \u03c8 : Rd \u2192H is referred to as a feature map from Rd to a Hilbert space H. For matrix inputs A \u2208Rm\u00d7d and B \u2208Rn\u00d7d, we let K(A, B) \u2208Rm\u00d7n such that K(A, B)ij = K(Ai, Bj) where Ai and Bj denote the ith and jth rows of A and B respectively.\nKernel smoother. Given a kernel K : Rd \u00d7 Rd \u2192R and a set of points (xi, yi)n i=1 where xi \u2208Rd and yi \u2208R, the kernel smoother estimate at point x is a function of the form\ne will reference kernel smoothers in Section 5.\n estimate. The Hilbert estimate is a kernel smoother using the k\nwhere \u2225\u00b7\u22252 denotes the \u21132-norm in Rd. The key property of the Hilbert estimate that we use is that it is a asymptotically optimal (consistent) estimate. In particular, at almost all x, as the number of samples n goes to infinity, \u02c6fH,n \u2192f \u2217in probability where f \u2217(x) = E[y|X = x] denotes the optimal predictor [9].\n# 4 Simplified transformer model performs ICL\nIn this section, we introduce our simplified transformer model, SGPT, and demonstrate that it is competitive with GPT-2-type architectures on various ICL tasks. To construct SGPT, we fix all key, query, and value weights to be the identity matrix in the GPT-2 architecture. Consequently, the attention mechanism (defined in equation 3) reduces to the function:\ng(H) := \u03d5(HH\u22a4)H \u2208RN\u00d7(d+1).\nWe define \u03d5 to be a function that performs row-wise \u21131 normalization on its argument. To further simplify our model, we remove the final linear layer of each MLP block (i.e., our MLP blocks have one linear layer), along with batch normalization and the skip connection after the MLP layer. These details are further\nWe define \u03d5 to be a function that performs row-wise \u21131 normalization on its argument. To further simplify our model, we remove the final linear layer of each MLP block (i.e., our MLP blocks have one linear layer), along with batch normalization and the skip connection after the MLP layer. These details are further outlined in Appendix A. We consider the following ICL tasks from prior work: (1) linear regression with a single noise level [2], (2) linear regression with multiple noise levels used in [4], (3) sparse linear functions used in [10], (4) two-layer ReLU neural networks [10], and (5) decision trees [10]. Below, we explain the problem setup and state our results for each of these five synthetic tasks.\n# on with fixed noise level. The problem setting is as follows:\n# ear regression with fixed noise level. The problem setting is\nx \u2208Rd \u223cN(0, Id), y = \u03b2\u22a4x + \u03f5 with \u03b2 \u223cN \ufffd 0, Id d \ufffd , \u03f5 \u223cN(0, \u03c32).\nx \u2208Rd \u223cN(0, Id), y = \u03b2\u22a4x + \u03f5 with \u03b2 \u223cN \ufffd 0, Id d \ufffd , \u03f5 \u223cN\nIn this setting, prior work by [4], showed that on all context lengths, GPT-2 architecture transformers can perform comparably to task-specific, optimally-tuned ridge regression. In Figure 2, we provide evidence that SGPT matches the performance of these GPT-2 models.\nx \u2208Rd \u223cN(0, Id), yi = \u03b2\u22a4xi + \u03f5 \ufffd\n \u2208 \u223cN with \u03b2 \u223cN \ufffd 0, Id d \ufffd and \u03f5 \u223c \ufffd N(0, \u03c32 1), with probability 1 2 N(0, \u03c32 2), with probability 1 2\n(4)\n(5)\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4f51/4f51125c-e3b2-4024-a04d-522a8b563675.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Linear regression with a single noise level. Left panel. Performance across varying context lengths (context-scaling). Right panel. Effect of regularization on performance for a fixed number of in-context examples. Experimental details are given in Appendix A.</div>\nFigure 2: Linear regression with a single noise level. Left panel. Performance across varying context lengths (context-scaling). Right panel. Effect of regularization on performance for a fixed number of in-context examples. Experimental details are given in Appendix A.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/60d5/60d59ec5-c1fa-48ee-ac9e-a9c0905498f7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Linear regression with multiple noise levels. Left and middle panels: Performance acros varying context lengths (context-scaling). Right panel: Effect of regularization on performance for a fixe number of in-context examples. Experimental details are given in Appendix A.</div>\nIn this setting, prior work by [4] demonstrated that GPT-2 architecture transformers can achieve performance comparable to that of task-specific, optimally tuned ridge regression across all context lengths and for both noise levels. They refer to the model\u2019s ability to adapt to the noise level as algorithm selection. In Figure 3, We demonstrate that SGPT performs comparably to GPT-2 architecture transformers in this setting, exhibiting similar algorithm selection capabilities.\nmance comparable to that of task-specific, optimally tuned ridge regression across all context lengths and for both noise levels. They refer to the model\u2019s ability to adapt to the noise level as algorithm selection. In Figure 3, We demonstrate that SGPT performs comparably to GPT-2 architecture transformers in this setting, exhibiting similar algorithm selection capabilities. Two-layer ReLU Neural Networks. Following the work of [10], we consider the following nonlinear problem setting where data for each task are generated using two-layer neural networks. In particular, data are generated according to\nTwo-layer ReLU Neural Networks. Following the work of [10], we consider the following nonlinea problem setting where data for each task are generated using two-layer neural networks. In particular, data are generated according to\nx \u2208Rd \u223cN(0, Id), y = r \ufffd j=1 \u03b1j\u03d5(w\u22a4 j x);\nwhere \u03b1j, wj are randomly initialized parameters of a fixed two-layer neural network, \u03d5 denotes the elementwise ReLU activation function, and r = 100, d = 20 (as selected in prior work). The work [10] demonstrated that GPT-2 architecture transformers can match the performance of student networks (i.e., networks of the same architecture initialized differently and trained using Adam optimizer [13]). In Figure 4(B), we show that SGPT can match the performance of GPT-2 architecture transformers on this task.\n<div style=\"text-align: center;\">Regularizer parameters</div>\nDecision Tree. Following the work of [10], we consider a nonlinear problem setting where data for task are generated using depth-four trees. For a task corresponding to a tree f, we have:\nx \u223cN(0, Id), y = f(x),\nPreviously, [10] demonstrated that GPT-2 architecture transformers can perform in-context learning on this family of non-linear functions, outperforming XGBoost as a baseline. In Figure 4(A), we show that SGPT is also capable of in-context learning (ICL) in this setting, performing comparably to GPT-2 architecture and similarly outperforming XGBoost. We trained XGBoost models using the same hyperparameters as in [10]. Sparse linear functions. Following the work of [10], we consider the class sparse linear regression problems. In this setting, data are generated according to\nwhere \u03b2 \u223cN (0, Id) and we zero out all but s coordinates of \u03b2 uniformly at random for each task. As in prior work, we select d = 20, s = 3. In Figure 4C , we demonstrate that SGPT is capable of ICL for this class of functions, performing comparably to GPT-2 architecture transformers and closely to the Lasso estimator [22], while significantly outperforming the ordinary least square (OLS) baseline.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/300f/300f1835-c6bd-42eb-a082-f2580d8361e9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Nonlinear ICL tasks. Context-scaling capability of SGPT versus GPT-2 architecture transformer when trained on 2 million pre-training tasks. In all cases, the errors are normalized so that the trivial zer predictor achieves an error of 1.\u2217Experimental details are given in Appendix A.</div>\nThus far, we have demonstrated SGPT is comparable to GPT-2 across various ICL tasks. In Appendix C we revisit the experiment introduced in the introduction and show that GPT-2 and SGPT are capable o both context and task-scaling.\n# Kernel smoothing can perform context-s\nWe begin this section by demonstrating that even one layer of SGPT is capable of context scaling. In particular, in Figure 5, we train a one-layer model on five different context lengths and test on the same lengths for the tasks considered in the previous section. In all four problem settings, it is evident that using more context improves performance. Below, we analyze this simplified one layer model in order to pinpoint how it is capable of context scaling. In particular, the model we analyze is identical to one layer of SGPT up to the omission of remaining skip connections (for more details, see Appendix A). The model implements a function f : RN\u00d7(d+1) \u2192R and takes the form below:\n(7)\n(8)\n(9)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1df/d1df9dc0-72c2-4d01-806a-d9dd298c215f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"># In-context examples</div>\nFigure 5: Context-scaling with one-layer SGPT. Experimental details are provided in Appendix A. here \u03c8 : RN\u00d7(d+1) \u2192RN\u00d7(d+1) is a feature map (generalizing the attention function defined in equation 6) \u2208R(N)\u00d7(d+1) denotes the input data, and W (1) \u2208Rd+1\u00d7k, W (2) \u2208Rk\u00d71.\n<div style=\"text-align: center;\">Figure 5: Context-scaling with one-layer SGPT. Experimental details are provided in Appendix A.</div>\nwhere \u03c8 : RN\u00d7(d+1) \u2192RN\u00d7(d+1) is a feature map (generalizing the attention function defined in equation 6 A \u2208R(N)\u00d7(d+1) denotes the input data, and W (1) \u2208Rd+1\u00d7k, W (2) \u2208Rk\u00d71.\n# 5.1 Feature-map that enables context-scaling\nThe key aspect distinguishing the model in equation 9 from a standard MLP operating on vectorized inputs A \u2208 RN(d+1) is the feature map \u03c8. As such, we analyze how the feature map \u03c8 transforms an input\nFirst, we note that upon varying the function \u03c8, the bottom-right element of \u03c8(A), denoted \u03c8(A)N,d+1, is capable of implementing several well-known estimators, which we describe below. To ease notation, we let X := [x1, x2, \u00b7 \u00b7 \u00b7 , xN]\u22a4\u2208RN\u00d7d and y := [y1, \u00b7 \u00b7 \u00b7 , yN]\u22a4. Detailed derivations of the explicit forms for \u03c8(A)N,d+1 below are presented in Appendix B. (1) 1-step GD estimate. Let \u03c8L(A) := (AA\u22a4)A. Then,\n(10)\nThus, \u03c8L computes the estimate arising from a linear predictor trained for one step of gradient descent on the data (X, y). This estimate has been previously considered as a mechanism through which transformers performed ICL, but there have been no theoretical guarantees for this approach for general ICL tasks beyond linear regression [1, 17, 26, 30, 31]. (2) Kernel smoother. Given a kernel K, let \u03c8K(A) = \u02c6K(X, X)A, where\n, let\nIn this case, \u03c8K(A)N,: has the following form\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/87cf/87cfdd5c-e882-4270-bf62-051d4249cb8a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1aff/1aff440e-ab90-4344-99cc-835a0ff2808d.png\" style=\"width: 50%;\"></div>\nFigure 6: Comparison between using a row of features given by \u03c8K(\u00b7)N,: and using the scalar given by \u03c8K(\u00b7)N,d+1 for K \u2208{L, H}. Here, the feature maps \u03c8L, \u03c8H are defined in equation 10 and equation 5 respectively.\n(11)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca8a/ca8adb05-df5e-463b-ae91-764b7ad49208.png\" style=\"width: 50%;\"></div>\nFigure 7: Comparison of MLPs trained using (1) vectorized inputs; (2) features from \u03c8K for K \u2208L, H defined in equation 10 and equation 5; (3) both features from \u03c8K and vectorized inputs. We compare performance across two ICL tasks: linear regression and two-layer teacher-student neural networks. (A) Task-scaling ability of MLPs using different inputs. (B) Context-scaling ability of MLPs using different inputs. MLPs trained on both vectorized inputs and features from \u03c8K are able to simultaneously context-scale and taskscale. Experimental details are provided in Appendix A.\nand the last element \u03c8K(A)N,d+1 is the kernel smoother estimate,\n# and the last element \u03c8K(A)N,d+1 is the kernel smoother estimate,\n\u03c8K(A)N,d+1 = \ufffdN\u22121 i=1 K(xN, xi)yi \ufffdN\u22121 i=1 K(xN, xi) .\n\ufffd Below, we provide key examples of kernel smoothers that can be implemented by equation 12 upo changing the kernel K.\n1. When K is the exponential kernel, i.e., K(z, z\u2032) = e\u2212z\u22a4z, then \u03c8K implements softmax attention, an \u03c8K(A)N,d+1 is the kernel smoother corresponding to the exponential kernel.\n2. When using the kernel H defined in equation 5, then, \u03c8H(A)N,d+1 implements the Hilbert estimate which is consistent as the number of in-context examples goes to infinity [9].\nIn our experiments in Figure 5, we trained an MLP on features computed using \u03c8K(A)N,:. Yet, the results above suggest that the scalar \u03c8K(A)N,d+1 alone should be sufficient for context scaling. Indeed, in the case of the Hilbert estimate, this entry alone provides a consistent estimate as the context length goes to infinity. To this end, in Figure 6, we compare the performance of two MLPs when the number of tasks is fixed and the context length increases. The first MLP is trained using \u03c8K(A)N,: \u2208Rd+1, and the second is trained on only \u03c8K(A)N,d+1. The results in this figure confirm that using \u03c8K(A)N,d+1 is as good as using \u03c8K(A)N,: for context-scaling.\n# 5.2 Training MLPs that simultaneously context-scale and task-scale\nAs the Hilbert estimate provides a consistent estimate, our results above show that transformers provably generalize to unseen tasks, when the context length approaches infinity. Nevertheless, the issue with using the Hilbert estimate alone is that the Hilbert estimate is only computed using examples provided in a context. As such, it cannot task-scale unlike MLPs trained on vectorized inputs. We now show that training MLPs on vectorized inputs concatenated with features estimated using \u03c8K(A) result in MLPs that can both context-scale and task-scale.\n(12)\nNamely, we revisit the experiment presented in Figure 1 and extend our analysis by training MLPs on three distinct input configurations: (1) vectorized input data; (2) features from \u03c8K(A)N,:; and (3) the concatenation of vectorized input data and features from \u03c8K(A)N,:. In our experiments, we consider the feature maps \u03c8L and \u03c8H discussed in the previous section. The results of training these MLPs is presented in Figure 7 and we summarize the results below.\n1. MLPs with vectorized input data: Figure 7A demonstrates that these MLPs exhibit task-scaling. Yet, Figure 7B reveals that these MLPs fail to context-scale and performance can even deteriorate with increased context length. 2. MLPs with features from \u03c8K(\u00b7)N,:: Figure 7A illustrates that these MLPs do not task-scale, as performance does not improve with an increasing number of tasks. This behavior matches intuition as the Hilbert smoother and 1-step gradient descent features are task-specific and do not leverage intertask relationships. Yet, Figure 7B shows that these MLPs successfully context-scale, which happens provably for the particular case of \u03c8H. 3. MLPs with both vectorized inputs and features from \u03c8K(\u00b7)N,:: Figure 7A demonstrates that these MLPs are capable of task-scaling, consistent with the performance of MLPs on vectorized data alone. Moreover, in Figure 7B, we now observe that these MLPs are now capable of context-scaling, consistent with the performance of MLPs using the features from \u03c8K(\u00b7)N,: alone.\nThese results underscore the importance of the feature map \u03c8K for context-scaling and highlight the effectiveness of using both vectorized inputs and features from \u03c8K in improving the ability of models to learn in-context.\n# 6 Conclusion\nSummary. In this work, we observed that transformers, unlike MLPs, are able to simultaneously contextscale (improve performance as the context length increases) and task-scale (improve performance as the number of pre-training tasks increases). To better understand this property of transformers, we first identified a simplified transformer (SGPT) that could solve ICL tasks competitively with GPT-2 architecture transformers despite having no trainable key, query, and value weights in attention layers. By studying a one-layer version of SGPT, we identified that the attention operator of SGPT applied a feature map, \u03c8, on input data that enabled context-scaling. In particular, we showed that this feature map could implement kernel smoothers such as the Hilbert estimate, which is a statistically consistent estimator as the context length goes to infinity. As such, our work provably demonstrates that transformers can context-scale, generalizing to new, unseen tasks when provided a large context. We demonstrated the effectiveness of the feature map, \u03c8, for context-scaling by showing that MLPs trained on both features from \u03c8 and vectorized inputs could simultaneously context-scale and task-scale.\nSummary. In this work, we observed that transformers, unlike MLPs, are able to simultaneously contextscale (improve performance as the context length increases) and task-scale (improve performance as the number of pre-training tasks increases). To better understand this property of transformers, we first identified a simplified transformer (SGPT) that could solve ICL tasks competitively with GPT-2 architecture transformers despite having no trainable key, query, and value weights in attention layers. By studying a one-layer version of SGPT, we identified that the attention operator of SGPT applied a feature map, \u03c8, on input data that enabled context-scaling. In particular, we showed that this feature map could implement kernel smoothers such as the Hilbert estimate, which is a statistically consistent estimator as the context length goes to infinity. As such, our work provably demonstrates that transformers can context-scale, generalizing to new, unseen tasks when provided a large context. We demonstrated the effectiveness of the feature map, \u03c8, for context-scaling by showing that MLPs trained on both features from \u03c8 and vectorized inputs could simultaneously context-scale and task-scale. Future work and limitations. While we have provably established that one-layer transformers can context-scale, we empirically observe that one-layer transformers are not as sample-efficient as deep transformers for both context-scaling and task-scaling. Thus, an important future direction is understanding how depth improves the sample complexity of transformers in both context-scaling and task-scaling settings. Exploring this aspect remains a promising avenue for future research and could provide a comprehensive understanding of ICL, and more broadly, a better understanding of how transformers are able to generalize to new tasks when provided large contexts.\nFuture work and limitations. While we have provably established that one-layer transformers ca context-scale, we empirically observe that one-layer transformers are not as sample-efficient as deep trans formers for both context-scaling and task-scaling. Thus, an important future direction is understandin how depth improves the sample complexity of transformers in both context-scaling and task-scaling settings Exploring this aspect remains a promising avenue for future research and could provide a comprehensiv understanding of ICL, and more broadly, a better understanding of how transformers are able to generaliz to new tasks when provided large contexts.\n# References\n1] K. Ahn, X. Cheng, H. Daneshmand, and S. Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[2] E. Aky\u00a8urek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2022. [3] Anthropic. Claude. https://www.anthropic.com, 2023. Large language model. [4] Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [6] S. Chen, S. Heejune, W. Tianhao, and Y. Zhuoran. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. In The Thirty Seventh Annual Conference on Learning Theory, pages 4573\u20134573. PMLR, 2024. [7] X. Cheng, Y. Chen, and S. Sra. Transformers implement functional gradient descent to learn non-linear functions in context. In Forty-first International Conference on Machine Learning, 2024. [8] L. Collins, A. Parulekar, A. Mokhtari, S. Sanghavi, and S. Shakkottai. In-context learning with transformers: Softmax attention adapts to function lipschitzness. arXiv preprint arXiv:2402.11639, 2024. [9] L. Devroye, L. Gy\u00a8orfi, and A. Krzy\u02d9zak. The hilbert kernel regression estimate. Journal of Multivariate Analysis, 65(2):209\u2013227, 1998. 10] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. 11] K. Gatmiry, N. Saunshi, S. J. Reddi, S. Jegelka, and S. Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning? In Forty-first International Conference on Machine Learning, 2024. 12] T. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y. Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. In The Twelfth International Conference on Learning Representations, 2023. 13] D. P. Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 14] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594. PMLR, 2023. 15] L. Lin, Y. Bai, and S. Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. In The Twelfth International Conference on Learning Representations, 2023. 16] Y. M. Lu, M. I. Letey, J. A. Zavatone-Veth, A. Maiti, and C. Pehlevan. Asymptotic theory of in-context learning by linear attention. arXiv preprint arXiv:2405.11751, 2024. 17] A. Mahankali, T. B. Hashimoto, and T. Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In The Twelfth International Conference on Learning Representations, 2024. 18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n[19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019. [20] A. Ravent\u00b4os, M. Paul, F. Chen, and S. Ganguli. Pretraining task diversity and the emergence of nonbayesian in-context learning for regression. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 14228\u201314246, 2023. [21] B. Sch\u00a8olkopf, A. J. Smola, F. Bach, et al. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002. [22] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267\u2013288, 1996. [23] W. L. Tong and C. Pehlevan. Mlps learn in-context. arXiv preprint arXiv:2405.15618, 2024. [24] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov. Transformer dissection: An unified understanding for transformer\u2019s attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4344\u20134353, 2019. [25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \ufffdL. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [26] J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023. [27] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, and et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2020. [28] J. Wu, D. Zou, Z. Chen, V. Braverman, Q. Gu, and P. Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In The Twelfth International Conference on Learning Representations, 2024. [29] Y. Yu, S. Buchanan, D. Pai, T. Chu, Z. Wu, S. Tong, B. Haeffele, and Y. Ma. White-box transformers via sparse rate reduction. Advances in Neural Information Processing Systems, 36, 2024. [30] R. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1\u201355, 2024. [31] R. Zhang, J. Wu, and P. L. Bartlett. In-context learning of a linear transformer block: Benefits of the mlp component and one-step gd initialization. In Advances in Neural Information Processing Systems, 2024.\n# A Experiments details\nWe provide all experimental details below.\nProblem formulation. For all ICL tasks studied in our work, we consider T pretraining tasks, each with input data of the form: \ufffd \ufffd\nwhere t \u22081, . . . , T indexes the tasks, N denotes the maximum context length, and d denotes the input dat dimension. We define the loss function L(\u03b8) as:\nwhere\nand M\u03b8 denotes the model with trainable parameters \u03b8.\nVectorized input. By vectorizing input, we mean flattening the input into a vector. After  Ai t defined above becomes,\nGPT-2. We used the GPT-2 implementation from prior work [4, 10], which is based on the Hugging Face implementation [27]. Following the approach in these prior works, we modified the embedding layer with a learnable linear layer that maps from the ambient dimension to an embedding dimension.\nSGPT. To construct SGPT, we make the following modifications to the GPT-2 architecture: (1) we fix all key, query, value weights to the identity; (2) we eliminate all batch-normalization layers; and (3) we remove the second linear layer from each MLP. Following prior work [4, 10], we modified the embedding layer with a linear layer that maps from the ambient dimension to an embedding dimension. In SGPT, this linear layer is not trainable and serves as a fixed random map. We outline the architecture below. Let A \u2208RN\u00d7(d+1) be the input of the model. We initialize a random matrix W0 \u2208R(d+1)\u00d7k, where k is the embedding dimension. Defining the input of the i-th layer as H(i), we have H(0) := AW0,\nwhere:\n\u2022 \u03c3 is the activation function, chosen to be GeLU, \u2022 g is as defined in Equation 6, \u03d5 is row wise l1 normalziation. \u2022 W (i) proj \u2208Rk\u00d7k is the projection matrix, \u2022 W (i) MLP \u2208Rk\u00d7k is the MLP weight matrix for the i-th layer. The last layer of the network is a linear layer with weights WO \u2208Rk\u00d71.\n\u2022 \u03c3 is the activation function, chosen to be GeLU, \u2022 g is as defined in Equation 6, \u03d5 is row wise l1 normalziation. \u2022 W (i) proj \u2208Rk\u00d7k is the projection matrix, \u2022 W (i) MLP \u2208Rk\u00d7k is the MLP weight matrix for the i-th layer. The last layer of the network is a linear layer with weights WO \u2208\n# MLP architectures. In all experiments, we use a standard 2-layer ReLU MLP with a width of 1024 units. Given an input vector x \u2208Rdin, the MLP implements a function f of the form\nf(x) := \u03c3(xW0)W1\nwhere W0 \u2208Rdin\u00d71024, W1 \u2208R1024\u00d71, and \u03c3 is the ReLU activation function.\nZero-padding input for MLP. In all experiments, we always trained a single MLP for all context lengths by zero-padding to the largest context length. For example, if the input data is in Rd and the largest context length is Nmax, then the input dimension of the MLP is dNmax.\nZero-padding input for MLP. In all experiments, we always trained a single MLP for all context lengths by zero-padding to the largest context length. For example, if the input data is in Rd and the largest context length is Nmax, then the input dimension of the MLP is dNmax.\n(13)\n(14)\nxpeirmental details for Figure 1. In this experiment, we trained an 8-layer GPT-2 model with 8 ttention heads and a width of 256. The MLP configuration is the same as that in equation 14. We trained nd tested the models on the same set of context lengths: 5, 10, 20, 30, and 40. xperimental details for Section 4. We trained both standard GPT-2 architecture transformers and ur proposed SGPT with the following configurations: \u2022 Widths: {256, 512, 1024}, \u2022 Number of layers: {2, 4, 6, 8}, \u2022 Number of attention heads for GPT-2: {2, 4, 8}. The best performance was achieved with an 8-layer model with a width of 256. For the original GPT-2, he optimal configuration used 8 attention heads. We outline per-task observations and configurations below: 1. Linear regression with single noise level: Following the prior work [10], the input dimension is d = 20 and noise level is \u03c3 = 0.5. We trained on context lengths from 10 to 40 with a step size of 5. 2. Linear regression with two noise levels: Following the prior work of [10], the input dimension is d = 20 and noise levels are \u03c31 = 0.1, \u03c32 = 0.5. We trained on context lengths from 1 to 40 with a step size of 1. 3. Decision tree: Following the prior work of [10], the input dimension is d = 20 with a tree depth of 4. We note that with the input structure equation 2, GPT-2 performs poorly, so in our figure we used the pretrained model from [10]. 4. Two-layer ReLU Neural Networks. As mentioned before, we chose the width of this family of neural networks to be r = 100 and the input dimension to be d = 20. We trained on context lengths from 1 to 100 with a step size of 10. We observed that unlike our model, GPT-2 does not generalize well for context lengths that it has not been trained on. 5. Sparse Linear Regression As mentioned previously, the ambient dimension of the input is d = 20, consistent with prior work[10], and the effective dimension is s = 3. We used scikit-learn [18] for the Lasso and Ordinary least sauare performances. xperimental details for Figure 5. In all tasks, we used input dimension d = 8 following the setting n [23]. We trained and tested both models on context lengths of 10, 20, 30, 40, and 50. xperimental details for Figure 6. We train an MLP (equation 14) on features extracted using \u03c8K(\u00b7)N,: nd linear regression on the scalar \u03c8K(\u00b7)N,d+1. xperimental details for Figure 7: \u2022 Linear regression: We used the same settings as used for the model in equation 3 with d = 8 and \u03c3 = 0.22. We trained and tested models on the context lengths 5, 10, 20, 30, and 40. \u2022 2-layer NN task: We used the same settings as used for the model in 4 with d = 8, r = 100. Trained and tested models on the context lengths 10, 20, 30, 40, 50, 60, 70, and 80. Hardware. We used machines equipped with NVIDIA A100 and A40 GPUs, featuring V-RAM capacities f 40GB. These machines also included 8 cores of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz with up to 50 GB of RAM. For all our experiments, we never used more than one GPU, and no model was trained for\nExperimental details for Figure 5. In all tasks, we used input dimension d = 8 following the setting in [23]. We trained and tested both models on context lengths of 10, 20, 30, 40, and 50. Experimental details for Figure 6. We train an MLP (equation 14) on features extracted using \u03c8K(\u00b7)N, and linear regression on the scalar \u03c8K(\u00b7)N,d+1.\nExperimental details for Figure 6. We train an MLP (equation 14) on features extracted using \u03c8K(\u00b7)N,: and linear regression on the scalar \u03c8K(\u00b7)N,d+1.\n# Experimental details for Figure 7:\n\u2022 Linear regression: We used the same settings as used for the model in equation 3 with d = 8 and \u03c3 = 0.22. We trained and tested models on the context lengths 5, 10, 20, 30, and 40.\n\u2022 2-layer NN task: We used the same settings as used for the model in 4 with d = 8, r = 100. Trained and tested models on the context lengths 10, 20, 30, 40, 50, 60, 70, and 80.\nHardware. We used machines equipped with NVIDIA A100 and A40 GPUs, featuring V-RAM capacities of 40GB. These machines also included 8 cores of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz with up to 150 GB of RAM. For all our experiments, we never used more than one GPU, and no model was trained for more than two days.\nHardware. We used machines equipped with NVIDIA A100 and A40 GPUs, featuring V-RAM capacities of 40GB. These machines also included 8 cores of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz with up to 150 GB of RAM. For all our experiments, we never used more than one GPU, and no model was trained for more than two days.\nFeature map derivation\n1-step of GD. In this case, we have\n\u03c8L(A) = (AA\u22a4)A \uf8ee\nThus, \u03c8L(A)N,d+1 = x\u22a4 NX\u22a4y, where X :=\nNow last row of the \u03c8K(A) is given by\n<div style=\"text-align: center;\">Now last row of the \u03c8K(A) is given by</div>\n\ufffd \ufffd \ufffd \ufffd Thus, \u03c8K(A)N,d+1 is equvialent to the prediction for xN given by using a kernel smoother with kernel K\n<div style=\"text-align: center;\">\ufffd \ufffd \ufffd \ufffd Thus, \u03c8K(A)N,d+1 is equvialent to the prediction for xN given by using a kernel smoother with kernel K.</div>\n# C Additional experiments\nIn the following experiments, we trained both an 8-layer, 8-head GPT-2 model and an 8-layer SGPT model using identical context lengths during training and testing. As shown in Figure 8, both models are capabl of scale and context scaling. Here for both tasks we set d = 8 and for the linear regression task \u03c3 = 0.2 and for 2-layer neural network we set the width of the networks to r = 100.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d0f/2d0f90c1-ecf6-48c8-a673-b6d98f676bec.png\" style=\"width: 50%;\"></div>\nFigure 8: Task-scaling and context-scaling of GPT-2 architecture transformers versus SGPT for linear re gression and 2-layer neural networks tasks. (A) Task-scaling abilities of these models with 10 in-contex examples. (B) Context-scaling abilities of these models with 105 (left) and 106 (right) pre-training tasks.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of In-Context Learning (ICL) in transformers, highlighting the importance of context-scaling and task-scaling, and identifying the need for a new method to understand how transformers can perform context-scaling, which standard Multi-Layer Perceptrons (MLPs) cannot.",
        "problem": {
            "definition": "The problem is to understand the mechanisms allowing transformers to perform context-scaling in ICL, while MLPs can only perform task-scaling.",
            "key obstacle": "The main difficulty is identifying why existing methods, particularly MLPs, fail to achieve context-scaling, which is crucial for generalizing to unseen tasks based on context examples."
        },
        "idea": {
            "intuition": "The idea emerged from observing that transformers can leverage a larger context to improve performance, unlike MLPs which do not benefit from increased context length.",
            "opinion": "The proposed idea is to construct a simplified transformer architecture, SGPT, that can perform ICL tasks comparably to GPT-2 without requiring trainable attention weights.",
            "innovation": "The key innovation is the introduction of a feature map in the SGPT architecture that allows for context-scaling and task-scaling simultaneously, a capability not present in standard MLPs."
        },
        "method": {
            "method name": "Simplified GPT (SGPT)",
            "method abbreviation": "SGPT",
            "method definition": "SGPT is a simplified transformer architecture where all key, query, and value weights are fixed to the identity matrix, enabling the model to perform ICL tasks.",
            "method description": "SGPT utilizes a feature map applied to input data to enable context-scaling while maintaining competitive performance with GPT-2.",
            "method steps": [
                "Construct a simplified transformer by fixing key, query, and value weights.",
                "Apply a feature map to the input data.",
                "Train an MLP on the output of the feature map concatenated with vectorized data."
            ],
            "principle": "SGPT is effective because it leverages a feature map that allows for context-dependent predictions, enabling generalization to new tasks with large contexts."
        },
        "experiments": {
            "evaluation setting": "The experiments involved various ICL tasks including linear regression, decision trees, and two-layer neural networks, comparing the performance of SGPT against GPT-2 and standard MLPs across different context lengths and pre-training tasks.",
            "evaluation method": "The performance was assessed by measuring the accuracy of predictions made by SGPT and comparing it to the performance of GPT-2 and MLPs on the same tasks, focusing on context-scaling and task-scaling."
        },
        "conclusion": "The study concludes that SGPT can achieve both context-scaling and task-scaling, demonstrating its competitive performance with standard transformers like GPT-2, and establishes a foundation for understanding how transformers generalize in ICL settings.",
        "discussion": {
            "advantage": "The main advantage of SGPT is its ability to simultaneously perform context-scaling and task-scaling, which enhances its generalization capabilities compared to MLPs.",
            "limitation": "A limitation of the study is that while SGPT can context-scale, it is not as sample-efficient as deep transformers, suggesting that further research is needed to explore the benefits of depth in transformer architectures.",
            "future work": "Future work will focus on understanding how increasing the depth of transformers can improve sample efficiency and exploring additional methods for enhancing ICL capabilities."
        },
        "other info": {
            "info1": "The experiments utilized NVIDIA A100 and A40 GPUs with specific hardware configurations.",
            "info2": {
                "info2.1": "The study identifies two regimes for characterizing generalization in ICL: context-scaling and task-scaling.",
                "info2.2": "SGPT is shown to be competitive with GPT-2 across various ICL tasks, providing insights into the architecture's capabilities."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses In-Context Learning (ICL) in transformers, highlighting the importance of context-scaling and task-scaling."
        },
        {
            "section number": "1.3",
            "key information": "Transformers can leverage a larger context to improve performance, unlike Multi-Layer Perceptrons (MLPs) which do not benefit from increased context length."
        },
        {
            "section number": "3.1",
            "key information": "The main advantage of SGPT is its ability to simultaneously perform context-scaling and task-scaling, enhancing generalization capabilities compared to MLPs."
        },
        {
            "section number": "3.2",
            "key information": "The study identifies two regimes for characterizing generalization in ICL: context-scaling and task-scaling."
        },
        {
            "section number": "3.4",
            "key information": "SGPT utilizes a feature map applied to input data to enable context-scaling while maintaining competitive performance with GPT-2."
        },
        {
            "section number": "6.1",
            "key information": "The main difficulty is identifying why existing methods, particularly MLPs, fail to achieve context-scaling, which is crucial for generalizing to unseen tasks based on context examples."
        },
        {
            "section number": "6.2",
            "key information": "A limitation of the study is that while SGPT can context-scale, it is not as sample-efficient as deep transformers."
        }
    ],
    "similarity_score": 0.7002383250788057,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Context-Scaling versus Task-Scaling in In-Context Learning.json"
}