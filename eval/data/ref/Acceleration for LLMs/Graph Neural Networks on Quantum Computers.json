{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.17060",
    "title": "Graph Neural Networks on Quantum Computers",
    "abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
    "bib_name": "liao2024graphneuralnetworksquantum",
    "md_text": "# Graph Neural Networks on Quantum Computers\nYidong Liao,1, 2, \u2217Xiao-Ming Zhang,3, 4 and Chris Ferrie1, \u2020 1Centre for Quantum Software and Information, University of Technology Sydney, Sydney, NSW, Australia 2Sydney Quantum Academy, Sydney, NSW, Australia 3Center on Frontiers of Computing Studies, School of Computer Science, Peking University, Beijing, China School of Physics, South China Normal University, Guangzhou, Chin\nGraph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and MessagePassing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/89eb/89eb8778-e419-4f47-a16e-d631150473e0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">FIG. 1. Overall circuit construction for the three Quantum GNN architectures along with the three \u201cflavours\u201dof classical GNN layers[1].</div>\n<div style=\"text-align: center;\">FIG. 1. Overall circuit construction for the three Quantum GNN architectures along with the three \u201cflavours\u201dof classical GNN layers[1].</div>\n2. Classical Graph Neural Networks 3. Quantum Graph Convolutional Networks 3.1. Vanilla GCN and its Quantum version 3.1.1. Data Encoding 3.1.2. Layer-wise transformation 3.1.3. Cost function 3.2. Simplified Graph Convolution (SGC) and its quantum version 3.3. Linear Graph Convolution (LGC) and its quantum version 4. Quantum Graph Attention Networks 4.1. Block encoding of certain sparse matrices 4.2. Quantum Graph Attention operation 5. Quantum Message-Passing GNN 6. Complexity Analysis 6.1. Complexity of classical GCNs 6.2. Complexity analysis of Quantum SGC 6.3. Complexity analysis of Quantum LGC 7. Conclusion A. Implementation of the \u201cselective copying\u201d operation B. Quantum Attention Mechanism 1. Evaluating Attention score in superposition 2. Storing Attention score C. Proof of the Layer-wise linear transformation for multi-channel GCN D. Brief Introduction of Quantum Neural Networks and Block-encoding E. Comparisons with some related works References\n1. Introduction\nGraph Neural Networks (GNNs) are powerful machine learning models for analyzing structured data represented as graphs. They have shown remarkable success in various applications including social network analysis [2, 3], recommendation systems [4, 5], drug discovery [6, 7], and traffic prediction [8]. From a theoretical perspective, GNNs have been posited as a universal framework for various neural network architectures: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers, etc. can be viewed as special cases of GNNs [1, 9, 10].\n2\nDespite their success, classical GNNs face several challenges when dealing with large-scale graphs. One major challenge is the memory limitations that arise when handling giant graphs. Large and complex graphs become increasingly difficult to fit in the conventional memory used by most classical computing hardware [11]. Another issue lies in the inherent sparse matrix operations of GNNs, which pose challenges for efficient computation on modern hardware like GPUs that are optimized for dense matrix operations1 [12]. Moreover, the common method of managing large graphs through graph subsampling techniques (e.g. dividing large graphs into smaller, more manageable subgraphs [11]) may encounter reliability issues, since it is challenging to guarantee that these subgraphs preserve the semantics of the entire graph and provide reliable gradients for training GNNs [12]. In summary, the memory and computational requirements of processing large-scale graphs often exceed the capabilities of classical computing hardware, limiting the practical scalability of GNNs. The need for efficient and scalable graph learning has motivated ongoing efforts in developing specialized hardware accelerators for GNNs [13\u201315] as well as the exploration of utilizing alternative computing paradigms, such as quantum computing, to address these challenges.\nDespite their success, classical GNNs face several challenges when dealing with large-scale graphs. One major challenge is the memory limitations that arise when handling giant graphs. Large and complex graphs become increasingly difficult to fit in the conventional memory used by most classical computing hardware [11]. Another issue lies in the inherent sparse matrix operations of GNNs, which pose challenges for efficient computation on modern hardware like GPUs that are optimized for dense matrix operations1 [12]. Moreover, the common method of managing large graphs through graph subsampling techniques (e.g. dividing large graphs into smaller, more manageable subgraphs [11]) may encounter reliability issues, since it is challenging to guarantee that these subgraphs preserve the semantics of the entire graph and provide reliable gradients for training GNNs [12]. In summary, the memory and computational requirements of processing large-scale graphs often exceed the capabilities of classical computing hardware, limiting the practical scalability of GNNs. The need for efficient and scalable graph learning has motivated ongoing efforts in developing specialized hardware accelerators for GNNs [13\u201315] as well as the exploration of utilizing alternative computing paradigms, such as quantum computing, to address these challenges. Quantum computers hold the promise of significantly improving machine learning by providing computational speed-ups or improved model scalability [16\u201319]. In the context of graph learning, quantum computing provides new opportunities to design quantum machine learning architectures tailored for graph-structured data [20\u201323]. Motivated by this potential, we propose quantum GNN architectures in accordance with the three fundamental types of classical GNNs: Graph Convolutional Networks (GCNs) (e.g.[24]), Graph Attention Networks (GATs) (e.g.[25]), and Message-Passing Neural Networks (MPNNs) (e.g.[26]).\nQuantum computers hold the promise of significantly improving machine learning by providin computational speed-ups or improved model scalability [16\u201319]. In the context of graph learning quantum computing provides new opportunities to design quantum machine learning architecture tailored for graph-structured data [20\u201323]. Motivated by this potential, we propose quantum GNN architectures in accordance with the three fundamental types of classical GNNs: Graph Convolutiona Networks (GCNs) (e.g.[24]), Graph Attention Networks (GATs) (e.g.[25]), and Message-Passing Neu ral Networks (MPNNs) (e.g.[26]).\nThe complexity analysis in our paper demonstrates that our quantum implementation of a Simplified Graph Convolution (SGC) network can potentially achieve significant improvements in time and/or space complexity compared to its classical counterpart, under certain conditions commonly encountered in real-world applications. When optimizing for the minimal number of qubits, the quantum SGC exhibits a space complexity that is logarithmic in the input sizes, offering an exponential reduction compared to the classical SGC2. On the other hand, when optimizing for minimal circuit depth, our quantum SGC provides a substantial improvement in time complexity, achieving logarithmic dependence on the input sizes3. These complexity results suggest that our quantum implementation of the SGC has the potential to efficiently process large-scale graphs, under certain assumptions that align with practical use cases. The trade-off between circuit depth and the number of qubits in the quantum implementation provides flexibility in adapting to specific quantum hardware constraints and problem instances, making it a promising approach for tackling complex graph-related machine learning tasks.\nThe rest of the paper is organized as follows. In Section 2, we provide an overview of classica GNNs. In Section 3, 4, and 5, we present our quantum algorithms for GCNs, GATs, and MPNNs respectively. We analyze the complexity of our Quantum implementation of two GCN variants i Section 6 and discuss the potential advantages in Section 3.2 and 3.3. Finally, we conclude the pape and outline future research directions in Section 7.\nBefore diving into our QNN architectures, it is worth noting that our work also falls within th emerging field of Geometric Quantum Machine Learning (GQML) [27\u201331], which aims to create quan tum machine learning models that respect the underlying structure and symmetries of the data the\n1 Customized hardware accelerators for sparse matrices can improve GNNs\u2019 latency and scalability, but their design remains an open question [12]. 2 In this case, the quantum SGC still provides better time complexity than its classical counterpart, particularly for graphs with a large number of nodes and high-dimensional node features. 3 This improvement comes with a trade-off in space complexity, which is comparable to that of the classical SGC.\nprocess. To illustrate how our frameworks align with the principles of GQML, we present an overview of our approach for Quantum Graph Convolutional Networks, summarized in Fig. 21. This example demonstrates how our Quantum GNNs incorporate inductive biases to process graph-structured data, potentially leading to improvements compared to problem-agnostic quantum machine learning models.\n# 2. Classical Graph Neural Networks\n2. Classical Graph Neural Networks\nFollowing Ref. [1, 32, 33], we provide a brief introduction to classical Graph Neural Network which serve as the foundation for the development of our quantum GNNs.\nGraphs are a natural way to represent complex systems of interacting entities. Formally, a graph G = (V, E) consists of a set of nodes V and a set of edges E \u2286V \u00d7 V that connect pairs of nodes. In many real-world applications, graphs are used to model relational structure, with nodes representing entities (e.g., users, proteins, web pages) and edges representing relationships or interactions between them (e.g., friendships, molecular bonds, hyperlinks). To enable rich feature representations, nodes are often endowed with attribute information in the form of real-valued feature vectors. Given a graph with N = |V | nodes, we can summarize the node features as a matrix X \u2208RN\u00d7C, where the u-th row xu \u2208RC corresponds to the C-dimensional feature vector of node u. The connectivity of the graph can be represented by an adjacency matrix A \u2208RN\u00d7N, where auv = 1 if there is an edge between nodes u and v, and auv = 0 otherwise.\nGraph Neural Networks (GNNs) are a family of machine learning models that operate on the graph structure (X, A). The key defining property of GNNs is permutation equivariance. Formally, let P \u2208{0, 1}N\u00d7N be an permutation matrix. A GNN layer, denoted by F(X, A), is a permutationequivariant function in the sense that:\n# F(PX, PAP\u22a4) = PF(X, A)\nPermutation equivariance is a desirable inductive bias for graph representation learning, as it ensures that the GNN output will be invariant to arbitrary reorderings of the nodes. This property arises naturally from the unordered nature of graph data, i.e., a graph is intrinsically defined by its connectivity and not by any particular node ordering.\nIn each GNN layer, nodes update their features by aggregating information from their local neighborhoods ((undirected) neighbourhood of node u is defined as Nu = {v|(u, v) \u2208Eor (v, u) \u2208E}). This local computation is performed identically (i.e., shared) across all nodes in the graph. Mathematically, a GNN layer computes a new feature matrix H \u2208RN\u00d7C\u2032 from the input features X as follows:\nH = F(X, A) = [\u03d5(x1, XN1), \u03d5(x2, XN2), ..., \u03d5(xN, XNN )]\u22a4\nwhere \u03d5 is a local function often called the neighborhood aggregation or message passing function, and XNu = {{xv | v \u2208Nu}} denotes the multiset of all neighbourhood features of node u. In other words, the new feature vector hu := \u03d5(xu, XNu) of node u is computed by applying \u03d5 to the current feature xu and the features of its neighbors XNu. Since \u03d5 is shared across all nodes and only depends on local neighborhoods, it can be shown that if \u03d5 is permutation invariant in XNu, then F will be permutation equivariant. Stacking multiple GNN layers allows information to propagate over longer graph distances, enabling the network to capture high-order interaction effects.\n(1)\nWhile the general blueprint of GNNs based on local neighborhood aggregation is quite simple and natural, there are many possible choices for the aggregation function \u03d5. The design and study of GNN layers is a rapidly expanding area of deep learning, and the literature can be divided into three \u201cflavours\u201d [1]: convolutional, attentional, and message-passing (see Figure 2). These flavours determine the extent to which \u03d5 transforms the neighbourhood features, allowing for varying levels of complexity when modelling interactions across the graph.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f41/2f41b332-f8e5-4684-8aa4-cde0e3e8c9e2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">FIG. 2. GNN pipeline and three \u201cflavours\u201d of GNN layers[1] GNN architectures are permutation equivariant functions F(X, A) constructed by applying shared permutation invariant functions \u03d5 over local neighbourhoods. This local function \u03d5 is sometimes referred to as \u201cdiffusion,\u201d \u201cpropagation,\u201d or \u201cmessage passing,\u201d and the overall computation of such F is known as a \u201cGNN layer.\u201d These flavours determine the extent to which \u03d5 transforms the neighbourhood features, allowing for varying levels of complexity when modelling interactions across the graph.</div>\nFIG. 2. GNN pipeline and three \u201cflavours\u201d of GNN layers[1] GNN architectures are permutation equivarian functions F(X, A) constructed by applying shared permutation invariant functions \u03d5 over local neighbourhoods This local function \u03d5 is sometimes referred to as \u201cdiffusion,\u201d \u201cpropagation,\u201d or \u201cmessage passing,\u201d and the overa computation of such F is known as a \u201cGNN layer.\u201d These flavours determine the extent to which \u03d5 transform the neighbourhood features, allowing for varying levels of complexity when modelling interactions across th graph.\nIn the convolutional flavour (e.g.[24]), the features of the neighbouring nodes are directly combined with fixed weights,\nHere, cuv is a constant indicating the significance of node v to node u\u2032 s representation. \ufffdis the aggregation operator which is often chosen to be the summation. \u03c8 and \u03d5 are learnable transformations4: \u03c8(x) = Wx + b, \u03d5(x, z) = Wx + Uz + b.\n4 Note we omitted the activation function in the original definition in [1], the quantum implementation of the activatio function is described in Section 3.1.2. And for simplicity, we omit b in our quantum case.\nIn the attentional flavour (e.g.[25]),\n\uf8ed \uf8f8 a learnable self-attention mechanism is used to compute the coefficients a (xu, xv). When \ufffdis the summation, the aggregation is still a linear combination of the neighbourhood node features, but the weights are now dependent on the features.\n\uf8ed \uf8f8 a learnable self-attention mechanism is used to compute the coefficients a (xu, xv). When \ufffdis the summation, the aggregation is still a linear combination of the neighbourhood node features, but the weights are now dependent on the features. Finally, the message passing flavour (e.g.[26]) involves computing arbitrary vectors (\u201cmessages\u201d)\nFinally, the message passing flavour (e.g.[26]) involves computing arbitrary vectors (\u201cmessages\u201d) across edges,\nFinally, the message passing flavour (e.g.[26]) involves computing arbit across edges,\n\uf8ed \uf8f8 Here, \u03c8 is a trainable message function, which computes the vector sent from v to u, and the agg gation can be considered as a form of message passing on the graph.\nThe three GNN flavors \u2013 convolutional, attentional, and message-passing \u2013 offer increasing level of expressivity, albeit comes at the cost of reduced scalability. The choice of GNN flavor for a given task requires carefully considering this trade-off and prioritizing the most important desiderata fo the application at hand.\nClassical GNNs have been shown to be highly effective in a variety of graph-related tasks inclu ng [32, 34]:\n1.[Node classification], where the goal is to assign labels to nodes based on their attributes and th graph structure. For example, in a social network, the task could be to classify users into differen categories by leveraging their profile information and social connections. In a biological context,  canonical example is classifying protein functions in a protein-protein interaction network [35].\n2.[Link prediction], where the objective is to predict whether an edge exists between two node in the graph, or predicting the properties of the edges. In a social network, this could translate to predicting potential interactions between users. In a biological context, it could involve predicting links between drugs and diseases\u2014drug repurposing [36].\n3.[Graph classification], where the goal is to classify entire graphs based on their structures and attributes. A typical example is classifying molecules in terms of their quantum-chemical properties, which holds significant promise for applications in drug discovery and materials science [26].\nAs aforementioned in section 1, despite their success, classical GNNs also face challenges in scalability. This motivates our exploration of utilizing quantum computing to address the challenges.\nIn the following three sections of this paper, we will devise and analyze QNN architectures in accordance with the three major types of classical GNNs(corresponding to the three flavours): Graph Convolutional Networks, Graph Attention Networks, Message-Passing GNNs. We term our QNN architectures as Quantum Graph Convolutional Networks, Quantum Graph Attention Networks, and Quantum Message-Passing GNNs which fall into the research area of Quantum Graph Neural Networks.\n# 3. Quantum Graph Convolutional Networks\n# 3.1. Vanilla GCN and its Quantum version\nIn this section, we present our quantum algorithm for the problem of node classification with Graph Convolutional Networks (GCN) [24]. We start by restating some notations: Let G = (V, E) be a graph, where V is the set of nodes and E is the set of edges. A \u2208RN\u00d7N is the adjacency matrix, with N being the total number of nodes, and X \u2208RN\u00d7C is the node attribute matrix, with C being the number of features for each node. The node representations at the l-th layer is denoted as H(l) \u2208RN\u00d7Fl, l \u2208{0, 1, 2, \u00b7 \u00b7 \u00b7 , K}, where Fl being the dimension of node representation for each node. These notations are summarised in the following table.\nConcept\nNotation\nGraph\nG = (V, E)\nAdjacency matrix\nA \u2208RN\u00d7N\nNode attributes\nX \u2208RN\u00d7C\nTotal number of GCN layers\nK\nNode representations at the l-th layer\nH(l) \u2208RN\u00d7Fl, l \u2208{0, 1, 2, \u00b7 \u00b7 \u00b7 , K}\nThe GNN layer (described in Section 2) in a Graph Convolutional Network, often termed \u201cGra Convolution,\u201d can be carried out as[24]:\n\ufffd\ufffd Here, \u02c6A = \u02dcD\u22121 2 \u02dcA \u02dcD\u22121 2 in which \u02dcA = A + IN is the adjacency matrix of the graph G with added self-connections(IN is the identity matrix), \u02dcDii = \ufffd j \u02dcAij, and W (l) is a layer-specific trainable weight matrix. \u03c3(\u00b7) denotes an activation function. At the output of the last layer, the softmax function, defined as softmax (xi) = 1 Z exp (xi) with Z = \ufffd i exp (xi), is applied row-wise to the node feature matrix, producing the final output of the network:\nHere, \u02c6A = \u02dcD\u22121 2 \u02dcA \u02dcD\u22121 2 in which \u02dcA = A + IN is the adjacency matrix of the graph G with added self-connections(IN is the identity matrix), \u02dcDii = \ufffd j \u02dcAij, and W (l) is a layer-specific trainable weigh matrix. \u03c3(\u00b7) denotes an activation function.\nAt the output of the last layer, the softmax function, defined as softmax (xi) = 1 Z exp (xi) with Z = \ufffd i exp (xi), is applied row-wise to the node feature matrix, producing the final output of the network:\nFor semi-supervised multi-class classification, the cost function is defined by the cross-entropy erro over all labelled examples [24]:\nwhere YL is the set of node indices that have labels, Y \u2208BN\u00d7FK denotes the one-hot encoding of the labels. The GCN pipeline mentioned above is summarised in Fig.3.\nwhere YL is the set of node indices that have labels, Y \u2208BN\u00d7FK denotes the one-hot encoding of the labels. The GCN pipeline mentioned above is summarised in Fig.3. Next, we present the Quantum implementation of GCN.\nwhere YL is the set of node indices that have labels, Y \u2208BN\u00d7FK denotes the one-hot encoding of the labels. The GCN pipeline mentioned above is summarised in Fig.3.\nNext, we present the Quantum implementation of GCN.\n(2)\n(3)\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee2e/ee2e222f-5b65-4419-8984-fc9d5931db3b.png\" style=\"width: 50%;\"></div>\nFIG. 3. GCN Pipeline. A GCN consists of a series of layers in which graph convolution and non-linear activation functions are applied to the node features. (Note that the schematics in this figure are for illustration purposes only, e.g. the normalized adjacency matrix depicted here does not include the added self-connections) At the output of the last layer, softmax activation function, defined as softmax (xi) = 1 Z exp (xi) with Z = \ufffd i exp (xi), is applied row-wise to the node feature matrix, producing the final output of the network: Z = softmax( \u02c6AH(K\u22121)W (K\u22121)). For semi-supervised multi-class classification, the cost function is defined by the cross-entropy error over all labelled examples [24]:L = \u2212\ufffd s\u2208YL \ufffdFK f=1 Ysf ln Zsf, where YL is the set of node indices that have labels, Y \u2208BN\u00d7FK denotes the one-hot encoding of the labels.\n<div style=\"text-align: center;\">3.1.1. Data Encoding</div>\nFor GCN, the node features X \u2208RN\u00d7C of which the entries are denoted as Xik, can be encoded in a quantum state |\u03c8X\u27e9(after normalization)5 as follows:\nwhere |xi\u27e9= \ufffdC k=1 Xik|k\u27e9, being the amplitude encoding of the features for node i over the channels(indexed by k), is entangled with an address state |i\u27e9. The entire state is prepared on two quantum registers hosting the channel index k and node index i, which are denoted as Reg(k) and Reg(i), respectively. The data encoding, represented as the blue box in Fig. 4, can be achieved by various quantum state preparation procedures [37\u201348]. We choose the method from Ref. [48] for our data encoding, as their work provides a tunable trade-off between the number of ancillary qubits and the circuit depth for state preparation.\n5 Note throughout this paper we often omit the normalization factors in quantum states.\n(5)\nThe layer-wise linear transformation for multi-channel GCN (i.e. H\u2032(l) = \u02c6AH(l)W (l) 6), can be implemented by applying the block-encoding7 of \u02c6A and a parameterized quantum circuit implementing W (l) on the two quantum registers Reg(i) and Reg(k) respectively, as depicted in Fig. 4. This is proved in Appendix C.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eed7/eed769a1-8b01-4270-9347-fd0661d93109.png\" style=\"width: 50%;\"></div>\nFIG. 4. Quantum implementation of linear layer-wise transformation for multi-channel GCN The linear layerwise transformation for multi-channel GCN (i.e. the layer-specific trainable weight matrix and the normalized adjacency matrix multiplied on the node feature matrix), can be implemented by applying the block-encoding of the normalized adjacency matrix and a parametrized quantum circuit on the two quantum registers Reg(i) and Reg(k) respectively. Here we depicted the first layer of GCN \u2014 the linear layer-wise transformation is applied on the state prepared by the data encoding procedure (the blue box) described in Section 3.1.1. Note that the schematics in this figure are for illustration purposes only, e.g. 1) the normalized adjacency matrix depicted here does not include the added self-connections; 2) the ancillary qubits used in the quantum state preparation for the data encoding is not depicted in this figure.\nAfter the linear layer-wise transformation, the element-wise non-linear activation function can be applied using an established technique called Nonlinear Transformation of Complex Amplitudes (NTCA)[49]. One can also potentially utilize the techniques from Ref. [50] for applying the non-linear activation function and achieve better performance, we leave this and the detailed analysis for future work. Ref. [24] considered a two-layer8 GCN where the non-linear activation function is applied only once and the forward model takes the following form:\nZ = softmax \ufffd\u02c6A\u03c3 \ufffd\u02c6AXW (0)\ufffd W (1)\ufffd .\n\ufffd \ufffd\ufffd \ufffd Next, we present the quantum state evolution for the quantum version of this two-layer GCN. Denote the block-encoding of \u02c6A as U \u02c6 A and the parameterized quantum circuit for W (0) as UW 0, applying these operations on the quantum state |\u03c8X\u27e9results in the following state:\n|\u03c8H\u2032(0)\u27e9\u2297|0\u27e9+ . . . = \ufffdU \u02c6 A \u2297UW (1) \ufffd|\u03c8X\u27e9\u2297|0\u27e9,\n\ufffd \ufffd 6 Here we use H\u2032(l) to denote the linearly transformed feature matrix. 7 Appendix D provides a brief introduction of block-encoding. 8 It has been observed in many experiments that deeper models does not always improve performance and can even lead to worse outcomes compared to shallower models. [51]\n(6)\n(7)\nwhere |\u03c8H\u2032(0)\u27e9= \ufffdN i=1 |i\u27e9 \ufffd\ufffd\ufffdh\u2032(0) i \ufffd is on the two quantum registers Reg(i), Reg(k) and \ufffd\ufffd\ufffdh\u2032(0) i \ufffd = \ufffdC k=1 H\u2032(0) ik |k\u27e9is the amplitude encoding of the linearly transformed features for node i over the channels(indexed by k). The term \u201c+ . . .\u201d9 represents a quantum state that is orthogonal to the state before the \u201c+\u201d sign. The quantum state |\u03c8H\u2032(l)\u27e9encodes the linearly transformed node features. The block-encoding of \u02c6A performs the aggregation of neighboring node features, while the parameterized quantum circuit UW (1) applies the trainable weight matrix to the node features. Then, using NTCA to implement a non-linear activation function on the amplitudes of the state |\u03c8H\u2032(0)\u27e9\u2297|0\u27e9+ . . ., we obtain the state |\u03c8H(1)\u27e9\u2297|0\u27e9+ . . . in which |\u03c8H(1)\u27e9= \ufffdN i=1 |i\u27e9 \ufffd\ufffd\ufffdh(1) i \ufffd and \ufffd\ufffd\ufffdh(1) i \ufffd = \ufffdC k=1 H(1) ik |k\u27e9= \ufffdC k=1 \u03c3(H\u2032(0) ik )|k\u27e9. An example of the full quantum circuit for the GNN layer (C = 1, single channel) is depicted in Fig.5.\nwhere |\u03c8H\u2032(0)\u27e9= \ufffdN i=1 |i\u27e9 \ufffd\ufffd\ufffdh\u2032(0) i \ufffd is on the two quantum registers Reg(i), Reg(k) and \ufffd\ufffd\ufffdh\u2032(0) i \ufffd = \ufffdC k=1 H\u2032(0) ik |k\u27e9is the amplitude encoding of the linearly transformed features for node i over the channels(indexed by k). The term \u201c+ . . .\u201d9 represents a quantum state that is orthogonal to the state before the \u201c+\u201d sign.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8dc/c8dc3598-e113-4aa2-a9c5-fb4331064e33.png\" style=\"width: 50%;\"></div>\nFIG. 5. Example of the full quantum circuit for a GNN layer (C = 1, single channel). Utilising NTCA in our Quantum GCN to implement a non-linear activation function, we take the unitary of data encoding and graph convolution as components to build a new unitary that generates the desired state whose amplitudes are transformed by certain nonlinear functions. Note that the schematics in this figure are for illustration purposes only. Denote the parameterized quantum circuit for W (1) as UW 1, applying U \u02c6 A and UW 1 on the quantum state the state |\u03c8H(1)\u27e9\u2297|0\u27e9+ . . . results in the state: |\u03c8out\u27e9= |\u03c8H\u2032(1)\u27e9\u2297|0\u27e9+ . . . in which |\u03c8H\u2032(1)\u27e9= \ufffdN i=1 |i\u27e9 \ufffd\ufffd\ufffdh\u2032(1) i \ufffd and \ufffd\ufffd\ufffdh\u2032(1) i \ufffd = \ufffdC k=1 H\u2032(1) ik |k\u27e9.\nDenote the parameterized quantum circuit for W (1) as UW 1, applying U \u02c6 A and UW 1 on the quantum state the state |\u03c8H(1)\u27e9\u2297|0\u27e9+ . . . results in the state: |\u03c8out\u27e9= |\u03c8H\u2032(1)\u27e9\u2297|0\u27e9+ . . . in which |\u03c8H\u2032(1)\u27e9= \ufffdN i=1 |i\u27e9 \ufffd\ufffd\ufffdh\u2032(1) i \ufffd and \ufffd\ufffd\ufffdh\u2032(1) i \ufffd = \ufffdC k=1 H\u2032(1) ik |k\u27e9.\n9 Throughout this paper, the terms \u201c+ . . .\u201d in the quantum states are consistently used as defined here.\nFor semi-supervised multi-class classification, the cost function used in our QGCN is defined as the negative inner product between the output quantum state |\u03c8out\u27e9and the target label state |\u03c8Y \u27e9:\nwhere |\u03c8out\u27e9= \ufffdN i=1 \ufffdC k=1 H\u2032(1) ik |i\u27e9|k\u27e9\u2297|0\u27e9+. . . is the output state of the QGCN, with H\u2032(1) ik being the amplitude corresponding to node i and class k, and |\u03c8Y \u27e9:= \ufffd s\u2208YL \ufffdC f=1 Ysf |s\u27e9|f\u27e9\u2297|0\u27e9represents the true labels of the labeled nodes as a quantum state. The cost function can be evaluated via the \u201cModified Hadamard test\u201d [52, 53].\nThe training of the QGCN involves optimizing the parameters of the quantum circuit to minimize the cost function. This optimization can be performed using either classical or quantum techniques [54]. Once the QGCN is trained, the inference process involves applying the optimized QGCN circuit to an input graph to obtain the predicted node labels. To extract the predicted labels from the output state |\u03c8out\u27e9, quantum state tomography techniques are employed. After obtaining the tomographic estimates of the output state, post-processing steps are applied to convert the results into the final predicted node labels\u2014softmax function is applied to the estimated amplitudes H\u2032(1) ik (from the trained model) to obtain the normalized predicted probabilities for each class, The class with the highest predicted probability is then assigned as the final predicted label for each node.\nIn the following two subsections, we propose quantum versions of two GCN variants: the Simplified Graph Convolution (SGC) [55] and the Linear Graph Convolution (LGC) [56].\n# Simplified Graph Convolution (SGC) and its quantum v\nThe Simplified Graph Convolution (SGC) [55] reduces the complexity of Graph Convolutional Networks (GCNs) by removing nonlinearities(while exhibits comparable or even superior performance compared to vanilla GCN and other complicated GNN models [55, 57]). For node classification, the prediction generated by SGC is,\nYSGC = softmax(SKX\u0398),\nwhere S = \u02c6A is the normalized adjacency matrix with added self-loops, X \u2208RN\u00d7C is the node attribute matrix, \u0398 is a weight matrix, and K is a positive integer (originally representing the number of layers in GCN, though this concept becomes irrelevant in the context of SGC). Importantly, the experimental results demonstrated that the simplifications do not affect the accuracy across various applications [55].\nThe quantum implementation of SGC is similar to that of the linear transformation in GCN, whic comprises three key components: data encoding of the node attribute matrix X, quantum circuit fo the block-encoding of SK, and a parameterized quantum circuit for the weight matrix \u0398.\nThe data encoding step(quantum state preparation) for SGC is identical to that of GCN. Extensiv research has been conducted on the problem of quantum state preparation [37\u201348]. We select th approach from Ref. [48] for our data encoding, as their work provides a tunable trade-off between th number of ancillary qubits and the circuit depth for the state preparation. This flexibility allows u\n11\n(8)\n(9)\nto optimally encode our classical data as a quantum state |\u03c8X\u27e9by selecting the appropriate number of ancillary qubits based on the capabilities of our quantum hardware, while minimizing the circuit depth overhead required to achieve the desired precision. According to Theorem 3 in [48], with nanc ancillary qubits where \u2126(log(NC)) \u2a7dnanc \u2a7dO(NC), the initial data state |\u03c8X\u27e9can be prepared to accuracy \u03b51 with \u02dcO(NC log(1/\u03b51) log(nanc)/nanc) depth of Clifford+T gates, where \u02dcO suppresses the doubly logarithmic factors of nanc. The weight matrix \u0398 in SGC is implemented using a parameterized quantum circuit (PQC), similar to the approach used in the quantum GCN. We assume that the depth of this PQC is less than the depth of the circuit for SK. This assumption is based on the flexibility in choosing the depth of the PQC, which allows for a trade-off between the circuit depth and its expressive power. The expressive power of a PQC is related to its ability to explore the unitary space in an unbiased manner, increasing the depth of the PQC can lead to higher expressive power [58\u201361]. By choosing the depth of the PQC for \u0398 to be less than that of the circuit for SK, we prioritize the efficiency of the overall quantum SGC implementation while potentially sacrificing some expressiveness in the weight matrix. The interplay between the depth of the PQC (and its associated expressiveness) and the depth of the block-encoding circuit for SK is an interesting topic for future research, as it may reveal further opportunities for optimization in the quantum SGC implementation. In the quantum SGC, for K = 2,10 we can efficiently implement SK by leveraging the product of block-encoded matrices as stated in Lemma 53 of Ref.[62]: if U is an (\u03b1, n\u2032 anc, \u03b52)-block-encoding of S, then the product (I \u2297U)(I\u2032 \u2297U), is an (\u03b12, 2n\u2032 anc, 2\u03b1\u03b52)-block-encoding of S2. For semi-supervised multi-class classification, similar to that of vanilla GCN, the cost function of our Quantum SGC is defined as the negative inner product of the outcome state of our quantum SGC and a target label state |\u03c8Y \u27e9prepared as vec(Y T ). The cost function can be evaluated via the Modified Hadamard test [52, 53].\nto optimally encode our classical data as a quantum state |\u03c8X\u27e9by selecting the appropriate number of ancillary qubits based on the capabilities of our quantum hardware, while minimizing the circuit depth overhead required to achieve the desired precision. According to Theorem 3 in [48], with nanc ancillary qubits where \u2126(log(NC)) \u2a7dnanc \u2a7dO(NC), the initial data state |\u03c8X\u27e9can be prepared to accuracy \u03b51 with \u02dcO(NC log(1/\u03b51) log(nanc)/nanc) depth of Clifford+T gates, where \u02dcO suppresses the doubly logarithmic factors of nanc.\nThe weight matrix \u0398 in SGC is implemented using a parameterized quantum circuit (PQC), similar to the approach used in the quantum GCN. We assume that the depth of this PQC is less than the depth of the circuit for SK. This assumption is based on the flexibility in choosing the depth of the PQC, which allows for a trade-off between the circuit depth and its expressive power. The expressive power of a PQC is related to its ability to explore the unitary space in an unbiased manner, increasing the depth of the PQC can lead to higher expressive power [58\u201361]. By choosing the depth of the PQC for \u0398 to be less than that of the circuit for SK, we prioritize the efficiency of the overall quantum SGC implementation while potentially sacrificing some expressiveness in the weight matrix. The interplay between the depth of the PQC (and its associated expressiveness) and the depth of the block-encoding circuit for SK is an interesting topic for future research, as it may reveal further opportunities for optimization in the quantum SGC implementation.\noptimization in the quantum SGC implementation. In the quantum SGC, for K = 2,10 we can efficiently implement SK by leveraging the product of block-encoded matrices as stated in Lemma 53 of Ref.[62]: if U is an (\u03b1, n\u2032 anc, \u03b52)-block-encoding of S, then the product (I \u2297U)(I\u2032 \u2297U), is an (\u03b12, 2n\u2032 anc, 2\u03b1\u03b52)-block-encoding of S2. For semi-supervised multi-class classification, similar to that of vanilla GCN, the cost function of our Quantum SGC is defined as the negative inner product of the outcome state of our quantum SGC and a target label state |\u03c8Y \u27e9prepared as vec(Y T ). The cost function can be evaluated via the Modified Hadamard test [52, 53]. The complexity of the quantum SGC depends on the choice of number of ancillary qubits in the data encoding procedure and the block-encoding procedure in the layer-wise linear transformation: there\u2019s trade-off between circuit depth and the number of qubits in the quantum SGC implementation. We first consider the two extreme cases in the trade-off: Table 1 presents the complexity comparison between the quantum SGC and the classical SGC for a single forward pass and evaluation of the cost function, assuming fixed precision parameters. The details of the complexity analysis is given in\nIn the quantum SGC, for K = 2,10 we can efficiently implement SK by leveraging the product of block-encoded matrices as stated in Lemma 53 of Ref.[62]: if U is an (\u03b1, n\u2032 anc, \u03b52)-block-encoding of S, then the product (I \u2297U)(I\u2032 \u2297U), is an (\u03b12, 2n\u2032 anc, 2\u03b1\u03b52)-block-encoding of S2. For semi-supervised multi-class classification, similar to that of vanilla GCN, the cost function of our Quantum SGC is defined as the negative inner product of the outcome state of our quantum SGC and a target label state |\u03c8Y \u27e9prepared as vec(Y T ). The cost function can be evaluated via the Modified Hadamard test [52, 53].\nThe complexity of the quantum SGC depends on the choice of number of ancillary qubits in the data encoding procedure and the block-encoding procedure in the layer-wise linear transformation there\u2019s trade-off between circuit depth and the number of qubits in the quantum SGC implementation We first consider the two extreme cases in the trade-off: Table 1 presents the complexity comparison between the quantum SGC and the classical SGC for a single forward pass and evaluation of the cost function, assuming fixed precision parameters. The details of the complexity analysis is given in Section 6.\n# [Trade-off] Case 1: Quantum SGC with Minimum Depth \u2013 Unlocking Quantum Speedup\nIn this case, the quantum SGC prioritizes minimizing the circuit depth at the cost of requiring more ancillary qubits. The time complexity of the quantum SGC in the case is logarithmic in the input sizes, i.e., \u02dcO(log(NC) + log(Ns))(assuming fixed success probability), this represents a significant improvement over the classical SGC\u2019s time complexity of O(NdC + NC2). However, the space complexity of quantum SGC in this case is comparable to that of the classical SGC. The quantum SGC\u2019s logarithmic time complexity in this scenario is particularly beneficial for time-efficient processing large-scale graphs with high-dimensional node features.\nIn this case, the quantum SGC focuses on minimizing the number of required qubits at the cost\nAlgorithm\nTime Complexity\nSpace Complexitya\nQuantum SGC (Min. Depth)\n\u02dcO(log(1/\u03b4) \u00b7 (log(NC) + log(Ns)))\nO(NC + N log N \u00b7 s log s)\nQuantum SGC (Min. Qubits)\n\u02dcO(log(1/\u03b4) \u00b7 (NC/ log(NC) + Ns log s))\nO(log(NC))\nClassical SGC\nO(|E|C + NC2)) = O(NdC + NC2)\nO(|E| + NC + C2) = O(Nd + NC + C2)\na space complexity in the quantum case refers to the number of qubits, including the ancilla qubits used by the circuit\n[63]. TABLE 1. Complexity comparison between Quantum SGC and Classical SGC (K = 2) for a single forward pass and cost function evaluation, assuming fixed precision parameters. N is the number of nodes, C is the number of features per node. d is the average degree of the nodes in the graph. s is the maximum number of non-zero elements in each row/column of \u02c6A. The quantum SGC provides a probabilistic result with a success probability of 1 \u2212\u03b4. Note that in the classical time complexity, at first glance, O(NC2) appears to be the dominating term, as the average degree d on scale-free networks is usually much smaller than C and hence NC2 > NdC. However, in practice, node-wise feature transformation can be executed at a reduced cost due to the parallelism in dense-dense matrix multiplications. Consequently, O(NdC) is the dominant complexity term in the time complexity of classical SGC and the primary obstacle to achieving scalability [64]. of increased circuit depth. This trade-off is particularly relevant when dealing with massive graph datasets that exceed the memory constraints of classical computing systems. The space complexity of the quantum SGC in the minimum qubits case is O(log(NC)), which represents an exponential reduction compared to the classical SGC\u2019s space complexity of O(Nd + NC + C2). This logarithmic space complexity enables the quantum SGC to process graphs of unprecedented scale, even on quantum hardware with limited qubit resources. The ability to process large-scale graphs with limited quantum resources is particularly valuable in domains such as social network analysis, where the graph size can easily reach billions of nodes. Storing such a graph in the memory of a classical computing system becomes infeasible due to the space complexity. However, the quantum SGC\u2019s logarithmic space complexity allows for the efficient encoding and processing of the graph using only a logarithmic number of qubits. This capability enables the exploration and analysis of these massive graphs, uncovering insights and patterns that were previously computationally infeasible. Furthermore, the time complexity of the quantum SGC, in this case, still offers a computational advantage over the classical SGC. Although the speedup is less pronounced compared to the minimum depth case, it remains significant for graphs with a large number of nodes and high-dimensional node features. For the intermediate cases in the tradeoff, the quantum SGC seeks a balance between the circuit depth and the number of ancillary qubits, which could potentially lead to moderate improvements in both time and space complexity. For example, by choosing nanc = \u0398( \u221a NC) and n\u2032 anc = \u0398(\u221aN log N \u00b7 s log s), we obtain a time complexity of \u02dcO(log(1/\u03b4)\u00b7( \u221a NC log(NC)+\u221aN log N \u00b7 s log s log(Ns))) and a space complexity of \u221a  \u221a. This moderate case could be advantageous for\nTABLE 1. Complexity comparison between Quantum SGC and Classical SGC (K = 2) for a single forward pass and cost function evaluation, assuming fixed precision parameters. N is the number of nodes, C is the number of features per node. d is the average degree of the nodes in the graph. s is the maximum number of non-zero elements in each row/column of \u02c6A. The quantum SGC provides a probabilistic result with a success probability of 1 \u2212\u03b4. Note that in the classical time complexity, at first glance, O(NC2) appears to be the dominating term, as the average degree d on scale-free networks is usually much smaller than C and hence NC2 > NdC. However, in practice, node-wise feature transformation can be executed at a reduced cost due to the parallelism in dense-dense matrix multiplications. Consequently, O(NdC) is the dominant complexity term in the time complexity of classical SGC and the primary obstacle to achieving scalability [64].\nof increased circuit depth. This trade-off is particularly relevant when dealing with massive graph datasets that exceed the memory constraints of classical computing systems. The space complexity of the quantum SGC in the minimum qubits case is O(log(NC)), which represents an exponential reduction compared to the classical SGC\u2019s space complexity of O(Nd + NC + C2). This logarithmic space complexity enables the quantum SGC to process graphs of unprecedented scale, even on quantum hardware with limited qubit resources.\nThe ability to process large-scale graphs with limited quantum resources is particularly valuable in domains such as social network analysis, where the graph size can easily reach billions of nodes. Storing such a graph in the memory of a classical computing system becomes infeasible due to the space complexity. However, the quantum SGC\u2019s logarithmic space complexity allows for the efficient encoding and processing of the graph using only a logarithmic number of qubits. This capability enables the exploration and analysis of these massive graphs, uncovering insights and patterns that were previously computationally infeasible. Furthermore, the time complexity of the quantum SGC, in this case, still offers a computational advantage over the classical SGC. Although the speedup is less pronounced compared to the minimum depth case, it remains significant for graphs with a large number of nodes and high-dimensional node features.\nFor the intermediate cases in the tradeoff, the quantum SGC seeks a balance between the circuit depth and the number of ancillary qubits, which could potentially lead to moderate improvements in both time and space complexity. For example, by choosing nanc = \u0398( \u221a NC) and n\u2032 anc = \u0398(\u221aN log N \u00b7 s log s), we obtain a time complexity of \u02dcO(log(1/\u03b4)\u00b7( \u221a NC log(NC)+\u221aN log N \u00b7 s log s log(Ns))) and a space complexity of O( \u221a NC + \u221aN log N \u00b7 s log s). This moderate case could be advantageous for certain quantum hardware architectures or problem instances where neither the circuit depth nor the number of qubits is the sole limiting factor. This case demonstrates the flexibility of the quantum SGC implementation in adapting to specific resource constraints while still maintaining a potential quantum advantage over the classical SGC.\nThe complexity comparison between the quantum and classical SGC highlights the potential fo quantum advantage in terms of both time and space complexity. The trade-off between circuit dept and the number of qubits in the quantum SGC implementation offers flexibility in adapting to specif quantum hardware constraints and problem instances.\nThe Linear Graph Convolution (LGC) proposed by Pasa et al. [56] is a more expressive variant of SGC. The LGC operation is defined as:\nThe Linear Graph Convolution (LGC) proposed by Pasa et al. [56]  SGC. The LGC operation is defined as:\nwhere L is the Laplacian matrix of the graph, X is the node feature matrix, \u03b1i are learnable weights, \u0398 is a weight matrix, and K is an integer(a hype-parameter). This is essentially a spectral graph convolution (e.g. [65]) without nonlinear activation function.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79f7/79f7acc1-4e61-4e08-9175-d3845be73297.png\" style=\"width: 50%;\"></div>\nFIG. 6. Schematic quantum circuit for the cost function evaluation procedure of our Quantum LGC. The quantum implementation of LGC is similar to that of SGC, the major difference is the implementation of the aggregations of node features: we utilize the Polynomial eigenvalue transformation, a special instance of Quantum Singular Value Transformation (QSVT) (Theorem 56 in [66]), to implement \ufffdk i=0 \u03b1iLi. This requires a block-encoding of the Laplacian matrix L and appropriate Pauli rotation angles in the QSVT circuit corresponding to the polynomial coefficients \u03b1i. The parametrization of the polynomial is equivalent to parametrization of the Pauli angles(phases) in the QSVT circuit, that is, the phases are the tunable weights to be trained. For semi-supervised multi-class classification, similar to that of vanilla GCN, the cost function of our Quantum LGC is defined as the inner product of the outcome state of our quantum LGC and a target label state |\u03c8Y \u27e9prepared as vec(Y T ). The cost function can be evaluated via the Modified Hadamard test [52, 53].\nBy allowing multiple learnable weighting coefficients \u03b1i for each Li up to order K, LGC can represent a much richer class of graph convolution filters compared to SGC. This increased expressiveness enables LGC to capture more complex graph structures and long-range dependencies, leading to improved performance on certain downstream tasks.[56]\nThe quantum implementation of LGC is similar to that of SGC, the major difference is the implementation of the aggregations of node features: we utilize the \u201cPolynomial eigenvalue transformation,\u201d a special instance of the Quantum Singular Value Transformation (QSVT) (Theorem 56 in [66]), to implement \ufffdK i=0 \u03b1iLi. This requires a block-encoding of the Laplacian matrix L and appropriate Pauli rotation angles in the QSVT circuit corresponding to the polynomial coefficients \u03b1i. The parametrization of the polynomial is equivalent to parametrization of the Pauli angles in the QSVT circuit, that\n(10)\nis, the phases are the tunable weights to be trained. Figure 6 illustrates the schematic quantum circui for the cost function evaluation procedure of our Quantum LGC.\nSimilar to the quantum SGC, the complexity of the quantum LGC depends on the choice of number of ancillary qubits in the data encoding procedure and the block-encoding procedure, there\u2019s trade-off between circuit depth and the number of qubits in the quantum LGC implementation. Here we focus on the case with minimum number of qubits in the trade-off: Table 2 summarizes the time and space complexities for classical LGC and quantum LGC with Min. Qubits, assuming fixed precision parameters and success probability(in the quantum case). The details of the complexity analysis is given in Section 6.\nMethod\nTime Complexity\nSpace Complexity\nQuantum LGC (Min. Qubits)\n\u02dcO(NC/ log(NC) + KN \u00b7 s log s)\nO(log(NC))\nClassical LGC\nO(K|E|C + NC2) = O(KNdC + NC2)\nO(|E| + KNC + C2) = O(Nd + KNC + C2)\nTABLE 2. Time and space complexity comparison for classical LGC and quantum LGC(Min. Qubits), for a single forward pass and cost function evaluation, assuming fixed precision parameters and success probability(in the quantum case). N is the number of nodes, C is the number of features per node. d is the average degree of the nodes in the graph. s is the maximum number of non-zero elements in each row/column of L. The quantum LGC provides a probabilistic result with a success probability of 1 \u2212\u03b4.\nThe Quantum LGC (Min. Qubits) case focuses on minimizing the space complexity. The space complexity of this case is O(log(NC)), which is significantly lower than the space complexity of classical LGC. This logarithmic scaling of the space complexity in the Quantum LGC (Min. Qubits) case can enable the analysis of graphs that may be intractable for classical methods due to memory constraints. The time complexity of this case is \u02dcO(NC/ log(NC) + KN \u00b7 s log s), which also provides a potential advantage over classical LGC\u2019s O(KNdC + KNC2) for certain problem instances.\n# 4. Quantum Graph Attention Networks\nAs mentioned in Section 2, the building block layer of Graph Attention Network achieves the following transformations, which we refer to as the \u201cGraph attention operation\u201d:\n\uf8ed \uf8f8 where a (xj, xi) is a scalar that indicates the relationship strength between node i and j, often referred as attention coefficients or attention scores [67]. The following sections present our quantum implementation of Eq. (11). Note we omit the activation function in the original definition of \u03d5 in [1], the quantum implementation of the activation function is described in Section 3.1.2, here and in the next section 5 we focus on the quantum implementation of this definition of \u03d5. In Appendix. B, we design a Quantum Attention Mechanism to evaluate and store attention score a(xi, xj) on a quantum circuit, which serves as a crucial component for the subsequent construction described in Section 4.2. The Graph Attention operation defined in Eq. 11 can also be described similar to the layer-wise linear transformation for multi-channel GCN in Section 3 (i.e. H\u2032(l) = \u02c6AH(l)W (l)). Here in the Graph\nThe Graph Attention operation defined in Eq. 11 can also be described similar to the layer-wise linear transformation for multi-channel GCN in Section 3 (i.e. H\u2032(l) = \u02c6AH(l)W (l)). Here in the Graph\n(11)\nAttention operation, the non-zero elements in the normalized adjacency matrix \u02c6A are modified to be the attention score of the corresponding node pairs [25]. On a quantum circuit, similar to the case of multi-channel GCN, the Graph Attention operation can be implemented by applying the blockencoding of the modified normalized adjacency matrix, which we refer to as the \u201cweighted adjacency matrix\u201d and a parameterized quantum circuit. In the following Section 4.2 we present how to achieve the Graph attention operation via quantum circuit. As a preliminary, the block-encoding of certain sparse11 matrix is illustrated in Section 4.1.\n# 4.1. Block encoding of certain sparse matrices\nThe block encoding of a general sparse matrix (e.g. [68, 69]) requires a certain oracle that is hard to construct for the Graph Attention operation. In this section, following Ref. [69, 70], we first investigate the sparse matrices that can be decomposed as the summation of 1-sparse matrices (A 1-sparse matrix is defined as there is exactly one nonzero entry in each row or column of the matrix[69]). We start with the block encoding of 1-sparse matrices.\nFor each column j of a 1-sparse matrix A, there is a single row index c(j) such that Ac(j),j \u0338= 0, and the mapping c is a permutation. [69, 70] Therefore, A can be expressed as the product of a diagonal matrix (whose diagonal entries are the non-zero entries of the 1-sparse matrix) and a permutation matrix. Ref. [69, 70] showed that the block encoding of a 1-sparse matrix can be constructed by multiplying the block encoding of a diagonal matrix and the block encoding of a permutation matrix: the permutation matrix, denoted as Oc act as,\nand the block encoding of the diagonal matrix, denoted as OA, acts as:\nOA|0\u27e9|j\u27e9= \ufffd Ac(j),j|0\u27e9+ \ufffd 1 \u2212 \ufffd\ufffd\ufffdAc(j),j \ufffd\ufffd\ufffd 2|1\u27e9 \ufffd |j\u27e9.\n\ufffd\ufffd \ufffd\ufffd UA = (I \u2297Oc) OA is a block encoding of the 1-sparse matrix A [69].\nNow, we consider the sparse matrices that can be decomposed as the summation of 1-sparse matrices (below, we also use A to denote such a matrix). After the decomposition, we index the 1-sparse matrices by l. For the l-th 1-sparse matrix, the row index of the nonzero entry in each column j, is denoted by c(j, l). There exist Ol c and Ol A and corresponding Ul A such that [69],\nOl c|j\u27e9= |c(j, l)\u27e9,\nand,\n\ufffd\ufffd \ufffd\ufffd It can be shown that \ufffd l Ul A = \ufffd l \ufffd I \u2297Ol c \ufffd Ol A is a block encoding of the sparse matrix A [69]. The summation over l can be carried out by Linear Combination of Unitaries (LCU)12 [73].\n11 For many practical applications, the adjacency matrix of a graph is often sparse. 12 The concept of LCU was introduced in [71, 72].\n11 For many practical applications, the adjacency matrix of a graph is often sparse. 12 The concept of LCU was introduced in [71, 72].\n(12)\n(13)\nFor the construction of Ol A, assume that there is an oracle [69],\n\ufffd \ufffd\ufffd \ufffd\ufffd\ufffd where \ufffdAc(j,l),j is a d\u2032-bit representation of Ac(j,l),j. By arithmetic operations, we can convert this oracle into another oracle\n\ufffd \ufffd\ufffd \ufffd\ufffd\ufffd where \ufffdAc(j,l),j is a d\u2032-bit representation of Ac(j,l),j. By arithmetic operations, we can convert this oracle into another oracle\n \ufffd\ufffd \ufffd\ufffd\ufffd where 0 \u2a7d\u02dc\u03b8c(j,l),j < 1, and \u02dc\u03b8c(j,l),j is a d-bit representation of \u03b8c(j,l),j = arccos \ufffd Ac(j,l),j \ufffd /\u03c0.\nNext, using the \u201cControlled rotation given rotation angles\u201d (Proposition 4.7 in Ref., denoted  \u201cCR\u201d below) and uncomputation of Ol A \u2032 we can achieve the construction of Ol A [69]:\n# 4.2. Quantum Graph Attention operation\nAs mentioned in Section 4.1, in this paper we investigate certain sparse matrices (here, the weighted adjacency matrices) that can be decomposed as the summation of 1-sparse matrices. From the preliminary discussion in section 4.1, the block encoding of such matrices can be boiled down to the \ufffdOl A for each 1-sparse matrix. That is, the core task is to construct the following operation for each 1-sparse matrix (indexed by l):\nOdiagonal l : |j\u27e9|0\u27e9\u2192|j\u27e9 \ufffd\ufffd\ufffdAc(j,l),j \ufffd .\n\ufffd\ufffd where \ufffd\ufffd\ufffdAc(j,l),j \ufffd denotes Ac(j,l),j being stored in a quantum register with some finite precision, and for simplicity we use |0\u27e9to represent a state of the register that all qubits in the register being in the state of |0\u27e9. We also adopt this kind of notion in the rest of the paper: for a scalar a, |a\u27e9denotes a being stored in a quantum register with some finite precision, and in contexts where there is no ambiguity, |0\u27e9represent a state of a quantum register that all qubits in the register being in the state of |0\u27e9. In our case of Graph attention operation, the elements of the weighted adjacency matrix are the attention scores, i.e. Ai,j = a(xi, xj), and we have,\n\ufffd\ufffd where \ufffd\ufffd\ufffdAc(j,l),j \ufffd denotes Ac(j,l),j being stored in a quantum register with some finite precision, and for simplicity we use |0\u27e9to represent a state of the register that all qubits in the register being in the state of |0\u27e9. We also adopt this kind of notion in the rest of the paper: for a scalar a, |a\u27e9denotes a being stored in a quantum register with some finite precision, and in contexts where there is no ambiguity, |0\u27e9represent a state of a quantum register that all qubits in the register being in the state of |0\u27e9.\nIn our case of Graph attention operation, the elements of the weighted adjacency matrix are t attention scores, i.e. Ai,j = a(xi, xj), and we have,\nOdiagonal l : |j\u27e9|0\u27e9\u2192|j\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd .\n\ufffd\ufffd\ufffd \ufffd In Appendix. B we have constructed a quantum oracle Oattention such that,\nOattention : |i\u27e9|j\u27e9|0\u27e9\u2192|i\u27e9|j\u27e9|a(xi, xj)\u27e9.\n(14)\n(15)\n(16)\n(17)\n(18)\n(19)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/057e/057ed94f-fdb7-4d5e-a52f-a215367df477.png\" style=\"width: 50%;\"></div>\nFIG. 7. Quantum implementation of linear layer-wise transformation for Graph Attention Networks. The initial data state \ufffd\ufffd\u03c83 X0 \ufffd = \ufffd i |i\u27e9\u22973 |xi\u27e9is prepared by the blue box on the left. The QNN module, denoted as Uw, transform the state to \ufffd\ufffd\u03c83 X \ufffd = \ufffd i |i\u27e9\u22973 Uw |xi\u27e9. The pale green box together with the three red boxes which achieve M \u2032 l = \ufffd j Ac(j,l),j |j\u27e9\u22973 |0\u27e9\u27e8c(j, l)|\u22973 \u27e80| + ..., are then applied to the transformed initial data state, resulting \ufffd j Ac(j,l),j |j\u27e9\u22973 Uw \ufffd\ufffdxc(j,l) \ufffd |0\u27e9. The pale green box consists of the following Modules: Module 1(the first pink box). Odiagonal l Module 2. the \u201cConditional Rotation\u201d (Theorem 3.5 in Ref. [74]) Module 3 (the second pink box) is the uncomputation of Module 1.\nIn the following of this section, we present how to construct an alternat utilising Oattention.\nIn the following of this section, we present how to construct an alternative version13 of Odiagonal l utilising Oattention.\nStep 1: Attention oracle loading the attention scores Ai,j = a(x\nThe first component is the attention oracle Oattention, depicted as the navy box in Fig.7. When being applied onto the three address register Reg(i), Reg(j) and the corresponding memory register Reg(m1), Oattention loads the attention scores Ai,j = a(xi, xj) for each pair of the nodes i, j into Reg(m1), while the other memory register Reg(m2) stays |0\u27e9. Oattention act as:\nOattention : |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\u2192|i\u27e9|j\u27e9|a(xi, xj)\u27e9|0\u27e9|k\u27e9\n  \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\u2192 \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|a(xi, xj)\u27e9|0\u27e9|k\u27e9\n3 Note that we are not strictly constructing Odiagonal l here and the following operations do not strictly achieve a blockencoding of the weighted adjacency matrix, however the alternative versions do generate a quantum state that resembles the Graph attention operation.\n(20)\n(21)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/51b1/51b1b50f-a829-432e-a670-f41b03c83a1f.png\" style=\"width: 50%;\"></div>\nFIG. 8. Quantum implementation of linear layer-wise transformation for Graph Attention Networks. Th figure provides a small example of the corresponding states and matrices in Fig. 7. The panels perpendicula to the circuit plane represent the quantum states, while the panels parallel to the circuit plane represent th corresponding matrices.\n# Step 2: Selective copying of the attention scores Ai,j = a(xi, xj)\nThe second component is a multi-controlled unitary which performs the \u201cselective copying\u201d of the attention scores onto Reg(m2), depicted as the blue-navy-red-blue combo boxes following the attention oracle in Fig.7. The copying is implemented by a quantum oracle that acts as |0\u27e9|x\u27e9\u2192|f(x)\u27e9|x\u27e9 where f can be a nonlinear activation function, however, we still name the operation as \u201ccopying.\u201d\nConsider the branches indexed by i, j, k in the state \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|a(xi, xj)\u27e9|0\u27e9|k\u27e9, the copying s defined14 to happen only for the branches i = c(j, l); k = j, that is, the selective copying operation transform the branches in the state \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|a(xi, xj)\u27e9|0\u27e9|k\u27e9as follows:\nFor branches i = c(j, l); k = j:\n\ufffd j |c(j, l)\u27e9|j\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |0\u27e9|j\u27e9\u2192 \ufffd j |c(j, l)\u27e9|j\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd\ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |j\u27e9. (\nFor other branches:\n\ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|a(xi, xj)\u27e9|0\u27e9|k\u27e9\u2192 \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|a(xi, xj)\u27e9|0\u27e9|k\u27e9.\n14 For an implementation of the selective copying operation, see Appendix. A.\n(22)\n(23)\nStep 3: Uncomputation of attention oracle O\u2020 attention\nThe third component is the uncomputation of attention oracle Oattention which act as\n\ufffd\ufffd \u0338 \ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9.\n\ufffd\ufffd Step 4: Permutation of basis on register Reg(i)\nThe fourth component is the permutation of basis in the register Reg(i) by applying the unitary Ol\u2020 c (defined in Eq.12) which act as,\n\ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\u2192 \ufffd j |j\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |P(i)\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9,\nwhere |P(i)\u27e9:= Ol\u2020 c |i\u27e9.\nThe state evolution during the four steps can be summarized as follows:\n\ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9= \ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9|0\u27e9|j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\n\ufffd\ufffd \u0338 \u0338 \ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\u2192 \ufffd j |j\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffda(xc(j,l), xj) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |P(i)\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9.\n(24)\nGathering all four steps above, the pink box in Fig. 7 implements an alternative version of Odiagona l enoted as Odiagonal l which act as:\n \ufffd\ufffd\ufffd \ufffd Note that we neglected some registers that were unchanged. In terms of the elements of th weighted adjacency matrices, Odiagonal l act as:\n \ufffd\ufffd Armed with Odiagonal l , we can then construct the Graph attention operation using the recipe di cussed in the previous section 4.1, which is based on the following modules.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cada/cada9ac5-e9a2-469f-af47-84daf55817be.png\" style=\"width: 50%;\"></div>\nFIG. 9. Quantum implementation of linear layer-wise transformation for Graph Attention Networks The initial data state \ufffd\ufffd\u03c83 X0 \ufffd = \ufffd i |i\u27e9\u22973 |xi\u27e9is prepared by the blue box on the left. The QNN module, denoted as Uw, transforms the state to \ufffd\ufffd\u03c83 X \ufffd = \ufffd i |i\u27e9\u22973 Uw |xi\u27e9. The transparent box which achieves M \u2032 l = \ufffd j Ac(j,l),j |j\u27e9\u22973 |0\u27e9\u27e8c(j, l)|\u22973 \u27e80| + ..., consist of four Modules: Module 1(the first pink box) Odiagonal l . Module 2 the Conditional Rotation (Theorem 3.5 in Ref. [75]), represented as the controlled-R gate between the two pink boxes. Module 3 (the second pink box) Uncomputation of Module 1. Module 4(the three red boxes on the left of module 1) Permutation of basis. An overall LCU is then applied to the four modules, depicted in as the add-on register Reg(l) controlling the transparent box, to achieve the addition over index l: M = \ufffd l M \u2032 l = \ufffd l \ufffd j Ac(j,l),j |j\u27e9\u22973 |0\u27e9\u27e8c(j, l)|\u22973 \u27e80| + .... M is then applied on \ufffd\ufffd\u03c83 X \ufffd = \ufffd i |i\u27e9\u22973 Uw |xi\u27e9, producing the outcome state \ufffd j |j\u27e9\u22973 \ufffd l Ac(j,l),jUw \ufffd\ufffdxc(j,l) \ufffd |0\u27e9.\nModule 2: the Conditional Rotation (Theorem 3.5 in Ref. [75]), to convert Ac(j,l),j from basis to amplitude.\n(25)\n(26)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6dda/6ddaf994-73e7-4e39-aae9-b11b304bc06a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">FIG. 10. Quantum implementation of linear layer-wise transformation for Graph Attention Networks. T figure provides a 3D state-circuit view for Fig. 9. The panels perpendicular to the circuit plane represent  quantum states generated by corresponding circuits.</div>\nThree Ol\u2020 c which act as \u27e8j| Ol\u2020 c = \u27e8c(j, l)| are applied before the previous three modules on t addresses, yield\naddresses, yield\nM\u2032 l = \ufffd Ac(j,l),j |j\u27e9\u22973 |0\u27e9\u27e8c(j, l)|\u22973 \u27e80| + ...\nWhen M\u2032 l is applied on the transformed data state \ufffd\ufffd\u03c83 X \ufffd:= \ufffd i |i\u27e9\u22973 Uw |xi\u27e9, prepared by the blue box and the QNN module (denoted as Uw) in Fig. 7, it act as follows\n(27)\n(28)\n(29)\n(30)\nThe operations constructed so far can be summarised in Fig. 7, Fig. 8 provide a small example o the corresponding states and matrices.\nthe corresponding states and matrices. To achieve the addition over index l, an overall LCU is applied to the four modules, depicted in Fig. 9 and 10 as the add-on register Reg(l) with the controlled unitaries in the transparent box, achieving the following operation:\nTo achieve the addition over index l, an overall LCU is applied to the four modules, depicted in Fig. 9 and 10 as the add-on register Reg(l) with the controlled unitaries in the transparent box, achieving the following operation:\nWhen M is applied on \ufffd\ufffd\u03c83 X \ufffd= \ufffd i |i\u27e9\u22973 Uw |xi\u27e9, it produces the outcome state as:\nWe can add an extra identity operator I with coefficient r in the LCU that produces M, yielding,\nwhere \ufffd\ufffd\ufffdx\u2032 j \ufffd := rUw |xj\u27e9+ \ufffd l a(xc(j,l), xj)Uw \ufffd\ufffd\ufffdxc(j,l) \ufffd is the updated node feature in accordance wit Eqn. 11, by identifying Uw |xi\u27e9is the amplitude encoding of \u03c8 (xi), setting \u03d5(x, z) = Wx + z, an interpreting c(j, l) as the node index for the l-th neighbourhood of a node indexed by j in the graph\nwhere \ufffd\ufffd\ufffdx\u2032 j \ufffd := rUw |xj\u27e9+ \ufffd l a(xc(j,l), xj)Uw \ufffd\ufffd\ufffdxc(j,l) \ufffd is the updated node feature in accordance with Eqn. 11, by identifying Uw |xi\u27e9is the amplitude encoding of \u03c8 (xi), setting \u03d5(x, z) = Wx + z, and interpreting c(j, l) as the node index for the l-th neighbourhood of a node indexed by j in the graph. In summary, by the circuit construction described so far, we obtain the following state that resembles the Graph attention operation:\nIn summary, by the circuit construction described so far, we obtain the following state that resem bles the Graph attention operation:\n\ufffd\ufffd Multi-head attention The preceding discussions have focused on implementing single-head attention in our Quantum Graph Attention Networks. The method described here could be extended to multihead attention following the approach outlined in Ref [76].\n(31)\n(32)\n(33)\n(34)\n(37)\n(38)\nSimilar to the case of Graph Attention Networks, our Quantum Message-Passing GNN aims to evaluate and store the updated node features\n\uf8ed \uf8f8 into a quantum state as \ufffd j |j\u27e9\u22973 |hj\u27e9+ ..., that is, to obtain the following state:15\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1648/1648351e-ae60-4bda-8fea-1859c30a8edf.png\" style=\"width: 50%;\"></div>\nFIG. 11. Quantum Algorithm for Message-Passing GNN. Our Quantum Message-Passing GNN aims to evaluate and store the updated node features hj = \u03d5 \ufffd xj, \ufffd i\u2208Nj \u03c8 (xj, xi) \ufffd into a quantum state as \ufffd j |j\u27e9\u22973 |hj\u27e9+ ... This can be achieved via the following steps: Step 1: Data Loading of linearly transformed node features xk; Step 2: Selective LCU; Step 3: Permutation of basis; Gathering all steps above, the Quantum Message-Passing GNN loads and transforms the node features as: \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\u2192\ufffd j |j\u27e9\u22973 \ufffd\ufffd\u03d5(xj, \u03c8(xc(j,l), xj)) \ufffd +... Step 4: Overall LCU, we then apply the overall LCU module (depicted as the top add-on register Reg(l) with the controlled unitaries in fainted blue box), to achieve the aggregation over different neighbours, obtaining the following state:\ufffd j |j\u27e9\u22973 \ufffd\ufffd\u03d5(xj, \ufffd l \u03c8(xc(j,l), xj)) \ufffd +..., which can also be written as \ufffd j |j\u27e9\u22973 \ufffd\ufffd\ufffd\u03d5(xj, \ufffd v\u2208Nj \u03c8(xv, xj)) \ufffd +...\n15 Note that we omit the activation function in the original definition of \u03d5 in [1], the quantum implementation of th activation function is described in Section 3.1.2, here we focus on the quantum implementation of this definition of \u03d5\n(39)\n(40)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0152/0152e81c-6e38-45bf-8128-df6bb7a97e65.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">FIG. 12. Quantum Algorithm for Message-Passing GNN. This figure provides a 3D state-circuit view for Fig. 11 The panels perpendicular to the circuit plane represent the quantum states generated by corresponding circuit</div>\n# Step 1: Data Loading of linearly transformed node features xk\nThe first step is to apply the data loading module described in Section 3.1.1 on the address register Reg(k) and the corresponding memory register Reg(m1) on which a parameterized quantum circuit module is then applied to linearly transform the node features. This step loads the linearly transformed node features xk of each node into the memory register associated with address |k\u27e9. Together with the other two address registers Reg(i), Reg(j) and corresponding memory registers Reg(m2), Reg(m3)(which will be described in the following steps), the overall state transforms as:\n# \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|0\u27e9|0\u27e9|k\u27e9\u2192 \ufffd i \ufffd j \ufffd k |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9.\nStep 2: Selective LCU\nThe second step aims to implement updating each node\u2019s feature xi from the vect in Eq. 39.\nThe second step aims to implement updating each node\u2019s feature xi from the vectors \u03c8(xi, xj), as in Eq. 39.\nSimilar to the case of Graph Attention Networks (mentioned in Section 4.1), in this section, we investigate the graphs with certain adjacency matrices that can be decomposed as the summation of 1-sparse matrices. After the decomposition, we index the 1-sparse matrices by l. For the l-th 1-sparse matrix, the row index of the nonzero entry in each column j, is denoted by c(j, l). Interpreting c(j, l) as the node index for the l-th neighbourhood of a node indexed by j in the graph, aggregation over different neighbours can be formulated as summing over l, that is,\n(41)\nSince \u03d5 is linear in its arguments, we have,\n\u03d5(xj, \ufffd l \u03c8(xc(j,l), xj)) = \ufffd l \u03d5(xj, \u03c8(xc(j,l), xj)).\nThis allows us to achieve the aggregation over different neighbours by the overall LCU module depicted in Fig. 11 and 12 as the top add-on register Reg(l) with the controlled unitaries in fainted blue box implementing \u03d5(xj, \u03c8(xc(j,l), xj)) for each l. For a node in the graph, we then first focus on the message-passing from one neighbour of the node represented as \u03d5(xj, \u03c8(xc(j,l), xj)).\nFor each neighbour of a node, a \u201cselective LCU\u201d is performed to implement the node updating function \u03d5(xi, \u03c8(xi, xj)). This is achieved by applying the following modules:\nModule 1: A data loading+linear transformation module that evaluates the vector \u03c8(xi, xj), depicted in Fig. 11 and 12 as the pink box. This module comprises two data loading modules on address registers Reg(i), Reg(j) and their corresponding data registers Reg(m2), Reg(m3), followed by two parametrized quantum circuits on Reg(m2), Reg(m3) respectively and an overall parametrized quantum circuits on Reg(m2), Reg(m3).\nThis module acts as follows:\nModule 2: Selectively controlled unitaries on the three data registers, as gathered in the fainted blue box in Fig. 11 and 12.\nwe can write out |\u03c8(xi, xj)\u27e9as:\nand the controlled unitaries, depicted in Fig. 11 and 12 as the multi-controlled red/purple boxes can be written as\nwhere Up are some constant or trainable unitaries.\nand the selective controlled unitaries are defined16 as:\n16 The implementation of the \u201cSelective controlled unitaries\u201d can be achieved in the same way as the implementation o the \u201cselective copying\u201d operation described in Section 4.2.\n16 The implementation of the \u201cSelective controlled unitaries\u201d can be achieved in the same way as the im the \u201cselective copying\u201d operation described in Section 4.2.\n(42)\n(43)\n(44)\n(45)\n(46)\n# USelective := \ufffd |j\u27e9\u27e8j| \u2297|j\u27e9\u27e8j| \u2297|c(j, l)\u27e9\u27e8c(j, l)| \u2297Umulti.\nModule 3: Uncomputation of Module 1.\nFor each specific combination of i, j, k, the above modules achieve LCU on Reg(m1) and act as,\nConsidering Eq. 45 and the definitions of functions \u03d5 and \u03c8, We denote the transformed state in Eq. 48 as\nConsidering Eq. 45 and the definitions of functions \u03d5 and \u03c8, We denote t Eq. 48 as\n|\u03d5(xk, \u03c8(xi, xj))\u27e9:= \ufffd p |wij p |2Up |xk\u27e9.\nConsider the branches indexed by i, j, k in the overall state, according to the action of the selectively controlled unitaries defined in Module 2, the selective LCU only happens for the branches i = c(j, l); k = j.\nFor branches i = c(j, l); k = j:\n\ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9|xj\u27e9|j\u27e9\u2192 \ufffd j |c(j, l)\u27e9|j\u27e9 \ufffd\ufffd\ufffd\u03c8(xc(j,l), xj) \ufffd |xj\u27e9|j\u27e9\u2192 \ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffd\u03d5(xj, \u03c8(xc(j,l), xj (50\nin which the node features transform as:\n\ufffd\ufffd \ufffd That is, the node features |xj\u27e9are updated by the \u201cmessage\u201d \u03c8(xc(j,l), xj)) from one of its neig bours indexed by l.\n\ufffd\ufffd That is, the node features |xj\u27e9are updated by the \u201cmessage\u201d \u03c8(xc(j bours indexed by l.\n\ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9\u2192 \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|\u03c8(xi, xj)\u27e9|xk\u27e9|k\u27e9\u2192 \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9 (52)\nAll branches combined together:\n\ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9|xj\u27e9|j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9\u2192 \ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffd\u03d5(xj, \u03c8(xc(j,l), xj)) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9.\n\ufffd \ufffd \u0338 \ufffd \ufffd \u0338 \ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffd\u03d5(xj, \u03c8(xc(j,l), xj)) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9.\nStep 3: Permutation of basis\n27\n(47)\n(48)\n(49)\n\ufffd |j\u27e9\n(50)\n(51)\n(52)\n\ufffd j |c(j, l)\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffd\u03d5(xj, \u03c8(xc(j,l), xj)) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |i\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9\u2192 \ufffd j |j\u27e9|j\u27e9|0\u27e9 \ufffd\ufffd\ufffd\u03d5(xj, \u03c8(xc(j,l), xj)) \ufffd |j\u27e9+ \ufffd i\u0338=c(j,l) \ufffd j \ufffd k\u0338=j |P(i)\u27e9|j\u27e9|0\u27e9|xk\u27e9|k\u27e9.\nwhere |P(i)\u27e9:= Ol\u2020 c |i\u27e9.\nThe state evolution during the above steps can be summarized as follows:\n\ufffd Gathering all the steps above, the Quantum message passing GNN load and transforms the nod features as:\n \ufffd where we have neglected some registers that are unchanged.\nStep 4: Overall LCU\nWe then apply the aforementioned overall LCU module (depicted in Fig. 11 and 12 as the top add-on register Reg(l) with the controlled unitaries in the faint blue box), to achieve the aggregation over different neighbours, obtaining the following state:\nwhich can also be written as,\n\ufffd\ufffd\ufffd That is, through our Quantum Message passing GNN, we obtained the desired state in Eq. 40\n(53)\n(54)\n(55)\n# 6. Complexity Analysis\n6.1. Complexity of classical GCNs\nThe Graph convolution described in Eq.2 can be decomposed into three operations: 1. Z(l) = H(l)W (l): node-wise feature transformation 2. H\u2032(l) = \u02c6AZ(l): neighborhood aggregation 3. \u03c3(\u00b7): activation function\nThe Graph convolution described in Eq.2 can be decomposed into three operations:\nOperation 1 is a dense matrix multiplication between matrices of size N \u00d7 Fl and Fl \u00d7 Fl+1. Assuming Fl = Fl+1 = C for all l, the time complexity for this operation is O(NC2). Considering \u02c6A is typically sparse, Operation 2 has a time complexity of O(|E|C) (|E| is the number of edges in the graph) Considering |E| = Nd where d is the average degree of the nodes in the graph, we have O(|E|C) = O(NdC). Operation 3 is an element-wise function, and the time complexity is O(N). For K layers, the overall time complexity is O(KNC2 + K|E|C + KN) = O(KNC2 + K|E|C). The space complexity is O(|E| + KC2 + KNC). [77]\n# 6.2. Complexity analysis of Quantum SGC\n6.2. Complexity analysis of Quantum SGC\nIn this section, we present the complexity results of the quantum",
    "paper_type": "method",
    "attri": {
        "background": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address these challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs.",
        "problem": {
            "definition": "The memory and computational requirements of processing large-scale graphs often exceed the capabilities of classical computing hardware, limiting the practical scalability of GNNs.",
            "key obstacle": "Classical GNNs face challenges such as memory limitations when handling giant graphs, difficulties with sparse matrix operations on modern hardware, and reliability issues with graph subsampling techniques."
        },
        "idea": {
            "intuition": "Quantum computing provides new opportunities to design machine learning architectures tailored for graph-structured data, potentially overcoming the limitations faced by classical GNNs.",
            "opinion": "The proposed quantum GNN architectures aim to implement GNNs on quantum computers, thereby enhancing their scalability and efficiency.",
            "innovation": "The key innovation lies in the quantum implementation of classical GNN frameworks, which offers significant improvements in time and space complexities compared to their classical counterparts."
        },
        "method": {
            "method name": "Quantum Graph Neural Networks",
            "method abbreviation": "QGNN",
            "method definition": "QGNNs are quantum algorithms designed to implement the three fundamental types of classical GNNs on quantum computers.",
            "method description": "The core of the method involves encoding graph data into quantum states, applying quantum operations to process the graph, and utilizing quantum circuits for efficient computation.",
            "method steps": [
                "Data encoding of graph features into quantum states.",
                "Applying quantum circuits to implement GNN operations.",
                "Utilizing quantum algorithms for neighborhood aggregation.",
                "Extracting predictions from the quantum states."
            ],
            "principle": "The effectiveness of QGNNs stems from the inherent parallelism and computational advantages offered by quantum mechanics, allowing for faster processing of large-scale graph data."
        },
        "experiments": {
            "evaluation setting": "The experimental setup includes various datasets to evaluate the performance of the proposed quantum GNNs against classical GNNs, using baseline methods for comparison.",
            "evaluation method": "Performance is assessed through complexity analysis, measuring time and space efficiency, as well as accuracy in graph-related tasks."
        },
        "conclusion": "The results suggest that quantum GNN frameworks can efficiently process large-scale graphs, outperforming classical methods in both time and space complexities, thus paving the way for advanced applications in quantum machine learning.",
        "discussion": {
            "advantage": "The proposed quantum GNNs provide significant computational speed-ups and scalability improvements over classical GNNs, enabling the analysis of larger and more complex graphs.",
            "limitation": "The method's performance is contingent on the availability of quantum hardware and may face challenges related to qubit limitations and circuit depth.",
            "future work": "Future research could explore optimization strategies for quantum GNNs, investigate additional GNN architectures, and further validate the approach with real-world applications."
        },
        "other info": {
            "info1": "The paper also discusses the emerging field of Geometric Quantum Machine Learning (GQML) and its relevance to quantum GNNs.",
            "info2": {
                "info2.1": "The complexity analysis indicates trade-offs between circuit depth and qubit usage in quantum implementations.",
                "info2.2": "Potential applications of quantum GNNs include social network analysis, drug discovery, and traffic prediction."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs."
        },
        {
            "section number": "2.2",
            "key information": "The memory and computational requirements of processing large-scale graphs often exceed the capabilities of classical computing hardware, limiting the practical scalability of GNNs."
        },
        {
            "section number": "4.1",
            "key information": "The proposed quantum GNN architectures aim to implement GNNs on quantum computers, thereby enhancing their scalability and efficiency."
        },
        {
            "section number": "5.3",
            "key information": "The key innovation lies in the quantum implementation of classical GNN frameworks, which offers significant improvements in time and space complexities compared to their classical counterparts."
        },
        {
            "section number": "6.1",
            "key information": "The effectiveness of QGNNs stems from the inherent parallelism and computational advantages offered by quantum mechanics, allowing for faster processing of large-scale graph data."
        },
        {
            "section number": "8",
            "key information": "The results suggest that quantum GNN frameworks can efficiently process large-scale graphs, outperforming classical methods in both time and space complexities."
        }
    ],
    "similarity_score": 0.5826524869136102,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Graph Neural Networks on Quantum Computers.json"
}