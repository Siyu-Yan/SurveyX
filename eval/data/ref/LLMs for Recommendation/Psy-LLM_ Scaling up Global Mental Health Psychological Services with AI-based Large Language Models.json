{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.11991",
    "title": "Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models",
    "abstract": "The demand for psychological counselling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counselling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based assistive tool leveraging Large Language Models (LLMs) for question-answering in psychological consultation settings to ease the demand for mental health professions. Our framework combines pre-trained LLMs with real-world professional Q\\&A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and extrinsic evaluation metrics, with human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions. This article discusses the potential and limitations of using large language models to enhance mental health support through AI technologies.",
    "bib_name": "lai2023psyllmscalingglobalmental",
    "md_text": "# A B S T R AC T\nThe demand for psychological counselling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counselling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based assistive tool leveraging Large Language Models (LLMs) for question-answering in psychological consultation settings to ease the demand for mental health professions. Our framework combines pre-trained LLMs with real-world professional Q&A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and extrinsic evaluation metrics, with human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions. This article discusses the potential and limitations of using large language models to enhance mental health support through AI technologies.\n# 1 Introduction\nThe field of AI utilising dialogue technology has witnessed significant growth, particularly in the domain of automatic chatbots and ticket support systems (Handoyo, Arfan, Soetrisno, et al. 2018). This application of dialogue technology has emerged as a cutting-edge and increasingly popular approach in the realm of AI-powered support systems. With changing global dynamics, the severity of the ongoing pandemic, and an upsurge in psychological challenges the public faces, the mental well-being of young individuals, in particular, is a cause for concern. The pressures of urbanisation and the internet have led to various psychological issues (Trivedi, Sareen, and Dhyani 2008), including depression, procrastination, anxiety, obsessive-compulsive disorder, and social phobia (Tian, Li, Tian, et al. 2020), which have become prevalent ailments of our time. Psychological counselling involves the utilisation of psychological methods to provide assistance to individuals experiencing difficulties in psychological adaptation and seeking solutions. The demand for psychological counselling has witnessed a significant surge in recent years (Chen and Lucock 2022), while the availability of professional psychological consultants remains insufficient. The profession of psychological consulting imposes high standards and qualifications. For instance, registered psychologists within Psychological Associations require students to possess a master\u2019s degree in psychology-related disciplines, undergo a minimum of 150 hours of direct counselling, and receive\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\nface-to-face supervision by registered supervisors for no less than 100 hours (Gay and Swank 2021). Additionally, the burnout rate among mental health professionals further exacerbates this shortage (Joshi and Sharma 2020). In 2020, the global outbreak of COVID-19 exacerbated the need for timely and professional psychological counselling due to the tremendous stress it imposed on society (Kontoangelos, Economou, and Papageorgiou 2020). Consequently, online psychological counselling through the internet has progressively become the dominant mode of delivering counselling services (Yurayat and Tuklang 2023). AI-based assistive psychological support not only addresses the severe supply-demand gap in the consulting industry but also enhances the responsiveness of online psychological counselling services, thereby promoting the implementation of mental health strategies. Such an assistive tool serves to ease the shortage of mental health support when no human counsellors are available to help. In light of these circumstances, our team is determined to develop an assistive mental health consulting framework to serve as a constant source of support. Creating an AI-powered framework can allow users to engage with it comfortably, given its non-human identity, thereby reducing feelings of shame among users (Prochaska, Vogel, Chieng, et al. 2021). In particular, with the absence of available psychological support, our framework serves as the second-best approach to providing timely support to patients. Amid the challenges posed by the pandemic, online psychological counselling has proven instrumental and has gradually become the predominant form of counselling. However, the growing disparity between supply and demand within our society\u2019s psychological consultation industry is a pressing concern. The application of AI technology to mental health and psychological counselling is an emerging and promising field. Conversation frameworks, chatbots, and virtual agents are computer programs that simulate human conversation (Deryugina 2010). They can engage in natural and effective interactions with individuals, providing them with emotional experiences through the incorporation of emotional and human-like characteristics. In practical terms, dialogue frameworks hold significant potential for supporting the demand in online consultations and addressing supply-demand imbalances. In this study, we propose an AI-based Psychological Support with Large Language Models (Psy-LLM) framework designed for question-answering, with the purpose of providing online consultation services to alleviate the demand for mental health professionals during pandemics and beyond. Psy-LLM is an online psychological consultation model pre-trained with Large Language Models (LLMs) and further trained with Q&A from professional psychologists and large-scale crawled psychological articles. The framework can give professional answers to users\u2019 requests for psychological support. In particular, Psy-LLM can provide mental health advice both as recommendations for health professionals and as standalone tools for patients when no human counsellors are available due to time constraints or staff shortages. Our model is built upon large-scale pre-training corpus models, specifically PanGu (Zeng, Ren, Su, et al. 2021) and WenZhong (Wang, Zhang, Zhang, Yang, et al. 2022). The PanGu model, developed by Huawei\u2019s Pengcheng Laboratory, and the WenZhong model, developed by the Idea Research Institute, served as the basis for our work. For data acquisition, we collected a substantial number of Chinese psychological articles from public websites. Additionally, we obtained permission from the Artificial Intelligence Research Institute of Tsinghua University to utilise the PsyQA dataset, which comprises many question-answer pairs related to psychological counselling. Each answer in the dataset was reviewed and adjusted by professionals holding master\u2019s degrees or above in psychological counselling to ensure its quality. We fine-tuned the model in downstream tasks using the acquired dataset and PsyQA (Sun, Lin, Zheng, et al. 2021). As part of the evaluation process, we established a dedicated website and deployed the fine-tuned model on a server, allowing users to provide timely ratings. Based on the scoring results, we iteratively refined and re-fine-tuned the model. Our contribution includes proposing a framework for AI-based psychological consultation framework and an empirical study on its effectiveness. We have successfully developed a mental health consulting model that effectively provides clear and professional responses to users\u2019 psychological inquiries. Empirically, we have tested deploying the model on a server, and the model can respond to users within seconds. Our framework has the potential to offer a practical tool for professionals to efficiently screen and promptly respond to individuals in urgent need of mental support, thereby addressing and alleviating pressing demands within the healthcare industry.\n# 2 Related Works\nIn recent years, there has been increasing interest in utilising AI for tackling difficult problems in traditional domains like adopting AI in the construction industry (Regona, Yigitcanlar, Xia, and Li 2022), localisation in robotic applications (Lai 2022a), assistance systems in the service sector (Link, Dukino, Ganz, et al. 2020), financial forecast (Wang, Zhang, Zhang, Wang, et al. 2022), improving workflow in the oil and gas industry (Koroteev and Tekic 2021), planning and scheduling (Lai 2022b), monitoring ocean contamination (Xu, Lai, Jahan, et al. 2022), remote sensing for search and rescue (Lai 2023), and even used in the life cycle of material discovery (Li, Lim, Yang, et al. 2020). Health care industry has been adopting AI-based machine-learning techniques for classifying medical images (Castiglioni, Rundo, Codari, et al. 2021), guiding cancer diagnosis (Chugh, Kumar, and Singh 2021), as screening tools for diabetes (Hu,\net al. 2018). One area of research focuses on using conversational agents, also known as chatbots, for mental health support. Chatbots have the potential to provide accessible and cost-effective assistance to individuals in need. For example, Martinengo, Lum, and Car (2022) (Martinengo, Lum, and Car 2022) qualitatively analysed user-conversational agents and found that these types of chatbots can offer anonymous, empathetic, and non-judgemental interactions that align with face-to-face psychotherapy. Chatbots can utilise NLP techniques to engage users in therapeutic conversations and provide personalised support. Results showed promising outcomes, indicating the potential effectiveness of chatbots in delivering mental health interventions (Denecke, Abd-Alrazaq, and Househ 2021). Pre-trained language models have also gained attention in the field of mental health counselling. These models, such as GPT-3 (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et al. 2020a), provide a foundation for generating human-like responses to user queries. Wang, Shi, Yu, et al. (2023) (Wang, Shi, Yu, et al. 2023) explored the application of LLMs in providing mental health counselling. They found that LLMs demonstrated a certain level of understanding and empathy, providing responses that were perceived as helpful by users. However, limitations in controlling the model\u2019s output and ensuring ethical guidelines were highlighted. Furthermore, there is a growing body of research on using NLP techniques to analyse mental health-related text data (Gonzalez-Hernandez, Sarker, O\u2019Connor, and Savova 2017). Researchers have applied machine learning algorithms to detect mental health conditions (Abd Rahman, Omar, Noah, et al. 2020), predict suicidal ideation (Ji, Pan, Li, et al. 2020), and identify linguistic markers associated with psychological well-being (Akstinaite, Garrard, and Sadler-Smith 2022). For instance, De Choudhury, Gamon, Counts, and Horvitz (2013) (De Choudhury, Gamon, Counts, and Horvitz 2013) analysed social media data to predict depression among individuals. By extracting linguistic features and using machine learning classifiers, they achieved promising results in identifying individuals at risk of depression. Additionally, several studies have investigated the integration of modern technologies into existing mental health interventions. For instance, Lui, Marcus, and Barry (2017) (Lui, Marcus, and Barry 2017) investigates the use of mobile applications to support the delivery of psychotherapy. Shaikh and Mhetre (2022) (Shaikh and Mhetre 2022) developed a friendly AI-based chatbot using deep learning and artificial intelligence techniques. The chatbot aimed to help individuals with insomnia by addressing harmful feelings and increasing interactions with users as they experienced sadness and anxiety. In another line of research, chatbots have been extensively studied in the domain of customer service. Many companies have adopted chatbots to assist customers in making purchases and understanding products. These chatbots provide prompt replies, enhancing customer satisfaction (Tamrakar and Badholia 2022). Furthermore, advancements in language models such as BERT and GPT have influenced the development of conversational chatbots. Researchers have leveraged BERT-based questionanswering models to improve the accuracy and efficiency of chatbot responses(Kanodia, Ahmed, and Miao 2021). The GPT models, including GPT-2 and GPT-3, have introduced innovations such as zero-shot and few-shot learning, significantly expanding their capabilities in generating human-like text(Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et al. 2020b). However, limitations in generating coherent and contextual responses and the interpretability of the models have been identified. The model incorporated a 48-layer Transformer stack and achieved a parameter count of 1.5 billion, resulting in enhanced generalisation abilities(Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et al. 2020b). In summary, previous work in AI and NLP for mental health support has demonstrated the potential of chatbots, pretrained language models, and data analysis techniques. These approaches offer new avenues for delivering accessible and personalised mental health interventions. Nonetheless, further research is needed to address ethical, privacy, and reliability issues and optimise integrating AI technologies into existing counselling practices.\n# 3 Mental Health and Social Well-being in Overly Populated Cities\nThe availability of mental health professionals has always been a major problem in overpopulated cities such as China. The World Health Organisation has reported that the prevalence of depression in China exceeds 54 million people even before the onset of the COVID-19 pandemic (Organization et al. 2017). The situation has been exacerbated by the implementation of quarantine measures and social distancing, leading to a worsening condition (Gou, Wu, Xia, et al. 2022). Unfortunately, only a small fraction of the affected population receives adequate medical treatment, as there are only 2 psychiatrists per 100,000 people in China (Xiang, Ng, Yu, and Wang 2018). Consequently, there is a pressing need for a dynamic system that can assist patients effectively. Contemporary conversational chatbots have demonstrated their ability to emulate human-like conversations. Hence, it is imperative to develop a user-friendly AI-based chatbot specifically designed to address anxiety and\nThis project aims to construct a Chinese psychological dialogue model capable of comprehending the semantic meaning of a consultant\u2019s request and offering appropriate advice, particularly to address the shortage of mental health workers during demanding periods. The trained model will be integrated into a website, featuring a user interface (UI) that ensures ease of operation, thereby enhancing the efficiency of psychological counselling.\n# 3.1 Research Questions\nPsychology is an intricate and advanced discipline gaining increasing significance as society progresses. However, due to its high barriers to entry, resources for psychological counselling have long been scarce. In numerous cases, individuals face challenges accessing adequate mental health support (Cataldo, Chang, Mendoza, and Buchanan 2021). Furthermore, the high cost of psychological counselling often prevents many individuals from prioritising their mental well-being. This issue is particularly prominent in China, a country with a large population where psychological problems have been historically overlooked. China needs a robust foundation for psychological counselling, including a deficient knowledge base and limited data. Consequently, intelligent assistance in psychology must be improved in the Chinese context. Traditional psychological counselling primarily focuses on privacy and employs a one-on-one question-and-answer approach, inherently leading to inefficiencies. However, in today\u2019s high-pressure society, where mental health issues are pervasive, relying solely on scarce psychologists is arduous. Additionally, influenced by traditional culture, individuals often hesitate to acknowledge and address their psychological problems due to feelings of shame and perceiving such discussions as signs of weakness (Sandhu 1994). This reluctance is especially prominent when conversing with real humans, let alone seeking assistance from unfamiliar psychologists. Furthermore, with the advancement of modern Natural Language Processing (NLP) artificial intelligence models, there is a possibility to optimise the conventional and widely adopted question-and-answer model specifically for the field of psychology. When interacting with AI, people are more inclined to express their true thoughts and emotions without fear of prejudice and discrimination than real humans. However, the lack of verbal cues and continuous monitoring of patients\u2019 emotional progress is also a cause of concern in an online psychological consultation context, even when performed by human counsellors (Novella, Ng, and Samuolis 2022).\nThrough our project, we aim to make a meaningful contribution to the field of mental well-being. By leveraging our AI model and proposing a framework for an internet-accessible consultation, we intend to enhance the accessibility of mental health support, making it more affordable and providing an avenue for psychological question-and-answer interactions, particularly to address the shortage of human counsellors for mental health support. To achieve this goal, we gather professional counselling question-and-answer data and psychologically relevant knowledge data to construct a robust question-and-answer model specific to this domain. The success of our project relies on the utilisation of high-quality question-and-answer models. We plan to employ established Chinese pre-trained models with exceptional human-computer interaction and communication skills, characterised by fluent language, logical reasoning, and semantic understanding. However, these existing pre-trained models need more specialised psychological expertise and emotional understanding for counselling purposes. To address this limitation, we intend to integrate two models, namely the WenZhong model and the PanGu model, and evaluate their performance to determine the more suitable choice as our final model.\nSubsequently, it is imperative to make our model accessible to a broader audience. Leveraging the internet provides th most effective means to accomplish this objective. Together with the guidance of professional mental health experts our model can provide an additional venue for the general public to access mental health support by easing the stres and demand on mental health staff through an open and online platform.\n# 4 Psy-LLM Framework\nThe Psy-LLM framework aims to be an assistive mental health tool to support the workflow of professional counsellor particularly to support those who might be suffering from depression or anxiety.\n# 4.1 Target Audience and Model Usage\nThe contributing factor of Psy-LLM\u2014an AI-powered conversational model is two-fold. (1) Firstly, Psy-LLM is trained with a corpus of mental health supportive Q&A from mental health professionals, which enables Psy-LLM to be used as an assistive tool in an online consultation context. When users want to seek support from an online chat\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc0e/dc0e5d92-4b71-4d45-bea2-aaa9607a4534.png\" style=\"width: 50%;\"></div>\nPsy-LLM can provide suggestive answers to human counsellors to ease the staff\u2019s workload. Such an approach eases the entry barrier for newly trained mental health staff to provide useful and supportive comments for those in need. (2) Furthermore, in the absence of human counsellors (e.g. during off-hours or high-demand periods), Psy-LLM can also be a web frontend for users to interact with the system in an online consultation manner. Providing timely support to help-seeking individuals is especially important among suicidal individuals (Hom, Stanley, and Joiner Jr 2015). Therefore, an AI-powered online consultation might be the next best venue to respond to the absence of human counsellors.\n# 4.2 Large-scale Pre-trained LLMs\nOur project involves leveraging two large-scale pre-training models, namely WenZhong and PanGu to develop the question-answering language model. The utilisation of pre-training models offers several advantages, including: (1) Enhanced Language Representations: Pre-training on extensive unlabeled data enables the model to acquire more comprehensive language representations, which in turn can positively impact downstream tasks. (2) Improved Initialisation Parameters: Pre-training provides a superior initialisation point for the model, facilitating better generalisation performance on the target task and expediting convergence during training. (3) Effective Regularisation: Pre-training acts as an effective regularisation technique, mitigating the risk of overfitting when working with limited or small datasets. This is especially valuable as a randomly initialised deep model is susceptible to overfitting on such datasets. By harnessing the advantages of pre-training models, we aim to enhance the performance and robustness of our question-answering language model for psychological counselling.\n# 4.3 PanGu Model\nPanGu model is the first Chinese large-scale pre-training autoregressive language model with up to 200 billion (Zeng, Ren, Su, et al. 2021). In an autoregressive model, the process of generating sentences can be likened to a Markov chain, where the prediction of a token is dependent on the preceding tokens. The PanGu model, developed within the MindSpore framework, was trained using 2048 Ascend AI processors provided by Huawei and trained on a highquality corpus of 1.1TB. It was officially released in April 2021 and has achieved the top rank in the Chinese Language Comprehension Benchmark (CLUE), a widely recognised benchmark for Chinese language comprehension (Xu, Hu, Zhang, et al. 2020).\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8619/861953f4-faf8-403c-a7bc-6024553e6da0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The Query Layer in the PanGu Model</div>\nThe architecture of the PanGu model follows a similar structure to that of GPT-3, employing standard transformer layers Fig. 1. Each transformer layer comprises two sub-layers: multi-head attention (MHA) and a fully connected feed-forward network (FFN). The MHA involves three primary steps: calculating the similarity between the Query and Key, applying a softmax function to obtain attention scores, and multiplying the attention scores with the Value to obtain the attention output. The attention output then passes through a linear layer and undergoes softmax to generate the output embedding. The output embedding is combined with the FFN input through a residual module. The FFN consists of two linear layers with a GeLU activation function between each consecutive layer. The MHA and FFN utilise the pre-layer normalisation scheme, facilitating faster and easier training of the Transformer model. However, the last layer of the PanGu model deviates from the standard transformer layer structure. Instead, it incorporates a query layer designed to predict the next token, thereby enhancing the model\u2019s positional awareness and improving generation effectiveness. The query layer is a narrow yet powerful decoder that relies solely on position information. The structure of the query layer is illustrated in Fig. 2. The primary distinction between the query layer and the transformer layer lies in the query input of self-attention. While the inputs of Query, Key, and Value in other self-attention layers of the transformer remain standard, the query layer introduces a query embedding, which functions similarly to position embedding, as the query input for self-attention in the last layer. The PanGu model is available in four distinct variations, each characterised by different parameter sizes (table 1). These variations include PanGu 350M, PanGu 2.6B, PanGu 13B, and PanGu 200B (which is not open source). The parameter sizes differ across these models, reflecting their varying levels of complexity and capacity for language understanding and generation.\n# 4.4 WenZhong Model\nIn addition to the PanGu model, we have also incorporated the WenZhong model as one of the models used. The Wen Zhong model is a pre-trained model based on the GPT-2 architecture and trained on a large-scale Chinese corpus. Over the past few years, pre-trained models have become the foundation of cognitive intelligence, enabling advancements in natural language processing and computer vision algorithms.\nThe scale of pre-trained models has been rapidly increasing, growing by a factor of 10 each year, starting from the initial BERT model with 100 million parameters to the more recent GPT models with over 100 billion parameters Given the nature of our task, which requires a generation model with expertise in different professional domains, we have opted for the WenZhong model. Due to the model size of LLMs like GPT, computing resources are the limiting factor hindering further progress in the field. Universities and research institutions often need more computing power to train and utilise large-scale pre-trained models. This limitation impedes the broader implementation of AI technologies. Hence, we have adopted the WenZhong model, which is built upon a large pre-trained model trained on a Chinese corpus to avoid training the model from scratch. The WenZhong model series consists of one-way language models dominated by a Decoder structure and a series of powerful generation models. The WenZhong-3.5B model, with 3.5 billion parameters, employs 100G data and 256 A100 GPUs for 28 hours of training, exhibiting strong generation capabilities. Thus, the WenZhong model is highly powerful, featuring 30 decoder layers and billions of parameters. We have also utilised the WenZhong-GPT2-110M version in this project, comprising 110 million parameters and 12 layers. It is important to note that the WenZhong model has been pre-trained on the Wudao Corpus (300G version).\n# 4.5 Collecting Large Scale Dataset\nTwo types of data sources were obtained for this project. The first dataset, PsyQA (Sun, Lin, Zheng, et al. 2021), consisting of question and answer pairs, focuses on Chinese psychological health support. The authors authorised us to use this dataset containing 22,000 questions and 56,000 well-structured, lengthy answers. The PsyQA dataset includes numerous high-quality questions and answers related to psychological support, and it had already undergone basic cleaning before we received it. We selected a test set of 5,000 samples from this PsyQA dataset for our experiments.\n# 4.5.1 Data Crawling\nThe second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, whereas other users can also help to respond to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine-tuning training due to the model\u2019s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5,000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., \u201d\\n\\n\u201d. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2GB. The datasets from Zhihu and Yixinli were 500MB and 200MB, respectively. Overall, we spent approximately 70 hours on data collection. Although the data collected from the Internet was abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data. To address the time-consuming nature of web crawling, we implemented a distributed crawl technology that utilised idle computers connected to the Internet or other networks, effectively harnessing additional processing power (Thelwall 2001). Our approach involved obtaining sub-websites from the main website and saving them using custom crawling code. This code primarily relied on Python libraries such as \u201crequests\u201d, \u201cBeautifulSoup\u201d, and \u201cwebdriver\u201d. In addition,\n<div style=\"text-align: center;\">Table 1: The parametric size of the various settings in the PanGu model.</div>\nModel\nParameters\nLayers\nHidden Size\nHead\nSeq Length\nPanGu 350M\n350M\n24\n1024\n16\n1024\nPanGu 2.6B\n2.6B\n32\n2560\n40\n1024\nPanGu 13B\n13.1B\n40\n5120\n40\n1024\nPanGu 13B\n207.0B\n64\n16384\n128\n1024\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\n<div style=\"text-align: center;\">Table 2: dataset crawled from different platforms</div>\nPlatform\nData Size\nCrawling Time\nTianya\n2GB\n40h+\nZhihu\n500Mb\n20h+\nYixinli\n200Mb\n8h+\nwe employed dynamic web crawlers capable of collecting clickable elements, simulating user actions, comparing web page states, manipulating the DOM tree, and handling various user-invoked events (Li, Han, Liu, and Fang 2018). Unlike static page structures that cannot handle dynamic local refresh and asynchronous loading (Yao, Daling, Shi, et al. 2012), dynamic crawlers could extract data from behind search interfaces. The process of the dynamic crawler involved leveraging web developer tools within the browser to obtain XHR (XMLHttpRequest) information, which included requests containing headers, previews, and responses. We acquired relevant files by systematically searching through these layers of data and capturing network packets. After obtaining the sub-websites using static and dynamic crawling methods, we distributed them across multiple idle computers. Each computer was assigned specific sub-websites, and we collected project-related data using a combination of static and dynamic crawling techniques. Ultimately, we utilised eight computers for the crawling process, which took approximately 70 hours.\n# 4.5.2 Data Cleaning\nn line with the PanGu paper (Zeng, Ren, Su, et al. 2021), we adopted the original data cleaning method utilised in the PanGu model. Additionally, we incorporated some additional cleaning steps. The following are the cleaning steps we employed:\nFollowing the data cleaning process, the dataset could be directly inputted into the PanGu model. However, for training with the WenZhong model, the samples needed further processing. Specifically, all punctuations were removed, and the samples were tokenised to ensure a consistent length of 1000 tokens for compatibility with the WenZhong model.\n# 4.5.3 Data Analysis\nData analysis plays a crucial role in understanding the fundamental characteristics of textual data. In the context of the Chinese language, the exploratory data analysis (EDA) methods may be less diverse than those used for English. In this study, we primarily employed two common methods: word frequency analysis and sentence length analysis, to gain insights into the dataset. To analyse the distribution of characters in each sample, we referred to the character number data presented in table 3. By visualising this information using a box chart, we examined the range of character counts across the samples. Some samples are empty after the data cleaning, which we then prune from our dataset. Fig. 3 displays the distribution of sample lengths, indicating that the majority of samples fall within the range of 10,000 characters. Overall, these preliminary analyses allowed us to gain initial insights into the dataset and provided a foundation for further exploration and understanding of the textual data.\n<div style=\"text-align: center;\">Table 3: Data distribution of the length of each sample.</div>\nCount\nMean\nStd\nMin\n25%\n50%\n70%\nMax\n371,434\n5,343\n11,335\n0\n653\n1,835\n6,039\n454,611\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/58dd/58dda6d2-8e1a-49a9-9f6a-40464d7943aa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The number of characters in each sample</div>\nTo examine the word frequency in our dataset, we conducted an analysis after removing the stop words. The word cloud visualisation in Fig. 4 illustrates the most frequent words in the dataset. Notably, the prominent words observed include \u201cAnxiety\u201d, \u201cAppearance\u201d, \u201cMarriage\u201d, \u201cRelationship\u201d, \u201cFamily\u201d, and \u201cStressful\u201d. These words are highly relevant to the topic of mental health, indicating that our dataset is robust and aligns well with the focus of our task. The presence of these mental health-related terms further underscores the suitability of our data for addressing the objectives of our study. It suggests that our dataset encompasses significant content related to psychological aspects, allowing us to effectively explore and address relevant topics in the context of our research.\n# 4.6 Model Training\nModel Size We plan to use PanGu 350M to generate language considering the computational power, which contains 350 million parameters, 24 layers, 1024 hidden sizes and 16 attention heads. Besides, we also want to train the WenZhong-110M model that contains 12 layers and has 110M parameter.\nTraining Data We employ the 2.85GB psychology corpus data crawled from psychology platforms like Yixinli and Tianya, to train the original PanGu 350M model. After that, we use 5,6000 question-answer pairs from PsyQA dataset to fine-tune the model.\nTraining Platform We train the PanGu model on the OpenI platform with a free V100 graphics card GPU because OpenI is the open source platform of PanGu model, convenient for us to deploy the required files, image and GPU. The batch size is set to 8, and the training iteration is set to 100,000 because we found that 50,000 iterations is not enough for the model\u2019s loss to converge. We train the WenZhong model in jupyter notebook. To fine-tune this model, we need to tokenise the data, which can transform words into tokens. Besides, we also need to isolate the max length of each sentence as 500.\n# 4.7 Dataset Evaluation\nDetermining the cleaning rules and data filtering thresholds are important aspects of the data cleaning process. We employed a data quality evaluation method that combined both manual and model-based evaluations to evaluate the dataset obtained from website crawling. For the model-based evaluation, we utilised the PanGu 350M model and calculated the perplexity metric after each data cleaning stage. A lower perplexity value indicates a more effective cleaning process and higher dataset quality. In\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/37d8/37d884b6-c09a-4ccf-b941-8045fc025b87.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Word cloud of the frequent word within our dataset</div>\naddition to the model-based evaluation, we sought input from experts in psychology. We invited two members from our University\u2019s School of Psychology, Faculty of Science, to perform a random sample check on the dataset after it had undergone the cleaning process. While this method does not cover the entire corpus comprehensively, it provides valuable insights and plays a role in data cleaning and quality evaluation. The evaluation process involved the following steps: First, we provided the experts with a sample of the cleaned dataset and asked them to assess its quality based on their expertise and domain knowledge. They evaluated the dataset for accuracy, relevance, and coherence, providing feedback and suggestions for further improvements. Next, we conducted a comparative analysis between the model-based evaluation and the expert evaluation. We examined the perplexity scores obtained from the PanGu 350M model and compared them with the feedback provided by the experts. This allowed us to identify any discrepancies or areas of improvement in the dataset. Overall, the combination of model-based evaluation and expert assessment comprehensively evaluated the dataset quality. It allowed us to identify and address any issues or shortcomings in the data cleaning process, ensuring that the final dataset used for training and evaluation was high quality and suitable for our research purposes.\n# 4.8 Model Training Setting\nModels: For our training, we utilise the PanGu 350M model, considering the available computational resources. This model consists of 350 million parameters, 24 layers, a hidden size 1024, and 16 attention heads. Additionally, we target the WenZhong-110M model, which contains 12 layers and 110 million parameters. Training Data: We collected a psychology corpus dataset totalling 2.85GB, which was crawled from psychology platforms such as Yixinli and Tianya. This dataset was used for training the original PanGu 350M model. Subsequently, we fine-tuned the model using 56,000 question-answer pairs from the PsyQA dataset. Training Platform: The PanGu model was trained on the OpenI platform, utilising a free 1 V100 graphics card GPU. OpenI is an open-source platform specifically designed for the PanGu model, allowing us to easily deploy the necessary files, images, and GPU resources. For training with the V100 graphics card (32 GB memory), the minimum recommended configuration is one card, while the recommended configuration is two. The graphics card requirements can be adjusted based on the memory size (for example, a 16GB memory card would require twice as many cards as the V100). Increasing the number of graphics cards can help improve the training speed if the dataset is large. We set the batch size to 8 and performed training for 100,000 iterations, as we observed that 50,000 iterations were insufficient\nTraining Platform: The PanGu model was trained on the OpenI platform, utilising a free 1 V100 graphics card GPU. OpenI is an open-source platform specifically designed for the PanGu model, allowing us to easily deploy the necessary files, images, and GPU resources. For training with the V100 graphics card (32 GB memory), the minimum recommended configuration is one card, while the recommended configuration is two. The graphics card requirements can be adjusted based on the memory size (for example, a 16GB memory card would require twice as many cards as the V100). Increasing the number of graphics cards can help improve the training speed if the dataset is large. We set the batch size to 8 and performed training for 100,000 iterations, as we observed that 50,000 iterations were insufficient\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\n<div style=\"text-align: center;\">Table 4: Hardware and Software Versions</div>\nHardware and Software\nVersion\nOperating System\nWindows 10\nnumpy\n1.18.5\npandas\n1.3.4\ntorch\n1.11.0\ntokenizers\n0.13.1\ntqdm\n4.64.1\njupyter\n1.0.0\ntransformers\n4.23.1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4374/43743883-56b7-4fd1-923e-e6f6423dd155.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Prepareing the training corpus</div>\nfor the model\u2019s loss to converge. For the WenZhong model, we used Jupyter Notebook to run the pre-trained model an fine-tuned it on a system with 64GB memory and an RTX3060 graphics card. The version details of the hardware an software components are listed in table 4.\n# 4.8.1 Training Process\nAccording to the guide of training PanGu model with GPU, the first step is environment configuration. We prepared Pytorch, PanGu image, one V100 graphics card and some PanGu model files like vocabulary. The second step is data preprocessing. We put the training corpus into a text file, and each sample is a paragraph separated by two newlines, then converted it into a binary file because it is the required input format of the training PanGu model. The third step is model training. We uploaded the PanGu model and the bin file to the OpenI platform and set some parameters like iteration to train it. Our training procedure of PanGu model consists of two steps. Firstly, we train the original PanGu 350M model with all the crawling data for 100,000 iterations. The model starts to converge at about 60,000. This model has learned psychology domain knowledge based on pre-trained data. Secondly, we fine-tune it with the PsyQA dataset to improve the model\u2019s capability to provide useful answers for users on mental health support. We used the early stop method to choose appropriate iterations. Stopping the training of the network before the validation loss increased effectively prevented the model from overfitting. For example, when the model has more than 9,000 training iterations, the validation loss of the model starts to rise, which means the phenomenon of overfitting occurs. A similar approach is also used for the WenZhong model.\nIn this section, we assess the performance and effectiveness of our proposed language model for online psychological consultation. We employ a combination of intrinsic and human evaluation metrics to evaluate the model\u2019s capabilities comprehensively. We begin by utilising perplexity, ROUGE-L, and Distinct-n metrics to measure the model\u2019s language generation quality, similarity to the reference text, and diversity. Additionally, we recognise the limitations of these metrics and emphasise the importance of human evaluation in providing subjective assessments of the model\u2019s outputs, considering factors such as coherence, relevance, and overall quality. Through this comprehensive evaluation approach, we aim to gain a comprehensive understanding of our model\u2019s strengths, weaknesses, and suitability for its intended purpose in the context of online psychological consultation.\nPerplexity is a widely used intrinsic evaluation metric that measures how well a language model predicts a given sample. Mathematically, perplexity is defined as the reciprocal of the average probability assigned to each token in the dataset by the language model (Chen, Beeferman, and Rosenfeld 1998). In simpler terms, a lower perplexity value indicates better language model performance. Since perplexity is based on the average log-likelihood of the dataset, it can be computed quickly and is statistically robust, as it is not easily affected by outliers.\nwhere PP is the perplexity, P is the probability of the i word, and N is the length of a sentence. It is important to note that perplexity tends to decrease as the dataset size increases, indicating better performance. However, it is crucial to understand that low perplexity does not necessarily equate to high accuracy. Perplexity is primarily used as a preliminary measure and should not be solely relied upon for evaluating model accuracy. Additionally, comparing the performance of models on different datasets with varying word distributions can be challenging (Chen, Beeferman, and Rosenfeld 1998). Therefore, while perplexity provides valuable insights into model performance, it should be complemented with other evaluation metrics and considerations when assessing model accuracy. ROUGE-L (Longest Common Subsequence) is an evaluation metric that measures the number of overlapping units between the predicted text generated by a language model and the actual reference text (Lin 2004). ROUGE-L measures how closely the generated text matches the desired output by quantifying the similarity between the predicted and reference texts. Distinct-1 and Distinct-2 are evaluation metrics that assess the diversity of the generated text. Distinct-1 calculates the number of distinct unigrams (individual words) divided by the total number of generated words. In contrast, Distinct-2 calculates the number of distinct bigrams (pairs of adjacent words) divided by the total number of generated bigrams (Li, Galley, Brockett, et al. 2016). These metrics reflect the degree of diversity in the generated text by quantifying the presence of unique unigrams and bigrams. The formulas for calculating Distinct-n are as follows:\nHere, Count(unique, n-gram) represents the number of n-grams that are not repeated in a reply, and Count(word) indicates the total number of n-gram words in the reply. A higher value of Distinct(n) indicates a greater diversity in the distinct generations. These evaluation metrics, including perplexity, ROUGE-L, Distinct-1, and Distinct-2, provide insights into the quality, similarity, and diversity of the generated text by the language model. They serve as valuable tools for assessing the performance and effectiveness of the model in generating accurate and diverse outputs. While perplexity and Distinct-n provide insights into the language model\u2019s performance in language generation, they do not necessarily indicate high accuracy. Therefore, in order to evaluate models more convincingly, human evaluation is still necessary. Human evaluators can provide subjective assessments of the generated text, considering factors such\n(1)\n(2)\n(3)\nas coherence, relevance, and overall quality, which are important aspects that cannot be fully captured by automated evaluation metrics alone.\n# 4.9.2 Human evaluation\nFor human evaluation, we have developed an online marking system to assess the performance of our language model in the context of online psychological consultation. This evaluation system aims to streamline the process and ensure effective assessment by focusing on four key metrics: Helpfulness, Fluency, Relevance, and Logic. Each metric is scored on a scale of 1 to 5, allowing evaluators to provide a quantitative assessment of each aspect. The four metrics are defined as follows: 1. Helpfulness: This metric evaluates whether the generated response is helpful for patients seeking psychological support. 2. Fluency: Fluency refers to the degree of coherence and naturalness exhibited in the generated response. 3. Relevance: Relevance assesses the extent to which the response\u2019s content directly relates to the posed question. 4. Logic: Logic examines the logical consistency and coherence of the meaning conveyed in the generated response.\nTo conduct the human evaluation, we invited six students from the psychological faculty to assess a set of 200 questionanswer pairs generated by our model. We employed two evaluation methods to understand the model\u2019s performance comprehensively. In the first method, evaluators compared responses generated by the PanGu model and the WenZhong model in response to the same question. They assigned scores to these answers based on the predetermined metrics, allowing for a direct comparison between the two models. The second method involved incorporating the actual answers alongside the predicted responses as a whole, allowing evaluators to assess the differences and similarities between the generated and actual responses. By employing these human evaluation methods, we aim to gain valuable insights into the performance of our language model, particularly in terms of the disparities between predicted and actual responses. This comprehensive evaluation approach will provide a deeper understanding of the model\u2019s capabilities and guide further improvements in its performance for online psychological consultation.\n# 5 Experimental Results\nIn this section, we present the findings and outcomes of the evaluation and experimentation conducted to assess the performance and effectiveness of our proposed language model for online psychological consultation. This section provides a comprehensive analysis of the model\u2019s performance based on intrinsic and human evaluation metrics. We discuss the results obtained from metrics such as perplexity, ROUGE-L, and Distinct-n, which shed light on language generation quality, similarity to reference text, and diversity of the generated responses. Additionally, we present the outcomes of the human evaluation, which includes scores given by evaluators based on metrics such as Helpfulness, Fluency, Relevance, and Logic. Through these rigorous evaluations, we aim to provide an in-depth understanding of the strengths and weaknesses of our language model and its suitability for the task of online psychological consultation.\n# 5.1 Result of Intrinsic Evaluation\nThe results of the intrinsic evaluation comparing the performance of the PanGu model and the WenZhong model are presented in table 5. The metrics used for evaluation include perplexity, ROUGE-L, Distinct-1, and Distinct-2. As shown in table 5, the PanGu model outperforms the WenZhong model across all metrics. The PanGu model achieves a lower perplexity value of 34.56 compared to 38.40 for the WenZhong model, indicating that the PanGu model better predicts the sample probabilities in the dataset. Furthermore, the ROUGE-L score, which measures the similarity between the generated responses and the reference text, is higher for the PanGu model (28.18) than the WenZhong model (23.56). This suggests that the PanGu model generates responses more aligned with the expected answers. In terms of diversity in generated responses, the PanGu model also exhibits higher Distinct-1 (4.57) and Distinct-2 (12.74) scores compared to the WenZhong model (3.55 and 9.67, respectively). This indicates that the PanGu model produces a wider range of distinct unigrams and bigrams in its responses, leading to a more diverse language generation.\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\n<div style=\"text-align: center;\">Table 5: The result of Intrinsic Evaluation between two models</div>\nModel\nPerplexity\nRouge-L\nDistinct1\nDistinct2\nWenZhong\n38.40\n23.56\n3.55\n9.67\nPanGu\n34.56\n28.18\n4.57\n12.74\nThe intrinsic evaluation results consistently demonstrate that the PanGu model performs better than the WenZhong model across all metrics. These findings highlight the superior language generation capabilities of the PanGu model in the context of online psychological consultation.\nIn the following, we will qualitatively evaluate the outputs of both models by asking our Psy-LLM a common ques for individuals suffering from depression.\n# 5.2.1 PanGu Model\nTo demonstrate the capabilities of the PanGu model, we provide an example of question answering using the model on coping with depression (Fig. 6), which we have translated into English. The question posed to the model is,\n\u201cQuestion: How to deal with depression?\u201d,\nand the model\u2019s response is as follows:\n\u201cAnswer: Hello, first let me give you a small hug. I hope it brings you some warmth. Typically, individuals with depression are often misunderstood and even scolded by their families, which can be really disheartening. I understand that you are feeling very sad and disappointed with your family situation. I want you to know that you are a remarkable person, and you are not at fault. Remember, change starts with you, and you are incredibly strong and sensible.\u201d\nThis response showcases the model\u2019s fluency and ability to employ different strategies. Analysing the response in Fig. 6, we can observe that the model utilises various strategies. The initial part, offering a virtual hug, demonstrates a reassurance strategy Sun, Lin, Zheng, et al. 2021. It provides emotional support and encouragement to individuals seeking consultation, helping them relax. The second part, addressing family dynamics, employs a restatement strategy by clearly reiterating the question to provide a more realistic understanding of the individual\u2019s mental health condition. The final part utilises a guidance strategy, advising the individual on steps to improve their situation. Overall, the model\u2019s response is coherent and consistent with normal human logic, making it highly valuable. The training of the PanGu model consists of two steps. Initially, the model is trained with a vast amount of crawled data, enabling it to learn extensive domain knowledge in psychology. Subsequently, the model undergoes fine-tuning using the downstream PsyQA dataset. During this stage, the model primarily focuses on learning the pattern of \u201cQuestion:\u201d and \u201cAnswer:\u201d to generate responses in the same format as the downstream tasks. When we input a question in the format \u201cQuestion: \u00b7 \u00b7 \u00b7 \u201d to the model, the model outputs a response in the format \u201cAnswer: \u00b7 \u00b7 \u00b7 \u201d. This demonstrates that the model has successfully learned the desired pattern and can generate appropriate responses.\n# 5.2.2 WenZhong Model\nIn WenZhong model, after fine-tuning the model using Psychology data, the model can output very suitable answer. We also utilised the WenZhong model to answer the same question (Fig. 7), and the generated response was:\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E \n\nFigure 7: A demonstration of the WenZhong model\u2019s Q&A capability\n\u201cOur experience tells us that there are many times when you just have to do things by your own will, then there will be a lot of discomfort. For example, if you are studying, the time of day, the place of study, the attitude of study, the ability to study, etc., these must be treated carefully, especially these should be avoided as much as possible, for example, then if you do not want to do something negative, leave it alone and do as much as possible to do some preventive measures for depression, so that it will not lead to the development of this disease.\u201d\nThis example demonstrates that the WenZhong model can provide suitable answers in the field of psychology. Howeve one challenge we encountered is that some generated outputs do not directly address the question. This issue may b attributed to the limited fine-tuning data specific to psychology. In order to further improve the performance of th WenZhong model in psychology-related tasks, a larger and more diverse dataset from the field of psychology could b incorporated during the fine-tuning process.\n# 5.3 Human Evaluation\nTo conduct an empirical evaluation of Psy-LLM\u2019s effectiveness, we enlisted the participation of six students from the psychological faculty to assess a set of 200 question-answer pairs generated by our language model. In order to obtain a comprehensive understanding of the model\u2019s performance, we employed two evaluation methods for the participants to provide ratings on the responses. We have created a web front-end for users to access our Psy-LLM platform, and their technical details are discussed in section 6. The first method directly compared responses generated by both the PanGu and the WenZhong models in response to the same question. Evaluators assigned scores to these answers based on predetermined metrics, enabling a clear and direct comparison between the two models\u2019 performance. In the second method, we presented evaluators with a combined set of predicted and actual responses. This allowed them to evaluate and assess the differences and similarities between the generated responses and the ground truth answers. By utilising these human evaluation methods, we aim to gain valuable insights into the performance of our language model, particularly in terms of the disparities between predicted and actual responses. This comprehensive evaluation approach will provide a deeper understanding of the model\u2019s capabilities and guide further improvements in its performance for online psychological consultation. The human evaluation results, using two different methods, are presented in tables 6 and 7. These evaluate humanperceived metrics of Helpfulness, Fluency, Relevance, and Logic. Table 6 shows the results of the first human evaluation method, where evaluators provided scores for each metric. Consistent with the findings from the intrinsic evaluation, the PanGu model outperforms the WenZhong model in terms of Helpfulness (3.87 vs. 3.56), Fluency (4.36 vs. 4.14), Relevance (4.09 vs. 3.87), and Logic (3.83 vs. 3.63). These results indicate that human evaluators generally consider the PanGu model\u2019s generated responses more helpful, fluent, relevant, and logical than the WenZhong model. However, a notable observation is made when comparing the scores obtained in table 6 with the scores from table 7. Table 7 presents the scores for the predicted answers of both models as well as the actual answers. Interestingly, the scores for the actual answers are significantly higher than those for the predicted answers of both models across all metrics. This discrepancy suggests that the evaluators, who had the opportunity to compare the actual answers with the predicted answers, marked the predicted answers relatively lower. This finding highlights the importance of incorporating human evaluation in assessing the performance of language models and the need for further improvement in generating more accurate and satisfactory responses. In summary, the human evaluation results align with the intrinsic evaluation findings, indicating that the PanGu model performs better than the WenZhong model. However, it is important to note that the scores for the actual answers are considerably higher than those for the predicted answers, implying room for improvement in the generated responses of the language models.\nMetrics\nWenZhong\nPanGU\nHelpfulness\n3.56\n3.87\nFluency\n4.14\n4.36\nRelevance\n3.87\n4.09\nLogic\n3.63\n3.83\n<div style=\"text-align: center;\">Table 7: Average Human ratings of Psy-LLM responses, alongside the ground-truths from datasets.</div>\nRating Metrics\nWenZhong\nPanGU\nGround Truth\nHelpfulness\n3.45\n3.54\n4.52\nFluency\n3.95\n4.12\n4.83\nRelevance\n3.77\n3.96\n4.72\nLogic\n3.61\n3.75\n4.56\n# 6 Web-Interface for Accessible Online Consultation\nOne of the primary objectives was to explore the provision of online AI-powered consultation and question-andanswer services in psychology. We adopted a distributed architecture, separating the model\u2019s front-end, back-end, and computing servers into modular components. Each module was developed with distinct responsibilities, allowing for easier upgrades and interchangeability of combinations. Communication between the modules was achieved through API interactions, enabling them to function independently without relying on the internal functionality of other modules.\nanswer services in psychology. We adopted a distributed architecture, separating the model\u2019s front-end, back-end, and computing servers into modular components. Each module was developed with distinct responsibilities, allowing for easier upgrades and interchangeability of combinations. Communication between the modules was achieved through API interactions, enabling them to function independently without relying on the internal functionality of other modules. Furthermore, we placed a strong emphasis on security during the design process. We implemented measures to encrypt and protect our modular systems at a product level. The common API interface was productised and encrypted, ensuring secure communication between the components. Additionally, we implemented the HTTPS web system architecture, enhancing security by encrypting each cloud server with TLS (SSL). By adopting a distributed and modular approach and prioritising security, we aimed to address the challenges of hosting a large-scale online consultation service model. These design choices allowed for flexibility, scalability, and enhanced security in our system architecture, contributing to our project\u2019s overall success and reliability.\nFurthermore, we placed a strong emphasis on security during the design process. We implemented measures to encrypt and protect our modular systems at a product level. The common API interface was productised and encrypted, ensuring secure communication between the components. Additionally, we implemented the HTTPS web system architecture, enhancing security by encrypting each cloud server with TLS (SSL). By adopting a distributed and modular approach and prioritising security, we aimed to address the challenges of hosting a large-scale online consultation service model. These design choices allowed for flexibility, scalability, and enhanced security in our system architecture, contributing to our project\u2019s overall success and reliability.\n# 6.1 Web Technologies\n\u2022 ReactJS: ReactJS was our front-end framework due to its extensive library support. ReactJS offers a wide range of reusable components and follows a modular, component-based architecture, making designing and enhancing the front end easier. ReactJS is responsive and provides excellent cross-platform support. \u2022 AWS Amplify: AWS Amplify is a rapid front-end deployment service provided by Amazon. It enables us to quickly deploy the front end of our website and seamlessly communicate with other system components. Amplify provides fully managed CI/CD (Continuous Integration/Continuous Deployment) and hosting, ensuring fast, secure, and reliable encryption services. \u2022 Google Domain: We utilised Google Domain services for secure encapsulation of our EC2 host DNS. \u2022 Amazon EC2: EC2 provides virtual server instances with highly available underlying designs. It offers reliable, scalable, and flexible access in terms of cost and performance. EC2 provides powerful computing resources and pre-configured environments, making it an excellent choice for running large models. Its robust network performance and high-performance computing clusters allow for high throughput and low-latency online processing. We used simple Flask-based scripts to handle concurrent requests. \u2022 Python & Flask: We used Python as our scripting language to run the models and build APIs. Flask, a web framework written in Python, was used for creating API endpoints and handling request-response interactions. \u2022 Apache: We used Apache, an open-source web server software, for configuring port forwarding, reverse proxies, and listening. \u2022 Let\u2019s Encrypt & Certbot: We employed Let\u2019s Encrypt and Certbot for TLS (Transport Layer Security) encryption, ensuring secure communication between the website and users.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/09f0/09f0cb86-7f36-4e36-827d-5fb9ca5d83b8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Online Web-frontend Architecture</div>\nThe diagram in Fig. 8 illustrates the architecture of our independently developed web system for the cloud-based site. The website\u2019s user interface is accessible through the front end, deployed on the AWS Amplify service. Built on the ReactJS framework, the front-end communicates with the back-end database through an internal API, enabling storage of user evaluation data for model effectiveness optimisation. The database is hosted within Amplify Hosting. The standalone website interacts with the model runtime server via a public API. To ensure privacy and protect the host address, we register a public domain name through Google Domains and link it to the host server\u2019s DNS. The pre-trained model is deployed on an Amazon EC2 instance host configured as an AWS Linux virtual server. The model uses Python code and Flask scripts, allowing for local server calls. Apache is used for HTTP reverse proxy communication, forwarding external model input data to the local server where the model is waiting and generating results. To provide secure HTTPS encryption for the web products deployed on AWS Amplify, we employ TSL encryption for the EC2 instance DNS addresses. This is achieved using Let\u2019s Encrypt and Certbot as cost-effective alternatives to commercial SSL certificates. The website\u2019s front end is designed with simplicity, featuring an input box for users to enter Chinese questions, as depicted in Fig. 9. Upon submission, the system communicates with the back-end model through the API. It awaits the completion of model processing (Fig. 10a) before returning the results to the output box, as depicted in (Fig. 10b). Users can rate the results using the built-in rating system, and there is a link to an additional evaluation site at the bottom of the page.\n# 7 Discussion\nThe discussion section provides a comprehensive analysis of the project outcome, product perspective, website per spective, model perspective, and evaluation perspective.\n# 7.1 Project Outcome\nWe have successfully developed and implemented an effective chatbot for mental health counselling. Through the training and fine-tuning large-scale Chinese pre-training models on mental health datasets, the chatbot has acquired valuable knowledge in psychology, enhancing its ability to provide counselling services. The deployment of the chatbot on a website interface has created a convenient and accessible platform for users seeking mental health support\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0035/0035221f-63b9-455c-9bb0-bd52bdfc694c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Website Initial Status</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/32b6/32b6f8f7-4b70-4199-8c3e-25771374f740.png\" style=\"width: 50%;\"></div>\nAlthough the chatbot is currently in its prototype stage, our project demonstrates the feasibility of building an AI-based counselling system. It is a valuable reference for future research and development in this area. From a model perspective, our evaluation results demonstrate the superiority of the PanGu model over the WenZhong model, as expected due to its larger size and advanced architecture. The PanGu model\u2019s design contributes to its outperformance, particularly its incremental learning ability and enhanced natural language understanding capabilities. However, both models fall short of achieving human-level performance, which can be attributed to the quality of the training dataset and the inherent limitations of autoregressive language models. Enhancing the dataset quality and exploring alternative language model architectures hold promise for addressing these limitations and further improving performance.\n# 7.2 Evaluation Perspective\nThe human evaluation results indicate that both the PanGu model and the WenZhong model have yet to reach humanlevel performance. Even though training the models on our dataset crawled from websites, the predicted answers strongly focus on psychological content but lack logical coherence. One potential reason for this is the quality of our dataset, which may need to be higher to provide comprehensive and reliable training examples. Although we conducted\nhuman evaluation during the data cleaning stage, the sheer volume of data made it challenging to cover every instance. To address this, we recommend thoroughly evaluating the website data before crawling to ensure a higher-quality dataset. Another factor impacting human evaluation is the limited computing conditions during model training. Our model requires a specific training environment and numerous parameters, making it time-consuming to adjust and fine-tune it effectively. We could not optimise the parameters and achieve optimal testing results with our current resources. Consequently, the model\u2019s performance may have been hindered by these limitations. Furthermore, the autoregressive nature of both the PanGu model and the WenZhong model poses challenges in comprehending contextual information. As autoregressive language models, their training processes are unidirectional, focused on modelling the joint probability from left to right. The next predicted word is solely based on the preceding word, limiting their ability to capture information from broader contexts. This lack of contextual background reference makes it difficult for language models to handle reading comprehension tasks like humans. In summary, the evaluation results shed light on the areas where improvements can be made. Enhancing the dataset quality through pre-evaluation and addressing the limitations of our computing conditions are crucial steps toward advancing the model\u2019s performance. Additionally, exploring alternative language model architectures that can effectively capture contextual information may contribute to bridging the gap between model-generated responses and human-level performance.\n# 7.3 Product and Practicality Perspective\nThe performance of the online consultation service indicates its significant potential for streamlining mental health support with minimal resources. The user experience has been a priority in product design, and the cloud infrastructure deployment ensures easy access via mobile devices. As part of our future improvements, we plan to incorporate an automatic emotion recognition system into the website, enabling the identification of users in distress and facilitating timely intervention. The design and development of our product hold substantial societal value in the mental health support field, providing a promising avenue for further exploration and refinement. Regarding the website, we have designed and implemented a modern, cloud-based network architecture that boasts lightweight, scalable, and highly secure features. This architecture allows for low-cost, large-scale model computing sites, enabling widespread accessibility to AI-based question-and-answer services. Our approach serves as a reference for small organisations and enterprises with limited resources, showcasing the possibilities of deploying AI capabilities effectively.\n# 8 Limitations and Future Works\nWhile we have presented promising results with our Psy-LLM model for usage in assisting mental health workers, our study is exploratory in nature, and hence, there exist numerous limitations that we would like to raise in the following\n# 8.1 Model Capability and Usage in Real-World\nWhile there are numerous benefits in deploying an AI-powered Large Language Model for supporting the demand in the mental health sector, one should consider several ethical and practical issues. Firstly, as a language-based model, the model\u2019s output is based purely on the input text. However, studies have shown that nonverbal communication is one of the key factors in counselling outcome (Hill, Siegelman, Gronsky, et al. 1981). In fact, a well-trained counsellor can often pick up subtle cues even when there is a lack of response from the patient. Standalone LLM models like Psy-LLM cannot address such an issue (unless techniques like facial emotion detection from the computer vision community are integrated as a unified system (Jaiswal, Raju, and Deb 2020)). Furthermore, rapport-building with clients is often a crucial step in clinical psychology. However, an AI-based model would face severe difficulties in building trusted client relationships. As a result, it is critical to realise that such an AI-powered system cannot replace real-world counselling setups. A practical approach would be to pair the model output under the supervision of a trained counsellor as a good psychoeducational tool. The model output can be used as an initial guideline or suggestion for assisting human counsellors in providing useful and trusted consultations with patients.\n# 8.2 Data Collections\nSeveral strategies can be implemented in future work to overcome the limitations in data collection. Firstly, to address the issue of anti-crawler rules on different websites, developing a more robust and adaptable crawler that can handle different anti-crawler mechanisms would be beneficial. The access limitation could involve implementing dynamic IP\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\nrotation or utilising proxies to avoid IP blocking. Machine learning techniques, such as automatic rule extraction or rule adaptation, could also help automate handling anti-crawler mechanisms. Incorporating more advanced data-cleaning techniques can also improve the quality of the crawled data. More advanced data-cleaning procedures may involve NLP methods, such as entity recognition, part-of-speech tagging, and named entity recognition to identify and filter out irrelevant or noisy data. Additionally, leveraging machine learning algorithms, such as anomaly detection or outlier detection, can aid in identifying and removing low-quality or erroneous data points. In terms of dataset standardisation, establishing a unified standard for data generation in the online domain would greatly facilitate the cleaning process. This could involve collaborating with website administrators or data providers to develop guidelines or formats for data representation. Furthermore, using human annotators or experts in the domain to manually review and clean a subset of the dataset can provide valuable insights and ensure a higher-quality dataset. However, it is important to acknowledge that achieving a completely clean dataset is challenging, particularly when dealing with large-scale datasets. As such, future work should strike a balance between the manual review and automated cleaning techniques while also considering the cost and scalability of the data cleaning process.\n# 8.3 Model Improvement\nIncreasing the scale of model training by utilising larger models or ensembles of models can enhance the performance and capabilities of the chatbot. Larger models can capture more nuanced patterns and relationships in the data, leading to more accurate and coherent responses. Exploring different model architectures beyond autoregressive language models may provide valuable insights. Bidirectional models (e.g. Transformer-XL) or models that incorporate external knowledge sources (e.g. knowledge graphs) can improve the chatbot\u2019s contextual understanding and generate more informative responses. Moreover, integrating feedback mechanisms into the training process can help iteratively improve the chatbot\u2019s performance. This could involve collecting user feedback on the generated responses and incorporating it into the model training through reinforcement learning or active learning. Several disadvantages were also identified in the LLM architecture. Firstly, the maximum likelihood training approach of the WenZhong model is susceptible to exposure bias, which occurs when samples are drawn from the target language distribution. This bias can lead to errors for which researchers have yet to find effective solutions. Additionally, training the WenZhong model multiple times can significantly decrease its quality. Furthermore, the WenZhong model follows an autoregressive architecture, which models joint probability from left to right. This unidirectional training process limits its ability to capture information from all contexts, particularly hindering its performance in tasks requiring reading comprehension that rely on contextual background references. Similar to the WenZhong model, the PanGu model also exhibits autoregressive characteristics. Although it inherits the ability to estimate the joint probability of language models, it suffers from the same limitations of unidirectional modelling. It lacks bidirectional context information and may produce duplicate results requiring resolve deduplication. We also have reservations about the Jieba tokeniser used in the PanGu model. Its performance and tokenisation ability need to handle complex Chinese tokenisation accurately. Furthermore, as neural networks and pre-trained models advance, Chinese NLP tasks increasingly demonstrate that tokenisation is only sometimes necessary. Large models can effectively learn character-to-character relationships without word segmentation. For instance, Google is considering discarding tokenisation and using bytes directly. Adopting a more flexible tokeniser could make the model more suitable for various industrial applications, even when sacrificing some performance.\n# 8.4 User Experience and User Interface\nEnhancing the chatbot\u2019s user experience and user interface can significantly impact its adoption and effectiveness. Future work should focus on improving the simplicity, intuitiveness, and accessibility of the website interface. This includes optimising response times, refining the layout and design, and incorporating user-friendly features such as autocomplete suggestions or natural language understanding capabilities. Furthermore, personalised recommendations and suggestions to users based on their preferences and previous interactions can enhance the user experience. Techniques like collaborative filtering or user profiling can enable the chatbot to understand better and cater to individual user needs. Usability testing and user feedback collection should be conducted regularly to gather insights on user preferences, pain points, and suggestions for improvement. Iterative design and development based on user-centred principles can ensure that the chatbot meets user expectations and effectively addresses their mental health support needs.\nP S Y- L L M: L A R G E L A N G UAG E M O D E L S F O R M E N TA L H E A LT H P S Y C H O L O G I C A L S E RV I C E S\nAs with any AI-based system, ethical considerations and user privacy are paramount. Future work should address these concerns by implementing robust privacy protection mechanisms and ensuring transparency in data usage. This includes obtaining explicit user consent for data collection and usage, anonymising sensitive user information, and implementing strict data access controls. Developing mechanisms to handle potentially sensitive or harmful user queries is crucial. The chatbot should have appropriate safeguards and guidelines to avoid providing inaccurate or harmful advice. Integrating a reporting system where users can report problematic responses or seek human intervention can help mitigate potential risks. Furthermore, monitoring and auditing the chatbot\u2019s performance and behaviour can help identify and rectify biases or discriminatory patterns. Regular evaluations by domain experts and user feedback analysis can improve the chatbot\u2019s reliability, fairness, and inclusivity. While this project has made significant progress in developing an AI-based chatbot for mental health support, there are various limitations and areas for improvement. Overcoming challenges related to data quality, model performance, ethical considerations, and user experience will contribute to the overall effectiveness and reliability of the chatbot. By addressing these limitations and exploring future research directions, we can continue to advance the field of AI-powered mental health support systems and provide valuable assistance to individuals in need.\n# 9 Conclusion\nIn conclusion, our project on Psy-LLM, an exploratory study on using Large Language Models as an assistive mental health tool, has been successfully completed and implemented. While there are areas identified for improvement based on specific evaluation indicators, we are confident that with improved equipment conditions, we can enhance the performance of this platform. The experimental results obtained from this project hold significant potential to contribute to the fields of supportive natural language generation and psychology, driving advancements at the intersection of these domains. The deployment of such a system offers a practical approach to promoting the overall mental well-being of our society by providing timely responses and support to those in need.\n# References\nAbd Rahman, Rohizah, Khairuddin Omar, Shahrul Azman Mohd Noah, Mohd Shahrul Nizam Mohd Danuri, and Mohammed Ali Al-Garadi (2020). \u201cApplication of machine learning methods in mental health detection: a systematic review\u201d. In: Ieee Access 8, pp. 183952\u2013183964. Akstinaite, Vita, Peter Garrard, and Eugene Sadler-Smith (2022). \u201cIdentifying linguistic markers of CEO hubris: a machine learning approach\u201d. In: British Journal of Management 33.3, pp. 1163\u20131178. Brattain, Laura J, Brian A Telfer, Manish Dhyani, Joseph R Grajo, and Anthony E Samir (2018). \u201cMachine learning for medical ultrasound: status, methods, and future opportunities\u201d. In: Abdominal radiology 43, pp. 786\u2013799. Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. (2020a). \u201cLanguage models are few-shot learners\u201d. In: Advances in neural information processing systems 33, pp. 1877\u20131901. Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei (2020b). \u201cLanguage Models are Few-Shot Learners\u201d. In: ArXiv abs/2005.14165. Castiglioni, Isabella, Leonardo Rundo, Marina Codari, Giovanni Di Leo, Christian Salvatore, Matteo Interlenghi, Francesca Gallivanone, Andrea Cozzi, Natascha Claudia D\u2019Amico, and Francesco Sardanelli (2021). \u201cAI applications to medical images: From machine learning to deep learning\u201d. In: Physica Medica 83, pp. 9\u201324. Cataldo, Francesco, Shanton Chang, Antonette Mendoza, and George Buchanan (2021). \u201cA Perspective on ClientPsychologist Relationships in Videoconferencing Psychotherapy: Literature Review\u201d. In: JMIR Ment Health 8.2, e19004. Chen, Stanley F., Doug Beeferman, and Roni Rosenfeld (1998). \u201cEvaluation Metrics For Language Models\u201d. In. Chen, Tianhua and Mike Lucock (2022). \u201cThe mental health of university students during the COVID-19 pandemic: An online survey in the UK\u201d. In: PloS one 17.1, e0262562. Chugh, Gunjan, Shailender Kumar, and Nanhay Singh (2021). \u201cSurvey on machine learning and deep learning applications in breast cancer diagnosis\u201d. In: Cognitive Computation, pp. 1\u201320. De Choudhury, Munmun, Michael Gamon, Scott Counts, and Eric Horvitz (2013). \u201cPredicting depression via social media\u201d. In: Proceedings of the international AAAI conference on web and social media. Vol. 7. 1, pp. 128\u2013137.\nDenecke, Kerstin, Alaa Abd-Alrazaq, and Mowafa Househ (2021). \u201cArtificial intelligence for chatbots in mental health: opportunities and challenges\u201d. In: Multiple perspectives on artificial intelligence in healthcare: Opportunities and challenges, pp. 115\u2013128. Deryugina, OV (2010). \u201cChatterbots\u201d. In: Scientific and Technical Information Processing 37, pp. 143\u2013147. Gay, Jan L and Jacqueline M Swank (2021). \u201cSchool counselor educators\u2019 experiences navigating practicum and internship during COVID-19\u201d. In: Counselor Education and Supervision 60.4, pp. 291\u2013305. Gonzalez-Hernandez, Graciela, Abeed Sarker, Karen O\u2019Connor, and Guergana Savova (2017). \u201cCapturing the patient\u2019s perspective: a review of advances in natural language processing of health-related text\u201d. In: Yearbook of medical informatics 26.01, pp. 214\u2013227. Gou, Yue, Nianwei Wu, Jing Xia, Yanjun Liu, Huawu Yang, Haibo Wang, Tong Yan, and Dan Luo (2022). \u201cProvinceand individual-level influential factors of depression: multilevel cross-provinces comparison in China\u201d. In: Frontiers in Public Health 10, p. 893280. Handoyo, Eko, M Arfan, Yosua Alvin Adi Soetrisno, Maman Somantri, Aghus Sofwan, and Enda Wista Sinuraya (2018). \u201cTicketing chatbot service using serverless NLP technology\u201d. In: 2018 5th International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE). IEEE, pp. 325\u2013330. Hill, Clara E, Larry Siegelman, Barbara R Gronsky, Frank Sturniolo, and Bruce R Fretz (1981). \u201cNonverbal communication and counseling outcome.\u201d In: Journal of Counseling Psychology 28.3, p. 203. Hom, Melanie A, Ian H Stanley, and Thomas E Joiner Jr (2015). \u201cEvaluating factors and interventions that influence help-seeking and mental health service utilization among suicidal individuals: A review of the literature\u201d. In: Clinical psychology review 40, pp. 28\u201339. Hu, Hansel, Tin Lai, and Farnaz Farid (2022). \u201cFeasibility Study of Constructing a Screening Tool for Adolescent Diabetes Detection Applying Machine Learning Methods\u201d. In: Applications of Body Worn Sensors and Wearables, Special Issue of Sensors 22.16, p. 6155. Jaiswal, Akriti, A Krishnama Raju, and Suman Deb (2020). \u201cFacial emotion detection using deep learning\u201d. In: 2020 international conference for emerging technology (INCET). IEEE, pp. 1\u20135. Ji, Shaoxiong, Shirui Pan, Xue Li, Erik Cambria, Guodong Long, and Zi Huang (2020). \u201cSuicidal ideation detection: A review of machine learning methods and applications\u201d. In: IEEE Transactions on Computational Social Systems 8.1, pp. 214\u2013226. Joshi, Gunjan and Ginni Sharma (2020). \u201cBurnout: A risk factor amongst mental health professionals during COVID19\u201d. In: Asian journal of psychiatry 54, p. 102300. Kanodia, Nikita, Khandakar Ahmed, and Yuan Miao (2021). \u201cQuestion Answering Model Based Conversational Chatbot using BERT Model and Google Dialogflow\u201d. In: 2021 31st International Telecommunication Networks and Applications Conference (ITNAC), pp. 19\u201322. Kontoangelos, Konstantinos, Marina Economou, and Charalambos Papageorgiou (2020). \u201cMental health effects of COVID-19 pandemia: a review of clinical and psychological traits\u201d. In: Psychiatry investigation 17.6, p. 491. Koroteev, Dmitry and Zeljko Tekic (2021). \u201cArtificial intelligence in oil and gas upstream: Trends, challenges, and scenarios for the future\u201d. In: Energy and AI 3, p. 100041. Lai, Tin (2022a). \u201cA Review on Visual-SLAM: Advancements from Geometric Modelling to Learning-Based Semantic Scene Understanding using Multi-Modal Sensor Fusion\u201d. In: Simultaneous Localization and Mapping for Mobile Robot Navigation, Special Issue of Sensors 22.19, p. 7265. Lai, Tin (2022b). \u201cDiscover Life Skills for Planning as Bandits via Observing and Learning How the World Works\u201d. In: IEEE/RSJ Proceedings of The International Conference on Intelligent Robots and Systems (IROS). IEEE, pp. 11360\u2013 11365. Lai, Tin (2023). \u201cReal-Time Aerial Detection and Reasoning on Embedded-UAVs in Rural Environments\u201d. In: IEEE Transactions on Geoscience and Remote Sensing 61, pp. 1\u20137. Li, Jiali, Kaizhuo Lim, Haitao Yang, Zekun Ren, Shreyaa Raghavan, Po-Yen Chen, Tonio Buonassisi, and Xiaonan Wang (2020). \u201cAI applications through the whole life cycle of material discovery\u201d. In: Matter 3.2, pp. 393\u2013432. Li, Jiwei, Michel Galley, Chris Brockett, Jianfeng Gao, and William B. Dolan (2016). \u201cA Diversity-Promoting Objective Function for Neural Conversation Models\u201d. In: North American Chapter of the Association for Computational Linguistics. Li, Yan, Peiyi Han, Chuanyi Liu, and Binxing Fang (2018). \u201cAutomatically Crawling Dynamic Web Applications via Proxy-Based JavaScript Injection and Runtime Analysis\u201d. In: 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC), pp. 242\u2013249. Lin, Chin-Yew (2004). \u201cROUGE: A Package for Automatic Evaluation of Summaries\u201d. In: Annual Meeting of the Association for Computational Linguistics. Link, Maike, Claudia Dukino, Walter Ganz, Karin Hamann, and Kathrin Schnalzer (2020). \u201cThe Use of AI-Based Assistance Systems in the Service Sector: opportunities, challenges and applications\u201d. In: Advances in Human Factors and Systems Interaction: Proceedings of the AHFE 2020 Virtual Conference on Human Factors and Systems Interaction, July 16-20, 2020, USA. Springer, pp. 10\u201316.\nLui, Joyce HL, David K Marcus, and Christopher T Barry (2017). \u201cEvidence-based apps? A review of mental health mobile applications in a psychotherapy context.\u201d In: Professional Psychology: Research and Practice 48.3, p. 199. Martinengo, Laura, Elaine Lum, and Josip Car (2022). \u201cEvaluation of chatbot-delivered interventions for selfmanagement of depression: content analysis\u201d. In: Journal of affective disorders 319, pp. 598\u2013607. Novella, Jocelyn K, Kok-Mun Ng, and Jessica Samuolis (2022). \u201cA comparison of online and in-person counseling outcomes using solution-focused brief therapy for college students with anxiety\u201d. In: Journal of American College Health 70.4, pp. 1161\u20131168. Organization, World Health et al. (2017). Depression and other common mental disorders: global health estimates. Tech. rep. World Health Organization. Prochaska, Judith J, Erin A Vogel, Amy Chieng, Matthew Kendra, Michael Baiocchi, Sarah Pajarito, and Athena Robinson (2021). \u201cA Therapeutic Relational Agent for Reducing Problematic Substance Use (Woebot): Development and Usability Study\u201d. In: Journal of medical Internet research. Regona, Massimo, Tan Yigitcanlar, Bo Xia, and Rita Yi Man Li (2022). \u201cOpportunities and adoption challenges of AI in the construction industry: a PRISMA review\u201d. In: Journal of Open Innovation: Technology, Market, and Complexity 8.1, p. 45. Sandhu, Daya S (1994). \u201cAn examination of the psychological needs of the international students: Implications for counselling and psythotherapy\u201d. In: International Journal for the Advancement of Counselling 17.4, pp. 229\u2013239. Shaikh, Talha Abdul Hakeem and Manisha Mhetre (2022). \u201cAutonomous AI Chat Bot Therapy for Patient with Insomnia\u201d. In: 2022 IEEE 7th International conference for Convergence in Technology (I2CT). IEEE, pp. 1\u20135. Sun, Hao, Zhenru Lin, Chujie Zheng, Siyang Liu, and Minlie Huang (2021). \u201cPsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support\u201d. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, pp. 1489\u20131503. Tamrakar, Manoj Kumar and Abhishek Badholia (2022). \u201cScientific Study of Technological Chatbot Adoption in Customer Service\u201d. In: 2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC), pp. 1117\u20131123. Thelwall, Mike A (2001). \u201cA web crawler design for data mining\u201d. In: Journal of Information Science 27, pp. 319\u2013325. Tian, Fangyuan, Hongxia Li, Shuicheng Tian, Jie Yang, Jiang Shao, and Chenning Tian (2020). \u201cPsychological symptoms of ordinary Chinese citizens based on SCL-90 during the level I emergency response to COVID-19\u201d. In: Psychiatry research 288, p. 112992. Trivedi, Jitendra K, Himanshu Sareen, and Mohan Dhyani (2008). \u201cRapid urbanization-Its impact on mental health: A South Asian perspective\u201d. In: Indian journal of psychiatry 50.3, p. 161. Wang, Jiaqi, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, et al. (2023). \u201cPrompt engineering for healthcare: Methodologies and applications\u201d. In: arXiv preprint arXiv:2304.14670. Wang, Junjie, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang (2022). \u201cFengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence\u201d. In: CoRR abs/2209.02970. Wang, Xipei, Haoyu Zhang, Yuanbo Zhang, Meng Wang, Jiarui Song, Tin Lai, and Matloob Khushi (2022). \u201cLearning Non-Stationary Time-Series with Dynamic Pattern Extractions\u201d. In: IEEE Transactions on Artificial Intelligence (TAI) 3.5, pp. 778\u2013787. Xiang, Yu-Tao, Chee H Ng, Xin Yu, and Gang Wang (2018). \u201cRethinking progress and challenges of mental health care in China\u201d. In: World Psychiatry 17.2, p. 231. Xu, Liang, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. (2020). \u201cCLUE: A Chinese language understanding evaluation benchmark\u201d. In: arXiv preprint arXiv:2004.05986. Xu, Xiaoting, Tin Lai, Sayka Jahan, Farnaz Farid, and Abubakar Bello (2022). \u201cA Machine Learning Predictive Model to Detect Water Quality and Pollution\u201d. In: Machine Learning Perspective in the Convolutional Neural Network Era, Special Issue of Future Internet 14.11, p. 324. Yao, Zhang, W. D. Daling, Feng Shi, Zhang Yifei, and Leng Fangling (2012). \u201cAn Approach for Crawling Dynamic WebPages Based on Script Language Analysis\u201d. In: 2012 Ninth Web Information Systems and Applications Conference, pp. 35\u201338. Yurayat, Phamornpun and Supachai Tuklang (2023). \u201cUniversity Student Counselees\u2019 Attitudes and Experiences Towards Online Counseling During the Covid-19 Pandemic: A Mixed Methods Study.\u201d In: Journal of Higher Education Theory & Practice 23.4. Zeng, Wei, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jia-xin Yu, Qiwei Guo, Yue Yu, Yan Zhang, Jin Wang, Heng Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fan Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhengping Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng\n",
    "paper_type": "method",
    "attri": {
        "background": "The demand for psychological counselling has surged recently, especially due to the COVID-19 pandemic, which has highlighted the need for accessible mental health support. Traditional methods of psychological counselling are insufficient to meet this demand, prompting the need for innovative solutions like AI-based frameworks.",
        "problem": {
            "definition": "The problem addressed in this paper is the significant shortage of mental health professionals, particularly in urban areas, which has been exacerbated by the COVID-19 pandemic, leading to a growing mental health crisis.",
            "key obstacle": "The primary challenge is the high qualifications required for mental health professionals, combined with a burnout crisis in the field, creating a supply-demand gap for psychological support services."
        },
        "idea": {
            "intuition": "The inspiration for the Psy-LLM framework stems from the increasing reliance on technology for mental health support, particularly during times of crisis when human resources are limited.",
            "opinion": "The proposed idea is to develop an AI-powered framework that utilizes Large Language Models (LLMs) to provide immediate psychological support and screening for urgent cases, thus alleviating the burden on human professionals.",
            "innovation": "The key innovation of the Psy-LLM framework lies in its integration of pre-trained LLMs with professional Q&A data, allowing it to generate contextually relevant and coherent responses tailored to psychological inquiries."
        },
        "method": {
            "method name": "Psy-LLM",
            "method abbreviation": "Psy-LLM",
            "method definition": "Psy-LLM is an AI-based framework that leverages large language models to assist in psychological consultations by providing timely responses to users' queries.",
            "method description": "The Psy-LLM framework combines pre-trained language models with a curated dataset of psychological Q&A to facilitate online psychological consultations.",
            "method steps": [
                "Collect a large dataset of psychological articles and Q&A.",
                "Pre-train the language models on this dataset.",
                "Fine-tune the models for specific psychological consultation tasks.",
                "Deploy the model to a web interface for user interaction."
            ],
            "principle": "The effectiveness of the Psy-LLM method is underpinned by its ability to generate human-like responses based on a vast corpus of psychological knowledge, thus providing relevant support when human professionals are unavailable."
        },
        "experiments": {
            "evaluation setting": "The framework was evaluated using intrinsic metrics like perplexity and extrinsic metrics through human assessments of response helpfulness, fluency, relevance, and logic, based on a dedicated website for user interaction.",
            "evaluation method": "The evaluation involved measuring the model's performance using various metrics and conducting human evaluations to assess the quality of generated responses, comparing them to actual expert answers."
        },
        "conclusion": "The results indicate that the Psy-LLM framework is effective in generating coherent and relevant responses for psychological inquiries, demonstrating its potential to enhance mental health support through AI technologies.",
        "discussion": {
            "advantage": "The main advantage of the Psy-LLM framework is its ability to provide immediate and accessible psychological support, particularly during times of high demand when human resources are limited.",
            "limitation": "A significant limitation is that the model's responses may lack the depth and nuance of human interaction, which is critical in psychological counselling.",
            "future work": "Future research should focus on improving the model's contextual understanding, expanding the dataset quality, and integrating additional features like emotion recognition to enhance user experience."
        },
        "other info": {
            "info1": "The framework is designed to operate as both a front-end tool for professionals and a standalone support system for patients.",
            "info2": {
                "info2.1": "The models used include PanGu and WenZhong, both of which are large-scale pre-trained language models.",
                "info2.2": "The dataset for training included over 400,000 samples collected from various sources, ensuring a diverse range of psychological content."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The demand for psychological counselling has surged recently, especially due to the COVID-19 pandemic, which has highlighted the need for accessible mental health support."
        },
        {
            "section number": "1.2",
            "key information": "The proposed idea is to develop an AI-powered framework that utilizes Large Language Models (LLMs) to provide immediate psychological support and screening for urgent cases."
        },
        {
            "section number": "2.1",
            "key information": "Psy-LLM is an AI-based framework that leverages large language models to assist in psychological consultations by providing timely responses to users' queries."
        },
        {
            "section number": "3.2",
            "key information": "The key innovation of the Psy-LLM framework lies in its integration of pre-trained LLMs with professional Q&A data, allowing it to generate contextually relevant and coherent responses tailored to psychological inquiries."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of the Psy-LLM method is underpinned by its ability to generate human-like responses based on a vast corpus of psychological knowledge."
        },
        {
            "section number": "10.1",
            "key information": "A significant limitation is that the model's responses may lack the depth and nuance of human interaction, which is critical in psychological counselling."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on improving the model's contextual understanding, expanding the dataset quality, and integrating additional features like emotion recognition to enhance user experience."
        }
    ],
    "similarity_score": 0.7251026333794157,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Psy-LLM_ Scaling up Global Mental Health Psychological Services with AI-based Large Language Models.json"
}