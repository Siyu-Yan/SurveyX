{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.19592",
    "title": "Why Larger Language Models Do In-context Learning Differently?",
    "abstract": "Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of incontext learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer singlehead linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.",
    "bib_name": "shi2024largerlanguagemodelsincontext",
    "md_text": "# Why Larger Language Models Do In-context Learning Differently?\nZhenmei Shi 1 Junyi Wei 1 Zhuoyan Xu 1 Yingyu Liang 1 2\n# Zhenmei Shi 1 Junyi Wei 1 Zhuoyan Xu 1 Yingyu Liang 1 2\n# Abstract\nLarge language models (LLM) have emerged as a powerful tool for AI, with the key ability of incontext learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer singlehead linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.\narXiv:2405.19592v1\n# 1. Introduction\nAs large language models (LLM), e.g., ChatGPT (OpenAI, 2022) and GPT4 (OpenAI, 2023), are transforming AI development with potentially profound impact on our societies, it is critical to understand their mechanism for safe and efficient deployment. An important emergent ability (Wei et al., 2022b; An et al., 2023), which makes LLM successful, is in-context learning (ICL), where models are given a few exemplars of input\u2013label pairs as part of the prompt\n1University of Wisconsin-Madison, 2The University of Hong Kong. Correspondence to: Zhenmei Shi, Yingyu Liang <zhmeishi, yliang@cs.wisc.edu, yingyul@hku.hk>.\n1University of Wisconsin-Madison, 2The University of Hong Kong. Correspondence to: Zhenmei Shi, Yingyu Liang <zhmeishi, yliang@cs.wisc.edu, yingyul@hku.hk>.\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\nbefore evaluating some new input. More specifically, ICL is a few-shot (Brown et al., 2020) evaluation method without updating parameters in LLM. Surprisingly, people find that, through ICL, LLM can perform well on tasks that have never been seen before, even without any finetuning. It means LLM can adapt to wide-ranging downstream tasks under efficient sample and computation complexity. The mechanism of ICL is different from traditional machine learning, such as supervised learning and unsupervised learning. For example, in neural networks, learning usually occurs in gradient updates, whereas there is only a forward inference in ICL and no gradient updates. Several recent works, trying to answer why LLM can learn in-context, argue that LLM secretly performs or simulates gradient descent as meta-optimizers with just a forward pass during ICL empirically (Dai et al., 2022; Von Oswald et al., 2023; Malladi et al., 2023) and theoretically (Zhang et al., 2023b; Ahn et al., 2023; Mahankali et al., 2023; Cheng et al., 2023; Bai et al., 2023; Huang et al., 2023; Li et al., 2023b; Guo et al., 2024; Wu et al., 2024). Although some insights have been obtained, the mechanism of ICL deserves further research to gain a better understanding.\nRecently, there have been some important and surprising observations (Min et al., 2022; Pan et al., 2023; Wei et al., 2023b; Shi et al., 2023a) that cannot be fully explained by existing studies. In particular, Shi et al. (2023a) finds that LLM is not robust during ICL and can be easily distracted by an irrelevant context. Furthermore, Wei et al. (2023b) shows that when we inject noise into the prompts, the larger language models may have a worse ICL ability than the small language models, and conjectures that the larger language models may overfit into the prompts and forget the prior knowledge from pretraining, while small models tend to follow the prior knowledge. On the other hand, Min et al. (2022); Pan et al. (2023) demonstrate that injecting noise does not affect the in-context learning that much for smaller models, which have a more strong pretraining knowledge bias. To improve the understanding of the ICL mechanism, to shed light on the properties and inner workings of LLMs, and to inspire efficient and safe use of ICL, we are interested in the following question:\nWhy do larger language models do in-context learning differently?\nWhy do larger language models do in-context learning differently?\nTo answer this question, we study two settings: (1) onelayer single-head linear self-attention network (Schlag et al., 2021; Von Oswald et al., 2023; Akyurek et al., 2023; Ahn et al., 2023; Zhang et al., 2023b; Mahankali et al., 2023; Wu et al., 2024) pretrained on linear regression in-context tasks (Garg et al., 2022; Raventos et al., 2023; Von Oswald et al., 2023; Akyurek et al., 2023; Bai et al., 2023; Mahankali et al., 2023; Zhang et al., 2023b; Ahn et al., 2023; Li et al., 2023c; Huang et al., 2023; Wu et al., 2024), with rank constraint on the attention weight matrices for studying the effect of the model scale; (2) two-layer multiple-head transformers (Li et al., 2023b) pretrained on sparse parity classification in-context tasks, comparing small or large head numbers for studying the effect of the model scale. In both settings, we give the closed-form optimal solutions. We show that smaller models emphasize important hidden features while larger models cover more features, e.g., less important features or noisy features. Then, we show that smaller models are more robust to label noise and input noise during evaluation, while larger models may easily be distracted by such noises, so larger models may have a worse ICL ability than smaller ones. We also conduct in-context learning experiments on five prevalent NLP tasks utilizing various sizes of the Llama model families (Touvron et al., 2023a;b), whose results are consistent with previous work (Min et al., 2022; Pan et al., 2023; Wei et al., 2023b) and our analysis.\n# Our contributions and novelty over existing work:\n\u2022 We formalize new stylized theoretical settings for studying ICL and the scaling effect of LLM. See Section 4 for linear regression and Section 5 for parity.\n\u2022 We characterize the optimal solutions for both settings (Theorem 4.1 and Theorem 5.1).\n\u2022 The characterizations of the optimal elucidate different attention paid to different hidden features, which then leads to the different ICL behavior (Theorem 4.2, Theorem 4.3, Theorem 5.2).\n\u2022 We further provide empirical evidence on large base and chat models corroborating our theoretical analysis (Figure 1, Figure 2).\nNote that previous ICL analysis paper may only focus on (1) the approximation power of transformers (Garg et al., 2022; Panigrahi et al., 2023; Guo et al., 2024; Bai et al., 2023; Cheng et al., 2023), e.g., constructing a transformer by hands which can do ICL, or (2) considering one-layer single-head linear self-attention network learning ICL on linear regression (Von Oswald et al., 2023; Akyurek et al., 2023; Mahankali et al., 2023; Zhang et al., 2023b; Ahn et al., 2023; Wu et al., 2024), and may not focus on the robustness\nanalysis or explain the different behaviors. In this work, (1) we extend the linear model linear data analysis to the non-linear model and non-linear data setting, i.e., two-layer multiple-head transformers leaning ICL on sparse parity classification and (2) we have a rigorous behavior difference analysis under two settings, which explains the empirical observations and provides more insights into the effect of attention mechanism in ICL.\n# 2. Related Work\nLarge language model. Transformer-based (Vaswani et al., 2017) neural networks have rapidly emerged as the primary machine learning architecture for tasks in natural language processing. Pretrained transformers with billions of parameters on broad and varied datasets are called large language models (LLM) or foundation models (Bommasani et al., 2021), e.g., BERT (Devlin et al., 2019), PaLM (Chowdhery et al., 2022), Llama(Touvron et al., 2023a), ChatGPT (OpenAI, 2022), GPT4 (OpenAI, 2023) and so on. LLM has shown powerful general intelligence (Bubeck et al., 2023) in various downstream tasks. To better use the LLM for a specific downstream task, there are many adaptation methods, such as adaptor (Hu et al., 2022; Zhang et al., 2023c; Gao et al., 2023; Shi et al., 2023b), calibration (Zhao et al., 2021; Zhou et al., 2023a), multitask finetuning (Gao et al., 2021b; Xu et al., 2023; Von Oswald et al., 2023; Xu et al., 2024b), prompt tuning (Gao et al., 2021a; Lester et al., 2021), instruction tuning (Li & Liang, 2021; Chung et al., 2022; Mishra et al., 2022), symbol tuning (Wei et al., 2023a), black-box tuning (Sun et al., 2022), chain-of-thoughts (Wei et al., 2022c; Khattab et al., 2022; Yao et al., 2023; Zheng et al., 2024), scratchpad (Nye et al., 2021), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and many so on.\nIn-context learning. One important emergent ability (Wei et al., 2022b) from LLM is in-context learning (ICL) (Brown et al., 2020). Specifically, when presented with a brief series of input-output pairings (known as a prompt) related to a certain task, they can generate predictions for test scenarios without necessitating any adjustments to the model\u2019s parameters. ICL is widely used in broad scenarios, e.g., reasoning (Zhou et al., 2022), negotiation (Fu et al., 2023), self-correction (Pourreza & Rafiei, 2023), machine translation (Agrawal et al., 2022) and so on. Many works trying to improve the ICL and zero-shot ability of LLM (Min et al., 2021; Wang et al., 2022; Wei et al., 2022a; Iyer et al., 2022). There is a line of insightful works to study the mechanism of transformer learning (Geva et al., 2021; Xie et al., 2022; Garg et al., 2022; Jelassi et al., 2022; Arora & Goyal, 2023; Li et al., 2023a;d; Allen-Zhu & Li, 2023; Luo et al., 2023; Tian et al., 2023a;b; Zhou et al., 2023b; Bietti et al., 2023; Xu et al., 2024a; Gu et al., 2024a;b;c;d;e) and in-context\nlearning (Dai et al., 2022; Mahankali et al., 2023; Raventos et al., 2023; Bai et al., 2023; Ahn et al., 2023; Von Oswald et al., 2023; Pan et al., 2023; Li et al., 2023b;c;e; Akyurek et al., 2023; Zhang et al., 2023a;b; Huang et al., 2023; Cheng et al., 2023; Wibisono & Wang, 2023; Wu et al., 2024; Guo et al., 2024; Reddy, 2024) empirically and theoretically. On the basis of these works, our analysis takes a step forward to show the ICL behavior difference under different scales of language models.\n# 3. Preliminary\nNotations. We denote [n] := {1, 2, . . . , n}. For a positive semidefinite matrix A, we denote \u2225x\u22252 A := x\u22a4Ax as the norm induced by a positive definite matrix A. We denote \u2225\u00b7 \u2225F as the Frobenius norm. diag() function will map a vector to a diagonal matrix or map a matrix to a vector with its diagonal terms.\nIn-context learning. We follow the setup and notation of the problem in Zhang et al. (2023b); Mahankali et al. (2023); Ahn et al. (2023); Huang et al. (2023); Wu et al. (2024). In the pretraining stage of ICL, the model is pretrained on prompts. A prompt from a task \u03c4 is formed by N examples (x\u03c4,1, y\u03c4,1), . . . , (x\u03c4,N, y\u03c4,N) and a query token x\u03c4,q for prediction, where for any i \u2208[N] we have y\u03c4,i \u2208R and x\u03c4,i, x\u03c4,q \u2208Rd. The embedding matrix E\u03c4, the label vector y\u03c4, and the input matrix X\u03c4 are defined as:\nGiven prompts represented as E\u03c4\u2019s and the corresponding true labels y\u03c4,q\u2019s, the pretraining aims to find a model whose output on E\u03c4 matches y\u03c4,q. After pretraining, the evaluation stage applies the model to a new test prompt (potentially from a different task) and compares the model output to the true label on the query token. Note that our pretraining stage is also called learning to learn in-context (Min et al., 2021) or in-context training warmup (Dong et al., 2022) in existing work. Learning to learn in-context is the first step to understanding the mechanism of ICL in LLM following previous works (Raventos et al., 2023; Zhou et al., 2023b; Zhang et al., 2023b; Mahankali et al., 2023; Ahn et al., 2023; Huang et al., 2023; Li et al., 2023b; Wu et al., 2024). Linear self-attention networks. The linear self-attention network has been widely studied (Schlag et al., 2021; Von Oswald et al., 2023; Akyurek et al., 2023; Ahn et al., 2023; Zhang et al., 2023b; Mahankali et al., 2023; Wu et al., 2024; Ahn et al., 2024), and will be used as the learning model or a component of the model in our two theoretical\nsettings. It is defined as:\nwhere \u03b8 = (WP V , WKQ), E \u2208R(d+1)\u00d7(N+1) is the embedding matrix of the input prompt, and \u03c1 is a normalization factor set to be the length of examples, i.e., \u03c1 = N during pretraining. Similar to existing work, for simplicity, we have merged the projection and value matrices into WP V , and merged the key and query matrices into WKQ, and have a residual connection in our LSA network. The prediction of the network for the query token x\u03c4,q will be the bottom right entry of the matrix output, i.e., the entry at location (d + 1), (N + 1), while other entries are not relevant to our study and thus are ignored. So only part of the model parameters are relevant. To see this, let us denote\nwhere WP V 11 , WKQ 11 \u2208Rd\u00d7d; wP V 12 , wP V 21 , wKQ 12 , wKQ 21 \u2208 Rd; and wP V 22 , wKQ 22 \u2208R. Then the prediction is:\n(2)\n# 4. Linear Regression\nIn this section, we consider the linear regression task for incontext learning which is widely studied empirically (Garg et al., 2022; Raventos et al., 2023; Von Oswald et al., 2023; Akyurek et al., 2023; Bai et al., 2023) and theoretically (Mahankali et al., 2023; Zhang et al., 2023b; Ahn et al., 2023; Li et al., 2023c; Huang et al., 2023; Wu et al., 2024).\nIn this section, we consider the linear regression task for incontext learning which is widely studied empirically (Garg et al., 2022; Raventos et al., 2023; Von Oswald et al., 2023; Akyurek et al., 2023; Bai et al., 2023) and theoretically (Mahankali et al., 2023; Zhang et al., 2023b; Ahn et al., 2023; Li et al., 2023c; Huang et al., 2023; Wu et al., 2024). Data and task. For each task \u03c4, we assume for any i \u2208[N] tokens x\u03c4,i, x\u03c4,q i.i.d. \u223cN(0, \u039b), where \u039b is the covariance matrix. We also assume a d-dimension task weight w\u03c4 i.i.d. \u223c N(0, Id\u00d7d) and the labels are given by y\u03c4,i = \u27e8w\u03c4, x\u03c4,i\u27e9 and y\u03c4,q = \u27e8w\u03c4, x\u03c4,q\u27e9. Model and loss. We study a one-layer single-head linear self-attention transformer (LSA) defined in Equation (1) and we use \ufffdy\u03c4,q := fLSA,\u03b8(E)(d+1),(N+1) as the prediction. We consider the mean square error (MSE) loss so that the empirical risk over B independent prompts is defined as\n\ufffd \ufffd Measure model scale by rank. We first introduce a lemma from previous work that simplifies the MSE and justifies our\nmeasurement of the model scale. For notation simplicity, we denote U = WKQ 11 , u = wP V 22 . Lemma 4.1 (Lemma A.1 in Zhang et al. (2023b)). Let \u0393 := \ufffd 1 + 1 N \ufffd \u039b + 1 N tr(\u039b)Id\u00d7d \u2208Rd\u00d7d. Let\nLemma 4.1 (Lemma A.1 in Zhang et al. (2023b)). Let \u0393 := \ufffd 1 + 1 N \ufffd \u039b + 1 N tr(\u039b)Id\u00d7d \u2208Rd\u00d7d. Let\nwe have L(fLSA,\u03b8) = \u02dc\u2113(U, u) + C, where C is a constant independent with \u03b8.\nLemma 4.1 tells us that the loss only depends on uU. If we consider non-zero u, w.l.o.g, letting u = 1, then we can see that the loss only depends on U \u2208Rd\u00d7d,\nNote that U = WKQ 11 , then it is natural to measure the size of the model by rank of U. Recall that we merge the key matrix and the query matrix in attention together, i.e., WKQ = (WK)\u22a4WQ. Thus, a low-rank U is equivalent to the constraint WK, WQ \u2208Rr\u00d7d where r \u226ad. The low-rank key and query matrix are practical and have been widely studied (Hu et al., 2022; Chen et al., 2021; Bhojanapalli et al., 2020; Fan et al., 2021; Dass et al., 2023; Shi et al., 2023c). Therefore, we use r = rank(U) to measure the scale of the model, i.e., larger r representing larger models. To study the behavior difference under different model scale, we will analyze U under different rank constraints.\n# 4.1. Low Rank Optimal Solution\nSince the token covariance matrix \u039b is positive semidefinite symmetric, we have eigendecomposition \u039b = QDQ\u22a4, where Q is an orthonormal matrix containing eigenvectors of \u039b and D is a sorted diagonal matrix with nonnegative entries containing eigenvalues of \u039b, denoting as D = diag([\u03bb1, . . . , \u03bbd]), where \u03bb1 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd \u22650. Then, we have the following theorem.\nTheorem 4.1 (Optimal rank-r solution for regression).\nRecall the loss function \u02dc\u2113in Lemma 4.1. Let\nU\u2217, u\u2217=\nargmin\nU\u2208Rd\u00d7d,rank(U)\u2264r,u\u2208R\n\u02dc\u2113(U, u).\nThen U\u2217= cQV\u2217Q\u22a4, u = 1\nc, where c is any nonzero\nconstant, and V\u2217= diag([v\u2217\n1, . . . , v\u2217\nd]) satisfies for any\ni \u2264r, v\u2217\ni =\nN\n(N+1)\u03bbi+tr(D) and for any i > r, v\u2217\ni = 0.\nWe now formalize our insight into the behavior difference based on our analysis on the optimal solutions. We consider the evaluation prompt to have M examples (may not be equal to the number of examples N during pretraining for a general evaluation setting), and assume noise in labels to\nProof sketch of Theorem 4.1. We defer the full proof to Appendix B.1. The proof idea is that we can decompose the loss function into different ranks, so we can keep the direction by their sorted \u201cvariance\u201d, i.e.,\nwhere Ti = \ufffd 1 + 1 N \ufffd \u03bbi + tr(D) N . We have that v\u2217 i \u22650 for any i \u2208[d] and if v\u2217 i > 0, we have v\u2217 i = 1 Ti . Denote \ufffd \ufffd\n\n\ufffd \ufffd g(x) is an increasing function on [0, \u221e).\nTheorem 4.1 gives the closed-form optimal rank-r solution of one-layer single-head linear self-attention transformer learning linear regression ICL tasks. Let fLSA,\u03b8 denote the optimal rank-r solution corresponding to the U\u2217, u\u2217above. In detail, the optimal rank-r solution fLSA,\u03b8 satisfies\nWhat hidden features does the model pay attention to? Theorem 4.1 shows that the optimal rank-r solution indeed is the truncated version of the optimal full-rank solution, keeping only the most important feature directions (i.e., the first r eigenvectors of the token covariance matrix). In detail, (1) for the optimal full-rank solution, we have for any i \u2208[d], v\u2217 i = N (N+1)\u03bbi+tr(D); (2) for the optimal rank-r solution, we have for any i \u2264r, v\u2217 i = N (N+1)\u03bbi+tr(D) and for any i > r, v\u2217 i = 0. That is, the small rank-r model keeps only the first r eigenvectors (viewed as hidden feature directions) and does not cover the others, while larger ranks cover more hidden features, and the large full rank model covers all features.\nRecall that the prediction depends on U\u2217x\u03c4,q = cQV\u2217Q\u22a4x\u03c4,q; see Equation (2) and (3). So the optimal rank-r model only uses the components on the first r eigenvector directions to do the prediction in evaluations. When there is noise distributed in all directions, a smaller model can ignore noise and signals along less important directions but still keep the most important directions. Then it can be less sensitive to the noise, as empirically observed. This insight is formalized in the next subsection.\n# 4.2. Behavior Difference\nfacilitate the study of the behavior difference (our results can be applied to the noiseless case by considering noise level \u03c3 = 0). Formally, the evaluation prompt is:\nwhere w is the weight for the evaluation task, and for any i \u2208[M], the label noise \u03f5i i.i.d. \u223cN(0, \u03c32). Recall Q are eigenvectors of \u039b, i.e., \u039b = QDQ\u22a4and D = diag([\u03bb1, . . . , \u03bbd]). In practice, we can view the large variance part of x (top r directions in Q) as a useful signal (like words \u201cpositive\u201d, \u201cnegative\u201d), and the small variance part (bottom d \u2212r directions in Q) as the less important or useless information (like words \u201ceven\u201d, \u201cjust\u201d). Based on such intuition, we can decompose the evaluation task weight w accordingly: w = Q(s+\u03be), where the r-dim truncated vector s \u2208Rd has si = 0 for any r < i \u2264d, and the residual vector \u03be \u2208Rd has \u03bei = 0 for any 1 \u2264i \u2264r. The following theorem (proved in Appendix B.2) quantifies the evaluation loss at different model scales r which can explain the scale\u2019s effect.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a9f/2a9fca0b-8856-4227-8b5a-820b2342abc7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Implications. If N is large enough with N\u03bbr \u226btr(D) (which is practical as we usually pretrain networks on long text), then</div>\n \ufffd The first term \u2225\u03be\u22252 D is due to the residual features not covered by the network, so it decreases for larger r and becomes 0 for full-rank r = d. The second term 1 M (\u00b7) is significant since we typically have limited examples in evaluation, e.g., M = 16 \u226aN. Within it, (r + 1)\u2225s\u22252 D corresponds to the first r directions, and r\u03c32 corresponds to the label noise. These increase for larger r. So there is a trade-off between the two error terms when scaling up the model: for larger\nr the first term decreases while the second term increases. This depends on whether more signals are covered or more noise is kept when increasing the rank r. To further illustrate the insights, we consider the special case when the model already covers all useful signals in the evaluation task: w = Qs, i.e., the label only depends on the top r features (like \u201cpositive\u201d, \u201cnegative\u201d tokens). Our above analysis implies that a larger model will cover more useless features and keep more noise, and thus will have worse performance. This is formalized in the following theorem (proved in Appendix B.2).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c97/3c970fa7-6a35-4f89-8717-559282183114.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Implications. By Theorem 4.3, in this case,</div>\nL(f2; \ufffdE) \u2212L(f1; \ufffdE) \u2248r\u2032 \u2212r M \u2225s\u22252 D \ufffd \ufffd\ufffd \ufffd input noise + r\u2032 \u2212r M \u03c32 \ufffd \ufffd\ufffd \ufffd label noise .\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd We can decompose the above equation to input noise and label noise, and we know that \u2225s\u22252 D + \u03c32 only depends on the intrinsic property of evaluation data and is independent of the model size. When we have a larger model (larger r\u2032), we will have a larger evaluation loss gap between the large and small models. It means larger language models may be easily affected by the label noise and input noise and may have worse in-context learning ability, while smaller models may be more robust to these noises as they only emphasize important signals. Moreover, if we increase the label noise scale \u03c32 on purpose, the larger models will be more sensitive to the injected label noise. This is consistent with the observation in Wei et al. (2023b); Shi et al. (2023a) and our experimental results in Section 6.\n# 5. Sparse Parity Classification\nWe further consider a more sophisticated setting with nonlinear data which necessitates nonlinear models. Viewing sentences as generated from various kinds of thoughts and knowledge that can be represented as vectors in some hidden feature space, we consider the classic data model of dictionary learning or sparse coding, which has been widely used for text and images (Olshausen & Field, 1997; Vinje & Gallant, 2000; Blei et al., 2003). Furthermore, beyond linear separability, we assume the labels are given by the\n(d, 2)-sparse parity on the hidden feature vector, which is the high-dimensional generalization of the classic XOR problem. Parities are a canonical family of highly non-linear learning problems and recently have been used in many recent studies on neural network learning (Daniely & Malach, 2020; Barak et al., 2022; Shi et al., 2022; 2023d).\n(d, 2)-sparse parity on the hidden feature vector, which is the high-dimensional generalization of the classic XOR problem. Parities are a canonical family of highly non-linear learning problems and recently have been used in many recent studies on neural network learning (Daniely & Malach, 2020; Barak et al., 2022; Shi et al., 2022; 2023d). Data and task. Let X = Rd be the input space, and Y = {\u00b11} be the label space. Suppose G \u2208Rd\u00d7d is an unknown dictionary with d columns that can be regarded as features; for simplicity, assume G is orthonormal. Let \u03d5 \u2208{\u00b11}d be a hidden vector that indicates the presence of each feature. The data are generated as follows: for each task \u03c4, generate two task indices t\u03c4 = (i\u03c4, j\u03c4) which determines a distribution T\u03c4; then for this task, draw examples by \u03d5 \u223cT\u03c4, and setting x = G\u03d5 (i.e., dictionary learning data), y = \u03d5i\u03c4 \u03d5j\u03c4 (i.e., XOR labels). We now specify how to generate t\u03c4 and \u03d5. As some of the hidden features are more important than others, we let A = [k] denote a subset of size k corresponding to the important features. We denote the important task set as S1 := A \u00d7 A \\ {(l, l) : l \u2208A} and less important task set as S2 := [d] \u00d7 [d] \\ ({(l, l) : l \u2208[d]} \u222aS1). Then t\u03c4 is drawn uniformly from S1 with probability 1 \u2212pT , and uniformly from S2 with probability pT , where pT \u2208[0, 1 2) is the less-important task rate. For the distribution of \u03d5, we assume \u03d5[d]\\{i\u03c4 ,j\u03c4 } is drawn uniformly from {\u00b11}d\u22122, and assume \u03d5{i\u03c4 ,j\u03c4 } has good correlation (measured by a parameter \u03b3 \u2208(0, 1 4)) with the label to facilitate learning. Independently, we have\n\uf8ed \uf8f8 Suppose N \u2192\u221eand let the optimal solution of L\u03bb(g) be g\u2217= argmin g lim \u03bb\u21920+ L\u03bb(g).\nNote that without correlation (\u03b3 = 0), it is well-known sparse parities will be hard to learn, so we consider \u03b3 > 0. Model. Following Wu et al. (2024), we consider the reduced linear self-attention fLSA,\u03b8(X, y, xq) = y\u22a4X N WKQxq (which is a reduced version of Equation (1)), and also denote WKQ as W for simplicity. It is used as the neuron in our two-layer multiple-head transformers:\nwhere \u03c3 is ReLU activation, a = [a1, . . . , am]\u22a4 \u2208 [\u22121, 1]m, W(i) \u2208Rd\u00d7d and m is the number of attention heads. Denote its parameters as \u03b8 = (a, W(1), . . . , W(m)). This model is more complicated as it uses non-linear activation, and also has two layers with multiple heads.\nMeasure model scale by head number. We use the attention head number m to measure the model scale, as a larger m means the transformer can learn more attention patterns. We consider hinge loss \u2113(z) = max(0, 1 \u2212z), and the population loss with weight-decay regularization:\nWe first introduce some notations to describe the optimal. Let bin(\u00b7) be the integer to binary function, e.g., bin(6) = 110. Let digit(z, i) denote the digit at the i-th position (from right to left) of z, e.g., digit(01000, 4) = 1. We are now ready to characterize the optimal solution (proved in Appendix C.1).\nTheorem 5.1 (Optimal solution for parity). Consider\nk = 2\u03bd1, d = 2\u03bd2, and let g\u2217\n1 and g\u2217\n2 denote the optimal\nsolutions for m = 2(\u03bd1 + 1) and m = 2(\u03bd2 + 1),\nrespectively.\nWhen 0 < pT <\n1\n4 \u2212\u03b3\nd(d\u22121)\n2\n( 1\n4 +\u03b3)+ 1\n4 \u2212\u03b3 , g\u2217\n1 neurons are a\nsubset of g\u2217\n2 neurons. Specifically, for any i \u2208[2(\u03bd2 + 1)],\nlet V\u2217,(i) be diagonal matrix and\n\u2022 For any i \u2208[\u03bd2] and i\u03c4 \u2208[d], let a\u2217\ni = \u22121 and\nV\u2217,(i)\ni\u03c4 ,i\u03c4 = (2 digit(bin(i\u03c4 \u22121), i) \u22121)/(4\u03b3).\n\u2022 For i = \u03bd2 + 1 and any i\u03c4 \u2208[d], let a\u2217\ni = +1 and\nV\u2217,(i)\ni\u03c4 ,i\u03c4 = \u2212\u03bdj/(4\u03b3) for g\u2217\nj .\n\u2022 For i \u2208[2(\u03bd2 + 1)] \\ [\u03bd2 + 1], let a\u2217\ni = a\u2217\ni\u2212\u03bd2\u22121 and\nV\u2217,(i) = \u2212V\u2217,(i\u2212\u03bd2\u22121).\nLet W\u2217,(i) = GV\u2217,(i)G\u22a4. Up to permutations, g\u2217\n2 has\nneurons (a\u2217, W\u2217,(1), . . . , W\u2217,(m)) and g\u2217\n1 has the\n{1, . . . , \u03bd1, \u03bd2 + 1, \u03bd2 + 2 . . . , \u03bd2 + \u03bd1 + 1, 2\u03bd2 + 2}-th\nneurons of g\u2217\n2.\nProof sketch of Theorem 5.1. The proof is challenging as the non-linear model and non-linear data. We defer the full proof to Appendix C.1. The high-level intuition is transferring the optimal solution to patterns covering problems. For small pT , the model will \u201cprefer\u201d to cover all patterns in S1 first. When the model becomes larger, by checking the sufficient and necessary conditions, it will continually learn to cover non-important features. Thus, the smaller model will mainly focus on important features, while the larger model will focus on all features.\nBy carefully checking, we can see that the neurons in g\u2217 1 (i.e., the {1, 4, 5, 8}-th neurons of g\u2217 2) are used for parity classification task from S1, i.e, label determined by the first k = 2\u03bd1 = 2 dimensions. With the other neurons (i.e., the {2, 3, 6, 7}-th neurons of g\u2217 2), g\u2217 2 can further do parity classification on the task from S2, label determined by any two dimensions other than the first two dimensions.\n# What hidden features does the model pay attention to?\nTheorem 5.1 gives the closed-form optimal solution of twolayer multiple-head transformers learning sparse-parity ICL tasks. It shows the optimal solution of the smaller model indeed is a sub-model of the larger optimal model. In detail, the smaller model will mainly learn all important features, while the larger model will learn more features. This again shows a trade-off when increasing the model scale: larger models can learn more hidden features which can be beneficial if these features are relevant to the label, but also potentially keep more noise which is harmful.\nSimilar to Theorem 4.3, to illustrate our insights, we will consider a setting where the smaller model learns useful features for the evaluation task while the larger model covers extra features. That is, for evaluation, we uniformly draw a task t\u03c4 = (i\u03c4, j\u03c4) from S1, and then draw M samples to form the evaluation prompt in the same way as during pretraining. To present our theorem (proved in Appendix C.2 using Theorem 5.1), we introduce some notations. Let\nwhere for any i \u2208[2(\u03bd2 + 1)], V\u2217,(i) is defined in Theorem 5.1. Let \u02c6\u03d5\u03c4,q \u2208Rd satisfy \u02c6\u03d5\u03c4,q,i\u03c4 = \u03d5\u03c4,q,i\u03c4 , \u02c6\u03d5\u03c4,q,j\u03c4 = \u03d5\u03c4,q,j\u03c4 and all other entries being zero. For a matrix Z and a vector v, let PZ denote the projection of v to the space of Z, i.e., PZ(v) = Z(Z\u22a4Z)\u22121Z\u22a4v.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6b0e/6b0e9466-743f-4622-af6c-2b53b0e2824a.png\" style=\"width: 50%;\"></div>\nImplications. Theorem 5.2 shows that during evaluation, we can decompose the input into two parts: signal and noise. Both the larger model and smaller model can capture the signal part well. However, the smaller model has a much smaller influence from noise than the larger model, i.e., the ratio is \u03bd1+1 \u03bd2+1. The reason is that smaller models emphasize important hidden features while larger ones cover more hidden features, and thus, smaller models are more robust to noise while larger ones are easily distracted, leading to different ICL behaviors. This again sheds light on where transformers pay attention to and how that affects ICL.\nRemark 5.1. Here, we provide a detailed intuition about Theorem 5.2. \u039e is the input noise. When we only care about the noise part, we can rewrite the smaller model as g1 = h(\u03b81, PD1(\u039e)), and the larger model as g2 = h(\u03b82, PD2(\u039e)), where they share the same h function. Our conclusion says that E[\u2225PD1(\u039e)\u22252 2]/E[\u2225PD2(\u039e)\u22252 2] = (\u03bd1 +1)/(\u03bd2 +1), which means the smaller model\u2019s \u201ceffect\u201d input noise is smaller than the larger model\u2019s \u201ceffect\u201d input noise. Although their original input noise is the same, as the smaller model only focuses on limited features, the smaller model will ignore part of the noise, and the \u201ceffect\u201d input noise is small. However, the larger model is the opposite.\n# 6. Experiments\nBrilliant recent work (Wei et al., 2023b) runs intensive and thorough experiments to show that larger language models do in-context learning differently. Following their idea, we conduct similar experiments on binary classification datasets, which is consistent with our problem setting in the parity case, to support our theory statements.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebf5/ebf547fd-4eb2-4f2a-8099-3871893f7896.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different datasets and model families (chat/with instruct turning). Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs with different random seeds for each evaluation, using M = 16 in-context exemplars.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f56c/f56c7ef1-17cb-479b-a354-3b03510dba02.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. Larger models are easier to be affected by noise (flipped labels) and override pretrained biases than smaller models for different datasets and model families (original/without instruct turning). Accuracy is calculated over 1000 evaluation prompts per dataset and over 5 runs with different random seeds for each evaluation, using M = 16 in-context exemplars.</div>\nsentence: show us a good time The answer is positive. sentence: as dumb and cheesy The answer is negative. sentence: it \u2019s a charming and often affecting journey The answer is\nExperimental setup. Following the experimental protocols in Wei et al. (2023b); Min et al. (2022), we conduct experiments on five prevalent NLP tasks, leveraging datasets from GLUE (Wang et al., 2018) tasks and Subj (Conneau & Kiela, 2018). Our experiments utilize various sizes of the Llama model families (Touvron et al., 2023a;b): 3B, 7B, 13B, 70B. We follow the prior work on in-context learning (Wei et al., 2023b) and use M = 16 in-context exemplars. We aim to assess the models\u2019 ability to use inherent semantic biases from pretraining when facing in-context examples. As part of this experiment, we introduce noise by inverting an escalating percentage of in-context example labels. To illustrate, a 100% label inversion for the SST-2 dataset implies that every \u201cpositive\u201d exemplar is now labeled \u201cnegative\u201d. Note that while we manipulate the in-context example labels, the evaluation sample labels remain consistent. We use the same templates as (Min et al., 2021), a sample evaluation for SST-2 when M = 2:\n# 6.1. Behavior Difference\nFigure 1 shows the result of model performance (chat/with instruct turning) across all datasets with respect to the proportion of labels that are flipped. When 0% label flips, we observe that larger language models have better in-context\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e4f0/e4f0fdf9-5d1c-4105-afe0-4ddf413ad61d.png\" style=\"width: 50%;\"></div>\nFigure 3. The magnitude of attention between the labels and input sentences in Llama 2-13b and 70b on 100 evaluation prompts; see the main text for the details. x-axis: indices of the prompts. y-axis: the norm of the last row of attention maps in the final layer. Correct: original label; wrong: flipped label; relevant: original input sentence; irrelevant: irrelevant sentence from other datasets. The results show that larger models focus on both sentences, while smaller models only focus on relevant sentences.\nabilities. On the other hand, the performance decrease facing noise is more significant for larger models. As the percentage of label alterations increases, which can be viewed as increasing label noise \u03c32, the performance of small models remains flat and seldom is worse than random guessing while large models are easily affected by the noise, as predicted by our analysis. These results indicate that large models can override their pretraining biases in-context inputlabel correlations, while small models may not and are more robust to noise. This observation aligns with the findings in Wei et al. (2023b) and our analysis. We can see a similar or even stronger phenomenon in Figure 2: larger models are more easily affected by noise (flipped labels) and override pretrained biases than smaller models for the original/without instruct turning version (see the \u201cAverage\u201d sub-figure). On the one hand, we conclude that both large base models and large chat models suffer from ICL robustness issues. On the other hand, this is also consistent with recent work suggesting that instruction tuning will impair LLM\u2019s in-context learning capability.\n# 6.2. Ablation Study\nTo further verify our analysis, we provide an ablation study. We concatenate an irrelevant sentence from GSM-IC (Shi et al., 2023a) to an input-label pair sentence from SST-2 in GLUE dataset. We use \u201ccorrect\u201d to denote the original label and \u201cwrong\u201d to denote the flipped label. Then, we measure the magnitude of correlation between labelinput, by computing the norm of the last row of attention\nmaps across all heads in the final layer. We do this between \u201ccorrect\u201d/\u201cwrong\u201d labels and the original/irrelevant inserted sentences. Figure 3 shows the results on 100 evaluation prompts; for example, the subfigure Correct+Relevant shows the correlation magnitude between the \u201ccorrect\u201d label and the original input sentence in each prompt. The results show that the small model Llama 2-13b mainly focuses on the relevant part (original input) and may ignore the irrelevant sentence, while the large model Llama 2-70b focuses on both sentences. This well aligns with our analysis.\n# 7. More Discussions about Noise\nThere are three kinds of noise covered in our analysis: Pretraining noise. We can see it as toxic or harmful pretraining data on the website (noisy training data). The model will learn these features and patterns. It is covered by \u03be in the linear regression case and S2 in the parity case. Input noise during inference. We can see it as natural noise as the user\u2019s wrong spelling or biased sampling. It is a finite sampling error as x drawn from the Gaussian distribution for the linear regression case and a finite sampling error as x drawn from a uniform distribution for the parity case. Label noise during inference. We can see it as adversarial examples, or misleading instructions, e.g., deliberately letting a model generate a wrong fact conclusion or harmful solution, e.g., poison making. It is \u03c3 in the linear regression case and S2 in the parity case. For pretraining noise, it will induce the model to learn noisy or harmful features. During inference, for input noise and label noise, the larger model will pay additional attention to these noisy or harmful features in the input and label pair, i.e., y \u00b7 x, so that the input and label noise may cause a large perturbation in the final results. If there is no pretraining noise, then the larger model will have as good robustness as the smaller model. Also, if there is no input and label noise, the larger model will have as good robustness as the smaller model. The robustness gap only happens when both pretraining noise and inference noise exist simultaneously.\nFor pretraining noise, it will induce the model to learn noisy or harmful features. During inference, for input noise and label noise, the larger model will pay additional attention to these noisy or harmful features in the input and label pair, i.e., y \u00b7 x, so that the input and label noise may cause a large perturbation in the final results. If there is no pretraining noise, then the larger model will have as good robustness as the smaller model. Also, if there is no input and label noise, the larger model will have as good robustness as the smaller model. The robustness gap only happens when both pretraining noise and inference noise exist simultaneously.\n# 8. Conclusion\nIn this work, we answered our research question: why do larger language models do in-context learning differently? Our theoretical study showed that smaller models emphasize important hidden features while larger ones cover more hidden features, and thus the former are more robust to noise while the latter are more easily distracted, leading to different behaviors during in-context learning. Our empirical results provided positive support for the theoretical analysis. Our findings can help improve understanding of LLMs and ICL, and better training and application of these models.\n# Acknowledgements\nThe work is partially supported by Air Force Grant FA955018-1-0166, the National Science Foundation (NSF) Grants 2008559-IIS, 2023239-DMS, and CCF-2046710.\n# Impact Statement\nOur work aims to improve the understanding of the incontext learning mechanism and to inspire efficient and safe use of ICL. Our paper is purely theoretical and empirical in nature and thus we foresee no immediate negative ethical impact. We hope our work will inspire effective algorithm design and promote a better understanding of large language model learning mechanisms.\n# References\nAgrawal, S., Zhou, C., Lewis, M., Zettlemoyer, L., and Ghazvininejad, M. In-context examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022. Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Ahn, K., Cheng, X., Song, M., Yun, C., Jadbabaie, A., and Sra, S. Linear attention is (maybe) all you need (to understand transformer optimization). In The Twelfth International Conference on Learning Representations, 2024. Akyurek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. Allen-Zhu, Z. and Li, Y. Physics of language models: Part 1, context-free grammar. arXiv preprint arXiv:2305.13673, 2023. An, S., Lin, Z., Fu, Q., Chen, B., Zheng, N., Lou, J.-G., and Zhang, D. How do in-context examples affect compositional generalization? arXiv preprint arXiv:2305.04835, 2023. Arora, S. and Goyal, A. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023. Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nBarak, B., Edelman, B., Goel, S., Kakade, S., Malach, E., and Zhang, C. Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems, 35:21750\u2013 21764, 2022. Bhojanapalli, S., Yun, C., Rawat, A. S., Reddi, S., and Kumar, S. Low-rank bottleneck in multi-head attention models. In International conference on machine learning. PMLR, 2020. Bietti, A., Cabannes, V., Bouchacourt, D., Jegou, H., and Bottou, L. Birth of a transformer: A memory viewpoint. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet allocation. the Journal of machine Learning research, 3: 993\u20131022, 2003. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 2020. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R\u00b4e, C. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 2021. Cheng, X., Chen, Y., and Sra, S. Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Conneau, A. and Kiela, D. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings\nof the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA), 2018.\nof the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA), 2018. Dai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., and Wei, F. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. Daniely, A. and Malach, E. Learning parities with neural networks. Advances in Neural Information Processing Systems, 33:20356\u201320365, 2020. Dass, J., Wu, S., Shi, H., Li, C., Ye, Z., Wang, Z., and Lin, Y. Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2023. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2019. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Fan, X., Liu, Z., Lian, J., Zhao, W. X., Xie, X., and Wen, J.-R. Lighter and better: low-rank decomposed selfattention networks for next-item recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, 2021. Fu, Y., Peng, H., Khot, T., and Lapata, M. Improving language model negotiation with self-play and incontext learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 2021a. Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In Proceedings\nGao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 2021a.\nGao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 2021b. Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 2022. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5484\u20135495, 2021. Gu, J., Li, C., Liang, Y., Shi, Z., and Song, Z. Exploring the frontiers of softmax: Provable optimization, applications in diffusion model, and beyond. arXiv preprint arXiv:2405.03251, 2024a. Gu, J., Li, C., Liang, Y., Shi, Z., Song, Z., and Zhou, T. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. arXiv preprint arXiv:2402.09469, 2024b. Gu, J., Liang, Y., Liu, H., Shi, Z., Song, Z., and Yin, J. Convbasis: A new paradigm for efficient attention inference and gradient computation in transformers. arXiv preprint arXiv:2405.05219, 2024c. Gu, J., Liang, Y., Shi, Z., Song, Z., and Zhou, Y. Tensor attention training: Provably efficient learning of higherorder transformers. arXiv preprint arXiv:2405.16411, 2024d. Gu, J., Liang, Y., Shi, Z., Song, Z., and Zhou, Y. Unraveling the smoothness properties of diffusion models: A gaussian mixture perspective. arXiv preprint arXiv:2405.16418, 2024e. Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. How do transformers learn in-context beyond simple functions? a case study on learning with representations. In The Twelfth International Conference on Learning Representations, 2024. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Huang, Y., Cheng, Y., and Liang, Y. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023. Iyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.\nJelassi, S., Sander, M., and Li, Y. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 2022. Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledgeintensive nlp. arXiv preprint arXiv:2212.14024, 2022. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021. Li, H., Wang, M., Liu, S., and Chen, P.-Y. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. In The Eleventh International Conference on Learning Representations, 2023a. Li, H., Wang, M., Lu, S., Wan, H., Cui, X., and Chen, P.-Y. Transformers as multi-task feature selectors: Generalization analysis of in-context learning. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023b. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Association for Computational Linguistics, 2021. Li, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. Transformers as algorithms: Generalization and stability in in-context learning. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2023c. Li, Y., Li, Y., and Risteski, A. How do transformers learn topic structure: Towards a mechanistic understanding. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2023d. Li, Y., Sreenivasan, K., Giannou, A., Papailiopoulos, D., and Oymak, S. Dissecting chain-of-thought: Compositionality through in-context filtering and learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023e. Luo, Z., Wu, S., Weng, C., Zhou, M., and Ge, R. Understanding the robustness of self-supervised learning through topic modeling. In The Eleventh International Conference on Learning Representations, 2023.\nLi, Y., Li, Y., and Risteski, A. How do transformers learn topic structure: Towards a mechanistic understanding. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2023d.\nLi, Y., Sreenivasan, K., Giannou, A., Papailiopoulos, D., and Oymak, S. Dissecting chain-of-thought: Compositionality through in-context filtering and learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023e.\nLuo, Z., Wu, S., Weng, C., Zhou, M., and Ge, R. Understanding the robustness of self-supervised learning through topic modeling. In The Eleventh International Conference on Learning Representations, 2023.\nMahankali, A., Hashimoto, T. B., and Ma, T. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023. Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D., Chen, D., and Arora, S. Fine-tuning language models with just forward passes. Advances in Neural Information Processing Systems, 2023. Michalowicz, J., Nichols, J., Bucholtz, F., and Olson, C. An isserlis\u2019 theorem for mixed gaussian variables: Application to the auto-bispectral density. Journal of Statistical Physics, 2009. Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022. Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Crosstask generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. Olshausen, B. and Field, D. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research, 37:3311\u20133325, 1997. OpenAI. Introducing ChatGPT. https://openai. com/blog/chatgpt, 2022. Accessed: 2023-09-10. OpenAI. GPT-4 technical report. arXiv preprint arxiv:2303.08774, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 2022. Pan, J., Gao, T., Chen, H., and Chen, D. What in-context learning \u2019learns\u2019 in-context: Disentangling task recognition and task learning. In Findings of Association for Computational Linguistics (ACL), 2023.\nOpenAI. GPT-4 technical report. arXiv preprint arxiv:2303.08774, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 2022.\nPan, J., Gao, T., Chen, H., and Chen, D. What in-context learning \u2019learns\u2019 in-context: Disentangling task recognition and task learning. In Findings of Association for Computational Linguistics (ACL), 2023.\nPanigrahi, A., Malladi, S., Xia, M., and Arora, S. Trainable transformer in transformer. arXiv preprint arXiv:2307.01189, 2023. Pourreza, M. and Rafiei, D. Din-sql: Decomposed incontext learning of text-to-sql with self-correction. arXiv preprint arXiv:2304.11015, 2023. Raventos, A., Paul, M., Chen, F., and Ganguli, S. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Reddy, G. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In The Twelfth International Conference on Learning Representations, 2024. Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning. PMLR, 2021. Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Sch\u00a8arli, N., and Zhou, D. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning. PMLR, 2023a. Shi, Z., Wei, J., and Liang, Y. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. In International Conference on Learning Representations, 2022. Shi, Z., Chen, J., Li, K., Raghuram, J., Wu, X., Liang, Y., and Jha, S. The trade-off between universality and label efficiency of representations from contrastive learning. In The Eleventh International Conference on Learning Representations, 2023b. Shi, Z., Ming, Y., Fan, Y., Sala, F., and Liang, Y. Domain generalization via nuclear norm regularization. In Conference on Parsimony and Learning (Proceedings Track), 2023c. Shi, Z., Wei, J., and Liang, Y. Provable guarantees for neural networks via gradient feature learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023d. Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Blackbox tuning for language-model-as-a-service. In International Conference on Machine Learning. PMLR, 2022. Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 2023a.\nTian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 2023a.\nTian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention, 2023b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 2017. Vinje, W. E. and Gallant, J. L. Sparse coding and decorrelation in primary visual cortex during natural vision. Science, 287(5456):1273\u20131276, 2000. Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. PMLR, 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics, 2018. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085\u20135109, 2022. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022b.\nVon Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. PMLR, 2023.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics, 2018.\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085\u20135109, 2022.\nWei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022b.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 2022c.\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 2022c. Wei, J., Hou, L., Lampinen, A. K., Chen, X., Huang, D., Tay, Y., Chen, X., Lu, Y., Zhou, D., Ma, T., and Le, Q. V. Symbol tuning improves in-context learning in language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023a. Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023b. Wibisono, K. C. and Wang, Y. On the role of unstructured training data in transformers\u2019 in-context learning capabilities. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023. Wick, G.-C. The evaluation of the collision matrix. Physical review, 1950. Wu, J., Zou, D., Chen, Z., Braverman, V., Gu, Q., and Bartlett, P. L. How many pretraining tasks are needed for in-context learning of linear regression? In The Twelfth International Conference on Learning Representations, 2024. Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. Xu, Z., Shi, Z., Wei, J., Li, Y., and Liang, Y. Improving foundation models for few-shot learning via multitask finetuning. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. Xu, Z., Shi, Z., and Liang, Y. Do large language models have compositional ability? an investigation into limitations and scalability. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024a. Xu, Z., Shi, Z., Wei, J., Mu, F., Li, Y., and Liang, Y. Towards few-shot adaptation of foundation models via multitask finetuning. In The Twelfth International Conference on Learning Representations, 2024b. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. R. Tree of thoughts: Deliberate problem solving with large language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.\n# Wick, G.-C. The evaluation of the collision matrix. Physical review, 1950.\nWu, J., Zou, D., Chen, Z., Braverman, V., Gu, Q., and Bartlett, P. L. How many pretraining tasks are needed for in-context learning of linear regression? In The Twelfth International Conference on Learning Representations, 2024.\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022.\nXu, Z., Shi, Z., Wei, J., Li, Y., and Liang, Y. Improving foundation models for few-shot learning via multitask finetuning. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\nXu, Z., Shi, Z., and Liang, Y. Do large language models have compositional ability? an investigation into limitations and scalability. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024a.\nXu, Z., Shi, Z., Wei, J., Mu, F., Li, Y., and Liang, Y. Towards few-shot adaptation of foundation models via multitask finetuning. In The Twelfth International Conference on Learning Representations, 2024b.\nZhang, H., Zhang, Y.-F., Yu, Y., Madeka, D., Foster, D., Xing, E., Lakkaraju, H., and Kakade, S. A study on the calibration of in-context learning. arXiv preprint arXiv:2312.04021, 2023a. Zhang, R., Frei, S., and Bartlett, P. L. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023b. Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient finetuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning. PMLR, 2021. Zheng, H. S., Mishra, S., Chen, X., Cheng, H.-T., Chi, E. H., Le, Q. V., and Zhou, D. Step-back prompting enables reasoning via abstraction in large language models. In The Twelfth International Conference on Learning Representations, 2024. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., YU, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022. Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? a study in length generalization. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS\u201923, 2023b.\nZhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.\n# Appendix\n# A. Limitations\nWe study and understand an interesting phenomenon of in-context learning: smaller models are more robust to noise, while larger ones are more easily distracted, leading to different ICL behaviors. Although we study two stylized settings and give the closed-form solution, our analysis cannot extend to real Transformers easily due to the high non-convex function and complicated design of multiple-layer Transformers. Also, our work does not study optimization trajectory, which we leave as future work. On the other hand, we use simple binary classification real-world datasets to verify our analysis, which still has a gap for the practical user using the LLM scenario.\n# B. Deferred Proof for Linear Regression\nHere, we provide the proof of Theorem 4.1. Theorem 4.1 (Optimal rank-r solution for regression). Recall the loss function \u02dc\u2113in Lemma 4.1. Let\nTheorem 4.1 (Optimal rank-r solution for regression). Recall the loss function \u02dc\u2113in Lemma 4.1. Let\nU\u2217, u\u2217= argmin U\u2208Rd\u00d7d,rank(U)\u2264r,u\u2208R \u02dc\u2113(U, u).\nThen U\u2217= cQV\u2217Q\u22a4, u = 1 c, where c is any nonzero constant, and V\u2217= diag([v\u2217 1, . . . , v\u2217 d]) satisfies for any i \u2264r, v\u2217 i = N (N+1)\u03bbi+tr(D) and for any i > r, v\u2217 i = 0.\nProof of Theorem 4.1. Note that,\nThus, we may consider Equation (7) in Lemma B.1 only. On the other hand, we have\nWe denote D\u2032 = \ufffd 1 + 1 N \ufffd D + 1 N tr(D)Id\u00d7d. We can see \u039b 1 2 = QD 1 2 Q\u22a4, \u0393 1 2 = QD\u2032 1 2 Q\u22a4, and \u0393\u22121 = QD\u2032\u22121Q\u22a4. We denote V = uQ\u22a4UQ. Since \u0393 and \u039b are commutable and the Frobenius norm (F-norm) of a matrix does not change after multiplying it by an orthonormal matrix, we have Equation (7) as\n\ufffd\ufffd As WKQ is a matrix whose rank is at most r, we have V is also at most  argminV\u2208Rd\u00d7d,rank(V)\u2264r \ufffd\ufffd\ufffdD\u2032 1 2 D 1 2 \ufffd V \u2212D\u2032\u22121\ufffd D 1 2 \ufffd\ufffd\ufffd 2 F . We can see that V\u2217is\n\ufffd\ufffd \ufffd\ufffd As WKQ is a matrix whose rank is at most r, we have V is also at most rank r. Then, we denote V\u2217= argminV\u2208Rd\u00d7d,rank(V)\u2264r \ufffd\ufffd\ufffdD\u2032 1 2 D 1 2 \ufffd V \u2212D\u2032\u22121\ufffd D 1 2 \ufffd\ufffd\ufffd 2 F . We can see that V\u2217is a diagonal matrix. Denote D\u2032 =\ndiag([\u03bb\u2032 1, . . . , \u03bb\u2032 d]) and V\u2217= diag([v\u2217 1, . . . , v\u2217 d]). Then, we have\n\ufffd \ufffd As V\u2217is the minimum rank r solution, we have that v\u2217 i \u22650 for any i \u2208[d] and if v\u2217 i > 0, we have v\u2217 i = 1 (1+ 1 N )\u03bbi+ tr(D) N . Denote g(x) = \ufffd\ufffd 1 + 1 N \ufffd x + tr(D) N \ufffd x2 \ufffd 1 (1+ 1 N )x+ tr(D) N \ufffd2 = x2 \ufffd 1 (1+ 1 N )x+ tr(D) N \ufffd . It is easy to see that g(x) is an increasing function on [0, \u221e). Now, we use contradiction to show that V\u2217only has non-zero entries in the first r diagonal entries. Suppose i > r, such that v\u2217 i > 0, then we must have j \u2264r such that v\u2217 j = 0 as V\u2217is a rank r solution. We find that if we set v\u2217 i = 0, v\u2217 j = 1 (1+ 1 N )\u03bbj+ tr(D) N and all other values remain the same, Equation (6) will strictly decrease as g(x) is an increasing function on [0, \u221e). Thus, here is a contradiction. We finish the proof by V\u2217= uQ\u22a4U\u2217Q.\n# B.2. Behavior Difference\nTheorem 4.2 (Behavior difference for regression). Let w = Q(s + \u03be) \u2208Rd where s, \u03be \u2208Rd are truncated and residu vectors defined above. The optimal rank-r solution fLSA,\u03b8 in Theorem 4.1 satisfies:\nProof of Theorem 4.2. By Theorem 4.1, w.l.o.g, letting c = 1, the optimal rank-r solution fLSA,\u03b8 satisfies \u03b8 = (WP V , WKQ), and\nwhere U\u2217= QV\u2217Q\u22a4.\nwhere U\u2217= QV\u2217Q\u22a4. We can see that U\u2217and \u039b commute. Denote \ufffd\u039b := 1 M \ufffdM i=1 xix\u22a4 i . Note that we have\nWe can see that U\u2217and \u039b commute. Denote \ufffd\u039b := 1 M \ufffdM i=1 xix\u22a4 i . Note that we have\n(4)\n(6)\n\nThen, we have\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eacf/eacf6dfc-e962-4ebd-8e44-5113f8bbac1b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd where the last equality is due to i.i.d. of \u03f5i. We see that the label noise can only have an effect in the second term. For t term (I) we have,</div>\n\ufffd \ufffd\ufffd \ufffd where the last equality is due to E[\ufffd\u039b] = \u039b and \ufffd\u039b is independent with xq. Note the fact that U\u2217and \u039b commute. Fo (III) term, we have\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd here the last equality is due to E[\ufffd\u039b] = \u039b and \ufffd\u039b is independent with xq. Note the fact that U\u2217and \u039b commute. For the II) term, we have</div>\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd where the last equality is due to E[\ufffd\u039b] = \u039b and \ufffd\u039b is independent with xq. Note the fact that U\u2217and \u039b commute. For the (III) term, we have</div>\n\ufffd By the property of trace, we have,\nwhere the third last equality",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in large language models (LLMs), emphasizing the importance of understanding their mechanisms for safe and efficient deployment as they transform AI development. ICL allows LLMs to perform well on unseen tasks based on task examples without adjusting model parameters, yet recent observations indicate that larger models exhibit different ICL behaviors, particularly sensitivity to noise.",
        "problem": {
            "definition": "The primary problem investigated is why larger language models perform in-context learning differently compared to smaller models, specifically their differing robustness to noise and distraction during ICL.",
            "key obstacle": "A significant challenge is understanding the underlying mechanisms that lead to these different ICL behaviors, particularly how model scale affects attention to relevant versus irrelevant features."
        },
        "idea": {
            "intuition": "The idea is inspired by recent empirical observations that larger models are more easily distracted by noise during ICL, while smaller models maintain robustness.",
            "opinion": "The authors propose that the difference in ICL behavior between model sizes stems from the way these models emphasize important hidden features.",
            "innovation": "The main innovation lies in formalizing new theoretical settings for studying ICL and characterizing the optimal solutions, revealing that smaller models focus on fewer but more important features, whereas larger models cover a broader range of features, including less relevant ones."
        },
        "Theory": {
            "perspective": "The theoretical perspective involves analyzing the scaling effect of LLMs on ICL through stylized settings, specifically linear regression and parity classification.",
            "opinion": "The authors hypothesize that larger models' performance issues in ICL are due to their tendency to overfit to noisy contexts and lose prior knowledge.",
            "proof": "The paper provides closed-form optimal solutions for both linear regression and parity classification tasks, illustrating how different attention mechanisms lead to varying robustness in ICL."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on five prevalent NLP tasks using various sizes of the Llama model families (3B, 7B, 13B, 70B), with M = 16 in-context exemplars. The evaluation included datasets from GLUE and Subj.",
            "evaluation method": "The evaluation involved manipulating in-context example labels by inverting a percentage to introduce noise, measuring the models' performance across different noise levels."
        },
        "conclusion": "The study concludes that larger language models exhibit different ICL behaviors due to their tendency to cover more hidden features, making them more susceptible to noise, while smaller models focus on crucial features, leading to greater robustness.",
        "discussion": {
            "advantage": "The paper provides a deeper theoretical understanding of ICL in LLMs, highlighting the importance of model scale on performance and robustness.",
            "limitation": "The analysis is limited to stylized settings and may not easily extend to complex real-world transformer architectures, leaving gaps in understanding optimization trajectories.",
            "future work": "Future research could explore the optimization dynamics of larger models and extend the analysis to more complex tasks and architectures."
        },
        "other info": [
            {
                "info1": "The work is partially supported by multiple grants, including from the Air Force and the National Science Foundation."
            },
            {
                "info2": {
                    "info2.1": "The research aims to improve the understanding of ICL mechanisms.",
                    "info2.2": "The paper is primarily theoretical and empirical, with no immediate negative ethical impacts anticipated."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) allows large language models (LLMs) to perform well on unseen tasks based on task examples without adjusting model parameters."
        },
        {
            "section number": "1.3",
            "key information": "Larger models exhibit different ICL behaviors, particularly sensitivity to noise, which impacts their performance."
        },
        {
            "section number": "3.1",
            "key information": "The primary problem investigated is why larger language models perform in-context learning differently compared to smaller models, specifically their differing robustness to noise and distraction during ICL."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective involves analyzing the scaling effect of LLMs on ICL through stylized settings, specifically linear regression and parity classification."
        },
        {
            "section number": "4.1",
            "key information": "The authors propose that the difference in ICL behavior between model sizes stems from the way these models emphasize important hidden features."
        },
        {
            "section number": "6.1",
            "key information": "The study concludes that larger language models exhibit different ICL behaviors due to their tendency to cover more hidden features, making them more susceptible to noise."
        },
        {
            "section number": "6.4",
            "key information": "The analysis is limited to stylized settings and may not easily extend to complex real-world transformer architectures, leaving gaps in understanding optimization trajectories."
        }
    ],
    "similarity_score": 0.7603451789882293,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Why Larger Language Models Do In-context Learning Differently_.json"
}