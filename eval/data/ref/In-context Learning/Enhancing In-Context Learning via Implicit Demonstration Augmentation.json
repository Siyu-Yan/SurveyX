{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.00100",
    "title": "Enhancing In-Context Learning via Implicit Demonstration Augmentation",
    "abstract": "The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL\u2019s effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions. 3 3 ( , ) y x",
    "bib_name": "zhou2024enhancingincontextlearningimplicit",
    "md_text": "# ontext Learning via Implicit Demonstration Au Deep feature distribution h \uf025 \uf043 h  \uf064\uf04d\nXiaoling Zhou1, Wei Ye1\u2217, Yidong Wang1, Chaoya Jiang1, Zhemg Lee2, Rui Xie1, Shikun Zhang1\u2217 1National Engineering Research Center for Software Engineering, Peking University, China 2Tianjin University, Tianjin, China xiaolingzhou@stu.pku.edu.cn, {wye,zhangsk}@pku.edu.cn h \uf043 h\n# Abstract\nThe emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL\u2019s effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions. 3 3 ( , ) y x\narXiv:2407.00100v1\n# 1 Introduction\nLarge pre-trained language models (PLMs) have showcased exceptional abilities in in-context learning (ICL) (Brown et al., 2020; Wang et al., 2023; Rubin et al., 2022), which assists the model in discerning the underlying patterns within demonstrations and make more accurate predictions (Chan et al., 2022; Wu et al., 2023). As a new paradigm, ICL offers compelling advantages, allowing for natural language interaction with PLMs (Wei et al., 2022; Yang et al., 2023), as well as reduced computational costs (Li et al., 2023a; Rubin et al., 2022). While promising, ICL\u2019s performance is highly dependent on provided demonstrations and templates (Liu et al., 2022; Zhang et al., 2022b;\n*Corresponding authors.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f9d5/f9d536f6-96b1-4090-a496-cd2e4e743418.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Illustration for demonstration augmentation using semantic directions (vectors) sampled from the deep feature distribution of demonstration examples.</div>\nSorensen et al., 2022), resulting in subpar and unstable performance. This promotes research aimed at improving the quality (Rubin et al., 2022; Li et al., 2023b), quantity (Li et al., 2023a; Choi et al., 2022), and permutations (Lu et al., 2022; Tang et al., 2023) of demonstrations. Other research avenues include prediction adjustment (Zhao et al., 2021; Han et al., 2023; Fei et al., 2023) and learning process design (e.g., channel models (Min et al., 2022a) and meta-training frameworks (Min et al., 2022b)). Despite ongoing efforts, ICL still struggles with efficiently and reliably capturing sufficient knowledge from context, leaving performance stability as a persistent bottleneck. In this study, we propose enriching contextual knowledge for PLMs by augmenting demonstrations. We first attempt to enhance the representation of demonstrations by transforming them along semantic directions sampled from the deep feature space of demonstration examples, as depicted in Figure 1. This operation stems from the observation that the deep features in a network are usually linearized (Bengio et al., 2013; Cheung and Yeung, 2021; Cho, 2016), implying the existence of numerous semantic directions within the deep feature space, hence potentially enabling us to incorporate richer contextual knowledge without extending input length. From this novel perspective, we theoretically prove that when the number of augmented\npieces approaches infinity, its effect approximately equals a logit adjustment operation. Specifically, we derive a refined Softmax function that integrates he statistical properties of demonstrations. Consequently, rather than explicitly executing the augmentation procedure, we can efficiently conduct mplicit demonstration augmentation using the derived prediction function, obtaining an improved ICL method with theoretical guidance. We conduct extensive experiments across seven PLMs and various classification tasks. The empirical results demonstrate that our approach remarkably enhances prediction accuracy and reduces performance variability across different demonstraions, permutations, and templates. Notably, our method is straightforward, effective, and generalizable, enabling seamless integration with other ICL methods to enhance their performance. Our contributions can be summarized as follows: \u2022 We introduce Implicit Demonstration Augmentation-based ICL (IDAICL), a pioneering work that incorporates demonstration augmentation into ICL. Instead of solely enhancing demonstration quality, quantity, or order, our method explores context augmentation within the deep feature space, offering a new perspective to enrich demonstrations bypassing input length limitations. \u2022 We theoretically establish that as the number of augmented pieces approaches infinity, our augmentation strategy approximates a logitadjusted prediction function that integrates statistical properties derived from the input data distribution. Equipped with this function, IDAICL provides a straightforward yet theoryguided solution to enhance ICL. \u2022 Extensive experiments conducted across diverse tasks and PLMs conclusively illustrate that IDAICL considerably improves average and worst-case accuracy compared to existing ICL methods. Moreover, it effectively enhances performance stability.\n# 2 Background and Related Work 2.1 In-Context Learning\n# 2 Background and Related Work\n# 2.1 In-Context Learning\nBrown et al. (2020) showcased the ICL capability of PLMs, wherein PLMs generate predictions solely based on a concatenation of training examples for few-shot learning without updating parameters. Subsequent studies (Holtzman et al., 2021;\nMin et al., 2022a,b) have developed this approach, yielding promising outcomes across various tasks. Nevertheless, recent research has uncovered certain limitations. To begin with, the volume of input knowledge for each query is constrained by the maximum input length of PLMs (Hao et al., 2022), and the computational cost increases as the number of demonstrations grows (Li et al., 2023a), making it challenging to integrate significant knowledge from demonstrations to PLMs. Additionally, ICL\u2019s performance is sensitive to the input of PLMs (Davison et al., 2019; Jiang et al., 2020), thus exhibiting high variance and poor worst-case accuracy (Perez et al., 2021; Lu et al., 2022). Researchers have explored various techniques to address the biases and instability of ICL. These techniques encompass learning process design (Min et al., 2022a,b), demonstration retrieval (Rubin et al., 2022; Zhang et al., 2022b), prompt engineering (Sorensen et al., 2022; Lu et al., 2022), and prediction calibration (Zhao et al., 2021; Fei et al., 2023). However, these methods have yet to fully address the issue of severely limited knowledge transfer from demonstrations to large PLMs.\n# 2.2 Data Augmentation\nData augmentation (Chen et al., 2023), which involves artificially creating training data through transformations, is a well-established research area in machine learning. Although data augmentation techniques have undergone extensive exploration in diverse machine learning domains (Maharana et al., 2022; Shorten and Khoshgoftaar, 2019), applying them to text data poses challenges due to the complexity of preserving labels during textual transformations (Kobayashi, 2018). Nonetheless, data augmentations in the latent space, such as adversarial training (Zhang et al., 2022a; Zhu et al., 2020; Cheng et al., 2020), interpolation (Chen et al., 2022b; Wu et al., 2022), and generative techniques (Li et al., 2022; Malandrakis et al., 2019), have demonstrated notable enhancements when applied alongside large PLMs. Recently, Wang et al. (2019) introduced the concept of implicit data augmentation in the context of image classification. This approach involves transforming training data within the deep feature space and boils down to the optimization of a novel robust loss function. Subsequent studies (Chen et al., 2022c; Li et al., 2021; Zhou and Wu, 2023a) for image classification tasks have further improved\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2899/28991dcb-a326-4665-9fc1-6b9310837c05.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Deep feature distribution</div>\n<div style=\"text-align: center;\">Figure 2: An overview of IDAICL: For each contextual input, our goal is to augment the deep feature of demonstrations for M pieces, using semantic vectors \u03b4 drawn from the deep feature distribution N(\u00b5, \u03a3) of demonstration examples linked to all queries. When M approaches infinity, we derive a novel prediction function, which incorporates two modulating factors: M(\u00b5) and N(\u03a3), to calibrate the original predictions.</div>\nFigure 2: An overview of IDAICL: For each contextual input, our goal is to augment the deep feature of demonstrations for M pieces, using semantic vectors \u03b4 drawn from the deep feature distribution N(\u00b5, \u03a3) of demonstration examples linked to all queries. When M approaches infinity, we derive a novel prediction function, which incorporates two modulating factors: M(\u00b5) and N(\u03a3), to calibrate the original predictions.\n3h upon this approach. This study introduces an algorithm for implicitly augmenting demonstrations within the realm of ICL.\n# 3 Methodology\n# 3.1 In-Context Learning with PLMs\nConsidering a PLM G, this study focuses on the following task: given a query input text x and a candidate answer set Y = {y1, y2, \u00b7 \u00b7 \u00b7 , y|Y|}, we aim to predict the answer \u02c6y based on m demonstration examples C ={c1, c2, \u00b7 \u00b7 \u00b7 , cm}, where each ci represents a training example (xi, yi) after template formulation and m denotes the quantity of demonstration examples for each test sample. Formally, give a model G, we first compute the probability of each answer yj:\n(1)\nSubsequently, the ultimate prediction \u02c6y, characterized by the highest probability is chosen from the candidate answer set Y:\n(2)\nTo simplify, the contextual input is denoted as \u02dcx = [C, x] in the subsequent text. Then, the probability of answer yj, represented as PG(yj|\u02dcx), is computed using the Softmax function1:\n(3)\n\ufffd where h\u02dcx = G(\u02dcx) signifies the hidden state of the last block at the final position for \u02dcx. wk and bk are the weight vector and bias corresponding to the final fully connected layer for the k-th token.\n1We begin by examining situations in which the answer comprises a single token, and our subsequent analysis is equally applicable to scenarios involving multiple tokens.\n# 3.2 Demonstration Augmentation\nAugmentation Augmented  Demonstration Recognizing the established efficacy of data augmentation in machine learning (Feng et al., 2021), this study investigates demonstration augmentation and suggests enhancing the deep features of demonstrations by transforming them along semantic directions sampled from the deep feature space of demonstration examples. This strategy is motivated by the intriguing observation that the deep features in networks are often linearized (Bengio et al., 2013; Chen et al., 2022a). Building on this observation, we hypothesize that h\u02dcx lies within the subspace spanned by hC and hx: h\u02dcx = \u03b1hC + \u03b2hx, where hC and hx represent the components of h\u02dcx linked respectively to the demonstrations and the query. The necessity of this assumption stems from intricate relationships among token representations and the exclusive augmentation of the component related to demonstrations. Notably, this decomposition is not necessary in practical applications. In the subsequent text, we directly refer to \u03b1hC and \u03b2hx as hC and hx. To augment hC, we randomly sample vectors from the deep feature space of demonstrations. In particular, vectors are drawn from a multivariate normal distribution N(\u00b5, \u03a3), where \u00b5 and \u03a3 denote the feature mean and covariance matrix. These statistical properties are estimated from the deep features of the demonstration set D, which includes demonstration examples linked to all queries. The feature mean \u00b5 is computed as\n(4)\nwhere hi = G(ci) represents the hidden state of the last block at the final position for the i-th demonstration example ci in D, and |D| denotes the size\nof D. The covariance matrix \u03a3 is computed as\n(5)\n\ufffd Subsequently, hC is shifted in the extracted semantic vectors, resulting in augmented features, \u02dchC, which follows\n(6)\nwhere \u03bb refers to a positive coefficient controlling the strength of semantic augmentation. In realworld applications, it can be directly assigned a value of 0.5. Sensitivity tests for \u03bb are discussed in Section 5.4.\n# 3.3 Novel Prediction Function\nSelecting the answer with the highest probability is equivalent to favoring the answer with the lowest inverse probability. Therefore, the prediction can be determined by\n(7)\nAssume that each hC is augmented for M times, resulting in an augmented demonstration feature set {\u02dch 1 C, \u00b7 \u00b7 \u00b7 , \u02dch M C } with size M. Here, \u02dch i C represents the i-th augmented feature for hC. Then, the final prediction for the query x depends on all augmented features of hC and can be expressed as\n(8)\n(9)\nGiven that the performance of ICL benefits from an increased number of demonstration instances (Liu et al., 2022; Wu et al., 2023), we explore the scenario of augmenting an infinite number of times for the deep representation of demonstrations. Subsequently, an easily computable surrogate for the expected prediction can be derived, resulting in a highly efficient implementation. The whole pipeline of IDAICL is depicted in Figure 2. As M \u2192\u221e, on the basis of the aforementioned decomposition of h\u02dcx, the expected prediction for answer yj (denoted as P \u221e yj ) within the augmented feature set can be expressed as follows:\nwhere \u2206wk,yj = wk \u2212wyj and \u2206bk,yj = bk \u2212byj.\nHowever, accurately calculating P \u221e yj is challenging. Alternatively, we proceed to derive a surrogate calculation for it. Applying the linearity of expectation, Eq. (10) can be expressed as:\n (11)\nGiven that \u02dchC is a Gaussian random variable conforming to N (hC + \u03bb\u00b5, \u03bb\u03a3), we know that \u2206wT k,yj \u02dchC follows the multivariate normal distribution: N(\u2206wT k,yj (hC + \u03bb\u00b5) , \u03bb\u2206wT k,yj\u03a3\u2206wk,yj). Then, utilizing the moment-generating function\n(12)\nEq. (11) can be derived as\n(13)\nexp( \u03bb 2\u2206wT k,yj\u03a3\u2206wk,yj). Subsequently, our newly proposed prediction function, referred to as IDA-Softmax, is defined as\n2 k,yjj Subsequently, our newly proposed prediction function, referred to as IDA-Softmax, is defined as\n(14)\nConsequently, instead of conducting the augmentation process explicitly, we can directly employ IDA-Softmax, P IDA yj , for prediction. IDA-Softmax essentially utilizes two modulating factors associated with statistical properties derived from D to calibrate the sample logits. Previous studies (Min et al., 2022c; Chan et al., 2022) have underscored the pivotal role of knowledge about the input data distribution in predictions made by PLMs. Intuitively, PLMs can better capture the patterns and underlying structures within data, such as the spatial relationships between demonstrations and queries, ultimately enhancing their prediction performance. Furthermore, to mitigate the imbalance among different answer types in demonstrations (Holtzman et al., 2021; Zhao et al., 2021), we adopt a post-hoc adjustment approach inspired by Menon et al. (2021), which adjusts predictions by considering the class proportions within D. Thus, the prediction for answer yj is computed as\n(15)\nwhere \u03c4 is a positive hyperparameter, and \u03c0yj demotes the proportion of answer yj in D. In practical applications, the value of \u03c4 can be fixed at 1.\nPLM\nMethod\nm\nSST-2\nSST-5\nMR\nCR\nAmazon\nSubj\nTREC\nDBPedia\nAGNews\nCB\nGPT-2 0.8B\nVanilla ICL\n4\n57.67.1\n30.46.3\n59.36.5\n56.88.4\n32.78.5\n57.65.4\n34.910.3\n40.57.2\n44.57.9\n35.19.3\nIDAICL\n86.41.4\n38.32.9\n82.22.3\n78.40.7\n46.73.5\n77.02.3\n47.52.0\n81.31.8\n73.92.4\n41.52.0\nVanilla ICL\n8\n69.79.0\n32.48.6\n63.97.7\n60.88.1\n34.16.2\n59.78.7\n40.46.3\n62.613.6\n49.28.4\n38.87.6\nIDAICL\n88.02.3\n39.61.9\n84.92.4\n85.62.5\n47.92.6\n79.90.8\n50.33.3\n86.52.9\n76.81.7\n43.33.4\nVanilla ICL\n12\n74.78.3\n33.77.6\n64.49.4\n68.79.7\n36.06.6\n60.77.7\n40.57.8\n64.55.4\n51.18.0\n40.48.5\nIDAICL\n88.52.1\n40.12.7\n85.23.1\n86.81.4\n49.62.2\n80.42.1\n51.41.6\n87.32.7\n77.92.0\n44.62.2\nMetaICL\n12\n80.86.2\n35.84.7\n75.35.6\n77.68.1\n48.96.7\n73.58.8\n48.66.1\n80.47.8\n66.80.7\n43.14.1\n+IDAICL\n89.31.7\n42.62.4\n85.81.7\n87.91.5\n51.70.7\n82.62.4\n53.72.5\n89.44.1\n78.31.1\n47.92.8\nChannel ICL\n12\n85.23.6\n38.44.3\n80.84.7\n82.04.6\n43.65.1\n69.89.8\n44.18.7\n77.612.9\n69.56.7\n42.45.2\n+IDAICL\n90.52.3\n41.82.7\n87.71.6\n89.51.2\n50.82.4\n80.50.9\n52.91.6\n87.82.4\n81.02.5\n46.33.3\nEPR\n12\n81.92.1\n39.91.8\n78.12.4\n80.60.6\n49.12.4\n80.12.2\n76.21.1\n87.11.0\n80.90.8\n44.82.3\n+IDAICL\n90.11.1\n43.91.2\n86.42.0\n88.60.6\n52.51.7\n83.61.0\n79.10.9\n90.80.7\n83.70.5\n46.72.1\nGPT-2 1.5B\nVanilla ICL\n4\n66.38.6\n30.38.9\n56.56.6\n53.48.1\n34.77.5\n54.25.5\n30.88.1\n61.98.7\n54.69.9\n40.87.8\nIDAICL\n87.41.5\n38.81.7\n80.91.2\n82.12.1\n48.10.6\n77.83.0\n49.51.9\n87.42.6\n79.21.8\n54.12.7\nVanilla ICL\n8\n57.27.0\n30.86.1\n64.98.3\n57.66.4\n38.66.4\n57.310.3\n39.55.3\n67.48.1\n56.35.4\n47.45.1\nIDAICL\n89.51.8\n40.81.9\n82.11.2\n84.32.1\n50.23.4\n80.12.9\n51.52.5\n89.81.7\n80.30.9\n55.50.6\nVanilla ICL\n12\n70.99.6\n34.76.7\n65.25.6\n59.96.7\n38.310.2\n59.68.1\n40.77.5\n72.511.6\n57.69.5\n48.55.7\nIDAICL\n90.02.8\n41.11.3\n83.42.3\n85.62.4\n51.62.9\n80.52.5\n51.83.6\n90.52.7\n81.13.0\n55.72.1\nMetaICL\n12\n79.17.0\n38.63.7\n76.46.3\n75.34.5\n50.57.1\n73.97.6\n46.76.3\n86.87.8\n76.45.4\n53.11.6\n+IDAICL\n89.62.2\n42.92.3\n84.23.4\n87.91.1\n53.81.2\n83.43.2\n53.61.3\n91.90.9\n84.31.4\n57.31.5\nChannel ICL\n12\n83.35.9\n37.54.6\n80.64.1\n77.15.5\n48.96.7\n68.28.3\n43.37.2\n70.49.3\n67.95.5\n53.68.9\n+IDAICL\n91.22.1\n40.81.5\n86.52.6\n88.21.8\n52.42.9\n82.32.4\n50.51.8\n88.71.2\n82.60.9\n56.52.1\nEPR\n12\n82.82.6\n40.62.1\n79.51.4\n74.72.7\n50.72.3\n83.30.7\n82.22.4\n91.50.8\n83.21.6\n54.81.9\n+IDAICL\n90.51.5\n43.81.0\n87.40.9\n86.51.5\n52.91.8\n85.80.5\n84.71.1\n93.52.5\n86.42.2\n57.51.5\nGPT-Neo\nMetaICL\n12\n87.86.7\n42.56.1\n82.25.9\n80.74.8\n51.55.3\n72.28.2\n54.16.8\n84.45.5\n74.38.2\n50.36.4\n+IDAICL\n92.11.1\n44.32.3\n88.82.1\n88.11.8\n53.21.7\n84.32.1\n64.31.9\n94.31.2\n86.50.9\n53.42.1\nChannel ICL\n12\n83.45.4\n39.86.4\n79.55.7\n79.45.9\n50.13.8\n70.68.2\n50.85.1\n78.37.1\n72.56.9\n48.74.5\n+IDAICL\n91.52.2\n41.61.8\n85.41.9\n87.22.5\n52.72.2\n83.71.4\n62.80.7\n93.53.3\n84.63.1\n52.01.8\nEPR\n12\n88.21.6\n45.72.2\n81.81.9\n71.82.9\n49.91.1\n89.42.4\n92.32.2\n96.11.2\n88.81.1\n49.40.7\n+IDAICL\n93.20.8\n47.21.3\n88.51.2\n86.62.0\n52.10.4\n93.11.2\n94.42.4\n97.81.5\n91.20.7\n52.10.5\nTable 1: Comparison results of three PLMs. Two numbers indicate the mean accuracy (%) and standard deviation over different seeds. The best and second-best results per PLM per dataset are highlighted in bold and underlined, respectively. \"+IDAICL\" means that the current approach is used in conjunction with IDAICL. The results for different numbers of demonstration examples (i.e., m values) using the GPT-Neo model are illustrated in Figure 3.\nThis approach compensates for predictions of minor classes. When different answers are uniformly distributed, \u03c4 log \u03c0yj exerts an equal influence on all answer types. Consequently, the final prediction is given by\n(16)\n# 4 Experimental Setup\n# 4 Experimental Setup 4.1 Models and Datasets\n# 4.1 Models and Datasets\nWe evaluated the performance of IDAICL across seven large PLMs, including GPT-2 (Radford et al., 2019) (with 0.1B, 0.3B, 0.8B, and 1.5B parameters), GPT-Neo (Black et al., 2021) (with 2.7B parameters), and LLaMA (Touvron et al., 2023) (with 13B and 33B parameters). Following previous research (Min et al., 2022a; Han et al., 2023; Lu et al., 2022), our evaluation encompasses ten text classification datasets. Among these, SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), MR (Pang\nand Lee, 2005), CR (Hu and Liu, 2004), and Amazon (McAuley and Leskovec, 2013) are five sentiment classification tasks. Subj (Pang and Lee, 2004), TREC (Voorhees and Tice, 2000), DBPedia (Lehmann et al., 2015), and AGNews (Zhang et al., 2015) cater to subjectivity, question, ontology, and news classification tasks, respectively. Additionally, CB (De Marneffe et al., 2019) is utilized for natural language inference. Among these datasets, SST-5, Amazon, TREC, and CB are characterized by imbalanced training data. Details of all datasets are provided in Section A of the Appendix.\n# 4.2 Compared Baselines\nBesides Vanilla ICL, we compared and integrated IDAICL with three popular ICL algorithms, focusing on learning process design and demonstration retrieval. These include MetaICL (Min et al., 2022b), Channel ICL (Min et al., 2022a), and Efficient Prompt Retrieval (EPR) (Rubin et al., 2022). Moreover, we compared IDAICL with other ad-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0491/049199b8-442e-4a81-8de4-368cffa72b2c.png\" style=\"width: 50%;\"></div>\nPLM\nMethod\nSST-2\nSST-5\nMR\nCR\nSubj\nTREC\nDBPedia\nAGNews\nCB\nAvg.\nLLaMA 13B\nVanilla ICL\n95.67.1\n29.56.2\n90.05.8\n91.47.4\n72.96.9\n62.89.1\n80.97.6\n80.25.9\n51.58.2\n72.8\nConCa\n96.75.4\n40.36.2\n91.77.3\n90.84.2\n79.69.1\n68.25.6\n94.34.1\n85.27.5\n46.65.0\n77.0\nPROCA\n95.43.8\n43.45.7\n90.39.6\n92.13.1\n84.82.5\n69.92.1\n92.54.9\n81.63.6\n51.44.2\n77.9\nD-ConCa\n96.33.8\n42.54.5\n92.04.1\n90.52.9\n82.94.5\n73.73.9\n87.47.2\n82.53.3\n52.24.1\n77.8\nIDAICL\n96.72.5\n47.11.1\n93.01.9\n93.30.8\n87.82.3\n76.02.6\n94.91.0\n87.72.4\n59.41.9\n81.8\nLLaMA 33B\nVanilla ICL\n95.57.2\n29.45.6\n91.75.4\n91.58.1\n85.16.0\n70.94.4\n86.64.5\n76.26.1\n59.25.3\n76.2\nConCa\n95.96.5\n39.14.4\n90.37.2\n91.23.6\n74.65.7\n76.76.2\n92.43.9\n87.35.7\n57.96.0\n78.4\nPROCA\n95.54.2\n39.26.3\n92.44.1\n91.33.5\n88.32.2\n64.73.8\n86.95.1\n85.87.1\n59.93.8\n78.2\nD-ConCa\n95.43.8\n40.74.5\n92.14.2\n91.02.9\n76.43.6\n80.22.1\n87.64.2\n87.74.3\n56.53.4\n78.6\nIDAICL\n96.51.1\n46.82.4\n93.61.3\n92.33.3\n89.32.4\n79.11.5\n95.62.3\n88.41.9\n64.62.8\n82.9\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/226d/226d7029-18dc-4ccd-9d32-958ef9640deb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparison results between Vanilla ICL and IDAICL across different values of m on the GPT-Neo model IDAICL significantly outperforms Vanilla ICL, particularly when the number of demonstration examples is small.</div>\nvanced prediction calibration methods: Contextual Calibration (ConCa) (Zhao et al., 2021), Prototypical Calibration (PROCA) (Han et al., 2023), and Domain-Context Calibration (D-ConCa) (Fei et al., 2023). Introductions to all compared methods and comprehensive experimental settings are presented in Sections B and C of the Appendix.\n# 5 Experimental Results\n# 5.1 Main Results\nTable 1 displays the comparison results between IDAICL and four ICL baselines (Vanilla ICL, MetaICL, Channel ICL, and EPR) across GPT-2 models (with 0.8B and 1.5B parameters) and the GPT-Neo model. These results lead to three main findings. Firstly, IDAICL consistently exhibits high effectiveness across various model sizes and datasets, highlighting its strong generalization capacity, even under scenarios involving imbalanced training data. Compared to Vanilla ICL, IDAICL outperforms by an average of 17.7% and 18.4% across diverse datasets and m values for GPT-2 with 0.8B and 1.5B parameters, respectively. Secondly, in comparison to other ICL baselines like Channel ICL, MetaICL, and EPR, the integration of\nIDAICL consistently delivers notable performance improvements, emphasizing the efficacy of enhancing demonstrations for refined predictions. The inclusion of IDAICL led to an average performance boost of 7.3% for MetaICL and 8.2% for Channel ICL. Lastly, IDAICL notably enhances worstcase accuracy and diminishes performance variance across different seeds, showcasing its ability to improve prediction stability. Additional results on LLaMA and smaller GPT-2 models are available in Tables 7 and 8 of the Appendix.\n# 5.2 Comparison with Calibration Methods\nWe compared IDAICL with three advanced prediction calibration methods (ConCa, PROCA, and D-ConCa) across three PLMs: GPT-2, GPT-Neo, and LLaMA. Table 2 presents the comparison results for the LLaMA models, where IDAICL consistently achieves state-of-the-art performance, except for TREC using the LLaMA model with 33B parameters. These findings suggest that IDAICL which leverages statistical information derived from the input data distribution for prediction calibration, generally outperforms methods relying on estimated biases for correction. Further comparison results can be found in Table 9 of the Appendix.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f619/f61967dd-2bd5-4793-a031-63da17a16696.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: (a) and (b): Macro-F1 of SST-5 and AGNews datasets using the LLaMA model with 33B parameters under three demonstration selection settings, setting m to 4. (c) and (d): Accuracy of Vanilla ICL and IDAICL on the SST-2 dataset using the GPT-2 model with 1.5B parameters across six templates, setting m to 12. IDAICL demonstrates greater robustness across various demonstration examples and templates compared to Vanilla ICL.</div>\n# 5.3 Stability Analysis\nPrevious studies (Zhao et al., 2021; Sorensen et al., 2022; Min et al., 2022a; Zhang et al., 2022b) have highlighted the considerable variability in ICL\u2019s performance. In this section, we verified that IDAICL can effectively enhance performance stability across diverse scenarios.\nVarying numbers of demonstrations We have presented the results across different numbers of demonstrations in Table 1. For a clearer depiction, the outcomes regarding GPT-Neo are illustrated in Figure 3. As the number of demonstration examples (represented by m) increases, both Vanilla ICL and IDAICL exhibit improved performance, emphasizing the importance of comprehensive statistical properties of the input data for IDAICL\u2019s effectiveness. Notably, IDAICL significantly enhances performance stability across various numbers of demonstrations and consistently outperforms Vanilla ICL. The performance improvement is particularly pronounced when m takes on smaller values, indicating the efficacy of IDAICL in enriching the available knowledge for PLMs.\n# Varying demonstrations T\n# Varying demonstrations To confirm tha\nVarying demonstrations To confirm that augmenting demonstrations can enhance the robustness of the ICL strategy across various demonstrations, we investigated three distinct demonstration selection settings. Setting I: Training samples most similar to the test sample are chosen. Setting II: Samples are randomly selected from the training data. Setting III: Training samples exhibiting the greatest dissimilarity from the test sample are selected. As shown in Figures 4(a) and (b), IDAICL significantly outperforms Vanilla ICL and demonstrates greater robustness across the three selection settings. Additionally, our discoveries suggest that selecting demonstrations that are more similar to the test samples leads to better performance than\nexclusively selecting dissimilar ones, which aligns with the findings obtained by Wang et al. (2022).\nVarying templates To assess the performance of IDAICL across various templates, we employed fifteen templates on the SST-2 dataset following those outlined by Zhao et al. (2021). The templates are elaborated in Table 10 of the Appendix. Figures 4(c) and (d) display the performance of Vanilla ICL and IDAICL across six templates. Some templates achieve higher average performance than others. Nevertheless, IDAICL consistently enhances both average and worst-case accuracy, simultaneously reducing performance variance across different templates. The complete results are available in Figure 7 of the Appendix.\nImpact of imbalance in labels Figures 5(a) and (b) depict comparison results among Vanilla ICL, MetaICL, Channel ICL, and IDAICL across different degrees of imbalances. It is evident that the performance of Vanilla ICL is sensitive to class imbalance, while that of IDAICL and Channel ICL exhibit robustness to the imbalance. Moreover, notable performance improvements are observed with higher levels of imbalance. Additionally, Figures 5(c) and (d) illustrate the confusion matrices for CR and Subj datasets, with the proportion of one category (i.e., \"Negative\" and \"Subjective\") in demonstrations setting to 0.1 and 0.2. IDAICL significantly improves the accuracy of the underrepresented classes when compared to Vanilla ICL, thereby contributing to enhanced fairness among classes. In the subsequent section, we demonstrate that the strong performance of IDAICL in handling imbalanced label distributions stems from both the statistical properties and the class proportion term.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f00a/f00a3de7-9705-429e-a785-02b742a1fa3b.png\" style=\"width: 50%;\"></div>\nFigure 5: (a) and (b): Accuracy comparison of the SST-2 and MR datasets, where the proportions of the negative class in demonstrations (denoted as p) are varied from 0.1 to 0.5. (c) and (d): Confusion matrices for the CR and Subj datasets, representing scenarios where the proportions of one category in demonstrations are set to 0.1 and 0.2. The analysis is conducted using the GPT-2 model with 1.5B parameters, with m setting to 12. IDAICL demonstrates greater robustness in handling imbalanced class distributions within demonstrations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4223/4223bbb7-a439-4a20-a1b1-95ece065d82a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Accuracy across different \u03bb and \u03c4 values, using GPT-2 with 0.8B parameters, setting m to 12. \u03bb= 0 and \u03c4 =0 signify that the two modulating factors and the class proportion term are not utilized, respectively.</div>\n# 5.4 Sensitivity and Ablation Studies\nWe conducted ablation studies on IDAICL to investigate the influence of the two modulating factors and the class proportion term. The parameters \u03bb and \u03c4 govern the augmentation strength and the impact of the class proportion term, respectively. In Figure 6(a), a significant performance drop is observed when predictions are not calibrated using statistical properties derived from the demonstrations. Additionally, optimal performance is achieved when \u03bb equals 0.5. Figure 6(b) showcases the accuracy of SST-2 and MR datasets with the negative class proportion in demonstrations setting to 0.1. Results indicate that solely leveraging statistical properties (i.e.,\nDataset\n0-shot\n1-shot\n4-shot\nIDAICL\nSST-2\n63.2\n61.39.4\n57.67.1\n76.3\nSST-5\n25.0\n27.37.9\n30.46.3\n33.5\nMR\n58.9\n54.36.8\n59.36.5\n71.2\nSubj\n48.9\n47.18.3\n57.65.4\n67.3\nTable 3: Accuracy comparison between Vanilla ICL and IDAICL based solely on statistical properties, using the GPT-2 model with 0.8B parameters.\n\u03c4 equals 0) enhances performance under imbalanced demonstrations, with further improvements observed upon the inclusion of the class proportion term. Additionally, optimal performance is attained when \u03c4 equals 1. Consequently, we recommend setting \u03bb to 0.5 and \u03c4 to 1 for practical applications. More results are presented in Appendix F.\n# 5.5 Further Discussion\nTo further investigate the effect of statistical properties within demonstrations on model performance, we exclusively employed queries along with statistical information for inference, excluding the inclusion of demonstrations for each test sample. These statistics were estimated using deep features of all training samples. As shown in Table 3, IDAICL relying solely on statistical properties distinctly outperforms Vanilla ICL across scenarios with zero, one, and even four demonstrations. This emphasizes the crucial role of prior statistics obtained from training data in PLMs\u2019 predictions. This phenomenon is understandable as statistical properties inherently encompass richer global information compared to individual demonstrations.\n# 6 Conclusion\nThis study introduces IDAICL, a novel ICL approach designed to enhance demonstrations by utilizing semantic directions sampled from the deep feature distribution of demonstration examples. Our augmentation strategy enriches the knowledge available to PLMs without extending the context length. A new prediction function is then theoretically established considering the number of augmented pieces approaching infinity. This eliminates the need for explicit augmentation and allows for direct utilization of this derived function for\npredictions. Our extensive experiments, spanning various tasks and PLMs, demonstrate that IDAICL significantly enhances both prediction accuracy and stability when compared to other ICL baselines.\n# Limitations\nWhile IDAICL proves to be competitive in few-shot learning, there are limitations that open up avenues for future research. First, due to the necessity of accessing the parameters of the final fully connected layer in PLMs, IDAICL is exclusively suitable for open-source models. Future research is expected to develop alternative augmentation strategies tailored for black-box PLMs. Second, our evaluation of IDAICL focused on seven PLMs and ten text classification tasks. We defer further explorations involving other PLMs and non-classification tasks for future work. Additionally, IDAICL relies on a small set of demonstrations to estimate the feature mean and covariance matrix. If such a collection is unavailable or extremely scarce, IDAICL may need to be used in conjunction with demonstration generation methods. Other avenues for future work involve exploring more effective augmentation distributions. This entails exploring finer-grained distributions, such as category-level or sample-level distributions, to emphasize the unique characteristics of individual categories or samples, and extending these distributions beyond the constraints of training data. Furthermore, given the effectiveness of data augmentation in model training, future research could explore the utilization of our derived prediction function in both the training and fine-tuning phases of large PLMs.\n# References\nYoshua Bengio, Gr\u00e9goire Mesnil, Yann Dauphin, and Salah Rifai. 2013. Better mixing via deep representations. In Proceedings of 30th International Conference on Machine Learning, pages 552\u2013560, Atlanta, Georgia, USA. ACM. Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Rose Biderman. 2021. Gpt-neo: Large scale autoregressive language modeling with meshtensorflow.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, page 1877\u20131901, Online and Vancouver, Canada. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. 2022. Data distributional properties drive emergent in-context learning in transformers. In Proceedings of the 36th Advances in Neural Information Processing Systems, pages 18878\u201318891, New Orleans, USA. Dong Chen, Yueting Zhuang, Zijin Shen, Carl Yang, Guoming Wang, Siliang Tang, and Yi Yang. 2022a. Cross-modal data augmentation for tasks of different modalities. IEEE Transactions on Multimedia, 99:1\u2013 11. Hui Chen, Wei Han, Diyi Yang, and Soujanya Poria. 2022b. DoubleMix: Simple interpolation-based data augmentation for text classification. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4622\u20134632, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. 2023. An empirical survey of data augmentation for limited data learning in NLP. Transactions of the Association for Computational Linguistics, 11:191\u2013211. Xiaohua Chen, Yucan Zhou, Dayan Wu, Wanqian Zhang, Yu Zhou, Bo Li, and Weiping Wang. 2022c. Imagine by reasoning: A reasoning-based implicit semantic data augmentation for long-tailed classification. In Proceedings of the 36th AAAI Conference on Artificial Intelligence, pages 356\u2013364, Online. AAAI Press. Yong Cheng, Lu Jiang, Wolfgang Macherey, and Jacob Eisenstein. 2020. AdvAug: Robust adversarial augmentation for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5961\u20135970, Online. Association for Computational Linguistics. Tsz-Him Cheung and Dit-Yan Yeung. 2021. Modals: Modality-agnostic automated data augmentation in the latent space. In Proceedings of 9th International Conference on Learning Representations, Online. Kyunghyun Cho. 2016. Noisy parallel approximate decoding for conditional recurrent language model. arXiv preprint arXiv:1605.03835. Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. 2022. Prompt injection: Parameterization of fixed inputs. arXiv preprint arXiv:2206.11349.\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, page 1877\u20131901, Online and Vancouver, Canada.\nJoe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173\u20131178, Hong Kong, China. Association for Computational Linguistics.\nSteven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968\u2013988, Online. Association for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816\u20133830, Online. Association for Computational Linguistics.\nZhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. 2023. Prototypical calibration for fewshot learning of language models. In Proceedings of 11st International Conference on Learning Representations, Kigali, Rwanda.\nYaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn\u2019t always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038\u20137051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nMinqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168\u2013177, Seattle, Washington, USA. ACM.\nMinqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168\u2013177, Seattle, Washington, USA. ACM.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438.\nosuke Kobayashi. 2018. Contextual augmentation: Data augmentation by words with paradigmatic relations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 452\u2013457, New Orleans, Louisiana. Association for Computational Linguistics.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S\u00f6ren Auer, and Christian Bizer. 2015. Dbpedia a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6(2):167\u2013195.\nRobert Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. 2022. Cutting down on prompts and parameters: Simple\nRobert Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. 2022. Cutting down on prompts and parameters: Simple\nfew-shot learning with language models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2824\u20132835, Dublin, Ireland. Association for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Kiran Maharana, Surajit Mondal, and Bhushankumar Nemade. 2022. A review: Data pre-processing and data augmentation techniques. Global Transitions Proceedings, 3(1):91\u201399. Nikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, Abhishek Sethi, and Angeliki Metallinou. 2019. Controlled text generation for data augmentation in intelligent artificial agents. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 90\u201398, Hong Kong. Association for Computational Linguistics. Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172, Hong Kong, China. ACM. Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. 2021. Long-tail learning via logit adjustment. In Proceedings of the 9th International Conference on Learning Representations, Vienna, Austria. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022b. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022c. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates. Association for\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, Dublin, Ireland. Association for Computational Linguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022b. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022c. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nBo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278, Barcelona, Spain.\nBo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278, Barcelona, Spain. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann Arbor, Michigan. Association for Computational Linguistics. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 8026\u20138037, Vancouver, Canada. Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. In Proceedings of the 35th Conference on Neural Information Processing Systems, pages 11054\u201311070, Online. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics. Connor Shorten and Taghi M. Khoshgoftaar. 2019. A survey on image data augmentation for deep learning. Journal of Big Data, 6:1\u201348. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann Arbor, Michigan. Association for Computational Linguistics.\n# Connor Shorten and Taghi M. Khoshgoftaar. 2019. A survey on image data augmentation for deep learning. Journal of Big Data, 6:1\u201348.\nTaylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 819\u2013862, Dublin, Ireland. Association for Computational Linguistics.\n<div style=\"text-align: center;\">Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.</div>\nXing Wu, Chaochen Gao, Meng Lin, Liangjun Zang, and Songlin Hu. 2022. Text smoothing: Enhance various data augmentation methods on text classification tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 871\u2013875, Dublin, Ireland. Association for Computational Linguistics. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423\u20131436, Toronto, Canada. Association for Computational Linguistics. Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, et al. 2023. Supervised knowledge makes large language models better in-context learners. In The Twelfth International Conference on Learning Representations. Minjia Zhang, Niranjan Uma Naresh, and Yuxiong He. 2022a. Adversarial data augmentation for taskspecific knowledge distillation of pre-trained transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11685\u201311693, online. AAAI Press. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Proceedings of the 28th International Conference on Neural Information Processing Systems, page 649\u2013657, Montreal, Canada. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, pages 12697\u201312706, Online. ACM. Xiaoling Zhou and Ou Wu. 2023a. Implicit counterfactual data augmentation for deep neural networks. arXiv preprint arXiv:2304.13431. Xiaoling Zhou and Ou Wu. 2023b. Which samples should be learned first: Easy or hard? IEEE Transactions on Neural Networks and Learning Systems, pages 1\u201315. Xiaoling Zhou, Ou Wu, and Chao Jiang. 2022. Increasing naturalness of human\u2013machine dialogue: The users\u2019 choices inference of options in machine-raised questions. Knowledge-Based Systems, 243:108485.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for natural language understanding. In Proceedings of the 8th International Conference on Learning Representations, Online.\n# A Details of Applied Datasets\nTable 4 presents comprehensive statistics for all datasets utilized in this study. The information includes task descriptions, average sentence lengths, class counts, and details on class imbalance. Additionally, Table 5 provides sample instances and label names for each of the datasets.\n# B Details of Compared Baselines\nThe compared methods are described as follows:\n\u2022 Vanilla ICL: We use the PLMs as they are and implement ICL by conditioning it on a concatenation of m training examples, following the approach outlined by Brown et al. (2020). \u2022 MetaICL: The fundamental concept underlying MetaICL is to utilize a multi-task learning framework across a diverse range of metatraining tasks (Min et al., 2022b).\ncatenation of m training examples, following the approach outlined by Brown et al. (2020). \u2022 MetaICL: The fundamental concept underlying MetaICL is to utilize a multi-task learning framework across a diverse range of metatraining tasks (Min et al., 2022b).\n\u2022 MetaICL: The fundamental concept underlying MetaICL is to utilize a multi-task learning framework across a diverse range of metatraining tasks (Min et al., 2022b).\n\u2022 Channel ICL: It employs a noisy channel approach for language model prompting in few-shot text classification (Min et al., 2022a).\n\u2022 EPR: It employs language models to autonomously label examples that are suitable as effective prompts and subsequently trains a prompt retriever based on this acquired signal (Rubin et al., 2022).\n\u2022 ConCa: It assesses the model\u2019s inclination towards specific answers by introducing a dummy test input that lacks content (Zhao et al., 2021).\n\u2022 PROCA: The prediction of PROCA is calibrated based on the likelihood of prototypical clusters (Han et al., 2023).\n# \u2022 PROCA: The prediction of PROCA is calibrated based on the likelihood of prototypical clusters (Han et al., 2023).\n D-ConCa: It initially assesses the impacts of various label biases by employing randomly sampled words from the task corpus. During inference, it utilizes the estimated label bias to calibrate the model\u2019s output probabilities (Fei et al., 2023).\n# C More Details of Experimental Settings\nThe entire implementation is conducted utilizing PyTorch (Paszke et al., 2019) and Transformers (Wolf et al., 2020). We follow the parameter configurations and details specified in previous research (Min et al., 2022a). The number of demonstrations is primarily set to m = 12, but we also explore m values of {1, 4, 8, 12, 16} in the ablations, with the specific settings detailed in the respective sections. Demonstration examples for each test sample are randomly selected from the training data, unless specific methods employ a specially designed selection method, such as EPR (Rubin et al., 2022). The values of the feature mean and covariance matrix are estimated from the demonstration set containing demonstration examples corresponding to all test samples. We depart from the assumption made in previous studies, which presupposes an equal distribution of training examples across all classes (Gao et al., 2021; Logan IV et al., 2022), in order to facilitate a more realistic and demanding evaluation. Each experiment is repeated under five different random seeds. The batch size is set to 32, and the sequence length is configured to 128 for datasets with shorter texts, including SST2 (Socher et al., 2013), SST-5 (Socher et al., 2013), MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), and TREC (Voorhees and Tice, 2000). On the other hand, for datasets with longer input texts, including AGNews (Zhang et al., 2015), DBPedia (Lehmann et al., 2015), Subj (Pang and Lee, 2004), CB (De Marneffe et al., 2019), and Amazon (McAuley and Leskovec, 2013), a batch size of 16 and a sequence length of 256 are employed. Regarding the hyperparameters in IDAICL, the values of \u03bb and \u03c4 are fixed at 0.5 and 1, respectively, except in sensitivity tests. The settings used for the compared methods adhere to those specified in the original papers (Min et al., 2022a,b; Rubin et al., 2022; Zhao et al., 2021; Han et al., 2023; Fei et al., 2023). Accuracy serves as the primary evaluation metric, alongside the provided values of Macro-F1 for the LLaMA model. For each task, a specific template is utilized for inference, as detailed in Table 6. Additionally, we also examine the impact of different templates on the performance of IDAICL following those outlined by Zhao et al. (2021), which include question-answer templates, conversation-style templates, prompts resembling web pages, and variations on label names,\nDataset\nTask\nAvg. length\nClasses\nBalanced\nSST-2 (Socher et al., 2013)\nSentiment analysis\n12.4\n2\nYes\nSST-5 (Socher et al., 2013)\nSentiment analysis\n23.1\n5\nNo\nMR (Pang and Lee, 2005)\nSentiment analysis\n25.7\n2\nYes\nCR (Hu and Liu, 2004)\nSentiment analysis\n22.1\n2\nYes\nAmazon (McAuley and Leskovec, 2013)\nSentiment analysis\n78.5\n5\nNo\nSubj (Pang and Lee, 2004)\nSubjectivity classification\n28.9\n2\nYes\nTREC (Voorhees and Tice, 2000)\nQuestion classification\n11.6\n6\nNo\nDBPedia (Lehmann et al., 2015)\nOntology classification\n65.5\n14\nYes\nAGNews (Zhang et al., 2015)\nNews classification\n53.8\n4\nYes\nCB (De Marneffe et al., 2019)\nNatural language inference\n69.7/8.4\n3\nNo\n<div style=\"text-align: center;\">able 4: Statistical information of ten datasets. The average length is calculated based on the GPT-2 sentence-piece ength. For tasks involving sentence pairs, we provide the average length for each individual sentence.</div>\nTable 4: Statistical information of ten datasets. The average length is calculated based on the GPT-2 sentence-pie length. For tasks involving sentence pairs, we provide the average length for each individual sentence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/904c/904c3333-a0bb-4657-b33b-a569adabf6d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Comparison results between Vanilla ICL and IDAICL across fifteen templates. The evaluation is conducted using the GPT-2 model with 1.5B parameters. The performance of IDAICL exceeds that of Vanilla ICL and demonstrates greater robustness across various templates.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0af8/0af8a760-3da5-4cd4-976e-2c2646e94f7e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Results of sensitivity tests for two hyperparameters within IDAICL, i.e., \u03bb and \u03c4, using the GPT-2 model with 0.8B parameters, with m setting to 12. Optimal performance is achieved when \u03bb \u22480.5 and \u03c4 \u22481.</div>\nas listed in Table 10.\n# D More Comparison Results\nThe comparison results between Vanilla ICL and IDAICL on LLaMA models with 13B and 33B parameters across various datasets are presented in Table 7. Additionally, the corresponding results for GPT-2 models with 0.1B and 0.3B parameters are outlined in Table 8. It is evident that IDAICL\nconsistently outperforms Vanilla ICL across all datasets and different model sizes, highlighting the high generalization capability of IDAICL. Additionally, IDAICL showcases reduced performance variance and significantly enhances the worst-case performance. Based on the findings presented in Table 9, IDAICL generally outperforms other prediction calibration methods, demonstrating the significance of statistical properties derived from the input data distribution in the predictions of PLMs.\n# E More Results for Varying Templates\nThe comparison results between Vanilla ICL and IDAICL under all fifteen prompt templates are presented in Figure 7, illustrating that IDAICL consistently enhances both average and worst-case accuracy across all templates. Furthermore, the performance variance of IDAICL among different templates is notably smaller when compared to Vanilla ICL, highlighting the robustness of IDAICL\u2019s performance across diverse templates.\nDataset\nInstances\nLabel names\nSST-2\n1. This movie is amazing! (Label = \"Positive\")\n2. Horrific movie, don\u2019t see it. (Label = \"Negative\")\nPositive, Negative\nSST-5\n1. A pretensions \u2013 and disposable story \u2014 sink the movie. (Label =\n\"Great\")\n2. Apparently reassembled from the cutting-room floor of any given\ndaytime soap. (Label = \"Terrible\")\nTerrible, Bad, Okay, Good, Great\nMR\n1. Lame sweet home leaves no southern stereotype unturned. (Label\n= \"Negative\")\n2. Not so much farcical as sour. (Label = \"Negative\")\nNegative, Positive\nCR\n1. It takes excellent pics and is very easy to use, if you read the\nmanual. (Label = \"Negative\")\n2. Bluetooth does not work on this phone. (Label = \"Negative\")\nNegative, Positive\nAmazon\n1. Don\u2019t waste your money if you already have 2003... There isn\u2019t\none reason to get this update if you already have MS Money 2003\nDeluxe and Business. (Label =\"Terrible\")\n2. The game was in perfect condition! came before it said it should\nhave by 2 days!! I love the game and I suggest it to a lot of my\nfriends! (Label =\"Great\")\nTerrible, Bad, Okay, Good, Great\nSubj\n1. This is a story about the warm relationship between a little girl\nand her father despite the difficult conditions they have to live in.\n(Label = \"Objective\")\n2. Too slow, too boring, and occasionally annoying. (Label =\n\"Subjective\")\nSubjective, Objective\nTREC\n1. When did the neanderthal man live? (Label = \"Number\")\n2. How do you get a broken cork out of a bottle? (Label = \"Descrip-\ntion\")\nDescription, Entity, Expression,\nHuman, Location, Number\nDBPedia\n1. CMC Aviation is a charter airline based in Nairobi Kenya. (Label\n= \"Company\")\n2. Dialectica aemula is a moth of the Gracillariidae family. (Label =\n\"Animal\")\nCompany, School, Artist, Athlete,\nPolitics, Transportation, Building,\nNature, Village, Animal, Plant,\nAlbum, Film, Book\nAGNews\n1. Walk in park for Yankees Drained by a difficult week, the New\nYork Yankees needed an uplifting victory. (Label = \"Sports\")\n2. NASA Mountain View claims world\u2019s fastest computer. (Label =\n\"Technology\")\nWorld, Sports, Business,\nTechnology\nCB\n1. It was a complex language. Not written down but handed down.\nOne might say it was peeled down.\nThe language was peeled down.\n(Label = \"True\")\n2. \u201cDo you mind if I use your phone?\u201d Ronni could see that Guido\u2019s\nbrain was whirring.\nGuido\u2019s brain was whirring.\n(Label = \"True\")\nTrue, False, Neither\n<div style=\"text-align: center;\">Table 5: Examples and label names from all datasets.</div>\n# F More Sensitivity and Ablation Studies\nWe performed sensitivity tests on two hyperparameters within IDAICL: \u03bb and \u03c4. These values govern the strength of implicit augmentation and the influence of the class proportion term, respectively. As depicted in Figure 8, optimal performance is achieved when \u03bb\u22480.5 and \u03c4 \u22481 for both datasets. Furthermore, Figures 9(a) and (b) illustrate the average performance of ten datasets across different hyperparameter settings. Much like the earlier findings, the best average performance is identified at \u03bb = 0.5 and \u03c4 = 1. Consequently, setting \u03bb as 0.5 and \u03c4 as 1 is recommended for real applications. Furthermore, the performance remains sta-\nble within the ranges of \u03bb \u2208{0.25, 0.5, 0.75} and \u03c4 \u2208{0.5, 1, 1.5}, indicating that adjustments can be made within these stable ranges.\n# G More Results for Imbalanced Labels\nThe imbalanced label distribution in the training data has a significant impact on the classification performance of the model (Zhou and Wu, 2023b; Zhou et al., 2022). We depicted the confusion matrices for the SST-2 and MR datasets under two imbalance levels in Figures 9(c) and (d), in which the proportion of the negative class in demonstrations is set to 0.1 and 0.2. These results manifest that IDAICL significantly enhances the performance of the underrepresented classes in comparison to\nDataset\nTemplate\nLabel mapping\nSST-2\nReview: {Sentence}\nSentiment: {Label}\nPositive / Negative\nSST-5\nReview: {Sentence}\nSentiment: {Label}\nterrible / bad / okay / good / great\nMR\nReview: {Sentence}\nSentiment: {Label}\nPositive / Negative\nCR\nReview: {Sentence}\nSentiment: {Label}\nPositive / Negative\nSubj\nInput: {Sentence}\nType: {Label}\nobjective / subjective\nTREC\nQuestion: {Sentence}\nType: {Label}\ndescription / entity / expression / human / location / number\nAmazon\nReview: {Sentence}\nSentiment: {Label}\nterrible / bad / okay / good / great\nAGNews\nInput: {Sentence}\nType: {Label}\nworld / sports / business / technology\nDBPedia\nInput: {Sentence}\nType: {Label}\ncompany / school / artist / athlete / politics / transportation\nbuilding / nature / village / animal / plant / album / film / book\nCB\nPremise: {Sentence}\nHypothesis: {Sentence}\nPrediction: {Label}\ntrue / false / neither\n<div style=\"text-align: center;\">Table 6: Prompt templates and label mappings for each dataset.</div>\nPLM\nMethod\nm\nSST-2\nSST-5\nMR\nCR\nSubj\nTREC\nDBPedia\nAGNews\nCB\nAvg.\n13B\nVanilla ICL\n4\n95.67.1\n29.56.2\n90.05.8\n91.47.4\n72.96.9\n62.89.1\n80.97.6\n80.25.9\n51.58.2\n72.8\nIDAICL\n96.72.5\n47.11.1\n93.01.9\n93.30.8\n87.82.3\n76.02.6\n94.91.0\n87.72.4\n59.41.9\n81.8\nVanilla ICL\n8\n96.77.1\n39.45.6\n92.36.2\n92.24.8\n70.85.1\n71.29.1\n83.74.2\n79.56.3\n52.43.7\n75.4\nIDAICL\n96.92.1\n49.21.9\n93.41.6\n92.91.9\n87.53.0\n79.92.1\n93.60.9\n88.01.7\n62.42.5\n82.6\n33B\nVanilla ICL\n4\n95.57.2\n29.45.6\n91.75.4\n91.58.1\n85.16.0\n70.94.4\n86.64.5\n76.26.1\n59.25.3\n76.2\nIDAICL\n96.51.1\n46.82.4\n93.61.3\n92.33.3\n89.32.4\n79.11.5\n95.62.3\n88.41.9\n64.62.8\n82.9\nVanilla ICL\n8\n96.87.3\n34.35.4\n93.45.8\n92.76.4\n83.55.5\n66.94.8\n84.16.2\n84.75.5\n62.05.2\n77.6\nIDAICL\n96.92.3\n50.31.5\n93.92.2\n93.01.4\n89.01.0\n83.11.7\n95.92.0\n88.01.2\n70.41.8\n84.5\nTable 7: Comparison results of Macro-F1 between Vanilla ICL and IDAICL under varying values of m on the LLaMA models with 13B and 33B parameters.\nVanilla ICL, thus proving its capability to address the class imbalance in demonstrations.\n# H Varying Demonstration Permutations\nResearch has substantiated that the performance of ICL is sensitive to the permutation of demonstrations (Lu et al., 2022; Zhao et al., 2021). We assessed the performance of IDAICL under varying demonstration permutations. Specifically, we selected ten different sets of twelve training examples from the SST-2 datasets. For each set of examples, we shuffled the order ten times and calculated the accuracy for each permutation. The findings are depicted in Figure 10, indicating that IDAICL exhibits relatively stable performance across different demonstrations and permutations, while Vanilla ICL demonstrates high variance.\nPLM\nMethod\nm\nSST-2\nSST-5\nMR\nCR\nAmazon\nSubj\nTREC\nDBPedia\nAGNews\nCB\nGPT-2 0.1B\nVanilla ICL\n4\n56.37.1\n28.48.8\n55.47.4\n54.26.2\n30.88.4\n52.97.9\n32.25.1\n44.36.2\n42.89.3\n42.19.6\nIDAICL\n69.52.6\n35.31.1\n66.42.3\n67.22.7\n39.32.9\n57.22.6\n44.31.8\n62.22.3\n65.52.7\n49.21.9\nVanilla ICL\n8\n60.88.3\n30.66.9\n57.59.7\n56.05.1\n33.67.8\n53.75.6\n33.010.7\n52.15.8\n45.69.1\n45.46.2\nIDAICL\n71.41.8\n36.12.9\n67.61.8\n68.62.2\n40.00.7\n58.52.5\n45.61.9\n63.61.1\n66.91.6\n50.62.7\nVanilla ICL\n12\n64.56.0\n30.87.1\n59.35.6\n59.18.4\n33.95.5\n56.68.9\n35.87.1\n52.311.4\n47.46.0\n47.47.7\nIDAICL\n72.21.1\n36.72.2\n70.11.7\n69.31.8\n40.81.2\n60.91.5\n47.02.7\n65.51.9\n67.82.2\n51.23.3\nVanilla ICL\n16\n64.36.1\n33.57.1\n59.96.6\n61.77.5\n34.66.9\n56.16.2\n36.95.7\n54.17.2\n47.98.0\n48.97.7\nIDAICL\n72.92.5\n38.02.4\n69.71.3\n69.92.1\n41.70.9\n60.61.1\n46.61.9\n65.92.6\n65.71.0\n51.82.2\nGPT-2 0.3B\nVanilla ICL\n4\n60.87.5\n26.66.8\n50.57.1\n52.36.1\n30.55.2\n53.28.3\n32.88.1\n50.54.8\n41.35.9\n42.77.1\nIDAICL\n78.41.7\n33.12.5\n66.60.9\n70.32.3\n40.11.5\n69.41.7\n45.63.3\n66.22.1\n62.83.7\n50.41.8\nVanilla ICL\n8\n58.98.7\n29.46.1\n52.48.9\n54.88.2\n32.77.9\n53.56.7\n34.08.2\n59.19.7\n43.86.4\n46.97.6\nIDAICL\n80.81.7\n34.81.9\n69.51.1\n71.50.8\n41.51.7\n70.32.6\n46.22.2\n68.11.7\n63.32.1\n51.52.5\nVanilla ICL\n12\n62.914.4\n30.67.8\n55.26.2\n56.16.7\n34.27.5\n56.87.1\n36.29.8\n58.07.3\n46.59.3\n48.66.6\nIDAICL\n82.22.3\n36.11.8\n68.92.4\n72.01.5\n43.70.6\n71.42.4\n48.31.3\n70.51.9\n65.22.2\n52.91.4\nVanilla ICL\n16\n67.46.3\n31.77.1\n57.68.6\n56.65.2\n34.76.2\n57.05.3\n38.16.9\n59.38.2\n45.27.6\n49.48.7\nIDAICL\n81.52.8\n36.81.2\n70.41.7\n72.92.1\n43.11.3\n71.92.7\n48.71.1\n70.92.9\n65.81.2\n52.41.8\nTable 8: Accuracy comparison between Vanilla ICL and IDAICL under varying values of m on the GPT-2 models with 0.1B and 0.3B parameters.\nPLM\nMethod\nSST-5\nMR\nAGNews\nTREC\nSST-2\nSubj\nDBPedia\nAvg.\nGPT-2 1.5B\nVanilla ICL\n30.86.1\n64.98.3\n57.56.7\n40.45.1\n57.27.0\n57.310.3\n67.67.5\n53.7\nConCa\n32.87.1\n74.55.1\n62.76.1\n45.82.5\n73.98.6\n68.37.4\n75.04.0\n61.9\nPROCA\u2217\n36.54.4\n80.86.4\n75.53.2\n46.02.5\n88.01.3\n80.23.3\n89.40.7\n70.9\nD-ConCa\n31.73.3\n80.93.7\n77.04.1\n47.12.8\n86.54.4\n76.85.2\n86.16.3\n69.4\nIDAICL\n40.81.9\n82.11.2\n80.82.4\n52.02.5\n89.51.8\n80.12.9\n91.02.5\n73.8\nGPT-Neo\nVanilla ICL\n31.56.4\n70.68.1\n71.96.8\n53.06.9\n74.98.3\n57.96.3\n78.56.5\n62.6\nConCa\n33.94.3\n78.25.3\n73.63.8\n55.97.2\n82.09.5\n71.36.4\n90.03.6\n69.3\nPROCA\u2217\n39.44.0\n77.813.9\n78.92.5\n56.03.6\n91.91.2\n81.33.8\n92.01.5\n73.9\nD-ConCa\n32.94.1\n84.62.8\n81.23.9\n57.64.7\n91.65.3\n70.92.9\n85.73.1\n72.1\nIDAICL\n42.22.5\n85.91.6\n83.11.9\n61.41.7\n91.22.4\n82.33.1\n93.01.5\n77.0\nTable 9: Accuracy comparison between IDAICL and other prediction calibration approaches using the GPT-2 (with 1.5B parameters) and GPT-Neo models, with m setting to 8. The templates used align with those utilized by Han et al. (2023). \u2217indicates that the results were derived from the original paper.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be03/be03cef1-446a-435f-8ac9-e2df27366c98.png\" style=\"width: 50%;\"></div>\nFigure 9: (a) and (b): Average accuracy across ten datasets for various values of \u03bb and \u03c4. Optimal average performance is attained when \u03bb = 0.5 and \u03c4 = 1. (c) and (d): Confusion matrices for the SST-2 and MR datasets under two levels of imbalance, where the proportions of the negative class in demonstrations are set to 0.1 and 0.2, respectively. When compared to Vanilla ICL, IDAICL improves the performance of the minor class. These experiments are conducted on the GPT-2 model with 1.5B parameters, setting m to 12.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f973/f973cade-269c-4c9f-9b0f-dc0d1e878501.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Comparison results between Vanilla ICL and IDAICL across various demonstrations and permutations. The GPT-2 model with 0.8B parameters is employed for this analysis, setting m to 12. IDAICL exhibits smaller performance variance across different demonstrations and permutations compared to Vanilla ICL.</div>\nFormat ID\nPrompt\nLabel names\n1\nReview: This movie is amazing!\nAnswer: Positive\nReview: Horrific movie, don\u2019t",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) allows large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. However, ICL's effectiveness is heavily reliant on the quality, quantity, and permutation of demonstrations, which often leads to suboptimal performance. This paper addresses the challenge of demonstration augmentation to improve ICL performance.",
        "problem": {
            "definition": "The problem addressed in this paper is the instability and suboptimal performance of ICL due to the dependence on the quality and arrangement of demonstrations.",
            "key obstacle": "The main obstacle is the difficulty in efficiently capturing sufficient knowledge from context, which leads to high variance and poor worst-case accuracy in predictions."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that deep features in neural networks are often linearized, suggesting the existence of numerous semantic directions within the deep feature space that can enrich contextual knowledge.",
            "opinion": "The proposed idea involves augmenting demonstrations by transforming them along semantic directions sampled from the deep feature distribution, thereby enhancing their representation without increasing input length.",
            "innovation": "The key innovation is the introduction of a novel logit calibration mechanism derived from the statistical properties of demonstrations, which approximates the effects of infinite augmentation without needing explicit augmentation."
        },
        "method": {
            "method name": "Implicit Demonstration Augmentation-based In-Context Learning (IDAICL)",
            "method abbreviation": "IDAICL",
            "method definition": "IDAICL is a method that enhances ICL by augmenting demonstrations through implicit transformations based on their deep feature distribution.",
            "method description": "The core of IDAICL is to leverage deep feature distributions for implicit augmentation to improve prediction accuracy and stability.",
            "method steps": "1. Sample semantic vectors from the deep feature distribution of demonstration examples. 2. Transform the demonstration features using these vectors. 3. Utilize the transformed features to derive predictions using a novel prediction function.",
            "principle": "The effectiveness of IDAICL lies in its ability to capture richer contextual knowledge through implicit augmentation, which stabilizes predictions and enhances accuracy."
        },
        "experiments": {
            "evaluation setting": "The experiments involved evaluating IDAICL across seven PLMs and ten text classification datasets, including sentiment analysis and news classification tasks.",
            "evaluation method": "Performance was assessed by comparing the accuracy of IDAICL against baseline methods under various conditions, including different numbers of demonstrations and imbalanced datasets."
        },
        "conclusion": "IDAICL significantly improves both average and worst-case accuracy for ICL, demonstrating greater stability and robustness compared to existing methods.",
        "discussion": {
            "advantage": "The main advantages of IDAICL include improved prediction accuracy, reduced performance variance, and enhanced stability across different demonstration settings.",
            "limitation": "IDAICL's reliance on the parameters of the final fully connected layer limits its application to open-source PLMs, and it may struggle with very small demonstration sets.",
            "future work": "Future research could explore more effective augmentation strategies, extend IDAICL to black-box models, and investigate its application in training and fine-tuning phases."
        },
        "other info": {
            "recommendations": {
                "\u03bb": "0.5",
                "\u03c4": "1"
            },
            "additional insights": "IDAICL has shown to be effective in handling class imbalance and improving underrepresented classes' performance."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) allows large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters."
        },
        {
            "section number": "1.2",
            "key information": "The effectiveness of ICL is heavily reliant on the quality, quantity, and permutation of demonstrations, which often leads to suboptimal performance."
        },
        {
            "section number": "3.1",
            "key information": "The main obstacle in ICL is the difficulty in efficiently capturing sufficient knowledge from context, leading to high variance and poor worst-case accuracy in predictions."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method, Implicit Demonstration Augmentation-based In-Context Learning (IDAICL), enhances ICL by augmenting demonstrations through implicit transformations based on their deep feature distribution."
        },
        {
            "section number": "4.1",
            "key information": "The key innovation of IDAICL is the introduction of a novel logit calibration mechanism derived from the statistical properties of demonstrations, which approximates the effects of infinite augmentation without needing explicit augmentation."
        },
        {
            "section number": "5.1",
            "key information": "The experiments involved evaluating IDAICL across seven PLMs and ten text classification datasets, including sentiment analysis and news classification tasks."
        },
        {
            "section number": "6.1",
            "key information": "IDAICL's reliance on the parameters of the final fully connected layer limits its application to open-source PLMs and may struggle with very small demonstration sets."
        },
        {
            "section number": "7",
            "key information": "IDAICL significantly improves both average and worst-case accuracy for ICL, demonstrating greater stability and robustness compared to existing methods."
        }
    ],
    "similarity_score": 0.7221014054934359,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Enhancing In-Context Learning via Implicit Demonstration Augmentation.json"
}