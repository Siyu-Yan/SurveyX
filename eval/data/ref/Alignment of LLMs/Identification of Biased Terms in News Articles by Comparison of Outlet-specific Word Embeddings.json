{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2112.07384",
    "title": "Identification of Biased Terms in News Articles by Comparison of Outlet-specific Word Embeddings",
    "abstract": "Slanted news coverage, also called media bias, can heavily influence how news consumers interpret and react to the news. To automatically identify biased language, we present an exploratory approach that compares the context of related words. We train two word embedding models, one on texts of left-wing, the other on right-wing news outlets. Our hypothesis is that a word's representations in both word embedding spaces are more similar for non-biased words than biased words. The underlying idea is that the context of biased words in different news outlets varies more strongly than the one of non-biased words, since the perception of a word as being biased differs depending on its context. While we do not find statistical significance to accept the hypothesis, the results show the effectiveness of the approach. For example, after a linear mapping of both word embeddings spaces, 31% of the words with the largest distances potentially induce bias. To improve the results, we find that the dataset needs to be significantly larger, and we derive further methodology as future research direction. To our knowledge, this paper presents the first in-depth look at the context of bias words measured by word embeddings.",
    "bib_name": "spinde2021identificationbiasedtermsnews",
    "md_text": "# Identification of Biased Terms in News Articles  by Comparison of Outlet-specific Word Embeddings\nTimo Spinde1,2[0000-0003-3471-4127], Lada Rudnitckaia1[0000-0003-2444-8056], Felix Hamborg1 [0000-0003-2444-8056], and Bela Gipp2[0000-0001-6522-3019] \n1 University of Konstanz, Konstanz, Germany  {firstname.lastname}@uni-konstanz.de 2 University of Wuppertal, Wuppertal, Germany  {last}@uni-wuppertal.de  \nAbstract. Slanted news coverage, also called media bias, can heavily influence how news consumers interpret and react to the news. To automatically identify biased language, we present an exploratory approach that compares the context  of related words. We train two word embedding models, one on texts of leftwing, the other on right-wing news outlets. Our hypothesis is that a word's representations in both word embedding spaces are more similar for non-biased words than biased words. The underlying idea is that the context of biased words in different news outlets varies more strongly than the one of non-biased words, since the perception of a word as being biased differs  depending on its context. While we do not find statistical significance to accept the hypothesis, the results  show the effectiveness of the approach. For example, after a linear mapping of both word embeddings spaces, 31% of the words with the largest distances potentially induce bias. To improve the results, we find that the dataset needs to be significantly larger, and we derive further methodology as future research direction. To our knowledge, this paper presents the first in-depth look at the context of bias words measured by word embeddings. \nKeywords: Media bias, news slant, context analysis, word embeddings\n# 1  Introduction\nNews coverage is not just the communication of facts; it puts facts into context and  transports specific opinions. The way how \"the news cover a topic or issue can decisively impact public debates and affect our collective decision making\" [12], slanted  news can heavily influence the public opinion [11].  However, only a few research  projects yet focus on automated methods to identify such bias.  One of the reasons that make the creation of automated methods more difficult is the  complexity of the problem: How we perceive bias is not only dependent on the word  itself, but also its context, the medium, and the background of every reader. While many  current research projects focus on collecting linguistic features to describe media bias,  we present an implicit approach to the issue.  The main question we want to answer is: \nComparing biased words among word embeddings created from different news outlets,  are they more distant (or close) to each other than non-biased words?  To answer this question, we measure any word's context by word embeddings, which  reflect the specific usage of a word in a particular medium [15]. We focus on the definition of language bias given by Recasens et al. [10], describing biased words as subjective and linked to a particular point of view. Such words can also change the believability of a statement [10].   Overall, our objectives are to:   1) Analyse and compare the word embeddings of potential bias inducing words  trained on different news outlets.  2) Test the assumption that distances between vectors of similar bias words trained  on different corpora are larger than between neutral words due to usage in a specific  context. \n# 2  Related work\nWhile some scholars propose methods to create bias lexica automatically, none of them is in the domain of news articles. Recasens et al. [10] create a static bias lexicon based on Wikipedia bias-driven edits, which they combine with a set of various linguistic features. Ultimately, they aim to classify words as being biased or not. Hube & Fetahu [5] extend this approach by manually selecting bias-inducing words from a highly biased source (Conservapedia) and retrieving semantically close words in a Wikipedia word embedding space.  Since there is no large-scale dataset from which initial knowledge about biased language can be derived, implementation and extending of the approaches of Recasens et al. and Hube & Fetahu may be relevant for news data. However, in the context of media bias identification, creating static bias lexica is inefficient because the interpretation of language and wording strongly depends on its context [3].   It is therefore desirable to either evaluate every word independent of pre-defined lexica or, even more, enable existing biased lexica to be context-aware. In this regard, exploiting the properties of word embeddings is especially interesting [15]. Word embeddings are highly dependent on training corpora they are obtained from and accurately reflect biases and stereotypes in the training corpora [15]. Kozlowski et al. [6] use word embeddings trained on literature from different decades to estimate the evolution of social class markers over the 20th century.  Mikolov et al. [9] compare word embeddings obtained from different languages and show that similar words have minimal cosine similarity. Tan et al. [15] analyze the usage of the same words in Twitter and Wikipedia by comparing their different word representations \u2013 one trained on Twitter data and another on Wikipedia. \n# 3  Methodology\nWe seek to devise an automated method that ultimately finds biased words by comparing two (or more) word embeddings spaces, each trained on a differently slanted group \nof text documents. In this exploratory study, we devise a one-time process that consists of four tasks: selection of a word embedding model, selection of biased words, data processing and analysis, and linear mapping of the word embedding spaces.   \n# 3.1  Word embeddings and parameter selection\nTo calculate our embeddings, we use Word2Vec [8] with the Continuous Skip-gram  (SG) architecture, which achieves better semantic accuracy and slightly better overall  accuracy than the Bag-of-Words architecture [9]. We evaluated our word representations via an estimation of word semantic similarity on two datasets \u2013 WordSim-353 [2]  and MEN [1] and the Google analogy test set [8].   WordSim-353 consists of 353 pairs assessed by semantic similarity with a scale from  0 to 10. MEN consists of 3,000 pairs assessed by semantic relatedness and scaled from  0 to 50. We use these datasets since they focus on topicality and semantics. The Google  analogy test set consists of 8,869 semantic and 10,675 syntactic questions. Generally,  for our task, the data sets are not ideal, which we discuss in section 5.   We summarize our hyper-parameters in Table 1 and the summary evaluation of our  word embeddings in Table 2. We train the word embeddings on the data preprocessed  with Genism simple preprocessing and n-grams generated within two passes. \n<div style=\"text-align: center;\">Table 1. Hyper-parameters for training the word embeddings </div>\nHyper-parameter \nValue \nHyper-parameter \nValue \ndimensionality \n300 \nmaximum token length \n28 \nwindow size \n8 \nn-grams threshold (1st pass) \n90 \nsubsampling rate \n10-5 \nn-grams threshold (2nd pass) 120 \n# of iterations \n10 \narticles titles \nincluded \nminimum frequency \n25 \ntraining sentence \nthe whole article \nfunction \nhierarchical softmax \n \n \n<div style=\"text-align: center;\">Table 2. Evaluation of the word embeddings</div>\nCorpora \n# articles \n# tokens \nVocabulary size \nSemantic similarity \nAnalogy \nWordSim-353 \nMEN \nGoogle \nHuffPost \n101K \n68M \n53K \n0.65 \n0.71 \n0.50 \nBreitbart \n81K \n39M \n37K \n0.57 \n0.59 \n0.38 \n \n# 3.2  Manual selection of bias inducing words \nWe follow the approach proposed by Hube & Fetahu [5] and manually select a small  set of \"seed\" words that are very likely to be related to controversial opinions. They also have a high density of bias-inducing words surrounding them in the embedding  space. The 87 seed words are selected based on the description of controversial left and right topics on Allsides.com (Table 3, see also https://www.allsides.com/mediabias/left, \u2026/right). From the list of the closest twenty words to each seed word, we\nmanually extracted words that convey a strong opinion [5]. As the identification of bias is not trivial for humans [10], we validated the extended seed words by four student volunteers, age 23 - 27, who labeled each word as being biased or not. We discarded any words where less than three students agreed on.  \n<div style=\"text-align: center;\">Table 3. The seed words that are likely to be related to controversial opinions and to have a  high density of bias-inducing words surrounding them in the word embedding space </div>\nDivisive issue \nSeed words \nThe role of the gov-\nernment \nregulation(s), involvement, control, unregulated, government, centraliza-\ntion, law \nEconomics \ntax(es), taxation, funding, spending, corporation(s), business(es), econ-\nomy \nEquality \nequality, inequality, rights, equal_rights, wealth, living_wage, welfare, \nwelfare_state \nSocial services \nservices, government_services, social_security, benefit(s), help, stu-\ndent(s), loan(s), student_loan(s), education, healthcare, individual, per-\nsonal_responsibility, collective \nSecurity \nsecurity, military, military_force, defense, intervention, protect, protec-\ntion, border, border_security, migration, migrant(s), immigration, immi-\ngrant(s), terror, terrorist(s) \nTraditions, religion, \nand culture \ntradition, norms, cultural_norms, progress, change(s), race, racism, gen-\nder, sexual, orientation, sexual_orientation, identity, religion, Islam, tol-\nerance, multiculturalism, values, family_values, bible, constitution \nMiscellaneous \nfreedom, speech, freedom_of_speech, free_speech, hate_speech, gun(s), \ngun_owner(s), abortion, environment, media \n \n# 3.3  Data\nWe choose two news outlets that are known to take different views and potentially use  different words to describe the same phenomena. We based the choice of news outlets  for analysis on the media bias ratings provided by Allsides.com. The news aggregator  aims to estimate the overall slant of an article and a news outlet by combining users'  feedback and expert knowledge [3, 13].  We choose The HuffPost as a left-wing news  outlet and Breitbart News as right-wing. We scraped articles from both news outlets,  published in the last decade, from 2010 to 2020, from Common Crawl [4]. For preprocessing, we use Genism simple preprocessing and generate n-grams.   \n# 3.4  Linear mapping between vector spaces\nSince the goal is to compare word vectors between two different word embedding  spaces, it is necessary to make sure that these two word embedding spaces have similar  dimensionality. We use the approach proposed by Mikolov et al. [15] and Tan et al.  [20].  The results of training two different mapping matrices \u2013 trained on 3,000 most frequent words and on the whole common vocabulary \u2013 are presented in Table 4. The only \nmetric to evaluate mapping quality is the number of distant words. Ideally, similar words should be close to each other after linear mapping. A large number of very distant words can be an indicator of a poorly trained matrix.  We assessed the distance between words in an embedding space with cosine similarity. In case the distance between similar words after mapping from one source to another depends on the frequency of the word in either a left- or right-wing source, according to Tan et al. [15], adjusted distances should be compared. The larger an adjusted distance, the less similar the word is between the two sources since positive adjusted distance values belong to words that are less similar than at least half of the words in their frequency bucket. \n<div style=\"text-align: center;\">Table 4. Comparison of two linear mappings</div>\nMapping \nmatrix \ntrained on \n# tokens \nin com-\nmon vo-\ncab. \nMedian \ncos. \nsim. \n# distant words \n# close \nwords \nCorrelation of cos. \nsim. with freq. in \ncos. sim.  \n\u2264 0.4 \nadj. cos. \nsim. \u2265 0.1 \ncos. sim.  \n\u2265 0.6 \nHuffPost Breitbart \n3K  \n30K \n0.48 \n8K (25%) 5K (17%) \n6K (20%) \n0.14 \n0.13 \nwhole vo-\ncab.  \n0.56 \n2K (6%) \n4K (14%) \n10K \n(35%) \n0.12 \n0.12 \n \nFor both variants, we still obtained many distant words after linear mapping, i.e., median cosine similarity is 0.48 and 0.56 for the first and the second variant, respectively. Ideally, similar words should be close to each other after linear mapping, except those used in different contexts. Possible reasons for having many distant words are:  \u2022  low quality of trained word embeddings and, thus non-stable word vectors,  \u2022  low quality of mapping matrix, possibly nonlinear transformation is needed,  \u2022  a high number of \"bad\" n-grams,  \u2022  a high number of noisy words.  We tried to address the first two causes by training different word embeddings models and different mappings. Since Tan et al. [15] did not discuss the threshold for the definition of distant words, we choose the thresholds to define distant words as lower than 0.4 and higher than 0.1 for pure and adjusted cosine similarities, respectively. Comparing n-grams and unigrams based on their cosine similarity statistics, we conclude that there is no apparent reason to think that the generated n-grams are more distant than the unigrams: median cosine similarity for n-grams is 0.62, whereas for unigrams it is 0.55. We manually inspected the distant words to estimate the possible influence of flaws in preprocessing and connection to bias words (Section 4.1).  The matrix trained on the whole vocabulary maps similar words better since there are fewer very distant words, and the median cosine similarity for the words is higher. Therefore, we used this mapping for further analysis.  At the two-dimensional graph obtained by reducing the dimensions of the word vectors from 300 to 2 with PCA, it can be seen that the mapping works quite well for the most frequent words, here, the pronouns: the word vectors mapped from HuffPost to Breitbart are indeed closer to the vectors from Breitbart than the initial vectors from the\nHuffPost (Figure 1 a). However, we also get a high number of distant words (Figure 1  b). We can also see that the higher the frequency, the higher the chance that the words  are better mapped from one source to another (Figure 1 c). Simultaneously, for less  frequent words, the results of mapping vary: some words are mapped very well and  some very poorly (Figure 1 d). We can see the same patterns can for adjusted distances in Figure 1, e-h.   \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cce0/cce053ed-645d-4ad7-9d33-a86c8905237e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7b28/7b2893a6-8276-4a41-9d47-0905e79a0b4a.png\" style=\"width: 50%;\"></div>\nFig. 1. Linear mapping from HuffPost to Breitbart trained on the whole vocabulary: a) highfrequency word vectors before and after mapping, b) distribution of cosine similarities after mapping, c) dependency of frequency and cosine similarities, d) dependency of frequency and cosine  similarities for the words less frequent than 2K, e) median cosine similarities per frequency  bucket, f) distribution of adjusted cosine similarities after mapping, g) dependency of frequency  and adjusted cosine similarities, h) dependency of frequency and adjusted cosine similarities for  the words less frequent than 2K.  \n# 4  Results\n# 4.1  Distant words\nWe manually examine the top 1,000 most distant words, both with low cosine similarity and high adjusted cosine similarity. The lists highly overlap, i.e., the top 1,000  words with high adjusted cosine similarity introduce only 126 new words. \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4538/4538fbd3-2116-47a8-af75-1090e111df6a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nNames and surnames make up a large part of the 1,000 most distant words with 387  occurrences. Fifty-nine words are short words that are either abbreviations or noise.  Among the rest, the number of words that can potentially induce bias is 172 (31%), if  not to consider mentioned names and short words.  \n# 4.2  Relation of distant words to bias inducing words\nFor the manually selected bias words, we find no pattern regarding the distance of  their vectors between HuffPost and Breitbart. The median cosine similarity is 0.60, which is slightly higher than the median cosine similarity of the words in the whole  common vocabulary. Median adjusted cosine similarity is -0.05, which also shows that the words in this group are, in general, even slightly closer to each other than the words  in the same frequency buckets.   Since the number of manually selected words is quite small, we also check the relation of distant words to the words from the bias lexicon automatically created by Hube  & Fetahu [9]. Out of 9,742 words in the lexicon, we encountered 3,334  in the common vocabulary of the HuffPost and Breitbart. This check contradicts with the main idea \u2013 that bias words differ from context to context and from outlet to outlet \u2013 but we conduct it for additional insight and confirmation that bias inducing words are not directly related to distant ones.   Similarly to the manually selected bias words, for the words from the bias lexicon,  we found no pattern regarding the distance of their vectors between different outlets.  The median cosine similarity is 0.52, slightly lower than the median for all the words  in the common vocabulary. The Median adjusted distance is 0.03, which means that the words in this group are, in general, just slightly more distant than other words with the same frequency. Therefore, this finding does not allow to claim that bias words are in  general more distant than other words but rather corroborates that bias inducing words  are not directly connected with distant words.    Overall, there are no salient differences when comparing the context of biased words between HuffPost and Breitbart. The most noticeable differences are between  the context of the words \"regulations,\" \"welfare,\" \"security,\" \"border,\" \"immigration,\"  \"immigrants,\" \"hate_speech,\" and \"abortion\". We also notice the differences in the context of the words that have more than one meaning, e.g., the word \u201cnut\u201d is surrounded  by the words describing food in the word embeddings trained on the HuffPost corpus. In contrast, in the word embeddings trained on the Breitbart corpus, it is surrounded by  such words as \u201chorrid\u201d, \u201chater\u201d, etc. Such findings are rare, and their statistical significance should be proved on the exhaustive biased words lexicon and the word embeddings trained on larger datasets.   \n# 5  Conclusion and future work\nWe present experimental results of an approach for the automated detection of media  bias using the implicit context of bias words, derived through fine-tuned word embeddings. Our key findings are:  \n1) Among the words with large distances after linear mapping, some can potentially induce bias. Their percentage is around 25% (if not to consider names, surnames, and short words that can be either abbreviations or noise, otherwise the ratio is about 15%).  2) In the small set of manually selected bias inducing words, median cosine similarity after the linear mapping is 0.6 which is even slightly higher than for the whole vocabulary. A direct relation to large distances also did not show on the words from the bias lexicon provided by Hube et al. [5].  3) There are no salient differences in the context of seed words apart from several words.  Obtained results are either point to the absence of a relation of bias and distant words or can be explained by the following flaws of the current project, which serve as future research directions. First, the data for training our word embeddings are relatively scarce. Intrinsic evaluation of the word embeddings trained on the Breitbart corpora shows low results. Our current evaluation methods do not reflect the actual suitability of the word embeddings for our specific task. Second, we did not test other word embedding models than Word2Vec, which might show a better overall performance, e.g., GloVe, BERT, Elmo, and Context2Vec [7]. We did also not integrate other features, such as lexical cues or sentiment. Third, bias inducing words are selected manually by a tiny group of non-native English speakers. Fourth, we based the comparison of context on the top 20 most similar words. But among these top twenty for one source, the similarity can be on average high and for another on average low.  \n# References \n# How to cite this paper:\n# BibTex:\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of media bias in news coverage, emphasizing the complexity of identifying biased language due to the influence of context, medium, and reader background. Previous methods have focused on collecting linguistic features, but this study presents an implicit approach by comparing word embeddings from different news outlets to assess bias.",
        "problem": {
            "definition": "The problem is the difficulty in automatically identifying biased language in news articles, which is influenced by the context in which words are used.",
            "key obstacle": "The main challenge is that bias perception varies based on context, making it hard for existing methods to reliably identify biased language."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that biased words have varying contexts in different news outlets, leading to differences in their word embeddings.",
            "opinion": "The proposed method involves comparing word embeddings trained on left-wing and right-wing news articles to identify biased terms based on their contextual usage.",
            "innovation": "The innovation lies in using word embeddings to capture the contextual differences of biased words across different news sources, moving beyond static bias lexicons."
        },
        "method": {
            "method name": "Contextual Word Embedding Comparison",
            "method abbreviation": "CWEC",
            "method definition": "A method that compares the word embeddings of biased and non-biased words from different news outlets to identify potential biases.",
            "method description": "This method analyzes the distances between word vectors in different embedding spaces to detect bias.",
            "method steps": [
                "Select word embedding models for different news outlets.",
                "Manually select seed words likely to induce bias.",
                "Process the data and generate word embeddings.",
                "Perform linear mapping of the word embedding spaces."
            ],
            "principle": "The effectiveness of this method is based on the assumption that biased words will have greater distances in word embedding spaces compared to non-biased words."
        },
        "experiments": {
            "evaluation setting": "The experiments involved training word embeddings on articles from HuffPost (left-wing) and Breitbart (right-wing) between 2010 and 2020, using datasets like WordSim-353 and MEN for evaluation.",
            "evaluation method": "Performance was assessed through cosine similarity metrics and manual inspection of the most distant words to identify potential biases."
        },
        "conclusion": "The study concludes that while the method shows promise in identifying biased words, the results indicate a need for larger datasets and further refinement of the methodology to enhance accuracy in detecting media bias.",
        "discussion": {
            "advantage": "The key advantage of this approach is its ability to uncover contextual biases in language that static lexicons may miss.",
            "limitation": "A limitation is the small dataset size used for training word embeddings, which may not capture the full range of biases present in media language.",
            "future work": "Future research should focus on expanding the dataset, exploring different word embedding models, and integrating additional linguistic features to improve bias detection."
        },
        "other info": {
            "additional notes": "The paper provides insights into the relationship between biased language and its context, highlighting the need for more comprehensive datasets and methodologies in future studies."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of media bias in news coverage, emphasizing the complexity of identifying biased language due to the influence of context, medium, and reader background."
        },
        {
            "section number": "2",
            "key information": "The problem is the difficulty in automatically identifying biased language in news articles, which is influenced by the context in which words are used."
        },
        {
            "section number": "2",
            "key information": "The main challenge is that bias perception varies based on context, making it hard for existing methods to reliably identify biased language."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method involves comparing word embeddings trained on left-wing and right-wing news articles to identify biased terms based on their contextual usage."
        },
        {
            "section number": "3.3",
            "key information": "The effectiveness of this method is based on the assumption that biased words will have greater distances in word embedding spaces compared to non-biased words."
        },
        {
            "section number": "8",
            "key information": "Future research should focus on expanding the dataset, exploring different word embedding models, and integrating additional linguistic features to improve bias detection."
        },
        {
            "section number": "9",
            "key information": "The study concludes that while the method shows promise in identifying biased words, the results indicate a need for larger datasets and further refinement of the methodology to enhance accuracy in detecting media bias."
        }
    ],
    "similarity_score": 0.5639378741951169,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1730_natur/papers/Identification of Biased Terms in News Articles by Comparison of Outlet-specific Word Embeddings.json"
}