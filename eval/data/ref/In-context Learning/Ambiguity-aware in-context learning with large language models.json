{
    "from": "google",
    "scholar_id": "jY84x4QqSFgJ",
    "detail_id": null,
    "title": "Ambiguity-aware in-context learning with large language models",
    "abstract": " Abstract\n\nIn-context learning (ICL), i.e., showing large language models (LLMs) only a few taskspecific demonstrations, has led to downstream gains without task-specific fine-tuning. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM\u2019s existing knowledge about that task. From prior work (Lyu et al., 2023), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether  considering LLM\u2019s existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demonstrations but also to choose those demonstrations that help resolve the inherent label ambiguity surrounding the test example. Interestingly, we find that including demonstrations that the LLM previously mis-classified and also fall on the test example\u2019s decision boundary, brings the most performance gain.\n\n# Introduction\n\nLeveraging LLMs (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022) via in-context learning  (ICL) is now a popular strategy for improving downstream task performance, wherein the model is able to perform a task by simply being conditioned on the task definition and/or few task demonstrations (input-output examples) (Brown et al., 2020; Xie et al., 2021). As ICL gets increasingly adopted, it has brought to light (Lester et al., 2021; Liu et al., 2022; Zhang\n\u2217 Work done as an intern at Google Research.\n\nGoogle Research\nbemike@google.com\n\net al., 2022; Lu et al., 2022) that LLMs are sensitive to the choice of prompts, makin",
    "bib_name": "gao2023ambiguity",
    "md_text": "Lingyu Gao \u2217\nAditi Chaudhary\nToyota Technological Institute at Chicago\nGoogle Research\nlygao@ttic.edu\naditichaud@google.com\n\nLingyu Gao \u2217\nAditi Chaudhary\nKrishna Srini\nToyota Technological Institute at Chicago\nGoogle Research\nGoogle Resear\nlygao@ttic.edu\naditichaud@google.com\nkrishnaps@googl\n\nLingyu Gao \u2217\nToyota Technological Institute at Chicago\nlygao@ttic.edu\n\n# Kazuma Hashimoto\n\n# Karthik Raman\n\nGoogle Research\nkazumah@google.com\n\nGoogle Research\nkarthikraman@google.com\n\n# Abstract\n\nIn-context learning (ICL), i.e., showing large language models (LLMs) only a few taskspecific demonstrations, has led to downstream gains without task-specific fine-tuning. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM\u2019s existing knowledge about that task. From prior work (Lyu et al., 2023), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether  considering LLM\u2019s existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demonstrations but also to choose those demonstrations that help resolve the inherent label ambiguity surrounding the test example. Interestingly, we find that including demonstrations that the LLM previously mis-classified and also fall on the test example\u2019s decision boundary, brings the most performance gain.\n\n# Introduction\n\nLeveraging LLMs (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022) via in-context learning  (ICL) is now a popular strategy for improving downstream task performance, wherein the model is able to perform a task by simply being conditioned on the task definition and/or few task demonstrations (input-output examples) (Brown et al., 2020; Xie et al., 2021). As ICL gets increasingly adopted, it has brought to light (Lester et al., 2021; Liu et al., 2022; Zhang\n\u2217 Work done as an intern at Google Research.\n\nGoogle Research\nbemike@google.com\n\net al., 2022; Lu et al., 2022) that LLMs are sensitive to the choice of prompts, making \u201cprompt engineering\u201d for different tasks challenging and time-consuming. However, prompt engineering does not have to be a complete guessing game; rather it can be governed by some data-derived signals. For example, selecting demonstrations that are semantically similar to a new input has shown to be more effective over randomly sampled demonstrations (Das et al., 2021; Liu et al., 2022; Margatina et al., 2023), wherein a text retriever is used to select the topk training examples for each test example based on the input text. The motivation is that using information from existing similar situations will help solve a new problem (Aamodt and Plaza, 1994). However, the solely input-based selection does not explicitly capture the LLM\u2019s existing knowledge about the task-specific label space of both the ICL demonstration as well as the test input. For example, on a five-way sentiment classification task (SST (Socher et al., 2013)), we have observed that the Flan-PaLM 2 model (size L) (Anil et al., 2023) is confused between two specific labels, \u2018Very Negative\u2019 and \u2018Negative,\u2019 a lot more than say between \u2018Neutral\u2019 and \u2018Very Negative\u2019, as shown in  Figure 2. This motivates us to investigate whether the model\u2019s existing knowledge can also be leveraged to select even more effective demonstrations. Specifically, we derive signals from the underlying LLM about the output label space of both the new test example and the training data from which we select the demonstrations. As motivated above, the model\u2019s ambiguity around the new test example\u2019s output label will help us know what the model is most confused about, which in turn can be used to select those demonstrations that help reduce this confusion. For selecting such demonstrations from the training data, we propose to consider not only the ground truth labels paired with these demonstrations, but also the usefulness by looking at their\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8ada/8ada831b-ede5-4989-99ab-293a33155583.png\" style=\"width: 50%;\"></div>\nFigure 1: Overview of our proposed method for selecting ICL demonstrations: For each test example, we first use a retriever to rank training data by semantic similarity. At the same time, we identify the ambiguous label set for each test example and also obtain the output predictions on the retrieved training data. Next, we apply three constraints on the top-ranked demonstrations which are: 1) select those demonstrations whose gold label is in the ambiguous label set, 2) select those which are also mis-classified by the model, and 3) select those mis-classified examples whose predicted label is in the ambiguous label set. Finally, we construct prompts with selected ICL demonstrations to get the final model predictions.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6d5/d6d516a2-2056-4cce-b721-9f66b2f6f95b.png\" style=\"width: 50%;\"></div>\nFigure 2: Confusion Matrix of zero-shot experiments on SST with Flan-PaLM 2 (L). Labels: VPos (Very Positive), Pos (Positive), Neu (Neutral), Neg (Negative), VNeg (Very Negative).\n\nmodel prediction. First, given a test example and pool of training data, for each test example we use an off-the-shelf retriever to retrieve topk examples that have similar input text. For each test example, we identify an ambiguous label set of two output labels that the model is most confused about. Next, we select top-ranked demonstrations such that their ground truth labels lie in the above label set. To further find useful demonstrations, we identify those which are mis-classified by the model; the intu\n\n<div style=\"text-align: center;\">LLM Model prediction\n</div>\nition is that showing the model a previously misclassified demonstration could force it to correct it (Tan, 2006; Wang et al., 2020). Finally, on top of the mis-classified demonstrations we add a constraint to select only those demonstrations whose model prediction falls within the ambiguous label set, i.e., on the test example\u2019s decision boundary. To test our hypothesis, we focus on multi-class text classification tasks that have fine-grained nuance in the label space. We conduct extensive experimentation across three tasks, namely SST (Socher et al., 2013), GoEmotions (Demszky et al., 2020), and EDOS (Task-B) (Kirk et al., 2023), all of which have fine-grained label space, making the model more likely to be confused across labels. Our key observations are:\n\n1.  Incrementally adding constraints, i.e., 1) considering label ambiguity of test example, 2) limiting ICL demonstrations to mis-classified demonstrations, and 3) considering  label ambiguity of training examples leads to +1.5%, +2.2%, +2.6% improvement in F1 macro scores over the retriever-based ICL, averaged across all datasets (Table 3).\n\n2.  We find that adding such label-based constraints helps more on a smaller model, i.e., on Flan-PaLM 2 (M) (+3.9% gain) compared\n\n. We also attribute this success of our proposed methods to the observation that the  ambiguous label set acts as a good proxy to the gold test label, and as noted by Min et al. (2022), labels in the ICL demonstrations bias the model predictions the most. Therefore, showing the models the \u2018likely\u2019 gold label guides the model to make the correct prediction (Table 5).\n\n# 2 Proposed Method\n\nTypically, in an ICL regime, we assume access to training data D train = {(x 0, y 0), \u00b7 \u00b7 \u00b7, (x T, y T)} from which the goal is to select d demonstrations to be used as prompt. As motivated in the introduction, we follow a three-step approach for selecting demonstrations, for each test example, we need to 1) extract semantically similar examples from D train, 2) identify the ambiguous label-set and 3) extract model predictions for D train  to identify misclassified examples. Below, we describe each step in more detail and how they are used together to select the \u201cbest\u201d demonstrations.\n\nTypically, in an ICL regime, we assume access to training data D train = {(x 0, y 0), \u00b7 \u00b7 \u00b7, (x T, y T)} from which the goal is to select d demonstrations to be used as prompt. As motivated in the introduction, we follow a three-step approach for selecting demonstrations, for each test example, we need to 1) extract semantically similar examples from D train, 2) identify the ambiguous label-set and 3) extract model predictions for D train  to identify misclassified examples. Below, we describe each step in more detail and how they are used together to select the \u201cbest\u201d demonstrations.\nExtract Semantically Similar Demonstrations Typically, in this approach, demonstrations are selected for each test example x t by finding those examples from the D train that are semantically similar to the test input. The motivation being that observing demonstrations that are similar to the new input text will act as a hint for the model (Margatina et al., 2023). This requires the use of a retriever R, either an off-the-shelf one such as (Liu et al., 2022; Agrawal et al., 2023; Margatina et al., 2023; Luo et al., 2023) or a retriever trained specifically for that task (Das et al., 2021; Rubin et al., 2022). For each test example x t, the retriever R is used to rank examples from D train  based on semantic similarity of the text inputs. Topk input-output pairs are then selected from the ranked D train to be used as ICL demonstrations.\nIdentify Ambiguous Label-Set As we can observe from the confusion matrix in Figure 2, the model is often confused between two labels. We hypothesize that in addition to semantic similarity, providing demonstrations that help the model resolve this ambiguity will help the model correct itself. Thus, as a next step, we construct a prompt \u03b8 for the test example x t, and use the model loglikelihood to score each output label l \u2208 L given\n\nIdentify Ambiguous Label-Set As we can observe from the confusion matrix in Figure 2, the model is often confused between two labels. We hypothesize that in addition to semantic similarity, providing demonstrations that help the model resolve this ambiguity will help the model correct itself. Thus, as a next step, we construct a prompt \u03b8 for the test example x t, and use the model loglikelihood to score each output label l \u2208 L given\n\nthe prompt. Using this we identify top-2 labels that have the highest scores, which we refer to as the \u201cambiguous label set\u201d of x t, denoted as L ambig,t = {\u02c6 y (1) t, \u02c6 y (2) t}, where \u02c6 y (1) t and \u02c6 y (2) t are the first and second most likely labels, respectively.\n\nL ambig,t = {\u02c6 y (1) t, \u02c6 y (2) t}, where \u02c6 y (1) t and \u02c6 y (2) t are the first and second most likely labels, respectively.\nExtract Mis-classified Demonstrations The final component in our recipe is to consider the model prediction of the training data. While prior work Min et al. (2022); Yoo et al. (2022); Margatina et al. (2023) has looked at training data label-space from the lens of ground-truth labels, i.e., whether to retain them in the ICL or not, we aim to look at label-space from the perspective of model predictions. Specifically, we are interested in identifying \u201chard\u201d demonstrations, i.e., examples on which the model makes mistakes. We hope that by showing the model such examples with their ground truth labels will force the model to correct itself. Prior work has underscored the potential value of leveraging mis-classified examples from the training set to enhance model performance (Tan, 2006; Wang et al., 2020), but they haven\u2019t tested it for ICL demonstration selection on text classification. In addition to the mis-classified examples, we further constrain the model prediction of these mis-classified examples to be one of the ambiguous labels, identified in the above step. Given that we already know which output labels the model is confused between for the test examples, showing the model those demonstrations (with their ground truth labels) which fall on the decision boundary will likely guide the model to choose the correct label for the test input.\n\n# 3 Experimental Setup\n\n# 3.1 Model\n\nWe experiment with the Flan-PaLM 2 model, an instruction-tuned model which is finetuned on the Flan dataset (Chung et al., 2022; Longpre et al., 2023) based on PaLM-2 (Anil et al., 2023), a multilingual large language model pretrained on web documents, books, code, mathematics and conversational data. We chose these models as Luo et al., 2023 find that retrieved demonstration for ICL works better with instruction-tuned models over general LLMs (e.g., GPT). In particular, we experiment with two variants of the model, namely Flan-PaLM-2 (M) and Flan-PaLM-2 (L), where the\n\nlatter is a larger parameter model. 1 The ICL demonstrations are selected using an off-the-shelf retriever which is finetuned on mT5-base (Xue et al., 2021) using the unsupervised objective proposed by  Izacard et al. (2021). Since the order of demonstrations may impact the model performance (Kumar and Talukdar, 2021; Lu et al., 2022), we randomly shuffle the order of demonstrations for three random seeds and report the average results.\n\nAs mentioned above, the Flan-PaLM 2 models are finetuned on the Flan dataset which is a mixture of many supervised datasets. Specifically, we choose three text classification datasets that satisfy the following desiderata, 1) the output label space shows fine-grained nuance that spans multiple labels, and 2) these datasets are not part of the Flan mixture to avoid any inherent bias from the underlying model. We describe them below, with dataset statistics shown in Table 1. All datasets are in English.\n\nEDOS (Task-B): The Task B of Explainable Detection of Online Sexism (Kirk et al., 2023), is a topic classification task where the sexist content is classified into four categories, i.e., 1) Threats, plans to harm & incitement, 2) Derogation, 3) Animosity, and 4) Prejudiced Discussion.\n\nSST: The Stanford Sentiment Treebank (SST,\nSocher et al., 2013) is a 5-way  sentiment classification dataset for movie reviews with labels: Very Negative, Negative, Neutral, Positive, and Very Positive.\n\nGoEmotions: The GoEmotions (Demszky et al.,\n2020) is a multi-class sentiment classification dataset with \u201cneutral\u201d and 27 emotional classes, e.g., \u201cadmiration\u201d and \u201cfear\u201d, collected from Reddit comments. As the label space is very large and given that we have limited sequence length, it becomes even more crucial to select a concise but effective prompt. 2\n\n# 3.3 Baselines\n\nWe compare our proposed method against the following baselines:\n\n1 Please refer to Anil et al. (2023) for more details on the models. 2 We exclude 24,848 examples (19,925 from training set, 2,474 and 2,449 from dev and test set, respectively) that have multiple labels annotated for a single input, for a simpler experimental setting. We refer the reader to Demszky et al. (2020) for more information on the single-label setting.\n\n1 Please refer to Anil et al. (2023) for more details on the models. 2 We exclude 24,848 examples (19,925 from training set, 2,474 and 2,449 from dev and test set, respectively) that have multiple labels annotated for a single input, for a simpler experimental setting. We refer the reader to Demszky et al. (2020) for more information on the single-label setting.\n\ntrain\ndev\ntest\nEDOS\n3,398\n486\n970\nSST\n8,544\n1,101\n2,210\nGoEmotions\n23,485\n2,952\n2,978\nTable 1: Number of examples in each dataset split.\n\nFrequent Label (FREQ). Select the most frequent label as the model prediction for all test examples.\n\nquent label as the model prediction for all test examples.\nZero-shot ICL (ZERO). For each test example x t, we prepend the task definition to each test input and prompt the models. 3 To obtain the model prediction, we use the model log-likelihood to score each output label l \u2208 L, given the prompt. Then, we select the label with the highest score. y t = arg max L score (l, \u03b8) where \u03b8 refers to the prompt specifically used for this setting, and score refers to the model\u2019s log-likelihood.\nStatic N-shot ICL (STATIC N). We manually select N demonstrations from D train, one for each of the N output labels (N = |L|). Note that these demonstrations are static for all test examples. Thus, we concatenate the task definition, N demonstrations and test example x t as the prompt for ICL and use the log-likelihood scores, as described above, to get the model prediction.\nRetriever-based ICL (RETR). Unlike above, where we used the same prompt for all test inputs, in this baseline, we retrieve demonstrations for each test input x t. We use an off-the-shelf retriever R (subsection 3.1) to retrieve k nearest neighbors {x 1,t, \u00b7 \u00b7 \u00b7, x k,t} from D train, similar to Das et al. (2021). We encode the input text of training set and the test example, rank the training data by the inner product of the vectors. Of these k examples, we select n = 4, 8 as ICL demonstrations. 4\n\nRetriever-based ICL (RETR). Unlike above, where we used the same prompt for all test inputs, in this baseline, we retrieve demonstrations for each test input x t. We use an off-the-shelf retriever R (subsection 3.1) to retrieve k nearest neighbors {x 1,t, \u00b7 \u00b7 \u00b7, x k,t} from D train, similar to Das et al. (2021). We encode the input text of training set and the test example, rank the training data by the inner product of the vectors. Of these k examples, we select n = 4, 8 as ICL demonstrations. 4\n\n# 3.4 Proposed Method: A MBIG-ICL\n\nAs described in section 2, our proposed method considers both semantic similarity and the label ambiguity for selecting demonstrations. Below,\n\n3 Please refer to Appendix A.1 for the exact prompt and prompt template used in this setting, as well as for few shot settings such as the subsequent STATIC N and RETR. 4 We chose k = 4, 8  for two reasons: a) to limit the sequence length to 1024 tokens for faster inference, and b) in some settings we found k = 4 often outperforming k = 8 (Table 2), which led us to believe that adding more examples will not benefit much.\n\nEDOS\nSST\nGoEmotions\nAvg.\nM\nL\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n15.9\n15.9\n7.5\n7.5\n0.8\n0.8\n8.1\n8.1\nZERO\n50.7\n60.5\n49.2\n54.1\n40.5\n43.4\n46.8\n52.7\nSTATIC-N\n51.1\u00b10.3\n58.5\u00b10.4\n50.3\u00b10.4\n56.5\u00b10.3\n34.3\u00b10.5\n44.4\u00b10.3\n45.2\n53.1\nRETR-4\n48.5\u00b10.3\n62.3\u00b10.4\n49.9\u00b10.3\n55.4\u00b10.3\n38.3\u00b10.3\n46.2\u00b10.4\n45.6\n54.6\nRETR-8\n47.1\u00b10.2\n61.8\u00b10.1\n51.5\u00b10.1\n55.2\u00b10.4\n37.5\u00b10.2\n46.7\u00b10.1\n45.4\n54.6\nOurs\nAMBIG-4\n+GOLD\n49.3\u00b10.6\n62.6\u00b10.2\n51.5\u00b10.4\n56.1\u00b10.0\n40.7\u00b10.3\n48.2\u00b10.2\n47.2\n55.6\n+GOLD+MIS\n52.2\u00b10.5\n61.7\u00b10.9\n52.3\u00b10.1\n57.4\u00b10.1\n40.1\u00b10.2\n47.6\u00b10.1\n48.2\n55.6\n+GOLD+MIS+PRED\n53.9\u00b10.5\n62.9\u00b10.4\n53.3\u00b10.4\n58.0\u00b10.0\n42.3\u00b10.5\n47.7\u00b10.2\n49.8\n56.2\nAMBIG-8\n+GOLD\n47.5\u00b10.1\n63.2\u00b10.2\n52.9\u00b10.1\n56.5\u00b10.6\n42.0\u00b11.2\n47.7\u00b10.1\n47.5\n55.8\n+GOLD+MIS\n50.4\u00b10.4\n62.0\u00b10.4\n53.4\u00b10.1\n57.7\u00b10.1\n43.9\u00b10.2\n47.6\u00b10.4\n49.2\n55.8\n+GOLD+MIS+PRED\n50.9\u00b10.6\n62.7\u00b10.2\n54.3\u00b10.2\n57.2\u00b10.3\n41.3\u00b10.3\n47.4\u00b10.3\n48.8\n55.8\nTable 2: F1 macro (%) comparison between our baselines (top) and our proposed methods (bottom) with Flan-PaLM 2 (M/L). 4 or 8 refers to the number of ICL demonstrations. The best performance across all method is highlighted and the best performing baseline is underlined. The \u201cAvg.\u201d column shows the average scores across all datasets. The standard deviations are computed over three random seeds, with the order of demonstrations shuffled.\n\nZERO\nSTATIC-N\nAMBIG-ICL\n+GOLD\n+MIS\n+PRED\nM\n1.3\n-0.2\n1.9\n3.3\n3.9\nL\n-1.9\n-1.5\n1.1\n1.1\n1.4\nall\n-0.3\n-0.9\n1.5\n2.2\n2.6\n<div style=\"text-align: center;\">We omitted RETR in the table, which are inherently zero as we compare against RETR. For both RETR and A MBIG-ICL, we average results on both 4 and 8 shots before computing differences.\n</div>\nTable 3: F1 macro (%) differences compared to RETR, averaged across all datasets as detailed in Table 2. M and L refers to Flan-PaLM 2 sizes, and \u201call\u201d is averaged on results of size M and L. \u201c+ MIS\u201d and \u201c+ PRED\u201d refer to \u201c+ GOLD + MIS\u201d and \u201c+ GOLD + MIS + PRED\u201d, respectively.\nwe summarize our proposed model variants. For each setting, we first retrieve the topk  most similar examples from the training data D train for each test example x t. We denote these candidates by R (x t) = {(x 0,t, y 0,t), \u00b7 \u00b7 \u00b7, (x k,t, y k,t)}. At the same time, for each x t, we also identify the ambiguous label-set L ambig,t = {l i, l j | l \u2208 L}. This set contains the top-2 labels, l i and l j, that the model is most confused about, where both labels belong to the set L of all output labels.\n+ GOLD Select those examples from R (x t) as demonstrations where the ground truth label of each demonstration belongs to the ambiguous label set of x t denoted by:\n\n+ GOLD + MIS Select those examples from R (x t) as demonstrations where the ground truth labels\n\nfall in L ambig,t and they are mis-classified, denote by:\n\nSame as above, the model predictions on the training data are obtained from ZERO. For all our proposed model variants, we select n demonstrations where n = 4 and n = 8.\n\n# 4 Results and Discussion\n\nWe report all our results in Table 2. Specifically, we use the F1 macro scores to compare the model performance, as all our tasks have unbalanced datasets. 5 First, we note across all three tasks, our proposed methods outperform the baselines. We also note that the zero-shot model (ZERO) which only uses a task definition but no task demonstrations, already is a strong baseline for both the Flan-PaLM 2 models (M/L). In particular, comparing the average scores of the few-shot baselines\n\n5 We report the accuracy, precision and recall in A.2.\n\nTest Example: Ok! I like making friends\nLambig,t: Love, Joy\nGold label: Love\nRETR\n1. Disappointment: I want to make friends too :( but I feel like I have nothing good\nto offer\n2. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!\n3. Gratitude: Thanks. I am. I make some new friends.\n4. Disapproval: Not really. My group of friends are awesome in every way possible\nexcept they are homophobic\nPredicted:\nJoy\nAMBIG-ICL\n+GOLD\n1. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!\n2. Love: I ... I like you\n3. Love: Married to the love of my life. LOL\n4. Love: I do. but some people love it\nPredicted:\nLove\n+GOLD+MIS\n1. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!\n2. Love: Too cute for me. Why cant i have a boyfriend *[NAME]*\n3. Joy: FaceTime with wifey!! Happy anniversary!\n4. Love: Stick around! Would love your input POV!\nPredicted:\nLove\n+GOLD+MIS+PRED\n1. Joy: FaceTime with wifey!! Happy anniversary!\n2. Joy: She want to take it slow, I can see that... I deal with those girls all the time,\nthey my favorite\n3. Love: Ha! I like that one.\n4. Love: Ooh I like that one :)\nPredicted:\nLove\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e6a0/e6a0b890-6514-4c3d-a69f-157bff108395.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Test Example: Ok! I like making friends\n</div>\nTable 4: Example demonstrations selected by the RETR and our proposed method A MBIG-ICL for the GoEmotions task, for n = 4. Each demonstration comprises of the input text and the ground truth label, as selected from the training data. On Flan-PaLM 2 (L), where RETR mis-classified it as \u201cJoy\u201d, A MBIG-ICL predicted correctly under all three settings.\n\nand ZERO, we find that ZERO outperforms few-shot baselines by 1.4% on Flan-PaLM 2 (M), but the larger model Flan-PaLM 2 (L) benefits from the addition of ICL demonstrations (+1.4% gain). This is because larger-parameter models make better use of in-context learning (Chan et al., 2022; Aky\u00fcrek et al., 2023; Wei et al., 2023). Interestingly, we also observe that for SST and GoEmotions, the FlanPaLM 2 (L) model achieves higher performance with n = 4 over n = 8, which highlights that quantity does not necessarily lead to better performance.\nConsidering output label space is more important than semantic similarity. Within the fewshot methods, where we use ICL demonstrations along with the task definition, we compute from  Table 3 that our proposed methods AMBIG-* outperforms retriever-based models (RETR-*) by +3.0% (avg.) for Flan-PaLM 2 (M), and by +1.2% (avg.) for Flan-PaLM 2 (L), suggesting that considering output label space for selecting demonstrations is as important as considering the input similarity. In particular, we find that considering mis-classified demonstrations that fall on the test example\u2019s decision boundary leads to the overall best performance. In Table 4, we show the demonstrations selected for the n = 4  setting for one example of the GoEmotions task. We see that for the test input \u201cOk! I like making friends\u201d, the RETR method retrieved\n\nand ZERO, we find that ZERO outperforms few-shot baselines by 1.4% on Flan-PaLM 2 (M), but the larger model Flan-PaLM 2 (L) benefits from the addition of ICL demonstrations (+1.4% gain). This is because larger-parameter models make better use of in-context learning (Chan et al., 2022; Aky\u00fcrek et al., 2023; Wei et al., 2023). Interestingly, we also observe that for SST and GoEmotions, the FlanPaLM 2 (L) model achieves higher performance with n = 4 over n = 8, which highlights that quantity does not necessarily lead to better performance.\n\nsimilar examples from D train (all examples refer to friends). Now from the ZERO model, we calculated the model prediction scores and found that Love and Joy  are the two labels the model is most confused about. However, because we do not consider any test example ambiguity in RETR, only one of the retrieved examples represent the labels Love or Joy, which are the two labels the model is most confused about for this test example. Whereas, in the A MBIG-ICL setting, because of our constraints, all the examples chosen for ICL belong to the ambiguous label set. This allows all our proposed methods to better understand this fine-grained nuance across label space and make the correct model prediction of Love. Below, we conduct some analysis to further explain the way our proposed methods work.\n\n# Considering output label space compensates for\n\nthe sacrifice in semantic similarity. As we introduce more constraints (i.e., + GOLD, + MIS, and + PRED), we find that we need to sacrifice the semantic similarity to the test input. For example, consider the 4-shot A MBIG-ICL experiment on EDOS (Task-B), to satisfy the constraints for the + GOLD setting we need to select up to top-16 retrieved examples in order to obtain the 4 ICL demonstrations; for + GOLD + MIS we need top-55 retrieved examples and more than top-250 retrieved exam\n\nples for + GOLD + MIS + PRED. 6 Clearly, by selecting lower ranked examples from the retrieved set R (x t) we are sacrificing the semantic similarity to the test input. While previous studies, such as (Das et al., 2021; Liu et al., 2022; Margatina et al., 2023), have indicated that greater semantic similarity can enhance model performance, we can see that our methods can still outperform the retriever-based baselines which prioritize it.\n\n# The ambiguous label set is a good proxy for th\n\ntest gold label. While Min et al. (2022) find that using pseudo-demonstrations i.e. demonstrations with random labels instead of the ground truth labels, does not affect the downstream performance much, Lyu et al. (2023) find that for demonstrations that are similar to the test input, such as those from a retriever, pseudo-demonstrations hurt the performance. They refer to this as the copying-effect hypothesis which says that the \u201cmodel prediction is biased towards the labels paired with the inputs in the demonstrations, especially when the inputs are similar to the test inputs\u201d. This, in turn, suggests that the best performance could be achieved if the labels paired with the inputs are same as the gold label of the test example. Given that we do not know the gold label of the test example apriori, the question then becomes how do we approximate the gold label?. We find that our ambiguous label set acts as a close proxy. In Table 5, we compute how many times is the label paired with ICL demonstrations the same as the test example gold label. We find that 44.2% of our proposed methods\u2019 (AMBIG) demonstrations have the same gold label as the test example on average, compared to 30.9% from the\nRETR  method. This is why including the ambiguous label set in the demonstration selection process leads to a higher performance. This analysis also sheds light on the effectiveness of retriever-based ICL. From Table 5  we can see that the demonstrations selected solely based on input text similarity is only 13.3% points (avg.) behind our proposed methods. This confirms that finding demonstrations similar to the input text also leads to selecting demonstrations that have the \u2018likely\u2019 gold label.\n\nA MBIG-ICL helps reduce the model confusion. To understand whether including test label ambi\n\n6 We set a strict constraint on our selection (top-250 retrieved example for + GOLD, and top-250 misclassified retrieved examples for the other two). If there aren\u2019t sufficient examples for + GOLD + MIS + PRED  within the top-250 misclassified retrieved example, we fall-back on the previous setting (+ GOLD + MIS).\n\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\n4-shot\n42.6\n29.6\n21.6\n8-shot\n42.5\n28.6\n20.5\nAMBIG-4\n+GOLD\n49.5 50.3\n46.5 47.1\n41.3\n41.9\n+GOLD+MIS\n46.4 44.3\n46.1 44.3\n38.7\n38.8\n+GOLD+MIS+PRED\n48.3 42.3\n46.1 44.6\n37.8\n40.7\nAMBIG-8\n+GOLD\n50.3 50.3\n46.0 46.8\n41.2\n41.7\n+GOLD+MIS\n46.9 43.8\n46.4 44.7\n38.7\n38.6\n+GOLD+MIS+PRED\n48.8 42.9\n46.5 44.9\n37.5\n40.3\nTable 5: Average percentage (%) of examples in the top 4, 8 retrieved demonstrations that share the same gold labels with test example.\n\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nuniform\n2.00\n2.32\n4.75\nZERO\n0.98 1.08\n1.58 1.19\n2.44 1.92\nSTATIC-N\n0.87 1.07\n1.41 1.11\n1.76\n1.77\nRETR-4\n0.78 0.97\n1.40 1.06\n1.89\n1.70\nRETR-8\n0.82 0.96\n1.38 1.04\n1.79\n1.69\nAMBIG-4\n+GOLD\n0.77 0.93\n1.39 1.02\n1.86\n1.43\n+GOLD+MIS\n0.85 0.98\n1.41 1.06\n1.92\n1.48\n+GOLD+MIS+PRED\n0.86 1.00\n1.42 1.07\n1.92\n1.46\nAMBIG-8\n+GOLD\n0.81 0.91\n1.36 0.98\n1.68\n1.33\n+GOLD+MIS\n0.89 0.97\n1.39 1.03\n1.74\n1.39\n+GOLD+MIS+PRED\n0.90 1.00\n1.40 1.04\n1.76\n1.37\nTable 6: Average entropy of predicted probability distribution. \u201cuniform\u201d refers to the entropy computed for an uniform probability distribution over the labels. Lower entropy is better.\n\nguity indeed helps decrease the model confusion, we calculate the model entropy over the predicted probability distribution of the output labels in  Table 6. 7 Overall, we observe that our A MBIG-* methods achieve the lowest entropy across all three datasets and models. This suggests that by explicitly identifying the point of model confusion (in this case the confusion across fine-grained labels) and selecting demonstrations that help resolve this confusion is indeed effective in reducing the confusion across labels, and thereby resulting in higher downstream performance (Table 2). In particular, we find that for the Flan-PaLM 2 (L), the gap between the few-shot baselines and the A MBIG-* methods is larger, perhaps because larger models are better able to use the ICL demonstrations (Chan et al., 2022; Aky\u00fcrek et al., 2023; Wei et al., 2023).\n\n7 We compute entropy with a base of 2.\n\nWe also compute the Pearson correlation coefficient between F1 macro scores and average entropy of predicted probability distribution (shown in  Table 2 and Table 6, respectively), for all the three datasets. We find that for the Flan-PaLM 2 (L) model, there is a negative correlation for all three datasets, i.e., r = \u2212 0. 78 for EDOS, \u2212 0. 48 for SST and \u2212 0. 92 for GoEmotions, which suggests that lower entropy translates to higher task performance. However, for the Flan-PaLM 2 (M), we have mixed results, as r is positive for EDOS (0. 47), negative for SST (\u2212 0. 55), and close to zero for GoEmotions (0. 03).\n\n# 5 Related Work\n\nThe performance of large language models (LLMs) is significantly influenced by the quality of ICL demonstrations, as demonstrated in multiple studies (Zhao et al., 2021; Liu et al., 2022; Zhang et al., 2022). Consequently, the focus on retrieving superior demonstrations has increased. One prominent strategy is to finetune a retriever for specific tasks by similarity metrics (Das et al., 2021; Hu et al., 2022; Poesia et al., 2022) or by scores derived from language models (Rubin et al., 2022; Shi et al., 2022). While some works introduce an unified retriever trained across various tasks (Li et al., 2023; Cheng et al., 2023) for generalizabilty, another direction is to leverage off-the-shelf retrievers. Liu et al., 2022 propose a KNN-based method to select ICL demonstrations based on semantic similarities; Margatina et al., 2023 select ICL demonstrations with active learning algorithms based on uncertainty, diversity, and similarity, and show that selecting based on input text similarity consistently outperforms other methods; and Agrawal et al., 2023 focus on selecting diverse demonstrations as well as promoting n-gram overlap between demonstrations and test examples. In our work, we adopt the off-the-shelf retriever approach as our focus is to show the generalizability of our approach across different classification tasks. However, we expect that our method will also benefit from a task-specific retriever. Additionally, to the best of our knowledge, we are the first ones to leverage the LLM\u2019s existing knowledge surrounding the test example for selecting demonstrations. Prior works have typically explored the LLM\u2019s existing knowledge, considering the model prediction for the training data.\n\nLuo et al., 2023 use the LLM prediction score on the training data to train a task-specific retriever, and also use Chain-of-Thought prompting (Wei et al., 2022) to improve model performance. Some works (Kumar and Talukdar, 2021; Lu et al., 2022) have found that ordering of the ICL demonstrations also affects the downstream performance, that is why in Table 2 we report the results across three shuffle orders. These works are orthogonal to our work but can be used in combination with our proposed methods.\n\n# 6 Conclusion and Next Steps\n\nIn this work, we find that using LLM\u2019s existing knowledge (e.g., the model prediction) regarding the output label space of both the test example and the ICL demonstration pool is as important as considering the semantic similarity of the input text alone. We find that our proposed method consistently outperform the baselines for all three tasks. Although, we only consider the top-2 most ambiguous labels in selecting the ICL demonstrations, it would be interesting to expand the ambiguous label set to more than two labels. This would especially be more important for datasets like GoEmotions where the label space is large and much more fine-grained. We leave this effort for future work. Furthermore, in this work, we focus on sentence classification tasks, thus paving the way for others to use our proven techniques to also explore label ambiguity for other token/span-level tasks such as Named Entity Recognition (NER), and Part-Of-Speech (POS) tagging.\n\n# 7 Limitations\n\nWe focus on reducing LLM\u2019s label ambiguity by incorporating demonstrations that are misclassified by the LLM and reside on the test example\u2019s decision boundary. While we show this methodology\u2019s effectiveness across datasets, even those with a granular label structure, potential pitfalls remain. If the actual gold label of test example often deviates from the LLM\u2019s top two label choices in a particular dataset or model, this can be indicative of subpar zero-shot performance or flawed ambiguous label set selection. In these scenarios, our method may lead to unsatisfying performance, necessitating further enhancements.\n\n# 8 Ethics Statement\n\nWe use pretrained large language models (LLMs) for text classification. Notably, LLMs are shown to exhibit biases, which is a well-recognized challenge and the broader community is currently working to address. Since our main goal is to improve the downstream task performance, an improved performance on an offensive content classification task could be misused. In particular, the EDOS dataset used in our work, contains offensive content. We selected this dataset for its fine-grained label nuances and to ensure our research isn\u2019t biased by models inherently familiar with the data.\n\n# References\n\nAgnar Aamodt and Enric Plaza. 1994.  Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI Commun., 7(1):39\u2013 59.\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023.  Incontext examples selection for machine translation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8857\u20138873, Toronto, Canada. Association for Computational Linguistics.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023.  What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hern\u00e1ndez \u00c1brego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023. Palm 2 technical report. CoRR, abs/2305.10403.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\n\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In  Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.  Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\n\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.\n\n2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021.  Casebased reasoning for natural language queries over knowledge bases. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9594\u20139611, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. 2020.  GoEmotions: A dataset of fine-grained emotions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4040\u20134054, Online. Association for Computational Linguistics.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2627\u20132643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. CoRR, abs/2112.09118.\nHannah Kirk, Wenjie Yin, Bertie Vidgen, and Paul R\u00f6ttger. 2023. SemEval-2023 task 10: Explainable detection of online sexism. In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 2193\u20132210, Toronto, Canada. Association for Computational Linguistics.\nSawan Kumar and Partha Talukdar. 2021. Reordering examples helps during priming-based few-shot learning. In  Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4507\u20134518, Online. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023.  Unified demonstration retriever for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4644\u20134668, Toronto, Canada. Association for Computational Linguistics.\n\nlearning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics.\nPeng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022.  XRICL: Cross-lingual retrieval-augmented incontext learning for cross-lingual text-to-SQL semantic parsing. In  Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5248\u2013 5259, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In  Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.\nSongbo Tan. 2006. An effective refinement strategy for KNN text classifier. Expert Syst. Appl., 30(2):290\u2013 298.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Ag\u00fcera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR, abs/2201.08239.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9 Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272.\nXiaobo Wang, Shifeng Zhang, Shuo Wang, Tianyu Fu, Hailin Shi, and Tao Mei. 2020.  Mis-classified vector guided softmax loss for face recognition. In The\n\nXiaobo Wang, Shifeng Zhang, Shuo Wang, Tianyu Fu, Hailin Shi, and Tao Mei. 2020.  Mis-classified vector guided softmax loss for face recognition. In The\n\nThirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 12241\u2013 12248. AAAI Press.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022.  Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. CoRR, abs/2303.03846.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2422\u2013 2437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022.  Active example selection for in-context learning. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.  Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\n\n# A Appendix\n\n# A.1 Prompt Construction\n\nWe show our templates in Table 7 (we use 4-shot as an example for few-shot). Task definitions are listed below, denoted by x defn:\n\n\u2022 EDOS: Given a text input, the task is to classify the input as being a Threat, Prejudiced, Animosity, or Derogation category of sexism. Threat refers to language where an individual expresses intent andor encourages others to take action against women which inflicts or incites serious harm and violence against them. It includes threats of physical, sexual or privacy harm. Prejudiced refers to language which denies the existence of discrimination, and justifies sexist treatment. It includes denial and justification of gender inequality, excusing women\u2019s mistreatment, and the ideology of male victimhood. Animosity refers to language which expresses implicit or subtle sexism, stereotypes or descriptive statements. It includes benevolent sexism, i.e., framed as a compliment. Derogation refers to language which explicitly derogates, dehumanises, demeans or insults women. It includes negative descriptions and stereotypes about women, objectification of their bodies, strong negative emotive statements, and dehumanising comparisons. It covers negative statements directed at a specific woman and women in general.\n\u2022 SST: Given sentences from movie reviews, the task is to classify the sentences as being a Great, Good, Okay, Bad, or Terrible category of sentiment. Great refers to language that expresses extremely positive sentiment. Good refers to language that expresses positive sentiment, but not to the extreme. Okay refers to language that is neutral, i.e., neither expresses clear positive nor negative sentiments. Bad refers to language that expresses negative sentiment, but not to the extreme. Terrible refers to language that expresses extremely negative sentiment.\n\u2022 GoEmotions: Given sentences from Reddit comments, the task is to classify the sentences as being an Admiration, Approval, Annoyance, Gratitude, Disapproval, Amusement, Curiosity, Love, Optimism, Disappointment, Joy, Realization, Anger, Sadness, Confusion, Caring, Excitement, Surprise, Disgust, Desire, Fear, Remorse, Embarrassment, Nervousness, Pride, Relief, or Grief category of emotions.\n\n# A.2 Accuracy, Precision, Recall\nPlease refer to Table 8, 9 and 10.\n\nA.3 Label-wise Percentage Analysis of Gold Label Inclusion in L ambig, t\n\n# A.3 Label-wise Percentage Analysis of Gold Label Inclusion in L ambig, t\n\nL We compute the percentage of times that the test example\u2019s gold label is in L ambig,t (as obtained with ZERO) in Table 11, and we present label-wise results in Table 12.\n\n# A.4 Sorting Order with Predicted Probability Distribution Entropy\n\nSince we have the predicted probability distribution of ICL demonstrations, we tried to sort the ICL demonstrations by increasing entropy order. However, it doesn\u2019t consistently improve model performance, which is shown in Table 13 and 14.\n\nxdefn\nThus given the following input:\ninput: xt\nanswer:\nxdefn\nSome examples are:\ninput: x1,t\nanswer: y1,t\ninput: x2,t\nanswer: y2,t\ninput: x3,t\nanswer: y3,t\ninput: x4,t\nanswer: y4,t\nThus given the following input:\ninput: xt\nanswer:\nTable 7: Prompt templates for zero-shot and few-shot ICL. x t refers to the test example, and x i,t, y i,t refers to the text inputs and gold labels of ICL demonstrations selected for x t, respectively.\n\n# A.5 Example on SST for Comparison between\nRETR and A MBIG-ICL\n\nWe list example demonstrations on SST where the model correctly predict the test example in our proposed method, but wrongly classify it in RETR in Table 15.\n\n# A.6 Responsible AI Checklist\n\nPackages for Evaluations. We use SciPy (Virtanen et al., 2020) and scikit-learn (Buitinck et al., 2013) for evaluations.\n\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n11.7\n11.7\n4.6\n4.6\n0.4\n0.4\nZERO\n65\n60.7\n54\n56.2\n42.6\n46.3\nSTATIC-N\n65.2\u00b10.6\n58.1\u00b10.4\n54.5\u00b10.6\n58.2\u00b10.3\n42.6\u00b11.2\n46.2\u00b10.3\nRETR-4\n67.1\u00b11.1\n63.6\u00b10.5\n53.4\u00b10.3\n57.4\u00b10.4\n43.7\u00b10.4\n47.6\u00b10.4\nRETR-8\n65.0\u00b10.2\n63.9\u00b10.3\n54.4\u00b10.1\n57.6\u00b10.5\n43.7\u00b10.4\n48.3\u00b10.1\nOurs\nAMBIG-4\n+GOLD\n65.9\u00b10.8\n63.6\u00b10.4\n54.1\u00b10.3\n57.7\u00b10.1\n45.7\u00b10.3\n50.5\u00b10.2\n+GOLD+MIS\n66.6\u00b11.1\n63.6\u00b11.0\n54.1\u00b10.2\n58.8\u00b10.1\n44.8\u00b10.4\n49.2\u00b10.1\n+GOLD+MIS+PRED\n67.4\u00b10.4\n65.0\u00b10.5\n54.8\u00b10.5\n59.4\u00b10.0\n46.9\u00b11.3\n47.9\u00b10.2\nAMBIG-8\n+GOLD\n66.4\u00b11.1\n64.8\u00b10.1\n54.7\u00b10.2\n58.5\u00b10.7\n48.0\u00b11.8\n49.9\u00b10.1\n+GOLD+MIS\n68.4\u00b10.8\n64.4\u00b10.6\n54.5\u00b10.1\n59.6\u00b10.1\n48.7\u00b10.5\n48.8\u00b10.5\n+GOLD+MIS+PRED\n66.6\u00b11.2\n66.4\u00b10.3\n54.9\u00b10.2\n59.1\u00b10.4\n43.7\u00b10.5\n47.4\u00b10.3\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n25\n25\n20\n20\n3.7\n3.7\nZERO\n46\n62.8\n53.8\n55.2\n42.4\n47.2\nSTATIC-N\n46.2\u00b10.3\n63.0\u00b10.3\n54.0\u00b10.4\n56.5\u00b10.2\n34.8\u00b10.5\n49.5\u00b10.4\nRETR-4\n44.8\u00b10.3\n63.4\u00b10.2\n53.4\u00b10.3\n55.7\u00b10.3\n38.5\u00b10.2\n49.7\u00b10.3\nRETR-8\n44.0\u00b10.1\n62.1\u00b10.2\n54.2\u00b10.1\n55.3\u00b10.4\n37.8\u00b10.3\n50.1\u00b10.3\nOurs\nAMBIG-4\n+GOLD\n45.1\u00b10.6\n64.1\u00b10.2\n54.6\u00b10.4\n56.4\u00b10.1\n41.4\u00b10.3\n51.3\u00b10.2\n+GOLD+MIS\n48.0\u00b10.4\n62.1\u00b10.9\n54.9\u00b10.1\n57.3\u00b10.1\n40.9\u00b10.1\n51.0\u00b10.4\n+GOLD+MIS+PRED\n49.5\u00b10.4\n63.1\u00b10.3\n55.6\u00b10.4\n57.7\u00b10.0\n42.7\u00b10.2\n51.7\u00b10.4\nAMBIG-8\n+GOLD\n43.6\u00b10.1\n64.0\u00b10.2\n55.0\u00b10.1\n56.5\u00b10.6\n41.9\u00b10.9\n50.8\u00b10.5\n+GOLD+MIS\n47.3\u00b10.4\n61.8\u00b10.3\n54.9\u00b10.1\n57.3\u00b10.1\n44.4\u00b10.2\n51.4\u00b10.3\n+GOLD+MIS+PRED\n48.0\u00b10.4\n61.7\u00b10.2\n55.6\u00b10.2\n56.7\u00b10.2\n43.2\u00b10.3\n51.3\u00b10.1\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n46.8\n46.8\n23.1\n23.1\n11.7\n11.7\nZERO\n55.4\n59.2\n49.9\n57.1\n47.1\n46.2\nSTATIC-N\n54.3\u00b10.3\n57.6\u00b10.1\n50.5\u00b10.4\n59.0\u00b10.2\n39.8\u00b10.2\n46.7\u00b10.2\nRETR-4\n53.6\u00b10.3\n61.0\u00b10.5\n50.0\u00b10.3\n58.5\u00b10.3\n45.9\u00b10.0\n50.1\u00b10.2\nRETR-8\n53.8\u00b10.3\n61.1\u00b10.2\n51.8\u00b10.1\n58.6\u00b10.4\n45.3\u00b10.0\n51.0\u00b10.2\nOurs\nAMBIG-4\n+GOLD\n54.3\u00b10.4\n61.3\u00b10.4\n51.5\u00b10.4\n58.9\u00b10.1\n46.6\u00b10.2\n50.3\u00b10.1\n+GOLD+MIS\n56.1\u00b10.2\n60.9\u00b10.6\n52.1\u00b10.1\n59.7\u00b10.1\n45.6\u00b10.1\n49.5\u00b10.1\n+GOLD+MIS+PRED\n56.5\u00b10.1\n61.4\u00b10.4\n53.0\u00b10.4\n60.1\u00b10.1\n45.6\u00b10.1\n50.0\u00b10.2\nAMBIG-8\n+GOLD\n53.6\u00b10.2\n61.8\u00b10.0\n52.9\u00b10.1\n59.5\u00b10.6\n46.8\u00b10.1\n50.4\u00b10.2\n+GOLD+MIS\n55.4\u00b10.6\n61.1\u00b10.3\n53.2\u00b10.1\n60.2\u00b10.1\n45.8\u00b10.2\n50.0\u00b10.3\n+GOLD+MIS+PRED\n55.1\u00b10.6\n61.5\u00b10.3\n54.2\u00b10.2\n59.6\u00b10.3\n44.9\u00b10.2\n50.2\u00b10.2\nEDOS\nSST\nGoEmotions\nFlan-PaLM 2 (M)\n91.2\n85.8\n61.2\nFlan-PaLM 2 (L)\n88.2\n87.6\n61.6\nTable 11: Percentage of times the test example\u2019s gold label is in L ambig,t (as obtained from ZERO model).\n\nTable 11: Percentage of times the test example\u2019s gold label is in L ambig,t (as obtained from ZERO model).\n\nEDOS\nM\nAnimosity 99.1, Derogation 97.4, Prejudiced 52.1, Threat 71.9\nL\nAnimosity 90.1, Derogation 90.1, Prejudiced 68.1, Threat 93.3\nSST\nM\nBad 78.4, Good 98.0, Great 88.0, Okay 73.8, Terrible 93.9\nL\nBad 89.3, Good 99.2, Great 99.2, Okay 59.1, Terrible 85.3\nGoEmotions\nM\nAdmiration 72.7, Amusement 90.3, Anger 58.8, Annoyance 44.3, Approval 24.6, Caring 52.9, Confu-\nsion 73.2, Curiosity 64.2, Desire 35.7, Disappointment 58.0, Disapproval 52.3, Disgust 55.3, Embarrass-\nment 30.4, Excitement 56.4, Fear 75.4, Gratitude 75.7, Grief 100.0, Joy 80.4, Love 86.8, Nervousness\n83.3, Optimism 51.4, Pride 42.9, Realization 27.0, Relief 28.6, Remorse 38.6, Sadness 71.6, Surprise\n62.1\nL\nAdmiration 40.2, Amusement 84.9, Anger 52.7, Annoyance 40.2, Approval 36.0, Caring 36.5, Confu-\nsion 74.2, Curiosity 64.8, Desire 58.9, Disappointment 59.1, Disapproval 81.0, Disgust 38.2, Embarrass-\nment 30.4, Excitement 61.8, Fear 73.8, Gratitude 88.4, Grief 100.0, Joy 84.8, Love 86.2, Nervousness\n83.3, Optimism 65.4, Pride 71.4, Realization 41.6, Relief 42.9, Remorse 70.5, Sadness 67.6, Surprise\n64.4\nTable 12: Label-wise percentage of times the test example\u2019s gold label is in L ambig,t (as obtained f where M and L refers to Flan-PaLM 2 sizes.\n\nEDOS\nSST\nM\nL\nM\nL\nAMBIG-4\n+GOLD\n50.2\n62.9\n51.6\n55.8\n+GOLD+MIS\n51.2\n62.7\n53.0\n57.0\n+GOLD+MIS+PRED\n53.4\n63.8\n52.7\n57.7\nAMBIG-8\n+GOLD\n48.1\n63.3\n53.2\n56.5\n+GOLD+MIS\n50.4\n62.9\n53.6\n57.4\n+GOLD+MIS+PRED\n50.3\n62.9\n54.3\n57.1\nTable 13: F1 macro scores (%) of our method. M and L refers to size of Flan-PaLM 2. The ICL demonstrations are sorted by increased entropy order.\n\nEDOS\nSST\nM\nL\nM\nL\nAMBIG-4\n+GOLD\n0.9\n0.3\n0.1\n-0.3\n+GOLD+MIS\n-1\n1\n0.7\n-0.4\n+GOLD+MIS+PRED\n-0.5\n0.9\n-0.6\n-0.3\nAMBIG-8\n+GOLD\n0.6\n0.1\n0.3\n0\n+GOLD+MIS\n0\n0.9\n0.2\n-0.3\n+GOLD+MIS+PRED\n-0.6\n0.2\n0\n-0.1\nTable 14: The difference of F1 macro scores (%) between the \u201cincreased entropy order\u201d and the \u201caveraged over 3 random seeds\u201d.\n\nTest Example: A hip ride into hyper-time, Clockstoppers is a lively and enjoyable adventure for all ages at any time. L ambig, t: Great, Good Gold label: Great\nPredicted: Good\nRETR 1. Bad: See Clockstoppers if you have nothing better to do with 94 minutes. 2. Bad: Time stands still in more ways that one in Clockstoppers, a sci-fi thriller as lazy as it is interminable. 3. Bad: Clockstoppers is one of those crazy, mixed-up films that doesn\u2019t know what it wants to be when it grows up. 4. Good: Even with all its botches, Enigma offers all the pleasure of a handsome and well-made entertainment.\nPredicted: Great\nA MBIG-ICL + GOLD 1. Good: Even with all its botches, Enigma offers all the pleasure of a handsome and well-made entertainment. 2. Great: A breathtaking adventure for all ages, Spirit tells its poignant and uplifting story in a stunning fusion of music and images. 3. Great: A rollicking ride, with jaw-dropping action sequences, striking villains, a gorgeous color palette, astounding technology, stirring music and a boffo last hour that leads up to a strangely sinister happy ending. 4. Great: This gorgeous epic is guaranteed to lift the spirits of the whole family.\nPredicted: Great\n+ GOLD + MIS 1. Good: As action-adventure, this space-based homage to Robert Louis Stevenson\u2019s Treasure Island fires on all plasma conduits. 2. Good: Horns and Halos benefits from serendipity but also reminds us of our own responsibility to question what is told as the truth. 3. Great: Return to Never Land is reliable, standard Disney animated fare, with enough creative energy and wit to entertain all ages. 4. Great: It\u2019s a smart, solid, kinetically-charged spy flick worthy of a couple hours of summertime and a bucket of popcorn.\nPredicted: Great\n+ GOLD + MIS + PRED 1. Good: As action-adventure, this space-based homage to Robert Louis Stevenson\u2019s Treasure Island fires on all plasma conduits. 2. Good: Horns and Halos benefits from serendipity but also reminds us of our own responsibility to question what is told as the truth. 3. Great: Return to Never Land is reliable, standard Disney animated fare, with enough creative energy and wit to entertain all ages. 4. Great: It\u2019s a smart, solid, kinetically-charged spy flick worthy of a couple hours of summertime and a bucket of popcorn.\n\nTable 15: Example demonstrations selected by the RETR and our proposed method A MBIG-ICL for the SST task, for n = 4. The model used here for prediction is Flan-PaLM 2 (L).\n\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) has shown to improve performance in large language models (LLMs) by using few task-specific demonstrations. However, LLMs are sensitive to prompt selection, leading to the need for better demonstration selection strategies. Previous methods primarily rely on semantic similarity, which does not account for the LLM's existing knowledge about the task, necessitating a new approach to enhance demonstration effectiveness.",
        "problem": {
            "definition": "The problem addressed in this paper is the ineffective selection of demonstrations for in-context learning in LLMs, which can hinder model performance due to the model's confusion over similar labels.",
            "key obstacle": "The core obstacle is that existing methods primarily focus on semantic similarity without leveraging the model's understanding of the output label space, resulting in suboptimal demonstration selection."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that leveraging the model's existing knowledge about the task and its output label space can lead to more effective demonstration selection, particularly in resolving label ambiguities.",
            "opinion": "The proposed idea involves selecting demonstrations based not only on semantic similarity but also on their relevance to the model's confusion regarding the output labels, particularly those that help clarify ambiguous labels.",
            "innovation": "The innovation lies in the method's dual focus on semantic similarity and label ambiguity, which differentiates it from existing approaches that rely solely on input similarity for demonstration selection."
        },
        "method": {
            "method name": "AMBIG-ICL",
            "method abbreviation": "AMBIG",
            "method definition": "AMBIG-ICL is a method for selecting in-context learning demonstrations that incorporates both semantic similarity and the model's understanding of label ambiguity to improve classification performance.",
            "method description": "The method selects demonstrations by first retrieving semantically similar examples, identifying ambiguous labels, and then selecting demonstrations that help resolve confusion around those labels.",
            "method steps": [
                "Extract semantically similar examples from the training data.",
                "Identify the ambiguous label set for the test example.",
                "Select misclassified demonstrations that are relevant to the ambiguous labels."
            ],
            "principle": "The effectiveness of this method stems from its ability to guide the model towards the correct label by presenting examples that clarify its confusion, thus reducing uncertainty in predictions."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three text classification tasks: SST, GoEmotions, and EDOS, using the Flan-PaLM 2 model to evaluate the performance of the proposed method against several baselines.",
            "evaluation method": "Performance was assessed using F1 macro scores, comparing the proposed method to baseline methods across multiple runs with varying demonstration counts."
        },
        "conclusion": "The proposed AMBIG-ICL method consistently outperformed baseline methods by effectively leveraging the model's existing knowledge about label ambiguity, leading to improved performance across all evaluated tasks.",
        "discussion": {
            "advantage": "Key advantages of the proposed approach include its ability to reduce model confusion by selecting demonstrations that clarify ambiguous labels, resulting in enhanced classification accuracy.",
            "limitation": "A limitation of the method is its reliance on the model's predictions; if the model's initial label predictions are poor, the method may not yield satisfactory results.",
            "future work": "Future research could explore expanding the ambiguous label set beyond two labels to further improve performance, especially in tasks with more complex label structures."
        },
        "other info": {
            "authors": [
                "Lingyu Gao",
                "Aditi Chaudhary",
                "Kazuma Hashimoto",
                "Karthik Raman"
            ],
            "affiliations": [
                "Toyota Technological Institute at Chicago",
                "Google Research"
            ],
            "datasets": [
                "SST",
                "GoEmotions",
                "EDOS"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) improves performance in large language models (LLMs) by using few task-specific demonstrations."
        },
        {
            "section number": "1.4",
            "key information": "Prompt selection is critical in in-context learning as LLMs are sensitive to it, leading to the need for better demonstration selection strategies."
        },
        {
            "section number": "3.3",
            "key information": "The AMBIG-ICL method selects demonstrations by retrieving semantically similar examples and identifying ambiguous labels to enhance classification performance."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method reduces model confusion by selecting demonstrations that clarify ambiguous labels, enhancing classification accuracy."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the AMBIG-ICL method is its reliance on the model's predictions; poor initial predictions may lead to unsatisfactory results."
        },
        {
            "section number": "5.1",
            "key information": "The experiments evaluated the proposed method on three text classification tasks: SST, GoEmotions, and EDOS, using the Flan-PaLM 2 model."
        }
    ],
    "similarity_score": 0.7414623505462171,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8ada/8ada831b-ede5-4989-99ab-293a33155583.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6d5/d6d516a2-2056-4cce-b721-9f66b2f6f95b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e6a0/e6a0b890-6514-4c3d-a69f-157bff108395.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Ambiguity-aware in-context learning with large language models.json"
}