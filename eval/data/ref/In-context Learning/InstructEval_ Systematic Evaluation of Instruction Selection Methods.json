{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.00259",
    "title": "InstructEval: Systematic Evaluation of Instruction Selection Methods",
    "abstract": "In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instructionchoice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite for benchmarking instruction selection approaches and enabling more generalizable methods in this space.1",
    "bib_name": "ajith2023instructevalsystematicevaluationinstruction",
    "md_text": "Anirudh Ajith\u2217 Chris Pan\u2217 Mengzhou Xia Ameet Deshpande Karthik Narasimhan Department of Computer Science, Princeton University {anirudh.ajith, chrispan, mengzhou, asd, karthikn}@princeton.edu\n# Abstract\nIn-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instructionchoice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite for benchmarking instruction selection approaches and enabling more generalizable methods in this space.1\narXiv:2307.00259v2\n# 1 Introduction\nOne of the most effective insights in NLP research in recent years has been that large language models trained to perform next-token prediction show emergent in-context learning (ICL) abilities (Brown et al., 2020; Scao et al., 2022a; Zhang et al., 2022a). While the bulk of research interest has shifted away from task-specific models and towards creating \u201cfoundation models\" to perform a variety of tasks using appropriately constructed\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed54/ed54ccfb-3319-4669-9d10-f240fefd5e71.png\" style=\"width: 50%;\"></div>\nFigure 1: InstructEval allows the assessment of instruction selection methods for ICL across a range of models and tasks along five metrics.\n<div style=\"text-align: center;\">Figure 1: InstructEval allows the assessment of instruction selection methods for ICL across a range of models and tasks along five metrics.</div>\nprompts, the performance of ICL remains sensitive to the precise details of prompt construction. Prompt engineering remains critical for achieving optimal ICL performance (Perez et al., 2021; Zhao et al., 2021; Webson and Pavlick, 2022). In practice, ICL typically involves prompting a language model using a concatenation of a taskspecific instruction, a short sequence of annotated in-context examples known as demonstrations, and a test example (Figure 2). Much of the research interest surrounding in-context learning has focused on understanding the optimal selection, ordering of demonstrations, and label-space choices (Liu et al., 2021a; Su et al., 2022; Rubin et al., 2022; Wang et al., 2023a; Lu et al., 2021a; Wei et al., 2023; Pan et al., 2023). However, instruction choice remains\na relatively underexplored aspect of prompt engineering despite its established significance (Mishra et al., 2022) on downstream performance. Even among recent works exploring automatic instruction selection (Honovich et al., 2022; Gonen et al., 2022; Deng et al., 2022; Zhou et al., 2022), the use of different evaluation protocols makes the comparison of their relative performances difficult. Existing studies typically limit their analyses to specific models or tasks; for example, Zhou et al. (2022) focus on a single model, and while Deng et al. (2022) consider multiple model scales, they all belong to a single model family. Moreover, evaluations often span disparate task selections with minimal overlap and are primarily dominated by classification tasks, neglecting other task types like multiple-choice QA or generation. Lastly, most previous works tend to emphasize zero-shot accuracy, overlooking other pertinent ICL metrics such as few-shot accuracy and robustness measures. To address these issues, we build InstructEval, an evaluation suite for the comprehensive evaluation of instruction selection methods. The suite covers a diverse collection of 13 open-sourced autoregressive LLMs from four model families and nine tasks spanning three task types. Additionally, it also incorporates three accuracy metrics and two sensitivity metrics that are of interest to ICL. We perform evaluations of seven popular instruction selection methods including trivial instruction baselines, manually curated instructions, and sophisticated automatic methods using our suite. Overall, we find that the relative effectiveness of these approaches varies significantly across different models and task types. We discover that curated manually-written instructions and task-agnostic instructions can elicit better aggregated performance (over models) than automatically induced ones, highlighting the lack of generalizability of the latter. We also find that including instructions in few-shot prompts usually tends to hurt ICL performance at the model scales we consider. Our findings suggest that it may be optimal for ICL practitioners to omit instructions in few-shot settings and use curated manually-written instructions in zero-shot settings, rather than contemporary automatic induction techniques that require substantial computation and hyperparameter tuning to achieve competitive performance. We release the evaluation suite we develop to aid the systematic study of even more questions regarding prompt engineering that we do\nnot explicitly address in our work.\n# 2 Related Work\nIn-Context Learning and Existing Benchmarks As language models have scaled, in-context learning has emerged as a popular paradigm and remains ubiquitous among several autoregressive LLM families (Brown et al., 2020; Touvron et al., 2023; Scao et al., 2022b; Black et al., 2021; Zhang et al., 2022b). Benchmarks like BigBench (Srivastava et al., 2022) and HELM (Liang et al., 2022) have been created for the holistic evaluation of these models. BigBench focuses on few-shot abilities of state-of-the-art large language models, while HELM extends to consider metrics like robustness and bias. However, these benchmarks focus on evaluating and ranking language models, and do not address the systematic evaluation of prompting methods. Although contemporary work by Yang et al. (2023) also aims to perform a similar systematic analysis of prompting methods, they focus on simple probability-based prompt selection while we evaluate a broader range of methods including trivial instruction baselines, curated manually selected instructions, and sophisticated automated instruction selection.\n# Automated Prompt Engineering Method\nThere has been interest in performing automated prompt-engineering for target downstream tasks within ICL. This has led to the exploration of various prompting methods, ranging from simple heuristics such as selecting instructions with the lowest perplexity (Gonen et al., 2022), inducing instructions from large language models using a few annotated input-output pairs (Zhou et al., 2022), to utilizing RL objectives to create discrete token sequences as prompts (Deng et al., 2022). However, these works restrict their evaluation to small sets of models and tasks with little intersection, hindering their objective comparison.\nbeen much recent work attempting to understand the mechanisms that drive in-context learning. Studies have found that the selection of demonstrations included in prompts significantly impacts fewshot accuracy across most tasks (Liu et al., 2021b; Agrawal et al., 2022; Xu et al., 2023). Works like (Lu et al., 2021b) also show that altering the ordering of a fixed set of demonstrations can affect downstream accuracy. Prompts sensitive to demon-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c9f/4c9f4e52-030f-4eb3-8392-a7312cbc37f4.png\" style=\"width: 50%;\"></div>\nFigure 2: An example of a prompt following the template we use for IMDB. By \u2018prompt\u2019 we refer to the concatenation of the instruction, solved demonstrations and an unsolved test example.\nstration permutation often exhibit lower accuracies (Chen et al., 2023), making them less reliable, particularly in low-resource domains. Our work aims to bridge these gaps by systematically evaluating the efficacy of popular instruction selection approaches over a diverse set of tasks and models, facilitating objective comparison. We evaluate these methods not only on accuracy metrics, but also on sensitivity metrics to glean additional insights. We recognize that other facets of prompting not covered by instruction engineering exist (Wei et al.; Yao et al., 2023; Wang et al., 2023b), and defer these explorations to future work.\n# 3 Evaluation Suite\n# 3.1 Prompt format\nWe define a \u2018prompt\u2019 as the full textual input provided to an LLM. Our evaluation suite supports the use of any number of demonstrations, arbitrary demonstration templates and the inclusion of custom strings anywhere within the prompt. Since the instructions used can be set to any arbitrary strings, users are free to use any external means to select instructions and have them evaluated by our suite. For consistency, we conduct all experiments in this work using prompts that begin with an instruction, continue with a sequence of annotated training\ndemonstrations, and conclude with an unsolved test example2 (Figure 2), and express each example in a minimal, task-specific key-value format (Table 8) that reflects task semantics.\n# 3.2 Metrics\nAccuracy metrics Accuracy is typically the primary metric of interest in ICL. While ICL is most commonly performed in few-shot settings where a handful of annotated demonstrations are included in the prompt, models are also prompted zero-shot without the use of such demonstrations. Since realworld scenarios can often contain grammatical errors and misspellings in the test input, it is desirable to find prompts robust to these perturbations. Hence, we measure zero-shot accuracy, few-shot accuracy, and perturbation accuracy3 in our evaluations. Following Liang et al. (2022), we measure perturbation accuracy by introducing random capitalization, spacing, contractions and common misspellings in the test input. Sensitivity metrics Previous work has shown that the accuracy obtained using a prompt template can fluctuate significantly as a function of the set of demonstrations included in the prompt (Liu et al., 2021a; Su et al., 2022; Rubin et al., 2022; Wang et al., 2023a) and the order they are presented in (Lu et al., 2021b). It may be desirable in practice to identify prompt templates and instructions that offer consistent performance regardless of the choice of demonstrations and their arrangement. Hence, we introduce selectional sensitivity and permutational sensitivity metrics to measure the sensitivity of chosen instructions respectively to selected demonstrations, and the order in which they are arranged. We quantify the sensitivity of an instruction (given a model and task) using the standard deviation of accuracies obtained on varying the selection or permutation of the demonstrations used, each across 16 random choices.\n# 3.3 Aggregating metrics across Models\nEach instruction selection method being tested across N models and M datasets yields NM values per metric. Comparing these NM-dimensional vectors directly is complex. It can be challenging\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee56/ee56ec8a-f7dd-403b-b39c-8a0ac1fa77de.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c83/0c83c0ee-fbcd-4b10-93ee-f06f80f4bf3c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Selectional sensitivity</div>\n<div style=\"text-align: center;\">(a) Perturbation accuracy</div>\nFigure 3: We provide schematic diagrams that show prompts are modified to measure perturbation accuracy, selectional sensitivity and permutational sensitivity. We perturb the test input to measure perturbation accuracy, and demonstration selection and permutation respectively while measuring selectional and permutational sensitivity.\nto reduce them to a single representative scalar. Simple approaches such as computing the mean of these NM values can prove inadequate since the resulting scores would tend to be heavily influenced by metric values that exhibit a high variance across different inspected methods. We opt against using aggregation techniques used by previous works (Liang et al., 2022; Srivastava et al., 2022) due to their drawbacks (Section C) and instead adopt \u2018mean relative gain\u2019 as a means to aggregate accuracy metrics across multiple models. We rely on simple averaging for sensitivity metrics, partly because we observe that these quantities do not show much variation across methods.\nConsidering the range of models and datasets in our evaluation suite, we unsurprisingly observe substantial variation in accuracy magnitudes across model scales and tasks. However, we notice that the degree of variation in accuracy due to instruction choice is usually considerably smaller than the degree of variation due to model and task choice. To meaningfully compare and aggregate the relative performance of different instruction selection methods across models, we use a measure called mean relative gain. First, we define the relative gain for a value x from a population P as the percentage by which x exceeds the mean value of P:\nConsider a collection of models M and instructions I for a task t. Given a model m, we calculate the raw accuracy scores stmi for each instruction i \u2208I. Taking this set Stm to be the population, we compare the performances of the instructions against each other by computing their correspond-\n<div style=\"text-align: center;\">(c) Permutational sensitivity</div>\ning relative gains rtmi = r-gainStm(stmi). Each rtmi represents the degree by which method i outperforms the average performance along the metric on task t for model m. We now define the mean relative gain as\nThese rti values, tabulated and analyzed in Section 4, capture not only the ordinal information about each method\u2019s performance on a given task but also provide an intuitive sense of the magnitude by which these methods outperform others. Specifically, if an induction method i has a mean relative gain rti on task t, this means that method i exceeds average performance (across I) on task t by rti percent when averaged across models M.\n# 3.3.2 Sensitivity metrics\nTo aggregate the sensitivity of an instruction selection/induction method i over all models for a task t, we simply compute the average of the raw sensitivity scores (described in Section 3.2). Specifically, if \u03c3tmi is the raw sensitivity score obtained for model m and task t when using instruction i, then the aggregated sensitivity score \u03c3ti is given by\nWe choose to avoid more sophisticated aggregation strategies like relative gain for sensitivity metrics since standard deviations are already secondary metrics making it unintuitive to discuss the relative gain of the standard deviation obtained using a method over the average.\nTask Type\nTasks\nClassification (CLS)\nAG News (Zhang et al., 2015)\nANLI (Nie et al., 2020)\nBoolQ (Clark et al., 2019)\nIMDB (Maas et al., 2011)\nTweetEval Emotion (Mohammad et al., 2018)\nMultiple-choice (MCQ)\nCosmosQA (Huang et al., 2019)\nHellaSwag (Zellers et al., 2019)\nGenerative QA (GQA)\nNQ-Open (Kwiatkowski et al., 2019)\nTriviaQA (Joshi et al., 2017)\nModel Family\nSize\nBLOOM (Scao et al., 2022b)\n1.1B, 1.7B, 3B, 7.1B\nGPT Neo (Black et al., 2021, 2022)\n1.3B, 2.7B, 20B\nLLaMA (Touvron et al., 2023)\n7B, 13B\nOPT (Zhang et al., 2022b)\n1.3B, 2.7B, 6.7B, 13B\nTable 2: Model families and corresponding model scales included in our evaluation suite.\nWhile previous instruction induction (Zhou et al., 2022; Deng et al., 2022) work has tended to focus mostly on classification tasks, we include 9 tasks (Table 1) in our evaluation suite spanning classification (CLS), multiple-choice question-answering (MCQ) and generative question-answering (GQA) to assess the applicability of instruction selection and induction methods to other task-types as well. We concentrate on tasks that are challenging to contemporary language models, and yet are not so demanding that the performance of these models does not exceed random chance. We exclude certain generative tasks, like summarization, which are challenging to assess objectively. 4\n# 3.5 Models\nWe include a diverse range of 13 autoregressive LLMs (Table 2) from 4 model families of sizes ranging from 1.1 billion to 20 billion parameters in our evaluation suite. We choose contemporary models that span different architectures and training paradigms which are known to show good ICL performance. This diversity bolsters the generalizability of insights obtained using our evaluation suite while mitigating potential bias towards any specific model family. Moreover, we select opensource models which are large enough to show non-trivial ICL performance while still being small enough to run on reasonable consumer hardware to\nMethod\nTask-specific\nAutomatic induction\nNull instruction\n\u2717\n\u2717\nGeneric instruction\n\u2717\n\u2717\nPromptSource (Bach et al., 2022)\n\u2713\n\u2717\nAd hoc\n\u2713\n\u2717\nLow Perplexity (Gonen et al., 2022)\n\u2713\n\u2713\nAPE (Zhou et al., 2022)\n\u2713\n\u2713\nRLPrompt (Deng et al., 2022)\n\u2713\n\u2713\nensure the practical significance of our findings.\n# 4 Experimental setup\nWe perform experiments evaluating 3 families of instruction selection methods (listed in Table 3).\nTask-agnostic instructions In practical ICL settings, it is straightforward to use instructions that contain no task-specific information.\nManual task-specific instructions We evaluate manually-written task-specific instructions that ICL practitioners may use in practice.\n\u2022 PromptSource: PromptSource (Bach et al., 2022) is a public collection of manually-curated prompt templates pertaining to 170+ datasets which are often used off-the-shelf for ICL and are generally considered high-quality.\n Ad hoc: ICL practitioners often create taskspecific instructions ad hoc, based on the semantics of the given task. We simulate this mode of instruction selection by asking ChatGPT to generate several paraphrases of task-specific seed instructions we obtain from PromptSource and randomly sampling from the generated set.\nAutomatically synthesized task-specific instructions We evaluate 3 popular automated instruction selection and induction methods that are representative of previous work.\n Low Perplexity: (Gonen et al., 2022) find that the perplexity a model associates with an instruction is negatively correlated with its ICL performance when using that instruction. We use the SPELL algorithm proposed by Gonen et al. (2022) to select the least perplexity instructions (for each model) from a large pool of ChatGPT paraphrased instructions.\n APE: (Zhou et al., 2022) is an automatic few-shot method for inducing instructions by prompting a language model to describe the given task, and refining the set of generated prompts using accuracy on a small held-out validation set. While Zhou et al. (2022) limit their evaluation to GPT3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022), we assess APE\u2019s applicability to a significantly larger set of models and tasks.\n RLPrompt (Deng et al., 2022) is a reinforcement-learning-based approach for few-shot prompt induction. While the original authors only evaluate their method using GPT-2 on a few classification tasks, we expand this assessment to many more models and tasks. Notably, we assess the extensibility of RLPrompt to MCQ tasks, but do not test RLPrompt performance on GQA tasks since the algorithm is not directly applicable to generation tasks.\n# 5 Results\nWe tabulate the mean relative gain values over accuracy metrics in Table 4, and the mean standard deviations corresponding to selectional and permutational sensitivity metrics in Table 5.\n# 5.1 Less sophisticated instruction selection methods tend to show higher accuracy\nWe find that task-agnostic instructions dominate in few-shot settings with Null instructions and Generic instructions achieving the highest aggregated performance in 5/9 tasks for few-shot accuracy and 6/9 tasks for perturbation accuracy. Although both these methods show above-average performance in few-shot settings, Null instructions tend to perform better among the two. Although PromptSource instructions only show an average performance in few-shot settings, their manually curated task-specific instructions prove most effective in zero-shot settings, achieving the highest aggregated performance in 6/9 tasks\nand usually achieving markedly higher mean relative gain values than even the runner-up method for the task. This is especially true of GQA tasks where PromptSource instructions outperform the average by >17%. Automatic task-specific instructions are usually outperformed by simple baselines. They fail to achieve the best zero-shot performance on any task we consider. While they do sometimes perform competitively with simpler baselines in the few-shot setting, emerging as the best-performing instructions in 2/9 tasks, this behavior is inconsistent. Although Low Perplexity instructions and APE instructions seldom show above-average performance in either setting, RLPrompt instructions show above-average performance in 5/7 tasks in both settings. They are still usually outperformed by instructions obtained through simpler means such as Null and PromptSource instructions.\n# 5.2 Ranges of variation of aggregated scores\nWe notice that instructions have a more significant impact in zero-shot settings as compared to few-shot settings. For most tasks, we find that the highest mean relative gain values achieved in the zero-shot setting are markedly greater than those in the few-shot setting. Accordingly, the minimum values for each task are also relatively lower in zero-shot settings. This finding suggests that instructions play a significant role in informing models of semantics in zero-shot settings whereas in few-shot settings, most of a model\u2019s understanding of task-semantics comes from the demonstrations. The degree of variation in accuracy due to instruction choice varies considerably across tasks. AG News and Emotion show the highest variability in few-shot performance while GQA tasks show the most variability in zero-shot settings. Table 5 shows that selectional and permutational sensitivities vary dramatically across tasks even though they are roughly consistent across all methods for a given task implying that all the methods we evaluate are comparable in sensitivity, which is unsurprising since none of them explicitly optimize for it. We also find that most methods show comparable, but usually lower permutational sensitivity than selectional sensitivity across all tasks.\n# 5.3 Analysis\nWe tabulate the mean relative gain values for zeroshot and few-shot accuracies computed separately for \u201csmall\" models with < 6 billion parameters and\nCLS\nMCQ\nGQA\n# wins\nMethod\nAG News\nANLI\nBoolQ\nIMDB\nEmotion\nHellaSwag\nCosmosQA\nTriviaQA\nNQ-Open\nZero-shot accuracy (mean relative gain) \u2191\nNull Instruction\n2.26\n1.07\n2.48\n\u22123.52\n\u22125.30\n2.54\n5.94\n\u22123.08\n\u221225.67\n3\nGeneric Instruction\n3.55\n\u22120.39\n0.03\n1.69\n2.39\n\u22120.13\n\u22121.67\n\u22121.52\n\u22125.99\n0\nPromptSource\n5.81\n1.38\n\u22120.65\n4.34\n5.13\n\u22121.54\n\u22123.42\n17.02\n22.15\n6\nAd hoc\n\u22120.33\n0.21\n0.55\n1.41\n0.66\n\u22120.27\n\u22122.46\n\u22122.03\n2.31\n0\nLow Perplexity\n\u22120.59\n1.22\n0.56\n0.84\n\u22124.07\n\u22121.38\n\u22122.18\n\u22125.87\n2.81\n0\nAPE\n\u221215.63\n\u22123.86\n\u22121.07\n\u22121.77\n\u22120.26\n\u22121.06\n0.00\n\u22124.70\n4.39\n0\nRLPrompt\n4.92\n0.37\n\u22121.89\n\u22122.99\n1.46\n1.85\n3.79\n\u2212\n\u2212\n0\nFew-shot accuracy (mean relative gain) \u2191\nNull Instruction\n4.09\n\u22120.22\n0.87\n\u22120.80\n5.89\n0.17\n1.33\n0.45\n\u22120.02\n4\nGeneric Instruction\n5.16\n\u22120.20\n\u22120.10\n0.45\n4.84\n0.04\n\u22120.18\n0.11\n0.11\n1\nPromptSource\n0.83\n0.14\n\u22120.79\n0.39\n\u22124.39\n\u22120.06\n\u22120.94\n\u22120.36\n0.61\n1\nAd hoc\n2.18\n\u22120.10\n\u22120.05\n0.60\n\u22125.63\n\u22120.21\n\u22120.59\n0.09\n\u22120.49\n1\nLow Perplexity\n\u22121.96\n0.31\n\u22120.40\n0.20\n\u22126.79\n\u22120.23\n\u22120.61\n\u22120.06\n\u22120.02\n1\nAPE\n\u221215.43\n0.10\n0.06\n\u22120.69\n1.17\n0.02\n0.17\n\u22120.24\n\u22120.19\n0\nRLPrompt\n5.13\n\u22120.02\n0.40\n\u22120.14\n4.90\n0.27\n0.81\n\u2212\n\u2212\n1\nFew-shot perturbation accuracy (mean relative gain) \u2191\nNull Instruction\n4.09\n\u22120.08\n0.11\n\u22120.27\n5.98\n0.11\n1.10\n0.81\n1.28\n4\nGeneric Instruction\n5.15\n\u22120.18\n\u22120.16\n0.56\n4.23\n\u22120.02\n\u22120.02\n0.08\n0.10\n2\nPromptSource\n1.14\n0.27\n\u22120.02\n0.33\n\u22123.92\n0.06\n\u22120.53\n\u22120.65\n0.04\n0\nAd hoc\n1.68\n0.51\n\u22120.34\n0.37\n\u22125.87\n\u22120.08\n\u22120.63\n\u22120.28\n\u22120.61\n0\nLow Perplexity\n\u22122.39\n0.68\n\u22120.12\n\u22120.20\n\u22126.61\n\u22120.09\n\u22120.66\n\u22120.03\n\u22120.78\n1\nAPE\n\u221214.32\n\u22121.20\n0.28\n\u22120.82\n1.26\n\u22120.13\n0.21\n0.06\n\u22120.03\n1\nRLPrompt\n4.65\n\u22120.01\n0.24\n0.03\n4.94\n0.15\n0.53\n\u2212\n\u2212\n1\nTable 4: Mean relative gain values associated with zero-shot accuracy, and few-shot accuracy with unperturbed and perturbed test inputs. Only values that correspond to the same task and metric should be compared. Positive values represent above-average performance, and negative values represent below-average performance. The \u2018# wins\u2019 column shows the number of tasks where a method achieved the highest aggregated performance.\nCLS\nMCQ\nGQA\n# wins\nMethod\nAG News\nANLI\nBoolQ\nIMDB\nEmotion\nHellaSwag\nCosmosQA\nTriviaQA\nNQ-Open\nSelectional sensitivity (mean standard deviation) \u2193\nNull Instruction\n6.69\n2.45\n4.73\n5.28\n6.97\n2.46\n8.10\n2.59\n2.28\n3\nGeneric Instruction\n6.87\n2.50\n4.76\n5.40\n6.97\n2.48\n8.16\n2.61\n2.26\n0\nPromptSource\n6.73\n2.26\n4.85\n5.37\n6.43\n2.43\n8.26\n2.59\n2.28\n1\nAd hoc\n6.95\n2.41\n4.62\n5.38\n6.34\n2.42\n8.20\n2.65\n2.37\n1\nLow Perplexity\n7.07\n2.17\n4.69\n5.64\n6.25\n2.42\n8.27\n2.59\n2.30\n2\nAPE\n7.44\n2.98\n4.63\n5.70\n6.67\n2.43\n8.16\n2.65\n2.21\n1\nRLPrompt\n6.76\n2.30\n4.79\n5.50\n6.96\n2.36\n8.16\n\u2212\n\u2212\n1\nPermutational sensitivity (mean standard deviation) \u2193\nNull Instruction\n6.02\n1.99\n3.82\n4.14\n5.48\n1.12\n1.87\n1.52\n1.28\n2\nGeneric Instruction\n6.01\n2.19\n3.89\n4.56\n5.49\n1.15\n1.68\n1.33\n1.22\n2\nPromptSource\n6.06\n2.15\n3.61\n4.69\n4.30\n1.07\n1.67\n1.47\n1.17\n2\nAd hoc\n6.10\n2.37\n3.77\n4.61\n4.37\n1.11\n1.66\n1.41\n1.23\n0\nLow Perplexity\n6.13\n2.24\n3.50\n4.61\n4.29\n1.13\n1.69\n1.46\n1.27\n2\nAPE\n6.14\n2.36\n3.69\n4.84\n5.08\n1.10\n1.78\n1.41\n1.21\n0\nRLPrompt\n6.26\n2.06\n3.82\n4.89\n5.64\n1.08\n1.65\n\u2212\n\u2212\n1\n<div style=\"text-align: center;\">\u22656B parameters</div>\n< 6B parameters\n\u22656B parameters\nMethod\nCLS\nMCQ\nGQA\nCLS\nMCQ\nGQA\nZero-shot accuracy (mean relative gain) \u2191\nNull Instruction\n\u22122.89\n1.71\n\u221215.86\n2.07\n7.19\n\u221212.64\nGeneric Instruction\n1.71\n0.69\n\u22120.64\n1.16\n\u22122.76\n\u22127.40\nPromptSource\n2.77\n\u22122.18\n25.03\n3.70\n\u22122.83\n13.23\nAd hoc\n1.87\n\u22120.94\n4.56\n\u22121.11\n\u22121.86\n\u22125.01\nLow Perplexity\n\u22122.35\n\u22121.09\n\u22128.24\n1.85\n\u22122.58\n6.54\nAPE\n\u22123.13\n\u22120.54\n\u22124.85\n\u22126.14\n\u22120.51\n5.33\nRLPrompt\n2.01\n2.37\n\u2212\n\u22121.54\n3.34\n\u2212\nVariation Range\n5.90\n4.55\n40.89\n9.84\n10.02\n25.87\nFew-shot accuracy (mean relative gain) \u2191\nNull Instruction\n2.63\n0.75\n0.89\n1.20\n0.76\n\u22120.57\nGeneric Instruction\n3.09\n\u22120.10\n\u22120.15\n0.80\n\u22120.03\n0.41\nPromptSource\n\u22121.18\n\u22120.58\n\u22120.20\n\u22120.28\n\u22120.41\n0.51\nAd hoc\n\u22120.55\n\u22120.45\n0.04\n\u22120.65\n\u22120.35\n\u22120.47\nLow Perplexity\n\u22122.57\n\u22120.48\n\u22120.30\n\u22120.75\n\u22120.35\n0.27\nAPE\n\u22124.10\n0.13\n\u22120.28\n\u22121.62\n0.06\n\u22120.13\nRLPrompt\n2.69\n0.73\n\u2212\n1.31\n0.32\n\u2212\nVariation Range\n7.19\n1.33\n1.19\n2.93\n1.17\n1.08\nTable 6: Mean relative gain values for zero-shot and few-shot accuracy computed separately over models with < 6 and \u22656 billion parameters, and averaged by task-type. We also tabulate the total range of variation of these values in each setting.\n\u201clarge\" models with \u22656 billion parameters in Table 6. For ease of comparison, we average the mean relative gain values thus obtained by task-type. Although the observations that PromptSource and task-agnostic instructions tend to perform the best across zero- and few-shot settings persist across model scales, we find that the ranges of variation in the few-shot mean relative gain values for large models are consistently smaller than those for small models for every task-type. This suggests that large models are able to grasp task semantics from demonstrations (when provided) while small models are more sensitive to the instruction used.\n# 5.4 Discussion\nOur findings reveal that in practical in-context learning settings, simpler prompting methods, such as task-agnostic or expert manually written instructions, often outperform automatically synthesized ones at the model scales we consider. Task-agnostic methods show strong performance in few-shot settings, whereas expert manual instructions appear crucial for achieving good zero-shot accuracy. The superiority of these straightforward methods over automatically induced instructions, which are often not competitive even with simple baselines, suggests a lack of transferability and generalizability among automatic induction methods. The competitive performance of automatic induction methods like APE and RLPrompt as reported by their au-\nthors implies either a limitation in their generalizability to a broader range of models and tasks, or the need for substantial hyperparameter tuning to get them to work well across models and tasks. Our findings suggest that ICL practitioners may often be better off forgoing computationally expensive instruction induction or selection methods in favor of task-agnostic or manually written instructions, which seem to generalize better. Interestingly, we also find that methods that excel for one model and task do not necessarily also perform well for other tasks and models. Consequently, ICL practitioners may be forced to experiment with various instruction selection methods on a model- and task-specific basis in a manner reminiscent of hyperparameter tuning to find the best choice. On the other hand, since few-shot ICL performance remains largely consistent regardless of the choice of instruction, practitioners could perhaps benefit from simply providing a few in-context demonstrations when available. The fact that null instructions tend to outperform all other methods in our study in few-shot settings suggests that it can be challenging to find instructions that reliably inform diverse models about task semantics. When models fail to grasp the semantics signaled by instructions, these may simply serve as a source of noise, hence impairing ICL performance. Our findings underscore a broader issue regarding the inconsistent and often insufficient evaluation of instruction selection and induction techniques. We call for more comprehensive evaluations in this space and encourage the use of our evaluation suite to facilitate this process.\n# 6 Conclusion\nWe conduct the broadest attempt to our knowledge, to systematically study the generalizability of popular instruction selection and induction methods for ICL in LLMs. We find that simpler approaches such as using task-agnostic instructions, expert manual instructions, or even omitting instructions entirely tend to show good performance more consistently when evaluating across a wide variety of tasks and models. Our work indicates the need for more systematic and consistent evaluations in the instruction induction space. To facilitate such analyses, we release the InstructEval suite which provides coverage over 13 diverse autoregressive LLMs and 9 tasks spanning classification, multiplechoice QA, and generative QA.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8a85/8a85defb-0fd8-43c3-b22c-c956f401a371.png\" style=\"width: 50%;\"></div>\n# Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. 2023. On the relation between sensitivity and accuracy in in-context learning.\nOr Honovich, Uri Shaham, Samuel R Bowman, and\nOmer Levy. 2022.\nInstruction induction: From\nfew examples to natural language task descriptions.\narXiv preprint arXiv:2205.10782.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos QA: Machine reading\ncomprehension with contextual commonsense rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 2391\u20132401, Hong Kong, China. Association\nfor Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. triviaqa: A Large Scale Distantly\nSupervised Challenge Dataset for Reading Compre-\nhension. arXiv e-prints, page arXiv:1705.03551.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Nat-\nural questions: A benchmark for question answer-\ning research.\nTransactions of the Association for\nComputational Linguistics, 7:453\u2013466.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Nat-\nural questions: A benchmark for question answer-\ning research.\nTransactions of the Association for\nComputational Linguistics, 7:453\u2013466.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021b. What makes good in-context examples for gpt-3?\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021b. Fantastically\nordered prompts and where to find them: Over-\ncoming few-shot prompt order sensitivity. CoRR,\nabs/2104.08786.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142\u2013150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470\u20133487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. Semeval-\n2018 task 1: Affect in tweets. In Proceedings of the\n12th international workshop on semantic evaluation,\npages 1\u201317.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020.\nAdversarial nli:\nA new benchmark for natu-\nral language understanding.\nIn Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\n<div style=\"text-align: center;\">Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. 2023. What in-context learning\"learns\"in-context: Disentangling task recognition and task learning.</div>\nTeven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili\u2019c, Daniel Hesslow,\nRoman Castagn\u2019e, Alexandra Sasha Luccioni, Franccois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, et al. 2022a. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100.\neven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, ZhengXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, JanChristoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u02c7ek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Karen Fort, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio MirandaEscalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang,\nNatasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022b. Bloom: A 176b-parameter open-access multilingual language model.\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. ArXiv, abs/2303.03846.\nBenfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu,\nQiaoqiao She, and Yongdong Zhang. 2023.\nknn\nprompting: Beyond-context learning with calibration-\nfree nearest neighbor inference.\nSohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon\nYe, Hyunji Lee, and Minjoon Seo. 2023. Improving\nprobability-based prompt selection through unified\nevaluation and analysis.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. Opt: Open\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers.\n# A Variation across model families\nWe also tabulate the mean relative gain values for zero-shot and few-shot accuracies computed separately for each model family in Table 7, to understand the effect that model family has on instruction performance. Although the trends we discuss in Section 4 regarding task-agnostic instructions and PromptSource instructions tending to dominate few-shot and zero-shot settings persist, we note that the instruction selection method that emerges the\nbest-performing alternative often changes on varying the choice of model family and task-type. For instance, the automatic instruction induction methods APE and RLPrompt do show above-average performance for certain model families and tasktypes, but this behavior does not consistently extend to other families and types as well. This indicates a lack of generalizability in these methods.\n# B Implementation details\n# B.1 Evaluation\nWe ameliorate the effect of statistical noise by rerunning each instruction selection/induction method we study using 5 random seeds independently for every task (and for every model, where applicable) and report results for each instruction selection/induction method by averaging the aggregated scores associated with all 5 instructions. We use K = 6 demonstrations randomly sampled from the task\u2019s training set for every experiment we perform in the few-shot setting. To maintain consistency, we perform all our experiments using fixed task-specific prompt templates. Each prompt begins with the instruction being tested, and continues into a sequence annotated demonstrations and a test example, each of which follow the templates listed in Table 8.\n# B.2 Instruction selection methods\nPromptSource We sample and evaluate a random subset of instructions from those included in the public PromptSource repository for each task in our evaluation suite.\nAd hoc We obtain the set of ad hoc instructions we evaluate for a task by tasking ChatGPT with generating 40 paraphrases of instructions for the task that we obtain from PromptSource. We then select a random sample of instructions from this 40-instruction pool and perform evaluations using each sampled instruction.\nLow Perplexity For each task, we rerank a pool of ChatGPT paraphrases of PromptSource instructions using the SPELL algorithm described by (Gonen et al., 2022). When prompting a specific model, we choose the instruction with the lowest perplexity as measured by that model.\nAPE We use the official repository released by (Zhou et al., 2022) to generate instructions for each of the tasks we consider. To remain consistent\nBLOOM\nGPT Neo\nLLaMA\nOPT\nMethod\nCLS\nMCQ\nGQA\n# wins\nCLS\nMCQ\nGQA\n# wins\nCLS\nMCQ\nGQA\n# wins\nCLS\nMCQ\nGQA\n# wins\nZero-shot accuracy (mean relative gain) \u2191\nNull Instruction\n\u22121.40\n3.60\n\u221211.93\n3\n\u22121.80\n1.34\n\u221210.46\n1\n4.02\n9.25\n\u22129.73\n3\n\u22121.22\n4.54\n\u221222.07\n4\nGeneric Instruction\n5.03\n\u22121.27\n\u22120.72\n1\n\u22120.35\n\u22120.20\n\u22121.86\n1\n\u22122.73\n\u22122.09\n\u221213.25\n0\n1.33\n\u22120.47\n\u22123.47\n0\nPromptSource\n2.03\n\u22122.89\n14.22\n2\n1.61\n\u22120.69\n35.75\n2\n10.01\n\u22123.89\n7.82\n2\n2.16\n\u22122.70\n18.70\n4\nAd hoc\n0.45\n\u22121.15\n2.93\n0\n2.23\n0.56\n0.08\n2\n\u22123.70\n\u22122.94\n\u22122.56\n0\n1.35\n\u22122.23\n\u22121.26\n0\nLow Perplexity\n\u22123.76\n\u22122.19\n3.94\n1\n\u22122.85\n0.45\n\u221220.87\n1\n4.97\n\u22124.87\n5.10\n2\n2.09\n\u22121.50\n4.32\n1\nAPE\n\u22126.10\n0.75\n\u22128.44\n0\n0.14\n\u22122.00\n\u22122.87\n1\n\u22127.01\n\u22120.09\n12.62\n2\n\u22125.18\n\u22120.92\n3.78\n0\nRLPrompt\n3.76\n3.15\n\u2212\n2\n1.02\n0.54\n\u2212\n1\n\u22125.57\n4.64\n\u2212\n0\n\u22120.53\n3.28\n\u2212\n0\nFew-shot accuracy (mean relative gain) \u2191\nNull Instruction\n3.04\n1.11\n0.85\n4\n1.31\n0.33\n\u22120.11\n2\n1.40\n0.79\n\u22120.35\n4\n1.67\n0.70\n0.11\n1\nGeneric Instruction\n3.64\n\u22120.24\n\u22120.44\n2\n1.27\n0.31\n0.04\n2\n0.24\n\u22120.11\n0.23\n2\n1.89\n\u22120.17\n0.64\n3\nPromptSource\n\u22121.21\n\u22120.83\n0.19\n1\n\u22120.44\n\u22120.17\n\u22120.08\n2\n\u22120.30\n\u22120.65\n0.18\n0\n\u22120.79\n\u22120.34\n0.20\n1\nAd hoc\n\u22120.35\n\u22120.57\n0.41\n1\n\u22121.02\n\u22120.12\n\u22120.59\n0\n\u22121.26\n\u22120.62\n0.31\n0\n\u22120.20\n\u22120.33\n\u22120.77\n0\nLow Perplexity\n\u22123.00\n\u22120.58\n\u22120.23\n0\n\u22121.04\n\u22120.15\n\u22120.15\n0\n\u22120.94\n\u22120.59\n0.25\n1\n\u22121.37\n\u22120.38\n0.07\n1\nAPE\n\u22125.26\n0.29\n\u22120.78\n0\n\u22121.14\n\u22120.29\n0.85\n2\n0.29\n0.38\n\u22120.62\n1\n\u22123.64\n0.05\n\u22120.24\n0\nRLPrompt\n3.14\n0.82\n\u2212\n1\n1.06\n0.10\n\u2212\n1\n0.57\n0.80\n\u2212\n1\n2.45\n0.47\n\u2212\n3\nTable 7: Mean relative gain values for zero-shot accuracy and few-shot accuracy computed separately over individual model families and averaged by task-type. Positive values represent above-average performance, and negative values represent below-average performance. We also tabulate the number of tasks where a method achieved highest aggregated performance in the \u2018# wins\u2019 column under every model family.\nTasks\nDemonstration template\nAG News (Zhang et al., 2015)\nNews:\n(text)\\nCategory:\n(label)\nANLI (Nie et al., 2020)\nPremise:\n(premise)\\nHypothetisis:\n(hypothesis)\\nRelation:\n(label)\nBoolQ (Clark et al., 2019)\nPassage:\n(passage)\\nQuestion:\n(question) \\nAnswer:\n(label)\nIMDB (Maas et al., 2011)\nReview:\n(text)\\nSentiment:\n(label)\nTweetEval Emotion (Mohammad et al., 2018)\nTweet:\n(text)\\nEmotion:\n(label)\nCosmosQA (Huang et al., 2019)\nPassage:\n(context)\\nQuestion:\n(question)\\nAnswer:\n(answer)\nHellaSwag (Zellers et al., 2019)\nSentence:\n(ctx)\\nAnswer:\n(answer)\nNQ-Open (Kwiatkowski et al., 2019)\nQuestion:\n(question)\\nAnswer:\n(answer)\nTriviaQA (Joshi et al., 2017)\nQuestion:\n(question)\\nAnswer:\n(answer)\nwith the original methodology, we use the OpenAI DaVinci to induce and evaluate instructions during the induction phase. We opt to use the simpler version of the methodology proposed by the authors since they report that the computationally intensive Monte-Carlo search strategy only provides marginal improvements in accuracy.\nRLPrompt We use the public repository released by Deng et al. (2022) to induce instructions for the RLPrompt baseline in our evaluations. Although the original work only performs evaluations over classification datasets with a fixed label-space, we augment the codebase to allow instruction induction for MCQ tasks as well by formulating these as cloze-style completion tasks. We create instructions for all tasks using the default settings of hyperparameters included with the codebase.\nTask-agnostic We completely omit instructions from the prompt when evaluating null instructions. We list the set of generic instructions we evaluate\nin Table 10. We include examples of the instructions we obtain for each method in Table 9.\n# C Drawbacks of aggregation techniques used in previous work\nSome previous works like the HELM (Liang et al., 2022) benchmark also face similar challenges when attempting to compare high-dimensional vectors \u2013 each representing a model evaluated over a variety of tasks \u2013 against each other. HELM resorts to scoring models using head-to-head win rates. The win rate associated with a model indicates the fraction of head-to-head comparisons between the given model and all other models, across all scenarios, where the given model performs better along a specific metric. A notable disadvantage of this scoring technique is that it obscures the magnitude of variation in the metric associated with each test model and only conveys ordinal information about the relative performances of each model. This char-\n<div style=\"text-align: center;\">Example instruction</div>\nMethod\nExample instruction\nNull instruction\n(empty string)\nGeneric instruction\nSolve the following task:\nPromptSource (Bach et al., 2022)\nWhat label best describes this news article?\nAd hoc\nWhich newspaper section is most likely to feature this news article?\nLow Perplexity (Gonen et al., 2022)\nWhich part of a newspaper do you think this article belongs to?\nWorld News,\nSports, Business or Science and Technology?\nAPE (Zhou et al., 2022)\nclassify each input into one of the following categories:\nWorld, U.S.,\nBusiness, Sci/Tech, or Sports.\nRLPrompt (Deng et al., 2022)\nTools undergradCam firmwareCam\nGeneric Instructions\nSolve the following task:\nFind the answer below:\nComplete the problem.\nFind the best solution to the question below:\nComplete the question below:\n<div style=\"text-align: center;\">Table 10: Sample generic instructions</div>\nacteristic of head-to-head win rates makes them unsuitable for spotting broad trends across families of prompting methods. In other works like BIG-bench (Srivastava et al., 2022), raw metric scores representing task performance are normalized to vary from a range of 0-100 such that a normalized score of 0 corresponds to poor performance, while a normalized score of 100 corresponds to excellent performance on the task. This is done in an attempt to be able to compare the performance of a model across a variety of tasks of varying difficulty such that the normalization proves more forgiving on difficult tasks. While this score does capture cardinal information associated with the underlying variable, it relies on the knowledge of human experts to determine raw score thresholds that constitute poor or excellent performance along a given metric. To apply such a normalization scheme in our case, one would need access to a large array of such threshold scores corresponding to each model scale, task, and metric we consider. Obtaining such threshold scores across all our settings is challenging given the number of tests we perform and the variety of metrics we consider. Hence, this type of normalization proves infeasible in our case.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The emergence of in-context learning (ICL) abilities in large language models (LLMs) has shifted research focus from task-specific models to foundation models that can perform various tasks using well-constructed prompts. However, the performance of ICL is sensitive to prompt details, particularly instruction choice, which has been underexplored.",
            "purpose of benchmark": "InstructEval is developed to evaluate instruction selection methods for ICL comprehensively, enabling comparisons across different models and tasks."
        },
        "problem": {
            "definition": "The benchmark is designed to assess the effectiveness of various instruction selection techniques in improving ICL performance across multiple tasks and model families.",
            "key obstacle": "Existing benchmarks primarily focus on evaluating model performance rather than systematically analyzing instruction selection methods, leading to difficulties in comparing results across studies."
        },
        "idea": {
            "intuition": "The creation of InstructEval stems from the observation that instruction choice significantly impacts ICL performance, yet this aspect has been largely neglected in prior research.",
            "opinion": "The authors emphasize that understanding instruction selection is crucial for advancing ICL methodologies and improving model performance.",
            "innovation": "InstructEval introduces a diverse evaluation suite that includes 13 LLMs and 9 tasks, allowing for a broader analysis of instruction selection methods compared to previous benchmarks.",
            "benchmark abbreviation": "InstructEval"
        },
        "dataset": {
            "source": "The dataset comprises open-sourced LLMs and tasks sourced from various public datasets, ensuring a wide representation of tasks and models.",
            "desc": "The dataset includes 13 autoregressive LLMs from four model families and covers nine tasks across three categories, providing a comprehensive evaluation framework.",
            "content": "The dataset includes text data related to classification, multiple-choice question-answering, and generative question-answering tasks.",
            "size": "1,000,000",
            "domain": "Natural Language Processing",
            "task format": "Classification"
        },
        "metrics": {
            "metric name": "Zero-shot accuracy, Few-shot accuracy",
            "aspect": "Model performance and robustness in ICL tasks.",
            "principle": "Metrics were selected based on their relevance to ICL performance and the need to capture both accuracy and sensitivity to instruction choice.",
            "procedure": "Model performance is evaluated using mean relative gain across various models and tasks, along with sensitivity metrics to assess the stability of instruction effectiveness."
        },
        "experiments": {
            "model": "The benchmark tests 13 autoregressive LLMs from four different model families, including state-of-the-art and baseline models.",
            "procedure": "Models were evaluated using fixed task-specific prompt templates across multiple trials to ensure consistent results.",
            "result": "Results indicate that simpler instruction selection methods often outperform more complex automated techniques, highlighting the variability in performance across tasks and models.",
            "variability": "Variability was accounted for by averaging results across multiple trials and random selections of demonstrations."
        },
        "conclusion": "InstructEval demonstrates that simpler instruction selection methods, such as task-agnostic and expert manual instructions, generally yield better performance across various tasks and models, emphasizing the need for systematic evaluations in instruction induction techniques.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive evaluation framework that enables the comparative analysis of instruction selection methods, contributing valuable insights to the field of ICL.",
            "limitation": "The benchmark may not encompass all potential instruction selection methods, and its findings may be model- and task-specific, limiting generalizability.",
            "future work": "Future research should explore additional instruction selection methods and extend evaluations to more diverse tasks and models to enhance understanding of ICL dynamics."
        },
        "other info": {
            "info1": "The evaluation suite is publicly available for researchers to utilize in further studies.",
            "info2": {
                "info2.1": "InstructEval covers tasks that are challenging yet achievable for contemporary language models.",
                "info2.2": "The suite facilitates a systematic study of prompt engineering beyond instruction selection alone."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The emergence of in-context learning (ICL) abilities in large language models (LLMs) has shifted research focus from task-specific models to foundation models that can perform various tasks using well-constructed prompts."
        },
        {
            "section number": "1.4",
            "key information": "The performance of ICL is sensitive to prompt details, particularly instruction choice, which has been underexplored."
        },
        {
            "section number": "2",
            "key information": "The benchmark InstructEval is developed to evaluate instruction selection methods for ICL comprehensively, enabling comparisons across different models and tasks."
        },
        {
            "section number": "3.1",
            "key information": "Results indicate that simpler instruction selection methods often outperform more complex automated techniques, highlighting the variability in performance across tasks and models."
        },
        {
            "section number": "4.1",
            "key information": "Understanding instruction selection is crucial for advancing ICL methodologies and improving model performance."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark may not encompass all potential instruction selection methods, and its findings may be model- and task-specific, limiting generalizability."
        },
        {
            "section number": "7",
            "key information": "Future research should explore additional instruction selection methods and extend evaluations to more diverse tasks and models to enhance understanding of ICL dynamics."
        }
    ],
    "similarity_score": 0.7132641689758952,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/InstructEval_ Systematic Evaluation of Instruction Selection Methods.json"
}