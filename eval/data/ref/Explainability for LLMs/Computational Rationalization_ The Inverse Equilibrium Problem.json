{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1103.5254",
    "title": "Computational Rationalization: The Inverse Equilibrium Problem",
    "abstract": "Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward \u2014 it must speculate on how the other agents may act to influence the game\u2019s outcome. Employing the game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior, as well as recovering a reward function in these domains.",
    "bib_name": "waugh2011computationalrationalizationinverseequilibrium",
    "md_text": "# Computational Rationalization: The Inverse Equilibrium Problem\nKevin Waugh Brian D. Ziebart J. Andrew Bagnell Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA, USA 15213\n# Abstract\nModeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward \u2014 it must speculate on how the other agents may act to influence the game\u2019s outcome. Employing the game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior, as well as recovering a reward function in these domains.\n# 1. Introduction\nPredicting the actions of others in complex and strategic settings is an important facet of intelligence that guides our interactions\u2014from walking in crowds to negotiating multi-party deals. Recovering such behavior from merely a few observations is an important and challenging machine learning task.\nWhile mature computational frameworks for decisionmaking have been developed to prescribe the behavior that an agent should perform, such frameworks are\nThis is a preliminary draft. Copyright 2011 by the author(s)/owner(s).\nwaugh@cs.cmu.edu bziebart@cs.cmu.edu dbagnell@ri.cmu.edu\noften ill-suited for predicting the behavior that an agent will perform. Foremost, the standard assumption of decision-making frameworks that a criteria for preferring actions (e.g., costs, motivations and goals) is known a priori often does not hold. Moreover, real behavior is typically not consistently optimal or completely rational; it may be influenced by factors that are difficult to model or subject to various types of error when executed. Meanwhile, the standard tools of statistical machine learning (e.g., classification and regression) may be equally poorly matched to modeling purposeful behavior; an agent\u2019s goals often succinctly, but implicitly, encode a strategy that would require tremendous amounts of data to learn.\nIn the single-agent decision-theoretic setting, inverse optimal control methods have been used to bridge this gap between the prescriptive frameworks and predictive applications (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008a; 2010). Successful applications include learning and prediction tasks in personalized vehicle route planning (Ziebart et al., 2008a), robotic crowd navigation (Henry et al., 2010), quadruped foot placement and grasp selection (Ratliff et al., 2009). A reward function is learned by these techniques that both explains demonstrated behavior\nand approximates the optimality criteria of prescriptive decision-theoretic frameworks.\nAs these methods only capture a single reward function and do not reason about competitive or cooperative motives, inverse optimal control proves inadequate for modeling the strategic interactions of multiple agents. In this paper, we consider the gametheoretic concept of regret as a necessary stand-in for the optimality criteria of the single-agent work. As with the inverse optimal control problem, the result is fundamentally ill-posed. We address this by requiring that for any utility function linear in known features, our learned model must have no more regret than that of the observed behavior. We demonstrate that this requirement can be re-cast as a set of equivalent convex constraints that we denote the inverse correlated equilibrium (ICE) polytope. As we are interested in the effective prediction of behavior, we will use a maximum entropy criteria to select behavior from this polytope. We demonstrate that optimizing this criteria leads to mini-max optimal prediction of behavior subject to approximate rationality. We consider the dual of this problem and note that it generalizes the traditional log-linear maximum entropy family of problems (Della Pietra et al., 2002). We provide a simple and computationally efficient gradient-based optimization strategy for this family and show that only a small number of observations are required for accurate prediction and transfer of behavior. We conclude by considering a matrix routing game and compare the ICE approach to a variety of natural alternatives.\nBefore we formalize imitation learning in matrix games, motivate our assumptions and describe and analyze our approach, we will review the game-theoretic notions of regret and the correlated equilibrium.\n# 2. Game Theory Background\nMatrix games are the canonical tool of game theorists for representing strategic interactions ranging from illustrative toy problems, such as the \u201cPrisoner\u2019s Dilemma\u201d and the \u201cBattle of the Sexes\u201d games, to important negotiations, collaborations, and auctions. In this work, we employ a class of games with payoffs or utilities that are linear functions of features defined over the outcome space.\nDefinition 1. A linearly parameterized normalform game, or matrix game, \u0393 = (N, A, F), is composed of: a finite set of players, N; a set of joint-actions or outcomes, A = \u00d7i\u2208NAi, consisting of a finite set of actions for each player, Ai; a set\nFor notational convenience, we let a\u2212i denote the vector a excluding component i and let A\u2212i = \u00d7j\u0338=i,j\u2208NAi be the set of such vectors.\nWe model the players with a distribution \u03c3 \u2208\u2206A over the game\u2019s joint-actions. Coordination between players can exist, thus, this distribution need not factor into independent strategies for each player. Conceptually, a signaling mechanism, such as a traffic light, can be thought to sample a joint-action from \u03c3 and communicate to each player ai, its portion of the joint-action. Each player can then consider deviating from ai using a modification function, fi : Ai \ufffd\u2192Ai (Blum & Mansour, 2007).\n#   The switch modification function, for instance,\n(1)\nsubstitutes action y for recommendation x. Instantaneous regret measures how much a player would benefit from a particular modification function when the coordination device draws joint-action a,\n(2) (3) (4)\nPlayers do not have knowledge of the complete jointaction; thus, each must reason about the expected regret with respect to a modification function,\n(5) (6)\nIt is helpful to consider regret with respect to a class of modification functions. Two classes are particularly important for our discussion. First, internal regret corresponds to the set of modification functions where a single action is replaced by a new action, \u03a6int i =\n{switchx\u2192y i (\u00b7) : \u2200x, y \u2208Ai}. Second, swap regret corresponds to the set of all modification functions, \u03a6swap i = {fi}. We denote \u03a6 = \u222ai\u2208N \u03a6i.\n# The expected regret with respect to \u03a6 and outcome distribution \u03c3,\n(7)\nis important for understanding the incentive to deviate from, and hence the stability of, the specified behavior. The most general modification class, \u03a6swap, leads to the notion of \u03b5-correlated equilibrium (Osborne & Rubinstein, 1994), in which \u03c3 satisfies R\u03a6swap(\u03c3, w\u2217) \u2264 \u03b5. Thus, regret can be thought of as a substitute for utility when assessing the optimality of behavior in multi-agent settings.\n# 3. Imitation Learning in Matrix Games\nWe are now equipped with the tools necessary to introduce our approach for imitation learning in multiagent settings. As input, we observe a sequence of outcomes, {am}M m=1, sampled from \u03c3, the true behavior. We denote the empirical distribution of this sequence, \u02dc\u03c3, the demonstrated behavior. We aim to learn a predictive behavior distribution, \u02c6\u03c3 from these demonstrations. Moreover, we would like our learning procedure to extract the motives and intent for the behavior so that we may imitate the players in similarly structured, but unobserved games.\nImitation appears hard barring further assumptions. In particular, if the agents are unmotivated or their intentions are not coerced by the observed game, there is little hope of recovering principled behavior in a new game. Thus, we require some form of rationality.\n# 3.1. Rationality Assumptions\nWe say that agents are rational under their true preferences when they are indifferent between \u02c6\u03c3 and their true behavior if and only if R\u03a6(\u02c6\u03c3, w\u2217) \u2264R\u03a6(\u03c3, w\u2217).\nAs agents\u2019 true preferences w\u2217are unknown to the observer, we must consider an encompassing assumption that requires any behavior that we estimate to satisfy this property for all possible utility weights, or\n\u2200w \u2208RK, R\u03a6(\u02c6\u03c3, w) \u2264R\u03a6(\u03c3, w).\n(8)\nAny behavior achieving this restriction, strong rationality, is also rational, and, by virtue of the contrapositive, we see that unless we have additional information regarding the agents\u2019 true preferences, we must assume this strong assumption or we risk violating rationality.\nLemma 1. If strong rationality does not hold for alternative behavior \u02c6\u03c3 then there exist agent utilities such that they would prefer \u03c3 to \u02c6\u03c3.\nBy restricting our attention to behavior that satisfies strong rationality, at worst, agents acting according to unknown true preference w\u2217will be indifferent between our predictive distribution and their true behavior.\n# 3.2. Inverse Correlated Equilibria\nUnfortunately, a direct translation of the strong rationality requirement into constraints on the distribution \u02c6\u03c3 leads to a non-convex optimization problem as it involves products of varying utility vectors and the behavior to be estimated. Fortunately, however, we can provide an equivalent concise convex description of the constraints on \u02c6\u03c3 that ensures any feasible distribution satisfies strong rationality. We denote this set of equivalent constraints as the Inverse Correlated Equilibria (ICE) polytope:\n(9)\nTheorem 1. A distribution, \u02c6\u03c3, satisfies the constraints above for some \u03b7 if and only if it satisfies strong rationality. That is, \u2200w \u2208RK, R\u03a6(\u02c6\u03c3, w) \u2264 R\u03a6(\u02dc\u03c3, w) if and only if \u2200fi \u2208\u03a6, \u2203\u03b7fi \u2208\u2206\u03a6 such that \u02c6\u03c3TRfi i = \ufffd fj\u2208\u03a6 \u03b7fi fj \u02dc\u03c3TRfj j .\n \ufffd The proof of Theorem 1 is provided in the Appendix (Waugh et al., 2011).\nWe note that this polytope, perhaps unsurprisingly, is similar to the polytope of correlated equilibrium itself, but here is defined in terms of the behavior we observe instead of the (unknown) reward function. Given any observed behavior \u03c3, the constraints are feasible as the demonstrated behavior satisfies them; our goal is to choose from these behaviors without estimating a full joint-action distribution. While the ICE polytope establishes a basic requirement for estimating rational behavior, there are generally infinitely many distributions consistent with its constraints.\n# 3.3. Principle of Maximum Entropy\nAs we are interested in the problem of statistical prediction of strategic behavior, we must find a mechanism to resolve the ambiguity remaining after accounting for the rationality constraints. The principle of maximum entropy provides a principled method for\nchoosing such a distribution. This choice leads to not only statistical guarantees on the resulting predictions, but to efficient optimization.\nThe Shannon entropy of a distribution \u02c6\u03c3 is defined as H(\u02c6\u03c3) = \u2212\ufffd x\u2208X \u02c6\u03c3x log \u02c6\u03c3x. The principle of maximum entropy advocates choosing the distribution with maximum entropy subject to known (linear) constraints (Jaynes, 1957):\n(10)\nThe resulting log-linear family of distributions (e.g., logistic regression, Markov random fields, conditional random fields) are widely used within statistical machine learning. For our problem, the constraints are precisely that the distribution is in the ICE polytope, ensuring that whatever distribution is learned has no more regret than the demonstrated behavior.\nThe proof of Lemma 2 follows immediately from the result of Gr\u00a8unwald and Dawid (2003).\nIn the context of multi-agent behavior, the principle of maximum entropy has been employed to obtain correlated equilibria with predictive guarantees in normalform games when the utilities are known a priori (Ortiz et al., 2007). We will now leverage its power with our rationality assumption to select predictive distributions in games where the utilities are unknown.\n# 3.4. Prediction of Behavior\nLet us first consider prediction of the demonstrated behavior using the principle of maximum entropy and our strong rationality condition. After, we will extend to behavior transfer and analyze the error introduced as a by-product of sampling \u02dc\u03c3 from \u03c3.\nThe mathematical program that maximizes the entropy of \u02c6\u03c3 under strong rationality with respect to \u02dc\u03c3,\n(11)\nis convex with linear constraints, feasible, and bounded. That is, it is simple and can be efficient solved in this form. Before presenting our preferred dual optimization procedure, however, let us describe an approach for behavior transfer that further illustrates the advantages of this approach over directly estimating \u03c3.\n# 3.5. Transfer of Behavior\nA principal justification of inverse optimal control techniques that attempt to identify behavior in terms of utility functions is the ability to consider what behavior might result if the underlying decision problem were changed while the interpretation of features into utilities remain the same (Ng & Russell, 2000; Ratliff et al., 2006). This enables prediction of agent behavior in a no-regret or agnostic sense in problems such as a robot encountering novel terrain (Silver et al., 2010) as well as route recommendation for drivers traveling to unseen destinations (Ziebart et al., 2008b). Econometricians are interested in similar situations, but for much different reasons. Typically, they aim to validate a model of market behavior from observations of product sales. In these models, the firms assume a fixed pricing policy given known demand. The econometrician uses this fixed policy along with product features and sales data to estimate or bound both the consumers\u2019 utility functions as well as unknown production parameters, like markup and production cost (Berry et al., 1995; Nevo, 2001; Yang, 2009). In this line of work, the observed behavior is considered accurate to start with; it is not suitable for settings with limited observations. Until now, we have considered the problem of identifying behavior in a single game. We note, however, that our approach enables behavior transfer to games equipped with the same features. We denote this unobserved game as \u00af\u0393. As with prediction, to develop a technique for behavior transfer we assume a link between regret and the agents\u2019 preferences across the known space of possible preferences. Furthermore, we assume a relation between the regrets in both games. Property 1 (Transfer Rationality). For some constant \u03ba > 0,\n(12)\nRoughly, we assume that under preferences with low regret in the original game, the behavior in the unobserved game should also have low regret. By enforcing this property, if the agents are performing well with respect to their true preferences, then the transferred behavior will also be of high quality.\nAs we are not privileged to know \u03ba and this property is not guaranteed to hold, we introduce a slack variable to allow for violations of the strong rationality constraints to guaranteeing feasibility. Intuitively, the transfer-ICE polytope we now optimize over requires that for any linear reward function and for every player, the predicted behavior in a new game must have no more regret than demonstrated behavior does in the observed game using the same parametric form of reward function. The corresponding mathematical program is:\n(13)\nOne could choose to institute multiple slack variables, say one for each fi \u2208\u00af\u03a6, instead of a single slack across all modification functions. Our choice is motivated by the interpretation of the dual multipliers presented in the next section. There, we will also address selection of an appropriate value for C.\n# 4. Duality and Efficient Optimization\nIn this section, we will derive, interpret and describe a procedure for optimizing the dual program for solving the MaxEnt ICE problem. We will see that the dual multipliers can be interpreted as utility vectors and that optimization in the dual has computational advantages. We begin by presenting the dual of the\nAlgorithm 1 Dual MaxEnt ICE\nInput: T, \u03b3, C > 0, R, \u00afR, \u03a6 and \u00af\u03a6\n\u2200fi \u2208\u00af\u03a6, \u03b1fi, \u03b2fi \u21901/(|\u00af\u03a6|K + 1)\nfor t from 1 to T do\n/* compute the gradient */\n\u2200a \u2208\u00af\nA, za \u2190exp\n\ufffd\n\u2212\ufffd\nfi\u2208\u00af\u03a6 \u00afrfiT\ni,a (\u03b1fi \u2212\u03b2fi)\n\ufffd\nZ \u2190\ufffd\na\u2208\u00af\nA za\nfor fi \u2208\u00af\u03a6 do\nf \u2217\nj \u2190argmaxfj\u2208\u03a6 \u02dc\u03c3TRfj\nj (\u03b1fi \u2212\u03b2fi)\ngfi \u2190\u02dc\u03c3TR\nf \u2217\nj\nj\u2217\u2212\ufffd\na\u2208\u00af\nA za\u00afrfi\ni,a/Z\nend for\n/* descend and project */\n\u03b3t \u2190\u03b3/\n\u221a\nt\n\u03c1 \u21901 + \ufffd\nfi,k \u03b1fi\nk exp(\u2212\u03b3tgfi\nk ) + \u03b2fi\nk exp(\u03b3tgfi\nk )\n\u2200fi \u2208\u00af\u03a6, k \u2208K, \u03b1fi\nk \u2190C\u03b1fi\nk exp(\u2212\u03b3tgfi\nk )/\u03c1\n\u2200fi \u2208\u00af\u03a6, k \u2208K, \u03b2fi\nk \u2190C\u03b2fi\nk exp(\u03b3tgfi\nk )/\u03c1\nend for\nreturn (\u03b1, \u03b2)\ntransfer program.\nwhere Z(\u03b1, \u03b2) is the partition function,\nRemoving the equality constraint is equivalent to disallowing any slack. We derive the dual in the appendix (Waugh et al., 2011). For C > 0, the dual\u2019s feasible set has non-empty interior and is bounded. Therefore, by Slater\u2019s condition, strong duality holds \u2013 there is no duality gap. In particular, we can use a dual solution to recover \u02c6\u03c3. Lemma 4. Given a dual solution, (\u03b1, \u03b2), we can recover the primal solution, \u02c6\u03c3. Specifically,\n(15)\nIntuitively, the probability of predicting an outcome is small if that outcome has high regret. In general, the dual multipliers are utility vectors associated with each modification function in \u00af\u03a6. Under\nthe slack formulation, there is a natural interpretation of these variables as a single utility vector. Given a dual solution, (\u03b1, \u03b2) with slack penalty C, we choose\n(17)\n(18)\nThat is, we can associate with each modification function a probability, \u03c0fi, and a utility vector, \u03bbfi. Thus, a natural estimate for \u02c6w is the expected utility vector. Note, \ufffd fi\u2208\u00af\u03a6 \u03c0fi need not sum to one. The remaining mass, \u03be, is assigned to the zero utility vector. The above observation implies that introducing a slack variable coincides with bounding the L1 norm of the utility vectors under consideration by C. This insight suggests that we choose C \u2265||w\u2217||1, if possible, as smaller values of C will exclude w\u2217from the feasible set. If a bound on the L1 norm is not available, we may solve the prediction problem on the observed game without slack and use || \u02c6w||1 as a proxy. The dual formulation of our program has important inherent computational advantages. First, it is a optimization over a simple set that is particularly wellsuited for gradient-based optimization, a trait not shared by the primal program. Second, the number of dual variables, 2|\u03a6|K, is typically much fewer than the number of primal variables, |A| + 2|\u03a6|2. Though the work per iteration is still a function of |A| (to compute the partition function), these two advantages together let us scale to larger problems than if we consider optimizing the primal objective. Computing the expectations necessary to descend the dual gradient can leverage recent advances in the structured, compact game representations: in particular, any graphical game with low-treewidth or finite horizon Markov game (Kakade et al., 2003) enables these computations to be performed in time that scales only polynomially in the number of decision makers or time-steps. Algorithm 1 employs exponentiated gradient descent (Kivinen & Warmuth, 1995) to find an optimal dual solution. The step size parameter, \u03b3, is commonly taken to be \ufffd 2 log |\u00af\u03a6|K/\u2206, with \u2206being the largest value in any Rfi i . With this step size, if the optimization is run for T \u22652\u22062 log \ufffd |\u00af\u03a6|K \ufffd /\u03f52 iterations then the dual solution will be within \u03f5 of optimal. Alternatively, one can exactly measure the duality gap on each iteration and halt when the desired accuracy is\nAlgorithm 1 employs exponentiated gradient descent (Kivinen & Warmuth, 1995) to find an optimal dual solution. The step size parameter, \u03b3, is commonly taken to be \ufffd 2 log |\u00af\u03a6|K/\u2206, with \u2206being the largest value in any Rfi i . With this step size, if the optimization is run for T \u22652\u22062 log \ufffd |\u00af\u03a6|K \ufffd /\u03f52 iterations then the dual solution will be within \u03f5 of optimal. Alternatively, one can exactly measure the duality gap on each iteration and halt when the desired accuracy is achieved. This is often preferred as the lower bound on the number of iterations is conservative in practice.\n# 6. Experimental Results\nTo evaluate our approach experimentally, we designed a simple routing game shown in Figure 1. Seven drivers in this game choose how to travel home during rush hour after a long day at the office. The different road segments have varying capacities, visualized by the line thickness in the figure, that make some of them more or less susceptible to congestion or to traffic accidents. Upon arrival home, each driver records the total time and distance they traveled, the gas that they used, and the amount of time they spent stopped at intersections or in traffic jams \u2013 their utility features.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a1ab/a1ab5d23-dbf6-48cf-bc4e-987a870fcd3d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48fc/48fc8b24-79dc-4ad4-938a-f7f3793a12a8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. A visualization of the routing game.</div>\n011 In this game, each of the drivers chooses from four possible routes (solid lines in Figure 1), yielding over 16,000 possible outcomes. We obtained an \u03b5-social welfare maximizing correlated equilibrium for those drivers where the drivers preferred mainly to minimize their travel time, but were also slightly concerned with gas usage. The demonstrated behavior \u02dc\u03c3 was sampled from this true behavior distribution \u03c3. First, we evaluate the differences between the true behavior distribution \u03c3 and the predicted behavior distribution \u02c6\u03c3 trained from observed behavior sampled from \u02dc\u03c3. In Figure 2 we compare the prediction accuracy when varying the number of observations using log-loss, \u2212\ufffd a\u2208A \u03c3a log \u02c6\u03c3a. The baseline algorithms we compare against are: a maximum likelihood estimate of the distribution over the joint-actions with a uniform prior, an exponential family distribution parameterized by the outcome\u2019s utilities trained with logistic regression, and a maximum entropy inverse optimal control approach (Ziebart et al., 2008a) trained individually for each player. In Figure 2, we see that MaxEnt ICE predicts behavior with higher accuracy than all other algorithms when the number of observations is limited. In particular, it achieves close to its best performance with as few at 16 observations. The maximum likelihood estimator eventually overtakes it, as expected since it will ultimately converge to \u03c3, but only after 10,000 observations, or about as many observations as there are outcomes in the game. This experiment demonstrates that learning underlying utility functions to estimate observed behavior can be much more data-efficient for small sample sizes, and additionally, that the regretbased assumptions of MaxEnt ICE are both reasonable\n<div style=\"text-align: center;\">Figure 2. Prediction error (log-loss) as a function of number of observations.</div>\n<div style=\"text-align: center;\">Table 1. Transfer error (log-loss) on unobserved games.</div>\nProblem\nLogistic Model\nMaxEnt Ice\nAdd Highway\n4.177\n3.093\nAdd Driver\n4.060\n3.477\nGas Shortage\n3.498\n3.137\nCongestion\n3.345\n2.965\nand beneficial in our strategic routing game setting. Next, we evaluate behavior transfer from this routing game to four similar games, the results of which are displayed in Table 1. The first game, Add Highway, adds the dashed route to the game. That is, we model the city building a new highway. The second game, Add Driver, adds another driver to the game. The third game, Gas Shortage, keeps the structure of the game the same, but changes the reward function to make gas mileage more important to the drivers. The final game, Congestion, adds construction to the major roadway, delaying the drivers. These transfer experiments even more directly demonstrate the benefits of learning utility weights rather than directly learning the joint-action distribution; direct strategy-learning approaches are incapable of being applied to general transfer setting. Thus, we only compare against the Logistic Model. We see from Table 1 that MaxEnt ICE outperforms the Logistic Model in all of our tests. For reference, in these new games, the uniform strategy has a loss of approximately 6.8 in all games, and the true behavior has a loss of approximately 2.7.\n# 7. Conclusion\nIn this paper, we extended inverse optimal control to multi-agent settings by combining the principle of maximum entropy with the game-theoretic notion of regret. We observed that our formulation has a particularly appealing dual program, which led to a simple gradient-based optimization procedure. Perhaps the most appealing quality of our technique is its theoretical and practical sample complexity. In our experiments, MaxEnt ICE performed exceptionally well after only 0.1% of the game had been observed.\n# Acknowledgments\nThis work is supported by the ONR MURI grant N00014-09-1-1052 and by the National Sciences and Engineering Research Council of Canada (NSERC).\n# References\nAbbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the International Conference on Machine Learning, 2004.\nJaynes, E. T. Information theory and statistical mechanics. Physical Review, 106(4):620\u2013630, May 1957.\nKakade, S., Kearns, M., Langford, J., and Ortiz, L. Correlated equilibria in graphical games. In Proceedings of Electronic Commerce, pp. 42\u201347, 2003.\nAppendix Rationality Properties and Primal Programs The proof of Theorem 1 relies upon the following technical lemmas. Lemma 5.\nbTw \u2264max ai\u2208A ai Tw \u21d4\u2203\u03bb \u2208\u2206A s.t. bTw \u2264\u03bbTAw\nProof of Lemma 5. Given bTw \u2264maxai\u2208A aiTw, choose\nThus, bTw \u2264maxai\u2208A aiTw = \u03bbTAw. Given \u2203\u03bb \u2208\u2206A s.t. bTw \u2264\u03bbTAw,\nLemma 6.\n\u2200w \u2208RK, bTw \u2264max i\u2208N ai Tw \u21d4\u2203\u03bb \u2208\u2206A s.t. b = \u03bbTA.\n\u21d4the following linear program has optimal value 0\nBy strong duality for linear programming, the primal has value 0 iff the dual is feasible, which is exactly when \u2203\u03bb \u2208\u2206A s.t. b = \u03bbTA.\n(19)\n(20) (21)\n(22)\n(23)\n(24) (25) (26)\n(28)\nProof of Theorem 1.\n\u2200w \u2208RK, R\u03a6(\u02c6\u03c3, w) \u2264R\u03a6(\u02dc\u03c3, w) \u21d4\u2200w \u2208RK, max fi\u2208\u03a6 \u02c6\u03c3TRfi i w \u2264max fi\u2208\u03a6 \u02dc\u03c3TRfi i w \u21d4\u2200fi \u2208\u03a6, \u2200w \u2208RK, \u02c6\u03c3TRfi i w \u2264max fj\u2208\u03a6 \u02dc\u03c3TRfj j w \u21d4\u2200fi \u2208\u03a6, \u2203\u03b7fi \u2208\u2206\u03a6 s.t. \u02c6\u03c3TRfi i = \ufffd fj\u2208\u03a6 \u03b7fi fj \u02dc\u03c3TRfj j\nThe last step makes use of our second technical lemma.\nDerivation of the Dual Program The Lagrange dual is\nTo solve the unconstrained inner optimization, we take derivatives w.r.t. \u03c3, \u03b7 and \u03bd and set equal to \nSubstituting into the Lagrangian, we get\n(29) (30) (31) (32)\n(32)\n\n(34)\n(35)\n(36)\n(38)\n(41)\n(44)\nWe eliminate \u03b4 by setting its partial derivative to 0, solving for \u03b4\nand substituting back into the objective\nBy inspection, at optimality, \u03b3fi = maxfj\u2208\u03a6 \u02dc\u03c3TRfj j (\u03b1fi \u2212\u03b2fi). Thus an equivalent program i\nAll that remains is to exponentiate both sides.\n(45)\n(46)\n(48)\n(49)\n(50)\n(53)\n(54)\n(55)\n(56)\n(57)\n(58)\n\nSample Complexity Proof of Theorem 2.\nWe use the union bound in step 2, and Hoeffding\u2019s inequality in step 3. Solving for M, we get our result\nProof of Corollary 1. We have \u2200w, R\u03a6(\u02c6\u03c3, w) \u2264R\u03a6(\u02dc\u03c3, w) + \u03bd ||w||1, where \u03bd depends on the choice of the slack\u2019s penalty. Thus, we have R\u03a6(\u02c6\u03c3, w\u2217) \u2264R\u03a6(\u02dc\u03c3, w\u2217)+\u03bd ||w||1 \u2264R\u03a6(\u03c3, w\u2217)+(\u03f5\u2206+\u03bd) ||w\u2217||1 with probability at least 1 \u2212\u03b4, so long as M is as large as Theorem 2 deems. We can make \u03bd as small as we like by increasing the slack penalty.\n(59)\n(61)\n(62)\n(63)\n(64)\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of modeling the purposeful behavior of imperfect agents in competitive and cooperative multi-agent domains, where existing single-agent inverse optimal control techniques are insufficient. It highlights the necessity for a new method to effectively predict behavior and recover reward functions in these complex settings.",
        "problem": {
            "definition": "The problem is to accurately predict and generalize the behavior of agents in multi-agent settings, where agents must consider the potential actions of others, rather than just maximizing their own rewards.",
            "key obstacle": "The main difficulty lies in the inadequacy of existing methods to model strategic interactions among multiple agents, as they do not account for competitive or cooperative motives."
        },
        "idea": {
            "intuition": "The idea is inspired by the game-theoretic notion of regret, which serves as a substitute for the optimality criteria used in single-agent decision-making frameworks.",
            "opinion": "The proposed method, termed Inverse Correlated Equilibrium (ICE), aims to learn a predictive behavior distribution from observed behavior while ensuring that the learned model does not incur more regret than the observed behavior.",
            "innovation": "The key innovation of this method is the introduction of the ICE polytope, which reformulates the problem of learning in multi-agent settings into a convex optimization problem, allowing for effective predictions based on limited observations."
        },
        "method": {
            "method name": "Inverse Correlated Equilibrium (ICE)",
            "method abbreviation": "ICE",
            "method definition": "ICE is a technique that predicts and generalizes agent behavior in multi-agent settings by learning a distribution that minimizes regret compared to observed behavior.",
            "method description": "The method involves optimizing the distribution of actions using the principle of maximum entropy while satisfying rationality constraints derived from observed behavior.",
            "method steps": [
                "Define the observed behavior distribution from a sequence of outcomes.",
                "Establish rationality constraints that the learned distribution must satisfy.",
                "Formulate the optimization problem to maximize entropy subject to these constraints.",
                "Use a gradient-based optimization approach to solve the optimization problem."
            ],
            "principle": "The principle of maximum entropy ensures that the learned distribution is as uniform as possible while still conforming to the constraints imposed by observed behavior, leading to effective predictions."
        },
        "experiments": {
            "evaluation setting": "The evaluation was conducted in a routing game involving multiple drivers who choose routes home during rush hour, with varying road capacities and traffic conditions.",
            "evaluation method": "The performance of the ICE method was assessed by comparing the predicted behavior distribution to the true behavior distribution using log-loss as the metric, across varying numbers of observations."
        },
        "conclusion": "The experiments demonstrated that the ICE method effectively predicts agent behavior with high accuracy from a small number of observations, outperforming traditional methods. The combination of maximum entropy and regret-based learning provides a robust framework for multi-agent behavior modeling.",
        "discussion": {
            "advantage": "The primary advantage of the ICE approach is its ability to efficiently learn from limited data, achieving high prediction accuracy compared to existing methods.",
            "limitation": "A limitation of the method is that it relies on the assumption of rationality among agents, which may not always hold in practice.",
            "future work": "Future research could explore relaxing the rationality assumptions or applying the ICE framework to a broader range of multi-agent scenarios to enhance its applicability."
        },
        "other info": {
            "acknowledgments": "This work is supported by the ONR MURI grant N00014-09-1-1052 and by the National Sciences and Engineering Research Council of Canada (NSERC)."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem is to accurately predict and generalize the behavior of agents in multi-agent settings, where agents must consider the potential actions of others, rather than just maximizing their own rewards."
        },
        {
            "section number": "2.1",
            "key information": "The main difficulty lies in the inadequacy of existing methods to model strategic interactions among multiple agents, as they do not account for competitive or cooperative motives."
        },
        {
            "section number": "3.4",
            "key information": "The proposed method, termed Inverse Correlated Equilibrium (ICE), aims to learn a predictive behavior distribution from observed behavior while ensuring that the learned model does not incur more regret than the observed behavior."
        },
        {
            "section number": "3.4",
            "key information": "The key innovation of this method is the introduction of the ICE polytope, which reformulates the problem of learning in multi-agent settings into a convex optimization problem, allowing for effective predictions based on limited observations."
        },
        {
            "section number": "4.1",
            "key information": "A limitation of the method is that it relies on the assumption of rationality among agents, which may not always hold in practice."
        },
        {
            "section number": "6.2",
            "key information": "Future research could explore relaxing the rationality assumptions or applying the ICE framework to a broader range of multi-agent scenarios to enhance its applicability."
        }
    ],
    "similarity_score": 0.5554872987705586,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0115_inter/papers/Computational Rationalization_ The Inverse Equilibrium Problem.json"
}