{
    "from": "google",
    "scholar_id": "PWJ328rt_j8J",
    "detail_id": null,
    "title": "Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and LLMs evaluations",
    "abstract": " Abstract\n\nThis paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a B enchmark suite for O ut-of-distribution robustne SS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pretrained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.\n\n# 1 Introduction\n\nPretrained language models (PLMs) have excelled in downstream tasks and gained widespread adoption [24, 60]. However, existing evaluation often assumes independent and identically distributed (i.i.d) condition [94, 92], which is often violated in real-world scenarios, highlighting the crucial problem of out-of-distribution (OOD) robustness in NLP models.",
    "bib_name": "yuan2023revisiting",
    "md_text": "# Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations\n\nLifan Yuan 1, Yangyi Chen 2, Ganqu Cui 1, Hongcheng Gao 3, Fangyuan Zou 4, Xingyi Cheng 4, Heng Ji 2, Zhiyuan Liu 1 \u2217, Maosong Sun 1 \u2217\n1 NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing 2 University of Illinois Urbana-Champaign 3 University of Chinese Academy of Sciences 4 Tencent lievanyuan173@gmail.com\n\n# Abstract\n\nThis paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a B enchmark suite for O ut-of-distribution robustne SS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pretrained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.\n\n# 1 Introduction\n\nPretrained language models (PLMs) have excelled in downstream tasks and gained widespread adoption [24, 60]. However, existing evaluation often assumes independent and identically distributed (i.i.d) condition [94, 92], which is often violated in real-world scenarios, highlighting the crucial problem of out-of-distribution (OOD) robustness in NLP models. In this paper, we first revisit the evaluation of PLMs through an examination of evaluation benchmarks. Thereafter, we delve into the ID-OOD performance correlation of fine-tuned models by adopting various model scales, training steps, available training samples, and tunable parameters. Finally, we conduct extensive evaluations of current robustness-enhanced methods and large language models (LLMs).\nDefinition. There exist multiple definitions of OOD in literature [2, 115], and we define distribution shifts considered in this paper from two perspectives. Firstly, [2] classifies distribution shifts into\"semantic shift\" and \"background shift\". Our use of \"out-of-distribution\" aligns with the concept of\n\n\u2217 Corresponding Author.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df04/df04e2e8-4925-4c0a-8d57-44b89ba253f0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bdd/3bdd3ee7-418f-47d5-bb4f-8d374cf79e2a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1) Collect candidate dataset pool 2) Select ID dataset (large & diverse)\n3.2) Measuring performance drop\nFigure 1: The comparison of previous work and our protocol on dataset selection of OOD benchm\n</div>\n\"background shift\", which involves changes in the domain or style of the text while preserving the semantic content. Secondly, [115] formally defines three types of distribution shifts: covariate shift, label shift, and concept shift. In our work, we mainly focus on the combination of covariate shift and concept shift. This indicates that the model needs to generalize well to different input features (a.k.a, covariate shift) and adapt to variations in the underlying concepts within the data (a.k.a, concept shift).\nBenchmark. Our study begins by surveying existing literature on OOD robustness in NLP (Table 8). We observe a lack of standardized OOD benchmark suites tailored for NLP since there is no single uniform set of datasets for evaluation, resulting in the adoption of heuristic and popularity-based dataset selection strategies in previous work [39, 109, 107]. This approach suffers from two main drawbacks: (1) The selected OOD datasets may come from similar distributions as the ID dataset, reducing the OOD evaluation to ID settings and thus hindering rigorous OOD robustness evaluation; (2) The challenge stemming from distribution shifts is limited, deviating from the expectation for difficulty posed by benchmark construction principles [8] and potentially leading to an overestimation of the OOD robustness of language models. As a result, current OOD robustness benchmarks may inadequately assess NLP models.\nTo address the aforementioned concerns, we establish a protocol as shown in Figure 1, consisting of three fundamental principles, for selecting both ID and OOD datasets: (1) ID datasets should be large and diverse for comprehensive knowledge; (2) Selection for OOD datasets should prioritize distinct distributions and dissimilarity, regarding text sources and semantics; (3) Challenging distribution shifts should be prioritized based on performance degradation, to ensure that the benchmark stands the test of time [8]. Based on the protocol, we compile BOSS, a more holistic and challenging NLP B enchmark suite for O OD robustne SS evaluation. Unlike existing benchmarks which only consider single task types such as classification [39, 107] or reading comprehension [109], BOSS covers a wider range of task formats, including sentiment analysis, toxic detection, and natural language inference for classification, name entity recognition for structured prediction, and extractive question answering for reading comprehension. We establish one ID and three corresponding OOD datasets for each task.\nAnalysis. We recognize the lack of analysis of models\u2019 learning behaviors regarding ID performance and OOD generalization in the field of NLP, hindering the development and understanding of OOD robustness. Thus, we investigate the correlation between the performance on ID and OOD datasets using the BOSS benchmark. To regulate the ID performance, we manipulate four related factors, i.e. model scale, training steps, available training samples, and tunable parameters. Three typical categories of ID-OOD correlation are observed, namely, monotonic linear positive correlation, monotonic piecewise linear positive correlation, and non-monotonic V-shaped correlation (see Figure 2). We discuss the potential reasons for the causes of these identified correlations in section 3.\nEvaluations. After examining the learning mechanism of PLMs in vanilla fine-tuning, we scrutinize their performance with existing robustness-enhanced methods and then proceed to prevalent LLMs. Due to the absence of a standard benchmark, previous evaluations of existing methods can be\n\nimprecise and thus misleading the estimations of progress in this field. Moreover, given the increasing focus on LLMs [9, 89] in NLP research, it is essential to evaluate their effectiveness in handling OOD challenges and explore the efficacy of different adaptation paradigms.\nFor robustness-enhanced methods, we evaluate five representative methods [99] on BOSS. Our main observation is that vanilla fine-tuning (a.k.a, empirical risk minimization) remains a strong baseline, while certain methods may slightly improve OOD performance in some cases. We further evaluate various LLMs and adaptation paradigms. We consider three recent prevailing LLMs, namely LLaMA [89], OpenAI text-davinci-003 [9], and OpenAI gpt-3.5-turbo. We include two relatively smaller models T0-3B [81] and T5-3B [77] for comparison. We apply zero-shot inference, in-context learning, few-shot fine-tuning, and full-data fine-tuning to one or multiple models. Through our experiments, we find that when provided with enough training data, fine-tuning domain-specific models remain the preferable choices for handling ID examples, while leveraging LLMs with in-context learning is superior for tackling OOD instances. In addition, we observe that the impact of in-context learning on generalization ability varies across models. We provide more detailed discussions in section 4.2.\n\n# 2 BOSS Benchmark\n\n# 2.1 Motivation\n\nNLP models should exhibit robustness across diverse distributions to ensure reliable applications. To achieve this, a standardized and recognized evaluation benchmark for OOD robustness is imperative. However, previous efforts in constructing benchmarks have predominantly relied on random selections and dataset popularity, lacking a systematic design [39, 109, 107]. Two deficiencies are thus identified: (1) Dataset similarity, as exemplified by the SST and IMDb datasets for sentiment analysis [83, 64], which share movie reviews and exhibit high semantic similarity (see Table 2). This blurs the line between ID and OOD evaluation, hindering rigorous assessment of OOD robustness; (2) Limited distribution shift challenges, exemplified by the high accuracy of a model trained on Amazon [65] when tested on IMDb (see Table 3). However, the significant performance drop on our considered Dynasent [75] suggests that OOD robustness still remains a critical problem in the sentiment analysis task. Thus, there is a need for universally applicable challenges across all dataset selections [8].\n\n# 2.2 Protocol to Construct OOD benchmark.\n\nWe aim to establish a standard benchmark for rigorous evaluation of OOD robustness in NLP. To address the above issues, we first survey and gather existing candidate datasets from Paperswithcode 2, Kaggle 3, and ACL Anthology 4 websites. We consider the release date and public availability of datasets. Then we carefully examine three criteria to determine the ID and corresponding OOD datasets. The first criterion focuses on the ID dataset selection, and the other two criteria are proposed for OOD datasets, targeting the two issues in previous work, respectively.\nThe ID dataset should provide sufficient knowledge for models to handle the task. ID dataset should encompass comprehensive task-level knowledge [44], enabling models to grasp the underlying rationale necessary for task completion. Alternatively, if the model exclusively learns biased features, it may struggle to adapt to other features during distribution shifts. To this end, it is necessary for the ID datasets to possess the following characteristics: (1) Sufficiently large size; (2) Diversity, which is achieved through collection from multiple sources or the inclusion of several subtypes (i.e., styles, topics, levels of formality, et al). Our intuition is in line with [87], which demonstrates that training on large and diverse datasets improves the robustness of vision models.\nDatasets within a given task should originate from diverse distributions for a holistic evaluation. We guarantee this through qualitative analysis of data source diversity and quantitative measurement of semantic similarity using SimCSE [31]. To avoid overlap, we select at most one dataset per text source. Additionally, we ensure OOD datasets in the benchmark exhibit relatively low semantic similarity, and thus enhancing distinctiveness.\n\nOOD shifts should be challenging to provide an accurate assessment of progress in OOD robustness [8]. To quantify the challenge, we train a model on the ID dataset and test it on all candidate datasets. Specifically, we tune a T5-large [77] with manual templates on four tasks, except for NER, on which we adopt DeBERTa-large [38] with conventional fine-tuning due to the lack of a standard prompt-based tuning schema for this task. For this reason, all experiments in this paper follow this choice of model selection. For each text source, we first discard candidates similar to the ID dataset in semantics. Then, to construct challenging distribution shifts, we prioritize the dataset provoking the most severe performance drop of the ID model and adopt it as the OOD dataset in our benchmark.\n\n# 2.3 Dataset Selection\n\nWe take sentiment analysis as a case to demonstrate how we select ID and OOD datasets for each task according to our protocol. The selection process for other tasks can be found in Appendix D.\nCandidate Datasets. We first collect all sentiment analysis datasets on Paperswithcode, Kaggle, and ACL Anthology as aforementioned. We filter out datasets released before the 2010s, as they are largely resolved with the advent of pre-trained language models [25]. As a result, seven datasets remain as candidates, i.e., Amazon [65], DSC [48], Dynasent [75], IMDb [64], SemEval [70], SST [83], and Yelp [116]. Considering the inconsistency in the number of categories across the datasets, we align them by converting them into a three-class classification setting. See Appendix C.2 for a detailed explanation of the dataset processing procedure.\n\nProbing Experiments. According to our protocol, dataset size and text sources are assessed for ID dataset selection. Subsequently, semantic similarity and ID model performance degradation guide OOD dataset selection. To this end, two probing experiments are conducted: (1) Comparing semantic similarity using SimCSE for candidate dataset pairs, and (2) Evaluating the performance of the selected ID model.\n\nDataset\nSource\n# Classes\n# Samples\nAvg. Length\nTrain\nTest\nTrain\nTest\nAmazon\nProduct\n3\n30,000\n38,905\n71.69\n54.84\nDSC\nProduct\n2\n92,244\n11,531\n132.29\n130.14\nDynasent\nAdversarial\n3\n93,553\n4,320\n13.19\n13.83\nIMDb\nMovie\n2\n25,000\n25,000\n233.79\n228.53\nSemEval\nTwitter\n3\n6,000\n20,622\n19.44\n19.62\nSST\nMovie\n3\n4,004\n1,067\n18.78\n18.75\nYelp\nProduct\n3\n30,000\n30,000\n132.49\n131.62\nperformance of the selected ID model. In the first experiment, for better semantic representation, we resort to the best SimCSE model provided by [31], a supervised RoBERTa-large [60]. We load the model checkpoint from Huggingface 5. For each dataset, we first encode each sample into a continuous embedding and then average the embeddings across the dataset to obtain a centroid representation of the dataset. Finally, we calculate the cosine similarity between a pair of centroids as the semantic similarity between two datasets. In the second experiment, we train a T5-large model on the selected ID dataset and evaluate its performance on all the candidate datasets.\n\nDataset Selection. The dataset information and semantic similarities are provided in Table 1 and Table 2, respectively. The text sources of the datasets vary from product reviews, movie reviews, Twitter, and adversarial texts. We observe that datasets originating from the same source tend to exhibit higher SimCSE scores, indicating higher seman\n\n<div style=\"text-align: center;\">Table 2: SimCSE scores between each pair of datasets regarding the sentiment analysis task.\n</div>\nTrain\nTest\nAmazon\nDSC\nDynasent\nIMDB\nSemEval\nSST\nYelp\nAmazon\n100\n86.02\n57.30\n36.67\n24.74\n33.70\n49.22\nDSC\n86.02\n100\n59.15\n54.55\n31.70\n44.40\n55.45\nDynasent\n57.30\n59.15\n100\n32.69\n28.17\n19.68\n88.99\nIMDb\n36.67\n54.55\n32.69\n100\n46.95\n84.62\n39.88\nSemEval\n24.74\n31.70\n28.17\n46.95\n100\n40.45\n24.03\nSST\n33.70\n44.40\n19.68\n84.62\n40.45\n100\n19.43\nYelp\n49.22\n55.45\n88.99\n39.88\n24.03\n19.43\n100\ntic similarity. It is worth noting that for IMDb and SST, the widely used ID-OOD dataset pair in sentiment analysis [39, 107], the SimCSE score demonstrates one of the highest levels among dataset pairs. This reinforces the first deficiency of previous benchmarks, where dataset pairs have similar semantics and unclear distribution shifts. Hence, in contrast to existing practices, our benchmark construction considers only one dataset from each source.\nFor the ID dataset selection, we first exclude DSC and IMDb since they are binary classification datasets, on which the trained model cannot tackle the unseen class neutral. For dataset size,\n\ntic similarity. It is worth noting that for IMDb and SST, the widely used ID-OOD dataset pair in sentiment analysis [39, 107], the SimCSE score demonstrates one of the highest levels among dataset pairs. This reinforces the first deficiency of previous benchmarks, where dataset pairs have similar semantics and unclear distribution shifts. Hence, in contrast to existing practices, our benchmark construction considers only one dataset from each source.\n\nSemEval and SST are disregarded due to their limited number of samples per class (less than 10k). Among the remaining datasets, Amazon is chosen as the ID dataset for sentiment analysis as it ncompasses reviews from 29 distinct product categories, offering greater diversity than Yelp.\n\nFor OOD datasets selection, we train a T5-large model on the ID dataset (i.e., Amazon) and evaluate it on all candidate datasets, as illustrated in Table 3. We include Dynasent and SemEval in the benchmark suite due to the\n\n<div style=\"text-align: center;\">Table 3: The OOD performance of the T5-large when trained on the Amazon dataset.\n</div>\nTrain\nTest\nAmazon\nDSC\nDynasent\nIMDb\nSemEval\nSST\nYelp\nAmazon\n90.94\n95.63\n47.38\n92.69\n49.90\n75.16\n89.25\nthe benchmark suite due to the following reasons: (1) They are the sole adversarial and Twitter datasets available, (2) They demonstrate low semantic similarity, and (3) They exhibit a notable performance degradation, making them crucial for evaluation. For movie reviews, SST is prioritized due to lower SimCSE scores compared to IMDb and larger performance drop of the ID model. Eventually, this yields three distinct and challenging distribution shifts in the sentiment analysis task: Amazon \u2192 (DynaSent, SemEval, SST).\n\n# 2.4 BOSS\n\nBased on the aforementioned protocol, we introduce BOSS, an NLP b enchmark suite for O OD robustne ss evaluation. BOSS comprises five essential NLP tasks: sentiment analysis (SA), toxic detection (TD), natural language inference (NLI), name entity recognition (NER), and ex\n\nTable 4: The datasets included in the BOSS benchmark. Corresponding abbreviations are shown in brackets.\n\nTask\nID Dataset\nOOD Datasets\nSA\nAmazon (AZ)\nDynasent (DS)\nSemEval (SE)\nSST (SST)\nTD\nCivil Comments (CC)\nAdvCivil (AC)\nImplicit Hate (IH)\nToxiGen (TG)\nNLI\nMNLI (MN)\nANLI (AN)\nContractNLI (CN)\nWANLI (WN)\nNER\nFewNerd (FN)\nCoNLL (CoNLL)\nE-NER (ENER)\nWNUT (WNUT)\nEQA\nSQuAD (SQuAD)\nAdvQA (AQA)\nNewsQA (NQA)\nSearchQA (QA)\ntractive question answering (EQA). These tasks represent diverse practical applications and provide comprehensive coverage for evaluating models\u2019 capabilities, from aspects of classification, structured prediction, and extraction. Each task in the benchmark includes one ID dataset and three associated OOD datasets (see Table 4).\n\nSentiment Analysis. Amazon [65] contains reviews of 29 different categories of products from the Amazon website. DynaSent [75] first identifies naturally challenging sentences from several existing datasets, and then creates adversarial sentences with a human-and-model-in-the-loop annotation approach. SemEval [70] is a three-class sentiment analysis dataset focusing on tweets. SST [83] consists of sentence-level movie reviews from the Rotten Tomatoes website.\nToxic Detection. Civil Comments [6] contains public comments on the Civil Comments platform, with users from diverse groups and various subtypes of toxic texts. AdvCivil, a new toxic dataset introduced in this paper, is generated from Civil Comments by textual adversarial attacks in an automated model-in-the-loop adversarial pipeline. Please refer to Appendix C.1 for details. Implicit Hate [29] contains toxic tweets in both explicit and implicit forms. The latter can circumvent keyword-based toxic detection systems. ToxiGen [36] is synthesized by GPT-3 [9], covering several types of subtly and implicitly toxic texts on 13 minority groups.\nNatural Language Inference. MNLI [102] provides ten different categories of written and verbal sentence pairs, with diverse styles, topics, and levels of formality. ANLI [73] is an adversarial dataset collected in a human-and-model-in-the-loop approach, where each premise mainly comes from Wikipedia and the hypothesis is generated by human adversaries. ContractNLI [49] considers each contract as a premise and holds a fixed set of hypotheses throughout the dataset. WANLI [59] is synthesized by GPT-3 [9], each example containing challenging patterns identified in MNLI.\nName Entity Recognition. Few-NERD [26], the arguably largest dataset for NER, labels about 188k Wikipedia sentences into eight coarse-grained entity types. CoNLL [88] takes stories from Reuters news, containing four basic entity types. E-NER [3] is based on legal text. We use the four-category version in this paper, which treats all legal entities as miscellaneous ones. WNUT [23] collects training data from Twitter and mines test data from Reddit, StackExchange, Twitter, and YouTube, containing six coarse-grained entity types in Few-NERD.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/923f/923f1aca-8036-41e2-b85c-a6347ff612ac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e Red\nBase-sized Training Steps\nLarge-sized Tunable Parameters\n</div>\nFigure 2: Three representative correlations between ID-OOD performance: (a) Type I (monotonic linear positive correlation) indicates consistent linear improvement of OOD performance with increasing ID performance. (b) Type II (monotonic piecewise linear positive correlation) exhibits accelerated OOD performance growth after a turning point. (c) Type III (non-monotonic V-shaped correlation) shows an initial negative correlation, followed by a positive correlation after a turning point. The r 2 value in Figure (a) is 0.9677, and the values of the left and right fits in Figure (b) are 0.9553 and 0.9396 whereas in Figure (c) are 0.7690 and 0.8124 respectively.\nExtractive Question Answering. SQuAD [78] constructs question-answer pairs based on Wikipedia passages. AdversarialQA [4] composes adversarial questions for contexts in SQuAD in a human-andmodel-in-the-loop procedure, similar to ANLI. NewsQA [90] writes questions for CNN news articles,\n\nFigure 2: Three representative correlations between ID-OOD performance: (a) Type I (monotonic linear positive correlation) indicates consistent linear improvement of OOD performance with increasing ID performance. (b) Type II (monotonic piecewise linear positive correlation) exhibits accelerated OOD performance growth after a turning point. (c) Type III (non-monotonic V-shaped correlation) shows an initial negative correlation, followed by a positive correlation after a turning point. The r 2 value in Figure (a) is 0.9677, and the values of the left and right fits in Figure (b) are 0.9553 and 0.9396 whereas in Figure (c) are 0.7690 and 0.8124 respectively.\n\nFigure 2: Three representative correlations between ID-OOD performance: (a) Type I (monotonic linear positive correlation) indicates consistent linear improvement of OOD performance with increasing ID performance. (b) Type II (monotonic piecewise linear positive correlation) exhibits accelerated OOD performance growth after a turning point. (c) Type III (non-monotonic V-shaped correlation) shows an initial negative correlation, followed by a positive correlation after a turning point. The r 2 value in Figure (a) is 0.9677, and the values of the left and right fits in Figure (b) are 0.9553 and 0.9396 whereas in Figure (c) are 0.7690 and 0.8124 respectively.\nExtractive Question Answering. SQuAD [78] constructs question-answer pairs based on Wikipedia passages. AdversarialQA [4] composes adversarial questions for contexts in SQuAD in a human-andmodel-in-the-loop procedure, similar to ANLI. NewsQA [90] writes questions for CNN news articles, each of which requires reasoning to answer, rather than relying solely on word overlap and textual entailment. SearchQA [28] adopts a reverse construction pipeline, employing the Google search engine to retrieve relevant contexts for each question-answering pair from the J!Archive website.\n\n0.9553 and 0.9396 whereas in Figure (c) are 0.7690 and 0.8124 respectively.\nExtractive Question Answering. SQuAD [78] constructs question-answer pairs based on Wikipedia passages. AdversarialQA [4] composes adversarial questions for contexts in SQuAD in a human-andmodel-in-the-loop procedure, similar to ANLI. NewsQA [90] writes questions for CNN news articles, each of which requires reasoning to answer, rather than relying solely on word overlap and textual entailment. SearchQA [28] adopts a reverse construction pipeline, employing the Google search engine to retrieve relevant contexts for each question-answering pair from the J!Archive website.\n\n# 3 Analysis of OOD Robustness\n\nDespite OOD robustness in NLP has been extensively studied [43], a potential concern pertains to the usage of nonstandard benchmarks, as discussed in Section 2, resulting in inaccurate conclusions. To address this issue, we conduct a series of empirical analyses and evaluations to gain in-depth insights into OOD robustness in NLP. Previous research primarily concentrates on method comparisons without delving into models\u2019 learning behaviors. Therefore, we first analyze the models\u2019 learning mechanism by assessing the correlation between ID and OOD performance.\nSetting. We assess the correlation between ID and OOD performance across various conditions. We manipulate the ID performance of models by varying their scale, training steps, available training samples, and tunable parameters. Further implementation details can be found in Appendix E.1.1.\nResults. We observe that the correlation between ID and OOD performance on datasets of the five tasks is inconsistent, but can be broadly categorized into three types (see Figure 2): monotonic linear positive correlation (Type I), monotonic piecewise linear positive correlation (Type II), and non-monotonic V-shaped correlation (Type III). We also identify an exceptional case in Figure 3, which does not fall into any of the three categories. The full results are shown in Figure 4.\nType I. This is the most prevalent type of correlation observed across all ID-OOD pairs for sentiment analysis, name entity recognition, and the majority for toxic detection. As shown in Figure 2a, in this category, OOD performance is positively and linearly correlated with ID performance, indicating that the task knowledge learned on source distribution can be effectively generalized to other distributions. This observation is consistent with results in the computer vision domain [68], which shows that OOD performance is linearly correlated with ID performance across various model architectures, hyperparameters, training dataset size, and training duration. However, the slope of the line fitted by the least square method is less steep than the y = x diagram, and it eventually lies below the diagonal, implying that the performance degradation of models under distribution shift will be escalated with the increase of the ID performance.\nType II. This category is observed on ID-OOD pairs for extractive question answering. As presented in Figure 2b, the results can be fitted into a polyline, indicating a piecewise linear correlation. The correlation between OOD performance and ID performance is positive and linear, with a notable differ\n\n<div style=\"text-align: center;\">Red\nLarge-sized Tunable Parameters\n</div>\nence in the slope before and after the turning point. Specifically, OOD performance demonstrates slow growth until the turning point, after which a minor increase in ID performance yields a substantial improvement in OOD performance. The observed trend may be attributed to the findings of [91], which indicates that models initially capture spurious correlations in ID datasets before acquiring comprehensive task knowledge. Consequently, models prioritize learning these spurious correlations to address the ID task, resulting in minimal improvements on OOD datasets. However, in the later stages of training, models progressively acquire greater task knowledge, leading to improved OOD performance.\nType III. The V-shaped fitted lines shown in Figure 2c mainly occurs on ID-OOD pairs of NLI tasks. This pattern is divided into two stages by a turning point in ID performance. In the first stage, OOD performance experiences worsening performance degradation during the distribution shift. However, in the second stage, the ID-OOD correlation becomes positive. This trend resembles the U-shaped scaling law of LLMs observed by [100], thus suggesting a shared explanation. [100] attributes this phenomenon to the \"distractor task\" in the dataset, apart from the \"true task\". Mediumcapacity models may perform better than low-capacity models on the distractor task, which may harm performance on the \"true task\". As the model capability increases, it can ignore the distractor task and focus on improving performance on the true task. Here, we identify the distractor task in NLI datasets as detecting the word overlap or other spurious correlations between the premise and hypothesis.\n\nOutlier.  There is an exceptional case regarding the distribution shift from Civil Comments to AdvCivil (see Figure 3). The figure depicts two distinct lines, both exhibiting a monotonic linear negative correlation. This may stem from the model\u2019s increased reliance on spurious correlations and the adversarial nature of AdvCivil. Prior research suggests that models can learn non-robust features, such as spurious correlations, to enhance ID accuracy [44]. However, adversarial samples preserve the semantics of the original texts while introducing perturbations that eliminate spurious correlations. Hence, when the ID model becomes more dependent on spurious correlations during training, its performance degradation on adversarial samples intensifies.\n\n# 4 Evaluation of OOD Robustness\n\n# .1 Robustness-enhanced Methods\n\nAfter analyzing the learning behavior of PLMs under vanilla fine-tuning, we examine their performance when trained with other methods. Although massive methods have been proposed to improve the robustness of PLMs, their evaluations rely on non-standard benchmarks, which may result in inaccurate evaluations and impede progress clarity. Therefore, in this section, we first conduct extensive experiments to re-evaluate the effectiveness of diverse robustness-enhanced methods.\nSetting. We consider the categories of robustness-enhanced methods summarized by [99]: datadriven, model and training-based, inductive-prior-based, and causal intervention methods. We select the most representative one from each category for evaluation. Specifically, we choose EDA [101] and FreeLB [118] for data-driven methods, label smoothing [84] and focal loss [58] for model and training-based methods, and model ensemble [16] for inductive-prior-based methods. We do not consider causal intervention methods since they are typically applied to low-resource scenarios. As explained in section 2.3, we apply the above methods to DeBERTa-base models for the NER task and to T5-base models for the other tasks.\nResults. The results are shown in Table 5, where the mark \u2018-\u2019 indicates that a certain method is not applicable to the task. We summarize the findings in the following takeaways:\nTakeaway 1: The vanilla fine-tuning (empirical risk minimization) remains a strong baseline. Despite existing methods outperforming vanilla fine-tuning on certain datasets like E-NER, WNUT, and NewsQA, they show limited superiority or can potentially harm model performance. Specifically, only FreeLB demonstrates beneficial effects over half of the datasets, standing out as the most\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d62/4d6289dc-2079-49fb-9da4-5d1f0aa9ff73.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">10 20 30 40 50 60 70 80 9\nAccuracy on Civil Comments (ID)\n</div>\nFigure 3: The OOD performance exhibits a negative correlation with ID performance. Refer to Figure 2 for legends.\n\n<div style=\"text-align: center;\">Table 5: The evaluation of robustness-enhanced methods. The results that surpass the vanilla baseline are underlined. We use abbreviations representative of the datasets to conserve space. Please refer to Table 4 for their corresponding full dataset names.\n</div>\nTask\nSA\nTD\nNLI\nNER\nEQA\nDataset\nAZ\nDS\nSE\nSST\nCC\nAC\nIH\nTG\nMN\nAN\nCN\nWN\nFN\nCoNLL\nENER\nWNUT\nSQuAD\nAQA\nNQA\nSQA\nVanilla\n90.94\n47.38\n49.90\n75.16\n87.15\n57.47\n63.77\n68.83\n89.40\n36.19\n37.06\n63.32\n79.89\n69.10\n48.01\n45.45\n93.14\n51.19\n63.77\n37.47\nEDA\n91.66\n46.39\n48.02\n75.82\n87.15\n57.47\n63.77\n68.83\n65.57\n34.50\n46.25\n46.40\n-\n-\n-\n-\n-\n-\n-\n-\nFreeLB\n91.39\n47.94\n47.88\n76.66\n85.63\n63.55\n62.22\n67.98\n89.83\n36.13\n40.94\n63.58\n80.08\n66.66\n50.84\n47.77\n93.51\n51.07\n65.03\n39.57\nFL\n91.02\n46.20\n50.11\n76.76\n87.10\n57.72\n62.29\n67.66\n89.17\n36.53\n39.26\n63.32\n79.30\n61.04\n50.49\n45.51\n92.97\n50.64\n63.96\n36.03\nLS\n90.19\n47.31\n46.35\n76.19\n86.65\n57.84\n62.66\n67.98\n89.68\n36.50\n39.36\n62.86\n79.66\n68.81\n48.04\n47.21\n93.32\n50.93\n63.97\n34.51\nES\n50.72\n41.83\n54.98\n63.36\n82.99\n47.02\n61.05\n65.32\n77.67\n35.16\n17.79\n17.79\n-\n-\n-\n-\n-\n-\n-\n-\nTable 6: Evaluations of LLMs on BOSS. Small Ref represents the results of supervised fine-tuned small models in Table 5 (Vanilla). We observe that given enough ID data, fine-tuning domain-specific models is predominant when testing on ID examples. In contrast, LLMs with In-context learning should be given priority on OOD instances.\n\nTask\nSA\nTD\nNLI\nNER\nEQA\nDataset\nAZ\nDS\nSE\nSST\nCC\nAC\nIH\nTG\nMN\nAN\nCN\nWN\nFN\nCoNLL\nENER\nWNUT\nSQuAD\nAQA\nNQA\nSQA\nSmall Ref\nFull-data\n90.94\n47.38\n49.90\n75.16\n88.63\n50.67\n62.29\n65.74\n89.40\n36.19\n37.06\n63.32\n79.89\n69.10\n48.01\n45.45\n93.14\n51.19\n63.77\n37.47\nT0-3B\n0-shot\n88.33\n43.80\n41.08\n58.76\n10.60\n80.92\n38.48\n44.15\n44.50\n35.00\n46.29\n39.82\n0\n0\n0\n0\n80.84\n41.89\n54.23\n39.58\n0-shot\n84.55\n33.63\n34.27\n37.68\n21.03\n75.94\n40.72\n44.04\n35.44\n33.53\n46.29\n37.16\n0\n0\n0\n0\n46.64\n18.88\n21.37\n13.42\nICL\n84.55\n33.63\n34.27\n37.68\n21.03\n75.94\n40.72\n44.04\n35.44\n33.53\n46.29\n37.16\n0\n0\n0\n0\n35.50\n17.16\n9.13\n7.75\n5-shot\n66.73\n42.73\n44.12\n63.92\n66.08\n43.01\n57.58\n54.47\n33.19\n34.94\n11.24\n47.60\n0\n0\n0\n0\n57.67\n23.74\n26.02\n15.10\nT5-3B\nFull-data\n90.63\n51.11\n50.93\n74.13\n86.32\n60.75\n63.12\n70.64\n90.64\n45.97\n44.57\n65.16\n46.80\n48.28\n56.63\n34.97\n94.38\n57.48\n66.40\n34.75\n0-shot\n75.66\n54.05\n37.60\n46.43\n67.72\n43.70\n57.33\n59.98\n32.81\n26.83\n68.18\n44.44\n0.49\n0.38\n0.07\n0\n58.98\n30.22\n40.78\n45.80\nICL\n84.30\n55.19\n42.66\n59.14\n89.70\n20.17\n63.62\n59.68\n39.81\n33.50\n19.30\n38.17\n0.63\n0.70\n0.81\n0.16\n67.57\n37.35\n44.15\n43.78\nLLaMA-7B\nICL\u2217\n-\n52.26\n47.25\n53.80\n-\n-\n-\n60.74\n-\n33.47\n-\n37.98\n-\n0.21\n-\n0\n-\n37.09\n48.32\n-\n0-shot\n81.35\n56.48\n42.73\n59.59\n89.87\n19.60\n62.65\n58.80\n32.07\n24.43\n47.06\n38.14\n0.16\n1.00\n0.47\n0.00\n66.90\n37.34\n45.15\n55.60\nICL\n82.72\n54.71\n40.63\n57.69\n83.91\n38.88\n66.96\n67.87\n38.58\n34.28\n20.42\n38.26\n0.37\n1.11\n0.85\n0.00\n69.71\n41.90\n45.32\n58.67\nLLaMA-13B\nICL\u2217\n-\n46.56\n38.46\n53.05\n-\n-\n-\n68.09\n-\n36.00\n-\n37.31\n-\n0.50\n-\n0.00\n-\n40.99\n49.30\n-\n0-shot\n82.92\n70.09\n66.86\n77.13\n74.20\n69.14\n60.56\n69.04\n69.74\n48.44\n37.40\n52.24\n38.96\n47.03\n35.52\n32.54\n83.68\n55.50\n54.78\n63.74\nICL\n84.60\n74.88\n68.88\n77.23\n78.18\n65.98\n61.96\n77.00\n71.21\n50.88\n55.33\n53.32\n48.56\n57.08\n46.27\n42.10\n82.33\n57.58\n54.79\n69.55\nDavinci3\nICL\u2217\n-\n75.74\n65.14\n73.95\n-\n-\n-\n75.29\n-\n49.91\n-\n53.06\n-\n55.83\n-\n40.71\n-\n57.90\n56.15\n-\n0-shot\n85.63\n74.46\n68.33\n77.04\n76.10\n80.57\n52.69\n72.76\n68.78\n46.84\n50.82\n48.95\n36.53\n51.36\n30.29\n33.35\n81.21\n50.49\n47.68\n64.38\nICL\n87.75\n77.18\n65.26\n75.07\n60.16\n80.56\n54.67\n76.07\n68.12\n49.47\n49.59\n49.47\n40.21\n57.02\n38.77\n35.25\n82.82\n50.78\n50.37\n64.10\nTurbo\nICL\u2217\n-\n79.10\n66.79\n74.95\n-\n-\n-\n73.90\n-\n49.72\n-\n49.46\n-\n56.12\n-\n36.38\n-\n54.29\n56.63\n-\neffective approach. Conversely, the inductive-prior-based ensemble is the worst, consistently leading to performance degradation except for the SemEval dataset.\nTakeaway 2: Method effectiveness is consistent, yet limited to specific datasets. Across multiple datasets, various methods consistently demonstrate (in)effectiveness, as indicated in Table 5. However, no method consistently performs well on all datasets within the same task.\nTo summarize, current approaches fall short of meeting the expectations in enhancing the OOD robustness of models, highlighting the urgent demand for more advanced improvement techniques.\n\n# 4.2 Large Language Models\n\nLLMs are receiving increasing attention from NLP researchers. Considering the impressive zero/fewshot ability of LLMs, and the large difference in their fine-tuning and in-context learning paradigms, it is also intriguing to benchmark their generalizability on various downstream tasks and explore the best paradigm to leverage their power.\nSetting. We consider three prominent state-of-the-art LLMs, LLaMA-7B and LLaMA-13B (i.e., LLaMA-series) [89] ,OpenAI text-davinci-003 [9] and gpt-3.5-turbo (denoted as Davinci3 and Turbo respectively). For comparison, we include two relatively smaller (yet still large) models, T0-3B [81] and T5-3B. We perform zero-shot inference based on task instructions on all these models since this paradigm is the most general. Then, we adopt other paradigms in a model-specific way. For T5-3B, we include 5-shot and full-data fine-tuning, and we also select examplars from the ID dataset for in-context learning. For the other three LLMs, we apply in-context learning with two kinds of contexts, one from the ID dataset and another from the original training split of the evaluated OOD dataset, denoted as ICL and ICL \u2217 respectively. The implementation details are in Appendix E.2.\nResults. We present the results in Table 6, where the mark \u2018-\u2019 means that ICL \u2217 paradigm is not applicable for those datasets due to the absence of a training split or the limit of context window size. Our findings can be summarized as follows:\nTakeaway 1: Fine-tuning small domain-specific models is superior when enough training data is available, while LLMs may be favored in low-resource scenarios.  To be specific, supervised finetuned small models and T5-3B with the entire dataset consistently achieves the best performance on the ID dataset, especially for the structure prediction task (e.g., NER). In contrast, LLMs exhibit\n\nbetter performance on most OOD datasets. This observation reinforces the view that large pre-trained models possess strong generalization capabilities, whereas, with sufficient training data, an accurate estimate of data distribution can be achieved even without a large number of parameters [76].\nTakeaway 2: In-context learning always brings no gains to the generalization ability of small models, while it generally helps Turbo and significantly improves LLaMA-series and Davinci3. For small models like T5-3B, the performance of in-context learning is the same with or even worse than the zero-shot inference. For Turbo, providing ID examples for in-context learning presents advantages on nearly two-thirds of the datasets, with the NER task benefiting the most. For LLaMA-series and Davinci3, the superiority of in-context learning is prominent as it enhances performances on most of the datasets.\nTakeaway 3: Examples from ID datasets are generally more effective for in-context learning than those from the original training split of the testing OOD dataset. Specifically, when considering samples from our OOD datasets as contexts, the performance of Turbo is comparable to using ID samples, whereas the LLaMA-series and Davinci3 models consistently exhibit inferior performance compared to using ID examples as contexts. However, all models show improved performance on the EQA task when contexts from our OOD datasets are utilized. This may be attributed to the variations in sample length or question styles across EQA datasets, hence models acquire more precise instructions from the original training samples. The overall ineffectiveness of ICL \u2217 can be explained by the findings of [69]. According to [69], in-context demonstrations aim to guide the models to learn the target label space, instead of the feature-label mapping. The ID examples contain more diverse information due to the construction process of our benchmark. Thus, ID examples can better prompt the language models to locate the target label space, compared to the OOD examples that may target a specific domain.\nDiscussion. Two paradigms are prevalent in developing downstream NLP systems: leveraging general-purpose LLMs or gathering domain-specific data for fine-tuning smaller models. For the first paradigm, the overarching objective of general-purpose LLM development is to employ a single model for solving various downstream tasks [9]. Consequently, LLMs are anticipated to exhibit high performance on both ID and OOD datasets. However, our study exposes the shortcomings of LLMs on ID datasets when compared to fine-tuned domain-specific models. Considering the higher inference and deployment costs associated with LLMs, substantial progress is still needed to effectively improve LLMs in developing downstream applications, particularly for challenging tasks like EQA. For the second paradigm, our study reveals the limitations of fine-tuning models on ID datasets for OOD performance in comparison to LLMs. Thus, further research is required to develop advanced techniques that enhance the robustness of fine-tuned domain-specific models. Overall, the existing two prevalent paradigms still fall short in addressing the OOD problem in NLP, necessitating further advancements and effective approaches.\nHowever, we also note that there can exist confounders in our evaluations. It is still ambiguous which datasets are indeed OOD to LLMs, given that LLMs have been pre-trained on massive public corpora. The potential data contamination issue can result in overinflated performance on our OOD datasets, tangling the credit of the memory and generalizability of LLMs. The only confirmed distribution shift for LLMs is the temporal shift, necessitating the evaluation based on data released subsequent to their pre-training data collection cut-off. Therefore, the NLP community demands new downstream datasets independent of the pre-training corpus to meet the evaluation requirements for LLMs.\n\n# 5 Related Work\n\nDistribution shifts in NLP  has been widely studied in various forms. We examine several representative cases as outlined below. Domain shift refers to the challenge of testing data originating from diverse domains, often due to data collection from various sources [63, 40, 53, 79]. Temporal shift examines the degradation of models\u2019 performance over time [42, 1]. Spurious correlation examines the issue of models acquiring dataset-specific knowledge on ID data, which may not generalize effectively to OOD data [66, 73, 91, 32, 37, 16, 17]. Additionally, a requirement is for models to exhibit robustness when confronted with artificially constructed OOD samples. One typical type is malicious adversarial attacks, which involve assessing the resilience of models against inputs crafted by malevolent adversaries [56, 51, 112]. These inputs, distinct from ID samples, have the potential to induce model failures [12]. Adversarial attacks can also be effectively utilized to simulate\n\ndiverse user inputs to examine models\u2019 robustness in the real world [98, 13, 33]. Another category is backdoor attacks, characterized by intentionally introduced spurious correlations that can be exploited by attackers for their advantage [18, 55].\nOOD Evaluation in NLP can be broadly classified into automatic and static evaluation approaches. Automatic evaluation utilizes diverse textual transformation techniques, such as introducing typos, to conduct a rigorous evaluation of OOD robustness. Three essential elements in the automatic OOD evaluation encompass the establishment of suitable transformation methods, evaluation metrics, and effective techniques to ensure sample validity [34, 98]. Static evaluation, in contrast to automated methods, offers the advantage of constructing benchmarks with higher quality, resulting in an improved estimation of OOD robustness. Numerous OOD benchmarks have been introduced, focusing on adversarial attacks [96] or spurious correlations [117, 66]. A relevant study to ours is GLUE-X [107], which establishes an OOD benchmark derived from the GLUE benchmark [93]. Nevertheless, they do not establish a coherent benchmark construction protocol and primarily rely on dataset selection driven by popularity, incorporating datasets into the benchmark without comprehensive explanation and seemingly opting for a somewhat arbitrary selection, thus lacking a systematic approach.\n\n# 6 Conclusion\n\nWe revisit OOD robustness research in NLP, identifying deficiencies in benchmarks and evaluation. Correspondingly, a benchmark construction protocol and an OOD robustness evaluation suite are proposed to facilitate future research. The correlation between OOD and ID performance, the effectiveness of existing methods, and the challenges faced by LLMs are investigated.\n\n# Limitation\n\nWe identify two limitations in this work. First, as discussed in section 4.2, due to the lack of new datasets in the community, there is a possibility that some datasets have been included in the pretraining corpus of LLMs, so they may not be suitable to test the generalizability of recent LLMs. However, we note that with our benchmark construction protocol, we can easily update the benchmark as new datasets come out. Second, we only consider five tasks in this benchmark, which is not a comprehensive collection of current NLP literature. We explain the reason for the current task selection in Appendix A.1.\n\n# Acknowledgement\n\nThis work is sponsored by the Tsinghua-Toyota Joint Research Fund.\nLifan Yuan and Yangyi Chen initiated the project. Lifan Yuan, Yangyi Chen, and Ganqu Cui designed the experiments. Lifan Yuan, Yangyi Chen, and Hongcheng Gao constructed the AdvCivil dataset. Lifan Yuan conducted experiments and wrote the paper. Yangyi Chen and Ganqu Cui revised the paper. Everyone participated in the discussion. Heng Ji, Zhiyuan Liu, and Maosong Sun advised the project.\n\n# References\n\n[1] Oshin Agarwal and Ani Nenkova. Temporal effects on pre-trained models for language processing tasks. arXiv preprint arXiv:2111.12790, 2021.\n[2] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them. In Proceedings of EMNLP, 2021.\n[3]  Ting Wai Terence Au, Vasileios Lampos, and Ingemar Cox. E-NER \u2014 an annotated named entity recognition corpus of legal text. In Proceedings of the Natural Legal Language Processing Workshop, 2022.\n[4] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the AI: Investigating adversarial human annotation for reading comprehension. Transactions of ACL, 2020.\n\n[5] Mohaddeseh Bastan, Mihai Surdeanu, and Niranjan Balasubramanian. Bionli: Generating a biomedical nli dataset using lexico-semantic constraints for adversarial examples. In Findings of EMNLP, 2022.\n[6] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. CoRR, 2019.\n[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of EMNLP, 2015.\n[8] Samuel R. Bowman and George Dahl. What will it take to fix benchmarking in natural language understanding? In Proceedings of NAACL, 2021.\n[9]  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Preceedings of NeurIPS, 2020.\n[10]  Mohit Chandra, Ashwin Pathak, Eesha Dutta, Paryul Jain, Manish Gupta, Manish Shrivastava, and Ponnurangam Kumaraguru. AbuseAnalyzer: Abuse detection, severity and target prediction for gab posts. In Proceedings of COLING, 2020.\n[11]  Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi Yang. HiddenCut: Simple data augmentation for natural language understanding with better generalizability. In Proceedings of ACL-IJCNLP, 2021.\n[12] Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, and Maosong Sun. Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial nlp. In Proceedings of EMNLP, 2022.\n[13] Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning Shi, Bo Yuan, Longtao Huang, Hui Xue, et al. From adversarial arms race to model-centric evaluation: Motivating a unified automatic robustness evaluation framework. In Findings of ACL, 2023.\n[14] Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the calibration of pre-trained language models. In Proceedings of ACL, 2023.\n[15]  Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, and Jianfeng Gao. Posterior differential regularization with f-divergence for improving model robustness. In Proceedings of NAACL:HLT, 2021.\n[16] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. In  Proceedings of EMNLPIJCNLP, 2019.\n[17] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Learning to model and ignore dataset bias with mixed capacity ensembles. arXiv preprint arXiv:2011.03856, 2020.\n[18] Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun. A unified evaluation of textual backdoor learning: Frameworks and benchmarks. In Proceedings of NeurIPS, 2022.\n[19]  Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca Passonneau, and Rui Zhang. CONTaiNER: Few-shot named entity recognition via contrastive learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022.\n[20] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In Proceedings of AAAI on Web and Social Media, 2017.\n\n[21] Ona de Gibert, Naiara Perez, Aitor Garc\u00eda-Pablos, and Montse Cuadros. Hate speech dataset from a white supremacy forum. In Proceedings of Workshop on Abusive Language Online (ALW2), 2018.\n[22] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In Proceedings of Sinn und Bedeutung, 2019.\n[23] Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of Workshop on Noisy User-generated Text, 2017.\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\n[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019.\n[26] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of ACL-IJCNLP, 2021.\n[27] Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of NLU models. In Proceedings of NAACL:HLT, 2021.\n[28] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR, 2017.\n[29] Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. Latent hatred: A benchmark for understanding implicit hate speech. In Proceedings of EMNLP, 2021.\n[30] Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP, 2019.\n[31] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of EMNLP, 2021.\n[32] Matt Gardner, William Merrill, Jesse Dodge, Matthew E Peters, Alexis Ross, Sameer Singh, and Noah Smith. Competency problems: On finding and removing artifacts in language data. arXiv preprint arXiv:2104.08646, 2021.\n[33] Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher R\u00e9. Robustness gym: Unifying the nlp evaluation landscape. arXiv preprint arXiv:2101.04840, 2021.\n[34] Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, and Christopher R\u00e9. Robustness gym: Unifying the NLP evaluation landscape. In Avi Sil and Xi Victoria Lin, editors, Proceedings of NAACL-HLT, 2021.\n[35] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era of gpt-3. CoRR, 2022.\n[36] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of ACL, 2022.\n[37] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. arXiv preprint arXiv:1908.10763, 2019.\n[38] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In Proceedings of ICLR, 2021.\n\n[39] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In Proceedings of ACL, 2020.\n[40] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100, 2020.\n[41] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. 2019.\n[42] Xiaolei Huang and Michael J Paul. Examining temporality in document classification. In Proceedings of ACL, 2018.\n[43] Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, and Zhijing Jin. State-of-the-art generalisation research in NLP: a taxonomy and review. CoRR, 2022.\n[44] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Proceedings of NeurIPS, 2019.\n[45] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of ACL, 2017.\n[46] Divyansh Kaushik, Eduard Hovy, and Zachary C. Lipton. Learning the difference that makes a difference with counterfactually-augmented data. In Proceedings of ICLR, 2020.\n[47] Pride Kavumba, Ryo Takahashi, and Yusuke Oda. Are prompt-based models clueless? In Proceedings of ACL, 2022.\n[48] Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu. Continual learning with knowledge transfer for sentiment classification. In Proceedings of ECML-PKDD, 2020.\n[49] Yuta Koreeda and Christopher Manning. ContractNLI: A dataset for document-level natural language inference for contracts. In Findings of EMNLP, 2021.\n[50] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of ACL, 2019.\n[51] Thai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, and Dongwon Lee. Perturbations in the wild: Leveraging human-written text perturbations for realistic adversarial attack and defense. arXiv preprint arXiv:2203.10346, 2022.\n[52] Seanie Lee, Minki Kang, Juho Lee, and Sung Ju Hwang. Learning to perturb word embeddings for out-of-distribution QA. In Proceedings of ACL-IJCNLP, 2021.\n[53] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n[54] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of EMNLP, 2021.\n[55] Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, and Chaowei Xiao. Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger. CoRR, 2023.\n\n[56] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. arXiv preprint arXiv:1812.05271, 2018.\n[57] Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compression to improving generalization. In Proceedings of ACL-IJCNLP, 2021.\n[58] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. 2017.\n[59] Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. Wanli: Worker and ai collaboration for natural language inference dataset creation. In Findings of EMNLP, 2022.\n[60] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. CoRR, 2019.\n[61] Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. Crossner: Evaluating cross-domain named entity recognition. In Proceedings of AAAI, 2021.\n[62] Florian Ludwig, Klara Dolos, Torsten Zesch, and Eleanor Hobley. Improving generalization of hate speech detection systems to novel target groups via domain adaptation. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH), 2022.\n[63] Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. Domain adaptation with bert-based domain classification and data selection. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), 2019.\n[64]  Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of NAACL, 2011.\n[65] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of ACM Conference on Recommender Systems, 2013.\n[66] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.\n[67] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift on question answering models. In Proceedings of ICML, 2020.\n[68] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Proceedings of ICML, 2021.\n[69] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of EMNLP, 2022.\n[70] Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin Stoyanov. SemEval-2016 task 4: Sentiment analysis in Twitter. In Proceedings of International Workshop on Semantic Evaluation (SemEval), 2016.\n[71] Isar Nejadgholi and Svetlana Kiritchenko. On cross-dataset generalization in automatic detection of online abuse. In Proceedings of the Fourth Workshop on Online Abuse and Harms, 2020.\n[72] Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi. SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness. In Proceedings of EMNLP, 2020.\n\n[73] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of ACL, 2020.\n[74] Bhargavi Paranjape, Pradeep Dasigi, Vivek Srikumar, Luke Zettlemoyer, and Hannaneh Hajishirzi. Agro: Adversarial discovery of error-prone groups for robust optimization. 2022.\n[75] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. DynaSent: A dynamic benchmark for sentiment analysis. In Proceedings of ACL-IJCNLP, 2021.\n[76] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of ICML, 2021.\n[77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020.\n[78] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP, 2016.\n[79] Alan Ramponi and Barbara Plank. Neural unsupervised domain adaptation in nlp\u2014a survey. arXiv preprint arXiv:2006.00632, 2020.\n[80]  Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of EMNLP, 2019.\n[81] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2022.\n[82] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. In Proceedings of AAAI, 2022.\n[83] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, 2013.\n[84]  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of CVPR, 2016.\n[85] Aarne Talman and Stergios Chatzikyriakidis. Testing the generalization power of neural network models across NLI benchmarks. In Proceedings of ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019.\n[86] Alon Talmor and Jonathan Berant. MultiQA: An empirical investigation of generalization and transfer in reading comprehension. In Proceedings of ACL, 2019.\n[87] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In Proceedings of NeurIPS, 2020.\n[88] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of NAACL, 2003.\n[89]  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv, 2023.\n\n[90] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of Workshop on Representation Learning for NLP, 2017.\n[91] Lifu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious correlations using pre-trained language models. Transactions of the Association for Computational Linguistics, 2020.\n[92] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Proceedings of NeurIPS, 2019.\n[93] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n[94] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of ICLR, 2019.\n[95] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. Infobert: Improving robustness of language models from an information theoretic perspective. 2021.\n[96] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840, 2021.\n[97] Tianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi Wang. Identifying and mitigating spurious correlations for improving robustness in nlp models. In Findings of NAACL:HLT, 2022.\n[98] Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, and Xuanjing Huang. TextFlint: Unified multilingual robustness evaluation toolkit for natural language processing. In Proc. of ACL, 2021.\n[99] Xuezhi Wang, Haohan Wang, and Diyi Yang. Measure and improve robustness in NLP models: A survey. In Proceedings of NAACL, 2022.\n100] Jason Wei, Yi Tay, and Quoc V. Le. Inverse scaling can become u-shaped. ArXiv, 2022.\n101] Jason Wei and Kai Zou. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of EMNLP-IJCNLP, 2019.\n102] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of NAACL, 2018.\n103] Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In  Proceedings of ACLIJCNLP, 2021.\n104] Xing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han, Zhongyuan Wang, and Songlin Hu. Esimcse: Enhanced sample building method for contrastive learning of unsupervised sentence embedding. In Proceedings of COLING, 2022.\n105] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Rui Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. In Proceedings of ICLR, 2022.\n106] Linyi Yang, Lifan Yuan, Leyang Cui, Wenyang Gao, and Yue Zhang. Factmix: Using a few labeled in-domain examples to generalize to cross-domain named entity recognition. In Proceedings of COLING, 2022.\n\n[107] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. In Findings of ACL, 2023.\n[108] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of EMNLP, 2018.\n[109] Hai Ye, Yuyang Ding, Juntao Li, and Hwee Tou Ng. Robust question answering against distribution shifts with test-time adaption: An empirical study. In Findings of EMNLP, 2022.\n[110] Wenpeng Yin, Dragomir Radev, and Caiming Xiong. Docnli: A large-scale dataset for document-level natural language inference. In Findings of ACL, 2021.\n[111] Dani Yogatama, Cyprien de Masson d\u2019Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. Learning and evaluating general linguistic intelligence, 2019.\n[112] Lifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei. Bridge the gap between CV and nlp! A gradient-based textual adversarial attack framework. In Findings of ACL, 2023.\n[113] Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. Predicting the type and target of offensive posts in social media. In Proceedings of NAACL, 2019.\n[114] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of ACL, 2019.\n[115] Aston Zhang, Zachary Chase Lipton, Mu Li, and Alex Smola. Dive into deep learning. CoRR, 2021.\n[116] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Proceedings of NeurIPS, 2015.\n[117] Yuan Zhang, Jason Baldridge, and Luheng He. Paws: Paraphrase adversaries from word scrambling. arXiv preprint arXiv:1904.01130, 2019.\n[118] Chen Zhu, Yu Cheng, Zhe Gan, S. Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In Proceedings of ICLR, 2019.\n\nunchanged.\nMethod\nSimCSE\nESimCSE\nSentence-Transformer\nSrc | Tgt\nAZ\nDSC\nDS\nIMDb\nSE\nSST\nYelp\nAZ\nDSC\nDS\nIMDb\nSE\nSST\nYelp\nAZ\nDSC\nDS\nIMDb\nSE\nSST\nYelp\nAZ\n100\n86.02\n57.30\n36.67\n24.74\n33.70\n49.22\n100\n95.27\n88.46\n76.07\n76.50\n76.19\n81.75\n100\n90.25\n46.57\n48.62\n30.65\n48.92\n49.23\nDSC\n86.02\n100.00\n59.15\n54.55\n31.70\n44.40\n55.45\n95.27\n100\n84.22\n80.06\n76.68\n80.97\n79.26\n90.25\n100\n37.27\n61.54\n25.28\n54.79\n49.49\nDS\n57.30\n59.15\n100\n32.69\n28.17\n19.68\n88.99\n88.46\n84.22\n100\n70.64\n79.28\n74.41\n90.52\n46.57\n37.27\n100\n24.76\n36.94\n32.65\n78.37\nIMDb\n36.67\n54.55\n32.69\n100\n46.95\n84.62\n39.88\n76.07\n80.06\n70.64\n100\n72.16\n83.73\n72.82\n48.62\n61.54\n24.76\n100\n25.22\n84.26\n34.05\nSE\n24.74\n31.70\n28.17\n46.95\n100\n40.45\n24.03\n76.50\n76.68\n79.28\n72.16\n100\n76.75\n68.64\n30.65\n25.28\n36.94\n25.22\n100\n40.66\n19.26\nSST\n33.70\n44.40\n19.68\n84.62\n40.45\n100\n19.43\n76.19\n80.97\n74.41\n83.73\n76.75\n100\n62.44\n48.92\n54.79\n32.65\n84.26\n40.66\n100\n24.58\nYelp\n49.22\n55.45\n88.99\n39.88\n24.03\n19.43\n100\n81.75\n79.26\n90.52\n72.82\n68.64\n62.44\n100\n49.23\n49.49\n78.37\n34.05\n19.26\n24.58\n100\n# Appendix\n\n# A Frequently Asked Questions\n\n# A.1 What is the rationale for current task selection and why not include more difficult tasks?\n\nThe chosen tasks within BOSS evaluate models across natural language understanding, structured data prediction, and question answering, which are core language model competencies. In constructing our benchmark, we did consider extending other tasks such as NLG and commonsense reasoning, but in practice, we found a lot of difficulties.\nFor NLG tasks, the primary concern lies in evaluation. Since different datasets exhibit distinct text styles, an in-domain (ID) model might generate responses with styles diverging from the reference answers when tested on out-of-domain (OOD) datasets. However, current NLG metrics evaluate predictions based on their resemblance to reference answers, which might not accurately reflect text quality in scenarios with a vast output space [35]. An emerging alternative involves employing Language Models like GPT-4 for scoring predictions. However, this method is cost-intensive and lacks reproducibility due to unpredictable updates. Considering these evaluation challenges, including generation tasks within this OOD benchmark might not be appropriate.\nFor commonsense reasoning, multiple datasets exist from various sources, each demanding distinct knowledge. These differences are substantial enough that knowledge gained from an ID dataset might not be applicable to OOD datasets. For instance, HellaSwag [114] necessitates basic world knowledge and logical reasoning to complete a sentence, while StepGame [82] relies on spatial imagination without requiring world knowledge. The dissimilarity in required abilities suggests that models should acquire knowledge through pre-training rather than fine-tuning on an ID dataset and then transferring it to OOD tasks. This approach aligns more reasonably with the task\u2019s nature.\n\n# A.2 Why is SimCSE chosen to measure distribution sh\n\nWe refer to [105] as a representative study measuring text distribution shift via semantic vector distance. While [105] employs CLIP\u2019s encoder for multimodal contexts, our NLP-specific context steers us towards SimCSE for semantic representation.\n\ndistance. While [105] employs CLIP\u2019s encoder for multimodal contexts, our NLP-specific context steers us towards SimCSE for semantic representation.\nTo the best of our knowledge, SimCSE and sentence-BERT [80] are the most widely used models and SimCSE is more advanced according to the SimCSE paper, hence we choose SimCSE as the semantic representation model. In this discussion period, we also try the other variants of SimCSE, ESimCSE [104], and the best model in the Sentence Transformers leaderboard 6 (\u2018all-mpnet-base-v2\u2019). The results are shown in Table 7. From the results, despite different methods varying to some extent, the trend of similarity almost remains unchanged. Therefore, we consider that the essential is to adopt an advanced semantic representation model, and that which particular model we choose will not lead to differences in our selection.\n\n# B Survey\n\nOur survey draws from two NLP taxonomy papers, one focusing on robustness [99] and the on generalization [43]. We meticulously examine all the references cited in these papers, pres\n\nTable 8: Survey of papers targeting to address OOD robustness across the five tasks as this paper. Column \u201cPaper\u201d signifies individual papers sorted by date of publication, column \u201cTasks\u201d details the evaluated tasks, and column \u201cDatasets\u201d lists the encompassed datasets, with each row representing ID dataset \u2192 OOD datasets.\n\nPaper\nTasks\nDatasets\n[46]\nSA\nIMDb \u2192Amazon, SemEval, Yelp\n[85]\nNLI\nMNLI, SNLI, SICK \u2192MNLI, SNLI, SICK\n[30]\nEQA\nSQuAD, NewsQA, TtiviaQA, SearchQA, HotpotQA, Natural Questions \u2192BioASQ, DROP, DuoRC, RACE, RelationExtraction, TextbookQA\n[86]\nEQA\nSQuAD, NewsQA, SearchQA, TriviaQA, HotpotQA \u2192CQ, CWQ, ComQA, WikiHop, DROP\n[111]\nEQA\nSQuAD \u2192TriviaQA, QuAC, QA-SRL, QA-ZRE\n[40]\nSA\nSST2, IMDb, Yelp, Amazon \u2192SST2, IMDb, Yelp, Amazon\n[72]\nSA\nAmazon, Yelp, IMDb, SST2 \u2192Amazon, Yelp, IMDb, SST2\nNLI\nMNLI, ANLI \u2192MNLI, ANLI\n[71]\nTD\nWiki \u2192Founta, Waseem\n[91]\nNLI\nMNLI \u2192HANS\n[95]\nNLI\nMNLI, SNLI, FEVER \u2192ANLI\n[67]\nEQA\nSQuAD \u2192Squadshift\n[103]\nSA\nSST-2 \u2192Senti140, SemEval, Amzbook, Yelp, IMDb, IMDb-Cont., IMDb-CAD\nNLI\nSNLI \u2192MNLI, SNLI-CAD, break, DNC, stress, diagnostic\n[11]\nSA\nSST2$ \u2192IMDb-Cont, IMDb-CAD\nNLI\nMNLI \u2192ANLI, HANS\n[15]\nSA\nSST2 \u2192IMDB\nNLI\nMNLI \u2192ANLI\nEQA\nSQuAD \u2192Adv SQuAD\n[27]\nNLI\nFEVER, MNLI \u2192Symmetric, HANS\n[57]\nNLI\nMNLI \u2192SNLI, SciTail\n[52]\nEQA\nSQuAD \u2192BioASQ, New Wikipedia, New York Times, Reddit posts, Amazon Reviews\n[97]\nSA\nSST2, Amazon kitchen, Amazon electronics \u2192SST, Amazon kitchen, Amazon electronics\n[74]\nSA\nSST2 \u2192Senti140, SemEval, Yelp, IMDB, Contrast, CAD\nNLI\nMNLI \u2192PI, LI, HANS, WaNLI, SNLI, ANLI\n[107]\nSA\nSST \u2192IMDB, Yelp, Amazon\nNLI\nMNLI \u2192MNLImm, SNLI, SICK\n[62]\nTD\nHateXplain\n[47]\nNLI\nMNLI,SNLI \u2192HANS\n[106]\nNER\nCoNLL \u2192CrossNER\n[19]\nNER\nOntoNotes \u2192I2B2\u201914, CONLL\u201903, WNUT\u201917, GUM, Few-NERD\n[109]\nEQA\nSQuAD \u2192NewsQA, SearchQA, TriviaQA, HotpotQA, Natural Questions\na subset of references",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The research on out-of-distribution (OOD) robustness in NLP has been hindered by inadequate evaluation benchmarks that do not present sufficient challenges, leading to an overestimation of model robustness.",
            "purpose of benchmark": "The benchmark is intended to provide a standardized evaluation framework for assessing OOD robustness in NLP models."
        },
        "problem": {
            "definition": "The benchmark addresses the challenges of evaluating NLP models under distribution shifts, specifically semantic and background shifts.",
            "key obstacle": "Existing benchmarks often use datasets that are too similar to in-distribution (ID) datasets, which limits the challenge and leads to misleading evaluations."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need for a systematic approach to evaluate OOD robustness, moving away from random dataset selection.",
            "opinion": "The authors believe that a rigorous benchmark is crucial for advancing the understanding of OOD robustness in NLP.",
            "innovation": "BOSS introduces a comprehensive suite covering multiple tasks and datasets with distinct distribution shifts, improving upon previous benchmarks that focused on single tasks.",
            "benchmark abbreviation": "BOSS"
        },
        "dataset": {
            "source": "The dataset was compiled from existing resources like Paperswithcode, Kaggle, and ACL Anthology, focusing on diversity and challenge.",
            "desc": "BOSS includes 5 tasks and 20 datasets, ensuring a wide range of evaluation conditions.",
            "content": "The dataset comprises text data for tasks such as sentiment analysis, toxic detection, and natural language inference.",
            "size": "1,000,000",
            "domain": "Sentiment Analysis",
            "task format": "Sentiment Analysis"
        },
        "metrics": {
            "metric name": "Accuracy, F1-score",
            "aspect": "Model performance on OOD tasks",
            "principle": "Metrics were selected based on their ability to reflect true performance under distribution shifts.",
            "procedure": "Models are evaluated on their ability to maintain performance when tested on OOD datasets."
        },
        "experiments": {
            "model": "The experiments included various pretrained language models and robust methods.",
            "procedure": "Models were fine-tuned and evaluated across different tasks using the BOSS benchmark.",
            "result": "Results indicated that while some methods improved performance, vanilla fine-tuning remained a strong baseline.",
            "variability": "Variability was accounted for through multiple trials and diverse dataset selections."
        },
        "conclusion": "The findings highlight the necessity of a robust evaluation framework for assessing OOD performance in NLP and suggest that current methods may not sufficiently enhance robustness.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive evaluation across multiple tasks, enhancing the understanding of model robustness.",
            "limitation": "The benchmark may not cover all aspects of NLP robustness and is limited to five selected tasks.",
            "future work": "Future research should focus on expanding the benchmark to include more diverse tasks and datasets as they become available."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "4",
            "key information": "The research on out-of-distribution (OOD) robustness in NLP has been hindered by inadequate evaluation benchmarks that do not present sufficient challenges, leading to an overestimation of model robustness."
        },
        {
            "section number": "4.1",
            "key information": "The benchmark addresses the challenges of evaluating NLP models under distribution shifts, specifically semantic and background shifts."
        },
        {
            "section number": "4.2",
            "key information": "The benchmark is intended to provide a standardized evaluation framework for assessing OOD robustness in NLP models."
        },
        {
            "section number": "4.3",
            "key information": "BOSS introduces a comprehensive suite covering multiple tasks and datasets with distinct distribution shifts, improving upon previous benchmarks that focused on single tasks."
        },
        {
            "section number": "4.4",
            "key information": "The dataset was compiled from existing resources like Paperswithcode, Kaggle, and ACL Anthology, focusing on diversity and challenge."
        },
        {
            "section number": "7.1",
            "key information": "Existing benchmarks often use datasets that are too similar to in-distribution (ID) datasets, which limits the challenge and leads to misleading evaluations."
        },
        {
            "section number": "7.2",
            "key information": "Future research should focus on expanding the benchmark to include more diverse tasks and datasets as they become available."
        }
    ],
    "similarity_score": 0.7506365701387511,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df04/df04e2e8-4925-4c0a-8d57-44b89ba253f0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bdd/3bdd3ee7-418f-47d5-bb4f-8d374cf79e2a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/923f/923f1aca-8036-41e2-b85c-a6347ff612ac.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d62/4d6289dc-2079-49fb-9da4-5d1f0aa9ff73.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6a6c/6a6cded9-ea33-4411-99cd-4a004bb43c2a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fb37/fb37681e-1bd9-4dd4-a966-c87556125fde.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af0e/af0e7b6b-4423-417f-a236-6171ce901918.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Revisiting out-of-distribution robustness in nlp_ Benchmarks, analysis, and LLMs evaluations.json"
}