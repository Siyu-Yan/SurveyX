{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2003.05731",
    "title": "SUOD: Accelerating Large-Scale Unsupervised Heterogeneous Outlier Detection",
    "abstract": "Outlier detection (OD) is a key machine learning (ML) task for identifying abnormal objects from general samples with numerous high-stake applications including fraud detection and intrusion detection. Due to the lack of ground truth labels, practitioners often have to build a large number of unsupervised, heterogeneous models (i.e., different algorithms with varying hyperparameters) for further combination and analysis, rather than relying on a single model. How to accelerate the training and scoring on new-coming samples by outlyingness (referred as prediction throughout the paper) with a large number of unsupervised, heterogeneous OD models? In this study, we propose a modular acceleration system, called SUOD, to address it. The proposed system focuses on three complementary acceleration aspects (data reduction for high-dimensional data, approximation for costly models, and taskload imbalance optimization for distributed environment), while maintaining performance accuracy. Extensive experiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness in heterogeneous OD acceleration, along with a real-world deployment case on fraudulent claim analysis at IQVIA, a leading healthcare firm. We open-source SUOD for reproducibility and accessibility.",
    "bib_name": "zhao2021suodacceleratinglargescaleunsupervised",
    "md_text": "# SUOD: ACCELERATING LARGE-SCALE UNSUPERVISED HETEROGENEOUS OUTLIER DETECTION\nYue Zhao * 1 Xiyang Hu * 1 Cheng Cheng 1 Cong Wang 2 Changlin Wan 3 Wen Wang 1 Jianing Yang 1 Haoping Bai 1 Zheng Li 4 Cao Xiao 5 Yunlong Wang 5 Zhi Qiao 5 Jimeng Sun 6 Leman Akoglu 1\n# Yue Zhao * 1 Xiyang Hu * 1 Cheng Cheng 1 Cong Wang 2 Changlin Wan 3 Wen Wang 1 Jianing Yang 1 Haoping Bai 1 Zheng Li 4 Cao Xiao 5 Yunlong Wang 5 Zhi Qiao 5 Jimeng Sun 6 Leman Akoglu 1\nABSTRACT\n]  5 Mar 2021\nOutlier detection (OD) is a key machine learning (ML) task for identifying abnormal objects from general samples with numerous high-stake applications including fraud detection and intrusion detection. Due to the lack of ground truth labels, practitioners often have to build a large number of unsupervised, heterogeneous models (i.e., different algorithms with varying hyperparameters) for further combination and analysis, rather than relying on a single model. How to accelerate the training and scoring on new-coming samples by outlyingness (referred as prediction throughout the paper) with a large number of unsupervised, heterogeneous OD models? In this study, we propose a modular acceleration system, called SUOD, to address it. The proposed system focuses on three complementary acceleration aspects (data reduction for high-dimensional data, approximation for costly models, and taskload imbalance optimization for distributed environment), while maintaining performance accuracy. Extensive experiments on more than 20 benchmark datasets demonstrate SUOD\u2019s effectiveness in heterogeneous OD acceleration, along with a real-world deployment case on fraudulent claim analysis at IQVIA, a leading healthcare firm. We open-source SUOD for reproducibility and accessibility.\n# 1 INTRODUCTION\nOutlier detection (OD) aims at identifying the samples that are deviant from the general data distribution (Zhao et al., 2019b; Lai et al., 2020), which has been used in various applications (Chandola et al., 2009; Zha et al., 2020). Notably, most of the existing OD algorithms are unsupervised due to the high cost of acquiring ground truth (Zhao et al., 2019a). Model selection and hyperparameter tuning in OD have been shown to be non-trivial problems (Zhao et al., 2020; Li et al., 2020). To reduce the risk and instability of using a single OD model, practitioners prefer to build a large corpus of OD models with variation and diversity, e.g., different algorithms, varying parameters, distinct views of the datasets, etc (Aggarwal & Sathe, 2017). This approach is known as heterogeneous OD. Ensemble methods that select and combine diversified base models can be leveraged to analyze heterogeneous OD models (Aggarwal, 2012; Zimek et al., 2013; Aggarwal & Sathe, 2017), and more reliable results may be achieved. The simplest combination is to take the average or maximum across all the base models as the final result (Aggarwal & Sathe, 2017), along with more complex * 1 2\nProceedings of the 4 th MLSys Conference, San Jose, CA, USA, 2021. Copyright 2021 by the author(s).\ncombination approaches in both unsupervised (Zhao et al., 2019a) and semi-supervised manners (Zhao & Hryniewicki, 2018).\nHowever, training and prediction with a large number of heterogeneous OD models is computationally expensive on high-dimensional, large datasets. For instance, proximitybased algorithms, assuming outliers behave differently in specific regions (Aggarwal, 2013), can be prohibitively slow or even completely fail to work under this setting. Representative methods such as k nearest neighbors (kNN) (Ramaswamy et al., 2000), local outlier factor (LOF) (Breunig et al., 2000), and local outlier probabilities (LoOP) (Kriegel et al., 2009), operate in Euclidean space for distance/density calculation, suffering from the curse of dimensionality (Schubert et al., 2015). Numerous works have attempted to tackle this scalability challenge from various angles, e.g., data projection (Keller et al., 2012), subspacing (Liu et al., 2008), and distributed learning for specific OD algorithms (Lozano & Acu\u02dcna, 2005; Oku et al., 2014). However, none of them provides a comprehensive solution by considering all aspects of large-scale heterogeneous OD, leading to limited practicability and efficacy. To tap the gap, we propose a comprehensive acceleration framework called SUOD. As shown in Fig. 1, SUOD has three modules focusing on complementary levels: random projection (data level), pseudo-supervised approximation (model level), and balanced parallel scheduling\nTo tap the gap, we propose a comprehensive acceleration framework called SUOD. As shown in Fig. 1, SUOD has three modules focusing on complementary levels: random projection (data level), pseudo-supervised approximation (model level), and balanced parallel scheduling\n(execution level). For high-dimensional data, SUOD generates a random low-dimensional subspace for each base model by Johnson-Lindenstrauss projection (Johnson & Lindenstrauss, 1984), in which the corresponding base model is trained. If prediction on new-coming samples is needed, fast supervised models are employed to approximate costly unsupervised outlier detectors (e.g., kNN and LOF). To train the supervised approximators, we regard the unsupervised models\u2019 outputs on the train set as \u201cpseudo ground truth\u201d. Intuitively, this may be viewed as distilling knowledge from complex unsupervised models (Hinton et al., 2015) by fast and more interpretable supervised models. We also build a taskload predictor to reduce the scheduling imbalance in distributed environment. Other than generically assigning the equal number of models to each worker, our balanced parallel scheduling mechanism forecasts OD model cost, e.g., training time, before scheduling them, so that the taskload is evenly distributed among workers. Notably, all three acceleration modules are designed to be independent but complementary, which can be used alone or combined as a system. It is noted that SUOD is designed for offline learning with a stationary assumption, although it may be further extended to online setting for streaming data with extra effort (Wang et al., 2019a; 2020). It is noted that offline training and prediction is one of the primary scenarios in machine learning applications (Amazon, 2020; Nakandala et al., 2020).\n# Our contributions are summarized as follows:\n1. The First Comprehensive OD Acceleration System: We propose SUOD, (to our knowledge) the most comprehensive system for heterogeneous OD acceleration by a holistic view on data, model, and execution level. 2. Analysis of Data Compression: We examine various data compression methods and identify the best performing method(s) for large-scale outlier ensembles. 3. Exploration of Model Approximation for OD: We analyze the use of pseudo-supervised regression models\u2019 performance in approximating costly unsupervised OD models, as the first research effort on this topic. 4. Forecasting-based Scheduling System: We fix an imbalance scheduling issue in distributed heterogeneous OD efficiency, saving up to 61% execution time. 5. Effectiveness: We conduct extensive experiments to show the effectiveness of the acceleration modules independently, and of the entire framework as a whole, along with a real-world case on fraud detection.\nTo foster accessibility and reproducibility, We release SUOD with industry-level implementation1, which also becomes a core component of the leading OD library PyOD2.\n1https://github.com/yzhao062/SUOD 2https://github.com/yzhao062/PyOD\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/57d9/57d9e976-14ec-4186-a009-e9f42226f8f7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. SUOD focuses on three independent levels.</div>\n# 2 RELATED WORK\n# 2.1 Outlier Detection and Ensemble Learning\nOutlier detection has numerous important applications, such as rare disease detection (Li et al., 2018), healthcare utilization analysis (Hu et al., 2012), video surveillance (Lu et al., 2017), fraudulent online review analysis (Akoglu et al., 2013), and network intrusion detection (Lazarevic et al., 2003). Yet, detecting outliers is challenging due to various reasons (Aggarwal, 2012; Zhao et al., 2019a;b). Most of the existing detection algorithms are unsupervised as ground truth is often absent in practice, and acquiring labels can be prohibitively expensive. We include 8 popular OD algorithms in this study for experimentation, including Isolation Forest (Liu et al., 2008), Local Outlier Factor (LOF) (Breunig et al., 2000), Angle-based Outlier Detection (ABOD) (Kriegel et al., 2009), Feature Bagging (Lazarevic & Kumar, 2005), Histogram-based Outlier Score (HBOS) (Goldstein & Dengel, 2012), and Clustering-Based Local Outlier Factor (CBLOF) (He et al., 2003). See Appendix B for details on OD models. Consequently, relying on a single unsupervised model has inherently high risk, and outlier ensembles that leverage a group of diversified (e.g., heterogeneous) detectors have become increasingly popular (Aggarwal, 2012; Zimek et al., 2013; Aggarwal & Sathe, 2017). There are a group of unsupervised outlier ensemble frameworks proposed in the last several years from simple combination like averaging (Aggarwal & Sathe, 2017) to more complex model selection approaches like SELECT (Rayana & Akoglu, 2016), LSCP (Zhao et al., 2019a), and MetaOD (Zhao et al., 2020). In addition to fully unsupervised outlier ensembles, there are (semi-)supervised ensembling frameworks that can incorporate existing label information such like XGBOD (Zhao & Hryniewicki, 2018). For both unsupervised and (semi)supervised methods, a large group of unsupervised, heterogeneous OD models are used as base for robustness and performance\u2014SUOD is hereby proposed to accelerate for this scenario.\n# 2.2 Scalability and Efficiency Challenges in OD\nEfforts have been made through various channels to accelerate large-scale OD. On the data level, researchers try to project high-dimensional data onto lower-dimensional subspaces (Achlioptas, 2001), including simple Principal Component Analysis (PCA) (Shyu et al., 2003) and more complex subspace method HiCS (Keller et al., 2012). However, deterministic projection methods, e.g., PCA, are not ideal for building diversified heterogeneous OD\u2014they lead to the same or similar subspaces with limited diversity by nature, resulting in the loss of outliers (Aggarwal, 2013). Complex projection and subspace methods may bring performance improvement for outlier mining, but the generalization capacity is limited. Hence, projection methods preserving pairwise distance relationships for downstream tasks should be considered. SUOD\u2019s considers both diversity induction and pairwise distance preservation for downstream tasks, leading to diversified and meaningful feature spaces (see \u00a73.3). On the model level, knowledge distillation emerges as a good way of compressing large neural networks (Hinton et al., 2015), while its usage in outlier detection is still underexplored. Knowledge distillation refers to the notion of compressing a large, often cumbersome model(s) into a small and more interpretable one(s). Under the context of OD, proximity-based models, such as LOF, can be slow (high time complexity) for predicting on new-coming samples with limited interpretability, which severely restricts their usability. SUOD adapts a similar idea to outlier mining by \u201cdistilling\u201d complex unsupervised models. Although SUOD shares a similar concept as knowledge distillation for computational cost optimization, there are a few notable differences (see \u00a73.4). There are also engineering cures on the execution level. For various reasons, OD has no mature and efficient distributed frameworks with thousands of clusters\u2014distributed computing for OD mainly falls into the category of \u201cscale-up\u201d that focuses on leveraging multiple local cores on a single machine more efficiently. To this end, specific OD algorithms can be accelerated by distributed computing with multiple workers (e.g., CPU cores) (Oku et al., 2014; Lozano & Acu\u02dcna, 2005). However, these frameworks are not designed for a group of heterogeneous models but only a single algorithm, which limits their usability. It is noted that a group of heterogeneous detection models can have significantly varied computational cost. As a simple example, let us split 100 heterogeneous models into 4 groups for parallel training.\nEfforts have been made through various channels to accelerate large-scale OD. On the data level, researchers try to project high-dimensional data onto lower-dimensional subspaces (Achlioptas, 2001), including simple Principal Component Analysis (PCA) (Shyu et al., 2003) and more complex subspace method HiCS (Keller et al., 2012). However, deterministic projection methods, e.g., PCA, are not ideal for building diversified heterogeneous OD\u2014they lead to the same or similar subspaces with limited diversity by nature, resulting in the loss of outliers (Aggarwal, 2013). Complex projection and subspace methods may bring performance improvement for outlier mining, but the generalization capacity is limited. Hence, projection methods preserving pairwise distance relationships for downstream tasks should be considered. SUOD\u2019s considers both diversity induction and pairwise distance preservation for downstream tasks, leading to diversified and meaningful feature spaces (see \u00a73.3). On the model level, knowledge distillation emerges as a good way of compressing large neural networks (Hinton et al., 2015), while its usage in outlier detection is still underexplored. Knowledge distillation refers to the notion of compressing a large, often cumbersome model(s) into a small and more interpretable one(s). Under the context of OD, proximity-based models, such as LOF, can be slow (high time complexity) for predicting on new-coming samples with limited interpretability, which severely restricts their usability. SUOD adapts a similar idea to outlier mining by \u201cdistilling\u201d complex unsupervised models. Although SUOD shares a similar concept as knowledge distillation for computational cost optimization, there are a few notable differences (see \u00a73.4). There are also engineering cures on the execution level. For\nThere are also engineering cures on the execution level. For various reasons, OD has no mature and efficient distributed frameworks with thousands of clusters\u2014distributed computing for OD mainly falls into the category of \u201cscale-up\u201d that focuses on leveraging multiple local cores on a single machine more efficiently. To this end, specific OD algorithms can be accelerated by distributed computing with multiple workers (e.g., CPU cores) (Oku et al., 2014; Lozano & Acu\u02dcna, 2005). However, these frameworks are not designed for a group of heterogeneous models but only a single algorithm, which limits their usability. It is noted that a group of heterogeneous detection models can have significantly varied computational cost. As a simple example, let us split 100 heterogeneous models into 4 groups for parallel training. If group #2 takes significantly longer time than the others to finish, it behaves like the bottleneck of the system. More formally, imbalanced task scheduling causes the system efficiency to be curbed by the worker taking the most time. There are also a line of system researches focus on more\nefficient task scheduling for \u201cshorter tasks on larger degree of parallelism\u201d, e.g., Sparrow (Ousterhout et al., 2013) and Pigeon (Wang et al., 2019b). For instance, Sparrow discusses scheduling millions of tasks (at millisecond scale) on thousands of machines. Differently, Heterogeneous OD applications typically use a few OD models (e.g., 5 to 1,000) and each of them takes a few seconds to hours to run (a considerable number of tasks; each takes time to run), which is \u201clonger tasks with a small number of workers\u201d. SUOD is, therefore, proposed to reduce the inefficiency in distributed heterogeneous OD specifically.\n# 3 SYSTEM DESIGN\n# 3.1 Problem Formulation\nOD applications and research are primarily running a single, on-prime, powerful machine with multiple cores/workers, due to its high-stake nature (e.g., data sensitive of financial transactions). Therefore, we formulate unsupervised heterogeneous OD training and prediction tasks with: \u2022 m unsupervised heterogeneous OD models, where M = {M1, ..., Mm}. We refer the combination of an algorithm and its hyperparamters as a model. \u2022 train data Xtrain \u2208Rn\u00d7d without ground truth labels. \u2022 (optional) test data Xtest \u2208Rl\u00d7d for prediction. The OD models should be trained first. \u2022 (optional) t available workers for distributed computing, e.g., t cores on a single machine. This constructs the worker pool as W = {W1, ..., Wt}. By default, a single worker setting (t = 1) is assumed.\nWithout SUOD, one will train each model in M on Xtrain iteratively, e.g., with a for loop. If there are multiple workers available (t > 1), a generic scheduling system will equally split m models into t groups, so each available worker will process roughly \u2308m t \u2309models. Prediction on new-coming samples Xtest follows the similar manner as training. See Algorithm 1 for detailed symbol definition.\n# 3.2 System Design\nSUOD is designed to accelerate the above procedures with three independent modules targeting different levels (data, model, and execution). Each module can be flexibly enabled or disabled as shown in Algorithm 1. For highdimensional data, SUOD can randomly project the original feature onto low-dimensional spaces (\u00a73.3). Pairwise distance relationships are expected to be maintained, and the diversity is induced for ensemble construction. A fast supervised regressor could be used to approximate the output of each slow and costly unsupervised detector. We could use the supervised regressor for fast prediction (\u00a73.4). If there are multiple available workers for distributed computing, we propose a forecasting-based scheduling mechanism (\u00a73.5)\nAlgorithm 1 SUOD: Training and Prediction\nInput: m unsupervised OD models M; train data Xtrain \u2208Rn\u00d7d;\ntarget dimension k; the number of available workers t (op-\ntional, default to 1); test data Xtest \u2208Rl\u00d7d (optional); super-\nvised regressor R (optional)\nOutput: Trained OD models M; fitted pseudo-supervised regres-\nsors R (optional); test prediction results \u02c6ytest (optional)\n1: for each model Mi in M do\n2:\nif random projection is enabled (\u00a73.3) then\n3:\nInitialize a JL transformation matrix W \u2208Rd\u00d7k\n4:\nGet feature subspace \u03c8i := \u27e8Xtrain, W\u27e9\u2208Rn\u00d7k\n5:\nelse\n6:\nUse the original space \u03c8i := Xtrain \u2208Rn\u00d7d\n7:\nend if\n8: end for\n9: if number of available workers t > 1 then\n10:\nScheduling the training of m models to t workers by mini-\nmizing Eq. 2 (see \u00a73.5). Models are trained on the corre-\nsponding feature space [\u03c81, ..., \u03c8m].\n11: else\n12:\nTrain each model Mi in M on its corresponding \u03c8i.\n13: end if\n14: Return trained models M\n15: if Scoring on newcoming samples Xtest then\n16:\nAcquire the pseudo ground truth target\u03c8i as the output of\nMi on \u03c8i, i.e., target\u03c8i := Mi(\u03c8i)\n17:\nfor each costly model Mi in Mc do\n18:\nInitialize a supervised regressor Ri\n19:\nTrain Ri by {\u03c8i, target\u03c8i} (see \u00a73.4)\n20:\nPredict by supervised Ri, \u02c6yi\ntest = Ri.predict(Xtest)\n21:\nend for\n22:\nReturn \u02c6ytest and approximation regressors R\n23: end if\nto reduce taskload imbalance in heterogeneous OD. SUOD\u2019s API design follows scikit-learn (Pedregosa et al., 2011) and PyOD (Zhao et al., 2019b), with an initialization, fit, and prediction schema (see Codeblock 1).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/764a/764a093e-57d0-4af8-94e3-e76d70ad50b2.png\" style=\"width: 50%;\"></div>\nimport SUOD\n# initialize a group of heterogeneous OD models\nbase_estimators=[\nLOF(n_neighbors=40), ABOD(n_neighbors=50),\nLOF(n_neighbors=60), IForest(n_estimators=100)]\n# initialize SUOD with module flags\nclf=SUOD(base_estimators=base_estimators,\nrp_flag_global=True,\napprox_clf=RandomForestRegressor(),\nbps_flag=True,\napprox_flag_global=True)\n# fit and make prediction\nclf.fit(X_train)\ntest_labels=clf.predict(X_test)\ntest_scores=clf.decision_function(X_test)}\n# 3.3 Data Level: Random Projection (RP) for Data Compression\n# 3.3 Data Level: Random Projection (RP) for Data\nFor high-dimensional datasets, many proximity-based OD algorithms suffer from the curse of dimensionality (Lazarevic & Kumar, 2005). A widely used dimensionality reduction to cure this is the Johnson-Lindenstrauss (JL) projection (Johnson & Lindenstrauss, 1984), which has been applied to OD because of its great scalability (Schubert et al., 2015). Unlike PCA discussed in \u00a72.2, JL projection could compress the data without heavy distortion on the Euclidean space\u2014outlyingness information is therefore preserved in the compression. Moreover, its built-in randomness can be useful for diversity induction in heterogeneous OD\u2014data randomness can also serve as a source of heterogeneity. Additionally, JL projection (O(ndk)) is more efficient than popular PCA (O(nd2 + n3)) with lower time complexity, where k is the target dimension for compression. JL projection is defined as: given a set of n samples X = {x1, x2, ...xn}, each xi \u2208Rd, let W be a k \u00d7 d projection matrix with each entry drawing independently from a predefined distribution F, e.g., Gaussian, so that W \u223cF. Then the JL projection is a function f : Rd \u2192Rk such that f(xi) = 1 \u221a kxiWT . JL projection randomly projects high-dimensional data (d dimensions) to lower-dimensional subspaces (k dimensions), but preserves the distance relationship between points. In fact, if we fix some v \u2208Rd, for every \u03f5 \u2208(0, 3), we have (Schubert et al., 2015):\n(1)\nLet v to be the differences between vectors. Then, the above bound shows that for a finite set of n vectors X = {x1, x2, ...xn} \u2208Rd, the pairwise Euclidean distance is preserved within a factor of (1 \u00b1 \u03f5), reducing the vectors to k = O( log(n) \u03f52 ) dimensions. Four distributions F for JL projection are considered in this study: (i) basic: the transformation matrix is generated by standard Gaussian; (ii) discrete: the transformation matrix is picked randomly from Rademacher distribution (uniform in {\u22121, 1}); (iii) circulant: the transformation matrix is obtained by rotating the subsequent rows from the first row which is generated from standard Gaussian and (iv) toeplitz: the first row and column of the transformation matrix are generated from standard Gaussian, and each diagonal uses a constant value from the first row and column. A more thorough empirical study on JL methods can be found in (Venkatasubramanian & Wang, 2011). For Xtrain, RP can reduce the original feature space d to the target dimension k. Specifically, SUOD initializes a JL transformation matrix W \u2208Rd\u00d7k by drawing from one of\nLet v to be the differences between vectors. Then, the above bound shows that for a finite set of n vectors X = {x1, x2, ...xn} \u2208Rd, the pairwise Euclidean distance is preserved within a factor of (1 \u00b1 \u03f5), reducing the vectors to k = O( log(n) \u03f52 ) dimensions.\nFour distributions F for JL projection are considered in this study: (i) basic: the transformation matrix is generated by standard Gaussian; (ii) discrete: the transformation matrix is picked randomly from Rademacher distribution (uniform in {\u22121, 1}); (iii) circulant: the transformation matrix is obtained by rotating the subsequent rows from the first row which is generated from standard Gaussian and (iv) toeplitz: the first row and column of the transformation matrix are generated from standard Gaussian, and each diagonal uses a constant value from the first row and column. A more thorough empirical study on JL methods can be found in (Venkatasubramanian & Wang, 2011).\nthe four distributions F. Xtrain is therefore projected onto the k dimension feature space as X\u2032 train = \u27e8Xtrain, W\u27e9\u2208 Rn\u00d7k. The transformation matrix W should be kept for transforming newcoming samples: X\u2032 test = \u27e8Xtest, W\u27e9\u2208 Rm\u00d7k. It is noted that RP module should be used with caution. First, projection may not be helpful or even detrimental for subspace methods like Isolation Forest and HBOS. Second, if the number of samples n is too small, the bound in Eq. (1) does not hold.\n<div style=\"text-align: center;\">Figure 2. Flowchart of balanced parallel scheduling, which aims to assign nearly equal rank sum by model cost predictor Ccost.</div>\n# 3.4 Model Level: Pseudo-Supervised Approximation (PSA) for Fast Prediction\nPSA module is designed to speed up prediction on newcoming samples. Specifically, after the models in M are trained, SUOD uses PSA to approximate and replace each costly unsupervised model by a faster supervised regressor for fast offline prediction. Notably, only costly unsupervised models should be replaced; the cost can be measured through time complexity analysis. For instance, proximity-based algorithms like kNN and LOF are costly in prediction (upper bounded by O(nd)), and can be effectively replaced by \u201ccheaper\u201d supervised models like random forest (Breiman, 2001) (upper bounded by O(ph) where p denotes the number of base trees and h denotes the max depth of a tree; often p \u226an and h \u2264d). This \u201cpseudosupervised\u201d model uses the output of unsupervised models (outlyingness score) as \u201cthe pseudo ground truth\u201d\u2014the goal is to approximate the output of the underlying unsupervised model. It is noted that the approximator\u2019s prediction cost (i.e., time complexity) must be lower than the underlying unsupervised model, while maintaining a comparable level of prediction accuracy. For instance, fast (low time complexity) OD algorithms like Isolation Forest and HBOS should not be approximated and replaced. To facilitate this process, we predefine the pool of costly OD algorithm Mc. If a model Mi is in Mc, it will be approximated by default. As shown in Algorithm 1, for each costly trained unsupervised model Mi belonging to Mc, a supervised regressor Ri is trained by {Xtrain, yi}; yi is the outlyingness score by Mi on the train set (referred as pseudo ground truth)1. Ri is then used to predict on unseen data Xtest. Remark 1: Supervised tree ensembles are recommended for PSA due to their outstanding scalability, robustness to overfitting, and interpretability (e.g., feature importance) (Hu et al., 2019). In addition to the execution time reduction, supervised models are generally more interpretable. For instance, random forest used in the experiments can yield feature importance automatically to facilitate understanding. Remark 2: Notably, PSA may be viewed as using supervised regressors to distill knowledge from unsupervised OD 1If RP is enabled, Xtrain is replaced by the compressed space.\nPSA module is designed to speed up prediction on newcoming samples. Specifically, after the models in M are trained, SUOD uses PSA to approximate and replace each costly unsupervised model by a faster supervised regressor for fast offline prediction. Notably, only costly unsupervised models should be replaced; the cost can be measured through time complexity analysis. For instance, proximity-based algorithms like kNN and LOF are costly in prediction (upper bounded by O(nd)), and can be effectively replaced by \u201ccheaper\u201d supervised models like random forest (Breiman, 2001) (upper bounded by O(ph) where p denotes the number of base trees and h denotes the max depth of a tree; often p \u226an and h \u2264d). This \u201cpseudosupervised\u201d model uses the output of unsupervised models (outlyingness score) as \u201cthe pseudo ground truth\u201d\u2014the goal is to approximate the output of the underlying unsupervised model. It is noted that the approximator\u2019s prediction cost (i.e., time complexity) must be lower than the underlying unsupervised model, while maintaining a comparable level of prediction accuracy. For instance, fast (low time complexity) OD algorithms like Isolation Forest and HBOS should not be approximated and replaced. To facilitate this process, we predefine the pool of costly OD algorithm Mc. If a model Mi is in Mc, it will be approximated by default. As shown in Algorithm 1, for each costly trained unsupervised model Mi belonging to Mc, a supervised regressor Ri is trained by {Xtrain, yi}; yi is the outlyingness score by\nRemark 1: Supervised tree ensembles are recommended for PSA due to their outstanding scalability, robustness to overfitting, and interpretability (e.g., feature importance) (Hu et al., 2019). In addition to the execution time reduction, supervised models are generally more interpretable. For instance, random forest used in the experiments can yield feature importance automatically to facilitate understanding. Remark 2: Notably, PSA may be viewed as using supervised regressors to distill knowledge from unsupervised OD 1If RP is enabled, Xtrain is replaced by the compressed space.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab01/ab01e999-e47a-4009-8657-91b9db024d3d.png\" style=\"width: 50%;\"></div>\nmodels. However, it works in a fully unsupervised manner, unlike the classic distillation under supervised settings.\n# 3.5 Execution level: Balanced Parallel Scheduling (BPS) for Taskload Imbalance Reduction\nTaskload Imbalance within Distributed Systems: It is likely to observe system inefficiency due to taskload imbalance among workers when a large number of heterogeneous models are used. For instance, we want to train 25 OD models with varying parameters from each of the four algorithms {kNN, Isolation Forest, HBOS, OCSVM} (100 heterogeneous models in total). The existing distributed frameworks, e.g., the voting machine in scikit-learn (Pedregosa et al., 2011) or general frameworks like joblib2, may simply split the models into 4 subgroups by order and schedule the first 25 models (all kNNs) on worker 1, the next 25 models on worker 2, etc. This does not account for the fact that within a group of heterogeneous models, the computational cost varies. Scheduling the task with the equal number of models can result in highly imbalanced load. In the worst-case scenario, one worker may be assigned significant more load than the rest, resulting in halt to the entire process. In this example, the kNN subgroup will be the system curb due to high time complexity. One naive solution is to shuffle the base models randomly. However, there is no guarantee this heuristic could work, and it may be practically infeasible.\n# The proposed BPS focuses on delivering a more bal-\nanced task scheduling among workers via forecasting their cost in advance. Ideally, all workers can finish the scheduled tasks within a similar duration and return the results. To achieve this goal, SUOD comes with a model cost predictor Ccost to forecast the model execution time (sum of 10 trials) given the meta-features (descriptive features) of a dataset (Zhao et al., 2020). Ccost is trained on 11 algorithm family with 47 benchmark datasets by 10-fold cross validation, yielding an effective regressor (random forest is used in this study). Ccost\u2019s outputs show high Spearman\u2019s Rank correlation (Spearman, 1904) (rs > 0.9) to the true\nmodel cost rank with low p-value (p < 0.0001), in all folds. Given a dataset and a model, Ccost can predict the execution time of the model\u2019s execution time with high accuracy. It is noted that the model cost predictor is only trained for the major methods in Python Outlier Detection Toolbox (PyOD) (Zhao et al., 2019b). For unseen models, they are classified as \u201cunknown\u201d to be assigned with the max cost to prevent overoptimistic scheduling.\nAs a result, a scheduling strategy is proposed by enforcing a nearly equal rank sum by the forecasted execution time among all available workers. Fig. 2 provides a simple example of scheduling m models to 4 workers. More formally, before scheduling m models for training (or prediction), cost predictor Ccost is invoked to forecast the execution time of each model M in M as Ccost(M) and output the model cost rank of each model in {1, m} (the higher the rank, the longer the forecasted execution time). If there are t cores (workers), each worker will be assigned a group of models to achieve the objective of minimizing the taskload imbalance among workers (Eq. 2).\n(2)\n\ufffd \ufffd Consequently, each worker is assigned with a group of models with the rank sum close to the average rank sum (1+m)m 2 /t = m2+m 2t . Indeed, the accurate running time prediction is less relevant as it depends on the hardware\u2014the rank is more useful as a relevance measure with the transferability to other hardware. That is, the running time will vary on different machines, but the relative rank should preserve. One issue around the sum of ranks is the overestimation of high-rank models. For instance, rank f-th model will be counted f times more heavily than rank 1 model during the sum calculation, even their actual running time difference will not be as big as f times. To fix this, we introduce a discounted rank by rescaling model rank f to 1+ \u03b1f m , where \u03b1 denotes the scaling strength (default to 1). A larger \u03b1 therefore puts more emphasis on costly models.\n# 4 EXPERIMENTS & DISCUSSION\nFirst, three experiments are conducted to understand the effectiveness of individual modules independently: Q1: how will different compression methods affect the performance of downstream OD accuracy (\u00a74.1); Q2: will use pseudosupervised regressors lead to more efficient prediction in comparison to the original unsupervised models (\u00a74.2) and Q3: how does the proposed balanced scheduling strategy perform under different settings (varying number of models m, number of workers t, etc.) (\u00a74.3). Then, the full SUOD with all three modules enabled is evaluated regarding time cost and prediction accuracy (on new samples) (\u00a74.4). Fi-\nnally, a real-world deployment case on fraudulent claim analysis at IQVIA (a leading healthcare organization), is described (\u00a74.5). The details of experiment setting, e.g., datasets, evaluation metrics, and the pool of heterogeneous OD models, can be found in the Appendix.\n# 4.1 Q1: Comparison of Model Compression Methods\nIn this section, we demonstrate the effectiveness of RP module in high-dimensional OD tasks. To evaluate the effect of data projection, we choose three costly outlier detection algorithms, ABOD, LOF, and kNN to measure their execution time, and prediction accuracy (ROC and P@N), before and after projection. These methods directly or indirectly measure sample similarity in Euclidean space, e.g., pairwise distance, which is prone to the curse of dimensionality, where data compression can help.\nTable 1 shows the comparison results on four datasets (see Appendix Table A); the reduced dimension is set as k = 2 3d (33% compression). We compare the proposed four JL projection methods (see \u00a73.3 for details of basic, discrete, circulant, and toeplitz) with original (no projection), PCA, and RS (randomly select k features from the original d features, used in Feature Bagging (Lazarevic & Kumar, 2005) and LSCP (Zhao et al., 2019a)). First, all compression methods show superiority regarding time cost. Second, original (no compression) method rarely outperforms, possibly due to the curse of dimensionality and lack of diversity (Zhao et al., 2019a). Third, PCA is inferior to original regarding prediction accuracy (see LOF performance in Table 1e, 1h, and 1k). The observation supports our claim that PCA is not suited in this scenario (see \u00a72.2). Fourth, JL methods generally lead to equivalent or better prediction performance than original regarding both time and prediction accuracy. Lastly, among all JL methods, circulant and toeplitz generally outperform others. For instance, toeplitz brings more than 60% time reduction for kNN, demonstrating the effectiveness of RP and chosen as the default choice.\n# 4.2 Q2: The Visual and Quantitative Analysis of PSA\nThrough both visualization and quantitative analysis, we observe PSA is useful for accelerating prediction of proximity-based OD algorithms. To better understand the effect of pseudo-supervised approximation, we first generate a synthetic dataset with 200 two-dimensional samples, consisting of 40 outliers generated by Normal distribution and 160 normal samples generated from Uniform distribution. In Fig. 3, we plot the decision surfaces of four costly unsupervised models (ABOD, Feature Bagging, kNN, and LOF) and of their corresponding supervised approximators (random forest regressor), with accuracy errors reported. In general, the faster pseudo supervised approximators do not lead to more errors, justifying the effectiveness of approx-\nTable 1. Comparison of various data compression methods on different outlier detectors and datasets (see Appendix Table A). Each column corresponds to an evaluation metric (execution time is measured in seconds); the best performing method is indicated in bold. JL projection methods, especially circulant and toeplitz, outperform regarding both time cost and prediction accuracy.\n Cardio\nMethod\nTime\nROC\nP@N\noriginal\n0.98\n0.59\n0.25\nPCA\n0.82\n0.59\n0.26\nRS\n0.92\n0.63\n0.29\nbasic\n0.83\n0.62\n0.28\ndiscrete\n0.82\n0.62\n0.28\ncirculant\n0.83\n0.62\n0.27\ntoeplitz\n0.83\n0.62\n0.28\nMethod\nTime\nROC\nP@N\noriginal\n0.08\n0.55\n0.17\nPCA\n0.04\n0.56\n0.19\nRS\n0.04\n0.57\n0.15\nbasic\n0.04\n0.60\n0.20\ndiscrete\n0.04\n0.59\n0.19\ncirculant\n0.04\n0.59\n0.20\ntoeplitz\n0.04\n0.60\n0.21\n<div style=\"text-align: center;\">(d) ABOD on MNIST</div>\n<div style=\"text-align: center;\">(e) LOF on MNIST</div>\n MNIST\nMethod\nTime\nROC\nP@N\noriginal\n12.89\n0.80\n0.39\nPCA\n8.93\n0.81\n0.37\nRS\n8.27\n0.74\n0.32\nbasic\n8.94\n0.80\n0.38\ndiscrete\n8.86\n0.80\n0.39\ncirculant\n9.33\n0.80\n0.38\ntoeplitz\n8.96\n0.80\n0.38\n MNIST\nMethod\nTime\nROC\nP@N\noriginal\n7.64\n0.68\n0.29\nPCA\n4.92\n0.67\n0.27\nRS\n3.65\n0.63\n0.23\nbasic\n4.87\n0.70\n0.31\ndiscrete\n5.21\n0.70\n0.32\ncirculant\n5.06\n0.69\n0.31\ntoeplitz\n4.97\n0.71\n0.31\n<div style=\"text-align: center;\">(h) LOF on Satellite</div>\n<div style=\"text-align: center;\">(g) ABOD on Satellite</div>\n(g) ABOD on Satellite\nMethod\nTime\nROC\nP@N\noriginal\n4.03\n0.59\n0.41\nPCA\n3.01\n0.62\n0.44\nRS\n3.53\n0.63\n0.44\nbasic\n3.10\n0.64\n0.45\ndiscrete\n3.12\n0.65\n0.46\ncirculant\n3.14\n0.66\n0.48\ntoeplitz\n3.14\n0.66\n0.47\n Satellite\nMethod\nTime\nROC\nP@N\noriginal\n0.82\n0.55\n0.37\nPCA\n0.23\n0.54\n0.36\nRS\n0.39\n0.54\n0.37\nbasic\n0.31\n0.54\n0.37\ndiscrete\n0.32\n0.54\n0.37\ncirculant\n0.39\n0.55\n0.38\ntoeplitz\n0.37\n0.54\n0.37\n<div style=\"text-align: center;\">(k) LOF on Satimage-2</div>\n<div style=\"text-align: center;\">(j) ABOD on Satimage-2</div>\n(k) LOF on Satimage-2\nMethod\nTime\nROC\nP@N\noriginal\n0.79\n0.54\n0.07\nPCA\n0.20\n0.52\n0.04\nRS\n0.37\n0.53\n0.08\nbasic\n0.29\n0.52\n0.08\ndiscrete\n0.30\n0.53\n0.07\ncirculant\n0.43\n0.59\n0.11\ntoeplitz\n0.32\n0.54\n0.09\n Satimage-2\nMethod\nTime\nROC\nP@N\noriginal\n3.68\n0.85\n0.28\nPCA\n2.70\n0.88\n0.30\nRS\n3.20\n0.89\n0.28\nbasic\n2.78\n0.91\n0.29\ndiscrete\n2.79\n0.91\n0.31\ncirculant\n2.85\n0.91\n0.29\ntoeplitz\n2.83\n0.92\n0.30\nimation. Fig. 3 subfigure 4 and 6 show that the approximators have even lower errors than the original (Feature Bagging and kNN). With a closer look at the decision surfaces, we assume that the approximation process improves the generalization ability of the model by \u201cignoring\u201d some overfitted points. However, the approximation does not work with ABOD, possibly due to its extremely coarse decision surface (see Fig. 3, subfigure 1). Table 2 and 3 compare prediction performance (scoring on new-coming samples) between the original unsupervised\n Cardio\nMethod\nTime\nROC\nP@N\noriginal\n0.09\n0.71\n0.34\nPCA\n0.03\n0.73\n0.34\nRS\n0.03\n0.69\n0.38\nbasic\n0.03\n0.74\n0.35\ndiscrete\n0.03\n0.74\n0.37\ncirculant\n0.03\n0.74\n0.34\ntoeplitz\n0.03\n0.73\n0.35\n<div style=\"text-align: center;\">(f) kNN on MNIST</div>\n MNIST\nMethod\nTime\nROC\nP@N\noriginal\n7.13\n0.84\n0.42\nPCA\n3.92\n0.84\n0.40\nRS\n3.33\n0.77\n0.34\nbasic\n4.17\n0.84\n0.42\ndiscrete\n4.11\n0.84\n0.41\ncirculant\n4.13\n0.84\n0.41\ntoeplitz\n4.11\n0.84\n0.42\n<div style=\"text-align: center;\">(i) kNN on Satellite</div>\n Satellite\nMethod\nTime\nROC\nP@N\noriginal\n0.71\n0.67\n0.49\nPCA\n0.18\n0.67\n0.50\nRS\n0.31\n0.68\n0.49\nbasic\n0.24\n0.68\n0.49\ndiscrete\n0.25\n0.69\n0.50\ncirculant\n0.33\n0.70\n0.50\ntoeplitz\n0.30\n0.70\n0.51\n<div style=\"text-align: center;\">(l) kNN on Satimage-2</div>\n Satimage-2\nMethod\nTime\nROC\nP@N\noriginal\n0.68\n0.94\n0.39\nPCA\n0.15\n0.94\n0.39\nRS\n0.29\n0.94\n0.38\nbasic\n0.23\n0.94\n0.38\ndiscrete\n0.20\n0.95\n0.37\ncirculant\n0.36\n0.96\n0.37\ntoeplitz\n0.25\n0.96\n0.39\nmodels and pseudo-supervised approximators on 10 datasets with 6 costly algorithms, regarding ROC and P@N. Since these algorithms are more computationally expensive than random forest regressors for prediction (by time complexity analysis), we skip the prediction time comparison where the gain is clear. Consequently, the focus is whether the approximators could predict unseen samples as good as the original unsupervised models. The tables reveal that not all the algorithms are suited for PSA, which is in line with the visual analysis. For instance, ABOD shows per-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b95/0b959952-027f-49ec-b206-aade0517c618.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 3. Decision surface comparison among unsupervised models and their pseudo-supervised approximators (in pairs). The approxi r\u2019s decision boundary shows a tentative regularization effect, leading to even fewer detection errors.</div>\nformance decrease on half of the datasets. Notably, ABOD looks for a low-dimensional subspace to embed the normal samples (Aggarwal, 2013), leading to a complex decision surface to approximate. In contrast, proximity-based models benefit from the approximation. Both tables show, kNN, LoF, and akNN (average kNN) experience a performance gain. Specifically, all three algorithms yield around 100% ROC increase on HTTP. Other algorithms, such as Feature Bagging and CBLOF, show a minor performance variation within the acceptable range. In other words, it is useful to perform PSA for these estimators as the time efficiency is greatly improved with little to no loss in prediction accuracy.\n# 4.3 Q3: Time Reduction of Balanced Scheduling\nTo evaluate the effectiveness of the proposed BPS algorithm, we run the following experiments by varying: (i) the size (n) and the dimension (d) of the datasets, (ii) the number of estimators (m) and (iii) the number of CPU cores (t). Due to the space limit, we only show the training time comparison between the generic scheduling and BPS on Cardio, Letter, PageBlock, and Pendigits, by setting m \u2208{100, 500} and t \u2208{2, 4, 8}, consistent with the single machine setting in real-world applications.\nTable 4 shows that the proposed BPS has a clear edge over the generic scheduling mechanism (denoted as Generic in the tables) that equally splits the tasks by order. It yields a significant time reduction (denoted as % Redu in the table), which gets more remarkable if more cores are used along with large datasets. For instance, the time reduction is more\nthan 40% on PageBlock and Pendigits when 8 cores are used. This agrees with our assumption that model cost vary more drastically on large datasets given the time complexity increase non-linearly to the size\u2014the proposed BPS method is particularly helpful.\nTable 5 shows the performance of SUOD with all three modules enabled, even not all of them are always needed in practice. In total, 600 hundred randomly selected OD models from PyOD are trained and tested on 10 datasets. To simulate the \u201cworst-case scenario\u201d, we build the model pool M by randomly select OD models, which minimizes the intrinsic task load imbalance. In real-world applications, this order randomization may not be possible as discussed in \u00a73.5\u2014two adjacent models are often from the same algorithm family and more prone to scheduling imbalance. Although this setting will make the effectiveness of BPS module less impressive, we choose it to provide an empirical worst-case performance guarantee\u2014the framework should generally perform better in practice.\nSUOD consistently yields promising results even we deliberately choose the unfavored setting. Fit B and Pred B denote the fit and prediction time of the baseline setting (no compression, no approximation, generic parallel task scheduling; see \u00a72.2). In comparison, SUOD (denoted as Fit S and Pred S) brings time reduction on majority of the datasets with minor to no performance degradation. To measure the prediction performance, we measure the ROC and\n<div style=\"text-align: center;\">Table 2. Prediction ROC scores of unsupervised models (Orig) and their pseudo-supervised approximators (Appr) by the average of 10 independent trials. The better method within each pair is indicated in bold. The approximators (Appr) outperform in most cases.</div>\nindependent trials. The better method within each pair is indicated in bold. The approximators (Appr) outperform in most cases.\nDataset\nAnnthyroid\nBreastw\nCardio\nHTTP\nMNIST\nPendigits\nPima\nSatellite\nSatimage-2\nThyroid\nModel\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nABOD\n0.83\n0.71\n0.92\n0.93\n0.63\n0.53\n0.15\n0.13\n0.81\n0.79\n0.67\n0.82\n0.66\n0.70\n0.59\n0.68\n0.89\n0.99\n0.96\n0.67\nCBLOF\n0.67\n0.68\n0.96\n0.98\n0.73\n0.76\n1.00\n1.00\n0.85\n0.89\n0.93\n0.93\n0.63\n0.68\n0.72\n0.77\n1.00\n1.00\n0.92\n0.97\nFB\n0.81\n0.45\n0.34\n0.10\n0.61\n0.70\n0.34\n0.97\n0.72\n0.83\n0.39\n0.51\n0.59\n0.63\n0.53\n0.64\n0.36\n0.40\n0.83\n0.46\nkNN\n0.80\n0.79\n0.97\n0.97\n0.73\n0.75\n0.19\n0.85\n0.85\n0.86\n0.74\n0.87\n0.69\n0.71\n0.68\n0.75\n0.96\n0.99\n0.97\n0.98\nakNN\n0.81\n0.82\n0.97\n0.97\n0.67\n0.72\n0.19\n0.88\n0.84\n0.85\n0.72\n0.87\n0.69\n0.71\n0.66\n0.74\n0.95\n0.99\n0.97\n0.98\nLOF\n0.74\n0.85\n0.44\n0.45\n0.60\n0.68\n0.35\n0.75\n0.72\n0.76\n0.38\n0.47\n0.59\n0.65\n0.53\n0.66\n0.36\n0.38\n0.80\n0.95\n<div style=\"text-align: center;\">able 3. Prediction P@N scores of unsupervised models (Orig) and their pseudo-supervised approximators (Appr) by the average of 10 ndependent trials. The better method within each pair is indicated in bold. The approximators (Appr) outperform in most cases.</div>\n bold\nDataset\nAnnthyroid\nBreastw\nCardio\nHTTP\nMNIST\nPendigits\nPima\nSatellite\nSatimage-2\nThyroid\nModel\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nABOD\n0.31\n0.08\n0.80\n0.83\n0.27\n0.20\n0.00\n0.00\n0.40\n0.27\n0.05\n0.05\n0.48\n0.52\n0.41\n0.46\n0.21\n0.64\n0.36\n0.00\nCBLOF\n0.25\n0.24\n0.86\n0.90\n0.31\n0.34\n0.02\n0.01\n0.42\n0.48\n0.35\n0.36\n0.43\n0.48\n0.54\n0.57\n0.96\n0.96\n0.26\n0.38\nFB\n0.24\n0.02\n0.03\n0.07\n0.23\n0.26\n0.02\n0.04\n0.34\n0.36\n0.03\n0.07\n0.37\n0.44\n0.37\n0.42\n0.03\n0.04\n0.05\n0.02\nkNN\n0.30\n0.32\n0.89\n0.89\n0.37\n0.46\n0.03\n0.03\n0.42\n0.45\n0.08\n0.06\n0.47\n0.47\n0.49\n0.53\n0.32\n0.43\n0.33\n0.42\nAkNN\n0.30\n0.33\n0.88\n0.89\n0.34\n0.40\n0.03\n0.03\n0.41\n0.45\n0.05\n0.13\n0.48\n0.49\n0.47\n0.52\n0.25\n0.43\n0.31\n0.44\nLOF\n0.27\n0.36\n0.19\n0.35\n0.23\n0.23\n0.01\n0.03\n0.33\n0.32\n0.03\n0.08\n0.40\n0.44\n0.37\n0.42\n0.04\n0.07\n0.19\n0.25\nTable 4. Training time comparison (in seconds) between Simple scheduling and BPS against various number of OD models and workers. Percent of time reduction, Redu (%), is indicated in bold. BPS consistently outperform to Generic scheduling .\n<div style=\"text-align: center;\">Table 4. Training time comparison (in seconds) between Simple scheduling and BPS against various number of OD models and workers. Percent of time reduction, Redu (%), is indicated in bold. BPS consistently outperform to Generic scheduling .</div>\n.\nDataset\nn\nd\nm\nt\nGeneric\nBPS\nRedu (%)\nCardio\n1831\n21\n500\n2\n240.12\n221.34\n7.82\nCardio\n1831\n21\n500\n4\n185.44\n154.43\n16.72\nCardio\n1831\n21\n500\n8\n140.63\n120.02\n14.65\nCardio\n1831\n21\n1000\n2\n199.77\n185.63\n7.08\nCardio\n1831\n21\n1000\n4\n130.82\n110.60\n15.45\nCardio\n1831\n21\n1000\n8\n97.75\n73.43\n24.88\nLetter\n1600\n32\n500\n2\n111.95\n109.52\n2.17\nLetter\n1600\n32\n500\n4\n92.69\n86.24\n6.94\nLetter\n1600\n32\n500\n8\n57.21\n48.72\n14.84\nLetter\n1600\n32\n1000\n2\n224.61\n222.59\n0.90\nLetter\n1600\n32\n1000\n4\n228.08\n172.07\n24.56\nLetter\n1600\n32\n1000\n8\n109.50\n89.51\n17.80\nPageBlock\n5393\n10\n100\n2\n51.11\n35.17\n31.19\nPageBlock\n5393\n10\n100\n4\n42.49\n16.23\n61.80\nPageBlock\n5393\n10\n100\n8\n38.45\n16.97\n55.86\nPageBlock\n5393\n10\n500\n2\n197.84\n137.46\n30.52\nPageBlock\n5393\n10\n500\n4\n167.36\n76.14\n54.51\nPageBlock\n5393\n10\n500\n8\n127.08\n66.29\n47.84\nPendigits\n6870\n16\n500\n2\n351.97\n287.14\n18.42\nPendigits\n6870\n16\n500\n4\n288.51\n146.50\n49.22\nPendigits\n6870\n16\n500\n8\n180.86\n102.11\n43.33\nPendigits\n6870\n16\n1000\n2\n697.20\n561.15\n19.51\nPendigits\n6870\n16\n1000\n4\n579.70\n288.11\n50.33\nPendigits\n6870\n16\n1000\n8\n365.20\n182.32\n50.08\nP@N by averaging the base model results (denoted as Avg ) and the maximum of average of the base models (denoted as MOA ), a widely used two-phase outlier score combination framework (Aggarwal & Sathe, 2017). Surprisingly, SUOD even leads to small performance boost in scoring new samples on most of the datasets (Annthyroid, Cardio, MNIST, Optdigits, Pendigits, and Thyroid). This performance gain may be jointly credited to the regularization effect by the randomness injected in JL projection (\u00a73.3) and\nthe pseudo-approximation (\u00a73.4)\u2014the baseline setting may be overfitted on certain datasets. It is noted that SUOD leads to more improvement on high-dimensional, large datasets. For instance, the fit time is significantly reduced on Shuttle (more than 50%). On the contrary, SUOD is less meaningful for small datasets like Pima and Cardio, although they may also yield performance improvement. Again, our settings mimics the worst case scenario for SUOD (the model order is already randomly shuffled) but still observe a great performance improvement; real-world applications should generally expect more significant results.\n# 4.5 Real-World Deployment: Fraudulent Medical Claim Analysis at IQVIA\nEstimated by the United States Government Accountability Office and Federal Bureau of Investigation, healthcare frauds cost American taxpayers tens of billions dollars a year (Aldrich et al., 2014; Bagdoyan, 2018). Detecting fraudulent medical claims is crucial for taxpayers, pharmaceutical companies and insurance companies. To further demonstrate SUOD\u2019s performance on industry data, we deploy it on a proprietary pharmacy claim dataset owned by IQVIA (a leading healthcare firm) consisting of 123,720 medical claims among which 19,033 (15.38%) are labeled as fraudulent. In each of the claim, there are 35 features including information such as drug brand, copay amount, insurance details, location and pharmacy/patient demographics. The current system in use is based on a group of selected detection models in PyOD, and an averaging method is applied on top of the base model results as the initial result. The cases marked as high risk are then transferred to human investigators in special investigation unit (SIU) for verification. It is important to provide prompt and accurate first-round screening for SIU, which leads to huge expense save.\n<div style=\"text-align: center;\">Table 5. Comparison between the baseline (denoted as B) and SUOD (denoted as S) regarding time cost, and prediction accuracy (ROC and P@N). The better method within each pair is indicated in bold (Optdigits fail to yield meaningful P@N). SUOD generally brings time reduction with no loss in prediction accuracy on majority of datasets.</div>\nime reduction with no loss in prediction accuracy on majority of datasets.\nData Information\nTime Cost (in seconds)\nEnsemble Model Performance (ROC)\nEnsemble Model Performance (P@N)\nDataset\nn\nd\nt\nFit B\nFit S\nPred B\nPred S\nAvg B\nAvg S\nMOA B\nMOA S\nAvg B\nAvg S\nMOA B\nMOA S\nAnnthyroid\n7200\n6\n5\n73.91\n65.23\n47.48\n44.26\n0.91\n0.93\n0.91\n0.93\n0.46\n0.54\n0.46\n0.55\nAnnthyroid\n7200\n6\n10\n71.00\n42.94\n44.68\n38.66\n0.91\n0.93\n0.92\n0.93\n0.46\n0.54\n0.46\n0.54\nAnnthyroid\n7200\n6\n30\n42.80\n33.98\n30.92\n25.67\n0.91\n0.93\n0.92\n0.93\n0.46\n0.54\n0.46\n0.54\nCardio\n1831\n21\n5\n78.84\n79.70\n46.09\n46.68\n0.91\n0.93\n0.91\n0.93\n0.46\n0.54\n0.45\n0.55\nCardio\n1831\n21\n10\n72.04\n53.43\n44.57\n38.31\n0.91\n0.93\n0.91\n0.93\n0.46\n0.54\n0.46\n0.54\nCardio\n1831\n21\n30\n47.53\n44.57\n31.31\n31.43\n0.91\n0.93\n0.92\n0.93\n0.46\n0.54\n0.46\n0.55\nMNIST\n7603\n100\n5\n856.53\n748.40\n453.39\n324.76\n0.77\n0.81\n0.77\n0.81\n0.29\n0.35\n0.28\n0.34\nMNIST\n7603\n100\n10\n726.76\n573.66\n367.85\n328.95\n0.78\n0.81\n0.78\n0.81\n0.29\n0.35\n0.30\n0.34\nMNIST\n7603\n100\n30\n357.40\n329.71\n260.80\n134.08\n0.78\n0.81\n0.78\n0.81\n0.29\n0.35\n0.29\n0.34\nOptdigits\n5216\n64\n5\n295.38\n267.71\n162.28\n149.19\n0.73\n0.75\n0.75\n0.77\n0.00\n0.00\n0.00\n0.00\nOptdigits\n5216\n64\n10\n247.24\n224.82\n136.12\n125.54\n0.73\n0.75\n0.74\n0.75\n0.00\n0.00\n0.00\n0.00\nOptdigits\n5216\n64\n30\n825.23\n791.95\n110.06\n62.63\n0.73\n0.75\n0.73\n0.76\n0.00\n0.00\n0.00\n0.00\nPendigits\n6870\n16\n5\n287.75\n282.25\n184.20\n158.26\n0.92\n0.95\n0.92\n0.94\n0.19\n0.23\n0.19\n0.20\nPendigits\n6870\n16\n10\n281.49\n155.06\n179.83\n160.94\n0.92\n0.95\n0.92\n0.94\n0.19\n0.25\n0.19\n0.23\nPendigits\n6870\n16\n30\n149.93\n145.59\n104.25\n89.85\n0.92\n0.94\n0.93\n0.94\n0.19\n0.25\n0.19\n0.22\nPima\n768\n8\n5\n28.72\n31.94\n21.16\n23.79\n0.71\n0.71\n0.71\n0.70\n0.51\n0.51\n0.53\n0.51\nPima\n768\n8\n10\n27.38\n20.15\n20.81\n25.03\n0.71\n0.70\n0.71\n0.70\n0.51\n0.51\n0.51\n0.51\nPima\n768\n8\n30\n19.36\n17.89\n13.83\n17.43\n0.71\n0.70\n0.71\n0.70\n0.51\n0.50\n0.52\n0.50\nShuttle\n49097\n9\n5\n3326.54\n1453.93\n2257.50\n1956.12\n0.99\n0.99\n0.99\n0.99\n0.95\n0.95\n0.95\n0.95\nShuttle\n49097\n9\n10\n2437.10\n1396.21\n1549.97\n1321.16\n0.99\n0.99\n0.99\n0.99\n0.95\n0.95\n0.95\n0.95\nShuttle\n49097\n9\n30\n1378.29\n1258.69\n837.41\n651.00\n0.99\n0.99\n0.99\n0.99\n0.95\n0.95\n0.95\n0.95\nSpamSpace\n4207\n57\n5\n247.98\n244.39\n130.95\n110.08\n0.57\n0.56\n0.56\n0.56\n0.45\n0.45\n0.46\n0.45\nSpamSpace\n4207\n57\n10\n233.39\n186.91\n128.24\n115.83\n0.57\n0.56\n0.56\n0.56\n0.46\n0.45\n0.46\n0.46\nSpamSpace\n4207\n57\n30\n604.00\n538.91\n70.19\n61.38\n0.57\n0.56\n0.57\n0.56\n0.46\n0.46\n0.46\n0.45\nThyroid\n3772\n6\n5\n87.90\n71.34\n49.51\n48.20\n0.91\n0.93\n0.91\n0.93\n0.46\n0.54\n0.46\n0.55\nThyroid\n3772\n6\n10\n74.76\n46.91\n44.81\n38.60\n0.91\n0.93\n0.91\n0.93\n0.46\n0.54\n0.46\n0.54\nThyroid\n3772\n6\n30\n45.84\n43.86\n28.90\n26.75\n0.91\n0.93\n0.92\n0.93\n0.46\n0.54\n0.46\n0.54\nWaveform\n3443\n21\n5\n167.98\n147.00\n109.94\n94.46\n0.78\n0.76\n0.78\n0.76\n0.11\n0.13\n0.11\n0.13\nWaveform\n3443\n21\n10\n154.72\n94.36\n91.69\n55.17\n0.78\n0.76\n0.78\n0.77\n0.11\n0.11\n0.11\n0.11\nWaveform\n3443\n21\n30\n97.11\n95.77\n53.47\n48.04\n0.78\n0.76\n0.78\n0.76\n0.11\n0.13\n0.11\n0.13\nSUOD is applied on top of the aforementioned dataset (74,220 records are used for training and 49,500 records are set aside for validation). Similarly to the full framework evaluation in \u00a74.4, the new system with SUOD (all three modules enabled) is compared with the current distributed system on 10 cores. The fit time is reduced from 6232.54 seconds to 4202.30 seconds (32.57% reduction), and the prediction time is reduced from 3723.45 seconds reduced to 2814.92 seconds (24.40%). In addition to the time reduction, ROC and P@N also show improvements at 3.59% and 7.46%, respectively. Through this case, we are confident the proposed framework can be useful for many real-world applications for scalable outlier detection.\n# 5 CONCLUSION & FUTURE DIRECTIONS\nIn this work, we propose SUOD to expedite the training and prediction of a large number of unsupervised heterogeneous outlier detection models. It consists of three modules with focus on different levels (data, model, execution): (i) Random Projection module compresses data into lowdimensional subspaces to alleviate the curse of dimensionality; (ii) Pseudo-supervised Approximation module could accelerate costly unsupervised models\u2019 prediction by replacing them by faster supervised regressors, which also brings\nthe extra benefit, e.g., interpretability and (iii) Balanced Parallel Scheduling module ensures that nearly equal amount of workload is assigned to available workers in distributed computing. The extensive experiments on more than 20 benchmark datasets and a real-world claim fraud analysis case show the great potential of SUOD, and many intriguing results are observed. For reproducibility and accessibility, all code, figures, and datasets are openly shared1. More investigations are currently underway. First, we plan to demonstrate SUOD\u2019s effectiveness as an end-to-end framework on more complex downstream combination models like unsupervised LSCP (Zhao et al., 2019a) and supervised XGBOD (Zhao & Hryniewicki, 2018). Second, we would further emphasize the interpretability provided by the pseudo-supervised approximation, which can be beyond feature importance provided in tree regressors. Third, there is room to investigate why and how the pseudo-supervised approximation could work in a more strict and theoretical way. This study, as the first step, empirically shows that proximity-based models can benefit from the approximation. , whereas linear models may not. Lastly, we may incorporate the emerging automated OD, e.g., MetaOD (Zhao et al., 2020), to trim down the model space for further acceleration.\n# REFERENCES\nsigmod/sigmod2000.html#BreunigKNS00. SIGMOD Record 29(2), June 2000.\n# Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531.\nu, X., Rudin, C., and Seltzer, M. I. Optimal sparse decision trees. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00b4e-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 7265\u2013 7273, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/ ac52c626afc10d4075708ac4c778ddfc-Abstra html.\nJohnson, W. B. and Lindenstrauss, J. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.\nKeller, F., M\u00a8uller, E., and B\u00a8ohm, K. Hics: High contrast subspaces for density-based outlier ranking. In Kementsietsidis, A. and Salles, M. A. V. (eds.), IEEE 28th International Conference on Data Engineering (ICDE\n2012), Washington, DC, USA (Arlington, Virginia), 15 April, 2012, pp. 1037\u20131048. IEEE Computer Society, 2012. doi: 10.1109/ICDE.2012.88. URL https: //doi.org/10.1109/ICDE.2012.88.\nURL https://doi.org/10.1109/ICDM50108. 2020.00135. Liu, F. T., Ting, K. M., and Zhou, Z. Isolation forest. In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM 2008), December 15-19, 2008, Pisa, Italy, pp. 413\u2013422. IEEE Computer Society, 2008. doi: 10.1109/ICDM.2008.17. URL https://doi.org/ 10.1109/ICDM.2008.17. Liu, Y., Li, Z., Zhou, C., Jiang, Y., Sun, J., Wang, M., and He, X. Generative adversarial active learning for unsupervised outlier detection. IEEE Trans. Knowl. Data Eng., 32(8):1517\u20131528, 2020. doi: 10.1109/ TKDE.2019.2905606. URL https://doi.org/10. 1109/TKDE.2019.2905606. Lozano, E. and Acu\u02dcna, E. Parallel algorithms for distancebased and density-based outliers. In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), 27-30 November 2005, Houston, Texas, USA, pp. 729\u2013732. IEEE Computer Society, 2005. doi: 10.1109/ICDM.2005.116. URL https://doi.org/ 10.1109/ICDM.2005.116. Lu, W., Cheng, Y., Xiao, C., Chang, S., Huang, S., Liang, B., and Huang, T. S. Unsupervised sequential outlier detection with deep architectures. IEEE Trans. Image Process., 26(9):4321\u20134330, 2017. doi: 10.1109/TIP.2017. 2713048. URL https://doi.org/10.1109/TIP. 2017.2713048. Nakandala, S., Saur, K., Yu, G., Karanasos, K., Curino, C., Weimer, M., and Interlandi, M. A tensor compiler for unified machine learning prediction serving. In 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020, Virtual Event, November 4-6, 2020, pp. 899\u2013917. USENIX Association, 2020. URL https://www.usenix.org/conference/ osdi20/presentation/nakandala. Oku, J., Tamura, K., and Kitakami, H. Parallel processing for distance-based outlier detection on a multi-core cpu. In IEEE International Workshop on Computational Intelligence and Applications (IWCIA), pp. 65\u201370. IEEE, 2014. Ousterhout, K., Wendell, P., Zaharia, M., and Stoica, I. Sparrow: distributed, low latency scheduling. In Kaminsky, M. and Dahlin, M. (eds.), ACM SIGOPS 24th Symposium on Operating Systems Principles, SOSP \u201913, Farmington, PA, USA, November 3-6, 2013, pp. 69\u201384. ACM, 2013. doi: 10.1145/2517349.2522716. URL https: //doi.org/10.1145/2517349.2522716. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\n5 April, 2012, pp. 1037\u20131048. IEEE Computer Society, 2012. doi: 10.1109/ICDE.2012.88. URL https: //doi.org/10.1109/ICDE.2012.88. Kriegel, H., Kr\u00a8oger, P., Schubert, E., and Zimek, A. Loop: local outlier probabilities. In Cheung, D. W., Song, I., Chu, W. W., Hu, X., and Lin, J. J. (eds.), Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009, pp. 1649\u20131652. ACM, 2009. doi: 10.1145/1645953.1646195. URL https: //doi.org/10.1145/1645953.1646195. Kriegel, H.-P., Schubert, M., and Zimek, A. Anglebased outlier detection in high-dimensional data. In Li, Y., Liu, B., and Sarawagi, S. (eds.), KDD, pp. 444\u2013452. ACM, 2008. ISBN 978-1-60558-193-4. URL http://dblp.uni-trier.de/db/conf/ kdd/kdd2008.html#KriegelSZ08. Lai, K., Zha, D., Wang, G., Xu, J., Zhao, Y., Kumar, D., Chen, Y., Zumkhawaka, P., Wan, M., Martinez, D., and Hu, X. TODS: an automated time series outlier detection system. CoRR, abs/2009.09822, 2020. URL https: //arxiv.org/abs/2009.09822. Lazarevic, A. and Kumar, V. Feature bagging for outlier detection. In Grossman, R., Bayardo, R. J., and Bennett, K. P. (eds.), Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Illinois, USA, August 21-24, 2005, pp. 157\u2013166. ACM, 2005. doi: 10. 1145/1081870.1081891. URL https://doi.org/ 10.1145/1081870.1081891. Lazarevic, A., Ert\u00a8oz, L., Kumar, V., Ozgur, A., and Srivastava, J. A comparative study of anomaly detection schemes in network intrusion detection. In Barbar\u00b4a, D. and Kamath, C. (eds.), Proceedings of the Third SIAM International Conference on Data Mining, San Francisco, CA, USA, May 1-3, 2003, pp. 25\u201336. SIAM, 2003. doi: 10.1137/1.9781611972733.3. URL https: //doi.org/10.1137/1.9781611972733.3. Li, W., Wang, Y., Cai, Y., Arnold, C. W., Zhao, E., and Yuan, Y. Semi-supervised rare disease detection using generative adversarial network. CoRR, abs/1812.00547, 2018. URL http://arxiv.org/abs/1812.00547. Li, Z., Zhao, Y., Botta, N., Ionescu, C., and Hu, X. COPOD: copula-based outlier detection. In Plant, C., Wang, H., Cuzzocrea, A., Zaniolo, C., and Wu, X. (eds.), 20th IEEE International Conference on Data Mining, ICDM 2020, Sorrento, Italy, November 17-20, 2020, pp. 1118\u20131123. IEEE, 2020. doi: 10.1109/ICDM50108.2020.00135.\nLazarevic, A., Ert\u00a8oz, L., Kumar, V., Ozgur, A., and Srivastava, J. A comparative study of anomaly detection schemes in network intrusion detection. In Barbar\u00b4a, D. and Kamath, C. (eds.), Proceedings of the Third SIAM International Conference on Data Mining, San Francisco, CA, USA, May 1-3, 2003, pp. 25\u201336. SIAM, 2003. doi: 10.1137/1.9781611972733.3. URL https: //doi.org/10.1137/1.9781611972733.3.\nLi, W., Wang, Y., Cai, Y., Arnold, C. W., Zhao, E., and Yuan, Y. Semi-supervised rare disease detection using generative adversarial network. CoRR, abs/1812.00547, 2018. URL http://arxiv.org/abs/1812.00547.\nLi, Z., Zhao, Y., Botta, N., Ionescu, C., and Hu, X. COPOD: copula-based outlier detection. In Plant, C., Wang, H., Cuzzocrea, A., Zaniolo, C., and Wu, X. (eds.), 20th IEEE International Conference on Data Mining, ICDM 2020, Sorrento, Italy, November 17-20, 2020, pp. 1118\u20131123. IEEE, 2020. doi: 10.1109/ICDM50108.2020.00135.\nURL https://doi.org/10.1109/ICDM50108. 2020.00135. Liu, F. T., Ting, K. M., and Zhou, Z. Isolation forest. In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM 2008), December 15-19, 2008, Pisa, Italy, pp. 413\u2013422. IEEE Computer Society, 2008. doi: 10.1109/ICDM.2008.17. URL https://doi.org/ 10.1109/ICDM.2008.17. Liu, Y., Li, Z., Zhou, C., Jiang, Y., Sun, J., Wang, M., and He, X. Generative adversarial active learning for unsupervised outlier detection. IEEE Trans. Knowl. Data Eng., 32(8):1517\u20131528, 2020. doi: 10.1109/ TKDE.2019.2905606. URL https://doi.org/10. 1109/TKDE.2019.2905606. Lozano, E. and Acu\u02dcna, E. Parallel algorithms for distancebased and density-based outliers. In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), 27-30 November 2005, Houston, Texas, USA, pp. 729\u2013732. IEEE Computer Society, 2005. doi: 10.1109/ICDM.2005.116. URL https://doi.org/ 10.1109/ICDM.2005.116. Lu, W., Cheng, Y., Xiao, C., Chang, S., Huang, S., Liang, B., and Huang, T. S. Unsupervised sequential outlier detection with deep architectures. IEEE Trans. Image Process., 26(9):4321\u20134330, 2017. doi: 10.1109/TIP.2017. 2713048. URL https://doi.org/10.1109/TIP. 2017.2713048. Nakandala, S., Saur, K., Yu, G., Karanasos, K., Curino, C., Weimer, M., and Interlandi, M. A tensor compiler for unified machine learning prediction serving. In 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020, Virtual Event, November 4-6, 2020, pp. 899\u2013917. USENIX Association, 2020. URL https://www.usenix.org/conference/ osdi20/presentation/nakandala. Oku, J., Tamura, K., and Kitakami, H. Parallel processing for distance-based outlier detection on a multi-core cpu. In IEEE International Workshop on Computational Intelligence and Applications (IWCIA), pp. 65\u201370. IEEE, 2014. Ousterhout, K., Wendell, P., Zaharia, M., and Stoica, I. Sparrow: distributed, low latency scheduling. In Kaminsky, M. and Dahlin, M. (eds.), ACM SIGOPS 24th Symposium on Operating Systems Principles, SOSP \u201913, Farmington, PA, USA, November 3-6, 2013, pp. 69\u201384. ACM, 2013. doi: 10.1145/2517349.2522716. URL https: //doi.org/10.1145/2517349.2522716. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., VanderPlas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in python. J. Mach. Learn. Res., 12:2825\u20132830, 2011. URL http: //dl.acm.org/citation.cfm?id=2078195. Ramaswamy, S., Rastogi, R., and Shim, K. Efficient algorithms for mining outliers from large data sets. In Chen, W., Naughton, J. F., and Bernstein, P. A. (eds.), Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, May 16-18, 2000, Dallas, Texas, USA, pp. 427\u2013438. ACM, 2000. doi: 10.1145/342009.335437. URL https://doi.org/ 10.1145/342009.335437. Rayana, S. and Akoglu, L. Less is more: Building selective anomaly ensembles. ACM Trans. Knowl. Discov. Data, 10(4):42:1\u201342:33, 2016. URL http://dblp.uni-trier.de/db/journals/ tkdd/tkdd10.html#RayanaA16.\nchubert, E., Zimek, A., and Kriegel, H. Fast and scalable outlier detection with approximate nearest neighbor ensembles. In Renz, M., Shahabi, C., Zhou, X., and Cheema, M. A. (eds.), Database Systems for Advanced Applications - 20th International Conference, DASFAA 2015, Hanoi, Vietnam, April 20-23, 2015, Proceedings, Part II, volume 9050 of Lecture Notes in Computer Science, pp. 19\u201336. Springer, 2015. doi: 10.1007/978-3-319-18123-3\\ 2. URL https://doi. org/10.1007/978-3-319-18123-3_2.\nSch\u00a8olkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., and Williamson, R. C. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443\u20131471, July 2001. doi: 10.1162/ 089976601750264965. URL https://doi.org/10. 1162%2F089976601750264965.\nSpearman, C. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72\u2013101, 1904. ISSN 00029556. URL http:// www.jstor.org/stable/1412159.\nVenkatasubramanian, S. and Wang, Q. The johnsonlindenstrauss transform: An empirical study. In M\u00a8ullerHannemann, M. and Werneck, R. F. F. (eds.), Proceedings of the Thirteenth Workshop on Algorithm Engineering and Experiments, ALENEX 2011, Holiday Inn San Francisco Golden Gateway, San Francisco, California, USA, January 22, 2011, pp. 164\u2013173. SIAM, 2011. doi: 10.1137/1.9781611972917.16",
    "paper_type": "method",
    "attri": {
        "background": "Outlier detection (OD) is a key machine learning task for identifying abnormal objects from general samples, used in applications like fraud detection and intrusion detection. Previous methods have struggled with the computational expense of training and predicting with numerous unsupervised, heterogeneous models, necessitating a new approach for efficiency.",
        "problem": {
            "definition": "The problem addressed is the computational inefficiency in training and predicting with a large number of unsupervised heterogeneous outlier detection models on high-dimensional datasets.",
            "key obstacle": "Existing methods are slow and often fail under high-dimensional settings, leading to limited practical applicability."
        },
        "idea": {
            "intuition": "The idea originated from the need to enhance the efficiency of outlier detection models, inspired by the challenges faced in computational scalability.",
            "opinion": "The proposed idea, SUOD, is a modular acceleration system that focuses on data reduction, model approximation, and balanced scheduling to improve performance in outlier detection.",
            "innovation": "SUOD innovatively combines three complementary acceleration strategies that address data, model, and execution levels, setting it apart from existing methods."
        },
        "method": {
            "method name": "SUOD",
            "method abbreviation": "SUOD",
            "method definition": "SUOD is a comprehensive framework designed to accelerate the training and scoring of unsupervised heterogeneous outlier detection models.",
            "method description": "It integrates data reduction, model approximation, and balanced parallel scheduling to enhance efficiency.",
            "method steps": [
                "Randomly project high-dimensional data into lower-dimensional subspaces.",
                "Use pseudo-supervised models to approximate costly unsupervised outlier detectors.",
                "Implement a balanced scheduling mechanism for distributing model training across available workers."
            ],
            "principle": "The effectiveness of SUOD lies in its ability to maintain performance accuracy while significantly reducing computational costs through its modular approach."
        },
        "experiments": {
            "evaluation setting": "Extensive experiments were conducted on more than 20 benchmark datasets, comparing the performance of SUOD with traditional methods and demonstrating its effectiveness in heterogeneous outlier detection acceleration.",
            "evaluation method": "Performance was measured through execution time and prediction accuracy, utilizing metrics such as ROC and P@N."
        },
        "conclusion": "The experiments confirmed that SUOD significantly reduces execution time while maintaining or improving prediction accuracy, showcasing its potential for scalable outlier detection in real-world applications.",
        "discussion": {
            "advantage": "SUOD stands out due to its comprehensive approach, effectively addressing scalability issues in outlier detection through modular enhancements.",
            "limitation": "The method may not perform optimally for all types of outlier detection algorithms, particularly those that are inherently efficient.",
            "future work": "Future research will explore the integration of SUOD with more complex models and further investigate the theoretical foundations of the pseudo-supervised approximation."
        },
        "other info": {
            "info1": "SUOD is open-sourced for reproducibility and accessibility.",
            "info2": {
                "info2.1": "The framework has been implemented in the leading outlier detection library PyOD.",
                "info2.2": "Real-world application demonstrated in fraudulent claim analysis at IQVIA."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Outlier detection (OD) is a key machine learning task for identifying abnormal objects from general samples, used in applications like fraud detection and intrusion detection."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed is the computational inefficiency in training and predicting with a large number of unsupervised heterogeneous outlier detection models on high-dimensional datasets."
        },
        {
            "section number": "3.5",
            "key information": "The proposed idea, SUOD, is a modular acceleration system that focuses on data reduction, model approximation, and balanced scheduling to improve performance in outlier detection."
        },
        {
            "section number": "6.1",
            "key information": "The effectiveness of SUOD lies in its ability to maintain performance accuracy while significantly reducing computational costs through its modular approach."
        },
        {
            "section number": "7.1",
            "key information": "Existing methods are slow and often fail under high-dimensional settings, leading to limited practical applicability."
        },
        {
            "section number": "7.2",
            "key information": "Future research will explore the integration of SUOD with more complex models and further investigate the theoretical foundations of the pseudo-supervised approximation."
        }
    ],
    "similarity_score": 0.6762065070503085,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/SUOD_ Accelerating Large-Scale Unsupervised Heterogeneous Outlier Detection.json"
}