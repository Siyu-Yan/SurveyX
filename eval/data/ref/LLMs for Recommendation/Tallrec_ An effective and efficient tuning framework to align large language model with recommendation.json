{
    "from": "google",
    "scholar_id": "6W9QlDtFL7MJ",
    "detail_id": null,
    "title": "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendations, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec.",
    "bib_name": "bao2023tallrec",
    "md_text": "# TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation\nKeqin Bao* baokq@mail.ustc.edu.cn University of Science and Technology of China China Jizhi Zhang* cdzhangjizhi@mail.ustc.edu.cn University of Science and Technology of China China Yang Z zy2015@mai University of Scien of Ch Chi\nJizhi Zhang* gjizhi@mail.ust\nng mail.com  Singapore Fuli Feng\u2020 fulifeng93@gmail.com University of Science and Technology of China China Xiangnan He\u2020 xiangnanhe@gmail.com University of Science and Technology of China China\n# ABSTRACT\nLarge Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendations, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec.\narXiv:2305.00447v3\narXiv:2305.00\n# CCS CONCEPTS\n# \u2022 Information systems \u2192Recommender systems.\nKEYWORDS\nRecommendation, Instruction Tuning, Large Language Models\n*The two authors contributed equally to this work and are listed alphabetically. \u2020The corresponding author.\n*The two authors contributed equally to this work and are listed alphabetically. \u2020Th corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0241-9/23/09...$15.00 https://doi.org/10.1145/3604915.3608857\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0241-9/23/09...$15.00 https://doi.org/10.1145/3604915.3608857\nACM Reference Format: Keqin Bao*, Jizhi Zhang*, Yang Zhang, Wenjie Wang, Fuli Feng\u2020, and Xiangnan He\u2020. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In Seventeenth ACM Conference on Recommender Systems (RecSys \u201923), September 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 8 pages. https: //doi.org/10.1145/3604915.3608857\n# 1 INTRODUCTION\nLarge Language Models (LLMs) have exhibited remarkable proficiency in generating text that closely resembles human language and in performing a wide range of tasks [69], including Natural Language Processing [4, 32, 54], Robotics [10, 43, 56], and Information Retrieval [1, 24, 25, 48]. Prior research has also demonstrated the knowledge-rich and compositional generalization capabilities of LLMs [36, 41, 53]. Only given appropriate instructions, these models are able to learn how to solve unseen tasks and inspire their own knowledge to achieve a high level of performance [33]. The aforementioned capabilities of LLM present promising opportunities to address the current challenges requiring strong generalization and rich knowledge in the recommendation field. In this light, it is valuable to explore the integration of LLMs into recommender systems, which has received limited attention in prior research. In recent initial attempts [13, 47], achieving the target relies on In-context Learning [3], which is typically implemented through the official OpenAI API [2]. They regard the LLM as a toolformer [42] of traditional recommendation models (such as MF [27] and LightGCN [16]), i.e., the LLM is used for re-ranking the candidate items filtered by these models. However, these approaches only reach a comparable performance with traditional models [13, 47]. Worse still, using only In-context Learning may fail to make recommendations. As shown in Figure 1, we find that ChatGPT either refuses to answer or always gives positive predictions (likes). Therefore, it is critical to further explore an appropriate way for more effective leverage of LLMs in the recommendation. We postulate that the failure of using only In-context Learning is because of two reasons: 1) LLMs may not align well with the recommendation task due to the huge gap between language processing tasks for training LLMs and recommendation. Besides, the recommendation-oriented corpus is very limited during the training phase of LLMs. 2) The effect of LLMs is restricted by the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1aef/1aefd66c-cb9a-47b7-944f-dc5bb4fadfbb.png\" style=\"width: 50%;\"></div>\nFigure 1: Illustration of LLMs for the recommendations. Given users\u2019 interaction history, LLMs predict whether a user will like a new item through In-context Learning. However, the representative LLMs, e.g., ChatGPT, either refuse to answer or always give positive predictions (likes) on Movie and Book recommendation tasks. If we ignore the refused answers and calculate AUC on the remaining samples, we find that LLMs perform similarly with random guessing (AUC=0.5). Refer to Section 3 for more experimental details.\nunderlying recommendation models, which may fail to include target items in their candidate lists due to their limited capacity. Therefore, we consider building a Large Recommendation Language Model (LRLM) to bridge the gap between LLMs and the recommendation task and better stimulate the recommendation capabilities of LLMs in addition to In-context Learning. Toward this goal, we focus on tuning LLMs with the recommendation task. Considering that instruction tuning is core to letting the LLM learn to solve different tasks and have strong generalization ability [22, 23, 37], we propose a lightweight tuning framework to adapt LLMs for recommendations, named TALLRec. Elaborately, TALLRec structures the recommendation data as instructions and tunes the LLM via an additional instruction tuning process. Moreover, given that LLM training necessitates a substantial amount of computing resources, TALLRec employs a lightweight tuning approach to efficiently adapt the LLMs to the recommendation task. Specifically, we apply the TALLRec framework on the LLaMA7B model [46] with a LoRA [21] architecture, which ensures the framework can be deployed on an Nvidia RTX 3090 (24GB) GPU. Furthermore, to investigate the minimal computational resources required, we do experiments in a few-shot setting, utilizing only a limited number of tuning examples. We conduct detailed experiments in knowledge-rich recommendation scenarios of movies and books, where the tuned LLaMA-7B model outperforms traditional recommendation models and In-context Learning with GPT3.5, a much stronger LLM than LLaMA-7B. The results validate the efficiency and robustness of our framework: 1) our TALLRec framework can quickly inspire the recommendation capability of LLMs in the few-shot setting. and 2) LLMs trained via the TALLRec framework have a strong generalization ability across different domains (e.g., movie \u2192book). In total, our contributions are summarized as follows:\n\u2022 We study a new problem in recommendation \u2014 aligning the LLMs with the recommendation, where we reveal the\n<div style=\"text-align: center;\">Table 1: A tuning sample for a translation task.</div>\nInstruction Input\nTask Instruction:\nTranslate from English to Chinese.\nTask Input:\nWho am I ?\nInstruction Output\nTask Output:\n\u6211\u662f\u8c01?\nlimitations of In-context Learning-based approaches and underscore the significance of instruction tuning. \u2022 We introduce a new TALLRec framework to build Large Recommendation Language Models, which enables the effective and efficient tuning of LLMs for recommendation with low GPU costs and few tuning samples. \u2022 We conduct extensive experiments, validating the effectiveness and efficiency of the proposed framework, and uncovering its exceptional robustness with seamless navigation across different domains.\n# 2 TALLREC\nIn this section, we first introduce the preliminary knowledge for tuning LLMs and our task formulation, and then present the proposed TALLRec framework.\n# 2.1 Preliminary\n\u2022 Instruction Tuning is a crucial technique to train LLMs with human-annotated instructions and responses [36]. Generally, instruction tuning has four steps (see the example in Table 2). Specifically, Step 1: Define a task and articulate a \u201cTask Instruction\u201d using natural language, which usually encompasses a clear definition of the task, as well as specific solutions to address it. Step 2: Formulate and construct the input and output of the task in natural language, denoted as \u201cTask Input\u201d and \u201cTask Output\u201d. Step 3: Integrate the \u201cTask Instruction\u201d and \u201cTask Input\u201d together to form the \u201cInstruction\n<div style=\"text-align: center;\">Table 2: A tuning sample for rec-tuning.</div>\nInstruction Input\nTask Instruction:\nGiven the user\u2019s historical interactions, please determine\nwhether the user will enjoy the target new movie by\nanswering \"Yes\" or \"No\".\nTask Input:\nUser\u2019s liked items: GodFather.\nUser\u2019s disliked items: Star Wars.\nTarget new movie: Iron Man\nInstruction Output\nTask Output:\nNo.\nInput\u201d, and take the \u201cTask Output\u201d as the corresponding \u201cInstruction Output\u201d, for each tuning sample. Step 4: Instruction tuning on LLMs based on the formatted pairs of \u201cInstruction Input\u201d and \u201cInstruction Output\u201d. \u2022 Rec-tuning Task Formulation. We aim to utilize LLM, denoted as M, to construct an LRLM, which can predict whether a new item will be enjoyed by a user. To achieve this objective, we do recommendation tuning (rec-tuning) on LLMs with recommendation data. As shown in Table 2, we format recommendation data into a pattern of instruction tuning. We begin by composing a \u201cTask Instruction\u201d that directs the model to determine whether the user will like the target item based on their historical interactions, and to respond with a binary answer of \u201cYes\u201d or \u201cNo\u201d. To format the \u201cTask Input\u201d, we categorize the user\u2019s historically interacted items into two groups based on ratings: the user\u2019s liked items and disliked items, where items are sequentially ranked by interaction time and represented by textual descriptions (e.g., title and brief introduction). Besides, \u201cTask Input\u201d also includes a target new item that the user has never seen. Lastly, we merge \u201cTask Instruction\u201d and \u201cTask Input\u201d to create a \u201cInstruction Input\u201d, and then set the expected \u201cInstruction Output\u201d as \u2018Yes\u201d or \u201cNo\u201d for rec-tuning.\n# 2.2 TALLRec Framework\nIn this subsection, we introduce the TALLRec framework, which aims to facilitate the effective and efficient alignment of LLMs with recommendation tasks, particularly in low GPU memory consumption settings. Specifically, we first present two TALLRec tuning stages with lightweight implementation, followed by the backbone selection. As shown in Figure 2, TALLRec comprises two tuning stages: alpaca tuning and rec-tuning. The former stage is the common training process of LLM that enhances LLM\u2019s generalization ability, while the latter stage emulates the pattern of instruction tuning and tunes LLMs for the recommendation task. \u2022 TALLRec Tuning Stages. For alpaca tuning, we employ the self-instruct data made available by Alpaca [45] to train the LLM. Specifically, we utilize the conditional language modeling objective during the alpaca tuning, as exemplified in the Alpaca repository1.\nIn this subsection, we introduce the TALLRec framework, which aims to facilitate the effective and efficient alignment of LLMs with recommendation tasks, particularly in low GPU memory consumption settings. Specifically, we first present two TALLRec tuning stages with lightweight implementation, followed by the backbone selection. As shown in Figure 2, TALLRec comprises two tuning stages: alpaca tuning and rec-tuning. The former stage is the common training process of LLM that enhances LLM\u2019s generalization ability, while the latter stage emulates the pattern of instruction tuning and tunes LLMs for the recommendation task.\n\u2022 TALLRec Tuning Stages. For alpaca tuning, we employ the self-instruct data made available by Alpaca [45] to train the LLM. Specifically, we utilize the conditional language modeling objective during the alpaca tuning, as exemplified in the Alpaca repository1.\n1https://github.com/tloen/alpaca-lora.\nFormally,\nwhere \ud835\udc65and \ud835\udc66represent the \u201cInstruction Input\u201d and \u201cInstruction Output\u201d in the self-instruct data, respectively, \ud835\udc66\ud835\udc61is the \ud835\udc61-th token of the \ud835\udc66, \ud835\udc66<\ud835\udc61represents the tokens before \ud835\udc66\ud835\udc61, \u03a6 is the original parameters of M, and Z is the training set. For rec-tuning, we can leverage the rec-tuning sample as described in Table 2 to tune the LLM, similar to alpaca tuning.\n\u2022 Lightweight Tuning. However, directly tuning the LLM is computationally intensive and time-consuming. As such, we propose to adopt a lightweight tuning strategy to execute both alpaca tuning and rec-tuning. The central premise of lightweight tuning is that contemporary language models may possess an excessive number of parameters, and their information is concentrated on a low intrinsic dimension [21]. Consequently, we can achieve comparable performance to that of the entire model by tuning only a small subset of parameters [20, 28, 31]. Specifically, we employ LoRA [21], which involves freezing the pre-trained model parameters and introducing trainable rank decomposition matrices into each layer of the Transformer architecture to facilitate lightweight tuning. Therefore, by optimizing rank decomposition matrices, we can efficiently incorporate supplementary information while maintaining the original parameters in a frozen state. In total, the final learning objective can be computed as:\n(2)\nwhere \u0398 is the LoRA parameters and we only update LoRA parameters during the training process. Through LoRA, we can complete training with only one-thousandth of the original LLM parameters to complete the training process [21]. \u2022 Backbone Selection. At present, there are large amounts of LLMs released, such as GPT series, PaLM, CHinchilla, and LLaMA [3, 4, 18, 46]. Among these, a considerable number of LLMs (such as PaLM and Chinchilla) do not provide access to their model parameters or APIs, rendering them challenging to utilize for research or other applications. Additionally, data security concerns are significant issues in the recommendation field. Consequently, the utilization of third-party APIs (such as ChatGPT and text-davinci-003) to leverage LLMs necessitates further discussion. To replicate the issues that require consideration in real-world recommendation scenarios, we intend to simulate the practical utilization of a public LLM and update its parameters for recommendation purposes. After careful consideration, we have opted to conduct experiments using LLMsLLaMA, which is presently the best-performing open-source LLM, and whose training data is also publicly available [46].\n# 3 EXPERIMENTS\nIn this section, we conduct experiments to answer the following research questions: - RQ1: How does TALLRec perform compared with current LLMbased and traditional recommendation models?\nIn this section, we conduct experiments to answer the followin research questions:\nresearch questions: - RQ1: How does TALLRec perform compared with current LLMbased and traditional recommendation models?\n- RQ1: How does TALLRec perform compared with current LLMbased and traditional recommendation models?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a1b/0a1b920d-0a04-45bc-a020-29e42555c156.png\" style=\"width: 50%;\"></div>\nFigure 2: Illustration of the TALLRec framework constructed by alpaca tuning and rec-tuning two stages. During rec-tuning, we use the rec-tuning samples with instruction input and output constructed from recommendation data. Notably, we employ lightweight tuning technology to enhance the efficiency of our TALLRec framework.\n<div style=\"text-align: center;\">Figure 2: Illustration of the TALLRec framework constructed by alpaca tuning and rec-tuning two stages. During rec-tuning we use the rec-tuning samples with instruction input and output constructed from recommendation data. Notably, we employ lightweight tuning technology to enhance the efficiency of our TALLRec framework.</div>\n# - RQ2: How do the different components in TALLRec affect its effectiveness? - RQ3: How does TALLRec perform under cross-domain recommendation?\n- RQ2: How do the different components in TALLRec affect its effectiveness? - RQ3: How does TALLRec perform under cross-domain recommendation?\n\u2022 Dataset. We conduct experiments on two datasets. The statistics and more details can be found in our released data.\n# - Movie.\nThis is a processed dataset from MovieLens100K [12] , which comprises user ratings on movies and comprehensive textual descriptions of movies such as \u201ctitle\u201d and \u201cdirector\u201d. Because we conduct experiments in a few-shot training setting that requires limited tuning samples, we process the original dataset by sampling the most recent 10,000 interactions and split them into training, validation, and testing sets with a ratio of 8:1:1. To construct a rec-tuning sample, 10 interactions prior to the target item are retained as historical interactions. Following [16, 66], we only treat the interactions with ratings > 3 as \u201clikes\u201d, and those with ratings \u22643 as \u201cdislike\u201d. - Book. This is a book recommendation dataset processed from BookCrossing [71]. The BookCrossing dataset has user ratings (110) and textual descriptions of books, such as the information of \u2018Book-Author\u2019 and \u2018Book-Title\u2019. For each user, we randomly select an item interacted by this user as the prediction target, and sample 10 interacted items as historical interactions2. Subsequently, we partition constructed rec-tuning samples into training, validation, and testing sets with the same ratio of 8:1:1. Additionally, we binarize the ratings according to a threshold of 5.\nThis is a processed dataset from MovieLens100K [12] , which comprises user ratings on movies and comprehensive textual descriptions of movies such as \u201ctitle\u201d and \u201cdirector\u201d. Because we conduct experiments in a few-shot training setting that requires limited tuning samples, we process the original dataset by sampling the most recent 10,000 interactions and split them into training, validation, and testing sets with a ratio of 8:1:1. To construct a rec-tuning sample, 10 interactions prior to the target item are retained as historical interactions. Following [16, 66], we only treat the interactions with ratings > 3 as \u201clikes\u201d, and those with ratings \u22643 as \u201cdislike\u201d. - Book. This is a book recommendation dataset processed from BookCrossing [71]. The BookCrossing dataset has user ratings (110) and textual descriptions of books, such as the information of \u2018Book-Author\u2019 and \u2018Book-Title\u2019. For each user, we randomly select an item interacted by this user as the prediction target, and sample 10 interacted items as historical interactions2. Subsequently, we partition constructed rec-tuning samples into training, validation, and testing sets with the same ratio of 8:1:1. Additionally, we binarize the ratings according to a threshold of 5. \u2022 Few-shot Training Setting. We adopt a few-shot training setting, where only a limited number of samples are randomly selected from the training set for model training. It is referred to as \u2018\ud835\udc3e-shot\u2019 training setting, where \ud835\udc3erepresents the number of training samples used. By setting an extremely small value for K, such as 64, we could test whether a method can rapidly acquire recommendation capability from LLMs with severely limited training data. \u2022 Baseline. We compare TALLRec against both LLM-based and traditional recommendation methods. 1) Existing LLM-based methods adopt In-context Learning to directly generate recommendations [13, 47]. For a fair comparison, we align these methods with TALLRec by using the same instructions. Specifically, we perform\n\u2022 Few-shot Training Setting. We adopt a few-shot training setting, where only a limited number of samples are randomly selected from the training set for model training. It is referred to as \u2018\ud835\udc3e-shot\u2019 training setting, where \ud835\udc3erepresents the number of training samples used. By setting an extremely small value for K, such as 64, we could test whether a method can rapidly acquire recommendation capability from LLMs with severely limited training data.\n\u2022 Baseline. We compare TALLRec against both LLM-based and traditional recommendation methods. 1) Existing LLM-based methods adopt In-context Learning to directly generate recommendations [13, 47]. For a fair comparison, we align these methods with TALLRec by using the same instructions. Specifically, we perform\n2BookCrossing lacks interaction timestamps, thus we can only construct historical interaction by random sampling.\nIn-context Lerning on different LLMs: 1) Alpaca-LoRA, 2) TextDavinvi-002, 3) Text-Daviniv-003, and 4) ChatGPT. Alpaca-LoRA is a model for reproducing Alpaca results of the LLaMA model by using LoRA and alpaca tuning. The latter three are GPT series models from OpenAI. 2) Traditional methods. Since our approach utilizes historical interactions to predict the subsequent interaction, similar to the sequential recommendation, we consider the following sequential models: (i) GRU4Rec [17] is an RNN-based sequential recommender, which utilizes GRU to encode historical interactions. (ii) Caser [44] utilizes CNN to encode historical interaction sequences. (iii) SASRec [26] is a classic transformer-based sequential recommender. (iv) DROS [60] is a state-of-the-art sequential recommender model, which harnesses distributionally robust optimization for robust recommendations. We use the version implemented by GRU4Rec, provided by the authors.3 The sequential models above rely on item ID features without considering textual descriptions of items. However, in our setting, we assume item text descriptions are available for LLM tuning. To ensure fair comparisons, we further consider comparing the following variants of GRU4Rec and DROS: (v) GRU-BERT is a variant of GRU4Rec that incorporates a pre-trained BERT [7] to encode text descriptions. Specifically, BERT encodes text descriptions and outputs a CLS embedding, which is then concatenated with the original ID embeddings of GRU4Rec as the item representations. (vi) DROS-BERT is integrated with BERT, similar to GRU-BERT. \u2022 Evaluation Metric. Since TALLRec aims to predict user preference over a given target item, i.e., a binary classification problem, we adopt a popular evaluation metric used in recommendation: Area Under the Receiver Operating Characteristic (AUC).\n\u2022 Implementation Details. To ensure uniform sequence lengths, we use the user\u2019s last interacted item to pad the historical interaction sequences with lengths < the threshold, 10. For all methods, we optimize parameters using Adam with MSE loss and a learning rate of 1e-3. We search the weight decay of all methods in {1e-3, 1e-4, 1e5, 1e-6, 1e-7}. Following [60], regarding specific hyperparameters of baselines, we adhered to their original settings. For GRU-BERT and DROS-BERT, we utilize BERT released by Hugging Face4, while setting the number of GRU layers to 4 and the hidden size to 1024 for aligning with BERT\u2019s embedding size. Lastly, we run all methods\n3https://github.com/YangZhengyi98/DROS. 4https://huggingface.co/bert-base-uncased.\n<div style=\"text-align: center;\">comparison between conventional sequential recommendation baselines and TALLRec under different ings. The reported result is the AUC multiplied by 100, with boldface indicating the highest score. \u2021:</div>\n<div style=\"text-align: center;\">Table 3: Performance comparison between conventional sequential recommendation baselines and TALLRec under differ few-shot training settings. The reported result is the AUC multiplied by 100, with boldface indicating the highest score significantly better than all baselines with t-test \ud835\udc5d<0.01.</div>\nsignificantly better than all baselines with t-test \ud835\udc5d<0.01.\nFew-shot\nGRU4Rec\nCaser\nSASRec\nDROS\nGRU-BERT\nDROS-BERT\nTALLRec\nmovie\n16\n49.07\n49.68\n50.43\n50.76\n50.85\n50.21\n67.24\u2021\n64\n49.87\n51.06\n50.48\n51.54\n51.65\n51.71\n67.48\u2021\n256\n52.89\n54.20\n52.25\n54.07\n53.44\n53.94\n71.98\u2021\nbook\n16\n48.95\n49.84\n49.48\n49.28\n50.07\n50.07\n56.36\n64\n49.64\n49.72\n50.06\n49.13\n49.64\n48.98\n60.39\u2021\n256\n49.86\n49.57\n50.20\n49.13\n49.79\n50.20\n64.38\u2021\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e0b/6e0b316e-1179-4c18-98fd-0f3c7c31cfa9.png\" style=\"width: 50%;\"></div>\nFigure 3: Figure (a) shows the performance comparison between LLM-based baselines (zero-shot setting) and ours TALLRec, where the TALLRec is trained on only 64 rec-tuning samples (i.e., in the 64-shot training setting). Figure (b) demonstrates the performance tendency of TALLRec\u2019s variants and conventional sequential recommendation methods w.r.t. the number of training samples used, ranging from 1 to 256. TALLRec has three variants: \u201cAT\u201d for alpaca tuning only, \u201cRT\u201d for rec-tuning only, and \u201cTALLRec\u201d for the full version.\nfive times with different random seeds and report the averaged results.\n# 3.1 Performance Comparison (RQ1)\nWe aim to investigate the recommendation performance of various methods under the few-shot training setting, which enables us to evaluate how LLMs can be quickly adjusted for recommendation with limited rec-tuning samples. The evaluation results against traditional methods are presented in Table 3, while the comparison against LLM-based methods is depicted in Figure 3 (a). From the figure and table, we draw the following observations: 1) Our method significantly outperforms both traditional and LLMbased methods, verifying the superiority of tuning LLM via our TALLRec framework. TALLRec successfully unlocks the knowledge and generalization capabilities of LLMs for recommendations. 2) LLM-based methods perform similarly to random guessing (AUC\u22480.5). However, the LLMs trained via TALLRec achieves significant improvements. These results demonstrate a considerable gap between recommendation and language tasks, showing the importance of using recommendation data for rec-tuning on LLMs. 3) Traditional recommender methods consistently yield inferior performance under our few-shot training settings, implying that traditional methods are incapable of quickly learning the recommendation capability with limited training samples. 4) GRU-BERT and DROS-BERT do not show significant improvement over their backend models, GRU4Rec and DROS. This indicates that purely\nadding textual descriptions cannot enhance the traditional recommender models in the few-shot training setting.\n# 3.2 Ablation Study (RQ2)\nTo demonstrate the effectiveness of alpaca tuning and rec-tuning in TALLRec, we conduct ablation studies with varying \ud835\udc3eunder the \ud835\udc3e-shot training setting. Specifically, we compare the performance of TALLRec with that of two variants, \u201cAT\u201d and \u201cRT\u201d, where \u201cAT\u201d only conducts the alpaca tuning, while \u201cRT\u201d solely implements rec-tuning. By varying \ud835\udc3e, we further investigate the impact of the number of training samples. We summarize the results in Figure 3 (b), from which we have the following observations: 1) The performance of \u201cAT\u201d significantly declines compared to that of \u201cRT\u201d and TALLRec, indicating the essential effect of rec-tuning, which effectively inspires the LLM\u2019s recommendation capability 2) With limited rec-tuning samples (\u2264128), TALLRec generally outperforms \u201cRT\u201d, confirming that alpaca tuning can enhance the LLM\u2019s generalization ability on new tasks, especially when the training data in the new tasks are insufficient. As the quantity of rec-tuning samples grows, the results of TALLRec and \u201cRT\u201d become closer. This makes sense, as the significance of generalization abilities derived from other tasks diminishes when there is an ample amount of training data for the new tasks. 3) With the increase of rec-tuning sample number,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/88a6/88a6ba52-636f-46ee-b6a5-16b983acdb62.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Cross-domain generalization performance of LRLMs trained via TALLRec using Book data (TALLRec (Book)), Movie data (TALLRec (Movie)), and both (TALLRec (Both)). The left figure shows the testing results on the Movie dataset with varying numbers of rec-tuning samples, while the right figure shows the testing results on the Book dataset.</div>\nTALLRec consistently performs better than the baselines. It is attributed to rec-tuning, which can utilize limited samples to inspire the LLM\u2019s recommendation capability.\n# 3.3 Cross-domain Generalization Analyses (RQ3)\n# 3.3 Cross-domain Generalization Analyses (RQ3)\nTo further investigate the generalization ability of TALLRec, we conduct experiments on cross-domain recommendations. Specifically, we tune TALLRec with different rec-tuning samples, including 1) \u201cTALLRec (Book)\u201d, only using the samples from the Book dataset; 2) \u201cTALLRec (Movie)\u201d, solely using samples from the Movie dataset; and 3) \u201cTALLRec (Both)\u201d, tuned with both the Book and Movie samples. We vary \ud835\udc3ein {16, 64, 258} under the few-shot training setting, and evaluate the models on the testing sets of Book and Movie, respectively. The results are summarized in Figure 4, from which we can find: 1) TALLRec demonstrates remarkable crossdomain generalization ability. For instance, after tuning only on movie samples, \u201cTALLRec (Movie)\u201d exhibits good performance on Book data, comparable to \u201cTALLRec (Book)\u201d. This is impressive and suggests that TALLRec has cross-domain generalization ability instead of only fitting a single domain like traditional recommenders. 2) \u201cTALLRec (Both)\u201d surpasses \u201cTALLRec (Movie)\u201d and \u201cTALLRec (Book)\u201d on two testing sets when the number of rec-tuning samples exceeds 64. This finding indicates that TALLRec can seamlessly integrate data from different domains to enhance its generalization performance. In future work, it is promising to pre-train TALLRec with large-scale recommendation data from heterogeneous domains.\n# 4 RELATED WORK\n\u2022 LMs for Recommendation. There have been several attempts to integrate language models (LMs) with recommendation systems. Despite the incorporation of LMs [14, 29], some attempts persist in utilizing traditional user/item IDs to represent users/items. Thereby, they disregard the semantic understanding capabilities of LMs, such as reviews, which other work has incorporated the language information as part of the users/items embedding [19]. In addition, other methods either utilize an undisclosed model that already possesses preliminary recommendation capabilities [6]. or employ small models to train on large-scale downstream task data [68]. Moreover, the aforementioned models are also limited to small models, while\nthis paper is orthogonal about how to adapt large language models to recommendation tasks. In recommendation systems, there is currently little research on applying LLMs in recommendation scenarios. Those works utilize the interaction ability of GPT3.5 series models and apply In-context Learning [13, 47]. In detail, Chat-Rec [13] endeavors to harness the interaction capabilities of ChatGPT and link the ChatGPT with traditional recommendation models (e.g. MF [27], LightGCN [16]) to formulate a conversational recommendation system. NIR [47] shares a similar concept with Chat-Rec, which employs conventional recommendation models to generate candidate items subjected to a three-stage multi-step prompting process for re-ranking.\nthis paper is orthogonal about how to adapt large language models to recommendation tasks. In recommendation systems, there is currently little research on applying LLMs in recommendation scenarios. Those works utilize the interaction ability of GPT3.5 series models and apply In-context Learning [13, 47]. In detail, Chat-Rec [13] endeavors to harness the interaction capabilities of ChatGPT and link the ChatGPT with traditional recommendation models (e.g. MF [27], LightGCN [16]) to formulate a conversational recommendation system. NIR [47] shares a similar concept with Chat-Rec, which employs conventional recommendation models to generate candidate items subjected to a three-stage multi-step prompting process for re-ranking. \u2022 Sequential Recommendation. Our setup is close to the sequential recommendation, which aims to infer the user\u2019s next interaction based on users\u2019 historical interaction sequences [11, 50]. In the early time, the Markov chain plays an important role in sequential recommendation [15, 34, 40, 49]. Recently, deep learning-based methods have become mainstream. Extensive work using different kinds of neural network structures, like RNN [5, 9, 17], CNN [44, 59, 62], and attention [26, 58, 65], to model the user interaction sequences. However, limited by only using IDs to represent users and items, such work cannot fastly adapt and generalize to new scenarios. Thus, some works focus on the generalization ability of sequential recommendation models by pre-training [35, 61], data augmentation [38, 39, 51, 57], debiasing [8, 52, 67, 70], and robust optimization [55, 60]. However, they ignore the strong generalization ability of existing LLMs, leading to inadequate exploration.\n# 5 CONCLUSION\nWith the advancement of LLMs, people are gradually recognizing their potential in recommendation systems [30, 63, 64]. In this work, we explored the feasibility of using LLMs for the recommendation. Our initial findings reveal that even the existing best LLM models do not perform well in recommendation tasks. To address this issue, we proposed a TALLRec framework that can efficiently align LLM with recommendation tasks through two tuning stages: alpaca tuning and rec-tuning. Our experimental results demonstrate that the LLMs trained using our TALLRec framework outperform traditional models and exhibit strong cross-domain generalization abilities. Moving forward, we plan to explore more efficient methods to activate the recommendation ability of larger models and tune LLMs to handle multiple recommendation tasks simultaneously.\n# ACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation of China (62272437), and the CCCD Key Lab of Ministry of Culture and Tourism.\n# REFERENCES\n[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. arXiv preprint arXiv:2307.09751 (2023). [2] Greg Brockman, Mira Murati, Peter Welinder, and OpenAI. 2022. OpenAI API. [3] Tom Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language models are few-shot learners. NeurIPS 33 (2020), 1877\u20131901. [4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022). [5] Qiang Cui, Shu Wu, Qiang Liu, Wen Zhong, and Liang Wang. 2020. MV-RNN: A Multi-View Recurrent Neural Network for Sequential Recommendation. IEEE Trans. Knowl. Data Eng. 32, 2 (2020), 317\u2013331. [6] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv preprint arXiv:2205.08084 (2022). [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. Association for Computational Linguistics, 4171\u20134186. [8] Sihao Ding, Fuli Feng, Xiangnan He, Jinqiu Jin, Wenjie Wang, Yong Liao, and Yongdong Zhang. 2022. Interpolative Distillation for Unifying Biased and Debiased Recommendation. In SIGIR \u201922, July 11 - 15, 2022. ACM, 40\u201349. [9] Tim Donkers, Benedikt Loepp, and J\u00fcrgen Ziegler. 2017. Sequential user-based recurrent neural network recommendations. In Proceedings of the eleventh ACM conference on recommender systems. 152\u2013160. [10] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, et al. 2023. PaLM-E: An Embodied Multimodal Language Model. abs/2303.03378 (2023). arXiv:2303.03378 [11] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations. ACM Trans. Inf. Syst. 39, 1 (2020), 10:1\u201310:42. [12] F.Maxwell and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context.. In ACM Transactions on Interactive Intelligent Systems (TiiS). [13] Yunfan Gao et al. 2023. Chat-REC: Towards Interactive and Explainable LLMsAugmented Recommender System. arXiv preprint:2303.14524 (2023). [14] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems. 299\u2013315. [15] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th international conference on data mining (ICDM). IEEE, 191\u2013200. [16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR\u201920. 639\u2013648. [17] Bal\u00e1zs Hidasi et al. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR. [18] Jordan Hoffmann et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 (2022). [19] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 585\u2013593. [20] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, et al. 2019. ParameterEfficient Transfer Learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Vol. 97. 2790\u20132799. [21] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. [22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, et al. 2022. OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. arXiv preprint arXiv:2212.12017 (2022). [23] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the Benefits of Training Expert Language Models over Instruction Tuning. (2023). arXiv:2302.03202 [24] Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. abs/2301.01820 (2023). arXiv:2301.01820\n[25] Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023. InferFix: End-to-End Program Repair with LLMs. (2023). arXiv:2303.07263 [26] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM. 197\u2013206. [27] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30\u201337. [28] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP,7-11 November, 2021. Association for Computational Linguistics, 3045\u20133059. [29] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Personalized prompt learning for explainable recommendation. ACM Transactions on Information Systems 41, 4 (2023), 1\u201326. [30] Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan. 2023. Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. arXiv preprint arXiv:2305.11700 (2023). [31] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In ACL/IJCNLP 2021, August 1-6, 2021. Association for Computational Linguistics, 4582\u20134597. [32] Percy Liang, Rishi Bommasani, Tony Lee, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022). [33] Xi Victoria Lin, Todor Mihaylov, et al. 2021. Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668 (2021). [34] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive recommender systems. In Proceedings of the ninth international conference on Electronic commerce. 75\u201384. [35] Yabo Ni, Dan Ou, Shichen Liu, et al. 2018. Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks. In KDD 2018, August 19-23, 2018. ACM, 596\u2013605. [36] Long Ouyang et al. 2022. Training language models to follow instructions with human feedback. NeurIPS 35 (2022), 27730\u201327744. [37] Baolin Peng et al. 2023. Instruction Tuning with GPT-4. (2023). arXiv:2304.03277 [38] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2021. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. (2021). arXiv:2110.05730 [39] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. In WSDM \u201922: The Fifteenth ACM International Conference on Web Search and Data Mining, February 21 - 25, 2022. ACM, 813\u2013823. [40] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web. 811\u2013820. [41] Victor Sanh, Albert Webson, Colin Raffel, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv:2110.08207 (2021). [42] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 (2023). [43] Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. 2022. LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. In CoRL 2022, 14-18 December 2022 (Proceedings of Machine Learning Research, Vol. 205). PMLR, 492\u2013504. [44] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In WSDM. 565\u2013573. [45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. [46] Hugo Touvron et al. 2023. LlaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [47] Lei Wang et al. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153 (2023). [48] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. (2023). arXiv:2303.07678 [49] Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2015. Learning hierarchical representation model for nextbasket recommendation. In Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval. 403\u2013412. [50] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and Mehmet A. Orgun. 2019. Sequential Recommender Systems: Challenges, Progress and Prospects. In IJCAI 2019, Macao, China, August 10-16, 2019. ijcai.org, 6332\u2013 6338. [51] Ziyang Wang, Huoyu Liu, Wei Wei, Yue Hu, Xian-Ling Mao, Shaojian He, Rui Fang, and Dangyang Chen. 2022. Multi-level Contrastive Learning Framework for Sequential Recommendation. (2022). arXiv:2208.13007 [52] Zhenlei Wang, Shiqi Shen, Zhipeng Wang, Bo Chen, Xu Chen, and Ji-Rong Wen. 2022. Unbiased Sequential Recommendation with Latent Confounders. In WWW\n\u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. ACM, 2195\u20132204. [53] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021). [54] Jason Wei, Yi Tay, Rishi Bommasani, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). [55] Hongyi Wen, Xinyang Yi, Tiansheng Yao, Jiaxi Tang, Lichan Hong, and Ed H. Chi. 2022. Distributionally-robust Recommendations for Improving Worst-case User Experience. In WWW \u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. ACM, 3606\u20133610. [56] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan Tompson. 2022. Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. abs/2211.11736 (2022). arXiv:2211.11736 [57] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022. IEEE, 1259\u20131273. [58] Chengfeng Xu, Jian Feng, Pengpeng Zhao, Fuzhen Zhuang, Deqing Wang, Yanchi Liu, and Victor S. Sheng. 2021. Long- and short-term self-attention network for sequential recommendation. Neurocomputing 423 (2021), 580\u2013589. [59] An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, and Julian J. McAuley. 2019. CosRec: 2D Convolutional Neural Networks for Sequential Recommendation. In CIKM 2019, November 3-7, 2019. ACM, 2173\u20132176. [60] Zhengyi Yang et al. 2023. A Generic Learning Framework for Sequential Recommendation with Distribution Shifts. (2023). [61] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation. In SIGIR 2020, July 25-30, 2020. ACM, 1469\u20131478. [62] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose, and Xiangnan He. 2019. A Simple Convolutional Generative Network for Next Item Recommendation. In WSDM 2019, February 11-15, 2019. ACM, 582\u2013590.\n[63] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? IDvs. Modality-based Recommender Models Revisited. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.). ACM, 2639\u20132649. https://doi.org/10.1145/3539618.3591932 [64] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems (RecSys \u201923). Association for Computing Machinery. [65] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, et al. 2019. Feature-level Deeper Self-Attention Network for Sequential Recommendation. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, August 10-16, 2019. ijcai.org, 4320\u20134326. [66] Yang Zhang et al. 2023. Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation. (2023). [67] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation. In SIGIR \u201921, July 11-15, 2021. ACM, 11\u201320. [68] Zizhuo Zhang and Bang Wang. 2023. Prompt Learning for News Recommendation. arXiv preprint arXiv:2304.05263 (2023). [69] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. CoRR abs/2303.18223 (2023). https://doi.org/10.48550/arXiv. 2303.18223 arXiv:2303.18223 [70] Yu Zheng et al. 2021. Disentangling User Interest and Conformity for Recommendation with Causal Embedding. In WWW \u201921. 2980\u20132991. [71] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005. Improving Recommendation Lists through Topic Diversification. In Proceedings of the 14th International Conference on World Wide Web (WWW \u201905). Association for Computing Machinery, 22\u201332.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of aligning Large Language Models (LLMs) with recommendation tasks, highlighting the limitations of previous methods that primarily relied on In-context Learning, which often fails to effectively utilize LLMs for recommendations due to a significant gap between language processing tasks and recommendation tasks.",
        "problem": {
            "definition": "The problem is to enhance the recommendation capabilities of LLMs, which currently underperform in recommendation tasks due to inadequate alignment with the specific requirements of these tasks and limited training data.",
            "key obstacle": "The main challenge is the substantial disparity between the training tasks for LLMs and the requirements of recommendation tasks, compounded by the insufficient amount of recommendation-oriented data available during the pre-training of LLMs."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that LLMs can be tuned to better suit recommendation tasks by leveraging recommendation data, thus bridging the gap between traditional LLM training and the specific needs of recommendation systems.",
            "opinion": "The proposed idea involves developing a Tuning framework, TALLRec, which structures recommendation data as instructions to fine-tune LLMs for recommendation tasks, enhancing their performance significantly.",
            "innovation": "The innovation lies in the TALLRec framework, which introduces a lightweight tuning approach that allows LLMs to be efficiently adapted for recommendation tasks with low GPU costs and minimal tuning samples, distinguishing it from existing methods."
        },
        "method": {
            "method name": "TALLRec",
            "method abbreviation": "TALLRec",
            "method definition": "TALLRec is a tuning framework designed to align LLMs with recommendation tasks through a two-stage process of alpaca tuning and rec-tuning.",
            "method description": "TALLRec structures recommendation data as instructions and fine-tunes LLMs to enhance their recommendation capabilities efficiently.",
            "method steps": [
                "Step 1: Define a task and create a 'Task Instruction' using natural language.",
                "Step 2: Formulate the input and output of the task as 'Task Input' and 'Task Output'.",
                "Step 3: Combine 'Task Instruction' and 'Task Input' to form 'Instruction Input', and set the expected 'Instruction Output'.",
                "Step 4: Perform instruction tuning on LLMs based on the formatted pairs."
            ],
            "principle": "The effectiveness of TALLRec is based on the principle that through structured instruction tuning, LLMs can learn to adapt their responses to the specific requirements of recommendation tasks, thereby improving their performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using two datasets: a processed MovieLens100K dataset for movie recommendations and a BookCrossing dataset for book recommendations, with a few-shot training setting using limited tuning samples.",
            "evaluation method": "The performance of TALLRec was evaluated by comparing it against both LLM-based and traditional recommendation models using the Area Under the Receiver Operating Characteristic (AUC) as the evaluation metric."
        },
        "conclusion": "The experimental results demonstrate that the TALLRec framework significantly enhances the recommendation capabilities of LLMs, outperforming traditional models and exhibiting strong cross-domain generalization abilities, thus confirming the framework's effectiveness in aligning LLMs with recommendation tasks.",
        "discussion": {
            "advantage": "The key advantages of TALLRec include its ability to efficiently tune LLMs for recommendation tasks with minimal computational resources and few training samples, leading to superior performance compared to existing methods.",
            "limitation": "A limitation of the method is that it may still rely on the availability of sufficient recommendation data for fine-tuning, which could be a challenge in scenarios with very limited data.",
            "future work": "Future research directions include exploring more efficient methods to further enhance the recommendation capabilities of larger models and adapting TALLRec to handle multiple recommendation tasks simultaneously."
        },
        "other info": {
            "code_url": "https://github.com/SAI990323/TALLRec",
            "acknowledgments": "This work is supported by the National Natural Science Foundation of China (62272437) and the CCCD Key Lab of Ministry of Culture and Tourism."
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The paper introduces TALLRec, a tuning framework designed to align LLMs with recommendation tasks through structured instruction tuning."
        },
        {
            "section number": "4.2",
            "key information": "TALLRec enhances the recommendation capabilities of LLMs by efficiently adapting them for recommendation tasks with low GPU costs and minimal tuning samples."
        },
        {
            "section number": "4.3",
            "key information": "The experimental results demonstrate that the TALLRec framework significantly enhances LLMs' recommendation capabilities, outperforming traditional models."
        },
        {
            "section number": "2.3",
            "key information": "The paper addresses the limitations of previous methods that relied on In-context Learning, highlighting the need for better alignment between LLMs and recommendation tasks."
        },
        {
            "section number": "3.2",
            "key information": "The TALLRec framework structures recommendation data as instructions to fine-tune LLMs, bridging the gap between traditional LLM training and the specific needs of recommendation systems."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions include exploring more efficient methods to enhance the recommendation capabilities of larger models and adapting TALLRec for multiple recommendation tasks."
        }
    ],
    "similarity_score": 0.7368227086838703,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1aef/1aefd66c-cb9a-47b7-944f-dc5bb4fadfbb.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a1b/0a1b920d-0a04-45bc-a020-29e42555c156.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e0b/6e0b316e-1179-4c18-98fd-0f3c7c31cfa9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/88a6/88a6ba52-636f-46ee-b6a5-16b983acdb62.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Tallrec_ An effective and efficient tuning framework to align large language model with recommendation.json"
}