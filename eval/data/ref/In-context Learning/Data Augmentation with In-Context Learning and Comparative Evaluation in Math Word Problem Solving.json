{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.03938",
    "title": "Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving",
    "abstract": "Math Word Problem (MWP) solving presents a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for data augmentation by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new in-context learning augmentation method, employing the Llama-7b language model. This approach involves instruction-based prompting for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance.",
    "bib_name": "yigit2024dataaugmentationincontextlearning",
    "md_text": "# Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving\nGulsum Yigit1,2*\u2020 and Mehmet Fatih Amasyali1\u2020\n1*Department of Computer Engineering, Yildiz Technical University, Istanbul, Turkey. 2Department of Computer Engineering, Kadir Has University, Istanbul, Turkey.\n*Corresponding author(s). E-mail(s): gulsum.yigit@std.yildiz.edu.tr; Contributing authors: amasyali@yildiz.edu.tr; \u2020These authors contributed equally to this work.\n# *Corresponding author(s). E-mail(s): gulsum.yigit@std.yildiz.edu.tr; Contributing authors: amasyali@yildiz.edu.tr; \u2020These authors contributed equally to this work.\nAbstract\nMath Word Problem (MWP) solving presents a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for data augmentation by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new in-context learning augmentation method, employing the Llama-7b language model. This approach involves instruction-based prompting for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance. Keywords: Question Answering, Math Word Problem Solving, Data Augmentation, In-Context Learning, Llama-7b\nKeywords: Question Answering, Math Word Problem Solving, Data Augmentation In-Context Learning, Llama-7b\n# 1 Introduction\nQuestion Answering (QA) systems are instrumental in Natural Language Processing (NLP). QA systems are responsible for understanding and reacting to the user questions in a manner resembling a human [1\u20134]. This makes them essential for various applications such as search engines, virtual assistants, information retrieval systems, etc. Enhancing these systems is critical as the demand for instant and accurate information retrieval continues. The structure of QA systems varies based on the domain and the types of questions they have [5\u20137]. One important form is solving Math Word Problems (MWPs). MWPs are a complex category within the field of QA systems [8\u201313]. Challenges in these systems require knowledge beyond simple pattern recognition and keyword matching. These special systems involve understanding given mathematical operators, quantities, and their complex relationships with each other. MWPs aim to produce a solution equation. Therefore, it requires identifying numerical values provided in context, carefully selecting appropriate mathematical operations, and transforming them into particular mathematical expressions involving unknown variables. Various datasets have been produced in MWPs with unique features and purposes to benefit research and growth in this field. These datasets vary in source, complexity, scale, and the number of unknowns. For example, Draw1K [14], HMWP [15], ALG514 [16], MAWPS [17] emphasize complex multi-unknown word problems, while SVAMP [18], MAWPS-Single [17], ASDIV-A [19], Math23K [20] focus on elementary math questions with a single unknown. Despite their differences, these datasets share common challenges, such as the need for semantic understanding, extracting numerical information, and changing textual context into mathematical expressions. Addressing these challenges is essential for developing practical solutions in MWP scenarios, highlighting the significance of these datasets in advancing the state-of-the-art of MWP systems. MWPs include a variety of approaches. Statistical methods often use statistical patterns and correlations within the dataset [21, 22]. Rule-based approaches involve applying predefined rules to solve MWPs [16, 23\u201325]. Semantic parsing methods attempt to understand the underlying meaning of the text and transform it into mathematical expressions. On the other hand, deep learning-based models use neural networks to extract contextual information and capture complex relationships between textual context and mathematical concepts [20, 26]. Moreover, there are MWP solvers that have been developed to include pre-trained models. Pre-trained models such as BERT, GPT and their variants have gained great importance recently. These models are first pre-trained on massive amounts of textual data, which used to capture complex linguistic patterns and contextual information. When fine-tuned to solve MWP, they show significant improvements in understanding the language used in MWPs. These different approaches enable a more comprehensive exploration of potential solutions in the field of MWPs. Data augmentation in NLP is essential to improve the performance and robustness of the models. It helps to increase model robustness and reduce overfitting by providing multiple variations of existing data. However, challenges arise, especially in\nmaintaining semantics. Simple transformations on textual data can change the meaning of the original context. There are other significant challenges to implementing data augmentation in an MWP dataset. One issue is model bias, where augmented data tends towards specific problems. This leads to inaccurate predictions and biased results. Processing numerical values in the generated augmented samples introduces another problem: the augmentation procedure may produce inconsistent numerical information. Additionally, modifying the original data distribution through data augmentation increases the likelihood of overfitting or underfitting for specific problems. Factors such as data sparsity issues and computational overhead contribute to the difficulties in data augmentation in the MWP datasets. In this study, our primary focus has been on enriching MWP datasets via useful data augmentation methods. We aim to augment training data by modifying the source text and equations. Building on our previous study in [27], we introduced three approaches for data augmentation: Rule-Based: Question Replacement, Rule-Based: Reversing Question and Substitution: Synonym Replacement. Besides, we extended our work by presenting a novel in-context learning augmentation method, leveraging the Llama-7b language model [28]. This method employs instruction-based prompting for rephrasing problem texts. The model generates a rephrased version for each training example, resulting in new samples after filtering and numeric modification steps. In this study, we have the following contributions. \u2022 We proposed several augmentation methods: Rule-Based: Question Replacement, Rule-Based: Reversing Question, and Substitution: Synonym Replacement. Besides, we extended our work by presenting a novel in-context learning augmentation method, leveraging the Llama-7b language model. \u2022 We also extended prior experiments and tested these augmentation methods on 9 different baseline models using the MWPS-Single and SVAMP datasets. By comparing the results with the baseline of an earlier study [29], we found that these augmentation methods consistently lead to improved performance in MWP solving. This paper is structured as follows: Section 2 provides a review of MWP solving systems. Section 3 delves into the introduced augmentation methods. In Section 4, detailed information about the experiments is presented. We performed comprehensive experiments on 2 MWP datasets and evaluated 9 different models. Section 5 provides a discussion about approaches and Section 6 concludes the paper.\nThis paper is structured as follows: Section 2 provides a review of MWP solving systems. Section 3 delves into the introduced augmentation methods. In Section 4, detailed information about the experiments is presented. We performed comprehensive experiments on 2 MWP datasets and evaluated 9 different models. Section 5 provides a discussion about approaches and Section 6 concludes the paper.\n# 2 Related Work\nThe investigation of automatic MWP solvers has attracted considerable attention by operating various approaches. One early method is the rule-based approach, where decisions are based on a predefined set of rules. This method maps equations into templates, extracting predefined patterns from the problem texts [16, 23\u201325]. Studies using rule-based systems have demonstrated satisfactory performances, specifically on smallscale MWP datasets. Nevertheless, a noteworthy disadvantage of this method is its dependency on human intervention to create templates. Preparing effectual templates\nrequires a profound understanding of problem structures. It makes the rule-based systems challenging as the complexity of the problems increases. The work in [25] presents MSWPAS-CP, a computer simulation system developed to help students solve multistep arithmetic word problems. The system processes natural language word problems into frames. It performs calculus based on these frames. Another method in earlier investigations concerns a statistical-based procedure, where a statistical classifier makes decisions. This approach depends on analyzing statistical patterns to make predictions [21, 30, 31]. The study in [31] introduces an algorithm for automatically solving algebra word problems. It analyzes a hypothesis space that contains all potential equations derived from assigning numbers. Via training a log-linear model to optimize the margin between correct and false assignments, the algorithm efficiently addresses a quadratic programming problem. Moreover, some earlier works apply semantic parsing [31\u201333]. Zhou et al. introduced a semantic parsing and reasoning approach that utilizes a new meaning representation language (DOL) to connect natural language text and mathematical expressions, employing a parser to convert text into DOL trees [31]. Research has moved towards utilizing deep learning-based approaches to reduce human intervention and improve the MWP solvers. One well-known method concerns using Sequence-to-Sequence (Seq2Seq) models to enhance the performance of MWP solvers [20, 26, 34, 35]. Huang et al. suggested incorporating a copy-and-alignment mechanism into the traditional Seq2Seq model [35]. Notably, in a study by Wang et al., Recurrent Neural Networks (RNNs) were utilized to transform MWPs into solution equations [20]. This usage of Seq2Seq models seeks to follow the sequential nature of language and problem-solving structures. Similarly, in [26], the approach comprises equation normalization to handle the challenge of duplicate equations. However, one disadvantage of Seq2Seq models is their potential to yield invalid solution equations due to a lack of management over the decoder during the generation process. The decoder, accountable for constructing the output sequence, may generate solutions that may not be valid in the context of the given problem. This restriction underlines the need to distill Seq2Seq models to create proper and contextually appropriate solution equations. Some studies have considered integrating expression trees to address the issue of generating valid solution equations using Seq2Seq models. These models are generalized as Seq2Tree models [8, 11]. These models go beyond the conventional Seq2Seq architectures, which construct the output as an expression tree, seeking more specific and contextually relevant solution equations. In a study by Xie et al., the authors utilized a Long Short-Term Memory (LSTM) encoder to encode the problem text and presented a tree-based decoder to generate the equation expression [8]. This Seq2Tree model denotes a more hierarchical and structured representation of solution equations. A template-based model was presented in another approach proposed in [11]. This model interests constructing a Seq2Seq model to predict the tree-structured template for the solution equation. The predicted numeric values are depicted as leaf nodes, while the operators perform as non-leaf nodes in the solution expression tree. This Seq2Tree model provides a more structured outcome and mitigates the case of generating invalid or contextually improper solution equations. Various other models, such as\nChiang et al. with a semantic tracking stack [36], Li et al. [37] incorporating different functional multihead attentions, and Meng et al. [38] applying double sequence-based decoders, have been explored. MWP solvers have seen an expansion towards Graph2Tree approaches [12, 13]. The Graph2Tree method parses the problem text to construct binary trees that preserve numerical data and mathematical operators. Li et al. combined the dependency parse tree and constituency tree from text descriptions, while Zhang et al. developed the quantity cell and comparison graphs [12, 39]. These approaches include structural information from text descriptions. Shen et al. applied a hybrid technique, integrating a sequence-based encoder with a graph-based decoder [13]. This fusion is proposed to improve text representations and generate various solution equations. Incorporating a graph-based decoder permitted the model to offer a more comprehensive understanding and enabled the generation of varied and contextually appropriate solution equations. Beyond Graph2Tree strategies, some investigators have examined the integration of pre-trained language models [10, 40\u201342]. Liang et al. presented MWP-BERT, leveraging the capabilities of pre-trained language models tailored explicitly for MWP solvers [10]. This approach emphasizes the contextual understanding grabbed by pre-trained models, achieving higher performances by producing more accurate and contextually relevant solution equations. The emergence of Large Language Models (LLMs) has prompted a surge in innovative approaches to solving MWPs. Many researchers have made notable contributions to this field, as evidenced by significant advancements [43\u201348]. Prompt-based learning has gained attraction for its ability to leverage LLMs\u2019 inherent prediction capabilities. Unlike traditional supervised learning, prompt-based learning utilizes text prompts to guide the model\u2019s responses, often in a few-shot or zero-shot format. Lazaridou et al. and Chen et al. have explored the optimization of prompts for different tasks, facilitating the generation of desired responses from large models without requiring extensive fine-tuning [47, 49]. Furthermore, Chain of Thought (CoT) prompting has emerged as a promising approach, generating reasoning steps to guide the model in deriving the actual answer. This method not only allows the model to arrive at reasonable conclusions but also enhances its explainability. Wei et al. demonstrated the effectiveness of CoT prompting compared to conventional prompt methods in MWPs [50]. Li et al. presented DIVERSE (Diverse Verifier on Reasoning Step) as a novel approach to enhance language models\u2019 reasoning capability [44]. It contains three main parts: diverse prompts generation to investigate different reasoning paths for the same question, utilization of a verifier to filter out incorrect answers based on a weighted voting scheme, and verification of each reasoning step individually instead of the entire chain. Wang et al. presented a novel decoding strategy, self-consistency by sampling diverse reasoning paths and selecting the most consistent answer. Empirical evaluations across various arithmetic reasoning benchmarks show that self-consistency significantly improves the performance of CoT prompting [45].\nChen et al. introduced a \u201cProgram of Thoughts\u201d (PoT), which employs language models, primarily Codex, to generate both text and programming language statements, culminating in an answer [47]. PoT underwent evaluation in both few-shot and zero-shot settings, revealing improvements in performance. The authors highlight significant limitations observed in CoT, including the propensity of LLMs to encounter arithmetic errors, particularly with large numbers, the challenge faced by LLMs in solving complex mathematical expressions such as polynomial or differential equations, and the inefficiency of LLMs in expressing iteration, especially with numerous steps. In PoT, computation is delegated to an external language interpreter, while reasoning steps are articulated as Python programs by Language Models.\n# 3 Proposed Augmentation Methods\n# 3.1 Rephrase with In-Context Learnin\n3.1 Rephrase with In-Context Learning\nn the previous study, an investigation into the impact of paraphrasing models on MWPs highlighted challenges in generating rephrased texts while preserving the ntegrity of numerical quantities and their relationships [27]. In this study, we present a new method that introduces a sophisticated approach employing in-context learning to create rephrased examples for MWP datasets. This approach follows the following three steps. \u2022 Step 1: In-Context Learning: In the initial step of the proposed method, 15 examples from an MWP dataset are presented to the Llama-7b language model [28], a powerful tool for natural language understanding. The 15 MWP samples were randomly selected. No specific heuristic was employed in the selection process. Then, the instruction as given below was followed by 15 examples and the rephrased text of these examples. ChatGPT is used to generate the rephrased texts for these 15 examples [51]. The model is instructed to generate a rephrased version of each training example. The objective is to leverage in-context learning to produce texts while preserving the inherent mathematical structure.\nIn the previous study, an investigation into the impact of paraphrasing models on MWPs highlighted challenges in generating rephrased texts while preserving the integrity of numerical quantities and their relationships [27]. In this study, we presen a new method that introduces a sophisticated approach employing in-context learning to create rephrased examples for MWP datasets. This approach follows the following three steps.\n Step 1: In-Context Learning: In the initial step of the proposed method, 15 examples from an MWP dataset are presented to the Llama-7b language model [28], a powerful tool for natural language understanding. The 15 MWP samples were randomly selected. No specific heuristic was employed in the selection process. Then, the instruction as given below was followed by 15 examples and the rephrased text of these examples. ChatGPT is used to generate the rephrased texts for these 15 examples [51]. The model is instructed to generate a rephrased version of each training example. The objective is to leverage in-context learning to produce texts while preserving the inherent mathematical structure.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af0f/af0f301c-0d26-4278-a890-df9797f99154.png\" style=\"width: 50%;\"></div>\n\u2022 Step 2: Filtering Mechanism: Following the generation process, a specific filtering mechanism is implemented to pick the appropriate ones from the rephrased examples. This process involves two key aspects:\ntering mechanism is implemented to pick the appropriate ones from the rephrased examples. This process involves two key aspects: \u2013 Validity Check: The filtering mechanism excluded samples lacking a \u201c?\u201d to ensure they represent valid questions in line with MWPs structures. However, this may not always catch all the valid questions, especially when problem statements end with phrases like \u201cFind the value of ...\u201d. While \u201c?\u201d serves as a straightforward indicator, it may not capture all valid problem structures. As a result, we risk excluding valid rephrased versions without question marks. Furthermore, there are instances where the model generates empty strings, leading to the absence of rephrased versions for certain samples. It\u2019s noteworthy that, as shown in Table 2, occurrences of these conditions are relatively low, especially in the SVAMP dataset. \u2013 Diversity Enhancement: Rephrased examples identical to the original ones are omitted to enhance diversity in the augmented dataset. In step 1, we employ an LLM to generate diverse variations of the original questions. However, due to the inherent nature of the model, there were cases where the rephrased produced questions were identical to the original ones. Therefore, we implemented an automatic process to identify and remove these duplicate examples to address this issue. If an exact match was found, indicating redundancy in the augmented dataset, those examples were eliminated to ensure the diversity and uniqueness of the dataset. The filtration process is done automatically. There is no involvement of humans. \u2022 Step 3: Numerical Modification: The subsequent phase of the algorithm focuses on numerical modification to introduce variability in the dataset. This involves random replacement of numerical values, where randomly generated integers replace integer values, and float numbers are replaced by randomly generated float numbers. Crucially, to maintain consistency, the algorithm ensures that modifications applied to numerical values are consistently propagated to the corresponding equations and answers. Equations and answers are reconstructed to preserve the logical and mathematical integrity of the examples. Note that, changing the numerical values is significant for several reasons. While introducing quantity-agnostic tags/tokens could be an alternative approach, changing the numerical values directly offers several advantages. Firstly, altering the numerical values allows for generating a more diverse set of problem equations/answers. This variation is crucial for training models to handle various scenarios. Secondly, changing the numerical values helps prevent the model from memorizing specific samples, equations/answers and encourages it to learn underlying patterns and relationships. This enables better generalization to unseen data and improves the model\u2019s robustness. While quantity-agnostic tags/tokens may offer some advantages, changing the numerical values directly remains a salient approach for enhancing the diversity, generalization, and coherence of generated MWPs. The augmented dataset comprises the modified examples with the original examples. Through this new approach, our study offers a nuanced and practical approach to\nenhance the quality of rephrased examples in MWPs. The prompts used to generate the rephrased versions of the problems are provided in the appendices for reproducibility purposes (see Appendix A).\n# 3.2 Rule-Based: Reversing Question\nLiu et al. introduced the concept of using inverted versions of problem statements in MWPs [52]. Additionally, it is important to mention that this methodology has been investigated in other studies as well [18, 53].A similar approach is adopted in the previous study [27]. The actual problem text has been altered to make additional samples. The question and valid answer from the problem text generate a renewed supporting sentence. Then, a randomly picked sentence retaining a numerical value is converted into a new problem text, with the numerical value as the solution. This procedure forms new samples and assures contextual relevance. All possible equations related to the numerical values in the question text are created, increasing the data with various perspectives and helping the model to adapt to diverse challenges.\n# 3.3 Rule-Based: Question Replacement\nThe previous study modified the question text to generate diverse problem types [27]. In this method, key phrases such as \u201cHow many\u201d and \u201cHow much\u201d in the original problem are replaced with \u201cWhat is x/y of,\u201d where x and y vary from 1 to 10. After this modification, a T5-Based model fine-tuned for grammar checking used to generate a modified version of the question text while maintaining consistency among the question, equation, and answer. This technique presented complexity by producing different texts for the same underlying question, diversifying the dataset, and preventing direct answer mapping. By incorporating such variations, the model becomes more robust at handling various problem formulations and answer types.\n# 3.4 Substitution: Synonym Replacement\nThis method introduced synonyms from the NLTK WordNet to the original problems, adding semantic variations without varying the mathematical logic [27]. By randomly picking terms for replacement, the method increases the vocabulary of the problem text, contributing to a more diverse dataset. Using NLTK WordNet ensures that the selected synonyms are contextually pertinent, preserving the mathematical reasoning in the original question text. This strategy desires to create a dataset with various linguistic expressions while protecting the underlying mathematical structure in MWP solving. Table 1 shows examples of the proposed augmentation techniques on MWP. It includes the original text, equation, and answer, followed by the samples generated by proposed augmentation methods involving synonym replacement, rule based: reversing question, rule-based: question replacement, and in-context learning, and the corresponding equations and the answers. The modifications made to the original example are emphasized for each proposed augmentation method.\nText\nEquation\nAnswer\nOriginal\nFred had 7 dimes in his bank . His sister borrowed\n3 of his dimes . How many dimes does Fred have now?\nX=7-3\n4\nSubstitution:\nSynonym\nReplacement\nFred had 7 dimes in his depository financial institution .\nHis Sister lent 3 of his dimes. How many dimes\ndoes Fred have now?\nX=7-3\n4\nRule based:\nReversing\nQuestion\nFred has 4 dimes now. His sister borrowed 3 of\nhis dimes. How many dimes in his bank did Fred have?\n4=X-3\n7\nRule based:\nQuestion\nReplacement\nFred had 7 dimes in his bank. His sister borrowed\n3 of his dimes. What is 9/10 of all the dimes Fred\nhas now?\nX=(7-3) *(9/10)\n3.6\nRephrase with\nIn-Context\nLearning\nFred initially had 23 dimes in his bank, but after\nhis sister borrowed 9 dimes, how many dimes does Fred\nhave remaining?\nX=23-9\n14\nTable 1 Examples from augmentation methods\n# 4 Experiments\n# 4.1 Datasets and Models\nDatasets: The following datasets are used in the experiment\n\u2022 MAWPS-Single: MAWPS (MAth Word ProblemS) is a framework for building an online repository of MWPs, offering a diverse collection of MWPs along with their answers and equation templates [17]. MAWPS is designed to be a comprehensive and customizable resource for evaluating various algorithms. In the context of MAWPS and its sub-datasets, the term \u201csingle equation\u201d indicates that the problems involve mathematical expressions with a single unknown variable. The MAWPS-Single subset is designed to perform experiments or evaluations requiring or emphasizing one unknown problem type in MWPs. This subset comprises 1,987 MWPs, each featuring a single equation. \u2022 SVAMP: SVAMP is a challenging dataset strategically collected from existing datasets with a specific focus on addressing the interesting observation that certain word problems can be solved without the complete problem text [18]. The creation of SVAMP involved the application of various modifications to seed examples sourced from the ASDiv-A dataset [19]. This choice was driven by the perceived higher quality and increased difficulty of ASDiv-A compared to the MAWPS dataset. This dataset comprised 3,138 problems in the training set and 1,000 problems in the test set. These two datasets comprise the basis for the experiments, providing a varied and specialized set of MWPs for analyzing and evaluating algorithms in MWP solving. In addition to the previously mentioned augmentation sets, including the original examples, 4 additional trainsets have been constructed: Combined V1, Combined V2, Combined V3, and Combined V4. These sets are formulated by combining the samples generated via various augmentation methods.\ntrainset\nQuestion\nRepl.\nReversing\nQuestion\nSynonym\nRepl.\nRephrase with\nIn-Context\nLearning\nCombined\nV1\nCombined\nV2\nCombined\nV3\nCombined\nV4\nasdiv-a\nSVAMP\n3138\n5998\n5553\n6274\n5744\n11545\n16810\n14244\n19509\nMAWPS-Single\n1589\n3043\n2557\n3178\n2996\n5599\n8022\n7080\n9503\n\u2022 Combined V1 contains instances generated by Rule-Based procedures such as Question Replacement and Reversing Questions, along with the actual training samples. \u2022 Combined V2 comprises instances created by Rule-Based methods and actual training samples. Then, Substitution: Synonym Replacement approach is applied to all those examples, so it comprises more instances than Combined V1. \u2022 Combined V3 contains all examples from Combined V1 and adds examples generated by the in-context learning augmentation approach, ensuring no duplication of the original training data. \u2022 Finally, Combined V4 includes all examples from Combined V3 and incorporates additional samples generated through the in-context augmentation approach, also ensuring that there is no duplication of the original training data. Table 2 shows the number of training instances utilized for the proposed methods across the two datasets, including the combined sets. Having diversified sets aims to enhance the robustness and generalization of the training data for improved performance in MWP solving. Models: This study assesses and compares diverse neural models for MWP solving. A comparative table of these models is given in Table 3. Each model is designed to overcome distinct challenges in the domain. GTS presents a novel tree-structured model that works goal-driven, recursively decomposing the problem into sub-goals until getting a known quantity. DNS converts MWPs directly into equation templates using an RNN, improving performance with a hybrid model integrating a similaritybased retrieval model. RobertaGen, built upon RoBERTa, makes a powerful tool for a nuanced understanding of mathematical complexities. RNNVAE leverages directions from variational autoencoders and RNNs, incorporating VAE\u2019s text modeling with RNN\u2019s sequential processing to obtain accurate MWP answers. Moreover, Graph2Tree addresses challenges in capturing relationships between quantities in tree-based neural models. MathEN focuses on the issue of multiple correct equations in MWPs. It introduces an ensemble model integrating the strengths of individual Seq2Seq models. Saligned utilizes a neural encoder-decoder framework focused on the semantic meanings of symbols in the text. MWPBert, a BERT-based model, addresses number representation challenges by injecting numerical properties into symbolic placeholders. SAUSolver generates universal expression trees based on the semantic meanings of previously generated symbols. The models employed in this study can be classified based on their architectures: GTS, MWPBert, and SAUSolver serve as Seq2Tree models; RNNVAE, DNS, MathEN, and Saligned utilize Seq2Seq architectures; Graph2Tree is specifically designed as a Graph2Tree model, and RobertaGen stands out as a pre-trained model.\nModel\nYear\nType\nDescription\nEncoder\nDecoder\nPretrained\nmodel\nRNNVAE [34]\n2016\nSeq2Seq\nHybrid variational encoder-decoder\nsystem integrating VAE and RNN principles.\nLeverages VAE strengths in text modeling\nand sequential processing abilities of RNNs.\nLSTM\nLSTM\n-\nDNS [20]\n2017\nSeq2Seq\nDeep neural solver for MWPs.\nUtilizes a hybrid model with a\nsimilarity-based retrieval model for\nperformance improvements.\nGRU\nLSTM\n-\nMathEN [26]\n2018\nSeq2Seq\nAddresses the issue of multiple correct\nequations in Seq2Seq models for MWPs.\nIntroduces an equation normalization\nmethod.\nBiLSTM\nLSTM\n-\nGTS [8]\n2019\nSeq2Tree\nGoal-driven tree-structured model for MWP\nsolving. Uses two-layer gated feed forward\nnetworks for goal decomposition.\nGRU\nTreeDecoder\n-\nRobertaGen [41]\n2019\nPre-trained\nAdvanced language model based\non RoBERTa. Specializes in MWPs excelling\nin contextual and semantic understanding.\nRoBERTa\nTransformer\nRoBERTa\nSaligned [36]\n2019\nSeq2Seq\nNeural encoder-decoder framework\nfor MWP solving. Focuses on the semantic\nmeanings of symbols in the text.\nBiLSTM\nLSTM\n-\nGraph2Tree [12]\n2020\nGraph2Tree\nDeep learning architecture for improved\nsolution equation generation in MWPs.\nUses Quantity Cell and Quantity Comparison\nGraphs for representation.\nLSTM\n+\nGraph\nConvolutional\nNetworks\nTreeDecoder\n-\nSAUSolver [15]\n2020\nSeq2Tree\nSemantically aligned universal tree structured\nsolver. Encoder-decoder framework generating\na universal expression tree based on semantic\nmeanings.\nGRU\nTreeDecoder\n-\nMWPBert [54]\n2021\nSeq2Tree\nAddresses the issue of number representation\nin MWP solving. BERT-based model injecting\nnumerical properties into symbolic placeholders.\nBert\nTreeDecoder\nBert\nTable 3 Comparison of MAWP Solving Models\nTable 3 Comparison of MAWP Solving Models\nThis categorization emphasizes the study\u2019s diverse architectural strategies for MWP solving.\n# 4.2 Experimental Results\nOur experiments mainly focus on the MAWPS-Single and SVAMP datasets, which are widely used one-unknown MWP datasets. The performances of the conducted experiments on MAWPS-Single and SVAMP datasets are illustrated in Table 4 and 5. Rows and columns describe models and presented augmentation strategies. Results are underlined and bold for instances where the augmentation technique performs less than the reproduced performances. Further, the highest two performances on the test set are highlighted in red. We employ two evaluation metrics, equation accuracy and answer accuracy, to assess the performance of the models. Equation accuracy determines whether the predicted solution equation matches the expected equation precisely. Moreover, answer accuracy evaluates the exact matches between predicted and expected answers. By utilizing both metrics, we comprehensively assess the effectiveness of our models in generating accurate solutions to MWPs. In addition, our choice of accuracy for comparison aligns with the evaluation metric used by state-of-the-art baseline models in the field. This ensures a direct and fair comparison between our proposed methods and existing approaches. Our study employed proposed augmentation methods across various models, including 4 Seq2Seq, 3 Seq2Tree, 1 Graph2Tree, and 1 RobertaGen model. In our\nMAWPS-Single\nAcc\nin [29]\nReproduced\nQuestion\nRepl.\nReversing\nQuestion\nSynonym\nRepl.\nRephrase\nin\ncontext\nlearning\nCombined\nV1\nCombined\nV2\nCombined\nV3\nCombined\nV4\nequ\nacc\n78,9\n79,4\n79,4\n79,4\n79,4\n79,9\n81,4\n80,4\n79,9\n80,9\nDNS\nval\nacc\n86,3\n88,4\n87,9\n87,9\n87,9\n87,9\n88,9\n88,9\n87,4\n87,4\nequ\nacc\n85,9\n84,4\n87,9\n85,9\n86,9\n85,9\n87,9\n86,9\n86,4\n86,4\nMathEN\nval\nacc\n86,4\n85,9\n88,9\n86,9\n87,9\n86,9\n88,9\n87,9\n87,4\n87,4\nequ\nacc\n86\n77,4\n78,4\n79,9\n78,4\n77,4\n80,9\n79,9\n80,4\n78,4\nSaligned\nval\nacc\n86,3\n85,4\n88,9\n87,9\n87,9\n85,9\n88,9\n87,9\n87,9\n86,4\nequ\nacc\n79,8\n87,9\n88,9\n87,4\n87,4\n87,9\n88,9\n89,4\n87,9\n89,4\nSeq2Seq\nRNNVAE\nval\nacc\n88,2\n88,9\n89,9\n88,9\n88,4\n88,9\n89,9\n90,5\n88,9\n89,9\nequ\nacc\n-\n84,4\n86,4\n82,4\n86,4\n86,4\n86,9\n85,9\n86,9\n88,4\nMWPBert\nval\nacc\n-\n84,9\n86,9\n82,9\n86,9\n86,9\n87,4\n86,4\n86,9\n88,4\nequ\nacc\n83,4\n83,9\n85,9\n86,4\n84,9\n85,9\n85,4\n85,4\n85,9\n86,4\nSAUSolver\nval\nacc\n84\n84,9\n86,4\n86,9\n85,4\n86,4\n85,9\n85,9\n86,4\n86,4\nequ\nacc\n83,5\n83,9\n86,9\n84,9\n85,9\n87,4\n85,4\n84,9\n85,4\n87,4\nSeq2Tree\nGTS\nval\nacc\n84,1\n84,4\n87,4\n85,4\n86,4\n87,9\n85,9\n85,4\n85,9\n87,9\nequ\nacc\n84,9\n84,9\n86,4\n84,9\n87,4\n86,4\n86,9\n86,4\n88,4\n87,9\nGraph2Tree\nGraph2Tree\nval\nacc\n85,6\n85,4\n86,9\n85,4\n87,9\n85,4\n87,4\n86,9\n89,4\n88,9\nequ\nacc\n80,8\n83,4\n85,9\n84,9\n84,4\n84,4\n83,9\n84,4\n85,9\n85,4\nPre-trained\nRobertaGen\nval\nacc\n88,4\n84,9\n86,9\n85,9\n84,9\n85,4\n84,4\n85,9\n86,9\n85,9\nTable 4 Experiments on MAWPS-Single dataset\nanalysis, we compared the obtained performances with the baseline in [29]. Through a comparison of these performances, the following outcomes can be inferred. \u2022 In general, the proposed data augmentation methods performed better across nine distinct models. \u2022 The combined augmentation sets mostly outperformed other individual methods. \u2022 On the MAWPS-Single dataset, \u2013 The Seq2Seq models revealed their highest performances mainly over Combined V1. \u2013 Seq2Tree, Graph2Tree, and pre-trained models performed better when trained on Combined V3.\n\u2022 Similarly, for the Svamp dataset, the superiority of combined datasets, specifically Combined v4, was evident as it consistently achieved the highest performances.\nMoreover, Table 6 provides a detailed comparison of the performance of different augmentation approaches across datasets and evaluation metrics. Each row corresponds to a specific dataset with an evaluation metric, while the columns represent different augmentation approaches employed. Each cell in the table contains a value indicating how each augmentation approach performs relative to baseline methods. For instance, the value \u201c3/9\u201d in a cell indicates that the corresponding augmentation approach outperformed the baseline in three out of nine models considered for evaluation. According to this table:\nSVAMP\nAcc\nin [29]\nReproduced\nQuestion\nRepl.\nReversing\nQuestion\nSynonym\nRepl.\nRephrase\nin\ncontext\nlearning\nCombined\nV1\nCombined\nV2\nCombined\nV3\nCombined\nV4\nequ\nacc\n22.1\n19\n23,7\n20,2\n23,1\n21,2\n23,9\n23,6\n22,5\n23,3\nDNS\nval\nacc\n24.2\n22,6\n26,8\n23,6\n26,8\n23,5\n26,9\n26,5\n25,2\n26,2\nequ\nacc\n21.8\n23,7\n25,1\n24,8\n24,3\n23,2\n25,1\n25,7\n26,5\n26,3\nMathEN\nval\nacc\n25.0\n24,3\n25,6\n25,2\n24,9\n23,9\n25,6\n26,3\n27,3\n27\nequ\nacc\n23.9\n24,1\n24,3\n21,6\n24,5\n24,5\n25\n26\n24\n25,5\nSaligned\nval\nacc\n26.1\n27,2\n27\n24,6\n27,6\n27,6\n28,5\n28,5\n27,9\n28,7\nequ\nacc\n23.2\n21,7\n24,1\n23,4\n22,6\n23,6\n24,6\n23,4\n25,2\n24,7\nSeq2Seq\nRNNVAE\nval\nacc\n25.9\n25,4\n27,2\n26,2\n26,7\n26,9\n28,3\n27,4\n28,4\n28,5\nequ\nacc\n-\n24,4\n25,1\n24,2\n26,6\n28,5\n26\n27,4\n27,4\n27,7\nMWPBert\nval\nacc\n-\n26,6\n28,5\n26\n29,4\n32,8\n28,3\n30,1\n31,1\n31,4\nequ\nacc\n27.1\n25,1\n26\n24,3\n25\n25,7\n26,9\n25,8\n26,3\n27\nSAUSolver\nval\nacc\n29.7\n27,6\n28,3\n27\n28,3\n28,5\n30,2\n28,4\n29,1\n29\nequ\nacc\n25.6\n25,3\n26,7\n24,2\n25,5\n25,9\n26\n25,5\n26,3\n27\nSeq2Tree\nGTS\nval\nacc\n29.1\n28,5\n29\n27,2\n28,6\n28,9\n29,1\n28,4\n29,4\n29\nequ\nacc\n31.6\n32,5\n33,6\n31,4\n32,6\n34\n34\n33,2\n31\n31,4\nGraph2Tree\nGraph2Tree\nval\nacc\n35.0\n35,3\n36,7\n34,7\n35,7\n35\n36,8\n36,6\n33,2\n33\nequ\nacc\n27.9\n20,9\n21,9\n20,8\n22,1\n21,8\n21,9\n21,5\n23,4\n20,6\nPre-trained\nRobertaGen\nval\nacc\n30.3\n24\n24,4\n23,3\n24,9\n25,1\n25\n24\n26,3\n24,4\nTable 5 Experiments on SVAMP dataset\nQuestion\nRepl.\nReversing\nQuestion\nSynonym\nRepl.\nRephrase\nwith\nIn-Context\nLearning\nCombined\nV1\nCombined\nV2\nCombined\nV3\nCombined\nV4\nequ\nacc\n9/9\n3/9\n8/9\n8/9\n9/9\n9/9\n7/9\n7/9\nSVAMP\nval\nacc\n8/9\n3/9\n9/9\n8/9\n9/9\n9/9\n8/9\n7/9\nequ\nacc\n9/9\n7/9\n7/9\n9/9\n9/9\n9/9\n9/9\n9/9\nMAWPS-Single\nval\nacc\n8/9\n7/9\n8/9\n8/9\n8/9\n9/9\n8/9\n8/9\nTable 6 Performance Comparison of the Models with Various Augmentation Techniques\n\u2022 The models generally performed well with Rule Based: Question Replacement, Synonym Replacement, and augmentation with in-context learning approaches, achieving 7/9, 8/9, and 9/9 in both equation and value accuracy. \u2022 Training with Combined V2, the models consistently demonstrated strong performance with 9/9 in both equation and value accuracy. \u2022 Rule-based: Reversing Question method had a mixed impact on model performance. Of the 9 models examined, 3 displayed superior performance, outperforming the reproduced performances. This observation underscores the importance of understanding the impact of augmentation techniques, on model outcomes for the SVAMP dataset. In summary, experimental results highlight the significance of dataset composition and augmentation strategies on various model performances.\n\u2022 The models generally performed well with Rule Based: Question Replacement, Synonym Replacement, and augmentation with in-context learning approaches, achieving 7/9, 8/9, and 9/9 in both equation and value accuracy. \u2022 Training with Combined V2, the models consistently demonstrated strong performance with 9/9 in both equation and value accuracy. \u2022 Rule-based: Reversing Question method had a mixed impact on model performance. Of the 9 models examined, 3 displayed superior performance, outperforming the reproduced performances. This observation underscores the importance of understanding the impact of augmentation techniques, on model outcomes for the SVAMP dataset. In summary, experimental results highlight the significance of dataset composition and augmentation strategies on various model performances.\nOur proposed methods address several challenges inherent in MWPs augmentation. Synonym replacement ensures that the meaning and context of the original problem statements are maintained by introducing variation while preserving the semantic meaning of the original problems. Similarly, rule-based question replacement and reversing methodologies are applied strategically to ensure the generation of diverse and contextually relevant problem statements, thus maintaining the semantic integrity of the dataset. The in-context learning based approach ensures that generated problem statements are contextually relevant, preserving semantic integrity. Moreover, our proposed methods introduce diversity into the dataset, thereby reducing the risk of model bias. By incorporating diverse synonyms and applying predefined rules, we expose models to a broader range of examples, reducing the risk of bias in the training data, especially on concatenated sets. Similarly, by leveraging the capabilities of the Llama-7b language model in in-context learning based approach, the risk of model bias is reduced. The methodologies of Synonym Replacement, Rule-Based Question Replacement, and Rule-Based Reversing Question are employed in a controlled manner to maintain uniformity in the textual information while preserving mathematical relationships. These methods ensure that the semantic integrity of the dataset is preserved by introducing variation while maintaining coherence in the problem statements. Additionally, these techniques apply no numerical modifications, ensuring that the numerical information remains consistent across the dataset. In in-context learning approach, numerical modifications are applied, but so in a controlled manner. This ensures that changes to numerical information are uniform and coherent, preserving the underlying mathematical relationships. In in-context learning based approach, we utilize prompt-based generation using an LLM. These models are trained on extensive datasets, which helps ensure their semantic integrity and robustness. While we acknowledge the importance of consistency in the rephrased statements with the equations and answers, we rely on the inherent capabilities of the LLM to maintain semantic coherence during the generation process. Additionally, the problem texts provided to the models are simple, enhancing the likelihood of consistent and accurate rephrased statements aligning with the equations and answers. To ensure that our proposed augmentation approaches are not solely relying on shallow heuristics or memorizing augmented sample templates to achieve improved accuracy,\n\u2022 The proposed methods are evaluated on multiple datasets to assess their ability to generalize across different problem domains and variations. \u2022 The experiments are conducted on 9 baseline models, which serve as benchmarks for comparison. This allows us to assess the relative performance of our approaches and verify that any improvements in accuracy are not simply due to overfitting or memorization.\n# \u2022 The \n\u2022 The diversity of our training data is enhanced by concatenating examples generated by different proposed methods. Increasing the variety of training examples encourages the models to learn robust and generalizable patterns, rather than relying on memorized templates or shallow heuristics. We have also thoroughly assessed the efficacy of our proposed in-context learning approach using the MWPBert model. The examples provided in Table 7 demonstrate a set of MWPs samples that were initially incorrectly solved by the MWPBert model prior to its training with augmented samples. However, after training with the augmented samples generated through the in-context learning approach, the model successfully generates correct answers. This improvement provides the claim that our augmentation approach enhances the model\u2019s reasoning ability when solving MWPs.\n\u2022 The diversity of our training data is enhanced by concatenating examples generated by different proposed methods. Increasing the variety of training examples encourages the models to learn robust and generalizable patterns, rather than relying on memorized templates or shallow heuristics.\n<div style=\"text-align: center;\">Table 7 The MWP samples were incorrectly solved by MWPBert, correctly solved after training with in-context learning based approach</div>\nQuestion\nEquation\nAnswer\nThere are 20 different books in the \u2019crazy silly\nschool\u2019 series. If you are yet to read 5 of the\nbooks, how many books have you already read?\nX=(20.0 - 5.0)\n15\nPaul got a box of 440 crayons for his birthday.\nDuring the school year he gave 111 crayons to\nhis friends while he lost 106 crayons. How many\ncrayons did he have left?\nX=(440.0 - (111.0 + 106.0))\n223\nBobby had 21 pieces of candy. He ate 5 pieces\nof candy. Then he ate 9 more. How many pieces\nof candy does he still have left?\nX=(21.0 - (5.0 + 9.0))\n7\nDan has $4. He bought a candy bar for $7\nand a chocolate for $6. How much money did\nhe spend buying the candy bar and chocolate?\nX=(7.0 + 6.0)\n13\n# 6 Conclusion and Future Work\nIn conclusion, addressing the challenges posed by MWP solving in NLP is critical for improving the performance of existing models. The need for improved generalization highlights the significance of this study\u2019s objective to enhance MWP datasets with high-quality data. While this study\u2019s primary focus is on English, the study points to the potential generalization of the presented ideas to other languages in MWP solving. Several data augmentation techniques have been introduced and tested on MAWPSSingle and SVAMP datasets. The experiment results demonstrated the progress in performance on both datasets. In the future, we aim to investigate adversarial MWPs as part of our ongoing efforts, to improve the robustness and performance of our approach. We will also explore the effectiveness of our in-context learning based augmentation approach with various other LLMs such as GPT-3 (ada, cabbage, curie, davinci), GPT-3.5 etc., as part of our future work.\nAcknowledgments. This research is supported by The Scientific and Technological Research Council of Turkey (T\u00a8UB\u02d9ITAK) in part of the project with 120E100 grant number. G.Yigit is supported by TUB\u02d9ITAK - B\u02d9IDEB 2211/A national fellowship program for Ph.D. studies. Declarations The authors declare that they have no conflict of interest. Data Availability. Data sharing not applicable to this article as no datasets were generated or analyzed during the current study.\n# Appendix A\n# References\n[1] Yigit, G., Amasyali, M.F.: Enhancing multiple-choice question answering through sequential fine-tuning and curriculum learning strategies. Knowledge and Information Systems, 1\u201318 (2023) [2] Wu, L., Wu, P., Zhang, X.: A seq2seq-based approach to question answering over knowledge bases. In: Semantic Technology: 9th Joint International Conference, JIST 2019, Hangzhou, China, November 25\u201327, 2019, Revised Selected Papers 9, pp. 170\u2013181 (2020). Springer [3] Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., Auli, M.: Eli5: Long form question answering. arXiv preprint arXiv:1907.09190 (2019) [4] Jin, S., Lian, X., Jung, H., Park, J., Suh, J.: Building a deep learning-based qa system from a cqa dataset. Decision Support Systems, 114038 (2023) [5] Abdel-Nabi, H., Awajan, A., Ali, M.Z.: Deep learning-based question answering: a survey. Knowledge and Information Systems 65(4), 1399\u20131485 (2023) [6] Rogers, A., Gardner, M., Augenstein, I.: Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension. ACM Computing Surveys 55(10), 1\u201345 (2023) [7] Yigit, G., Amasyali, M.F.: Ask me: A question answering system via dynamic memory networks. In: 2019 Innovations in Intelligent Systems and Applications Conference (ASYU), pp. 1\u20135 (2019). IEEE [8] Xie, Z., Sun, S.: A goal-driven tree-structured neural model for math word problems. In: IJCAI, pp. 5299\u20135305 (2019) [9] Zhang, J., Lee, R.K.-W., Lim, E.-P., Qin, W., Wang, L., Shao, J., Sun, Q.: Teacher-student networks with multiple decoders for solving math word problem. (2020). IJCAI\n[10] Liang, Z., Zhang, J., Wang, L., Qin, W., Lan, Y., Shao, J., Zhang, X.: Mwp-bert: Numeracy-augmented pre-training for math word problem solving. In: Findings of the Association for Computational Linguistics: NAACL 2022, pp. 997\u20131009 (2022) [11] Wang, L., Zhang, D., Zhang, J., Xu, X., Gao, L., Dai, B.T., Shen, H.T.: Templatebased math word problem solvers with recursive neural networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 7144\u20137151 (2019) [12] Zhang, J., Wang, L., Lee, R.K.-W., Bin, Y., Wang, Y., Shao, J., Lim, E.-P.: Graph-to-tree learning for solving math word problems. (2020). Association for Computational Linguistics [13] Shen, Y., Jin, C.: Solving math word problems with multi-encoders and multidecoders. In: Proceedings of the 28th International Conference on Computational Linguistics, pp. 2924\u20132934 (2020) [14] Upadhyay, S., Chang, M.-W.: Annotating derivations: A new evaluation strategy and dataset for algebra word problems. arXiv preprint arXiv:1609.07197 (2016) [15] Qin, J., Lin, L., Liang, X., Zhang, R., Lin, L.: Semantically-aligned universal tree-structured solver for math word problems. arXiv preprint arXiv:2010.06823 (2020) [16] Kushman, N., Artzi, Y., Zettlemoyer, L., Barzilay, R.: Learning to automatically solve algebra word problems. In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 271\u2013281 (2014) [17] Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., Hajishirzi, H.: Mawps: A math word problem repository. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152\u20131157 (2016) [18] Patel, A., Bhattamishra, S., Goyal, N.: Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191 (2021) [19] Miao, S.-Y., Liang, C.-C., Su, K.-Y.: A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772 (2021) [20] Wang, Y., Liu, X., Shi, S.: Deep neural solver for math word problems. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 845\u2013854 (2017) [21] Roy, S., Roth, D.: Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics 6, 159\u2013172 (2018)\n[22] Mitra, A., Baral, C.: Learning to use formulas to solve simple arithmetic problems. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2144\u20132153 (2016) [23] Fletcher, C.R.: Understanding and solving arithmetic word problems: A computer simulation. Behavior Research Methods, Instruments, & Computers 17(5), 565\u2013 571 (1985) [24] Bakman, Y.: Robust understanding of word problems with extraneous information. arXiv preprint math/0701393 (2007) [25] Yuhui, M., Ying, Z., Guangzuo, C., Yun, R., Ronghuai, H.: Frame-based calculus of solving arithmetic multi-step addition and subtraction word problems. In: 2010 Second International Workshop on Education Technology and Computer Science, vol. 2, pp. 476\u2013479 (2010). IEEE [26] Wang, L., Wang, Y., Cai, D., Zhang, D., Liu, X.: Translating a math word problem to an expression tree. arXiv preprint arXiv:1811.05632 (2018) [27] Yigit, G., Amasyali, M.F.: Exploring the benefits of data augmentation in math word problem solving. In: 2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA), pp. 1\u20136 (2023). IEEE [28] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023) [29] Lan, Y., Wang, L., Zhang, Q., Lan, Y., Dai, B.T., Wang, Y., Zhang, D., Lim, E.-P.: Mwptoolkit: an open-source framework for deep learning-based math word problem solvers. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 13188\u201313190 (2022) [30] Hosseini, M.J., Hajishirzi, H., Etzioni, O., Kushman, N.: Learning to solve arithmetic word problems with verb categorization. In: EMNLP, pp. 523\u2013533 (2014) [31] Zhou, L., Dai, S., Chen, L.: Learn to solve algebra word problems using quadratic programming. In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 817\u2013822 (2015) [32] Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal, A., Etzioni, O., Ang, S.D.: Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics 3, 585\u2013597 (2015) [33] Huang, D., Shi, S., Lin, C.-Y., Yin, J.: Learning fine-grained expressions to solve math word problems. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 805\u2013814 (2017)\ntranslation. arXiv preprint arXiv:1605.07869 (2016) [35] Huang, D., Liu, J., Lin, C.-Y., Yin, J.: Neural math word problem solver with reinforcement learning. In: Proceedings of the 27th International Conference on Computational Linguistics, pp. 213\u2013223 (2018) [36] Chiang, T.-R., Chen, Y.-N.: Semantically-aligned equation generation for solving and reasoning math word problems. arXiv preprint arXiv:1811.00720 (2018) [37] Li, J., Wang, L., Zhang, J., Wang, Y., Dai, B.T., Zhang, D.: Modeling intrarelation in math word problems with different functional multi-head attentions. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6162\u20136167 (2019) [38] Meng, Y., Rumshisky, A.: Solving math word problems with double-decoder transformer. arXiv preprint arXiv:1908.10924 (2019) [39] Li, S., Wu, L., Feng, S., Xu, F., Xu, F., Zhong, S.: Graph-to-tree neural networks for learning structured input-output translation with applications to semantic parsing and math word problem. arXiv preprint arXiv:2004.13781 (2020) [40] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018) [41] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach (2019) [42] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) [43] Shao, Z., Huang, F., Huang, M.: Chaining simultaneous thoughts for numerical reasoning. arXiv preprint arXiv:2211.16482 (2022) [44] Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., Chen, W.: On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336 (2022) [45] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.: Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022) [46] Pi, X., Liu, Q., Chen, B., Ziyadi, M., Lin, Z., Fu, Q., Gao, Y., Lou, J.-G., Chen, W.: Reasoning like program executors. arXiv preprint arXiv:2201.11473 (2022) [47] Chen, W., Ma, X., Wang, X., Cohen, W.W.: Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022) [48] Liang, Z., Yu, W., Rajpurohit, T., Clark, P., Zhang, X., Kaylan, A.: Let gpt be a math tutor: Teaching math word problem solvers with customized exercise generation. arXiv preprint arXiv:2305.14386 (2023) [49] Lazaridou, A., Gribovskaya, E., Stokowiec, W., Grigorev, N.: Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115 (2022) [50] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824\u201324837 (2022) [51] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020) [52] Liu, Q., Guan, W., Li, S., Cheng, F., Kawahara, D., Kurohashi, S.: Roda: reverse operation based data augmentation for solving math word problems. IEEE/ACM Transactions on Audio, Speech, and Language Processing 30, 1\u201311 (2021) [53] Raiyan, S.R., Faiyaz, M.N., Kabir, S.M.J., Kabir, M., Mahmud, H., Hasan, M.K.: Math word problem solving by generating linguistic variants of problem statements. arXiv preprint arXiv:2306.13899 (2023) [54] Liang, Z., Zhang, J., Wang, L., Qin, W., Lan, Y., Shao, J., Zhang, X.: Mwpbert: Numeracy-augmented pre-training for math word problem solving. arXiv preprint arXiv:2107.13435 (2021)\nYour task is to rephrase the given texts while preserving the numerical values and relationships inherent in the original statements. Text: A store had 27 coloring books in stock. They ended up putting them on sale and getting rid of 6 of them. The put the ones they still had onto shelves with 7 on each shelf. How many shelves did they use? Rephrased: In stock, there were 27 coloring books at a local store. During a sale, 6 were sold, and the remaining ones were neatly arranged on shelves, with 7 books on each shelf. How many shelves were utilized for this arrangement? Text: Shawn has 13 blocks. Mildred has with 2 blocks. Mildred finds another 84. How many blocks does Mildred end with? Rephrased: Shawn possesses 13 blocks, while Mildred starts with 2. If Mildred discovers an additional 84 blocks, how many blocks does Mildred have in total? Text: Melanie grew 139 turnips. Benny grew 113 turnips. How many turnips did they grow in all ? Rephrased: Melanie cultivated 139 turnips, and Benny grew 113. How many turnips did they grow in total? Text: A teacher had 29 worksheets to grade. If she graded 25, but then another 29 were turned in, how many worksheets would she have to grade? Rephrased: A teacher had 29 worksheets to grade. After grading 25, an additional 29 were turned in. How many more worksheets does the teacher need to grade? Text: A pet store has 6 bird cages. If each cage has 2 parrots and 7 parakeets in it, how many birds does the pet store have total? Rephrased: In a pet store, there are 6 bird cages. If each cage contains 2 parrots and 7 parakeets, how many birds are there in total? Text: A restaurant served 6 cakes during lunch and 9 during dinner today . How many cakes were served today ? Rephrased: Today, a restaurant served 6 cakes during lunch and 9 during dinner. How many cakes were served in total? Text: Jessica spent 10.22onacattoy, andacagecosther 11.73 . What was the total cost of Jessica \u2019s purchases ? Rephrased: Jessica spent 10.22onacattoy, andacagecosther11.73. What was the total cost of Jessica\u2019s purchases? Text: Sam had to wash 40 short sleeve shirts and 23 long sleeve shirts before school. If he had only washed 29 of them by the time school started, how many did he not wash? Rephrased: Sam had 40 short sleeve shirts and 23 long sleeve shirts to wash before schoo If he had only washed 29 of them by the time school started, how many shirts did he not wash? Text: George had 30 dollars. For his birthday he got 16 more dollars but spent 38 on a new game. How much money does he have now? Rephrased: For his birthday, George received 16morebutspent38 on a new game. How much money does he have now? Text: A pet store had 41 siamese cats and 28 house cats. During a sale they sold 15 cats. How many cats do they have left? Rephrased: In a pet store, there were 41 Siamese cats and 28 house cats. During a sale, 15 cats were sold. How many cats are left in the store? Text: Greg and Sharon own neighboring cornfields. Greg harvested 0.4 of an acre of corn on Monday and Sharon harvested 0.1 of an acre. How many more acres did Greg harvest than Sharon? Rephrased: Greg and Sharon, owners of neighboring cornfields, embarked on their harvest. Greg gathered 0.4 acres of corn on Monday, while Sharon reaped 0.1 acres. What is the difference in the number of acres Greg harvested compared to Sharon? Text: A painter needed to paint 6 rooms in a building. Each room takes 5 hours to paint. If he already painted 2 rooms, how much longer will he take to paint the rest? Rephrased: Tasked with painting a building\u2019s 6 rooms, a diligent painter spends 5 hours on each. Having completed 2 rooms, how much additional time will it take for the painter to finish the remaining ones? Text: The Spurs basketball team has 22 players. Each player has 11 basketballs. How many basketballs do they have in all? Rephrased: The Spurs basketball team boasts 22 players, each equipped with 11 basketballs. What is the total number of basketballs in their possession? Text: Haley has 63 magazines in her cabinet. She plans to send it to the recycling office in their area. If she places it in boxes which can contain 9 magazines, how many boxes will Hayley use? Rephrased: Haley intends to send 63 magazines from her cabinet to the recycling office. If she organizes them into boxes, each capable of holding 9 magazines, how many boxes will she need? Text: Frank worked 8 hours on the first 4 days of the week. How many hours did he work in all? Rephrased: Throughout the initial 4 days of the week, Frank devoted 8 hours each day to work. What is the total number of hours he worked during this period? Text: A restaurant served 6 cakes during lunch and 9 during dinner today . How many cakes were served today ? 21\nYour task is to rephrase the given texts while preserving the numerical values and relationships inherent in the original statements. Text: While playing a trivia game , Mike answered 3.0 questions correct in the first half and 5.0 questions correct in the second half . If each question was worth 3.0 points,what was his final score ? Rephrased: Engaged in a trivia game, Mike accurately responded to 3.0 questions in the initial half and 5.0 questions in the latter half. If each question carried a value of 3.0 points, what total score did Mike achieve? Text: Sam invited 9.0 friends to a birthday party , but 6.0 could n\u2019t come . If he wanted to buy enough cupcakes so each person could have exactly 2.0 , how many should he buy ? Rephrased: Inviting 9.0 friends to a birthday celebration, Sam faced the absence of 6.0 attendees. To ensure each person could enjoy exactly 2.0 cupcakes, how many should Sam purchase? Text: Keith grew 29.0 cantelopes , Fred grew 16.0 cantelopes , and Jason grew 20.0 cantelopes. How many cantelopes did they grow in total ? Rephrased: Keith, Fred, and Jason cultivated 29.0, 16.0, and 20.0 cantaloupes, respectively. What is the combined count of cantaloupes grown by the three? Text: Ezra drew a white line that was 7 inches long . Then he drew a blue line that was 3 inches long . How much longer was the white line than the blue line ? Rephrased: Creating drawings, Ezra crafted a white line measuring 7 inches and a blue line of 3 inches. What is the difference in length between the white and blue lines? Text: I have a pet golden retriever . Each year he gains 11.0 pounds . He is 8.0 years old . How many pounds does he weigh ? Rephrased: Caring for a golden retriever, he accumulates 11.0 pounds annually. At the age of 8.0 years, what is the total weight of the golden retriever? Text: finally they had to roam around 169 factories to make sure they are throwing their wastes properly . if their group went to 69 factories and the second went to 52 how many factories remain unchecked ? Rephrased: To ensure proper waste disposal, the team needed to inspect 169 factories. If one group covered 69 factories and another visited 52, how many factories are yet to be checked? Text: There are 96.0 oranges in a box . Jonathan takes 45.0 oranges . How many are left ? Rephrased: Within a box, there exist 96.0 oranges. Jonathan claims 45.0 of them. What is the remaining count? Text: james has 1222 balloons . amy has 513 balloons . how many more balloons does james have than amy ? Rephrased: Possessing 1222 balloons, James exceeds Amy\u2019s collection by how many balloons, given that Amy has 513? Text: A dealer pays 6000.0 dollars for a car . The dealer wants to make a profit that is 25.0 % of the selling price . For how much should the dealer sell the car ? Rephrased: If a dealer invests 6000.0 dollars in acquiring a car and aims for a profit constituting 25.0% of the selling price, what should be the selling price? Text: In fourth grade there were 11.0 students at the start of the year . During the year 6.0 students left and 42.0 new students came to school . How many students were in fourth grade at the end ? Rephrased: Commencing fourth grade with 11.0 students, the class experienced the departure of 6.0 students and the arrival of 42.0 new students. What is the final count of students in fourth grade? Text: If Joan bicycled 25.0 miles at 5.0 miles per hour , how long was Joan travelling ? Rephrased: Covering a distance of 25.0 miles on a bicycle moving at 5.0 miles per hour, what was the duration of Joan\u2019s travel? Text: A furniture store has a chair , originally priced at 78.0 dollars , on sale for 46.0 dollars . What is the percent of decrease , rounded to the nearest tenth ? Rephrased: The furniture store offers a chair initially valued at 78.0 dollars for a sale price of 46.0 dollars. What is the percentage decrease, rounded to the nearest tenth? Text: mrs. hilt ran 3 miles on monday 2 miles on wednesday and 7 miles on friday. how many total miles did she run that week ? Rephrased: Covering 3 miles on Monday, 2 miles on Wednesday, and 7 miles on Friday, what was the overall distance Mrs. Hilt ran during the week? Text: A petri dish originally contained 600.0 bacteria . A scientist let the bacteria grow and now there are 8917.0 of them . How many more bacteria are there now ? Rephrased: Starting with 600.0 bacteria, the population in a petri dish increased to 8917.0 due to growth. What is the additional count of bacteria? Text: Ryan has 72.0 marbles and 17.0 blocks . If he shares the marbles among 9.0 friends , how many marbles does each friend get ? Rephrased: With a possession of 72.0 marbles and 17.0 blocks, if Ryan distributes the marbles equally among 9.0 friends, what is the share per friend? Text: A restaurant served 6 cakes during lunch and 9 during dinner today . How many cakes were served today ? Rephrased: 22\n",
    "paper_type": "method",
    "attri": {
        "background": "Math Word Problem (MWP) solving presents a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for data augmentation by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new in-context learning augmentation method, employing the Llama-7b language model. This approach involves instruction-based prompting for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance.",
        "problem": {
            "definition": "The problem addressed in this paper is the challenge of solving Math Word Problems (MWPs) in NLP, which requires understanding complex relationships between numerical values and mathematical operations.",
            "key obstacle": "A significant obstacle is the lack of diversity in training data, which limits the effectiveness of existing MWP solvers, leading to overfitting and poor generalization to unseen problems."
        },
        "idea": {
            "intuition": "The idea originated from the observation that augmenting training datasets with diverse examples can enhance the performance of MWP solvers.",
            "opinion": "The proposed idea involves using various data augmentation techniques, including rule-based methods and in-context learning, to create a more extensive and varied training dataset.",
            "innovation": "The key innovation lies in the introduction of an in-context learning method using the Llama-7b model, which generates rephrased problem statements while preserving their mathematical integrity."
        },
        "method": {
            "method name": "In-Context Learning for Data Augmentation",
            "method abbreviation": "ICL-DA",
            "method definition": "This method uses instruction-based prompting with the Llama-7b language model to generate diverse rephrased examples of math word problems.",
            "method description": "The core of the method involves presenting original MWP examples to the model and generating rephrased versions that maintain the mathematical relationships.",
            "method steps": [
                "Step 1: Present original examples to the Llama-7b model for rephrasing.",
                "Step 2: Implement a filtering mechanism to ensure diversity and validity of rephrased examples.",
                "Step 3: Apply numerical modifications to introduce variability while preserving logical consistency."
            ],
            "principle": "The effectiveness of this method is based on the ability of the Llama-7b model to understand and generate contextually relevant text, thereby enhancing the diversity of training data and improving model performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on two datasets: MAWPS-Single and SVAMP, comparing the performance of 9 baseline models across various augmentation methods.",
            "evaluation method": "Performance was assessed using equation accuracy and answer accuracy metrics, comparing results against baseline performances to evaluate improvements."
        },
        "conclusion": "The results demonstrate that the proposed data augmentation methods, particularly the in-context learning approach, significantly enhance the performance of MWP solvers across different models and datasets.",
        "discussion": {
            "advantage": "The main advantages include improved model generalization, reduced overfitting, and enhanced robustness through diverse training examples.",
            "limitation": "A limitation of the proposed methods is the potential for model bias if the augmented data does not adequately represent the diversity of real-world problems.",
            "future work": "Future research will explore adversarial MWPs to further improve robustness and test the in-context learning approach with other large language models."
        },
        "other info": {
            "acknowledgments": "This research was supported by The Scientific and Technological Research Council of Turkey (T\u00dcB\u0130TAK) under grant number 120E100.",
            "data availability": "Data sharing is not applicable as no datasets were generated or analyzed during the current study."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning is introduced as a method for enhancing the diversity of training datasets in Math Word Problem (MWP) solving, which is a challenging task in Natural Language Processing (NLP)."
        },
        {
            "section number": "1.3",
            "key information": "The study employs the Llama-7b language model to facilitate in-context learning through instruction-based prompting, generating diverse rephrased examples of math word problems."
        },
        {
            "section number": "3.1",
            "key information": "The proposed in-context learning method enhances model generalization and reduces overfitting by providing diverse training examples."
        },
        {
            "section number": "3.2",
            "key information": "The introduction of the in-context learning augmentation method, ICL-DA, is based on the principle that the Llama-7b model can generate contextually relevant text, improving performance."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts in the ICL-DA method is critical, as it involves presenting original MWP examples to the model for rephrasing, thereby influencing the outcomes of in-context learning."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the proposed in-context learning methods is the potential for model bias if the augmented data does not adequately represent the diversity of real-world problems."
        },
        {
            "section number": "7",
            "key information": "The results demonstrate that the proposed data augmentation methods, particularly the in-context learning approach, significantly enhance the performance of MWP solvers across different models and datasets."
        }
    ],
    "similarity_score": 0.6917836968408014,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving.json"
}