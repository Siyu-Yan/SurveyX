{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.01929",
    "title": "Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?",
    "abstract": "Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that supervised instruction tuning has the best trade-off between performance and resource requirements. As another contribution, we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages.",
    "bib_name": "razumovskaia2024analyzingadaptinglargelanguage",
    "md_text": "# Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?\nEvgeniia Razumovskaia Ivan Vuli\u00b4c Anna Korhonen Language Technology Lab, University of Cambridge, UK {er563, iv250, alk23}@cam.ac.uk\nAbstract\nSupervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that supervised instruction tuning has the best trade-off between performance and resource requirements. As another contribution, we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages.\n 4 Mar 2024\n[cs.CL]\narXiv:2403.01929v1\n# 1 Introduction and Motivation\nRecent advances in data-efficient, few-shot learning have been crucial for increasing and promoting language inclusiveness of NLP technology (Devlin et al., 2019; Conneau et al., 2020; ImaniGooghari et al., 2023), substantially lowering the dataset sizerelated \u2018entry point\u2019 for a new language. This was made possible by pretrained language models which can generalise to a new task or language from the knowledge stored in their parameters complemented with only a handful of in-task data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e71a/e71a4d7e-0884-41d2-8acd-ca16bc992f65.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Spanish</div>\nFigure 1: Comparison of practical aspects of different learning paradigms (\u00a73.1) in the intent detection task from Multi3NLU++ (Moghe et al., 2023), with exactly the same data setup, for Amharic and Spanish. In-context learning (ICL) has low performance and high inference and computational costs while being comparatively inexpensive. Supervised fine-tuning (SFT) and supervised instructiontuning (SIT), on the other hand, have a larger financial cost but they are much more efficient in terms of inference aspects and computational resources while also performing much better both for Amharic as a representative low-resource language (1a) and Spanish as a high-resource language (1b). The standard approaches for such few-shot adaptations are Supervised Fine-Tuning (SFT), which also subsumes more recent Supervised InstructionTuning (SIT), and In-Context Learning (ICL). SFT and SIT use knowledge in pretrained model parameters for initialisation and then adapt the parameters to a language-task combination via supervised\nFigure 1: Comparison of practical aspects of different learning paradigms (\u00a73.1) in the intent detection task from Multi3NLU++ (Moghe et al., 2023), with exactly the same data setup, for Amharic and Spanish. In-context learning (ICL) has low performance and high inference and computational costs while being comparatively inexpensive. Supervised fine-tuning (SFT) and supervised instructiontuning (SIT), on the other hand, have a larger financial cost but they are much more efficient in terms of inference aspects and computational resources while also performing much better both for Amharic as a representative low-resource language (1a) and Spanish as a high-resource language (1b).\nThe standard approaches for such few-shot adaptations are Supervised Fine-Tuning (SFT), which also subsumes more recent Supervised InstructionTuning (SIT), and In-Context Learning (ICL). SFT and SIT use knowledge in pretrained model parameters for initialisation and then adapt the parameters to a language-task combination via supervised\ntraining on available, even if scarce, resources. Importantly, they yield a model specialised for a single language-task combination and can get increasingly better at the task if a larger training dataset becomes available. ICL, in contrast, uses one model \u2018as is\u2019 to complete any task, without any parameter adaptation or fine-tuning. Instead, the model is adapted via prompting: given an explanation of a task (i.e., instruction) and a set of \u2018training\u2019 examples (i.e., annotated demonstrations), the model is tasked to generate the label for every input (Radford et al., 2019). Due to the model\u2019s context size, the number of demonstrations used in the input is limited, meaning that the ICL performance is capped by the model\u2019s pretraining and the demonstrations that fit into the input context. Existing generative models used for ICL (termed Large Language Models, or LLMs henceforth) are usually pretrained in an English-centric manner with the vast majority of the pretraining corpus in English and only limited coverage of other languages (Sitaram et al., 2023), even with \u2018accidentally encountered\u2019 bilingual and translation data (Briakou et al., 2023). As a result, current LLMs are very far from serving the world\u2019s languages equally: while demonstrating impressive ICL results in English (Mishra et al., 2022a), they still face difficulties when transferring to other languages (Winata et al., 2021; Tanwar et al., 2023), especially low-resource ones (Ojo et al., 2023). In contrast, a number of encoder and encoder-decoder models, such as XLM-R (Conneau et al., 2020) or mT5 (Xue et al., 2021), used for initialisation in SFT are pretrained with much wider language coverage1 (termed multilingually Pretrained Language Models, or mPLMs) (Conneau et al., 2020; ImaniGooghari et al., 2023). This property enables sample-efficient transfer and adaptation of natural language understanding (NLU) models to a much larger array of languages (Ansell et al., 2021) than what is supported by ICL-based LLMs. While SFT, SIT and ICL are comparable approaches for few-shot multilingual NLU, there has been little attention drawn to which of the techniques works better in practice. Therefore, this paper aims to delve deeper into analysing a variety of factors which critically impact effective use of 1For instance, XLM-R (Conneau et al., 2020), mBERT (Devlin et al., 2019), LaBSE (Feng et al., 2022), and mT5 (Xue et al., 2021) cover \u223c100 languages at pretraining (albeit with different pretraining data amounts), while Glot500 (ImaniGooghari et al., 2023) covers up to 500 languages.\n1For instance, XLM-R (Conneau et al., 2020), mBERT (Devlin et al., 2019), LaBSE (Feng et al., 2022), and mT5 (Xue et al., 2021) cover \u223c100 languages at pretraining (albeit with different pretraining data amounts), while Glot500 (ImaniGooghari et al., 2023) covers up to 500 languages.\neither from a more practical point of view. Our first aim is to provide answers to the following question: (Q1) Given the same annotated examples, which of the approaches is better in practice? In particular, the sometimes vague term \u2018practice\u2019 in our work comprises the following crucial aspects: 1) sample efficiency (i.e., \u2018data cost\u2019); 2) computational requirements (\u2018computational cost\u2019); 3) latency (\u2018inference cost\u2019); and 4) overall financial or \u2018economic\u2019 cost. Prior work has been mainly focused on benchmarking ICL on subgroups of languages (Ojo et al., 2023) and only considered and optimised task performance of the models as the ultimate comparison criterion. In contrast, our work presents an extensive analysis evaluating crosslingual capabilities of SFT and ICL both on high and low-resource languages, considering not only the task performance but also the above listed practical aspects, as illustrated in Figure 1. Furthermore, prior work has demonstrated the effectiveness of parameter-efficient fine-tuning (PEFT) to improve the model\u2019s cross-task capabilities and to promote aspects of its generation abilities (e.g., open-domain chat; Dettmers et al., 2023). In this work, we also analyse how language adaptation of LLMs \u2018beyond English\u2019 impacts their NLU and NLG performance in a target language. This gives rise to another core research question: (Q2) Given the benefits of ICL as a learning paradigm (but its inferior performance in comparison to SFT), could we use the standard adaptation strategies to improve LLMs\u2019 generation and understanding capabilities in other languages? To our knowledge, this is the first work analysing how multilingual NLU capabilities of ICL with LLMs are effected by their target language adaptations, as well as studying the trade-offs for NLG. Contributions. 1) Related to Q1, we conduct a comprehensive analysis of ICL versus SFT and SIT paradigms in the context of multilingual few-shot adaptation, with the focus on multiple practical angles and cost. Our analyses show that not only the SFT and SIT approaches with smaller models lead to improved task performance but also they remain more data-, computation-, inference-effective than ICL with general-purpose LLMs. 2) Related to Q2, we investigate the effectiveness of target language adaptation, adopted from the work on mPLMs, for ICL with LLMs. The main finding is that language adaptation leads to superficially improved generation capabilities in the target language with only\nlimited improvements on the actual tasks, calling for further research that will mitigate the large language gap in LLM development and deployment between English and other languages.\n# 2 Related Work\nInstruction-Tuning LLMs aims to increase their cross-task generalisation capabilities. Instruction tuning is in essence an SFT technique where the input includes textual description of the task, demonstrations and user input queries while the output is the desirable model output for a given task in text form. Through inclusion of task descriptions into the input, at inference time the model becomes capable of completing tasks unseen during training when provided with task description (Sanh et al., 2022; Chung et al., 2022, interalia). Instruction tuning has become a standard approach to turn an LLM into a model with general capabilities to perform any task, given the instructions, off-the-shelf (Wei et al., 2022a; Mishra et al., 2022b). Extending LLMs to Other Languages. Although there is a growing trend to make NLP systems more linguistically inclusive (Bender, 2011; Doddapaneni et al., 2021), widely used generative LLMs remain predominantly English. For instance, pretraining data of LLaMA-2 and PaLM consists of 90% and 82% English text, respectively (Touvron et al., 2023; Sitaram et al., 2023), which substantially hinders their capabilities in languages other than English (Ojo et al., 2023). In an attempt to equate the models\u2019 performance across languages, there is an increasing interest in extending their multilingual capabilities. A wide range of techniques including continued pretraining (Cui et al., 2023), using self-instruction (Wei et al., 2023) or vocabulary extension (Zhao et al., 2024) have been applied. Due to wide adoption of ICL, another line of work focuses on improving cross-lingual instruction following capabilities via parameter-efficient multilingual instruction tuning (Li et al., 2023b), multilingual pretraining (Shliazhko et al., 2022) and injection of several multilingual examples in fine-tuning (Shaham et al., 2024). The methods show gains in various aspects of model\u2019s target language generation capabilities, while providing no systematic empirical comparisons to prove that improved NLG necessarily correlates with stronger NLU performance via ICL. These works provide initial insights into LLMs processing for languages other than English. In-\nterestingly, the success of mPLMs in cross-lingual transfer has always been attributed to their massively multilingual pretraining while, at first sight, LLMs seem to operate differently: they perform surprisingly well while having only a small percentage of multilingual text in their pretraining corpora (Blevins and Zettlemoyer, 2022). At the same time, little to no work has studied multilingual performance of these models in direct comparison with standard mPLMs, and even more so the practical aspects such as memory requirements or latency.\n# 3 Preliminaries: On Learning Paradigms and Practical Aspects\nWe analyse three established learning paradigms for few-shot learning in monolingual and multilingual setups, which are compared across four practical aspects: data cost, computational cost, inference cost and financial cost. We now outline each learning paradigm and practical aspect.\n# 3.1 Learning Paradigms\nLet D = (x1, y1), ..., (xN, yN) denote a training dataset where xi is the model input, yi is the label annotation and N is the number of training examples, and let M refer to a pretrained language model (LLM or mPLM). Supervised Fine-Tuning (SFT). M is adapted to a task or a language (or both) by fine-tuning its parameters on D and minimising a loss function. Note that here we use SFT in its narrower sense, to refer to \u2018standard\u2018 fine-tuning where an encoderbased model (such as mBERT) or encoder-decoder model (e.g., mT5) is tuned directly for the target task (Devlin et al., 2019; Wei et al., 2022b). At inference, the fine-tuned model M\u2032 is then used. In-Context Learning (ICL). Unlike with SFT, the parameters of M stay fixed and the model is treated as a \u2018black box\u2019. ICL relies on generative capabilities of general-purpose LLMs (Brown et al., 2020; Han et al., 2023). The model is adapted to a task by conditioning it on task instructions and in-context examples (demonstrations). Each demonstration included into a prompt consists of an input x and ground-truth annotated label y. In other words, the demonstrations are an alternative way to use the data available in D. Then, M is expected to generate the label for the test input usually included at the end of the prompt. While in SFT the model parameters are adapted to a target task, with ICL the model is expected to learn the\ntask by analogy, via the provided task description combined with demonstrations.\ntask by analogy, via the provided task description combined with demonstrations. Supervised Instruction-style Tuning (SIT). To unlock full potential of ICL, sufficiently large language models need to be used (Wei et al., 2022c), drastically raising the computational overhead at inference in comparison with SFT. SIT thus presents the middle ground between the two. Here, one fine-tunes small(er) instruction-based models to specific tasks. While SFT fine-tunes the model directly on annotated data D, SIT extends each input in D with task-specific instructions leveraging model\u2019s instruction-following capabilities (Wei et al., 2022b) obtained during pretraining. SIT typically does not include demonstrations into input, although including them there is also possible (Min et al., 2022; Chen et al., 2022), typically with small to negligible performance gains in few-shot setups but increased computational cost (Li et al., 2023a). For simplicity, we experiment only with the SIT variant without any demonstrations.\n# 3.2 Practical Aspects\nWe consider practical costs of the \u2018full cycle\u2018 of model development \u2013 from data collection costs to inference cost, and aim to associate those costs with the learning paradigms described in \u00a73.1. Data Cost. One key limiting factor for the model adaptation to new task-language (or even finergrained task-language-domain) combinations is the costly and complex data collection process, especially for low-resource languages and specialised domains. Therefore, it is crucial to develop methods which can efficiently generalise from a small number of annotated examples. In \u00a75, we analyse this data cost, that is, sample efficiency as the relationship between the number of training examples and task performance. Computational Cost. The memory requirements of LLMs keep growing proportionally to the number of their parameters. Deploying such a model to the users means that one needs to have access to and support costly infrastructure with large vRAM (Aminabadi et al., 2022; Alizadeh et al., 2023). Here, we analyse the memory requirements of each learning paradigm both for model storage and training, where applicable, and how they correlate with the target task performance. Inference Cost. Latency, or time needed by the model to complete the prediction (Huyen, 2022, Chapter 1), has the largest impact on user-facing\napplications such as task-oriented dialogue. To make the system usable, it is critical to strike a balance between strong performance and low latency. We thus analyse the inference cost in two ways as: 1) wall-clock inference time, aiming to directly approximate (relative) latency of different models; and 2) inference FLOPs, a hardware-independent metric to compare inference complexity. Financial Cost. Each of the aspects above contributes to the overall cost of each model\u2019s life cycle. As financial resources are usually limited, we also aim to (roughly) estimate the overall financial expenditure needed for each learning paradigm, including data collection, GPU and inference costs.\n# 4 Experimental Setup\nWe focus on the comparison between SFT, SIT and ICL in few-shot multilingual and cross-lingual setups, aiming to make the comparison as fair as possible across languages, learning paradigms and models, and targeting the following setups: In-Language Generalisation. We evaluate the model\u2019s ability to generalise on new examples in the same language in which fine-tuning examples or demonstrations were provided to the model. Cross-Language Generalisation. We use a model trained in one language to perform the task in another one, where the transfer typically proceeds from a high-resource language to a low-resource one. In our experiments, we assume the typical transfer direction with English as the (highresource) source language. In-Domain and Cross-Domain Generalisation. For many NLU tasks (e.g., for task-oriented dialogue) it is common to consider transferring the systems between different domains, e.g., from flight booking to the restaurant booking domain. If a model can be transferred across domains, it means that it has in-depth understanding of the classes used in the respective domain definitions/ontologies.\n# 4.1 Evaluation Tasks and Datasets\nThe main focus of the analyses, revolving around Q1 and Q2 from \u00a71, is on NLU tasks for taskoriented dialogue as one widely used and established practical application of NLP technology, due to multiple reasons. 1) Dialogue is a user-facing application where computational and memory requirements, data collection cost, inference latency and other practical concerns of the model development cycle are of ultimate importance. 2) Dia-\nDataset\nLANGS\n# TEST EX.\n# CLASSES\nMulti3NLU++\nID\nAM, EN, ES, MR, TR\n300\n62\nVE\n17\nXNLI\nNLI\nEN, RU, TR, ES\n5,010\n3\nlogue NLU tasks provide well-defined ontologies and evaluation setups, with evaluation benchmarks that comprise comparable and semantically aligned training and test data across multiple languages, including high- and low-resource ones (Moghe et al., 2023; Hu et al., 2023a), and multiple domains. 3) In contrast to standard \u2018non-dialogue\u2019 NLU tasks, dialogue NLU datasets are unlikely to have been seen and \u2018absorbed\u2018 by LLMs during their pretraining, which avoids test data leakage (Balloccu et al., 2024; Sainz et al., 2023). Dialogue-oriented evaluation is conducted on the tasks of intent detection (ID) and value extraction (VE). ID aims to classify user\u2019s utterance into a set of intent classes predefined in the domain ontology. The aim of VE is to identify the presence of ontology-related domain-specific slotvalue pairs in a given sentence. Here, we use the Multi3NLU++ dataset (Moghe et al., 2023), covering English (Casanueva et al., 2022) and the following 4 languages: Amharic (AM), Marathi (MR), Spanish (ES) and Turkish (TR). The dataset also spans two different domains: BANKING and HOTELS with a partial overlap in intent classes and slots. For both tasks we report micro-F1 scores.2 To verify that our findings extend beyond only dialogue-related NLU tasks, we also evaluate on the standard NLI task with XNLI (Conneau et al., 2018) which provides training and evaluation data in 14 languages, while we focus on a subset of 3: ES, TR, and Russian (RU), and report accuracy as the evaluation metric. Additional information on the evaluation datasets is provided in Table 1. Cross-Language Parallel Few-Shot Setup. To ensure fair comparisons of all learning paradigms across languages, we make use of the multi-parallel nature of the datasets we use. For each languagedomain combination in Multi3NLU++ (or just lan2We also note that 1) each utterance in Multi3NLU++ may have multiple intents; ID is thus a multi-label classification task. 2) Further, for VE, we consider the slot value as correctly\n# Cross-Language Parallel Few-Shot Setup. \nensure fair comparisons of all learning paradigms across languages, we make use of the multi-parallel nature of the datasets we use. For each languagedomain combination in Multi3NLU++ (or just lan-\n2We also note that 1) each utterance in Multi3NLU++ may have multiple intents; ID is thus a multi-label classification task. 2) Further, for VE, we consider the slot value as correctly labelled only if it exactly matches the gold value. Finally, 3) as in prior work (Casanueva et al., 2022; Moghe et al., 2023), the cross-domain performance for the two tasks is evaluated only on the intents and slots shared across domains. We refer to the original Multi3NLU++ work for further details.\nguage in XNLI) we sample 300 test examples.3 We also randomly sample training sets consisting of {30, 50, 100, 500, 1,000} examples which are kept exactly the same across all languages to ensure the content in training data does not coincidentally favour one of the languages.4 SFT Evaluation. We use two standard models, XLM-R-Base (Conneau et al., 2020) and LaBSE (Feng et al., 2022). LaBSE is a sentence encoder model where, following prior work (Moghe et al., 2023), we train only task-specific classifiers on top of the fixed encoder. We refer to this approach, applied only to sentence-level tasks (ID and NLI), as LaBSE+CL.5 ICL Evaluation. We evaluate the following models: Flan-T5-XL, mT0-XL, LLaMA-2-7B and GPT3.5.6 Flan-T5-XL (3B parameters; Chung et al., 2022), mT0-XL (3.7B; Muennighoff et al., 2022) and GPT-3.5 (Achiam et al., 2023) are massively instruction-tuned models. While Flan-T5 was pretrained mostly in English and several high-resource languages, mT0-XL offers a more comprehensive and balanced multilingual pretraining set. The inputs for ICL were designed in a crosslingual manner, where the task descriptions and context were in English while the few-shot examples and the sentence to be analysed were provided in the target language. This follows the recommendations from prior work where it was empirically verified that English instructions led to stronger results than in-language instructions (Shi et al., 2022; Lin et al., 2022). For each task we design the instructions (i) to match the instructions in pretraining as closely as possible, while (ii) yielding reasonable output when tested on several validation examples.7 Note that, given a fixed input context of each model, the number of demonstrations to be 3We conduct the sampling step due to a large number of experiments run; preliminary experiments with full test sets indicated exactly the same relative trends, but with much increased computational cost and time overheads. While sampling, we ensured that each intent and slot in the domain ontology occurred at least twice in the test set 4To ensure reproducibility, unique ids of the examples in the training and test splits will be made publicly available. 5For NLI as a single-label classification task, the softmax output activation function is used. In contrast, for ID we use sigmoid and consider all intents where the sigmoid activation is larger than a predefined threshold value \u03b8. Following prior work, we set \u03b8 = 0.3. 6We use GPT-3.5-turbo-instruct due to its instructionfollowing capabilities proven in prior work (Ye et al., 2023) 7For reproducibility, we will share the full instructions templates for all languages and tasks.\nused for ICL is limited: for all the models in our comparison it is less than 30, and 30 is the lowest amount of training samples we use in SFT. SIT Evaluation. Here, we include individual perclass questions into instructions: this design (i) was previously shown to result in much improved SIT performance (Fuisz et al., 2022; Razumovskaia et al., 2023), while (ii) it also fits into the model input context for tasks with a large number of classes. We rely on the same instructions as with ICL. We experiment with two models: (i) (mostly English pretrained) Flan-T5-Base (250M parameters) and multilingually oriented mT0-Base (580M).8\n# 5 (Q1) Results and Discussion: Learning Paradigms and Practical Aspects\nWe first delve into comparisons revolving around Q1 (\u00a71). The main results across different setups, models, training data sizes and learning paradigms are summarised in Figure 2, while full (numerical) results are provided in Appendix C. We now zoom into discussions originating from the results.\nWe first delve into comparisons revolving around Q1 (\u00a71). The main results across different setups, models, training data sizes and learning paradigms are summarised in Figure 2, while full (numerical) results are provided in Appendix C. We now zoom into discussions originating from the results. Data Efficiency. One of the core reasons to use ICL is its inherent data/sample efficiency. Comparing the supervised methods against ICL, we observe that the former reach or overcome the performance of ICL with all tested open-source models (Flan-T5-XL, mT0-XL and LLaMA-2-7B), even when fine-tuned with mere 30 in-task examples, while they outperform GPT-3.5 with 50 or 100 in-task examples. These findings hold across all evaluation tasks and setups. At the same time, the results also reveal several key differences between tasks and languages. Comparing the trends for ID and VE (cf., Figures 2a and 2d): for sentence classification tasks where the outputs are language-independent, the improvements of SFT over ICL are less pronounced. For instance, the gains over GPT-3.5 with 100 training examples for MR and ES are 3.15 and 4.17 F1 points,9 respectively. In contrast, the gains over ICL for value extraction as a more language-specific task are very large, 19.8 and 25.28 for MR and ES, respectively, when comparing the best-performing SFT method with ICL. Moreover, for VE, the gaps with ICL are considerable even with 30 training examples are used (3.4 and 7.5 F1 points for MR and ES).\n8SFT and SIT training hyperparameters are in Appendix B. 9The cited numbers are for in-language in-domain setups; the trends are the same in the other setups.\nFor AM a supervised model surpasses GPT-3.5 performance already with 30 training examples, while for ES 50 or 100 training examples are required, depending on the setup. For high-resource languages (EN, ES) SIT-based Flan-Base with 30 examples performs consistently better than ICL with GPT-3.5 ICL for ID and VE across the setups. We hypothesise that high performance of SIT-based Flan is caused by i) its instruction following capabilities, and ii) large-scale English pretraining which is helpful for both few-shot in-language generalisation and cross-lingual transfer from English to Spanish, a linguistically close language. For lowresource languages (AM, MR) we notice a different tendency across the domain setups: LaBSE+CL and SIT-based mT0 show the highest performance for ID and VE, respectively. This shows the importance of multilingual pretraining for the model to generalise to unseen or \u2018less seen\u2019 languages. In-Domain vs Cross-Domain Evaluation. The comparison in cross-domain setups consistently shows that SIT outperforms SFT and ICL (see Figure 2b), corroborating findings from prior work on English (Razumovskaia et al., 2023). We speculate that the success of SIT in cross-domain setups stems from the model\u2019s ability to follow instructions obtained during pretraining and the ability to extract the class semantics from instructions obtained during fine-tuning. The best-performing instruction-tuned LLM, however, depends on the target language: on low-resource languages multilingually pretrained models such as mT0 perform consistently better than English-pretrained models such as Flan, while we observe reversed trends for high-resource languages (ES). Cross-Lingual Zero-Shot Transfer. Figure 2c presents the results for zero-shot transfer in indomain setups: performance across languages for all approaches is substantially lower than the performance in English. Further, as expected, performance on low-resource languages is considerably lower than on high resource languages. The results in Table 2 show that for ICL, unlike for SFT (Lauscher et al., 2020), providing the model with data examples in the target language does not always improve the final performance. Target language demonstrations seem to be helpful to the models which have strong instruction-following capabilities and are familiar with the target language (e.g., ES performance of Flan and GPT-3.5). Seen vs Unseen Tasks. Figure 2e demonstrates\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c2d1/c2d1ae84-7d80-4c58-af0e-9aa711d6db52.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) ID: In-Language In-Domain</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6b09/6b09f957-f4d6-4231-83ce-f818f8e11e64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) ID: In-Language Cross-Domain</div>\n<div style=\"text-align: center;\">(c) ID: Cross-Lingual In-Domain</div>\n<div style=\"text-align: center;\">(e) XNLI</div>\n<div style=\"text-align: center;\">Figure 2: Intent detection, value extraction and NLI results for the six languages in our evaluation. This performance is in line with other prior work (Hu et al., 2023b). We exclude LLaMa-2 results as its performance was 0.0 across all tasks. Results for VE in other setups are provided in Appendix D.</div>\nthe effectiveness of ICL with Flan and GPT-3.5 for XNLI as the \u2018seen task\u2019,10 with different patterns observed for the tasks with unseen data (i.e., ID and VE). This discrepancy is especially pronounced for high-resource languages. SIT vs ICL. In general, the results indicate that SIT consistently leads to better results than ICL in few-shot setups. Smaller SIT-based models can\n10XNLI is based on the English MultiNLI data (Williams et al., 2018), which has been used for instruction-training of many LLMs (Muennighoff et al., 2022; Chung et al., 2022).\n10XNLI is based on the English MultiNLI data (Williams et al., 2018), which has been used for instruction-training of many LLMs (Muennighoff et al., 2022; Chung et al., 2022).\nModel\nAM\nMR\nES\nTR\nGPT-3.5\nICLt\n19.19\n48.28\n63.25\n59.27\nICLen\n14.87\n38.67\n57.64\n47.50\nFlan\nICLt\n3.28\n3.02\n45.26\n31.36\nICLen\n6.33\n6.68\n44.70\n28.04\nmT0\nICLt\n3.61\n4.71\n4.60\n3.36\nICLen\n7.19\n7.99\n7.23\n6.45\nTable 2: ICL results on the ID task with English (ICLen) or target language (ICLt) demonstrations. even outperform ICL with GPT3.5 when 100+ task examples are available. Due to its sample efficiency and strong performance in cross-domain and cross-\nTable 2: ICL results on the ID task with English (ICLen) or target language (ICLt) demonstrations.\neven outperform ICL with GPT3.5 when 100+ task examples are available. Due to its sample efficiency and strong performance in cross-domain and cross-\nParadigm: Model\nMEMORY cost\nINFERENCE cost\nMax (GB)\nStorage (GB)\nTime (s)\nFLOPs (109)\nSFT: LaBSE+CL\n1.80\n1.80\n0.004\n2.05\nSFT: XLM-R\n4.14\n1.04\n0.004\n11.18\nSIT: Flan-Base\n3.32\n0.85\n0.059\n23.23\nSIT: mT0-Base\n5.82\n1.45\n0.081\n20.44\nICL: Flan-XL\n10.37\n10.37\n1.58\n39.03\nICL: mT0-XL\n12.03\n12.03\n1.20\n55.00\nICL: GPT-3.5\n-\n-\n2.78\n-\nTable 3: Memory and inference costs of SFT, SIT and ICL, measured on the ID test dataset. MEMORY Max is the peak fine-tuning or storage memory cost of each approach. MEMORY Storage refers to storage requirements per models. For all models but GPT-3.5 the measurements were conducted on a single RTX-3090 GPU. For GPT-3.5, we report average response time per example.\nlingual setups, SIT also mitigates the issue of using a separate model for each task-language or tasklanguage-domain combination.\n# 5.1 Analyses of Practical Costs\nGiven that the results above indicate that the two supervised paradigms (SFT and SIT) substantially outscore ICL in terms of task performance in general, we now focus on comparing them in terms of practical aspects. The summary is presented in Figure 1 (see \u00a71) and Table 3. Computational (Memory) Cost. Besides improved task performance, another advantage of SFT and SIT is that the underlying high-performing models are much smaller and thus have lower memory requirements. The largest memory cost for ICL is storing the model\u2019s parameters at inference time while for SFT and SIT it is the memory requirements during fine-tuning. We rely on HuggingFace Memory Calculator to establish vRAM needed for training and inference of every paradigm. We measure the memory requirements in full precision and using the AdamW optimiser (Loshchilov and Hutter, 2019), when applicable.11 The results indicate that models used for ICL have more than 2\u00d7 higher memory needs than mT0 and Flan-T5-base used for SIT in our experiments. Another angle to memory requirements is the storage cost, i.e., how much memory is needed to store a given model (\u2018as is\u2019 for ICL and after fine-tuning for SFT and SIT). Table 3 suggests that storage cost for models used for ICL is at least 4\u00d7 higher than the models used in SIT and SFT. Inference Cost. Beyond average wall-clock in-\n11Closed-source GPT-3.5 is excluded from the comparison.\n11Closed-source GPT-3.5 is excluded from the comparison.\nference time per test example. we also report the number of FLOPs measured using fvcore, also averaged per test example. As expected, the inference cost scales with the size of the underlying model, with inference time of GPT-3.5 being more than 3x higher than that of SIT-ed models, and inference FLOPs of open-source ICL models being 2.5\u00d7 higher than for their smaller SIT-ed counterparts. While SFT methods demonstrate even higher inference efficiency, we observe that SIT has the best trade-off between inference cost and performance. Financial Cost. Having demonstrated considerably higher inference costs of ICL, we also consider overall economic costs required for SFT, SIT, and ICL. Target language data annotation accounts for the largest expenditure in the process. We calculate the annotation cost based on Moghe et al. (2023). ICL consumes up to 30 annotated examples, with total costs of \u00a315.9 and \u00a318.6 for high-resource and low-resource languages, respectively, where the annotations get obtained both for ID and VE. In the VE task, SIT-based methods reach or surpass the ICL performance of the strongest model (GPT3.5) already with 20 extra examples (i.e., with 50+ training examples for supervised learning), which only adds \u00a311 or \u00a310 to the overall cost for lowand high-resource languages, respectively. Given the larger inference time and computational costs of the ICL, the total ongoing costs are likely to be larger than the one-time additional annotation budget. To put the numbers in context, the inference cost of 300 test examples with GPT-3.5 is between \u00a33 and \u00a34 for high- and low-resource languages, respectively. Put simply, the actual cost balance should take into account also the tentative number of inference calls. Further, while increasing the input context length of LLMs is an active research area (Press et al., 2022; Rubin and Berant, 2023, among others), many models relying on the ICL paradigm are still constrained by context length, and there is evidence that ICL performance even gets quickly saturated with the addition of extra in-context examples (Chen et al., 2023; Li et al., 2023a) and that the long context is not leveraged adequately (Liu et al., 2023). On the contrary, unlike with ICL, our experiments demonstrate that performance of SFT and SIT improves with more annotated examples (both in-language and cross-lingually, see Figure 2). Data annotation of 100 training examples raises the annotation cost by an average of \u00a337.5 while in-\ncreasing the ID and VE performance by an average of 15 F-1 points over ICL with GPT-3.5.\n# 6 (Q2) Results and Discussion: Target Language Adaptation of LLMs\n\u00a75 indicates that ICL is consistently inferior to the two supervised learning paradigms, SFT and SIT, not only in terms of task performance but also concerning computational and inference costs. At the same time, ICL relies on a single model and is thus appealing when extending a system to a large number of language-task combinations. Prior work on \u2018decoder-only\u2019 LLMs demonstrated the effectiveness of parameter-efficient finetuning (PEFT) to improve their cross-task generalisation capabilities (Page-Caccia et al., 2024), whereas PEFT is a standard approach for crosslingual adaptation of \u2018encoder-only\u2019 and \u2018encoderdecoder\u2019 models such as XLM-R or mT5 (Conneau et al., 2020; Xue et al., 2021). In this work, we explore whether such language-specific PEFT-style adaptation can improve ICL and generation capabilities of LLMs in languages other than English. We focus on LLaMA-2-7B as our base model, as: (i) it is a \u2018decoder-only\u2019 model that (ii) has been trained as the \u2018English-first\u2019 model, with almost 90% of its pretraining data in English; and (iii) it displayed the lowest performance in our experiments in \u00a74 while being the largest model in our evaluation. Language Adaptation Setup. We use QLoRA (Dettmers et al., 2023) as a standard PEFT-based language adaptation technique. QLoRA performs two modifications to the base LLM. The model is first quantised to reduce the memory requirements and then a low-rank adapter (Hu et al., 2022) is trained on top of the quantised model. In our experiments the adapter is tuned on the target language data, aiming to boost the target language capabilities of the underlying LLM. For the adaptation experiments, we focus on three languages: Spanish, Turkish and Marathi. The adapter for each language is trained on the respective portion of mC4 (Xue et al., 2021). Hyperparameters are set following Dettmers et al. (2023), with exact details available in Appendix E.12\n# 6.1 Generation after Language Adaptation?\nFirst, we assess whether target language adaptation boosts generation capabilities of the LLM in the target language. To this end, we use the 12Training each QLoRA adapter requires over 24 GPU-h.\nBactrian-X dataset (Li et al., 2023b), a multilingual instruction dataset containing parallel instructionresponse pairs in 52 languages. For our evaluation, we use a subset of 100 randomly sampled examples ensuring the same parallel examples across the three languages in our evaluation (ES, TR, MR). Generation Evaluation: True or Superficial Improvements? We focus on the three aspects of generation capabilities: (i) whether the model outputs text in the same language as expected by the input (i.e., I/O language agreement, similarly to Kew et al., 2023); (ii) naturalness of the generated text; (iii) lexical overlap between golden responses and generation outputs. I/O language agreement involves doing automated language identification of the generated text and establishing whether it corresponds to the input text. For this purpose, we use the current state-of-the-art language identification model, GlotLID-500 (Kargaran et al., 2023). We evaluate naturalness via MAUVE (Pillutla et al., 2021) which measures the distributional gap between human written and generated texts. For lexical overlap, we report ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002). Figure 3 shows consistent gains of generation capabilities over the three evaluation aspects after target language adaptation, with especially large improvements for Marathi as the lowest-resource language. The I/O agreement scores suggest that through language adaptation LLM\u2019s abilities to generate text in the target languages are reinforced. However, those standard metrics still do not fully capture the potential usefulness of generated output and (improved) generation capabilities. We thus also conduct human-based evaluation for Spanish across the following two axes: naturalness and usefulness. Each output is evaluated on a simple 3point Likert-like scale.13 Interestingly, the average naturalness score raises from 1.4 to 2.2 after language adaptation while usefulness only increases from 1.4 to 1.6. In practice, this means that even after language adaptation the model is still far from being useful for the target language speakers. This finding corroborates preliminary observations of Kew et al. (2023) that the English-centric models can learn to generate text in a target language comparatively easily, but useful instruction-following capabilities still remain largely out of reach. Put simply, while generated text in the target language becomes more fluent, its coherence and relevance 13Annotation instructions are provided in Appendix G.\n13Annotation instructions are provided in Appendix G.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65a9/65a92c4d-e0b5-427f-a286-4bb72c5c21a0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Generation evaluation after target language adaptation (LLaMA-2).</div>\nremain limited.\n# 6.2 NLU after Language Adaptation?\nGiven only superficial improvements in generation capabilities, we now assess whether the ICL capabilities improve for NLU tasks. For brevity, we focus on XNLI as the least complex NLU task. Even for XNLI, we observe only a negligible nonsignificant improvement from the average accuracy score of 30.5 to 30.7.14 Performance is in fact below random (33%), supporting the observations from \u00a75 that resource-efficient ICL requires both multilingual pretraining and instruction tuning. From qualitative assessment of the outputs, we notice that the models struggle to follow the task description and instructions, and often do not adhere to output formatting requirements. Massively Multilingually Adapted LLMs in NLU tasks. The results above suggest that direct target language adaptation of \u2018English-first\u2019 LLMs such as LLaMA-2 does not yield any benefits to ICL performance in NLU tasks. Next, we study whether massively multilingual adaptation of \u2018English-first\u2019 models, as done in very recent work, can improve their ICL capabilities in different languages. We evaluate the MaLa-500 model (Lin et al., 2024) which was adapted for 534 languages using the Glot-500-c dataset (ImaniGooghari et al., 2023), and is also based on LLaMA-2.15 We focus on the ID task to evaluate MaLa\u2019s ICL performance in the (easiest) in-language in-domain setup, with results summarised in Table 4. They reveal that, while adaptation gives marginal improvements for ICL, the performance is still extremely low and 14Per-language scores are provided in Appendix F. Similar relative trends have been observed in preliminary experiments on another, more complex NLU task: Belebele (Bandarkar et al., 2023), where the results are on-par or lower than the random choice baseline, as well as in more complex NLU tasks from our evaluation in \u00a75. 15MaLA-500 was adapted using: (i) LoRA-based parameter adaptation; (ii) vocabulary extension to accommodate for languages that do not use the Latin script.\nModel\nAM\nEN\nMR\nES\nTR\nICL: LLaMA-2\n0.0\n0.0\n0.0\n0.0\n0.0\nICL: MaLA-500\n1.0\n3.01\n0.0\n1.0\n3.01\nICL: GPT-3.5\n19.19\n64.22\n48.28\n58.25\n46.12\nSIT: mT0\n26.13\n68.00\n51.44\n61.69\n51.60\nTr-Test + ICL: GPT-3.5\n32.25\n\u2013\n48.49\n47.95\n48.60\nTable 4: ICL results on the ID task in the inlanguage in-domain setup.\nlags substantially behind GPT-3.5 performance and SIT with mT0-Base with 100 training examples. A comparison with translate-test baseline shows that while translate-test benefits low-resource AM, it is still outperformed by SIT on all other languages. Overall, the scores suggest that more work is needed on multilingual adaptation of LLMs to unlock their ICL capabilities in other languages, and current adaptation strategies do not yield models with competitive (nor even useful at all) NLU.\n# 7 Conclusions and Future Work\nThis work has provided a series of in-depth analyses of multilingual capabilities of three learning paradigms, two supervised ones versus in-context learning (ICL), with the focus on few-shot learning and NLU tasks. Besides task performance, the focus of the analyses has also been on multiple practical aspects (e.g., data efficiency, memory requirements, inference latency). As some of the key findings, we highlight that supervised approaches outperform ICL, even when substantially larger LLMs with higher inference costs are used for ICL. In addition, the analysis of target language adaptation on top of standard LLMs also does not paint a bright picture for multilingual NLP at the moment: while fluency of generated output improves post-adaptation, the output coherence and usefulness remains limited, plus the adapted LLMs lag substantially behind other (weakly supervised) approaches in NLU tasks for target languages. In general, our work has affirmed the importance of multilingual pretraining and the potential of supervised training on top of LLMs. Future work should invest more effort into the creation of massively multilingual- and multitask-pretrained LLMs with higher language coverage. Further, our analysis in \u00a76 calls for new and improved language adaptation methods atop the LLMs.\n# Acknowledgments\nThe work has been in part supported by a Huawei research donation to the Language Technology Lab\nat the University of Cambridge. It has also been supported by the UK Research and Innovation (UKRI) Frontier Research Grant EP/Y031350/1 EQUATE (the UK government\u2019s funding guarantee for ERC Advanced Grants) awarded to Anna Korhonen at the University of Cambridge. The work of Ivan Vuli\u00b4c has been supported in part by a Royal Society University Research Fellowship \u2018Inclusive and Sustainable Language Technology for a Truly Multilingual World\u2019 (no 221137; 2022-).\n# References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. ArXiv preprint, abs/2303.08774.\nKeivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. 2023. Llm in a flash: Efficient large language model inference with limited memory. ArXiv preprint, abs/2312.11514.\nReza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeedinference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315. IEEE.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glava\u0161, Ivan Vuli\u00b4c, and Anna Korhonen. 2021. MAD-G: Multilingual adapter generation for efficient crosslingual transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762\u20134781, Punta Cana, Dominican Republic. Association for Computational Linguistics.\nSimone Balloccu, Patr\u00edcia Schmidtov\u00e1, Mateusz Lango, and Ond\u02c7rej Du\u0161ek. 2024. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms. ArXiv preprint, abs/2402.03927.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023. The Belebele Benchmark: a parallel reading comprehension dataset in 122 language variants. ArXiv preprint, abs/2308.16884.\nEmily M Bender. 2011. On achieving and evaluating language-independence in nlp. Linguistic Issues in Language Technology, 6.\nTerra Blevins and Luke Zettlemoyer. 2022. Language contamination helps explain the crosslingual capabilities of english pretrained models. ArXiv preprint, abs/2204.08110.\nEleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a haystack: On the role of incidental bilingualism in PaLM\u2019s translation capability. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9432\u20139452, Toronto, Canada. Association for Computational Linguistics.\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nnigo Casanueva, Ivan Vuli\u00b4c, Georgios Spithourakis, and Pawe\u0142 Budzianowski. 2022. NLU++: A multi-label, slot-rich, generalisable dataset for natural language understanding in task-oriented dialogue. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1998\u20132013, Seattle, United States. Association for Computational Linguistics.\nJiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023. How many demonstrations do you\nneed for in-context learning? In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11149\u201311159, Singapore. Association for Computational Linguistics.\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719\u2013730, Dublin, Ireland. Association for Computational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instructionfinetuned language models. ArXiv preprint, abs/2210.11416.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485, Brussels, Belgium. Association for Computational Linguistics.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama and alpaca. ArXiv preprint, abs/2304.08177.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. ArXiv preprint, abs/2305.14314.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. Sumanth Doddapaneni, Gowtham Ramesh, Mitesh M Khapra, Anoop Kunchukuttan, and Pratyush Kumar. 2021. A primer on pretrained multilingual language models. ArXiv preprint, abs/2107.00676. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Languageagnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878\u2013891, Dublin, Ireland. Association for Computational Linguistics. Gabor Fuisz, Ivan Vulic, Samuel Gibbons, I\u00f1igo Casanueva, and Pawe\u0142 Budzianowski. 2022. Improved and efficient conversational slot labeling through question answering. ArXiv preprint, abs/2204.02123. Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. 2023. Understanding in-context learning via supportive pretraining data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12660\u201312673, Toronto, Canada. Association for Computational Linguistics. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Songbo Hu, Han Zhou, Mete Hergul, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Ivan Vuli\u00b4c, and Anna Korhonen. 2023a. Multi 3 woz: A multilingual, multi-domain, multi-parallel dataset for training and evaluating culturally adapted task-oriented dialog systems. Transactions of the Association for Computational Linguistics, 11:1396\u20131415. Songbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Anna Korhonen, and Ivan Vuli\u00b4c. 2023b. A systematic study\nGabor Fuisz, Ivan Vulic, Samuel Gibbons, I\u00f1igo Casanueva, and Pawe\u0142 Budzianowski. 2022. Improved and efficient conversational slot labeling through question answering. ArXiv preprint, abs/2204.02123.\nXiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. 2023. Understanding in-context learning via supportive pretraining data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12660\u201312673, Toronto, Canada. Association for Computational Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nongbo Hu, Han Zhou, Mete Hergul, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Ivan Vuli\u00b4c, and Anna Korhonen. 2023a. Multi 3 woz: A multilingual, multi-domain, multi-parallel dataset for training and evaluating culturally adapted task-oriented dialog systems. Transactions of the Association for Computational Linguistics, 11:1396\u20131415.\nSongbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Anna Korhonen, and Ivan Vuli\u00b4c. 2023b. A systematic study\nof performance disparities in multilingual taskoriented dialogue systems. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6825\u20136851, Singapore. Association for Computational Linguistics.\nChip Huyen. 2022. Designing machine learning systems. \" O\u2019Reilly Media, Inc.\".\nAyyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andr\u00e9 Martins, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2023. Glot500: Scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1082\u20131117, Toronto, Canada. Association for Computational Linguistics.\nAmir Kargaran, Ayyoob Imani, Fran\u00e7ois Yvon, and Hinrich Schuetze. 2023. GlotLID: Language identification for low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6155\u20136218, Singapore. Association for Computational Linguistics.\nTannon Kew, Florian Schottmann, and Rico Sennrich. 2023. Turning english-centric llms into polyglots: How much multilinguality is needed? ArXiv preprint, abs/2312.12683.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli\u00b4c, and Goran Glava\u0161. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483\u20134499, Online. Association for Computational Linguistics.\nChengzu Li, Han Zhou, Goran Glava\u0161, Anna Korhonen, and Ivan Vuli\u00b4c. 2023a. On task performance and model calibration with supervised and self-ensembled in-context learning.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023b. Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation. ArXiv preprint, abs/2305.15011.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\nPeiqin Lin, Shaoxiong Ji, J\u00f6rg Tiedemann, Andr\u00e9 FT Martins, and Hinrich Sch\u00fctze. 2024. Mala-500: Massive language adaptation of large language models. ArXiv preprint, abs/2401.13303.\ni Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019\u20139052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. ArXiv preprint, abs/2307.03172.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics.\n# Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learn-\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022a. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics.\nfor Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022b. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics. Nikita Moghe, Evgeniia Razumovskaia, Liane Guillou, Ivan Vuli\u00b4c, Anna Korhonen, and Alexandra Birch. 2023. Multi3NLU++: A multilingual, multi-intent, multi-domain dataset for natural language understanding in task-oriented dialogue. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3732\u20133755, Toronto, Canada. Association for Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. ArXiv preprint, abs/2211.01786. Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and David I Adelani. 2023. How good are large language models on african languages? ArXiv preprint, abs/2311.07978. Lucas Page-Caccia, Edoardo Maria Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni. 2024. Multi-head adapter routing for cross-task generalization. Advances in Neural Information Processing Systems, 36. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Za\u00efd Harchaoui. 2021. MAUVE: measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems 34:\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022b. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics.\nLucas Page-Caccia, Edoardo Maria Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni. 2024. Multi-head adapter routing for cross-task generalization. Advances in Neural Information Processing Systems, 36.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Za\u00efd Harchaoui. 2021. MAUVE: measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 4816\u20134828.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nEvgeniia Razumovskaia, Goran Glava\u0161, Anna Korhonen, and Ivan Vuli\u00b4c. 2023. Sqatin: Supervised instruction tuning meets question answering for improved dialogue nlu. ArXiv preprint, abs/2311.09502.\nOhad Rubin and Jonathan Berant. 2023. Longrange language modeling with self-retrieval. ArXiv preprint, abs/2306.13421.\nebastian Ruder, Jonathan Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel Sarr, Xinyi Wang, John Wieting, Nitish Gupta, Anna Katanova, Christo Kirov, Dana Dickinson, Brian Roark, Bidisha Samanta, Connie Tao, David Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle, Melvin Johnson, Dmitry Panteleev, and Partha Talukdar. 2023. XTREME-UP: A user-centric scarce-data benchmark for under-represented languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1856\u20131884, Singapore. Association for Computational Linguistics.\nOscar Sainz, Jon Campos, Iker Garc\u00eda-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10776\u201310787.\nOscar Sainz, Jon Campos, Iker Garc\u00eda-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10776\u201310787.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler,\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler,\nArun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nUri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024. Multilingual instruction tuning with just a pinch of multilinguality. ArXiv preprint, abs/2401.01854.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2022. mgpt: Fewshot learners go multilingual. ArXiv preprint, abs/2204.07580.\nSunayana Sitaram, Monojit Choudhury, Barun Patra, Vishrav Chaudhary, Kabir Ahuja, and Kalika Bali. 2023. Everything you need to know about multilingual llms: Towards fair, performant and reliable models for languages of the world. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 21\u201326.\nshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy Chakraborty. 2023. Multilingual LLMs are better cross-lingual incontext learners with alignment. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6292\u20136307, Toronto, Canada. Association for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022b. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022c. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022.\nXiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. 2023. Polylm: An open source polyglot large language model. ArXiv preprint, abs/2307.06018.\ndina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale Fung. 2021. Language models are few-shot multilingual learners. In Proceedings of the 1st Workshop on Multilingual Representation Learn-\ning, pages 1\u201315, Punta Cana, Dominican Republic. Association for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. ArXiv preprint, abs/2303.10420. Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. Llama beyond english: An empirical study on language capability transfer. ArXiv preprint, abs/2401.01055.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. ArXiv preprint, abs/2303.10420.\nJun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. Llama beyond english: An empirical study on language capability transfer. ArXiv preprint, abs/2401.01055.\nTask\nInstruction Text\nID\nThe aim is to understand user\u2019s intent from the\nutterance.\nInclude all applicable options exactly as they\nare provided. Separate the classes\nby hyphen.\nIf no options are applicable, return an empty\nstring.\nOptions:\n- to deny something\n- to ask about savings account\n<list of all options applicable in the domain>\nUtterance: {demonstration1}\nIntents: {intent1}-{intent2}-{intent3}\n<all in-context demonstrations>\nUtterance: {test example}\nIntents:\nVE\nThe aim is to extract slot values from the user\nutterance.\nUse $$ as delimiter between slot-value pairs.\nThe slot values should be tagged as:\n- amount_of_money: specific amount of money\n- adults: number of adults\n<list of all slot classes applicable in a given\ndomain>\nUtterance: {demonstration1}\nValues: {slot_class1}:{value1}$${slot_class2}:{value2}\n<all in-context demonstrations>\nUtterance: {test example}\nValues:\nNLI\nThe aim is to determine whether the premise\nentails, contradicts or is neutral\nwith respect to the hypothesis. Only output the\nlabel.\nPremise: {premise-demonstration1}\nHypothesis: {hypothesis-demonstration2}\nDoes the premise entail, contradict, is neutral to\nthe hypothesis?\nAnswer: {label1}\n<all in-context demonstrations>\nPremise: {premise-test}\nHypothesis: {hypothesis-test}\nDoes the premise entail, contradict or is neutral\nto the hypothesis?\nAnswer:\nTable 5: Text of instructions used in ICL. ID and NLI: instructions were adapted from Flan (Chung et al., 2022) with intent descriptions from the ontology provided with NLU++ (Casanueva et al., 2022). VE: instructions were adapted from XTREME-UP (Ruder et al., 2023) and Ojo et al. (2023).\nHyperparameter\nValue\nLaBSE+CL: dim\n512\nLaBSE+CL: non-linearity\ntanh\nBatch size\n32\nLearning rate\n2e-5\nWeight Decay\n0.1\nEvaluation Frequency\n500 steps\nMax Epochs\n500\nOptimiser\nAdamW\nTable 6: Fine-tuning hyperparameters used across supervised training experiments. The rest of the parameters were set to the default values in Huggingface Transformers.\n# C Full Experimental Results\n# D Further Value Extraction Results\nD Further Value Extraction Results\nIn-domain results\nCross-domain results\nSamples\nAM\nEN\nMR\nES\nTR\nAM\nEN\nMR\nES\nTR\nSFT: LaBSE+CL\n30\n0.2998\n0.3253\n0.308\n0.3295\n0.3224\n0.2041\n0.1978\n0.1507\n0.178\n0.1773\n50\n0.3502\n0.3863\n0.3679\n0.4014\n0.3826\n0.2362\n0.2305\n0.1700\n0.2123\n0.1962\n100\n0.4409\n0.5007\n0.4773\n0.4836\n0.4815\n0.2688\n0.2774\n0.2304\n0.2485\n0.2432\n500\n0.6606\n0.7509\n0.7235\n0.7412\n0.7328\n0.4728\n0.5204\n0.4780\n0.4936\n0.5169\n1000\n0.7116\n0.7978\n0.7736\n0.7900\n0.7825\n0.5119\n0.5759\n0.5225\n0.5516\n0.5539\nSFT: XLM-R\n30\n0.1434\n0.1435\n0.1457\n0.1857\n0.1317\n0.0284\n0.0456\n0.0463\n0.0799\n0.0544\n50\n0.1676\n0.1946\n0.1696\n0.2060\n0.1750\n0.0200\n0.0042\n0.0226\n0.0318\n0.0021\n100\n0.2879\n0.3363\n0.3115\n0.3421\n0.288\n0.1196\n0.1176\n0.0848\n0.1009\n0.1123\n500\n0.5882\n0.742\n0.6592\n0.7075\n0.6898\n0.4107\n0.5076\n0.4441\n0.4694\n0.4806\n1000\n0.6715\n0.8066\n0.7391\n0.7862\n0.7721\n0.4943\n0.5822\n0.5282\n0.5223\n0.5584\nSIT: Flan-T5-Base\n30\n0.156\n0.6625\n0.1542\n0.4969\n0.2727\n0.0750\n0.5369\n0.0677\n0.4081\n0.1419\n50\n0.1520\n0.7110\n0.1434\n0.5468\n0.3282\n0.0999\n0.5794\n0.0904\n0.4535\n0.1888\n100\n0.1432\n0.7483\n0.1501\n0.6242\n0.3865\n0.1168\n0.6103\n0.0638\n0.4882\n0.2094\n500\n0.1769\n0.8601\n0.1780\n0.7873\n0.6341\n0.1716\n0.7355\n0.1620\n0.6421\n0.4434\n1000\n0.2040\n0.8877\n0.1680\n0.8333\n0.6957\n0.1699\n0.7602\n0.1679\n0.7017\n0.5453\nSIT: mT0-Base\n30\n0.0560\n0.3735\n0.1558\n0.2646\n0.1505\n0.0125\n0.097\n0.0269\n0.0684\n0.0228\n50\n0.0962\n0.5375\n0.3309\n0.4614\n0.3068\n0.0169\n0.2868\n0.1059\n0.1957\n0.0825\n100\n0.2613\n0.68\n0.5142\n0.6169\n0.516\n0.0936\n0.4795\n0.3009\n0.4209\n0.3151\n500\n0.6488\n0.8222\n0.7466\n0.7978\n0.7579\n0.5394\n0.6711\n0.5985\n0.6441\n0.6163\n1000\n0.6980\n0.8559\n0.7889\n0.8393\n0.8157\n0.5892\n0.7113\n0.6264\n0.6798\n0.6681\nICL: Flan-T5-XL\n0.0328\n0.4927\n0.0302\n0.4526\n0.3136\n0.0554\n0.5375\n0.0581\n0.4176\n0.3012\nICL: mT0-XL\n0.0361\n0.064\n0.0471\n0.0460\n0.0336\n0.0969\n0.0989\n0.0947\n0.1049\n0.1006\nICL: GPT-3.5\n0.1919\n0.6422\n0.4828\n0.5825\n0.4612\n0.1501\n0.5552\n0.3283\n0.4728\n0.4320\nTable 7: Per-language intent detection results for in-domain and cross-domain setups.\nIn-domain results\nCross-domain results\nSamples\nAM\nEN\nMR\nES\nTR\nAM\nEN\nMR\nES\nTR\nSFT: XLM-R\n30\n0.1566\n0.2748\n0.1953\n0.2444\n0.2681\n0.0275\n0.0336\n0.0369\n0.03\n0.0232\n50\n0.2199\n0.3234\n0.2603\n0.3221\n0.3098\n0.049\n0.0543\n0.0329\n0.0462\n0.031\n100\n0.4003\n0.4991\n0.3598\n0.4615\n0.4665\n0.0362\n0.0279\n0.0229\n0.0469\n0.0072\n500\n0.6130\n0.7392\n0.6118\n0.6508\n0.6937\n0.036\n0.06\n0.05\n0.103\n0.098\n1000\n0.6468\n0.7801\n0.6614\n0.6855\n0.7539\n0.05\n0.087\n0.083\n0.137\n0.117\nSIT: Flan-T5-Base\n30\n0.0191\n0.327\n0.0156\n0.2091\n0.1174\n0.0019\n0.2514\n0.0018\n0.07\n0.0511\n50\n0.0362\n0.4486\n0.0083\n0.2627\n0.1537\n0.009\n0.3006\n0.0031\n0.1089\n0.0887\n100\n0.0555\n0.5728\n0.0198\n0.3678\n0.2705\n0.0103\n0.4043\n0.005\n0.1987\n0.1395\n500\n0.0896\n0.7314\n0.042\n0.5073\n0.4615\n0.0313\n0.5956\n0.0141\n0.3689\n0.3272\n1000\n0.1055\n0.8041\n0.0484\n0.5707\n0.5552\n0.0445\n0.6244\n0.014\n0.3975\n0.3577\nSIT: mT0-Base\n30\n0.1193\n0.3182\n0.1246\n0.2893\n0.1886\n0.0433\n0.1615\n0.0688\n0.1162\n0.1011\n50\n0.1774\n0.3954\n0.1899\n0.347\n0.2488\n0.0511\n0.2153\n0.1118\n0.1679\n0.1371\n100\n0.3313\n0.58\n0.3167\n0.4723\n0.4055\n0.0972\n0.3345\n0.14\n0.212\n0.1927\n500\n0.6093\n0.779\n0.5596\n0.6458\n0.6596\n0.3864\n0.5838\n0.3562\n0.",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Recent advances in data-efficient, few-shot learning have been crucial for increasing and promoting language inclusiveness of NLP technology, substantially lowering the dataset size-related \u2018entry point\u2019 for a new language. This was made possible by pretrained language models which can generalize to a new task or language from the knowledge stored in their parameters complemented with only a handful of in-task data.",
            "purpose of benchmark": "The benchmark is intended to facilitate a systematic comparison of three approaches to few-shot multilingual natural language understanding (NLU): supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context learning (ICL). It aims to evaluate their performance across various practical aspects, including computational costs and efficiency."
        },
        "problem": {
            "definition": "The benchmark addresses the challenges of few-shot multilingual NLU, particularly the effectiveness of different learning paradigms under varying resource conditions.",
            "key obstacle": "Existing benchmarks have primarily focused on performance metrics without considering practical aspects such as computational costs, data efficiency, and inference latency."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the need to evaluate the practical effectiveness of different few-shot learning paradigms in multilingual contexts, especially given the growing interest in language inclusivity in NLP.",
            "opinion": "The authors emphasize the importance of this benchmark in providing insights into the trade-offs between performance and resource requirements across different learning paradigms.",
            "innovation": "This benchmark innovatively incorporates a comprehensive analysis of both performance and practical aspects, which have been largely overlooked in previous works.",
            "benchmark abbreviation": "Multi3NLU++"
        },
        "dataset": {
            "source": "The dataset is sourced from existing multilingual NLU datasets, specifically designed for task-oriented dialogue and includes data from various languages.",
            "desc": "The dataset consists of examples from six languages (Amharic, English, Spanish, Marathi, Turkish, and Russian) across multiple domains, focusing on intent detection and value extraction tasks.",
            "content": "The dataset includes text data related to user intents and slot-value pairs for dialogue systems.",
            "size": "5,010",
            "domain": "Task-oriented dialogue",
            "task format": "Intent detection"
        },
        "metrics": {
            "metric name": "micro-F1",
            "aspect": "Task performance and efficiency in terms of computational resources.",
            "principle": "The choice of metrics is based on their ability to reflect both the accuracy of model predictions and the efficiency of the learning paradigms being tested.",
            "procedure": "Model performance is evaluated using standard metrics calculated from the predictions made on the test datasets, comparing the results across different approaches."
        },
        "experiments": {
            "model": "The models tested include XLM-R, LaBSE for SFT, and various instruction-tuned models like Flan-T5-XL, mT0-XL, and GPT-3.5 for ICL.",
            "procedure": "Models were trained and evaluated on the provided datasets using a fixed number of training examples across different setups to ensure fair comparisons.",
            "result": "The results indicate that supervised approaches (SFT and SIT) generally outperform ICL in terms of task performance across multiple languages and tasks.",
            "variability": "Variability was accounted for by conducting multiple trials and ensuring that the same training examples were used across different language-domain combinations."
        },
        "conclusion": "The findings demonstrate that supervised fine-tuning and instruction tuning provide better performance and resource efficiency compared to in-context learning, highlighting the need for improved multilingual adaptation strategies.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by providing a comprehensive evaluation of multilingual NLU capabilities across different learning paradigms, emphasizing practical aspects.",
            "limitation": "The benchmark may not fully capture the nuances of every language or task, potentially limiting its applicability to specific scenarios.",
            "future work": "Future research should focus on developing more efficient multilingual models and exploring new adaptation techniques to enhance performance across low-resource languages."
        },
        "other info": {
            "acknowledgments": "The work has been supported by a Huawei research donation and the UK Research and Innovation (UKRI) Frontier Research Grant."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recent advances in data-efficient, few-shot learning have been crucial for increasing and promoting language inclusiveness of NLP technology, substantially lowering the dataset size-related 'entry point' for a new language."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark is intended to facilitate a systematic comparison of three approaches to few-shot multilingual natural language understanding (NLU): supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context learning (ICL)."
        },
        {
            "section number": "2",
            "key information": "The benchmark addresses the challenges of few-shot multilingual NLU, particularly the effectiveness of different learning paradigms under varying resource conditions."
        },
        {
            "section number": "2.1",
            "key information": "The dataset consists of examples from six languages (Amharic, English, Spanish, Marathi, Turkish, and Russian) across multiple domains, focusing on intent detection and value extraction tasks."
        },
        {
            "section number": "3.1",
            "key information": "The results indicate that supervised approaches (SFT and SIT) generally outperform ICL in terms of task performance across multiple languages and tasks."
        },
        {
            "section number": "6.2",
            "key information": "The benchmark may not fully capture the nuances of every language or task, potentially limiting its applicability to specific scenarios."
        },
        {
            "section number": "7",
            "key information": "Future research should focus on developing more efficient multilingual models and exploring new adaptation techniques to enhance performance across low-resource languages."
        }
    ],
    "similarity_score": 0.7090994674704566,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU_ Are We There Yet_.json"
}