{
    "from": "google",
    "scholar_id": "ZxNEnDuB-lwJ",
    "detail_id": null,
    "title": "Integrating large language models into recommendation via mutual augmentation and adaptive aggregation",
    "abstract": " ABSTRACT\n\nConventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their own strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper, we introduce a general and model-agnostic framework known as L arge La nguage model with m utual augmentation and a daptive aggregation for Rec ommendation (Llama4Rec). Llama4Rec synergistically integrates conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and the LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent and significant improvements in recommendation performance over baseline methods.\n\n\n# 1 INTRODUCTION\n\nRecommender systems have emerged as crucial solutions for mitigating the challenge of information overload [11, 12, 26, 27]. Recommender systems encompass a multitude of tasks, such as rating prediction [19, 36] and top\ud835\udc58 recommendation [22, 28]. The top\ud835\udc58 recommendation, which encompasses collaborative filteringbased direct rec",
    "bib_name": "luo2024integrating",
    "md_text": "# grating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation\n\nn Luo 1, Yuxuan Yao 1, Bowei He 1, Yinya Huang 1, A\nyi Zhang 3, Yuanzhang Xiao 4, Mingjie Zhan 2, Lin\n1 City University of Hong Kong 2 The Chinese University of Hong Kong 3 Capital University of Economics and Business 4 University of Hawaii sichun.luo@my.cityu.edu.hk,linqi.song@cityu.edu.h\n\nn Luo 1, Yuxuan Yao 1, Bowei He 1, Yinya Huang 1, Aojun Zhou 2\nnyi Zhang 3, Yuanzhang Xiao 4, Mingjie Zhan 2, Linqi Song 1 \u2020\n1 City University of Hong Kong 2 The Chinese University of Hong Kong 3 Capital University of Economics and Business 4 University of Hawaii sichun.luo@my.cityu.edu.hk,linqi.song@cityu.edu.hk\n\nang 3, Yuanzhang Xiao 4, Mingjie Zhan 2, Linq\n1 City University of Hong Kong 2 The Chinese University of Hong Kong 3 Capital University of Economics and Business 4 University of Hawaii ichun.luo@my.cityu.edu.hk,linqi.song@cityu.edu.hk\n\nsichun.luo@my.cityu.edu.hk,linqi.song@cityu.edu.hk\n\n# ABSTRACT\n\nConventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their own strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper, we introduce a general and model-agnostic framework known as L arge La nguage model with m utual augmentation and a daptive aggregation for Rec ommendation (Llama4Rec). Llama4Rec synergistically integrates conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and the LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent and significant improvements in recommendation performance over baseline methods.\n\n\n# 1 INTRODUCTION\n\nRecommender systems have emerged as crucial solutions for mitigating the challenge of information overload [11, 12, 26, 27]. Recommender systems encompass a multitude of tasks, such as rating prediction [19, 36] and top\ud835\udc58 recommendation [22, 28]. The top\ud835\udc58 recommendation, which encompasses collaborative filteringbased direct recommendation [13, 14], sequential recommendation [3, 18, 37], and more, has found wide applications in various areas. However, recommender systems still suffer from the data sparsity and long-tail problem. Data sparsity arises from sparse user-item interactions, making the task of accurately capturing user preferences more challenging. The long-tail problem further intensifies data sparsity issue, as a substantial number of less popular items (i.e., long-tail items) are infrequently interacted with, leading to\n\n\u2020 Coresponding Author.\n\nommendation quality. In recent years, Large Language Models (LLMs) have emerged, exhibiting exceptional capabilities in language understanding, text generation, and complex reasoning tasks [30, 39\u2013 41, 56]. Recent studies have started exploring their applicability in recommender systems [2, 24, 51]. For example, Liu et al. employed ChatGPT with in-context learning (ICL) for various recommendation tasks [24]. Further progress has been achieved by adopting the instruction tuning technique [25, 31] to align general-purpose LLMs with recommendation tasks for improved performance [2, 51]. For instance, TALLRec [2] reformulates the recommendation problem as a binary classification task and introduces an effective instruction fine-tuning framework for adapting the LLaMA model [40]. However, these LLM-based recommendation methods may not perform optimally as they do not harness the collaborative or sequential information captured by conventional recommendation models. Conventional recommendation models and LLM-based recommendation methods each have their respective strengths and weaknesses. Conventional methods excel in mining collaborative information and modeling sequential behaviors, while LLMs are proficient in leveraging rich textual contexts. As such, the integration of LLMs into recommender systems presents a significant opportunity to amalgamate the advantages of both methodologies while circumventing their respective shortcomings. There have been initial attempts to harness the strengths of both conventional and LLMbased recommenders [8, 45, 54, 55]. Some efforts have sought to integrate collaborative/sequential information by enabling LLMs to comprehend user/item ID information [8, 54, 55]. For instance, a concurrent study by Zhang et al. encoded the semantic embedding into the prompt [54] and send it to LLM. On the other hand, some research works have aimed to augment conventional models using LLMs via data or knowledge augmentation [45, 47]. LLMRec [45] enhances recommender systems by deploying LLMs to augment the interaction graph, thereby addressing the challenges of data sparsity and low-quality side information. However, existing methods have several limitations. Firstly, current methods lack generalizability. The strategy of integrating ID information proves challenging to generalize across different domains and necessitates additional training. The current data augmentation method is not universally applicable, as it only addresses a limited number of recommendation scenarios. Secondly, current research primarily focuses on the integration at the data-level (e.g., data augmentation) or model-level (e.g., make LLM understand ID\n\nsemantics), leaving the result-level integration largely unexplored. Lastly, there is an absence of a comprehensive framework that combines and integrates these methods into a single construct. In light of these limitations, our objective is to explore the integration of conventional recommendation models and LLM-based recommendation methods in depth to address the above limitations and enhance recommendation performance. In this paper, we introduce a general framework known as L arge la nguage model with m utual augmentation and a daptive aggregation for Rec ommendation, referred to as Llama4Rec, for brevity. The core idea of Llama4Rec  is to allow conventional recommendation models and LLM-based recommendation models to mutually augment each other, followed by an adaptive aggregation of the augmented models to yield more optimized results. Specifically, Llama4Rec  performs data augmentation for conventional recommendation models by leveraging instruction-tuned LLM to alleviate the data sparsity and long-tail problem. The data augmentation is tailored with different strategies depending on the recommendation scenarios. Furthermore, we use conventional recommendation models to perform prompt augmentation for LLMs. The prompt augmentation includes enriching collaborative information from similar users and providing prior knowledge from the conventional recommendation model within the prompt. We also propose an adaptive aggregation module that merges the predictions of the LLM and conventional models in an adaptive manner. This module is designed as a simple yet effective way to combine the strengths of both models and refine the final recommendation results. We conduct empirical studies on three real-world datasets, encompassing three different recommendation tasks, to validate the superiority of our proposed method. The results consistently demonstrate its superior performance over baseline methods, highlighting notable improvements in recommendation performance. In a nutshell, the contributions of this work are threefold.\nWe introduce, a general and model-agnostic frame\n\nsemantics), leaving the result-level integration largely unexplored. Lastly, there is an absence of a comprehensive framework that combines and integrates these methods into a single construct. In light of these limitations, our objective is to explore the integration of conventional recommendation models and LLM-based recommendation methods in depth to address the above limitations and enhance recommendation performance. In this paper, we introduce a general framework known as L arge la nguage model with m utual augmentation and a daptive aggregation for Rec ommendation, referred to as Llama4Rec, for brevity. The core idea of Llama4Rec  is to allow conventional recommendation models and LLM-based recommendation models to mutually augment each other, followed by an adaptive aggregation of the augmented models to yield more optimized results. Specifically, Llama4Rec  performs data augmentation for conventional recommendation models by leveraging instruction-tuned LLM to alleviate the data sparsity and long-tail problem. The data augmentation is tailored with different strategies depending on the recommendation scenarios. Furthermore, we use conventional recommendation models to perform prompt augmentation for LLMs. The prompt augmentation includes enriching collaborative information from similar users and providing prior knowledge from the conventional recommendation model within the prompt. We also propose an adaptive aggregation module that merges the predictions of the LLM and conventional models in an adaptive manner. This module is designed as a simple yet effective way to combine the strengths of both models and refine the final recommendation results. We conduct empirical studies on three real-world datasets, encompassing three different recommendation tasks, to validate the superiority of our proposed method. The results consistently demonstrate its superior performance over baseline methods, highlighting notable improvements in recommendation performance. In a nutshell, the contributions of this work are threefold.\n\u2022 We introduce Llama4Rec, a general and model-agnostic framework to integrate LLM into conventional recommendation models. Llama4Rec performs the data augmentation for conventional models to alleviate the data sparsity problem and improve model performance. The prompt augmentation is applied to LLM for leveraging the information captured by the conventional models.\n\u2022 Llama4Rec  employs an adaptive aggregation approach to combine the prediction from the conventional recommendation model and LLM for improved recommendation performance via leveraging and merging the information by both kinds of models.\n\u2022 To validate the effectiveness of Llama4Rec, we conduct extensive experiments on three real-world datasets across three diverse recommendation tasks. The empirical results demonstrate that Llama4Rec outperforms existing baselines, exhibiting notable improvements across multiple performance metrics.\n\n# 2 RELATED WORK 2.1 Conventional Recommendation Methods\n\n# 2 RELATED WORK\n\n# 2 RELATED WORK 2.1 Conventional Recommendation Methods\n\n# 2.1 Conventional Recommendation Methods\n\nConventional recommendation methods serve as the cornerstone for the contemporary landscape of recommender systems [53]. Representative recommendation tasks include rating prediction, collaborative filtering-based direct recommendation, and sequential recommendation, where the latter two are usually formulated as\n\ntop\ud835\udc58 recommendation problems. Specifically, one of the seminal techniques is the use of matrix factorization for rating prediction, popularized by methods such as Singular Value Decomposition (SVD) [20]. Collaborative filtering (CF) is another commonly used technique for the recommender systems [1]. Recent advancements have evolved CF techniques into more complex neural network architectures and graph-based models [14, 44] to enhance the model performance. Sequential recommendation models incorporate temporal patterns into the recommendation pipeline. Techniques such as recurrent neural networks have been adapted for this purpose [15]. Recent research focuses on applying attention mechanisms to further refine these models, leading to a noteworthy boost in performance [18, 37]. Although conventional recommendation techniques are wellsuited for capturing latent information associated with users and items, they often require a substantial amount of user-item interactions to provide accurate recommendations, which limits their effectiveness in data sparse and long-tail scenarios [32].\n\n# Large Language Model for Recommendation\n\nLLMs have brought a paradigm shift in numerous areas of machine learning, including recommendation methods [7]. One of the most compelling advantages of LLM-based recommendation methods is their capacity for contextual understanding and in context learning [5]. Inspired by this, reference [24] utilized ChatGPT across diverse recommendation tasks and found it to be effective in specific contexts, underpinned by robust experiments and human evaluations. Similarly, Wang et al. [42] introduced a novel zero-shot technique for next-item recommendations, further substantiating the utility of LLMs in this arena. However, it is noteworthy that these methods do not consistently demonstrate a marked improvement over conventional recommendation algorithms, which is largely attributable to the inherent misalignment between the general-purpose capabilities of LLMs and the specialized requirements of the recommendation task. To address this issue, recent studies further attempt to instruct tuning the specific LLM to align with human preference [2, 51]. Typically, these approaches involve creating an instruction tuning dataset in line with recommendation tasks, which is then used to tune the LLM for recommendation. Such methodologies have demonstrated improved performance in generating more aligned and accurate recommendations. Nevertheless, while LLMs excel at capturing intricate textual patterns, they may encounter challenges in comprehensively encoding user and item collaborative or sequential information. Though some concurrent studies [54, 55] aim to address this gap, they often lack in terms of generalizability and comprehensibility. In response to this challenge, we propose a novel framework designed to mitigate this issue.\n\n# 3 PRELIMINARY\n\nWe consider a recommender system with a set of users, denoted U = {\ud835\udc62 1,\ud835\udc62 2, . . . ,\ud835\udc62 \ud835\udc5b}, and a set of items, denoted I = {\ud835\udc56 1,\ud835\udc56 2, . . . ,\ud835\udc56 \ud835\udc5a}. The rating prediction task aims to estimate the unknown values of \ud835\udc5f \ud835\udc62\ud835\udc56 in the user-item interaction matrix \ud835\udc45 \u2208 R \ud835\udc5b \u00d7 \ud835\udc5a, where each entry \ud835\udc5f \ud835\udc62\ud835\udc56 is the rating assigned by user \ud835\udc62 to item \ud835\udc56. Different from rating prediction, top\ud835\udc58 recommendation focuses on identifying a subset\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e07/2e07bfd1-a9fe-4a67-a1b2-dc510564a823.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nverall framework architecture of the proposed Llama4Rec consists of two main components: mutual adaptive aggregation. The mutual augmentation includes data augmentation and prompt augmentation. (ii) data augmentation process encompasses three diverse recommendation scenarios. (iii) The pipeline of the on module, which merges the predictions from the conventional recommendation model and the LLM.\n\nFigure 1: (i) The overall framework architecture of the proposed Llama4Rec consists of two main components: mutual augmentation and adaptive aggregation. The mutual augmentation includes data augmentation and prompt augmentation. (ii) Illustration of the data augmentation process encompasses three diverse recommendation scenarios. (iii) The pipeline of the adaptive aggregation module, which merges the predictions from the conventional recommendation model and the LLM.\n4.2 Data Augmentation for Conventional Recommendation Model\nof items S \ud835\udc62 \u2282I for each user \ud835\udc62. The subset is chosen to maximize a user-specific utility \ud835\udc48 (\ud835\udc62, S) with the constraint |S| = \ud835\udc58, which can be formally expressed as:\nWe design ad-hoc data augmentation strategies for different recom\n\nFigure 1: (i) The overall framework architecture of the proposed Llama4Rec consists of two main components: mutual augmentation and adaptive aggregation. The mutual augmentation includes data augmentation and prompt augmentation. (ii) Illustration of the data augmentation process encompasses three diverse recommendation scenarios. (iii) The pipeline of the adaptive aggregation module, which merges the predictions from the conventional recommendation model and the LLM.\n4.2 Data Augmentation for Conventional\n\n(1)\n\nIn the context of LLM-based recommendation methods, let \ud835\udc3f\ud835\udc3f\ud835\udc40 represent the original LLM. These kinds of methods first utilize prompt to interpret the recommendation task into natural language. The LLM-based recommendation for user \ud835\udc62 with in-context learning is denoted by \ud835\udc5f \ud835\udc62 = \ud835\udc3f\ud835\udc3f\ud835\udc40 (P \ud835\udc62) where P \ud835\udc62 is the recommendation prompt for user \ud835\udc62. The recommendation prompt could either ask LLM to predict a rating towards a target item, or rank candidate items derived by retrieval models in topk recommendation. To instruction fine-tune LLM, a dedicated dataset D \ud835\udc56\ud835\udc5b\ud835\udc60 consisting of various instructions is utilized. The resulting instruction-tuned LLM is denoted as \ud835\udc3f\ud835\udc3f\ud835\udc40 \u2032. Therefore, the recommendation process in the fine-tuned model can be succinctly represented as \ud835\udc5f \ud835\udc62 = \ud835\udc3f\ud835\udc3f\ud835\udc40 \u2032 (P \ud835\udc62).\n\n# 4 METHODOLOGY\n\n# 4.1 Overview\n\nFigure 1 depicts the architecture of Llama4Rec, which consists of three components: data augmentation, prompt augmentation, and adaptive aggregation. More specifically, Llama4Rec leverages an instruction-tuned LLM to enhance conventional recommendation systems through data augmentation. The specific data augmentation strategies for different recommendation situations are detailed in Section 4.2. In addition, we employ conventional recommendation models to augment the LLM via prompt augmentation, with details in Section 4.3. To further refine the predictions of the conventional model and the LLM, we propose a simple yet effective adaptive aggregation module in Section 4.4. Lastly, we describe the training strategy for LLM in Section 4.5.\n\n<div style=\"text-align: center;\"></div>\n# 4.2 Data Augmentation for Conventional Recommendation Model\n\nWe design ad-hoc data augmentation strategies for different recommendation scenarios to mitigate prevalent issues of data sparsity and the long-tail problem. This design is motivated by the fact that data distribution and tasks significantly vary across different recommendation scenarios. In the context of direct recommendation, we capitalize on the power of the instruction-tuned LLM to predict items that a user may like or dislike. We form pairs of these items to calculate the Bayesian Personalized Ranking (BPR) [34] loss. For sequential recommendation, we harness the capabilities of the instruction-tuned LLM to predict items that are highly preferred by the user. These predicted items are then randomly inserted into the sequence of items the user has interacted with. For rating prediction, we utilize the LLM to extract valuable side information (i.e., missing attributes), which is then seamlessly integrated as additional features within the training data.\n4.2.1 Data Augmentation for Direct Recommendation. For direct\n\n4.2.1\n\n4.2.1 Data Augmentation for Direct Recommendation. For direct recommendation, the Bayesian Personalized Ranking (BPR) loss is commonly used to optimize the model [34]. The objective of BPR is to maximize the score difference between correctly recommended items and incorrectly recommended items, thereby improving the accuracy of recommendations. The BPR loss is defined as:\n\u2211\ufe01\n\n(2)\n\nwhere (\ud835\udc62,\ud835\udc56, \ud835\udc57) refers to a triple of user-item pairs, and the user \ud835\udc62 has interacted with item \ud835\udc56 (positive item) and item \ud835\udc57 (negative item). D represents the set of such user-item pairs in the training data. \u02c6 \ud835\udc66 \ud835\udc62\ud835\udc56 denotes the predicted score or preference of user \ud835\udc62 for item \ud835\udc56. Inspired by this, we propose a data augmentation strategy where we randomly select pairs of items for a user \ud835\udc62 and prompt the LLM\n\nto rank each pair based on the user\u2019s likely preference. The ranking prediction based on LLM is then combined with the original data and used to train a direct recommendation model. Formally, let (\ud835\udc56 \ud835\udc57,\ud835\udc56 \ud835\udc58) denote a pair of items for a user \ud835\udc62. The LLM is prompted to rank these items, denoted as \ud835\udc56 +,\ud835\udc56 \u2212 = LLM (P 1), where P 1 is the corresponding prompt and \ud835\udc56 + is the item preferred over \ud835\udc56 \u2212. The training data D is updated as D \u2032 = D \u222a(\ud835\udc62,\ud835\udc56 +,\ud835\udc56 \u2212). The BPR loss is then updated as:\n\n(3)\n\nThis data augmentation strategy leverages the power of the instruction tuned LLM to enhance the performance of the direct recommendation model.\n\n4.2.2 Data Augmentation for Sequential Recommendation.  For sequential recommendation, the data augmentation strategy involves enriching the sequence of interacted items with additional items predicted by the LLM. Let\u2019s consider a user \ud835\udc62 with a corresponding sequence of interacted items {\ud835\udc56 1, ...,\ud835\udc56 \ud835\udc59}. We randomly sample a list of un-interacted items {\ud835\udc56 \ud835\udc62 1, ...,\ud835\udc56 \ud835\udc62\ud835\udc58}, and adopt the prompt P 2 to ask the LLM to predict the item most likely to be preferred by the user, denoted as \ud835\udc56 \ud835\udc5d = LLM (P 2). This predicted item \ud835\udc56 \ud835\udc5d is then randomly inserted into the user\u2019s sequence, resulting in an augmented sequence {\ud835\udc56 1, ...,\ud835\udc56 \ud835\udc5d, ...,\ud835\udc56 \ud835\udc59}. This augmented data is then used to train a more powerful sequential recommendation model. By including additional items predicted by the LLM, we can enrich the sequence of items for each user, providing a more comprehensive representation of the user\u2019s preferences. This, in turn, can enhance the performance of the conventional recommendation model, leading to more accurate recommendations.\n\n4.2.3 Data Augmentation for Rating Prediction. In rating prediction tasks, we introduce the use of in-context learning (ICL) in LLMs to provide side information. This is primarily due to the fact that recommendation datasets may contain incomplete information. For instance, the popular Movielens dataset [10] lacks information about the director of the movies, which can hinder the performance of a conventional rating prediction model. To mitigate this issue, we leverage the extensive world knowledge contained in an LLM. We prompt the LLM to provide side information, acting as additional attributes for users/items. Formally, we denote the rating prediction model as M \ud835\udc5f, and the attribute set as A = {\ud835\udc4e 1,\ud835\udc4e 2, ...,\ud835\udc4e \ud835\udc5b}, where \ud835\udc4e \ud835\udc56 \u2208A denotes a distinct attribute. The model predicts the rating as Pred = M \ud835\udc5f (A). We then prompt the LLM to provide additional attributes, where the prompt P 3  contains some corresponding examples followed by detailed instructions. The process is denoted as {\ud835\udc4e \ud835\udc51 1,\ud835\udc4e \ud835\udc51 2, ...} = LLM (P 3). The augmented attribute set is then formed as A \u2032 = A \u222a{\ud835\udc4e \ud835\udc51 1,\ud835\udc4e \ud835\udc51 2, ...}. The model then predicts the rating using the augmented attribute set, denoted as Pred \u2032 = M \ud835\udc5f (A \u2032). This approach allows us to leverage the LLM\u2019s world knowledge to enhance the performance of the rating prediction model.\n\nTopk Recommendation Prompt Example: Instruction: Rank the candidate movies based on user historical interactions and make the top k recommendations. Interaction History: Beyond Rangoon (1995); Alien (1979); Hollow Reed (1996); Primary Colors (1998); ...; Birds, The (1963) Candidate Items: Last Dance (1996); Remains of the Day, The (1993); Assassins (1995); ...; Fatal Instinct (1993) Similar User Interaction History: L.A. Confidential (1997);\n\nApt Pupil (1998); Kolya (1996); ...; Star Wars (1977) Conventional Model Prediction: Remains of the Day, The (1993); Addiction, The (1995); ...; Fugitive, The (1993) Output: Fugitive, The (1993); Angel Baby (1995); ...; Remains of the Day, The (1993)\n\n# Rating Prediction Prompt Example: Instruction: Predict the rating of a target movie based on th\n\nRating Prediction Prompt Example: Instruction: Predict the rating of a target movie based on the user\u2019s historical movie ratings. Rating History: Independence Day (1996): 3; Grosse Fatigue (1994): 3; Face/Off (1997): 4; ...; Shall We Dance? (1996): 3 Candidate Item: Pink Floyd - The Wall (1982) Similar User Rating History: L.A. Confidential (1997): 3; Apt Pupil (1998): 4; ...; English Patient, The (1996): 3 Conventional Model Prediction: 3.2 Output: 3\n\nFigure 2: Examples of instructions for topk recommendation and rating prediction. The prompt augmentation component is underlined. To improve readability and facilitate better experimental evaluation, we introduce certain modifications to the original instructions employed in our experiments.\n\n# 4.3 Prompt Augmentation for Large Language Model\n\n# Prompt Augmentation for Large Language\n\nPrevious works [2, 51] instruction tuning LLM for recommendation in a standard manner. However, these methods can be suboptimal due to the challenges of distinguishing users based solely on text-based prompt descriptions. Although some concurrent studies [54, 55] incorporate unique identifiers to differentiate users, these approaches require complex semantic understanding of IDs and additional training, limiting their generalizability. In this section, we introduce two text-based prompt augmentation strategies for LLM-based recommendations, i.e., we incorporate additional information within the prompt to enhance the model performance. First, we propose prompt augmentation with similar users, identifying users with analogous preferences to enrich the prompt, thereby enhancing the LLM\u2019s ability to leverage collaborative information and generate personalized recommendations. Second, we propose prompt augmentation with conventional model prediction, providing prior knowledge to guide the LLM toward recommendations that align with user preferences. Collectively, these strategies harness the strengths of both LLMs and conventional recommendation models, ensuring generalizability across a wide range of recommendation scenarios. The illustration of prompt augmentation is underlined in Figure 2.\n\n4.3.1 Prompt Augmentation with Collaborative Information from Similar User. To incorporate collaborative information within the prompt and facilitate LLM reasoning, we introduce a prompt augmentation strategy with similar user. Initially, we utilize a pretrained conventional recommendation model to acquire embeddings for each user. These embeddings represent users in a latent space, which encapsulates their preferences and behaviors. Specifically, for a user \ud835\udc62, in conjunction with a conventional recommendation model M \ud835\udc50, we use M \ud835\udc50 to obtain embeddings for each user, denoted as {\ud835\udc52 1, ...,\ud835\udc52 \ud835\udc5b}. These embeddings encapsulate the preferences and behaviors of the users, serving as a compact representation of the users in a latent space. We then calculate the similarity between these embeddings in the latent space. Various measurements, such as cosine similarity, Jaccard similarity, and Euclidean distance, could be employed in this context. In this paper, we calculate the cosine similarity to measure how closely two vectors align, denoted as:\n\n(4)\n\n|||| \u00b7 ||||\nwhere \ud835\udc52 \ud835\udc62 and \ud835\udc52 \ud835\udc63 are the embeddings of user \ud835\udc62 and \ud835\udc63, respectively, and \ud835\udc62, \ud835\udc63 \u2208U. The || \u00b7 || denotes the Euclidean norm and \u00b7 denotes the dot product. We identify the pair of users (\ud835\udc62, \ud835\udc63) that have the highest similarity, indicating that they are the most similar in terms of their preferences and behaviors. We then use the items interacted with by the most similar user to enrich the prompt for the target user. This strategy leverages the collaborative information gleaned from similar users to generate more relevant and accurate prompts, thereby enhancing the recommendation performance of the LLM.\n4.3.2 Prompt Augmentation with Prior Knowledge from Conventional Recommendation Model Prediction. To enable the LLM to leverage information captured by conventional models, we propose a prompt augmentation method that incorporates information from conventional recommendation models. More specifically, the augmented prompt is formed by concatenating the original prompt with the prediction from the conventional recommendation model in natural language form. It\u2019s important to note that the prediction from the conventional model varies depending on the recommendation scenarios and base models. Through augmenting prompts with predictions from conventional recommendation models, our method integrates collaborative or sequential information captured by these models, thereby enhancing the LLM\u2019s contextual understanding and reasoning capabilities and resulting in better recommendation performance. Notably, unlike ID-based methods such as [54, 55], our approach relies entirely on text, enabling easy adaptation to new situations. Also, the prompt augmentation could be used as a plug-and-play component for recommendation with closed source LLM, such as the GPT-4 model [30].\n\n# 4.4 Adaptive Aggregation\n\nWe endeavor to aggregate the outputs of LLM and conventional recommendation models at the result level for improved performance, considering the disparate model structures. However, indiscriminate aggregation of model predictions can potentially lead to suboptimal results. Conventional recommendation models, known for their susceptibility to the long tail issue, often struggle when\n\ndealing with the tail segment. In contrast, LLMs, by leveraging contextual information, are able to maintain a relatively uniform performance across all segments. Motivated by these observations, we first define the long-tail coefficient and subsequently adaptively aggregate the predictions from both model types. We first define the long-tail coefficient \u2113 \ud835\udc62 for user \ud835\udc62 to quantify where the user is located in the tail of the distribution. The long-tail coefficient is defined as follows:\n\n(5)\n\nwhere \ud835\udc41 (\ud835\udc62) is the number of interaction for user \ud835\udc62. A lower longtail coefficient value indicates that the user has fewer feedback. While the overarching architecture remains consistent, the implementation details are different for the two tasks considered, namely rating prediction and top\ud835\udc58 recommendation.\n\n4.4.1 Adaptive Aggregation for Rating Prediction. For the rating prediction task, we employ an instruction-tuned LLM to predict user-item utility scores directly. This approach incorporates the understanding of complex semantics and context by the LLM, which might be overlooked by traditional models. Similarly, conventional recommendation methods leverage collaborative information and user/item features for predicting the user rating. Specifically, the utility weight for user \ud835\udc62, denoted as \ud835\udc48 \ud835\udc62, is directly set as its user rating. Subsequently, the LLM is engaged to predict the rating, symbolized as \ud835\udc48 \ud835\udc3f\ud835\udc3f\ud835\udc40. There are various methods to derive a final result based on the utility scores, such as training a neural network to process the utility scores from LLM and conventional models, yielding a final output via learning the complex reflection. However, for the sake of simplicity in this paper, we adopt a simple yet effective linear interpolation approach. The final utility score for a user \ud835\udc62 amalgamates the values from both models, represented as:\n\n(6)\n\nwhere \ud835\udefc \ud835\udc62 is the adaptive parameter to control the weight for each model\u2019s utility value for user \ud835\udc62. We define the \ud835\udefc \ud835\udc62 as:\n\ufffd\n\ufffd\n\n(7)\n\nwhere \u2113 \ud835\udc5a\ud835\udc4e\ud835\udc65 and \u2113 \ud835\udc5a\ud835\udc56\ud835\udc5b are the maximum and minimum long-tail coefficients of the users, respectively, \ud835\udefc 1 is a hyper-parameter that controls the weight, and \ud835\udefc 2 <1 is a cut-off weight. From Equation (7), we can observe that for user \ud835\udc62, the further they are positioned in the long tail (i.e., the fewer items they have interacted with), the lower is the value of \u2113 \ud835\udc62 and the higher is the value of \ud835\udefc \ud835\udc62. As a result, in Equation (6), the weight of the utility score from the LLM model becomes more pronounced. This aligns with the motivation we previously discussed.\n\n4.4.2 Adaptive Aggregation for Top\ud835\udc58 Recommendation.  For the top\ud835\udc58 recommendation task, the LLM is employed to re-rank the item list generated by a conventional recommendation model. Specifically, from conventional recommendation methods, we curate a top-ranked list comprising \ud835\udc58 \u2032 items, denoted as {\ud835\udc56 1, ...,\ud835\udc56 \ud835\udc58 \u2032}. Each item in this list is assigned a utility weight, \ud835\udc48 \ud835\udc56 \ud835\udc45\ud835\udc52\ud835\udc50 = \u2212 \ud835\udc60 \u00b7C, where C is a constant and \ud835\udc60 represents the position of item \ud835\udc56, i.e., \ud835\udc60 \u2208{1, ...,\ud835\udc58 \u2032}. A higher utility weight indicates a stronger inclination of the user\u2019s\n\npreference. For listwise comparison conducted by the LLM, the process begins by using the LLM to directly output the predicted order of these candidate items. Then we assign utility scores for items at each position, denoted as \ud835\udc48 1,\ud835\udc48 2, ...,\ud835\udc48 \ud835\udc58 \u2032, where \ud835\udc48 1 \u2265 \ud835\udc48 2 \u2265... \u2265 \ud835\udc48 \ud835\udc58 \u2032. The final utility score for an item amalgamates the values from both the original rating and the LLM prediction, similar to the Equation (6).\n\n# 4.5 Training Strategy for LLM\n\n4.5.1 Instruction Tuning Dataset Construction. This section details the creation of an instruction-tuning dataset that encompasses two types of recommendation tasks catering to top\ud835\udc58 recommendation and rating prediction scenarios. A depiction of these two tasks, specifically referred to as listwise ranking and rating prediction, can be found in Figure 2. It is noteworthy that we also employ the LLM to execute pointwise ranking  within top\ud835\udc58 recommendation scenarios, i.e., utilizing LLM to predict ratings for each item within the topk recommendations and sorting the predicted ratings to derive the final result.\n\n4.5.2 Optimization via Instruction Tuning.  In this work, we perform full parameter instruction tuning to optimize LLMs using generated instruction data. Due to our need for customization, we chose LLaMA-2 [40], an open-source, high-performing LLM, which permits task-specific fine-tuning. During supervised fine-tuning, we apply a standard cross-entropy loss following Alpaca [38]. The training set D \ud835\udc56\ud835\udc5b\ud835\udc60 consists of instruction input-output pairs (\ud835\udc65,\ud835\udc66), which have been represented in natural language. The objective is to fine-tune the pre-trained LLM by minimizing the cross-entropy loss, formalized as:\n\n(8)\n\nwhere \u0398 are the original parameters for LLM, \ud835\udc43 \u0398 is the conditional probability, | \ud835\udc66 | is the number of tokens in \ud835\udc66, \ud835\udc66 \ud835\udc61 is the \ud835\udc61-th token in the target output \ud835\udc66, and \ud835\udc66 [1: \ud835\udc61 \u2212 1] represents tokens preceding \ud835\udc66 \ud835\udc61 in \ud835\udc66. By minimizing this loss function, the model fine-tunes its parameters \u0398 to adapt to the specifics of the new instruction tuning dataset D \ud835\udc56\ud835\udc5b\ud835\udc60, while leveraging the general language understanding and reasoning that has been acquired during pre-training [52]. In this manner, LLM can capture the user\u2019s preferences for items expressed in natural language, facilitating diverse recommendation tasks, including topk recommendation and rating prediction.\n\n# 5 EXPERIMENT\n\nIn this section, we present a thorough empirical evaluation to validate the effectiveness of our proposed framework. Specifically, our objective is to investigate whether the incorporation of our proposed Llama4Rec could enhance existing recommendation models. The overarching goal is to answer the following research questions:\n\u2022 RQ1: Does our proposed Llama4Rec framework enhance the performance of existing recommendation models?\n\u2022 RQ2: How do the various modules in Llama4Rec  affect the recommendation performance?\n\u2022 RQ3: How do different hyper-parameters impact the overall performance of the framework?\n\n<div style=\"text-align: center;\">Table 1: Dataset Description.\n</div>\nML-100K\nML-1M\nBookCrossing\n# of User\n943\n6,040\n6,851\n# of Item\n1,682\n3,706\n9,085\n# of Rating\n100,000\n1,000,209\n115,219\nDensity\n0.063046\n0.044683\n0.001851\nUser Features\nGender, ZipCode,\nGender, ZipCode,\nLocation, Age\nOccupation, Age\nOccupation, Age\nItem Features\nTitle, Genres\nTitle, Genres\nTitle, Author,\nYear\nYear, Publisher\nAugmented\nMovie Director,\nMovie Director,\nBook Genres,\nFeatures\nMovie Star\nMovie Star\nPage Length\n# 5.1 Experiment Setup\n\n5.1.1 Dataset. Following [2], we rigorously evaluate the performance of our proposed framework by employing three heterogeneous, real-world datasets. MovieLens 1 [10] serve as benchmark datasets in the realm of movie recommendation methods. We employ two variants of the dataset: MovieLens-100K (ML-100K) and MovieLens-1M (ML-1M). The former consists of approximately 100,000 user-item ratings, while the latter scales up to roughly 1,000,000 ratings. BookCrossing 2 [57] includes user-generated book ratings on a scale of 1 to 10, alongside metadata such as \u2018BookAuthor\u2019 and \u2018Book-Title\u2019. We employ LLM to augment the \u2018director\u2019 and \u2018star\u2019 features for ML-100K and ML-1M datasets, and augment the \u2018genre\u2019 and \u2018page length\u2019 features for the BookCrossing dataset. To ensure the data quality, we adopt the 5-core setting, i.e., we filter unpopular users and items with fewer than five interactions for the BookCrossing dataset. The key characteristics of these datasets are delineated in Table 1.\n\n5.1.2 Evaluation Metrics. Aligning with [13, 37], for the top\ud835\udc58 recommendation task, we turn to two well-established metrics: Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), denoted by H and N, respectively. In our experiments, \ud835\udc58 is configured to be either 3 or 5 for a comprehensive evaluation, similar to the experiment setting in [51]. In accordance with [6], we employ Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) as evaluation metrics to ascertain the performance of the rating prediction task.\n\n5.1.3 Data Preprocessing. Following the methodology of prior works [29, 51], we adopt a leave-one-out evaluation strategy. More specifically, within each user\u2019s interaction sequence, we choose the most recent item as the test instance. The item immediately preceding this serves as the validation instance, while all remaining interactions are used to constitute the training set. Moreover, regarding the instruction-tuning dataset construction, we randomly sampled 5K instructions for each recommendation task on the ML-100K, ML-1M, and BookCrossing datasets, respectively. We eliminated instructions that were repetitive or of low quality (identified by\n\n1 https://grouplens.org/datasets/movielens/ 2 Due to the absence of timestamp data, we synthesize historical interactions through random sampling.\n\n<div style=\"text-align: center;\">Table 2: Performance achieved by different direct recommendation method\n</div>\nBackbone\nMethod\nML-100K\nML-1M\nBookCrossing\nH@3 \u2191\nN@3 \u2191\nH@5 \u2191\nN@5 \u2191\nH@3 \u2191\nN@3 \u2191\nH@5 \u2191\nN@5 \u2191\nH@3 \u2191\nN@3 \u2191\nH@5 \u2191\nN@5 \u2191\nMF\nBase\n0.0455\n0.0325\n0.0690\n0.0420\n0.0255\n0.0187\n0.0403\n0.0248\n0.0294\n0.0227\n0.0394\n0.0269\nIFT\n0.0546\n0.0388\n0.0790\n0.0488\n0.0242\n0.0175\n0.0410\n0.0244\n0.0247\n0.0177\n0.0377\n0.0230\nLlama4Rec\n0.0645*\n0.0474*\n0.0919*\n0.0588*\n0.0281*\n0.0203*\n0.0433*\n0.0265*\n0.0365*\n0.0284*\n0.0462*\n0.0324*\nImpro.\n18.13%\n22.16%\n16.33%\n20.49%\n10.20%\n8.56%\n5.61%\n6.85%\n24.15%\n25.55%\n17.26%\n20.45%\nLightGCN\nBase\n0.0492\n0.0343\n0.0744\n0.0447\n0.0283\n0.0203\n0.0432\n0.0264\n0.0358\n0.0272\n0.0480\n0.0322\nIFT\n0.0537\n0.0381\n0.0846\n0.0507\n0.0268\n0.0193\n0.0441\n0.0263\n0.0287\n0.0202\n0.0448\n0.0268\nLlama4Rec\n0.0647*\n0.0476*\n0.0967*\n0.0608*\n0.0304*\n0.0222*\n0.0461*\n0.0286*\n0.0434*\n0.0338*\n0.057*\n0.0394*\nImpro.\n20.48%\n24.93%\n14.30%\n19.92%\n7.42%\n9.36%\n4.54%\n8.33%\n21.23%\n24.26%\n18.75%\n22.36%\nMixGCF\nBase\n0.0526\n0.0401\n0.0757\n0.0496\n0.0159\n0.0115\n0.0238\n0.0147\n0.0426\n0.0330\n0.0556\n0.0384\nIFT\n0.0617\n0.0452\n0.0906\n0.0570\n0.0162\n0.0114\n0.0259\n0.0154\n0.0337\n0.0243\n0.0506\n0.0312\nLlama4Rec\n0.0690*\n0.0515*\n0.0949*\n0.0621*\n0.0174*\n0.0128*\n0.0259\n0.0162*\n0.0495*\n0.0384*\n0.0635*\n0.0441*\nImpro.\n11.83%\n13.94%\n4.75%\n8.95%\n7.41%\n11.30%\n0.00%\n5.19%\n16.20%\n16.36%\n14.21%\n14.84%\nSGL\nBase\n0.0505\n0.0380\n0.0729\n0.0472\n0.0284\n0.0206\n0.0434\n0.0267\n0.0419\n0.0319\n0.0566\n0.0380\nIFT\n0.0520\n0.0392\n0.0792\n0.0503\n0.0275\n0.0202\n0.0438\n0.0269\n0.0326\n0.0237\n0.0499\n0.0307\nLlama4Rec\n0.0632*\n0.0479*\n0.0917*\n0.0596*\n0.0308*\n0.0224*\n0.0480*\n0.0294*\n0.0501*\n0.0393*\n0.0634*\n0.0448*\nImpro.\n21.54%\n22.19%\n15.78%\n18.49%\n8.45%\n8.74%\n9.59%\n9.29%\n19.57%\n23.20%\n12.01%\n17.89%\n<div style=\"text-align: center;\">Table 3: Performance achieved by different sequential recommendation methods.\n</div>\nBackbone\nMethod\nML-100K\nML-1M\nBookCrossing\nH@3 \u2191\nN@3 \u2191\nH@5 \u2191\nN@5 \u2191\nH@3 \u2191\nN@3 \u2191\nH@5 \u2191\nN@5 \u2191\nH@3 \u2191\nN@3 \u2191\nH@5 \u2191\nN@5 \u2191\nSASRec\nBase\n0.0187\n0.0125\n0.0385\n0.0205\n0.0277\n0.0165\n0.0502\n0.0257\n0.0086\n0.0049\n0.0163\n0.0081\nIFT\n0.0204\n0.0136\n0.0379\n0.0207\n0.0241\n0.0159\n0.0473\n0.0254\n0.0124\n0.0086\n0.0185\n0.0111\nLlama4Rec\n0.0238*\n0.0155*\n0.0449*\n0.0240*\n0.0293*\n0.0201*\n0.0504\n0.0287*\n0.0142*\n0.0098*\n0.0227*\n0.0131*\nImpro.\n16.67%\n13.97%\n16.62%\n15.94%\n5.78%\n21.82%\n0.40%\n11.67%\n14.52%\n13.95%\n22.70%\n18.02%\nBERT4Rec\nBase\n0.0153\n0.0104\n0.0294\n0.0161\n0.0107\n0.0069\n0.0211\n0.0112\n0.0088\n0.0058\n0.0161\n0.0088\nIFT\n0.0174\n0.0119\n0.0326\n0.0100\n0.0106\n0.0071\n0.0188\n0.0104\n0.0127\n0.0092\n0.0180\n0.0113\nLlama4Rec\n0.0198*\n0.0134*\n0.0332\n0.0189*\n0.0115*\n0.0078*\n0.0206\n0.0115*\n0.0154*\n0.0108*\n0.023*\n0.0139*\nImpro.\n13.79%\n12.61%\n1.84%\n17.39%\n7.48%\n9.86%\n-2.37%\n2.68%\n21.26%\n17.39%\n27.78%\n23.01%\nCL4SRec\nBase\n0.0243\n0.0143\n0.0436\n0.0222\n0.0259\n0.0153\n0.0492\n0.0248\n0.0083\n0.0048\n0.0165\n0.0082\nIFT\n0.0230\n0.0149\n0.0428\n0.0230\n0.0234\n0.0155\n0.0447\n0.0241\n0.0102\n0.0071\n0.0177\n0.0102\nLlama4Rec\n0.0255*\n0.0182*\n0.0440\n0.0255*\n0.0278*\n0.0185*\n0.0482\n0.0268*\n0.0138*\n0.0093*\n0.0220*\n0.0127*\nImpro.\n4.94%\n22.15%\n0.92%\n10.87%\n7.34%\n19.35%\n-2.03%\n8.06%\n35.29%\n30.99%\n24.29%\n24.51%\nusers with fewer than three interactions in their interaction history), leaving approximately 25K high-quality instructions. These instructions are mixed to create an instruction-tuning dataset to fine-tune the LLM.\n\n5.1.4 Backbone Models. We incorporate our Llama4Rec with the following recommendation models that are often used for various recommendation tasks as the backbone models:\n\u2022 Direct Recommendation.  In the scenario of direct recommendation, We adopt four representative methods, including: MF [20], LightGCN [13], MixGCF [17], and SGL [46].\n\u2022 Sequential Recommendation.  Regarding sequential recommendation, we opt for three widely used models, including: SASRec [18], BERT4Rec [37], and CL4SRec [49].\n\u2022 Rating Prediction. We consider the following classical models for rating prediction, including: DeepFM [9], NFM [14], DCN [43], AFM [48], xDeepFM [23], and AutoInt [35].\n\nWe employ the LLaMA-2 7B version as the backbone LLM across all experiments, unless specifically mentioned otherwise. Our primary comparison is with the standard I nstruction F ineT uning (IFT) method adopted in TALLRec [2] and InstructRec [51]. For the rating prediction task, LLaMA-2 with IFT is used to directly predict the rating. For the top-k recommendation task, the tuned LLM is used to re-rank the list predicted by the backbone model, in accordance with [16], referred to as listwise ranking. Besides, we also adopt LLM for predicting the rating for each item and sort by the predicted scores, referred to as pointwise ranking.\n5.1.5 Implementation Details. During training for LLaMA 2 (7B) with full-parameter tuning, we use a uniform learning rate of 2 \u00d7 10 \u2212 5 and a context length of 2048, and we set the batch size as 16. Additionally, we use a cosine scheduler for three epochs in total with a 50-step warm-up period. To efficiently train the computationally intensive models, we simultaneously employ DeepSpeed training with ZeRO-3 stage [33] and flash attention [4]. We trained the 7B\n\nWe employ the LLaMA-2 7B version as the backbone LLM across all experiments, unless specifically mentioned otherwise. Our primary comparison is with the standard I nstruction F ineT uning (IFT) method adopted in TALLRec [2] and InstructRec [51]. For the rating prediction task, LLaMA-2 with IFT is used to directly predict the rating. For the top-k recommendation task, the tuned LLM is used to re-rank the list predicted by the backbone model, in accordance with [16], referred to as listwise ranking. Besides, we also adopt LLM for predicting the rating for each item and sort by the predicted scores, referred to as pointwise ranking.\n\n5.1.5 Implementation Details. During training for LLaMA 2 (7B) with full-parameter tuning, we use a uniform learning rate of 2 \u00d7 10 \u2212 5 and a context length of 2048, and we set the batch size as 16. Additionally, we use a cosine scheduler for three epochs in total with a 50-step warm-up period. To efficiently train the computationally intensive models, we simultaneously employ DeepSpeed training with ZeRO-3 stage [33] and flash attention [4]. We trained the 7B\n\nTable 4: Performance achieved by different methods in rating prediction task.\n\nBackbone\nMethod\nML-100K\nML-1M\nBookCrossing\nRMSE \u2193\nMAE \u2193\nRMSE \u2193\nMAE \u2193\nRMSE \u2193\nMAE \u2193\nLLaMA\nIFT\n1.2792\n0.8940\n1.2302\n0.8770\n2.0152\n1.3782\nDeepFM\nBase\n1.0487\n0.8082\n0.9455\n0.7409\n1.7738\n1.3554\nLlama4Rec\n1.0306*\n0.7987*\n0.9360*\n0.7321*\n1.6958*\n1.2843*\nImpro.\n1.73%\n1.18%\n1.00%\n1.19%\n4.40%\n5.25%\nNFM\nBase\n1.0284\n0.8005\n0.9438\n0.7364\n2.121\n1.5984\nLlama4Rec\n1.0189*\n0.7961\n0.9369*\n0.7303*\n1.9253*\n1.4473*\nImpro.\n0.92%\n0.55%\n0.73%\n0.83%\n9.23%\n9.45%\nDCN\nBase\n1.0478\n0.8063\n0.9426\n0.7342\n2.0216\n1.4622\nLlama4Rec\n1.0367*\n0.8033\n0.9345*\n0.7272*\n1.8518*\n1.3566*\nImpro.\n1.06%\n0.37%\n0.86%\n0.95%\n8.40%\n7.22%\nAFM\nBase\n1.0471\n0.8035\n0.9508\n0.7464\n1.6516\n1.2614\nLlama4Rec\n1.0340*\n0.7996\n0.9426*\n0.7394*\n1.6244*\n1.2259*\nImpro.\n1.25%\n0.49%\n0.86%\n0.94%\n1.65%\n2.81%\nxDeepFM\nBase\n1.1472\n0.8836\n0.9519\n0.7428\n2.1756\n1.6461\nLlama4Rec\n1.0947*\n0.8483*\n0.9401*\n0.7336*\n1.9610*\n1.4833*\nImpro.\n4.58%\n4.00%\n1.24%\n1.24%\n9.86%\n9.89%\nAutoInt\nBase\n1.0500\n0.8120\n0.9471\n0.7404\n1.9148\n1.4501\nLlama4Rec\n1.0369*\n0.8059*\n0.9382*\n0.7326*\n1.7917*\n1.3492*\nImpro.\n1.25%\n0.75%\n0.94%\n1.05%\n6.43%\n6.96%\nmodel on 16 NVIDIA A800 80GB GPUs. For the inference stage, we employed the vLLM framework [21] with greedy decoding, setting the temperature to 0. Only one GPU was utilized during the inference phase. We only evaluate the instruction-tuned LLaMA model for rating prediction task since it is not applicable for directly making top\ud835\udc58 recommendations. We implement the models for rating prediction task using the DeepCTR-Torch 3 library. For the top\ud835\udc58 recommendation task, we utilize the SELFRec 4 library [50] for implementation. As for the hyper-parameter settings, \ud835\udefc 1 and \ud835\udefc 2 are selected from {0.1, 0.3, 0.5, 0.7, 0.9} respectively for all experiments. C is fixed to 1. We repeat the experiment five times and calculate the average. We report the best results obtained when the ranking method is selected from pointwise and listwise ranking. For all experiments, the best results are highlighted in boldfaces. * indicates the statistical significance for \ud835\udc5d \u2264 0. 05 compared to the best baseline method based on the paired t-test. Improv. denotes the improvement of our method over the best baseline method.\n\n# 5.2 Main Results (RQ1)\n\nWe conducted an extensive evaluation of our proposed Llama4Rec and the baseline methods on three datasets to assess the model\u2019s performance under diverse recommendation scenarios. The experiment results for rating prediction, direct recommendation, and sequential recommendation are shown in Table 2, Table 3, and Table 4, respectively. We have the following key observations. \u2022 Llama4Rec consistently outperforms baseline methods in almost all scenarios, with particularly significant improvements observed in the direct recommendation task. Moreover, our findings reveal that direct instruction fine-tuning LLMs for recommendation tasks does not consistently yield promising performance. These results highlight the effectiveness of integrating LLMs into\n\n3 https://github.com/shenweichen/DeepCTR-Torch 4 https://github.com/Coder-Yu/SELFRec\n\nTable 5: Ablation study on key components of Llama4Rec on the ML-1M dataset.\n\nModels\nML-1M\nBookCrossing\nH@3 \u2191\nN@3 \u2191\nH@3 \u2191\nN@3 \u2191\nLightGCN\n0.0283 (-)\n0.0203 (-)\n0.0358 (-)\n0.0272 (-)\nIFT\n0.0268 (-5.30%)\n0.0193 (-4.93%)\n0.0287 (-19.84%)\n0.0202 (-25.74%)\nLlama4Rec w/o DA\n0.0294 (+3.89%)\n0.0209 (+2.96%)\n0.0408 (+13.97%)\n0.0319 (+17.28%)\nLlama4Rec w/o PA\n0.0277 (-2.12%)\n0.0199 (-1.97%)\n0.0372 (+3.92%)\n0.0279 (+2.57%)\nLlama4Rec w/o AA\n0.0298 (+5.30%)\n0.0218 (+7.39%)\n0.0429 (+19.83%)\n0.0332 (+22.06%)\nLlama4Rec\n0.0304 (+7.42%)\n0.0222 (+9.36%)\n0.0434 (+21.23%)\n0.0338 (+24.26%)\nconventional recommendation models, underscoring the importance of incorporating the mechanism that utilizes instructiontuned LLM to mutually augment and adaptively aggregate with conventional recommendation models.\n\u2022  In the scenario of the rating prediction task, while the instructiontuned LLaMA model significantly underperforms when compared to conventional recommendation models, integrating the LLM yields a marked performance improvement. This suggests that the LLM and conventional recommendation models learn distinct aspects of information. Consequently, integrating the LLM with conventional recommendation models could enhance recommendation performance.\n\u2022  In the context of top\ud835\udc58 recommendations, Llama4Rec exhibits a more pronounced improvement for direct recommendations task. In addition, a more significant enhancement is observed on the Bookcrossing dataset, which can be attributed to the more finegrained and distinguishable rating of the Bookcrossing dataset.\n\n# 5.3 Ablation Study (RQ2)\n\nWe conducted an ablation study to analyze the contributions of different components in our model. Table 5 summarizes the results of the ablation studies across three variants on the ML-1M dataset. It is evident that the full model performs considerably better than all its variants, indicating that all the main components contribute significantly to overall performance improvement. Moreover, compared to the conventional model, the instruction-tuned LLM does not achieve superior results, underscoring the importance of model aggregation. We further analyze the specific impact of each component, and our observations are as follows: \u2022 w/o Data Augmentation (w/o DA): In this variant, we remove the data augmentation module while maintaining other components the same. Experimental results reveal a obvious decline in performance when this module is excluded. This indicates the module\u2019s capacity to mitigate data sparsity and long-tail problem, consequently enhancing model performance.\n\u2022 w/o Prompt Augmentation (w/o PA): In this variant, we remove the prompt augmentation component, a crucial element of the proposed framework. Experimental results demonstrate a significant degradation in model performance when this module is excluded, thereby validating its essential role. By employing the instruction-tuned LLM with prompt augmentation from prior knowledge by conventional recommendation models, we achieve an enhanced model performance, attributable to the capture of different aspects of information.\n\u2022 w/o Adaptive Aggregation (w/o AA): In this variant, we substitute adaptive aggregation with uniform aggregation and keep\n\n# \u2022 w/o Data Augmentation (w/o DA): In\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4702/47023efd-ccd3-4de0-812f-8bc7d5d719f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Impact of hyper-parameters \ud835\udefc 1 and \ud835\udefc 2 on ML-1M dataset with backbone model LightGCN.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a5e2/a5e258f4-d7b6-47eb-a8c2-8cfb5b757823.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance comparison w.r.t different LLaMA-2 size for training Llama4Rec on the Bookcrossing dataset.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1716/1716167c-527e-4135-b0de-3f943bc5fbb0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Performance comparison w.r.t different numbers of instructions for training Llama4Rec on the ML-1M dataset.\n</div>\nother modules unchanged. Experimental results demonstrate a drop in model performance, underscoring the significance of accounting for the user\u2019s long-tail coefficient and employing adaptive aggregation.\n\n# 5.4 Hyper-parameter Study (RQ3)\n\n5.4.1 Analysis of Hyper-parameters \ud835\udefc 1 and \ud835\udefc 2. We conducted an analysis of the effects of hyper-parameters \ud835\udefc 1 and \ud835\udefc 2. These parameters play crucial roles in controlling the weight in adaptive aggregation, as illustrated in Equation (7). Figure 3 presents the results on the ML-1M dataset using LightGCN as the backbone model. As \ud835\udefc 1 increases, we observe an initial surge in the model\u2019s performance, followed by a decline. This trend suggests appropriate selection of \ud835\udefc 1 would enhance the model performance. With respect to \ud835\udefc 2, we observe a similar trend but the decline is more pronounced. This observation is consistent with the principle of adaptive aggregation, which emphasizes the importance of assigning suitable weights to tail users.\n\n5.4.2 Analysis of Model Scaling. We further instruction-tuned the LLaMA-2 model with different model size. 5 A comparative analysis was conducted between the 7B and 13B variants of the instructiontuned models, with performance differences specifically evaluated\n\n5 Due to resource constraints, training the LLaMA-2 (70B) model with identical ex perimental settings was unfeasible, consistently leading to Out-Of-Memory (OOM errors.\n\nacross various backbone models within the Bookcrossing dataset, as depicted in Figure 4. Our findings suggest that the LLaMA-2 (13B) model generally surpasses the 7B version in performance. This can be attributed to the superior language comprehension and reasoning abilities of the larger model, which contribute to improved recommendation results. However, it\u2019s worth noting that the improvements are not substantial, indicating that while larger models may provide some performance benefits, the degree of improvement may not always justify the increased computational resources and training time required. It underscores the importance of considering the trade-off between model size, performance gain, and resource efficiency in the design and application of large language models.\n5.4.3 Analysis of Data Scaling. We evaluated the effect of data size on LLM training by varying the number of instructions in the instruction-tuning dataset. Proportionality with our original configuration, the model with 2.5K instructions underwent 250 training steps, while the 12.5K instructions version was trained over 1250 steps. As depicted in Figure 5, a clear trend emerges: model performance improves with an increase in the number of instructions, particularly for direct recommendation models. This highlights the importance of utilizing larger and more diverse datasets for instruction tuning LLMs to optimize performance.\n\n# 5.5 Further Discussion\n\nIn this part, we discuss about the computational efficiency and future improvements. In the Llama4Rec framework, additional training with augmented data is required, which may present a potential limitation. In the current experimental setup, we train a new model from scratch. However, this process could be optimized by continuing to train a previously tuned model, thereby reducing time costs. Additionally, in our experiment, we observed that training the LLaMA-2 7B model with around 25K instructions on 16 A800 GPUs with 2500 steps took approximately 1.94 hours. The inference time for each instruction averaged about 17 instructions per second, translating to a requirement of around 0.059 seconds per item for computation by a single A800 GPU. This training and inference duration significantly exceeds that of conventional recommendation models, highlighting the limitations of current LLM-based recommender systems. The substantial demand for computational resources also represents a significant challenge. Consequently, employing instruction LLMs for largescale industrial recommender systems, such as those with millions of users, is presently impractical. However, future advancements in accelerated and parallel computing algorithms for language model inference could potentially reduce inference times and computation resources. This improvement might make the integration of LLMs into large-scale recommender systems feasible, especially by leveraging many GPUs for parallel computation.\n\n# 6 CONCLUSION AND FUTURE WORK\n\nIn this study, we present Llama4Rec, a general and model-agnostic framework tailored to facilitate mutual augmentation between conventional recommendation models and LLMs through data augmentation and prompt augmentation. Data augmentation for conventional recommendation models could alleviate issues of data\n\nsparsity and the long-tail problem, thus improving conventional recommendation model performance. Prompt augmentation, on the other hand, allows the LLM to externalize additional collaborative or sequential information and further enhance the model capability. Furthermore, adaptive aggregation is employed to merge the predictions from both kinds of augmented models, resulting in more optimized recommendation performance. Comprehensive experimental results across three diverse recommendation tasks on three real-world datasets demonstrate the effectiveness of Llama4Rec. While our current approach focuses on mutual augmentation within a single step, our future work will explore expanding mutual augmentation in an iterative manner, potentially unlocking further improvements in model performance.\n\n# REFERENCES\n\n[1]  Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE transactions on knowledge and data engineering 17, 6 (2005), 734\u2013749.\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447 (2023).\n[3] Xiong-Hui Chen, Bowei He, Yang Yu, Qingyang Li, Zhiwei Qin, Wenjie Shang, Jieping Ye, and Chen Ma. 2023. Sim2Rec: A Simulator-based Decision-making Approach to Optimize Real-World Long-term User Engagement in Sequential Recommender Systems. arXiv preprint arXiv:2305.04832 (2023).\n[4]  Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344\u201316359.\n[5] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234 (2022).\n[6] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In The world wide web conference. 417\u2013426.\n[7] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).\n[8] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems. 299\u2013315.\n[9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017).\n[10] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1\u201319.\n[11] Bowei He, Xu He, Renrui Zhang, Yingxue Zhang, Ruiming Tang, and Chen Ma. 2023. Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 741\u2013750.\n[12]  Bowei He, Xu He, Yingxue Zhang, Ruiming Tang, and Chen Ma. 2023. Dynamically Expandable Graph Convolution for Streaming Recommendation. In Proceedings of the ACM Web Conference 2023. 1457\u20131467.\n[13] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639\u2013648.\n[14] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web. 173\u2013182.\n[15] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015).\n[16] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845 (2023).\n[17] Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021. Mixgcf: An improved training method for graph neural network-based recommender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 665\u2013674.\n\n[18]  Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197\u2013206.\n[19] Zahid Younas Khan, Zhendong Niu, Sulis Sandiwarno, and Rukundo Prince. 2021. Deep learning techniques for rating prediction: a survey of the state-of-the-art. Artificial Intelligence Review 54 (2021), 95\u2013135.\n[20]  Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30\u201337.\n[21] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. arXiv preprint arXiv:2309.06180 (2023).\n[22] Dung D Le and Hady Lauw. 2021. Efficient retrieval of matrix factorization-based top-k recommendations: A survey of recent approaches. Journal of Artificial Intelligence Research 70 (2021), 1441\u20131479.\n[23] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1754\u20131763.\n[24] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149 (2023).\n[25] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688 (2023).\n[26]  Sichun Luo, Chen Ma, Yuanzhang Xiao, and Linqi Song. 2023. Improving LongTail Item Recommendation with Graph Augmentation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 1707\u20131716.\n[27] Sichun Luo, Yuanzhang Xiao, and Linqi Song. 2022. Personalized federated recommendation via joint representation learning, user clustering, and model adaptation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 4289\u20134293.\n[28] Sichun Luo, Yuanzhang Xiao, Xinyi Zhang, Yang Liu, Wenbo Ding, and Linqi Song. 2023. PerFedRec++: Enhancing Personalized Federated Recommendation with Self-Supervised Pre-Training. arXiv preprint arXiv:2305.06622 (2023).\n[29] Sichun Luo, Xinyi Zhang, Yuanzhang Xiao, and Linqi Song. 2022. HySAGE: A hybrid static and adaptive graph embedding network for context-drifting recommendations. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 1389\u20131398.\n[30] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[32] Yoon-Joo Park and Alexander Tuzhilin. 2008. The long tail of recommender systems and how to leverage it. In Proceedings of the 2008 ACM conference on Recommender systems. 11\u201318.\n[33] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In  SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1\u201316.\n[34] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012).\n[35] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM international conference on information and knowledge management. 1161\u20131170.\n[36]  Harald Steck. 2013. Evaluation of recommendations: rating-prediction and ranking. In Proceedings of the 7th ACM conference on Recommender systems. 213\u2013220.\n[37] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 1441\u20131450.\n[38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_ alpaca.\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\n[40]  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n\n[41] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. arXiv preprint arXiv:2310.03731 (2023).\n[42] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153 (2023).\n[43] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD\u201917. 1\u20137.\n[44] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 165\u2013174.\n[45] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023. Llmrec: Large language models with graph augmentation for recommendation. arXiv preprint arXiv:2311.00423 (2023).\n[46] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In  Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 726\u2013735.\n[47] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models. arXiv preprint arXiv:2306.10933 (2023).\n[48] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617 (2017).\n[49] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In\n\n2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 1259\u2013 1273.\n[50] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2023. Self-supervised learning for recommender systems: A survey. IEEE Transactions on Knowledge and Data Engineering (2023).\n[51] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023).\n[52] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).\n[53]  Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recommender system: A survey and new perspectives. ACM computing surveys (CSUR) 52, 1 (2019), 1\u201338.\n[54] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023. Collm: Integrating collaborative embeddings into large language models for recommendation. arXiv preprint arXiv:2310.19488 (2023).\n[55]  Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and JiRong Wen. 2023. Adapting large language models by integrating collaborative semantics for recommendation. arXiv preprint arXiv:2311.09049 (2023).\n[56] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921 (2023).\n[57] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Konstan, and Georg Lausen. 2005. Improving recommendation lists through topic diversification. In Proceedings of the 14th international conference on World Wide Web. 22\u201332.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of integrating conventional recommendation methods and large language models (LLMs) to enhance recommendation performance, highlighting the limitations of both approaches in handling data sparsity and the long-tail problem.",
        "problem": {
            "definition": "The problem centers on the ineffective utilization of collaborative and sequential information in LLM-based recommendation methods, which limits their performance in scenarios where such data is crucial.",
            "key obstacle": "The main challenge is the lack of a comprehensive framework that effectively integrates conventional recommendation models with LLMs, preventing optimal performance due to their individual strengths and weaknesses."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of LLMs to enhance conventional recommendation models through mutual augmentation, addressing their respective limitations.",
            "opinion": "The proposed idea, Llama4Rec, involves a framework that allows conventional models and LLMs to augment each other and refine predictions through adaptive aggregation.",
            "innovation": "Llama4Rec innovatively combines data augmentation for conventional models with prompt augmentation for LLMs, along with an adaptive aggregation module, distinguishing it from existing methods that focus on either data-level or model-level integration."
        },
        "method": {
            "method name": "Large Language model with mutual augmentation and adaptive aggregation for Recommendation",
            "method abbreviation": "Llama4Rec",
            "method definition": "Llama4Rec is a model-agnostic framework that integrates conventional recommendation models with LLMs through mutual augmentation strategies and adaptive aggregation of predictions.",
            "method description": "The core of Llama4Rec lies in its ability to enhance recommendation performance by leveraging both conventional and LLM-based models.",
            "method steps": [
                "Perform data augmentation for conventional recommendation models using instruction-tuned LLMs.",
                "Apply prompt augmentation to LLMs using collaborative information from conventional models.",
                "Implement an adaptive aggregation module to combine predictions from both models."
            ],
            "principle": "The effectiveness of Llama4Rec stems from its ability to utilize the strengths of both LLMs and conventional models, thereby addressing the limitations of data sparsity and enhancing overall recommendation performance."
        },
        "experiments": {
            "evaluation setting": "Empirical studies were conducted on three real-world datasets: ML-100K, ML-1M, and BookCrossing, encompassing diverse recommendation tasks.",
            "evaluation method": "Performance was assessed using metrics such as Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE)."
        },
        "conclusion": "The experiments demonstrate that Llama4Rec consistently outperforms baseline methods across multiple recommendation tasks, validating its effectiveness in enhancing recommendation performance through mutual augmentation and adaptive aggregation.",
        "discussion": {
            "advantage": "Llama4Rec uniquely integrates the strengths of both conventional recommendation models and LLMs, providing significant improvements in recommendation accuracy and robustness.",
            "limitation": "The method requires additional training with augmented data, which may increase computational costs and complexity in large-scale applications.",
            "future work": "Future research will explore iterative mutual augmentation strategies to further enhance model performance and efficiency."
        },
        "other info": {
            "info1": "Llama4Rec is designed to be model-agnostic, allowing its application across various recommendation frameworks.",
            "info2": {
                "info2.1": "The framework employs instruction tuning for LLMs to align them with specific recommendation tasks.",
                "info2.2": "Adaptive aggregation is tailored to account for the long-tail distribution of user interactions."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "Llama4Rec is a model-agnostic framework that integrates conventional recommendation models with LLMs through mutual augmentation strategies and adaptive aggregation of predictions."
        },
        {
            "section number": "4.2",
            "key information": "The effectiveness of Llama4Rec stems from its ability to utilize the strengths of both LLMs and conventional models, thereby addressing the limitations of data sparsity and enhancing overall recommendation performance."
        },
        {
            "section number": "3.2",
            "key information": "The proposed idea, Llama4Rec, involves a framework that allows conventional models and LLMs to augment each other and refine predictions through adaptive aggregation."
        },
        {
            "section number": "10.1",
            "key information": "The main challenge is the lack of a comprehensive framework that effectively integrates conventional recommendation models with LLMs, preventing optimal performance due to their individual strengths and weaknesses."
        },
        {
            "section number": "10.2",
            "key information": "Future research will explore iterative mutual augmentation strategies to further enhance model performance and efficiency."
        }
    ],
    "similarity_score": 0.8050493071011471,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e07/2e07bfd1-a9fe-4a67-a1b2-dc510564a823.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4702/47023efd-ccd3-4de0-812f-8bc7d5d719f2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a5e2/a5e258f4-d7b6-47eb-a8c2-8cfb5b757823.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1716/1716167c-527e-4135-b0de-3f943bc5fbb0.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Integrating large language models into recommendation via mutual augmentation and adaptive aggregation.json"
}