{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.15585",
    "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
    "abstract": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings.",
    "bib_name": "saley2024synergizingincontextlearninghints",
    "md_text": "# Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems\n# Vishal Vivek Saley 1, Rocktim Jyoti Das\u22173, Dinesh Raghu 2 and Mausam 1 1 Indian Institute of Technology, Delhi 2 IBM Research, New Delhi, India  \nVishal.Vivek.Saley@cse.iitd.ac.in, rocktimjyotidas@gmail.com diraghu1@in.ibm.com, mausam@cse.iitd.ac.in\nAbstract\nEnd-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings.\n# 1 Introduction\nThe rise of large-language models (LLMs) has progressed the field of NLP by leaps and bounds (Google, 2023; Touvron et al., 2023). Pre-trained over massive data, LLMs work remarkably well with just in-context learning for many NLP tasks like natural language inference, summarization, and dialogs (Kavumba et al., 2023; Hu et al., 2022; Zheng et al., 2023). One specific domain within dialogs where LLMs show promise is in building Task-Oriented Dialogs (TOD) systems, where they generate agent responses based on the dialog history and taskspecific knowledge. TOD systems, in general, can be divided into two types: modular (Young et al., 2013) and end-to-end (Madotto et al., 2018). Modular systems require domain experts to define dialog states and annotate each train dialog with state annotations. Unlike modular, end-to-end systems do \u2217*Work done when author was at IIT Delhi.\nName\nFood\n...\nArea\nPricerange\nGraffiti\nBritish\n...\nwest\nexpensive\nThe Oak Bistro\nBritish\n...\ncentre\nmoderate\nThe Copper Kettle\nBritish\n...\ncentre\nmoderate\nTravellers Rest\nBritish\n...\nwest\nexpensive\nSaint Johns Chop House\nBritish\n...\nwest\nmoderate\nRestaurant One Seven\nBritish\n...\ncentre\nmoderate\nContext\nUser: Are there any restaurants that serve proper\nBritish food in town?\nGold\nOh yes quite a few. Which part of town will you\nbe dining in?\nMAKER\nThere are no British restaurants. Would you like\nto try a different cuisine?\nFew-shot\n(GPT-4)\nYes, there are quite a few restaurants that serve\nBritish food. One of them is The Oak Bistro\nlocated at 6 Lensfield Road, with the postcode\ncb21eg. Another option is The Copper Kettle\nat 4 Kings Parade City Centre, postcode cb21sj.\nWould you like more options?\nSyncTOD\n(GPT-4)\nYes, there are quite a few. Do you have a prefer-\nence for the price range or area?\nTable 1: GPT-4 lists many potential options and extraneous details instead of seeking user input and lacks alignment with the gold.\nnot require such expensive state annotations. In this work, we focus on end-to-end TOD systems. Existing end-to-end task-oriented dialogue (TOD) systems perform well when a reasonable number of training dialogues are available. However, in many practical scenarios, only a limited number of expert-curated training dialogues are available. Figure 2 showcases the performance of MAKER, a state-of-the-art (SoTA) end-to-end TOD model, on varying training data scales. When training data is limited, existing TOD approaches fail to learn the underlying task effectively, leading to a performance drop. In contrast, large language models (LLMs) with in-context learning perform better than supervised models when the training dataset is limited. The inherent reasoning capabilities of LLMs help them learn the associated task with just a few examples.\nUnfortunately, LLM-based TOD systems do not align well with the language and style in the training dialogs, often generating overly comprehensive responses. This alignment is crucial, particularly in scenarios like in-car voice assistants, where responses must be concise and easily consumable without causing distraction. As an illustrative example, see the responses generated by various models in Table 1. We see that GPT-4 is good at reasoning but lacks alignment in presenting information. When the gold seeks additional user input when posed with excessive options, GPT-4 tends to be overly comprehensive, listing many potential options and extraneous details. This verbosity, while informative, can hinder users from easily grasping the information. On the other hand, MAKER, a SoTA supervised approach, is well aligned with agent utterances in training but makes many mistakes in reasoning. Contributions: We propose Synergizing in-context learning with hints for TOD (SyncTOD), that aligns LLMs with the stylings of the available training data. In particular, it trains auxiliary models to provide LLMs (accessed via an API) with hints (such as expected entity types in the response and response length) on how to phrase the response; selecting exemplars conditioned on these hints further improves the alignment of the responses. On three publicly available datasets, SyncTOD consistently outperforms both vanilla prompting and SoTA supervised models in low-data settings while maintaining competitive performance compared to supervised models in full-data settings.\n# 2 Related Work\nConventional TOD systems follow a modular design (Young et al., 2013; Rojas-Barahona et al., 2016; Hosseini-Asl et al., 2020; Qin et al., 2023) and require annotations for DST, PL and NLG. This work, however, focuses on end-to-end TOD systems (Eric et al., 2017; Madotto et al., 2018; Raghu et al., 2019; Wu et al.; Qin et al., 2023) that alleviate the need for annotations by directly predicting the response given dialog history and knowledge base (KB). Though LLMs have been explored for TOD tasks (Hu et al., 2022; Hude\u02c7cek and Du\u0161ek, 2023; Bang et al., 2023; Li et al., 2024), to the best of our knowledge, we are the first to explore them in an end-to-end setting. Directional Stimulus Prompting (DSP), an approach closer to ours, uses keywords\nand dialog acts as hints for summarization and response generation tasks, respectively (Li et al., 2024). However, unlike DSP, SyncTOD uses multiple hints \u2013 entity types, response length, and dialog closure \u2013 relevant to the TOD task. Further, SyncTOD also uses these hints to improve the incontext exemplars\u2019 quality using a retrieve-rerank approach. A natural approach for combining training data with in-context learning is via retrieval-augmented generation (RAG) (Lewis et al., 2020; Guu et al., 2020). Here, a retriever model infuses LLM input with exemplars from the training that are similar to the test sample (Lewis et al., 2020; Meade et al., 2023; Shi et al., 2024; Ram et al., 2023). Although out-of-box retrievers work reasonably well (Ram et al., 2023), many recent works strive to improve the retriever model further. (Zhang et al., 2018; Wang et al., 2024) employ reward-based and contrastive learning to improve retrieval quality. Specifically, they use LLMs to obtain soft rewards to fine-tune the retriever model. Recently, Patidar et al. (2024) fused multiple retriever models learned from training data with LLMs for knowledge-based question-answering tasks. What sets SyncTOD apart from RAG is its use of hints not only for selecting the informative exemplars but also for steering LLM generation from within the prompt.\n# 3 SyncTOD\nLet c = [u1, a1, u2, a2, ..., uj] be a user-agent dialog history with u and a being user and agent utterances respectively. Let y = aj be the next system response. The task of a TOD system is to predict the next system response \u02c6y given the dialog history c and a knowledge base (KB) K associated with the user\u2019s task. Let D = {(hi, Ki, yi)}n i=1 denote the train dialogs. In the in-context learning setup, an LLM is queried (via API) with an input prompt containing task instructions, a few exemplars, and (c, K) to generate \u02c6y. A popular technique for leveraging train dialogs in the in-context learning setup is retrieval augmented generation (RAG) (Zhang et al., 2023; Guu et al., 2020). In RAG, the exemplars that are most similar to c are retrieved from D and are used for generating \u02c6y. Our proposed approach, SyncTOD, synergizes incontext learning of LLMs with hints to better align with agent utterances in the training data D. Figure 1 shows the overall architecture. SyncTOD has two\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6502/6502a796-d4d2-42cf-8854-078ed96ae929.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: SyncTOD predicts useful hints \u02c6H about the expected response. The hints improve exemplar quality via re-ranking and steer the LLM (accessed via API) toward the expected response from within the prompt.</div>\nmain components: hint predictors and exemplar selector. The hint predictors output a set of hints \u02c6H given the dialog history c. These hints are domainagnostic clues, such as the entity types that should be included in the response and the length of the response, that can guide the generation to follow the same style as the train dialogs. The second component, exemplar selector, first retrieves relevant exemplars from D based on c, and then re-ranks the retrieved exemplars based on \u02c6H. Both these components are aimed at aligning the language and style of LLM responses to agent responses in the train dialogs D. As the gold responses y are available for the exemplars, we simply infer the corresponding hints from y and add the hints to the exemplars. The predictors are only used to infer hints for the given input dialog with history c. Please refer to appendix H for the exact prompt.\n# 3.1 Hint Predictors\nSyncTOD uses three types of hints: entity types (in response), response length, and dialog closure. Entity Types (ET): Entities are the informationrich elements in the agent\u2019s response. For example, the hotel name \"Lovell Lodge\" is the crucial element in the agent response \u201cHow does the Lovell Lodge sound?\". We posit that for a given dialog context and KB, the set of entity types in the agent response (e.g., {hotel name}) captures the crux of the response. Hence using expected entity types in the response as hints would align the LLM generation to D. Specifically, for given (c, K), SyncTOD predicts a list of entity types \u02c6et present in the expected system response. Then, SyncTOD amends the prompt with the rule \u2013 The response must only include entities of type: \u02c6et. To predict \u02c6et, SyncTOD learns an ET predictor model P(et|c, K) on the dataset\n{(ci, Ki, eti)}n i=1, where gold etis are the types of entities in gold response.\n{} entities in gold response. Dialog Closure (DC): The style of the dialog closures varies depending on the task at hand, and each dataset has a different way of closing the dialog. But ChatGPT generates similar, verbose and open-ended responses to the user\u2019s closing salutations. To alleviate this, SyncTOD uses dialog closure prediction dc for a given dialog (c, K) as a hint to steer LLM towards a successful closure of the dialog. Specifically, SyncTOD amends the input prompt with a rule: The response must close the dialog., when dc is true. For a training dialog (ci, Ki, yi), we define dc = True if and only if yi is the last utterance in the dialog. Response size (RS): For a (ci, Ki, yi) \u2208D, response size rs equals the number of words in the response yi. SyncTOD learns an RS predictor P(rs|c, K) on the dataset {(ci, Ki, rsi)}n i=1 and amends the input with rule: The response must be rs words or shorter. For a test dialog (c, K), SyncTOD predicts the hints \u02c6H = ( \u02c6et, \u02c6rs, \u02c6dc) using ET, RS, and DC hint predictors, respectively.\n# 3.2 Exemplar Selector\nRetrieval: SyncTOD has a retrieve-rerank mechanism to select in-context exemplars (Nogueira and Cho, 2019). Following Liu et al. (2021), SyncTOD selects points from D semantically closer to the given test dialog (c, K). Specifically, it encodes the dialog history c using a pre-trained encoder and performs a maximum inner-product search over D to retrieve the top-k points. All our experiments use BAAI/bge-large-en-v1.5 encoder model (Xiao et al., 2023). Re-ranking: Intuitively, an example with the same\ndialog state as the input is an ideal choice for an exemplar. However, end-to-end TOD datasets do not include dialog state annotations. Instead, we posit that dialog history and hints are reasonable proxies for the dialog state. SyncTOD thus re-ranks the retrieved datapoints based on hints. Let (ci, Ki, yi) be a retrieved datapoint and His be its associated hints. SyncTOD computes similarity score between hints \u02c6H and Hi as follows\nwhere 1 is an indicator function and J is Jaccard similarity. From k retrieved samples, SyncTOD selects the top two with the highest hint similarity score as exemplars.\n# 4 Experimental Setup\nDatasets For our evaluation, we use the MultiWOZ2.1 (Budzianowski et al., 2018), Stanford Multi-domain (SMD) (Eric et al., 2017), and BiTOD (English) (Lin et al.) multi-domain datasets. Appendix A provides additional details about the datasets. Baselines: We compare SyncTOD against the recent baselines - GLMP (Wu et al.), FG2Seq (He et al., 2020a), CDNet (Raghu et al., 2021), UnifiedSKG (Xie et al., 2022), and MAKER (Wan et al., 2023). We also compare against RAG with BAAI/bge-large-en-v1.5 model for exemplar retriever. Further, we report the performance of ChatGPT (gpt-3.5-turbo) and GPT-4(gpt-4-0613) in a standard few-shot setting with fixed exemplars1. Training details for hint predictors and retrieval of SyncTOD are in Appendix D. Evaluation Metric: For evaluating model performance, we use the Entity F1 (Wu et al.) and BLEU (Papineni et al., 2002) metrics prevalent in the endto-end TOD paradigm (Wu et al.; He et al., 2020a; Raghu et al., 2021; Xie et al., 2022; Wan et al., 2023, inter alia).\n# 5 Results\nFull Data Setting: Table 2 summarizes the performance of various models under full-data setting. Across all datasets, SyncTOD variants demonstrate competitive Entity F1 scores, with SyncTOD (GPT4) outperforming all the supervised baseline models. Further, ChatGPT and GPT-4 enjoy consistent performance gains when coupled with SyncTOD. 1We set temperature = 0 for LLMs generations.\nModel\nMultiWOZ\nSMD\nBiTOD\nBLEU\nEntity F1\nBLEU\nEntity F1\nBLEU\nEntity F1\nGLMP\n6.9\n32.4\n13.9\n60.7\n23.55\n68.87\nFG2Seq\n14.6\n36.5\n16.8\n61.1\n32.09\n82.91\nCDNet\n11.9\n38.7\n17.8\n62.9\n25.49\n77.13\nUnifiedSKG (T5-Large)\n13.69\n46.04\n17.27\n65.85\n36.73\n88.62\nMAKER (T5-Large)\n18.77\n54.72\n25.91\n71.3\n32.21\n80.00\nZero-shot (ChatGPT)\n3.39\n28.16\n6.91\n60.11\n3.37\n38.37\nFew-shot (ChatGPT)\n8.83\n40.25\n17.21\n70.58\n12.09\n55.50\nFew-shot (GPT-4)\n6.25\n36.47\n10.08\n63.57\n16.67\n83.43\nRAG (ChatGPT)\n8.89\n40.2\n16.71\n70.25\n10.33\n53.62\nRAG (GPT-4)\n7.64\n41.14\n13.44\n71.02\n8.09\n56.93\nSyncTOD (ChatGPT)\n14.33\n52.99\n22.08\n71.60\n19.81\n86.04\nSyncTOD (GPT-4)\n13.01\n54.99\n19.08\n72.99\n19.34\n89.04\nTable 2: Performance of SyncTOD and baselines on MultiWOZ, SMD and BiTOD datasets.\nInterestingly, RAG LLMs display a stronger Entity F1 performance on SMD than other datasets. In SMD, users express preferences differently than the other two datasets. In MultiWOZ and BiTOD, users give detailed preferences for area, price, rating, etc., and can change these during the conversation. In SMD, preferences are simpler, like the nearest parking, city weather, or meeting times. Thus, MultiWOZ and BiTOD present a more challenging problem for LLMs than SMD. Unlike Entity F1, SyncTOD variants perform poorly in BLEU. Entity F1 measures whether the system response includes relevant entities from the KB and dialog history. Whereas BLEU computes n-gram precision between the system response and the gold response. Notably, a system response that includes all relevant entities, can still receive a low BLEU score due to differences in phrasing. We find that SyncTOD responses are meaningful and include relevant entities, resulting in good Entity F1 scores. However, they use different phrasing and have less lexical overlap with gold responses, leading to lower BLEU scores. We verify the quality of SyncTOD responses via human evaluations. Human Evaluations: We had two annotators evaluate responses from Gold, MAKER, 2, and SyncTOD (GPT-4) models. They assessed the responses for a) appropriateness to the dialog history and KB, b) fluency and c) consistency on a 1-5 Likert Scale (Likert, 1932). The results in Table 3 demonstrate that SyncTOD surpasses MAKER in appropriateness and fluency across datasets, indicating higher response quality. Consistency evaluation showcases SyncTOD is truthfulness to the dialog history and the KB. Appendix F details our evaluation protocol.\n2We used resources at https://github.com/ 18907305772/MAKER to obtain MAKER responses.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d626/d626dd93-3115-4975-a122-a27e9e9248b3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: SyncTOD performance across varying training data sizes.</div>\nModel\nMultiWOZ\nSMD\nAppro.\nFluency\nConsist.\nAppro.\nFluency\nConsist.\nMAKER\n4.38\n4.79\n4.29\n4.51\n4.97\n4.57\nGold\n4.62\n4.9\n4.51\n4.79\n4.95\n4.8\nSyncTOD (GPT-4)\n4.68\n4.8\n4.74\n4.81\n4.98\n4.71\n<div style=\"text-align: center;\">Table 3: Human evaluation results.</div>\nModel\nMultiWOZ\nSMD\nBiTOD\nAvg Len\nAvg Ent\nAvg Len\nAvg Ent\nAvg Len\nAvg Ent\nGold\n17.86\n1.49\n10.87\n1.65\n13.5\n1.18\nRAG (ChatGPT)\n24.19\n2.92\n12.91\n2.25\n22.33\n1.42\nSyncTOD (ChatGPT)\n15.83\n2.14\n9.37\n1.75\n14.75\n0.99\nTable 4: SyncTOD is better aligned with Gold than RAG.\nLow Data Setting: Figure 2 shows the evaluation with varying training data sizes. SyncTOD (ChatGPT) consistently enhances ChatGPT performance and outperforms MAKER with limited data. In MultiWOZ, SyncTOD (ChatGPT) leads until MAKER catches up at around 1000 dialogs. In SMD, SyncTOD (ChatGPT) achieves Entity F1 similar to MAKER with less than 20 examples, while MAKER needs 16x more data. In BiTOD, SyncTOD (ChatGPT) significantly surpasses MAKER across training data scales. Alignment Study: SyncTOD aligns LLM responses with the dataset style. We validate this by comparing the average response length (Avg Len) and average entity count (Avg Ent) of gold and SyncTOD responses from the test set (Table 4). SyncTOD stats are closer to gold than RAG, indicating better alignment. Ablations: We perform ablations on SyncTOD (ChatGPT), with results in Table 5. Hints and exemplar retrieval are critical for SyncTOD\u2019s performance across datasets. Dropping exemplar re-ranking significantly impacts MultiWOZ and BiTOD but not SMD, likely due to SMD\u2019s simpler dialogs, which allow SyncTOD to retrieve highquality exemplars without re-ranking.\nMultiWOZ\nSMD\nBiTOD\nSyncTOD (ChatGPT)\n52.99\n71.60\n86.03\nw\\o hint prediction\n40.2\n70.25\n53.62\nw\\o exemplar retrieval\n45.47\n66.84\n63.44\nw\\o exemplar reranking\n49.94\n71.60\n78.04\nTable 5: Ablation Study: Entity F1 on MultiWOZ, SMD and BiTOD datasets\n# 6 Conclusion\nWe propose SyncTOD that leverages LLMs for endto-end TOD. Given a dialog history and KB, SyncTOD obtains hints about the expected response using auxiliary models. It then uses predicted hints to retrieve quality exemplars and guide LLMs toward the desired response. With automatic/human evaluation, we showed that SyncTOD outperforms the SoTA baseline models. Further, SyncTOD showcases a strong performance in the low-data setting.\n# Limitations\nIt would be interesting to see how SyncTOD benefits from advanced prompting techniques like chainof-thought and self-consistency. Further, SyncTOD is only tested on English datasets, though the model can easily be extended to different languages by its design. Additionally, SyncTOD performance can further be improved by designing much more sophisticated hints. Finally, SyncTOD involves both training the hint prediction modules and prompting an LLM, resulting in the cost of using LLMs and training the model.\n# Ethics Statement\nIn this work, we use OpenAI\u2019s ChatGPT and GPT4 which are commercial LLMs whose training details are not publicly available. Thus, it is unclear whether these models have seen the datasets used in this work during their training. In our experiments, we benchmark Zero-shot (ChatGPT) on all\nthe datasets and report the performance in table 2. As zero-shot (ChatGPT) performs poorly, we believe that our datasets were not part of ChatGPT\u2019s training set.\n# Acknowledgements\nThis work is supported by IBM AI Horizons Network grant, grants by Google, Verisk, and Microsoft, an IBM SUR award and the Jai Gupta chair fellowship by IIT Delhi. Vishal is supported by a Google Fellowship. We thank the IIT Delhi HPC facility for its computational resources. We are grateful to Microsoft AFMR for supporting this work.\n# References\nGoogle. 2023. Palm 2 technical report. ArXiv, abs/2305.10403.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations.\nZhenhao He, Yuhong He, Qingyao Wu, and Jian Chen. 2020a. Fg2seq: Effectively encoding knowledge for end-to-end task-oriented dialog. In ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8029\u2013 8033. IEEE. Zhenhao He, Jiachun Wang, and Jian Chen. 2020b. Task-oriented dialog generation with enhanced entity representation. In INTERSPEECH, pages 3905\u2013 3909. Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems, 33:20179\u2013 20191. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Conference on Empirical Methods in Natural Language Processing. Guanhuan Huang, Xiaojun Quan, and Qifan Wang. 2022. Autoregressive entity generation for end-toend task-oriented dialog. In Proceedings of the 29th International Conference on Computational Linguistics, pages 323\u2013332. Vojt\u02c7ech Hude\u02c7cek and Ond\u02c7rej Du\u0161ek. 2023. Are large language models all you need for task-oriented dialogue? In Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 216\u2013228. Pride Kavumba, Ana Brassard, Benjamin Heinzerling, and Kentaro Inui. 2023. Prompting for explanations improves adversarial nli. is this true?{Yes} it is {true} because {it weakens superficial cues}. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2165\u20132180. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474. Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2024. Guiding large language models via directional stimulus prompting. Advances in Neural Information Processing Systems, 36. Rensis Likert. 1932. A technique for the measurement of attitude scales. Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Peng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and Pascale Fung. Bitod: A bilingual multi-domain dataset for task-oriented dialogue modeling. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? In Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. In International Conference on Learning Representations.\nAndrea Madotto, Samuel Cahyawijaya, Genta Indra Winata, Yan Xu, Zihan Liu, Zhaojiang Lin, and Pascale Fung. 2020. Learning knowledge bases with parameters for task-oriented dialogue systems. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2372\u20132394.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1468\u20131478.\nNicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, and Dilek Hakkani-Tur. 2023. Using in-context learning to improve dialogue safety. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11882\u201311910.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking with bert. ArXiv, abs/1901.04085.\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics.\nMayur Patidar, Riya Sawhney, Avinash Singh, Biswajit Chatterjee, Indrajit Bhattacharya, et al. 2024. Fewshot transfer learning for knowledge base question answering: Fusing supervised models with in-context learning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9147\u20139165.\nLibo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen, Yangming Li, and Ting Liu. 2019. Entity-consistent end-to-end task-oriented dialogue system with kb retriever. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 133\u2013142.\nLibo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, and Min Li. 2023. End-to-end task-oriented dialogue: A survey of tasks, methods, and future directions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5925\u20135941.\nLibo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and Ting Liu. 2020. Dynamic fusion network for multidomain end-to-end task-oriented dialog. In Annual Meeting of the Association for Computational Linguistics. Dinesh Raghu, Nikhil Gupta, et al. 2019. Disentangling language and knowledge in task-oriented dialogs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1239\u20131255. Dinesh Raghu, Atishya Jain, Sachindra Joshi, et al. 2021. Constraint based knowledge base distillation in endto-end task oriented dialogs. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5051\u20135061. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316\u20131331. Lina Maria Rojas-Barahona, Milica Ga\u0161i\u00b4c, Nikola Mrksic, Pei hao Su, Stefan Ultes, Tsung-Hsien Wen, Steve J. Young, and David Vandyke. 2016. A network-based end-to-end trainable task-oriented dialogue system. In Conference of the European Chapter of the Association for Computational Linguistics. Md Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022. Dialokg: Knowledge-structure aware task-oriented dialogue generation. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2557\u20132571. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. Replug: Retrievalaugmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8364\u20138377. Xin Tian, Yingzhan Lin, Mengfei Song, Siqi Bao, Fan Wang, H. He, Shuqi Sun, and Hua Wu. 2022. Q-tod: A query-driven task-oriented dialogue system. In Conference on Empirical Methods in Natural Language Processing. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. 2022. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 602\u2013631.\nSteve J. Young, Milica Gasic, Blaise Thomson, and J. Williams. 2013. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101:1160\u20131179.\nJianguo Zhang, Stephen Roller, Kun Qian, Zhiwei Liu, Rui Meng, Shelby Heinecke, Haiquan Wang, Silvio Savarese, and Caiming Xiong. 2023. Enhancing performance on seen and unseen dialogue scenarios using retrieval-augmented end-to-end task-oriented system. In SIGDIAL Conferences.\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2018. Retrieve anything to augment large language models. Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Chatbot arena: Benchmarking llms in the wild with elo ratings. https://lmsys.org/blog/ 2023-05-03-arena/.\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2018. Retrieve anything to augment large language models. Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao\n# A Dataset Details\nFor MultiWOZ and SMD datasets, we use the versions of the dataset released by Wan et al. (2023). We adapt BiTOD dataset (Lin et al.) to end-to-end setting by associating KB to the English dialogs available in the dataset.\nDataset\nDomain\n#train\n#val\n#test\nMultiWOZ\nRestaurant, Hotel, Attraction\n1839\n117\n141\nSMD\nNavigate, Schedule, Weather\n2425\n302\n304\nBiTOD\nRestaurant, Hotel, Attraction\n1614\n169\n152\n<div style=\"text-align: center;\">Table 6: Evaluation Dataset Details</div>\n# B Rules Ablation Study\nWe conducted an ablation study using the MultiWOZ dataset by removing individual hints from SyncTOD. The results are in the table 7. Each hint is crucial for SyncTOD performance, especially the entity types hint, whose removal significantly lowers performance.\n# C Additional Baselines\nWe compared our model against the following endto-end TOD baselines - We compare SyncTOD against the following baselines - DSR (Wen et al., 2018), KB-Retriever (Qin et al., 2019), GLMP (Wu et al.), DF-Net (Qin et al., 2020), GPT-2+KE (Madotto et al., 2020), EER (He et al., 2020b), FG2Seq (He et al., 2020a), CDNet (Raghu et al., 2021), GraphMemDialog (Wu et al., 2022), ECO (Huang et al., 2022), DialoKG (Rony et al., 2022), UnifiedSKG (Xie et al., 2022), Q-TOD (Tian et al., 2022) and MAKER (Wan et al., 2023). Results are shown in table 8.\nConfiguration\nEntity F1\nSyncTOD\n52.99\nw\\o Entity Types\n41.85\nw\\o Dialog Closure\n51.38\nw\\o Response Length\n49.23\nTable 7: Rules ablation results on MultiWOZ dataset.\nModel\nMultiWOZ\nSMD\nBLEU\nEntity F1\nBLEU\nEntity F1\nDSR\n9.1\n30\n12.7\n51.9\nKB-Retriever\n-\n-\n13.9\n53.7\nGLMP\n6.9\n32.4\n13.9\n60.7\nDF-Net\n9.4\n35.1\n14.4\n62.7\nGPT-2+KE\n15.05\n39.58\n17.35\n59.78\nEER\n13.6\n35.6\n17.2\n59\nFG2Seq\n14.6\n36.5\n16.8\n61.1\nCDNet\n11.9\n38.7\n17.8\n62.9\nGraphMemDialog\n14.9\n40.2\n18.8\n64.5\nECO\n12.61\n40.87\n-\n-\nDialoKG\n12.6\n43.5\n20\n65.9\nUnifiedSKG (T5-Large)\n13.69\n46.04\n17.27\n65.85\nQ-TOD (T5-Large)\n17.62\n50.61\n21.33\n71.11\nMAKER (T5-large)\n18.77\n54.72\n25.91\n71.3\nZero-shot (ChatGPT)\n3.39\n28.16\n6.91\n60.11\nFew-shot (ChatGPT)\n8.83\n40.25\n17.21\n70.58\nFew-shot (GPT-4)\n6.25\n36.47\n10.08\n63.57\nRAG (ChatGPT)\n8.98\n40.2\n16.71\n70.25\nRAG (GPT-4)\n7.64\n41.14\n13.44\n71.02\nFew-shot (LLaMA2 70B)\n5.26\n39.68\n3.29\n46.20\nFew-shot (LLaMA2 Chat 70B)\n3.34\n30.33\n3.15\n53.27\nSyncTOD (LLaMA2 70B)\n14.44\n50.51\n15.37\n63.33\nSyncTOD (LLaMA2 Chat 70B)\n8.35\n48.01\n7.92\n63.31\nSyncTOD (ChatGPT)\n14.33\n52.99\n22.08\n71.60\nSyncTOD (GPT-4)\n13.01\n54.99\n19.08\n72.99\nTable 8: Performance of SyncTOD and baselines on MultiWOZ and SMD datasets.\n# D Training SyncTOD with Full Training Set\nWe use Nvidia V100 GPUs to train all our models.\nET Predictors: We model all the ET predictors as flan-t5-large (Chung et al., 2024) sequence predictors and train them for 8 epochs with a learning rate (LR) of 1e \u22124 and batch size (BS) of 32. We use a linear decay LR scheduler with a warm-up ratio of 0.1. We use AdamW optimizer (Loshchilov and Hutter, 2017). Training time was around 10 hours. DC Predictors: We model all the DC predictors as deberta-V3-base (He et al.) binary classifiers and train them for 5 epochs with an LR of 3e\u22125, BS of 16, and linear decay LR scheduler with a warm-up ratio of 0.1. We use AdamW optimizer. Training time was around 1 hour. RS Predictors: During our experiments, we found that the training RS predictor is unstable. Thus, we use a constant RS predictor with a value equal to the mean response size in training data. Exemplar Retrieval: For the MultiWOZ dataset, we use the last user utterance in the dialog context to dense retrieve k = 30 samples from the training data. We then re-rank them based on the hints and pick the top two. For the SMD dataset, we found that retrieval\nusing the entire dialog context works the best. We attribute it to shorter dialog context and utterances in the SMD dataset. Further, we use k = 2 as exemplars are already of high quality.\n# E Hint Predictors Performance\nAccuracy\nMultiWOZ\nSMD\nBiTOD\nClosure Prediction\n0.9564\n0.9109\n0.9570\nEntity Type Prediction\n0.6805\n0.7436\n0.8778\nTable 9 reports the performance of SyncTOD hint predictors. We report accuracy for the DC predictor and micro F1 for the ET predictor. We observe that the DC predictor achieves high performance across datasets. However, ET predictors still show room for improvement, which indicates SyncTOD performance can be pushed further.\n# F Human Evaluation Details\nA snapshot of our human evaluation portal is given in figure 3. Detailed evaluation guidelines are given at the end of this section. We human-evaluate responses from three TOD systems - Gold, MAKER, and SyncTOD (GPT-4). From MultiWOZ and SMD datasets, we sample 80 context-response pairs to evaluate appropriateness and fluency. Two annotators, undergraduate and graduate student volunteers, then independently score TOD system responses for these samples on a Likert scale (Likert, 1932). Here, the interannotator agreement was Kendall\u2019s Tau \u03c4 = 0.47 at (p < 0.0001). To evaluate consistency, we randomly sample 60 context-response pairs from the two datasets. Two student volunteers rated responses from the Gold, MAKER, and SyncTOD systems on a 15 point Likert scale. One volunteer is a PhD scholar, while the other is a graduate student with a background in machine learning and NLP. Here, the inter-annotator agreement was Kendall\u2019s Tau \u03c4 = 0.45 at (p < 0.0001). The detailed evaluation guidelines are given below.\n# Task Overview\nTask Overview\nThere are several dialog context response pairs in the html file. Each context response pair dictates a scenario where user is enquiring the agent about hotels, restaurant and attractions to visit.\n\u2022 Agent is expected to suggest hotel, restaurant and attraction with the highest rating among available options.\n\u2022 In each scenario, agent re-confirms details like user\u2019s name, selected hotel/restaurant/attraction, number of people, rooms and dates before making the final booking.\nAlong with the context response pair, there are outputs of different dialog systems (randomly shuffled). You are requested to annotate each system generated output along two dimensions: appropriateness and fluency using the following scale:\n# 1. SA: Strongly Agree 2. A : Agree 3. N : Neutral 4. D : Disagree 5. SD: Strongly Disagr\n5. SD: Strongly Disagree\n# How to judge appropriateness?\n1. Strongly Agree - when the generated output conveys the intended information \u2013correct entity (hotel/restaurant/attraction) and its attributes (address, phone, rating, etc). Also, when generated output requests correct input from the user.\n2. Agree \u2013 when generated output contains partial information (e.g., when user request address and phone number but output contains only address).\n3. Neutral \u2013 when generated output is hard to decide whether its right or wrong.\n4. Disagree - when the generated response is somewhat unacceptable (e.g., re-querying already known information like cuisine for restaurants and name of the user for booking).\n5. Strongly Disagree \u2013 when the generated output contains incorrect information (entities or attributes) for given conversation context.\n# How to judge fluency?\nEvaluate the linguistic quality of the response, including grammar, coherence, and readability. The fluency of the response is independent of the dialog context or ground truth. A system output can be marked strongly disagree for appropriateness and still be marked strongly agree for fluency. You can make your own rules about what each rating in the scale means for fluency, but please be consistent with the rules you come up with.\n# How to judge Consistency?\nConsistency of system response is the degree to which the system\u2019s response accurately reflects and logically aligns with the dialogue history and the knowledge base. Please rate each system response on the following scale.\n\u2022 Strongly Disagree (SD): The response is completely inconsistent with the dialogue history and the knowledge base. It provides incorrect information, contradicts previous dialog, and does not align with known facts.\n\u2022 Disagree (D): The response has significant inconsistencies with the dialogue history and knowledge base. It may provide some correct information but contains major errors or contradictions.\n\u2022 Neutral (N): The response is generally consistent with the dialogue history and knowledge base but may include minor errors or inconsistencies. The response mostly aligns with the previous context but might have inaccuracies or ambiguities.\n\u2022 Agree (A): The response is consistent with the dialogue history and knowledge base. It correctly addresses the context and facts, with only minor issues that do not significantly impact the overall coherence.\n\u2022 Strongly Agree (SA): The response is fully consistent with the dialogue history and knowledge base. It accurately reflects the context, aligns perfectly with known facts, and shows no contradictions or irrelevant information.\n\u2022 Strongly Agree (SA): The response is fully consistent with the dialogue history and knowledge base. It accurately reflects the context, aligns perfectly with known facts, and shows no contradictions or irrelevant information.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f949/f949a1df-1b63-4ca6-bccc-2821ade98835.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Human Evaluation Portal</div>\nModel\nAppropriateness\nFluency\nMAKER\n4.71\n4.97\nGold\n4.89\n4.85\nSyncTOD (GPT-4)\n4.94\n4.94\n# F.1 BiTOD Human Evaluation\nWe evaluate the MAKER, Gold, and SyncTOD (GPT-4) systems on the BiTOD dataset for Appropriateness and Fluency. The results, shown in Table 10, indicate that SyncTOD outperforms MAKER in terms of Appropriateness, while all models demonstrate strong performance in Fluency.\n# G Case Studies\nWe present examples from MultiWOZ (table 11), SMD (tables 12 and 13) and BiTOD (table 14) comparing responses from SyncTOD, MAKER and the gold systems.\n# H Prompt Specification\nSyncTOD prompts are comprised of instructions followed by tuples (database, rule, dialog, follow-up response) for exemplars and test sample.\ninstructions - Task definitions and ontology details for the dataset.\ndialog history - User and system utterances in the dialog context c.\nfollow-up response - For exemplars, we succinctly re-iterate the task definition and the entity types expected in the response, followed by gold entities and the response. For the test sample, we only provide task definition and entity types expected in the response and prompt the LLM to generate entities and the final response in order.\n# H.2 Creating rules from hints\nWe transform hints H = (et, dc, rs) to rules in the prompt as follows. For response size, We add a rule The response must be rs words or shorter. For dialog closure dc = True(False), we add a rule The response must (not) close the dialog.. For entity types et = [t1, t2, t3], we add a rule The response must only include entities of type - t1, t2, t3.. We also introduce a rule The response must not include any entities of type - t\u2032 1, t\u2032 2, .. where t\u2032 are entity types not present in et. We find that explicitly presenting negative entity types demotivates LLM from including extraneous entities in the response.\nName\nAddress\nArea\nPhone\nPostcode\nPricerange\nStars\nType\nChoice\nRef\nParking\nInternet\nalpha-milton guest house\n63 milton road\nnorth\n01223311625\ncb41xa\nmoderate\n3 star\nguesthouse\n-\nepwuawm5\nno\nno\nalexander bed and breakfast\n56 saint barnabas road\ncentre\n01223525725\ncb12de\ncheap\n4 star\nguesthouse\n-\nepwuawm5\nyes\nyes\nacorn guest house\n154 chesterton road\nnorth\n01223353888\ncb41da\nmoderate\n4 star\nguesthouse\n-\nepwuawm5\nyes\nyes\nallenbell\n517a coldham lane\neast\n01223210353\ncb13js\ncheap\n4 star\nguesthouse\n-\nepwuawm5\nyes\nyes\nbridge guest house\n151 hills road\nsouth\n01223247942\ncb28rj\nmoderate\n3 star\nguesthouse\n-\nepwuawm5\nyes\nyes\na and b guest house\n124 tenison road\neast\n01223315702\ncb12dp\nmoderate\n4 star\nguesthouse\n-\nepwuawm5\nno\nyes\nContext\nUser: I\u2019 m looking for a place. I think it\u2019s called like bridge house or bridge guesthouse or something.\nGold\nYou are looking for the Bridge Guest House.\nMAKER\nI\u2019m sorry, I don\u2019t have any information on that. Would you like to try a different name?\nSyncTOD\n(ChatGPT)\nBridge guest house is a moderate priced guesthouse in the south with a 3 star rating. Can I help you book a room\nthere?\nSyncTOD\n(GPT-4)\nYes, the Bridge Guest House is a 3 star guesthouse in the south with moderate pricing.\nName\nAddress\nArea\nPhone\nPostcode\nPricerange\nStars\nType\nChoice\nRef\nParking\nInternet\nalpha-milton guest house\n63 milton road\nnorth\n01223311625\ncb41xa\nmoderate\n3 star\nguesthouse\n-\nepwuawm5\nno\nno\nalexander bed and breakfast\n56 saint barnabas road\ncentre\n01223525725\ncb12de\ncheap\n4 star\nguesthouse\n-\nepwuawm5\nyes\nyes\nacorn guest house\n154 chesterton road\nnorth\n01223353888\ncb41da\nmoderate\n4 star\nguesthouse\n-\nepwuawm5\nyes\nyes\nallenbell\n517a coldham lane\neast\n01223210353\ncb13js\ncheap\n4 star\nguesthouse\n-\nepwuawm5\nyes\nyes\nbridge guest house\n151 hills road\nsouth\n01223247942\ncb28rj\nmoderate\n3 star\nguesthouse\n-\nepwuawm5\nyes\nyes\na and b guest house\n124 tenison road\neast\n01223315702\ncb12dp\nmoderate\n4 star\nguesthouse\n-\nepwuawm5\nno\nyes\nTable 11: SyncTOD models understand user\u2019s requirement for bridge guest house and present required informatio from the KB. MAKER, however, produces incorrect results.\nEvent\nRoom\nAgenda\nTime\nDate\nParty\nconference\nconference room 102\ngo over budget\n7pm\nmonday\nboss\nlab appointment\n-\n-\n9am\ntuesday\ntom\ndinner\n-\n-\n7pm\ntuesday\nmartha\noptometrist appointment\n-\n-\n5pm\nthe 5th\nbrother\nContext\nUser: I need to know my conference agenda, date and who is attending.\nSystem: Your conference is set for Monday with your boss to go over budget.\nUser: Great, and who all will be there?\nGold\nThe conference is with your boss.\nMAKER\nYou have three conference agendas, one for Monday with your boss to go over budget, one for Friday with your\nboss to go over the budget, and one for Monday with your boss to go over the budget.\nSyncTOD\n(ChatGPT)\nYour boss will be attending the conference.\nSyncTOD\n(GPT-4)\nYour boss will attend the conference.\nEvent\nRoom\nAgenda\nTime\nDate\nParty\nconference\nconference room 102\ngo over budget\n7pm\nmonday\nboss\nlab appointment\n-\n-\n9am\ntuesday\ntom\ndinner\n-\n-\n7pm\ntuesday\nmartha\noptometrist appointment\n-\n-\n5pm\nthe 5th\nbrother\nPoi\nDistance\nAddress\nPoi Type\nTraffic Info\nCoupa\n6 miles\n394 van ness ave\ncoffee or tea place\nmoderate traffic\nMandarin Roots\n2 miles\n271 springer street\nchinese restaurant\nheavy traffic\nFour Seasons\n3 miles\n465 arcadia pl\nrest stop\nno traffic\nRavenswood Shopping Center\n1 miles\n434 arastradero rd\nshopping center\nmoderate traffic\nContext\nUser: Give me directions to the local starbuck\nSystem: I do not have info on the local starbucks but i do see another coffee place. Would you like to hear info\non it?\nUser: Yes, please! Which one is its address? There is any quick route available?\nGold\nThere is Coupa located at 394 van ness ave, there is moderate traffic but you\u2019re just 6 miles away\nMAKER\nI\u2019m sorry, I do not have that info on you, I sent it on your screen, drive carefully!\nSyncTOD\n(ChatGPT)\nCoupa is located at 394 van ness ave. There is moderate traffic on the way.\nSyncTOD\n(GPT-4)\nCoupa at 394 van ness ave has moderate traffic.\nPoi\nDistance\nAddress\nPoi Type\nTraffic Info\nCoupa\n6 miles\n394 van ness ave\ncoffee or tea place\nmoderate traffic\nMandarin Roots\n2 miles\n271 springer street\nchinese restaurant\nheavy traffic\nFour Seasons\n3 miles\n465 arcadia pl\nrest stop\nno traffic\nRavenswood Shopping Center\n1 miles\n434 arastradero rd\nshopping center\nmoderate traffic\nContext\nUser: Give me directions to the local starbuck\nSystem: I do not have info on the local starbucks but i do see another coffee place. Would you like to hear info\non it?\nTable 13: SyncTOD models accurately answer user\u2019s query.\nname\naddress\ncuisine\ndietary_restrictions\nlocation\nphone_number\nprice_level\nrating\n0\nchocoduck_bistro\n3_kin_san_lane_cheung_chau\nitalian\nvegan_options\nnew_territories\n852_9735_0049\nmoderate\n10\n1\nla_table_french_brasserie\n72_mody_road_1f_new_world_millennium_hong_kong...\nfrench\nvegan_options\ntsim_sha_tsui\n852_2313_4222\nmoderate\n10\n2\nfleur_de_sel\nshop_2j_po_foo_building_foo_ming_street_causew...\nfrench\nvegan_options\nwan_chai_causeway_bay\n852_2805_6678\nmoderate\n10\n3\nserendib\nshop_2_gf_nam_wo_hong_building_148_wing_lok_st...\nasian\nvegan_options\nsheung_wan\n852_3705_2429\nmoderate\n10\n4\nbombay_dreams\n77_wyndham_street\nindian\nvegan_options\ncentral\n852_2971_0001\nmoderate\n9\n5\ndivino_wine_bar_restaurant\n73_wyndham_street_central\nitalian\nvegan_options\ncentral\n852_2167_8883\nmoderate\n9\n6\nenchanted_garden_restaurant\nhong_kong_disneyland_resort_disneyland_hotel_l...\nseafood\nvegan_options\nnew_territories\n852_3510_6000\nmoderate\n9\n7\njw_cafe\n5f_jw_marriott_hotel_hong_kong_pacific_place_n...\ncafe\nvegan_options\nhong_kong_island\n852_2810_8366\nmoderate\n9\n1\nla_table_french_brasserie\n72_mody_road_1f_new_world_millennium_hong_kong...\nfrench\nvegan_options\ntsim_sha_tsui\n852_2313_4222\nmoderate\n10\n2\nfleur_de_sel\nshop_2j_po_foo_building_foo_ming_street_causew...\nfrench\nvegan_options\nwan_chai_causeway_bay\n852_2805_6678\nmoderate\n10\n3\nserendib\nshop_2_gf_nam_wo_hong_building_148_wing_lok_st...\nasian\nvegan_options\nsheung_wan\n852_3705_2429\nmoderate\n10\n4\nbombay_dreams\n77_wyndham_street\nindian\nvegan_options\ncentral\n852_2971_0001\nmoderate\n9\n5\ndivino_wine_bar_restaurant\n73_wyndham_street_central\nitalian\nvegan_options\ncentral\n852_2167_8883\nmoderate\n9\n6\nenchanted_garden_restaurant\nhong_kong_disneyland_resort_disneyland_hotel_l...\nseafood\nvegan_options\nnew_territories\n852_3510_6000\nmoderate\n9\n7\njw_cafe\n5f_jw_marriott_hotel_hong_kong_pacific_place_n...\ncafe\nvegan_options\nhong_kong_island\n852_2810_8366\nmoderate\n9\nContext\nUser: Hello. Can you help me find a place to eat ?\nSystem: Would you like to choose the rating value for the restaurant?\nUser: Not really . I just want to make sure they have vegan_options .\nSystem: Did you have a price range or cuisine type in mind ?\nUser: Any cuisine will work , but it needs to be moderately priced .\nSystem: I\u2019ve found #210 options with those criteria . May i recommend an Italian place with a rating of 10\ncalled chocoduck_bistro ?\nUser: That sounds good . can i make a reservation ?\nSystem: Definitely ! how many will be in your party and what time would you like to book ?\nUser: There will be 14 of us , and we can be there by 410 in the afternoon on Sunday .\nSystem: Wonderful , and what name should i put that under ?\nUser: Put it under Danielle , please .\nGold\nGreat ! Just to confirm your reservation for Sunday at 410 in the afternoon at chocoduck_bistro for a party of\n14 , correct ?\nMAKER\nOkay , and what is your booking time ?\nSyncTOD\n(ChatGPT)\nGot it, I can make a reservation for 14 people at chocoduck_bistro on Sunday at 410 pm under the name\nDanielle.\nSyncTOD\n(GPT-4)\nSo, a reservation for 14 at chocoduck_bistro, 410 pm on Sunday.\nTable 14: SyncTOD models assists user in making the reservation.\n# H.3 Sample Prompts\nMultiWOZ\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/69a0/69a09f31-d7ed-4958-911b-5442ce922ac7.png\" style=\"width: 50%;\"></div>\nHenceforth, assume that you are an expert in in-car infotainment. I will give you an incomplete dialog between a user and an in-car infotainment system. As an expert, you must suggest the most appropriate follow-up response to the dialog. Ensure you also include correct information (entities) from the given database. Entities can be of the following types 1. poi - name of a point of interest, e.g., home, starbucks, pizza chicago, etc. 2. address - address of a poi, e.g, 783 arcadia pl. 3. poi type - the type of a poi, e.g., tea or coffee place, hospital, shopping center, etc. 4. traffic info - traffic status on the way to a poi, e.g., heavy traffic, no traffic, road block nearby, etc. 5. distance - distance of a poi from the user\u2019s current location, e.g., 2 miles, 4 miles, etc. 6. event - an event in the user\u2019s calendar 7. date - date in a month like the 1st or the 4th or day of a week like monday, wednesday. 8. time - the time on which an event is scheduled 9. party - party attending an event, e.g., tom, boss, brother, executive team, etc. 10. agenda - agenda associated with an event, e.g., discuss dress code, go over budget, etc. 11. room - meeting place of an event, e.g., conference room 100, etc. 12. location - a location for which the user may request the weather information, e.g, boston, los angeles, etc. 13. weather attribute - weather description in a location, e.g., cloudy, warm, hot, overcast etc. 14. temperature - the in a location, e.g., 60f, 100f, etc. 15. weekly time - temporal indicators like today, tomorrow, next week etc. As an expert, you are very strict about following rules. Make sure that the follow-up response you write follows all the given rules. Here are the examples [example 1] [database 1] { \"trader joes\":{ \"address\":\"408 university ave\", \"poi type\":\"grocery store\", \"traffic info\":\"no traffic\", \"distance\":\"5 miles\" },.... } [rules 1] The response must be 11 words or shorter. The response must not close the dialog. The response must only include entities of type - poi, poi type. The response must not include any entities of type - address, traffic info, distance, event, date, time, party, agenda, room, location, weather attribute, temperature, weekly time. [dialog history 1] user: give me directions to the nearest grocery store [follow-up response 1] Let\u2019s think step-by-step. As an expert, I must understand the user\u2019s requirements from [dialog history 1], identify the relevant information from the [database 1], follow all the [rules 1] and write the response. I will include entities of type [\u2019poi\u2019, \u2019poi type\u2019] in my response. I will include these entities - [(\u2019poi\u2019, \u2019trader joes\u2019), (\u2019poi type\u2019, \u2019grocery store\u2019)] system: the nearest grocery store is trader joes , would you like directions ? [example 2] [database 2] { \"safeway\":{ \"address\":\"452 arcadia pl\", \"poi type\":\"grocery store\", \"traffic info\":\"heavy traffic\", \"distance\":\"4 miles\" },.... } [rules 2] The response must be 23 words or shorter. The response must not close the dialog. The response must only include entities of type - distance, poi, traffic info. The response must not include any entities of type - address, poi type, event, date, time, party, agenda, room, location, weather attribute, temperature, weekly time. [dialog history 2] user: give me directions to the closest grocery store [follow-up response 2] Let\u2019s think step-by-step. As an expert, I must understand the user\u2019s requirements from [dialog history 2], identify the relevant information from the [database 2], follow all the [rules 2] and write the response. I will include entities of type [\u2019distance\u2019, \u2019poi\u2019, \u2019traffic info\u2019] in my response. I will include these entities - [(\u2019distance\u2019, \u20194 miles\u2019), (\u2019poi\u2019, \u2019safeway\u2019), (\u2019traffic info\u2019, \u2019heavy traffic\u2019)] system: we \u2019 re 4 miles away from safeway but there is heavy traffic in this moment : do i set the gps to go there ? [example 3] [database 3] { \"sigona farmers market\":{ \"address\":\"638 amherst st\", \"poi type\":\"grocery store\", \"traffic info\":\"no traffic\", \"distance\":\"4 miles\" },.... } [rules 3] The response must be 10 words or shorter. The response must not close the dialog. The response must only include entities of type - distance, poi, poi type. The response must not include any entities of type - address, traffic info, event, date, time, party, agenda, room, location, weather attribute, temperature, weekly time. [dialog history 3] user: give me directions to the closest grocery store [follow-up response 3] Let\u2019s think step-by-step. As an expert, I must understand the user\u2019s requirements from [dialog history 3], identify the relevant information from the [database 3], follow all the [rules 3] and write the response. I will include entities of type [\u2019distance\u2019, \u2019poi\u2019, \u2019poi type\u2019] in my response. I will include these entities -\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b07/1b073e9f-e78c-4e00-8410-5d479f608e48.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a1a9/a1a9da38-16c0-44c2-bc9c-ccfdd02d1039.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly.",
        "problem": {
            "definition": "Existing end-to-end task-oriented dialogue systems perform well when a reasonable number of training dialogues are available. However, in many practical scenarios, only a limited number of expert-curated training dialogues are available, leading to a performance drop.",
            "key obstacle": "LLM-based TOD systems do not align well with the language and style in the training dialogs, often generating overly comprehensive responses."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to improve the alignment of LLM-generated responses with the style of training data, particularly in low-data settings.",
            "opinion": "The proposed idea, SyncTOD, synergizes LLMs with task-specific hints to improve alignment in low-data settings.",
            "innovation": "SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts, which differentiates it from existing approaches."
        },
        "method": {
            "method name": "SyncTOD",
            "method abbreviation": "SyncTOD",
            "method definition": "SyncTOD synergizes in-context learning of LLMs with hints to better align with agent utterances in the training data.",
            "method description": "SyncTOD uses hints to guide LLM responses in task-oriented dialogues.",
            "method steps": [
                "Predict hints based on dialog history.",
                "Retrieve relevant exemplars from training data.",
                "Re-rank exemplars based on predicted hints.",
                "Generate responses using LLMs guided by hints."
            ],
            "principle": "The method is effective because it aligns LLM responses with the expected style and content of training data, improving user comprehension."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using three publicly available datasets: MultiWOZ2.1, Stanford Multi-domain (SMD), and BiTOD.",
            "evaluation method": "Performance was assessed using Entity F1 and BLEU metrics, with comparisons against several baseline models."
        },
        "conclusion": "SyncTOD outperforms state-of-the-art baseline models in low-data settings and demonstrates strong performance in full-data settings, showcasing its effectiveness in task-oriented dialogue systems.",
        "discussion": {
            "advantage": "The key advantage of SyncTOD is its ability to improve response alignment with training data style, enhancing user experience.",
            "limitation": "SyncTOD's performance may vary based on the sophistication of the hints used and has only been tested on English datasets.",
            "future work": "Future research could explore advanced prompting techniques, test on non-English datasets, and improve hint sophistication to enhance performance."
        },
        "other info": {
            "ethics statement": "The work uses commercial LLMs whose training details are not publicly available, raising questions about potential biases and data exposure."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning allows large language models (LLMs) to excel in task-oriented dialogue (TOD) systems even with limited data."
        },
        {
            "section number": "1.3",
            "key information": "LLM-based TOD systems can learn tasks through in-context exemplars but often lack alignment with the style of responses in training data."
        },
        {
            "section number": "3.1",
            "key information": "SyncTOD synergizes in-context learning of LLMs with hints to better align with agent utterances in the training data."
        },
        {
            "section number": "3.4",
            "key information": "SyncTOD uses hints to guide LLM responses in task-oriented dialogues, improving user comprehension."
        },
        {
            "section number": "4.1",
            "key information": "The design of hints in SyncTOD significantly influences the outcomes of in-context learning by improving response alignment."
        },
        {
            "section number": "6.1",
            "key information": "SyncTOD's performance may vary based on the sophistication of the hints used, highlighting issues related to model bias."
        },
        {
            "section number": "6.4",
            "key information": "SyncTOD has only been tested on English datasets, indicating scalability and applicability challenges in non-English contexts."
        }
    ],
    "similarity_score": 0.6986621084643677,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems.json"
}