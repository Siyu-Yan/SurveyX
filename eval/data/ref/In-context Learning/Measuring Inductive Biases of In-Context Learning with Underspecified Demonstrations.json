{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.13299",
    "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
    "abstract": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases\u2014for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.1",
    "bib_name": "si2023measuringinductivebiasesincontext",
    "md_text": "# Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations\nChenglei Si1\u2217 Dan Friedman2\u2217 Nitish Joshi3 Shi Feng4 Danqi Chen2 He He3 1University of Maryland 2Princeton University 3New York University 4University of Chicago clsi@umd.edu, dfriedman@cs.princeton.edu\n# Abstract\nIn-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases\u2014for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.1\narXiv:2305.13299v1\n# 1 Introduction\nIn-context learning (ICL) is an increasingly popular paradigm for adapting large language models (LLMs) to downstream tasks (Brown et al., 2020). It works by prompting LLMs with a small set of examples that demonstrate the input and output of a task, without requiring any parameter updates. However, a key limitation of ICL is that it can only incorporate a small number of demonstration examples (e.g., 16) due to the context length limit of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfec/dfecf28c-2a54-48d3-9db0-5c1136f85e0d.png\" style=\"width: 50%;\"></div>\nSentence \nLabel\nI've seen this movie more \nthan once and it's worth it.\n1\nThe most disappointing \nChinese take-out I've had.\n0\nSoup was served cold twice \neven after we complained.\n0\nGreat movie, great actors, \ngreat soundtrack! I loved it!\n1\nGPT-3\nTwo hypotheses:\nh1:  Sentiment \n positive - 1, negative - 0\nh2:  Topic \nmovie - 1, food - 0\nDemonstrations\nTest example\nThe steak is incredible, and \nis reasonably priced.\nWords can\u2019t describe how \nutterly abysmal this movie is.\n0                 1\nWithout intervention\nh1: 92.4% h2: 7.6%\nWith instruction:\n\u201cClassify based on the topic.\u201d\n h1:  1.1%  h2: 98.9%\nBetter verbalizers:\n\u201c1\u201d \u2192 \u201cmovie\u201d,  \u201c0\u201d  \u2192 \u201cfood\u201d\nh1: 0.5% \nh2: 99.5%\nModel preference\n1                 0\nh1               h2\nFigure 1: We prompt language models with underspecified demonstrations in which two features are equally predictive of the label. In this case, the decision rule could be based on either sentiment (positive vs. negative) or topic (movie vs. food). We measure feature biases by testing the model on disambiguation examples where the two hypotheses disagree, such as positive restaurant reviews. Here, GPT-3 strongly favors the sentiment hypothesis, but we can encourage it to prefer the topic feature using various interventions, e.g., adding a natural-language instruction or using verbalized labels.\nTransformer models, meaning that most tasks will be highly underspecified. For example, as shown in Figure 1, we present the model with an underspecified text classification problem. The sentences with label \u20181\u2019 are positive reviews of movies, and the sentences with label \u20180\u2019 are negative reviews of restaurants. From the demonstrations, it is unclear whether the labels are determined by the sentiment feature (positive vs. negative) or the topic feature (movie vs. food). Moreover, due to the limited context window, it is difficult to specify the task\nby supplying a large number of additional training examples. Instead, ICL can succeed only if (a) the LLM has an inductive bias that happens to align well with the given task or (b) there is a mechanism for imposing an inductive bias on the system, which can specify the task (e.g., whether it is sentiment classification or topic classification) without incorporating more training examples. In this paper, we study the inductive biases of ICL with LLMs and measure the effectiveness of different interventions to steer ICL towards a particular hypothesis. We focus on feature biases: a tendency to use one feature rather than another given a prompt in which the two features predict the label equally well. As illustrated in Figure 1, by evaluating the model on sentences where the two features disagree\u2014such as positive restaurant reviews\u2014we can measure the feature bias of the learning algorithm, and we can attempt to modify the feature biases through various interventions applied to the prompt. In the first part of the paper, we characterize the feature biases of ICL by constructing underspecified demonstrations from a variety of NLP datasets and feature pairs. We find that ICL models exhibit some clear feature biases. For example, in a sentiment analysis setting, the LLMs we study exhibit a strong tendency to generalize on the basis of sentiment rather than other equally predictive features, including topics of sentences. On sentence-pair tasks, such as question answering and natural language inference, the GPT-3 model (Brown et al., 2020) generally prefers shallow lexical features, while the instruction-tuned models (Ouyang et al., 2022) generalize more consistently with the labels associated with those datasets. Such feature biases could potentially be problematic\u2014users could have intended either of the two predictive features as the actual task. When the model\u2019s feature bias does not align with the intended task, we want the model to be steerable with appropriate interventions. In the second part of the paper, we measure whether simple modifications of the prompt can supply an inductive bias to the ICL learning algorithm, steering the model to generalize according to one feature rather than another. These interventions include using natural-language instructions or explanations and using label words that are semantically related to the intended feature. As a baseline, we compare these methods with unambiguous prompts, in which some demonstration\nexamples are consistent with one hypothesis but not the other. We find that these interventions are most effective when the model does not have a strong feature bias, or already has a bias in favor of the intended task feature. They are less effective at steering the model to use one feature when it has a strong bias to use another feature. For example, the instruction-tuned model generalizes on the basis of sentiment despite adding instructions and even disambiguating evidence in favor of lexical features like punctuation. Interestingly, we find that data-independent methods, like using semantically relevant label words, sometimes have a stronger effect than providing unambiguous data. These results underscore some of the challenges involved with using ICL as a general-purpose machine learning method, complementing a growing body of work that has attempted to explain how ICL works from an empirical (e.g., Min et al., 2022; Webson and Pavlick, 2022; Chan et al., 2022) and theoretical (Xie et al., 2022; Aky\u00fcrek et al., 2023; von Oswald et al., 2022) point of view. On one hand, the strong inductive biases of LLMs are helpful when they happen to be well aligned with the given task, enabling ICL to generalize successfully from very few training examples. Moreover, simple modifications to the prompt are often successful at steering the model towards a particular feature in underspecified settings. On the other hand, simple prompting methods cannot systematically align the model with user intention: they have limited effectiveness when the model\u2019s feature biases conflict with the intended task.\n# 2 Setup\n# 2.1 Measuring Feature Biases\nWe consider text classification problems, where x \u2208X is a text input and h1, h2 : X \u2192{0, 1} are two binary feature functions. For example, h1 may be a sentiment classifier (returning 0 if x is negative and 1 if it is positive), and h2 is a domain classifier indicating whether x is a review of a movie or a restaurant. We consider a learning algorithm \u2113, defined as a mapping from a dataset D \u2286X \u00d7 {0, 1} to a classifier f : X \u2192{0, 1}. Given a learning algorithm \u2113and a pair of feature functions h1, h2, our aim is to understand whether \u2113tends to return classifiers more similar to h1 or h2, given that h1 and h2 have the same accuracy on D. In the context of ICL, we measure this property behaviorally by prompting language models with a\nset of underspecified demonstrations Ddemo and evaluating the resulting function f = \u2113(Ddemo) on a disambiguating dataset Dtest. The underspecified demonstrations are examples Ddemo \u2208 X \u00d7 {0, 1} such that, for all (x, y) \u2208Ddemo, y = h1(x) = h2(x); and we ensure that the labels are balanced on D. The disambiguating dataset Dtest is constructed so that, for all x, h1(x) \u0338= h2(x), and the dataset is balanced with respect to h1(x) and h2(x). A simple example is illustrated in Figure 1. We measure whether f is more consistent with h1 or h2 by comparing the predictions of f, h1, and h2 on Dtest. For a given feature function h, we define the accuracy on h as the proportion of examples for which f(x) = h(x):\nThe difference between h1-accuracy and h2accuracy on Dtest can be interpreted as a feature bias: for example, a high value of h1-accuracy indicates that the model is more likely to predict the same label as h1 in situations where h1 and h2 disagree. h1-accuracy and h2-accuracy always add up to 1 and, because Dtest is balanced, an h1-accuracy of 0.5 indicates that the model does not exhibit a strong bias towards either feature.\n# 2.2 In-Context Learning\nThe learning algorithms we consider in this paper are based on in-context learning (ICL) of LLMs (Brown et al., 2020). A language model p\u03b8(w) assigns scores to strings w \u2208V\u2217, where V is a discrete vocabulary. The input to ICL is a language model p\u03b8(w) and a function that converts a dataset D and a single test example xtest into a prompt c(D, xtest) \u2208V\u2217. We consider the basic form of ICL, which consists of2: (1) an instance template t : X \u2192V\u2217that encodes each data instance x as a string; (2) a label verbalizer v : {0, 1} \u2192V\u2217that encodes each label as a string. For the first part of our analysis on measuring feature biases (Section 4), we adopt the simplest format and define the instance template as t(x) = \u201cInput: $x Label: \u201d, and the label verbalizer as v(0) = \u201c0\u201d and v(1) = \u201c1\u201d. The prompt c is then the concatenation of t(xi) and v(yi) for all demonstration examples (xi, yi) \u2208D; and lastly\nthe test instance t(xtest). The resulting classifier is: f(xtest; D) = arg max y p\u03b8(v(y) | c(D, xtest)).\nICL is known to be sensitive to the order of the demonstrations (Lu et al., 2022) and to demonstrate other biases that are orthogonal to this study, including majority label bias and recency bias (Zhao et al., 2021). We control for these by ordering the demonstration examples randomly and performing label calibration, following (Zhao et al., 2021).\n# 3 Data Construction\nWe choose datasets to cover four different NLP tasks, including both single-sentence and sentencepair classification. For sentiment analysis, we use IMDb (Maas et al., 2011) and Yelp (Asghar, 2016) datasets; for toxicity classification, we use the CivilComments dataset (Borkan et al., 2019); for natural language inference, we use the MNLI dataset (Williams et al., 2018); and for question answering, we use BoolQ (Clark et al., 2019). For each dataset, we select the original classification label as the default feature and denote it as h1. We select alternative comparison features (h2) using existing metadata or simple, deterministic functions, chosen to reflect realistic sources of ambiguity or spurious correlation that have been studied in prior work (McCoy et al., 2019; Gururangan et al., 2018; Poliak et al., 2018; Joshi et al., 2022), as well as common shallow features such as length, capitalization, and the presence of particular words or punctuation marks. All datasets and features we use are listed in Table 1, which we elaborate below: (1) For sentiment analysis, the default feature is the sentiment, and the alternative features include: domain or source of the review (based on whether it is from IMDb or Yelp), length of the review (longer or shorter than a threshold), the final punctuation mark of the review (exclamation mark or period), whether it contains certain keywords (\u201cfood\u201d and \u201cnice\u201d), and whether it contains uppercase words (e.g., \u201cTHIS\u201d). (2) For toxicity classification, the default feature is whether the comment is toxic. The alternative features are: gender, sexuality, religion, and race mentioned in the comment (all based on humanannotated meta-data), and its length and capitalization (whether containing uppercase words). (3) For NLI, the default feature is the entailment relationship between the sentence pair, and we con-\nTask\nDataset\nHypotheses\nSingle-sentence classification\nSentiment analysis\nIMDb + Yelp\nSentiment (positive vs. negative)\nDomain (IMDb vs. Yelp)\nLength (short vs. long)\nTerminal punctuation (exclamation vs. period)\nContains word (\u201cnice\u201d/\u201cfood\u201d)\nCapitalization (lowercase vs. uppercase)\nToxicity classification\nCivilComments\nToxicity (toxic vs. non-toxic)\nGender (female vs. male)\nSexuality (LGBTQ vs. non-LGBTQ)\nReligion (Muslim vs. Christian; Muslim vs. Jewish)\nRace (Black vs. White; Asian vs. White)\nLength (short vs. long)\nCapitalization (lowercase vs. uppercase)\nSentence-pair classification\nNatural language inference\nMultiNLI\nEntailment (entailment vs. non-entailment)\nDomain (government vs. fiction; government vs. telephone)\nLexical overlap (overlap vs. non-overlap)\nHypothesis length (long vs. short)\nHypothesis negation (contains \u201cnot\u201d, \u201cn\u2019t\u201d, \u201cno\u201d)\nQuestion answering\nBoolQ\nAnswer (yes vs. no)\nQuestion word (\u201cis/was\u201d vs. \u201cdo/does/did\u201d)\nLexical overlap (overlap vs. non-overlap)\nQuestion structure (\u201cis x the same as y\u201d)\nPassage length (short vs long)\nTable 1: The datasets and features we study in this paper. We treat the first feature for each task as the defau feature (referred to as h1) and the others as distractors (h2). We measure feature biases between the default featur (in bold) and each of the distractor features.\ndense the neutral and contradiction classes as nonentailment to cast the task as binary classification. For alternative features, we consider: domain of the text (from the genre meta-data); lexical overlap, following the definition in HANS (McCoy et al., 2019); whether the hypothesis is shorter or longer than the premise; and whether the hypothesis contains negation words (\u201cno\u201d, \u201cnot\u201d, \u201cn\u2019t\u201d). (4) For question answering, the default feature is whether the answer is yes or no, and the alternative features are: the question word, whether all words from the question also appear in the passage (lexical overlap), question structure (whether it is a comparison question), and passage length. These features are potential spurious features in QA that have been documented in prior work (Pezeshkpour et al., 2022; Shinoda et al., 2022).\n# 4 Measuring Feature Biases\n# 4.1 Experiment Details\nEvaluation protocol. For all experiments, we randomly sample a balanced set of 16 demonstration examples (randomly shuffled) to form the prompt. For eight of the examples, y = h1(x) = h2(x) = 0; for the other eight, y =\nh1(x) = h2(x) = 1. For each experiment, we randomly sample three sets of prompts and report the average performance on a set of 1,200 test examples, balanced between examples with h1(x) = 0/h2(x) = 1 and h1(x) = 1/h2(x) = 0. In this baseline setting, the instance template is t(x) = \u201cInput: $x Label: \u201d, and the label verbalizer provides no additional information about the task: v(0) = \u201c0\u201d and v(1) = \u201c1\u201d. Models. We focus on the TEXT-DAVINCI-002 and DAVINCI checkpoints of GPT-3 mainly because smaller-scale models often perform no better than random on tasks like NLI. The main differences between the two checkpoints are the pretraining data and the additional instruction tuning for TEXT-DAVINCI-002 (Ouyang et al., 2022). For all experiments, we use a temperature value of 0 in GPT-3 decoding, and all experiments involving the OpenAI API were conducted in January 2023. Metric. We report the h-accuracy for each feature. Higher h-accuracy indicates a higher preference for that feature. We denote the default feature h1 (sentiment, toxicity, entailment, answer) and the alternative features h2.\nh1(x) = h2(x) = 1. For each experiment, we randomly sample three sets of prompts and report the average performance on a set of 1,200 test examples, balanced between examples with h1(x) = 0/h2(x) = 1 and h1(x) = 1/h2(x) = 0. In this baseline setting, the instance template is t(x) = \u201cInput: $x Label: \u201d, and the label verbalizer provides no additional information about the task: v(0) = \u201c0\u201d and v(1) = \u201c1\u201d.\nMetric. We report the h-accuracy for each feature. Higher h-accuracy indicates a higher preference for that feature. We denote the default feature h1 (sentiment, toxicity, entailment, answer) and the alternative features h2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0aa9/0aa9e1e6-6b44-46d2-837f-ea115cf94346.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">O N (a) Sentiment analysis</div>\n<div style=\"text-align: center;\">(c) CivilComments</div>\nFigure 2: The generalization behavior of two LLMs given demonstration examples that could support both h1 (sentiment, entailment, toxicity, and question-answer) and an alternative feature h2. The y-axis is the accuracy on a balanced set of disambiguating examples, where h1(x) \u0338= h2(x), measuring accuracy using either h1(x) or h2(x) as the ground truth label. Both models generally show a strong preference for the sentiment feature. On the sentence-pair datasets (MultiNLI and BoolQ), the Davinci model tends to prefer the distractor feature, while the Text-Davinci-002 model, which was trained with human feedback, always prefers the original task hypothesis. Error bars represent the standard deviation over three random seeds, which correspond to three sets of randomly sampled demonstration examples as prompts.\n# 4.2 Results\nWe present ICL results on all the datasets and pairs of features in Figure 2, and note several interesting trends as follows:\nLLMs often have clear feature biases. For example, in the sentiment analysis setting (Figure 2a), both models generally show a strong preference to predict labels according to the sentiment of the sentence rather than other features, such as sentence length or the presence of individual lexical items like the word \u201cfood\u201d. Such biases can be helpful when they are aligned with the intended task. On the other hand, we do not observe clear feature preferences in the CivilComments dataset (Figure 2c), suggesting that these models may not have a strong feature biases in this setting.\n# The instruction-tuned model is generally more aligned with standard dataset labels. While\nboth DAVINCI and TEXT-DAVINCI-002 show similar feature biases in the sentiment analysis setting, they show very different biases on the the sentence pair datasets MultiNLI (Figure 2b) and BoolQ (Figure 2d): the DAVINCI model tends to prefer the shallow distractor features, such as lexical overlap,\n<div style=\"text-align: center;\">(d) BoolQ</div>\nwhile TEXT-DAVINCI-002 tends to prefer the semantic feature associated with the dataset\u2014either the entailment relationship or the answer to the question. This may be due to some aspect of instruction tuning, which might have exposed the model to similar tasks.\n# 5 Comparing Interventions\nOur findings that LLMs can have strong feature biases have important implications: when the LLMs\u2019 biases do not align with users\u2019 intended task, such biases would hurt performance. To resolve such misalignment, we explore a set of intervention methods designed to encourage the model to prefer one feature over another, examining whether the haccuracy for the intended feature indeed increases.\n# 5.1 Experiment Details\nWe now consider more variants of ICL that can be decomposed into four components that are commonly used in various prompting methods. In addition to the instance template t and label verbalizer v described in Section 2.2, we also introduce: (1) An instruction s \u2208V\u2217, which is prepended to the prompt; and (2) a free-form explanation\ne : X \u2192V\u2217after each input text t(x) and before the label v(x). The prompt c is then the concatenation of s, followed by t(xi); e(xi); v(yi) for all demonstration examples (xi, yi) \u2208D; and lastly the test instance t(xtest). Each intervention operates on a combination of the above components. We apply these interventions separately and compare with the baseline rather than applying all interventions on top of each other in order to analyze the impact of each of the methods. We compare interventions designed to steer the model towards h1 and h2 as the intended feature respectively, to understand the effectiveness of interventions towards different features.\n\u2022 Recall that in the baseline setting, there is no instruction or explanation (s and e are empty strings). We simply concatenate demonstration examples as the prompt, and use \u201c1\u201d and \u201c0\u201d as the verbalizer.\n In the semantic verbalizer setting, the verbalizer selects label words that are semantically related to a chosen feature in order to hint at the intended task. For example, if the intended feature is sentiment, then v(0) = \u201cnegative\u201d and v(1) = \u201cpositive\u201d; and if the intended feature is length, then v(0) = \u201cshort\u201d and v(1) = \u201clong\u201d. Our choice of verbalizers is inspired by prior work (Gao et al., 2021; Shi et al., 2022) and we list all of them in Table 6.\n In the instruction setting, we add a prefix string describing the intended feature and instructing the model to use this feature. We format our instructions mostly following prior work such as Natural Instructions (Mishra et al., 2021; Wang et al., 2022), and we list all our instructions in Tables 7 and 8.\n In the explanation setting, we append a template explanation after each demo example to explain why the prediction is made based on the intended feature, formatted in a similar manner as Chain-of-Thought prompting (Wei et al., 2022) and \u201cexplain-then-predict\u201d (Ye and Durrett, 2022). Since hand-written explanations are hard to obtain, we create fixed human-written templates for each feature value. For example, for the punctuation feature, all positive examples have the explanation \u201cThe review ends with an exclamation\nSteer towards h1\nSteer towards h2\nIntervention\nDavinci\nTD002\nDavinci\nTD002\nBaseline\n39.5\n59.1\n46.9\n30.5\nVerbalizer\n+11.9\n+7.1\n+15.6\n+24.4\nInstruction\n+1.6\n+12.2\n-2.4\n+24.2\nExplanation\n+14.4\n+6.9\n+14.3\n+33.8\nDisambig\n+12.9\n+9.4\n+18.6\n+21.1\nTable 2: The impact of different intervention strategies on h1-accuracy (left) or h2-accuracy (right), averaged over features and datasets. (The Steer towards h1 experiments exclude the sentiment analysis datasets, because the models already strongly prefer h1 even without interventions.) We report the change in accuracy relative to the baseline. Higher values indicate that the intervention is more effective at steering the model to predict labels according to the given feature. TD002: TEXT-DAVINCI-002.\nmark. Therefore, the answer is 1\u201d. We list all our template explanations in Tables 9 and 10.\n Finally, we include a disambiguation setting, in which we change half of the demonstration examples to those that disambiguate the task in favor of the intended feature. For example, to steer the model towards h1, the demonstration includes examples such that h1(x) \u0338= h2(x) and y = h1(x). Intuitively, this provides additional evidence for the model to differentiate the intended feature.\nWe measure the effectiveness of the intervention in terms of the increase in h-accuracy relative to the baseline model, where h is the intended feature.\n# 5.2 Results\nWhich interventions are effective? Table 2 summarizes the results of these experiments, comparing the effect of the different interventions on DAVINCI and TEXT-DAVINCI-002, averaged over features and datasets, and comparing between interventions designed to steer towards the default feature (h1) and the alternative features. Nearly all interventions increase the average h-accuracy, in many cases by a substantial amount. However, the effectiveness of the intervention varies depending on the model. For the DAVINCI model, the bestperforming interventions include semantic verbalizers and template-based explanations, while the worst-performing intervention is natural-language instructions. For the TEXT-DAVINCI-002 model, instructions are much more effective.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/56c5/56c5a526-38cc-456d-8847-2766dd2a3471.png\" style=\"width: 50%;\"></div>\nFigure 3: Generalization results of TEXT-DAVINCI-002 using different interventions, aggregated over feature pairs. In the first row, the intervention is designed to steer the model towards h1, so we expect an increase in the h1-accuracy (the blue bars) compared to the baselines (the first pair of bars in each plot). In the second row, the intervention is designed to steer the model towards h2, so we expect an increase in the h2-accuracy (red bars). The interventions are most successful when the model already has a feature bias for the intended feature (e.g. h1 for NLI and QA) or has low feature bias (Toxicity). They are less effective at overcoming prior feature biases. We omit results for steering towards h1 on sentiment analysis since the baseline already has a near-perfect preference for h1 with little room for improvement. Error bars represent the standard deviation over three random seeds, which correspond to three sets of randomly sampled demonstration examples as prompts.\nIn some cases, prompt-based interventions are more effective at steering the model than providing unambiguous demonstration examples. On one hand, this suggests that ICL can be effective even given highly underspecified data, but it also indicates that ICL models may fail to exploit the information provided in the demonstrations. This finding illustrates that ICL works very differently from standard supervised learning, and calls to mind existing empirical (Min et al., 2022) and theoretical results (Xie et al., 2022) suggesting that ICL might work in part by recognizing existing tasks rather than directly learning the input-output relation from the demonstration.\n# When are interventions effective?\ncompares the results of different interventions on the TEXT-DAVINCI-002 model, aggregated over features. (Detailed results for each feature and DAVINCI results are in Appendix A.1.) The effectiveness of the intervention varies depending on whether the prior feature bias and the intended feature are aligned. The interventions are most effective in two scenarios. First, interventions are effective when the model already has a feature bias for the intended features. This is evident in the interventions that steer the model towards h1 in\nNLI and QA, settings in which the model already has a feature bias in favor of h1. Second, interventions are effective when the model has a low feature bias. This is the case in the Toxicity Classification dataset, where the model does not exhibit a strong feature bias. In this setting, all interventions are moderately successful at steering the model towards h1, and more successful at steering the model towards h2. On the other hand, interventions are less effective at overriding feature biases. This trend is illustrated in the second row of Figure 3, in which the intervention is designed to steer the model towards h2 rather than the standard dataset label. While some interventions increase h2-accuracy, no intervention consistently gets the model to generalize according to the intended feature.\nWhich features are most susceptible to interventions? In Table 3, we compare the effect of interventions on different features in MultiNLI. Using meaningful label words works better on the genre features, where the label words are semantically similar to the input example, but it is more difficult to steer the model toward the use of features like length and lexical overlap, which are not related to the semantics of the sentences. More\nh2\nGovt/Fiction\nGovt/Telephone\nLength\nNegation\nOverlap\nDAVINCI\nBaseline\n87.9\n89.1\n52.3\n41.9\n69.5\nVerbalizer\n+3.0\n+1.2\n-2.0\n+10.3\n-6.6\nInstruction\n-2.2\n-10.2\n-1.1\n+6.1\n-6.3\nExplanation\n+5.2\n+6.3\n-1.2\n+8.2\n-19.5\nDisambig\n+2.3\n+7.8\n+14.4\n+12.7\n-0.6\nTEXT-DAVINCI-002\nBaseline\n28.7\n39.5\n27.3\n34.7\n39.2\nVerbalizer\n+66.8\n+58.2\n-4.5\n-0.9\n-3.2\nInstruction\n+42.0\n+41.3\n+6.8\n+28.7\n-3.8\nExplanation\n+47.7\n+51.8\n+26.5\n+9.1\n+3.1\nDisambig\n+37.3\n+45.9\n+12.8\n+0.7\n+12.7\nTable 3: Comparing the effectiveness of intervention strategies on steering the model towards different features in MultiNLI. This table reports the h2-accuracy in the baseline setting for each feature and the difference in h2-accuracy obtained by the different interventions. The interventions are generally more successful on simple semantic features like genre and less successful on semantically irrelevant features like lexical overlap and length.\nwork may be needed to develop interventions that work well for higher-order language features. Lastly, we compile a summary of practical takeaways for steering LLMs:\n\u2022 When using non-instruction-tuned LLMs (e.g., DAVINCI), specifying feature preferences as instructions is not effective, instead adding explanations or disambiguating examples can be more effective.\n\u2022 When using instruction-tuned LLMs (e.g., TEXT-DAVINCI-002), all interventions can be effective.\n\u2022 Features not related to semantics, such as sentence lengths or overlap, are difficult to intervene across all conditions.\n# 6 Related Work\nMeasuring inductive biases. Our work builds on existing research on measuring the inductive biases of learning algorithms in machine learning and NLP. Dasgupta et al. (2022) introduce a methodology for measuring feature bias as well as rulevs. exemplar-based generalization, and Chan et al. (2022) apply this approach to compare rule- vs. exemplar-based learning in ICL. We use a similar framing as Dasgupta et al. (2022), but focus on feature bias. In NLP, another line of work has studied the inductive biases of neural networks in the context of the poverty of the stimulus argument (Chomsky, 1980). These studies evaluate whether neural networks generalize in a manner consistent with structural or superficial linguistic rules (McCoy et al., 2018, 2020). Several studies\nhave found that models such as BERT acquire a preference for structural generalizations from largescale masked language model pretraining (Warstadt and Bowman, 2020; Warstadt et al., 2020; Zhang and Hashimoto, 2021; Mueller et al., 2022). We follow a similar poverty-of-the-stimulus experimental setup but focus on in-context learning and on features arising in common NLP tasks. Lovering et al. (2021) explore whether it is possible to predict the inductive biases of pre-trained models and show that models tend to generalize on the basis of features that are more \u201cextractable\u201d, measured using probing techniques (Voita and Titov, 2020), but it is not straightforward to extend the notion of extrability to in-context learning. Tamkin et al. (2023) also study how LLMs generalize to ambiguous classification tasks, but focus on ambiguous instructions and use template-generated datasets. Spurious correlations. A related line of work has explored the inductive biases of pretrained LMs in relation to spurious correlations, or shortcuts (e.g. Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019; Geirhos et al., 2020; Sagawa et al., 2020)\u2014shallow features that are correlated with the classification targets. Models can generalize successfully if they have an inductive bias that tends to favor the intended feature over the shortcut. Hendrycks et al. (2019, 2020); Tu et al. (2020) found that pre-trained models can generalize more successfully to such distribution shifts.\n# Explaining in-context learning. A number of\nempirical studies have attempted to characterize the behavior of ICL and explain why it works. These studies have found that ICL can be overly sensitive\nto certain aspects of the prompt, such as the order of the demonstration examples (Lu et al., 2022) and choice of label words (Zhao et al., 2021), but also surprisingly insensitive to others. In particular, Min et al. (2022) show that LLMs can largely ignore the relationship between inputs and labels in the demonstration example and Webson and Pavlick (2022) show that the performance of ICL can perform well on NLI even if the prompt is irrelevant or misleading. Relatedly, Wei et al. (2023); Pan et al. (2023) show that LLMs can perform ICL well with flipped labels or semantically-unrelated labels, but such ability of overriding semantic priors emerges with scale. Some theoretical work has also attempted to explain why prompt-based methods work, by drawing connections between the prompting setting and properties of the pretraining distribution (Saunshi et al., 2021; Wei et al., 2021; Xie et al., 2022) or by arguing that Transformers can act as meta-learners, implicitly performing gradient descent on the in-context examples (von Oswald et al., 2022; Aky\u00fcrek et al., 2023). Our results provide empirical evidence that there is a strong task bias from pretraining when the LLMs must infer the task by input-output relations. Improving in-context learning. Recent work studied the effect of including explanations in the prompt to produce better quality answers (Wei et al., 2022; Lampinen et al., 2022). While they show the benefit of high-quality human-annotated explanations for improving task performance, we demonstrated the effectiveness of simple template explanations in steering feature biases. Another line of work collects large pools of instructions from diverse tasks and uses them to tune and control language models (Wang et al., 2022; Chung et al., 2022). We also adopt instructions as an intervention method and show that it works particularly well on instruction-tuned models. In a similar manner, Si et al. (2023) studied prompting methods to make GPT-3 more reliable, such as instructing it to not rely on demographic biases.\n# 7 Conclusion\nIn this work, we constructed underspecified prompts from real-world datasets to study feature biases of large language models. We found that the instruction-tuned InstructGPT model prefers the \u201cdefault\u201d task features over distractor features more often than the base GPT-3 model, and we demonstrated the effectiveness of various intervention\nmethods in steering models to use the specified feature. These results not only shed new insights into the working mechanisms of ICL, but also have practical takeaways for discouraging models from exploiting unintended features such as demographic biases or shallow statistical cues.\n# Limitations\nModel coverage. Our study is targeted specifically at GPT-3 and it would be interesting to study feature bias patterns on other large language models such as OPT (Zhang et al., 2022) and BLOOM (Scao et al., 2022); and it is possible that our intervention methods may have different effects on these language models trained with different data sources and scales.\nTask coverage. Apart from model coverage, our analysis is focused on only four common binary classification tasks. Our main metric, h-accuracy, compares the predictions between a learned function f and a feature function h. For simplicity, we only study binary functions (consistent with prior work) to illustrate the main ideas, but the framework applies equally well if f and h are multi-class classifiers. For example, in the case of the threeway NLI task, we might set h1 to predict on the basis of entailment / contradiction / neutral, and h2 to predict on the basis of the genres \u2013 e.g. fiction / government / telephone. Future work could extend our framework to more tasks, and consider how to apply it to more complex tasks such as generation. Feature coverage. Our current experiments are limited to a set of hand-crafted features. One potential way to systematically scale our approach is to develop novel techniques for automatic feature discovery, for example, to cluster the data and treat each cluster as having a distinct feature.\nExplaining feature biases. While our empirical findings shed light on the feature bias patterns of GPT-3, we do not yet have a conclusion on how these biases arise from pretraining. Future work could attempt to draw connections to the pretraining data or to theoretical accounts of in-context learning.\nRisks and ethics. While we do not foresee any ethical risks resulting from our work, we caution against making extrapolations about the extent to which LLMs exhibit feature biases towards protected social attributes. Although we do not find\nevidence of strong feature biases in a particular toxicity classification setting, care should be taken to evaluate the fairness and reliability of these models directly before they are deployed in downstream applications.\n# Acknowledgement\nWe thank Alex Tamkin, Xi Ye, Sewon Min, and Jordan Boyd-Graber for their helpful feedback. This research is partially funded by the National Science Foundation (IIS-2211779), a Sloan research fellowship, Samsung Advanced Institute of Tech- nology (under the project Next Generation Deep Learning: From Pattern Recognition to AI) and AWS AI. NJ is supported by an NSF Graduate Research Fellowship under grant number 1839302.\n# References\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2019. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion Proceedings of the 2019 World Wide Web Conference.\nNoam Chomsky. 1980. Rules and representations. Behavioral and brain sciences, 3(1):1\u201315.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddharth Deepak Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hanna Hajishirzi, and Daniel Khashabi. 2022. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Empirical Methods in Natural Language Processing (EMNLP).\nAlex Warstadt and Samuel R. Bowman. 2020. Can neural networks acquire a structural bias from raw linguistic data? In CogSci.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, and Samuel Bowman. 2020. Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually). In Empirical Methods in Natural Language Processing (EMNLP), pages 217\u2013235.\n# A Appendix\n# A.1 Intervention Results Across Features\nWe present the complete set of intervention results broken down by features in Table 4 for DAVINCI and Table 5 for TEXT-DAVINCI-002. It is worth noting that the intervention methods\u2019 effectiveness often varies across features even on the same dataset. For example, all intervention methods can effectively steer models towards using the genre feature over the entailment feature on MNLI, but the success is limited for the lexical overlap feature on MNLI. We hypothesize this is because features like lexical overlap are harder for models to recognize.\n# A.2 List of Semantic Verbalizers\nWe present the full list of semantically meaningful verbalizers for the intervention experiments in Table 6.\n# A.3 List of Task Instructions\nWe present the full list of task instructions for the intervention experiments in Table 7 and Table 8.\n# A.4 List of Template Explanations\nWe present the full list of template explanations for the intervention experiments in Table 9 and Table 10.\nh2\nBaseline\n+ DisAmbig\n+ Verbalizers\n+ Instruction\n+ Explanation\nSteer Towards h1\nNLI\ngenre\n11.5 / 88.5\n53.4 / 46.6\n28.5 / 71.5\n13.5 / 86.5\n20.9 / 79.1\nlength\n47.7 / 52.3\n54.5 / 45.5\n53.3 / 46.7\n49.3 / 50.7\n53.4 / 46.6\nnegation\n58.1 / 41.9\n49.7 / 50.3\n52.6 / 47.4\n51.8 / 48.2\n45.4 / 54.6\noverlap\n30.5 / 69.5\n54.9 / 45.1\n29.4 / 70.6\n37.9 / 62.1\n44.5 / 55.5\nAggregate\n31.9 / 68.1\n53.2 / 46.8\n38.4 / 61.6\n33.2 / 66.8\n37.0 / 63.0\nTC\ngender\n47.9 / 52.1\n50.2 / 49.8\n53.1 / 46.9\n47.8 / 52.2\n58.6 / 41.4\nrace\n53.0 / 47.0\n51.1 / 48.9\n58.0 / 42.0\n52.2 / 47.8\n59.5 / 40.5\nreligion\n51.4 / 48.6\n50.7 / 49.3\n53.4 / 46.6\n51.9 / 48.1\n55.1 / 44.9\nlength\n36.6 / 63.4\n53.6 / 46.4\n33.8 / 66.2\n37.8 / 62.2\n62.6 / 37.4\ncapitalization\n32.4 / 67.6\n51.2 / 48.8\n51.5 / 48.5\n30.0 / 70.0\n60.3 / 39.7\nAggregate\n45.8 / 54.2\n51.1 / 48.9\n50.9 / 49.1\n45.6 / 54.4\n58.6 / 41.4\nQA\nQ word\n39.8 / 60.2\n63.5 / 36.5\n76.1 / 23.9\n45.1 / 54.9\n67.2 / 32.8\noverlap\n41.1 / 58.9\n55.1 / 44.9\n73.1 / 26.9\n44.9 / 55.1\n64.3 / 35.7\nstructure\n34.0 / 66.0\n49.3 / 50.7\n61.7 / 38.3\n37.4 / 62.6\n71.0 / 29.0\nlength\n37.8 / 62.2\n48.4 / 51.6\n64.2 / 35.8\n46.2 / 53.8\n64.8 / 35.2\nAggregate\n38.2 / 61.8\n54.1 / 45.9\n68.8 / 31.2\n43.4 / 56.6\n66.8 / 33.2\nSteer Towards h2\nNLI\ngenre\n11.5 / 88.5\n6.5 / 93.5\n9.4 / 90.6\n17.7 / 82.3\n5.7 / 94.3\nlength\n47.7 / 52.3\n33.3 / 66.7\n49.8 / 50.2\n48.8 / 51.2\n48.8 / 51.2\nnegation\n58.1 / 41.9\n45.4 / 54.6\n47.8 / 52.2\n52.0 / 48.0\n49.8 / 50.2\noverlap\n30.5 / 69.5\n31.1 / 68.9\n37.1 / 62.9\n36.7 / 63.3\n50.0 / 50.0\nAggregate\n31.9 / 68.1\n24.6 / 75.4\n30.7 / 69.3\n34.5 / 65.5\n32.0 / 68.0\nSA\npunctuation\n98.3 / 1.7\n73.1 / 26.9\n97.0 / 3.0\n98.0 / 2.0\n68.6 / 31.4\ndomain\n56.1 / 43.9\n0.3 / 99.7\n1.0 / 99.0\n77.6 / 22.4\n25.8 / 74.2\nlength\n98.4 / 1.6\n35.2 / 64.8\n30.7 / 69.3\n97.8 / 2.2\n62.1 / 37.9\nlexicon\n95.5 / 4.5\n63.2 / 36.8\n87.8 / 12.2\n96.5 / 3.5\n72.0 / 28.0\ncapitalization\n92.0 / 8.0\n43.5 / 56.5\n85.5 / 14.5\n81.5 / 18.5\n75.2 / 24.8\nAggregate\n89.3 / 10.7\n46.4 / 53.6\n65.0 / 35.0\n91.3 / 8.7\n62.6 / 37.4\nTC\ngender\n47.9 / 52.1\n41.3 / 58.7\n29.8 / 70.2\n48.3 / 51.7\n28.8 / 71.2\nrace\n53.0 / 47.0\n38.4 / 61.6\n26.2 / 73.8\n50.9 / 49.1\n27.1 / 72.9\nreligion\n51.4 / 48.6\n34.6 / 65.4\n16.8 / 83.2\n51.1 / 48.9\n9.8 / 90.2\nlength\n36.6 / 63.4\n29.7 / 70.3\n22.7 / 77.3\n40.4 / 59.6\n31.2 / 68.8\ncapitalization\n32.4 / 67.6\n18.1 / 81.9\n48.8 / 51.2\n31.6 / 68.4\n43.0 / 57.0\nAggregate\n45.8 / 54.2\n34.0 / 66.0\n27.3 / 72.7\n46.1 / 53.9\n25.5 / 74.5\nQA\nQ word\n39.8 / 60.2\n31.1 / 68.9\n11.5 / 88.5\n43.1 / 56.9\n1.7 / 98.3\noverlap\n41.1 / 58.9\n28.9 / 71.1\n50.0 / 50.0\n45.5 / 54.5\n49.9 / 50.1\nstructure\n34.0 / 66.0\n16.3 / 83.7\n55.5 / 44.5\n39.3 / 60.7\n48.3 / 51.7\nlength\n37.8 / 62.2\n43.4 / 56.6\n37.0 / 63.0\n47.7 / 52.3\n34.3 / 65.7\nAggregate\n38.2 / 61.8\n29.9 / 70.1\n38.5 / 61.5\n43.9 / 56.1\n33.6 / 66.4\nTable 4: The impact of different intervention strategies (applied separately on top of the baseline). This table is for DAVINCI. We report the ambiguous accuracy for supporting h1 and h2 respectively in each cell, higher h1 accuracy indicates a preference for the h1 hypothesis. Most interventions successfully steer the model preference in the intended direction. We omit results for steering towards h1 on sentiment analysis since both models already have a strong preference for the sentiment feature without any intervention. We average and condense results of features from the same category (such as gender and sexuality on CivilComments, and different lexicon features on sentiment analysis) since their trends are largely similar.\nh2\nBaseline\n+ DisAmbig\n+ Verbalizers\n+ Instruction\n+ Explanation\nSteer Towards h1\nNLI\ngenre\n65.9 / 34.1\n79.2 / 20.8\n74.9 / 25.1\n86.1 / 13.9\n75.9 / 24.1\nlength\n72.7 / 27.3\n83.6 / 16.4\n73.4 / 26.6\n81.6 / 18.4\n77.8 / 22.2\nnegation\n65.3 / 34.7\n72.5 / 27.5\n77.3 / 22.7\n80.1 / 19.9\n77.8 / 22.2\noverlap\n60.8 / 39.2\n73.1 / 26.9\n64.6 / 35.4\n70.9 / 29.1\n66.4 / 33.6\nAggregate\n66.1 / 33.9\n77.5 / 22.5\n73.0 / 27.0\n81.0 / 19.0\n74.8 / 25.2\nTC\ngender\n45.9 / 54.1\n55.2 / 44.8\n53.0 / 47.0\n58.4 / 41.6\n60.4 / 39.6\nrace\n50.9 / 49.1\n59.7 / 40.3\n55.8 / 44.2\n60.8 / 39.2\n60.7 / 39.3\nreligion\n42.0 / 58.0\n56.8 / 43.2\n49.5 / 50.5\n56.5 / 43.5\n57.7 / 42.3\nlength\n45.9 / 54.1\n60.8 / 39.2\n53.3 / 46.7\n66.8 / 33.2\n62.7 / 37.3\ncapitalization\n48.8 / 51.2\n58.1 / 41.9\n55.3 / 44.7\n63.8 / 36.2\n64.9 / 35.1\nAggregate\n45.9 / 54.1\n57.5 / 42.5\n52.8 / 47.2\n60.1 / 39.9\n60.6 / 39.4\nQA\nQ word\n79.7 / 20.3\n77.2 / 22.8\n85.2 / 14.8\n84.3 / 15.7\n86.1 / 13.9\noverlap\n71.3 / 28.7\n77.6 / 22.4\n81.1 / 18.9\n76.7 / 23.3\n81.6 / 18.4\nstructure\n68.0 / 32.0\n72.3 / 27.7\n78.8 / 21.2\n71.6 / 28.4\n74.9 / 25.1\nlength\n74.6 / 25.4\n78.7 / 21.3\n80.4 / 19.6\n82.6 / 17.4\n85.7 / 14.3\nAggregate\n73.4 / 26.6\n76.5 / 23.5\n81.4 / 18.6\n78.8 / 21.2\n82.0 / 18.0\nSteer Towards h2\nNLI\ngenre\n65.9 / 34.1\n24.3 / 75.7\n3.4 / 96.6\n24.2 / 75.8\n16.1 / 83.9\nlength\n72.7 / 27.3\n59.9 / 40.1\n77.2 / 22.8\n65.9 / 34.1\n46.2 / 53.8\nnegation\n65.3 / 34.7\n64.6 / 35.4\n66.2 / 33.8\n36.6 / 63.4\n56.2 / 43.8\noverlap\n60.8 / 39.2\n48.1 / 51.9\n63.9 / 36.1\n64.6 / 35.4\n57.7 / 42.3\nAggregate\n66.1 / 33.9\n44.2 / 55.8\n42.8 / 57.2\n43.1 / 56.9\n38.4 / 61.5\nSA\npunctuation\n99.1 / 0.9\n98.0 / 2.0\n96.6 / 3.4\n85.4 / 15.6\n50.5 / 49.5\ndomain\n92.4 / 7.6\n0.7 / 99.3\n0.5 / 99.5\n1.1 / 98.9\n26.4 / 73.6\nlength\n98.6 / 1.4\n76.1 / 23.9\n27.7 / 72.3\n81.1 / 18.9\n42.6 / 57.4\nlexicon\n97.8 / 2.2\n65.7 / 34.3\n87.6 / 22.4\n68.6 / 31.4\n67.4 / 32.6\ncapitalization\n98.4 / 1.6\n56.2 / 43.8\n87.2 / 12.8\n61.7 / 38.3\n56.0 / 44.0\nAggregate\n97.3 / 2.7\n60.4 / 39.6\n64.5 / 35.5\n61.1 / 38.9\n51.7 / 48.3\nTC\ngender\n45.9 / 54.1\n37.9 / 62.1\n41.0 / 59.0\n22.9 / 77.1\n12.1 / 87.9\nrace\n50.9 / 49.1\n42.7 / 57.3\n36.0 / 64.0\n33.1 / 66.9\n7.4 / 92.6\nreligion\n42.0 / 58.0\n21.8 / 78.2\n7.7 / 92.3\n9.4 / 90.6\n5.7 / 94.3\nlength\n45.9 / 54.1\n31.0 / 69.0\n20.3 / 79.7\n19.6 / 80.4\n3.6 / 96.4\ncapitalization\n48.8 / 51.2\n28.9 / 71.1\n41.3 / 58.7\n43.7 / 56.3\n48.0 / 52.0\nAggregate\n45.9 / 54.1\n31.7 / 68.3\n27.8 / 72.2\n23.0 / 77.0\n13.5 / 86.5\nQA\nQ word\n79.7 / 20.3\n68.9 / 31.1\n0.1 / 99.9\n83.5 / 16.5\n39.4 / 60.6\noverlap\n71.3 / 28.7\n66.1 / 33.9\n78.5 / 21.5\n69.3 / 30.7\n64.5 / 35.5\nstructure\n68.0 / 32.0\n62.0 / 38.0\n62.5 / 37.5\n48.9 / 51.1\n33.9 / 66.1\nlength\n74.6 / 25.4\n62.1 / 37.9\n36.0 / 64.0\n51.5 / 48.5\n49.9 / 50.1\nAggregate\n73.4 / 26.6\n64.8 / 35.2\n44.3 / 55.7\n63.3 / 36.7\n46.9 / 53.1\nTable 5: The impact of different intervention strategies (applied separately on top of the baseline). This table is for TEXT-DAVINCI-002. We report the ambiguous accuracy for supporting h1 and h2 respectively in each cell, higher h1 accuracy indicates a preference for the h1 hypothesis. Most interventions successfully steer the model preference in the intended direction. We omit results for steering towards h1 on sentiment analysis since both models already have a strong preference for the sentiment feature without any intervention. We average and condense results of features from the same category (such as gender and sexuality on CivilComments, and different lexicon features on sentiment analysis) since their trends are largely similar.\nTask\nFeature\nVerbalizer\nSentiment analysis\nSentiment\n1- \u201cpositive\u201d, 0 - \u201cnegative\u201d\nDomain\n\u201cSource?\u201d 1 - \u201cmovie\u201d, 0 - \u201cother\u201d\nLength\n\u201cLength?\u201d 1 - \u201cshort\u201d, 0 - \u201clong\u201d\nTerminal punctuation\n\u201cEnd punctuation?\u201d 1 - \u201cother\u201d, 0 - \u201cperiod\u201d\nContains word\n\u201cHas food / nice?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nCapitalization\n\u201cUppercase words?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nToxicity classification\nToxicity\n\u201cToxic?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nGender\n\u201cGender?\u201d 1 - \u201cfemale\u201d, 0 - \u201cmale\u201d\nSexuality\n\u201cLGBTQ?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nReligion\n\u201cReligion?\u201d 1 - \u201cMuslim\u201d, 0 - \u201cChristian\u201d / \u201cJewish\u201d\nRace\n\u201cRace?\u201d 1 - \u201cblack\u201d, 0 - \u201cwhite\u201d\nLength\n\u201cLength?\u201d 1 - \u201cshort\u201d, 0 - \u201clong\u201d\nCapitalization\n\u201cUppercase words?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nNatural language inference\nEntailment\n\u201cEntailed?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nDomain\n\u201cSource?\u201d 1 - \u201cgovernment\u201d, 0 - \u201cfiction\u201d / \u201ctelephone\u201d\nLexical overlap\n\u201cOverlap?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nHypothesis length\n\u201cShorter?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nHypothesis negation\n\u201cNegation?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nQuestion answering\nAnswer\n1 - \u201cyes\u201d, 0 - \u201cno\u201d\nQuestion word\n\u201cQuestion word?\u201d 1 - \u201cbe\u201d, 0 - \u201cdo\u201d\nLexical overlap\n\u201cOverlap?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nQuestion structure\n\u201cHas same as?\u201d 1 - \u201cyes\u201d, 0 - \u201cno\u201d\nPassage length\n\u201cLength?\u201d 1 - \u201clong\u201d, 0 - \u201cshort\u201d\nTable 6: All verbalizers we used in our experiments. For some hypotheses, it is difficult to use succinct word to specify the label (e.g., whether it contains negation words), we thus prepend a short question before the lab words for such cases to better specify the task (e.g., \u201cNegation? yes / no\u201d).\nTask\nFeature\nInstruction\nSentiment Analysis\nSentiment\nGiven a review, you need to predict whether the review is good (positive)\nor bad (negative). Please answer with \u201c1\u201d for positive and \u201c0\u201d for negative.\nDomain\nGiven a review, you need to predict whether it comes from IMDB movie\nreviews or Yelp restaurant and service reviews. Please answer with \u201c1\u201d for\nIMDB and \u201c0\u201d for Yelp.\nLength\nGiven a review, you need to predict whether it is longer than 40 words.\nPlease answer with \u201c1\u201d for shorter than 40 words and \u201c0\u201d for longer.\nTerminal punctuation\nGiven a review, you need to predict whether the review ends with an excla-\nmation mark (\u2018!\u2019) or period (\u2018.\u2019). Please answer with \u201c1\u201d for exclamation\nmark and \u201c0\u201d for period.\nContains word\nGiven a review, you need to predict whether the review mentions the word\n\u201cnice\u201d/\u201cfood\u201d. Please answer with \u201c1\u201d for mentioning \u201cnice\u201d/\u201cfood\u201d and\n\u201c0\u201d for not mentioning.\nCapitalization\nGiven a review, you need to predict whether the review contains any\nuppercase words where all letters are uppercase (like \u2018THIS\u2019). Please\nanswer with \u201c1\u201d for having uppercase words and \u201c0\u201d for not.\nToxicity classification\nToxicity\nCategorize the comment on the basis of toxicity. If the comment is toxic\n(harmful, malicious, derogatory, threat, insult, identity attack, etc.) output\n\u201c1\u201d, otherwise output \u201c0\u201d.\nGender\nGiven a comment, you need to predict whether the comment targets males\nor females. Please answer with \u201c1\u201d for female and \u201c0\u201d for male.\nSexuality\nGiven a comment, you need to predict whether the comment targets\nLGBTQ people. Please answer with \u201c1\u201d if it does and \u201c0\u201d if not.\nReligion\nGiven a comment, you need to predict whether the comment targets Mus-\nlim or Christian/Jewish people. Please answer with \u201c1\u201d for Muslim and \u201c0\u201d\nfor Christian/Jewish.\nRace\nGiven a comment, you need to predict whether the comment targets black\nor white people. Please answer with \u201c1\u201d for black people and \u201c0\u201d for white\npeople.\nLength\nGiven a comment, you need to predict whether the comment is longer than\n40 words. Please answer with \u201c1\u201d for shorter and \u201c0\u201d for longer.\nCapitalization\nGiven a comment, you need to predict whether the comment contains any\nuppercase words where all letters are uppercased (like \u2018THIS\u2019). Please\nanswer with \u201c1\u201d for having uppercase words and \u201c0\u201d for not.\nTask\nFeature\nInstruction\nNatural language inference\nEntailment\nIn this task, you will be presented with a premise sentence (the first\nsentence) and a hypothesis sentence (the second sentence). Determine\nwhether the premise sentence entails (implies) or does not entail the\nhypothesis sentence. Please answer with \u201c1\u201d for entailment and \u201c0\u201d\nfor non-entailment.\nDomain\nIn this task, you will be presented with a premise sentence (the first\nsentence) and a hypothesis sentence (the second sentence). Determine\nwhether they come from government files or fiction/telephone. Please\nanswer with \u201c1\u201d for government and \u201c0\u201d for fiction\nLexical overlap\nIn this task, you will be presented with a premise sentence (the first\nsentence) and a hypothesis sentence (the second sentence). Deter-\nmine whether all words in the second sentence also appear in the first\nsentence. If so, answer \u201c1\u201d; if not, answer \u201c0\u201d.\nHypothesis length\nIn this task, you will be presented with a premise sentence (the first\nsentence) and a hypothesis sentence (the second sentence). Determine\nwhether the second sentence is shorter than the first sentence. Please\nanswer with \u201c1\u201d for shorter and \u201c0\u201d for longer.\nHypothesis negation\nIn this task, you will be presented with a premise sentence (the first\nsentence) and a hypothesis sentence (the second sentence). Determine\nwhether there are any negation words in the second sentence (\u201cnot\u201d,\n\u201cno\u201d, \u201cn\u2019t\u201d). Please answer with \u201c1\u201d for not having negations and \u201c0\u201d\nfor having negations.\nQuestion answering\nAnswer\nBased on the information present in the given passage, decide whether\nthe answer to the given question is yes or no. Please answer with \u201c1\u201d\nfor yes and \u201c0\u201d for no.\nQuestion word\nGiven the passage and question, determine whether the question word\nis \u201cis/was\u201d or \u201cdo/does/did\u201d. Please answer with \u201c1\u201d for \u201cis/was\u201d and\n\u201c0\u201d for \u201cdo/does/did\u201d.\nLexical overlap\nGiven the passage and question, determine whether all words in the\nquestion also appear in the passage. If so, answer \u201c1\u201d; if not, answer\n\u201c0\u201d.\nQuestion structure\nGiven the passage and question, determine whether the question con-\ntains the phrase \u201csame as\u201d. Please answer with \u201c1\u201d for having \u201csame\nas\u201d and \u201c0\u201d if not.\nPassage length\nGiven the passage and question, determine whether the passage is\nlonger than 50 words. Please answer with \u201c1\u201d for longer than 50\nwords and \u201c0\u201d for shorter.\nTable 8: All instructions used in our experiments. We prepend the corresponding instruction of each hypothesis t the prompt. This table is for natural language inference and question answering.\nTask\nFeature\nExplanation\nSentiment Analysis\nSentiment (1)\nThe review has a positive sentiment. Therefore, the answer is 1.\nSentiment (0)\nThe review has a negative sentiment. Therefore, the answer is 0.\nDomain (1)\nThe review is from IMDB movie reviews. Therefore, the answer is 1.\nDomain (0)\nThe review is from Yelp reviews. Therefore, the answer is 0.\nLength (1)\nThe review is shorter than 40 words. Therefore, the answer is 1.\nLength (0)\nThe review is longer than 40 words. Therefore, the answer is 0.\nTerminal punctuation (1)\nThe review ends with an exclamation mark (\u2018!\u2019). Therefore, the answer\nis 1.\nTerminal punctuation (0)\nThe review ends with a period (\u2019.\u2019). Therefore, the answer is 0.\nContains word (1)\nThe review contains the word \u2018food\u2019/\u2018nice\u2019. Therefore, the answer is 1.\nContains word (0)\nThe review does not contain the word \u2018food\u2019/\u2018nice\u2019. Therefore, the\nanswer is 0.\nCapitalization (1)\nThe review contains an uppercase word with all uppercase letters.\nTherefore, the answer is 1.\nCapitalization (0)\nThe review does not contain an uppercase word with all uppercase\nletters. Therefore, the answer is 0.\nToxicity classification\nToxicity (1)\nThe comment is toxic. Therefore, the answer is 1.\nToxicity (0)\nThe comment is not toxic. Therefore, the answer is 0.\nGender (1)\nThe comment mentions females. Therefore, the answer is 1.\nGender (0)\nThe comment mentions males. Therefore, the answer is 0.\nSexuality (1)\nThe comment mentions LGBTQ. Therefore, the answer is 1.\nSexuality (0)\nThe comment does not mention LGBTQ. Therefore, the answer is 0.\nReligion (1)\nThe comment mentions Muslim people. Therefore, the answer is 1.\nReligion (0)\nThe comment mentions Christian/Jewish people. Therefore, the answer\nis 0.\nRace (1)\nThe comment mentions black people. Therefore, the answer is 1.\nRace (0)\nThe comment mentions white people. Therefore, the answer is 0.\nLength (1)\nThe comment is shorter than 40 words. Therefore, the answer is 1.\nLength (0)\nThe comment is longer than 40 words. Therefore, the answer is 0.\nCapitalization (1)\nThe comment contains an uppercase word with all uppercase letters.\nTherefore, the answer is 1.\nCapitalization (0)\nThe comment contains an uppercase word with all uppercase letters.\nTherefore, the answer is 0.\nTable 9: All template explanations used in our experiments. The explanation is appended after each input text and before the label for all demonstration examples. Such explanation would be induced during test inference as well We manually write a template explanation for each class of each hypothesis. This table is for sentiment analysis and toxicity classification.\nTask\nFeature\nExplanation\nNatural language inference\nEntailment (1)\nThe first sentence entails the second sentence. Therefore, the\nanswer is 1.\nEntailment (0)\nThe first sentence does not entail the second sentence. Therefore,\nthe answer is 0.\nDomain (1)\nThe text is from government files. Therefore, the answer is 1.\nDomain (0)\nThe text is from fiction / telephone recordings. Therefore, the\nanswer is 0.\nLexical overlap (1)\nAll words from the second sentence also appear in the first sentence.\nTherefore, the answer is 1.\nLexical overlap (0)\nNot all words from the second sentence also appear in the first\nsentence. Therefore, the answer is 0.\nHypothesis length (1)\nThe second sentence is shorter than the first sentence. Therefore,\nthe answer is 1.\nHypothesis length (0)\nThe second sentence is longer than the first sentence. Therefore,\nthe answer is 0.\nHypothesis negation (1)\nThe second sentence contains negation words. Therefore, the\nanswer is 1.\nHypothesis negation (0)\nThe second sentence does not contain negation words. Therefore,\nthe answer is 0.\nQuestion answering\nAnswer (1)\nThe answer to the question is yes. Therefore, the answer is 1.\nAnswer (0)\nThe answer to the question is no. Therefore, the answer is 0.\nQuestion word (1)\nThe question word is \u2018is\u2019 or \u2018was\u2019. Therefore, the answer is 1.\nQuestion word (0)\nThe question word is \u2018do\u2019 or \u2018does\u2019 or \u2018did\u2019. Therefore, the answer\nis 0.\nLexical overlap (1)\nAll words from the question also appear in the passage. Therefore,\nthe answer is 1.\nLexical overlap (0)\nNot all words from the question also appear in the passage. There-\nfore, the answer is 0.\nQuestion structure (1)\nThe question contains the phrase \u2018same as\u2019. Therefore, the answer\nis 1.\nQuestion structure (0)\nThe question does not contain the phrase \u2018same as\u2019. Therefore, the\nanswer is 0.\nPassage length (1)\nThe passage is longer than 50 words. Therefore, the answer is 1.\nPassage length (0)\nThe passage is shorter than 50 words. Therefore, the answer is 0.\nTable 10: All template explanations used in our experiments. The explanation is appended after each input text and before the label for all demonstration examples. Such explanation would be induced during test inference as well. We manually write a template explanation for each class of each hypothesis. This table is for natural language inference and question answering.\n",
    "paper_type": "theory",
    "attri": {
        "background": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. This paper investigates the inductive biases of ICL from the perspective of feature bias, focusing on how LLMs utilize features in underspecified demonstrations.",
        "problem": {
            "definition": "The problem of understanding which features LLMs are likely to use when presented with underspecified demonstrations where two features are equally predictive.",
            "key obstacle": "The main challenge is the limited context length of transformer models, which restricts the number of demonstration examples that can be incorporated."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to understand the feature biases of LLMs in ICL settings.",
            "opinion": "The authors believe that understanding feature biases can help in designing better interventions to guide LLMs towards the intended task.",
            "innovation": "The primary improvement compared to previous methods is the systematic evaluation of feature biases and the effectiveness of various interventions in steering the model's preferences."
        },
        "Theory": {
            "perspective": "The perspective of the theory revolves around feature biases in learning algorithms and how they influence the generalization behavior of LLMs in ICL.",
            "opinion": "The authors assume that LLMs exhibit inherent biases based on their training data and architecture, which can affect task performance.",
            "proof": "The paper provides empirical evidence through experiments that measure the feature biases of LLMs using underspecified demonstrations."
        },
        "experiments": {
            "evaluation setting": "The evaluation includes various NLP tasks with datasets such as IMDb, Yelp, CivilComments, MultiNLI, and BoolQ, focusing on feature pairs for classification.",
            "evaluation method": "The evaluation involves prompting LLMs with balanced sets of demonstrations and measuring their accuracy on disambiguating datasets."
        },
        "conclusion": "The paper concludes that instruction-tuned models exhibit stronger preferences for intended features compared to base models, and various interventions can effectively steer LLMs towards desired feature biases, though the success of these interventions varies.",
        "discussion": {
            "advantage": "The advantages of this paper include providing insights into the feature biases of LLMs and demonstrating effective intervention strategies.",
            "limitation": "A limitation is that the study is focused on GPT-3, and the findings may not generalize to other models or tasks.",
            "future work": "Future work could explore feature biases in other large language models and extend the analysis to more complex tasks and features."
        },
        "other info": [
            {
                "info1": "The study emphasizes the importance of understanding the inductive biases in the context of real-world applications of LLMs."
            },
            {
                "info2": {
                    "info2.1": "The paper acknowledges the potential for biases in LLMs to misalign with user intentions.",
                    "info2.2": "It suggests that simple prompting methods may not be sufficient to align models with user intentions when biases conflict."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, focusing on how LLMs utilize features in underspecified demonstrations."
        },
        {
            "section number": "1.2",
            "key information": "The paper emphasizes the importance of understanding the inductive biases in the context of real-world applications of LLMs."
        },
        {
            "section number": "1.3",
            "key information": "The authors assume that LLMs exhibit inherent biases based on their training data and architecture, which can affect task performance."
        },
        {
            "section number": "3.2",
            "key information": "The perspective of the theory revolves around feature biases in learning algorithms and how they influence the generalization behavior of LLMs in ICL."
        },
        {
            "section number": "3.3",
            "key information": "The primary improvement compared to previous methods is the systematic evaluation of feature biases and the effectiveness of various interventions in steering the model's preferences."
        },
        {
            "section number": "6.1",
            "key information": "The study acknowledges the potential for biases in LLMs to misalign with user intentions and suggests that simple prompting methods may not be sufficient to align models with user intentions when biases conflict."
        },
        {
            "section number": "6.4",
            "key information": "A limitation is that the study is focused on GPT-3, and the findings may not generalize to other models or tasks."
        },
        {
            "section number": "7",
            "key information": "Future work could explore feature biases in other large language models and extend the analysis to more complex tasks and features."
        }
    ],
    "similarity_score": 0.7126941832572695,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations.json"
}