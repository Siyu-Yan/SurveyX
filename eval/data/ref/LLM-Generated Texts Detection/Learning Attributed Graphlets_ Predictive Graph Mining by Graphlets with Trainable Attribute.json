{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.06932",
    "title": "Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute",
    "abstract": "The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naive implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search. Our pruning strategy can ensure that the quality of the solution is maintained compared to the result without pruning. We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including graph neural networks, while using only a small number of AGs in an interpretable manner.",
    "bib_name": "shinji2024learningattributedgraphletspredictive",
    "md_text": "# Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute\n# Tajima Shinji1, Ren Sugihara1, Ryota Kitahara1, and Masayuki Karasuyama\u22171\n1Nagoya Institute of Technology \u2217karasuyama@nitech.ac.jp\n1Nagoya Institute of Technology \u2217karasuyama@nitech.ac.jp\n1Nagoya Institute of Technology \u2217karasuyama@nitech.ac.jp\nAbstract\nThe graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a na\u00a8\u0131ve implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search. Our pruning strategy can ensure that the quality of the solution is maintained compared to the result without pruning. We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including graph neural networks, while using only a small number of AGs in an interpretable manner.\narXiv:2402.06932v1\n# 1 Introduction\nPrediction problems with a graph input, such as graph classification problems, have been widely studied in the data science community. A graph representation is useful to capture structural data, and graphbased machine learning algorithms have been applied to variety of application problems such as chemical composition analysis [1, 2] and crystal structure analysis [3, 4]. In real-word datasets, graphs often have node attributes as a continuous value vector (note that we only focus on node attributes throughout the paper, but the discussion is same for edge attributes). For example, a graph created by a chemical composition can have a three dimensional position of each atom as an attribute vector in addition to a categorical label such as atomic species. In this paper, we consider building an interepretable prediction model for a graph classification problem in which an input graph has continuous attribute vectors. As we will see in Section 3, this setting has not been widely studied despite its practical importance. Our framework can identify important small subgraphs, called graphlets, in which each node has an attribute vector. Note that we use the term graphlet simply to denote a small connected subgraph [5], though in some papers, it only indicates induced subgraphs [6]. Figure 1 shows an illustration of our prediction model. In the figure, the output of the prediction model f(G) = \u03b20 + \u03b2H1\u03c8(G; H1) + \u03b2H2\u03c8(G; H2)) + \u00b7 \u00b7 \u00b7 for an\n\u03c8 \u03c8 \u03c8 ( ) ( ) ( ) = + + + ; ; ; AG AG AG H1 H2 H3 \u03b20 +\u03b2H1 \u03b2H2 \u03b2H3 f (G) G G G\nFigure 1: Illustration of our attributed graphlet (AG) based prediction model. The colors of each graph node represents a graph node label, and a bar plot associated with each graph node represents a trainable attribute vector.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a759/a759ad14-cc80-482c-a29c-912a0044c902.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: An example of important attributed graphlets H+ and H\u2212identified by LAGRA in the AIDS dataset. H+ and H\u2212positively and negatively contribute to the prediction, respectively. The right plot is a scatter in which x- and y- axes are our graphlet features representing the how precisely H+ and H\u2212are included in the input graph Gi. Each point is from the test dataset.</div>\nFigure 2: An example of important attributed graphlets H+ and H\u2212identified by LAGRA in the AIDS dataset. H+ and H\u2212positively and negatively contribute to the prediction, respectively. The right plot is a scatter in which x- and y- axes are our graphlet features representing the how precisely H+ and H\u2212are included in the input graph Gi. Each point is from the test dataset. input attributed graph G is defined through a linear combination of attributed graphlets (AGs), represented as H1, H2, . . ., each one of which is weighted by parameters \u03b2H1, \u03b2H2, . . .. The function \u03c8(G; H) evaluates a matching score between G and an AG H in a sense that how precisely G contains the AG H. We apply a sparse regularization to the parameter \u03b2H by which a small number of important AGs for classification can be obtained, i.e., an AG with non-zero \u03b2H (in particular, if it has large |\u03b2H|) can be regarded as a discriminative AG. Important subgraphs and attribute vectors are usually unknown beforehand. The basic strategy of our proposed method, called LAGRA (Learning Attributed GRAphlets), is as follows: \u2022 To explore potentially important substructures of graphs, i.e., subgraphs, LAGRA uses graph mining by which all the subgraphs in the given dataset can be considered up to the given maximum graph size.\nFigure 2 is an example of identified AGs by LAGRA, which shows only two AGs clearly separate two classes (See Section 4.2 for detail). Since the number of the possible subgraphs is quite large and an attribute vector exists for each node in each one of subgraphs, a na\u00a8\u0131ve implementation becomes computationally intractable. For the efficient optimization, we employ a block coordinate update [7] based approach in which \u03b2 (a vector containing \u03b2H), the bias term \u03b20, and attribute vectors are alternately updated. In the alternate update of \u03b2, we apply the proximal gradient descent [8, 9], which is known as an effective algorithm to optimize sparse parameters. For this step, we propose an efficient pruning strategy, enabling us to identify dimensions\nthat are not required to update at that iteration. This pruning strategy has the three advantages. First, by combining the sparsity during the proximal update and the graph mining tree search, we can eliminate unnecessary dimensions without enumerating all the possible subgraphs. Second, for removed variables \u03b2H at that iteration, attribute vectors in H are also not required to be updated, which also accelerates the optimization. Third, our pruning strategy is designed so that it can maintain the update result compared with when we do not perform the pruning (In other words, our pruning strategy does not deteriorate the resulting model accuracy). Our contributions are summarized as follows:\n\u2022 We propose an interpretable graph classification model, in which the prediction is defined through a linear combination of graphlets that have trainable attribute vectors. By imposing a sparse penalty on the coefficient of each AG, a small number of important AGs can be identified.\n\u2022 To avoid directly handling an intractably large size of optimization variables, we propose an efficient pruning strategy based on the proximal gradient descent, which can safely ignore AGs that do not contribute to the update.\n\u2022 We verify effectiveness of LAGRA by empirical evaluations. Although our prediction model is simple and interpretable, we show that prediction performance of LAGRA was superior to or comparable with well-known standard graph classification methods, and in those results, LAGRA actually only used a small number of AGs. Further, we also show examples of selected AGs to demonstrate the high interpretability.\n# 2 Proposed Method: LAGRA\nIn this section, we describe our proposed method, called Learning Attributed GRAphlets (LAGRA). First in Section 2.1, we show the formulation of our model and the definition of the optimization problem. Second in Section 2.2, we show an efficient optimization algorithm for LAGRA.\n# 2.1 Formulation\n# 2.1.1 Problem Setting\nWe consider a classification problem in which a graph G is an input. A set of nodes and edges of G are written as VG and EG, respectively. Each one of nodes v \u2208VG has a categorical label Lv and a continuous attribute vector zG v \u2208Rd, where d is an attribute dimension. In this paper, an attribute indicates a continuous attribute vector. We assume that a label and an attribute vector are for a node, but the discussion in this paper is completely same as for an edge label and attribute. A training dataset is {(Gi, yi)}i\u2208[n], in which yi \u2208{\u22121, +1} is a binary label and n is the dataset size, where [n] = {1, . . . , n}. Although we only focus on the classification problem, our framework is also applicable to the regression problem just by replacing the loss function.\n# 2.1.2 Attributed Graphlet Inclusion Score\nWe consider extracting important small attributed graphs, which we call attributed graphlets (AGs), that contributes to the classification boundary. Note that throughout the paper, we only consider a connected graph as an AG for a better interpretability (do not consider an AG by a disconnected graph). Let \u03c8(Gi; H) \u2208 [0, 1] be a feature representing a degree that an input graph includes an AG H. We refer to \u03c8(Gi; H) as the\nAG inclusion score (AGIS). Our proposed LAGRA identifies important AGs by applying a feature selection to a model with this AGIS feature. Suppose that L(G) is a labeled graph having a categorical label Lv for each node, and in L(G), an attribute zG v for each node is excluded from G. We define AGIS so that it has a non-zero value only when L(H) is included in L(Gi):\n\uf8f3 where L(H) \u2291L(Gi) means that L(H) is a subgraph of L(Gi), and \u03d5H(Gi) \u2208(0, 1] is a function that provides a continuous inclusion score of H in Gi. The condition L(H) \u2291L(Gi) makes AGIS highly interpretable. For example, in the case of chemical composition data, if L(H) represents O-C (oxygen and carbon are connected) and \u03c8(Gi; H) > 0, then we can guarantee that Gi must contain O-C. Figure 3(a) shows examples of L(Gi) and L(H). The function \u03d5H(Gi) needs to be defined so that it can represent how strongly the attribute vectors in H can be matched to those of Gi. When L(H) \u2291L(Gi), there exists at least one injection m : VH \u2192VGi in which m(v) for v \u2208VH preserves node labels and edges among v \u2208VH. Figure 3(b) shows an example of when there exist two injections. Let M be a set of possible injections m. We define a similarity between H and a subgraph of Gi matched by m \u2208M as follows\n\ufffd\ufffd \ufffd\ufffd where \u03c1 > 0 is a fixed parameter that adjusts the length scale. In exp, the sum of squared distanc of attribute vectors between matched nodes are taken. To use this similarity in AGIS (1), we take th maximum among all the matchings M:\n\u03d5H(Gi) = MaxPooling({Sim(H, Gi; m) : m \u2208M})\nAn intuition behind (2) is that it evaluates inclusion of H in Gi based on the best macthing in a sense of Sim(H, Gi; m) for m \u2208M. If L(H) \u2291L(Gi) and there exists m such that zH v = zGi m(v) for \u2200v \u2208VH, then, \u03d5H(Gi) takes the maximum value (i.e., 1).\n# 2.1.3 Model definition\n# Our prediction model linearly combines the feature \u03c8(Gi; H) as follows:\nf(Gi) = \ufffd H\u2208H \u03c8(Gi; H)\u03b2H + \u03b20 = \u03c8\u22a4 i \u03b2 + \u03b20,\n\ufffd where \u03b2H and \u03b20 are parameters, H is a set of candidate AGs, and \u03b2 and \u03c8i are vectors containing \u03b2H and \u03c8(Gi; H) for H \u2208H, respectively. Let L = {L | L \u2286L(Gi), i \u2208[n], |L| \u2264maxpat} be a set of all the labeled subgraphs contained in the training input graphs {Gi}i\u2208[n], where |L| is the number of nodes in the labeled graph L and maxpat is the user-specified maximum size of AGs. The number of the candidate AGs |H| is set as the same size as |L|. We set H as a set of attributed graphs created by giving trainable attribute vectors zH v (v \u2208VH) to each one of elements in L. Figure 4 shows a toy example. Our optimization problem for \u03b2H, \u03b20 and zH v is defined as the following regularized loss minimization in which the sparse L1 penalty is imposed on \u03b2H:\n(1)\n(2)\n(3)\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33cb/33cb1ef6-b150-439d-88e7-a163d7fbef34.png\" style=\"width: 50%;\"></div>\nFigure 3: Examples of matchings between a graph and AGs (colors of graph nodes are node labels). (a) For two AGs H and H\u2032, L(Gi) only contains L(H), and L(H\u2032) is not contained. Then, \u03c8(Gi; H) > 0 and \u03c8(Gi; H\u2032) = 0. (b) An example of the set of injections M = {m, m\u2032}, where m(1) = 2, m(2) = 3, m\u2032(1) = 1, and m\u2032(2) = 4. The figure shows that m and m\u2032 are label and edge preserving.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f015/f015635a-e991-4d1d-a3b0-03d58419d21d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: An example of training data, L and H. Since L only includes subgraphs in the training data \u201c \u201d is not included in L. H is created from L by adding trainable attribute vectors zHi v (v \u2208VHi).</div>\n\u2113(yi, f(Gi)) = max(1 \u2212yif(Gi), 0)2.\nSince the objective function (4) induces a sparse solution for \u03b2H, we can identify a small number of important AGs as H having the non-zero \u03b2H. However, this optimization problem has an intractably large number of optimization variables (H contains all the possible subgraphs in the training dataset and each one of H \u2208H has the attribute vector zH v \u2208Rd for each one of nodes). We propose an efficient optimization algorithm that mitigates this problem.\n# 2.2 Optimization\nOur optimization algorithm is based on the block coordinate update [7] algorithm, in which the (proximal) gradient descent alternately updates a block of variables. We update one of \u03b2, \u03b20 and ZH alternately, while the other two parameters are fixed. First, the proximal gradient update is applied to \u03b2 because it has the L1 penalty. Second, for \u03b20, we calculate the optimal solution under fixing the other variable because it is\n<div style=\"text-align: center;\">(b)</div>\neasy to obtain. Third, for ZH, we apply the usual gradient descent update because it does not have sparse penalty. The difficulty of the optimization problem (4) originates from the size of H. We select a small size of a subset W \u2286H, and only \u03b2W \u2208R|W|, defined by \u03b2H for H \u2208W, and corresponding attribute vectors ZW = {zH v | v \u2208VH, H \u2208W} \u2286ZH are updated. We propose an efficient pruning strategy by combining the proximal gradient with the graph mining, which enables us to select W without enumerating all the possible subgraphs. A notable characteristics of this approach is that it can obtain the completely same result compared with when we do not restrict the size of variables.\n# 2.2.1 Update \u03b2, \u03b20 and ZH\n ZH Before introducing the subset W, we first describe update rules of each variable. First, we apply the proxi gradient update to \u03b2. Let\nBefore introducing the subset W, we first describe update rules of each variable. First, we apply the proximal gradient update to \u03b2. Let\nbe the derivative of the loss term in (4) with respect to \u03b2H. Then, the update of \u03b2 is defined by \u03b2(new) H \u2190prox (\u03b2H \u2212\u03b7 gH(\u03b2)) ,\nwhere \u03b7 > 0 is a step length, and\n\uf8f4 \uf8f4 \uf8f3 is a proximal operator (Note that the proximal gradient for the L1 penalty is often called ISTA [9], for which an accelerated variant called FISTA is also known. We here employ ISTA for simplicity). We select the step length \u03b7 by the standard backtrack search. The bias term \u03b20 is update by\nwhich is the optimal solution of the original problem (4) for given other variables \u03b2 and ZH. Since the objective function of \u03b20 is a differential convex function, the update rule of \u03b20 can be derived from the first order condition as\nwhere I(new) = {i | 1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b2(new) 0 ) > 0}. This update rule contains \u03b2(new) 0 in I(new). However, it is easy to calculate the update (6) without knowing \u03b2(new) 0 beforehand. Here, we omit detail because it is a simple one dimensional problem (see supplementary appendix A). For zH v \u2208ZH, we employ the standard gradient descent:\n \u03b1 > 0 is a step length to which we apply the standard backtrack s\n(5)\n(6)\n(7)\nIn every update of \u03b2, we incrementally add required H into W \u2286H. For the complement set W = H \\ W, which contains AGs that have never been updated, we initialize \u03b2H = 0 for H \u2208W. For the initialization of a node attribute vector zH v \u2208ZH, we set the same initial vector if the node (categorical) labels are same, i.e., zH v = zH\u2032 v\u2032 if Lv = Lv\u2032 for \u2200H, H\u2032 \u2208H (in practice, we use the average of the attribute vectors within each node label). This constraint is required for our pruning criterion, but it is only for initial values. After the update (7), all zH v can have different values. Since \u03b2H = 0 for H \u2208W, it is easy to derive the following relation from the proximal update (5):\n# |gH(\u03b2)| \u2264\u03bb and H \u2208W \u21d20 = prox (\u03b2H \u2212\u03b7 gH(\u03b2)) .\nThis indicates that if the conditions in the left side hold, we do not need to upd 0. Therefore, we set\nW \u2190W \u222a \ufffd H \ufffd\ufffd|gH(\u03b2)| > \u03bb, \u2200H \u2208W \ufffd ,\n\ufffd \ufffd\ufffd \ufffd and apply the update (5) only to H \u2208W. However, evaluating |gH(\u03b2)| > \u03bb for all H \u2208W can be computationally intractable because it needs to enumerate all the possible subgraphs. The following theorem can be used to avoid this difficulty: Theorem 2.1 Let L(H\u2032) \u2292L(H) and H, H\u2032 \u2208W. Then,\nwhere\ngH(\u03b2) = max \ufffd \ufffd i\u2208I\u2229{i|yi>0} yi\u03c8(Gi; H)(1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b20)),\n\u2212 \ufffd i\u2208I\u2229{i|yi<0} yi\u03c8(Gi; H)(1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b20)) \ufffd\nwhere I = {i | 1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b20) > 0}.\n I { | \u2212i i 0} See supplementary appendix B for the proof. Note that here I is defined by the current \u03b20 unlike (6). This theorem indicates that the gradient |gH\u2032(\u03b2)| for any H\u2032 whose L(H\u2032) contains L(H) as a subgraph can be bounded by gH(\u03b2). It should be noted that gH(\u03b2) can be calculated without generating H\u2032, and it mainly needs only the model prediction with the current parameter \u03c8\u22a4 i \u03b2 + \u03b20, which can be immediately obtained at each iteration, and AGIS \u03c8(Gi; H). The rule (8) reveals that, to identify \u03b2(new) H\u2032 = 0, we only require to know whether |gH\u2032(\u03b2)| \u2264\u03bb holds, and thus, an important consequence of theorem 2.1 is the following rule: gH(\u03b2) \u2264\u03bb and H \u2208W \u21d2|gH\u2032(\u03b2)| \u2264\u03bb for \u2200H\u2032 \u2208{H\u2032 | L(H\u2032) \u2292L(H), H\u2032 \u2208W}. (10) Therefore, if the conditions in the first line in (10) hold, any H\u2032 whose L(H\u2032) contains L(H) as a subgraph can be discarded during that iteration. Further, from (7), we can immediately see that attribute vectors zH v for \u2200v \u2208VH are also not necessary to be updated if \u03b2H = 0. This is an important fact because updates of a large number of variables can be omitted. Figure 5 shows an illustration of the forward and backward (gradient) computations of LAGRA. For the gradient pruning, an efficient algorithm can be constructed by combining the rule (10) and a graph\n I { | \u2212  } See supplementary appendix B for the proof. Note that here I is defined by the current \u03b20 unlike (6). This theorem indicates that the gradient |gH\u2032(\u03b2)| for any H\u2032 whose L(H\u2032) contains L(H) as a subgraph can be bounded by gH(\u03b2). It should be noted that gH(\u03b2) can be calculated without generating H\u2032, and it mainly needs only the model prediction with the current parameter \u03c8\u22a4 i \u03b2 + \u03b20, which can be immediately obtained at each iteration, and AGIS \u03c8(Gi; H). The rule (8) reveals that, to identify \u03b2(new) H\u2032 = 0, we only require to know whether |gH\u2032(\u03b2)| \u2264\u03bb holds, and thus, an important consequence of theorem 2.1 is the following rule:\n \u2264 \u2208W \u21d2|gH\u2032(\u03b2)| \u2264\u03bb for \u2200H\u2032 \u2208{H\u2032 | L(H\u2032) \u2292L(H), H\u2032 \u2208W}.\nTherefore, if the conditions in the first line in (10) hold, any H\u2032 whose L(H\u2032) contains L(H) as a subgraph can be discarded during that iteration. Further, from (7), we can immediately see that attribute vectors zH v for \u2200v \u2208VH are also not necessary to be updated if \u03b2H = 0. This is an important fact because updates of a large number of variables can be omitted. Figure 5 shows an illustration of the forward and backward (gradient) computations of LAGRA. For the gradient pruning, an efficient algorithm can be constructed by combining the rule (10) and a graph\n(8)\n(9)\n(10)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc1e/dc1e13e5-4f02-4b3a-9dae-7fb6edd2cb0f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">... (a) AGIS computation</div>\nFigure 5: An illustration of LAGRA. a) In the forward pass, only passes with |\u03b2H| > 0 contribute to the output. AGIS is defined by the best matching between an input graph and an AG. b) For the backward pass, the gradient can be pruned when the rule (10) is satisfied. In this illustration, H\u2032\u2032 is pruned by which graphs expanded from H\u2032\u2032 are not required to compute the gradient. mining algorithm. A well-known efficient graph mining algorithm is gSpan [11], which creates the tree by recursively expanding each graph in the tree node as far as the expanded graph is included in a given set of graphs as a subgraph. An important characteristics of the mining tree is that all graphs must contain any graph of its ancestors as subgraphs. Therefore, during the tree traverse (depth-first search) by gSpan, we can prune the entire subtree (all descendant nodes) if gH(\u03b2) \u2264\u03bb holds for the AG H in a tree node (Figure 5(b)). This means that we can update W by (9) without exhaustively investigating all the elements in W. gSpan has another advantage for LAGRA. To calculate the feature \u03d5H(Gi), defined in (2), LAGRA requires a set of injections M (an example is shown in Figure 3(b)). gSpan keeps the information of M during the tree traverse because it is required to expand a subgraph in each Gi (see the authors implementation https://sites.cs.ucsb.edu/~xyan/software/gSpan.htm). Therefore, we can directly use M created by gSpan to calculate (2).\nFigure 5: An illustration of LAGRA. a) In the forward pass, only passes with |\u03b2H| > 0 contribute to the output. AGIS is defined by the best matching between an input graph and an AG. b) For the backward pass, the gradient can be pruned when the rule (10) is satisfied. In this illustration, H\u2032\u2032 is pruned by which graphs expanded from H\u2032\u2032 are not required to compute the gradient.\n<div style=\"text-align: center;\">Figure 5: An illustration of LAGRA. a) In the forward pass, only passes with |\u03b2H| > 0 contribute to th output. AGIS is defined by the best matching between an input graph and an AG. b) For the backwar pass, the gradient can be pruned when the rule (10) is satisfied. In this illustration, H\u2032\u2032 is pruned by whic graphs expanded from H\u2032\u2032 are not required to compute the gradient.</div>\nmining algorithm. A well-known efficient graph mining algorithm is gSpan [11], which creates the tree by recursively expanding each graph in the tree node as far as the expanded graph is included in a given set of graphs as a subgraph. An important characteristics of the mining tree is that all graphs must contain any graph of its ancestors as subgraphs. Therefore, during the tree traverse (depth-first search) by gSpan, we can prune the entire subtree (all descendant nodes) if gH(\u03b2) \u2264\u03bb holds for the AG H in a tree node (Figure 5(b)). This means that we can update W by (9) without exhaustively investigating all the elements in W. gSpan has another advantage for LAGRA. To calculate the feature \u03d5H(Gi), defined in (2), LAGRA requires a set of injections M (an example is shown in Figure 3(b)). gSpan keeps the information of M during the tree traverse because it is required to expand a subgraph in each Gi (see the authors implementation https://sites.cs.ucsb.edu/~xyan/software/gSpan.htm). Therefore, we can directly use M created by gSpan to calculate (2).\n# 2.2.3 Algorithm\nWe here describe entire procedure of the optimization of LAGRA. We employ the so-called regularization path following algorithm (e.g., [12]), in which the algorithm starts from a large value of the regularization parameter \u03bb and gradually decreases it while solving the problem for each \u03bb. This strategy can start from highly sparse \u03b2, in which usually W also becomes small. Further, at each \u03bb, the solution obtained in the previous \u03bb, can be used as the initial value by which faster convergence can be expected (so-called warm start). Algorithm 1 shows the procedure of the regularization path following. We initialize \u03b2 = 0, which is\nobviously optimal when \u03bb = \u221e. In line 2 of Algorithm 1, we calculate \u03bbmax at which \u03b2 starts having nonzero values: \u03bbmax = maxH\u2208H \ufffd\ufffd\ufffd\ufffdn i\u2208[n] yi\u03c8(Gi; H)(1 \u2212yi\u03b20) \ufffd\ufffd\ufffd, where \u03b20 = \ufffd i\u2208[n] yi/n. See supplementary appendix C for derivation. \u03bbmax can also be written as \u03bbmax = maxH\u2208H |gH(0)|. To find maxH\u2208H, we can use almost the same gSpan based pruning strategy by using an upper bound of gH(\u03b2) as shown in Section 2.2.2 (the only difference is to search the max value only, instead of searching all H satisfying |gH(\u03b2)| > \u03bb), though in Algorithm 1, this process is omitted for brevity. After setting \u03bb0 \u2190\u03bbmax, the regularization parameter \u03bb is decreased by using a pre-defined decreasing factor R as shown in line 6 of Algorithm 1. For each \u03bb1 > \u00b7 \u00b7 \u00b7 > \u03bbK, the parameters \u03b2, \u03b20 and ZH are alternately updated as described in Section 2.2.1 and 2.2.2. We stop the alternate update by monitoring performance on the validation dataset in line 14 (stop by thresholding the decrease of the objective function is also possible). The algorithm of the pruning strategy described in Section 2.2.2 is shown in Algorithm 2. This function recursively traverses the graph mining tree. At each tree node, first, gH(\u03b2) is evaluated to prune the subtree if possible. Then, if |gH(\u03b2)| > \u03bbk, H is included in W. The expansion from H (creating children of the graph tree) is performed by gSpan, by which only the subgraphs contained in the training set can be generated (see the original paper [11] for detail of gSpan). The initialization of the trainable attribute zH\u2032 v is performed when H\u2032 is expanded (line 15).\nAlgorithm 1: Optimization of LAGRA\n1 function Reguralization-Path(K, R, MaxEpoch)\n2\nH0 \u2190a graph at the root node of the mining tree\n3\nW \u2190\u2205\n4\n\u03b2 \u21900, \u03b20 = \ufffd\ni\u2208[n] yi/n\n5\n\u03bb0 \u2190\u03bbmax\nCompute \u03bbmax\n6\nfor k = 1, 2, . . . , K do\n7\n\u03bbk \u2190R\u03bbk\u22121\n8\nfor epoch = 1, 2, . . . , MaxEpoch do\n9\nW \u2190W \u222aGradientPruning(H0, \u03bbk)\n10\nUpdate \u03b2 by (5) for H \u2208W\n11\nUpdate \u03b20 by (6)\n12\nUpdate zH\nv by (7) for H \u2208W\n13\nval loss \u2190Compute validation loss\n14\nif\nval loss has not been improved in the past q iterations then\n15\nbreak\nInner loop stopping condition\n16\nelse\n17\nM(k) \u2190(W, \u03b2, \u03b20)\n18\nreturn {M(k)}K\nk=0\n# 3 Related Work\nFor graph-based prediction problems, recently, graph neural networks (GNNs) [13] have attracted wide attention. However, interpreting GNNs is not easy in general. According to a recent review of explainable GNNs [14], almost all of explainability studies for GNNs are instance-level explanations, which provides\nAlgorithm 2: Gradient Pruning\n1 function GradientPruning(H, \u03bbk)\n2\nW \u2190\u2205\n3\nif gH(\u03b2) \u2264\u03bbk then\n4\nreturn \u2205\nPrune the subtree\n5\nif |gH(\u03b2)| > \u03bbk then\n6\nW \u2190W \u222a{H}\n7\nC \u2190CreateChildren(H)\n8\nfor H\u2032 \u2208C do\n9\nW \u2190W \u222aGradientPruning(H\u2032, \u03bbk)\n10\nreturn W\n11 function CreateChildren(H)\n12\nif\nchildren of H have never been created by gSpan then\n13\nC \u2190graphs expanded from H by gSpan\n14\nfor H\u2032 \u2208C do\n15\nzH\u2032\nv\n\u2190mean{zGi\nv\u2032 | v\u2032 \u2208VGi, Lv = Lv\u2032, i \u2208[n]}\n16\nCompute {\u03c8(Gi; H\u2032)}n\ni=1 using M created by gSpan\nTable 1: Classification accuracy and the number of selected non-zero \u03b2H by LAGRA (the bottom row). The average of five runs and its standard deviation are shown. The underlines indicate the best average accuracy for each dataset and the bold-face indicates that the result is comparable with the best method in a sense of one-sided t-test (significance level 5%). # best indicates frequency that the method is the best or comparable with the best method.\nAIDS\nBZR\nCOX2\nDHFR\nENZYMES\nPROTEINS\nSYNTHETIC\n# best\nGH\n0.9985 \u00b1 0.0020\n0.8458 \u00b1 0.0327\n0.7872 \u00b1 0.0252\n0.7250 \u00b1 0.0113\n0.6050 \u00b1 0.0857\n0.7277 \u00b1 0.0332\n0.6767 \u00b1 0.0655\n2\nML\n0.9630 \u00b1 0.0062\n0.8289 \u00b1 0.0141\n0.7787 \u00b1 0.0080\n0.7105 \u00b1 0.0300\n0.6000 \u00b1 0.0652\n0.6205 \u00b1 0.0335\n0.4867 \u00b1 0.0356\n0\nPA\n0.9805 \u00b1 0.0086\n0.8313 \u00b1 0.0076\n0.7809 \u00b1 0.0144\n0.7316 \u00b1 0.0435\n0.7500 \u00b1 0.0758\n0.6884 \u00b1 0.0077\n0.5400 \u00b1 0.0859\n1\nDGCNN\n0.9830 \u00b1 0.0046\n0.8169 \u00b1 0.0177\n0.8021 \u00b1 0.0401\n0.7289 \u00b1 0.0192\n0.7289 \u00b1 0.0192\n0.7509 \u00b1 0.0114\n0.9867 \u00b1 0.0125\n3\nGCN\n0.9840 \u00b1 0.0030\n0.8290 \u00b1 0.0460\n0.8340 \u00b1 0.0257\n0.7490 \u00b1 0.0312\n0.7000 \u00b1 0.0837\n0.6880 \u00b1 0.0202\n0.9630 \u00b1 0.0194\n2\nGAT\n0.9880 \u00b1 0.0041\n0.8220 \u00b1 0.0336\n0.7830 \u00b1 0.0274\n0.7110 \u00b1 0.0156\n0.7100 \u00b1 0.0768\n0.7160 \u00b1 0.0108\n0.9800 \u00b1 0.0267\n2\nLAGRA (Proposed)\n0.9900 \u00b1 0.0050\n0.8892 \u00b1 0.0207\n0.8043 \u00b1 0.0229\n0.8171 \u00b1 0.0113\n0.6450 \u00b1 0.0797\n0.7491 \u00b1 0.0142\n1.0000 \u00b1 0.0000\n4\n# non-zero \u03b2H\n50.4 \u00b1 17.1\n52.4 \u00b1 19.0\n45.4 \u00b1 14.9\n40.0 \u00b1 11.6\n7.2 \u00b1 8.4\n25.8 \u00b1 9.4\n35.8 \u00b1 35.0\n-\ninput-dependent explanations (Here, we do not mention each one of input-dependent approaches because the purpose is clearly different from LAGRA). An exception is XGNN [15], in which important discriminative graphs are generated for a given already trained GNN by maximizing the GNN output for a target label. However, unlike our method, the prediction model itself remains black-box, and thus, it is difficult to know underlying dependency between the identified graphs and the prediction. A classical approach to graph-based prediction problems is the graph kernel [16]. Although graph kernel itself does not identify important substructures, recently, [17] has proposed an interpretable kernel-based GNN, called KerGNN. KerGNN uses a graph kernel function as a trainable filter, inspired by the well-known convolutional networks, and the filter updates the node attributes of the input graph so that it embeds similarity to learned important subgraphs. Then, [17] claims that resulting graph filter can be seen as a key structure. However, a learned subgraph in a graph kernel filter is difficult to interpret. The kernel-based matching does not guarantee the existence of a subgraph unlike our AGIS (1), and further, only 1-hop neighbors of each node in the input graph are matched to a graph filter. Another graph mining based approach is [18]. This approach also uses a pruning based acceleration for the optimization, but it is based on the optimality of the convex problem while our proximal gradient pruning is applicable to the non-convex problem of LAGRA. Further, more importantly, [18] cannot deal with continuous attributes. The prediction model of LAGRA is inspired by a method for learning time-series shaplets (LTS) [19]. LTS is also based on a linear combination of trainable shaplets, which is a short fragment of a time-series sequence. Unlike time-series data, possible substructures in graph data have a combinatorial nature because of which our problem setting has a computational difficulty that does not exist in the case of LTS, for which LAGRA provide a graph mining based efficient strategy.\n# 4 Experiments\nHere, we empirically verify effectiveness of LAGRA. We used standard graph benchmark datasets, called AIDS, BZR, COX2, DHFR, ENZYMES, PROTEINS and SYNTHETIC, retrieved from https://ls11-www. cs.tu-dortmund.de/staff/morris/graphkerneldatasets (for ENZYMES, we only used two classes among original six classes to make a binary problem). To simplify comparison, we only used node labels and attributes, and did not use edge labels and attributes. Statistics of datasets are summarized in supplementary appendix D. The datasets are randomly divided into train : validation : test = 0.6 : 0.2 : 0.2. For the regularization path algorithm (Algorithm 1), we created candidate values of \u03bb by uniformly dividing [log(\u03bbmax), log(0.01\u03bbmax)] into 100 grid points. We selected \u03bb, maxpat \u2208{5, 10} and \u03c1 \u2208{1, 0.5, 0.1, 0.05, 0.01} based on the validation performance.\n# 4.1 Prediction Performance\nFor the prediction accuracy comparison, we used graph kernels and graph neural networks (GNN). We used three well-known graph kernels that can handle continuous attributes, i.e., graph hopper kernel (GH) [20], multiscale Laplacian kernel (ML) [21] and propagation kernel (PA) [22], for all of which the library called GraKeL [23] was used. For the classifier, we employed the k-nearest neighbor (k-NN) classification for which each kernel function k(Gi, Gj) defines the distance function as \u2225Gi\u2212Gj\u2225= \ufffd k(Gi, Gi) \u22122K(Gi, Gj) + k(Gj, Gj) The number of neighbors k is optimized by the validation set. For GNN, we used deep graph convolutional neural network (DGCNN) [24], graph convolutional network (GCN) [25], and graph attention network (GAT) [26]. For DGCNN, the number of hidden units {64, 128, 256} and epochs are optimized by the validation set. The other settings were in the default settings of the authors implementation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6016/6016b536-9739-46be-a4ca-c58ba949057a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Selected important AGs for DHFR dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for the two largest negative coefficients.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af23/af2367b8-bcab-4b31-ab2c-409bb931b502.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Selected important AGs for BZR dataset. (a) and (b): AGs for the two largest positive coefficients (c) and (d): AGs for the two largest negative coefficients.</div>\nhttps://github.com/muhanzhang/pytorch_DGCNN. For GCN and GAT, we also selected the number of hidden units and epochs as above. For other settings, we followed [27]. The results are shown in Table 1. LAGRA was the best or comparable with the best method (in a sense of one-sided t-test) for BZR, DHFR, PROTEINS and SYNTHETIC (4 out of 7 datasets). For AIDS and COX2, LAGRA has similar accuracy values to the best methods though they were not regarded as the best accuracy in t-test. The three GNNs also show stable performance overall. Although our main focus is to build an interepretable model, we see that LAGRA achieved comparable accuracy with the current standard methods. Further, LAGRA only used a small number of AGs shown in the bottom row of Table 1, which suggests high interpretability of the learned models.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bbd/3bbddad0-76e9-485d-bba9-ab936ba30d0a.png\" style=\"width: 50%;\"></div>\nFigure 8: Scatter plots defined by selected AGs with test dataset of DHFR. The horizontal and vertical axe are AGIS of Fig. 6(a) and Fig. 6(c), respectively.\n<div style=\"text-align: center;\">Table 2: The size of the candidate set H with maxpat 10. For the ENZYMES, PROTEINS, and SYNTHETIC datasets, only the lower bounds are shown because it took long time to count.</div>\nTable 2: The size of the candidate set H with maxpat 10. For the ENZYMES, PROTEINS,  datasets, only the lower bounds are shown because it took long time to count.\nAIDS\nBZR\nCOX2\nDHFR\nENZYMES\nPROTEINS\nSYNTHETIC\n134281\n148903\n101185\n137872\n> 15464000\n> 13987000\n> 699000\n# 4.2 Examples of Selected Attributed Graphlet\nWe here show examples of identified important AGs. Figure 6 and 7 show AGs having the two largest positive and negative \u03b2H for DHFR and BZR datasets, respectively. In each figure, a labeled graphlet L(H) is shown in the left side (the numbers inside the graph nodes are the graph node labels) and optimized attribute vectors for each one of nodes are shown as bar plots in the right side. We can clearly see important substractures not only by as structural information of a graph but also attribute values associated with each node. Surprisingly, in a few datasets, two classes can be separated even in two dimensional space of AGIS. Figure 2 and Figure 8 show scatter plots of the test dataset (not the training dataset) with the axes of identified features by the LAGRA training. Let H+ and H\u2212be AGs having the largest positive and negative \u03b2H, respectively. The horizontal and vertical axes of plots are \u03c8(Gi, H+) and \u03c8(Gi, H\u2212). In particular, in the AIDS dataset, for which classification accuracy was very high in Table 1, two classes are clearly separated. For DHFR, we can also see points in two classes tend to be located on the upper left side and the lower right side. The dashed lines are boundaries created by (class-balance weighted) logistic regression fitted to the test points in these two dimensional spaces. The estimated class conditional probability has AUC = 0.94 and 0.62 for AIDS and BZR, respectively, which indicate that differences of two classes are captured even only by two AGs in these datasets.\n# 4.3 Discussion on Computational Time\nFinally, we verify computational time of LAGRA. First, Table 2 shows the size of candidate AGs |H| in each dataset. As we describe in Section 2.1.3, this size is equal to |L|, i.e., the number of all the possible subgraphs in the training datasets. Therefore, it can be quite large as particularly shown in the ENZYMES, PROTEINS and SYNTHETIC datasets in Table 2. The optimization variables in the objective function (4) are \u03b2, \u03b20 and ZH. The dimension of \u03b2 is |H| and the node attribute vector zH v \u2208Rd exists for each one of nodes in H \u2208H. Thus, the number of optimization variables in (4) is 1 + |H| + \ufffd H\u2208H |H| \u00d7 d, which can be prohibitively large. Figure 9 shows the computational time during the regularization path. The horizontal axis is k of \u03bbk in Algorithm 1. The datasets are AIDS and ENZYMES. In the regularization path algorithm, the number of non-zero \u03b2H typically increases during the process of decreasing \u03bb, because the L1 penalty becomes weaker gradually. As a results, in both the plots, the total time increases with the \u03bb index. Although LAGRA performs the traverse of the graph mining tree in every iteration of the gradient update (line 9 in Algorithm 1), Figure 9 shows that the traverse time was not necessarily dominant (Note that the vertical axis is in log scale). In particular, when only a small number of tree nodes are newly expanded at that \u03bb, the calculation for the tree search becomes faster because AGIS \u03c8(Gi, H) is already computed at the most of tree nodes. The computational times were at most about 103 sec for these datasets. We do not claim that LGARA is computationally faster compared with other standard algorithms (such as graph kernels), but as the computational time of the optimization problem with 1 + |H| + \ufffd H\u2208H |H| \u00d7 d variables, the results obviously indicate effectiveness of our pruning based optimization approach.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/46ca/46cace2d-84c9-4298-8c6f-dc10fa062b6a.png\" style=\"width: 50%;\"></div>\nFigure 9: Transition of computational time (sec) on regularization path. For each \u03bb, the total time and the time required to traverse the graph mining tree (in other words, the time required to identify W) is shown separately.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e31c/e31c64a0-919a-40d3-8eac-f23861ce3bc4.png\" style=\"width: 50%;\"></div>\nFigure 10 shows the average number of traversed graph mining tree nodes and the size of selected |W| for AIDS and ENZYMES. In this figure, both the values increased with the decrease of \u03bb because the effect of the sparse penalty becomes weaker. As shown in Table 2, the total number of the tree nodes were 134281 and more than 15464000 for AIDS and ENZYMES, respectively. Figure 10 shows the number of traversed nodes were at most about 6 \u00d7 103/134281(\u22480.05) and 9 \u00d7 102/15464000(\u22486 \u00d7 10\u22125), respectively. This clearly indicates that our pruning strategy can drastically reduce the tree nodes, at which the evaluation of gH(\u03b2) is required as shown in Algorithm 2. In the figure, we can see that |W| was further small. This indicates the updated \u03b2 was highly sparse, by which the update of zH v becomes easier because it requires only for non-zero \u03b2H.\n# 5 Conclusion\nThis paper proposed LAGRA (Learning Attributed GRAphlets), which learns a prediction model that linearly combines attributed graphlets (AGs). In LAGRA, graph structures of AGs are generated through a graph mining algorithm, and attribute vectors are optimized as a continuous trainable parameters. To identify a small number of AGs, the L1 sparse penalty is imposed on coefficients of AGs. We employed a block coordinate update based optimization algorithm, in which an efficient pruning strategy was proposed by combining the proximal gradient update and the graph mining tree search. Our empirical evaluation\n<div style=\"text-align: center;\">(b) ENZYMES</div>\nshowed that LAGRA has superior or comparable performance with standard graph classification algorithms. We further demonstrated that LAGRA actually can identify a small number of discriminative AGs that have high interpretability.\n# Acknowledgments\nThis work was partially supported by MEXT KAKENHI (21H03498, 22H00300, 23K17817), and Interna tional Joint Usage/Research Project with ICR, Kyoto University (2023-34).\n[1] L. Ralaivola, S. J. Swamidass, H. Saigo, and P. Baldi, \u201cGraph kernels for chemical informatics,\u201d Neural Networks, vol. 18, no. 8, pp. 1093\u20131110, 2005. [2] F. A. Faber, L. Hutchison, B. Huang, J. Gilmer, S. S. Schoenholz, G. E. Dahl, O. Vinyals, S. Kearnes, P. F. Riley, and O. A. von Lilienfeld, \u201cPrediction errors of molecular machine learning models lower than hybrid dft error,\u201d Journal of Chemical Theory and Computation, vol. 13, no. 11, pp. 5255\u20135264, 2017. [3] T. Xie and J. C. Grossman, \u201cCrystal graph convolutional neural networks for an accurate and interpretable prediction of material properties,\u201d Phys. Rev. Lett., vol. 120, p. 145301, 2018. [4] S.-Y. Louis, Y. Zhao, A. Nasiri, X. Wang, Y. Song, F. Liu, and J. Hu, \u201cGraph convolutional neural networks with global attention for improved materials property prediction,\u201d Phys. Chem. Chem. Phys., vol. 22, pp. 18 141\u201318 148, 2020. [5] N. Shervashidze, S. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt, \u201cEfficient graphlet kernels for large graph comparison,\u201d in Artificial Intelligence and Statistics, 2009, pp. 488\u2013495. [6] N. Pr\u02c7zulj, \u201cBiological network comparison using graphlet degree distribution,\u201d Bioinformatics, vol. 23, no. 2, pp. e177\u2013e183, 2007. [7] Y. Xu and W. Yin, \u201cA globally convergent algorithm for nonconvex optimization based on block coordinate update,\u201d Journal of Scientific Computing, vol. 72, pp. 700\u2013734, 2017. [8] M. Teboulle, \u201cA simplified view of first order methods for optimization,\u201d Mathematical Programming, vol. 170, pp. 67\u201396, 2017. [9] A. Beck and M. Teboulle, \u201cA fast iterative shrinkage-thresholding algorithm for linear inverse problems,\u201d SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009. 10] K. Janocha and W. M. Czarnecki, \u201cOn loss functions for deep neural networks in classification,\u201d Schedae Informaticae, vol. 25, p. 49, 2016. 11] X. Yan and J. Han, \u201cgSpan: Graph-based substructure pattern mining,\u201d in Proceedings. 2002 IEEE International Conference on Data Mining. IEEE, 2002, pp. 721\u2013724. 12] J. Friedman, T. Hastie, H. H\u00a8ofling, and R. Tibshirani, \u201cPathwise coordinate optimization,\u201d The Annals of Applied Statistics, vol. 1, no. 2, pp. 302\u2013332, 12 2007. 13] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, \u201cGraph neural networks: A review of methods and applications,\u201d AI Open, vol. 1, pp. 57\u201381, 2020. 14] H. Yuan, H. Yu, S. Gui, and S. Ji, \u201cExplainability in graph neural networks: A taxonomic survey,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 15] H. Yuan, J. Tang, X. Hu, and S. Ji, \u201cXGNN: Towards model-level explanations of graph neural networks,\u201d in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New York, NY, USA: Association for Computing Machinery, 2020, pp. 430\u2013438. 16] N. M. Kriege, F. D. Johansson, and C. Morris, \u201cA survey on graph kernels,\u201d Applied Network Science, vol. 5, p. 6, 2020.\n[17] A. Feng, C. You, S. Wang, and L. Tassiulas, \u201cKerGNNs: Interpretable graph neural networks with graph kernels,\u201d in Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI. AAAI Press, 2022, pp. 6614\u20136622. [18] K. Nakagawa, S. Suzumura, M. Karasuyama, K. Tsuda, and I. Takeuchi, \u201cSafe pattern pruning: An efficient approach for predictive pattern mining,\u201d in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016, pp. 1785\u20131794. [19] J. Grabocka, N. Schilling, M. Wistuba, and L. Schmidt-Thieme, \u201cLearning time-series shapelets,\u201d in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New York, NY, USA: Association for Computing Machinery, 2014, p. 392\u2013401. [20] A. Feragen, N. Kasenburg, J. Petersen, M. de Bruijne, and K. Borgwardt, \u201cScalable kernels for graphs with continuous attributes,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 216\u2013224. [21] R. Kondor and H. Pan, \u201cThe multiscale Laplacian graph kernel,\u201d in Advances in Neural Information Processing Systems, 2016, pp. 2990\u20132998. [22] M. Neumann, R. Garnett, C. Bauckhage, and K. Kersting, \u201cPropagation kernels: efficient graph kernels from propagated information,\u201d Machine Learning, vol. 102, no. 2, pp. 209\u2013245, 2016. [23] G. Siglidis, G. Nikolentzos, S. Limnios, C. Giatsidis, K. Skianis, and M. Vazirgiannis, \u201cGraKeL: A graph kernel library in python,\u201d Journal of Machine Learning Research, vol. 21, no. 54, pp. 1\u20135, 2020. [24] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, \u201cAn end-to-end deep learning architecture for graph classification,\u201d in Proceedings of AAAI Conference on Artificial Inteligence, 2018. [25] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [26] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and Y. Bengio, \u201cGraph attention networks,\u201d in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [27] J. You, J. M. Gomes-Selman, R. Ying, and J. Leskovec, \u201cIdentity-aware graph neural networks,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 12, 2021, pp. 10 737\u201310 745.\n# Supplementary Appendix for \u201cLearning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute\u201d\nA Update \u03b20\nThe objective of \u03b20 can be re-written as\nmin \u03b20 \ufffd i=0 max(1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b20), 0)2 = min \u03b20 \ufffd i\u2208I (1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b20))2.\nFor simplicity, we assume that all \u03c8\u22a4 i \u03b2 for i \u2208[n] have different values (even when this does not hold, the optimal solution can be obtained by the same approach). Depending on \u03b2(new) 0 , elements in I(new) = {i | 1 \u2212yi(\u03c8\u22a4 i \u03b2 + \u03b2(new) 0 ) > 0} changes in a piecewise constant manner. The point that I(new) changes are characterized by the solution of the equation yi(\u03c8\u22a4 i \u03b2 + \u03b20) = 1(i \u2208[n]) with respect to \u03b20, i.e., there exist n + 1 segments on the space of \u03b20 \u2208R. Let B(k) = [\u03b2sk 0 , \u03b2ek 0 ] be the k-th segment (k \u2208{0, . . . , n}) and I(k) is I(new) when \u03b2(new) 0 \u2208B(k). Note that \u03b2ek 0 = \u03b2sk+1 0 (k \u2208[n]), which is the solution of yk(\u03c8\u22a4 k \u03b2 + \u03b20) = 1, and \u03b2s0 0 = \u2212\u221eand \u03b2en 0 = \u221e. Under the assumption of \u03b20 \u2208B(k), the optimal \u03b20 is\nSince (11) is convex with respect to \u03b20, the obtained \u02c6\u03b2(k) 0 must be the optimal solution if it satisfies \u02c6\u03b2(k) 0 \u2208 B(k). Thus, the optimal \u03b20 can be found by calculating \u02c6\u03b2(k) 0 for all k \u2208{0, . . . , n}.\n# B Proof of Theorem 2.1\nFrom the definition of AGIS, the following monotonicity property is guaranteed:\nL(H\u2032) \u2292L(H) and H, H\u2032 \u2208W \u21d2\u03c8(Gi; H) \u2265\u03c8(Gi; H\u2032\nL(H\u2032) \u2292L(H) and H, H\u2032 \u2208W \u21d2\u03c8(Gi; H) \u2265\u03c8(Gi; H\u2032)\nLet M(Gi; H) be the set of injections M between Gi and H. The above monotonicity property can be eas verified from the fact\nmin m\u2208M(Gi;H) D(m) H,Gi \u2264 min m\u2208M(Gi;H\u2032) D(m) H\u2032,Gi.\nDefine\n\uf8f1 \uf8f2 \uf8f3 ai = 0 otherwise Note that the sign of ai is same as yi. Using ai, we re-write gH\u2032(\u03b2) as\n\uf8f3 Note that the sign of ai is same as yi. Using ai, we re-write gH\u2032(\u03b2) as\n18\n(11)\nAIDS\nBZR\nCOX2\nDHFR\nENZYMES\nPROTEINS\nSYNTHETIC\n# instances\n2000\n405\n467\n756\n200\n1113\n300\nDim. of attribute vector d\n4\n3\n3\n3\n18\n1\n1\nAvg. # nodes\n15.69\n35.75\n41.22\n42.43\n32.58\n39.06\n100.00\nAvg. # edges\n16.20\n38.36\n43.45\n44.54\n60.78\n72.82\n196.00\nFrom the monotonicity inequality \u03c8(Gi; H) \u2265\u03c8(Gi; H\u2032), we see\nFrom these inequalities, we obtain\n# \uf8f3 C Derivation of \u03bbmax\nWhen \u03b2 = 0, the objective function of \u03b20 is written as the following piecewise quadratic function: \uf8f1\n# When \u03b2 = 0, the objective function of \u03b20 is written as the following piecewise quadratic function: \uf8f1\n# When \u03b2 = 0, the objective function of \u03b20 is written as the following piecewise quadratic function:\nmin \u03b20 1 2 \ufffd i\u2208I(\u03b20) (1 \u2212yi\u03b20)2 s.t. I(\u03b20) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 {i | yi > 0} \u03b20 \u2264\u22121, [n] \u03b20 \u2208[\u22121, 1], {i | yi < 0} \u03b20 \u22651.\nmin \u03b20 1 2 \ufffd i\u2208I(\u03b20) (1 \u2212yi\u03b20)2 s.t. I(\u03b20) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 {i | yi > 0} \u03b20 \u2264\u22121, [n] \u03b20 \u2208[\u22121, 1], {i | yi < 0} \u03b20 \u22651.\n\uf8f4 \uf8f4 \uf8f3 In the region \u03b20 \u2264\u22121, the minimum value is achieved by \u03b20 = \u22121, and for \u03b20 \u22651, the minimum value is achieved by \u03b20 = 1. This indicates that the optimal solution should exist in \u03b20 \u2208[\u22121, 1] because the objective function is a smooth convex function. Therefore, the minimum value in the region \u03b20 \u2208[\u22121, 1] achieved by \u03b20 = \ufffd i\u2208[n] yi/n, defined as \u00afy, becomes the optimal solution. When \u03b2 = 0 and \u03b20 = \u00afy, we obtain\ngH(0) = \ufffd i\u2208[n] yi\u03c8(Gi; H)(1 \u2212yi\u00afy).\ngH(0) = \ufffd yi\u03c8(Gi; H)(1 \u2212yi\u00afy).\nFrom (8), we see that |gH(0)| = \u03bb is the threshold that \u03b2H have a non-zero value. This means that H having the maximum |gH(0)| is the first H that start having a non-zero value by decreasing \u03bb from \u221e. Therefore, we obtain \u03bbmax = max \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd yi\u03c8(Gi; H)(1 \u2212yi\u00afy) \ufffd\ufffd\ufffd\ufffd\ufffd .\nFrom (8), we see that |gH(0)| = \u03bb is the threshold that \u03b2H have a non-zero value. This means that H having the maximum |gH(0)| is the first H that start having a non-zero value by decreasing \u03bb from \u221e. Therefore,\nFrom (8), we see that |gH(0)| = \u03bb is the threshold that \u03b2H have a non-zero value. This means that H having the maximum |gH(0)| is the first H that start having a non-zero value by decreasing \u03bb from \u221e. Therefore, we obtain \ufffd\ufffd \ufffd\ufffd\n# D Statistics of datasets\nStatistics of datasets is show in Table 3.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bd42/bd425a30-cbfc-4cdf-bcc7-3486c5860ff7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Selected important AGs for AIDS dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for the two largest negative coefficients.</div>\n# E Additional Examples of Selected AGs\nFigure 11 shows selected AGs for the AIDS dataset.\n",
    "paper_type": "method",
    "attri": {
        "background": "The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a na\u00efve implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search.",
        "problem": {
            "definition": "The problem addressed in this paper is the challenge of creating interpretable and high-performance predictive models for graph classification tasks, particularly when dealing with attributed graphs.",
            "key obstacle": "The main difficulty is the computational intractability of evaluating all possible subgraph structures and their associated attributes, which can lead to inefficiencies in existing methods."
        },
        "idea": {
            "intuition": "The idea behind LAGRA is inspired by the need for interpretable models that can effectively utilize the structural and attribute information present in attributed graphs.",
            "opinion": "LAGRA proposes a method to learn a model that combines attributed graphlets while optimizing their respective attribute vectors, allowing for a clearer understanding of the contributions of different graph structures to the classification task.",
            "innovation": "The primary innovation of LAGRA lies in its efficient pruning strategy that enables the exploration of a vast number of potential attributed graphlets without exhaustive computation, thus maintaining model accuracy while improving efficiency."
        },
        "method": {
            "method name": "Learning Attributed GRAphlets",
            "method abbreviation": "LAGRA",
            "method definition": "LAGRA is a graph classification algorithm that learns importance weights for attributed graphlets and optimizes their attribute vectors to enhance interpretability and predictive performance.",
            "method description": "LAGRA identifies important attributed graphlets through a linear combination of these graphlets weighted by learned parameters, facilitating a classification model that is both interpretable and effective.",
            "method steps": [
                "Identify all potential attributed graphlets from the training dataset.",
                "Calculate the inclusion scores for each graphlet in the input graphs.",
                "Apply a proximal gradient descent method to optimize the weights for the graphlets while enforcing sparsity.",
                "Utilize a pruning strategy to efficiently discard non-contributing graphlets during optimization."
            ],
            "principle": "The method is effective because it leverages the structural information encoded in attributed graphlets while optimizing the associated attributes, allowing for better discrimination between classes in graph classification tasks."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using standard graph benchmark datasets such as AIDS, BZR, COX2, DHFR, ENZYMES, PROTEINS, and SYNTHETIC, which were randomly divided into training, validation, and testing sets.",
            "evaluation method": "Performance was assessed by comparing LAGRA against standard graph classification algorithms, measuring accuracy and interpretability through the number of selected attributed graphlets."
        },
        "conclusion": "LAGRA successfully combines the strengths of attributed graphlets with an efficient optimization strategy, demonstrating superior or comparable performance to standard graph classification methods while maintaining high interpretability through a small number of selected graphlets.",
        "discussion": {
            "advantage": "LAGRA stands out due to its ability to provide interpretable models that effectively utilize structural and attribute information from graphs, achieving high predictive performance with fewer selected graphlets.",
            "limitation": "Despite its advantages, LAGRA may still face challenges with scalability in extremely large datasets or graphs with complex structures, which could impact computational efficiency.",
            "future work": "Future research could focus on enhancing the scalability of LAGRA, exploring its application to more complex graph structures, and integrating additional types of attributes beyond node attributes."
        },
        "other info": {
            "acknowledgments": "This work was partially supported by MEXT KAKENHI (21H03498, 22H00300, 23K17817), and International Joint Usage/Research Project with ICR, Kyoto University (2023-34).",
            "dataset statistics": {
                "AIDS": {
                    "instances": 2000,
                    "attribute dimension": 4,
                    "avg_nodes": 15.69,
                    "avg_edges": 16.2
                },
                "BZR": {
                    "instances": 405,
                    "attribute dimension": 3,
                    "avg_nodes": 35.75,
                    "avg_edges": 38.36
                },
                "COX2": {
                    "instances": 467,
                    "attribute dimension": 3,
                    "avg_nodes": 41.22,
                    "avg_edges": 43.45
                },
                "DHFR": {
                    "instances": 756,
                    "attribute dimension": 3,
                    "avg_nodes": 42.43,
                    "avg_edges": 44.54
                },
                "ENZYMES": {
                    "instances": 200,
                    "attribute dimension": 18,
                    "avg_nodes": 32.58,
                    "avg_edges": 60.78
                },
                "PROTEINS": {
                    "instances": 1113,
                    "attribute dimension": 1,
                    "avg_nodes": 39.06,
                    "avg_edges": 72.82
                },
                "SYNTHETIC": {
                    "instances": 300,
                    "attribute dimension": 1,
                    "avg_nodes": 100.0,
                    "avg_edges": 196.0
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "Define and explain essential terms and concepts relevant to the study of attributed graphs and graph classification."
        },
        {
            "section number": "3.1",
            "key information": "LAGRA (Learning Attributed GRAphlets) is a graph classification algorithm that learns importance weights for attributed graphlets and optimizes their attribute vectors to enhance interpretability and predictive performance."
        },
        {
            "section number": "3.2",
            "key information": "LAGRA identifies important attributed graphlets through a linear combination of these graphlets weighted by learned parameters, facilitating a classification model that is both interpretable and effective."
        },
        {
            "section number": "3.4",
            "key information": "The main difficulty is the computational intractability of evaluating all possible subgraph structures and their associated attributes, which can lead to inefficiencies in existing methods."
        },
        {
            "section number": "7.1",
            "key information": "Future research could focus on enhancing the scalability of LAGRA, exploring its application to more complex graph structures, and integrating additional types of attributes beyond node attributes."
        }
    ],
    "similarity_score": 0.5856548316095025,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0011_large/papers/Learning Attributed Graphlets_ Predictive Graph Mining by Graphlets with Trainable Attribute.json"
}