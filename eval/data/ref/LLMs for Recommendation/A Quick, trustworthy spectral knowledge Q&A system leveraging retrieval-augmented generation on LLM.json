{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.11557",
    "title": "A Quick, trustworthy spectral knowledge Q&A system leveraging retrieval-augmented generation on LLM",
    "abstract": "Large Language Model (LLM) has demonstrated significant success in a range of natural language processing (NLP) tasks within general domain. The emergence of LLM has introduced innovative methodologies across diverse fields, including the natural sciences. Researchers aim to implement automated, concurrent process driven by LLM to supplant conventional manual, repetitive and laborintensive work. In the domain of spectral analysis and detection, it is imperative for researchers to autonomously acquire pertinent knowledge across various research objects, which encompasses the spectroscopic techniques and the chemometric methods that are employed in experiments and analysis. Paradoxically, despite the recognition of spectroscopic detection as an effective analytical method, the fundamental process of knowledge retrieval remains both time-intensive and repetitive. In response to this challenge, we first introduced the Spectral Detection and Analysis Based Paper(SDAAP) dataset, which is the first open-source textual knowledge dataset for spectral analysis and detection and contains annotated literature data as well as corresponding knowledge instruction data. Subsequently, we also designed an automated Q&A framework based on the SDAAP dataset, which can retrieve relevant knowledge and generate high-quality responses by extracting entities in the input as retrieval parameters. It is worth noting that: within this framework, LLM is only used as a tool to provide generalizability, while RAG technique is used to accurately capture the source of the knowledge.This approach not only improves the quality of the generated responses, but also ensures the traceability of the knowledge. Experimental results show that our framework generates responses with more reliable expertise compared to the baseline.",
    "bib_name": "liang2024quicktrustworthyspectralknowledge",
    "md_text": "A QUICK, TRUSTWORTHY SPECTRAL KNOWLEDGE Q&A SYSTEM LEVERAGING RETRIEVAL-AUGMENTED GENERATION ON LLM\nA PREPRINT\nJiheng Liang\nSchool of Physics\nState Key Laboratory of Optoelectronic Materials and Technologies\nSun Yat-Sen University\nGuangzhou, 510275, China\nliangjh65@mail2.sysu.edu.cn\nZiru Yu\u2217\nSchool of Automation Science and Engineering\nSouth China University of Technology\nGuangzhou, 510641, China\n202010102782@mail.scut.edu.cn\nZujie Xie, Xiangyang Yu\nSchool of Physics\nState Key Laboratory of Optoelectronic Materials and Technologies\nNanchang Research Institute Sun Yat-Sen University\nGuangzhou, 510275, China\n(xiezj8,cesyxy)@mail2.sysu.edu.cn\n# Zujie Xie, Xiangyang Yu\nSchool of Physics State Key Laboratory of Optoelectronic Materials and Technologies Nanchang Research Institute Sun Yat-Sen University Guangzhou, 510275, China (xiezj8,cesyxy)@mail2.sysu.edu.cn\n# ABSTRACT\nLarge Language Model (LLM) has demonstrated significant success in a range of natural language processing (NLP) tasks within general domain. The emergence of LLM has introduced innovative methodologies across diverse fields, including the natural sciences. Researchers aim to implement automated, concurrent process driven by LLM to supplant conventional manual, repetitive and laborintensive work. In the domain of spectral analysis and detection, it is imperative for researchers to autonomously acquire pertinent knowledge across various research objects, which encompasses the spectroscopic techniques and the chemometric methods that are employed in experiments and analysis. Paradoxically, despite the recognition of spectroscopic detection as an effective analytical method, the fundamental process of knowledge retrieval remains both time-intensive and repetitive. In response to this challenge, we first introduced the Spectral Detection and Analysis Based Paper(SDAAP) dataset, which is the first open-source textual knowledge dataset for spectral analysis and detection and contains annotated literature data as well as corresponding knowledge instruction data. Subsequently, we also designed an automated Q&A framework based on the SDAAP dataset, which can retrieve relevant knowledge and generate high-quality responses by extracting entities in the input as retrieval parameters. It is worth noting that: within this framework, LLM is only used as a tool to provide generalizability, while RAG technique is used to accurately capture the source of the knowledge.This approach not only improves the quality of the generated responses, but also ensures the traceability of the knowledge. Experimental results show that our framework generates responses with more reliable expertise compared to the baseline.\nCorresponding author\n# 1 Introduction\nSince Chat-GPT[1] came out of nowhere in late 2022, the concept of Large Language Model (LLM), which is a sophisticated deep learning model based on the Transformer with parameter sizes reaching into the tens of billions, has come back to the forefront of researchers\u2019 minds. The development of LLM has got significant attention due to the extensive knowledge and impressive interaction with humans. What\u2019s more, the incredible ability of LLM about extracting implicit information from prompts with appropriate instruction-following distinguishes itself among most of previous deep learning models. Compared to their previous smaller counterparts, LLMs also demonstrate potent generalisation across various Natural Language Processing (NLP) tasks, illustrating their capacity to resolve unseen or intricate challenges in different domains. Currently, LLMs have demonstrated commendable performance in general domains[2][3][4][5]. Furthermore and naturally, researchers want to introduce LLM in the natural science, a field that places a high demand on logical thinking, to relieve the extremely time-consuming and labour-intensive in practical applications. For instance, spectroscopy-based detection technology is a widely used analytical method with important applications in both the natural sciences and industry. However, for arbitrary samples, the spectroscopic techniques (e.g., Ultraviolet spectrum; Near-Infrared spectrum) and stoichiometric methods (e.g., Preprocessing method; Machine learning method) used in the experiment need to be determined based on past relevant studies because of the poor migration of model. Researchers have to spend a lot of time on the information collection of relevant data in the preliminary stage of the research, which is the most time-consuming and repetitive task in spectral analysis. Now, LLM can learn from large knowledge database and then can be used to provide related information about analyzed object including different points mentioned earlier, which may fully accelerate the procedure of spectral detection in different objects. However, a significant challenge in adapting LLM to the domain of spectral detection is the phenomenon of hallucination[6] concerning specialized knowledge. The general knowledge possessed by these models frequently proves inadequate when applied to specialized fields, primarily due to a deficiency in domain-specific expertise. Figure 1 shows the limitations of Chat-GPT 4 in answering questions in the field of spectral detection without source. Without the support of professional knowledge, it is difficult for general large models such as Chat-GPT 4 to generate accurate answers in professional fields. Given the high demand for reliability in the field of spectral detection, it is essential to enhance the knowledge base associated with LLM. This initiative is critical to ensure that LLM can provide accurate and dependable information to researchers engaged in spectral analysis methodologies. Instruction-tuned LLM, exemplified by InstructGPT[7], serve as the primary method to mitigate LLM into novel domains. This adaptation is achieved by constructing Instruction Fine-Tuning (IFT) data from knowledge database, thereby augmenting the expertise of LLM through supervised fine-tuning (SFT) and improving its interactive capabilities. But Initially, it is important to note that the majority of existing relevant datasets are predominantly concentrated in the domains of bioscience[8] and medicine[9]. In contrast, the field of spectral analysis, which is considered relatively traditional, is characterized by a limited availability of open-source datasets. Most of datasets in this domain is primarily represented by spectral curves of specific sample types and is notably deficient in textual datasets suitable for NLP applications. Consequently, if there is an intention to adapt LLM to a spectral-related domain, it may be necessary to independently develop the requisite scientific datasets.Furthermore, it is indeed more challenging than one might expect for LLM can comprehend specialized knowledge and generate accurate, knowledge-consistent responses solely through SFT method[10] without undergoing additional pretraining. Recent studies have explored the integration of LLM with external application programming interfaces (APIs) to specific domains to improve the precision of model outputs[11][12]. However, these methodologies necessitate operation on external servers, and the associated tasks are both financially burdensome and subject to network limitations, which may impede the progress of LLMs in scientific advancement.Lastly, from the perspective of a natural science researcher, scholars frequently concentrate on the foundational sources of knowledge, such as specific literature or knowledge bases, to facilitate their subsequent exploration of additional relevant information. However, an approach that relies solely on Instruction-tuned for LLM refinement may necessitate further annotation of knowledge sources within the IFT data if the intention is to cite the source of knowledge in the generated responses. This method does not instill a high degree of confidence in the accuracy of the outputs, representing a significant limitation. An alternative strategy involves the use of Retrieval Augmented Generation (RAG)[13], which leverages specialized databases to retrieve pertinent knowledge from data sources and generate responses accordingly. In essence, RAG integrates information retrieval methodologies with the generative capabilities of LLM, thereby addressing the limitations of Instruction-tuned techniques by direct access to data sources through annotations within databases conveniently. Much previous work[14][15] has used a similar approach to source knowledge. In our work, considering that professional literature constitutes a valuable repository of advanced knowledge, research findings, and engineering methodologies, we firstly present the Spectral Detection and Analysis Based Paper (SDAAP) dataset as a foundational source of knowledge. SDAAP encompasses information from relevant publications\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b8ca/b8ca7600-fbc3-40b4-9d75-5b55abe273a5.png\" style=\"width: 50%;\"></div>\nspanning the years 2014 to 2023, with each entry meticulously categorized such as the research object, the spectroscopic techniques employed, and the associated chemometric parameters. In addition to the labeled literature, SDAAP incorporates IFT data derived from the insights of all the publications in dataset; each IFT data also includes the relevant knowledge and its corresponding literature source, facilitating SFT process. The total number of IFT data amounts to more than 20,000 entries. Subsequently, based on SDAAP, we developed a automatic Q&A framework in related domain, which can parse and extract the entity and question formats present in a query, employing the parsing outcomes as query parameters to retrieve pertinent spectral detection knowledge. LLM can then reference this retrieved knowledge to generate a response to the input query. Furthermore, our framework does not rely on instruction-tuning to facilitate the acquisition of new knowledge by LLM, but rather uses it as tool to provide generalizability. Instead, pertinent knowledge can be obtained in a relatively controlled manner through retrieval techniques, thereby enhancing the quality and reliability of the generated responses and ensuring the provision of accurate knowledge sources. In summary, our contributions can be summarized as follows: \u2022We develop the SDAAP dataset, the first systematically organized open-source textual knowledge dataset for spectral analysis and detection. This dataset comprises annotated literature data alongside corresponding knowledge instruction data, thereby addressing a significant gap in textual datasets within the domain and establishing a foundational resource for the subsequent application of LLM in this area. \u2022We designed a knowledge quiz framework based on the SDAAP dataset. The framework can generates high-quality and reliable responses by parsing questions and retrieving the knowledge associated with them, thereby responding the question of various Q&A scenarios within the domain of spectral detection analysis and reducing the repetitive labour. Further details regarding the project are accessible on our GitHub page: coming soon. \u2022Within our question-and-answer framework, we integrate techniques of Instruction tuning and retrieval-augmented generation (RAG). The LLM serves only as a tool to enhance generalizability, while RAG techniques are employed to accurately acquire the source of knowledge, thereby ensuring traceability of knowledge While augmenting quality of response.\n# 2 Datasets\nAs is mentioned previously, the process of retrieving pertinent information within the context of industrial grading and detection through spectral analysis is predominantly characterized by repetitive works, resulting significant\ninefficient use of time. Leveraging LLM, an advanced computer science technology, can effectively streamline these repetitive tasks and release a substantial amount of human resources. In order do this, our research endeavors to incorporate LLM into spectral analysis to provide rapid and dependable responses to queries based on existing knowledge. Recently, the mainstream approach to migrating LLM to a new domain has been to fine-tune LLM based on specialized corpora so that it can be adapted to the corresponding verticals[16]. However, very few existing open-source corpus datasets related to scientific and technical disciplines contain expertise related to spectral analysis. This means that there is a lack of professional datasets tailored for spectral analysis. Given the limited availability of open-source corpus dataset related to spectral analysis, this study opts to independently create a relevant dataset and its corresponding corpus from literature within the professional field. These corpus resources are then applied in our designed framework (see in Section 3) for reliable spectral detection knowledge question and answer (Q&A) tasks. This section outlines the methodology employed for dataset construction, as well as the distribution of labels sourced from the dataset literature Subsequently, a corpus dataset is constructed and made available, which can be utilized for various future research endeavors.\ninefficient use of time. Leveraging LLM, an advanced computer science technology, can effectively streamline these repetitive tasks and release a substantial amount of human resources. In order do this, our research endeavors to incorporate LLM into spectral analysis to provide rapid and dependable responses to queries based on existing\n# 2.1 Paper collection\nIn the context of paper collection focusing on spectral analysis using machine learning methods, the Web of Science was employed as an indexing tool to gather relevant scholarly literature comprising various domains (e.g. food, biology, energy...). Considering the prevalent use of machine learning methods in interdisciplinary research since 2015 approximately, scholarly paper published within the past decade (2013-2023) were selected for analysis. Various keyword combinations were utilized for the search, and subsequent operations, including paper de-duplication by human intervention, were conducted on the aggregated search results. A total of 4461 thesis were obtained through this screening process. It is important to note that all these papers are accessible in full-text format from reputable publishers like Nature, Springer, Elsevier, MDPI, among others. The scope of this resource transcends the English language to encompass a broader spectral detection of related knowledge. In the following Figure 2, an analysis of the publication timeline of these papers reveals a noticeable increase in the application of spectral analysis with machine learning method in detection and other areas. This trend underscores the significance of the research endeavor. Subsequently, a web-scraping tool was utilized to extract content from various publishers and convert it into plain text for further processing.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c337/c337ccba-979d-45a7-aa91-2c6414df3b48.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Distribution of scientific paper dataset</div>\n# 2.2 Labelling and Indexing\nDuring a comprehensive spectral analysis investigation, researchers consistently focus on specific configurations at first. To effectively retrieve essential information from academic literature, it is essential to categorize profession academic papers into distinct labels that can be seamlessly integrated into our spectral Q&A framework, thereby facilitating further research by independent scholars. In the first stage of spectral detection studies, researchers must select specific spectral methods, such as NearInfrared spectrum (NIR), Ultraviolet spectrum (UV), Raman spectroscopy, among others, depending on the characteristics of the object and properties under investigation. Traditionally, this kind of process has been time-consuming\nas researchers are required to repeatedly search for and review numerous papers that are relevant to their study topic. Consequently, we primarily categorize this type of information across all papers within our datasets into the special kind of Label A. Through the automated retrieval facilitated based on Label A by our framework, researchers are able to efficiently access pertinent information related to their own research with different spectral detection method, thereby streamlining the process and minimizing tedious tasks.\nas researchers are required to repeatedly search for and review numerous papers that are relevant to their study topic. Consequently, we primarily categorize this type of information across all papers within our datasets into the special kind of Label A. Through the automated retrieval facilitated based on Label A by our framework, researchers are able to efficiently access pertinent information related to their own research with different spectral detection method, thereby streamlining the process and minimizing tedious tasks. Once the spectral method employed in the experiment and its pertinent specifics have been identified, the selection of the machine learning technique and its associated parameters becomes crucial during the data processing stage. We synthesized and categorized the machine learning information extracted from the papers in our datasets into a class of Label B that is distinct from Label A, including preprocessing techniques, feature processing methods, and models for further analysis. In brief, the extracted labels from any one of papers in our datasets can be divided into two primary sections: Label A is utilizes for summarizing essential information, such as spectral method, and directing researchers towards papers that are most pertinent to their inquiry; Label B is focuses on providing insights into machine learning techniques employed in the paper. These labels aid in pinpointing relevant and valuable literature that pertains to various inquiries posed in the knowledge quiz concerning spectral detection. For instance, we presented a selection of labels derived from some papers within our datasets in the Figure 3 provided below. The methodology involves utilizing Chat-GPT to extract labels from the papers, followed by a manual data cleaning process to rectify any inaccuracies. Notably, majority of labels are only obtained from the abstract section of the papers. Since that abstracts encapsulate the core elements of the literature, encompassing objectives, methods, conclusions, etc., most of labels we need can be extracted from it, and this minimizes copyright issues. In instances where labels cannot be directly derived from the abstract, such as preprocessing methods and machine learning models, the paper is converted into embedding vectors. Subsequently, relevant answers are retrieved based on maximum cosine similarity to extract the corresponding labels.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66ef/66ef7f67-97e9-43ac-b8a8-403cbd798321.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Two categories of labels of scientific paper</div>\n# 2.3 Q&A corpus data based on spectral knowledge\nIt is widely acknowledged that Instruction Fine Tuning (IFT) data is crucial for the implementation of LLM in vertical domain and other Natural Language Processing tasks. Due to the absence of specialized IFT data for spectral detection and analysis, we have developed a framework to create IFT data automatically for each labeled literature in our datasets using Chat-GPT. This framework consists of four distinct stpdf as follows.\nDrawing upon prior knowledge within specific fields, we initially selected commonly encountered questions in practical scenarios. As an example, in the context of spectral analysis and detection research, we focus the category of spectral method, preprocessing methods, feature processing methods, metrics and outcomes, as well as machine learning models. These topics are frequently addressed in spectral analysis studies and are utilized to compile our Q&A corpus data.\n# 2.3.2 Question split and construction\nA template was developed to facilitate the generation of the question component of the corpus. This template consists of two parts: Part A, which serves to define the research object, and Part B, which is utilized to create various question. By inputting the appropriate label of the paper as a prompt, users can efficiently employ Chat-GPT to generate diverse formats of question. This template is designed to be easily adaptable to different research subjects. The structure of the template is illustrated in the Figure 4 below.\n# 2.3.3 Answer generation\nIn the same way as second step, we embed the question and the label corresponding to the answer from paper into the prompt, and ask Chat-GPT to generate a formatted answer based on the information provided. Here, we only constructed limited types of Q&A corpus data mentioned in section 2.3.1. To acquire additional Q&A data, researchers can simply substitute Part B questions corresponding to different labels.\n# 2.3.4 Data Cleaning\nWhile the utilization of Chat-GPT for generating IFT data has become prevalent in cutting-edge research, there are still challenges that require human intervention. For instance, the extensive automated generation of Q&A data using LLM driven approach always encounter obstacles related to inappropriate representation, just like \"This study used (Method A) to (Object B)\". Actually, the utilization of the phrase \"This study\" as the subject in typical question and answer interactions is not recommended. If a substantial portion of the generated data exhibits such issues, it may affect the application of these corpus data in downstream tasks. To address these concerns, manual intervention is employed to rectify similar problems, for instance, by replacing phrases like \" This study used (Method A) to (Object B)\" with \"Related studies show that (Method A) can be used in (Object B)\". Over all the following stpdf, we finally got high quality IFT data, totally including 22305 items based on different paper and each item include one question and matched answer with related knowledge in particular paper. All the corpus data were contained in our dataset and are publicly accessible.\n# 3 Method\nAs mentioned earlier, extensive language models have a broad spectrum of applications across various fields, with one prominent example being the utilization of large models to develop specialized domain question-and-answer (Q&A) systems[17]. By employing fine-tuning techniques, these large models can be further customized using specific domain data to enhance their understanding of vertical domain-specific information. This method significantly enhances the performance of large language models in a multitude of contexts[18]. The refined macro-models serve as modern expert systems, catering to the growing demand for tools that facilitate the rapid and accurate acquisition of pertinent knowledge to optimize time management. Nevertheless, the results produced by fine-tuned macro-language models may not always be dependable in domains where high-confidence outputs are essential, such as in the fields of biology, medicine, and industrial inspection. Furthermore, users often face challenges in accurately tracing the sources of the information provided in the responses, a particularly crucial aspect in scientific and technical research. To address these challenges, we employed a dual strategy involving the refinement of instructional methods and the utilization of RAG (Retrieval-Augmented Generation) to develop a question-and-answer (Q&A) framework centered around a comprehensive language model, as shown in the Figure 5 below. Leveraging a meticulously curated dataset enriched with specialized literature as a foundational knowledge repository, we operationalized the Q&A system within the domain of spectral detection and analysis. The framework, powered by an advanced language model, is adept at processing diverse question formats (qi), retrieving relevant information from the knowledge base, and delivering accurate responses (ri). Given that our knowledge repository is structured around annotated literature sources, each piece of information can be directly linked back to its respective source, enabling transparent tracking of the expertise\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c532/c532459f-9572-41d0-ad7e-4bf4ee6260ae.png\" style=\"width: 50%;\"></div>\nFigure 4: Process for knowledge-based response generation. Step 1: Extract the entity of the input question. Step 2 Acquire the knowledge from the SDAAP dataset. Step 3: Generate a response with acquiredknowledge.\nunderpinning the generated responses. Subsequently, we outline the three key elements comprising the framework entity extraction and question parsing, knowledge retrieval, and response generation.\nFor arbitrary question input, we need to obtain the object of related study indicated in the question at first. As an example, considering \u201cIn the related study on the prediction of sweetness in apples, ... \u201d, the process of extracting entity information related to the inquiry involves identifying the subject of the study (apples) and the specific aspect under investigation (sweetness). Typically, questions contain descriptive elements that pertain to the object of study, facilitating the extraction of relevant content in a more generalized manner.\nFurthermore, it is essential to extract information concerning entities that are pertinent to the orientation of the question. In the context of spectral detection, questions of interest to researchers can be categorized into two main groups: the first pertains to the selection of spectral detection methods in experiments, while the second involves the modeling and calculation procedures post data acquisition. These distinct types of questions necessitate different approaches for addressing them effectively. For questions falling under the first category, which may involve various spectral detection methods for a given study, it is crucial to identify and compare analytical methods from relevant literature, considering factors such as accuracy and experimental conditions. Conversely, questions related to the second category may require additional information on the spectral category used and the specific objective of the inquiry to facilitate knowledge retrieval and response generation.\nFurthermore, it is essential to extract information concerning entities that are pertinent to the orientation of the question. In the context of spectral detection, questions of interest to researchers can be categorized into two main groups: the first pertains to the selection of spectral detection methods in experiments, while the second involves the modeling and calculation procedures post data acquisition. These distinct types of questions necessitate different approaches for addressing them effectively. For questions falling under the first category, which may involve various spectral detection methods for a given study, it is crucial to identify and compare analytical methods from relevant literature, considering factors such as accuracy and experimental conditions. Conversely, questions related to the second category may require additional information on the spectral category used and the specific objective of the inquiry to facilitate knowledge retrieval and response generation. To differentiate between these two question categories, a dichotomous indicator termed \"task_indicator\" is introduced in the questioning process. This indicator helps to categorize questions and determine the need for acquiring additional entity information based on the nature of the inquiry. For the first category of questions, there is no need to obtain any additional entity information; for the second category of questions, in addition to the previously obtained\nTo differentiate between these two question categories, a dichotomous indicator termed \"task_indicator\" is introduced in the questioning process. This indicator helps to categorize questions and determine the need for acquiring additional entity information based on the nature of the inquiry. For the first category of questions, there is no need to obtain any additional entity information; for the second category of questions, in addition to the previously obtained relevant content of the research object, we need to additionally obtain the two parts of the adopted spectral category and the question objective for subsequent knowledge retrieval and response generation. Given its capacity for generalization across various input forms and adeptness at addressing diverse inquiries, LLM-1 was employed in this study to extract\n(1) (2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f545/f545800b-4e43-456e-b97a-95e6c8bd7245.png\" style=\"width: 50%;\"></div>\nentity information of two distinct types. Through fine-tuning the LLM-1 with task-specific data, it was able to efficiently and precisely retrieve the specified entity information. Further elaboration on the experimental results can be found in Section IV.\n# 3.2 Retrieval Datasets\nFollowing the extraction of entities, the query is transformed into a tuple comprising various instances, denoted as (qi, ei, task_indicator), which includes the original query, the identified entities, and the task indicator. During the process of knowledge retrieval, the parameter e_i is utilized to retrieve information from the knowledge repository. By organizing each document in the knowledge repository and extracting the relevant text labels, the challenge of document retrieval can be reframed as determining the similarity between the entity details extracted from the query and the document labels. Consequently, for knowledge retrieval, a cosine similarity retrieval approach based on vector embedding is employed. This involves evaluating the resemblance between the entities extracted from the query and the corresponding labels in the literature to identify highly pertinent literature related to the query subject. In addition to cosine similarity computation, two other methods were chosen as baseline techniques for comparison: BM25, a statistically driven retrieval method[19]; and the bag-of-words (BoW) model[20]. By contrasting the performance of cosine similarity retrieval with these two baseline methods within the knowledge repository, the effectiveness and precision of each method can be evaluated. Detailed experimental data and outcomes are presented in Section IV.\nM(PromptK, Taskindicator, Knowledges) = knowledge\nMoreover, following the identification of relevant literature using cosine similarity retrieval, diverse knowledge extracted from the literature label based on the task_indicator is utilized to address the specific query associated with corresponding label. For instance, in cases where the task_indicator is denoted as the first category of questions, the top 10 literature pieces with the highest cosine scores are selected to present various spectral analysis techniques applicable for examining the research subject. Conversely, when the task_indicator is identified as the second category of questions, pertinent information is derived from the structured labels of the literature, such as preprocessing methodologies, feature processing techniques, and the machine learning models employed. In the other hand, where the queried object in the task_indicator is not predefined within the labels, the entire Abstract section of literature is considered as the source of knowledge. Although the abstract may not contain specific information pertinent to the inquiry at hand, it serves as a valuable repository of literature that could potentially offer pertinent insights for the research.\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec12/ec125b31-12bb-437b-b49d-8b5b5ca018bb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Different approaches to two types of questions</div>\n<div style=\"text-align: center;\">3.3 Generating output according to knowledge and task indicator</div>\n# 3.3 Generating output according to knowledge and task indicator\nFinally, based on the original input query q, the retrieved expertise k, and the template prompt Promptk that splices the above two, we fine-tune a new LLM for generating the response R, viz:\nM(PromptK, q, c) = Rpre M(PromptA, q) = Apre LossAttribute(q) = LossAttribute(Apre, Groundtruth)\nHere, since we have already acquired knowledge matching the question asked as part of the input through the retrieval method, we do not need to infuse the knowledge into the big model by way of fine-tuning, thus circumventing the inherent shortcomings of inaccurate generation and untraceable sources that may result from acquiring knowledge directly from the big model. Our aim here in using LLM to generate responses is to ensure the diversity[21], specialization, and fluency of the generated responses; in other words, we want to generate responses with a tone style that is closer to that of real researchers, which is the part that fine-tuning techniques excel at, as they can generate responses of similar style with a small amount of manually processed data. Therefore, we first randomly extracted some knowledge from the dataset and constructed it into instances (Promptk, q, c) through a framework, and then generated responses using the OpenAI API[1]. We then recruited three annotators with research backgrounds in spectral analysis who manually adjusted the OpenAI-generated responses to more closely resemble the descriptive style of professionals, and we collated all manually processed responses for supervised fine-tuning of the LLM. We used both NLP metrics and AI to evaluate the performance of the fine-tuned model in generating responses and compared it with other baseline models, and the experimental results can be found in Section IV.\n# 4 Experiment\n# 4.1 Detail of Implementation\nOur experimental setup aims to effectively evaluate the performance of our proposed method in the field of searching for papers related to spectral detection. All experiments were performed on a single machine equipped with an NVIDIA RTX 4090 GPU and 24GB of memory to ensure efficient execution and minimal processing time. The experiment is mainly divided into three parts. The first step is to use the fine-tuned Llama2-7b model to extract entities\n(4) (5) (6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0cee/0ceefac6-c068-44b9-bae8-9328a8872f4f.png\" style=\"width: 50%;\"></div>\nfrom the input question and the second step is to use the TF-IDF cosine similarity retrieval method to retrieve the corresponding literature and knowledge in the paper database based on the extracted entity keywords. At last, the fine-tuned Llama3-8b model was used to generate answers based on the literature knowledge. In our experimental setup, we utilized the Low-Rank Adaptation (LoRA) technique to fine-tune both the Llama2-7b and Llama3-8b models, which is a parameter-efficient fine-tuning method that introduces trainable low-rank matrices into each layer of a pre-trained model, significantly reducing the number of trainable parameters and making the fine-tuning process more efficient and less resource-intensive[22]. The approach allows for the adaptation of large pre-trained models to specific tasks while preserving their performance. The hyperparameters set in the experiment is shown in Table 1.\n<div style=\"text-align: center;\">Table 1: Hyperparameters in experiment</div>\nHyperparameters\nValues\nBatch size\n128\nMax epoch\n15\nLearning rate\n3e-4\nLoRA rank\n16\nLoRA alpha\n32\nLoRA dropout\n0.05\nLoRA target modules\nq_proj, v_proj\nBy employing these parameters, we ensured that the LoRA fine-tuning process was both effective and efficient. This enabled the Llama2-7b model to accurately extract entities from input questions and the Llama3-8b model to generate high-quality answers based on the retrieved literature and knowledge. Consequently, we leveraged the strengths of large pre-trained models while maintaining computational feasibility within our hardware constraints, facilitating an effective evaluation of our proposed method in the domain of spectral detection.\nFor entity extraction from the input question, the Llama2-7b model fine-tuned by LoRA is utilized and called LLM1, which is an advanced language model known for its enhanced contextual understanding and language generation capabilities[23]. The task of LLM1 is to identify key entities related to spectral detection and focus the retrieval process on the most relevant aspects. The extracted entities include: the research object in the input question, the spectral method mentioned in the input question, and the question type of the input question. After entity extraction, we searched our database using a cosine similarity-based retrieval method which converts the text into TF-IDF feature vectors and calculates the cosine similarity between the extracted entity vectors and the entity vectors present in the research papers[24], allowing us to retrieve the papers with the highest similarity scores and ensuring that our retrieval process is both targeted and relevant. Two models were used respectively to generate detailed responses: Llama2-13b fine-tuned on LoRA and Llama138b fine-tuned on LoRA, collectively referred to as LLM2. Llama3, particularly the Llama13-8b model, represents a further advancement in language modeling, offering superior performance in both understanding and generating natural language[25]. During the process, the top five retrieved papers are fed into LLM2, which synthesizes relevant content to provide a comprehensive answer to the input question. This step leverages the advanced language generation capabilities of LLM2 to ensure high-quality and contextually accurate responses. The baseline model we used for the experiments is Chat-GPT 3.5 owing to its cost-effectiveness rather than GPT 4.0, which is a mature model known for its powerful language generation capabilities[26]. Chat-GPT 3.5 is pre-trained on a diverse dataset and performs well in natural language understanding and generation tasks, allowing us to benchmark the performance of our system in terms of entity extraction, retrieval accuracy, and response generation quality. The comparative analysis between Chat-GPT 3.5 and our Llama2-based model highlights the progress we have made with our focused approach in the area of searching for papers related to spectral detection.\n# 4.3 Metrics\nIn our experiments, we adopted a comprehensive set of evaluation metrics to evaluate the performance of the models in entity extraction (LLM1) and response generation (LLM2). For the entity extraction task using LLM1, the main evaluation metrics include BLEU[27], ROUGE[28], METEOR[29], BERTScore[30] and accuracy (Acc). BLEU, ROUGE, and METEOR are traditional metrics for evaluating text quality by comparing generated text with reference text. BERTScore leverages BERT\u2019s contextual embeddings to provide a more nuanced measure of similarity between generated and reference text. Accuracy (Acc) measures the overlap between the entities extracted by the model and the reference entities, providing a direct indicator of the correctness of entity extraction. For the response generation task using LLM2, we used similar metrics: BLEU, ROUGE, METEOR, BERTScore, and AI evaluation. BLEU, ROUGE, and METEOR evaluate the generated responses against the reference answers while BERTScore provides a more sophisticated semantic similarity measure. In addition, Chat-GPT4 is used for AI evaluation, which evaluates responses against specific criteria shown in the accompanying Figure 8 .\n# 4.4 Evaluation on LLM for extracting entity\nThe performance of the LLM1 model for extracting question entities was evaluated using several key metrics, including BLEU, ROUGE, METEOR, BERTScore, and accuracy (Acc), for the research object, spectral method, and question type to be extracted in the input question. At the same time, the performance of the Llama2-7b model after Lora fine-tuning and the baseline Chat-GPT model in extracting entities was compared. LLM1 significantly outperformed the baseline Chat-GPT model in all metrics, and the experimental results are shown in Table 2. For extracting research objects from questions, LLM1 achieved higher scores in BLEU, ROUGE1, METEOR, and BERTScore accuracy. Similarly, for extracting spectral methods mentioned in questions, LLM1 showed excellent performance. LLM1 also had significantly higher accuracy on both tasks. In terms of identifying question types, LLM1 consistently provided better results than Chat-GPT, demonstrating its enhanced ability to understand and extract relevant entities in the context of spectral detection.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/732e/732e59de-fa96-4c03-a635-00c3cbd123f6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Detail criteria for AI Evaluation</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/88f2/88f2b914-3a8e-449a-80b7-b3eaef60778d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Example for the process of extracting entity</div>\n<div style=\"text-align: center;\">Table 2: Evaluation results of LLM1 for extracting entity</div>\nModels\nBleu\nRouge1\nmeteor\nBert_score_precision\nAcc\nEt1\nEt2\nQ_type\nEt1\nEt2\nQ_type\nEt1\nEt2\nQ_type\nEt1\nEt2\nQ_type\nEt2\nQ_type\nLlama2-7b\n0.087\n0.423\n0\n0.528\n0.871\n0.978\n0.439\n0.831\n0.943\n0.911\n0.972\n0.997\n0.882\n0.977\nChat-GPT\n0.02\n0.1\n0\n0.093\n0.445\n0.711\n0.040\n0.278\n0.681\n0.874\n0.894\n0.956\n0.584\n0.534\n# 4.5 Evaluation of the knowledge Retrieve\nAfter extracting entities from the input question using LLM1, the retrieval method will retrieve relevant papers based on the entity keywords. The retrieval methods used in the experiment include the Bag of Words model, BM25, and TF-IDF cosine similarity retrieval methods. The Bag of Words (BoW) model is a basic and widely used method in text representation, in which each document is represented as an unordered collection of words without considering grammar and word order, but considering the frequency of words. Although the BoW model is simple, it usually lacks contextual understanding, which leads to low accuracy in complex retrieval tasks. BM25 is a ranking function based on a probabilistic retrieval framework that enhances the traditional TF-IDF method by considering word frequency, document length, and inverse document\nAfter extracting entities from the input question using LLM1, the retrieval method will retrieve relevant papers based on the entity keywords. The retrieval methods used in the experiment include the Bag of Words model, BM25, and TF-IDF cosine similarity retrieval methods.\nThe Bag of Words (BoW) model is a basic and widely used method in text representation, in which each document is represented as an unordered collection of words without considering grammar and word order, but considering the frequency of words. Although the BoW model is simple, it usually lacks contextual understanding, which leads to low accuracy in complex retrieval tasks. BM25 is a ranking function based on a probabilistic retrieval framework that enhances the traditional TF-IDF method by considering word frequency, document length, and inverse document\nfrequency. This method is recognized to be effective in information retrieval tasks. The TF-IDF (Term FrequencyInverse Document Frequency) cosine similarity retrieval method combines TF-IDF weighting with cosine similarity to measure the angle between document vectors in the vector space model. This method effectively captures the relevance between documents and queries, thereby improving the accuracy of retrieval tasks.\n<div style=\"text-align: center;\">Table 3: Evaluation results of different knowledge retrieve methods</div>\nMethods\nAccuracy(%)\nBag-of-Words model\n46.40\nBM25\n80.00\nTF-IDF cosine similarity retrieval method\n82.80\nThe accuracy of each method is calculated as follows: If the papers retrieved using the entity keyword-based retrieval method match the actual papers corresponding to the keyword, the accuracy is 100%. Then the average accuracy of all queries in the database is calculated as the final accuracy of each method. The experimental results are shown in Table 2. As can be seen from the table, the TF-IDF cosine similarity retrieval method has the highest accuracy of 82.80%, followed by BM25, which is 80.00%. The Bag-of-Words model has the lowest accuracy of 46.40%. These results demonstrate the superior performance of TF-IDF cosine similarity retrieval in accurately retrieving relevant papers based on the extracted entity keywords.\nBy adopting these methods, we are able to compare their retrieval performance and determine that the TFcosine similarity retrieval method outperforms other methods in terms of accuracy.\n# 4.6 Evaluation on LLM for generating response\nThe performance of LLM2 models (particularly Llama3-8b and Llama2-13b) was evaluated using several key metrics, including BLEU, ROUGE, METEOR, BERTScore, and AI evaluation, and the experimental results are shown in Table 4. These results are compared with the baseline Chat-GPT model. Both Llama2-13b and Llama3-8b models were fine-tuned using the LoRA technique.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/551e/551eb22c-3291-4939-8773-470955a0c018.png\" style=\"width: 50%;\"></div>\n\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6644/6644a6ed-4ccf-464b-badc-3660cd0d111e.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\">Figure 10: Example for generating response</div>\nLlama3-8b achieved the highest scores across most metrics, with a BLEU score of 0.304, ROUGE score of 0.558, METEOR score of 0.503, BERTScore of 0.881, and an AI evaluation score of 4.2. Llama2-13b followed with a BLEU score of 0.158, ROUGE score of 0.385, METEOR score of 0.344, BERTScore of 0.861, and an AI evaluation score of 3.5. The baseline Chat-GPT model scored significantly lower across all metrics, with a BLEU score of 0.001, ROUGE score of 0.065, METEOR score of 0.033, BERTScore of 0.834, and an AI evaluation score of 2.8.\n<div style=\"text-align: center;\">Table 4: Evaluation results of LLM2 for generating response</div>\nModels\nBleu\nrouge\nmeteor\nBert_score\nAI evaluate\nLlama3-8b\n0.304\n0.558\n0.503\n0.881\n4.2\nLlama2-13b\n0.158\n0.385\n0.344\n0.861\n3.5\nChat-GPT\n0.001\n0.065\n0.033\n0.834\n2.8\nThese results indicate that the Llama3-8b model, fine-tuned with LoRA, performed the best in generating responses demonstrating superior language generation capabilities and relevance to the spectral detection context compared to both Llama2-13b and the baseline Chat-GPT model.\n# 5 Conclusion\nIn this paper, to enable researchers in the field of spectral detection to retrieve spectral related knowledge faster and more accurately, we designed a fast and reliable spectral detection question answering system based on the SDAAP dataset and a large language model. Considering that professional literature is a valuable treasure trove of advanced knowledge, research results, and engineering methods, we first proposed the Spectral Detection and Analysis Paper (SDAAP) dataset as a basic knowledge source. Subsequently, we developed an automatic question answering framework in related fields based on SDAAP, which uses the Llama2-7b model fine-tuned by LoRA to parse and extract entities and question formats present in the query, and uses the parsed results as query parameters to retrieve relevant spectral detection knowledge through the cosine similarity retrieval method. Finally, the Llama3-8b fine-tuned by LoRA can refer to these retrieved knowledge to generate responses to input queries. Through experiments on the spectral detection knowledge question answering dataset we proposed, the two fine-tuned large language models used to extract entities and generate answers achieved higher accuracy and reliability in generating responses. Both models performed well in commonly used natural language indicator evaluations, with evaluation scores far exceeding Chat-GPT. They can accurately and quickly extract entities and generate answers, respectively, highlighting the domain adaptation potential of LLM in professional fields.\n# 6 Limitation\n\u2022Datasets: A major limitation of this study is the limited number of question-answer pairs and the reliance on a limited amount of literature data, which may inhibit the applicability and validity of the model. On the other hand, the manual cleaning process applied to the data could still be problematic. To address this issue, future research could focus on expanding the task to generate more question-answer pairs; or further structuring the relevant literature database and constructing knowledge graphs to enhance the interconnectivity of the IFT dataset and improve the performance of the model in the vertical domain. \u2022Evaluation methodology: In addition to the Type 1 questions, we select metrics such as Bleu and Rouge to measure the similarity between the model output and the golden facts. However, these metrics may not be well suited for model evaluation in verticals, and relying solely on the similarity of the output to the golden facts may not be a valid reflection of the quality of the generated answers. It is worth noting that in the first category of questions, metrics such as Bleu still yield high scores even if the answers are wrong on key spectral species. To address this issue, in the future we will consider introducing entirely new evaluation metrics or using expert evaluation for some of the responses.\n# References\n1] OpenAI. Chatgpt. https://chat.openai.com, 2022. 2] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. 2023.\n[1] OpenAI. Chatgpt. https://chat.openai.com, 2022. [2] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. 2023.\n[3] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2024. [4] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. [5] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. [6] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. Towards mitigating llm hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1827\u20131843, 2023. [7] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022. [8] Manojit Bhattacharya, Soumen Pal, Srijan Chatterjee, Sang-Soo Lee, and Chiranjib Chakraborty. Large language model (llm) to multimodal large language model (mllm): A journey to shape the biological macromolecules to biological sciences and medicine. Molecular Therapy-Nucleic Acids, 2024. [9] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930\u20131940, 2023. [10] Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, and Mingyi Hong. Getting more juice out of the sft data: Reward learning from human demonstration improves sft for llm alignment. arXiv preprint arXiv:2405.17888, 2024. [11] Yafeng Gu, Yiheng Shen, Xiang Chen, Shaoyu Yang, Yiling Huang, and Zhixiang Cao. Apicom: Automatic api completion via prompt learning and adversarial training-based data augmentation. In Proceedings of the 14th Asia-Pacific Symposium on Internetware, pages 259\u2013269, 2023. [12] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. [13] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey. arXiv preprint arXiv:2402.19473, 2024. [14] Kamal Raj Kanakarajan, Bhuvana Kundumani, and Malaikannan Sankarasubbu. Bioelectra: pretrained biomedical text encoder using discriminators. In Proceedings of the 20th workshop on biomedical language processing, pages 143\u2013154, 2021. [15] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. NPJ digital medicine, 4(1):86, 2021. [16] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565, 2023. [17] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023. [18] Yanrui Du, Sendong Zhao, Yuhan Chen, Rai Bai, Jing Liu, Hua Wu, Haifeng Wang, and Bing Qin. The calla dataset: Probing llms\u2019 interactive knowledge acquisition from chinese medical literature. arXiv preprint arXiv:2309.04198, 2023. [19] Ammar Ismael Kadhim. Term weighting for feature extraction on twitter: A comparison between bm25 and tf-idf. In 2019 international conference on advanced science and engineering (ICOASE), pages 124\u2013128. IEEE, 2019. [20] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. Understanding bag-of-words model: a statistical framework. International journal of machine learning and cybernetics, 1:43\u201352, 2010. [21] Haochun Wang, Sendong Zhao, Zewen Qiang, Zijian Li, Nuwa Xi, Yanrui Du, MuZhen Cai, Haoqiang Guo, Yuhan Chen, Haoming Xu, et al. Knowledge-tuning large language models with structured medical knowledge bases for reliable response generation in chinese. arXiv preprint arXiv:2309.04175, 2023. [22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [24] Shahzad Qaiser and Ramsha Ali. Text mining: use of tf-idf to examine the relevance of words to documents. International Journal of Computer Applications, 181(1):25\u201329, 2018. [25] AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024. [26] Colin G West. Ai and the fci: Can chatgpt project an understanding of introductory physics? arXiv preprint arXiv:2303.01067, 2023. [27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002. [28] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381, 2004. [29] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372, 2005. [30] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The emergence of Large Language Models (LLMs) has introduced innovative methodologies across diverse fields, including the natural sciences. However, the process of knowledge retrieval in spectral analysis remains time-intensive and repetitive, creating a need for an automated solution.",
            "purpose of benchmark": "The benchmark is intended to enable comparisons of different models and improve the efficiency and reliability of knowledge retrieval in spectral analysis."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of efficiently retrieving relevant knowledge for spectral analysis and detection, which is traditionally a manual and repetitive task.",
            "key obstacle": "Existing benchmarks are limited in their focus on spectral analysis, leading to a lack of specialized datasets that can effectively support LLM adaptation in this domain."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need to streamline the knowledge retrieval process in spectral analysis, which has historically been labor-intensive.",
            "opinion": "The authors believe that the benchmark will significantly enhance the capability of LLMs in providing accurate and reliable information in the field of spectral analysis.",
            "innovation": "The benchmark introduces the SDAAP dataset, which is the first open-source textual knowledge dataset specifically for spectral analysis, addressing a significant gap in existing resources.",
            "benchmark abbreviation": "SDAAP"
        },
        "dataset": {
            "source": "The dataset was created by collecting relevant scholarly literature on spectral analysis from the Web of Science, focusing on papers published from 2014 to 2023.",
            "desc": "The SDAAP dataset contains over 20,000 entries of annotated literature data and corresponding knowledge instruction data, making it suitable for spectral analysis tasks.",
            "content": "The dataset includes textual data from scientific papers, focusing on spectroscopic techniques and chemometric methods.",
            "size": "20,000",
            "domain": "Spectral Analysis",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "BLEU, ROUGE",
            "aspect": "The metrics measure the quality and accuracy of the generated responses in relation to the reference answers.",
            "principle": "The metrics were chosen based on their ability to evaluate text quality and similarity, which are crucial for assessing the performance of the Q&A system.",
            "procedure": "Model performance is evaluated by comparing generated responses against reference answers using the selected metrics."
        },
        "experiments": {
            "model": "The models tested include Llama2-7b and Llama3-8b, both fine-tuned for entity extraction and response generation.",
            "procedure": "The experimental setup involved entity extraction using Llama2-7b and response generation using Llama3-8b, with a focus on evaluating their performance in spectral detection tasks.",
            "result": "The Llama3-8b model achieved the highest scores in BLEU, ROUGE, METEOR, and AI evaluation metrics, outperforming the baseline Chat-GPT model.",
            "variability": "Variability in results was accounted for through multiple trials and by comparing performance against various baseline models."
        },
        "conclusion": "The study concludes that the developed benchmark and associated framework significantly improve the speed and accuracy of knowledge retrieval in spectral detection, demonstrating the potential of LLMs in specialized fields.",
        "discussion": {
            "advantage": "The benchmark provides a systematic and organized resource for spectral analysis, facilitating more efficient research and knowledge retrieval.",
            "limitation": "A major limitation is the reliance on a limited number of question-answer pairs and literature, which may affect the model's applicability.",
            "future work": "Future research could focus on expanding the dataset and improving the evaluation methodologies to enhance the model's performance."
        },
        "other info": {
            "info1": "The SDAAP dataset is publicly accessible for further research.",
            "info2": {
                "info2.1": "The framework integrates techniques of Instruction tuning and retrieval-augmented generation.",
                "info2.2": "The models used in the experiments were fine-tuned using the LoRA technique."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The emergence of Large Language Models (LLMs) has introduced innovative methodologies across diverse fields, including the natural sciences."
        },
        {
            "section number": "2.3",
            "key information": "The benchmark introduces the SDAAP dataset, which is the first open-source textual knowledge dataset specifically for spectral analysis."
        },
        {
            "section number": "4.1",
            "key information": "The Llama3-8b model achieved the highest scores in BLEU, ROUGE, METEOR, and AI evaluation metrics, outperforming the baseline Chat-GPT model."
        },
        {
            "section number": "10.2",
            "key information": "Future research could focus on expanding the dataset and improving the evaluation methodologies to enhance the model's performance."
        },
        {
            "section number": "11",
            "key information": "The study concludes that the developed benchmark and associated framework significantly improve the speed and accuracy of knowledge retrieval in spectral detection, demonstrating the potential of LLMs in specialized fields."
        }
    ],
    "similarity_score": 0.7375024535576056,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/A Quick, trustworthy spectral knowledge Q&A system leveraging retrieval-augmented generation on LLM.json"
}