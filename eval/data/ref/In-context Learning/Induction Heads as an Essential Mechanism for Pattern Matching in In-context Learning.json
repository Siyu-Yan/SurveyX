{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.07011",
    "title": "Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning",
    "abstract": "Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a fewshot ICL setting. We analyse two state-of-theart models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model\u2019s ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present finegrained evidence for the role that the induction mechanism plays in ICL.",
    "bib_name": "crosbie2024inductionheadsessentialmechanism",
    "md_text": "# Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning\nJoy Crosbie\u22c6 University of Amsterdam joy.m.crosbie@gmail.com Ekaterina Shutova University of Amsterdam e.shutova@uva.nl\nsbie\u22c6 Amsterdam @gmail.com Ekaterina Shutova University of Amsterdam e.shutova@uva.nl\nJoy Crosbie\u22c6 University of Amsterdam joy.m.crosbie@gmail.com\nAbstract\n# Abstract\nLarge language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a fewshot ICL setting. We analyse two state-of-theart models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model\u2019s ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present finegrained evidence for the role that the induction mechanism plays in ICL.\n# 1 Introduction\nLarge language models have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL) (Brown et al., 2020; Touvron et al., 2023b). In ICL, the model receives a demonstration context and a query question as a prompt for prediction. Unlike supervised learning, ICL utilises the pretrained model\u2019s capabilities to recognise and replicate patterns within the demonstration context, thereby enabling accurate predictions for the query without the use of gradient updates. Given the success and wide applicability of ICL, much recent research was dedicated to better understanding its properties. Several works explored its connections to gradient descent (Dai et al., 2023;\n\u22c6Corresponding author.\nEkaterina Shutova University of Amsterdam e.shutova@uva.nl\nVon Oswald et al., 2023), suggesting that ICL functions as an implicit form of fine-tuning at inference time. Other works investigated factors influencing ICL, showing that it is driven by the distributions of the training data (Chan et al., 2022) and scales with model size, revealing new abilities at certain parameter thresholds (Brown et al., 2020; Wei et al., 2022). During inference, the properties of demonstration samples also affect ICL performance , with aspects such as the label space, input distribution and input-label pairing playing a crucial role (Min et al., 2022; Webson and Pavlick, 2022). While this work identified interesting properties of ICL and effective ICL prompting strategies, a comprehensive understanding of its operational mechanisms within the models is still lacking. Our paper aims to fill this gap, by directly investigating the internal model computations that enable ICL. Our work is inspired by recent research in the field of mechanistic interpretability, which aims to reverse engineer the \"algorithm\" by which Transformer models process information (Geva et al., 2023; Olah et al., 2020; Wang et al., 2022). As a significant milestone in this area, Elhage et al. (2021) demonstrated the existence of induction heads in Transformer LMs. These heads scan the context for previous instances of the current token using a prefix matching mechanism, which identifies if and where a token has appeared before. If a matching token is found, the head employs a copying mechanism to increase the probability of the subsequent token, facilitating exact or approximate repetition of sequences and embodying the algorithm \"[A][B]...[A] \u2192[B]\". Building on this foundation, Olsson et al. (2022) hypothesised that induction heads are capable of abstract pattern matching and conducted a qualitative analysis of attention patterns observed in an example from an abstract classification task. They observed that these heads, when focused on the final token, tended to attend to previous instances of the correct label.\nHowever, they did not conduct any experiments to verify this. In a related study, Bansal et al. (2023) investigated which heads were important for NLP tasks and found some degree of overlap between these heads and those that had high scores associated with induction. However, they did not directly assess the contribution of induction heads to ICL performance of the models in these tasks.\nTo the best of our knowledge, our paper is the first one to directly, empirically link the specific computations performed by induction heads to measurable improvements in ICL performance. We investigate the role of induction heads as an underlying mechanism of few-shot ICL across a range of tasks: (1) abstract pattern recognition tasks; (2) popular NLP tasks. We focus on two state-of-theart models: Llama-3-8B (Dubey et al., 2024) and InternLM2-20B (Cai et al., 2024). We first compute prefix matching scores for all of their attention heads, and then perform head ablation experiments by removing 1% and 3% of the heads with the highest scores. For the NLP tasks, we additionally conduct experiments with semantically unrelated labels to encourage the models to rely on ICL for task completion. To confirm our hypothesis regarding the specific role of the induction mechanisms in these heads, we further perform attention knockout experiments, where we selectively inhibit each token\u2019s ability to attend to any tokens that directly followed tokens similar to the current token. This effectively simulates a loss of function in the prefix matching and copying mechanisms.\nWe find that the ablation of induction heads results in a substantial decrease in ICL performance, much more so than when an equivalent percentage of random heads are ablated. Additionally, when we block the induction attention pattern in these heads, performance drops to levels comparable to or worse than those seen with full head ablations. The latter confirms not merely the importance of induction heads for ICL, but also the specific role of the prefix matching and copying mechanism, retracing its application at the token level. Our work is thus the first to provide comprehensive experimental evidence, demonstrating (1) the essential role of induction heads in few-shot ICL in largescale, widely-used LMs and real-world tasks and (2) that induction heads utilise a \"fuzzy\" version of the prefix matching and copying mechanisms to enable pattern matching. Our code and data are\npublicly available.1\n# 2 Related Work\n# 2.1 Factors that influence ICL performance\nICL performance is influenced by various factors, both during the pretraining and inference stage. During pretraining, ICL abilities emerge when training data displays specific distributional properties, such as items frequently appearing in clusters and the presence of many infrequently occurring classes (Chan et al., 2022). Additionally, the scalability of these abilities correlates directly with model size (Brown et al., 2020), and emergent capabilities become apparent at particular scales of pretraining or beyond certain parameter thresholds (Wei et al., 2022; Lu et al., 2023). In the inference stage, the characteristics of demonstration examples, such as label space, input distribution, and input-label pairing format, significantly impact ICL performance (Min et al., 2022). Additionally, models are more affected by label choice than by instruction semantics (Webson and Pavlick, 2022), and larger models can adapt to input-label mappings even with flipped or unrelated labels (Wei et al., 2023b). Further, fine-tuning on semantically unrelated in-context input-label pairs enhances performance on new ICL tasks (Wei et al., 2023a).\n# 2.2 Mechanistic Interpretability\nResearch in mechanistic interpretability seeks to explain model behaviours in terms of their internal components (Olah et al., 2020). Elhage et al. (2021) demonstrated that small, attention-only Transformers can be deconstructed into circuits\u2014specific sub-graphs within the model\u2019s computational graph that perform distinct tasks (Wang et al., 2022). By analysing these circuits, Elhage et al. demonstrated the existence of induction heads. Circuit discovery has since led to the localisation of other behaviours within Transformer models (Meng et al., 2022; Vig et al., 2020; Wang et al., 2022). A common technique is mean ablation, where components\u2019 activations are replaced with their average activation value across a reference distribution to determine whether disrupting this specific component hinders the model\u2019s ability to perform the task. Wang et al. (2022) studied indirect object identification in GPT2 small (Radford et al., 2019), by obscuring the indirect object with random names, averaging these\n1Link will be added in a later version.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a12/4a121943-8ee4-453f-be45-6346a0593325.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: In the sequence \u201c...vintage cars ... vintage\u201d, an induction head identifies the initial occurrence of \u201cvintage\u201d, attends to the subsequent word \u201ccars\u201d for prefix matching, and predicts \u201ccars\u201d as the next word through the copying</div>\nactivations, and replacing the target activations during inference. This identified specific attention heads crucial for the task. Another recent technique\u2014attention knockout\u2014 systematically disables specific attention weights to assess their impact on model outputs (Geva et al., 2023). Geva et al. (2023) explored how factual knowledge is extracted from LLMs during inference, by blocking the final position from attending to both subject and non-subject positions across specific layers. They revealed that essential information from non-subject positions is accessed first. Additionally, Wang et al. (2023) blocked label tokens from accessing prior demonstration text in shallow layers, and demonstrated that label words gather information early in forward propagation.\n# 2.3 Induction heads & ICL\nBuilding on the work of Elhage et al. (2021), Olsson et al. (2022) provided initial evidence that induction heads might be an underlying mechanism of ICL, defined as the gradual reduction of loss with increasing token indices. They observed that induction heads emerge early in training, coinciding with significant improvements in the model\u2019s ability to learn from context. They also found that modifying or removing these heads impairs ICL performance, assessed by a heuristic measure of loss reduction. In contrast, we employ a few-shot prompting ICL setting and evaluate ICL performance directly through the model\u2019s accuracy on specific tasks. Using head importance pruning, Bansal et al. (2023) demonstrated that up to 70% of attention heads and 20% of feed forward layers can be removed from the OPT-66B model (Zhang et al., 2022) with minimal performance decline across various downstream tasks. They uncovered a notable consistency in the relevance of certain attention heads for ICL, irrespective of the task or the number of few-shot examples. They investigated whether these crucial heads were induction heads,\nfinding some degree of overlap with those that had high prefix matching scores. Our research extends these findings by not only identifying induction heads but also conducting ablation studies to directly evaluate their impact on ICL performance in diverse tasks.\n# 3 Background\nFollowing the framework established by Elhage et al. (2021) and further discussed by Ren et al. (2024), the operations within a multi-head attention (MHA) layer, comprising H attention heads, can be reformulated as follows: \ufffd \ufffd\n(1)\nwhere x = [xT 1 , xT 2 , ..., xT N]T \u2208RN\u00d7d represents the sequence of embeddings, with each xi = tiWe \u2208R1\u00d7d denoting the embedding of the i-th input token ti. We \u2208R|V|\u00d7d denotes the embedding matrix over the vocabulary V. Wh q , Wh k, Wh v \u2208Rd\u00d7dh denote the query, key, and value parameter matrices of the h-th attention head. Wo \u2208Rd\u00d7d represents the output transformation of the MHA layer, which can be deconstructed as Wo = [(W1 o)T (W2 o)T ...(WH o )T ], where Wh o \u2208Rdh\u00d7d. Mh is a casual attention mask, which ensures each position can only attend to preceding positions (i.e. Mh rc = \u2212\u221e\u2200c > r and zero otherwise). In Equation (1), Wh QK = Wh q (Wh k)T , termed the Query-Key (QK) circuit, calculates the attention pattern of the h-th head. The matrix Wh OV = Wh v(Wh o)T , termed the Output-Value (OV) circuit, determines each head\u2019s independent output for the current token. Leveraging this decomposition, Elhage et al. (2021) discovered a distinct behaviour in certain attention heads, which they named induction heads. This behaviour emerges when these\nheads process sequences of the form \"[A] [B] ... [A] \u2192\". In these heads, the QK circuit directs attention towards [B], which appears directly after the previous occurrence of the current token [A]. This behaviour is termed prefix matching. The OV circuit subsequently increases the output logit of the [B] token, termed copying. An overview of this mechanism is shown in Figure 1. The authors showed that these heads can operate on different distributions as long as the abstract property of repeated sequences\u2019 likelihood holds true.\n# 4 Methods\n# 4.1 Models\nWe utilise two recently developed open-source models, namely Llama-3-8B (Dubey et al., 2024) and InternLM2-20B (Cai et al., 2024), both of which are based on the original Llama (Touvron et al., 2023a) architecture. These models feature grouped-query attention mechanisms (Ainslie et al., 2023) to enhance efficiency. Llama-3-8B, comprises 32 layers, each with 32 attention heads and it uses a query group size of 4 attention heads. It has shown superior performance compared to its predecessors, even the larger Llama-2 models. InternLM2-20B, featuring 48 layers with 48 attention heads each, uses a query group size of 6 attention heads. We selected InternLM2-20B for its exemplary performance on the Needle-in-theHaystack2 task, which assesses LLMs\u2019 ability to retrieve a single critical piece of information embedded within a lengthy text. This mirrors the functionality of induction heads, which scan the context for prior occurrences of a token to extract relevant subsequent information.\n# 4.2 Identifying Induction Heads\nTo identify induction heads within models, we measure the ability of all attention heads to perform prefix matching on random input sequences.3 We follow the task-agnostic approach to computing prefix matching scores outlined by Bansal et al. (2023). We argue that focusing solely on prefix matching scores is sufficient for our analysis, as high prefix matching cores specifically indicate induction 2https://github.com/gkamradt/LLMTest_ NeedleInAHaystack 3In this work, the term \"induction heads\" refers to what we define as behavioural induction heads, not mechanistic ones. A true induction head must be verified mechanistically; however, our analysis employs prefix-matching scores as a proxy. We will continue to use the term \"induction heads\" for simplicity throughout the rest of the paper.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e42/4e425550-3725-4dfe-947b-84912a5aa372.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Prefix matching scores for Llama-3-8B.</div>\nheads, while less relevant heads tend to show high copying capabilities (Bansal et al., 2023). We generate a sequence of 50 random tokens, excluding the 4% most common and least common tokens. This sequence is repeated four times to form the input to the model. The prefix matching score is calculated by averaging the attention values from each token to the tokens that directly followed the same token in earlier repeats. The final prefix matching scores are averaged over five random sequences. The prefix matching scores for Llama-3-8B are shown in Figure 2. For IntermLM2-20B, we refer to Figure 8 in Appendix A.1. Both models exhibit heads with notably high prefix matching scores, distributed across various layers. In the Llama-38B model, ~3% of the heads have a prefix matching score of 0.3 or higher, indicating a degree of specialisation in prefix matching, and some heads have high scores of up to 0.98.\n# 4.3 Head Ablations\nTo investigate the significance of induction heads for a specific ICL task, we conduct mean-ablations of 1% and 3% of the heads with the highest prefix matching scores, aiming to introduce noise and disrupt their function. We utilise premises from the MNLI (Williams et al., 2018) training set, which encompasses data from ten diverse genres of written and spoken English, thereby capturing a wide spectrum of language complexity. For each model, premises are concatenated to create 500 samples, each comprising of 3000 tokens, exceeding the longest prompt used in our experiments. Each sample is passed through the model, during which head activations are recorded with the Pyvene (Wu et al., 2024) library. These activations are then averaged to generate a mean embedding vector for each head. During inference, the activations of the ablated heads are replaced with the mean activations by truncating them to the length of the current prompt.\nAs a control condition, we also randomly ablate 1% and 3% of the model\u2019s heads, which are selected following the layer distribution of the previously ablated induction heads. For example, if three induction heads were ablated in layer 24, we randomly select three heads from the same layer for ablation. This method allows us to control for layer-specific effects, ensuring that any observed differences in performance are attributable to the function of the induction heads themselves, rather than differences in the computational roles or importance of the various layers.\nTo better understand the function of induction heads in few-shot ICL, we conduct attention knockout experiments. We hypothesise that when induction heads process a structured dataset, they generate a specific \"induction pattern\" by consistently attending back to tokens that previously followed similar ones. Drawing on the methodology of Geva et al. (2023), we empirically test whether induction heads exhibit this pattern by blocking tokens from attending to tokens that previously followed similar ones. We define two positions r, c \u2208[1, N] where r < c. To inhibit attention, we prevent xh c from attending to xh r in head h by modifying the attention weights in that layer (Eq. 1) as follows: Mh rc = \u2212\u221e. This restricts the current position from obtaining information from the blocked positions for that particular head. By comparing the effects of completely removing a head with merely blocking its pattern-directed attention, we can gather empirical evidence suggesting whether these heads predominantly implement this specific induction pattern.\n# 5 Ablation Experiments: Abstract Pattern Recognition Tasks\nWe use a modified version of Eleuther AI\u2019s Language Model Evaluation Harness (Gao et al., 2023) for our experiments. While the default framework randomly samples different in-context examples for each query, we adapted it to ensure a balanced sampling approach, in terms of both the number of examples and classes in the dataset. Aside from this modification, all default settings were maintained. For predictions, we utilise the harness\u2019s multiplechoice method, which identifies the target word assigned the largest log probability among all target words. We report accuracy averaged over three\nTask\nPattern (Foo)\nNo pattern (Bar)\nRepetition\nX H X H\nZ E W F\nQ A Q A\nF I O E\nRecursion\nV D D D\nN O W T\nF L L L\nP Z X F\nCentre-embedding\nX B V B X\nL Q I F P\nV G M G V\nH M T B A\nWordSeq 1 (binary)\ngrape shark\ncouch soldier\nfig panda\nnightstand writer\nFigure 3: Each letter-sequence dataset features examples following the respective patterns labelled \"Foo\" and random sequences labelled \"Bar\". For word sequence tasks, examples include pairs of semantically categorised words.\nrandom seeds for example selection. For random head ablation, we report accuracy over three ablation seeds, totalling nine runs.\nDatasets We first conduct ablation experiments on abstract pattern recognition tasks. In these tasks, we expect the model to rely on ICL more so than leveraging information from its training data or weights. The first set of tasks focus on recognising predefined patterns in sequences of letters. In the REPETITION task, the model must determine whether a sequence of four letters consists of repeating two tokens (e.g. A B A B). The RECURSION task involves detecting whether four-lettersequences contain recursion (e.g. A B B B). In the CENTRE-EMBEDDING task, whether a sequence of five letters contains a centre-embedding (e.g. G L O L G). If the specific pattern is not present in any task, the sequence is a random sequence of letters. In the second set of tasks, the model is required to label sequences of words that contain a pattern rarely seen in natural language data (i.e. not a common semantic relation). We pair specific semantic categories of words\u2014such as <fruit, animal>, <furniture, profession> (WORDSEQ 1) and <vegetable, vehicle>, <body part, instrument> (WORDSEQ 2)\u2014 and conduct a binary or four-way classification of these pairs. Figure 3 presents examples from the letter-sequence tasks and one of the word-sequence tasks.\nExperimental setup We make use of the semantically unrelated labels \"Foo\" and \"Bar\" for all binary tasks. For the four-way tasks, we additionally use \"Mur\" and \"Res\". For the letter-sequence datasets, we randomly generate sequences of the specified length, ensuring each dataset contains 500 examples with class balance. For the binary\nLlama-3-8B\nTask\nShot\nFull\n1% ind.\n1% ind.\n1% rnd.\n3% ind.\n3% ind.\n3% rnd.\nmodel\nheads\npattern\nheads\nheads\npattern\nheads\nRepetition\n5\n67.3\n60.9 (-6.4)\n62.5 (-4.8)\n64.9 (-2.4)\n50.0 (-17.3)\n53.2 (-14.1)\n62.6 (-4.7)\n10\n91.3\n59.7 (-31.6)\n65.3 (-26.0)\n85.9 (-5.4)\n51.5 (-39.8)\n58.7 (-32.6)\n79.7 (-11.6)\nRecursion\n5\n67.8\n63.0 (-4.8)\n61.5 (-6.3)\n66.1 (-1.7)\n52.0 (-15.8)\n55.1 (-12.7)\n68.0 (+0.2)\n10\n91.5\n67.9 (-23.6)\n68.7 (-22.8)\n86.5 (-5.0)\n52.9 (-38.6)\n58.9 (-32.6)\n84.6 (-6.9)\nCentre-embedding\n5\n58.8\n54.9 (-3.9)\n55.0 (-3.8)\n57.2 (-1.6)\n49.1 (-9.7)\n50.5 (-8.3)\n56.4 (-2.4)\n10\n80.4\n53.0 (-27.4)\n56.5 (-23.9)\n74.6 (-5.8)\n50.7 (-29.7)\n52.3 (-28.1)\n71.5 (-8.9)\nWordSeq 1 (binary)\n5\n83.1\n72.1 (-11.0)\n71.8 (-11.3)\n82.1 (-1.0)\n51.6 (-31.5)\n56.9 (-26.2)\n78.8 (-4.3)\n10\n99.4\n96.2 (-3.2)\n97.3 (-2.1)\n99.3 (-0.1)\n69.4 (-30.0)\n82.2 (-17.2)\n98.3 (-1.1)\nWordSeq 2 (binary)\n5\n77.9\n65.4 (-12.5)\n65.2 (-12.7)\n76.8 (-1.1)\n52.0 (-25.9)\n55.7 (-22.2)\n72.4 (-5.5)\n10\n99.4\n94.9 (-4.5)\n96.4 (-3.0)\n98.8 (-0.6)\n67.2 (-32.2)\n81.1 (-18.3)\n97.7 (-1.7)\nWordSeq 1 (4-way)\n20\n78.3\n55.2 (-23.1)\n59.8 (-18.5)\n76.5 (-1.8)\n40.8 (-37.5)\n45.2 (-33.1)\n71.4 (-6.9)\nWordSeq 2 (4-way)\n20\n81.3\n55.9 (-25.4)\n59.8 (-21.5)\n76.0 (-5.3)\n42.3 (-39.0)\n47.5 (-33.8)\n68.6 (-12.7)\nInternLM2-20B\nTask\nShot\nFull\n1% ind.\n1% ind.\n1% rnd.\n3% ind.\n3% ind.\n3% rnd.\nmodel\nheads\npattern\nheads\nheads\npattern\nheads\nRepetition\n5\n68.7\n63.3 (-5.4)\n62.0 (-6.7)\n67.6 (-1.1)\n62.4 (-6.3)\n58.5 (-10.2)\n64.4 (-4.3)\n10\n88.1\n73.4 (-14.7)\n72.1 (-16.0)\n86.9 (-1.2)\n72.5 (-15.6)\n61.2 (-26.9)\n84.1 (-4.0)\nRecursion\n5\n68.1\n62.1 (-6.0)\n59.9 (-8.2)\n67.3 (-0.8)\n59.5 (-8.6)\n55.3 (-12.8)\n65.3 (-2.8)\n10\n88.5\n70.3 (-18.2)\n69.9 (-18.6)\n87.1 (-1.4)\n67.5 (-21.0)\n57.1 (-31.4)\n85.1 (-3.4)\nCentre-embedding\n5\n59.5\n56.9 (-2.6)\n56.1 (-3.4)\n58.6 (-0.9)\n55.3 (-4.2)\n54.1 (-5.4)\n56.7 (-2.8)\n10\n75.3\n60.1 (-15.2)\n58.5 (-16.8)\n74.3 (-1.0)\n60.2 (-15.1)\n54.3 (-21.0)\n70.7 (-4.6)\nWordSeq 1 (binary)\n5\n76.8\n66.5 (-10.3)\n64.9 (-11.9)\n76.1 (-0.7)\n61.5 (-15.3)\n56.8 (-20.0)\n77.6 (+0.8)\n10\n95.6\n90.0 (-5.6)\n88.2 (-7.4)\n94.6 (-1.0)\n89.1 (-6.5)\n83.3 (-12.3)\n96.3 (+0.7)\nWordSeq 2 (binary)\n5\n83.3\n74.2 (-9.1)\n70.1 (-13.2)\n82.5 (-0.8)\n68.3 (-15.0)\n64.1 (-19.2)\n83.2 (-0.1)\n10\n99.0\n98.1 (-0.9)\n96.8 (-2.2)\n99.0\n94.1 (-4.9)\n86.7 (-12.3)\n98.7 (-0.3)\nWordSeq 1 (4-way)\n20\n77.3\n67.9 (-9.4)\n65.4 (-11.9)\n76.8 (-0.5)\n41.3 (-36.0)\n37.4 (-39.9)\n73.1 (-4.2)\nWordSeq 2 (4-way)\n20\n80.4\n78.3 (-2.1)\n75.5 (-4.9)\n79.4 (-1.0)\n54.8 (-25.6)\n47.5 (-32.9)\n74.6 (-5.8)\nTable 1: Llama-3-8B (top) and InternLM2-20B (bottom) ablation experiments on the abstract pattern recognition tasks. For both models, zero-shot performance of the full model is ~50% in all tasks. Columns labelled \"1% ind heads\" and \"3% ind. heads\" show results from fully ablating 1% and 3% of heads with the highest prefix scores, respectively. \"1% ind. pattern\" and \"3% ind. pattern\" columns depict outcomes from blocking induction attention patterns in 1% and 3% of these heads (Sec. 7). Columns \"1% rnd. heads\" and \"3% rnd. heads\" illustrate the effects of randomly ablating 1% and 3% of all heads in the model. Performance differences due to the ablation, compared to the full model, are indicated in parentheses.\nword-sequence datasets, we adapted the sampler to ensure it never samples examples with the same instantiation of categories as the query. For instance, \"mango monkey: Foo\" would not be sampled when the query is \"mango shark:\". This adjustment tests whether the induction heads are capable of the fuzzy pattern matching necessary for ICL, rather than just copying. Each binary wordsequence dataset contains 512 examples with class balance and each four-way word-sequence dataset 1024. Detailed prompt information can be found in Appendix A.3.1. We report five- and ten-shot ICL performance for the binary classification tasks, and 20-shot performance for the four-way tasks.\nLlama-3-8B results The results for Llama-3-8B are presented in Table 1. Few-shot prompting outperforms a zero-shot baseline whose performance is close to random across all tasks (~50% in the binary case), showing evidence of ICL. Ablating 1% of induction heads leads to a substantial decrease in performance of up to 31.6% across all tasks and settings. Increasing this to 3% causes a further decline, with the letter-sequence tasks dropping to near-random performance. A similar trend is noted for the binary word-sequence tasks in the five-shot setting. Though the model achieves near-perfect performance in the ten-shot binary word-sequence tasks, induction head ablations still result in perfor-\nmance declines of up to 18.3%. These substantial performance decreases strongly indicate that induction heads play a crucial role in few-shot ICL. In contrast, a random ablation of 1% has a much milder impact, reducing performance by no more than 5.8% for the binary tasks, and 5.3% for the four-way tasks. Although increasing this ablation to 3% consistently leads to further performance decreases, these declines remain substantially milder compared to those observed with induction head ablation for all tasks and settings. This further confirms the importance of specifically induction heads in ICL.\nInternLM2-20B results The results for InternLM2-20B are presented in Table 1 (bottom). Few-shot prompting consistently outperforms a (close to random) zero-shot baseline across all tasks, providing clear evidence of ICL. The observed patterns are consistent with those seen in Llama-3-8B: Ablating 1% of induction heads leads to a substantial decrease in performance, though these declines are not as pronounced as those seen in the Llama-3-8B model. The trend persists with the ablation of 3% of induction heads, which results in performance decreases of up to 36%. Ablating 3% of randomly selected heads only results in a performance decrease of up to 5.8%. These findings further affirm that induction heads play a crucial role in few-shot ICL, showing that these results generalise across models.\n# 6 Ablation Experiments: NLP Tasks\nDatasets and experimental setup To assess the impact of induction heads on ICL performance in real-world tasks, we conduct ablation experiments on a range of NLP classification tasks. We make use of the following datasets from the SuperGLUE benchmark (Wang et al., 2019): BoolQ (question answering), RTE (natural language inference), WIC (word-sense disambiguation) and WSC (coreference resolution). Additionally, we include ETHOS (Mollas et al., 2020)4 (hate speech detection), SST-2 (Socher et al., 2013) (sentiment analysis) and SUBJ (Conneau and Kiela, 2018) (subjectivity). All are binary classification tasks. For all datasets except SST-2, we map the label space to target words \"Yes\"/\"No\", as this has been shown to improve ICL performance (Webson and Pavlick, 2022).5 For BoolQ, ETHOS and SUBJ,\n4We employ a 80/20% training-validation split. 5For SST-2, we found this to actually lead to poor zero-shot\nwe adopt the \"Input/Output\" template proposed by Wei et al. (2023b). For the other datasets, we use informative prefixes for each element of the prompt (e.g. \"Premise:\"). Our prompts are shown in App. A.3.2. For all tasks, demonstration examples are selected from the training set and evaluations are performed on the validation set. We report ten-shot prompting performance. We additionally conduct experiments with semantically-unrelated labels (SUL). Specifically, we map the label space from \"Yes\"/\"No\" to \"Foo\"/\"Bar\". This removes the semantic priors typically provided by labels, forcing the model to rely solely on input-label mappings for ICL (Wei et al., 2023b). Thus, we expect the model to strongly rely on induction heads to recognise patterns. We define ICL benefit as the metric quantifying the model\u2019s advantage from demonstration examples by measuring the performance difference between the ten-shot and zero-shot settings. For each model, we evaluate the full model\u2019s ICL benefit for each task and then quantify the percentage change in these benefits due to each ablation.6\nLlama-3-8B results We observe ICL benefits for all tasks except WSC, which we exclude from further analysis. The impact of ablations on these benefits are detailed in Figure 4 (left). Ablating 1% of induction heads reduces the ICL benefit considerably more than a 1% random ablation across all tasks except ETHOS. Specifically, in the SUBJ task, this targeted ablation results in a 63.8% reduction, whereas the random ablation leads to only a 11.8% decrease. For SST-2, we observe a decline of 113% with targeted ablation, indicating that ten-shot performance has dropped below zeroshot level. This suggests that the model may no longer leverage demonstration examples effectively for this task. Increasing the induction head ablation to 3% further decreases the ICL benefit for some tasks. We further find that for the induction head ablations, the decrease in ICL benefit is due to tenshot performance being affected more than zeroshot performance for all tasks except ETHOS (App. A.4). Such a pattern is not present in random head ablations. This supports the importance of induction heads for ICL specifically, as opposed to the general functioning of the LLM. In the SUL setting, the model achieves above-\nperformance, so we use the labels \"Positive\"/\"Negative\". 6A full overview of model accuracy is shown in App. A.4\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8110/81102f13-64f1-4a7c-8ab0-dc9b40e88d1d.png\" style=\"width: 50%;\"></div>\nFigure 4: Change in ICL benefit for Llama-3-8B (left) and InternLM2-20B (right), due to head ablations when compared to that of the full model. \"1% ind.\" and \"3% ind.\" denote ablating the top respective percentage of induction heads. \"1% rnd.\" and \"3% rnd.\" denote randomly ablating the respective percentage of all heads in the model.\n<div style=\"text-align: center;\">SUL Ablation Experiments: Llama-3-8B</div>\nSUL Ablation Experiments: Llama-3-8B\nFigure 5: SUL ablation experiments for Llama-3-8B (top) and InternLM2-20B (bottom) on NLP tasks. \"1% ind.\" and \"3% ind.\" denote ablating the top respective percentage of induction heads. \"1% rnd.\" and \"3% rnd.\" denote randomly ablating the respective percentage of all heads in the model.\nrandom performance only for the ETHOS, SST-2 and SUBJ tasks, which are presented in Figure 5 (top). Ablating 1% of induction heads leads to substantial performance declines of around 20% for all tasks. For random ablations at the same level, these declines are considerably smaller. These findings suggest that the model\u2019s capacity to reference previous examples for learning input-label mappings may be compromised. Increasing the induction head ablation to 3% did not reduce performance further, indicating that the heads responsible are\nInternLM2-20B results We observe ICL benefits on all tasks with the exceptions of BoolQ and SST-2, which we therefore exclude from further analysis. The results for the remaining tasks are shown in Figure 4 (right). Consistent with observations from the Llama-3-8B model, we note that: (1) targeted ablations of induction heads negatively affect ten-shot performance more than zero-shot performance across all tasks, whereas random ablations do not follow this pattern (App. A.4); (2) a 1% targeted ablation of induction heads diminishes the ICL benefit considerably more than a 1% random ablation. Unlike in Llama-3-8B, increasing the targeted ablation to 3% further reduces the ICL benefit in all tasks other than ETHOS. For WSC, the 3% random ablation appears to decrease the ICL benefit more than the targeted ablation. However, this effect is partly attributed to an increase in zero-shot performance from the random ablation (App. A.4). In the SUL setting, the model achieves aboverandom performance in all tasks except for WSC and the results are presented in Fig. 5 (bottom). Induction head ablations consistently lead to performance declines of up to 20%, which are generally substantially larger than those caused by random ablations across tasks, although the differences are smaller for SST-2 and WIC. We observed similar trends for the five-shot setting (App. A.4). These consistent findings across different models and settings reaffirm our conclusions on the critical role of induction heads in ICL.\n# 7 Attention Knockout Experiments\nAttention pattern analysis To further explore the functional significance of induction heads, we leverage the structure of the word-sequence datasets. For instance, in the binary WORDSEQ 1 task (classifying <fruit-animal> pairs vs. <furniture-profession> ones), if an induction head focuses on a furniture token, it should typically attend to profession tokens that have historically followed furniture. To validate this, we conduct a qualitative analysis of the attention patterns exhibited by the top five induction heads with the highest prefix matching scores from Llama-3-8B on the binary and four-way versions of the WORDSEQ 1 task (App. A.6). Figure 6 illustrates the attention pattern of the highest scoring induction head for the final \":\" to-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5095/5095303e-80ca-4402-95f1-ec9303803b11.png\" style=\"width: 50%;\"></div>\nFigure 7: Attention pattern for the \"ape\" token in head 30 of layer 15 in the Llama3-8B model.\nFigure 6: Attention pattern for the final \":\" token in head 30 of layer 15 in the Llama-3-8B model.\nken on a sample from the binary task. This token predominantly attends to label tokens, particularly focusing on \"Foo,\" the target label in this instance. Such patterns, while not universally consistent, are particularly evident in induction heads located in later layers, suggesting that induction mechanisms may facilitate the model\u2019s ability to selectively focus on relevant labels in certain contexts. Figure 7 displays the attention pattern for the \"ape\" token (from \u2019grape\u2019). We observe that the head attends to tokens that previously followed fruit tokens. This behaviour was consistent across token categories and observed in various heads and samples, suggesting that induction heads not only learn input-label mappings, but may also grasp the underlying pattern itself. Experimental setup To disrupt this induction pattern, we block attention from each token to all tokens that directly followed tokens of the same category, including word categories, labels, newlines, and colons. In contrast, the letter-sequence tasks require recognising abstract patterns rather than categorical sequences. Thus, our blocking strategy is exclusively focused on labels, newlines, and colons.\nken on a sample from the binary task. This token predominantly attends to label tokens, particularly focusing on \"Foo,\" the target label in this instance. Such patterns, while not universally consistent, are particularly evident in induction heads located in later layers, suggesting that induction mechanisms may facilitate the model\u2019s ability to selectively focus on relevant labels in certain contexts. Figure 7 displays the attention pattern for the \"ape\" token (from \u2019grape\u2019). We observe that the head attends to tokens that previously followed fruit tokens. This behaviour was consistent across token categories and observed in various heads and samples, suggesting that induction heads not only learn input-label mappings, but may also grasp the underlying pattern itself.\nLlama-3-8B results Table 4 (top) shows that blocking the induction pattern for 1% of the induc-\ntion heads results in performance declines comparable to those observed with complete head ablations. When the block is increased to 3% of the induction heads, performance declines are similarly consistent in most settings. Though in the ten-shot binary word sequence tasks the declines are notably lower, they are still substantial, exceeding 17% for both tasks. These persistent declines across both full and pattern-specific ablations provide strong empirical evidence that induction heads predominantly rely on this attention pattern. InternLM2-20B results Table 4 (bottom) shows that blocking the induction pattern in 1% of induction heads results in performance declines within 4.1% of full head ablations. Unlike in Llama-38B, this approach consistently causes greater performance decreases than full ablations. This gap widens further when the knockout is applied to 3% of the induction heads. These observations indicate that for this model, blocking the induction pattern has a more detrimental impact on head functionality than eliminating the head entirely, suggesting that the induction heads are dependent on their specific operational patterns for performance.\n# 8 Conclusion\nIn this paper, we explored the extent to which the prefix matching and copying capabilities of induction heads play a role in few-shot ICL. Our findings highlight a general trend where the ablation of induction heads affects few-shot performance more severely than zero-shot performance. This observation underscores the significance of induction heads in enabling the model to learn effectively from a limited number of examples. Moreover, even a minimal ablation of just 1% of these heads results in a substantial decrease in ICL, suggesting that induction heads are crucial for the model\u2019s few-shot learning capabilities. Additionally, when the induction pattern is blocked for a percentage of these heads, performance drops to levels comparable to full head ablations. This provides further empirical evidence that induction heads predominantly utilise this induction pattern and are dependent on it for optimal performance. Overall, our findings indicate that induction heads are a fundamental mechanism underlying ICL.\n# Limitations\nThe limitations of this study are twofold. First, we lack a mechanistic basis for definitively identi-\nfying induction heads; instead, we rely on prefixmatching scores as a proxy. Consequently, it remains unclear how the grouped-query attention mechanism influences the circuits and whether induction heads form differently as a result. Second, our investigation of induction attention patterns (attention knockout) is confined to abstract-pattern recognition tasks. Further research is required to verify these findings across NLP tasks. Furthermore, it is worth noting that our head ablation and attention knockout experiments demonstrate that induction heads play an important role in ICL. However, what we do not test for is whether they are the only computational process in the Transformer LM that plays such a role (this can not in principle be tested by a targeted ablation or blocking experiment where only a particular part of the computational process is disabled). Therefore, it is possible that additional mechanisms and/or circuits will be discovered in the future that contribute to ICL, alongside induction heads.\n# References\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895\u2013 4901, Singapore. Association for Computational Linguistics.\nStephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. 2022. Data distributional\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9cae/9cae6ef5-1fb8-4f6e-b796-d1b0117a4e23.png\" style=\"width: 50%;\"></div>\nproperties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216\u201312235, Singapore. Association for Computational Linguistics. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. 2023. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United\nJie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Xipeng Qiu, and Dahua Lin. 2024. Identifying semantic induction heads to understand in-context learning. arXiv preprint arXiv:2402.13055.\nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\nZhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah Goodman, Christopher Manning, and Christopher Potts. 2024. pyvene: A library for understanding and improving PyTorch\nZhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah Goodman, Christopher Manning, and Christopher Potts. 2024. pyvene: A library for understanding and improving PyTorch\nmodels via interventions. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations), pages 158\u2013165, Mexico City, Mexico. Association for Computational Linguistics. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\nmodels via interventions. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations), pages 158\u2013165, Mexico City, Mexico. Association for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bfd8/bfd8a0c6-9e8b-4f01-bb8a-aa62fdda16d2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Prefix matching scores for InternLM2-20B</div>\nTask\nSeeds\nPrefix Matching Sequences\n0, 1, 2, 3, 4\nDemonstration Example Selection\n42, 43, 44\nRandom Head Ablation\n42, 43, 44\nTable 2: Seed details.\n# A.3 Prompt Examples\nRecursion: Below is a list of various sequences. Your task is to classify each sequ the examples provided to understand how to classify each sequence correctl\nBelow is a list of various sequences. Your task is to classify each sequence. Us the examples provided to understand how to classify each sequence correctly.\nBelow is a list of various sequences. Your task is to classify each sequence. Use the examples provided to understand how to classify each sequence correctly. strawberry cat:Foo cherry rabbit:Foo nightstand nurse:Bar grape rabbit:Foo couch artist:Bar cabinet scientist: WordSeq 1 (four-way): Below is a list of various sequences. Your task is to classify each sequence. Use the examples provided to understand how to classify each sequence correctly. papaya writer:Res ottoman zebra:Mur\n# A.3.2 NLP Tasks\nInput: Natural-born-citizen clause \u2013 Status as a natural-born citizen of the United States is one of the eligibility requirements established in the United States Constitution for holding the office of President or Vice President. This requirement was intended to protect the nation from foreign influence. Can a canadian be president of the united states?\nOutput: No\nInput: Pepsi Zero Sugar \u2013 Pepsi Zero Sugar (sold under the names Diet Pepsi Max until early 2009 and then Pepsi Max until August 2016), is a zero-calorie, sugar-free, carbohydrate-free, ginseng-infused cola sweetened with aspartame, marketed by PepsiCo. In Fall 2016, PepsiCo renamed the drink Pepsi Zero Sugar from Pepsi Max. It has nearly twice the caffeine of Pepsi\u2019s other cola beverages. Pepsi Zero Sugar contains 69 milligrams of caffeine per 355ml (12 fl oz), versus 36 milligrams in Diet Pepsi.\nIs pepsi zero sugar the same as diet pepsi?\nOutput: No\nInput: Phone hacking \u2013 Phone hacking, being a form of surveillance, is illegal in many countries unless it is carried out as lawful interception by a government agency. In the News International phone hacking scandal, private investigator Glenn Mulcaire was found to have violated the Regulation of Investigatory Powers Act 2000. He was sentenced to six months in prison in January 2007. Renewed controversy over the phone hacking claims led to the closure of the News of the World in July 2011. Is it illegal to hack into someones phone?\nOutput: Yes\nInput: Legal threat \u2013 A legal threat is a statement by a party that it intends to take legal action on another party, generally accompanied by a demand that the other party take an action demanded by the first party or refrain from taking or continuing actions objected to by the demanding party. Can you threaten to take someone to court?\nCan you threaten to take someone to court?\nOutput: Yes\nInput: Devil\u2019s food cake \u2013 Devil\u2019s food cake is a moist, airy, rich chocolate layer cake. It is considered a counterpart to the white or yellow angel food cake. Because of differing recipes and changing ingredient availability over the course of the 20th century, it is difficult to precisely qualify what distinguishes devil\u2019s food from the more standard chocolate cake, though it traditionally has more chocolate than a regular chocolate cake, making it darker. The cake is usually paired with a rich chocolate frosting. Is devil\u2019s food cake the same as chocolate?\nOutput: No Input: Ethanol fuel \u2013 All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or \u201cenergy returned on energy invested\u201d). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline. Does ethanol take more energy make that produces? Output: ETHOS Does the input contain hate speech? Input: My God, the cancer is spreading Output: No Input: I\u2019m searching for a hitman right now to kill this whore and every other like her. H A T E W O M E N Output: Yes Input: why is it censored, why does todays generation get butt hurf and offended so easily, if u dont like it, keep walking then! this use to be a free country but its going down the toilet now. Output: No Input: Stand the fuck up and take them out all you fucking lazy white males! fuck being called a racist protect your woman and children. If you find it in the street kill it and bury it right there these animals will wipe you out if your not careful Output: Yes Input: Ugly white girls and white dudes would not get away with that.\nInput: Ethanol fuel \u2013 All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or \u201cenergy returned on energy invested\u201d). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline. Does ethanol take more energy make that produces?\nOutput:\nInput: why is it censored, why does todays generation get butt hurf and offended so easily, if u dont like it, keep walking then! this use to be a free country but its going down the toilet now.\nOutput: No\nInput: Stand the fuck up and take them out all you fucking lazy white males! fuck being called a racist protect your woman and children. If you find it in the street kill it and bury it right there these animals will wipe you out if your not careful\n# Output: Yes\nInput: Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot\n# Output:\nAnswer: Yes\nPremise: The discovery of the body of a warrior thought to have died in battle more than 2,000 years ago - could help archaeologists to pinpoint the site of an ancient holy site. The young warrior, aged about 30, with his spear, a sword, his belt and scabbard, stunned archaeologists who found his stone coffin. Hypothesis: Altai ice maiden triggers major dispute.\nAnswer: No\nPremise: Switzerland has ratified bilateral agreements with the members of the European Union in March 2004, but the new members (Cyprus , Czech Republic , Estonia, Hungary, Latvia, Lithuania, Malta, Poland, Slovakia and Slovenia) were not included in the deal. Hypothesis: Lithuania intends to introduce the use of the Euro as an official currency on January 1, 2007.\nAnswer: No\nPremise: Jill Pilgrim, general counsel of USA Track and Field, brought up the issue during a panel on women\u2019s sports at the sports lawyers conference. Pilgrim said the law regarding who is legally considered a woman is changing as sex-change operations become more common. Hypothesis: Sex-change operations become more common.\nAnswer: Yes\nPremise: Lastly, the author uses the precedent of marijuana legalization in other countries as evidence that legalization does not solve any social problems, but instead creates them. Hypothesis: Drug legalization has benefits.\nPremise: Dana Reeve, the widow of the actor Christopher Reeve, has d lung cancer at age 44, according to the Christopher Reeve Foundation. Hypothesis: Christopher Reeve had an accident.\nSST-2 Classify the review according to its sentiment. Review: \u2013 and especially williams , an american actress who becomes fully english Sentiment: Positive Review: each other so intensely , but with restraint Sentiment: Positive Review: tear your eyes away Sentiment: Negative Review: fascinating and playful Sentiment: Positive Review: been discovered , indulged in and rejected as boring before i see this piece of crap again Sentiment: Negative Review: it \u2019s a charming and often affecting journey . Sentiment: SUBJ Does the input contain personal opinions, feelings, or beliefs? Input: by taking entertainment tonight subject matter and giving it humor and poignancy , auto focus becomes both gut-bustingly funny and crushingly depressing . Output: Yes Input: if you open yourself up to mr . reggio \u2019s theory of this imagery as the movie \u2019s set . . . it can impart an almost visceral sense of dislocation and change . Output: Yes Input: but for one celebrant this holy week is different . Output: No Input: with grit and determination molly guides the girls on an epic journey , one step ahead of the authorities , over 1 , 500 miles of australia \u2019s outback\nOutput: No\nOutput: No Input: just about all of the film is confusing on one level or anoth making ararat far more demanding than it needs to be . Output: Yes Input: pirates of the caribbean is a sweeping action-adventure story se an era when villainous pirates scavenged the caribbean seas . Output: WIC You are given two sentences and a word. Tell me whether the word has the same me in both sentences. Word: right Sentence 1: He stood on the right. Sentence 2: The pharmacy is just on the right past the bookshop. Answer: Yes Word: act Sentence 1: She wants to act Lady Macbeth, but she is too young for the role. Sentence 2: The dog acts ferocious, but he is really afraid of people. Answer: No Word: fall Sentence 1: The hills around here fall towards the ocean. Sentence 2: Her weight fell to under a hundred pounds. Answer: No Word: Round Sentence 1: Round off the amount. Sentence 2: The total is $25,715 but to keep the figures simple, I\u2019ll round it to $25,000. Answer: Yes Word: have Sentence 1: What do we have here? Sentence 2: I have two years left. Answer: No Word: class Sentence 1: An emerging professional class.\nOutput: No\nOutput: Yes\nOutput:\nAnswer: No\nAnswer:\nWSC You are given a sentence, a prounoun and a noun. Tell me whether the specified pronoun and the noun phrase refer to the same entity in the sentence. Sentence: Jane knocked on Susan \u2019s door but she did not answer. Pronoun: she Noun: Susan\n# Answer: Yes\nSentence: The mothers of Arthur and Celeste have come to the town to fet them. They are very happy to have them back, but they scold them just the sa because they ran away. Pronoun: they Noun: Arthur and Celeste\n# Answer: No\nSentence: The scientists are studying three species of fish that have recently been found living in the Indian Ocean. They appeared two years ago. Pronoun: They Noun: The scientists\nAnswer: No\nSentence: Sergeant Holmes asked the girls to describe the intruder . Nancy not only provided the policeman with an excellent description of the heavyset thirty-year-old prowler, but drew a rough sketch of his face. Pronoun: his Noun: the intruder\nSentence: The boy continued to whip the pony , and eventually the pony threw him over. John laughed out quite loud. \"Served him right,\" he said. Pronoun: him\n# Answer: No\nSentence: Bernard , who had not told the government official that he was less than 21 when he filed for a homestead claim, did not consider that he had done anything dishonest. Still, anyone who knew that he was 19 years old could take his claim away from him . Pronoun: him Noun: anyone\nAnswer:\nLlama-3-8B\nTask\nShot\nFull\n1% ind.\n1% rnd.\n3% ind.\n3% rnd.\nmodel\nheads\nheads\nheads\nheads\nBoolQ\n0\n77.8\n76.7 (-1.1)\n71.9 (-5.9)\n75.7 (-2.1)\n70.5 (-7.3)\n5\n79.8\n77.8 (-2.0)\n78.1 (-1.7)\n73.6 (-6.2)\n76.5 (-3.3)\n10\n79.6\n78.2 (-1.4)\n77.1 (-2.5)\n69.9 (-9.7)\n75.6 (-4.0)\nSUL\n10\n46.3\n-\n-\n-\n-\nETHOS\n0\n70.0\n69.2 (-0.8)\n70.0\n66.8 (-3.2)\n70.0\n5\n82.5\n79.3 (-3.2)\n76.7 (-5.8)\n80.4 (-2.1)\n72.3 (-10.2)\n10\n80.2\n79.8 (-0.4)\n77.7 (-2.5)\n80.6 (+0.4)\n77.9 (-2.3)\nSUL\n10\n70.3\n51.4 (-18.9)\n62.3 (-8.0)\n56.5 (-13.8)\n63.0 (-7.3)\nRTE\n0\n71.1\n67.2 (-3.9)\n69.0 (-2.1)\n66.4 (-4.7)\n71.5 (+0.4)\n5\n79.5\n74.5 (-5.0)\n77.8 (-1.7)\n71.1 (-8.4)\n77.2 (-2.3)\n10\n79.4\n72.6 (-6.8)\n77.3 (-2.1)\n67.8 (-11.6)\n74.9 (-4.5)\nSUL\n10\n52.4\n-\n-\n-\n-\nSST-2\n0\n92.4\n92.8 (+0.4)\n92.2 (-0.2)\n93.0 (+0.6)\n91.9 (-0.5)\n5\n93.9\n92.2 (-1.7)\n94.2 (+0.3)\n92.6 (-1.3)\n93.6 (-0.3)\n10\n94.6\n92.5 (-2.1)\n94.6\n93.7 (-0.9)\n94.4 (-0.2)\nSUL\n10\n94.0\n70.7 (-23.3)\n93.0 (-1.0)\n70.5 (-23.5)\n92.5 (-1.5)\nSUBJ\n0\n52.6\n52.7 (+0.1)\n53.7 (+1.1)\n50.6 (-2.0)\n53.3 (+0.7)\n5\n58.1\n54.8 (-3.3)\n56.9 (-1.2)\n54.8 (-3.3)\n56.8 (-1.3)\n10\n74.7\n60.7 (-14.0)\n73.2 (-1.5)\n60.3 (-14.4)\n69.9 (-4.8)\nSUL\n10\n69.8\n49.7 (-20.1)\n65.2 (-4.6)\n52.8 (-17.0)\n65.3 (-4.5)\nWIC\n0\n51.4\n51.3 (-0.1)\n52.0 (+0.6)\n50.5 (-0.9)\n51.2 (-0.2)\n5\n59.6\n57.4 (-2.2)\n58.5 (-1.1)\n56.6 (-3.0)\n57.2 (-2.4)\n10\n61.6\n57.1 (-4.5)\n59.3 (-2.3)\n55.8 (-5.8)\n58.2 (-3.4)\nSUL\n10\n52.6\n-\n-\n-\n-\nInternLM2-20B\nTask\nShot\nFull\n1% ind.\n1% rnd.\n3% ind.\n3% rnd.\nmodel\nheads\nheads\nheads\nheads\nBoolQ\n0\n88.6\n-\n-\n-\n-\n5\n88.5\n-\n-\n-\n-\n10\n88.3\n-\n-\n-\n-\nSUL\n10\n80.7\n70.1 (-10.6)\n79.5 (-1.2)\n63.6 (-17.1)\n78.8 (-1.9)\nETHOS\n0\n82.0\n82.4 (+0.4)\n81.6 (-0.4)\n82.0\n81.7 (-0.3)\n5\n84.7\n80.5 (-4.2)\n81.8 (-2.9)\n82.4 (-2.3)\n82.3 (-2.4)\n10\n82.6\n81.6 (-1.0)\n82.1 (-0.5)\n82.4 (-0.2)\n82.4 (-0.2)\nSUL\n10\n82.5\n68.3 (-14.2)\n76.7 (-5.8)\n68.6 (-13.9)\n76.4 (-6.1)\nRTE\n0\n81.2\n79.8 (-1.4)\n79.8 (-1.4)\n79.1 (-2.1)\n79.5 (-1.7)\n5\n84.7\n81.6 (-3.1)\n84.3 (-0.4)\n80.4 (-4.3)\n83.3 (-1.4)\n10\n87.2\n83.6 (-3.6)\n86.5 (-0.7)\n82.0 (-5.2)\n85.6 (-1.6)\nSUL\n10\n80.1\n61.7 (-18.4)\n77.7 (-2.4)\n59.8 (-20.3)\n81.6 (+1.5)\nSST-2\n0\n96.0\n-\n-\n-\n-\n5\n95.1\n-\n-\n-\n-\n10\n95.3\n-\n-\n-\n-\nSUL\n10\n94.1\n93.4 (-0.7)\n93.9 (-0.2)\n92.3 (-1.8)\n94.1\nSUBJ\n0\n61.1\n61.6 (+0.5)\n60.3 (-0.8)\n61.5 (+0.4)\n59.8 (-1.3)\n5\n75.4\n69.2 (-6.2)\n74.8 (-0.6)\n70.5 (-4.9)\n74.1 (-1.3)\n10\n81.4\n73.5 (-7.9)\n81.2 (-0.2)\n70.5 (-10.9)\n78.8 (-2.6)\nSUL\n10\n64.9\n51.3 (-13.6)\n62.6 (-2.3)\n52.3 (-12.6)\n63.4 (-1.5)\nWIC\n0\n60.8\n61.0 (+0.2)\n61.1 (+0.3)\n59.7 (-1.1)\n61.1 (+0.3)\n5\n68.6\n67.4 (-1.2)\n69.1 (+0.5)\n62.0 (-6.6)\n67.9 (-0.7)\n10\n68.9\n66.5 (-2.4)\n68.1 (-0.8)\n63.5 (-5.4)\n67.8 (-1.1)\nSUL\n10\n57.2\n54.2 (-3.0)\n55.9 (-1.3)\n56.6 (-0.6)\n56.8 (-0.4)\nWSC\n0\n70.2\n67.3 (-2.9)\n68.9 (-1.3)\n66.4 (-3.8)\n72.1 (+1.9)\n5\n73.1\n67.6 (-5.5)\n72.7 (-0.4)\n69.9 (-3.2)\n73.2 (+0.1)\n10\n75.3\n69.6 (-5.7)\n71.6 (-3.7)\n68.6 (-6.7)\n71.7 (-3.6)\nSUL\n10\n43.3\n-\n-\n-\n-\nnd InternLM2-20B ablation experiments on the NLP tasks. Co\nTable 3: LLama-3-8B and InternLM2-20B ablation experiments on the NLP tasks. Columns labelled \"1% ind. heads\" and \"3% ind. heads\" show the results from fully ablating 1% and 3% of heads with the highest prefix scores, respectively. Columns \"1% rnd. heads\" and \"3% rnd. heads\" illustrate the effects of randomly ablating 1% and 3% of all heads in the model. The \"SUL\" row denotes settings using semantically unrelated labels. Performance differences due to the ablation, compared to the full model, are indicated in parentheses.\n# A.5 Zero Ablations\nTo investigate the significance of induction heads for a specific ICL task, we initially conducted zeroablations of 1% and 3% of the heads with the highest prefix matching scores. This ablation process involved masking the corresponding partition of the output matrix, denoted as Wh o in Eq. 1, by setting it to zero. This effectively renders the heads inactive and thereby prevents their contributions. As a control condition, we also randomly ablated 1% and 3% of the model\u2019s heads, which were selected from any layer except the final layer.\n<div style=\"text-align: center;\">A.5.1 Zero Ablations: Abstract Pattern Recognition Tasks</div>\nLlama-3-8B\nTask\nShot\nFull\n1% ind.\n1% ind.\n1% rnd.\n3% ind.\n3% ind.\n3% rnd.\nmodel\nheads\npattern\nheads\nheads\npattern\nheads\nRepetition\n5\n67.3\n60.9 (-6.4)\n62.5 (-4.8)\n68.5 (+1.2)\n51.3 (-16.0)\n53.2 (-14.1)\n68.5 (+1.2)\n10\n91.3\n59.5 (-31.8)\n65.3 (-26.0)\n90.3 (-1.0)\n54.3 (-37.0)\n58.7 (-32.6)\n91.2 (-0.1)\nRecursion\n5\n67.8\n62.1 (-5.7)\n61.5 (-6.3)\n69.8 (+2.0)\n54.9 (-12.9)\n55.1 (-12.7)\n70.9 (+3.1)\n10\n91.5\n66.1 (-25.4)\n68.7 (-22.8)\n91.5\n58.1 (-33.4)\n58.9 (-32.6)\n92.4 (+0.9)\nCentre-embedding\n5\n58.8\n54.0 (-4.8)\n55.0 (-3.8)\n59.2 (+0.4)\n48.3 (-10.5)\n50.5 (-8.3)\n59.7 (+0.9)\n10\n80.4\n53.1 (-27.3)\n56.5 (-23.9)\n81.6 (+1.2)\n50.3 (-30.1)\n52.3 (-28.1)\n80.3 (-0.1)\nWordSeq 1 (binary)\n5\n83.1\n72.1 (-11.0)\n71.8 (-11.3)\n80.6 (-2.5)\n54.7 (-28.4)\n56.9 (-26.2)\n81.0 (-3.1)\n10\n99.4\n96.4 (-3.0)\n97.3 (-2.1)\n98.4 (-1.0)\n79.2 (-20.2)\n82.2 (-17.2)\n97.9 (-1.5)\nWordSeq 2 (binary)\n5\n77.9\n66.0 (-11.9)\n65.2 (-12.7)\n74.4 (-3.5)\n53.5 (-24.4)\n55.7 (-22.2)\n75.1 (-2.8)\n10\n99.4\n95.3 (-4.1)\n96.4 (-3.0)\n97.3 (-2.1)\n77.2 (-22.2)\n81.1 (-18.3)\n97.6 (-1.8)\nWordSeq 1 (4-way)\n20\n78.3\n59.7 (-18.6)\n59.8 (-18.5)\n71.9 (-6.4)\n44.8 (-33.5)\n45.2 (-33.1)\n72.0 (-6.3)\nWordSeq 2 (4-way)\n20\n81.3\n58.6 (-22.7)\n59.8 (-21.5)\n71.1 (-10.2)\n46.5 (-34.8)\n47.5 (-33.8)\n71.5 (-9.8)\nInternLM2-20B\nTask\nShot\nFull\n1% ind.\n1% ind.\n1% rnd.\n3% ind.\n3% ind.\n3% rnd.\nmodel\nheads\npattern\nheads\nheads\npattern\nheads\nRepetition\n5\n68.7\n63.3 (-5.4)\n62.0 (-6.7)\n68.2 (-0.5)\n60.8 (-7.9)\n58.5 (-10.2)\n64.0 (-4.7)\n10\n88.1\n73.2 (-14.9)\n72.1 (-16.0)\n87.8 (-0.3)\n68.7 (-19.4)\n61.2 (-26.9)\n84.1 (-4.0)\nRecursion\n5\n68.1\n61.4 (-6.7)\n59.9 (-8.2)\n68.2 (+0.1)\n59.6 (-8.5)\n55.3 (-12.8)\n64.4 (-3.7)\n10\n88.5\n70.6 (-17.9)\n69.9 (-18.6)\n87.7 (-0.8)\n66.1 (-22.4)\n57.1 (-31.4)\n84.4 (-4.1)\nCentre-embedding\n5\n59.5\n56.3 (-3.2)\n56.1 (-3.4)\n58.9 (-0.6)\n55.1 (-4.4)\n54.1 (-5.4)\n56.3 (-3.2)\n10\n75.3\n60.3 (-15.0)\n58.5 (-16.8)\n74.9 (-0.4)\n56.9 (-18.4)\n54.3 (-21.0)\n71.2 (-4.1)\nWordSeq 1 (binary)\n5\n76.8\n67.4 (-9.4)\n64.9 (-11.9)\n76.7 (-0.1)\n59.9 (-16.9)\n56.8 (-20.0)\n76.9 (+0.1)\n10\n95.6\n91.7 (-3.9)\n88.2 (-7.4)\n94.8 (-0.8)\n85.7 (-9.9)\n83.3 (-12.3)\n95.6\nWordSeq 2 (binary)\n5\n83.3\n74.8 (-8.5)\n70.1 (-13.2)\n82.5 (-0.8)\n68.6 (-14.7)\n64.1 (-19.2)\n79.9 (-3.4)\n10\n99.0\n98.9 (-0.1)\n96.8 (-2.2)\n96.2 (-2.8)\n94.1 (-4.9)\n86.7 (-12.3)\n98.6 (-0.4)\nWordSeq 1 (4-way)\n20\n77.3\n68.8 (-8.5)\n65.4 (-11.9)\n76.0 (-1.3)\n37.3 (-40.0)\n37.4 (-39.9)\n75.5 (-1.8)\nWordSeq 2 (4-way)\n20\n80.4\n78.4 (-2.0)\n75.5 (-4.9)\n79.3 (-1.1)\n52.9 (-27.5)\n47.5 (-32.9)\n77.3 (-3.1)\nTable 4: Llama-3-8B (top) and InternLM2-20B (bottom) ablation experiments on the abstract pattern recognition tasks. For both models, zero-shot performance of the full model is ~50% in all tasks. Columns labelled \"1% ind. heads\" and \"3% ind. heads\" show results from fully ablating 1% and 3% of heads with the highest prefix scores, respectively. \"1% ind. pattern\" and \"3% ind. pattern\" columns depict outcomes from blocking induction attention patterns in 1% and 3% of these heads (Sec. 7). Columns \"1% rnd. heads\" and \"3% rnd. heads\" illustrate the effects of randomly ablating 1% and 3% of all heads in the model. Performance differences due to the ablation, compared to the full model, are indicated in parentheses.\nLlama-3-8B\nTask\nShot\nFull\n1% ind.\n1% rnd.\n3% ind.\n3% rnd.\nmodel\nheads\nheads\nheads\nheads\nBoolQ\n0\n77.8\n76.2 (-1.6)\n72.7 (-5.1)\n75.2 (-2.6)\n71.9 (-5.9)\n5\n79.8\n77.1 (-2.7)\n76.4 (-3.4)\n73.0 (-6.8)\n77.2 (-2.6)\n10\n79.6\n77.7 (-1.9)\n77.1 (-2.5)\n70.7 (-8.9)\n77.3 (-2.3)\nSUL\n10\n46.3\n-\n-\n-\n-\nETHOS\n0\n70.0\n72.0 (+2.0)\n68.6 (-1.4)\n70.0\n62.0 (-8.0)\n5\n82.5\n82.5\n79.5 (-3.0)\n81.2 (-1.3)\n75.3 (-7.2)\n10\n80.2\n80.2\n80.9 (+0.7)\n81.1 (-1.1)\n77.5 (-2.7)\nSUL\n10\n70.3\n52.8 (-17.5)\n66.8 (-3.5)\n57.9 (-12.4)\n63.8 (-6.5)\nRTE\n0\n71.1\n67.5 (-3.6)\n70.3 (-0.8)\n67.9 (-3.2)\n67.5 (-3.6)\n5\n79.5\n75.2 (-4.3)\n79.0 (-0.5)\n71.6 (-7.9)\n78.1 (-1.4)\n10\n79.4\n74.2 (-5.2)\n78.7 (-0.7)\n68.6 (-10.8)\n78.4 (-1.0)\nSUL\n10\n52.4\n-\n-\n-\n-\nSST-2\n0\n92.4\n92.1 (-0.3)\n92.7 (+0.3)\n92.2 (-0.2)\n91.9 (-0.5)\n5\n93.9\n92.4 (-1.5)\n94.3 (+0.4)\n93.0 (-0.9)\n93.9\n10\n94.6\n92.9 (-1.7)\n94.7 (+0.1)\n93.2 (-1.4)\n94.5 (-0.1)\nSUL\n10\n94.0\n72.8 (-21.2)\n93.6 (-0.4)\n71.8 (-22.2)\n92.6 (-1.4)\nSUBJ\n0\n52.6\n53.0 (+0.4)\n54.0 (+1.4)\n52.4 (-0.2)\n53.2 (+0.6)\n5\n58.1\n53.5 (-4.6)\n58.8 (+0.7)\n56.3 (-1.8)\n56.5 (-1.6)\n10\n74.7\n62.4 (-12.3)\n74.6 (-0.1)\n62.6 (-12.1)\n71.9 (-2.8)\nSUL\n10\n69.8\n49.9 (-19.9)\n67.0 (-2.8)\n53.1 (-16.7)\n70.4 (+0.6)\nWIC\n0\n51.4\n51.4\n51.0 (-0.4)\n50.9 (-0.5)\n51.7 (+0.3)\n5\n59.6\n58.2 (-1.4)\n59.0 (-0.6)\n56.3 (-3.3)\n56.8 (-2.8)\n10\n61.6\n56.8 (-4.8)\n59.6 (-2.0)\n56.2 (-5.4)\n57.0 (-4.6)\nSUL\n10\n52.6\n-\n-\n-\n-\nInternLM2-20B\nTask\nShot\nFull\n1% ind.\n1% rnd.\n3% ind.\n3% rnd.\nmodel\nheads\nheads\nheads\nheads\nBoolQ\n0\n88.6\n88.6\n88.5 (-0.1)\n87.7 (-0.9)\n86.9 (-1.7)\n5\n88.5\n88.5\n88.6 (+0.1)\n88.2 (-0.3)\n87.5 (-1.0)\n10\n88.3\n88.3\n88.2 (-0.1)\n87.9 (-0.4)\n87.1 (-1.3)\nSUL\n10\n80.7\n72.4 (-8.3)\n80.9 (+0.2)\n64.9 (-15.8)\n77.0 (-3.7)\nETHOS\n0\n82.0\n82.5 (+0.5)\n82.5 (+0.5)\n82.0\n81.5 (-0.5)\n5\n84.7\n81.3 (-3.4)\n84.2 (-0.5)\n81.8 (-2.9)\n82.6 (-2.1)\n10\n82.6\n81.0 (-1.6)\n82.9 (+0.3)\n82.1 (-0.5)\n82.8 (+0.2)\nSUL\n10\n82.5\n74.8 (-7.7)\n81.3 (-1.2)\n68.2 (-14.3)\n74.4 (-8.1)\nRTE\n0\n81.2\n80.1 (-1.1)\n80.6 (-0.6)\n80.5 (-0.7)\n79.9 (-1.3)\n5\n84.7\n82.3 (-2.4)\n84.4 (-0.3)\n81.4 (-3.3)\n84.4 (-0.3)\n10\n87.2\n84.0 (-3.2)\n86.8 (-0.4)\n82.3 (-4.9)\n86.4 (-0.8)\nSUL\n10\n80.1\n62.9 (-17.2)\n81.4 (+1.3)\n56.4 (-23.7)\n76.9 (-3.2)\nSST-2\n0\n96.0\n96.0\n95.7 (-0.3)\n95.6 (-0.4)\n95.7 (-0.3)\n5\n95.1\n95.1\n95.0 (-0.1)\n94.2 (-0.9)\n94.9 (-0.2)\n10\n95.3\n95.1 (-0.2)\n95.2 (-0.1)\n94.3 (-1.0)\n95.2 (-0.1)\nSUL\n10\n94.1\n93.6 (-0.5)\n94.1\n92.2 (-1.9)\n93.6 (-0.5)\nSUBJ\n0\n61.1\n61.7 (+0.6)\n61.6 (+0.5)\n62.6 (+1.5)\n59.9 (-1.2)\n5\n75.4\n69.6 (-5.8)\n75.0 (-0.4)\n70.0 (-5.4)\n73.4 (-2.0)\n10\n81.4\n75.1 (-6.3)\n81.4\n71.3 (-10.1)\n78.8 (-2.6)\nSUL\n10\n64.9\n53.4 (-11.5)\n65.1 (+0.2)\n52.2 (-12.7)\n61.3 (-3.6)\nWIC\n0\n60.8\n60.7 (-0.2)\n60.8\n59.9 (-0.9)\n60.1 (-0.7)\n5\n68.6\n67.2 (-1.4)\n68.7 (+0.1)\n61.9 (-6.7)\n68.9 (+0.3)\n10\n68.9\n67.0 (-1.9)\n68.6 (-0.3)\n63.5 (-5.4)\n68.3 (-0.6)\nSUL\n10\n57.2\n54.8 (-2.4)\n57.7 (+0.5)\n56.6 (-0.6)\n54.9 (-2.3)\nWSC\n0\n70.2\n67.3 (-2.9)\n69.6 (-0.6)\n68.3 (-1.9)\n70.8 (+0.6)\n5\n73.1\n68.0 (-5.1)\n73.0 (-0.1)\n69.6 (-3.5)\n73.1\n10\n75.3\n69.9 (-5.4)\n75.2 (-0.1)\n68.6 (-1.3)\n71.9 (-3.4)\nSUL\n10\n43.3\n-\n-\n-\n-\nnd InternLM2-20B ablation experiments on the NLP tasks. Co\nTable 5: LLama-3-8B and InternLM2-20B ablation experiments on the NLP tasks. Columns labelled \"1% ind. heads\" and \"3% ind. heads\" show the results from fully ablating 1% and 3% of heads with the highest prefix scores, respectively. Columns \"1% rnd. heads\" and \"3% rnd. heads\" illustrate the effects of randomly ablating 1% and 3% of all heads in the model. The \"SUL\" row denotes settings using semantically unrelated labels. Performance differences due to the ablation, compared to the full model, are indicated in parentheses.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/db37/db37edd0-35da-43fa-b968-26965edced1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Change in ICL benefit for Llama-3-8B (left) and InternLM2-20B (right), due to head ablations when compared to that of the full model. \"1% ind.\" and \"3% ind.\" denote ablating the top respective percentage of induction heads. \"1% rnd.\" and \"3% rnd.\" denote randomly ablating the respective percentage of all heads in the model.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/017e/017e0af6-c837-4571-b74d-15eda634fc7f.png\" style=\"width: 50%;\"></div>\n<div style=\"text",
    "paper_type": "method",
    "attri": {
        "background": "Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting, analyzing two state-of-the-art models, Llama-3-8B and InternLM2-20B, on abstract pattern recognition and NLP tasks.",
        "problem": {
            "definition": "The problem addressed is the lack of understanding of the internal mechanisms of in-context learning (ICL) in large language models, specifically the role of induction heads.",
            "key obstacle": "Existing methods do not adequately link the specific computations performed by induction heads to measurable improvements in ICL performance."
        },
        "idea": {
            "intuition": "The idea is inspired by the hypothesis that induction heads are capable of abstract pattern matching, which could significantly enhance the performance of LLMs in ICL tasks.",
            "opinion": "The proposed idea involves investigating the specific computations of induction heads and their impact on ICL performance across various tasks.",
            "innovation": "The innovation lies in empirically linking the role of induction heads to ICL performance, demonstrating that their ablation leads to significant declines in performance, unlike random head ablations."
        },
        "method": {
            "method name": "Induction Head Analysis",
            "method abbreviation": "IHA",
            "method definition": "The method involves measuring prefix matching scores of attention heads and conducting head ablation experiments to assess the impact on ICL performance.",
            "method description": "This method analyzes the performance of LLMs in ICL tasks by manipulating induction heads and observing the effects on accuracy.",
            "method steps": [
                "Identify induction heads using prefix matching scores.",
                "Conduct ablation experiments by removing a percentage of heads with the highest scores.",
                "Evaluate the ICL performance of the models before and after ablation."
            ],
            "principle": "The method is effective because it directly correlates the functionality of induction heads with the model's ability to leverage in-context learning for improved task performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on two models, Llama-3-8B and InternLM2-20B, using abstract pattern recognition and NLP datasets, with a focus on few-shot ICL settings.",
            "evaluation method": "Performance was measured by comparing accuracy across different head ablation conditions, including induction head ablation and random head ablation."
        },
        "conclusion": "The findings highlight that induction heads are essential for few-shot ICL, with even minimal ablation leading to significant performance declines. The results confirm the crucial role of induction heads in enabling effective learning from limited examples.",
        "discussion": {
            "advantage": "The proposed approach demonstrates a clear advantage in understanding the mechanisms behind ICL, providing insights into how induction heads contribute to improved model performance.",
            "limitation": "The study relies on prefix matching scores as a proxy for identifying induction heads, which may not capture all mechanistic aspects of their functionality.",
            "future work": "Future research should explore the mechanistic basis of induction heads and verify the findings across a broader range of NLP tasks."
        },
        "other info": {
            "code availability": "The code and data used in this study are publicly available.",
            "contact": {
                "author1": {
                    "name": "Joy Crosbie",
                    "email": "joy.m.crosbie@gmail.com"
                },
                "author2": {
                    "name": "Ekaterina Shutova",
                    "email": "e.shutova@uva.nl"
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper explores the role of induction heads in a few-shot in-context learning (ICL) setting, analyzing state-of-the-art models on abstract pattern recognition and NLP tasks."
        },
        {
            "section number": "1.3",
            "key information": "Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL)."
        },
        {
            "section number": "3.1",
            "key information": "The study demonstrates that induction heads are essential for few-shot ICL, with significant performance declines observed with their ablation."
        },
        {
            "section number": "3.2",
            "key information": "The paper empirically links the role of induction heads to ICL performance, showing that their specific computations enhance model capabilities."
        },
        {
            "section number": "4.1",
            "key information": "The method involves measuring prefix matching scores of attention heads to analyze the performance of LLMs in ICL tasks."
        },
        {
            "section number": "6.1",
            "key information": "The study highlights limitations in existing methods that do not adequately link specific computations performed by induction heads to measurable improvements in ICL performance."
        },
        {
            "section number": "6.4",
            "key information": "Future research should explore the mechanistic basis of induction heads and verify findings across a broader range of NLP tasks."
        }
    ],
    "similarity_score": 0.7421801935179905,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning.json"
}