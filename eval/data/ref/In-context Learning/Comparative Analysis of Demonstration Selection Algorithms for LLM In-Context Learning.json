{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.23099",
    "title": "Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning",
    "abstract": "In-context learning can help Large Language Models (LLMs) to adapt new tasks without additional training. However, this performance heavily depends on the quality of the demonstrations, driving research into effective demonstration selection algorithms to optimize this process. These algorithms assist users in selecting the best k input-label pairs (demonstration examples) based on a given test input, enabling LLMs to in-context learn the relationship between the provided examples and the test inputs. Despite all the proposed demonstration selection algorithms, their efficiency and effectiveness remain unclear. This lack of clarity make it difficult to apply these algorithms in real-world scenarios and poses challenges for future research aimed at developing improved methods. This paper revisits six proposed algorithms, evaluating them on five datasets from both efficiency and effectiveness perspectives. Our experiments reveal significant variations in algorithm performance across different tasks, with some methods struggling to outperform random selection in certain scenarios. We also find that increasing the number of demonstrations does not always lead to better performance, and that there are often trade-offs between accuracy and computational efficiency. Our code is available at https: //github.com/Tizzzzy/Demonstration Selection Overview.",
    "bib_name": "shu2024comparativeanalysisdemonstrationselection",
    "md_text": "# Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning\n# Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning Dong Shu1, Mengnan Du2\n1Northwestern University 2New Jersey Institute of Technology dongshu2024@u.northwestern.edu, mengnan.du@njit.edu\nAbstract\n# Abstract\nIn-context learning can help Large Language Models (LLMs) to adapt new tasks without additional training. However, this performance heavily depends on the quality of the demonstrations, driving research into effective demonstration selection algorithms to optimize this process. These algorithms assist users in selecting the best k input-label pairs (demonstration examples) based on a given test input, enabling LLMs to in-context learn the relationship between the provided examples and the test inputs. Despite all the proposed demonstration selection algorithms, their efficiency and effectiveness remain unclear. This lack of clarity make it difficult to apply these algorithms in real-world scenarios and poses challenges for future research aimed at developing improved methods. This paper revisits six proposed algorithms, evaluating them on five datasets from both efficiency and effectiveness perspectives. Our experiments reveal significant variations in algorithm performance across different tasks, with some methods struggling to outperform random selection in certain scenarios. We also find that increasing the number of demonstrations does not always lead to better performance, and that there are often trade-offs between accuracy and computational efficiency. Our code is available at https: //github.com/Tizzzzy/Demonstration Selection Overview.\narXiv:2410.23099v\n# Introduction\nLarge Language Models (LLMs) have achieved state-of-theart performance across a wide range of natural language processing tasks (Achiam et al. 2023; Dubey et al. 2024; AnthropicAI 2023). One of the key factors contributing to this success is their capability for in-context learning, which allows these models to adapt to new tasks without additional training (Xie et al. 2021). However, their performance is highly sensitive to the quality of the provided demonstrations. Recently, various demonstration selection algorithms have been developed to enhance this quality by selecting the most informative and relevant examples from the data pool. These algorithms have significantly reduced the time required for LLMs to address unseen tasks and have greatly improved their overall performance. Despite the success of these approaches, the effectiveness of selected examples and the efficiency of the selection and inference processes are not well understood. This Copyright \u00a9 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nDemonstration\nDemonstration 2\nDemonstration 1\nPositive\nNegative\nNeutral\nInput Question\n_____\nLLM\nPositive\nPrediction\nData Pool\n...\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a6a/9a6aaefa-a584-4065-84c8-67b6183637fe.png\" style=\"width: 50%;\"></div>\nFigure 1: An overview of demonstration selection algorithms: These algorithms select demonstrations from the data pool, which the LLMs then use to generate answers.\nlack of understanding makes it challenging for future research to identify areas for improvement and makes it difficult to use these algorithms in real-life situations. In this paper, we present a comparative analysis of six prominent demonstration selection algorithms, evaluating them on five diverse datasets from both efficiency and effectiveness perspectives. Our study aims to provide valuable insights into the strengths and limitations of current approaches, serving as a benchmark for future research and guiding practitioners in selecting appropriate demonstration selection methods for their specific use cases. Our comparative experiments yield the following key findings: \u2022 Contrary to expectations, not all demonstration selection algorithms consistently outperform random selection. Some algorithms struggle to surpass the random selection in certain scenarios. \u2022 We observe substantial accuracy gaps between different algorithms on the same dataset with the same number of demonstrations. For instance, in the MRPC dataset, this accuracy difference can be as high as 45% when comparing CBDS with RD-direct. \u2022 Our analysis reveals that increasing the number of demonstrations does not always lead to better performance. The relationship between the number of demonstrations and accuracy is not monotonic and varies significantly across different tasks and algorithms. \u2022 We find that while some algorithms like CBDS and UPRISE achieve high accuracy, they do so at the cost of poor efficiency, requiring over 5 and 3 seconds respectively per sample for demonstration selection. This trade-off poses challenges for real-world applications where quick response times are crucial.\nlack of understanding makes it challenging for future research to identify areas for improvement and makes it difficult to use these algorithms in real-life situations. In this paper, we present a comparative analysis of six prominent demonstration selection algorithms, evaluating them on five diverse datasets from both efficiency and effectiveness perspectives. Our study aims to provide valuable insights into the strengths and limitations of current approaches, serving as a benchmark for future research and guiding practitioners in selecting appropriate demonstration selection methods for their specific use cases. Our comparative experiments yield the following key findings:\n# Demonstration Selection Algorithms\nIn this study, we compare six demonstration selection algorithms for in-context learning: Concept Based Demonstration Selection (CBDS), Rethinking Demonstrations direct (RD-direct) and channel (RD-channel), LLM Retriever, UPRISE, and OpenICL TopK. These algorithms employ various strategies, from leveraging latent concepts to using retrieval models, to select the best examples for LLMs. Additionally, we include OpenICL Random as a baseline, which randomly selects examples from the data pool. Here are more details of the comparing algorithms: \u2022 Concept Based Demonstration Selection (CBDS) (Wang et al. 2024b): This algorithm proposes a Bayesian approach to select demonstration examples for in-context learning in LLMs. The approach involves two main steps: (1) The optimal value of the latent concept variable \u03b8 is learned as a set of new token embeddings using a small LLM. This step aims to align the latent concept variable with the token embedding space of the LLM. (2) After learning the optimal latent concept, the algorithm selects demonstrations that maximize the likelihood of inferring the optimal latent variable for the task at hand. These selected demonstrations can then be used with larger LLMs to improve performance. The selection process is mathematically grounded in the following equation:\nIn this study, we compare six demonstration selection algorithms for in-context learning: Concept Based Demonstration Selection (CBDS), Rethinking Demonstrations direct (RD-direct) and channel (RD-channel), LLM Retriever, UPRISE, and OpenICL TopK. These algorithms employ various strategies, from leveraging latent concepts to using retrieval models, to select the best examples for LLMs. Additionally, we include OpenICL Random as a baseline, which randomly selects examples from the data pool. Here are more details of the comparing algorithms:\n Concept Based Demonstration Selection (CBDS) (Wang et al. 2024b): This algorithm proposes a Bayesian approach to select demonstration examples for in-context learning in LLMs. The approach involves two main steps: (1) The optimal value of the latent concept variable \u03b8 is learned as a set of new token embeddings using a small LLM. This step aims to align the latent concept variable with the token embedding space of the LLM. (2) After learning the optimal latent concept, the algorithm selects demonstrations that maximize the likelihood of inferring the optimal latent variable for the task at hand. These selected demonstrations can then be used with larger LLMs to improve performance. The selection process is mathematically grounded in the following equation:\n(1)\n Rethinking Demonstrations (Min et al. 2022): This approach examines the factors that contribute to the success of in-context learning in LLMs. It explores the impact of different aspects of demonstrations, particularly focusing on the distribution of the input text, the label space, and the overall format. The key findings are encapsulated in the following:\nP(y | x1, y1, . . . , xk, yk, x) \u2248format + label space + input distribution (2\n(2)\nThis formula reflects the idea that in-context learning performance is driven by the structure and content of the demonstrations, rather than the accuracy of the label pairings.  LLM Retriever (Wang, Yang, and Wei 2023): This framework focuses on selecting high-quality in-context examples to enhance the performance of LLMs. The key approach involves an iterative process where an initial set of example candidates is retrieved using an unsupervised method like BM25 (Robertson, Zaragoza et al. 2009). These candidates are then ranked based on the conditional log probabilities of the ground truth outputs pro-\nThis formula reflects the idea that in-context learning performance is driven by the structure and content of the demonstrations, rather than the accuracy of the label pair-\n LLM Retriever (Wang, Yang, and Wei 2023): This framework focuses on selecting high-quality in-context examples to enhance the performance of LLMs. The key approach involves an iterative process where an initial set of example candidates is retrieved using an unsupervised method like BM25 (Robertson, Zaragoza et al. 2009). These candidates are then ranked based on the conditional log probabilities of the ground truth outputs pro-\nvided by the LLM. The ranking is formalized in the following equation:\n(3)\n | \u2200 \u2208{} where p(y | x, xi, yi) represents the conditional probability of the output y given the input x and the i-th candidate example (xi, yi). A reward model, based on a cross-encoder architecture, is then trained to distill these ranking preferences into a dense retriever. This retriever is further refined iteratively by leveraging the feedback from the LLM, ultimately improving the selection of incontext examples.  UPRISE (Cheng et al. 2023): UPRISE enhances the performance of LLMs in zero-shot settings by retrieving and utilizing effective prompts. The approach involves two main steps: first, the retriever retrieves a set of positive prompts P + from a pre-constructed pool P based on a given task input x, as formulated in the equation:\n(4)\nwhere R(x, P) is the retrieval function. Then, these retrieved prompts are concatenated with the input and fed into a frozen LLM to generate the output y:\n(5)\n | \u2295 This approach allows for cross-task and cross-model generalization, meaning the retriever is trained on diverse tasks with a smaller LLM but can be applied to larger LLMs and unseen tasks during inference.  OpenICL (Wu et al. 2023): It is designed to facilitate ICL research and improve the evaluation of LLMs. The core approach for selecting demonstration examples involves retrieving examples based on various methods like TopK, VoteK, and BM25, which are then used to construct the context for the LLM\u2019s inference. The retrieval of examples can be formalized as:\n \u2208 where R(X, Y ) represents the retrieval function applied to the training data X and Y . The selected examples are then concatenated with the test input to form a single sequence, which is fed into the LLM to generate predictions. In this paper, we adopt the TopK and Random retrieval strategies. Notice that the RD algorithm supports both direct and channel approaches for demonstrating in-context examples, denoted as E. As the Figure 2 shows, for each example ei in E, it is paired with a input x and a label y. In the direct approach, ei is structured with x presented first, followed by y. Conversely, in the channel approach, ei is structured with y presented first, followed by x. It is also important to note that we used \u201cdemonstrations with gold labels\u201d for both the direct and channel approaches. As for other algorithms, we use the direct approach. By evaluating these diverse approaches, we aim to provide a comprehensive analysis of current demonstration selection methods and their impact on LLM performance.\n<div style=\"text-align: center;\">Table 1: Datasets Statistics</div>\nDATASETS\nMRPC\nQNLI\nSST2\nCMSQA\nSWAG\nTask\nClassification\nClassification\nClassification\nMulti-choice\nMulti-choice\n# Training Set\n3,670\n104,743\n67,349\n9,741\n73,546\n# Validation Set\n408\n5,463\n872\n1,221\n20,000\n# Test Set\n1,730\n5,463\n1,821\n1,284\n20,000\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/652c/652cf329-9356-42c8-adc6-16bad0ddd80e.png\" style=\"width: 50%;\"></div>\nFigure 2: An visual understanding the difference between direct and channel approach\n# Experiments\nIn this section, we present a comprehensive evaluation of the six demonstration selection algorithms and a baseline random selection method.\n# Experimental Settings\nDatasets: The demonstration selection algorithms will be evaluated on 5 datasets, categorized into two groups: classification and multi-choice. The classification datasets include GLUE-MRPC, GLUE-QNLI, and GLUE-SST2. The multichoice datasets are CMSQA and SWAG. Their statistic is shown in Table 1, and below are more details of the datasets: \u2022 GLUE-MRPC (Wang et al. 2018): MRPC is a dataset consisting of sentence pairs, each annotated with a binary label indicating whether the sentences in the pair are semantically equivalent. \u2022 GLUE-QNLI: QNLI dataset is derived from the Stanford Question Answering Dataset (SQuAD). It is a binary classification task where the goal is to determine whether the context sentence contains the answer to the question. \u2022 GLUE-SST2: SST-2 is a binary sentiment classification dataset that includes movie review excerpts, where the task is to predict whether the sentiment of the review is positive or negative. \u2022 CMSQA (Talmor et al. 2018): The CommonsenseQA dataset consists of multiple-choice questions that require commonsense knowledge. Each question is paired with five answer choices, where only one is correct. \u2022 SWAG (Zellers et al. 2018): SWAG dataset is a largescale benchmark for grounded commonsense inference.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/493a/493a1a0c-e89c-497c-bfed-84f0d0a5a0d0.png\" style=\"width: 50%;\"></div>\nFigure 3: The effectiveness of the algorithms on MRPC dataset\n<div style=\"text-align: center;\">Figure 3: The effectiveness of the algorithms on MRPC dataset</div>\nIt contains multiple-choice questions about grounded situations, where the task is to select the most plausible continuation of a given scenario.\nMetrics: Given that all datasets are either classification or multi-choice, accuracy is an appropriate metric for evaluating the performance. To show the algorithm\u2019s computational efficiency, we present the results in seconds.\nImplementation Details: All of the demonstration selection algorithms were tested on LLaMa3-8B (Touvron et al. 2023) to evaluate their effectiveness. As shown in Figure 1, each algorithm first selects k demonstration examples from the available data pool based on the input question, which serve as in-context learning exemplars. These exemplars, along with the input question, are then fed into LLaMa3 for evaluation. We measured the absolute time each algorithm took to select k demonstration examples per single input, as well as the model inference time for the same input. To ensure fairness, we excluded all offline computational time, such as pre-training or other preliminary computations, for all algorithms. The values of k considered in this study were {4, 8, 10, 20}. All experiments were conducted using an NVIDIA RTX A6000 GPU, with the LLaMa3-8B model directly loaded from Huggingface, and we used the default sampling parameters such as temperature and top p.\n# Main Results\nTable 2 presents the effectiveness of each demonstration selection algorithm across different datasets and varying numbers of k in-context examples. The highest accuracy in each column is highlighted in bold format. We evaluated the computational efficiency of the six algorithms and a baseline,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cbd0/cbd0c1fd-33d3-4e1d-a232-cd1e37895545.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: An visualize trend of the algorithms effectiveness</div>\n<div style=\"text-align: center;\">Table 2: Effectiveness of the Demonstration Selection Algorithms</div>\nDatasets\nMRPC\nQNLI\nCMSQA\nSWAG\nSST2\nk=4 k=8 k=10 k=20 k=4 k=8 k=10 k=20 k=4 k=8 k=10 k=20 k=4 k=8 k=10 k=20 k=4 k=8 k=10 k=20\nCBDS\n62.3 80.8 78.8\n57.3 51.1 50.7 54.4\n54.8 29.2 29.4 34.4\n43.2 36.8 38.1 34.6\n36.9 91.4 93.9 93.5\n73.2\nRD-direct\n40.6 40.6 34.2\n34.0 33.6 35.1 35.9\n35.5 42.3 42.8 43.1\n41.2 56.2 56.6 57.3\n56.4 91.9 75.7 94.6\n83.8\nRD-channel\n44.0 45.1 45.3\n42.9 46.7 47.8 44.9\n38.8 53.6 53.4 56.6\n48.8 53.2 52.0 51.8\n57.8 91.2 87.9 79.3\n76.4\nLLM Retriever\n74.7 75.4 75.9\n76.2 71.5 72.3 72.9\n72.7 50.6 52.2 52.8\n53.8 65.1 64.7 64.9\n64.7 93.7 93.8 93.6\n93.5\nUPRISE\n75.2 74.7 75.9\n76.0 81.8 81.9 81.8\n81.1 61.4 63.2 64.4\n66.2 69.0 66.8 69.9\n61.9 93.4 92.5 92.4\n91.3\nOpenICL TopK\n64.1 64.2 65.0\n65.2 78.1 74.7 70.8\n69.9 34.2 37.6 36.7\n32.9 35.4 36.7 37.1\n34.9 91.2 92.1 92.3\n94.5\nRandom Selection 59.0 62.6 60.1\n62.3 51.8 48.3 47.9\n47.4 26.7 28.1 28.1\n28.5 29.6 31.8 31.4\n31.1 86.2 89.5 91.3\n91.7\nand report the results in Table 3. We will show our findings from two perspectives: effectiveness and efficiency.\nEffectiveness Perspective: Contrary to expectations, not all demonstration selection algorithms consistently outperform random selection. While some algorithms show significant improvements, others struggle to surpass the baseline set by random selection in certain scenarios. For instance, as shown in Table 2, we observed that for the MRPC dataset, both the RD-direct and RD-channel algorithms demonstrate limited effectiveness compared to random selection. This finding challenges the assumption that sophisticated selection methods always yield better results and highlights the importance of thorough evaluation before implementation. Another notable observation arises when we plot the effectiveness of each algorithm on the MRPC dataset, as shown in Figure 3. There is a substantial accuracy gap between different algorithms on the same dataset with the same k value. In the MRPC dataset, this accuracy difference can be as high as 45%, as seen when comparing CBDS with k = 10 to RDdirect with k = 10. A closer examination of the SST2 dataset in Table 2 reveals that for simpler classification tasks, the effectiveness of demonstration selection algorithms is often limited and does not significantly improve accuracy compared to more challenging multi-choice tasks like CommonsenseQA and SWAG. Furthermore, the RD algorithm provides both direct and channel approaches for presenting demonstration examples to the model. In our comparisons, we used \u201cdemonstrations with gold labels\u201d for both the direct and channel approaches. We found that the channel approach generally outperforms the direct approach. This observation aligns with the results presented in Figures 8-10 of the original paper (Min et al. 2022). A possible explanation for this is that the channel ap-\nproach better aligns the input features with the class labels, making it easier for the model to capture and understand the underlying relationships. Our analysis also reveals that increasing the number of demonstrations (k) does not always lead to better performance. The relationship between k and accuracy is not monotonic and varies significantly across tasks and algorithms. In Figure 4, we visualize the effectiveness trends of six algorithms on the CMSQA dataset. We observed two primary patterns: (1) as shown on the left, performance peaks at a certain k before declining, and (2) on the right, accuracy continues to improve as k increases. For the pattern on the left, we hypothesize that for some tasks, an excessive number of in-context examples may introduce conflicting knowledge, either with the model\u2019s internal knowledge or among the examples themselves. Additionally, some examples might be irrelevant to the input question and may act as noise. This finding underscores the importance of selecting an appropriate k value tailored to the specific task. Thus, it is crucial to tune the number of demonstrations for each specific task and algorithm combination. Efficiency Perspective: We also evaluated the computational efficiency of the six algorithms and a baseline. As shown in Table 3, we measured efficiency in two ways: (1) the absolute demonstration selection time (selection) required for an algorithm to select k in-context examples for a single input question, and (2) the absolute inference time (inference) required for the model to generate an answer for the same input question. All efficiency experiments were conducted with k = 10. Among the compared algorithms, CBDS and UPRISE stand out for their high accuracy but also for their poor efficiency. On average, these algorithms require over 5 seconds and 3 seconds respectively per sample for demonstra-\nproach better aligns the input features with the class labels, making it easier for the model to capture and understand the underlying relationships. Our analysis also reveals that increasing the number of demonstrations (k) does not always lead to better performance. The relationship between k and accuracy is not monotonic and varies significantly across tasks and algorithms. In Figure 4, we visualize the effectiveness trends of six algorithms on the CMSQA dataset. We observed two primary patterns: (1) as shown on the left, performance peaks at a certain k before declining, and (2) on the right, accuracy continues to improve as k increases. For the pattern on the left, we hypothesize that for some tasks, an excessive number of in-context examples may introduce conflicting knowledge, either with the model\u2019s internal knowledge or among the examples themselves. Additionally, some examples might be irrelevant to the input question and may act as noise. This finding underscores the importance of selecting an appropriate k value tailored to the specific task. Thus, it is crucial to tune the number of demonstrations for each specific task and algorithm combination.\nEfficiency Perspective: We also evaluated the computational efficiency of the six algorithms and a baseline. As shown in Table 3, we measured efficiency in two ways: (1) the absolute demonstration selection time (selection) required for an algorithm to select k in-context examples for a single input question, and (2) the absolute inference time (inference) required for the model to generate an answer for the same input question. All efficiency experiments were conducted with k = 10. Among the compared algorithms, CBDS and UPRISE stand out for their high accuracy but also for their poor efficiency. On average, these algorithms require over 5 seconds and 3 seconds respectively per sample for demonstra-\n<div style=\"text-align: center;\">Table 3: Efficiency of the Demonstration Selection Algorithms</div>\nDatasets\nMRPC\nQNLI\nCMSQA\nSWAG\nSST2\nselection\ninference\nselection\ninference\nselection\ninference\nselection\ninference\nselection\ninference\nCBDS\n5.44\n1.82\n5.35\n1.80\n5.43\n4.58\n5.45\n3.63\n5.44\n1.82\nRD-direct\n2.34 \u221710\u22123\n1.80\n2.37 \u221710\u22123\n1.81\n2.44 \u221710\u22123\n4.56\n2.50 \u221710\u22123\n3.63\n1.44 \u221710\u22123\n1.78\nRD-channel\n2.24 \u221710\u22123\n1.80\n2.50 \u221710\u22123\n1.79\n2.49 \u221710\u22123\n4.55\n2.46 \u221710\u22123\n3.62\n1.44 \u221710\u22123\n1.77\nLLM Retriever\n8.32 \u221710\u22121\n12.81\n6.74 \u221710\u22121\n12.84\n8.47 \u221710\u22121\n12.87\n8.83 \u221710\u22121\n11.85\n8.16 \u221710\u22121\n12.62\nUPRISE\n3.06\n3.67\n3.23\n3.66\n3.25\n3.69\n3.09\n3.68\n3.62\n3.43\nOpenICL TopK\n1.46 \u221710\u22121\n1.20\n2.22 \u221710\u22121\n1.19\n1.48 \u221710\u22121\n2.94\n2.60 \u221710\u22121\n2.61\n4.79 \u221710\u22121\n0.81\nOpenICL Random\n5.24 \u221710\u22126\n1.11\n5.63 \u221710\u22126\n1.10\n5.34 \u221710\u22126\n3.04\n6.48 \u221710\u22126\n2.61\n8.11 \u221710\u22126\n0.61\ntion selection. Referring to Table 2, out of 20 combinations (4 k values \u00d7 5 datasets), CBDS or UPRISE achieved the best accuracy in 14 cases. However, this superior accuracy comes at the cost of extremely high computational complexity. The low efficiency of these algorithms makes them challenging to apply in real-world applications where quick response times are crucial. We also found out that five algorithms are relatively efficient, taking less than one second per input question to select 10 examples. Notably, the OpenICL Random algorithm, along with both the RD-direct and RD-channel approaches, demonstrated particularly fast demonstration selection times. However, when cross-referencing these results with Table 2, it becomes clear that this speed often comes at a cost to accuracy. These faster algorithms generally do not perform as effectively as others. On the other hand, the LLM Retriever achieves a more balanced trade-off, offering reasonable demonstration selection speed while maintaining good overall effectiveness. We also observed variations in inference times across different algorithms. As shown in the \u2018inference\u2019 column, the CBDS and RD algorithms exhibit similar inference times, as both utilize model inference code from the MetaICL codebase (Min et al. 2021). An interesting finding emerges when comparing inference times for different datasets within the same algorithm: for most algorithms, including CBDS, RD, and OpenICL, inference times increase for more complex tasks, such as CMSQA and SWAG, compared to simpler binary classification tasks. As for the LLM Retriever, although it demonstrates fast demonstration selection times, the overall computational time remains slower when factoring in inference time. This slowdown is likely due to the constrained generation technique employed by the algorithm (Celikyilmaz, Clark, and Gao 2020). Specifically, when employing constrained generation with a prefix trie, the model needs to check each generated token against the trie to ensure it aligns with the allowed prefixes. This additional step can slow down the generation process because it introduces extra computational overhead to enforce these constraints. This finding suggests that future research should not only focus on improving demonstration selection efficiency but also consider optimizing model inference time to achieve better overall computational performance.\n# Related Work\nIn-context learning has emerged as a powerful capability of LLMs, enabling them to adapt to new tasks without fine-tuning, but the effectiveness of this approach heavily depends on the quality and relevance of the provided demonstrations. With numerous demonstration selection algorithms in the field, the optimal method for selecting these examples remains an open question. For instance, Wang et al. (2024b) emphasize aligning demonstrations with the model\u2019s latent structure, improving performance by uncovering latent variable explanations. Zhang, Feng, and Tan (2022); Qin et al. (2023) employ an active and iterative selection approach to identify the most informative examples. Retrieval-based methods are utilized by Wang, Yang, and Wei (2023); Wang et al. (2024a); Li et al. (2023); Xu et al. (2024) to select demonstrations. Liu, Zhu, and Dou (2024) adopt a ranking mechanism that considers relevance, similarity, and diversity for optimal demonstration selection. Ye et al. (2023) suggest constructing demonstrations through the combination of simpler examples to better capture complex task dynamics. Van, Wu et al. (2024) introduce InfICL, which uses influence functions to identify the most impactful examples for selection. Liu et al. (2024) uncover the importance of task-agnostic multi-level similarity and taskspecific label similarities, proposing methods that integrate these factors for improved performance across diverse tasks. Additionally, Kim et al. (2022); Zhou et al. (2024) introduce the concept of self-generated demonstrations, which involves generating examples. Together, these works underscore the critical role of demonstration selection in enhancing the effectiveness of in-context learning across varied applications. Our work differs from previous studies by providing a comparative analysis of multiple demonstration selection algorithms across various tasks and datasets. While existing literature has primarily focused on developing individual algorithms or comparing a limited set of methods, we offer a holistic evaluation of both the effectiveness and efficiency of six demonstration selection algorithms.\n# Conclusions and Future Work\nIn this paper, our comprehensive evaluation of demonstration selection algorithms reveals significant variations in both effectiveness and efficiency across different tasks and datasets. While some algorithms show promising results, the\ntrade-offs between accuracy and computational cost present challenges for real-world applications. Our findings highlight the need for task-specific optimization and suggest potential avenues for future research. These include developing adaptive algorithms that can dynamically adjust the number of demonstrations based on task complexity, integrating more advanced retrieval techniques, and exploring methods to balance effectiveness with computational efficiency.\n# Acknowledgment\nThe work is in part supported by NSF #2310261. The views and conclusions in this paper are those of the authors and should not be interpreted as representing funding agencies.\n# References\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. AnthropicAI. 2023. Introducing claude. Celikyilmaz, A.; Clark, E.; and Gao, J. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799. Cheng, D.; Huang, S.; Bi, J.; Zhan, Y.; Liu, J.; Wang, Y.; Sun, H.; Wei, F.; Deng, D.; and Zhang, Q. 2023. Uprise: Universal prompt retrieval for improving zero-shot evaluation. arXiv preprint arXiv:2303.08518. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Kim, H. J.; Cho, H.; Kim, J.; Kim, T.; Yoo, K. M.; and Lee, S.-g. 2022. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082. Li, X.; Lv, K.; Yan, H.; Lin, T.; Zhu, W.; Ni, Y.; Xie, G.; Wang, X.; and Qiu, X. 2023. Unified demonstration retriever for in-context learning. arXiv preprint arXiv:2305.04320. Liu, H.; Wang, W.; Sun, H.; Tian, C. X.; Kong, C.; Dong, X.; and Li, H. 2024. Unraveling the Mechanics of LearningBased Demonstration Selection for In-Context Learning. arXiv preprint arXiv:2406.11890. Liu, W.; Zhu, Y.; and Dou, Z. 2024. DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task. arXiv preprint arXiv:2406.16332. Min, S.; Lewis, M.; Zettlemoyer, L.; and Hajishirzi, H. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943. Min, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.; Hajishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837. Qin, C.; Zhang, A.; Dagar, A.; and Ye, W. 2023. In-context learning with iterative demonstration selection. arXiv preprint arXiv:2310.09881.\nAnadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. AnthropicAI. 2023. Introducing claude. Celikyilmaz, A.; Clark, E.; and Gao, J. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799. Cheng, D.; Huang, S.; Bi, J.; Zhan, Y.; Liu, J.; Wang, Y.; Sun, H.; Wei, F.; Deng, D.; and Zhang, Q. 2023. Uprise: Universal prompt retrieval for improving zero-shot evaluation. arXiv preprint arXiv:2303.08518. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Kim, H. J.; Cho, H.; Kim, J.; Kim, T.; Yoo, K. M.; and Lee, S.-g. 2022. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082. Li, X.; Lv, K.; Yan, H.; Lin, T.; Zhu, W.; Ni, Y.; Xie, G.; Wang, X.; and Qiu, X. 2023. Unified demonstration retriever for in-context learning. arXiv preprint arXiv:2305.04320. Liu, H.; Wang, W.; Sun, H.; Tian, C. X.; Kong, C.; Dong, X.; and Li, H. 2024. Unraveling the Mechanics of LearningBased Demonstration Selection for In-Context Learning. arXiv preprint arXiv:2406.11890. Liu, W.; Zhu, Y.; and Dou, Z. 2024. DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task. arXiv preprint arXiv:2406.16332. Min, S.; Lewis, M.; Zettlemoyer, L.; and Hajishirzi, H. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943. Min, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.; Hajishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837. Qin, C.; Zhang, A.; Dagar, A.; and Ye, W. 2023. In-context learning with iterative demonstration selection. arXiv preprint arXiv:2310.09881.\nRobertson, S.; Zaragoza, H.; et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval, 3(4): 333\u2013389. Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Van, M.-H.; Wu, X.; et al. 2024. In-Context Learning Demonstration Selection via Influence Analysis. arXiv preprint arXiv:2402.11750. Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Wang, H.; Wu, J.; Sun, H.; Xia, Z.; Cheng, D.; Wang, J.; Qi, Q.; and Liao, J. 2024a. MDR: Model-Specific Demonstration Retrieval at Inference Time for In-Context Learning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 4189\u20134204. Wang, L.; Yang, N.; and Wei, F. 2023. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164. Wang, X.; Zhu, W.; Saxon, M.; Steyvers, M.; and Wang, W. Y. 2024b. Large language models are latent variable models: Explaining and finding good demonstrations for incontext learning. Advances in Neural Information Processing Systems, 36. Wu, Z.; Wang, Y.; Ye, J.; Feng, J.; Xu, J.; Qiao, Y.; and Wu, Z. 2023. Openicl: An open-source framework for in-context learning. arXiv preprint arXiv:2303.02913. Xie, S. M.; Raghunathan, A.; Liang, P.; and Ma, T. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. Xu, X.; Liu, Y.; Pasupat, P.; Kazemi, M.; et al. 2024. Incontext learning with retrieved demonstrations for language models: A survey. arXiv preprint arXiv:2401.11624. Ye, J.; Wu, Z.; Feng, J.; Yu, T.; and Kong, L. 2023. Compositional exemplars for in-context learning. In International Conference on Machine Learning, 39818\u201339833. PMLR. Zellers, R.; Bisk, Y.; Schwartz, R.; and Choi, Y. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326. Zhang, Y.; Feng, S.; and Tan, C. 2022. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486. Zhou, X.; Ye, W.; Wang, Y.; Jiang, C.; Lee, Z.; Xie, R.; and Zhang, S. 2024. Enhancing In-Context Learning via Implicit Demonstration Augmentation. arXiv preprint arXiv:2407.00100.\nRobertson, S.; Zaragoza, H.; et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval, 3(4): 333\u2013389. Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Van, M.-H.; Wu, X.; et al. 2024. In-Context Learning Demonstration Selection via Influence Analysis. arXiv preprint arXiv:2402.11750. Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Wang, H.; Wu, J.; Sun, H.; Xia, Z.; Cheng, D.; Wang, J.; Qi, Q.; and Liao, J. 2024a. MDR: Model-Specific Demonstration Retrieval at Inference Time for In-Context Learning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 4189\u20134204. Wang, L.; Yang, N.; and Wei, F. 2023. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164. Wang, X.; Zhu, W.; Saxon, M.; Steyvers, M.; and Wang, W. Y. 2024b. Large language models are latent variable models: Explaining and finding good demonstrations for incontext learning. Advances in Neural Information Processing Systems, 36. Wu, Z.; Wang, Y.; Ye, J.; Feng, J.; Xu, J.; Qiao, Y.; and Wu, Z. 2023. Openicl: An open-source framework for in-context learning. arXiv preprint arXiv:2303.02913. Xie, S. M.; Raghunathan, A.; Liang, P.; and Ma, T. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. Xu, X.; Liu, Y.; Pasupat, P.; Kazemi, M.; et al. 2024. Incontext learning with retrieved demonstrations for language models: A survey. arXiv preprint arXiv:2401.11624. Ye, J.; Wu, Z.; Feng, J.; Yu, T.; and Kong, L. 2023. Compositional exemplars for in-context learning. In International Conference on Machine Learning, 39818\u201339833. PMLR. Zellers, R.; Bisk, Y.; Schwartz, R.; and Choi, Y. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326. Zhang, Y.; Feng, S.; and Tan, C. 2022. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486. Zhou, X.; Ye, W.; Wang, Y.; Jiang, C.; Lee, Z.; Xie, R.; and Zhang, S. 2024. Enhancing In-Context Learning via Implicit Demonstration Augmentation. arXiv preprint arXiv:2407.00100.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The performance of Large Language Models (LLMs) in in-context learning is highly sensitive to the quality of demonstration examples. This sensitivity has led to the development of various demonstration selection algorithms aimed at optimizing the selection of these examples. However, the effectiveness and efficiency of these algorithms remain unclear, complicating their application in real-world scenarios.",
            "purpose of benchmark": "The benchmark is intended to provide a comparative analysis of demonstration selection algorithms, evaluating their effectiveness and efficiency across different datasets to guide practitioners in selecting appropriate methods for their specific use cases."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of selecting the most effective demonstration examples for LLMs to enhance their in-context learning capabilities.",
            "key obstacle": "Existing benchmarks do not adequately assess the efficiency and effectiveness of demonstration selection algorithms, making it difficult to apply these methods in practice."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the need to better understand the performance variations among different demonstration selection algorithms and their impact on LLMs.",
            "opinion": "The authors believe that this benchmark is crucial for future research as it will provide insights into the strengths and weaknesses of current demonstration selection methods.",
            "innovation": "This benchmark differs from previous ones by providing a comprehensive evaluation of multiple demonstration selection algorithms across various tasks and datasets, emphasizing both effectiveness and efficiency.",
            "benchmark abbreviation": "DSL"
        },
        "dataset": {
            "source": "The datasets used in the benchmark are sourced from existing benchmarks in natural language processing, including MRPC, QNLI, SST2, CMSQA, and SWAG.",
            "desc": "The benchmark consists of five datasets, with varying sizes and characteristics suitable for classification and multi-choice tasks.",
            "content": "The datasets include types of data such as text pairs for classification tasks and multiple-choice questions for commonsense reasoning.",
            "size": "104,743",
            "domain": "Text Classification",
            "task format": "Classification"
        },
        "metrics": {
            "metric name": "Accuracy, Selection Time",
            "aspect": "The metrics measure the accuracy of model predictions and the computational efficiency of the demonstration selection process.",
            "principle": "The choice of metrics is guided by the need to evaluate both the performance of the models in terms of accuracy and the time taken for demonstration selection, which is crucial for real-world applications.",
            "procedure": "Models are evaluated using the selected metrics through systematic experimentation on the datasets, measuring both accuracy and the time taken for demonstration selection."
        },
        "experiments": {
            "model": "The models tested include various demonstration selection algorithms such as CBDS, RD-direct, RD-channel, LLM Retriever, UPRISE, and OpenICL TopK.",
            "procedure": "Each algorithm was evaluated on five datasets, with a focus on their accuracy and selection time across different values of k (the number of demonstrations).",
            "result": "The experiments revealed significant variations in algorithm performance, with some algorithms not consistently outperforming random selection.",
            "variability": "Variability in results was accounted for by conducting multiple trials and assessing performance across different datasets and task complexities."
        },
        "conclusion": "The analysis highlights the trade-offs between accuracy and computational efficiency among different demonstration selection algorithms, underscoring the need for task-specific optimization.",
        "discussion": {
            "advantage": "The benchmark provides a detailed comparative analysis that aids in understanding the strengths and limitations of current demonstration selection methods.",
            "limitation": "The benchmark may not encompass all possible demonstration selection algorithms, and the findings may vary with different datasets or tasks not included in this study.",
            "future work": "Future research should focus on developing adaptive algorithms that can optimize the number of demonstrations based on task complexity and enhancing retrieval techniques."
        },
        "other info": {
            "acknowledgment": "This work is supported by NSF #2310261."
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The benchmark provides a comparative analysis of demonstration selection algorithms, evaluating their effectiveness and efficiency across different datasets to guide practitioners in selecting appropriate methods for their specific use cases."
        },
        {
            "section number": "3.1",
            "key information": "The performance of Large Language Models (LLMs) in in-context learning is highly sensitive to the quality of demonstration examples."
        },
        {
            "section number": "4.2",
            "key information": "Existing benchmarks do not adequately assess the efficiency and effectiveness of demonstration selection algorithms, making it difficult to apply these methods in practice."
        },
        {
            "section number": "6.1",
            "key information": "The analysis highlights the trade-offs between accuracy and computational efficiency among different demonstration selection algorithms, underscoring the need for task-specific optimization."
        },
        {
            "section number": "2.1",
            "key information": "The datasets used in the benchmark are sourced from existing benchmarks in natural language processing, including MRPC, QNLI, SST2, CMSQA, and SWAG."
        }
    ],
    "similarity_score": 0.7333711160426135,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning.json"
}