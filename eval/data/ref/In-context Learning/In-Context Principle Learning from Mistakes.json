{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.05403",
    "title": "In-Context Principle Learning from Mistakes",
    "abstract": "In-context learning (ICL, also known as fewshot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then the model itself reflects on these mistakes, and learn explicit taskspecific \u201cprinciples\u201d from them without any human supervision, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4-turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.",
    "bib_name": "zhang2024incontextprinciplelearningmistakes",
    "md_text": "# In-Context Principle Learning from Mistakes\nTianjun Zhang * 1 Aman Madaan * 2 Luyu Gao * 2 Steven Zheng 3 Swaroop Mishra 3 Yiming Yang 2 Niket Tandon 4 Uri Alon 3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f82/9f8273d3-a9d2-459b-ab5d-fa2086a1adb8.png\" style=\"width: 50%;\"></div>\nLearned Principle\nBenchmark\nWhen solving a problem involving multiple\nsources of income and expenses, it is crucial\nto keep track of each component separately\nand calculate the total accurately.\nGSM8K\nWhen simplifying complex numbers raised\nto powers, it is important to remember the\nfollowing rules: (1) i2 = \u22121 (2) i3 = \u2212i (3)\ni4 = 1 (4) i\u22121 = 1\ni = \u2212i\nMATH\nPerform calculations using the full precision\navailable and only round as a final step, if\nnecessary.\nDROP\nWhen answering questions about commonal-\nities between two entities, it is important to\nconsider all relevant aspects and not just the\nmost obvious or prominent one.\nHotpotQA\nDouble negation, as in \u2018not not\u2019, cancels out\nand returns the original value.\nBoolean Expr-\nessions (BBH)\nSarcasm often involves saying the opposite\nof what is meant ... Paying attention to the\nincongruity between the literal meaning of\nthe words and the intended meaning can help\nin accurately identifying sarcasm.\nSnarks\n(BBH)\n# Abstract\nIn-context learning (ICL, also known as fewshot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then the model itself reflects on these mistakes, and learn explicit taskspecific \u201cprinciples\u201d from them without any human supervision, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4-turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.\narXiv:2402.05403v2\n# 1. Introduction\n# The rise of large language models (LLMs; Radford et al., 2019; Chowdhery et al., 2022; Zhang et al., 2022; Li et al., 2022; Anil et al., 2023; Touvron et al., 2023a;b) that are\n*Equal contribution 1UC Berkeley 2Carnegie Mellon University 3Google DeepMind 4AI2. Correspondence to: Tianjun Zhang <tianjunz@berkeley.edu>, Aman Madaan <amadaan@cs.cmu.edu>, Uri Alon <urialon@google.com>.\nFigure 1: Examples for learned principles using LEAP, with key idea of each principle highlighted.\ntoo costly to finetune for downstream tasks has led to the growing popularity of in-context learning (ICL), also known as few-shot prompting (Brown et al., 2020; Liu et al., 2023; Wei et al., 2023). In in-context learning, the LLM is provided with a few (e.g., three) input-output task-specific examples in its prompt, along with an unseen test input. Using this emergent ability (Wei et al., 2022b), the LLM is then expected to generate output for the test input. The LLM generates this output by implicitly learning the task from the few given examples, at inference time. ICL was shown to be extremely effective and data-efficient across a variety of tasks and domains (Min et al., 2022a; Alayrac et al., 2022; Liu et al., 2021; Lu et al., 2023), mainly because it allows for downstream task-adaptation without training. Further, ICL enables generalization using only a few annotated examples. Nonetheless, all ICL-based ap-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/44d0/44d06af7-8e83-4f67-ae9a-ffb45d0651c4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Few-shot Chain-of-Thought (Wei et al., 2022)</div>\nFigure 2: An illustration of LEAP: Given a few input-output examples, Chain-of-Thought (left) generates a response to the test question by directly learning from the (correct) examples. In contrast, Learning Principles (LEAP, right) first (a) generates mistaken zero-shot Chain-of-Thought response for each given input-output example by sampling with a non-zero temperature; (b) generates explicit principles by providing the LLM with the mistaken COT along with the correct output; and finally (c) generates a response to the test question, by providing the LLM with both the given input-output examples and the learned principles. Note that steps (a) and (b) are performed once per task.\nproaches only learn from correct input-output pairs (Min et al., 2022b); in contrast, learning from mistakes is a foundational aspect of the way humans learn (Edmondson, 1996; Chialvo & Bak, 1999; Edmondson, 1999). In fact, learning from mistakes is also a fundamental concept of machine learning, which goes back to classical work such as Wiener (1948); Rosenblatt (1957) and Minsky & Papert (1969), but is not utilized by current ICL and prompting methods. Differently from machines, humans can often verbalize their mistakes and articulate explicit principles, or \u201clessons\u201d;\nthese principles can further help humans avoid these and similar mistakes in the future. Inspired by this ability and the benefits it provides, we propose Learning Principles (LEAP): A prompting approach for learning principles from mistakes, and then conditioning on these principles when responding to other inputs. Instead of providing the model only with the given (correct) few-shot examples, LEAP begins by (a) inducing the LLM to make mistakes on each of these given few-shot inputs, in a zero-shot fashion, by sampling outputs with a non-zero temperature; (b) generating explicit principles by providing the same LLM with the\nmistaken outputs along with the correct output; and finally (c) generating a response to the test question as in standard few-shot ICL, while providing the LLM with both the given input-output examples and the learned principles. LEAP is illustrated in Figure 2; examples for some of the learned principles are provided in Figure 1. Crucially, LEAP does not require any more input than the standard few-shot prompting settings. Further, the principle learning step is applied once for a task, and then the same learned principles are used for the entire test set. If the few-shot examples are given as Chain-of-Thought (CoT; Wei et al., 2022c), LEAP is applied seamlessly by generating mistaken chains-of-thought, contrasting them with the correct chains-of-thought, and generating principles. We demonstrate the effectiveness of LEAP across a wide range of reasoning benchmarks, including mathematical reasoning in GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), multi-hop question answering tasks in HotpotQA (Yang et al., 2018b), textual reasoning in DROP (Dua et al., 2019a), and 27 Big-Bench Hard (Suzgun et al., 2022) tasks. LEAP outperforms the standard practice of few-shot prompting using strong models such as GPT-3.5-turbo, GPT-4, GPT-4-turbo and Claude-2.1, with and without Chain-of-Thought. For example, LEAP improves over the standard few-shot CoT using GPT-4 by 7.5% in DROP, by 3.3% in HotpotQA, and on 16 tasks in Big-Bench Hard. These results suggest that LEAP revolutionizes the \u201ctraditional\u201d concept of few-shot ICL, by leveraging the recently emerged abilities of LLMs to follow instructions (Mishra et al., 2021; Wei et al., 2022a; Ouyang et al., 2022) and explain mistakes given the correct answer or feedback (Madaan et al., 2023; Chen et al., 2023).\n# 2. Background: In-Context Learning\nIn-context learning, also known as few-shot prompting, uses a few (typically between 3 and 8) input-output task-specific examples for teaching a pre-trained LLM to solve a downstream task (Brown et al., 2020; Liu et al., 2021; 2023). These input-output pairs P = {(xi, yi)}k i=1 are concatenated to form a prompt p = \u27e8x1 \u00b7y1\u27e9\u2295\u27e8x2 \u00b7y2\u27e9\u2295. . .\u2295\u27e8xk \u00b7 yk\u27e9, where \u201c\u00b7\u201d denotes the concatenation of each input with its corresponding output, and \u201c\u2295\u201d denotes the sequential combination of these pairs. Then, every new test input xtest is appended to this prompt, resulting in p \u2295\u27e8xtest\u00b7, which is then provided to the LLM . The LLM completes this input, and generates the corresponding output \u02c6ytest.\nTypically, the answer y additionally contains an explanation (or thought) for deriving the answer (Wei et al., 2022c). In these settings, each solution y contains a thought t and the final answer a, forming: yi = ti \u00b7 ai, and the LLM is expected to generate the test-thought before generating the\nfinal test-answer: ttest \u00b7 atest. In this work, we focus on this chain-of-thought setup, because of its widely demonstrated effectiveness for reasoning tasks (Wang et al., 2022a; Wei et al., 2022c; Zhou et al., 2022; Wang et al., 2022b).\n# 3. LEAP: Learning Principles from Mistakes\nIn LEAP, our goal is to learn general principles that help the model avoid potential mistakes in a downstream task. Given the few-shot examples P = {\u27e8xi, yi\u27e9}k i=1 for a certain task, we start by generating mistakes to the few given examples.\nGenerating Mistakes For each input-output pair \u27e8xi, yi\u27e9\u2208P, we generate a diverse set of solutions in a zero-shot fashion. Specifically, we follow Kojima et al. (2022), and create a zero-shot chain-of-thought prompt using xi and the phrase Think step-by-step. For each input xi, we sample n =15 outputs with a non-zero temperature, producing a varied set of potential solutions {\u02c6yj i }n j=1 for each example xi, such that \u02c6yj i = \u02c6tj i \u00b7 \u02c6aj i, where \u02c6tj i represents the intermediate reasoning steps (thoughts), and \u02c6aj i denotes the final answer.\nWe identify incorrect solutions by comparing each \u02c6aj i with the ground-truth answer ai (which is given as part of the task), forming a set of mistakes for each \u27e8xi, yi\u27e9\u2208P pair: Mi = \ufffd\ufffd xi, yi, \u02c6yj i \ufffd\ufffdn\u2032 , such that \u2200j : \u02c6aj i \u0338= ai.\nGenerating Low-Level Principles Then, for each such mistake in M, we prompt the LLM to generate a natural language explanation of the mistake. In this step, the LLM is provided with the ground truth answer yi, to articulate the rationale behind the solution\u2019s inaccuracy, as illustrated in Figure 3. The insights of how to avoid these mistakes are aggregated across all examples to form a set of low-level principles, denoted as LLOW-LEVEL.\n# Generating High-Level Principles\n# Generating High-Level Principles Subsequently, we u\nthe LLM to condense the low-level principles into approximately 5 key bullet points, thus creating high-level principles, denoted as LHIGH-LEVEL. The motivation for this step is generating generic, example-agnostic, principles for solving the downstream task, that do not depend on mistakes made for any specific example.\n# Final Inference on Unseen Examples These principles,\neither low or high-level, are then appended to the prompt p , forming enhanced prompts: pLOW-LEVEL = LLOW-LEVEL \u2295p for low-level feedback, and pHIGH-LEVEL = LHIGH-LEVEL \u2295p for high-level feedback, where p is the prompt constructed using the standard concatenation of the few-shot examples, as described in Section 2. Finally, we use the enhanced prompts p or\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9804/98048e8f-79a6-49cc-ba66-d6c431743784.png\" style=\"width: 50%;\"></div>\nFigure 3: LEAP prompt to help LLM evaluate its own generated reasoning and answers, contrasting them with the correct reasoning and answers. The LLM is prompted to identify errors in its reasoning and extract key insights for improvement. This figure specifically represents the \u2018GenerateExplanation\u2018 step in the LEAP algorithm (Algorithm 1).\npHIGH-LEVEL for answering all unseen examples in the test set. In effect, the one-time process of generating principles helps learn and articulate insights from the model\u2019s previous mistakes, potentially improving its accuracy and reasoning capabilities in future responses for other inputs. The complete algorithm is summarized in Algorithm 1. Although different LLMs can be used for each step, in all our experiments, we fixed the LLM across all steps: generating mistakes, generating principles from those mistakes, and testing using these self-generated principles. This ensures that the difference in results arises only from LEAP rather than any kind of model ensemble or teaching.\nThe complete algorithm is summarized in Algorithm 1. Although different LLMs can be used for each step, in all our experiments, we fixed the LLM across all steps: generating mistakes, generating principles from those mistakes, and testing using these self-generated principles. This ensures that the difference in results arises only from LEAP rather than any kind of model ensemble or teaching.\n# 4. Evaluation\nWe evaluated LEAP across various reasoning tasks, including HotpotQA (Yang et al., 2018b), DROP (Dua et al., 2019a), MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and Big-Bench Hard (Suzgun et al., 2022). We follow the standard few-shot Chain-of-Thought (COT ); we apply LEAP on top of few-shot COT using the exact same number of labeled examples. Importantly, principles are generated once for every LLM and benchmark.\n# 4.1. Experiment Setup\nCompared Approaches Our baseline is the standard Fewshot prompting with COT . We compare this baseline with two variants of our proposed approach LEAP:\n\u2022 LEAP HIGH-LEVEL is our proposed approach, as described in Section 3.\n\u2022 LEAP LOW-LEVEL is similar to LEAP HIGH-LEVEL , except that we skip the step of \u201cGenerating High-Level Principles\u201d, and test the models on the downstream task using the few-shot examples and the low-level principles, using LLOW-LEVEL rather than LHIGH-LEVEL (Section 3).\nBenchmarks We used diverse reasoning benchmarks:\n Textual Reasoning: HotpotQA (Yang et al., 2018a) is a question-answering dataset of computational questions that require multi-hop reasoning. DROP (Dua et al., 2019b) is a reading comprehension dataset that requires numerical and logical reasoning over textual paragraphs; for evaluation, we randomly sampled 2000 questions from its dev set.\n2021) comprises a test set of 1,319 diverse grade school math word problems, curated by human problem writers. In MATH (Hendrycks et al., 2021), there are 5,000 diverse examples consisting of problems from mathematics competitions. These are the two most common mathematical reasoning benchmarks.\n\u2022 Big-Bench Hard (Suzgun et al., 2022): contains 27 challenging tasks that test various reasoning capabilities of LLMs . We repeated every run 3 times with a\nAlgorithm 1 LEAP Algorithm\nRequire: Few-shot examples P = {\u27e8xi, yi\u27e9}k\ni=1, a pretrained LLM , number of outputs per input n, high-temperature setting T\n1: for each input-output pair \u27e8xi, yi\u27e9in P do\n2:\nSi \u2190ZeroShotCoT (LLM , xi, n, T)\n\u25b7Generate solutions using zero-shot chain-of-thought prompting\n3:\nMi \u2190{\n\ufffd\nxi, yi, \u02c6yj\ni\n\ufffd\n\u2208Si : \u02c6aj\ni \u0338= aj\ni}\n\u25b7Identify incorrect solutions\n4:\nfor each xi, yi, \u02c6yi in Mi do\n5:\nLLOW-LEVEL,i \u2190GenerateLowLevelPrinciples (LLM , xi, \u02c6yi, yi)\n\u25b7Generate principles for each mistake\n6:\nend for\n7: end for\n8: LLOW-LEVEL \u2190\ufffdk\ni=1 LLOW-LEVEL,i\n\u25b7Aggregate low-level principles\n9: LHIGH-LEVEL \u2190GenerateHighLevelPrinciples (LLM , LLOW-LEVEL)\n\u25b7Generate high-level principles\n10: pLOW-LEVEL \u2190Concatenate (LLOW-LEVEL, P)\n\u25b7Create enhanced prompt with low-level principles\n11: pHIGH-LEVEL \u2190Concatenate (LHIGH-LEVEL, P)\n\u25b7Create enhanced prompt with high-level principles\n12: return pLOW-LEVEL, pHIGH-LEVEL\nTable 1: Textual Reasoning results: Accuracy in textual reasoning benchmarks. The best approach for each base LLM each dataset is in bold; the second-best approach is underlined. We see a good performance boost by adopting LEAP w high-level feedback and low-level feedback. Almost all the models can benefit from the principles learned and fix th previous mistakes.\nGPT-3.5-turbo\nGPT-4\nGPT-4-turbo\nGemini Pro\nHotpotQA\nFew-shot CoT\n29.10\n36.35\n38.10\n28.25\nLEAP LOW-LEVEL\n32.60\n39.30\n37.85\n23.70\nLEAP HIGH-LEVEL\n30.35\n39.65\n38.75\n25.50\nDROP\nFew-shot CoT\n63.20\n72.05\n83.40\n64.60\nLEAP LOW-LEVEL\n63.35\n79.55\n83.60\n67.15\nLEAP HIGH-LEVEL\n63.00\n78.60\n80.00\n67.60\n<div style=\"text-align: center;\">temperature of zero and report the average.1</div>\nModels We evaluated LEAP across a wide range of base models, including GPT-3.5-turbo (version -0613), GPT-4 (version -0613), GPT-4-turbo (version -1106), Claude-2.1, and Gemini Pro (Gemini Team et al., 2023).\nFew-shot examples In Big-Bench Hard, we used the COT prompts from Suzgun et al. (2022) with three given examples (3-shot) in each benchmark. In MATH and GSM8K , we used the standard training examples from each dataset, with 3 examples for each. In DROP, we used 3 given examples that we adopted from Least-to-Most (Zhou et al., 2022). In HotpotQA we used 6 examples from ReAct (Yao et al., 2022) (\u201cclosed-book\u201d). Importantly, in each benchmark, the exact same few-shot examples were used across all evaluated approaches, including the baseline and LEAP. Our complete prompts are provided in Appendix G.\n1Internal non-determinism causes different outputs even with a temperature of zero, but the variance was negligible.\n# 4.2. Textual Reasoning Results\nTable 1 shows the results on DROP and HotpotQA. As shown, LEAP improves over the Few-shot COT baseline by up to 3.5% on Hotpot QA and 7.5% on DROP. In HotpotQA, GPT-3.5-Turbo and GPT-4 are consistently improved when using LEAP. In DROP, GPT-4 is significantly improved by LEAP (an absolute gain of 7.5%), Gemini Pro is improved by 3%, while the improvement for GPT-3.5-Turbo and GPT4-turbo are more minor.\nIn most tasks and base models, both LEAP LOW-LEVEL and LEAP HIGH-LEVEL improve over the Few-shot COT baseline. The only case where the Few-shot COT baseline performs better than both LEAP LOW-LEVEL and LEAP HIGH-LEVEL is in HotpotQA using Gemini Pro. Observing the low-level principles that Gemini Pro learned in HotpotQA (Table 41), we believe that the learned principles are correct and useful, but they are overly focused on the examples they were generated for, more verbose, and similar to each other. These hinder the principles (and the high-level principles generated from them) from generalizing to other examples. For zero-shot prompting using principles learned from few examples, additional results are shown in Appendix A.\nTable 2: Math Reasoning Results: Accuracy in MATH and GSM8K . The best approach for each LLM and base task is in bold; the second-best approach is underlined. LEAP HIGH-LEVEL and LEAP LOW-LEVEL in both GSM8K and MATH datasets consistently improve the performance over the CoT baseline. We also observe that the learned mathmetical principles can be generalized to different test questions.\nGPT-3.5-turbo\nGPT-4\nClaude-2\nGemini Pro\nGSM8K\nFew-shot CoT\n76.4\n93.6\n84.3\n77.8\nLEAP LOW-LEVEL\n77.4\n94.2\n82.7\n77.3\nLEAP HIGH-LEVEL\n76.6\n93.8\n83.8\n78.7\nMATH\nFew-shot CoT\n55.6\n63.5\n43.2\n31.1\nLEAP LOW-LEVEL\n56.1\n64.5\n42.7\n29.7\nLEAP HIGH-LEVEL\n56.5\n64.0\n43.4\n30.3\n# 4.3. Math Reasoning Results\nTable 2 shows the results on MATH and GSM8K . As shown, in GPT-3.5-turbo and GPT-4, both LEAP LOW-LEVEL and LEAP HIGH-LEVEL outperform the Few-shot COT baseline. Claude-2 shows inconclusive results: in GSM8K , Fewshot COT performs better than LEAP; in MATH, LEAP HIGH-LEVEL achieves slightly higher accuracy than Few-shot COT . Gemini Pro also shows inconclusive results, where both LEAP LOW-LEVEL and LEAP HIGH-LEVEL outperform the baseline in GSM8K , but perform slightly worse in MATH. Additional results, for zero-shot prompting using principles learned from few examples, are shown in Appendix B.\nDoes leap work with open-source models? In our preliminary experiments with open-source models, LEAP did not improve over the few-shot COT baseline. While the open-source models did produce useful principles, the opensource models did not follow these principles at test time.\n# Does leap work with open-source models?\nTable 3 shows some of these experiments with Llama-2chat-70B : we used Llama-2-chat-70B as the base model, but generated the principles using either Llama-2-chat-70B (\u201cLLama-2 Critic\u201d) or with GPT-4 (\u201cGPT-4 Critic\u201d). As shown, even when the principles are generated by GPT-4 (which we assume to be useful), Llama-2-chat-70B does not manage to leverage them to generate better final responses with either LEAP LOW-LEVEL or LEAP HIGH-LEVEL .\nIn general, we believe that LEAP requires a base LLM with strong enough instruction following and reflection capabilities; we believe that, unfortunately, most open-source models are not as powerful as proprietary models yet.\n# 4.4. Big-Bench Hard Results\nFigure 4 shows the results on selected BBH tasks; results for the rest of the BBH tasks are shown in Table 7 in Appendix C. We selected tasks to Table 6 where the difference in results was the most meaningful: there were some tasks where GPT4-0613 achieved 100% accuracy with any of the approaches,\n# including both LEAP and the baseline.\nAs shown in Table 6, in 37 out of 42 combinations of task and LLM , one of LEAP LOW-LEVEL or LEAP HIGH-LEVEL outperforms the baseline Few-shot COT . In 24 of the cases, both LEAP LOW-LEVEL and LEAP HIGH-LEVEL outperform the Few-shot COT baseline. In two cases, the Few-shot COT baseline performs equally to LEAP LOW-LEVEL ; and in 3 cases the Few-shot baseline performs better than both LEAP LOW-LEVEL and LEAP HIGH-LEVEL . We could not identify any particular pattern as to which method should be used: LEAP LOW-LEVEL or LEAP HIGH-LEVEL ; it seems that this depends on the reasoning complexity of the task, the diversity across questions in the benchmark, and the quality of the principles that the LLM had managed to generate from the 3 given examples for that task. We thus suggest that in real-life scenarios, both approaches should be tested, and selected using a validation set. Additional results, for zero-shot prompting using principles learned from a few examples, are shown in Appendix C.\nAs shown in Table 6, in 37 out of 42 combinations of task and LLM , one of LEAP LOW-LEVEL or LEAP HIGH-LEVEL outperforms the baseline Few-shot COT . In 24 of the cases, both LEAP LOW-LEVEL and LEAP HIGH-LEVEL outperform the Few-shot COT baseline. In two cases, the Few-shot COT baseline performs equally to LEAP LOW-LEVEL ; and in 3 cases the Few-shot baseline performs better than both LEAP LOW-LEVEL and LEAP HIGH-LEVEL .\nWe could not identify any particular pattern as to which method should be used: LEAP LOW-LEVEL or LEAP HIGH-LEVEL ; it seems that this depends on the reasoning complexity of the task, the diversity across questions in the benchmark, and the quality of the principles that the LLM had managed to generate from the 3 given examples for that task. We thus suggest that in real-life scenarios, both approaches should be tested, and selected using a validation set. Additional results, for zero-shot prompting using principles learned from a few examples, are shown in Appendix C.\n# 5. Qualitative Analysis\nFigure 5 shows examples of questions from BBH , along with the task\u2019s learned principles, the baseline few-shot CoT response, and the LEAP LOW-LEVEL response, all generated by GPT-3.5-turbo. The learned principle in each case is highlighted, along with the mistake in the COT output and the correct reasoning generated by LEAP. Additional examples are shown in Appendix D. The actual learned principles for some of the tasks and LLMs are provided in Tables 9-43 in Appendix E.\nFigure 5 shows examples of questions from BBH , along with the task\u2019s learned principles, the baseline few-shot CoT response, and the LEAP LOW-LEVEL response, all generated by GPT-3.5-turbo. The learned principle in each case is highlighted, along with the mistake in the COT output and the correct reasoning generated by LEAP.\nAdditional examples are shown in Appendix D. The actual learned principles for some of the tasks and LLMs are provided in Tables 9-43 in Appendix E.\n# 6. Related Works\nThis section focuses on the few works most related to ours. For a complete survey of relevant papers, see Zhao et al. (2023); Qiao et al. (2023); Kaddour et al. (2023); Xi et al.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/232c/232ce888-c273-432c-9744-45646da349f1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7f3/a7f35218-24ee-4518-80b4-181ef146b899.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8da6/8da6a3cd-1803-49bf-a405-4da599e3697a.png\" style=\"width: 50%;\"></div>\nFigure 4: Accuracy in BBH tasks, across GPT-3.5-TURBO-0613 , GPT-4-0613 , and GEMINI-PRO . The figure presents the results using a scatter plot, where the y-axis represents scores achieved with LEAP, and the x-axis represents the baseline scores from COT . Each task is represented by a point on the plot, with different shapes assigned to different models for easy distinction. Tasks above the y = x line are those where LEAP leads to an improvement in performance. Table 6 shows the detailed results for all 27 Big-Bench hard tasks. We find that in 37 out of 42 combinations of task and LLM , one of LEAP LOW-LEVEL or LEAP HIGH-LEVEL outperforms the baseline Few-shot COT .\nFigure 4: Accuracy in BBH tasks, across GPT-3.5-TURBO-0613 , GPT-4-0613 , and GEMINI-PRO . The figure presents the results using a scatter plot, where the y-axis represents scores achieved with LEAP, and the x-axis represents the baseline scores from COT . Each task is represented by a point on the plot, with different shapes assigned to different models for easy distinction. Tasks above the y = x line are those where LEAP leads to an improvement in performance. Table 6 shows the detailed results for all 27 Big-Bench hard tasks. We find that in 37 out of 42 combinations of task and LLM , one of LEAP LOW-LEVEL or LEAP HIGH-LEVEL outperforms the baseline Few-shot COT . Table 3: Llama-2-chat-70B Results: Accuracy in MATH and GSM8K using Llama-2-chat-70B as the base LLM, while generating the principles either with Llama-2-chat-70B or GPT-4. Even when the principles are generated by GPT-4 (GPT-4 Critic), Llama-2-chat-70B does not leverage the learned principles, and does not improve over the baseline Few-shot CoT.\nLlama-2-chat-70B as the base model, with:\nLlama-2 Critic\nGPT-4 Critic\nGSM8K\nFew-shot CoT\n52.5\n52.5\nLEAP LOW-LEVEL\n50.6\n49.8\nLEAP HIGH-LEVEL\n47.0\n51.0\nMATH\nFew-shot CoT\n16.2\n16.2\nLEAP LOW-LEVEL\n13.8\n12.9\nLEAP HIGH-LEVEL\n14.2\n13.8\nComparison to Madaan et al. (2023) A related work to ours is Self-Refine (Madaan et al., 2023), which, similarly to LEAP, uses the LLM to reflect on its own previously generated outputs. However, a significant difference from Self-Refine is that applying Self-Refine to a test example requires feedback for that specific test example; contrarily, in this work, we show that principles that were learned from some examples generalize to other test examples which we do not have feedback for. Further, Self-Refine uses \u201cintrinsic self-correction\u201d, without any external feedback; as found by recent work (Huang et al., 2023a; Valmeekam et al., 2023), such intrinsic self-correction does not benefit reasoning tasks. In contrast, in this work, we show that the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d706/d7062aa0-9f73-4e56-8790-2d6930824199.png\" style=\"width: 50%;\"></div>\nexternal signal provided by as few as 3 examples is sufficient for effective self-reflection in reasoning if these are used to articulate explicit learned principles. Another related work is MemPrompt (Madaan et al., 2022), which stores feedback on mistakes in a memory, and retrieves relevant feedback for a new query; in contrast, LEAP is a one-time process that does not require continuous storage and retrieval.\nComparison to Yang et al. (2023) Optimization by PROmpting (OPRO) (Yang et al., 2023) uses LLMs to improve zero-shot prompts, and both OPRO and LEAP can be thought of as ways of improving a prompt for a specific task. However, applying OPRO for prompt optimization requires a training set, which can be as large as \u223c260 examples for GSM8K; in contrast, LEAP requires only 3 labeled\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5340/53407db9-0a2b-4618-9a0d-65e7f1ef9370.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f0da/f0dabbc7-abfa-43a7-8d5b-5414fecbea2a.png\" style=\"width: 50%;\"></div>\n# Boolean Expressions (BBH )\nexamples. Further, OPRO calls the LLM for each of these \u223c260 examples in each step; while the number of steps can be as low as 6, it can also be as high as 107. This results in between \u223c1500 calls and up to 107\u00d7260 \u224827,000 inference calls to the LLM. In contrast, LEAP uses only about 34 inference calls: 3 calls to generate mistakes; one call for each mistake to generate low-level principles, which results in about 30 calls; and a final call to generate high-level principles. Moreover, like OPRO, LEAP can also improve zero-shot prompting by learning principles from a few examples and applying them in a zero-shot fashion, as we show in Appendix A, Appendix B, and Appendix C. For example, Zero-shot-LEAP improves the strong GPT-4 model by 1% on GSM8K (Table 5). Similarly, EvoPrompt (Guo et al., 2023) requires around 10,000 LLM inference calls, which is also several orders of magnitude more than LEAP. Other related approaches are AutoPrompt (Shin et al., 2020), STaR (Zelikman et al., 2022), LMSI (Huang et al., 2023b), and Self-Align (Sun et al., 2023) but these are based on training the LLM , which is often either inaccessible or computationally infeasible.\nComparison to Chia et al. (2023) Contrastive Chain-ofThought (Chia et al., 2023) shares a similar motivation with our work: learning from negative in-context examples instead of learning only from positive examples. However, when we reproduced their results, we found that simple\n# Comparison to Chia et al. (2023)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a267/a2679fb6-c627-460d-9dfa-7bbd5ab16450.png\" style=\"width: 50%;\"></div>\n# Object Counting (BBH )\nimprovements to the post-processing code of extracting the final answer out of the LLM\u2019s raw output led to differences of more than 10 absolute points from their reported results in GSM8K , which made the proposed Contrastive COT approach perform similarly or worse than the baseline few-shot COT . We thus believe that generating explicit principles, as in LEAP, is the key ingredient in learning from negative in-context examples.\n# 7. Conclusion\nIn this paper, we introduce Learning Principles (LEAP), a novel approach that allows LLMs to learn more out of given few-shot examples, by intentionally making mistakes on these examples; reflecting on the mistakes; and finally articulating explicit task-specific principles, which helps avoid similar mistakes in the future. LEAP requires exactly the same number of labeled examples as fewshot prompting, and allows improving a variety of strong LLMs (GPT-3.5-turbo, GPT-4, GPT-4-turbo and Gemini Pro) across a broad range of reasoning tasks (DROP, HotpotQA, GSM8K , MATH, and Big-Bench Hard). We believe that LEAP unlocks new possibilities from learning in the traditional concept of few-shot in-context learning, by learning from mistakes, rather than learning from positive examples only.\n# References\nGemini Team, A., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023a. Huang, J., Gu, S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1051\u20131068, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.67. URL https: //aclanthology.org/2023.emnlp-main.67. Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and McHardy, R. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916, 2022. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022. Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? In Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out, 2021. URL https://api.semanticscholar. org/CorpusID:231632658. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023. Lu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H. T., and Gurevych, I. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809, 2023.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022. Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020. Chen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. Chia, Y. K., Chen, G., Tuan, L. A., Poria, S., and Bing, L. Contrastive chain-of-thought prompting. arXiv preprint arXiv:2311.09277, 2023. Chialvo, D. R. and Bak, P. Learning from mistakes. Neuroscience, 90(4):1137\u20131148, 1999. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways (no. arxiv: 2204.02311). arxiv, 2022. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019a. Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Chapter of the Association for Computational Linguistics, 2019b. URL https://api. semanticscholar.org/CorpusID:67855846. Edmondson, A. Psychological safety and learning behavior in work teams. Administrative science quarterly, 44(2): 350\u2013383, 1999. Edmondson, A. C. Learning from mistakes is easier said than done: Group and organizational influences on the detection and correction of human error. The Journal of Applied Behavioral Science, 32(1):5\u201328, 1996.\nMadaan, A., Tandon, N., Clark, P., and Yang, Y. Memoryassisted prompt editing to improve gpt-3 after deployment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2833\u20132861, 2022.\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nMin, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2791\u20132809, 2022a.\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11048\u2013 11064, 2022b.\n# Minsky, M. and Papert, S. An introduction to computational geometry. Cambridge tiass., HIT, 479(480):104, 1969.\nMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Crosstask generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nQiao, S., Ou, Y., Zhang, N., Chen, X., Yao, Y., Deng, S., Tan, C., Huang, F., and Chen, H. Reasoning with language model prompting: A survey. In Rogers, A., BoydGraber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5368\u2013 5393, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 294. URL https://aclanthology.org/2023. acl-long.294.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nRosenblatt, F. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.\nShin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u2013 4235, 2020.\nShin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u2013 4235, 2020. Sun, Z., Shen, Y., Zhou, Q., Zhang, H., Chen, Z., Cox, D., Yang, Y., and Gan, C. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023. Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Valmeekam, K., Marquez, M., and Kambhampati, S. Investigating the effectiveness of self-critiquing in LLMs solving planning tasks. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https: //openreview.net/forum?id=gGQfkyb0KL. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-Augmented Ensembles in Language Models. arXiv preprints arXiv:2207.00747, 2022a. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In ICLR, 2022a. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022b. ISSN 2835-8856. URL https:// openreview.net/forum?id=yzkSU5zdwD. Survey Certification.\nSingh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u2013 4235, 2020. Sun, Z., Shen, Y., Zhou, Q., Zhang, H., Chen, Z., Cox, D., Yang, Y., and Gan, C. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023. Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Valmeekam, K., Marquez, M., and Kambhampati, S. Investigating the effectiveness of self-critiquing in LLMs solving planning tasks. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https: //openreview.net/forum?id=gGQfkyb0KL. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-Augmented Ensembles in Language Models. arXiv preprints arXiv:2207.00747, 2022a. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In ICLR, 2022a. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022b. ISSN 2835-8856. URL https:// openreview.net/forum?id=yzkSU5zdwD. Survey Certification.\nSun, Z., Shen, Y., Zhou, Q., Zhang, H., Chen, Z., Cox, D., Yang, Y., and Gan, C. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023.\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nValmeekam, K., Marquez, M., and Kambhampati, S. Investigating the effectiveness of self-critiquing in LLMs solving planning tasks. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https: //openreview.net/forum?id=gGQfkyb0KL.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-Augmented Ensembles in Language Models. arXiv preprints arXiv:2207.00747, 2022a.\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In ICLR, 2022a.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022b. ISSN 2835-8856. URL https:// openreview.net/forum?id=yzkSU5zdwD. Survey Certification.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837, 2022c. Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Wiener, N. Cybernetics; or control and communication in the animal and the machine. 1948. Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing, 2018a. URL https://api. semanticscholar.org/CorpusID:52822214. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018b. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. STar: Bootstrapping reasoning with reasoning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=_3ELRdg2sgI. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhang, Z., Yao, Y., Zhang, A., Tang, X., Ma, X., He, Z., Wang, Y., Gerstein, M., Wang, R., Liu, G., et al. Igniting language intelligence: The hitchhiker\u2019s guide from chainof-thought reasoning to language agents. arXiv preprint arXiv:2311.11797, 2023.\n<div style=\"text-align: center;\">Wiener, N. Cybernetics; or control and communication in the animal and the machine. 1948.</div>\nZhang, Z., Yao, Y., Zhang, A., Tang, X., Ma, X., He, Z., Wang, Y., Gerstein, M., Wang, R., Liu, G., et al. Igniting language intelligence: The hitchhiker\u2019s guide from chainof-thought reasoning to language agents. arXiv preprint arXiv:2311.11797, 2023.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Leastto-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv preprint arXiv:2205.10625, 2022.\n# A. Additional Results on Text Reasoning\nTable 4 shows additional Text Reasoning results. The upper part of Table 4 is identical to Table 1; the lower part of Table 4 contains zero-shot results, with principles that were learned from the given few-shot examples; this setup is similar to related work such as (Yang et al., 2023).\nTable 4: Text Reasoning Results: Accuracy in textual reasoning datasets. The best approach for ea dataset is in bold; the second-best approach is underlined.\n<div style=\"text-align: center;\">Table 4: Text Reasoning Results: Accuracy in textual reasoning datasets. The best approach for each base LLM in eac dataset is in bold; the second-best approach is underlined.</div>\nGPT-3.5-turbo\nGPT-4\nGemini Pro\nFew-shot results, identical to Table 2:\nHotpotQA\nFew-shot CoT\n29.10\n36.35\n28.25\nLEAP LOW-LEVEL\n32.60\n39.30\n23.70\nLEAP HIGH-LEVEL\n30.35\n39.65\n25.50\nDROP\nFew-shot CoT\n63.20\n72.05\n64.60\nLEAP LOW-LEVEL\n63.35\n79.55\n67.15\nLEAP HIGH-LEVEL\n63.00\n78.60\n67.60\nGPT-3.5-turbo\nGPT-4\nGemini Pro\nZero-shot results, using principles learned from few-shot examples:\nHotpotQA\nZero-shot CoT\n12.55\n27.80\n19.75\nLEAP LOW-LEVEL\n6.65\n31.20\n3.30\nLEAP HIGH-LEVEL\n13.10\n30.90\n2.45\nDROP\nZero-shot CoT\n59.25\n82.20\n62.75\nLEAP LOW-LEVEL\n55.20\n84.10\n65.30\nLEAP HIGH-LEVEL\n55.25\n81.80\n62.80\nB. Additional Results on Mathematical Reasoning\nTable 5: Math Reasoning Results: Accuracy in MATH and GSM8K . The best approach for each LLM and base task is i bold; the second-best approach is underlined.\nGPT-3.5-turbo\nGPT-4\nClaude-2\nGemini Pro\nFew-shot results, identical to Table 2:\nGSM8K\nFew-shot CoT\n76.4\n93.6\n84.3\nLEAP LOW-LEVEL\n77.4\n94.1\n82.7\nLEAP HIGH-LEVEL\n76.6\n93.8\n83.8\nMATH\nFew-shot CoT\n55.6\n63.5\n43.2\nLEAP LOW-LEVEL\n56.1\n64.5\n42.7\nLEAP HIGH-LEVEL\n56.5\n64.0\n43.4\nGPT-3.5-turbo\nGPT-4\nClaude-2\nGemini Pro\nZero-shot results, using principles learned from few-shot examples:\nGSM8K\nZero-shot CoT\n76.9\n93.2\n75.4\nLEAP LOW-LEVEL\n74.4\n94.2\n76.7\nLEAP HIGH-LEVEL\n73.8\n94.1\n76.9\nMATH\nZero-shot CoT\n54.2\n63.5\n40.2\nLEAP LOW-LEVEL\n52.0\n63.2\n40.5\nLEAP HIGH-LEVEL\n50.0\n61.5\n41.8\nTable 5 shows additional Mathematical Reasoning results. The upper part of Table 5 is identical to Table 2; the lower part of Table 5 contains zero-shot results, with principles that were learned from the given few-shot examples; this setup is similar to related work such as (Yang et al., 2023).\n# C. Additional Results on BBH\nTable 7 shows results on additional BBH tasks that we could not fit into Table 6. We selected tasks to Table 6 in the main paper where the difference was more meaningful. As shown in Table 7, there were some tasks such as temporal_sequences, web_of_lies and tracking_shuffled_objects_five_objects where GPT-4 achieved 100% accuracy with any of the approaches, including both LEAP and the few-shot baseline.\n<div style=\"text-align: center;\">In-Context Principle Learning from Mistakes</div>\ntask\nwrong to\ncorrect\ncorrect to\nwrong\nwrong to\nwrong\ncorrect to\ncorrect\ntemporal_sequences\n31.0\n10.0\n66.0\n143.0\nsnarks\n18.0\n11.0\n39.0\n110.0\ndisambiguation_qa\n10.0\n8.0\n76.0\n156.0\nlogical_deduction_seven_objects\n27.0\n21.0\n129.0\n73.0\nobject_counting\n4.0\n1.0\n4.0\n241.0\nmovie_recommendation\n14.0\n11.0\n33.0\n192.0\nnavigate\n2.0\n1.0\n12.0\n235.0\nformal_fallacies\n14.0\n9.0\n99.0\n128.0\nsports_understanding\n5.0\n4.0\n10.0\n231.0\nboolean_expressions\n5.0\n2.0\n10.0\n233.0\nweb_of_lies\n2.0\n1.0\n1.0\n246.0\nmultistep_arithmetic_two\n12.0\n8.0\n33.0\n197.0\ncausal_judgement\n18.0\n18.0\n51.0\n100.0\nsalient_translation_error_detection\n9.0\n16.0\n102.0\n123.0\ntracking_shuffled_objects_three_objects\n12.0\n15.0\n30.0\n193.0\nword_sorting\n11.0\n13.0\n109.0\n117.0\nlogical_deduction_five_objects\n24.0\n27.0\n78.0\n121.0\ntracking_shuffled_objects_five_objects\n12.0\n16.0\n56.0\n166.0\nhyperbaton\n4.0\n19.0\n40.0\n187.0\nlogical_deduction_three_objects\n9.0\n16.0\n20.0\n205.0\ntracking_shuffled_objects_seven_objects\n10.0\n14.0\n79.0\n147.0\ndyck_languages\n0.0\n0.0\n0.0\n250.0\ndate_understanding\n3.0\n9.0\n31.0\n207.0\npenguins_in_a_table\n13.0\n13.0\n20.0\n100.0\nreasoning_about_colored_objects\n11.0\n19.0\n33.0\n187.0\nruin_names\n14.0\n38.0\n87.0\n111.0\ngeometric_shapes\n9.0\n25.0\n74.0\n142.0\nTable 8: Efficacy of LEAP Methods Across Various Tasks. This table provides an overview of the effectiveness of LEAP in modifying the correctness of responses across a range of tasks. It highlights the number of instances where answers changed from wrong to correct, correct to wrong, and the stability of responses (both correct and incorrect). Notable gains in tasks like \u2019temporal_sequences\u2019 and \u2019snarks\u2019 suggest significant improvements, whereas tasks like \u2019ruin_names\u2019 and \u2019geometric_shapes\u2019 show areas needing further methodological refinement.\nable 6: BBH Results: Accuracy in BBH tasks, across GPT-3.5-turbo, GPT-4, and Gemini Pro. The best approach for each ase LLM in each tasks is in bold; the second-best approach is underlined. Each number represents the average across 3 entical runs with a temperature of zero.\nTable 6: BBH Results: Accuracy in BBH tasks, across GPT-3.5-turbo, GPT-4, and Gemini Pro. The best approach fo base LLM in each tasks is in bold; the second-best approach is underlined. Each number represents the average ac identical runs with a temperature of zero.\nTask\nApproach\nGPT-3.5-turbo\nGPT-4\nGemini Pro\nboolean_expressions\nFew-shot CoT\n94.13\n96.80\n91.20\nLEAP LOW-LEVEL\n94.93\n98.00\n93.60\nLEAP HIGH-LEVEL\n95.07\n97.60\n92.40\ndisambiguation_qa\nFew-shot CoT\n65.73\n85.60\n66.00\nLEAP LOW-LEVEL\n67.20\n86.00\n66.00\nLEAP HIGH-LEVEL\n68.00\n85.20\n70.40\nformal_fallacies\nFew-shot CoT\n56.00\n81.60\n58.80\nLEAP LOW-LEVEL\n56.93\n84.00\n61.20\nLEAP HIGH-LEVEL\n56.80\n81.60\n57.60\nhyperbaton\nFew-shot CoT\n83.07\n98.00\n88.40\nLEAP LOW-LEVEL\n81.33\n99.20\n91.20\nLEAP HIGH-LEVEL\n84.80\n98.80\n87.60\nlogical_deduction_five_objects\nFew-shot CoT\n60.67\n85.20\n60.40\nLEAP LOW-LEVEL\n59.47\n85.60\n62.80\nLEAP HIGH-LEVEL\n57.73\n84.40\n60.80\nlogical_deduction_seven_objects\nFew-shot CoT\n38.67\n56.80\n49.20\nLEAP LOW-LEVEL\n40.00\n58.40\n50.00\nLEAP HIGH-LEVEL\n39.20\n59.60\n48.40\nmovie_recommendation\nFew-shot CoT\n81.47\n90.40\n83.20\nLEAP LOW-LEVEL\n82.53\n91.60\n84.80\nLEAP HIGH-LEVEL\n84.00\n92.00\n83.60\nmultistep_arithmetic_two\nFew-shot CoT\n81.73\n93.20\n79.60\nLEAP LOW-LEVEL\n82.00\n94.00\n78.00\nLEAP HIGH-LEVEL\n82.00\n92.40\n81.20\nnavigate\nFew-shot CoT\n94.27\n98.00\n95.20\nLEAP LOW-LEVEL\n95.20\n98.40\n95.20\nLEAP HIGH-LEVEL\n94.93\n98.40\n94.80\nobject_counting\nFew-shot CoT\n96.80\n98.80\n94.80\nLEAP LOW-LEVEL\n97.87\n99.20\n93.20\nLEAP HIGH-LEVEL\n97.60\n99.20\n91.60\nruin_names\nFew-shot CoT\n60.13\n87.60\n72.80\nLEAP LOW-LEVEL\n53.47\n89.60\n72.80\nLEAP HIGH-LEVEL\n68.13\n87.60\n74.40\nsnarks\nFew-shot CoT\n67.42\n89.89\n82.58\nLEAP LOW-LEVEL\n70.60\n89.33\n79.21\nLEAP HIGH-LEVEL\n66.48\n91.01\n80.90\nsports_understanding\nFew-shot CoT\n93.87\n94.40\n98.40\nLEAP LOW-LEVEL\n94.80\n95.20\n98.40\nLEAP HIGH-LEVEL\n88.00\n94.40\n98.00\nword_sorting\nFew-shot CoT\n53.07\n94.40\n82.00\nLEAP LOW-LEVEL\n52.00\n94.00\n83.20\nLEAP HIGH-LEVEL\n53.60\n95.60\n84.80\n<div style=\"text-align: center;\">Table 7: Additional results on Big-Bench Hard tasks. The best approach for each base LLM in each tasks is in bold; the second-best approach is underlined. Each number represents the average across 3 identical runs with a temperature of zero.</div>\nTask\nApproach\nGPT-3.5-turbo\nGPT-4\nGemini-Pro\ngeometric_shapes\nFew-shot\n67.33\n54.40\n50.00\n+ LEAP LOW-LEVEL\n59.60\n57.20\n50.00\n+ LEAP HIGH-LEVEL\n60.13\n58.80\n50.40\ncausal_judgement\nFew-shot\n63.64\n73.26\n62.57\n+ LEAP LOW-LEVEL\n63.64\n72.73\n62.57\n+ LEAP HIGH-LEVEL\n63.10\n72.73\n62.57\ndate_understanding\nFew-shot\n86.67\n90.00\n87.60\n+ LEAP LOW-LEVEL\n83.73\n91.20\n87.60\n+ LEAP HIGH-LEVEL\n85.60\n90.00\n86.40\ndyck_languages\nFew-shot\n35.73\n56.80\n0\n+ LEAP LOW-LEVEL\n33.07\n56.80\n0\n+ LEAP HIGH-LEVEL\n35.73\n54.00\n0\nlogical_deduction\n_three_objects\nFew-shot\n88.80\n99.20\n89.20\n+ LEAP LOW-LEVEL\n86.80\n98.80\n90.00\n+ LEAP HIGH-LEVEL\n85.73\n98.80\n90.80\npenguins_in_a_table\nFew-shot\n76.94\n97.26\n81.51\n+ LEAP LOW-LEVEL\n73.74\n96.58\n78.77\n+ LEAP HIGH-LEVEL\n73.97\n96.58\n76.71\nreasoning_about\n_colored_objects\nFew-shot\n82.13\n95.20\n83.20\n+ LEAP LOW-LEVEL\n77.87\n91.20\n84.00\n+ LEAP HIGH-LEVEL\n75.87\n94.00\n82.40\nsalient_translation\n_error_detection\nFew-shot\n55.73\n68.80\n56.00\n+ LEAP LOW-LEVEL\n55.20\n67.20\n52.80\n+ LEAP HIGH-LEVEL\n54.27\n67.20\n55.20\ntemporal_sequences\nFew-shot\n60.93\n100.0\n99.60\n+ LEAP LOW-LEVEL\n65.20\n100.0\n98.00\n+ LEAP HIGH-LEVEL\n62.80\n100.0\n99.20\ntracking_shuffled_objects\n_five_objects\nFew-shot\n72.40\n100.0\n64.00\n+ LEAP LOW-LEVEL\n70.93\n100.0\n64.40\n+ LEAP HIGH-LEVEL\n71.33\n100.0\n64.00\ntracking_shuffled_objects\n_seven_objects\nFew-shot\n65.07\n100.0\n56.8\n+ LEAP LOW-LEVEL\n62.53\n100.0\n60.0\n+ LEAP HIGH-LEVEL\n61.73\n98.8\n60.8\ntracking_shuffled_objects\n_three_objects\nFew-shot\n83.60\n100.0\n69.60\n+ LEAP LOW-LEVEL\n82.80\n100.0\n69.20\n+ LEAP HIGH-LEVEL\n85.87\n100.0\n68.40\nweb_of_lies\nFew-shot\n98.67\n100.0\n100.0\n+ LEAP LOW-LEVEL\n99.20\n100.0\n100.0\n+ LEAP HIGH-LEVEL\n99.20\n100.0\n100.0\n# D. Additional Qualitative Analysis\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dd44/dd44b00e-8041-473b-afe1-cd32c751f213.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/012d/012da157-ba32-43c3-bf3c-1a4e4d53a415.png\" style=\"width: 50%;\"></div>\nQuestion: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, Dropped Content. Please identify that error. Source: Neu Fahrland ist ein Ortsteil von Potsdam. Translation: Neu Fahrland is a district of Pots. The translation contains an error pertaining to Options: (A) Modifiers or Adjectives (B) Numerical Values (C) Negation or Antonyms (D) Named Entities (E) Dropped Content (F) Facts Principles: When analyzing translations, it is important to carefully consider the translation of named entities. Any changes or inaccuracies in the translation of names, places, or other entities should be identified as named entity errors. CoT Response: Let\u2019s think step by step. We solve this question by first translating the source sentence to English and then by comparing our translation with the provided translation. According to Google Translate, the correct translation of the source sentence from German to English is \"Neu Fahrland is a district of Potsdam.\" On the other hand, the provided translation is \"Neu Fahrland is a district of Pots.\" Note that the provided translation is missing the word \"dam\" at the end of Pots. Because a significant part of the translation is dropped, the translation contains an error pertaining to Dropped Content. So the answer is (E) . \u2717 LEAP LOW-LEVEL Response: Let\u2019s think step by step. We solve this question by first translating the source sentence to English and then by comparing our translation with the provided translation. According to Google Translate, the correct translation of the source sentence from German to English is \"Neu Fahrland is a district of Potsdam.\" On the other hand, the provided translation is \"Neu Fahrland is a district of Pots.\" Note that the provided translation omits the word \"dam\" from Potsdam. Because a significant part of the entity name is removed in the translation, the translation contains an error pertaining to Named Entities. So the answer is (D) . \u2713\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/245e/245e2254-296c-4a22-8a66-01a9aa31c36c.png\" style=\"width: 50%;\"></div>\nBoolean Expressions\nQuestion: not not ( False and not False ) is\nPrinciples: . . . 3. Double negation, as in \u2018not not\u2019, cancels out and returns the original value. . . .\nCoT Response: Let\u2019s think step by step. Remember that (i) expressions inside brackets are always evaluated first\nand that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first\nsimplify this expression \"Z\" as follows: \"Z = not not ( False and not False ) = not not ( A )\" where \"A = False and not\nFalse\". Let\u2019s evaluate A: A = False and not False = False and True = False. Plugging in A, we get: Z = not not ( A ) =\nnot not (False) = not not True = not False = True. So the answer is True . \u2717\nLEAP LOW-LEVEL Response: Let\u2019s think step by step. Remember that (i) expressions inside brackets are always\nevaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\",\nrespectively. We first simplify this expression \"Z\" as follows: \"Z = not not ( False and not False ) = not not ( A )\"\nwhere \"A = False and not False\". Let\u2019s evaluate A: A = False and not False = False and True = False. Plugging in A,\nwe get: Z = not not ( A ) = not not (False) = not not False = False. So the answer is False . \u2713\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca1d/ca1d7bd4-54fc-4842-b930-873b506ce939.png\" style=\"width: 50%;\"></div>\n# Date Understanding\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e151/e151888a-346e-4e29-bb6c-b071df2ce38f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f6e8/f6e8213c-1c00-41f3-ad22-a547ff7b447c.png\" style=\"width: 50%;\"></div>\n# Disambiguation QA\nQuestion: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous. Sentence: The undergraduate applied to work with the scientist because he needed a lab assistant. Options: (A) The undergraduate needed an assistant (B) The scientist needed an assistant (C) Ambiguous Principles: Pay attention to the causality and context of the sentence to identify the most likely referent for the pronoun. It is not enough to simply assume that the pronoun could refer to any possible noun mentioned in the\n# sentence.\nCoT Response: Let\u2019s think step by step. Here we need to determine who the pronoun \"he\" might be referring to. There are two possible referents for \"he\", namely the undergraduate and the scientist. The verb \"applied\" might be able to help us determine which one is more likely (if either). Let X be the undergraduate and Y the scientist. The sentence is then of the form \"X applied to work with Y because (X or Y) needed a lab assistant.\" Let\u2019s consider Y first: \"X applied to work with Y because Y needed a lab assistant.\" This case makes sense, because X applied to work with Y because Y had a need for a lab assistant. Now, consider X: \"X applied to work with Y because X needed a lab assistant.\" This case also makes sense, because X applied to work with Y because X had a need for a lab assistant. Because both X and Y are possible at the same time, we conclude that the antecedent of the pronoun is ambiguous. So the answer is (C) . LEAP LOW-LEVEL Response: Let\u2019s think step by step. Here we need to determine who the pronoun \"he\" might be referring to. There are two possible referents for \"he\", namely the undergraduate and the scientist. The verb \"applied\" might be able to help us determine which one is more likely (if either). Let X be the undergraduate and Y the scientist. The sentence is then of the form \"X applied to work with Y because (X or Y) needed a lab assistant.\" Let\u2019s consider Y first: \"X applied to work with Y because Y needed a lab assistant.\" This case makes sense, because X wanted to work with Y because Y needed assistance in the lab. Now, consider X: \"X applied to work with Y because X needed a lab assistant.\" This case doesn\u2019t make much sense, because X, as an undergraduate, would typically be the one seeking an opportunity to work with a scientist, not the other way around. Because in this context, X is the undergraduate and Y is the scientist, the answer should be the scientist. So the answer is (B) .\n# E. Learned Principles\nTables 9-39 show the high-level principles learned by LEAP. Tables 40-43 show low-level principles learned by LEAP\nHigh-level principles (LEAP HIGH-LEVEL ) learned by LEAP for boolean expressions (BBH ) using GPT-4-0613 1. Ensure clarity and conciseness: Responses should be clear and concise to avoid confusion and maintain the user\u2019s interest. 2. Avoid redundancies: Redundant information should be eliminated to maintain the user\u2019s attention and improve the efficiency of communication. 3. Maintain uniqueness: Each response should be unique to keep the user engaged and provide a personalized experience. 4. Incorporate feedback: User feedback should be analyzed and incorporated into future responses to improve the quality of interaction. 5. Focus on relevance: Responses should be directly related to the user\u2019s query or statement to maintain relevance and provide accurate information. 6. Prioritize logical reasoning: Logical reasoning should be the foundation of all responses to ensure accuracy and credibility. 7. Preserve specific details: Specific details provided by the user should be preserved in the response to show understanding and maintain context. 8. Continual improvement: Regular introspection and analysis should be conducted to continually improve the quality of responses.\nHigh-level principles (LEAP HIGH-LEVEL ) learned by LEAP for causal judgement (BBH ) using GPT-4-0613\n1. Consider all relevant details in the scenario: Ensure that all key details are taken into account when\nforming a conclusion. Do not overemphasize certain details at the expense of others.\n2. Understand the concept of intentionality: It\u2019s not enough to initiate an action; the action must be carried\nout as planned for it to be considered intentional.\n3. Consider all factors in a scenario: This includes the actor\u2019s control over the situation and the predictability\nof the outcome when determining intentionality.\n4. Distinguish between the intent to perform an action and the intent to cause a specific outcome: In questions\nof causation, it\u2019s important to understand these nuances to provide accurate analysis.\n5. Understand the context of causation: Intentionality involves not just the initial action but also the control\nand predictability of the outcome.\n6. Avoid overemphasis on initial decisions: The initial decision to perform an action should not be\noveremphasized at the expense of other factors, such as the accidental nature of the outcome.\n7. Improve understanding and interpretation of intent: This includes understanding the difference between\nthe intent to perform an action and the intent to cause a specific outcome.\n8. Ensure adequate consideration of crucial factors: In any scenario, ensure that all aspects, especially those\nthat are crucial to the outcome, are adequately considered before arriving at a conclusion.\nHigh-level principles (LEAP HIGH-LEVEL ) learned by LEAP for date understanding (BBH ) using GPT-4-0613 1. Ensure clarity and precision: Responses should be clear and concise, avoiding any ambiguity or unnecessary complexity. 2. Maintain relevance: The responses should be directly related to the query or topic at hand, avoiding any irrelevant information. 3. Avoid redundancy: Each response should provide unique information, avoiding repetition of previously stated facts or ideas. 4. Prioritize understanding: The responses should be designed to enhance the user\u2019s understanding of the topic, rather than simply providing information. 5. Foster engagement: The responses should be engaging and interactive, encouraging further dialogue and exploration of the topic. 6. Promote logical reasoning: The responses should be logically sound and well-reasoned, demonstrating a clear thought process. 7. Respect user\u2019s perspective: The responses should respect the user\u2019s perspective and knowledge level, avoiding any condescension or oversimplification. 8. Incorporate feedback: The responses should incorporate feedback from previous interactions, continuously improving in quality and relevance.\nHigh-level principles (LEAP HIGH-LEVEL ) learned by",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) has been the standard method of adapting LLMs to downstream tasks by learning from a few input-output examples. However, current ICL methods only learn from correct pairs, which limits their effectiveness. This paper introduces Learning Principles (LEAP), a method that allows models to learn from mistakes, thus enhancing their performance in various tasks.",
        "problem": {
            "definition": "The problem is that existing ICL methods do not utilize incorrect examples, which is a fundamental aspect of human learning.",
            "key obstacle": "The main challenge is that current methods fail to leverage the learning potential from mistakes, which hinders their ability to generalize and improve in similar tasks."
        },
        "idea": {
            "intuition": "The idea is inspired by the way humans learn from their mistakes and articulate principles that help avoid similar errors in the future.",
            "opinion": "LEAP involves inducing mistakes in the model's responses and prompting it to reflect on these errors to derive useful principles.",
            "innovation": "LEAP differs from existing methods by incorporating a reflection phase where the model learns from its mistakes, rather than only from correct examples."
        },
        "method": {
            "method name": "Learning Principles (LEAP)",
            "method abbreviation": "LEAP",
            "method definition": "LEAP is a prompting approach that allows LLMs to learn from mistakes made on a few examples and apply the learned principles to unseen test questions.",
            "method description": "LEAP enhances few-shot prompting by reflecting on mistakes to generate task-specific principles that guide future responses.",
            "method steps": [
                "Induce mistakes on the given few-shot examples.",
                "Prompt the model to reflect on these mistakes and generate explanations.",
                "Condense these explanations into high-level principles.",
                "Use the generated principles alongside the original examples to answer unseen questions."
            ],
            "principle": "LEAP is effective because it leverages the model's ability to learn from both correct and incorrect outputs, enhancing its reasoning and accuracy."
        },
        "experiments": {
            "evaluation setting": "LEAP was evaluated on benchmarks including HotpotQA, DROP, GSM8K, MATH, and Big-Bench Hard, using strong LLMs like GPT-3.5-turbo and GPT-4.",
            "evaluation method": "The performance of LEAP was compared against the standard few-shot prompting methods, measuring accuracy improvements across various tasks."
        },
        "conclusion": "LEAP significantly improves the performance of LLMs in reasoning tasks by enabling them to learn from mistakes, thus revolutionizing traditional few-shot ICL methods.",
        "discussion": {
            "advantage": "The key advantage of LEAP is its ability to enhance model performance by utilizing both correct and incorrect examples, leading to better generalization.",
            "limitation": "A limitation is that the effectiveness of LEAP may vary based on the complexity of tasks and the quality of principles generated.",
            "future work": "Future research could explore refining the principle generation process and testing LEAP with more diverse models and tasks."
        },
        "other info": {
            "authors": [
                "Tianjun Zhang",
                "Aman Madaan",
                "Luyu Gao",
                "Steven Zheng",
                "Swaroop Mishra",
                "Yiming Yang",
                "Niket Tandon",
                "Uri Alon"
            ],
            "publication": "arXiv:2402.05403v2",
            "keywords": [
                "In-context learning",
                "Large language models",
                "Learning from mistakes",
                "Prompting methods"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) has been the standard method of adapting LLMs to downstream tasks by learning from a few input-output examples."
        },
        {
            "section number": "1.2",
            "key information": "Current ICL methods only learn from correct pairs, which limits their effectiveness."
        },
        {
            "section number": "3",
            "key information": "LEAP is a prompting approach that allows LLMs to learn from mistakes made on a few examples and apply the learned principles to unseen test questions."
        },
        {
            "section number": "3.1",
            "key information": "The key advantage of LEAP is its ability to enhance model performance by utilizing both correct and incorrect examples, leading to better generalization."
        },
        {
            "section number": "4.1",
            "key information": "LEAP enhances few-shot prompting by reflecting on mistakes to generate task-specific principles that guide future responses."
        },
        {
            "section number": "6",
            "key information": "A limitation is that the effectiveness of LEAP may vary based on the complexity of tasks and the quality of principles generated."
        },
        {
            "section number": "7",
            "key information": "LEAP significantly improves the performance of LLMs in reasoning tasks by enabling them to learn from mistakes, thus revolutionizing traditional few-shot ICL methods."
        }
    ],
    "similarity_score": 0.7397759056002916,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Principle Learning from Mistakes.json"
}