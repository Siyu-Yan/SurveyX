{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.11323",
    "title": "Identifying and Analyzing Task-Encoding Tokens in Large Language Models",
    "abstract": "In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL\u2019s working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. For example, unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour. In this paper, we investigate this problem by identifying and analyzing taskencoding tokens on whose representations the task performance depends. Using experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding. In addition, we demonstrate experimentally that lexical meaning, repetition, and text formatting are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models (LLMs) learn to perform a task from demonstrations, deepens our understanding of the varied roles different types of tokens play in LLMs, and provides insights for avoiding instability from improperly utilizing task-encoding tokens.",
    "bib_name": "bai2024identifyinganalyzingtaskencodingtokens",
    "md_text": "# Identifying and Analyzing Task-Encoding Tokens in Large Language Models\n1Beijing Institute of Technology 2Mila \u2013 Quebec Artificial Intelligence Institute 3McGill University, 4Duke University 5Canada CIFAR AI Chair yubai@bit.edu.cn\nAbstract\n# Abstract\nIn-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL\u2019s working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. For example, unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour. In this paper, we investigate this problem by identifying and analyzing taskencoding tokens on whose representations the task performance depends. Using experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding. In addition, we demonstrate experimentally that lexical meaning, repetition, and text formatting are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models (LLMs) learn to perform a task from demonstrations, deepens our understanding of the varied roles different types of tokens play in LLMs, and provides insights for avoiding instability from improperly utilizing task-encoding tokens.\n# 1 Introduction\nIn-context learning (ICL) has become a popular technique employed with large language models (LLMs) (Brown et al., 2020). However, ICL has been shown to be unstable in that slight changes to the in-context prompts (e.g., reordering of demonstrations) can lead to substantial differences in performance (Lu et al., 2022; Zhang et al., 2022). This circumstance is difficult to control due to a lack of understanding of the model\u2019s working mechanisms, leaving us uncertain about the exact process by which LLMs learn to perform a task from demonstrations. Previous papers have begun to explore\n\u2217Work in progress. \u2020Corresponding author.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b487/b4875547-6926-4315-8566-5813d610c566.png\" style=\"width: 50%;\"></div>\nFigure 1: An illustration of the 4-way text classification on AGNews with different parts of its 4-shot ICL demonstrations masked with respect to the attention of the test example. Masking the representations of what we call the template and stopword tokens from the attention of the test example leads to a significant drop in performance while masking representations of the content tokens leaves the performance relatively unchanged.\nthis issue, focusing on specific aspects such as the label space (Min et al., 2022) and the hidden states of the last prompt token (Hendel et al., 2023; Todd et al., 2023), but have been limited in scope. In this work, we conduct a more comprehensive study on how LLMs extract information valuable to improving task performance from demonstrations, by characterizing the model\u2019s task-encoding tokens. Conceptually, task-encoding tokens are defined as tokens whose representations encode the tasksolving process. Since this cannot be extracted directly from the representations constructed by the model, as a practical proxy, we take task-\nencoding tokens to be those tokens whose representations LLMs depend on to achieve high-level performance. In other words, removing the representations of these tokens should lead to diminished task performance. In this paper, we identify and analyze the characteristics of task-encoding tokens, motivated by two main reasons: 1) The identification of task-encoding tokens is likely to provide new insights into how large language models leverage critical information useful for performing a task. 2) Analyzing the characteristics of task-encoding tokens helps us better understand how they are different from the tokens whose representations do not explicitly affect the ICL performance, which would provide insights into avoiding possible instability caused by improperly utilizing these tokens. In order to identify the task-encoding tokens, we divide tokens into three structural categories: template tokens, stopword tokens (including punctuations, conjunctions, etc.), and content tokens. Intuitively, template tokens structure the entire ICL prompt into organized text by indicating the task input and output, while stopword tokens contribute to the overall structure of the plain text. With these structural categories in mind, we ablate the representations of tokens of different types from the attention of ICL test examples, masking partial information during the model\u2019s solving of the task. We use the observed changes in task performance to draw conclusions about which types of tokens are likely task-encoding tokens, as shown in Figure 1. Results of these experiments provide evidence that template tokens and stopword tokens are the most prone to be task-encoding tokens as ablating their representations together significantly decreases performance. In contrast, content tokens have a negligible impact on performance. We further investigate every token in the template with the same ablation method, confirming that most of these tokens have representations necessary for preserving the task performance. Beyond identifying task-encoding tokens, we investigate other ways in which they may differ from other tokens. We find the following three distinguishing characteristics: the lexical meaning of tokens as it relates to the task being solved, the repetition of tokens throughout the prompt, and the text formatting which the tokens provide to the prompt. Our research indicates that the lexical meaning, repetition, and text formatting brought by task-encoding tokens contribute to task performance across all model sizes, suggesting that these\ncharacteristics should not be disrupted in order to avoid performance fluctuation, therefore maintaining its stability. Our work reveals that we can identify and characterize the types of tokens with representations that are most needed for an LLM to perform well on downstream tasks during the ICL process. We investigate the characteristics of lexical meaning, repetition, and text formatting related to task-encoding tokens which allow us to partially explain the presence of task-encoding tokens and help us better avoid the instability caused by them. Our findings deepen the understanding of the roles different types of tokens play in large language models, suggesting future work based on leveraging specific representations of different token types. Code and data will be released in the future.\n# 2 Related Work\n# 2.1 Working mechanisms of in-context learning\nSince the proposal of in-context learning (Brown et al., 2020), its working mechanisms have been extensively studied by the research community (Min et al., 2022; Liu et al., 2021; Olsson et al., 2022; Bhattamishra et al., 2023). Min et al. (2022) suggest that demonstrations primarily provide the label space, the distribution of the input text, and the format of the sequence for the test example. They argue that the precise ground truth labels do not have significant importance. Conversely, Yoo et al. (2022) propose a differing view, stating that the impact of the ground truth labels depends on the experimental configuration. Xie et al. (2021) explain ICL as implicit Bayesian inference, while Aky\u00fcrek et al. (2022) explore ICL learning process using linear models. Theoretical explanations (Guo et al., 2023; Bai et al., 2023; Li et al., 2023) and gradient descent explanations have also been proposed. Additional analyses exploring different aspects of ICL have also been studied. For instance, order sensitivity where task performance fluctuates based on the order of the same ICL demonstrations has been identified as a limitation of ICL (Lu et al., 2022). Yan et al. (2023) propose that repetitive patterns in the prompt could affect the ICL performance in both positive and negative ways. Pan et al. (2023) analyze the ICL process by disentangling it into task recognition and task learning. Our work investigates the execution of ICL in LLMs at inference time, demonstrating that certain\nspecific tokens are more likely to possess representations that could affect the processing of the final test sample, improving the task performance.\n# 2.2 Function vectors of in-context learning\nIn a similar line of work to ours, Todd et al. (2023) and Hendel et al. (2023) provide evidence of function vectors that store information used to solve a task in ICL. They probe and extract the hidden representations of the final tokens in the prompt. These vectors can then be added to, or used to replace, the corresponding vectors in a zero-shot example, achieving results comparable to those obtained when the model uses all demonstrations as context. In addition, Liu et al. (2023) also propose using an in-context vector to represent the target task and applying feature shifting to query examples. They first feed each input and its corresponding target separately into an LLM, then concatenate all the latent states. A PCA method is applied to derive a vector that is more closely aligned with the task. Finally, Wang et al. (2023) proposes that label words in the demonstration examples function as information anchors by aggregating the information from previous demonstrations and providing it to the test example. This finding suggests that we may view label tokens as satisfying our definition for task-encoding tokens. All these previous studies either solely focus on a single token (i.e., the last prediction prompt token or label token) of the ICL prompt or treat the entire demonstration as a single unit, neglecting the other individual tokens within it. Our research focuses on all the tokens in the prompt and reveals that there are additional tokens with specific characteristics whose representations significantly affect the final ICL performance.\n# 3 Preliminaries\n# 3.1 Notation\nIn-context learning (ICL) is a technique that enables large language models (LLMs) to perform tasks in a few-shot manner by placing task demonstrations (e.g., input-output pairs) in the context fed to a large language model (Brown et al., 2020). In ICL, these demonstrations are leveraged to construct a structured prompt that guides the model in predicting the final answer. Formally, the structural prompt consists of the following components: the instruction I, the templates Tin, Tout, and the demonstrations Din i , Dout i , where i denotes the\nComponent notation\nComponent example\nI\nClassify the news articles into the categories of World, Sports,\nBusiness, and Technology.\\n\\n\nTin\nArticle: {Din}\\n\nTout\nAnswer: {Dout}\\n\\n\nDin\n1\nRadio veteran Karmazin joins Sirius. Sirius Satellite Radio Inc.\nnamed former Viacom Inc. president Mel...\nDout\n1\nBusiness\nDin\n2\nNumbers point to NY. NEW YORK - The New York Yankees can\nachieve two milestones with one more victory...\nDout\n2\nSports\nICL\nPrompt\nClassify the news articles into the categories of World, Sports,\nBusiness, and Technology.\nArticle: Radio veteran Karmazin joins Sirius. Sirius Satellite\nRadio Inc. named former Viacom Inc. president Mel...\nAnswer: Business\nArticle: Numbers point to NY. NEW YORK - The New York\nYankees can achieve two milestones with one more victory...\nAnswer: Sports\nTable 1: An example of the components of a 2-shot ICL prompt in the AGNews dataset.\nith demonstration while in and out refer to the input text and output labels, respectively. These prompt component are concatenated to form the ICL prompt, as shown in Table 1. During inference, the templated version of the test example (without an answer) is appended to the ICL prompt and then sent to the large language model to predict the corresponding answer.\n# 3.2 Experimental Settings\nIn this section, we describe the experimental setup for all of our experiments. For the datasets, we consider the most widely used text classification datasets used by previous studies (Zhao et al., 2021). For topic classification, we use the 4-way and 14-way datasets AGNews and DBPedia (Zhang et al., 2015). For textual entailment, we use the 3-way CB (De Marneffe et al., 2019) and 2-way RTE dataset (Dagan et al., 2005). We also use SST2 (Socher et al., 2013) and TREC (Voorhees and Tice, 2000) for sentiment and question classification tasks. For each dataset, we randomly select 4 training demonstrations from the training set using 15 different random seeds limited by the computational cost of the inference stage of LLMs. For testing, we evaluate each setting on 500 randomly selected test examples. Instruction prompt I is retained in all the different kinds of ablations since it is essential for enhancing the classification performance of the model (Yin et al., 2023). For the LLMs, we utilize the 7B, 13B, and 30B versions of the Llama model and a 3B OpenLlama model. All the experiments are conducted using a single A100 80G GPU. For the 13B and 30B models, we apply 8-bit quantization to ensure the model\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6ed/d6ed2069-828c-48a6-b3d7-06e39f1f3552.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: An illustrative example of the representation and token-level ablation methods we use to test if template and stopword tokens are task-encoding tokens. The test example can only access partial information (shown in green) from the demonstrations.</div>\nFigure 2: An illustrative example of the representation and token-level ablation methods we use to test if template and stopword tokens are task-encoding tokens. The test example can only access partial information (shown in green) from the demonstrations.\nfits into a single GPU. The results are inferred using Huggingface Transformers (Wolf et al., 2020).\n# fits into a single GPU. The results are inferred using Huggingface Transformers (Wolf et al., 2020).\n# 4 Methodology\nIn this section, we aim to find the task-encoding tokens in the ICL prompt. We first structurally categorize all the tokens in the prompt into three types: template, stopword, and content tokens. Then, we provide supporting evidence from the view of task performance to show that the representations of template and stopword tokens are the most prone to be task-encoding tokens.\n# 4.1 Token types\nWe categorize ICL tokens based on the way the ICL prompt is structured according to our notation: Template tokens (TEMP) In defining template tokens, we include all the tokens which serve as templates for the ICL prompt. This collection of tokens includes the tokens in Tin and Tout. Stopword tokens (STOP) In defining stopword tokens, we include punctuation and conjunction word, such as [,], [.], etc., in the ICL prompt. We use the stopword tokens appear in the instructions1. The stopword token list is shown in Appendix B. Content tokens (CONT) In defining content tokens, we include all the tokens from Din except for the ones that are already stopword tokens. We use 1Ablation experiments with a more complete NLTK (Loper and Bird, 2002) stopwords list are conducted in Appendix B.\n1Ablation experiments with a more complete NLTK (Loper and Bird, 2002) stopwords list are conducted in Appendix B.\nthe term \u201ccontent\u201d as they convey the meaningful information found in the demonstrations. Given this categorization, we conduct ablation experiments to determine which tokens are more likely to be task-encoding tokens.\n# 4.2 Ablation on token types\nTo determine which token types are more likely to be task-encoding tokens whose representations affect the final performance largely, we design two experiments. The first involves keeping and masking the different kinds of token representations from the attention of the test example. The second involves dropping the various kinds of tokens from the ICL prompt. Illustrations of these two methods which we refer to as representation-level and token-level ablations are shown in Figure 2.\n# 4.2.1 Representation-level ablation\nOur first ablation stems from the intuition that if LLMs essentially rely on the representations of certain token types to achieve high-level performance, then the model should perform the target task adequately with only these representations. Meanwhile, performance should decrease significantly if we remove them from the attention of the test example. Hence, we first pass the entire ICL prompt to the LLM and then restrict the attention of the test example such that the LLM may only attend to the representations of tokens of a particular type (or types)2 during its solving of the task. This representation-level ablation is illustrated in the upper part of Figure 2. We compute task performances with every possible ablation combination, removing the representations of one (e.g., Standard ICL \u2212TEMP) or two token types (e.g., Zero-shot + CONT3) from the attention of the test example4. All the task performances and the averaged relative performance changes are reported, shown in Table 2 and Table 3. Overall, these results demonstrate that the representations of template tokens and stopword tokens are more likely to be task-encoding tokens than content tokens. On the one hand, template token representations are crucial for LLMs\u2019 task-solving ability via ICL, achieving an average performance 2 \n2Since Dout tokens have been shown to significantly impact performance (Wang et al., 2023), we always preserve the attention on the representations of the Dout tokens. 3Removing two types of tokens from Standard ICL is equivalent to adding the other type of tokens to Zero-shot. 4Both STOP and TEMP include the \u201c\\n\u201d token; therefore, we preserve the \u201c\\n\u201d as long as one of them is not ablated.\nModels\nSetting\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\n\u25b3Avg.\nOpenLlama\n3B\nZero-shot\n22.0\n20.0\n23.6\n5.4\n44.4\n1.8\n19.5\n+ CONT\n26.2\n52.1\n30.1\n7.4\n51.9\n37.9\n+14.8\n+ STOP\n36.7\n82.9\n32.0\u2217\n52.4\n58.8\n56.2\n+33.7\n+ TEMP\n56.5\n86.7\n27.1\n62.2\n56.4\n52.3\n+37.4\nLlama\n7B\nZero-shot\n25.0\n29.2\n41.4\n0.0\n54.2\n3.6\n25.6\n+ CONT\n32.4\n57.9\n42.5\n12.5\n55.5\n46.1\n+15.6\n+ STOP\n57.3\n83.7\n49.8\n43.0\n55.9\n50.7\n+31.1\n+ TEMP\n70.8\n90.2\n58.4\n66.2\n66.3\n73.5\n+45.3\nLlama\n13B\nZero-shot\n59.0\n18.0\n37.0\n0.0\n0.0\n0.0\n19.0\n+ CONT\n27.7\n52.4\n33.5\n10.9\n61.7\n41.7\n+19.0\n+ STOP\n72.2\n73.5\n46.8\n50.7\n58.6\n30.6\n+36.4\n+ TEMP\n80.0\n92.3\n58.6\n76.9\n68.5\n47.7\n+51.7\nLlama\n30B\nZero-shot\n70.2\n88.6\n60.6\n30.2\n58.1\n19.6\n54.6\n+ CONT\n24.4\n61.7\n62.1\n10.5\n65.2\n63.6\n\u22126.7\n+ STOP\n72.9\n92.7\n66.7\u2217\n69.1\n69.6\n63.0\n+17.7\n+ TEMP\n80.5\n95.2\n65.2\n75.2\n79.0\n80.0\n+24.6\nTable 2: The accuracy results of the representation-level ablation study where, for example, + TEMP refers to allowing attention only to template tokens. All values are presented as percentages. Except where noted with \u2217, all test statistics reported correspond to p-values < 0.05. The best results are in bold.\n39.8% higher than the zero-shot baseline by only utilizing these representations at inference time. In contrast, content token representations only bring an average improvement of 10.7%. If the representations of stopword tokens are further included (i.e., Standard ICL\u2212CONT), the performance is nearly equivalent to that of the Standard ICL. On the other hand, the performance decreases the most with Standard ICL\u2212TEMP, highlighting the significance of template tokens again. Rare exception cases appear when performance is relatively poor with Standard ICL (e.g., OpenLlama 3B in TREC). In some cases, masking the representations of the content tokens brings even better performance than the Standard ICL method, which is possibly due to the elimination of noisy information in the demonstration content. Another interesting observation is that the performance results of Standard ICL\u2212STOP and Standard ICL\u2212CONT where the attention to the content and stopword tokens is ablated respectively are close, with an average difference of only 0.7%. This indicates that the representation of stopword tokens may contain overlapping information with their preceding content tokens. We believe that this could enable LLMs to model long sequences without significant architectural changes (e.g., using stopword token representations as synthesis checkpoints) and leave the verification of this hypothesis to future work.\n# 4.2.2 Token-level ablation\nIn this section, we modify the ICL prompt by removing certain types of tokens from the ICL\nModels\nSetting\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\n\u25b3Avg.\nOpenLlama\n3B\nStandard ICL\n63.7\n91.2\n21.9\n61.9\n57.4\n52.0\n58.0\n\u2212CONT\n58.2\n86.9\u2217\n27.6\n61.9\n56.5\u221751.7\n\u22120.9\n\u2212STOP\n62.3\n91.0\n24.8\u2217\n62.9\n57.1\n51.1\u2217\n+0.2\n\u2212TEMP\n41.9\n87.2\n26.0\n56.3\n58.5\n57.4\n\u22125.4\nLlama\n7B\nStandard ICL\n82.4\n94.3\n63.5\n68.7\n68.6\n71.3\n74.8\n\u2212CONT\n77.9\n91.5\n58.5\n66.5\n67.8\n74.4\n\u22122.0\n\u2212STOP\n80.4\n94.6\n61.1\n68.0\n67.2\n72.0\n\u22120.9\n\u2212TEMP\n64.5\n84.1\n54.0\n58.0\n56.8\n54.3\n\u221212.8\nLlama\n13B\nStandard ICL\n81.6\n94.3\n60.0\n76.1\n70.6\n39.9\n70.4\n\u2212CONT\n81.4\n93.1\n58.9\n75.7\n69.6\n45.1\n+0.2\n\u2212STOP\n81.2\n94.1\n59.3\n76.9\n69.2\n40.6\n\u22120.2\n\u2212TEMP\n74.1\n80.0\n46.5\n30.6\n58.3\n25.4\n\u221217.9\nLlama\n30B\nStandard ICL\n85.0\n96.5\n68.1\n78.4\n78.5\n83.3\n81.6\n\u2212CONT\n82.3\n95.4\n64.9\n76.1\n80.4\n82.0\n\u22121.4\n\u2212STOP\n84.3\n95.6\n65.7\n77.6\n78.6\n81.8\n\u22121.0\n\u2212TEMP\n76.6\n93.9\u2217\n61.2\n72.7\n70.3\n59.6\n\u22129.2\nTable 3: The accuracy results of the representation-level ablation study where, for example, \u2212TEMP refers to allowing attention only to content and stopword tokens. All values are presented as percentages. Except where noted with \u2217, all test statistics reported correspond to pvalues < 0.05. The results showing the greatest decrease from ablation are underlined.\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nAvg.\nOpenLlama\n3B\nStandard ICL\n63.7\n91.2\n21.9\n61.9\n57.4\n52.0\n58.0\n\u2212CONT\n31.5\n63.0\n40.6\n25.4\n56.1\n48.9\n44.3\n\u2212STOP\n64.4\n91.5\n20.9\n62.3\n57.8\n52.6\n58.3\nLlama\n7B\nStandard ICL\n82.4\n94.3\n63.5\n68.7\n68.6\n71.3\n74.8\n\u2212CONT\n55.2\n67.2\n42.6\n50.8\n57.4\n56.3\n54.9\n\u2212STOP\n82.3\n93.8\n64.1\n69.7\n66.5\n70.0\n74.4\nLlama\n13B\nStandard ICL\n81.6\n94.3\n60.0\n76.1\n70.6\n39.9\n70.4\n\u2212CONT\n78.8\n81.7\n45.3\n75.1\n55.1\n54.5\n65.1\n\u2212STOP\n82.5\n92.5\n61.5\n76.5\n69.6\n40.5\n70.5\nLlama\n30B\nStandard ICL\n85.0\n96.5\n68.1\n78.4\n78.5\n83.3\n81.6\n\u2212CONT\n74.0\n89.6\n67.0\n73.0\n69.8\n49.0\n70.4\n\u2212STOP\n85.3\n96.4\n66.9\n77.9\n77.7\n81.3\n80.9\nTable 4: Results of the token-level ablation where, for example, \u2212STOP refers to the ablation where stopword tokens are dropped from the ICL prompt. Models without template tokens consistently yielded an accuracy of 0% and are thus omitted from this table.\nprompt5 to further determine the extent to which final performance relies on different token types. The token-level ablation is illustrated in the lower part of Figure 2. When we ablate the template tokens, we preserve the answer and next-line tokens in the templates to maintain a basic separator between the demonstration inputs and outputs. Results are presented in Table 4. Our main finding from this ablation is that removing template tokens causes the LLMs to completely lose their ability to solve tasks via ICL with an overall task accuracy performance of 0% for all sizes and all tasks. We hypothesize that this is because the model no longer has an explicit cue to generate the target label. In this case, if we add back the last prompt token after the next-line token, the results return to their original level due to the introduction of a template token. This finding con-\n5This includes both the demonstration tokens as well as the tokens in the test example.\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nOpenLlama\n3B\nTEMP with Dout\n56.5\n47.4\n86.7\n83.7\n27.1\n26.5\n62.2\n59.8\n56.4\n56.0\n52.3\n56.1\n\u2212Tin\n50.3\n47.1\n85.7\n84.4\n28.9\n24.4\n57.7\n57.7\n56.5\n56.1\n53.2\n55.2\n\u2212Tout\n34.6\n32.7\n86.9\n82.3\n28.2\n31.2\n55.5\n54.1\n58.3\n59.2\n55.4\n58.3\nLlama\n7B\nTEMP with Dout\n70.8\n57.3\n90.2\n87.1\n58.4\n46.7\n66.2\n63.8\n66.3\n59.5\n73.5\n69.6\n\u2212Tin\n62.7\n55.1\n91.6\n87.1\n52.8\n43.3\n61.6\n61.8\n58.9\n56.5\n59.2\n55.7\n\u2212Tout\n50.8\n48.6\n84.9\n82.8\n46.0\n50.2\n57.9\n55.2\n56.7\n56.7\n66.2\n64.5\nLlama\n13B\nTEMP with Dout\n80.0\n76.2\n92.3\n89.1\n58.6\n54.0\n76.9\n71.4\n68.5\n59.8\n47.7\n35.0\n\u2212Tin\n79.9\n76.3\n91.5\n88.9\n55.1\n47.8\n75.8\n70.7\n65.5\n59.0\n35.7\n24.5\n\u2212Tout\n72.0\n72.1\n81.1\n75.9\n47.1\n48.3\n60.3\n35.5\n61.2\n58.4\n36.2\n36.0\nLlama\n30B\nTEMP with Dout\n80.5\n75.0\n95.2\n93.3\n65.2\n66.7\n75.2\n73.5\n79.0\n77.1\n80.0\n70.7\n\u2212Tin\n78.7\n71.5\n95.2\n92.8\n68.1\n67.7\n75.1\n73.8\n77.4\n75.4\n73.3\n62.3\n\u2212Tout\n69.2\n69.5\n93.9\n92.9\n62.1\n66.2\n71.3\n70.1\n72.8\n70.0\n67.4\n63.5\n<div style=\"text-align: center;\">Table 5: Ablation for different template token representations, presented as percentages. The results showing greatest impact from ablation are underlined.</div>\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nwith \u201c:\u201d\nw/o \u201c:\u201d\nOpenLlama\n3B\nTEMP w/o Dout\n41.5\n54.6\n14.3\n73.2\n36.5\n42.0\n29.4\n21.7\n24.7\n45.7\n0.7\n3.5\n\u2212Tin\n42.2\n52.2\n18.5\n79.9\n39.7\n42.2\n22.6\n22.5\n49.8\n57.1\n3.1\n6.5\n\u2212Tout\n36.3\n35.5\n83.0\n83.4\n43.2\n41.9\n16.3\n18.7\n54.4\n56.8\n1.2\n4.2\nLlama\n7B\nTEMP w/o Dout\n50.4\n56.6\n68.2\n56.1\n55.3\n48.5\n0.2\n1.3\n40.7\n42.5\n28.5\n18.8\n\u2212Tin\n46.1\n50.6\n61.9\n55.1\n43.5\n44.7\n0.0\n0.2\n43.7\n49.9\n27.1\n26.5\n\u2212Tout\n21.4\n12.7\n86.2\n66.5\n54.4\n55.6\n0.0\n0.0\n56.0\n55.6\n39.4\n35.1\nLlama\n13B\nTEMP w/o Dout\n66.9\n77.0\n65.6\n87.9\n51.8\n53.1\n0.1\n0.1\n57.5\n53.7\n16.7\n21.9\n\u2212Tin\n72.9\n76.6\n83.0\n89.5\n45.5\n48.4\n0.0\n0.0\n53.6\n52.8\n16.0\n20.1\n\u2212Tout\n79.2\n77.7\n77.5\n47.5\n56.8\n43.2\n0.0\n0.0\n54.8\n53.7\n4.3\n2.4\nLlama\n30B\nTEMP w/o Dout\n77.3\n78.2\n17.3\n88.9\n65.4\n69.3\n31.0\n41.7\n71.8\n65.8\n23.8\n23.0\n\u2212Tin\n72.9\n72.4\n29.2\n87.4\n65.6\n70.9\n14.9\n37.9\n70.6\n67.8\n19.9\n21.1\n\u2212Tout\n69.5\n74.3\n92.6\n92.8\n70.0\n70.8\n42.0\n20.3\n67.0\n61.3\n23.1\n18.5\nTable 6: Ablation for different template token representations without the answer label token representations. All values are presented as percentages. The results showing the greatest decrease during the ablation are underlined.\nfirms previous claims that preserving the format of ICL prompts plays a significant role in retaining the task performance (Min et al., 2022). In additon, these experiments further demonstrate the importance of the template tokens. Notably, even without stopword or content tokens, the model can still acquire limited predictive ability.\n# 5 Analyses of task-encoding tokens\nIn this section, we provide analyses of the tokens whose representations we believe mainly store information that affects the performance of a task drastically. We focus on the template tokens since, as evidenced by the findings in Section 4.2.1 and 4.2.2, their representations are the most important to maintaining task performance. Our analyses include the effects of different template tokens on the final performance and the distinguishing characteristics of these tokens.\n# 5.1 Effects of different task-encoding tokens\nIn Section 4.2.1, we assume that the label token Dout is needed for ICL to achieve performance results on par with Standard ICL, as suggested by previous work (Wang et al., 2023). This circumstance raises the question: are the last prompt token and the label token Dout the only active components for affecting the task performance, or do other tokens in the template also matter? To answer\nthis question, we examine the performance of the model when the test example is allowed to attend only to specific tokens of the ICL template.\n# 5.1.1 Representation ablation with Dout\nFirst, we examine the performance effect of Tin, Tout, and the last prompt token \u201c:\u201d when Dout is always preserved, by removing or retaining their representations from the attention of the test example, similar to Section 4.2.1. The results of this experiment are presented in Table 5. We find that for most of the cases, the indicator and the Dout provide the majority of the information needed for the final prediction, which is consistent with previous work (Todd et al., 2023; Wang et al., 2023). Moreover, we observe that other parts of the template text, especially Tout, could also bring a large improvement in performance in most of the cases. This shows that all of the tokens in the template contribute to the final performance.\n# 5.1.2 Representation ablation without Dout\nTo further investigate the influence of the answer label token Dout while clarifying the effect of Tin and Tout, we include another set of experiments where all the Dout tokens are removed from the attention of test examples, shown in Table 6. In this case, the performance becomes less stable, where adding back a template token (e.g., \u201c:\u201d)\nSettings\nNotations\nExamples\nRandomfixed\nTin\ndsafjkldafdsajk: {Din}\\n\nTout\nreqwiorewsdafjl: {Dout}\\n\\n\nSwap\nTin\nAnswer: {Din}\\n\nTout\nArticle: {Dout}\\n\\n\nRandomnonfixed\nTin\n1\ndsafjkldaasdfjkl: {Din}\\n\nTout\n1\nxiadfjdsalgfweqrjl: {Dout}\\n\\n\nTin\n2\newqroudajfsdafq: {Din}\\n\nTout\n2\nyufoufgaddavfdnsl: {Dout}\\n\\n\nTin\nt\nvcxnkfgahvczxkl: {Din}\\n\nTout\nt\ndafhglajfdvcaol: {Dout}\\n\\n\nTable 7: An example of the ICL template with random strings used in AGNews.\ndoes not always bring performance improvements. However, we find that in AGNews, TREC, and RTE datasets, models can still achieve relatively high performance while the model outperforms the zero-shot baselines in other datasets (except in DBPedia). These results show that representations of other template tokens may also be seen as information anchors whose representations aggregates and serve information to the final prediction of LLMs, broadening the conclusions of Wang et al. (2023) who claim that answer tokens serve as information anchors. This result also emphasizes that representations of tokens besides Dout and the last prompt token in the ICL template can also have a major effect on the final task performance.\n# 5.2 Characteristics of task-encoding tokens\nWith the task-encoding tokens identified, we still lack insight into what distinguishes them as taskencoding from other tokens. Since they are the most important for LLMs\u2019 task performance, improper use of them (e.g., bad designs of the template) could cause instability in the final performance. Hence, to gain a deeper understanding of the presence of task-encoding tokens and avoid the potential instability issues, we further investigate the distinguishing characteristics of task-encoding tokens. We focus on the characteristics of lexical meaning referring to the task-related lexical meaning of a task-encoding token, repetition referring to the multiple appearances of the task-encoding tokens in the prompt, and text formatting referring to how task-encoding tokens format the ICL prompt into structured text. We design several experiments to test whether these characteristics affect the presence of taskencoding tokens by disrupting each characteristic in the ICL prompts. A characteristic is related if there is a performance drop after the disruption. We examine each characteristic in the following sections by disrupting an ICL prompt with differ-\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nAvg.\nOpenLlama\n3B\nStandard ICL\n63.7\n91.2\n21.9\n61.9\n57.4\n52.0\n58.0\nSwap\n64.4\n86.8\n21.7\n58.7\n60.6\n54.6\n57.8\nRandomfixed\n57.5\n71.4\n32.4\n51.2\n53.3\n49.8\n52.6\nLlama\n7B\nStandard ICL\n82.4\n94.3\n63.5\n68.7\n68.6\n71.3\n74.8\nSwap\n70.2\n11.4\n44.3\n58.2\n64.5\n50.1\n49.8\nRandomfixed\n19.5\n11.4\n13.2\n7.4\n19.7\n21.7\n15.5\nLlama\n13B\nStandard ICL\n81.6\n94.3\n60.0\n76.1\n70.6\n39.9\n70.4\nSwap\n81.5\n67.4\n36.4\n75.9\n69.1\n52.1\n63.7\nRandomfixed\n52.1\n76.8\n27.7\n48.9\n55.7\n34.5\n49.3\nLlama\n30B\nStandard ICL\n85.0\n96.5\n68.1\n78.4\n78.5\n83.3\n81.6\nSwap\n84.5\n94.9\n60.8\n75.5\n68.0\n55.5\n73.2\nRandomfixed\n78.7\n92.5\n52.2\n75.8\n68.9\n41.1\n68.2\nTable 8: Results validating the effect of lexical meanings of template tokens. The results showing the greatest decrease during the disruption are underlined.\nent kinds of random string templates. We use 5 different random string templates and average all the results for each setting. An example of the templates we used can be seen in Table 7. All the templates we used are attached to Appendix D.\n# 5.2.1 Lexical meaning\nA token might serve as a task-encoding token depending on its specific lexical meaning. One possible hypothesis is that if the token carries specific task-related meanings like \u201cArticle\u201d and \u201cAnswer\u201d, it is more likely to serve as a task-encoding token. To verify if lexical meanings could affect the formation of task-encoding tokens, we 1) Replace the tokens from Tin and Tout with the same random strings across the different demonstrations (Randomfixed), thus completely disrupting the lexical characteristic of these tokens; 2) Swap Tin and Tout (Swap), thus partially disrupting the lexical characteristic of these tokens. Shown in Table 8, we observe that for smaller models (OpenLlama 3B) disrupting the lexical meaning of tokens would slightly impact task performance. For larger models, the disruption causes more significant drops in performance. Specifically, Llama 7B is particularly sensitive to the lexical meaning of tokens and demonstrates poorer performance when semantics are disturbed via random strings or swapping. Therefore, the lexical meaning of tokens is likely to play a role in their task-encoding nature, especially in the case of larger models.\n# 5.2.2 Repetition\nThe presence of task-encoding tokens could also be influenced by their repetition throughout the ICL prompt. Intuitively, via the attention mechanism, repetitive patterns are more likely to propagate information through the processing of text. Yan et al. (2023) propose self-reinforcement in incontext learning, also suggesting that repetition could be a significant factor in in-context learning.\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nAvg.\nOpenLlama\n3B\nRandomfixed\n57.5\n71.4\n32.4\n51.2\n53.3\n49.8\n52.6\nRandomnonfixed\n30.2\n71.4\n17.1\n18.6\n47.9\n47.7\n38.8\nLlama\n7B\nRandomfixed\n19.5\n11.4\n13.2\n7.4\n19.7\n21.7\n15.5\nRandomnonfixed\n15.5\n11.6\n10.4\n1.8\n4.6\n25.6\n11.6\nLlama\n13B\nRandomfixed\n52.1\n76.8\n27.7\n48.9\n55.7\n34.5\n49.3\nRandomnonfixed\n32.1\n34.5\n19.2\n6.0\n21.0\n32.8\n24.3\nLlama\n30B\nRandomfixed\n78.7\n92.5\n52.2\n75.8\n68.9\n41.1\n68.2\nRandomnonfixed\n78.5\n87.5\n46.3\n63.1\n63.6\n46.1\n64.2\nTable 9: Experimental results validating the effect of repetitive patterns. We bold the highest accuracy for each classification task and model size.\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nAvg.\nOpenLlama\n3B\nRandomfixed\n47.5\n51.8\n32.6\n19.4\n51.8\n42.4\n40.9\nZero-shot+TEMP\n39.5\n49.8\n27.7\n13.3\n49.8\n44.9\n37.5\nZero-shot + \u201c:\u201d\n31.5\n35.9\n23.8\n8.0\n35.9\n33.8\n28.2\nLlama\n7B\nRandomfixed\n3.9\n16.9\n3.5\n9.6\n16.9\n10.4\n10.2\nZero-shot+TEMP\n2.1\n15.5\n7.6\n3.7\n15.5\n5.4\n8.3\nZero-shot + \u201c:\u201d\n3.6\n7.5\n14.6\n3.0\n7.5\n6.8\n7.2\nLlama\n13B\nRandomfixed\n46.1\n47.5\n25.0\n50.8\n47.5\n21.4\n39.7\nZero-shot+TEMP\n29.2\n48.9\n36.1\n35.7\n48.9\n14.0\n35.5\nZero-shot + \u201c:\u201d\n14.3\n22.4\n25.4\n22.5\n22.4\n28.9\n22.7\nLlama\n30B\nRandomfixed\n69.7\n53.0\n37.8\n72.8\n53.0\n37.6\n54.0\nZero-shot+TEMP\n61.2\n56.3\n41.1\n69.2\n56.3\n43.0\n54.5\nZero-shot + \u201c:\u201d\n43.3\n41.8\n37.4\n65.0\n41.8\n39.5\n44.8\nTable 10: One-shot representation masking experiments conducted to verify if structural template formats could influence the effectiveness of the task-encoding tokens. Dout is preserved in all the settings. The results showing the greatest decrease during the ablation are underlined. We experiment with the repetition characteristic by comparing the results of the previously discussed Randomfixed experiment with an experiment replacing each Tin and Tout with different random strings (Randomnonfixed), thus breaking the repetition of template tokens present in ICL demonstrations. We see from Table 9 that without consistent repetition of the task-encoding tokens, the performance for most models decreases. This decrease in performance suggests that information necessary for maintaining the performance of the task may not have been properly accumulated and stored in the representations of the template tokens. These experiments demonstrate that repetitive patterns significantly influence the presence of taskencoding tokens.\n# 5.2.3 Text formatting\nBeyond lexical meaning and repetition, the occurrence of task-encoding tokens may also be influenced by the formatting of ICL prompts. Similar to our definition of template and stopword tokens, ICL prompts are often formatted with structural cues that assist the model in differentiating between elements with distinct roles, such as task inputs and target labels, within a demonstration. For instance, template tokens (i.e., Tin and Tout) delimit the presentation of demonstration examples and labels in ICL prompts. These structural cues are similar\nto those found in an LLM\u2019s pretraining data (e.g., column names in SQL tables). As a result, we suspect that pretraining on such data enables the structuring nature of the task-encoding tokens to be recognized, causing its representations to store higher-level information. An intuitive method to verify the effect of the text formatting would be using the same random strings to replace Tin and Tout, making it harder for a model to parse the structure of the text. However, this would bring the factor of repetition into the process, potentially confounding the results. Hence, we instead conduct a one-shot Randomfixed experiment. The one-shot Randomfixed setting allows us to control both the characteristics of lexical meaning and repetition since the templates are made up of random strings and there is only one training demonstration. With these two characteristics controlled, we use the masking ablation method from Section 4.2.1 to confirm to what extent these random string tokens can function effectively as delimiters between inputs and outputs in ICL prompts. Specifically, we include results from the Zero-shot + \u201c:\u201d, Zero-shot + TEMP scenarios, as well as the standard results of one-shot Randomfixed, for a more comprehensive analysis, shown in Table 10. We observe that adding the attention to random template token representations in the one-shot setting often leads to performance increases while masking the attention to the template tokens and only attending to \u201c:\u201d +Dout leads to performance decreases. This indicates that the presence of these tokens is critical to maintaining task performance. With all other characteristics being controlled, this leads us to believe that the delimiting nature of template tokens is likely to be an important characteristic in their role as task-encoding tokens.\n# 6 Conclusion\nIn this paper, we have provided a fine-grained characterization of task-encoding tokens. Through a series of comprehensive experiments, we have examined the roles of template tokens and stopword tokens within ICL as potential task-encoding tokens. Our findings suggest more subtlety exists in previous claims made about ICL, for example, that tokens other than label words could also provide valuable information affecting the performance. Overall, our results demonstrate that model performance depends directly on the presence of these tokens and that their lexical meaning, their repeti-\ntion throughout the ICL prompt, and their formatting of ICL demonstrations are likely to play a role in how effectively they allow an LLM to recover the critical information to perform a task.\n# Limitations\nIn this paper, the token categorization is performed manually, leaving room for further refinement. While the results provide robust support to our categorization, the identification process itself lacks precision. For instance, stopwords may only represent a subset of all in-context task-encoding tokens. The manual nature of our categorization limits our ability to comprehensively track these tokens. Moreover, our experiments are limited to classification datasets, suggesting that our conclusions should be further validated for generation tasks. Additionally, our focus on task-encoding tokens, whose representations could impact task performance, may overlook other tokens responsible for other possible functions.\n# Ethics Statement\nThis work focuses on analyzing the working mechanisms of large language models and, as such, does not present any increased risks of harm beyond the existing norms of natural language processing or computational linguistics research. The associated risks include using a model trained on vast amounts of text, which may inadvertently contain biases. Another concern is the potential misuse of the model for generating misleading or harmful content. However, such a scenario is unlikely in our work, as we concentrate on classification tasks with fixed outputs.\n# References\nEkin Aky\u00fcrek, D. Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. International Conference on Learning Representations. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. 2023. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv: 2306.04637.\nSatwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. 2023. Understanding in-context learning in transformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016.\nSatwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. 2023. Understanding in-context learning in transformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine learning challenges workshop, pages 177\u2013190. Springer. Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107\u2013124. Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. 2023. How do transformers learn in-context beyond simple functions? a case study on learning with representations. Roee Hendel, Mor Geva, and Amir Globerson. 2023. In-context learning creates task vectors. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023. Transformers as algorithms: Generalization and stability in in-context learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19565\u201319594. PMLR. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, L. Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out. Sheng Liu, Lei Xing, and James Zou. 2023. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv: 2311.06668. Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. arXiv preprint cs/0205028. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv: 2202.12837. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv: 2202.12837. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\n# Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. 2023. Function vectors in large language models.\nJianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, and Yue Zhang. 2023. Understanding in-context learning from repetitions.\nFan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. In Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3063\u20133079, Toronto, Canada. Association for Computational Linguistics. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. arXiv preprint arXiv: 2205.12685. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\nDatasets\nNotations\nExamples\nAGNews\nI\nClassify the news articles into the categories of\nWorld, Sports, Business, and Technology.\\n\\n\nTin\nArticle: {Din}\\n\nTout\nAnswer: {Dout}\\n\\n\nSST2\nI\nClassify the reviews into the categories of Positive\nand Negative.\\n\\n\nTin\nReview: {Din}\\n\nTout\nSentiment: {Dout}\\n\\n\nRTE\nI\nClassify the entailment of the hypothesis and the\npremise into the categories of True and False.\\n\\n\nTin\nHypothesis: {DinA}\\n Premise: {DinB}\\n\nTout\nAnswer: {Dout}\\n\\n\nCB\nI\nClassify the entailment of the hypothesis and the\npremise into the categories of true, neither and\nfalse.\\n\\n\nTin\nHypothesis: {DinA}\\n Premise: {DinB}\\n\nTout\nAnswer: {Dout}\\n\\n\nTREC\nI\nClassify the questions based on whether their answer\ntype is a Number, Location, Person, Description, En-\ntity, or Abbreviation.\\n\\n\nTin\nQuestion: {Din}\\n\nTout\nAnswer Type: {Dout}\\n\\n\nDBPedia\nI\nClassify the documents based on whether they are\nabout a Company, School, Artist, Athlete, Politician,\nTransportation, Building, Nature, Village, Animal,\nPlant, Album, Film, or Book.\\n\\n\nTin\nArticle: {Din}\\n\nTout\nAnswer: {Dout}\\n\\n\nTable 11: An example of the ICL template used in all of our experiments.\nDatasets\nStopwords\nAGNews\n\u201cthe\u201d, \u201cinto\u201d, \u201cof\u201d, \u201cand\u201d, \u201c,\u201d , \u201c.\u201d, \u201c\\n\u201d\nSST2\n\u201cthe\u201d, \u201cinto\u201d, \u201cof\u201d, \u201cand\u201d, \u201c.\u201d, \u201c\\n\u201d\nRTE\n\u201cthe\u201d, \u201cof\u201d, \u201cinto\u201d, \u201cand\u201d, \u201cinto\u201d, \u201c.\u201d, \u201c\\n\u201d\nCB\n\u201cthe\u201d, \u201cof\u201d, \u201cand\u201d, \u201cinto\u201d, \u201c,\u201d ,\u201c.\u201d, \u201c\\n\u201d\nTREC\n\u201cthe\u201d, \u201cbased\u201d, \u201con\u201d, \u201cwhether\u201d, \u201ctheir\u201d, \u201cis\u201d, \u201ca\u201d, \u201c,\u201d, \u201cor\u201d, \u201c.\u201d, \u201c\\n\u201d\nDBPedia\n\u201cthe\u201d, \u201cbased\u201d, \u201con\u201d, \u201cwhether\u201d, \u201cthey\u201d, \u201care\u201d, \u201cabout\u201d, \u201ca\u201d, \u201c,\u201d, \u201cor\u201d, \u201c.\u201d, \u201c\\n\u201d\nTable 12: The stopwords used in our experiments. Words rarely exist in the task demonstrations are omitted.\n<div style=\"text-align: center;\">Table 12: The stopwords used in our experiments. Words rarely exist in the task demonstrations are omitted.</div>\n# A In-context Learning Templates\nIn this section, we present all the in-context learning templates used in this paper. The examples are provided in Table 1. For the RTE and CB datasets, there are two distinct inputs in the demonstrations (i.e., the hypothesis and the premise), which we denote as DinA and DinB, respectively.\n# B Stopword Tokens\nFor the results shown in the main paper, we used the stopword token list shown in Table 12. This list only includes the stopword tokens from the task instruction, aiming to minimize their presence. We made this choice under the assumption that taskaffecting information should be stored densely in a few tokens. Hence, the number of tokens whose representations affect the final task performance significantly should be small. Nevertheless, one might be curious about the re-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/76ca/76caf896-06a3-4ca1-9273-eb5e62c5d12d.png\" style=\"width: 50%;\"></div>\nTable 13: The whole stopword list from NLTK. We add the punctuation tokens in this case.\nsults if we used a more complete stopword list. In this case, we utilize a more comprehensive stopword token list of NLTK6 shown in Table 13 and conduct the representation-level ablation once more. The results are presented in Table 14. It can be observed that all the conclusions from Section 4.2.1 are still well established. A few results are different from Table 2 and Table 3 because we accidentally masked the representations of the \u201c</s>\u201d token in this set of experiments. We claim that this accidental masking does not impact the main findings of these experiments.\n# C Significance test for the representation-level ablation\nIn this section, we report the p-value of all the pair-wise comparisons in the representation-level ablation experiments in Table 2 and Table 3. Results are shown in Table 15\n# D Template used for the random string experiments\nIn this section, we present all the in-context learning templates used for the random experiments in Section 5.2. In the Randomfixed scenario, the Tin and Tout are consistent across all demonstrations. For the Randomnonfixed scenario, we employ different random string templates for each demonstration. We use 5 random string templates for each setting, shown in Table 16, Table 17, Table 18, Table 19, and Table 20. The results in Section 5.2 are averaged over the results with all the different random string templates. 6https://gist.github.com/sebleier/554280\nModels\nSetting\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nOpenLlama\n3B\nZero-shot\n22.0\n20.0\n23.6\n5.4\n44.4\n1.8\n+ CONT\n26.2\n52.1\n30.1\n7.4\n51.9\n37.9\n+ STOP\n38.0\n85.1\n31.6\n54.6\n58.8\n55.7\n+ TEMP\n56.5\n86.7\n27.1\n62.2\n56.4\n52.3\nStandard ICL\n63.7\n91.2\n21.9\n61.9\n57.4\n52.0\n- TEMP\n42.1\n87.2\n25.9\n56.3\n58.3\n57.4\n- CONT\n57.1\n88.4\n27.1\n62.6\n56.8\n52.4\n- STOP\n61.6\n90.7\n24.8\n62.2\n56.7\n51.9\nLlama\n7B\nZero-shot\n25.0\n29.2\n41.4\n0.0\n54.2\n3.6\n+ CONT\n32.4\n57.9\n42.5\n12.5\n55.5\n46.1\n+ STOP\n59.9\n85.9\n51.7\n28.9\n56.0\n52.7\n+ TEMP\n70.8\n90.2\n58.4\n66.2\n66.3\n73.5\nStandard ICL\n82.4\n94.3\n63.5\n68.7\n68.6\n71.3\n- TEMP\n64.7\n84.1\n54.0\n56.7\n56.1\n48.2\n- CONT\n75.4\n93.8\n59.8\n67.5\n66.8\n74.8\n- STOP\n81.4\n94.2\n60.5\n67.9\n67.6\n72.1\nLlama\n13B\nZero-shot\n59.0\n18.0\n37.0\n0.0\n0.0\n0.0\n+ CONT\n30.6\n52.4\n43.8\n13.0\n60.2\n45.5\n+ STOP\n72.7\n78.7\n49.2\n27.4\n58.5\n27.1\n+ TEMP\n78.5\n92.3\n59.0\n74.2\n67.4\n52.3\nStandard ICL\n81.6\n94.3\n60.0\n76.1\n70.6\n39.9\n- TEMP\n71.7\n80.1\n56.2\n8.7\n56.5\n29.3\n- CONT\n79.3\n93.4\n60.1\n74.1\n68.4\n47.6\n- STOP\n79.2\n94.1\n59.3\n73.8\n68.9\n44.6\nLlama\n30B\nZero-shot\n70.2\n88.6\n60.6\n30.2\n58.1\n19.6\n+ CONT\n27.8\n61.7\n61.9\n10.8\n64.2\n68.1\n+ STOP\n74.7\n93.6\n66.9\n70.8\n69.1\n63.8\n+ TEMP\n80.6\n95.2\n63.1\n71.9\n78.7\n84.0\nStandard ICL\n85.0\n96.5\n68.1\n78.4\n78.5\n83.3\n- TEMP\n79.5\n93.8\n58.5\n62.8\n68.0\n68.0\n- CONT\n82.7\n95.9\n62.9\n74.1\n79.6\n83.1\n- STOP\n84.4\n96.1\n61.8\n72.8\n79.4\n82.1\nTable 14: The accuracy results of the representation level ablation study where we use the more complete stopword token list of NLTK. All values are presented as percentages. The best results presented by the number of ablated token types are in bold.\nModels\nSettings\nAGNews\nSST2\nTREC\nDBPedia\nRTE\nCB\nP-value\np < 0.05\nP-value\np < 0.05\nP-value\np < 0.05\nP-value\np < 0.05\nP-value\np < 0.05\nP-value\np < 0.05\nOpenLlama\n3B\ntemp <-> cont\n0.0000581\nT\n0.0000000\nT\n0.1952165\nF\n0.0000000\nT\n0.0042427\nT\n0.0027293\nT\ntemp <-> stop\n0.0001605\nT\n0.0571278\nF\n0.0242797\nT\n0.0000663\nT\n0.0319815\nT\n0.0985942\nF\ncont <-> stop\n0.0023957\nT\n0.0000001\nT\n0.1940792\nF\n0.0000000\nT\n0.0000073\nT\n0.0000698\nT\ntemp_cont <-> cont_stop\n0.0000065\nT\n0.0385760\nT\n0.3206221\nF\n0.0000000\nT\n0.1237570\nF\n0.0544049\nF\ntemp_stop <-> cont_stop\n0.0001166\nT\n0.4514005\nF\n0.2549225\nF\n0.0000001\nT\n0.0545474\nF\n0.0534521\nF\ntemp_cont <-> temp_stop\n0.0000507\nT\n0.0096752\nT\n0.0005775\nT\n0.0000000\nT\n0.1208140\nF\n0.3193696\nF\nLlama\n7B\ntemp <-> cont\n0.0000000\nT\n0.0000001\nT\n0.0000020\nT\n0.0000001\nT\n0.0000004\nT\n0.0000001\nT\ntemp <-> stop\n0.0000083\nT\n0.0101283\nT\n0.0002883\nT\n0.1438193\nF\n0.0000000\nT\n0.0000031\nT\ncont <-> stop\n0.0000060\nT\n0.0000001\nT\n0.0019529\nT\n0.0000001\nT\n0.3392237\nF\n0.0016487\nT\ntemp_cont <-> cont_stop\n0.0000115\nT\n0.0030175\nT\n0.0005950\nT\n0.0000000\nT\n0.0000000\nT\n0.0001649\nT\ntemp_stop <-> cont_stop\n0.0002004\nT\n0.0227328\nT\n0.0015468\nT\n0.0000000\nT\n0.0000001\nT\n0.0000094\nT\ntemp_cont <-> temp_stop\n0.0086396\nT\n0.0003632\nT\n0.0089932\nT\n0.0000007\nT\n0.1637608\nF\n0.1553081\nF\nLlama\n13B\ntemp <-> cont\n0.0000000\nT\n0.0000000\nT\n0.0000082\nT\n0.0000001\nT\n0.0006445\nT\n0.1060226\nF\ntemp <-> stop\n0.0003841\nT\n0.0000012\nT\n0.0034370\nT\n0.0002018\nT\n0.0000000\nT\n0.0010178\nT\ncont <-> stop\n0.0000000\nT\n0.0000202\nT\n0.0002820\nT\n0.0000000\nT\n0.0098209\nT\n0.0022848\nT\ntemp_cont <-> cont_stop\n0.0010838\nT\n0.0000730\nT\n0.0004557\nT\n0.0000048\nT\n0.0000001\nT\n0.0002364\nT\ntemp_stop <-> cont_stop\n0.0007763\nT\n0.0000310\nT\n0.0016544\nT\n0.0000000\nT\n0.0000000\nT\n0.0000888\nT\ntemp_cont <-> temp_stop\n0.4411518\nF\n0.1158895\nF\n0.3323328\nF\n0.0000000\nT\n0.3148253\nF\n0.0144961\nT\nLlama\n30B\ntemp <-> cont\n0.0000000\nT\n0.0000003\nT\n0.1534319\nF\n0.0000000\nT\n0.0000000\nT\n0.0002244\nT\ntemp <-> stop\n0.0007359\nT\n0.0048547\nT\n0.1797405\nF\n0.0000023\nT\n0.0000002\nT\n0.0008789\nT\ncont <-> stop\n0.0000000\nT\n0.0000003\nT\n0.0204911\nT\n0.0000000\nT\n0.0002626\nT\n0.4319440\nF\ntemp_cont <-> cont_stop\n0.0001365\nT\n0.0788756\nF\n0.0032131\nT\n0.0000000\nT\n0.0000098\nT\n0.0003242\nT\ntemp_stop <-> cont_stop\n0.0006045\nT\n0.0609501\nF\n0.0165374\nT\n0.0000011\nT\n0.0000009\nT\n0.0003821\nT\ntemp_cont <-> temp_stop\n0.0012936\nT\n0.3583931\nF\n0.1415489\nF\n0.0001034\nT\n0.0009055\nT\n0.3979685\nF\nTable 15: The pair-wise t-test significance results. \u201cT\u201d means True while \u201cF\u201d means False. In this table, \u201ctemp\u201d means only keeping temp, which is zero-shot + TEMP. \u201ctemp_cont\u201d means ablating the stopword token representations, which is Standard ICL \u2212STOP.\nDatasets\nNotations\nExamples\nRandomfixed\nCB & RTE\nTin\nfdafdasjklfdadf: {DinA}\\n zcxvnmxcjkfdas: {DinB}\\n\nTout\nreqwiorewsdafjl: {Dout}\\n\\n\nOther tasks\nTin\ndsafjkldafdsajk: {Din}\\n\nTout\nreqwiorewsdafjl: {Dout}\\n\\n\nRandomnonfixed\nCB & RTE\nTin\n1\nfdafdasjklfdadf: {DinA}\\n zcxvnmxcjkfdas: {DinB}\\n\nTout\n1\nxiadfjdsalgfweqrjl: {Dout}\\n\\n\nTin\n2\ngfhdajkgfhdasfj: {DinA}\\n cvxhlkdadsajfk: {DinB}\\n\nTout\n2\nyufoufgaddavfdnsl: {Dout}\\n\\n\nTin\n3\nrrqetrizxcsdafq: {DinA}\\n vncmxasdgfadsl: {DinB}\\n\nTout\n3\nafdgvcxjlzxnvxzla: {Dout}\\n\\n\nTin\n4\nmvfvxadfawewqro: {DinA}\\n lkajsdfopsadfp: {DinB}\\n\nTout\n4\nfgsgfskjvcdafds: {Dout}\\n\\n\nTin\nt\nsdsajfjdsaczvvv: {DinA}\\n hkljfdiabasdfj: {DinB}\\n\nTout\nt\ndafhglajfdvcaol: {Dout}\\n\\n\nOther tasks\nTin\n1\ndsafjkldaasdfjkl: {Din}\\n\nTout\n1\nxiadfjdsalgfweqrjl: {Dout}\\n\\n\nTin\n2\newqroudajfsdafq: {Din}\\n\nTout\n2\nyufoufgaddavfdnsl: {Dout}\\n\\n\nTin\n3\neqdashcxzlreqguio: {Din}\\n\nTout\n3\nafdgvcxjlzxnvxzla: {Dout}\\n\\n\nTin\n4\ncxzvadeqrczxdsa: {Din}\\n\nTout\n4\nfgsgfskjvcdafds: {Dout}\\n\\n\nTin\nt\nvcxnkfgahvczxkl: {Din}\\n\nTout\nt\ndafhglajfdvcaol: {Dout}\\n\\n\nSwap\nCB & RTE\nTin\nAnswer: {DinA}\\n Hypothesis: {DinB}\\n\nTout\nPremise: {Dout}\\n\\n\nTable 16: Example #1 of the ICL template used in all of our random experiments.\nDatasets\nNotations\nExamples\nRandomfixed\nCB & RTE\nTin\neszycidpyopumzg: {DinA}\\n sgrlobvqgthjpwz: {DinB}\\n\nTout\nzbyygcrmzfnxlsu: {Dout}\\n\\n\nOther tasks\nTin\neszycidpyopumzg: {Din}\\n\nTout\nzbyygcrmzfnxlsu: {Dout}\\n\\n\nRandomnonfixed\nCB & RTE\nTin\n1\neszycidpyopumzg: {DinA}\\n sgrlobvqgthjpwz: {DinB}\\n\nTout\n1\nzbyygcrmzfnxlsu: {Dout}\\n\\n\nTin\n2\ncwknayjkywwvpty: {DinA}\\n muzprouhvtidhqe: {DinB}\\n\nTout\n2\nlnlgffeurextxme: {Dout}\\n\\n\nTin\n3\npdnizszmpkfjzvo: {DinA}\\n ujulhuzkkqlfwkl: {DinB}\\n\nTout\n3\ngflemobnbdjngii: {Dout}\\n\\n\nTin\n4\ngvsrxbdoxmpablo: {DinA}\\n ujulhuzkkqlfwkl: {DinB}\\n\nTout\n4\ngflemobnbdjngii: {Dout}\\n\\n\nTin\nt\ngvsrxbdoxmpablo: {DinA}\\n xipddzrshrhprrb: {DinB}\\n\nTout\nt\nnpkxdzaipdpkbrs: {Dout}\\n\\n\nOther tasks\nTin\n1\neszycidpyopumzg: {Din}\\n\nTout\n1\nzbyygcrmzfnxlsu: {Dout}\\n\\n\nTin\n2\ncwknayjkywwvpty: {Din}\\n\nTout\n2\nlnlgffeurextxme: {Dout}\\n\\n\nTin\n3\npdnizszmpkfjzvo: {Din}\\n\nTout\n3\ngflemobnbdjngii: {Dout}\\n\\n\nTin\n4\ngvsrxbdoxmpablo: {Din}\\n\nTout\n4\nnpkxdzaipdpkbrs: {Dout}\\n\\n\nTin\nt\ndgldzypdptzcekq: {Din}\\n\nTout\nt\nxobxfpnzsfzipol: {Dout}\\n\\n\nTable 17: Example #2 of the ICL template used in all of our random experiments.\nDatasets\nNotations\nExamples\nRandomfixed\nCB & RTE\nTin\nbcclfxzvjitgtbs: {DinA}\\n evtlfrwvtfmjtns: {DinB}\\n\nTout\nqtnheeipeustcwn: {Dout}\\n\\n\nOther tasks\nTin\nbcclfxzvjitgtbs: {Din}\\n\nTout\nqtnheeipeustcwn: {Dout}\\n\\n\nRandomnonfixed\nCB & RTE\nTin\n1\nbcclfxzvjitgtbs: {DinA}\\n evtlfrwvtfmjtns: {DinB}\\n\nTout\n1\nqtnheeipeustcwn: {Dout}\\n\\n\nTin\n2\nymupnggvmbnoobq: {DinA}\\n rrrnpgbmmgqymky: {DinB}\\n\nTout\n2\nxleuwtyqnnfgzjx: {Dout}\\n\\n\nTin\n3\npdnizszmpkfjzvo: {DinA}\\n qlfulxzxwfnwbum: {DinB}\\n\nTout\n3\njpnvgbnjjlawqfo: {Dout}\\n\\n\nTin\n4\nmfkqxjoxtpmzdrs: {DinA}\\n yyzdeayigwzjosn: {DinB}\\n\nTout\n4\npdsqooqrhvydszp: {Dout}\\n\\n\nTin\nt\nrerlkjfvlvyzpmc: {DinA}\\n iuumpcsevursgqe: {DinB}\\n\nTout\nt\ntuaqblysbipihsv: {Dout}\\n\\n\nOther tasks\nTin\n1\nbcclfxzvjitgtbs: {Din}\\n\nTout\n1\nqtnheeipeustcwn: {Dout}\\n\\n\nTin\n2\nymupnggvmbnoobq: {Din}\\n\nTout\n2\nxleuwtyqnnfgzjx: {Dout}\\n\\n\nTin\n3\npdwunmjronsmuvu: {Din}\\n\nTout\n3\njpnvgbnjjlawqfo: {Dout}\\n\\n\nTin\n4\nmfkqxjoxtpmzdrs: {Din}\\n\nTout\n4\npdsqooqrhvydszp: {Dout}\\n\\n\nTin\nt\nrerlkjfvlvyzpmc: {Din}\\n\nTout\nt\ntuaqblysbipihsv: {Dout}\\n\\n\nTable 18: Example #3 of the ICL template used in all of our random experiments.\n<div style=\"text-align: center;\">Datasets Notations Examples</div>\nDatasets\nNotations\nExamples\nRandomfixed\nCB & RTE\nTin\nhsreltpusctapir: {DinA}\\n woxwxgwctxdumok: {DinB}\\n\nTout\nprlhxooromawkcp: {Dout}\\n\\n\nOther tasks\nTin\nhsreltpusctapir: {Din}\\n\nTout\nprlhxooromawkcp: {Dout}\\n\\n\nRandomnonfixed\nCB & RTE\nTin\n1\nhsreltpusctapir: {DinA}\\n woxwxgwctxdumok: {DinB}\\n\nTout\n1\nprlhxooromawkcp: {Dout}\\n\\n\nTin\n2\ncbptgaytithxayh: {DinA}\\n bhxgcstisqmfnpz: {DinB}\\n\nTout\n2\nmvpvoeuvgczfemz: {Dout}\\n\\n\nTin\n3\nhtkbzfizxwpeqrm: {DinA}\\n felxgmjeuabznwd: {DinB}\\n\nTout\n3\nglfwilpyrwnsujg: {Dout}\\n\\n\nTin\n4\nfrskoasvqybxcob: {DinA}\\n bkepuhnckdaqmhx: {DinB}\\n\nTout\n4\nljttiywadveyzah: {Dout}\\n\\n\nTin\nt\ndfpqndhxehhtser: {DinA}\\n bvucjofrggmmcsh: {DinB}\\n\nTout\nt\nkoesxfmmjjjjvmp: {Dout}\\n\\n\nOther tasks\nTin\n1\nhsreltpusctapir: {Din}\\n\nTout\n1\nprlhxooromawkcp: {Dout}\\n\\n\nTin\n2\ncbptgaytithxayh: {Din}\\n\nTout\n2\nmvpvoeuvgczfemz: {Dout}\\n\\n\nTin\n3\nhtkbzfizxwpeqrm: {Din}\\n\nTout\n3\nglfwilpyrwnsujg: {Dout}\\n\\n\nTin\n4\nfrskoasvqybxcob: {Din}\\n\nTout\n4\nljttiywadveyzah: {Dout}\\n\\n\nTin\nt\ndfpqndhxehhtser: {Din}\\n\nTout\nt\nkoesxfmmjjjjvmp: {Dout}\\n\\n\nTable 19: Example #4 of the ICL template used in all of our random experiments.\nDatasets\nNotations\nExamples\nRandomfixed\nCB & RTE\nTin\nhjdxmpeccamrjzy: {DinA}\\n agxyhmkawezafde: {DinB}\\n\nTout\nndxtrwvqugyygku: {Dout}\\n\\n\nOther tasks\nTin\nhjdxmpeccamrjzy: {Din}\\n\nTout\nndxtrwvqugyygku: {Dout}\\n\\n\nRandomnonfixed\nCB & RTE\nTin\n1\nhjdxmpeccamrjzy: {DinA}\\n agxyhmkawezafde: {DinB}\\n\nTout\n1\nndxtrwvqugyygku: {Dout}\\n\\n\nTin\n2\nmcsgenpkdwsfknc: {DinA}\\n egnqobhzvxjhsxh: {DinB}\\n\nTout\n2\nijkdikcmiskofsg: {Dout}\\n\\n\nTin\n3\ncmaqcvtdkemdauv: {DinA}\\n oslzaygbefxlwqt: {DinB}\\n\nTout\n3\nmumrjhndwmidwmj: {Dout}\\n\\n\nTin\n4\ncgmylzvslxmojvq: {DinA}\\n tlwxsjmnfkolffl: {DinB}\\n\nTout\n4\nmitaowjyibjwwol: {Dout}\\n\\n\nTin\nt\npvockachyflybtk: {DinA}\\n wtjqmtwxbnpyqbp: {DinB}\\n\nTout\nt\nydediotfezhfnbx: {Dout}\\n\\n\nOther tasks\nTin\n1\nhsreltpusctapir: {Din}\\n\nTout\n1\nprlhxooromawkcp: {Dout}\\n\\n\nTin\n2\ncbptgaytithxayh: {Din}\\n\nTout\n2\nmvpvoeuvgczfemz: {Dout}\\n\\n\nTin\n3\nhtkbzfizxwpeqrm: {Din}\\n\nTout\n3\nglfwilpyrwnsujg: {Dout}\\n\\n\nTin\n4\nfrskoasvqybxcob: {Din}\\n\nTout\n4\nljttiywadveyzah: {Dout}\\n\\n\nTin\nt\ndfpqndhxehhtser: {Din}\\n\nTout\nt\nkoesxfmmjjjjvmp: {Dout}\\n\\n\nTable 20: Example #5 of the ICL template used in all of our random experiments.\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL\u2019s working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. Previous methods have shown instability in performance due to slight changes in prompts, necessitating a better understanding of the underlying mechanisms to improve task performance.",
        "problem": {
            "definition": "The problem addressed in this paper is the lack of understanding regarding how large language models (LLMs) extract and utilize task-encoding tokens from in-context learning demonstrations, which leads to performance instability.",
            "key obstacle": "The core obstacle is the empirical nature of prompt design, where small variations can lead to significant performance fluctuations, indicating that existing methods do not adequately address the dynamics of task-encoding tokens."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that certain tokens in the prompts significantly affect task performance, suggesting that a deeper analysis of these tokens could lead to improved stability in ICL.",
            "opinion": "The proposed idea involves identifying and analyzing task-encoding tokens to understand their roles and characteristics, which could provide insights into stabilizing ICL performance.",
            "innovation": "This method innovatively categorizes tokens into structural types (template, stopword, content) and investigates their specific contributions to task performance, contrasting with previous approaches that focused on isolated aspects of ICL."
        },
        "method": {
            "method name": "Task-Encoding Token Analysis",
            "method abbreviation": "TETA",
            "method definition": "TETA is a systematic approach to identify and analyze task-encoding tokens in in-context learning prompts by categorizing them and assessing their impact on model performance.",
            "method description": "The method involves categorizing tokens into template, stopword, and content types, followed by ablation studies to evaluate their contributions to task performance.",
            "method steps": [
                "Categorize tokens in ICL prompts into template, stopword, and content.",
                "Conduct representation-level ablation to assess the impact of each token type on task performance.",
                "Perform token-level ablation to determine the essential tokens for maintaining task performance."
            ],
            "principle": "The method is effective because it systematically identifies which tokens are critical for performance, allowing for a better understanding of how LLMs leverage information from prompts."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using widely recognized text classification datasets such as AGNews, SST2, TREC, DBPedia, RTE, and CB, with various model sizes (3B, 7B, 13B, 30B) from the Llama model family.",
            "evaluation method": "The evaluation involved ablation studies where different token types were masked or removed from prompts to observe the resulting changes in model performance."
        },
        "conclusion": "The study concludes that template and stopword tokens are crucial for maintaining task performance in ICL, as their absence significantly degrades performance. The findings enhance the understanding of how LLMs utilize task-encoding tokens and suggest that careful design of prompts can mitigate performance instability.",
        "discussion": {
            "advantage": "The proposed approach provides a clearer framework for understanding the roles of different token types in ICL, potentially leading to more stable and effective prompt designs.",
            "limitation": "The manual categorization of tokens may lack precision, and the findings are primarily based on classification tasks, which may not generalize to generative tasks.",
            "future work": "Future research should focus on refining token categorization methods, exploring the implications of task-encoding tokens in generative contexts, and developing strategies to enhance the stability of ICL across various tasks."
        },
        "other info": {
            "ethics statement": "The research does not present increased risks beyond existing norms in NLP and computational linguistics, but it acknowledges potential biases in training data and the misuse of models.",
            "limitations": "The study is limited to classification datasets and may not comprehensively cover the functions of other token types that could influence performance."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing."
        },
        {
            "section number": "1.3",
            "key information": "The problem addressed in this paper is the lack of understanding regarding how large language models (LLMs) extract and utilize task-encoding tokens from in-context learning demonstrations."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Task-Encoding Token Analysis (TETA), systematically identifies and analyzes task-encoding tokens in in-context learning prompts."
        },
        {
            "section number": "3.3",
            "key information": "The method involves categorizing tokens into template, stopword, and content types, followed by ablation studies to evaluate their contributions to task performance."
        },
        {
            "section number": "6.1",
            "key information": "The findings enhance the understanding of how LLMs utilize task-encoding tokens and suggest that careful design of prompts can mitigate performance instability."
        },
        {
            "section number": "6.4",
            "key information": "The manual categorization of tokens may lack precision, and the findings are primarily based on classification tasks, which may not generalize to generative tasks."
        },
        {
            "section number": "7",
            "key information": "Future research should focus on refining token categorization methods, exploring the implications of task-encoding tokens in generative contexts, and developing strategies to enhance the stability of ICL across various tasks."
        }
    ],
    "similarity_score": 0.712331681576688,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Identifying and Analyzing Task-Encoding Tokens in Large Language Models.json"
}