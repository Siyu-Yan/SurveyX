{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.14211",
    "title": "LUNA: A Model-Based Universal Analysis Framework for Large Language Models",
    "abstract": "Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, LLMs have made rapid advancements that have propelled AI to a new level, enabling even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large model scale, and autoregressive generation schema, differ from classic AI software based on CNNs and RNNs and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand. Towards bridging this gap, we initiate an early exploratory study and propose a universal analysis framework for LLMs, LUNA, designed to be general and extensible, to enable versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset, which is empowered by various abstract model construction methods. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes.",
    "bib_name": "song2024lunamodelbaseduniversalanalysis",
    "md_text": "# LUNA: A Model-Based Universal Analysis Framework for Large Language Models\nDa Song*, Xuan Xie*, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, Lei Ma \ufffd\nAbstract\u2014Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, Large Language Models (LLMs) have made rapid advancements that have propelled AI to a new level, enabling and empowering even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs, e.g., robustness and hallucination, have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large neural network scale, and autoregressive generation usage contexts, differ from classic AI software based on Convolutional Neural Networks and Recurrent Neural Networks and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand across diverse domains. Towards bridging such a gap, in this paper, we initiate an early exploratory study and propose a universal analysis framework for LLMs LUNA, which is designed to be general and extensible and enables versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset and proxy, which is empowered by various abstract model construction methods built-in LUNA. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both the abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes, e.g., abnormal behavior detection. To better understand the potential usefulness of our analysis framework LUNA, we conduct a large-scale evaluation, the results of which demonstrate that 1) the abstract model has the potential to distinguish normal and abnormal behavior in LLM, 2) LUNA is effective for the real-world analysis of LLMs in practice, and the hyperparameter settings influence the performance, 3) different evaluation metrics are in different correlations with the analysis performance. In order to encourage further studies in the quality assurance of LLMs, we made all of the code and more detailed experimental results data available on the supplementary website of this paper https://sites.google.com/view/llm-luna.\narXiv:2310.14211v2\n14211v2\nIndex Terms\u2014Large Language Models, Deep Neural Networks, Model-based Analysis, Quality Assurance\narXiv:231\nOver the last few years, a series of tremendous performance leaps in many real-world applications across domains have been empowered by the rapid advancement of LLMs, especially in the domain of Software Engineering (SE) and Natural Language Processing (NLP), e.g., code generation [1], program repair [2], sentiment analysis [3], and question answering [4]. Representative LLM-enabled applications such as ChatGPT [5], GPT-4 [6], and Llama [7] are often recognized as the early foundation towards Artificial General Intelligence (AGI) [8]. More recently, LLMs have presented the promising potential to become new enablers and boosters to further revolutionize intelligentization and automation for various key stages of the software production life-cycle.\n\u2022 *These authors contributed equally to this work. \u2022 \ufffdCorresponding author \u2022 Da Song, Xuan Xie, Jiayang Song, and Yuheng Huang are with the Department of Electrical and Computer Engineering at the University of Alberta, Canada. E-mail: {dsong4, xxie9, jiayan13, yuheng18}@ualberta.ca \u2022 Derui Zhu is with the Department of Computer Science at the Technical University of Munich, Germany. E-mail: derui.zhu@tum.de \u2022 Felix Juefei-Xu is with New York University, USA. E-mail: juefei.xu@nyu.edu \u2022 Lei Ma is with The University of Tokyo, Japan, and University of Alberta, Canada. E-mail: ma.lei@acm.org\nDespite the rapid development, the current quality [9], reliability [10], robustness [11], and explainability [12] of LLMs pose many concerns of social society and technical challenges, the research on which, on the other hand, is still at a very early stage. For example, recent research indicates that existing LLMs can occasionally generate content that is toxic, biased, insecure, or erroneous [9], [13], [14]. For example, a typically new type of quality issue is the phenomenon of hallucination [15], where LLMs confidently produce nonfactual or erroneous outputs, which poses significant challenges for their implementation, particularly in environments where safety and security are paramount. Moreover, the rapid industrial adoption of LLMs in various applications, e.g., robotic control [16] and medical image diagnosis [17], necessitates urgent analysis and risk assessment methodologies for LLMs. In recent years, there has come an increasing trend in research to tackle the quality assurance challenges of deep learning software, especially Deep Neural Networks (DNNs) [18]\u2013[39]. Some studies have focused on DNN testing with the goal of pinpointing inputs that a DNN struggles to manage [18]\u2013[24], [30]\u2013[35], [40]\u2013[43]. Concurrently, advancements in DNN debugging and repair [25]\u2013[29], [36]\u2013 [39], [44]\u2013[49] aim to understand the reasons behind a\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5879/58797638-e40a-4f18-b6ee-a27cea56c1c2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: The workflow summary of LUNA.</div>\nDNN\u2019s incorrect predictions and subsequently repair the model. These studies have made notable contributions to the advancement of quality assurance in DL-based software. However, a majority of these works are centered around Convolutional Neural Networks (CNNs) [50] and Recurrent Neural Networks (RNNs) [51]. Pei et al. propose DeepXplore, a white-box DNN testing framework combined with neuron coverage and differential testing to efficiently capture defects in DNN systems [18]. Zohdinasab et al. leverage illumination search to identify and quantify the dimensions of feature space in testing deep learning systems [52]. Hu et al. propose a framework for mapping between dangerous situations and the image transformations in the machine vision components [53]. Among these analysis techniques, model-based analysis [29], [54]\u2013[58] has been demonstrated as an effective approach to both provide analysis results, e.g., testing and monitoring, and human-explainable results. Pan and Rajan [59] propose to decompose a CNN model into modules for each output class, enabling reusability and lowering the environmental cost. Dong et al. [60] develop an approach to extract probabilistic automata for RNN interpretation, which integrates hidden states abstraction and automata learning. Qi et al. develop ArchRepair, which repairs DNNs by jointly optimizing architecture and weights at the block level [61]. Different from CNNs and RNNs, LLMs behave with distinct features such as the adaptation of the self-attention [62] mechanism as its core, the complex and large-scale model size (e.g., 6.7 billion to 65.2 billion parameters in LLaMA series released by Meta [7]), and the generative output scheme which highly depend on a broad spectrum of user\u2019s inputs. Such features make the analysis of LLMs\u2019 behavior more challenging compared to classification contexts, which are the main focus of existing research. Therefore, even up to the present, only very limited research has been conducted to probe general-purpose LLM-oriented analysis techniques to understand the quality and behavior characteristics of LLMs\nfrom various aspects. The prospective quality assurance methods should help better comprehend LLMs\u2019 internal behaviors, identify unwanted outputs, and aid in improving the trustworthiness of LLMs in practical usage. As described above, the philosophy of model-based analysis has been widely proven useful in providing quality assurance for traditional DNNs; however, its effectiveness for LLMs is still unknown and deserves further investigation. Therefore, to bridge this gap, we propose and design LUNA, a model-based universal analysis framework for large language models. The first step of LUNA is to extract an assistant model for analysis. Due to the high dimensional space and sparsely distributed states, we extract and build the abstract model such as Discrete-Time Markov Chain (DTMC) and Hidden Markov Model (HMM), to enable and ease the analysis procedure and capture LLM\u2019s probabilistic nature. With the obtained abstract model, we further perform semantics binding, which is the degree of satisfaction of the LLM with respect to the desired quality perspective, to the abstract model to enable in-depth quality analysis. Moreover, to evaluate the quality of the model, we collect abstract model-wise metrics and propose semantics-wise metrics to measure from the perspectives of models and semantics, respectively. Finally, we apply the constructed abstract model on abnormal behavior detection to detect potentially erroneous outputs from the LLMs, e.g., hallucination. It is worth noting that while LUNA aims to be universally applicable across various tasks and domains, its current implementation and validation are contingent on the availability of open-source LLMs due to the requirement of the LLM internal hidden information. In order to demonstrate the potential usefulness of LUNA, we conduct a large-scale evaluation across multiple applications of LLMs. The experimental results and in-depth analysis across three trustworthiness perspectives (e.g., out-of-distribution detection, adversarial robustness,\nand truthfulness) confirm that: 1) the constructed abstract model can capture and distinguish normal and abnormal behaviors of the LLM; 2) the quality of the abstract model is highly impacted by the techniques and corresponding hyperparameters used in model construction (e.g., dimension reduction and state partition); 3) LUNA is effective in abnormal behavior detection, e.g., the ROC AUC of adversarial attack detection can achieve 83%; 4) model-based quality measurement metrics (e.g., abstract model-wise and semantics-wise) assess the quality of the model from distinct aspects and are correlated with the performance of the framework differently.\nThe main contributions of this paper are summarized as follows:\nThe main contributions of this paper are summarized as\n\u2022 A universal white-box model-based analysis framework, which is designed for general-purpose quality analysis for LLMs, provides a human-interpretable way to characterize LLM\u2019s behavior. \u2022 A set of model quality measurement metrics for LLMs, which are collected from existing research (abstract model-wise metrics) and newly proposed (semantics-wise metrics). The correlations with the analysis performance show their potential in guiding the abstract model construction. \u2022 An extensive experiment is conducted to demonstrate the effectiveness of LUNA, which is on 3 trustworthiness perspectives, 3 LLMs, 7 datasets, 12 quality measurement metrics, 180 hyperparameter settings, and a total of more than 6, 300 CPU hours. The results demonstrate that LUNA is effective in LLM\u2019s abnormal behavior detection. \u2022 An exploratory study to investigate the effectiveness of model-based analysis in the context of LLMs. This paper also targets inspiring more relevant research in this direction towards approaching the goal of achieving trustworthy LLMs in practice.\nThe Contributions to the Software Engineering Field. With an increasing trend of adopting LLMs in the software production life cycle, LLMs would potentially draw significant impact to the domain of software engineering as they present explicit capabilities to accelerate the development process and implementation outcomes [63]\u2013[66]. Hence, the quality analysis of LLMs calls even more attention as it ridges the last gap to further deploy LLMs on safety, reliability, and security-concerned applications. Following this path, our work also endeavors to empower the interaction of LLMs within SE by establishing the early foundation of model-based analysis to enable more systematic investigations towards trustworthy LLMs across various SE applications. The rest of the paper is structured as follows. Section 2 introduces the corresponding background. Section 3 describes the different abstraction methods and model construction techniques. Section 4 details the experiment setup and reports the results. Section 5 discusses the potential impact and future directions. Section 6 inspects the threats that may affect the validity of our work. Section 7 summarizes the related words, and Section 8 concludes the paper. All code and study findings have been made available at https://sites.google.com/view/llm-luna.\n# 2 BACKGROUND\nIn this section, we first provide the background knowledge on the analyzed deep learning model, i.e., the Large Language Model (LLM).Trustworthiness perspectives of LLMs are then introduced, which are of serious concern to the quality and reliability of LLMs. In addition, we describe the key idea of model-based analysis at a high level, the main technique used in our analysis framework.\n# 2.1 Large Language Models\nWitnessed by various industrial and academic communities, LLMs, a new revolution in AI technology, have demonstrated human-competitive capabilities in various natural language tasks across domains (e.g., text generation, language translation, code development) [1], [67]\u2013[71]. Intuitively, LLMs are a type of neural network model that is usually established based on the Transformer architecture [62] with millions, even billions of parameters. Such models are pre-trained on large corpora of text data, which enclose numerous commonsense knowledge [72]. LLMs output a sequence of words following a probability distribution; namely, each output token is generated coherently based on the input prompt and prior outputs. Recent research increasingly demonstrates that LLMs are capable of delivering high-level problem-solving skills for a variety of downstream tasks, such as question-and-answer [73], sentiment analysis [74], text summarization [75], code generation [76] and code summarization [77]. Besides the large network scale, the superior performance of LLMs can also give credit to the transformer architecture and its central mechanism: self-attention [62]. Selfattention assesses each element within the input sequence by comparing them with one another and then alters the respective positions in the output sequence. For instance, consider the sentence: \u201cA baby sat on the chair.\u201d Selfattention enables the LLM to analyze the relevance of each word to others in the sentence. For example, when processing \u201csat,\u201d the model assesses its connection to \u201cbaby\u201d and \u201cchair\u201d more significantly than to \u201cthe\u201d. This mechanism highlights the relationships of actions (\u201csat\u201d) to subjects (\u201cbaby\u201d) and objects (\u201cchair\u201d), adjusting the word\u2019s representation in the output. With a self-attention mechanism, the model is able to analyze the contextual nuances of the input. This enhances the accuracy and coherence of the output generated by the model. The unique transformer architecture enables the LLMs to surpass the traditional RNNs regarding many challenges, such as long-range dependencies [78] and gradient vanishing [79]. Self-attention is encompassed in the decoder block, which is a basic unit of the decoder-only LLM, which will be introduced later in this section. Many studies confirm the information enclosed in the output of the decoder block can be an asset to characterize the behaviors of an LLM [80]\u2013[83]. Thus, in this work, we leverage the decoder block outputs and traces extracted from the LLM to construct an abstract model-based LLM analysis framework. We further detail the model construction in our study in Section 3.2. LLMs can be categorized into three main different types according to their transformer architectures and pre-trained\ntasks: encoder-only, encoder-decoder, and decoder-only. Encoderonly LLMs (e.g., BERT [84], DeBERTa [85], RoBERTA [86]) are pre-trained by masking a certain number of input tokens and aim to predict the masked elements retroactively. Alternatively, encoder-decoder LLMs, such as BART [87], FlanUL2 [88] and T5 [89], utilize an encoder to first covert the input sequence into a hidden vector, then a subsequent decoder further converts the hidden vector into the output sequence. This encode-then-decode architecture has advantages in processing sequence-to-sequence tasks involving intricate mapping between the input and output. Decoder-only LLMs are auto-regressive models that predict each token based on the input sequence and the prior generated tokens. Representative decoder-only LLMs like GPT-4 [6], GPT-3 [90], Llama [7], and CodeLlama [91] are recognized as prevailing attributable to their training efficiency and scalability for large-scale models and datasets. In this study, we mainly focus on decoder-only LLM, such as Llama and CodeLlama, considering its availability, commonality, and computation cost. In the following, we provide an example of decoder-only LLM for illustration: Consider an example where we prompt an LLM with the query, \u201cWho is the president of the USA?\u201d. The LLM, leveraging its autoregressive nature, begins predicting the next word in the sequence, finding \u201cJoe\u201d to be the most likely initial word following the prompt. It doesn\u2019t stop there; the model continues to generate tokens sequentially, each time considering all previously generated words to ensure coherence and relevance. This process continues, generating \u201cBiden\u201d after \u201cJoe\u201d, progressively constructing the answer \u201cJoe Biden\u201d. The model concludes its generation with an End-of-sentence (EOS) token, signifying the completion of the response. When training, LLMs are trained on a large-scale text corpus using next-token prediction as the major task. The training objective of an LLM can be formalized as maximizing the likelihood of predicting the next word in a sequence given the previous words. This is mathematically represented as:\n(1)\nwhere:\n\u2022 w1:t\u22121 represents the sequence of words from the first to the (t \u22121)-th word, \u2022 wt is the t-th word in the sequence, \u2022 \u03b8 denotes the parameters of the model and \u2022 T is the length of the sequence. The objective function seeks to adjust the model parameters (\u03b8) to maximize the probability of each word in the sequence given the previous words. Nevertheless, our framework itself can still be generalized and adapted to LLMs other than decoder-only ones. We introduce the subject LLM and corresponding settings in Section 4.2.1.\n\u2022 The objective function seeks to adjust the model parameters (\u03b8) to maximize the probability of each word in the sequence given the previous words. Nevertheless, our framework itself can still be generalized and adapted to LLMs other than decoder-only ones. We introduce the subject LLM and corresponding settings in Section 4.2.1.\n# 2.2 LLM Trustworthiness Perspective\nInterpreting and understanding the behaviors of machine learning models, especially LLMs, is one of the essential\ntasks in both AI and SE communities [9], [19], [54], [92]. Recently, some researchers and industrial practitioners have tried to seek to understand the capability boundary and characteristics of the models in order to deploy them in practical applications more confidently and adequately. Quality analysis is applied to initiate to approach a comprehensive and consistent view of model trustworthiness, such as safety [93], robustness [94] and security [95]. In this work, we also leverage our proposed analysis framework LUNA to conduct quality analysis of LLMs from three aspects: Out-ofDistribution Detection, Adversarial Attacks and Hallucination. Such three perspectives are notoriously known as vital factors that affect the trustworthiness of LLMs.\n# 2.2.1 Out-of-Distribution (OOD) Detection\nA fundamental premise of machine learning is the similarity in distribution between training and future unseen test data. In other words, DNN models might falter when encountering some data (in the future) deviating from the training distribution [96]. Empirical research has shown that DNNs may even be highly confident to offer an erroneous prediction in this scenario [97], [98]. To alleviate this issue, OOD detection has been introduced to improve the quality of data-driven software by detecting the irrelevant OOD data without letting a DNN make wrong decisions on it that would be incorrectly handled with high possibility [20], [97]\u2013[99]. The objective is to craft a probability distribution estimation function PX (X is the training distribution) that assigns a score to a given input x and sets a corresponding threshold \u03bb for the OOD detection [100]:\n(2)\nWhile early research predominantly focused on image classification tasks, some efforts have also been made in NLP domains [101]. This encompasses the detection of OOD instances in text classification [102], translation [103], and question-answering [104]. Yet, the OOD challenges associated with LLMs present greater difficulty [105]. Firstly, LLMs\u2019 training data are often either inaccessible or too large, making exploration challenging. Secondly, LLMs\u2019 emergent ability across varied tasks makes traditional OOD measurements on standalone tasks inappropriate to apply. In traditional OOD detection, Moreno-Torres et al. [106] present a unified framework to analyze the distribution shift. Given a classification task X \u2192Y, the joint probability of x \u2208X and y \u2208Y can be represented as p(y, x) = p(y|x)p(x). Either the input distribution p(x) changes or the relationship between the input and class variable p(y|x) change can both be regarded as the distribution shift. LLMs operate in a high-dimensional, multimodal space where task-specific boundaries are not clearly defined, making the conventional notion of distribution shift less applicable. To address this issue, researchers collect OOD data made public after a certain timestamp (e.g. 2022) because LLMbased systems such as ChatGPT typically disclose the conclusion date of their training data (e.g. 2021). However, with the continuous evolution of LLMs, related benchmarks may soon be out-of-date. In this study, we instead focus on the\nOOD style [9], which refers to where original data is stylistically transformed, for example, converting contemporary English text to a style reminiscent of Shakespeare at both word level and sentence level. We follow the settings of the DecodingTrust benchmark [9] and perform related studies on the SST-2 development set.\n2.2.2 Adversarial Attacks\n# 2.2.2 Adversarial Attacks\nThe sensitivity of DNNs\u2019 predictions against subtle perturbation in the inputs as an intriguing property has been studied for over a decade now [107], [108]. Such sensitivity is due to the highly nonlinear nature of DNNs and could be utilized by adversaries for malicious attacks [109]. Related attack models are first defined in image domains as subtle and continuous perturbations on image pixels that aim to fool classifiers [110]. Such attacks can be formulated as optimization problems, and the goal is to find perturbations in inputs that change the final predictions. These attacks have been adapted to the text domain, where perturbations are defined as discrete ones at the word, sentence, or entire input levels. In this context, adversarial GLUE [111] stands out as a comprehensive benchmark for text domain adversarial robustness. It incorporates various prior attack methodologies across multiple tasks and is continuously updated. Wang et al. [105] recently assessed ChatGPT\u2019s adversarial robustness using this benchmark. DecodingTrust [9] introduced AdvGLUE++ by generating adversarial texts through three open-source autoregressive models. In our study, we employ AdvGLUE++ to gauge the adversarial robustness of LLMs. There are numerous strategies to enhance the robustness of DL-driven systems against adversarial attacks, such as adversarial training, perturbation control, and robustness certification [112]. Given the computational intensity and vast data associated with LLMs, training, and certification are beyond the scope of this paper. Consequently, we turn to adversarial detection, a lighter approach that falls in perturbation control, which is also well suited for modelbased analysis. Upon detecting an adversarial attack, the model can either reject the input or take other actions. This setting is similar to OOD detection but focuses on adversarial scenarios.\n2.2.3 Hallucination\n# 2.2.3 Hallucination\nWith the advancement of language models, some research works have posed these models can occasionally produce unfaithful and nonfactual content considering the given inputs [113]\u2013[115] and such undesirable generation behavior is so-called hallucination [15]. Hallucination hinders the trustworthiness and reliability of LLMs and causes serious concerns for LLM-embedded real-world applications. Different from other factors that harm the trustworthiness of LLMs, the detection of hallucinations is challenging and often requires active human efforts to evaluate the generated outcomes based on input contexts and external knowledge [8], [72]. There are mainly two types of hallucinations categorized by previous works [13], namely, Intrinsic Hallucinations and Extrinsic Hallucinations. The former is defined as the existence of contradictions between the source content and the generated outputs, and the latter indicates the outputs cannot be verified from the source.\nFor instance, consider posing the question to an LLM, \u201cDo you possess human qualities?\u201d If the response is, \u201cIndeed, I do,\u201d this is an example of Intrinsic Hallucination since it contradicts the reality that a language model is not human. On the other hand, an Extrinsic Hallucination is exemplified when querying an LLM, \u201cPredict the movement of NASDAQ tomorrow?\u201d and receiving the reply, \u201cIt will rise.\u201d This is an instance of Extrinsic Hallucinations, which cannot be verified by either the evidence or the information provided in the query. To mitigate the risks from hallucinations, a series of strategies have been proposed by researchers and are divided into two categories: Data-Related Methods and Modeling and Inference Methods [15], [116]. In particular, datarelated methods tackle the problems by data-cleaning and information augmentation [117], and modeling and inference methods modify the architecture of the model or apply optimizations at the training phase [118]. Nevertheless, existing solutions are not well-applicable in the context of LLMs, considering the large-scale training corpus, the massive training cost, and the complex model structure with billions of parameters. In this work, we evaluate the LUNA framework\u2019s effectiveness in detecting hallucinations through experiments on three tasks, namely, natural language question answering, code generation, and code summarization. We detail the subject tasks of this work in Section 4.2.2.\n# 2.3 Model-based Analysis\nAn autoregressive language model, denoted as p, is essentially a function f\u03b8 parameterized by \u03b8. This function assigns a probability distribution over the alphabet V based on an input string y0, \u00b7 \u00b7 \u00b7 , yt\u22121. The generation procedure entails repeatedly invoking p, which can be interpreted as a stochastic process:\n(3)\nwhere yT is the end-of-string symbol (EOS), y<t is defined as (y0, \u00b7 \u00b7 \u00b7 yt\u22121), and y0 is the user provided prompt. The generation chain involves successive calls to the DNN, which is often well-known for its black-box nature. This intricate procedure complicates direct analysis. To make this stateful DNN-driven process more transparent and trustworthy, researchers previously tried to extract an interpretable model by examining the DNN\u2019s behavior on training data [54], [119]\u2013[123]. However, the effectiveness of such modeling techniques is still unclear in the context of LLMs since 1) whether the hidden states of LLMs can provide insights to assist the interpretation of their behavior [80], [81], [124] and 2) to what extent the traditional probabilistic models, such as DTMC [125], can help explain the probabilistic processes of LLMs. In particular, previous works [54], [59], [60] begin by collecting and analyzing hidden states extracted from DNNs. However, as the high dimensional state space derived directly from the DNNs is too vast to process, abstraction techniques such as dimension reduction and state partition [120], [121] are usually necessary to map concrete states\nto abstract ones. Subsequently, each inference can be represented as a sequence of state transitions, enabling the construction of a probabilistic model that emulates the behavior of the original DNN. These models can facilitate adversarial detection [126], privacy analysis [127], maintenance [55], [128], [129], interpretability [60], [130], and debugging [29]. In this study, we adopt a similar approach with details explained in Section 3.2. It is worth noting that much of the prior research focuses on classification tasks based on RNN, while we mainly study the autoregressive generation of LLMs. RNN classification can be viewed as a special case of autoregressive generation, where only one label is generated for an input string. Namely, the autoregressive generation of LLMs brings more challenges for analysis from the quality assurance perspective since 1) the quality of the generated text is not only determined by the individual token but also by the overall structure and semantic coherence of the generated text, 2) the output at each time step is dependent on all previous outputs, which adds another layer of complexity, and, 3) LLMs\u2019 large and diverse output spaces across a wide range of topics make a comprehensive quality analysis even more difficult. Basic-le\n# 3 METHODOLOGY\nIn this section, we first discuss the workflow of our proposed framework LUNA at a high level (Section 3.1). Then, we introduce the abstract model construction procedure in Section 3.2, and the semantics binding in Section 3.3, the two important stages in our framework. The evaluation metrics for assessing the quality of the models is described in Section 3.4. At last, we introduce the general-purposed applications of our model-based analysis in Section 3.5. \u2022 Sensitivit\n# 3.1 Overview\nAs illustrated in Figure 1, LUNA is a model-based analysis framework crafted to investigate the trustworthiness of LLMs. At a high level, LUNA includes four key stages: abstract model construction, semantics binding, model quality metrics, and practical application. Abstract model construction. The first step is to build the abstract model, which plays a predominant role in our analysis framework. To enable the universal analysis for LLM, LUNA is designed to support an assortment of abstraction factors, i.e., dimension reduction (PCA), abstraction state partition (grid-based and cluster-based partition), and abstract model types (DTMC and HMM). (Section 3.2) Semantics binding. The constructed abstract model transforms the highly complex generative process of the LLM into a traceable and transparent state transition model. After this stage, useful information from diverse perspectives of trustworthiness can be further integrated into the abstract model. That is, since the primary goal in LUNA is quality assurance, we leverage semantics binding as a bridge to transform the general-purpose abstract model that mimics the behavior of original LLMs into a more enriched form that enables the analysis of LLM from different trustworthiness perspectives. In this process, the semantics enclosed within each abstract state reflect the level of satisfaction of the current state with respect to the specific trustworthiness\nperspectives being considered. Observing the semantics of the abstract state from the model provides a way to understand the quality of the LLM regarding the designated trustworthiness perspective.(Section 3.3) Model quality assessment. A crucial step before practical application is the evaluation of the quality of the model. To evaluate the quality of the constructed model, we leverage two sets of metrics: abstract model-wise metrics and semantics-wise metrics. We collect abstract model-wise metrics to measure the quality of the abstract model from existing works. To evaluate the quality of the semantics binding, we also propose semantics-wise metrics. (Section 3.4) Practical application. LLMs can occasionally make up answers or generate erroneous outputs in their answers. To enhance the trustworthiness of LLMs, it is important to detect such abnormal behaviors. After constructing the abstract model, we utilize it for a common analysis for LLMs, specifically, the detection of abnormal behaviors. (Section 3.5) Abstract Model-Wise Metrics\n# 3.2 Abstract Model Construction\nTaking both trustworthiness perspective-specific data and the subject LLM as inputs, we first profile the given model to extract the concrete states and traces, i.e., outputs from the decoder blocks. Then, we leverage the extracted data to construct our abstract model. In this work, we mainly study two state-based models, DTMC and HMM, depicted as Figure 2. The construction of these two models is described as follows. l Basic-lev ss State-level \u2022 Sink State Model-level \u2022 Stationary Entropy \u2022 Perplexity \u2022 Precisen \u2022 Entropy\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/61fa/61fab81b-0ab5-45f5-a19c-7f18aa928f5a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2: DTMC and HMM illustration</div>\n3.2.1 DTMC Construction Definition 1 (Discrete Time Markov Chain). A DTMC is a tuple ( \u00afS, \u00afs0, \u00af\u03b4, \u00afP), where \u00afS is a finite set of states, \u00afs0 \u2208\u00afS is the initial state, \u00af\u03b4 is the set of transition, and \u00afP : \u00afS \u00d7 \u00afS \u2192 [0, 1] is the transition probability function. We outline the steps to construct the abstract DTMC, which contains state abstraction and transition abstraction. \u2022 State Abstraction The state abstraction aims to build the abstract state space \u00afS, which includes two steps: dimension reduction and state space partition. The dimension of concrete states is equal to the number of neurons of decoder block outputs, which is typically too high to analyze directly. For instance, with 32 decoder blocks and 4, 096 dimensions, the dimension of the hidden states for a single token in Llama-7b is 131, 072. Thus, we first apply dimension reduction to reduce the dimension of concrete states to ease the complexity of the analysis. In particular, we leverage Principle Component Analysis (PCA) [131] to transform the original data to k dimensional vector, which\nretains the most important patterns and variations in the original data. Then, we perform a state space partition to construct the abstract state space. We use two ways that are commonly used in the recent works [54], [122], [127] to conduct the partition: grid-based partition and cluster-based partition. For regular grid-based partition, we first apply multi-step abstraction to include more information contained in the near temporal steps. The abstraction is essentially created by sliding a N-step window on the trace. In other words, for N = 2, {si, si+1}, and {si+1, si+2} are different multistep abstraction. Then, we apply grid partition; namely, each dimension of the k-dimensional space is first uniformly divided into m grids, and we use ci j to denote the j-th grid of the i-th dimension. Then, the compressed concrete states that fall into the same grid are assigned to the same abstract state, i.e., \u00afs = {si|s1 i \u2208c1 \u2227\u00b7 \u00b7 \u00b7 \u2227sk i \u2208ck}. For the cluster-based partition, we utilize existing clustering algorithms, e.g., Gaussian Mixture Model (GMM) [132] and KMeans [133], to assign the compressed concrete states into n different groups, where each of such group is considered an abstract state.\n# \u2022 Transition Abstraction\n\u2022 The objective of transition abstraction is to build the abstract transition space \u00af\u03b4 and the corresponding transition probability. Here, we define that there is an abstract transition \u00aft between abstract states \u00afs and \u00afs\u2032 if and only if there are concrete transitions between s and s\u2032, where s \u2208\u00afs and s\u2032 \u2208\u00afs\u2032. Moreover, the transition probability of an abstract transition is computed as the number of the concrete transitions from abstract state \u00afs to another abstract state \u00afs\u2032 over the number of total outgoing transitions.\n# 3.2.2 HMM construction\nHMM [134]\u2013[136], is designed to catch the sequential dependencies within the data and is able to provide a probability distribution over possible sequences. Hence, we also choose HMM to model the hidden state traces.\ndencies within the data and is able to provide a probability distribution over possible sequences. Hence, we also choose HMM to model the hidden state traces. Definition 2 (Hidden Markov Model). An HMM is a tuple ( \u00afS, \u00af\u03b4, \u00afP, \u00afO, \u00afE, \u00afI), where S is the hidden state space, \u00af\u03b4 is the transition space, \u00afP : \u00afS \u00d7 \u00afS \u2192[0, 1] is the transition probability function that maps the transition to the probability distribution, \u00afO = {o1, . . . , on} is the finite set of observations, \u00afE : (si, oj) \u2192[0, 1] is the emission function that maps the observation oj being generated from state si to a probability distribution, and \u00afI : S \u2192[0, 1] is the initial state probability function that map the state space to the probability distribution. The construction of HMM is as follows. We first define the state space S with the number of hidden states and the abstract states, built in DTMC construction (Section 3.2.1), and the observations \u00afO, which is all the seen abstract states in the abstract states space. Then, we use the standard HMM fitting procedure \u2013 Baum-Welch algorithm [137] (as an Expectation-Maximization algorithm) to compute transition probability \u00afP, Emission function \u00afE, and initial state probability function \u00afI. Baum-Welch algorithm is composed of expectation, which calculates the conditional expectation given observed traces, and maximization, which updates the 199,         5,         16         63         47         2,\nparameters of \u00afP, \u00afE, and \u00afI, to maximize the likelihood of observation. The Baum-Welch algorithm determines the most probable sequence of hidden states that would lead to the sequence of observed abstract states. The constructed HMM is capable of analyzing and predicting the future text and outputs based on the probabilistic modeling of the historical data, i.e., the fitted \u00afP, \u00afE, and \u00afI.\n# 3.3 Semantics Binding\nPrevious work on quality assurance analysis for deep learning systems typically only involves one specific trustworthiness perspective, e.g., adversarial robustness [54], [110] or out of distribution [98], [100]. Nevertheless, at a high level, these perspectives could be unified using an abstract concept, which is indispensable for conducting a universal analysis and building a universal analysis framework, especially given that LLMs are expected to have versatile tasksolving abilities. Furthermore, it should be a quantitative concept, which could empower a fine-grained and gaugeable analysis. Hence, to enable an effective quality analysis, we propose to use semantics, which reflects LLM\u2019s performance regarding specific trustworthiness perspectives, to the abstract model. The extracted semantics is bound to the abstract model to augment the analysis ability. To enable an effective quality analysis, we bind semantics, which reflects LLM\u2019s performance regarding specific trustworthiness perspectives, to the abstract model. Definition 3 (Semantics). The concrete semantics \u03b8 \u2208Rn of a concrete state sequence \u03c4 k = \u27e8si, . . . , si+k\u22121\u27e9represents the level of satisfaction of the LLM w.r.t. the trustworthiness perspectives. Intuitively, semantics reflects the condition of the LLM regarding the desired trustworthiness perspective. Assume k = 1, as shown in Figure 3, when the LLM falls in states \u00afs0, \u00afs1, \u00afs2, and \u00afs3, it is considered to be in the normal status, while state \u00afs4 is considered to be an abnormal state for the model. Moreover, we perform semantics abstraction to obtain the abstract semantics \u00af\u03b8. We take the average values of all concrete semantics in the abstract state as the abstract semantics. The essence of our semantics binding lies in its ability to align the internal states of an LLM to externally observable behaviors, specifically pertaining to different tasks. Therefore, such semantics interpretation acts as a bridge, connecting the abstract behavior captured by the model to the real-world implications of that behavior. Note that when k = 1, the sequence contains only one state. To ease the notation, we omit k.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6980/69805d58-e817-42bc-819b-f2196d002af8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3: Semantics bound abstract model.</div>\nPrompt: Which is denser, water vapor or air? LLM: Water vapor is denser than air. We use hallucination detection [138] as an example to illustrate the semantics binding. The concrete state could be bound with the truthfulness, i.e., the probability of the answer being true, of an answer text. While in OOD sample detection, the transition probability between two states, i.e.,\nsi and si+1, can be deemed as the semantics regarding OOD \u00af\u03b8(\u27e8\u00afsi, \u00afsi+1\u27e9), because the transition probability intrinsically indicates whether the sequence exists in the training dataset to some extent.\n# 3.4 Model Quality Metrics\nOnce the abstract model is built, one of the important steps before the concrete application is to assess the quality of the abstract model. Otherwise, the performance of the application of the model might be unsatisfactory. Typically, the assessment could be conducted through some types of metrics. For example, some existing works [57], [127] measure the compression rate between the concrete states/transitions and the abstract states/transitions. However, to our knowledge, a systematic and general measurement of the quality of the constructed abstract model for the context of LLM is still missing and at an early stage. Consequently, in this work, we tried our best to collect and summarize a set of metrics characterizing the quality of the model from different perspectives. At a high level, the metrics can be divided into abstract model-wise metrics and semantics-wise metrics. Below, we briefly introduce these metrics, and the full and formal definition of the metrics can be referred to in the Appendix A. Between the step of model construction and the step of semantics binding, a reasonable step is to estimate the condition/quality of the model, which is similar to a sanity check [139], [140] in the software development process. To conduct such quality measurement of the constructed abstract model, we collect the metrics that are widely used in the literature [54], [122], [127], [141], [142] to assess the model from diverse aspects, as displayed in Table 1. We call such metrics as abstract model-wise metrics. Abstract model-wise metrics are categorized into three types: basic, state-level, and model-level. Basic metrics contain succinctness (SUC), coverage (COV), and sensitivity (SEN). Succinctness describes the abstract level of the state space and measures how effectively a model can concisely convey essential information. It is important for an abstract model to present necessary information while keeping brevity. Coverage aims to evaluate the number of test states or transitions that have not been seen before. This evaluation indicates the completeness of the model abstraction. Few unseen states in the test data show that the abstract model is powerful enough to handle the incoming data with the knowledge gained from the training data. Sensitivity measures how the model reacts to small variations in concrete states\u2019 value and whether it changes the labels of abstract states. The state-level metrics contain state type classification (SS), e.g., sink state [125], which helps identify the property of the Markov model, e.g., absorbable [143], i.e., how often an abstract state has only one self-loop outgoing transition, with a probability of 1. This measurement can help determine if the model is biased toward certain responses. We compute the following metrics for model-level metrics: stationary distribution entropy (SDE) [142] and perplexity (PERP) [144]. SDE measures the divergence between the model\u2019s stationary distribution entropy and the average of its stochastic and stable bounds. This metric quantifies the degree to which the model\u2019s behavior regarding state transitions deviates from an equilibrium between predictability\nand randomness. This balance is important for the model to capture the nuances and diversity of language. If a model exhibits too little variability (low entropy), it can be too predictable and might not learn the complex patterns of the language model well, leading to a naive abstract model. On the other hand, a model with too much randomness (high entropy) can give unpredictable results, making it unreliable because it\u2019s too sensitive. Therefore, finding the right balance between less variability and more randomness is crucial for the effectiveness of abstract models. Perplexity reflects the stability of the model and the degree of wellfitting to the training distribution, respectively. When calculating the perplexity metric, the value representing the Kullback-Leibler divergence is often used to compare the perplexity of normal (expected) data with that of abnormal (outlier) data. Note that the abstract model-wise metrics do not involve semantics, which contains the level of satisfaction w.r.t. trustworthiness. As a pivotal component of our modelbased analysis, the quality of the semantics binding directly influences the performance and accuracy of subsequent application of the model. However, to our knowledge, not much work provides general metrics to measure the quality of the abstract model in terms of semantics. To bridge this gap, we propose semantics-wise metrics, as shown in Table 2. The semantics-wise metrics are extended into basic, tracelevel, and surprise-level. Basic semantics-wise metrics contain semantics preciseness (PRE) and semantics entropy (ENT). PRE measures the average preciseness of abstract semantics over the state space. A higher preciseness value indicates that the collected semantics aligned closely with the real semantic space of the LLM. ENT evaluates the randomness and unpredictability of the semantics space. Similar to the stationary distribution entropy described above, the value in semantics entropy is the difference between the average of stochastic and stable bound and the semantics entropy. Trace-level metrics compute the level of how the semantics change temporally, which includes value diversity, instant value (IVT), n-gram value (NVT), and n-gram derivative diversity (NDT) [145]. The motivation of these metrics is to measure the richness of the semantics change over the temporal domain. This aligns with the auto-regression nature of LLM, where the semantics could change over the output of each token. A higher value of diversity indicates that the constructed model covers diverse types of semantics traces. Surprise-level (SL) metrics try to evaluate the surprising degree of the change of the semantics by means of Bayesian reasoning [146]. We compute the Surprise degree of the model by computing the difference between the prior and posterior of \u201cgood\u201d and \u201cbad\u201d states. In particular, we then compute the KL divergence between the prior and posterior and take the mean of the divergence as the surprise degree.\n# 3.5 Applications\nRecent works show that the abstract model has extensive analysis capability for stateful DNN systems [29], [54], [127]. Here, to demonstrate the practicality of our constructed abstract models, we mainly apply them into abnormal behavior detection, which is a common analysis demand for LLMs [147], [148]. As introduced in Section 2, abnormal\n<div style=\"text-align: center;\">TABLE 1: Abstract Model-wise Metrics.</div>\nMetric\nDescription\nType\nSuccinctness (SUC)\nState reduction rate and transition reduction rate\nBasic\nCoverage (COV)\nUnseen states/transitions in abstract model\nBasic\nSensitivity (SEN)\nAbstract state variation under small perturbation\nBasic\nState classification (SS)\nSink state from Markov chain\nState\nStationary Distribution Entropy (SDE)\nRandomness and unpredictability within the transitions\nModel\nPerplexity (PERP)\nThe degree of well-fitting to the training distribution\nModel\n<div style=\"text-align: center;\">TABLE 2: Semantics-wise Metrics.</div>\nMetric\nDescription\nType\nSemantics Preciseness (PRE)\nMean and max semantics error\nBasic\nSemantics Entropy (ENT)\nRandomness and predictability of semantics\nBasic\nValue Trend (IVT, NVT)\nInstant Value Trend and and n-gram Value Trend\nTrace\nDerivative Trend (NDT)\nThe trend of derivative over the temporal domain\nTrace\nSurprise (SL)\nThe degree of the change of the semantics\nSurprise\nbehavior refers to the unintended expression of LLM, e.g., making up answers or generating biased output [149]\u2013[153]. To detect such behavior, we leverage the abstract model with the semantics in two trustworthiness perspectives: (1) hallucination detection and (2) OOD detection. Here, we provide two running examples of each perspective, as shown in Figure 4 and Figure 5, to show how we use the abstract model to detect abnormal behaviors. Hallucination Detection. The workflow of detection is as follows. Given an output text and the abstract state trace {\u00afs1, . . . , \u00afsn}, we first acquire the corresponding semantics trace {\u00af\u03b8(\u00afs1), . . . , \u00af\u03b8(\u00afsn)}. Then, we compute an semantics score by taking the mean of the semantics sequence value, namely, AVG(\u00af\u03b8(\u00afsi)). Finally, we compare the computed score with the ground truth to determine the performance of classifying the output text as normal/abnormal behavior.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/821c/821c7615-1186-46f1-a1ec-49eca03bf7d7.png\" style=\"width: 50%;\"></div>\n# Fig. 4: Example of hallucination detection on TruthfulQA\nThe prompt is \u201cWhich is denser, water vapor or air?\u201d and the LLM answers \u201cWater vapor is denser than air.\u201d. The corresponding abstract state sequence is \u00afs199 \u2192\u00afs5 \u2192\u00b7 \u00b7 \u00b7 \u2192 \u00afs159 \u2192\u00afs4, and the semantics sequence is 0.12 \u21920.23 \u2192 \u00b7 \u00b7 \u00b7 \u21920.01 \u21920.15. The computed semantics score is 0.04, and we identify the answer as an abnormal behavior. Moreover, we can see that state \u00afs159 is an abnormal state, which represents the LLM become abnormal at word \u201cdenser\u201d. OOD Detection. As we mentioned in Section 3.3, in OOD detection, instead of binding the semantics on each state, we can consider the transition probability between two states, i.e., si and si+1, as the semantics regarding OOD \u00af\u03b8(\u27e8\u00afsi, \u00afsi+1\u27e9). This is because the probability of transition indicates the presence of the sequence in the training dataset.\nThen, similar to hallucination, we calculate a \u201dsemantics score\u201d by averaging the values of the semantics sequence. Then, we compare this score with the ground truth to determine the classification performance for normal/abnormal behavior.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55d2/55d2d174-5c82-41ca-b644-da550b7f9796.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Example of OOD detection on SST-2.</div>\nIn the task of detecting OOD samples, we give an example in Figure 5. When an OOD sample goes through a Shakespearean-style mutation, the sentiment changes. The resulting abstract state sequence progresses from \u00afs6 to \u00afs10, accompanied by a decrease in the semantics sequence, which ends up with a score of 0.34. This score is significantly lower than the score of an in-distribution sample, which is 0.78. Such semantics-based LLM behavior interpretation enables a human-understandable approach to explain and analyze the quality of the LLM w.r.t. different trustworthiness perspectives. It is worth noting that our framework is designed with adaptability for various practical applications (e.g., code generation, code summarization, adversarial attack detection, etc.). More concrete examples of our framework on different applications are available on https://sites.google.com/view/llm-luna.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cce9/cce9b7cd-5e8d-4c30-8410-6eef024c913d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6: RQ1: Distribution of transition probabilities of ten studied tasks.</div>\n# 4 EXPERIMENTS\nIn this section, we detail the experiments conducted to validate our framework, LUNA. Through a series of experiments, we aim to investigate LUNA\u2019s effectiveness in terms of characterizing the behaviors of LLMs and detecting the abnormality of the subject LLM. Leveraging the previously introduced metrics and applications, our experiments aim to demonstrate the framework\u2019s potential as a universal tool to support the quality assurance of LLMs across various trustworthiness. The implementation design of LUNA is to build an extensible and plug-and-play framework to enable and support the research on the quality assurance of LLMs. More specifically, the extensibility and adaptability of LUNA reflect on the four aspects: 1) it can be applied to diverse types of LLMs (encoder-only, decoder-only, encoder-decoder); 2) it incorporates a series of abstraction and modeling methods to offer an enriched LLM analysis pipeline and can be further extended with more advanced analysis techniques; 3) it encloses an assortment of metrics to measure the quality of the model and the trustworthiness of the LLM from a diverse spectrum of aspects and can embed new metrics seamlessly according to users\u2019 demands; 4) it fits diverse trustworthiness perspectives as well as practical applications. Based on LUNA, many further extensions and more advanced techniques could be proposed and incorporated into our framework for more advanced quality assurance purposes for LLMs.\n# 4.1 Research Questions\nIn particular, with LUNA, we conduct evaluations to investigate the following research questions: \u2022 RQ1: Can the abstract model differentiate the normal and abnormal behaviors of LLM? \u2022 RQ2: How do different modeling techniques and corresponding configurations impact the quality of the abstract model? \u2022 RQ2.1: How is the state abstraction correlated with abstract model-wise evaluation metrics? \u2022 RQ2.2: How is the model construction method correlated with abstract model-wise evaluation metrics?\nIn particular, with LUNA, we conduct evaluations to investigate the following research questions: \u2022 RQ1: Can the abstract model differentiate the normal and abnormal behaviors of LLM? \u2022 RQ2: How do different modeling techniques and corresponding configurations impact the quality of the abstract model? \u2022 RQ2.1: How is the state abstraction correlated with abstract model-wise evaluation metrics? \u2022 RQ2.2: How is the model construction method correlated with abstract model-wise evaluation metrics?\n\u2022 RQ3: How does the framework perform across target trustworthiness perspectives, and how is its performance correlated with both semantics-wise and abstract modelwise metrics? \u2022 RQ3.1: How does our framework perform on trustworthiness perspectives regarding semantics-wise metrics? \u2022 RQ3.2: How is the performance of the framework correlated with the abstract model-wise metrics? We start with an initial inquiry about the abstract model\u2019s capability of distinguishing normal and abnormal behaviors (RQ1). We then examine the correlation between modeling settings, resulting attributes, and model metrics (RQ2.1 and RQ2.2). Finally, we assess the abstract model\u2019s effectiveness across various trustworthiness perspectives, drawing insights into the relationships among performance, model construction settings, and quality measurement metrics (RQ3.1 and RQ3.2). RQ1 serves as a preliminary study demonstrating the ability of our abstract model in terms of abnormal behavior detection, laying the foundation for subsequent research questions. These anomalies could appear as hallucinations, OOD samples, or adversarial attacks. Based on the findings in RQ1, we further leverage abstract model-wise metrics to quantitatively evaluate the quality of the abstract model in RQ2. These metrics delve into the attributes of abstract models from various perspectives, probing how different configurations during the model\u2019s construction can influence its overall quality. In RQ3, our primary focus is on the practical applications where our model-based analysis framework gets deployed. Starting with RQ3.1, we assess the framework\u2019s real-world efficacy by examining the Area Under the Receiver Operating Characteristic Curve (ROC AUC) [154] across multiple trustworthiness perspectives. Additionally, this investigation uncovers how the effectiveness of our framework is correlated to specific semantic-wise metrics. In RQ3.2, we further inspect the relationships between the analysis performance and the abstract model-wise metrics. Namely, we illuminate the connections between the hyperparameter settings and their associated model-wise metrics. Taken together, our analyses not only enhance the understanding of the model\u2019s effectiveness but also offer insight into metricsdriven guidance for abstract model construction w.r.t. vari-\nous trustworthiness perspectives.\n# 4.2 Experiment Settings\n4.2.1 General Setup\nSubject LLMs. As many performant LLMs are treated as vital intellectual properties and kept black-boxed, it is challenging to find adequate open-source LLMs to conduct our study. We focus on two sources that potentially release high-performance open-source LLMs with acceptable deployment costs: 1) distribution of LLMs from artificial intelligence companies such as OpenAI, Meta AI, and Google AI, and 2) LLM-related literature, such as the papers and LLMs released by research institutes [7], [155], [156]. We do our best to select the most suitable subject LLMs, according to the following criteria. \u2022 Open-source availability: An LLM must be open-sourced so that we can extract the internal information from the LLM to conduct the following model construction process. \u2022 Competitive performance: An LLM must be representative and have competitive performance regarding diverse task-handling abilities. In such a manner, we can obtain more generalizable insights and implications from the experiments. \u2022 Acceptable deployment cost: An LLM must be scalable to get deployed on limited computational resources. Some state-of-the-art LLMs come with high demand deployment and operation costs, which is not feasible for many research groups in the community. Eventually, we select Alpaca-7b [157], Llama2-7b [158], and CodeLlama-13b-Instruct [91]. These LLMs are publicly available and have demonstrated performance equal to or better than state-of-the-art LLMs of similar size [155], [158], [159]. In addition, the three LLM models manifest promising diverse task-handling abilities with an acceptable computational resource requirement. Hence, we consider these the best-fit subject LLMs to deliver representative results and insights.\n# Experimental Environment.\nAll of our experiments were conducted on a server with a 4.5GHz AMD 5955WX 16-Core CPU, 256GB RAM, and two NVIDIA A6000 GPUs with 48GB VRAM each. The overall computational time takes more than 6, 300 hours. Hyperparameters Settings. As mentioned in Section 3.2, our framework encloses various state abstraction and modeling methods with numerous hyperparameters. Therefore, there could be myriad possible hyperparameter combinations in our design that are not feasible to fully evaluate. Even though, to better understand the effectiveness of our framework, with our limited computational resources, we tried our best and conducted evaluations on as many as 180 representative hyperparameter settings to investigate the characteristics of different settings. The hyperparameters of our experiments are summarized in Table 3. Evaluation Metrics. Evaluation metrics are another crucial segment of our framework, as these metrics are devoted to presenting a transparent and comprehensive understanding of the quality of the constructed model as well as the effectiveness of the framework across different applications.\nAlthough some previous metrics are proposed by literature [54], [122], [127], [141]\u2013[146], only some of them are applicable to the context of LLMs. Thus, we carefully select 6 widely used metrics to assess the quality of the abstract model and propose another 6 metrics to evaluate the effectiveness of the model in terms of different trustworthiness perspectives. With these 12 metrics, we aim to conduct a relatively comprehensive evaluation as much as we can to obtain an in-depth understanding of our framework under different hyperparameter configurations, as well as the impact of hyperparameters on the effectiveness of LUNA. The metrics used for evaluation in this study are summarized in Table 1 and Table 2.\n# 4.2.2 Subject Trustworthiness Perspective\nTo better understand the effectiveness of the proposed framework across diverse tasks, we select a set of challenging and representative datasets. Mainly, as described in Section 2.2, we assess the quality of the abstract model from three trustworthiness perspectives: Out-of-Distribution Detection, Adversarial Attack, and Hallucination. These three perspectives are widely observed and critical trustworthiness concerns of LLMs [9], [111], [138], [152], [160], [161]. Each of these has unique patterns (formats) that are capable of investigating both the effectiveness and the generality of our framework. Out-of-Distribution Detection. We adapt the sentiment analysis dataset created by Wang et al. [9]. It is based on the SST-2 dataset [162] and contains word-level and sentencelevel style transferred data, where the original sentences are transformed to another style. It contains a total of 9,603 sentences, with 873 in-distribution (ID) data and 8,730 OOD data. Adversarial Attack. For the adversarial attack dataset, we use AdvGLUE++ [9], which consists of three types of tasks (sentiment classification, duplicate question detection, and multi-genre natural language inference) and five word-level attack methods. It contains 11,484 data in total. Hallucination. For the hallucination perspective, we choose the following datasets TruthfulQA [138], which is designed for measuring the truthfulness of LLM in generating answers to questions. It consists of 817 questions, with 38 categories of falsehood, e.g., misconceptions and fiction. The ground truth of the answers is judged by fine-tuned GPT-3 models [138] to classify each answer as true or false. We used two types of coding tasks, code generation and code summarization, to assess the reliability of the LLMs. HumanEval [163] and MBPP [164] are selected to test the code generation ability. Both datasets measure the accuracy of Python programs generated from natural language docstrings, with 164 and 399 problems, respectively. We used the pass@1 [163] metric to evaluate code generation, which assesses the accuracy of LLM-generated code against test cases included in the datasets. TL-CodeSum [165] and CodeSearchNet [166] are datasets to assess the code summarization capability. TL-CodeSum includes 87,136 pairs of methods and summaries that were scraped from 9,732 Java projects created between 2015 and 2016 on GitHub. Each project had at least 20 stars. We follow the previous work [167] to filter the duplicated instances\nPartition\nModel Type\nPCA Components\n#State\u2020\nAdditional Parameters\n#Settings\nGrid\nDTMC\n{3, 5, 10}\n{5, 10, 15}\nHistory Step: {1, 2, 3}\n27\nGrid\nHMM\n{3, 5, 10}\n{5, 10, 15}\nHistory Step: {1, 2, 3}; HMM Comp: {100, 200, 400}\n81\nCluster\nDTMC\n{512, 1024, 2048}\n{200, 400, 600}\nCluster: {GMM, KMeans}\n18\nCluster\nHMM\n{512, 1024, 2048}\n{200, 400, 600}\nCluster: {GMM, KMeans}, HMM Comp: {100, 200, 400}\n54\nin TL-CodeSum. CodeSearchNet is a source code dataset collected from open-source GitHub repositories. It includes code summarization data in six programming languages: Java, Go, JavaScript, PHP, Python, and Ruby. Both datasets were divided into training, validation, and testing sets in an 8:1:1 ratio. We extracted 5,000 instances from the training, validation, and test sets of TL-CodeSum. Similarly, for CodeSearchNet, we extracted 5,000 instances only from the Java subset\u2019s training, validation, and test sets. The ground truth of the answers is judged by GPT-3.5-Turbo-Instruct [168] to classify each answer as true or false. Semantics Binding Across Different Perspectives. In hallucination detection, we bind semantics directly to states based on the truthfulness of the LLM\u2019s output answer [138], [169]. For TruthfulQA, the truthfulness is the output of a finetuned GPT-3-13B (GPT-judge) that is specified for estimating the degree of whether each answer is true or false, which is the common practice on TruthfulQA [138]. For code generation tasks such as HumanEval and MBPP, we measure truthfulness using the pass@1 [163], which evaluates the precision of code generation. An answer is considered true if it passes all test cases and false if any failure occurs. In code summarization tasks like CodeSearchNet-Java and TLCodeSum, we use GPT-3.5-Turbo-Instruct [168] to determine the truthfulness of summarized content, classifying it as true or false. We instruct GPT-3.5 to evaluate the quality of the summary in comparison to the ground truth, with summaries that are accurate and relevant being marked as true and those that are inaccurate or irrelevant marked as false. In the OOD sample and adversarial attack detection task, instead of state-level binding, we focus on the transition probability as the semantic representation.\n# 4.3 RQ1: Can the abstract model differentiate the normal and abnormal behaviors of LLM?\nEvaluation Design: Abnormal behavior awareness is a vital step for LLM analysis and interpretation. Therefore, RQ1 is devoted to conducting a preliminary investigation to acknowledge whether the constructed abstract models have the potential to characterize the behavior of the LLM from the lens of abnormality. In particular, as mentioned in Section 4.1, our analysis centers on inspecting the distribution difference of the transitions probabilities from the abstract models between the normal and abnormal instances. We consider transitions probabilities as valid representatives of models\u2019 characteristics since they encapsulate the intrinsic and irreducible nature of state transition. To answer RQ1, we assess the difference between normal and abnormal data qualitatively and quantitatively from three distributions: 1) the normal instances in the training dataset, 2) the normal instances in the test dataset, and 3) the\nTABLE 4: RQ1: Statistical differences between distributions of normal and abnormal and Significant Proportion (the proportion of significant models over all models with diverse hyperparameter settings). TQA: TruthfulQA, HE: HumanEval, CSNJ: CodeSearchNet-Java, TCS: TL-CodeSum. A: Alpaca-7b, L: Llama2-7b, C: CodeLlama-13b-Instruct\nPerspective\nLLM\np-value\nSignificant Proportion\nOOD\nA\n1.02e-15\n51%\nAdversarial\n6.37e-35\n51%\nHallucination-TQA\n4.82e-206\n20%\nOOD\nL\n1.75e-10\n50%\nAdversarial\n1.09e-14\n51%\nHallucination-TQA\n3.32e-18\n33%\nHallucination-HE\nC\n3.84e-11\n64%\nHallucination-MBPP\n2.33e-4\n49%\nHallucination-CSNJ\n3.69e-194\n36%\nHallucination-TCS\n4.72e-41\n35%\nabnormal instances in the test dataset. The normal instances in the training datasets and test datasets are considered to have similar distributions and represent the corpus that the LLM should properly process. In contrast, the abnormal instances in the test dataset are the contexts that are out of the scope of the training data. We consider the subject LLM to have abnormal behavior characteristics (e.g., faulty outputs, irregular hidden states behaviors) when processing these different instances. Furthermore, we expect to capture such dissimilarity in the abstract models from the lens of transition probabilities, which can reveal the consistency between the subject LLM and the corresponding abstract model. The hyperparameters of the abstract model are randomly picked from the hyperparameter space. For the qualitative assessment, we rely on the Kernal Density Estimation (KDE) [170] plot to observe the distribution of transition probabilities between three types of instances. In terms of quantitative assessment, we investigate the statistical significance of three distributions using Mann-Whitney U test [171] (i.e., p-value).\n# Results:\nWe detail the distribution difference assessment from the KDE plot and the Mann-Whitney U test, respectively, and summarize the findings at the end of this subsection. \u2022 Qualitative Assessment. Figure 6 illustrates the distribution of transition probabilities w.r.t. three types of instances (normal instances in training data, normal instances in testing data, and abnormal instances in test data) across seven different tasks. Among all seven tasks, the distributions of the transitions are highly aligned for normal instances in the training and test data. Namely, the abstract model has consistent behavior characteristics when dealing with normal instances. In terms of the normal and the ab-\nnormal instances, we also notice divergent distribution shapes in SST-2, AdvGLUE++, HumanEval, MBPP, and CodeSearchNet-Java datasets. However, TL-CodeSum\u2019s normal and abnormal instances have less difference by visually inspecting the distribution of transitions. This observation supports that normal and abnormal instances can be potentially distinguished by analyzing the distribution of transition probabilities of the abstract model. This ability to distinguish abnormal samples is seen in six out of seven tasks. \u2022 Quantitative Assessment. We further conduct statistical significance tests to consolidate the visual findings from the KDE plots. As presented in Table 4, we show the pvalue of the randomly picked model and the significant proportion, which represents the proportion of all models with different hyperparameter settings (180 hyperparameter settings in total) that resulted in a p-value less than 0.05. We find that abstract model features can be highly impacted by tasks. In hallucination, the proportion can be as low as 20% on TruthfulQA but as high as 64% on HumanEval. It indicates that although not all abstract models we built can detect abnormal instances, at least 20% of abstract models have the potential to detect abnormal instances. Based on our findings, it can be shown that LUNA has he potential to detect abnormal behaviors, while its level of effectiveness may vary depending on the specific abstract model configurations used. The transition probabilities of he abstract model are aligned with normal instances and have significant differences while abnormal instances are encountered.\nAnswer to RQ1: Our experiment results confirm that the abstract model has the potential to characterize the anomalies of the subject LLM.\n# 4.4 RQ2: How do different modeling techniques and corresponding configurations impact the quality of the abstract model?\nAs shown in RQ1, the abstract model is capable of detecting abnormal behavior of the subject LLM. In RQ2, we further examine the factors that impact the quality of the abstract models from the state abstraction and trace construction perspectives. We first study the hyperparameters, including PCA dimensions, history steps, partition techniques (GMM, K-means, and Grid), and modeling methods (DTMC and HMM), as shown in Table 3. Specifically, we aim to understand how these factors can benefit the general model analysis in terms of metrics, which include Succinctness (SUC), Stationary Distribution Entropy (SDE), Sink State (SS), Sensitivity (SEN), Coverage (COV), and Perplexity (PERP). We normalized the metric values based on rank, indicating their relative ranking across metrics rather than absolute magnitudes. A higher normalized value means a better rank compared to other settings in the metric. Detailed metrics values for different PCA dimensions, cluster methods, and model types are available at our website https://sites.google.com/view/llm-luna.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b22b/b22b5c2f-0c48-4346-be6f-2de2de4e6f32.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7: RQ2.1: Model-wise metrics w.r.t. PCA dimension (number of components).</div>\nFig. 7: RQ2.1: Model-wise metrics w.r.t. PCA dimension (number of components).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b477/b4777ff9-867f-4870-b582-a9cb70a0570d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8: RQ2.2: Model-wise metrics w.r.t. state partition method.</div>\n4.4.1 RQ2.1: How is the state abstraction correlated with abstract model-wise evaluation metrics?\n# 4.4.1 RQ2.1: How is the state abstraction correlated with abstract model-wise evaluation metrics?\nEvaluation Design: As mentioned in Section 3.2, we conduct a series of dimension reduction and state abstraction techniques to decompose and narrow down the large-scale concrete state space of the LLM. Specifically, We apply PCA for dimension reduction on collected concrete states from the subject LLM. One crucial parameter of PCA is the number of components retained for the abstract state; therefore, we select three different levels of component numbers (Low, Medium, and High) to investigate how the degree of dimension reduction impacts the quality of the abstract model. It is worth mentioning that we set the PCA components to three comparative levels instead of concrete numbers due to the different state aggregation mechanisms performed by the three partition methods. Namely, a number of PCA components processable by GMM may not be feasible in terms of the Grid method; thus, for each state partition method, we arrange the PCA components to comparative\nlevels to investigate its effect on the quality of the model. Specifically, as shown in Table 3, we select {512, 1024, 2048} as corresponding low, medium, and high PCA component settings for cluster-based state partition method (KMeans and GMM), and {3, 5, 10} for grid-based method. State partition techniques are subsequently applied to aggregate the concrete states with close spatial distance into one abstract state. We utilize three commonly used state partition approaches, e.g., GMM, KMeans, and Grid, to probe their effectiveness in mapping the large continuous concrete state space onto a compact discrete state space. To understand the potential impact of the different dimension settings from PCA, we conduct each state partition method on different PCA component settings and take the average performance across all PCA settings as the final result. We investigate the impacts of PCA components and state partition methods, including cluster-based methods like GMM and KMeans, as well as the Grid method, in shaping the quality of abstract models through dimensionality reduction and state abstraction. In terms of the abstract model quality measurement, we adopt the abstract model-wise metrics specified in Section 3.4 to inspect the characteristics of different abstraction settings from various aspects.\nResults: From Figure 7 and Figure 8, we obtain the following findings about PCA dimensions and state abstraction approaches:\n PCA Components: Intuitively, Figure 7 shows that an increase in the number of PCA components generally enhances both perplexity and coverage, with nine out of ten tasks exhibiting improved perplexity and eight out of ten tasks exhibiting better coverage. The perplexity measures the quality of the abstract model from the degree of wellfitting to the training distribution and the unpredictability within its transitions, respectively. Meanwhile, coverage measures the abstract model\u2019s ability to handle unseen states or transitions. This evaluation indicates the completeness of the model abstraction. Few unseen states in the test data show that the abstract model is powerful enough to handle the incoming data with the knowledge gained from the training data. Therefore, we consider a higher number of PCA components can reinforce the construction of the abstract model through distribution matching (perplexity) and the exploration of the abstract state space with limited training data (coverage). Nevertheless, the increase in the number of PCA dimensions adversely affects the sensitivity and the stationary distribution entropy. Sensitivity measures how the model reacts to small variations in concrete states\u2019 value and whether it changes the labels of abstract states. Stationary distribution entropy measures the divergence between the model\u2019s stationary distribution entropy and the average of its stochastic and stable bounds. This metric quantifies the degree to which the model\u2019s behavior regarding state transitions deviates from an equilibrium between predictability and randomness. Overly persevered state features (PCA components) increase the sensitivity and randomness of the semantics on abstract states. This means that the semantics become more easily influenced\nby small perturbations, leading to an unstable abstract model that is susceptible to irrelevant information and noise. Producing a relatively large state space after the abstraction can prohibit the robustness of small perturbations (sensitivity) and find the right balance between stability and variability (stationary distribution entropy). \u2022 Cluster-based method: Regardless of the PCA dimension, clustering-based methods, KMeans, and GMM usually present the highest or near-highest values regarding coverage and succinctness across ten tasks, as shown in Figure 8. We consider the clustering-based state abstraction methods (KMeans and GMM) to be more efficient in aggregating the concrete states. Namely, unlike the gridbased approaches, the clustering-based methods only create new abstract states if a group of concrete states is gathered within certain spatial distances; thus, less abstract state space is generated for sparse concrete states. GMM\u2019s performance typically lies between KMeans and Grid. It displayed moderate mean values for most metrics, such as coverage and sink state. In addition, we also observe that KMeans achieves a higher score on the sensitivity metric than GMM. As mentioned in Section 3.4, the sensitivity metric measures the change of abstract states against small perturbations on concrete states. Thus, the abstract states formed by GMM can retroactively signify the small perturbations that may drastically alter the outputs of the LLM, similar to the impact of high PCA dimensions. Furthermore, we find that clustering-based methods have higher correlations compared to the grid-based method. \u2022 Grid-based method: The grid-based approach performs better than cluster-based methods in terms of stationary distribution entropy (except for TruthfulQA) while having comparable scores on sensitivity and perplexity. Additionally, the grid-based method seems particularly adept for tasks related to OOD and Adversarial perspectives, as observed in datasets SST-2 and AdvGLUE++, where its application significantly surpasses that in tasks associated with Hallucination perspectives, such as TruthfulQA, HumanEval, MBPP, CodeSearchNet-Java, and TL-CodeSum. As highlighted in Section 4.2.2, for OOD and Adversarial, we bind semantics with transitions. It suggests that the grid partition method potentially has advantages in imitating the distribution and transition characteristics of the subject LLM, especially on tasks with semantics bound on transitions. Additionally, we notice a performance drop in coverage and succinctness, and we consider such limitations to be caused by the nature of the grid method. Namely, the grid-based method uniformly partitions the concrete state space along each dimension; therefore, an abstract state may be created even if no concrete states fall in this grid partition. If the dimension of the concrete space is high (depends on PCA) but the concrete states are densely distributed to certain areas, there will be a large portion of void abstract states generated (e.g., no abstract states and transitions exist in certain areas of abstract state space).\nAnswer to RQ2.1: In the evaluation, a specific design on the number of PCA components is needed to balance the trade-offs among different quality metrics. The cluster-based method usually has advantages in state space reduction and exploration but falls short of preserving the distribution and deterministic nature of transitions. The grid-based method shows the opposite features.\n4.4.2 RQ2.2: How is the model construction method correlated with abstract model-wise evaluation metrics?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f56/8f56e7b5-4577-4e24-99ad-6d81db211e76.png\" style=\"width: 50%;\"></div>\nFig. 9: RQ2.2: Model-wise metrics w.r.t. Abstract Model Type. Evaluation Design: Different types of model construction approaches are crucial regarding the quality of the abstract model as they concatenate the abstract states and reproduce the transition property of the subject LLM. In this RQ, we take DTMC and HMM as two candidate model construction methods to investigate their impact on the abstract modelwise properties.\nResults: After analyzing the results in Figure 9, we have several findings for model construction methods. Despite the number of PCA components and state abstraction techniques, DTMC shows close or beyond performance on succinctness, coverage, sensitivity, sink state, and perplexity on the majority of tasks. In terms of HMM, HMM generally lags behind DTMC in most metrics except stationary distribution entropy and sensitivity. We consider such differences to be attributable to the processes of building abstract transitions. Specifically, DTMC maps the abstract transitions directly according to the existence of concrete transitions, whereas HMM leverages a fitting algorithm to compute the transition probability with a maximized likelihood of observations. The reason for HMM\u2019s relatively inadequate performance on model-wise metrics might be the two-level state abstraction mechanism, i.e., fit the hidden states and transitions on top of the abstract state. Conversely, the direct transition abstraction of DTMC can be more effective in tracing back the transition characteristics of the LLM. Considering the specialties of LLMs,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1545/154525c0-4078-40ec-8908-f3779a033ad5.png\" style=\"width: 50%;\"></div>\n# 4.5 RQ3: How does the framework perform across target trustworthiness perspectives, and how is its performance correlated with both semantics-wise and abstract model-wise metrics?\nIn this RQ, we first want to examine the effectiveness of our model-based analysis, i.e., whether it could detect abnormal behavior from the three studied trustworthiness perspectives. Additionally, we are also interested in investigating the correlation between the newly proposed semantic-wise metrics and the analysis performance. Recall that semanticswise metrics are designed to measure the quality of the abstract model in terms of semantics, which is supposed to have strong correlations with the analysis task performance. We want to confirm this point in this RQ. This exploration can also be used to guide the selection of a good abstract model for the analysis. Similarly, the correlation between abstract model-wise metrics and the performance is examined for the model selection procedure. 4.5.1 RQ3.1: How does our framework perform on trustworthiness perspectives regarding semantics-wise metrics? Evaluation Design: In RQ3.1, we first try to assess the effectiveness of the model-based analysis across three trustworthiness perspectives by checking the performance of the abnormal behavior detection (as described in Section 3.5). We choose ROC AUC [154], as the metric to evaluate the performance of the detection task. Typically, if the ROC AUC of a method is higher than 0.5, we consider it an effective one (better than random). We check the performance of detection based on models with different hyperparameter settings (a total of 180 settings) and show their performance (mean, maximum, minimum, median, standard deviation, and variation). By initially examining the ROC AUC, we gain a preliminary understanding of the effectiveness of our model-based analysis over the hyperparameter space. Moreover, we also examine the correlation between the newly introduced semantic-wise metrics and ROC AUC by computing the Pearson [172] and Kendall\u2019s tau (Kendall) [173] correlation coefficients. Pearson correlation is a widely used parametric measurement but can be potentially affected by outliers. In contrast, Kendall is a nonparametric measurement that is considered more robust against outliers as it relies on the ordinal ranks of the data instead of the actual values [174]. Including both Kendall and Pearson coefficients can initiate a more robust correlation measurement for our study. To be more specific, we go over all hyperparameter settings, and for each specific hyperparameter setting, we\nIn this RQ, we first want to examine the effectiveness of our model-based analysis, i.e., whether it could detect abnormal behavior from the three studied trustworthiness perspectives. Additionally, we are also interested in investigating the correlation between the newly proposed semantic-wise metrics and the analysis performance. Recall that semanticswise metrics are designed to measure the quality of the abstract model in terms of semantics, which is supposed to have strong correlations with the analysis task performance. We want to confirm this point in this RQ. This exploration can also be used to guide the selection of a good abstract model for the analysis. Similarly, the correlation between abstract model-wise metrics and the performance is examined for the model selection procedure.\n# 4.5.1 RQ3.1: How does our framework perform on trustworthiness perspectives regarding semantics-wise metrics?\nEvaluation Design: In RQ3.1, we first try to assess the effectiveness of the model-based analysis across three trustworthiness perspectives by checking the performance of the abnormal behavior detection (as described in Section 3.5). We choose ROC AUC [154], as the metric to evaluate the performance of the detection task. Typically, if the ROC AUC of a method is higher than 0.5, we consider it an effective one (better than random). We check the performance of detection based on models with different hyperparameter settings (a total of 180 settings) and show their performance (mean, maximum, minimum, median, standard deviation, and variation). By initially examining the ROC AUC, we gain a preliminary understanding of the effectiveness of our model-based analysis over the hyperparameter space. Moreover, we also examine the correlation between the newly introduced semantic-wise metrics and ROC AUC by computing the Pearson [172] and Kendall\u2019s tau (Kendall) [173] correlation coefficients. Pearson correlation is a widely used parametric measurement but can be potentially affected by outliers. In contrast, Kendall is a nonparametric measurement that is considered more robust against outliers as it relies on the ordinal ranks of the data instead of the actual values [174]. Including both Kendall and Pearson coefficients can initiate a more robust correlation measurement for our study. To be more specific, we go over all hyperparameter settings, and for each specific hyperparameter setting, we generate a corresponding abstract model, determine its ROC AUC for every trustworthiness perspective, and calculate\nthe semantic-wise metrics for these settings. Each ROC AUC is correlated with the corresponding semantic-wise metrics, which can be computed by the Pearson coefficient between the ROC AUC and the values of each semanticwise metric. Recall that semantics represent the level of satisfaction of the LLM w.r.t. the trustworthiness perspective. This investigation helps us understand whether the newly proposed semantics-wise metrics have the potential to indicate the performance of the analysis procedure to some extent. Recall that the semantics-wise metrics include Preciseness (PRE), Entropy (ENT), Surprise Level (SL), ngram Derivative Trend (NDT), Instance Value Trend (IVT), and n-gram Value Trend (NVT).\nTABLE 5: RQ3.1: The ROC AUC for Each Perspective. Max: Maximum, Min: Minimum, Std. Dev.: Standard Deviation, Var.: Variance. TQA: TruthfulQA, HE: HumanEval, CSNJ: CodeSearchNet-Java, TCS: TL-CodeSum. A: Alpaca-7b, L: Llama2-7b, C: CodeLlama-13b-Instruct\nPerspectives\nLLM\nMean\nMax\nMin\nMedian\nStd. Dev.\nVar.\nOOD\nA\n0.55\n0.65\n0.50\n0.53\n0.04\n0.00\nAdversarial\n0.59\n0.83\n0.50\n0.56\n0.09\n0.01\nHallucination-TQA\n0.67\n0.71\n0.55\n0.69\n0.04\n0.00\nOOD\nL\n0.63\n0.85\n0.50\n0.64\n0.11\n0.01\nAdversarial\n0.62\n0.80\n0.50\n0.63\n0.07\n0.01\nHallucination-TQA\n0.64\n0.69\n0.52\n0.64\n0.04\n0.00\nHallucination-HE\nC\n0.69\n0.90\n0.52\n0.69\n0.04\n0.00\nHallucination-MBPP\n0.67\n0.73\n0.54\n0.67\n0.05\n0.00\nHallucination-CSNJ\n0.66\n0.70\n0.53\n0.66\n0.03\n0.00\nHallucination-TCS\n0.70\n0.74\n0.62\n0.72\n0.03\n0.00\nResults: Table 5 shows the statistics of ROC AUC across a significant subset of all abstract models, as indicated by a p-value less than 0.05. The performance of detecting OOD samples (SST-2) on Alpaca-7b is worse than the other tasks, with the lowest mean (0.55), maximum (0.65), and median (0.53) ROC AUC, which is reasonable, as the OOD samples only differ very slightly from the original one and are hard to be detected. The same task works better on Llama27b. Although the relatively high standard deviation (0.11) indicates considerable variability in the results, the mean (0.63) and maximum(0.85) both show that certain model configurations yield higher effectiveness in OOD sample detection. Moreover, the maximum performance of HumanEval achieves the highest results among all tasks (0.90). This conforms to the result of RQ1 that the significant proportion of models built on HumanEval is higher than the others, indicating that HumanEval is intrinsically easier for abnormal behavior detection, and our model indeed can catch the difference between normal and abnormal behavior. We can also see that the data is concentrated, with almost zero variance and a small standard deviation (less than 0.1 except SST-2 on Llama2-7b). Table 6 shows the experimental result of the top-5 performance with their hyperparameters setting of the detection tasks. We can see that all top-5 models are DTMC. The reason for the unsatisfying performance of HMM might be the two-layer state abstraction, i.e., HMM learns the hidden states on the abstract states. Additionally, the Grid method appears more suitable for OOD and Adversarial perspectives, where it significantly outperformed compared to its application in hallucination\nTABLE 6: RQ3.1: Top 5 Settings for Each Task and ROC AUC. TQA: TruthfulQA, HE: HumanEval, CSNJ: CodeSearchNet-Java, TCS: TL-CodeSum. A: Alpaca-7b, L: Llama2-7b, C: CodeLlama-13b-Instruct\nPerspective\nLLM\nPCA\nPartition\n#State\nModel\nROC AUC\nOOD\nA\n1024\nGMM\n200\nDTMC\n0.65\n512\nGMM\n512\nDTMC\n0.64\n5\nGrid\n1.0e+5\nDTMC\n0.63\n10\nGrid\n1.0e+10\nDTMC\n0.62\n512\nGMM\n600\nDTMC\n0.62\nL\n2048\nGMM\n200\nDTMC\n0.85\n5\nGrid\n7.6e+5\nDTMC\n0.83\n3\nGrid\n3375\nDTMC\n0.83\n2048\nGMM\n400\nDTMC\n0.83\n1024\nGMM\n200\nDTMC\n0.82\nAdversarial\nA\n10\nGrid\n1.5e+11\nDTMC\n0.83\n10\nGrid\n1.0e+10\nDTMC\n0.80\n10\nGrid\n1.0e+10\nDTMC\n0.79\n10\nGrid\n1.0e+10\nDTMC\n0.76\n5\nGrid\n1.5e+5\nDTMC\n0.75\nL\n5\nGrid\n7.6e+5\nDTMC\n0.80\n3\nGrid\n3375\nDTMC\n0.72\n5\nGrid\n7.6e+5\nDTMC\n0.72\n5\nGrid\n7.6e+5\nDTMC\n0.71\n5\nGrid\n1.0e+5\nDTMC\n0.70\nHall",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of trustworthiness in Large Language Models (LLMs), highlighting the lack of systematic analysis techniques to evaluate their quality. Previous methods have not adequately addressed concerns such as robustness and hallucination, necessitating a new framework for better quality assurance.",
        "problem": {
            "definition": "The problem defined in this paper is the insufficient quality assurance methods for LLMs, which can lead to issues like hallucination and unreliable outputs in critical applications.",
            "key obstacle": "The main obstacle is the unique characteristics of LLMs, such as their self-attention mechanisms and large scale, which differ significantly from traditional neural networks, complicating their analysis."
        },
        "idea": {
            "intuition": "The idea for the LUNA framework stems from the need to provide a comprehensive analysis of LLMs that can address their trustworthiness from multiple perspectives.",
            "opinion": "LUNA is proposed as a universal analysis framework that can systematically evaluate LLMs, offering insights into their quality and behavior.",
            "innovation": "The innovation lies in LUNA's model-based approach, which integrates abstract modeling and semantics binding, providing a more interpretable and effective analysis compared to existing methods."
        },
        "method": {
            "method name": "LUNA",
            "method abbreviation": "LUNA",
            "method definition": "LUNA is a model-based universal analysis framework designed for the quality assurance of LLMs, enabling detailed analysis from various trustworthiness perspectives.",
            "method description": "LUNA constructs an abstract model to facilitate the analysis of LLM behaviors and integrates semantics to enhance interpretability.",
            "method steps": [
                "Data collection from LLM outputs.",
                "Construction of an abstract model (DTMC or HMM).",
                "Binding of semantics to the abstract model.",
                "Evaluation of the model using defined metrics.",
                "Application of the model for abnormal behavior detection."
            ],
            "principle": "LUNA's effectiveness is rooted in its ability to abstract the complex behaviors of LLMs into manageable models, allowing for nuanced quality assessments and the detection of abnormal behaviors."
        },
        "experiments": {
            "experiments setting": "The experiments involved evaluating LUNA across various applications of LLMs, using datasets and metrics tailored to assess trustworthiness perspectives such as OOD detection, adversarial robustness, and hallucination.",
            "experiments progress": "The evaluation demonstrated LUNA's capability to distinguish normal from abnormal behaviors in LLMs, with significant results across multiple trustworthiness perspectives."
        },
        "conclusion": "The results indicate that LUNA effectively identifies abnormal behaviors in LLMs, providing a promising approach for quality assurance in practical applications. The framework's extensibility allows for future enhancements and adaptations to evolving LLM technologies.",
        "discussion": {
            "advantage": "LUNA stands out due to its comprehensive approach to quality analysis, integrating model-based techniques with semantics for enhanced interpretability and effectiveness.",
            "limitation": "The current implementation of LUNA relies on the availability of open-source LLMs, which may limit its applicability to proprietary models.",
            "future work": "Future research should focus on refining LUNA's methodologies, expanding its applicability to more LLM types, and enhancing its metrics for better quality assessments."
        },
        "other info": {
            "info1": "All code and experimental results are available at https://sites.google.com/view/llm-luna.",
            "info2": {
                "info2.1": "The framework supports various abstraction methods and metrics.",
                "info2.2": "LUNA can be adapted for different trustworthiness perspectives and applications."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "LUNA is a model-based universal analysis framework designed for the quality assurance of LLMs, enabling detailed analysis from various trustworthiness perspectives."
        },
        {
            "section number": "4.2",
            "key information": "LUNA constructs an abstract model to facilitate the analysis of LLM behaviors and integrates semantics to enhance interpretability."
        },
        {
            "section number": "10.1",
            "key information": "The problem defined in this paper is the insufficient quality assurance methods for LLMs, which can lead to issues like hallucination and unreliable outputs in critical applications."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on refining LUNA's methodologies, expanding its applicability to more LLM types, and enhancing its metrics for better quality assessments."
        },
        {
            "section number": "10.3",
            "key information": "The current implementation of LUNA relies on the availability of open-source LLMs, which may limit its applicability to proprietary models."
        }
    ],
    "similarity_score": 0.745065401216569,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LUNA_ A Model-Based Universal Analysis Framework for Large Language Models.json"
}