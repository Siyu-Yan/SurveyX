{
    "from": "google",
    "scholar_id": "uWxVynhWgJYJ",
    "detail_id": null,
    "title": "Recmind: Large language model powered agent for recommendation",
    "abstract": " Abstract\n\nWhile the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and finetune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLMpowered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring  algorithm to improve the planning ability. At each intermediate step, the LLM \u201cself-inspires\u201d to consider all previously explored states to plan for the next step. This mechanism greatly improves the model\u2019s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind\u2019s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.\n\n\n# 1 Introduction\n\nThe Recommender System (RS) plays a key role in search engines, e-commerce, and various other Internet platforms. An RS analyzes the historical interactions between users and items to recommend potential items (Koren et al., 2009b;  Linden et al., 2003). The RS has been enhanced by Deep Neural Networks (DNNs) to more effectively learn the representations of users, items, and sequential behaviors (Hidasi et al., 2015; He et al., 2020; Sun et al., 2019). However, most existing DNN-based methods (e.g., CNN and LSTM) and pre-trained language models (e.g., BERT) cannot\n\n\u2217 Work was done as an intern at Amazon Alexa AI. \u2020 Indicates equal contribution.\n\nsufficiently capture textual knowledge about users and items due to limitations in model scale and data size. Besides, most existing RS methods have been designed for specifi",
    "bib_name": "wang2023recmind",
    "md_text": "# Model Powered Agent For Recommen\n\nYancheng Wang 1 \u2217, Ziyan Jiang 2 \u2020, Zheng Chen 2 \u2020, Fan Yang 2 \u2020, Yingxue Zhou 2 \u2020, Eunah Cho 2, Xing Fan 2, Xiaojiang Huang 2, Yanbin Lu 2, Yingzhen Yang 1\n1 School of Computing and Augmented Intelligence, Arizona State University 2 Amazon Alexa AI {yancheng.wang, yingzhen.yang}@asu.edu {ziyjiang, zgchen, ffanyang, zyingxue, eunahch, fanxing, xjhuang, luyanbin}@amazon.com\n\nYancheng Wang 1 \u2217, Ziyan Jiang 2 \u2020, Zheng Chen 2 \u2020, Fan Yang 2 \u2020, Yingxue Zhou 2 \u2020, Eunah Cho 2, Xing Fan 2, Xiaojiang Huang 2, Yanbin Lu 2, Yingzhen Yang 1\n1 School of Computing and Augmented Intelligence, Arizona State University 2 Amazon Alexa AI\n\n{yancheng.wang, yingzhen.yang}@asu.edu {ziyjiang, zgchen, ffanyang, zyingxue, eunahch, fanxing, xjhuang, luyanbin}@amaz\n\n# Abstract\n\nWhile the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and finetune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLMpowered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring  algorithm to improve the planning ability. At each intermediate step, the LLM \u201cself-inspires\u201d to consider all previously explored states to plan for the next step. This mechanism greatly improves the model\u2019s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind\u2019s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.\n\n\n# 1 Introduction\n\nThe Recommender System (RS) plays a key role in search engines, e-commerce, and various other Internet platforms. An RS analyzes the historical interactions between users and items to recommend potential items (Koren et al., 2009b;  Linden et al., 2003). The RS has been enhanced by Deep Neural Networks (DNNs) to more effectively learn the representations of users, items, and sequential behaviors (Hidasi et al., 2015; He et al., 2020; Sun et al., 2019). However, most existing DNN-based methods (e.g., CNN and LSTM) and pre-trained language models (e.g., BERT) cannot\n\n\u2217 Work was done as an intern at Amazon Alexa AI. \u2020 Indicates equal contribution.\n\nsufficiently capture textual knowledge about users and items due to limitations in model scale and data size. Besides, most existing RS methods have been designed for specific tasks and are inadequate in generalizing to unseen recommendation tasks (Fan et al., 2023). Recent advances in Large Language Models (LLMs), such as GPT-3 (Brown et al., 2020), GPT4 (OpenAI, 2023), LLaMA (Touvron et al., 2023a), LLaMa-2 (Touvron et al., 2023b), and PaLM-2 (Anil et al., 2023) have demonstrated remarkable results in a wide range of tasks, which have motivated the research of leveraging LLMs for recommendation to mitigate the aforementioned challenges (Liu et al., 2023; Fan et al., 2023; Lin et al., 2023). However, existing studies primarily rely on knowledge stored within the model\u2019s weights, neglecting the potential benefits of leveraging external tools to access real-time information and external knowledge (Yang et al., 2023; Bao et al., 2023). Furthermore, the reasoning ability of LLMs is not fully utilized for recommendation, resulting in suboptimal predictions due to the intricate nature of recommendation-related tasks (Liu et al., 2023). To better utilize the strong reasoning and toolusing abilities of LLMs, we design a recommendation agent RecMind that leverages an LLMpowered API as its intellectual core and incorporates a few key components. The first key component is Planning which enables the agent to break complex recommendation tasks into manageable steps for efficient handling of complex situations. Each step of planning involves thought, action and observation (see Figure 1  for examples and Section 3 for details). The agent is also equipped with Memory consisting of Personalized Memory and World Knowledge, each accessible through specific tools. The Tools  enhance the agent\u2019s functionality on top of the LLM, such as retrieving relevant knowledge, or assisting with the reasoning process. To further enhance the planning ability of the\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/625e/625e3da7-f27a-49bc-a1cf-ef0167c001d7.png\" style=\"width: 50%;\"></div>\nFigure 1: Comparisons of rating prediction by RecMind-ToT (left) and RecMind-SI (right). After searching for the product category of the item in Step 2, RecMind-ToT first generates thought 3 (1) to retrieve the rating of a similar item. After being evaluated by the voting-based evaluator, RecMind-ToT prunes option 3 (1) and proposes another thought 3 (2) to retrieve the average rating of the item and then makes the prediction solely based on it. In contrast, although RecMind-SI proposed the same alternative options in step 3, it takes into account the thought, action, and observation from both options 3 (1) and 3 (2) to generate the thought for the next step.\n\nagent, we propose a new planning algorithm  SelfInspiring (SI). At each intermediate planning step, the agent \u201cself-inspires\u201d to consider all previously explored paths for the next planning. Unlike existing Chain-of-Thoughts (CoT) (Wei et al., 2022) and Tree-of-Thoughts (ToT) (Yao et al., 2023) which discards states (thoughts) in previously explored paths when generating a new state, SI retains all previous states from all history paths when generating new state. SI is inspired by the intuition that all historical states can provide useful information for better planning. Figure 1 provides an example of ToT and SI showing that SI achieves a more accurate rating than ToT due to better planning. To the best of our knowledge, this is the first public research work on an LLM-powered autonomous agent for recommendation. The main contributions of our work are:\n\u2022 We introduce RecMind, the first LLM-powered agent designed for general recommendation purposes, which operates without the need for finetuning for domain adaptation across datasets or tasks.\n\u2022 We incorporate a novel self-inspiring  (SI) planning technique in RecMind. This technique integrates multiple reasoning paths and offers an empirical improvement over currently popular methods, such as CoT and ToT.\n\u2022  We evaluate the effectiveness and generalizability of RecMind across five recommendation tasks and two datasets. Extensive experiments and analyses demonstrate that RecMind outperforms state-of-the-art (SOTA) LLM-based\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4f06/4f0696e9-3cf7-47d2-9c62-9cb2937ec9ad.png\" style=\"width: 50%;\"></div>\nbaselines that do not involve any fine-tuning and achieves competitive performance with a fully pre-trained expert recommendation model such as P5 (Geng et al., 2022). In addition, SI outperforms CoT and ToT on general reasoning tasks, showing that the proposed the impact of SI is beyond recommendation tasks.\n\n# 2 Related Work\n\nLLM-as-Agent There is an emerging trend where LLMs are augmented to become autonomous language agents. The central concept is to leverage LLMs to produce text-based outputs and actions that can be used for making API calls and performing operations within a specific environment. LLMs, with their strong reasoning abilities, can decompose challenging and complex tasks into smaller, more manageable steps (Wei et al., 2022;\nYao et al., 2023; Patil et al., 2023). A number of successful applications have emerged, including ReAct (Yao et al., 2022), Toolformer (Schick et al., 2023), HuggingGPT (Shen et al., 2023), generative agents (Park et al., 2023), WebGPT (Nakano et al., 2021), AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023). LLM for Recommendation Recently, LLMs have gained popularity in recommender systems, given their ability to understand a user\u2019s preferences or past interactions in natural language (Fan et al., 2023; Lin et al., 2023). Current LLM-based recommender systems are primarily designed for rating prediction (Kang et al., 2023; Bao et al., 2023) and sequential recommendation tasks (Wang and Lim, 2023; Yang et al., 2023; Hou et al., 2023). User\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/254e/254e2aec-14d3-4408-950e-2de00c89d0a2.png\" style=\"width: 50%;\"></div>\nFrom the item candidates listed below, choose the top 10 items to recommend to user_X and rank them in order of priority from highest to lowest. Candidates: [\u201cRogaine Women Hair Regrowth Treatment\u201d, \u2026\u2026]\n\nHow will user_X rate the item\"Kusco-Murphy Tart Hair\"? The rating should be an integer between 1 to 5, with 1 being lowest and 5 being highest.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5514/5514e1b9-00ae-4471-8fbe-c9f57c6bfca7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Here is an overview of our proposed RecMind architecture. It comprises four maj\"RecMind\" is built based on ChatGPT API, \"Tools\" support various API call to retrieve knowledge component, \"Planning\" component is in charge of thoughts generation.\n</div>\nsed RecMind architecture. It comprises four major components: ools\" support various API call to retrieve knowledge from \"Memory\"\n\ninteractions and optional data, including profiles, are input into an LLM prompt with options: no fine-tuning (Wang and Lim, 2023), full-model finetuning (Yang et al., 2023), or parameter-efficient fine-tuning (Bao et al., 2023). In sequential recommendation tasks, we use a pre-filtered set of item candidates in the prompts for focused ranking. (Liu et al., 2023) use prompts to assess ChatGPT\u2019s performance across five recommendation tasks showing LLM\u2019s strong in-context learning abilities and generalization (Wei et al., 2021). Unlike existing studies, our work harnesses the LLM\u2019s capabilities in reasoning, tool usage, and action.\n\n# 3 Architecture\n\nAs shown in Figure 2, RecMind consists of key components: LLM-powered API such as ChatGPT to drive the overall reasoning, planning which breaks down a task to smaller sub-tasks for step-bystep planning, memory which provides the agent with the capability to retain and recall information over extended periods, and tools  for obtaining relevant extra information from memory that is missing from the model weights and aiding the reasoning. We introduce the key components planning,  memory and tools for RecMind in the subsequent parts.\nPlanning Planning helps LLM Agents decompose tasks into smaller, manageable subgoals for handling complex tasks. Consider the setting where the goal is to generate the final result y  given problem x via an LLM Agent parameterized by \u03b8. The\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7de/b7de2f4d-c40b-4a15-8319-f9c3e4d343ff.png\" style=\"width: 50%;\"></div>\nHelp user_X to generate a 5-star explanation for item \"FoliGrowth Hair Growth Supplement\u201d.\n\n<div style=\"text-align: center;\">This product is essential for growing and maintaining healthy hair! This is a product to be bought in bulk because you can never have enough of it.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d107/d1074fb9-c655-47e3-9e02-7897c4d4c995.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2172/2172297d-e28c-4833-b450-a9b82f69e87f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Tree-of-Thoughts (DFS)\n</div>\nFigure 3: Comparison between Tree-of-Thoughts DFS and Self-Inspiring. Red arrows in the figure indicate the process for generating alternative thoughts at intermediate steps. Blue dashed arrows in the figure denote the backtracking process.\n\ntraditional input-output method gives the result by y \u223c p \u03b8 (y | x). With planning, RecMind generates the result y \u223c p \u03b8 (y | planing (x)), where planing (x) is a set of prompts that decomposes problem x into a series sub-tasks that is composed of t h ought h, a ction a, and o bservation o. Figure 1 provides examples of planning including thoughts, actions, and observations. We first review existing popular reasoning methods such as CoT and ToT which we have explored for RecMind. Then we present the proposed SI algorithm. All these planning methods can be viewed as traversing through a latent reasoning tree, as shown in Figure 3.\n\n# \u2022 Chain-of-Thoughts (CoT) (Wei et al., 2022)\n\nbeen used in ReAct (Yao et al., 2022) to synergize reasoning and action. This CoT planning method follows a single path in the reasoning tree. In our setting, at each time step t, the agent receives\n\nobservation o t followed by thought h t and action a t. Let s t = (h t, a t, o t) denote the RecMind state at step t. The CoT planning method generates the next state s t +1 = (h t +1, a t +1, o t +1) by sampling p \u03b8 (s t +1 | x, s 1, .., s t). Thus CoT only follows a single planning path S = {s 1, ..., s t, ..., s T} until reaching the final result y \u223c p \u03b8 (y | x, s 1, ..., s t, ..., s T) after T steps.\nTree-of-Thoughts (ToT) (Yao et al., 2023) extends CoT to explore multiple paths in the reasoning tree. At step t and state s t, ToTBFS explicitly generates multiple candidates {s 1 t +1, ..., s k t +1} for next state by i.i.d. sampling s i t +1 \u223c p \u03b8 (s t +1 | x, s 1, .., s t) for i \u2208 [k]. Then it applies majority vote to select the state s t +1 from {s 1 t +1, ..., s k t +1}. Eventually ToT-BFS generates a single path similar to CoT. In contrast, ToT-DFS explores one branch at a time, but might prune the current state, and backtracks to the previous state to start a new reasoning branch. Denote the first explored path as z (1) = {s (1) 1, ..., s (1) t, s (1) t +1}. If the last state s (1) t +1 is\npruned and it backtracks to the previous state s (1) t, and starts a new reasoning branch, then the path becomes z (2) = {s (1) 1, ..., s (1) t, s (2) t +1, ...}. After exploring n branches, we denote the final path of ToT as z (n) = {s 1, ..., s (1) j 1, ..., s (2) j 2, ..., s (n) T} and the final result y is obtained by y \u223c p \u03b8 (x, z (n)).\nWe find the discarded historical states from previously explored branches such as s (1) t +1 from branch z (1) usually contain helpful information for RecMind to generate a better state compared with only considering the final path of ToT. Thus, we propose Self-Inspiring (SI) as shown in Figure 3 (b) and Algorithm 1, a new planning method for RecMind. SI inspires  itself into exploring an alternaive reasoning branch, while retaining all previous states. At m-th path and step t, SI generates the next step of planning by considering all previous paths, i.e., s (m) t +1 \u223c p \u03b8 (s t +1 | z (1), ..., z (m)). After exploring n paths, the RecMind obtains the final esult y \u223c P \u03b8 (x, z (1), ..., z (n)). Figure 3 provides an example to illustrate the key difference between ToT and SI. In ToT (Figure 3 (a)), The new state N (2)  at the second path is generated by only considering state N \u2212 1. The state N (1) is discarded. However, in SI (Figure 3 (b)), the new state N (2) s generated based on both N \u2212 1 and N (1). The mechanism of SI makes it possible for the agent to analyze different perspectives of the obser\n\nWe find the discarded historical states from previously explored branches such as s (1) t +1 from branch z (1) usually contain helpful information for RecMind to generate a better state compared with only considering the final path of ToT. Thus, we propose Self-Inspiring (SI) as shown in Figure 3 (b) and Algorithm 1, a new planning method for RecMind. SI inspires  itself into exploring an alternative reasoning branch, while retaining all previous states. At m-th path and step t, SI generates the next step of planning by considering all previous paths, i.e., s (m) t +1 \u223c p \u03b8 (s t +1 | z (1), ..., z (m)). After exploring n paths, the RecMind obtains the final result y \u223c P \u03b8 (x, z (1), ..., z (n)). Figure 3 provides an example to illustrate the key difference between ToT and SI. In ToT (Figure 3 (a)), The new state N (2)  at the second path is generated by only considering state N \u2212 1. The state N (1) is discarded. However, in SI (Figure 3 (b)), the new state N (2) is generated based on both N \u2212 1 and N (1). The mechanism of SI makes it possible for the agent to analyze different perspectives of the obser\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a96/5a96ab80-d11f-4b13-9109-397c70ef6f99.png\" style=\"width: 50%;\"></div>\nvation of a previous step. For example, an agent for recommending a movie may summarize both the favorite movie director and the favorite movie genre of a user after retrieving the user\u2019s watching history. Next, it can make recommendations from a candidate list by considering both factors. In contrast, previous reasoning methods, such as CoT and ToT, generate the final output based on one single path. Even though ToT samples multiple options at intermediate steps, it only adopts the most confident option and proceeds to the next step. That might be enough for a simple reasoning task. However, recommendation tasks based on textual content require inclusive consideration of different perspectives of available content from both personalized memory and world knowledge.\nMemory Information stored in memory, including Personalized Memory and World Knowledge, enables the model to access knowledge beyond what is inherently present in the LLM\u2019s parameters. Using the Amazon Reviews dataset as an illustrative example, Personalized Memory includes individualized user information, such as their reviews or ratings for a particular item. World Knowledge consists of two components: the first component is item metadata information, which also falls under the domain-specific knowledge category; the second component involves real-time information that can be accessed through Web search tool. In Figure\n1, information of the product \u201cSewak Al-Falah\u201d retrieved from world knowledge using a Web search tool, aids the reasoning path and ultimately influences the final prediction.\n\nTool Use By empowering LLMs to utilize tools, we can access vastly larger and dynamic knowledge bases, allowing us to tackle complex computational tasks. In RecMind system, we\u2019ve incorporated three such tools:\n\nlanguage questions into SQL queries. Using this tool, the system can access domain-specific knowledge from memory that\u2019s essential for the final prediction. For instance, in the Amazon Reviews dataset, it encompasses personal information such as a user\u2019s reviews or ratings for an item, as well as item metadata like the item\u2019s description, brand, and price. When the database tool is called, the agent will prompt a question, such as \u201cWhat is the average rating of product Sewak Al-Falah?\", based on the database schema. Next, an LLM is called to transfer the question into an executable SQL query. The output of the SQL execution will then be passed to the agent.\n\u2022 Search Tool: This tool employs a search engine (e.g., Google) to access real-time information. For instance, in the Amazon Reviews dataset, this tool could assist us in obtaining the most recent information about each item. When the Search tool is called, the agent will prompt a question asking for external meta information, which is usually not available in the database, such as \u201cWhat is the product category of Sewak Al-Falah?\". Next, a search engine API will be called to search for the information and return it to the agent.\n\n# \u2022 Search Tool: This t\n\n# \u2022 Text Summarization Tool: This to\n\nmarize lengthy texts by invoking a text summarization model from the Hugging Face Hub. For example, within the Amazon Reviews dataset, this tool can produce a summarized description of an item by considering multiple reviews of that specific item from various users. It can generate summarization such as \u201cMost customers think this product is durable and has a good price.\", which can be easily used in different recommendation tasks related to the product.\n\nDetails on the prompts for using the tools and executing self-inspiring are deferred to Section A of the supplementary.\n\n# 4 Experiments\n\nWe evaluate the performance of the RecMind agent in various recommendation scenarios, i.e., rating\n\nprediction, sequential recommendation, direct recommendation, explanation generation, review summarization. First, we provide an overview of the datasets and evaluation metrics in Section 4.1 and Section 4.2. Subsequently, we present the experimental settings and results of RecMind on each recommendation task in Section 4.3 and Section 4.4. Next, we study the domain transfer capability of RecMind in Section 4.5  and how RecMind performs with different foundation LLMs 4.6. In the end, we further explore how the performance of SI in general reasoning tasks in 4.7  and the running time of RecMind based on SI compared to ToT. The comparison on running time deferred to Section B.3  of the supplementary shows that RecMind based on SI takes less inference time than the existing state-of-the-art diverse reasoning method ToT.\n\n# 4.1 Experimental Settings\n\nFollowing P5 (Geng et al., 2022), we conduct experiments on the Amazon Reviews (Ni et al., 2019) dataset. Since Amazon Reviews contains textual reviews and titles, it provides us the chance to explore how textual contents are utilized in performing recommendation tasks with LLM models. We evaluate RecMind and baselines on data in Sports &  Outdoors, Beauty, as well as Toys & Games domains from Amazon Reviews. For a more comprehensive evaluation, we also evaluate the RecMind on Yelp (Geng et al., 2022) dataset. We show the results on the Beauty domain of Amazon Reviews and Telp in Section 4.3 and Section 4.4. The results on the Sports and Toys domains of Amazon Reviews are deferred to Section B.4  of the supplementary. To quantitatively evaluate the proposed RecMind across various recommendation tasks, we employ different metrics. For rating prediction, we report Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). In the case of sequential and direct recommendations, we use metrics such as topk Hit Ratio (HR@ k) and topk  Normalized Discounted Cumulative Gain (NDCG@ k), specifically reporting results on HR@5,10 and NDCG@5,10. In addition, for the assessment of explanation generation, review summarization and conversational recommendation, we use n-gram Bilingual Evaluation Understudy (BLEUn) and n-gram Recall-Oriented Understudy for Gisting Evaluation (ROUGEn). We use gpt-3.5-turbo-16k (Schulman et al.,\n\n2022) as the core large language model in RecMind. To enable the access of RecMind to indomain knowledge, we store all the review data in a MySQL database, consisting of a table with the product meta information and a table with the interaction history of all the users.\n\n# 4.2 Compared Methods\n\nWe compare the performance of RecMind with the following baselines, including both LLM finetuning methods, such as P5 (Geng et al., 2022), and ChatGPT prompting methods (Liu et al., 2023). In addition, we implement RecMind with three different planning methods, namely Chain-Of-Thoughts (CoT), Tree-of-Thoughts (ToT) (Yao et al., 2023), and the proposed Self-Inspiring(SI). In summary, the compared methods include:\n\nP5 (Geng et al., 2022) unifies different recommendation tasks into a shared generative large language model. A collection of personalized prompts has been created for various recommendation-related tasks. All raw data including user-item interactions, user descriptions, item metadata, and users\u2019 reviews are transformed into natural language sequences. Subsequently, the large language model is fine-tuned based on these sequences. In our evaluation, to avoid the influence of factors such as randomness in selecting recommendation candidates, we run the pre-trained P5 model loaded from the Hugging Face repo https://huggingface.co/ makitanikaze/P5 on the same test data we use to evaluate our method.\n\n<div style=\"text-align: center;\">Table 1: Performance comparison in rating prediction on Amazon Reviews (Beauty) and Yelp.\n</div>\nMethods\nBeauty\nYelp\nRMSE\nMAE\nRMSE\nMAE\nMF\n1.1973\n0.9461\n1.2645\n1.0426\nMLP\n1.3078\n0.9597\n1.2951\n1.0340\nAFM\n1.1097\n0.8815\n1.2530\n1.0019\nP5 (pre-trained expert,few-shot)\n1.2982\n0.8474\n1.4685\n1.0054\nChatGPT (zero-shot)\n1.4173\n1.1897\n1.6725\n1.2359\nChatGPT (few-shot)\n1.1589\n0.7327\n1.4725\n1.0016\nRecMind-CoT (zero-shot)\n1.2250\n0.8612\n1.5302\n1.1673\nRecMind-CoT (few-shot)\n1.1326\n0.7167\n1.3925\n0.9794\nRecMind-ToT (BFS, zero-shot)\n1.1972\n0.8135\n1.4956\n1.0755\nRecMind-ToT (BFS, few-shot)\n1.1197\n0.7059\n1.3875\n0.9766\nRecMind-ToT (DFS, zero-shot)\n1.2006\n0.8279\n1.4937\n1.1076\nRecMind-ToT (DFS, few-shot)\n1.1205\n0.7103\n1.3826\n0.9774\nRecMind-SI (zero-shot)\n1.1894\n0.7883\n1.4530\n1.0009\nRecMind-SI (few-shot)\n1.0756\n0.6892\n1.3674\n0.9698\nshot and few-shot settings. In the zero-shot setting, the LLM is directly prompted for the final prediction, while in the few-shot setting, several in-context examples are provided. We name the ChatGPT baseline in these two settings as  ChatGPT (zero-shot) and ChatGPT (few-shot).\nRecMind-CoT, where the planning is based on ReAct-CoT (Yao et al., 2022). ReAct is a novel prompt-based paradigm for general task solving. It extends Chain-Of-Thoughts (CoT) (Wei et al., 2022) to synergize reasoning and acting with external tools. In our experiments, we adopt the same tools we used for the ReAct baseline. We also explore both zero-shot and few-shot for this method and name them as RecMind-CoT (zeroshot) and RecMind-CoT (few-shot).\nRecMind-ToT, where the planning is based on Tree-of-Thoughts (ToT) (Yao et al., 2023). ToT enables the exploration of coherent units of thought that serve as intermediate steps toward problem-solving. We implement RecMindToT with two strategies in searching among the choices in intermediate steps, which are breadthfirst search, named as  RecMind-CoT (BFS, fewshot) and depth-first search, named as  RecMindCoT (DFS, few-shot).\nn addition to the above methods, we have considered different additional baselines for each task. The additional baselines are introduced in corresponding subsections. Details on the prompts for\n\nIn addition to the above methods, we have considered different additional baselines for each task. The additional baselines are introduced in corresponding subsections. Details on the prompts for baseline methods are deferred to A  of the supplementary.\n\n# 4.3 Results on Precision-oriented Recommendation Tasks\n\nWe first evaluate RecMind and baselines on three precision-oriented recommendation tasks, i.e., rating prediction, sequential recommendation, and direct recommendation. Rating Prediction.  Rating prediction is an essential task in recommendation systems that aims to predict the rating that a user would give to a particular item. In rating prediction, we further traditional recommendation baselines matrix factorization (MF) (Koren et al., 2009a), multi-layer perception (MLP) (Cheng et al., 2016), and attentional factorization machines (AFM) (Xiao et al., 2017) trained with mean square root loss baselines. The results of rating prediction on Amazon Reviews (beauty domain) and Yelp are shown in Table 1. The results show that RecMind with different\n\n<div style=\"text-align: center;\">Table 2: Performance comparison in direct recommendation on Amazon Reviews (Beauty) and Yelp.\n</div>\nMethods\nBeauty\nYelp\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nBPR-MLP\n0.1392\n0.0848\n0.2542\n0.1215\n0.1876\n0.1184\n0.3066\n0.1566\nENMF\n0.1537\n0.1124\n0.2479\n0.1453\n0.2235\n0.1448\n0.3379\n0.1851\nP5 (pre-trained expert,few-shot)\n0.1478\n0.1003\n0.2159\n0.1289\n0.2105\n0.1360\n0.3182\n0.1746\nChatGPT (zero-shot)\n0.0146\n0.0107\n0.0705\n0.0235\n0.0479\n0.0265\n0.0751\n0.0326\nChatGPT (few-shot)\n0.0228\n0.0157\n0.0903\n0.0362\n0.0512\n0.0300\n0.0879\n0.0412\nRecMind-CoT (zero-shot)\n0.0497\n0.0325\n0.1129\n0.0637\n0.0992\n0.0719\n0.1673\n0.1170\nRecMind-CoT (few-shot)\n0.0682\n0.0387\n0.1345\n0.0814\n0.1262\n0.0897\n0.1840\n0.1359\nRecMind-ToT (BFS,zero-shot)\n0.0574\n0.0439\n0.1024\n0.0771\n0.1032\n0.0721\n0.1596\n0.1273\nRecMind-ToT (BFS, few-shot)\n0.0734\n0.0402\n0.1355\n0.0808\n0.1649\n0.0920\n0.2217\n0.1503\nRecMind-ToT (DFS,zero-shot)\n0.0564\n0.0432\n0.1011\n0.0751\n0.1022\n0.0733\n0.1587\n0.1266\nRecMind-ToT (DFS, few-shot)\n0.0705\n0.0407\n0.1302\n0.0812\n0.1601\n0.0904\n0.2079\n0.1453\nRecMind-SI (zero-shot)\n0.0675\n0.0524\n0.1259\n0.0923\n0.1055\n0.0791\n0.1674\n0.1293\nRecMind-SI (few-shot)\n0.0915\n0.0624\n0.1559\n0.1063\n0.1749\n0.0935\n0.2451\n0.1607\nMethods\n\nTable 3: Performance comparison in sequential recommendation on Amazon Reviews (Beauty) and Yelp.\nMethods\nBeauty\nYelp\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nS3-Rec\n0.0387\n0.0244\n0.0647\n0.0327\n0.0201\n0.0123\n0.0341\n0.0168\nSASRec\n0.0401\n0.0264\n0.0643\n0.0319\n0.0241\n0.0175\n0.0386\n0.0215\nP5 (pre-trained expert,few-shot)\n0.0459\n0.0347\n0.0603\n0.0411\n0.0565\n0.0389\n0.0702\n0.0441\nChatGPT (zero-shot)\n0.0089\n0.0053\n0.0103\n0.0060\n0.0102\n0.0062\n0.0143\n0.0089\nChatGPT (few-shot)\n0.0179\n0.0124\n0.0256\n0.0125\n0.0217\n0.0116\n0.0320\n0.0165\nRecMind-CoT (zero-shot)\n0.0182\n0.0139\n0.0297\n0.0160\n0.0368\n0.0239\n0.0554\n0.0316\nRecMind-CoT (few-shot)\n0.0349\n0.0187\n0.0486\n0.0302\n0.0427\n0.0305\n0.0590\n0.0380\nRecMind-ToT (BFS, zero-shot)\n0.0297\n0.0172\n0.0368\n0.0249\n0.0379\n0.0251\n0.0538\n0.0322\nRecMind-ToT (BFS, few-shot)\n0.0387\n0.0235\n0.0522\n0.0327\n0.0447\n0.0319\n0.0624\n0.0337\nRecMind-ToT (DFS, zero-shot)\n0.0299\n0.0168\n0.0359\n0.0241\n0.0358\n0.0240\n0.0519\n0.0324\nRecMind-ToT (DFS, few-shot)\n0.0365\n0.0211\n0.0497\n0.0355\n0.0455\n0.0328\n0.0622\n0.0349\nRecMind-SI (zero-shot)\n0.0339\n0.0200\n0.0469\n0.0310\n0.0396\n0.0281\n0.0569\n0.0340\nRecMind-SI (few-shot)\n0.0415\n0.0289\n0.0574\n0.0375\n0.0471\n0.0342\n0.0635\n0.0407\ntypes of planning mechanisms usually outperforms the fully-trained models for rating prediction tasks. Such improvement mainly stems from the advantage that RecMind has access to both the rating history of the user given to different items and the rating history of the item received from different users in the database. On the other side, fully trained models such as MLP and P5 usually have much higher RMSE, which can be attributed to the over-fitting on the training data.\nDirect Recommendation. In the scenario of the direct recommendation, RecMind predicts the recommended items from a candidate set of 100 items, where only one candidate is positive. For each test case, we randomly sample 99 candidates from items that the user has never interacted with as negative candidates. Figure 2 shows an example of direct recommendation in the beauty domain of Amazon Reviews. For a specific user {userID} with a list of products, the agent will be prompted, \u201cFrom the item candidates listed, choose the top 10 items to recommend to the user {userID} and rank them in order of priority from highest to lowest. Candidates: [\u2018Item List\u2019]\". We include traditional recommendation method baselines BPRMLP (Cheng et al., 2016) and ENMF (Chen et al.,\n\n2020) in this task. The results on direct recommendation are shown in Table 2. The results show that fully-trained models such as P5 usually perform better than RecMind. The reason for the performance gap is the long context of the names of 100 candidate items. Specifically, the LLM agent tends to first retrieve information related to items positioned in front of the candidate list. Such positional bias has also been observed in previous works (Liu et al., 2023). Table 2 shows that diverse reasoning planning, such as ToT and our proposed SI, benefit in alleviating this issue by gradually filtering out less-possible items. However, it is still hard for LLMs to fully explore the chances of a large candidate set, especially with limitations on prompt context length.\n\n2020) in this task. The results on direct recommendation are shown in Table 2. The results show that fully-trained models such as P5 usually perform better than RecMind. The reason for the performance gap is the long context of the names of 100 candidate items. Specifically, the LLM agent tends to first retrieve information related to items positioned in front of the candidate list. Such positional bias has also been observed in previous works (Liu et al., 2023). Table 2 shows that diverse reasoning planning, such as ToT and our proposed SI, benefit in alleviating this issue by gradually filtering out less-possible items. However, it is still hard for LLMs to fully explore the chances of a large candidate set, especially with limitations on prompt context length.\nSequential Recommendation. For sequential recommendation, the agent takes the names of the user\u2019s historically interacted items in order as input. Next, the agent is prompted to predict the title of the next item that the user might interact with. Figure 2 shows an example of sequential recommendation in the beauty domain of Amazon Reviews. For a specific user {userID} with the interaction history in chronological order, the agent will be prompted,\n\n# Sequential Recommendation. For sequen\n\nrecommendation, the agent takes the names of the user\u2019s historically interacted items in order as input. Next, the agent is prompted to predict the title of the next item that the user might interact with. Figure 2 shows an example of sequential recommendation in the beauty domain of Amazon Reviews. For a specific user {userID} with the interaction history in chronological order, the agent will be prompted, \u201cuser {userID} has interacted with the following\n\n<div style=\"text-align: center;\">Table 4: Performance comparison on explanation generation on Amazon Reviews (Beauty) and Yelp.\n</div>\nMethods\nBeauty\nYelp\nBLEU2\nROGUE1\nROGUE2\nROGUEL\nBLEU2\nROGUE1\nROGUE2\nROGUEL\nP5 (pre-trained expert,few-shot)\n0.9783\n17.0412\n1.8962\n12.1709\n1.2784\n18.1924\n2.9517\n13.2315\nChatGPT (zero-shot)\n0.0359\n9.7892\n0.7994\n5.1215\n0.0419\n8.9776\n0.8549\n6.1715\nChatGPT (few-shot)\n1.1766\n11.8905\n2.5894\n5.8920\n1.1766\n12.0901\n3.2170\n6.7823\nRecMind-CoT (zero-shot)\n0.8985\n11.0597\n1.9675\n7.7471\n1.1052\n12.5719\n2.1941\n7.7471\nRecMind-CoT (few-shot)\n1.3096\n12.7987\n2.7015\n8.0164\n1.2759\n13.9690\n3.0173\n9.1081\nRecMind-ToT (BFS, zero-shot)\n1.0279\n11.1584\n2.1024\n7.7026\n1.1135\n11.7230\n2.2355\n7.7910\nRecMind-ToT (BFS, few-shot)\n1.3054\n12.8249\n2.7050\n8.0596\n1.2960\n14.1728\n3.4539\n9.6125\nRecMind-ToT (DFS, zero-shot)\n1.0319\n11.3564\n2.1416\n7.7166\n1.1795\n11.8433\n2.2416\n7.8252\nRecMind-ToT (DFS, few-shot)\n1.3159\n12.8975\n2.7125\n8.1150\n1.2896\n14.2201\n3.6710\n9.6719\nRecMind-SI (zero-shot)\n1.1589\n11.6794\n2.2460\n7.8974\n1.1589\n11.6794\n2.2460\n7.8974\nRecMind-SI (few-shot)\n1.3459\n13.2560\n2.7479\n8.9614\n1.3094\n14.4220\n3.8974\n9.7125\nitems in chronological order: [\u2018Item List\u2019]. Please recommend the next item that the user might interact with. Choose the top 10 products to recommend in order of priority, from highest to lowest.\". We include traditional recommendation baselines S 3-Rec (Zhou et al., 2020) and SASRec (Kang et al., 2018). The results in Table 3  show that RecMind with Self-Inspiring achieves comparable performance as fully-trained models P5 and S 3-Rec. Without diverse planning methods such as tree-ofthoughts and our proposed self-inspiring, LLMs prefer items whose names are semantically similar to those of proceeding items. In contrast, with the help of explicit reasoning methods and access to domain knowledge, RecMind gradually explores helpful information, such as connections of items in the database with other users\u2019 interaction history.\n\n# 4.4 Results on Explainability-oriented Recommendation Tasks\n\nWith the development of NLP techniques on recommendation tasks, recent works (Geng et al., 2022) have started to explore how NLP models can improve the explainability of recommendation systems, such as generating text explanations for a given interaction between a user and an item. In this section, we evaluate the performance of RecMind in two explainability-oriented recommendation tasks, which are explanation generation and review summarization. The results on explanation generation are shown in Table 4. The results on review summarization are deferred to Section B.1 of the supplementary. Explanation Generation.  In explanation generation, we assess the performance of RecMind in crafting textual explanations that justify a user\u2019s interaction with a specific item. Figure 2 shows an example of explanation generation in the beauty domain of Amazon Reviews. The text review given\n\nby the user on the given item is taken as the ground truth. The results on explanation generation in Table 4 indicates that RecMind, when leveraging self-inspiring techniques, can achieve performance comparable to the fully trained P5 model. This is aided by the in-domain knowledge retrieved from personalized memory, such as reviews from other users on the same item. To better evaluate the quality and rationality of the explanation generated by RecMind and compare the results with baseline models, we perform human evaluation on the generated evaluation. The evaluation details and results are deferred to Section B.2 of the supplementary.\n\n# 4.5 Transfer to Items in Unseen Domains\n\n<div style=\"text-align: center;\">Table 5: Performance on domain transfer. Comparisons are performed on MAE for rating prediction, HR@5 for direct recommendation, and BLEU2 for explanation generation.\n</div>\nMethods\nDomain\nMAE \u2193\nHR@5 \u2191\nBLEU2 \u2191\nP5\nBeauty \u2192Toys\n0.7932\n0.0852\n1.4326\nBeauty \u2192Sports\n0.7013\n0.1007\n0.8924\nChatGPT\nBeauty \u2192Toys\n0.7354\n0.0649\n1.4416\nBeauty \u2192Sports\n0.6895\n0.7210\n0.8795\nRecMind-ToT\nBeauty \u2192Toys\n0.6845\n0.0841\n1.3994\nBeauty \u2192Sports\n0.6457\n0.0924\n1.0002\nRecMind-SI\nBeauty \u2192Toys\n0.6779\n0.0902\n1.5940\nBeauty \u2192Sports\n0.6245\n0.1124\n1.0537\nThe advantage of using a large language model as a unified recommendation model is that it can judge the likelihood of any event by expressing the event in natural language. In our experiments in Section 4.3, we found that RecMind with indomain few-shot examples achieves much better performance. In this section, we aim to test if the in-domain few-shot examples can generalize to unseen domains, so no parameters need to be trained in such domain transfer experiments. Specifically, we include few-shot examples in the Beauty domain and test the performance of RecMind on rating prediction, direct recommendation, and explanation generation with test data in the Toys and\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dcc1/dcc100db-e138-4eef-ae46-e866d8de4b10.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance comparison of RecMind-SI with different types of foundation LLMs.\n</div>\nSports domain. We include ChatGPT prompting baseline and P5 for comparisons. In the few-shot ChatGPT baseline, the user-specific examples included in the prompts are from the Beauty domain. In the P5, the model trained on the Beauty domain is used for evaluation. We evaluate the domain transfer capabilities of all approaches on rating prediction, direct recommendation, and explanation generation. We report the MAE for rating prediction, HR@5 for direct recommendation, and the BLEU2 for explanation in Table 5. It can be observed that RecMind shows better domain transfer performance compared with the baselines P5 and ChatGPT. In contrast, fine-tuned language model P5 tends to overfit to the domain of the training data.\n\n# 4.6 Ablation Study on Foundation LLMs\n\nIn this section, we study how RecMind performs with different types of foundation LLMs as the controller. We test RecMind-SI using different types of LLMs, including Llama2 70b (Touvron et al., 2023a), GPT-3.5, text-davinci-003, and GPT-4, for sequential recommendation on three different domains in Amazon Reviews. In each domain, we randomly sample 500 test data for evaluation. We run the evaluation on each model five times and calculate the mean and standard deviation of different runs. The results are shown in Figure 4. The results show that the performance of RecMind-SI is not sensitive to the selection of Foundation LLMs. Although RecMind-SI with GPT-4 demonstrates enhanced reasoning in addressing complex problems, RecMind-SI with GPT-3.5 can also deliver commendable performance when leveraging the superior capabilities of the RecMind framework. RecMind-SI with Llama2 70b, also achieves pretty good performance. However, due to its limited input context length, the performance with Llama2 has a larger variance.\n\n# 4.7 Experiments in general reasoning scenarios\n\n# Experiments in general reasoning\n\nTo show that our proposed self-inspiring (SI) method not only outperforms CoT and ToT on recommendation tasks but also on general reasoning scenarios. We evaluate SI on two additional reasoning tasks from [2], which are Game of 24 and Mini Crosswords. We follow the same experimental settings as in ToT [2]. In both tasks, ToT explores the 5 best candidate thoughts at each intermediate step. For a fair comparison, we also set the maximum number of alternative thoughts at each step as 5. We set the maximum number of intermediate steps for the Mini crosswords task to 100 following ToT. GPT-4 backend is used for CoT, ToT, and our SI. The results are shown in Table 6 and Table 7. It can be observed that SI outperforms CoT and ToT on both tasks.\n\n<div style=\"text-align: center;\">Table 6: Experiment Results for Game of 24.\n</div>\nTable 6: Experiment Results for Game of 24\nMethods\nCoT\nToT\nSI\nAccuracy\n4 %\n74 %\n80 %\n<div style=\"text-align: center;\">Table 7: Experiment Results for Mini Crosswords.\n</div>\nTable 7: Experiment Results for Mini Crosswords\nMethods\nCoT\nToT\nSI\nLetter-level Accuracy\n40.6 %\n78 %\n81 %\nWord-level Accuracy\n15.6 %\n60 %\n65 %\nGame-level Accuracy\n1 %\n20 %\n26 %\n# 5 Conclusions\n\n5 Conclusions In this work, we propose a novel LLM-powered autonomous agent, RecMind, for various recommendation tasks. The RecMind consists of three major components, i.e., planning, which breaks down a task into smaller sub-tasks; memory, which provides the agent with the capability to retain and recall information over extended periods; and external tools for obtaining relevant extra information from memory that is missing from model weights. We further propose a novel planning technique selfinspiring, which can integrate the merits of exploring multiple reasoning paths for better planning. We evaluate RecMind across various recommendation tasks, including both precision-oriented tasks and explanability-oriented tasks. The evaluation results show that RecMind with self-inspiring outperforms existing LLM-based recommendation methods in different recommendation tasks and achieves comparable performance to a recent model P5, which is fully trained for the recommendation task. Future works can explore utilizing more external tools in our recommendation agent.\n\nOne major limitation of our work is that more exploration of diverse reasoning paths greatly increases the prompt size, leading to well-known limitations of LLMs in long contexts and position bias. A future direction could be implementing a summarization step for historical paths, which might not only condense the long context but also potentially remove some of the noise in historical paths. In addition, only a small number of external tools are adopted in our current implementation.\n\n# Ethical Concerns and Broader Impacts\n\nAll experiments in our papers are performed on two widely used recommendation datasets, which are Amazon Reviews (Ni et al., 2019) and Yelp (Ni et al., 2019). To protect users\u2019 privacy, both datasets adopt anonymous user IDs to represent user identity. We follow the terms of use for both datasets and only use the datasets for academic purposes. The LLM-based recommendation system proposed in this work has the potential to influence consumer behavior and preferences. In addition, we have tested the method on top of different LLM models, including online and offline models, to avoid potential biases in pre-trained LLMs such as ChatGpt (Schulman et al., 2022).\n\n# References\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.\nKeqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\n\n# Harrison Chase. 2023. langchain. GitHub repository.\n\nChong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, and Shaoping Ma. 2020. Efficient neural matrix factorization without sampling for recommendation. ACM Transactions on Information Systems (TOIS), 38(2):1\u201328.\n\nChong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, and Shaoping Ma. 2020. Efficient neural matrix factorization without sampling for recommendation. ACM Transactions on Information Systems (TOIS), 38(2):1\u201328.\n\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pages 7\u201310.\nWenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046.\nShijie Geng et al. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In  16th ACM Conference on Recommender Systems.\nSignificant Gravitas. 2023. Auto-gpt. GitHub repository.\nXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In  Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 639\u2013648.\nBal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939.\nYupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845.\nWang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do llms understand user preferences? evaluating llms on user rating prediction. arXiv preprint arXiv:2305.06474.\nWang-Cheng Kang et al. 2018. Self-attentive sequential recommendation. In ICDM.\nYehuda Koren, Robert Bell, and Chris Volinsky. 2009a. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337.\nYehuda Koren, Robert M. Bell, and Chris Volinsky. 2009b.  Matrix factorization techniques for recommender systems. Computer, 42.\nJianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023.\nHow can recommender systems benefit from large language models: A survey. ArXiv, abs/2306.05817.\nGreg Linden, Brent Smith, and Jeremy York. 2003.\nAmazon.com recommendations: Item-to-item collaborative filtering. IEEE Distributed Syst. Online,\n\nJunling Liu et al. 2023. Is chatgpt a good recommender? a preliminary study. ArXiv, abs/2304.10149.\n\n# Yohei Nakajima. 2023. babyagi. GitHub repository.\n\nYohei Nakajima. 2023. babyagi. GitHub repository.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLPIJCNLP), pages 188\u2013197.\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages 2303\u201308774.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. 2022. Chatgpt: Optimizing language models for dialogue. OpenAI blog.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.\nFei Sun et al. 2019. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\n\nBhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\nLei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. ArXiv, abs/2304.03153.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.\nJun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617.\nFan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. 2023. Palr: Personalization aware llms for recommendation. arXiv e-prints, pages arXiv\u20132305.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In CIKM.\n\n# A Additional Implementation Details\n\nTool Descriptions in Agent Prompt To enable the LLM-based Agent to utilize external tools, the LLM Agent will be prompted an instruction with descriptions on different tools. The prompt is formulated as: Perform a recommendation task with interleaving Thought, Action, and Observation steps. Thought can reason about the current situation, and Action can be the following types:\n\n\u2022 SQL Tool: \u201cSQL {question}, which aims to search for the answer to a question from the database. You can only put forward questions based on the available information in the database. Available information and schema of the database is provided in {database_info}.\u201d\n\n\u2022  Text Summarization Tool: \u201cSummarize {content}, which condenses extensive text into a shorter version while retaining the core information and meaning by using a pre-trained text summarization model.\u201d\n\n\u2022  Search Tool: \u201cSearch {question}, which formulates a search query for Google search engine based on the question. This tool can be used to search for information that is unavailable in the database.\"\n\n\u2022  Finish: \u201cFinish {answer}, which returns the answer and finishes the task.\u201d\n\nSearch Tool Prompt In the search tool, we use SerpApi.com as our Google search API. Since the output of the search API is in a structured JSON format, we use the same LLM model of the agent to convert the output to a text response and then return it to the LLM agent. The prompt we use is \u201cYour mission is to convert the Google search result {search_result} from search engine to meaningful sentences, which can be a response to question {question}.\u201d SQL Tool Prompt In the SQL tool, we use the same LLM model of the agent to convert the question to an SQL query. The prompt we use in this text-to-SQL process is \u201cYour mission is to convert SQL query from given {question}. The information about the tables in the database is {database_info}. Only output the SQL query.\u201d Next, the obtained SQL query will be executed. Similar to the search tool, the output will then be converted to a text response to the question and\n\nreturned to the LLM agent. The prompt we use to convert the output is \u201cYour mission is to convert SQL query execution results to meaningful sentences, which should be the answer to the question {question}. The query generated for this question is {sql_query}. Here is the database result: {sql_result}\u201d Self-Inspiring Prompt In the implementation of self-inspiring, the same LLM model of the agent is used to decide whether another thought is necessary given the task and previously explored steps. The prompt for this request is \u201cYou are given multi-step problem-solving steps towards finishing the task {task}. The previous steps are {previous_steps}. You already have the thought, action, and observation in the current step {current_step}. Your mission is to decide if there is an alternative thought in the current step that can help finish this task following the previous steps. If there is, directly output the thought. If not, please respond {empty_response}.\u201d For ChatGPT (zero-shot) and ChatGPT (fewshot), we use the exact same prompt templates from (Liu et al., 2023). We will attach the prompt templates for all baseline methods in the appendix of the revised version of our paper. We follow (Yao et al., 2022) to design the prompt for CoT. The prompt is \u201cSolve a recommendation task with interleaving Thought, Action, and Observation steps.\u201d We follow (Yao et al., 2023) to design the prompts for ToT. In addition to the general instruction, \u201cSolve a recommendation task with interleaving Thought, Action, and Observation steps\", we also designed prompts for thought-sampling and decision-making. The thought sampling prompt is \u201cGiven the previous {previous_steps}, list five possible thoughts for the next step towards finishing the task {task}.\u201d The decision-making prompt is \u201cGiven an instruction and several choices, decide which choice is most promising. Your instruction is {task_sepcific_instruction}. Your available options are {option_list}. Analyze each choice, then conclude in the last line, \u2018The best choice is {s}\u2019, where s is the integer id of the choice.\u201d\n\n# B Additional Experiment Results\n\n# B Additional Experiment Results\n\n# B Additional Experiment Results\nB.1 Results on Review Summarization\n\n# B.1 Results on Review Summarization\n\nIn the review summarization task, we evaluate the performance of RecMind in summarizing review comments to shorter review titles. We filter out test data with automatically generated review titles\n\nsuch as \u2019Five Stars\u2019. Figure 2  shows an example of review summarization in the beauty domain of Amazon Reviews. The results of the review summarization on Amazon Reviews are shown in Table 8. The result shows that RecMind agent performs better that recent LLM such as ChatGPT. However, RecMind does not outperform P5 regarding the review summarization. This performans comes from the advantage of P5 which fully trained model towards optimizaing the review summarization task. In contrast, GPT-based models, such as RecMind, usually prioritize generating summaries after deeply understanding the reviews.\n\n<div style=\"text-align: center;\">Table 8: Performance comparison on review summarization on Amazon Reviews (Beauty).\n</div>\nMethods\nBeauty\nBLEU2\nROGUE1\nROGUE2\nROGUEL\nP5 (pre-trained expert,few-shot)\n2.0357\n8.3079\n1.5892\n7.4820\nChatGPT (zero-shot)\n0.6532\n3.8579\n0.3059\n3.3552\nChatGPT (few-shot)\n0.9137\n4.0179\n0.4179\n3.6790\nRecMind-CoT (zero-shot)\n1.3596\n5.0279\n0.7156\n4.7689\nRecMind-CoT (few-shot)\n1.3786\n5.5397\n0.8456\n4.8024\nRecMind-ToT (BFS, zero-shot)\n1.3592\n5.1103\n0.7596\n4.8069\nRecMind-ToT (BFS, few-shot)\n1.3737\n5.4187\n0.8254\n4.8157\nRecMind-ToT (DFS, zero-shot)\n1.3614\n5.1435\n0.7749\n4.7985\nRecMind-ToT (DFS, few-shot)\n1.3798\n5.5794\n0.8351\n4.8976\nRecMind-SI (zero-shot)\n1.3688\n5.4579\n0.8974\n4.9746\nRecMind-SI (few-shot)\n1.4014\n6.0354\n1.0128\n5.5716\n# B.2 Human Evaluation\n\nIn this section, we leverage human evaluation to assess the quality and rationality of the explanation generated by RecMind. Three human evaluators (Eva_1, Eva_2, Eva_3) are asked to rank the explanations generated by P5, few-shot ChatGPT, few-shot RecMind with tree-of-thoughts, few-shot RecMind with self-inspiring and the ground truth on 100 test data. We show the top-1 ratios on results generated by different methods in Table 9 for each evaluator. The top-1 ratio indicates the proportion of test data where the given method ranks first compared to other methods based on each annotator\u2019s selection. We also calculate the average top-1 ratios of all three evaluators on results generated by each method. Although annotators may have individual subjectivity, evaluations by different evaluators consistently show that the few-shot RecMind based on self-inspiring, i.e., RecMind-SI yields the most satisfactory results.\n\n<div style=\"text-align: center;\">Table 9: Human evaluation results on explanation generation\n</div>\nMethods\nEvaluator\nAverage\nEva_1\nEva_2\nEva_3\nGround Truth\n0.12\n0.13\n0.22\n0.157\nP5\n0.02\n0.06\n0.03\n0.037\nChatGPT\n0.15\n0.23\n0.18\n0.187\nRecMind-ToT\n0.29\n0.28\n0.25\n0.273\nRecMind-SI\n0.42\n0.30\n0.32\n0.347\n# B.3 Running Time Analysis\n\nIn this section, we provide a running time comparison between our proposed reasoning method SI and previous reasoning methods for the recommendation agent. We run RecMind with CoT, ToT, and SI on 100 randomly sampled test data from the Beauty domain of Amazon Reviews and calculate the average running time. We use GPT-3.5 as the base model. The results in Table 10 show that our proposed self-inspiring can not only improve the performance of the LLM-powered agent but also take less inference time than the existing state-ofthe-art diverse reasoning method ToT. Such merit mainly stems from the fact that SI only explores alternative options at an intermediate step when it recognizes that the explored options at that step are not good enough. In contrast, ToT directly samples multiple options for exploration, which can lead to a waste of time.\nTable 10: Average Running Time of RecMind with Different Reasoning Methods.\n\nMethods\nCoT\nToT\nSI\nAverage Running Time (s)\n18.9 s\n53.2 s\n29.7 s\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3352/3352cf89-8acc-497a-8b31-51abfb71d0ee.png\" style=\"width: 50%;\"></div>\n# B.4 Results on Sports and Toys Domains in Amazon Reviews\n\nIn this section, we provide additional experiment results of RecMind and all compared methods on the Sports domain and Toys domain in Amazon Reviews. The results in rating prediction on the Sports and Toys domains of Amazon Reviews are shown in Table 11. The results in the direct recommendation and sequential recommendation on the Sports domain of Amazon Reviews are shown in Table 12. The results in the direct recommendation and sequential recommendation on the Toys domain of Amazon Reviews are shown in Table 13. The results in text summarization and explanation\n\nTable 11: Performance comparison in rating prediction on Sports and Toys domains of Amazon Reviews.\n\n<div style=\"text-align: center;\">Table 11: Performance comparison in rating prediction on Sports and Toys domains of Amazon Reviews.\n</div>\nMethods\nSports\nToys\nRMSE\nMAE\nRMSE\nMAE\nMF\n1.0274\n0.7975\n1.0193\n0.8024\nMLP\n1.1277\n0.7626\n1.1215\n0.8097\nP5 (pre-trained expert,few-shot)\n1.0534\n0.6784\n1.0625\n0.7134\nChatGPT (zero-shot)\n1.2723\n1.0637\n1.3213\n1.0117\nChatGPT (few-shot)\n1.0929\n0.6957\n1.0519\n0.7047\nRecMind-CoT (zero-shot)\n1.1490\n0.8042\n1.1680\n0.8232\nRecMind-CoT (few-shot)\n1.0325\n0.6446\n1.0403\n0.6905\nRecMind-ToT (BFS, zero-shot)\n1.1322\n0.8014\n1.1559\n0.8164\nRecMind-ToT (BFS, few-shot)\n1.0307\n0.6289\n1.0279\n0.6823\nRecMind-ToT (DFS, zero-shot)\n1.1366\n0.8021\n1.1537\n0.8155\nRecMind-ToT (DFS, few-shot)\n1.0545\n0.6433\n1.0196\n0.6801\nRecMind-SI (zero-shot)\n1.1230\n0.7913\n1.1412\n0.8103\nRecMind-SI (few-shot)\n1.0124\n0.6122\n1.0086\n0.6712\ngeneration on the Sports domain of Amazon Reviews are shown in Table 14. The results in text summarization and explanation generation on the Toys domain of Amazon Reviews are shown in Table 15. As indicated in the experimental results, RecMind also performs well in different recommendation tasks on data from other domains of Amazon Reviews.\n\nTable 12: Performance comparison in direct recommendation and sequential recommendation on Sports domain of Amazon Reviews.\n\nMethods\nSports\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nDirect Recommendation\nBPR-MLP\n0.1520\n0.0927\n0.2671\n0.1296\nP5 (pre-trained expert,few-shot)\n0.1765\n0.1196\n0.2235\n0.1325\nChatGPT (zero-shot)\n0.0376\n0.0317\n0.0902\n0.0459\nChatGPT (few-shot)\n0.0388\n0.0267\n0.1003\n0.0502\nRecMind-CoT (zero-shot)\n0.0607\n0.0435\n0.1259\n0.0757\nRecMind-CoT (few-shot)\n0.0782\n0.0527\n0.1475\n0.1034\nRecMind-ToT (BFS, zero-shot)\n0.0741\n0.0512\n0.1320\n0.1054\nRecMind-ToT (BFS, few-shot)\n0.0874\n0.0542\n0.1475\n0.1218\nRecMind-ToT (DFS, zero-shot)\n0.0759\n0.0519\n0.1320\n0.1079\nRecMind-ToT (DFS, few-shot)\n0.0815\n0.0557\n0.1412\n0.1272\nRecMind-SI (zero-shot)\n0.0835\n0.0684\n0.1379\n0.1103\nRecMind-SI (few-shot)\n0.1115\n0.0814\n0.1769\n0.1303\nSequential Recommendation\nS3-Rec\n0.0251\n0.0161\n0.0385\n0.0204\nP5 (pre-trained expert,few-shot)\n0.0357\n0.0289\n0.0416\n0.0324\nChatGPT (zero-shot)\n0.0039\n0.0008\n0.0051\n0.0008\nChatGPT (few-shot)\n0.0130\n0.0075\n0.0207\n0.0070\nRecMind-CoT (zero-shot)\n0.0135\n0.0090\n0.0248\n0.0105\nRecMind-CoT (few-shot)\n0.0300\n0.0138\n0.0437\n0.0247\nRecMind-ToT (BFS, zero-shot)\n0.0205\n0.0134\n0.0319\n0.0243\nRecMind-ToT (BFS, few-shot)\n0.0338\n0.0186\n0.0473\n0.0272\nRecMind-ToT (DFS, zero-shot)\n0.0218\n0.0130\n0.0336\n0.0238\nRecMind-ToT (DFS, few-shot)\n0.0316\n0.0162\n0.0448\n0.0260\nRecMind-SI (zero-shot)\n0.0290\n0.0151\n0.0420\n0.0255\nRecMind-SI (few-shot)\n0.0366\n0.0240\n0.0525\n0.0320\nMethods\nSports\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nDirect Recommendation\nBPR-MLP\n0.1520\n0.0927\n0.2671\n0.1296\nP5 (pre-trained expert,few-shot)\n0.1765\n0.1196\n0.2235\n0.1325\nChatGPT (zero-shot)\n0.0376\n0.0317\n0.0902\n0.0459\nChatGPT (few-shot)\n0.0388\n0.0267\n0.1003\n0.0502\nRecMind-CoT (zero-shot)\n0.0607\n0.0435\n0.1259\n0.0757\nRecMind-CoT (few-shot)\n0.0782\n0.0527\n0.1475\n0.1034\nRecMind-ToT (BFS, zero-shot)\n0.0741\n0.0512\n0.1320\n0.1054\nRecMind-ToT (BFS, few-shot)\n0.0874\n0.0542\n0.1475\n0.1218\nRecMind-ToT (DFS, zero-shot)\n0.0759\n0.0519\n0.1320\n0.1079\nRecMind-ToT (DFS, few-shot)\n0.0815\n0.0557\n0.1412\n0.1272\nRecMind-SI (zero-shot)\n0.0835\n0.0684\n0.1379\n0.1103\nRecMind-SI (few-shot)\n0.1115\n0.0814\n0.1769\n0.1303\nSequential Recommendation\n<div style=\"text-align: center;\">Sequential Recommendation\n</div>\nTable 13: Performance comparison in direct recommendation and sequential recommendation on Toys domain of Amazon Reviews.\n\nTable 13: Performance comparison in direct recommendation and sequential recommendation on Toys domain of Amazon\n\nReviews.\nMethods\nToys\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nDirect Recommendation\nBPR-MLP\n0.1142\n0.0688\n0.2077\n0.0988\nP5 (pre-trained,few-shot)\n0.1278\n0.0743\n0.1859\n0.1089\nChatGPT (zero-shot)\n0.0114\n0.0075\n0.0638\n0.0191\nChatGPT (few-shot)\n0.0130\n0.0059\n0.0805\n0.0270\nRecMind-CoT (zero-shot)\n0.0399\n0.0233\n0.1031\n0.0542\nRecMind-CoT (few-shot)\n0.0580\n0.0295\n0.1247\n0.0719\nRecMind-ToT (BFS,zero-shot)\n0.0496\n0.0297\n0.1079\n0.0697\nRecMind-ToT (BFS, few-shot)\n0.0636\n0.0300\n0.1257\n0.0813\nRecMind-ToT (DFS,zero-shot)\n0.0510\n0.0301\n0.1094\n0.0712\nRecMind-ToT (DFS, few-shot)\n0.0603\n0.0315\n0.1204\n0.0817\nRecMind-SI (zero-shot)\n0.0577\n0.0432\n0.1161\n0.0828\nRecMind-SI (few-shot)\n0.0813\n0.0532\n0.1461\n0.0998\nSequential Recommendation\nS3-Rec\n0.0443\n0.0294\n0.0700\n0.0376\nP5 (pre-trained,few-shot)\n0.0612\n0.0524\n0.0702\n0.0569\nChatGPT (zero-shot)\n0.0192\n0.0158\n0.0212\n0.0165\nChatGPT (few-shot)\n0.0282\n0.0231\n0.0367\n0.0230\nRecMind-CoT (zero-shot)\n0.0285\n0.0246\n0.0408\n0.0265\nRecMind-CoT (few-shot)\n0.0452\n0.0294\n0.0597\n0.0407\nRecMind-ToT (BFS,zero-shot)\n0.0399\n0.0287\n0.0495\n0.0359\nRecMind-ToT (BFS, few-shot)\n0.0490\n0.0342\n0.0633\n0.0432\nRecMind-ToT (DFS,zero-shot)\n0.0412\n0.0295\n0.0507\n0.0376\nRecMind-ToT (DFS, few-shot)\n0.0468\n0.0318\n0.0608\n0.0420\nRecMind-SI (zero-shot)\n0.0442\n0.0307\n0.0580\n0.0415\nRecMind-SI (few-shot)\n0.0518\n0.0396\n0.0685\n0.0480\nTable 14: Performance comparison on review summarization and explanation generation on Sports domain of Amazon Reviews.\n\nTable 14: Performance comparison on review summarization and explanation generation on Sports domain of Amazon Re\n\n<div style=\"text-align: center;\">Table 14: Performance comparison on review summarization and explanation generation on Sports domain of Amazon Reviews.\n</div>\nviews.\nMethods\nSports\nBLEU2\nROGUE1\nROGUE2\nROGUEL\nReview Summarization\nP5 (pre-trained expert,few-shot)\n2.5874\n11.8971\n3.0257\n10.5472\nChatGPT (zero-shot)\n0.9024\n5.7402\n1.2493\n3.6791\nChatGPT (few-shot)\n1.2579\n6.3190\n1.4257\n3.8912\nRecMind-CoT (zero-shot)\n1.5840\n6.5310\n1.4390\n5.0140\nRecMind-CoT (few-shot)\n1.6014\n6.7125\n1.5479\n5.2175\nRecMind-ToT (BFS, zero-shot)\n1.5940\n6.5872\n1.4780\n5.1566\nRecMind-ToT (BFS, few-shot)\n1.7125\n6.7986\n1.5724\n5.3794\nRecMind-ToT (DFS, zero-shot)\n1.5874\n6.5531\n1.4726\n5.1530\nRecMind-ToT (DFS, few-shot)\n1.6542\n6.6540\n1.5639\n5.2960\nRecMind-SI (zero-shot)\n1.6120\n6.6259\n1.5029\n5.1891\nRecMind-SI (few-shot)\n1.7388\n6.8130\n1.6217\n5.5632\nExplanation Generation\nP5 (pre-trained expert,few-shot)\n1.1412\n14.0329\n2.1279\n11.1894\nChatGPT (zero-shot)\n0.0611\n7.2892\n0.9921\n5.6923\nChatGPT (few-shot)\n1.2358\n9.6405\n2.8723\n6.2824\nRecMind-CoT (zero-shot)\n0.9687\n8.3097\n2.1320\n7.1427\nRecMind-CoT (few-shot)\n1.3874\n11.0487\n3.0216\n8.1146\nRecMind-ToT (BFS, zero-shot)\n1.1032\n8.9895\n2.3810\n7.8419\nRecMind-ToT (BFS, few-shot)\n1.3765\n11.5749\n2.8023\n8.4256\nRecMind-ToT (DFS, zero-shot)\n1.1345\n9.0957\n2.4866\n7.9965\nRecMind-ToT (DFS, few-shot)\n1.4018\n11.6475\n3.0107\n8.6032\nRecMind-SI (zero-shot)\n1.2374\n9.4294\n2.5405\n8.2120\nRecMind-SI (few-shot)\n1.4287\n12.0060\n3.0481\n9.5812\n<div style=\"text-align: center;\">Methods\n</div>\nTable 15: Performance comparison in review summarization and explanation generation on Toys domain in Amazon Reviews.\n\nTable 15: Performance comparison in review summarization and explanation generation on Toys domain in Amazon Reviews.\n\nMethods\nToys\nBLEU2\nROGUE1\nROGUE2\nROGUEL\nReview Summarization\nP5 (pre-trained expert,few-shot)\n1.8760\n9.0351\n1.5230\n8.1746\nChatGPT (zero-shot)\n0.5941\n4.4571\n0.4052\n4.0612\nChatGPT (few-shot)\n0.8420\n4.8179\n0.3178\n4.2889\nRecMind-CoT (zero-shot)\n1.1579\n5.7276\n0.7158\n5.5691\nRecMind-CoT (few-shot)\n1.2394\n6.3395\n0.9453\n5.8123\nRecMind-ToT (BFS, zero-shot)\n1.1603\n5.9315\n0.8259\n5.4930\nRecMind-ToT (BFS, few-shot)\n1.2668\n6.3186\n0.9251\n5.6159\nRecMind-ToT (DFS, zero-shot)\n1.1725\n6.0014\n0.8551\n5.5012\nRecMind-ToT (DFS, few-shot)\n1.2515\n6.2791\n0.9356\n5.5976\nRecMind-SI (zero-shot)\n1.1897\n6.2578\n0.8976\n5.8724\nRecMind-SI (few-shot)\n1.2974\n6.8352\n1.1125\n6.2718\nExplanation Generation\nP5 (pre-trained expert,few-shot)\n2.2850\n15.0416\n3.6798\n12.1065\nChatGPT (zero-shot)\n0.1379\n9.7892\n1.5416\n5.3158\nChatGPT (few-shot)\n2.0169\n11.8905\n3.2049\n6.2689\nRecMind-CoT (zero-shot)\n2.1354\n11.0597\n2.1590\n7.1445\nRecMind-CoT (few-shot)\n2.4079\n12.7987\n3.5146\n7.4153\nRecMind-ToT (BFS, zero-shot)\n2.1930\n11.2874\n2.1782\n7.1854\nRecMind-ToT (BFS, few-shot)\n2.4565\n12.8249\n3.6327\n7.6234\nRecMind-ToT (DFS, zero-shot)\n2.1658\n11.2802\n2.1770\n7.1809\nRecMind-ToT (DFS, few-shot)\n2.4152\n12.8975\n3.6079\n7.7112\nRecMind-SI (zero-shot)\n2.2740\n11.6794\n2.2460\n7.2536\nRecMind-SI (few-shot)\n2.4674\n13.2560\n3.6920\n7.9987\n",
    "paper_type": "method",
    "attri": {
        "background": "The recommendation system (RS) has advanced significantly through deep learning, but existing methods are limited in their generalizability and ability to leverage external knowledge due to model scale and data size constraints. A new approach is necessary to overcome these limitations.",
        "problem": {
            "definition": "Current RS approaches are often trained on task-specific datasets, which restricts their adaptability to new recommendation tasks and external knowledge integration.",
            "key obstacle": "The main challenge is that existing methods cannot effectively generalize to unseen recommendation tasks or utilize external knowledge due to their reliance on fixed model weights and limited data."
        },
        "idea": {
            "intuition": "The idea arose from the observation that large language models (LLMs) have strong reasoning capabilities that can be harnessed for recommendation tasks, particularly through planning and utilizing external knowledge.",
            "opinion": "The proposed idea is RecMind, an LLM-powered autonomous recommender agent that can provide zero-shot personalized recommendations by leveraging external knowledge and tools.",
            "innovation": "RecMind introduces a Self-Inspiring (SI) planning technique that retains all previously explored states in planning, unlike existing methods that discard earlier states, thereby enhancing reasoning and decision-making."
        },
        "method": {
            "method name": "RecMind",
            "method abbreviation": "RM",
            "method definition": "RecMind is an LLM-powered agent designed to autonomously generate personalized recommendations without the need for fine-tuning on specific datasets.",
            "method description": "RecMind utilizes a planning mechanism that integrates historical states to improve decision-making in recommendations.",
            "method steps": [
                "Input user preferences and historical interactions.",
                "Decompose the recommendation task into smaller sub-tasks.",
                "Utilize external tools to gather relevant knowledge.",
                "Generate thoughts, actions, and observations at each step.",
                "Make a final recommendation based on the gathered information."
            ],
            "principle": "The effectiveness of RecMind lies in its ability to integrate multiple reasoning paths and utilize external knowledge, allowing for more informed and accurate recommendations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the Amazon Reviews dataset across various recommendation tasks, including rating prediction, sequential recommendation, and explanation generation.",
            "evaluation method": "Performance was assessed using metrics such as RMSE, MAE for rating prediction, and HR@k, NDCG@k for recommendation tasks, comparing RecMind against several baseline methods."
        },
        "conclusion": "RecMind demonstrates superior performance across various recommendation tasks compared to existing LLM-based methods and achieves results comparable to fully trained models like P5, showcasing the potential of integrating reasoning and external knowledge in recommendation systems.",
        "discussion": {
            "advantage": "RecMind's ability to leverage external knowledge and retain historical reasoning states allows it to outperform traditional models, particularly in generalization and adaptability to new tasks.",
            "limitation": "The exploration of diverse reasoning paths can lead to increased prompt sizes, which may result in limitations related to LLMs' context length and positional bias.",
            "future work": "Future research could focus on improving the summarization of historical paths to reduce context size and exploring additional external tools for enhanced functionality."
        },
        "other info": {
            "ethical concerns": "All experiments were conducted using anonymized datasets to protect user privacy, and the potential influence of LLM-based recommendations on consumer behavior was acknowledged.",
            "additional notes": "RecMind's architecture includes components for planning, memory, and tool usage, which collectively enhance its recommendation capabilities."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommendation systems (RS) have advanced significantly through deep learning, but existing methods are limited in their generalizability and ability to leverage external knowledge due to model scale and data size constraints."
        },
        {
            "section number": "1.2",
            "key information": "The proposed idea is RecMind, an LLM-powered autonomous recommender agent that can provide zero-shot personalized recommendations by leveraging external knowledge and tools."
        },
        {
            "section number": "2.1",
            "key information": "Current RS approaches are often trained on task-specific datasets, which restricts their adaptability to new recommendation tasks and external knowledge integration."
        },
        {
            "section number": "3.2",
            "key information": "RecMind utilizes a planning mechanism that integrates historical states to improve decision-making in recommendations."
        },
        {
            "section number": "4.1",
            "key information": "RecMind is an LLM-powered agent designed to autonomously generate personalized recommendations without the need for fine-tuning on specific datasets."
        },
        {
            "section number": "4.2",
            "key information": "The effectiveness of RecMind lies in its ability to integrate multiple reasoning paths and utilize external knowledge, allowing for more informed and accurate recommendations."
        },
        {
            "section number": "10.3",
            "key information": "All experiments were conducted using anonymized datasets to protect user privacy, and the potential influence of LLM-based recommendations on consumer behavior was acknowledged."
        }
    ],
    "similarity_score": 0.7637082247014043,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/625e/625e3da7-f27a-49bc-a1cf-ef0167c001d7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4f06/4f0696e9-3cf7-47d2-9c62-9cb2937ec9ad.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/254e/254e2aec-14d3-4408-950e-2de00c89d0a2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5514/5514e1b9-00ae-4471-8fbe-c9f57c6bfca7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7de/b7de2f4d-c40b-4a15-8319-f9c3e4d343ff.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d107/d1074fb9-c655-47e3-9e02-7897c4d4c995.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2172/2172297d-e28c-4833-b450-a9b82f69e87f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a96/5a96ab80-d11f-4b13-9109-397c70ef6f99.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dcc1/dcc100db-e138-4eef-ae46-e866d8de4b10.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3352/3352cf89-8acc-497a-8b31-51abfb71d0ee.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Recmind_ Large language model powered agent for recommendation.json"
}