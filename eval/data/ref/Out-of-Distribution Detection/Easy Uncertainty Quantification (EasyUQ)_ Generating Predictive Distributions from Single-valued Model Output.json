{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.08376",
    "title": "Easy Uncertainty Quantification (EasyUQ): Generating Predictive Distributions from Single-valued Model Output",
    "abstract": "How can we quantify uncertainty if our favorite computational tool - be it a numerical, a statistical, or a machine learning approach, or just any computer model - provides single-valued output only? In this article, we introduce the Easy Uncertainty Quantification (EasyUQ) technique, which transforms real-valued model output into calibrated statistical distributions, based solely on training data of model output-outcome pairs, without any need to access model input. In its basic form, EasyUQ is a special case of the recently introduced Isotonic Distributional Regression (IDR) technique that leverages the pool-adjacent-violators algorithm for nonparametric isotonic regression. EasyUQ yields discrete predictive distributions that are calibrated and optimal in finite samples, subject to stochastic monotonicity. The workflow is fully automated, without any need for tuning. The Smooth EasyUQ approach supplements IDR with kernel smoothing, to yield continuous predictive distributions that preserve key properties of the basic form, including both, stochastic monotonicity with respect to the original model output, and asymptotic consistency. For the selection of kernel parameters, we introduce multiple one-fit grid search, a computationally much less demanding approximation to leave-one-out cross-validation. We use simulation examples and forecast data from weather prediction to illustrate the techniques. In a study of benchmark problems from machine learning, we show how EasyUQ and Smooth EasyUQ can be integrated into the workflow of neural network learning and hyperparameter tuning, and find EasyUQ to be competitive with conformal prediction, as well as more elaborate input-based approaches.",
    "bib_name": "walz2023easyuncertaintyquantificationeasyuq",
    "md_text": "# Easy Uncertainty Quantification (EasyUQ): Generating Predictive Distributions from Single-valued Model Output\nJuly 25, 2023\nAbstract\nHow can we quantify uncertainty if our favorite computational tool \u2014 be it a numerical, a statistical, or a machine learning approach, or just any computer model \u2014 provides single-valued output only? In this article, we introduce the Easy Uncertainty Quantification (EasyUQ) technique, which transforms real-valued model output into calibrated statistical distributions, based solely on training data of model output\u2013outcome pairs, without any need to access model input. In its basic form, EasyUQ is a special case of the recently introduced Isotonic Distributional Regression (IDR) technique that leverages the pool-adjacent-violators algorithm for nonparametric isotonic regression. EasyUQ yields discrete predictive distributions that are calibrated and optimal in finite samples, subject to stochastic monotonicity. The workflow is fully automated, without any need for tuning. The Smooth EasyUQ approach supplements IDR with kernel smoothing, to yield continuous predictive distributions that preserve key properties of the basic form, including both, stochastic monotonicity with respect to the original model output, and asymptotic consistency. For the selection of kernel parameters, we introduce multiple one-fit grid search, a computationally much less demanding approximation to leave-one-out cross-validation. We use simulation examples and forecast data from weather prediction to illustrate the techniques. In a study of benchmark problems from machine learning, we show how EasyUQ and Smooth EasyUQ can be integrated into the workflow of neural network learning and hyperparameter tuning, and find EasyUQ to be competitive with conformal prediction, as well as more elaborate input-based approaches.\n# 1 Introduction\nIn an editorial that remains topical and relevant [71], SIAM President Nick Trefethen noted decade ago that\nIndeed, with the increasing reliance of real world decisions on the output of computer models \u2013 which might be numerical or statistical, parametric or nonparametric, simple or complex \u2013 1Institute for Stochastics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany and Computational Statistics (CST) group, Heidelberg Institute for Theoretical Studies, Heidelberg, Germany (eva-maria.walz@kit. edu) 2Seminar for Statistics, ETH Z\u00a8urich, Z\u00a8urich, Switzerland (alexander.henzi@stat.math.ethz.ch) 3Institute for Mathematical Statistics and Actuarial Science, University of Bern, Bern, Switzerland (johanna. ziegel@stat.unibe.ch) 4Computational Statistics (CST) group, Heidelberg Institute for Theoretical Studies, Heidelberg, Germany and Institute for Stochastics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany (tilmann.gneiting@ h-its.org).\nand the advent of uncertainty quantification as a scientific field of its own, there is a growing consensus in the computational sciences community that decisions ought to be informed by full predictive distributions, rather than single-valued model output. For recent perspectives on these issues and uncertainty quantification in general, we refer to topical monographs [24, 68, 70] and review articles [1, 5, 26, 60]. How can we quantify uncertainty if the computational model at hand provides single-valued output only? With Nick Trefethen\u2019s comment in mind, we address the following problem: Given single-valued, univariate model output, how can we generate a prediction interval or, more generally, a probabilistic forecast in the form of a full statistical distribution? In this work, we introduce the Easy Uncertainty Quantification (EasyUQ) technique that serves this task, based solely on a training archive of model output\u2013outcome pairs. The single-valued, univariate model output can be of any type \u2014 e.g., it may stem from a physics-based numerical model, might arise from a purely statistical or machine learning model, or might be based on human expertise. In a nutshell, EasyUQ applies the recently introduced Isotonic Distributional Regression (IDR, [35]) approach to generate discrete, calibrated predictive distributions, conditional on the model output at hand. The name stems from the three-fold reasons that EasyUQ operates on the final model output only, without any need for access to the original model input, that the method honors a natural assumption of isotonicity, namely, that higher values of the model output entail predictive distributions that are larger in stochastic order, and that the basic version of EasyUQ does not involve any tuning parameters, and thus does not require user intervention. The more elaborate Smooth EasyUQ approach introduced in this paper subjects the EasyUQ distribution to kernel smoothing, to yield predictive probability densities that preserve key properties of the basic approach. Prediction intervals are readily extracted; e.g., the equal-tailed 90% interval forecast is framed by the quantiles at level 0.05 and 0.95 of the predictive distribution. As the EasyUQ approach requires training data, it addresses general \u201cweather-like\u201d tasks ([5], p. 441), which are characterized by frequent repetition of the task \u2014 e.g., hourly, daily, monthly, at numerous spatial locations, or for a range of customers or patients \u2014 in concert with short to moderate lead times of the forecasts, thus enabling the development of a sizeable archive of forecast\u2013outcome pairs. EasyUQ makes the best possible use of single-valued model output in the sense of empirical score minimization on the training data, subject to the natural constraint of isotonicity. Specifically, the larger the model output, the larger the predictive distribution, in the technical sense of the familiar stochastic order [65], i.e., the respective cumulative distribution functions (CDFs) do not intersect and their graphs move to the right as the model output increases. Subject to the isotonicity constraint, the EasyUQ distributions are optimal with respect to a large class of loss functions that includes the popular continuous ranked probability score (CRPS, [28, 47]), all proper scoring rules for binary events, and all proper scoring rules for quantile forecasts, among others ([35], Thm. 2). For prediction, the EasyUQ and Smooth EasyUQ distributions are interpolated to the value of the model output at hand, while respecting isotonicity. Figure 1 illustrates the EasyUQ approach on WeatherBench [57], a benchmark dataset for weather prediction that serves as a running example in this paper. Panel a) shows singlevalued forecasts of upper air temperature from the HRES numerical weather prediction model run by the European Centre for Medium-Range Weather Forecasts (ECMWF, [49]) along with the associated observed temperatures in February 2017. The training data for EasyUQ, which converts the single-valued HRES model output into conditional predictive distributions, comprise the forecast\u2013outcome pairs from 2010 through 2016, as illustrated in the scatter plot in panel c). Panel d) shows the EasyUQ predictive distributions for February 2017, which derive from the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1063/1063317b-755e-4170-8b76-fcc2d4381dbd.png\" style=\"width: 50%;\"></div>\nFigure 1: EasyUQ illustrated on WeatherBench data. Time series of three days ahead a) singlevalued HRES model forecasts, b) state of the art ECMWF ensemble forecasts, and d) basic EasyUQ predictive distributions based on the single-valued HRES forecast along with associated outcomes of upper air temperature in February 2017 at a grid point over Poland, in degrees Kelvin. The boxplots show the quantiles at level 0.05, 0.25, 0.50, 0.75, and 0.95 of the predictive distributions. c) Scatterplot of HRES model output and associated outcomes in 2010 through 2016, which serve as training data. The inset diagrams show the ECMWF and EasyUQ predictive CDFs for (A) 9 February 2017 and (B) 15 February 2017, respectively. e) Basic and Smooth EasyUQ predictive CDFs and f) Smooth EasyUQ predictive densities at selected values of the single-valued HRES forecast. For further details see Section 2.3.\nsingle-valued HRES forecasts in panel a), and can be compared to the computationally much more expensive ECMWF ensemble forecasts in panel b). To facilitate the comparison, panel c) includes inset diagrams with the ECMWF ensemble and EasyUQ predictive CDFs for two particular days. Panels e) and f) show EasyUQ predictive CDFs and Smooth EasyUQ predictive densities when the HRES model output equals 263, 268, and 273 degrees Kelvin, respectively. The isotonicity property of the EasyUQ distributions is reflected by the non-intersecting CDFs. The boxes in panels b) and d) range from the 25th to the 75th percentile of the distribution and generate 50% prediction intervals, whereas the whiskers range from the 5th to the 95th percentile and form 90% intervals. The remainder of the paper is organized as follows. Section 2 provides comprehensive descriptions of IDR and the basic EasyUQ method, and we give detail, background information, and a comparison to conformal prediction [72, 75] on both the WeatherBench temperature forecast challenge and a precipitation forecast example. In Section 3, we introduce the Smooth EasyUQ technique, show that it retains the isotonocity property of the basic method, and discuss statistical large-sample consistency. For the selection of kernel parameters, we introduce multiple one-fit grid search, a computationally much less demanding approximate version of cross-validation. In Section 4, we demonstrate that EasyUQ can be integrated into the workflow of neural network learning and hyperparameter tuning, and use benchmark problems to compare its predictive performance to state-of-the-art techniques from machine learning and conformal prediction. The paper closes with remarks in Section 5, where we return to the discussion of input-based vs. output-based uncertainty quantification. While the basic version of EasyUQ arises as a special case of the extant IDR technique [35], we take the particular perspective of the conversion of single-valued model output into predictive distributions. Original contributions in this paper include the development of the Smooth EasyUQ method (Sections 3.1 and 3.2), a detailed comparison to conformal prediction in case studies (Sections 2.3, 2.4, and 3.3) and from computational and methodological perspectives (Sections 3.4 and 5), and the integration and benchmarking of EasyUQ and Smooth EasyUQ for neural networks (Section 4). In Appendix A, we prove the consistency of smoothed CDFs in general settings, which supports the usage of Smooth EasyUQ, but is a result of broader and independent interest.\n# 2 Basic EasyUQ\nWe begin the section with a prelude on the evaluation of predictions in the form of full statistica distributions. Then we describe the IDR and EasyUQ techniques, and illustrate EasyUQ on th WeatherBench data from [57] and on precipitation forecasts [35]. Generally, EasyUQ depend on the availability of training data of the form\n(xi, yi), i = 1, . . . , n,\nwhere xi \u2208R is the single-valued model output and yi \u2208R is the respective real-world outcome, for i = 1, . . . , n. For subsequent discussion, we note the contrast to more elaborate, inputbased ways of uncertainty quantification that require access to the features or covariates from which the model output xi is generated. In the WeatherBench example from Fig. 1, we have training data comprising twice daily HRES forecasts and the associated observed temperatures in 2010 through 2016 as illustrated in panel c), where n = 5, 114, but we do not have access to the excessively high-dimensional input to the HRES model. In practice, one needs to find a\n(1)\npredictive distribution given the value x of the model output at hand, which may or may not be among the training values x1 \u2264\u00b7 \u00b7 \u00b7 \u2264xn, and some form of interpolation is needed, while retaining isotonicity. In panel e) of Fig. 1 we illustrate predictive CDFs when x equals equals 263, 268, and 273 degrees Kelvin, respectively. Extensions of this setting to situations where single-valued output from multiple computational models is available can be handled within the IDR framework, as we discuss below. If model output and real-world outcome are vector-valued \u2014 e.g., when temperature is predicted at multiple sites simultaneously \u2014 EasyUQ can be applied to each component independently, and the EasyUQ distributions for the components can be merged by exploiting dependence structures in the training data, based on empirical copula techniques such as the Schaake shuffle [61].\n# 2.1 Evaluating predictive distributions\nA widely accepted principle in the generation of predictive distributions is that sharpness ought to be maximized subject to calibration [25]. Maximizing sharpness requires forecasters to provide informative, concentrated predictive distributions, and calibration posits that probabilities derived from these distributions conform with actual observed frequencies. This is in line with and generalizes the classical goal of prediction intervals being as narrow as possible while attaining nominal coverage.\nA key tool for evaluating and comparing predictive distributions under this principle are proper scoring rules [28, 47], which are functions S(P, y) mapping a predictive distribution P and the outcome y to a numerical score such that\nEY \u223cP [S(P, Y )] \u2264EY \u223cP [S(Q, Y )]\nfor all distributions P, Q in a given class P. Here EY \u223cP [\u00b7] denotes the expected value of the quantity in parentheses when Y follows the distribution P. From a decision-theoretic point of view, proper scoring rules encourage truthful forecasting, since forecasters minimize their expected score if they issue predictive distributions that correspond to their true beliefs.\nfor all distributions P, Q in a given class P. Here EY \u223cP [\u00b7] denotes the expected value of the quantity in parentheses when Y follows the distribution P. From a decision-theoretic point of view, proper scoring rules encourage truthful forecasting, since forecasters minimize their expected score if they issue predictive distributions that correspond to their true beliefs. Arguably the most widely used proper scoring rules are the continuous ranked probability score (CRPS),\nCRPS(F, y) = \ufffd\u221e \u2212\u221e (F(z) \u22121{z \u2265y})2 dz,\nwhich can be applied to cumulative distribution functions (CDFs) F on the real line for which the corresponding distribution has finite first moment; and the logarithmic score for a predictive CDF F with density f,\nThe popularity of the CRPS is due to the fact that it allows arbitrary types of predictive distributions (discrete, continuous, mixed discrete-continuous), is reported in the same unit as the outcomes, and reduces to the absolute error AE(x, y) = |x \u2212y| if F assigns probability one to a point x \u2208R. The LogS is (save for change of sign) the ubiquitous loss function in maximum likelihood estimation. Closed form expressions for the CRPS and LogS are available for the most commonly used parametric distributions and have been implemented in software packages\n(2)\n(3)\n# 38]. In practice, forecast methods are compared in terms of their average score over a collection Fj, yj) for j = 1, . . . , n,\n\u00afS = 1 n n \ufffd j=1 S(Fj, yj),\nand the method achieving the lowest average score is considered superior.\n# 2.2 Basic EasyUQ: Leveraging the Isotonic Distributional Regression (IDR) technique\nIn this section, it will be instructive to think of the quantities involved as random variables, which we emphasize by using upper case in the notation. If model output X serves to predict a future quantity Y , then one typically assumes that Y tends to attain higher values as X increases; in fact, the isotonicity assumption can be regarded as a natural requirement for X to be a useful forecast for Y . Isotonic Distributional Regression (IDR) is a recently introduced, nonparametric method for estimating the conditional distributions of a real-valued outcome Y given a covariate or feature vector X from a partially ordered space under general assumptions of isotonicity [35]. EasyUQ leverages the basic special case of IDR where X is the single-valued model output at hand. We review the construction and the most relevant properties of IDR for uncertainty quantification; for detailed formulations and proofs we refer to the original paper. Formally, EasyUQ assumes that the conditional distributions of the outcome Y given the model output X, which we identify with the CDFs Fx(y) = P(Y \u2264y | X = x), are increasing in stochastic order [65] in x, i.e., Fx(y) \u2265Fx\u2032(y) for all y \u2208R if x \u2264x\u2032, or equivalently qx(\u03b1) \u2264 qx\u2032(\u03b1) for all \u03b1 \u2208(0, 1), where qx(\u03b1) = F \u22121 x (\u03b1) is the conditional lower \u03b1-quantile. In plain words, the probability of the outcome Y exceeding any threshold y increases with the model output x. Isotonicity in this sense is a natural assumption that one expects to hold, to a reasonable degree of approximation, in many types of applications. An important exception arises under location-scale families. Specifically, the arguments in the proof of Proposition 1 in Gneiting and Vogel [30] imply that isotonicity is violated when the true predictive distributions come from a location-scale family with varying scale.5 However, the practical impact of this result is limited, due to the fact that in typical practice the scale parameter varies mildly only [29] and violations remain minor. Crucially, estimators that enforce isotonicity tend to be superior to estimators that do not, even when the key assumption is violated, provided the deviation from isotonicity remains modest. For an illustration in a simulation setting see the non-isotonic scenario (25) in Table 1 of Henzi et al. [35], where IDR retains acceptable performance relative to its competitors, despite the key assumption being violated. For a rigorous result, Thm. 7 of El Barmi and Mukerjee [20] demonstrates that, in the special case of discrete model output, EasyUQ has smaller large sample estimation error than non-isotonic alternatives even under mild violations of the isotonicity assumption. EasyUQ assumes isotonicity with respect to the usual stochastic order. In situations where this assumption is severely violated it may be worthwhile to consider isotonicity with respect to a weaker requirement for distributions to be ordered. An analogous method to IDR under\n5For example, if F1 = L(Y |X = x1) = N(\u00b51, \u03c32 1) and F2 = L(Y |X = x2) = N(\u00b52, \u03c32 2), where x1 \u0338= x2 and \u03c31 \u0338= \u03c32, then F1 and F2 are incomparable in stochastic order, whence isotonicity is violated. However, if \u03c31 and \u03c32 are close to each other, the CDFs of F1 and F2 cross in the far (left or right) tail only ([30], proof of Proposition 1), so violations remain minor.\nncreasing concave and convex stochastic ordering constraints has been introduced by [34]. An extension of EasyUQ in this direction is left for future work. To estimate conditional CDFs under the given stochastic order constraints from training data of he form (1), we define\n\u02c6Fxj(y) = min k=1,...,j max l=j,...,n 1 l \u2212k + 1 l \ufffd i=k 1{yi \u2264y}, j = 1, . . . , n.\nAt any single threshold y, the computation can be performed efficiently in O(n log(n)) complexity with the well-known pool-adjacent-violators (PAV) algorithm. Since the loss function in (4) is constant for y in between the unique values \u02dcy1 < \u00b7 \u00b7 \u00b7 < \u02dcyk of y1, . . . , yn, is suffices to compute (5) at the unique values, for which efficient recursive algorithms are available [34]. An estimate \u02c6Fx for the conditional CDF at model output x \u2208(xi, xi+1) is obtained by pointwise linear interpolation in x. For x \u2264x1 and x \u2265xn, we use \u02c6Fx1 and \u02c6Fxn, respectively. The EasyUQ conditional CDFs are step functions that correspond to discrete predictive distributions with mass at (a subset of) the unique values \u02dcy1 < \u00b7 \u00b7 \u00b7 < \u02dcyk only. The IDR approach has desirable properties that make it suitable for uncertainty quantification. By (4), the EasyUQ CDFs depend on the order of x1, . . . , xn only, but not on their values, hence the solution is invariant under strictly monotone transformations of the model output, except for interpolation choices when x \u0338\u2208{x1, . . . , xn}. Furthermore, the EasyUQ distributions are in-sample calibrated ([35], Thm. 2). Importantly, a comparison of the loss function in (4) and the definition of the CRPS in (2) reveals that EasyUQ minimizes the CRPS over all conditional distributions satisfying the stochastic order constraints. Furthermore, the EasyUQ solution is universal, in the sense that it is simultaneously in-sample optimal with respect to comprehensive classes of proper scoring rules in terms of conditional CDFs or conditional quantiles, such as, e.g., weighted forms of the CRPS with the Lebesgue measure in (2) replaced by a general measure ([35], Thm. 2). Other approaches to estimating conditional CDFs, e.g., based on parametric models, nearest neighbors, or kernel regression, do not share the universality property, and estimates change depending on the loss function at hand. In Fig. 1 we illustrate EasyUQ predictive CDFs in the empirical WeatherBench example. Simulation examples, to which we turn now, have the advantage of the true conditional CDFs being available, so we can compare to them. Figure 2 illustrates the construction of the discrete EasyUQ predictive distributions step by step, based on a training archive of the form (1) with n = 500 simulated from a bivariate distribution, where the model output X is uniform on (0, 10) and the outcome Y satisfies\nEasyUQ converts the single-valued model output X into conditional predictive CDFs close to the right-skewed true ones. Indeed, IDR, and hence, EasyUQ are asymptotically consistent: As the training archive size n grows, the estimated EasyUQ CDFs converge to the true conditional CDFs [20, 35, 50]. Of particular relevance to EasyUQ is the following recent result ([33], Thm. 5.1): If x1, . . . , xn themselves are not fixed but predictions from a statistical model that is estimated on\n(4)\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c7f2/c7f249cf-9043-4f11-a138-e01753b96811.png\" style=\"width: 50%;\"></div>\nFigure 2: Computation of EasyUQ predictive distributions from a training archive of n = 500 model output\u2013outcome pairs simulated according to (6). a) The minimizer \u02c6Fx(y) of (5) at y = 7, interpolated linearly in x. The jiggled dots show the indicators 1{yi \u2264y}. b) EasyUQ conditional CDFs \u02c6Fx (step functions) and the respective true conditional CDFs (smooth curves) at selected values of x. The vertical line at y = 7 highlights the values marked in the top panel. c) Training data (xi, yi) for i = 1, . . . , n, and conditional quantile curves \u02c6qx(p) resulting from inversion of the EasyUQ CDFs \u02c6Fx. The lowest and highest quantile curves (levels 0.05 and 0.95) together delineate equal-tailed 90% prediction intervals.\nthe same training data then IDR is a consistent estimator of the true conditional distributions, subject to mild regularity conditions. The basic EasyUQ method extends readily to vector-valued model output. If x1, . . . , xn are vectors in a space with a partial order \u2aaf, then the same approach (4) applies with the usual inequality \u2264replaced by the partial order \u2aaf. This allows more flexibility in the sense that distributions Fx and Fx\u2032 are allowed to be incomparable in stochastic order if x and x\u2032 are incomparable in the partial order. A prominent example concerns ensemble weather forecasts [27, 44, 52], where a numerical model is run several times under distinct conditions, and the partial order \u2aafthat underlies IDR can be tailored to this setting [35]. To summarize, the basic EasyUQ method provides a data driven, theoretically principled, and fully automated approach to uncertainty quantification that is devoid of any need for implementation choices. Based on training data, EasyUQ converts single-valued model output into calibrated predictive distributions that reflect the uncertainty in the model output and training data, as opposed to tuning intense methods, where uncertainty quantification might reflect implementation decisions and user choices. The EasyUQ predictive solution is invariant under strictly monotone transformations of the model output, it is in-sample calibrated, it is in-sample optimal with respect to comprehensive classes of loss functions, and subject to mild conditions it is asymptotically consistent both for output from deterministic models, and output from statistical or machine learning models, even when the model is learned on the same data.6\n# 2.3 Illustration on WeatherBench challenge\nIn a notable development, WeatherBench [57] introduces a benchmark dataset for the comparison of purely data driven and numerical weather prediction (NWP) model based approaches to weather forecasting. Following up on the illustration in Fig. 1, where we consider a grid point at (latitude, longitude) values of (53.4375, 16.875), we now provide background information and quantitative results at grid points worldwide. Our experiments are based on the setup in WeatherBench and consider forecasts of upper air temperature at a vertical level of 850 hPa pressure. The forecasts are issued twice daily at 00 and 12 Coordinated Universal Time (UTC) at lead times of three and five days ahead. The single-valued HRES forecast is from the high-resolution model operated by the European Centre for Medium-Range Weather Forecasts (ECMWF), which represents the physics and chemistry of the atmosphere and is generally considered the leading global NWP model. To reduce the amount of data, WeatherBench regrids the HRES model output and the respective outcomes, which originally are on a 0.25 degree latitude\u2013longitude grid (72 \u00d7 144), to coarser resolution (32 \u00d7 64) via bilinear interpolation. The CNN forecast also is single-valued; it is purely data driven and based on a Convolutional Neural Network (CNN), with trained weights being available in WeatherBench. The single-valued Climatology forecast is the best performing baseline model from WeatherBench; it is obtained as the arithmetic mean of the observed upper air temperature in the training data, stratified by 52 calender weeks. Conformal Prediction (CP, [72, 75]) is an increasingly popular, general technique for the construction of predictive distributions from single-valued model output. For a comparison with EasyUQ,\nIn a notable development, WeatherBench [57] introduces a benchmark dataset for the comparison of purely data driven and numerical weather prediction (NWP) model based approaches to weather forecasting. Following up on the illustration in Fig. 1, where we consider a grid poin at (latitude, longitude) values of (53.4375, 16.875), we now provide background information and quantitative results at grid points worldwide.\nwe employ CP in the form of the studentized Least Squares Prediction Machine (LSPM, [72], Algorithm 7.2) with the single-valued model output as sole covariate. We consider CP to be a key competitor, as it is an output-based method that shares desirable properties of EasyUQ. Specifically, the LSPM supplements a least squares based point prediction of the outcome with a conformal predictive system for uncertainty quantification. Based on training data (xi, yi), where i = 1, . . . , n \u22121, Algorithm 7.2 returns a fuzzy predictive distribution ([72], eq. (7.7)) that is defined in terms of quantities C1, . . . , Cn\u22121. Comparative evaluation requires a crisp predictive distribution, for which we use the empirical distribution of C1, . . . , Cn\u22121, which adheres to the bounds imposed by the fuzzy distribution.7 For moderate to large training sets and x the value of the model output at hand, Ci typically is very close to \u02c6y + yi \u2212\u02c6yi, where \u02c6y and \u02c6yi are least squares point predictions based on x and xi, respectively ([72], Section 7.3.4). Finally, we consider the state-of-the-art approach to uncertainty quantification in weather prediction, namely, ensemble forecasts [27, 44, 52], which are input-based methods. Specifically, we use the world leading ECMWF Integrated Forecast System (IFS, https://www.ecmwf.int/en/ forecasts), which comprises 51 NWP runs, namely, a control run and 50 perturbed members [49]. The control run is based on the best estimate of the initial state of the atmosphere, and the perturbed members start from slightly different states that represent uncertainty. Even a single NWP model run, such as the HRES run, is computationally very expensive, and computing power is the limiting factor to improving model resolution. Despite having coarser resolution, an ensemble typically requires 10 to 15 times more computing power than a single run [3]. In contrast, the implementation of the output-based CP and EasyUQ methods is fast, with hardly any resources needed beyond a single NWP model run. To compare CP and EasyUQ predictive CDFs to the respective single-valued forecasts we use the CRPS from (2) and recall that for single-valued forecasts the mean CRPS reduces to the mean absolute error (MAE). As evaluation period, we take calendar years 2017 and 2018; for estimating the CP and EasyUQ predictive distributions, we use training data from calendar years 2010 through 2016 and proceed grid point by grid point. The corresponding results are provided in Table 1. Not surprisingly, the ECMWF ensemble forecast has the lowest mean CRPS. However, CP and EasyUQ based on the HRES model output results in promising CRPS values, even though the methods require considerably less computing time and resources. The CP and EasyUQ predictive distributions show nearly identical predictive performance. To understand this behavior, we note that in the case of temperature, Gaussian predictive distributions with fixed variance typically are very adequate (see, e.g., [29], Table 3). In this light, key requirements of CP in the form of the LSPM (namely, fixed spread and fixed shape of the predictive distributions) and EasyUQ (namely, isotonicity) are reasonably met. While EasyUQ generates predictive distribution that vary in spread and shape, the variations remain modest (Fig. 1c\u2013f), and the CP distributions, which essentially are translates of each other, are competitive. The subsequent case study turns to a weather variable that is not covered by the WeatherBench challenge, but serves to illuminate and highlight difference between the CP and EasyUQ techniques.\n7Here and in Section 3.4, we adopt the convention in Vovk et al. ([72], Section 7.2) and assume that the size of the training set is n \u22121, rather than n, to allow for direct references to material therein. The respective crisp CDF is given by F(y) = i/n for y \u2208(C(i), C(i+1)) and i = 0, 1, . . . , n \u22121, and F(y) = i\u2032\u2032/n for y = C(i) and i = 1, . . . , n \u22121, where C(0) = \u2212\u221e, C(1) \u2264\u00b7 \u00b7 \u00b7 \u2264C(n\u22121) are the order statistics of C1, . . . , Cn\u22121, C(n) = \u221eand i\u2032\u2032 = max{ j : C(j) = C(i)}. For related discussion and alternative choices of a crisp CDF that is compatible with the fuzzy CDF, see Section 2 of Bostr\u00a8om et al. [7] and Section 5 of Vovk et al. [74].\nTable 1: Predictive performance in terms of mean CRPS for WeatherBench forecasts of upper air temperature at lead times of three and five days, in degrees Kelvin. The evaluation period comprises calendar years 2017 and 2018. CP and EasyUQ generate predictive CDFs that are fitted at each grid point individually, based on training data from 2010 through 2016. Forecasts are issued twice daily, and scores are averaged over 32 \u00d7 64 grid points, for a total of 2,990,080 forecast cases.\nForecast\nCRPS\nType\nMethod\nThree Days\nFive Days\nSingle-valued\nClimatology\n2.904\n2.904\nCNN\n2.365\n2.782\nHRES\n0.998\n1.543\nDistributional\nCP on Climatology\n2.055\n2.055\nCP on CNN\n1.673\n1.955\nCP on HRES\n0.731\n1.123\nDistributional\nEasyUQ on Climatology\n2.038\n2.038\nEasyUQ on CNN\n1.671\n1.949\nEasyUQ on HRES\n0.736\n1.122\nDistributional\nECMWF Ensemble\n0.696\n0.998\n# 2.4 Illustration on precipitation forecasts\nPrecipitation accumulation is generally considered the \u201cmost difficult weather variable to forecast\u201d [19]. Indeed, the uncertainty quantification for deterministic forecasts of precipitation is more challenging than for temperature, since precipitation accumulation follows a mixture distribution with a point mass at zero \u2014 for no precipitation \u2014 and a continuous part on the positive real numbers. Applying CP without corrections is bound to transfer mass to negative values of precipitation accumulation. Taking advantage of knowledge about the outcome distribution, a natural remedy is to censor at zero and use the CDF\nin lieu of F.8 In contrast, the EasyUQ predictive distributions reflect the nonnegativity of the outcomes in the training data, without any need for adaptation. We now investigate the performance of CP and EasyUQ within the experimental setup from Henzi et al. [35], taking forecasts and observations of 24-hour accumulated precipitation from 6 January 2007 through 1 January 2017 at Frankfurt airport, Germany. Just as in the WeatherBench example, we consider a weekly climatology, the HRES forecast, and the 51 member NWP ensemble from ECMWF. The weekly climatology is computed over the period 2007 to 2014, which is the same period that is used for CP and EasyUQ training. The evaluation period\nn lieu of F.8 In contrast, the EasyUQ predictive distributions reflect the nonnegativity of the outcomes in the training data, without any need for adaptation.\nWe now investigate the performance of CP and EasyUQ within the experimental setup from Henzi et al. [35], taking forecasts and observations of 24-hour accumulated precipitation from 6 January 2007 through 1 January 2017 at Frankfurt airport, Germany. Just as in the WeatherBench example, we consider a weekly climatology, the HRES forecast, and the 51 member NWP ensemble from ECMWF. The weekly climatology is computed over the period 2007 to 2014, which is the same period that is used for CP and EasyUQ training. The evaluation period\nTable 2: Predictive performance in terms of mean CRPS for forecasts of daily precipitation accumulation at Frankfurt airport at lead times from one to five days, in millimeter. CP and EasyUQ generate predictive CDFs based on training data from 2007 through 2014. The evaluation period comprises calendar years 2015 and 2016.\nForecast\nCRPS\nType\nMethod\n1 Day\n2 Days\n3 Days\n4 Days\n5 Days\nSingle-valued\nClimatology\n2.187\n2.187\n2.187\n2.187\n2.187\nHRES\n1.125\n1.294\n1.412\n1.478\n1.686\nDistributional\nCP on Climatology\n1.382\n1.382\n1.382\n1.382\n1.382\nCP on HRES\n0.886\n0.966\n1.063\n1.081\n1.129\nCensored CP on Climatology\n1.324\n1.324\n1.324\n1.324\n1.324\nCensored CP on HRES\n0.850\n0.925\n1.031\n1.050\n1.100\nDistributional\nEasyUQ on Climatology\n1.242\n1.242\n1.242\n1.242\n1.242\nEasyUQ on HRES\n0.732\n0.803\n0.876\n0.945\n1.001\nDistributional\nECMWF Ensemble\n0.752\n0.847\n0.856\n0.918\n0.981\ncomprises calendar years 2015 and 2016. Table 2 shows the mean CRPS over the evaluation period for the various types of forecasts at lead times from one to five days. Evidently, the climatological forecasts, along with their scores, do not depend on the lead time. In contrast to the WeatherBench temperature example, EasyUQ outperforms CP for both Climatology and the HRES model output, and at all lead times. While censoring improves the distributional forecasts from CP, the performance gap to EasyUQ remains pronounced. EasyUQ on the HRES model output even outperforms the raw ECMWF ensemble at lead times of one and two days.9 Figure 3 provides a graphical comparison of CP on HRES, Censored CP on HRES, EasyUQ on HRES, and ECMWF ensemble forecasts at small (x = 0.38), moderate (x = 3.40), and large (x = 11.93) values of the HRES model output x. We see that the CP predictive distributions essentially are translates of each other, with mass potentially being transfered to negative values of precipitation accumulation, and censoring shifting any such mass to zero. In contrast, the ECMWF ensemble and EasyUQ distributions do not have mass at negative values, and they vary in shape and scale. However, while the ECMWF ensemble tends to show forecast distributions that are too narrow, as frequently observed in practice [27] and illustrated by the right-hand example, the EasyUQ distributions, which are based on the single-valued HRES forecast only, show what appears to be adequate spread. Remarkably, and unlike any other method that we are\n9This is largely due to the fact that gridded ensemble predictions are compared against station observations. To counter these effects, the ensemble forecast itself can be subjected to statistical postprocessing, i.e., the application of statistical methods to correct for biases and dispersion errors [29, 55]. Parametric methods based on distributional regression [48, 62] model the distribution of precipitation accumulation with censored logistic or censored generalized extreme value distributions. An alternative approach is taken in Bayesian model averaging [67], which posits separate parametric forms for the probability of zero precipitation and the density at positive amounts. Evidently, discrete-continuous mixture distributions considerably complicate model building and estimation, and great efforts are to be undertaken to find suitable parametric families for specific weather variables. For a detailed performance comparison on the data on hand see Fig. 5 of Henzi et al. [35], whose study also includes versions of IDR with multivariate covariates derived from the full ECMWF ensemble and suitable partial orders on them, an option alluded to at the end of Section 2.2. These yield improvements compared to both the raw ensemble forecast and EasyUQ on HRES, at the price of higher conceptual complexity, higher computational costs, and the need for access to the full ensemble, rather than single-valued HRES model output.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d53f/d53fa698-bfe4-44ee-84bc-c1ba76452960.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: One-day ahead forecasts of daily precipitation accumulation at Frankfurt airport valid 23 January 2015 (left, HRES model output x equal to 0.38, as indicated by the blue cross), 14 January 2015 (middle, x = 3.40), and 21 February 2016 (right, x = 11.93), in millimeter. The predictive distributions for CP on HRES, Censored CP on HRES, EasyUQ on HRES, and ECMWF ensemble techniques are shown. The observed precipitation accumulation was at y = 0, y = 2, and y = 17 millimeter, respectively.</div>\naware of, EasyUQ achieves this desirable performance in its very basic form, without any need for implementation decisions, parameter tuning, or other forms of adaptation and intervention.\n# 3 Smooth EasyUQ\nEasyUQ provides discrete predictive distributions with positive probability mass at the outcomes from the training archive. For genuinely discrete outcomes, the variable of interest attains a small number of unique values only, and this is a desirable property. For genuinely continuous variables, it is preferable to use continuous predictive distributions. We now describe the Smooth EasyUQ technique, which turns the discrete basic EasyUQ CDFs into continuous Smooth EasyUQ CDFs with Lebesgue densities, while preserving isotonicity. To achieve this, Smooth EasyUQ applies kernel smoothing, which requires implementation choices, unlike basic EasyUQ that does not require any tuning. However, we provide default options.\nOur goal is to transform the discrete basic EasyUQ CDFs \u02c6Fx from (5) into smooth predictive CDFs \u02c7Fx that admit Lebesgue densities \u02c7fx, without abandoning the order relations honored by the basic technique. To this end, we define the Smooth EasyUQ CDF as\n(7)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0219/02199da3-61c7-4e9b-9b8d-b60487388d1e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Smooth EasyUQ predictive density (8) in the WeatherBench example from Fig. 1f at HRES model output x equal to 268 degrees Kelvin. The vertical bars show the weight w1(x), . . . , wk(x) that the discrete EasyUQ distribution \u02c6Fx assigns to the unique values \u02dcy1 < \u00b7 \u00b7 \u00b7 < \u02dcyk of the outcomes in the training set, where k = 76.</div>\nwhere Kh(u) = (1/h) \u03ba(u/h) for a smooth probability density function or kernel \u03ba, such as a standardized Gaussian or Student-t density, with bandwidth h > 0. While the convolution approach in (7) is perfectly general for the smoothing of CDFs, we henceforth focus the presentation on EasyUQ. The choice of the kernel and the bandwidth are critical, and we tend to their selection in the next section, where we introduce multiple one-fit grid search as a computationally efficient alternative to cross-validation. For now, recall that \u02c6Fx(y) from (5) is a step function with possible jumps at the unique values \u02dcy1 < \u00b7 \u00b7 \u00b7 < \u02dcyk of the outcomes y1, . . . , yn in the training set. Hence, we can write (7) as\nwhere \u02dcyk+1 = \u221e. To compute the density \u02c7fx = \u02c7F \u2032 x, we set \u02dcy0 = \u2212\u221e, note that \u02c6Fx assigns mass wj(x) = \u02c6Fx(\u02dcyj) \u2212\u02c6Fx(\u02dcyj\u22121) to \u02dcyj, and find that\n\u02c7fx(y) = k \ufffd j=1 \u02c6Fx(\u02dcyj) [Kh(y \u2212\u02dcyj) \u2212Kh(y \u2212\u02dcyj+1)] = k \ufffd j=1 wj(x) Kh(y \u2212\u02dcyj).\nIn words, the Smooth EasyUQ density \u02c7fx from (8) arises as a kernel smoothing of the discrete probability measure that corresponds to \u02c6Fx and assigns weight wj(x) to \u02dcyj. Consequently, \u02c7fx is a probability density function, \u02c7Fx is a proper CDF, and, notably, Smooth EasyUQ preserves the stochastic ordering of the basic EasyUQ estimates. In Fig. 4 we illustrate the interpretation of the Smooth EasyUQ density as a kernel smoothing of the EasyUQ point masses wj(x) on the WeatherBench example.\nSubject to mild regularity conditions, the asymptotic consistency of EasyUQ carries over to Smooth EasyUQ. To demonstrate this, we prove a general consistency theorem for estimates of conditional CDFs in Appendix A. Here, we sketch how the result applies in the special case of Smooth EasyUQ. Specifically, let \u02c7Fx;n denote the Smooth EasyUQ estimator from (7), where the basic estimate \u02c6Fx is trained on a sample of size n from a population with true conditional CDFs Fx that are H\u00a8older continuous with constants \u03b1 \u2208(0, 1] and L > 0. Suppose that the function Kh in (7) uses a kernel \u03ba with a finite absolute moment c\u03ba,\u03b1 of order \u03b1, and allow for\n(8)\nthe bandwidth hn > 0 to vary with the sample size. Crucially, we assume that the sup-error of the basic estimate \u02c6Fx at sample size n is upper bounded by \u03b5n in the asymptotic sense specified by (14); a choice of \u03b5n \u223c(log(n)/n)1/3 applies to EasyUQ, if on an interval (a, b) the covariate values x1, . . . , xn are sufficiently dense and the conditional CDFs are Lipschitz continuous in x. Then Thm. 1 implies that, for some sequence of \u03b4n > 0 converging to zero,\n\ufffd \ufffd If h\u03b1 n = O(\u03b5n) the Smooth EasyUQ CDFs converge with the same rate as the basic EasyUQ CDFs. For details, proofs, and discussion see Appendix A. In a nutshell, smoothness conditions on the true conditional CDFs are essential and unproblematic, as one should not be replacing the basic version of EasyUQ by Smooth EasyUQ in practice, unless there is subject matter indication of an absolutely continuous distribution of the outcome. For instance, in the precipitation forecasting example from Section 2.4, basic EasyUQ outperforms Smooth EasyUQ and a censored version of it at all lead times, cf. Tables 2 and 4.\n# 3.2 Choice of kernel and bandwidth: Multiple one-fit grid search\nIn order to compute the Smooth EasyUQ density \u02c7fx from (8) one needs to choose a kernel \u03ba and a bandwidth h > 0, to yield a mixture of translates of the density Kh(u) = (1/h) \u03ba(u/h). While there is a rich literature on bandwidth selection for kernel density estimation and kernel regression (see, e.g., [39, 66]), caution is needed when applying established approaches to Smooth EasyUQ, due to the fact that smoothing is applied to estimated conditional CDFs rather than raw data.\nFurthermore, while the extant literature focuses on bandwidth selection for a fixed kernel, approaches of this type are restrictive for our purposes. The Smooth EasyUQ density from (8) inherits the tail behavior of the kernel \u03ba, and so the properties of the kernel are of critical importance to the quality of the uncertainty quantification in the tails of the conditional distributions. To allow for distinct tail behaviour, we use the Student-t family and set K\u03bd,h(u) = (1/h) \u03ba\u03bd(u/h), where\nis a standardized Student-t probability density function with \u03bd > 0 degrees of freedom. It is well known that the Student-t distribution has a finite first moment if \u03bd > 1 and a finite variance if \u03bd > 2. In the limit as \u03bd \u2192\u221e, we find that \u03ba\u03bd(y) \u2192\u03ba\u221e(y) uniformly in y, where \u03ba\u221e(y) = (2\u03c0)\u22121/2 exp(\u2212y2/2) is the standard Gaussian density function, so the ubiquitous Gaussian kernel emerges as a limit case in (10). Turning to the choice of the tail parameter \u03bd \u2208(0, \u221e] and the bandwidth h > 0, we begin by discussing the latter. A popular approach for bandwidth selection, both in kernel regression and kernel density estimation, is leave-one-out cross-validation. Here the target criterion in terms of the bandwidth is\n(9)\n(10)\n(11)\nwhere S is a proper scoring rule, and \u02c7Fxi,\u2212i,h is the Smooth EasyUQ CDF with covariate xi and bandwidth h, estimated with all data from (1) except for the i-th instance. The optimization of the target criterion (11) either uses the CRPS as loss function S, as (implicitly) suggested for the estimation of conditional CDFs and quantile functions (see, e.g., [8], p. 801 and [45], p. 58) and yielding a target that is asymptotically equivalent to the integrated mean squared error ([35], Section S4), or the LogS, as proposed for ensemble smoothing [10]. We take the latter as default choice, since the LogS is much more sensitive to the choice of the bandwidth h than the more robust CRPS. However, there are a number of caveats. Empirical data typically are discrete to some extent, and might contain ties in the response variable, such as in the setting of Fig. 4, where there are only m = 76 unique values among the outcomes y1, . . . , yn, even though \u02c7fx is estimated from a training archive of size n = 5, 114. In such cases, the optimal cross-validation bandwidth under the LogS may degenerate to h = 0, a problem that is also known in density estimation ([66], pp. 51\u201355), in the estimation of Student-t regression models [21] and, in related form, in performance evaluation for forecast contests [40, 54]. Another issue is that leave-one-out crossvalidation is computationally expensive, as for each value of h it requires the computation of n distinct IDR solutions. While a potential remedy is to remove a higher percentage of observations in each cross-validation step, we use a considerably faster approach, which we term one-fit grid search, that addresses both issues simultaneously. One-fit grid search avoids repeated fits of IDR and computes EasyUQ only once, namely, on the full sample from (1). Specifically, given any fixed kernel \u03ba, one-fit grid search finds the optimal bandwidth h in terms of the target criterion\nOne-fit grid search avoids repeated fits of IDR and computes EasyUQ only once, namely, on the full sample from (1). Specifically, given any fixed kernel \u03ba, one-fit grid search finds the optima bandwidth h in terms of the target criterion\nwhere \u00afFxi,\u2212i,h removes the unique value \u02dcyj = yi from the support of \u02c7Fxi in (7), by setting wj(x) in (8) to zero and rescaling the remaining weights. We choose the LogS as the default option for the loss function S in the one-fit criterion (12), and we use Brent\u2019s algorithm [9] for optimization. Effectively, one-fit grid search is a fast approximation to cross-validation, and when n is small, leave-one-out cross-validation and the original criterion in (11) can be used instead, of course. To choose a Student-t kernel, we repeat the procedure, i.e., we consider values of \u03bd \u2208{2, 3, 4, 5, 10, 20, \u221e} in (10), with \u03bd = \u221eyielding the Gaussian limit, apply one-fit grid search for each of these values, to find the respective optimal bandwidth h, and select the combination of \u03bd and h for which the target criterion (12) is smallest overall. While being highly effective in our experience, multiple one-fit grid search is a crude approach, and we encourage further development.\n# 3.3 Illustration on temperature and precipitation forecast examples\nFor an initial illustration, we return to the WeatherBench challenge and the EasyUQ density in Figs. 1f) and 4, where n = 5, 114 and m = 76, and multiple one-fit grid search with respect to the LogS yields parameter values \u03bd = \u221eand h = 0.60 in the kernel density (10). Considering the 32 \u00d7 64 = 2, 048 grid points in WeatherBench and predictions three days ahead, the value of \u03bd selected the most frequently for EasyUQ on the HRES model output, namely, 619 times, is \u03bd = 10, with a median choice of h = 0.49. For Smooth EasyUQ on Climatology and CNN \u03bd = \u221e was most frequently selected, namely 1, 391 and 1, 361 times with median choices of h = 0.85 and h = 1.04, respectively.\n(12)\nTable 3: Predictive performance in terms of mean LogS and mean CRPS for WeatherBench density forecasts of upper air temperature at lead times of three and five days, in degrees Kelvin. The evaluation period comprises calendar years 2017 and 2018. The Single Gaussian, Smooth CP, and Smooth EasyUQ methods are trained at each grid point individually, based on data from 2010 through 2016. Forecasts are issued twice daily, and scores are averaged over 32 \u00d7 64 grid points, for a total of 2,990,080 forecast cases.\nDensity Forecast\nLogS\nCRPS\nDays Ahead\nThree\nFive\nThree\nFive\nSingle Gaussian on Climatology\n2.578\n2.578\n2.060\n2.060\nSingle Gaussian on CNN\n2.413\n2.553\n1.696\n1.983\nSingle Gaussian on HRES\n1.694\n2.073\n0.748\n1.153\nSmooth CP on Climatology\n2.562\n2.562\n2.059\n2.059\nSmooth CP on CNN\n2.384\n2.519\n1.672\n1.952\nSmooth CP on HRES\n1.627\n2.007\n0.732\n1.123\nSmooth EasyUQ on Climatology\n2.540\n2.540\n2.043\n2.043\nSmooth EasyUQ on CNN\n2.375\n2.509\n1.667\n1.945\nSmooth EasyUQ on HRES\n1.640\n2.006\n0.736\n1.122\nSmoothed ECMWF Ensemble\n1.503\n1.824\n0.685\n0.990\nA very simple, frequently used reference method for converting single-valued model output into a predictive density is the Single Gaussian technique [17]. It issues a Gaussian distribution with mean equal to the single-valued model output, and a constant variance that is optimal with respect to the mean LogS on a training set, which here we take to be the same as for EasyUQ. Evidently, both Smooth EasyUQ and the Single Gaussian technique could be trained in terms of the CRPS as well. We also compare to the Smooth CP technique, which converts the discrete CP distributions to densities, as described in the next section. In Table 3, we evaluate Smooth EasyUQ, Smooth CP, and Single Gaussian density temperature forecasts in the WeatherBench setting. For evaluation, we use both the CRPS and the LogS. Throughout, Smooth EasyUQ and Smooth CP outperform the Single Gaussian method, though they do not match the performance of the smoothed ECMWF ensemble forecast, which we construct as follows. Let \u02dcz1 < \u00b7 \u00b7 \u00b7 < \u02dczk be the unique values of the ensemble members z1, . . . , zl of an ensemble forecast of size l. The smoothed ensemble CDF then is of the form (7) with mass wj = 1 l \ufffdl i=1 1(zi = \u02dczj) for j = 1, . . . , k. Interestingly, this is the same as Br\u00a8ocker\u2013 Smith smoothing of ensemble forecasts ([10], relations (19)\u2013(21)) with parameters a = 1 and r1 = r2 = s2 = 0 being fixed. However, while Br\u00a8ocker and Smith use a Gaussian kernel and optimize the bandwidth parameter only, we take a more flexible approach and consider values of \u03bd \u2208{2, 3, 4, 5, 10, 20, \u221e} for a Student-t kernel, to find the optimal \u03bd and bandwidth h in terms of the LogS. Across the 2, 048 grid points, the most frequent choice is \u03bd = 5, namely, 743 times, with a median bandwidth value of h = 0.50. While smoothing is warranted for temperature forecasts, it is problematic for forecasts of precipitation accumulation, due to the nonnegativity of the outcome and the point mass at zero. Indeed, due to the kernel smoothing, the Smooth EasyUQ and smoothed ECMWF ensemble densities have mass on the negative halfaxis, unlike the discrete (basic) EasyUQ and (raw) ECMWF dis-\nTable 4: Predictive performance in terms of mean CRPS for density forecasts of daily precipitation accumulation at Frankfurt airport at lead times from one to five days, in millimeter. CP and EasyUQ generate predictive CDFs based on training data from 2007 through 2014. The evaluation period comprises calendar years 2015 and 2016.\nDensity Forecast\n1 Day\n2 Days\n3 Days\n4 Days\n5 Days\nSingle Gaussian on HRES\n1.244\n1.380\n1.547\n1.577\n1.724\nCensored Single Gaussian on HRES\n1.013\n1.145\n1.266\n1.276\n1.401\nSmooth CP on HRES\n0.886\n0.971\n1.064\n1.087\n1.132\nCensored Smooth CP on HRES\n0.849\n0.928\n1.028\n1.052\n1.098\nSmooth EasyUQ on HRES\n0.760\n0.828\n0.901\n0.968\n1.033\nCensored Smooth EasyUQ on HRES\n0.745\n0.817\n0.893\n0.960\n1.016\nSmoothed ECMWF Ensemble\n0.762\n0.855\n0.863\n0.924\n0.986\nCensored Smoothed ECMWF Ensemble\n0.750\n0.850\n0.860\n0.921\n0.984\ntributions, which are concentrated on the nonnegative halfaxis. Nonetheless, Table 4 compares the predictive performance of Single Gaussian, Smooth CP, Smooth EasyUQ, and smoothed ECMWF ensemble forecasts in the setting of Section 2.4, in both original and censored variants. The results mirror the findings in Table 2, in that censoring yields improvement and EasyUQ outperforms CP, whereas CP outperforms the Single Gaussian technique.\n# 3.4 Computational considerations\nWe add a brief discussion of the computational complexity of output-based methods for uncertainty quantification. For this comparison, we utilize the setting of Algorithm 7.2 in Vovk et al. [72], which requires predictive distributions for m new values of x, based on a training set of size n \u22121 with instances (x1, y1), . . . , (xn\u22121, yn\u22121). We report upper estimates of the computational complexity for the Single Gaussian technique, CP, and EasyUQ, considering both training (i.e., initial operations on the training data only) and inference (i.e., operations to be repeated for each new value). For the simplistic Single Gaussian technique, training requires O(n) operations and inference is straightforward. For EasyUQ, the main effort lies in training, where the complexity is upper bounded by O(n2) operations [34]. Training the EasyUQ CDFs only on a fixed grid of ordinates guarantees a cost reduction to O(n log n) operations, and Henzi et al. [35] describe approaches based on subset aggregation that reduce the computational burden for estimation. That said, the numerical experiments in our paper use the standard implementation throughout, without exception. For inference, each new value of x requires the determination of its position within the unique values across x1, . . . , xn\u22121, followed by interpolation of the trained EasyUQ CDFs at the predecessor and successor values, at up to O(mn) operations. For CP in the form of the studentized LSPM ([72], Algorithm 7.2) essentially no training is required, but inference incurs O(mn2) operations. Residual-based approximations to CP, which are instances of split conformal predictive systems ([72], Section 7.3.4, [73]), are much faster, shift the bulk of the cost to training at O(n) operations, and yield nearly identical predictive performance to CP in our experience, except when training sets are small.\nWe add a brief discussion of the computational complexity of output-based methods for uncertainty quantification. For this comparison, we utilize the setting of Algorithm 7.2 in Vovk et al. [72], which requires predictive distributions for m new values of x, based on a training set of size n \u22121 with instances (x1, y1), . . . , (xn\u22121, yn\u22121). We report upper estimates of the computational complexity for the Single Gaussian technique, CP, and EasyUQ, considering both training (i.e., initial operations on the training data only) and inference (i.e., operations to be repeated for each new value). For the simplistic Single Gaussian technique, training requires O(n) operations and inference is straightforward.\nFor both, CP and EasyUQ, we have implemented smoothing in ways that avoid cross-validation and honor the aforementioned bounds. Smooth EasyUQ uses one-fit grid search as developed in this paper. To generate the Smooth CP densities, we use kernel smoothing with a Gaussian kernel and bandwidth chosen according to Silverman\u2019s rule of thumb [66], applied to the quantities C1, . . . , Cn\u22121 that arise for each new instance separately. In the aforementioned experiments, we generally found the computational cost of EasyUQ to be nested in between the costs of CP and residual-based approximations to CP.10 Compared to the enormous effort of running the HRES model, or even the input-based ECMWF ensemble method, which require the operational use of supercomputers, run times and computational costs for the output-based Single Gaussian, CP, and EasyUQ techniques are negligible.\n# 4 EasyUQ and neural networks\nNeural networks and deep learning techniques have enabled unprecedented progress in predictive science. However, as they \u201ccan struggle to produce accurate uncertainties estimates [. . . ] there is active research directed toward this end\u201d ([2], p. 67), which has intensified in recent years [1, 14, 18, 22, 37, 41, 42, 46, 75]. We now discuss how EasyUQ and Smooth EasyUQ can be used to yield accurate uncertainty statements from neural networks. Evidently, our methods apply in the ways described thus far, where single-valued model output is treated as given and fixed, with subsequent uncertainty quantification via EasyUQ or Smooth EasyUQ being a completely separate add-on, as illustrated on our temperature and precipitation examples. In the context of neural networks, this means that the network parameters are optimized to yield single-valued output, and only then EasyUQ gets applied. We now describe a more elaborate approach where we integrate our methods within the typical workflow of neural network training and evaluation.\n# 4.1 Integrating EasyUQ into the workflow of neural network learning and hyperparameter optimization\nNeural networks and associated methods for uncertainty quantification are developed and evaluated in well-designed workflows that involve multiple splits of the available data into training, validation, and test sets. For each split, the training set is used to learn basic neural network parameters, the validation set is used to tune hyperparameters, and the test set is used for outof-sample evaluation. Scores are then averaged over the tests sets across the splits, and methods with low mean score are preferred.\nAlgorithm 1 describes how Smooth EasyUQ can be implemented within this typical workflow of neural network learning and hyperparameter tuning. In a nutshell, we treat the kernel parameters for Smooth EasyUQ, namely, the Student-t parameter \u03bd and the bandwidth h as supplemental\n10To provide intuition about computation times, we report mean run times for the Single Gaussian technique, CP, and EasyUQ applied to the HRES forecast in the setting of Table 2, where the training set is of size 2,896 and the evaluation set of size 721. The mean run time averaged over the five lead times is 0.005 seconds for the Single Gaussian technique, 0.45 seconds for CP, and 0.085 seconds for EasyUQ. We note that the computing time for CP on CPU is 33.64 seconds, but can be reduced to 0.45 seconds on GPU. Evidently, the comparison faces the usual challenges, given that execution times depend on factors including but not limited to hardware architecture, disk speed, memory availability, and programming language and compiler used. Specific to the situation at hand, we use code in Python, R, and C++, run some functions on GPU and others on CPU, and it is unlikely that every single of our implementations, which typically are based on packages, has been coded in the most efficient way.\nAlgorithm 1 Integration of Smooth EasyUQ into the workflow of neural network training\nand hyperparameter tuning.\nThe procedure returns the mean score of the Smooth EasyUQ\npredictions across data splits.\n1: for split in mysplit do\n2:\nseparate data into training set, validation set, and test set\n3:\nfor hyperpar in myhyperpar do\n4:\nlearn neural network with hyperpar on training set\n5:\nuse neural network output to fit basic EasyUQ on training set\n6:\nuse moderated grid search to select EasyUQ parameters \u03bd and h\n7:\nsave selected (\u03bd, h) and mean score on validation set\n8:\nend for\n9:\nselect best hyperpar and associated (\u03bd, h), based on smallest mean score\n10:\nre-learn network with best hyperpar on combined training and validation sets\n11:\nuse re-learned neural network output to re-fit basic EasyUQ on combined training and\nvalidation sets\n12:\nuse Smooth EasyUQ based on re-fitted EasyUQ with best (\u03bd, h) for predictions on test\nset\n13:\nsave scores on test set\n14: end for\n15: return mean score across splits\nhyperparameters, and optimize over both the neural network hyperparameters and kernel parameters. As the evaluation occurs out-of-sample, the issues associated with the choice of the kernel parameters discussed in Section 3.2 are mitigated, unless a dataset is genuinely discrete, in which case even out-of-sample estimates of the bandwidth h can degenerate to zero, thereby indicating that smoothing is ill-advised. To handle even such ill-advised cases, we use a procedure that we call moderated grid search [76]. Specifically, we first check if using \u03bd = 2 or a Gaussian kernel results in a degeneration of the optimal bandwidth h to zero, and if so, we use the latter with bandwidth chosen according to Silverman\u2019s rule of thumb [66]. Otherwise, we consider values of \u03bd \u2208{2, 3, 4, 5, 10, 20, \u221e} in (10), with \u03bd = \u221eyielding the Gaussian limit. For each value of \u03bd, we use Brent\u2019s method [9] to optimize the log score with respect to the bandwidth h on the validation set, and choose the optimal combination of \u03bd and h. Once network hyperparameters and kernel parameters have been determined, we re-learn the neural network on the combined training and validation sets, using the optimized hyperparameters, and apply EasyUQ on the re-learned single-valued neural network output. Finally, we apply Smooth EasyUQ based on the re-learned EasyUQ solution and the selected kernel parameters, to yield density forecasts on the test set. While optimization could be performed with respect to the CRPS, the LogS, or any other suitable proper scoring rule, we follow the machine learning literature, where benchmarking is typically in terms of the LogS. The CRPS serves as an attractive alternative, much in line with recent developments in neural network training, where optimization is performed with respect to the CRPS [16, 58]. Its use becomes essential in simplified versions of Algorithm 1 that work with the discrete basic EasyUQ distributions rather than Smooth EasyUQ densities.\nWhile optimization could be performed with respect to the CRPS, the LogS, or any other suitable proper scoring rule, we follow the machine learning literature, where benchmarking is typically in terms of the LogS. The CRPS serves as an attractive alternative, much in line with recent developments in neural network training, where optimization is performed with respect to the CRPS [16, 58]. Its use becomes essential in simplified versions of Algorithm 1 that work with the discrete basic EasyUQ distributions rather than Smooth EasyUQ densities.\n# 4.2 Application in benchmark settings from machine learning\nAs noted, our intent is to compare Smooth EasyUQ in the integrated version of Algorithm 1 to extant, state of the art methods for uncertainty quantification from the statistical and machine learning literatures. The comparison is on ten datasets for regression tasks using the experimental setup proposed and developed by Hernandez-Lobato and Adams [36], Gal and Gharahmani [22], Lakshminarayanan et al. [42], and Duan et al. [18]. Characteristics of the ten datasets are summarized in Table 5, including the size of the datasets, the number of unique outcomes, and the dimension of the input space for the regression problem. Each dataset is randomly split 20 times into training (72%), validation (18%), and test (10%) sets. However, for the larger datasets, Protein and Year, the train-test split is repeated only five and a single time(s), respectively. After finding the optimal set of (hyper)parameters, methods are re-trained on the combined training and validation set (90%) and the resulting predictions are evaluated on the held-out test set (10%). We use the exact same splits as the extant literature in the implementation from https://github.com/yaringal/DropoutUncertaintyExps, and the final score is obtained by computing the average score over the splits. Following the literature, we consider four techniques for the direct generation of conditional predictive distributions that do not use neural networks, namely, a semiparametric variant of the distributional forest technique [18, 63], generalized additive models for location, scale and shape (GAMLSS, [69]), Gaussian process (GP) regression [56], and natural gradient boosting (NGBoost, [18]). We adopt the exact implementation choices of [18] for these techniques, which in some cases involve smoothing. Except for NGBoost, scores for the Year dataset are unavailable (NA), in part, because methods fail to be computationally feasible for a dataset of this size. The remaining methods considered in Table 5 are based on neural networks, and we adopt the network architectures proposed by Hernandez-Lobato and Adams [36] and Gal and Ghahramani [22]. Specifically, we use the ReLU nonlinearity and either a single or two hidden layers, containing 50 hidden units for the smaller datasets, and 100 hidden units for the larger Protein and Year datasets. To tune the network hyperparameters, namely, the regularization parameter \u03bb and the batch size, we use grid search. Thus, the nested hyperparameter selection in the Smooth EasyUQ Algorithm 1 finds a best combination of \u03bb, the batch size, \u03bd, and h by optimizing the mean LogS. Our intent is to compare EasyUQ and Smooth EasyUQ to state of the art methods for uncertainty quantification from machine learning, namely, Monte Carlo (MC) Dropout [22] and Deep Ensembles [42], which perform uncertainty quantification directly within the workflow of neural network fitting. Furthermore, these methods are input-based, i.e., they require access to, and operate on, the original covariate or feature vector. As seen in the table, the dimensionality of the input space in the benchmark problems varies between 4 and 90. In contrast, EasyUQ, CP, and the Single Gausian technique operate on the basis of the final model output only, and so can be applied without the original, potentially high-dimensional covariate or feature vector being available. For CP we adapt our previously described implementation with further refined splits into training (57.6%), calibration (14.4%), validation (18%), and test (10%) sets. Smooth CP uses the respective variant of Algorithm 1. An intermediary role between inputbased and output-based methods is assumed by the recently developed Laplace approach [37, 59], which leverages scalable Laplace approximations based on weights of the trained network. For our numerical experiments we use the laplace software library for PyTorch [15]. A critical implementation decision in the intended comparisons is the number of training epochs\n 5: Characteristics of datasets and predictive performance for competing methods of uncertainty quantification in regr\nems, in terms of the mean logarithmic score (LogS) in a popular benchmark setting from machine learning [18, 22, 36, 42\ndataset, we show size, number of unique outcomes, and dimension of the input (covariate or feature) space. Italics indicate di\nets where the number of unique outcomes is small. For each method, we report the mean LogS from the reference stated\nr details provided in Section 4.2. For each of the lower three blocks of comparable methods, the best (lowest) mean score is \nolor. Two scores are numerically infinite; missing scores are marked NA.\nMethod / Dataset\nBoston\nConcrete\nEnergy\nKin8nm\nNaval\nPower\nProtein\nWine\nYacht\nYear\nSize\n506\n1,030\n768\n8,192\n11,934\n9,568\n45,730\n1,599\n308\n515,345\nUnique Outcomes\n229\n845\n586\n8,191\n51\n4,836\n15,903\n6\n258\n89\nDimension Input Space\n13\n8\n8\n8\n16\n4\n9\n11\n6\n90\nDistributional Forest [18]\n2.67\n3.38\n1.53\n\u22120.40\n\u22124.84\n2.68\n2.59\n1.05\n2.94\nNA\nGAMLSS\n[18]\n2.73\n3.24\n1.24\n\u22120.26\n\u22125.56\n2.86\n3.00\n0.97\n0.80\nNA\nGP Regression\n[18]\n2.37\n3.03\n0.66\n\u22121.11\n\u22124.98\n2.81\n2.89\n0.95\n0.10\nNA\nNGBoost\n[18]\n2.43\n3.04\n0.60\n\u22120.49\n\u22125.34\n2.79\n2.81\n0.91\n0.20\n3.43\n40 Deep Ensembles\n[42]\n2.41\n3.06\n1.38\n\u22121.20\n\u22125.36\n2.79\n2.83\n0.94\n1.18\n3.35\n40 Laplace\n[76]\n2.65\n3.14\n1.27\n\u22121.00\nNA\n2.87\n2.90\n0.97\n1.97\n3.61\n40 Single Gaussian\n[76]\n2.78\n3.20\n1.14\n\u22121.03\n\u22125.37\n2.83\n2.93\n0.98\n2.11\n3.61\n40 Smooth CP\n[76]\n2.89\n3.14\n1.20\n\u22121.00\n\u22125.52\n2.85\n2.88\n0.97\n1.88\nNA\n40 Smooth EasyUQ\n[76]\n2.83\n3.04\n0.79\n\u22121.05\n\u22126.51\n2.77\n2.48\n0.48\n1.36\n3.24\n400 MC Dropout\n[22]\n2.46\n3.04\n1.99\n\u22120.95\n\u22123.80\n2.80\n2.89\n0.93\n1.55\n3.59\n400 Laplace\n[76]\n2.61\n3.07\n0.80\n\u22121.11\nNA\n2.83\n2.87\n1.04\n1.18\n3.61\n400 Single Gaussian [76]\n3.41\n3.32\n0.85\n\u22121.09\n\u22126.32\n2.81\n2.87\n1.38\n2.04\n3.61\n400 Smooth CP\n[76]\n2.87\n3.05\n0.83\n\u22121.09\n\u22126.65\n2.78\n2.84\n1.01\n1.03\nNA\n400 Smooth EasyUQ [76]\n2.46\n2.94\n0.55\n\u22121.13\n\u22127.51\n2.75\n2.41\n1.07\n0.85\n3.24\n2L MC Dropout\n[22]\n2.34\n2.82\n1.48\n\u22121.10\n\u22124.32\n2.67\n2.70\n0.90\n1.37\nNA\n2L Laplace\n[76]\n2.57\n2.98\n0.56\n\u22121.13\nNA\n2.76\n2.81\n1.22\n1.24\n3.60\n2L Single Gaussian\n[76]\n\u221e\n3.78\n0.74\n\u22120.96\n\u22127.19\n2.76\n2.77\n10.51\n\u221e\n3.61\n2L Smooth CP\n[76]\n2.66\n2.94\n0.63\n\u22121.18\n\u22127.33\n2.70\n2.67\n1.01\n0.74\nNA\n2L Smooth EasyUQ [76]\n2.49\n2.71\n0.36\n\u22121.21\n\u22128.20\n2.67\n2.30\n0.95\n0.50\n3.23\nin learning the neural network.11 While the original setup specifies 40 training epochs [36], MC Dropout uses 400 or, in the 2-layer configuration, 4,000 iterations [22]. Therefore, to enable proper comparison, we apply the competing methods in three distinct neural network configurations, namely, a single-layer network with 40 training epochs (prefix 40 in Tables 5 and 6), a single-layer network with 400 training epochs (prefix 400), and a 2-layer architecture with 4,000 training epochs (prefix 2L). In Tables 5 and 6, key comparisons between techniques for uncertainty quantification then are within the respective three groups of methods, for which the neural network configurations used are identical.\n# 4.3 Comparison of predictive performance\nWe assess the predictive performance of EasyUQ and Smooth EasyUQ and other methods for probabilistic forecasting and uncertainty quantification, by comparing the mean LogS in Table 5. We use the LogS from (3) in negative orientation, so smaller values correspond to better performance. Evidently, the use of the LogS, which is customary in machine learning, prevents comparisons to the basic versions of EasyUQ and CP, to which we turn in Table 6.\nA first insight from Table 5 is that, generally, the methods in the second, third, and fourth block, which are based on neural networks, perform better relative to the direct, non neural network based methods in the first block (from top to bottom). Thus, we focus attention on the comparison of distinct methods for uncertainty quantification in neural networks, namely, Deep Ensembles [42] or MC droput [22], the Laplace approach [59], the Single Gaussian technique, Smooth CP, and Smooth EasyUQ. The 2-layer architecture generally improves results, compared to using a single layer for the neural network. Smooth EasyUQ dominates the Single Gaussian and Smooth CP techniques and generally yields lower mean LogS than Deep Ensembles, MC Dropout, or the Laplace approach. In 24 of the 3 \u00d7 10 = 30 five-fold comparisons across the bottom three blocks, Smooth EasyUQ achieves or shares the top score. For eight of the ten datasets considered, the best performance across all 19 methods considered, including both neural network and nonneural network based techniques, is achieved or shared by Smooth EasyUQ under the 2-layer network architecture. While this is not an exhaustive evaluation and no single method dominates universally, we note that Smooth EasyUQ is highly competitive with state of the art techniques for uncertainty quantification from machine learning. To allow comparison with the basic form of EasyUQ, which generates discrete predictive distributions, we use Table 6 and the mean CRPS from (2) to assess predictive performance. Each of the three blocks in the table allows for a seven-way comparison between either Deep Ensembles or MC Dropout, the Laplace approach, the Single Gaussian technique, Conformal Prediction in its basic (CP) and smoothed form (Smooth CP), the basic version of EasyUQ, and Smooth EasyUQ. As noted, the Naval, Wine, and Year datasets are distinctly discrete, with 51, 6, and 89 unique outcomes, respectively. For data of this type, predictive distributions ought to be discrete. Accordingly, there are no benefits of using Smooth EasyUQ for these datasets, as compared to using basic EasyUQ, which adapts readily to discrete outcomes. In six of the 3 \u00d7 3 = 9 seven-fold comparisons for the discrete datasets, the basic version of EasyUQ achieves the lowest mean score. Across the remaining seven datasets and for all three network configurations, smoothing is beneficial, and Smooth EasyUQ outperforms the basic version of EasyUQ. In 15 of the 3 \u00d7 7 = 21 seven-fold comparisons on these datasets, Smooth EasyUQ achieves or shares the top score. All but one of the binary comparisons between Smooth CP and Smooth EasyUQ, all\n 6: Predictive performance for competing methods of uncertainty quantification in regression problems, in terms of the mean opular benchmark setting from machine learning [18, 22, 36, 42]. For each dataset, we show size, number of unique outcom nsion of the input (covariate or feature) space. Italics indicate discrete datasets where the number of unique outcomes is sm mn and Naval the mean CRPS has been multiplied by factors of 10 and 1,000, respectively. For each block of comparable m est (lowest) mean score is set in blue color. For details see Section 4.2. Method / Dataset Boston Concrete Energy Kin8nm Naval Power Protein Wine Yacht Year Size 506 1,030 768 8,192 11,934 9,568 45,730 1,599 308 515,345 Unique Outcomes 229 845 586 8,191 51 4,836 15,903 6 258 89 Dimension Input Space 13 8 8 8 16 4 9 11 6 90 40 Deep Ensembles 1.59 3.04 0.78 0.48 0.41 2.23 2.40 0.34 0.45 4.35 40 Laplace 1.71 3.02 0.45 0.49 NA 2.46 2.46 0.35 0.80 4.72 40 Single Gaussian 1.72 3.03 0.41 0.48 0.66 2.24 2.48 0.35 0.83 4.72 40 CP 1.73 3.04 0.45 0.49 0.58 2.24 2.47 0.36 0.90 NA 40 Smooth CP 1.74 3.05 0.45 0.49 0.58 2.24 2.47 0.36 0.91 NA 40 EasyUQ 1.69 2.94 0.34 0.48 0.54 2.21 2.22 0.31 0.66 4.35 40 Smooth EasyUQ 1.64 2.89 0.33 0.48 0.55 2.20 2.20 0.32 0.64 4.34 400 MC Dropout 1.56 2.79 0.37 0.48 1.22 2.21 2.40 0.35 0.57 4.73 400 Laplace 1.66 2.67 0.29 0.44 NA 2.17 2.36 0.37 0.41 4.73 400 Single Gaussian 1.61 2.72 0.29 0.44 0.27 2.17 2.36 0.38 0.41 4.73 400 CP 1.70 2.77 0.30 0.45 0.20 2.16 2.38 0.37 0.42 NA 400 Smooth CP 1.71 2.77 0.30 0.45 0.20 2.16 2.38 0.37 0.43 NA 400 EasyUQ 1.75 2.72 0.26 0.44 0.12 2.16 2.10 0.35 0.39 4.33 400 Smooth EasyUQ 1.60 2.61 0.25 0.44 0.13 2.15 2.09 0.37 0.35 4.33 2L MC Dropout 1.45 2.19 0.33 0.41 1.07 1.92 1.95 0.33 0.47 4.63 2L Laplace 1.64 2.29 0.22 0.44 NA 2.01 2.15 0.42 0.41 4.65 2L Single Gaussian 1.89 2.27 0.25 0.41 0.11 2.03 2.04 0.45 0.25 4.69 2L CP 1.70 2.47 0.24 0.42 0.11 2.02 2.02 0.38 0.36 NA 2L Smooth CP 1.71 2.48 0.24 0.42 0.11 2.03 2.02 0.38 0.36 NA 2L EasyUQ 2.07 2.40 0.24 0.42 0.03 1.98 1.83 0.42 0.30 4.30 2L Smooth EasyUQ 1.66 2.14 0.21 0.40 0.04 1.97 1.82 0.40 0.27 4.31\nbut two of the comparisons between the Single Gaussian technique and Smooth EasyUQ, all but one of the comparisons between the Laplace method and Smooth EasyUQ, and all but eight of the comparisons between Deep Ensembles or MC Dropout and Smooth EasyUQ, are in favour of the latter.\n# 5 Discussion\nIn this paper we have proposed EasyUQ and Smooth EasyUQ as general methods for the conversion of single-valued computational model output into calibrated predictive distributions, based on a training set of model output\u2013outcome pairs and a natural assumption of isotonicity. Contrary to recent comments in review articles that lament an \u201cabsence of theory\u201d ([1], p. 244) for data-driven approaches to uncertainty quantification, the basic version of EasyUQ enjoys strong theoretical support, by sharing optimality and consistency properties of the general Isotonic Distributional Regression (IDR, [35]) method. The basic EasyUQ approach is fully automated, does not require any implementation choices, and the generated predictive distributions are discrete. The more elaborate Smooth EasyUQ approach developed in this paper generates predictive distributions with Lebesgue densities, based on a kernel smoothing of the original IDR distributions, while preserving the key properties of the basic approach. Code for the implementation of IDR in Python [53] and replication material for this article is openly available [76]. The method is general, handling both discrete outcomes, with the basic technique being tailored to this setting, and continuous outcomes, for which Smooth EasyUQ is the method of choice. It applies whenever single-valued model output is to be converted into a predictive distribution, covering both the case of point forecasts, as in the WeatherBench example, and computational model output in all facets, such as in the machine learning example, where EasyUQ and Smooth EasyUQ convert single-valued neural network output into predictive distributions. Percentiles extracted from the predictive distributions can be used to generate prediction intervals. The proposed term EasyUQ stems from various desirable properties. First, the basic version of EasyUQ does not involve tuning parameters nor require user intervention. Second, EasyUQ operates on the natural, easily interpretable and communicable assumption that larger values of the computational model output yield predictive distributions that are stochastically larger. Third, EasyUQ is an output-based technique, i.e., it merely requires training data in the form of model output\u2013outcome pairs (xi, yi) as in (1), without any need to access the potentially highdimensional covariate or feature vector zi, which serves as input to the computational model that generates xi. This property is shared with the widely used Single Gaussian technique and related methods, such as the early Geostatistical Output Perturbation (GOP, [23]) approach and the Quantile Regression Averaging (QRA, [51]) method for the generation of prediction intervals. The term Conformal Prediction (CP, [46, 75, 72]) refers to a family of output-based methods that yield predictive distributions and prediction intervals that enjoy attractive out-of-sample coverage guarantees, but often entail that shape and scale of the predictive distributions do not vary with the model output. In simple problems, where predictive distributions that essentially are translates of each other are appropriate, both CP and EasyUQ perform well, and typically yield very similar predictive performance, as illustrated by the temperature example in Section 2.3. The flexibility of EasyUQ, which allows for predictive distributions that vary in shape and/or scale, subject to the isotonicity condition, materializes in more challenging problems, where predictive distributions that are translates of each other fail. While EasyUQ adapts to such settings without any need for user intervention, CP might suffer considerable loss in\npredictive performance, even if adapted manually, as exemplified in the precipitation example  Section 2.4.\nWhile adaptive variants of CP are available, their predictive performance in both simulated and real-data settings has been mixed, compared to standard variants [75]. Recently, Bostr\u00a8om et al. [7] investigated Mondrian (i.e., covariate-conditional) CP as a flexible alternative, where conformal predictive distributions are built on separate categories formed by binning covariates (in our case, the model output). This requires additional implementation decisions, namely, on the choice of the bins. Bostr\u00a8om et al. [7] take five bins with equal numbers of training instances, which improves predictive performance in their experiments. From a methodological point of view, in situations where the isotonicity assumption of IDR is met, the bin",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of uncertainty quantification in computational models that provide single-valued outputs. Previous methods have struggled to generate predictive distributions from such outputs, necessitating a breakthrough method like EasyUQ to improve predictive performance and usability in various applications.",
        "problem": {
            "definition": "The problem is to generate predictive distributions from single-valued, univariate model outputs without access to the original model inputs, which limits the effectiveness of existing uncertainty quantification methods.",
            "key obstacle": "The main challenge is the lack of methods that can effectively convert single-valued outputs into calibrated predictive distributions while maintaining the necessary statistical properties, such as isotonicity."
        },
        "idea": {
            "intuition": "The idea behind EasyUQ stems from the observation that higher model outputs should yield higher predictive distributions, reflecting a natural assumption of isotonicity in many applications.",
            "opinion": "EasyUQ is a method designed to transform real-valued model outputs into calibrated statistical distributions based solely on training data of model output-outcome pairs.",
            "innovation": "The primary innovation of EasyUQ lies in its ability to produce discrete predictive distributions that are optimal in finite samples, without requiring tuning parameters or access to the original model inputs."
        },
        "method": {
            "method name": "Easy Uncertainty Quantification",
            "method abbreviation": "EasyUQ",
            "method definition": "EasyUQ is a technique that generates calibrated predictive distributions from single-valued model outputs using a training set of model output-outcome pairs, enforcing stochastic monotonicity.",
            "method description": "EasyUQ transforms single-valued outputs into discrete predictive distributions that are calibrated and optimal, based on training data.",
            "method steps": "1. Gather training data of model output-outcome pairs. 2. Apply the Isotonic Distributional Regression (IDR) technique to estimate conditional distributions. 3. Ensure the predictive distributions satisfy isotonicity. 4. Generate predictive distributions for new model outputs.",
            "principle": "The effectiveness of EasyUQ is based on the principle of isotonicity, which ensures that larger model outputs correspond to larger predictive distributions, thus preserving the order of uncertainty."
        },
        "experiments": {
            "evaluation setting": "The experiments utilize datasets from WeatherBench for temperature forecasts and historical precipitation data, comparing EasyUQ against existing methods like conformal prediction and ensemble forecasts.",
            "evaluation method": "Performance is assessed using the Continuous Ranked Probability Score (CRPS) and Logarithmic Score (LogS) across various lead times and datasets, with comparisons made to baseline and state-of-the-art methods."
        },
        "conclusion": "The experiments demonstrate that EasyUQ outperforms existing methods in terms of predictive accuracy and computational efficiency, validating its effectiveness as a robust tool for uncertainty quantification in various modeling contexts.",
        "discussion": {
            "advantage": "EasyUQ provides a fully automated, user-friendly approach to uncertainty quantification without requiring tuning parameters, making it accessible for practitioners.",
            "limitation": "The method may face challenges in scenarios where the isotonicity assumption does not hold strongly, potentially affecting predictive performance.",
            "future work": "Future research could explore adaptive versions of EasyUQ for more complex modeling scenarios, as well as its integration into neural network workflows for enhanced uncertainty quantification."
        },
        "other info": {
            "info1": "EasyUQ can handle both discrete and continuous outcomes, with the basic method tailored for discrete outcomes and Smooth EasyUQ for continuous outcomes.",
            "info2": {
                "info2.1": "The paper includes code for the implementation of IDR and EasyUQ methods in Python.",
                "info2.2": "The methods are applicable to a wide range of computational models, including those used in machine learning and statistical forecasting."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "6",
            "key information": "EasyUQ is a technique that generates calibrated predictive distributions from single-valued model outputs using a training set of model output-outcome pairs, enforcing stochastic monotonicity."
        },
        {
            "section number": "6.1",
            "key information": "The effectiveness of EasyUQ is based on the principle of isotonicity, which ensures that larger model outputs correspond to larger predictive distributions, thus preserving the order of uncertainty."
        },
        {
            "section number": "7.1",
            "key information": "The method may face challenges in scenarios where the isotonicity assumption does not hold strongly, potentially affecting predictive performance."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore adaptive versions of EasyUQ for more complex modeling scenarios, as well as its integration into neural network workflows for enhanced uncertainty quantification."
        },
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of uncertainty quantification in computational models that provide single-valued outputs, which is critical for developing reliable AI systems."
        }
    ],
    "similarity_score": 0.6613868118289371,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Easy Uncertainty Quantification (EasyUQ)_ Generating Predictive Distributions from Single-valued Model Output.json"
}