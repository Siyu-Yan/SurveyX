{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2111.01361",
    "title": "Outlier-Robust Optimal Transport: Duality, Structure, and Statistical Analysis",
    "abstract": "The Wasserstein distance, rooted in optimal transport (OT) theory, is a popular discrepancy measure between probability distributions with various applications to statistics and machine learning. Despite their rich structure and demonstrated utility, Wasserstein distances are sensitive to outliers in the considered distributions, which hinders applicability in practice. We propose a new outlier-robust Wasserstein distance W\u03b5 p which allows for \u03b5 outlier mass to be removed from each contaminated distribution. Under standard moment assumptions, W\u03b5 p is shown to achieve strong robust estimation guarantees under the Huber \u03b5-contamination model. Our formulation of this robust distance amounts to a highly regular optimization problem that lends itself better for analysis compared to previously considered frameworks. Leveraging this, we conduct a thorough theoretical study of W\u03b5 p, encompassing robustness guarantees, characterization of optimal perturbations, regularity, duality, and statistical estimation. In particular, by decoupling the optimization variables, we arrive at a simple dual form for W\u03b5 p that can be implemented via an elementary modification to standard, duality-based OT solvers. We illustrate the virtues of our framework via applications to generative modeling with contaminated datasets.",
    "bib_name": "nietert2023outlierrobustoptimaltransportduality",
    "md_text": "# Outlier-Robust Optimal Transport: Duality, Structure, and Statistical Analysis\n# Sloan Nietert\u2217, Rachel Cummings\u2020, and Ziv Goldfeld\u2217 March 2, 2023\n28 Feb 2023\n# Abstract\nThe Wasserstein distance, rooted in optimal transport (OT) theory, is a popular discrepancy measure between probability distributions with various applications to statistics and machine learning. Despite their rich structure and demonstrated utility, Wasserstein distances are sensitive to outliers in the considered distributions, which hinders applicability in practice. We propose a new outlier-robust Wasserstein distance W\u03b5 p which allows for \u03b5 outlier mass to be removed from each contaminated distribution. Under standard moment assumptions, W\u03b5 p is shown to achieve strong robust estimation guarantees under the Huber \u03b5-contamination model. Our formulation of this robust distance amounts to a highly regular optimization problem that lends itself better for analysis compared to previously considered frameworks. Leveraging this, we conduct a thorough theoretical study of W\u03b5 p, encompassing robustness guarantees, characterization of optimal perturbations, regularity, duality, and statistical estimation. In particular, by decoupling the optimization variables, we arrive at a simple dual form for W\u03b5 p that can be implemented via an elementary modification to standard, duality-based OT solvers. We illustrate the virtues of our framework via applications to generative modeling with contaminated datasets.\n# 1 Introduction\nDiscrepancy measures between probability distributions are a fundamental constituent of statistical inference, machine learning, and information theory. Among many such measures, Wasserstein distances (Villani, 2003) have recently emerged as a tool of choice for many applications. Specifically, for p \u2208[1, \u221e) and a pair of probability measures \u00b5, \u03bd on a metric space (X, d), the p-Wasserstein distance between them is1\n\u2217Cornell University \u2020Columbia University 1For p = \u221e, we set W\u221e(\u00b5, \u03bd) := inf\u03c0\u2208\u03a0(\u00b5,\u03bd) \u2225d\u2225L\u221e(\u03c0).\nwhere \u03a0(\u00b5, \u03bd) is the set of couplings for \u00b5 and \u03bd. The popularity of these metrics stems from a myriad of desirable properties, including rich geometric structure, metrization of the weak topology, robustness to support mismatch, and a convenient dual form. Modern applications thereof include generative modeling (Arjovsky et al., 2017; Gulrajani et al., 2017; Tolstikhin et al., 2018), domain adaptation (Courty et al., 2014, 2016), and robust optimization (Esfahani and Kuhn, 2018; Blanchet et al., 2018; Gao and Kleywegt, 2016). Despite their advantages, Wasserstein distances suffer from sensitivity to outliers due to the strict marginal constraints, as even a small outlier mass can contribute greatly to the distance. This has inspired a recent line of work into outlier-robust OT (Balaji et al., 2020; Mukherjee et al., 2021; Le et al., 2021), which relaxes the marginal constraints in various ways. These build upon the theory of unbalanced OT (Piccoli and Rossi, 2014; Chizat et al., 2018a,b; Liero et al., 2018; Schmitzer and Wirth, 2019) that quantifies the cost-optimal way to transform one measure into another via a combination of mass variation and transportation. We propose a new framework for outlier-robust OT that arises as the solution to a principled robust approximation problem. We conduct an in-depth theoretical study of the proposed robust distance, encompassing formal robustness guarantees, duality, characterization of primal minimizers / dual maximizers, regularity, and empirical convergence rates.\n# 1.1 Contributions\nWe introduce and study the \u03b5-outlier-robust Wasserstein distance defined by \ufffd \ufffd\nwhere \u00b5\u2032 and \u03bd\u2032 are positive measures, \u2225\u00b7 \u2225TV is the total variation (TV) norm, and \u2264denotes setwise inequality when appropriate. The minimization over \u00b5\u2032, \u03bd\u2032 allows outliers occupying less than fraction \u03b5 of probability mass to be omitted from consideration, after which the perturbed measures are renormalized. Compared to prior work employing TV constraints (Balaji et al., 2020; Mukherjee et al., 2021), our definition has several distinct features: (1) it is naturally derived as a robust proxy for Wp under the Huber \u03b5-contamination model; (2) it can be reframed as an optimization problem over a highly regular domain; and, consequently, (3) it admits a simple and useful duality theory. We show that when the clean distributions have bounded qth moments for q > p, W\u03b5 p nearly achieves the minimax optimal robust estimation risk of \u03b51/p\u22121/q under the Huber \u03b5-contamination model. Moreover, our dual formulation mirrors the classic Kantorovich dual with an added penalty proportional to the range of the potential function. This provides an elementary robustification technique which can be applied to any duality-based OT solver: one needs only to compute the argmin and argmax of the discriminative potentials over the batch samples, which can be done in conjunction with existing gradient evaluations. We demonstrate this the utility of this procedure with experiments for generative modeling with contaminated datasets using Wasserstein generative adversarial networks (WGANs) (Arjovsky et al., 2017). We also study structural properties of W\u03b5 p, characterizing the minimizers of (1) and maximizers of its dual, describing the regularity of the problem in \u03b5, and drawing a connection\n(1)\nbetween W\u03b5 p and loss trimming (Shen and Sanghavi, 2019). Finally, we study statistical aspects of W\u03b5 p, examining both one- and two-sample empirical convergence rates and providing additional robustness guarantees. The derived empirical convergence rates are at least as fast as the regular n\u22121/d rate for standard Wp; however, faster rates may be possible if only a small amount of high-dimensional mass is present.\n# 1.2 Related Work\nThe robust Wasserstein distance2 in (1) is closely related to the notions considered in Balaji et al. (2020) and Mukherjee et al. (2021). In Balaji et al. (2020), similar constraints are imposed with respect to (w.r.t.) general f-divergences, but the perturbed measures are restricted to probability distributions. This results in a more complex dual form (derived by invoking standard Kantorovich duality on the Wasserstein distance between perturbations) and requires optimization over a significantly larger domain. In Mukherjee et al. (2021), robustness w.r.t. the TV distance is added via a regularization term in the objective. This leads to a simple modified primal problem but the corresponding dual requires optimizing over two potentials, even when p = 1. Additionally, Le et al. (2021) and Nath (2020) consider robustness via Kullback-Leibler (KL) divergence and integral probability metric (IPM) regularization terms, respectively. The former focuses on Sinkhorn-based primal algorithms and the latter introduces a dual form that is distinct from ours and less compatible with existing duality-based OT computational methods. In Staerman et al. (2021), a median of means approach is used to tackle the dual Kantorovich problem from a robust statistics perspective. None of these works provide minimax error bounds for robust estimation. The robust OT literature is intimately related to unbalanced OT theory, which addresses transport problems between measures of different mass (Piccoli and Rossi, 2014; Chizat et al., 2018a; Liero et al., 2018; Schmitzer and Wirth, 2019; Hanin, 1992). These formulations are reminiscent of the problem (1) but with regularizers added to the objective (KL being the most studied) rather than incorporated as constraints. Sinkhorn-based primal algorithms (Chizat et al., 2018b) are the standard approach to computation, and these have recently been extended to large-scale machine learning problems via minibatch methods (Fatras et al., 2021). Fukunaga and Kasai (2021) introduces primal-based algorithms for semi-relaxed OT, where marginal constraints for a single measure are replaced with a regularizer in the objective. Partial OT (Caffarelli and McCann, 2010; Figalli, 2010), where only a fraction of mass needs to be moved, is another related framework. However, Caffarelli and McCann (2010) consider a different parameterization of the problem, arriving at a distinct dual, and Figalli (2010) is mostly restricted to quadratic costs with no discussion of duality. Recently, Chapel et al. (2020) has explored partial OT for positive-unlabeled learning, but dual-based algorithms are not considered.\nNotation and Preliminaries. Let (X, d) be a complete, separable metric space, and denote the diameter of a set A \u2282X by diam(A) := supx,y\u2208A d(x, y). Take Cb(X) as the set of continuous, bounded real functions on X, and let M(X) denote the set of signed Radon\nmeasures on X equipped with the TV norm3 \u2225\u00b5\u2225TV := |\u00b5|(X). Let M+(X) denote the space of finite, positive Radon measures on X. The Lebesgue measure on Rd is designated by \u03bb. For \u00b5, \u03bd \u2208M+(X) and p \u2208[1, \u221e], we consider the standard Lp(\u00b5) space with norm \u2225f\u2225Lp(\u00b5) = \ufffd\ufffd |f|p d\u00b5 \ufffd1/p, and we write \u00b5 \u2264\u03bd when \u00b5(B) \u2264\u03bd(B) for every Borel set B \u2286X. Let P(X) \u2282M+(X) denote the space of probability measures on X, and take Pp(X) := {\u00b5 \u2208P(X) : \ufffd d(x, x0)p d\u00b5(x) < \u221e} to be those with bounded pth moment. We write P\u221e(X) for probability measures with bounded support. Given \u00b5, \u03bd \u2208P(X), let \u03a0(\u00b5, \u03bd) denote the set of their couplings, i.e., \u03c0 \u2208P(X \u00d7 X) such that \u03c0(B \u00d7 X) = \u00b5(B) and \u03c0(X \u00d7 B) = \u03bd(B), for every Borel set B. When X = Rd, we write the covariance matrix for \u00b5 \u2208P2(X) as \u03a3\u00b5 := E[(X \u2212E[X])(X \u2212E[X])\u22ba] where X \u223c\u00b5. For f : X \u2192R, we define the range Range(f) := supx\u2208X f(x) \u2212infx\u2208X f(x). We write a \u2228b = max{a, b} and use \u2272, \u2273, \u224d to denote inequalities/equality up to absolute constants. Recall that Wp(\u00b5, \u03bd) < \u221e, for any \u00b5, \u03bd \u2208Pp(X). For any p \u2208[1, \u221e), Kantorovich duality states that \ufffd \ufffd\nwhere the c-transform f c : X \u2192R is defined by f c(y) = infx\u2208X d(x, y)p \u2212f(x) (with respe to the cost c(x, y) = d(x, y)p).\n# 2 Robust Estimation of Wp\nOutlier-robust OT is designed to address the fact that Wp \ufffd (1 \u2212\u03b5)\u00b5 + \u03b5\u03b4x, \u03bd \ufffd explodes as d(x, x0) \u2192\u221e, no matter how small \u03b5 might be. More generally, one considers an adversary that seeks to dramatically alter Wp by adding small pieces of mass to its arguments. To formalize this, we fix p \u2208[1, \u221e) and consider the Huber \u03b5-contamination model popularized in robust statistics (Huber, 1964), where a base measure \u00b5 \u2208P(X) is perturbed to obtain a contaminated measure \u02dc\u00b5 belonging to the ball\nB\u03b5(\u00b5) := (1 \u2212\u03b5)\u00b5 + \u03b5 P(X) = \ufffd (1 \u2212\u03b5)\u00b5 + \u03b5\u03b1 : \u03b1 \u2208P(X) \ufffd .\n\ufffd \ufffd The goal is to obtain a robust proxy \u02c6W : P(X)2 \u2192R which, for any clean distributions \u00b5, \u03bd \u2208P(X) with contaminated versions \u02dc\u00b5 \u2208B\u03b5(\u00b5), \u02dc\u03bd \u2208B\u03b5(\u03bd), achieves low error \ufffd\ufffd\u02c6W(\u02dc\u00b5, \u02dc\u03bd) \u2212 Wp(\u00b5, \u03bd) \ufffd\ufffd. In general, this error can be unbounded, so we require that the base measures belong to some family D capturing distributional assumptions, e.g., bounded moments of some order. For any \u03b5 \u2208[0, 1] and D \u2286P(X), define the minimax robust estimation risk by\nThe following theorem characterizes this risk under standard moment assumptions. Moreover, we show that W\u03b5 p achieves this risk up to an additional multiplicative error term.\n(2)\n(3)\nTheorem 1 (Robust estimation of Wp). Fix q > p and let Dq := {\u00b5 \u2208Pq(X) : \u2225d(\u00b7, x)\u2225Lq(\u00b5) \u2264 M for some x \u2208X} denote the family of distributions with centered qth moments uniformly bounded by an absolute constant M. Then, for 0 \u2264\u03b5 \u22640.494, we have\nand this bound is tight so long as X contains two points at distance M\u03b5\u22121/q. If furth \u03b5 \u22640.33, then for any \u00b5, \u03bd \u2208Dq with corrupted versions \u02dc\u00b5 \u2208B\u03b5(\u00b5), \u02dc\u03bd \u2208B\u03b5(\u03bd), we have\n \u2208D \u2208B \u2208B |W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd) \u2212Wp(\u00b5, \u03bd)| \u2264 \ufffd 1 \u2212(1 \u22123\u03b5)1/p\ufffd Wp(\u00b5, \u03bd) + O(M\u03b51/p\u22121/q).\n\ufffd \ufffd The distance assumption is quite mild and is satisfied, e.g., when X has a path connected component with diameter at least M\u03b5\u22121/q. The risk bounds follow by characterizing appropriate moduli of continuity, mirroring classic techniques of Donoho and Liu (1988). In particular, for any D \u2286P(X), we have\nwith the estimator achieving the upper bound returning \u02c6W(\u02dc\u00b5, \u02dc\u03bd) = Wp(\u02c6\u00b5, \u02c6\u03bd) for any \u02c6\u00b5, \u02c6\u03bd \u2208D such that \u02dc\u00b5 \u2208B\u03b5(\u00b5) and \u02dc\u03bd \u2208B\u03b5(\u03bd). To bound the larger modulus, we use that Wp(\u03b1, \u03b2) \u2264 \u2225d(\u00b7, x)\u2225Lp(\u03b1) + \u2225d(\u00b7, x)\u2225Lp(\u03b2) for any \u03b1, \u03b2 \u2208P(X) and x \u2208X, connecting Wp to moment bounds, and apply a bound from robust mean estimation. To control the smaller modulus, we simply consider \u03b1 = \u03b4x and \u03b2 = (1 \u2212\u03b5)\u03b4x + \u03b5\u03b4y for any x, y \u2208X such that d(x, y) = M\u03b5\u22121/q. Finally, the risk bound for W\u03b5 p, which matches the minimax risk up to a multiplicative error term of at most 3\u03b5Wp(\u00b5, \u03bd), relies on a lemma showing that |W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd) \u2212Wp(\u00b5, \u03bd)| \u2264 |(1 \u22123\u03b5)W3\u03b5 p (\u00b5, \u03bd) \u2212Wp(\u00b5, \u03bd)|. Full details are provided in Appendix A.2. We next specialize Theorem 1 to the common case of X = Rd with base measures whose covariance matrices \u03a3\u00b5, \u03a3\u03bd have bounded spectral norms. Such measures also have bounded moments in the sense of Theorem 1, so the previous upper bound applies. The lower bound for this case (given in Appendix A.3) uses the same technique as before but with a more careful choice of measures involving a multivariate Gaussian. Corollary 1 (Bounded covariance). Fix X = Rd and let Dcov 2 := {\u00b5 \u2208P2(X) : \u03a3\u00b5 \u2aafId}. For p < 2 and 0 \u2264\u03b5 \u22640.49, we have R(Dcov 2 , \u03b5) \u224d \u221a d \u03b51/p\u22121/2. When \u03b5 \u22640.33, this risk is achieved by \u02c6W = W\u03b5 p up to an additional term of O(\u03b5 Wp(\u00b5, \u03bd)), as in Theorem 1. Remark 1 (Comparison with robust mean estimation). In the setting of mean estimation under assumptions analogous to Corollary 1 (i.e., \u03a3\u00b5 \u2aafId, \u02dc\u00b5 \u2208B\u03b5(\u00b5)), the optimal error rate of \u221a\u03b5 is dimension-free (Chen et al., 2018). We interpret the factor of \u221a d present for our Wp rate as reflecting the high-dimensional optimization inherent to the Wasserstein distance. Remark 2 (Asymmetric contamination). The robust distance from (1) readily extends to an asymmetric distance W\u03b5\u00b5,\u03b5\u03bd p with distinct robustness radii, so that W\u03b5 p = W\u03b5,\u03b5 p . Extensions of our main results (including Theorem 1) to this setting are presented in Appendix B.1. The one-sided version W\u03b5 p(\u00b5\u2225\u03bd) := W\u03b5,0 p (\u00b5, \u03bd) is well-suited for applications such as generative modeling (see Section 6).\n(4)\nOur precise error bounds exploit the unique structure of W\u03b5 p and do not translate clearly to existing robust proxies for Wp. We note, however, that the TV-robustified Wp presented in Balaji et al. (2020) can be controlled and approximated to some extent by W\u03b5 p, via bounds presented in Appendix B.2.\n# 3 Duality Theory for W\u03b5 p\nIn addition to its robustness properties, W\u03b5 p enjoys a simple optimization structure that enables a useful duality theory. Unless stated otherwise, we henceforth assume that X is compact. To begin, we reformulate W\u03b5 p(\u00b5, \u03bd) as a minimization problem over Huber balls centered at \u00b5 and \u03bd.\n\u03b5 p(\u00b5, \u03bd) = (1 \u22122\u03b4)\u22121/p inf \u00b5\u2032\u2208B\u03b4(\u00b5) \u03bd\u2032\u2208B\u03b4(\u03bd) Wp(\u00b5\u2032, \u03bd\u2032),\nWhile the original definition (1) involves removing mass from the base measures and rescaling, (5) is optimizing over mass added to \u00b5 and \u03bd (up to scaling). Our proof in Appendix A.4 of this somewhat surprising result relies on the symmetric nature of the OT distance objective. Roughly, instead of removing a piece mass from one measure, we may always add it to the other. This reformulation is valuable because the updated constraint sets are simple and do not interact with the simplex boundary. Specifically, definition (3) reveals that the Huber ball B\u03b4(\u00b5) is always an affine shift of P(X), with scaling independent of \u00b5. Hence, linear optimization over B\u03b4(\u00b5) is straightforward:\nThe above stands in contrast to the TV balls (i.e., sets of the form {\u00b5\u2032 \u2208P(X) : \u2225\u00b5\u2032 \u2212\u00b5\u2225TV \u2264\u03b4}) that appear in existing robust OT formulations; these exhibit non-trivial boundary interactions as depicted in Fig. 1. Fortunately, Wp is closely tied to the linear form \u00b5 \ufffd\u2192 \ufffd f d\u00b5 via Kantorovich duality\u2014a cornerstone for various theoretical derivations and practical implementations. Combining this with a minimax result, we establish a related dual form for W\u03b5 p.\n the suprema are achieved by f \u2208Cb(X) with f = (f c)c.\n(5)\n(6)\n(7)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a78/9a78ad05-8ede-468a-ae94-fdec6f649b7f.png\" style=\"width: 50%;\"></div>\nThis new formulation differs from the classic dual (2) by a range penalty for the potential function. When p = 1, we have f c = \u2212f and f = (f c)c exactly when f is 1-Lipschitz. The theorem is proven in Appendix A.5, where we first apply Proposition 1 and then invoke Kantorovich duality for Wp(\u00b5\u2032, \u03bd\u2032), while verifying that the conditions for Sion\u2019s minimax theorem hold true. Applying minimax gives\nwhere \u03b4 = \u03b5/(1 + \u03b5). At this point, we employ (6), along with properties of the c-transform, to obtain the desired dual. We stress that if \u00b5\u2032 and \u03bd\u2032 instead varied within TV balls, the inner minimization problems would not admit closed forms due to boundary interactions. Theorem 2 reveals an elementary procedure for robustifying the Wasserstein distance against outliers: regularize its standard Kantorovich dual w.r.t. the sup-norm of the potential function. The simplicity of this modification is its main strength. As demonstrated in Section 6, this enables adjusting popular duality-based OT solvers, e.g., Arjovsky et al. (2017), to the robust framework and opens the door for applications to generative modeling with contaminated datasets. We provide an interpretation for the maximizing potentials in Section 4. Some concrete examples for computing W\u03b5 p are found in Appendix B.3. Remark 3 (TV as a dual norm). Recall that \u2225\u00b7 \u2225TV is the dual norm corresponding to the Banach space of measurable functions on X equipped with \u2225\u00b7 \u2225\u221e. An inspection of the proof of Theorem 2 reveals that our penalty scales with \u2225\u00b7 \u2225\u221eprecisely for this reason.\nFinally, we describe an alternative dual form which ties robust OT to loss trimming\u2014 popular practical tool for robustifying estimation algorithms when \u00b5 and \u03bd have finite suppor (Shen and Sanghavi, 2019).\n\uf8ed \uf8f8 The inner minimization problems above clip out the \u03b5n fraction of samples whose potentia evaluations are largest. This is similar to how standard loss trimming clips out a fraction o samples that contribute most to the considered training loss.\nFigure 1: Huber \u03b5-contamination balls (green) and \u03b5-TV balls (blue), centered at distinct points (red) within the 2-dimensional simplex. The Huber balls with different centers are translates of each other, while the rightmost TV ball interacts non-trivially with the simplex boundary.\n# 4 Structural Properties\nWe turn to structural properties of W\u03b5 p, exploring primal and dual optimizers, regularity of W\u03b5 p in \u03f5, as well as an alternative (near) coupling-based primal form.\n# 4.1 Primal and Dual Optimizers\nThus, the level sets of the dual potential encode the location of outliers in the origina measures, as depicted in Fig. 3. In fact, optimal perturbations \u00b5\u2212\u00b5\u2032 and \u03bd \u2212\u03bd\u2032 are sometimes determined exactly by an optimal potential f, often taking the form \u00b5|argmax(f) and \u03bd|argmax(f (though not always; we discuss this in Appendix A.11 along with the proof).\n# 4.2 Regularity in Robustness Radius\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c97e/c97e44b9-95b2-4180-bd58-4423bda40786.png\" style=\"width: 50%;\"></div>\n\ufffd \ufffd Figure 2: The gridded light blue and green regions each have mass \u03b5, respectively, and are removed to obtain optimal \u00b5\u2032 and \u03bd\u2032 for W\u03b5 1. No mass need be removed from the dark region designating \u00b5 \u2227\u03bd.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c19/0c19433c-c2b5-4f8b-ae0f-858f09309694.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Optimal potentials: (left) 1D densities plotted with their optimal potential for the W\u03b5 1 dual problem; (right) contour plots for optimal dual potentials to W1 and W\u03b5 1 between 2D Gaussian mixtures. Observe how optimal potentials for the robust dual are flat over outlier mass.</div>\nThe proof is given in Appendix A.7. More precise, diameter independent, bounds in the form of (iv) are provided in the proofs of the robustness results from Section 2, but these require \u00b5 and \u03bd to satisfy certain moment bounds that do not hold in general.\n# 4.3 Alternative Primal Form\nMirroring the primal Kantorovich problem for classic Wp, we derive in Appendix A.8 a alternative primal form for W\u03b5 p in terms of (near) couplings for \u00b5 and \u03bd.\nProposition 6 (Alternative primal form). For any p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X), we have\nProposition 6 (Alternative primal form). For any p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X), we have\nW\u03b5 p(\u00b5, \u03bd) = inf \u03c0\u2208P(X\u00d7X) \u00b5\u2208B\u03b5(\u03c01), \u03bd\u2208B\u03b5(\u03c02) \u2225d\u2225Lp(\u03c0),\nwhere \u03c01 and \u03c02 are the respective marginals of \u03c0.\nRemark 4 (Data privacy). From this, we deduce that W\u03b5 \u221e(\u00b5, \u03bd) \u2264M if and only if there exists a coupling (X, Y ) for \u00b5, \u03bd such that |X \u2212Y | \u2264M with probability at least 1 \u2212\u0398(\u03b5). In Appendix C, we state this more precisely and leverage this fact for an application to data privacy. Specifically, within the Pufferfish privacy framework, the so-called Wasserstein Mechanism (Song et al., 2017) maximizes W\u221eover certain pairs of distributions to provide a strong privacy guarantee. By substituting W\u221ewith W\u03b5 \u221e, we reach an alternative mechanism that satisfies a slightly relaxed guarantee and can introduce significantly less noise. Proposition 5 implies that lim\u03b5\u21920 W\u03b5 p = Wp, posing W\u03b5 p as a natural extension of Wp. Given the representation in Proposition 6, we now ask whether convergence of optimal (near) couplings also holds. A proof in Appendix A.9 provides an affirmative answer via a \u0393-convergence argument. Proposition 7 (Convergence of couplings). Fix p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X). If \u03b5n \u21980 and \u03c0n \u2208P(X \u00d7 X) is optimal for W\u03b5n p (\u00b5, \u03bd) via (8), for each n \u2208N, then {\u03c0n}n\u2208N admits a subsequence converging weakly to an optimal coupling for Wp(\u00b5, \u03bd). Finally, we consider a case of practical importance: the discrete setting where \u00b5 and \u03bd have finite supports. Like classic OT, computing W\u03b5 p(\u00b5, \u03bd) between discrete measures\n(8)\namounts to a linear program for p < \u221eand can be solved in polynomial time. The proof in Appendix A.10 starts from the alternative primal form of Proposition 6 and analyzes the feasible polytope when the support sizes are equal. Proposition 8 (Finite support). Let \u00b5 and \u03bd be uniform discrete measures over n points each. Then there exist optimal \u00b5\u2032, \u03bd\u2032 for W\u03b5 p(\u00b5, \u03bd) such that \u00b5\u2032 and \u03bd\u2032 each give mass 1/n to \u230a(1 \u2212\u03b5)n\u230bpoints and assign their remaining \u2308\u03b5n\u2309/n \u2212\u03b5 mass to a single point. When \u03b5 is a multiple of 1/n, the propositions says that there exist minimizers which assign equal mass to (1 \u2212\u03b5)n points, while eliminating the remaining \u03b5n that are identified as outliers.\namounts to a linear program for p < \u221eand can be solved in polynomial time. The proof in Appendix A.10 starts from the alternative primal form of Proposition 6 and analyzes the feasible polytope when the support sizes are equal.\n# 5 Statistical Analysis\nWe now examine estimation of W\u03b5 p from observed data, fixing \u03b5 \u22641/2 throughout.\n# 5.1 Empirical Convergence Rates\nIn practice, we often have access only to samples from \u00b5, \u03bd \u2208P(X), which motivates the study of empirical convergence under W\u03b5 p. Consider the empirical measures \u02c6\u00b5n := n\u22121 \ufffdn i=1 \u03b4Xi and \u02c6\u03bdn := n\u22121 \ufffdn i=1 \u03b4Yi, where X1, . . . , Xn and Y1, . . . , Yn are i.i.d. samples from \u00b5 and \u03bd, respectively. We examine both the one- and two-sample scenarios, i.e., the speed at which W\u03b5 p(\u00b5, \u02c6\u00b5n) and \ufffd\ufffdW\u03b5 p(\u02c6\u00b5n, \u02c6\u03bdn) \u2212W\u03b5 p(\u00b5, \u03bd) \ufffd\ufffdconverge to 0 as n grows. This section assumes that X \u2282Rd with d > 2; extensions to the non-Euclidean case are provided in the Appendices A.13 to A.15. To state the results, we need some definitions. Recall the covering number N\u03b4(\u03b3, \u03c4), defined for \u03b4 > 0, \u03c4 \u22650, and \u03b3 \u2208M+(X) as the minimum number of closed balls of radius \u03b4 needed to cover X up to a set A with \u03b3(A) \u2264\u03c4. We define the lower \u03c4-covering dimension for \u03b3 as\nWe note that lower bounds for standard Wp depend on the lower Wasserstein dimension given by lim\u03c4\u21920 d\u03c4 \u2217(\u00b5) (Weed and Bach, 2019). To provide meaningful bounds for W\u03b5 p, on the other hand, we require control of d\u03c4 \u2217(\u00b5) for some \u03c4 > \u03b5, which can be understood as a robust notion of dimension.\nProposition 9 (One-sample rates). Fix p \u2208[1, \u221e] and \u03c4 \u2208(\u03b5, 1]. If p < d/2, we have\n\ufffd \ufffd for a constant C independent of n and \u03b5. Furthermore, if d\u03c4 \u2217(\u00b5) > s , then W\u03b5 p(\u00b5, \u03b3) \u2265C\u2032 (\u03c4 \u2212\u03b5)1/p n\u22121/s\nThe robust \u03c4-covering dimension can be smaller than standard notions of intrinsic dimension when all but the outlier mass is lower dimensional. Our lower bound captures this fact. The upper bound depends on the ambient dimension d since there is no guarantee that only outlier mass is supported on a high-dimensional set5. Indeed, the following corollary shows that when a significant portion of mass is high-dimensional, the n\u22121/d rate is sharp. Corollary 2 (Simple one-sample rate). Fix p < d/2 and \u03b5 \u2208(0, 1]. If \u00b5 has absolutely continuous part f d\u03bb with \ufffd f d\u03bb > \u03b5 and f bounded from above, then d\u03c4 \u2217(\u00b5) = d and E \ufffd W\u03b5 p(\u00b5, \u02c6\u00b5n) \ufffd = \u0398(n\u22121/d). In words, if more than the \u03b5 mass that can be removed via robustification is absolutely continuous (w.r.t. Lebesgue on Rd), then the standard n\u22121/d \u201ccurse of dimensionality\u201d applies. Nevertheless, we conjecture that an upper bound that depends on a robust upper dimension can be derived under appropriate assumptions on \u00b5, as discussed in Section 7. The proofs of Proposition 9 and Corollary 2 are found in Appendices A.13 and A.14, respectively. Moving to the two-sample regime, we again have an upper bound which matches the standard rate for Wp.\n\ufffd\ufffd\ufffd for a constant C independent of n and \u03b5.\nThe proof in Appendix A.15 is a consequence of the dual form from Theorem 2, combined with standard one-sample rates for Wp. There, we also discuss obstacles to extending two-sample lower bounds for standard Wp to the robust setting.\n# 5.2 Additional Robustness Guarantees\nFinally, we provide conditions under which Wp(\u00b5, \u03bd) can be recovered precisely from W\u03b5 p(\u00b5, \u03bd) despite data contamination. Naturally, these conditions are stronger than those needed for approximate (minimax optimal) robust approximation, as studied in Section 2. We return to the Huber \u03b5-contamination model, taking \u02dc\u00b5 \u2208B\u03b5(\u00b5) and \u02dc\u03bd \u2208B\u03b5(\u03bd). One cannot hope to achieve exact recovery via W\u03b5 p in general, since W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd) < Wp(\u00b5, \u03bd) when \u02dc\u00b5 = \u00b5 and \u02dc\u03bd = \u03bd for p < \u221e. Nevertheless, we can provide exact recovery guarantees under appropriate mass separation assumptions.\nProposition 11 (Exact recovery). Fix \u00b5, \u03bd \u2208P(X), \u03b5 \u2208[0, 1], and suppose that \u02dc\u00b5 = (1 \u2212\u03b5)\u00b5 + \u03b5\u03b1 and \u02dc\u03bd = (1 \u2212\u03b5)\u03bd + \u03b5\u03b2, for some \u03b1, \u03b2 \u2208P(X). Let S = supp(\u00b5 + \u03bd). If d(supp(\u03b1), S), d(supp(\u03b2), S), and d(supp(\u03b1), supp(\u03b2)) are all greater than diam(S), then W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd) = Wp(\u00b5, \u03bd).\nfor minimizing Wp. The appendix also discussed more flexible albeit technical assumptions under which exact recovery is possible, and provides bounds for when the robustness radius does not match the contamination level \u03b5 exactly. Proposition 11 relies on the contamination level \u03b5 being known, which is often not the case in practice. To account for this, we prove in Appendix A.17 that, under the same assumptions, there exists a principled approach for selecting the robustness radius when \u03b5 is unknown.\nProposition 12 (Robustness radius for unknown \u03b5). Assume the setting of Proposition 11 and let D be the maximum of d(supp(\u03b1), S), d(supp(\u03b2), S), and d(supp(\u03b1), supp(\u03b2)). Then, for p \u2208[1, \u221e), we have\n# Proposition 12 (Robustness radius for unknown \u03b5). Assume the setting of Proposition 11 and let D be the maximum of d(supp(\u03b1), S), d(supp(\u03b2), S), and d(supp(\u03b1), supp(\u03b2)). Then, for p \u2208[1, \u221e), we have\nd d\u03b4 \ufffd (1\u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p\ufffd \u2264\u2212Dp, d d\u03b4 \ufffd (1\u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p\ufffd \u2265\u2212diam(S)p > \u2212Dp,\n\ufffd \ufffd at the (all but countably many) points where the derivative is defined.\nAs (1 \u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p is continuous and decreasing in \u03b4 by Proposition 5, we have identified an \u201celbow\u201d in this curve located exactly at the true contamination level \u03b5. Next, we return to the statistical setting and show how to obtain exact recovery assuming that the fraction of corrupted samples vanishes as n \u2192\u221e. Consider i.i.d. samples X1, X2, \u00b7 \u00b7 \u00b7 \u223c\u00b5 and Y1, Y2, \u00b7 \u00b7 \u00b7 \u223c\u03bd, arbitrarily corrupted to obtain \u02dcX1, \u02dcX2, . . . and \u02dcY1, \u02dcY2, . . . . We measure the level of corruption by the rates \u03c4\u00b5(n) = 1 n \ufffdn i=1 1{Xi\u0338= \u02dc Xi} and \u03c4\u03bd(n) = 1 n \ufffdn i=1 1{Yi\u0338= \u02dcYi}. For this result, X may be unbounded, and only the clean distributions \u00b5 and \u03bd need compact support. Let \u02dc\u00b5n and \u02dc\u03bdn denote the empirical measures associated with \u02dcX1, . . . , \u02dcXn and \u02dcY1, . . . , \u02dcYn, respectively.\nas n \u2192\u221e, almost surely.\nThat is, so long as the fraction of (potentially unbounded) corrupted samples converges to 0, there exists a schedule for robustness radii so that the correct distance is recovered. The proof in Appendix A.18 uses ideas from the previous result to alleviate potential problems arising from large corruptions.\nRemark 5 (Median-of-means). This consistency result mirrors that presented by Staerman et al. (2021) for a median-of-means (MoM) estimator WMoM. They produce a robust estimate for W1 by partitioning the contaminated samples into blocks and replacing the each mean appearing in the W1 dual with a median of block means, where the number of blocks depends on the contamination fractions. We remark that when \u03c4\u00b5 \u2228\u03c4\u03bd = \u2126(1) and contaminations are stochastic\u2014i.e., each \u02dcXi and \u02dcYi are sampled from some \u02dc\u00b5 \u2208B\u03b5(\u00b5) and \u02dc\u03bd \u2208B\u03b5\u2032(\u03bd), respectively\u2014we have WMoM(\u02dc\u00b5n, \u02dc\u03bdn) \u2192W1(\u02dc\u00b5, \u02dc\u03bd). Hence, this approach cannot provide guarantees in the vein of Section 2, since it may be that W1(\u02dc\u00b5, \u02dc\u03bd) \u226bW1(\u00b5, \u03bd).\n\u03b4 \u2208[0, \u03b5) \u03b4 \u2208(\u03b5, 1]\n# 6 Applications\nWe now move to applications of the proposed robust OT framework. We first discuss computational aspects and provide an algorithm to compute W\u03b5 p based on its dual form. The algorithm, which requires only a minor modification to standard OT solvers, is then used for generative modeling from contaminated data.\n# 6.1 Computation\nIn practice, similarity between datasets is often measured using the so-called neural network (NN) distance dF(\u00b5, \u03bd) = sup\u03b8\u2208\u0398 \ufffd f\u03b8 d\u00b5 \u2212 \ufffd f\u03b8 d\u03bd, where F = {f\u03b8}\u03b8\u2208\u0398 is a NN class (Arora et al., 2017). Given two batches of samples X1, . . . , Xn \u223c\u00b5 and Y1, . . . , Yn \u223c\u03bd, we approximate integrals by sample means and estimate the supremum via stochastic gradient ascent. Namely, we follow the update rule\n\ufffd \ufffd When F approximates the class of 1-Lipschitz functions, we approach a Kantorovich dual and obtain an estimate for W1(\u00b5, \u03bd), which is the core idea behind the WGAN (Arjovsky et al., 2017) (see Makkuva et al. (2020) for an extension to W2). By virtue of our duality theory, OT solvers as described above can be easily adapted to the robust framework. For the purpose of generative modeling, the one-sided robust distance defined by \ufffd \ufffd\nis most appropriate, since data may be contaminated but generated samples from the model are not. In Appendix B.1, we translate our duality result to this setting, finding that\nThis representation motivates a modified gradient update for the corresponding NN distance estimate:\n(9)\n(10)\nDue to the non-convex and non-smooth nature of the objective, formal optimization guarantees seem challenging to obtain, and we defer this exploration for future work. Nevertheless, as we will see next, this approach proves quite fruitful in practice. Full experimental details for the following results and comparisons with existing work are provided in Appendix B.4, and code is provided at https://github.com/sbnietert/robust-OT. Computations were performed on a cluster machine equipped with a NVIDIA Tesla V100.\n# 6.2 Generative Modeling\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b141/b1411a99-35d7-41a7-b61c-6c185f0815bf.png\" style=\"width: 50%;\"></div>\nWe examine outlier-robust generative modeling using the modification suggested above. We train two WGAN with gradient penalty (WGAN-GP) models (Gulrajani et al., 2017) on a contaminated dataset with 80% MNIST data and 20% random noise, running both with a standard selection of hyperparameters but adjusting one to compute gradient updates according to the robust objective with a selection of \u03b5 = 0.25. In Fig. 4 (top), we display generated samples produced by both networks after processing 125k contaminated batches of 64 samples. The effect of outliers is clearly mitigated by training with the robustified objective. One subtlety to the above is that WGANGP employs a regularizer R(f\u03b8) penalizing large gradients, rather than enforcing a Lipschitz constraint. In Appendix B.5, we provide results demonstrating that our duality result still holds in the presence of many types of regularizers. This further motivates us to apply the robustness technique to more sophisticated GANs which incorporate additional regularization, in particular Style-\nFigure 4: (top): samples generated by robustified (left) and standard (right) WGAN-GP after training on corrupted MNIST dataset. (bottom): samples generated by robustified (left) and standard (right) StyleGAN 2 after training on corrupted CelebA-HQ dataset (left).\nGAN 2 (Karras et al., 2020). We again train two off-the-shelf models using contaminated data\u2014this time, 80% CelebA-HQ face photos and 20% MNIST data\u2014with one tweaked to perform gradient updates according to the robust objective with \u03b5 = 0.25. We present generated samples in Fig. 4 (bottom). Once again, the modified objective enables learning a model that is largely free of outliers despite being trained on a contaminated dataset.\n# 7 Summary and Concluding Remarks\nThis paper introduced the outlier-robust Wasserstein distance W\u03b5 p, which measures proximity between probability distributions while discarding an \u03b5-fraction of outlier mass. We conducted\na theoretical study of its structural and statistical properties, covering robustness guarantees, strong duality, characteristics of optimal perturbations and dual potentials, regularity in \u03b5, and empirical convergence. The derived dual form amounts to a simple modification of classic Kantorovich duality that regularizes the objective w.r.t. the sup-norm of the potential function. This gave rise an elementary robustification technique for duality-based OT solvers (by introducing said penalty), which enables adapting computational methods for classic Wp to compute W\u03b5 p. Leveraging this, we demonstrated the utility of W\u03b5 p for generative modeling with contaminated data. Future research directions are abundant, both theoretical and practical. First, the derived duality can be leveraged for many high-dimensional real-world inference tasks where outlierrobustness is desired, although a large-scale empirical exploration is beyond the scope of this work. We also aim to sharpen our statistical bounds and provide finite-sample robustness guarantees. In the one-sample case, we expect a tighter upper bound that depends on an upper (robust) intrinsic dimension to hold when only a small amount of high-dimensional mass is present. Indeed, high-dimensional regions are harder to sample and we therefore expect W\u03b5 p to treat those as outliers in the empirical approximation setting. The following generalizations of the considered robust framework are also of interest: (i) general transportation costs (many of our structural results immediately generalize); (ii) unbounded domains; and (iii) other base and constraining distances (e.g., IPMs, NN distances, etc.).\n# Acknowledgements\nThe authors would like to thank Benjamin Grimmer for helpful conversations surrounding minimax optimization, as well as Jacob Steinhardt and Adam Sealfon for useful discussions on robust statistics. S. Nietert was supported by the National Science Foundation (NSF) Graduate Research Fellowship under Grant DGE-1650441. R. Cummings was supported in part by NSF grants CNS-1850187 and CNS-1942772 (CAREER), a Mozilla Research Grant, and a JPMorgan Chase Faculty Research Award. Z. Goldfeld was supported by NSF grants CCF-1947801 and CCF-2046018 (CAREER), and the 2020 IBM Academic Award.\n# References\n# Appendix A Proofs of Main Re\nWe begin with some further preliminaries and notation. When convenient, we write E\u00b5[f(X)] for the expectation of f(X) with X \u223c\u00b5. Next, we recall the general definition of Wp between non-negative measures of equal (but potentially not unit) mass. Given two measures \u00b5, \u03bd \u2208M+(X) with \u00b5(X) = \u03bd(X), let \u03a0(\u00b5, \u03bd) = {\u00b5(X)\u03c0 : \u03c0 \u2208\u03a0(\u00b5/\u00b5(X), \u03bd/\u03bd(X))}, and, for p \u2208[1, \u221e], define\n  We further recall the two-potential version of Kantorovich duality, which states that fo p \u2208[1, \u221e), \ufffd \ufffd\n  We further recall the two-potential version of Kantorovich duality, which states that for\n  We further recall the two-potential version of Kantorovich duality, wh p \u2208[1, \u221e), \ufffd \ufffd\nFinally, we define the push-forward of a measure \u00b5 \u2208M+(X) w.r.t. a measurable map T : X \u2192Y by T#\u00b5(B) := \u00b5(T \u22121(B)) for all measurable B \u2286Y.\n# A.1 Preliminary Results\nProof. Here, we use that optimal perturbed measures for the original primal (1) may be taken to have mass exactly 1 \u2212\u03b5 (since any feasible measures may be scaled down until this is the case, without changing the original objective due to its normalization). We further observe that \u00b5 \u2208B\u03b5(\u00b5\u2032) if and only if (1 \u2212\u03b5)\u00b5\u2032 \u2264\u00b5.\n# A.2 Proof of Theorem 1\nm\u2212(D, \u03b5) := sup \u03ba,\u03ba\u2032\u2208D \u03ba\u2208B\u03b5(\u03ba\u2032) Wp(\u03ba, \u03ba\u2032), m+(D, \u03b5) := sup \u03ba\u2208D, \u03ba\u2032\u2208P(X) \u03ba\u2208B\u03b5(\u03ba\u2032) Wp(\u03ba, \u03ba\u2032).\nThe following lemma shows that these quantities characterize minimax risk, mirrorin arguments from Donoho and Liu (1988).\n(11)\n(12)\n1 2m\u2212(D, \u03b5) \u2264R(D, \u03b5) \u22644m+(D, 2\u03b5).\nProof. For the upper bound, fix \u00b5, \u03bd \u2208D, and consider any corrupted versions \u02dc\u00b5 \u2208B\u03b5(\u00b5), \u02dc\u03bd \u2208 B\u03b5(\u00b5). To estimate Wp, take any \u02c6\u00b5, \u02c6\u03bd such that \u02dc\u00b5 \u2208B\u03b5(\u02c6\u00b5) and \u02dc\u03bd \u2208B\u03b5(\u02c6\u03bd), and set \u02c6W(\u02dc\u00b5, \u02dc\u03bd) = Wp(\u02c6\u00b5, \u02c6\u03bd). Since \u2225\u02c6\u00b5 \u2212\u00b5\u2225TV, \u2225\u02c6\u03bd \u2212\u03bd\u2225TV \u22644\u03b5, the midpoint distributions \u00b5\u2032 = 1 (\u00b5\u2227\u02c6\u00b5)(X)\u00b5 \u2227\u02c6\u00b5 and \u03bd\u2032 = 1 (\u03bd\u2227\u02c6\u03bd)(X)\u03bd \u2227\u02c6\u03bd must satisfy \u00b5, \u02c6\u00b5 \u2208B2\u03b5(\u00b5\u2032) and \u03bd, \u02c6\u03bd \u2208B2\u03b5(\u03bd\u2032). We thus bound\n|Wp(\u02dc\u00b5, \u02dc\u03bd) \u2212Wp(\u00b5, \u03bd)| = |Wp(\u02c6\u00b5, \u02c6\u03bd) \u2212Wp(\u00b5, \u03bd)| \u2264Wp(\u02c6\u00b5, \u00b5\u2032) + Wp(\u00b5\u2032, \u00b5) + Wp(\u02c6\u03bd, \u03bd\u2032) + Wp(\u03bd\u2032, \u03bd) \u22644m+(D, 2\u03b5).\nFor the lower bound, fix any feasible \u03ba, \u03ba\u2032 \u2208D with \u03ba \u2208B\u03b5(\u03ba\u2032), and suppose that both contaminated measures \u02dc\u00b5 and \u02dc\u03bd are equal to \u03ba. This is consistent with two cases: (1) the clean measures are \u00b5 = \u03bd = \u03ba\u2032 (in which case Wp(\u00b5, \u03bd) = 0); and (2) the clean measures are \u00b5 = \u03ba and \u03bd = \u03ba\u2032 (in which case Wp(\u00b5, \u03bd) = Wp(\u03ba, \u03ba\u2032)). Hence, no matter which distance a robust proxy \u02c6W assigns to \u02dc\u00b5 and \u02dc\u03bd, it must incur error at least Wp(\u03ba, \u03ba\u2032)/2 in one of these cases, proving the lemma. Next, we bound these moduli for the family of interest. For the upper modulus, we borrow a standard result from the robust mean estimation literature. Lemma 3. Suppose X = R and \u00b5 \u2208Dq for q \u22651. Then, for any \u03bd \u2208P(X) such that \u00b5 \u2208B\u03b5(\u03bd), we have |E\u00b5[X] \u2212E\u03bd[X]| \u2272M\u03b51\u22121/q \u2227M(1 \u2212\u03b5)\u22121/q. Proof. We apply Lemma E.2 of Zhu et al. (2022) with the Orlicz function \u03c8(t) = tq, function class F = {x \ufffd\u2192\u00b1x}, and \u03b7 = \u03b5. Lemma 4. For any q > p and 0 \u2264\u03b5 \u22640.99, we have m+(Dq, \u03b5) \u2272M\u03b51/p\u22121/q.\nProof. Without loss of generality, we suppose that M = 1 (otherwise, one can always apply the lemma to the same metric space with distances \u2014 and hence OT measurements \u2014 shrunk by a factor of M). To control this modulus, fix any \u03ba \u2208Dq and \u03ba\u2032 \u2208P(X) such that \u03ba = (1 \u2212\u03b5)\u03ba\u2032 + \u03b5\u03b1 for some \u03b1 \u2208P(X). Take y \u2208X with E\u03ba[d(X, \u00b7)q] \u22641. Writing \u03c4 = \u03b5 \u22281 \u2212\u03b5, we then have\np(\u03ba, \u03ba\u2032) \u2264\u03b51/p Wp(\u03ba\u2032, \u03b1) \u2264\u03b51/p (Wp(\u03ba\u2032, \u03b4y) + Wp(\u03b1, \u03b4y)) \u22642\u03b51/p sup \u03b2\u2208P(X) \u03ba\u2208B\u03c4(\u03b2) E\u03b2[d(X, y)p]1/p.\nTo bound | E\u03b2[d(X, y)p] \u2212E\u03ba[d(X, y)p]|, we observe that for X \u223c\u03ba, the random variab d(X, y)p has bounded q/pth moments. Thus, Lemma 3 gives | E\u03b2[d(X, y)p] \u2212E\u03ba[d(X, y)p]| \u2272\u03c4 1\u2212p/q \u2227(1 \u2212\u03c4)\u2212p/q.\nimplying the lemma.\nProof. Fix \u03ba = \u03b4x and \u03ba\u2032 = (1 \u2212\u03b5)\u03b4x + \u03b5\u03b4y. By design, both distributions belong to Dq (since E\u03ba\u2032[d(x, X)q] = \u03b5d(x, y)q = M) and Wp(\u03ba, \u03ba\u2032) = \u03b51/pd(x, y) = M\u03b51/p\u22121/q.\nProof. The second inequality of (13) follows directly from Lemma 1. For the other direction, we begin with \u00b5\u2032 0, \u03bd\u2032 0 \u2208M+(X) feasible for the original primal (1) for W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd), i.e., \u00b5\u2032 0 \u2264\u02dc\u00b5, \u03bd\u2032 0 \u2264\u02dc\u03bd and \u00b5\u2032 0(X), \u03bd\u2032 0(X) \u22651 \u2212\u03b5. Then, we intersect \u00b5\u2032 0 with (1 \u2212\u03b5)\u00b5, intersect \u03bd\u2032 0 with (1\u2212\u03b5)\u03bd, and remove up to \u03b5 additional mass from each as needed to obtain \u00b5\u2032 1 \u2264(1\u2212\u03b5)\u00b5 and \u03bd\u2032 1 \u2264(1\u2212\u03b5)\u03bd with equal mass such that Wp(\u00b5\u2032 1, \u03bd\u2032 1) \u2264Wp(\u00b5\u2032 0, \u03bd\u2032 0) and \u00b5\u2032 1(X) = \u03bd\u2032 1(X) \u22651\u22123\u03b5. Dividing both measures by 1 \u2212\u03b5, we obtain \u00b5\u2032 2, \u03bd\u2032 2 feasible for W2\u03b5/(1\u2212\u03b5) p (\u00b5, \u03bd) such that \ufffd \ufffd\n(13)\n(14)\nInfimizing over \u00b5\u2032 0 and \u03bd\u2032 0 gives W2\u03b5/(1\u2212\u03b5) p (\u00b5, \u03bd) \u2264(1\u22123\u03b5)\u22121/p W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd). Noting that W2\u03b5/(1\u2212\u03b5) p (\u00b5, \u03bd) \u2265 W3\u03b5 p (\u00b5, \u03bd) gives the first inequality of (13). To prove (14), we first bound\n\u2265Wp(\u00b5, \u03bd) \u22122m+(Dq, 3\u03b5).\nFinally, we compute\nas desired.\n# A.3 Proof of Corollary 1\nFor the upper bound, we observe that for \u03ba \u2208Dcov 2 with mean E\u03ba[X] = x\u03ba, we have E[\u2225x\u03ba \u2212X\u22252 2] = tr(\u03a3\u03ba) \u2264d.\nE[\u2225x\u03ba \u2212X\u22252 2] = tr(\u03a3\u03ba) \u2264d.\nHence, we obtain the desired upper bound as an application of Theorem 1. For the other direction, we apply Lemma 5 to lower bound E(Dcov 2 , \u03b5) by 1 2Wp(\u03ba, \u03ba\u2032) where \u03ba = \u03b40 and \u03ba\u2032 = (1 \u2212\u03b5)\u03b40 + \u03b5N(0, I/\u03b5). Indeed, both distributions belong to Dcov 2 (since, in case (2), \u03a3\u03bd = I) and Wp(\u00b5, \u03bd) \u2265\u03b51/p EX\u223cN(0,I/\u03b5)[\u2225X\u2225] = \u2126( \u221a d \u03b51/p\u22121/2), as desired.\n# A.4 Proof of Proposition 1\nProposition 1 (Mass addition). For all p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X), we have W\u03b5 p(\u00b5, \u03bd) = (1 \u22122\u03b4)\u22121/p inf \u00b5\u2032\u2208B\u03b4(\u00b5) \u03bd\u2032\u2208B\u03b4(\u03bd) Wp(\u00b5\u2032, \u03bd\u2032),\nwhere \u03b4 = \u03b5/(1 + \u03b5). Proof. To begin, we rewrite the RHS as\nFirst, we prove that W\u03b5 p(\u00b5, \u03bd) \u2265W \u03b5 p(\u00b5, \u03bd). Take \u00b5\u2032 \u2264\u00b5 and \u03bd\u2032 \u2264\u03bd optimal for the original formulation (1) of W\u03b5 p(\u00b5, \u03bd), with \u00b5\u2032(X) = \u03bd\u2032(X) = 1 \u2212\u03b5 (see Proposition 3 for existence of\n(5)\n(15)\nminimizers). Take \u03c0 \u2208\u03a0(\u00b5\u2032, \u03bd\u2032) to be an optimal coupling for Wp(\u00b5\u2032, \u03bd\u2032). Then, consider the alternative perturbed measures \u00b5\u2032 + := \u00b5 + (\u03bd \u2212\u03bd\u2032) and \u03bd\u2032 + := \u03bd + (\u00b5 \u2212\u00b5\u2032), which are feasible for W \u03b5 p(\u00b5, \u03bd), and define the coupling \u03c0+ \u2208\u03a0(\u00b5\u2032 +, \u03bd\u2032 +) by \u03c0+ = \u03c0 + (Id, Id)#(\u00b5 \u2212\u00b5\u2032 + \u03bd \u2212\u03bd\u2032). By construction, we have \u2225d\u2225Lp(\u03c0+) = \u2225d\u2225Lp(\u03c0), and so\nW \u03b5 p(\u00b5, \u03bd) \u2264(1 \u2212\u03b5)\u22121/p\u2225d\u2225Lp(\u03c0+) = (1 \u2212\u03b5)\u22121/p Wp(\u00b5\u2032, \u03bd\u2032) = W\u03b5 p(\u00b5, \u03bd),\nas desired. For the other direction, consider any \u00b5\u2032 \u2265\u00b5 and \u03bd\u2032 \u2265\u03bd feasible for W \u03b5 p(\u00b5, \u03bd), and write \u00b5\u2032 = \u00b5 + \u03b1, \u03bd\u2032 = \u03bd + \u03b2 for \u03b1, \u03b2 \u2208M+(X) with \u03b1(X) = \u03b2(X) = \u03b5. Take \u03c0 \u2208P(\u00b5\u2032, \u03bd\u2032) to be an optimal coupling for Wp(\u00b5\u2032, \u03bd\u2032), and let \u03c0(y|x) be the regular conditional probability distribution such that \u03bd\u2032(\u00b7) = \ufffd X \u03c0(\u00b7|x) d\u00b5\u2032(x). Informally, we next show that the added masses \u03b1 and \u03b2 need not be moved during transport, since we might as well replace them with their destinations after transport. Formally, this requires a bit of labeling. To start, we decompose \u03bd\u2032 into \u03bd\u2032 = \u03bd\u2032 \u2190\u00b5 + \u03bd\u2032 \u2190\u03b1, where \u03bd\u2032 \u2190\u00b5(\u00b7) := \ufffd X \u03c0(\u00b7|x) d\u00b5(x) denotes the mass transported from \u00b5 to \u03bd\u2032 via \u03c0 and \u03bd\u2032 \u2190\u03b1(\u00b7) := \ufffd X \u03c0(\u00b7|x) d\u03b1(x) denotes the mass transported from \u03b1. Similarly, we decompose \u03bd into \u03bd\u2190\u00b5 = \u03bd\u2032 \u2190\u00b5 \u2227\u03bd and \u03bd\u2190\u03b1 = \u03bd \u2212\u03bd\u2190\u00b5 and split \u03b2 into \u03b2\u2190\u00b5 = \u03bd\u2032 \u2190\u00b5 \u2212\u03bd\u2190\u00b5 and \u03b2\u2190\u03b1 = \u03b2 \u2212\u03b2\u2190\u00b5. By this construction, we have\nNext, we arbitrarily decompose \u00b5 into \u00b5\u2192\u03bd + \u00b5\u2192\u03b2 and \u03b1 into \u03b1\u2192\u03bd + \u03b1\u2192\u03b2 so that \u03bd(\u00b7) = \ufffd X \u03c0(\u00b7|x) d(\u00b5\u2192\u03bd + \u03b1\u2192\u03bd)(x). To see that this is always possible, consider the restricted coupling \u00af\u03c0 \u2208\u03a0(\u00b5, \u03bd\u2032 \u2190\u00b5) defined by \u00af\u03c0(A \u00d7 B) = \ufffd A \u03c0(B|x) d\u00b5(x), as well as the regular conditional probability distribution \u00af\u03c0(x|y) satisfying \u00b5(\u00b7) = \ufffd X \u00af\u03c0(\u00b7|y) d\u03bd\u2032 \u2190\u00b5(y). We can then set \u00b5\u2192\u03bd(\u00b7) = \ufffd X \u00af\u03c0(\u00b7|y) d\u03bd\u2190\u00b5(y) and \u00b5\u2192\u03b2 = \u00b5 \u2212\u00b5\u2192\u03bd. The same method works to construct \u03b1\u2192\u03bd and \u03b1\u2192\u03b2. Now, consider the alternative perturbed measures \u02dc\u00b5\u2032 = \u00b5 + \u03bd\u2032 \u2190\u03b1 = \u00b5\u2192\u03bd + \u03bd\u2032 \u2190\u03b1 + \u00b5\u2192\u03b2 and \u02dc\u03bd\u2032 = \u03bd + \u00b5\u2192\u03b2 + \u03b2\u2190\u03b1 = \u03bd\u2190\u00b5 + \u03bd\u2032 \u2190\u03b1 + \u00b5\u2192\u03b2 with \u02dc\u03c0 \u2208\u03a0(\u02dc\u00b5\u2032, \u02dc\u03bd\u2032) defined by\nNote that \u02dc\u00b5\u2032 and \u02dc\u03bd\u2032 are still feasible for the mass-addition problem and \u2225d\u2225Lp(\u02dc\u03c0) \u2264Wp(\u00b5\u2032, \u03bd\u2032) implying that \u2225d\u2225Lp(\u02dc\u03c0) = Wp(\u00b5\u2032, \u03bd\u2032). Finally, observe that \u00b5\u2192\u03bd \u2264\u00b5 and \u03bd\u2190\u00b5 \u2264\u03bd are feasible\nas desired.\nA.5 Proof of Theorem 2 Theorem 2 (Dual form). For p \u2208[1, \u221e), \u03b5 \u2208[0, 1], and \u00b5, \u03bd \u2208P(X), we have\nTheorem 2 (Dual form). For p \u2208[1, \u221e), \u03b5 \u2208[0, 1], and \u00b5, \u03bd \u2208P(X), we have\nBy compactness of X, the infimum set is itself compact w.r.t. the weak topology. Having that, it is readily verified that the conditions for Sion\u2019s minimax theorem (convexity, continuity of objective, and compactness of infimum set) apply, giving\n(7)\n(16)\n(17)\ng d\u03b2\nNoting that replacing g with f c preserves the constraints and can only increase the objective we further have\nusing the fact that infy f c(y) = infx,y d(x, y)p \u2212f(x) = \u2212supx f(x). The same reasoning allows us to restrict to f = (f c)c if desired. Since adding a constant to f decreases f c by the same constant, we are free to shift f so that the final term equals 2\u03b5\u2225f\u2225\u221e. Without shifting, we always have Range(f) \u22642\u2225f\u2225\u221e, so the problem simplifies to\nas desired. Since W\u03b5 p(\u00b5, \u03bd) \u2264Wp(\u00b5, \u03bd), we can assume that the supremum set is uniformly bounded with \u2225f\u2225\u221e\u2264Wp(\u00b5, \u03bd)/(2\u03b5). Furthermore, the argument preceding (Santambrogio, 2015, Proposition 1.11) proves that the supremum set is uniformly equicontinuous. Since X is compact, Arzel`a\u2013Ascoli implies that the supremum is achieved. See Appendix B.1 for an extension of this result to the asymmetric setting.\n# A.6 Proof of Proposition 3\nProposition 3 (Existence of minimizers). For p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X), the infimum in (1) is achieved, and there are minimizers \u00b5\u2032 \u2264\u00b5 and \u03bd\u2032 \u2264\u03bd such that \u00b5\u2032, \u03bd\u2032 \u2265\u00b5 \u2227\u03bd and \u00b5\u2032(X) = \u03bd\u2032(X) = 1 \u2212\u03b5.\nProof. We first prove existence via a compactness argument, and then turn to the lower envelope property \u00b5\u2032, \u03bd\u2032 \u2265\u00b5 \u2227\u03bd. This turns out to be considerably simpler to prove in the discrete setting, so we begin there and extend to the general case via a discretization argument. We already addressed the mass equality constraints in the proof of Lemma 1.\nExistence: Because measures in this feasible set are positive and bounded by \u00b5 and \u03bd, the set is tight, i.e. pre-compact w.r.t. the topology of weak convergence. If p < \u221e, then for any sequence \u00b5\u2032 n, \u03bd\u2032 n in the infimum set,\nsince \u00b5 \u2208Pp(X), with the same holding for the \u03bd\u2032 n sequence. Thus, by the characterization of convergence in Wp given by (Villani, 2009, Theorem 6.8), the infimum set is precompact w.r.t. Wp. Finally, the mass function in the constraints is continuous w.r.t. the weak topology, so the infimum set is compact w.r.t. Wp. As the objective is clearly continuous with w.r.t. Wp, the infimum is achieved.\n\nFor p = \u221e, we note that W\u221eis lower semicontinuous on P\u221e(X) w.r.t. the weak topology (and since supports are compact, the Wp topology, for all p < \u221e). Indeed, for any \u00b5\u2032 n w\u2192\u00b5\u2032, \u03bd\u2032 n w\u2192\u03bd\u2032 in P\u221e(X), we have\nThe infimum of a lower semicontinuous function over a compact set is always achieved, as desired.\nThe infimum of a lower semicontinuous function over a compact set is always achieved, a desired.\nLower envelope (discrete case): Having proven the existence of minimizers \u00b5\u2032 \u2264\u00b5 and \u03bd\u2032 \u2264\u03bd, it remains to show that we can take \u00b5\u2032, \u03bd\u2032 \u2265\u00b5 \u2227\u03bd. We begin with the case where X is countable and treat measures and their mass functions interchangeably. Then, if \u00b5\u2032 \u2265\u00b5 \u2227\u03bd fails to hold, there exists x0 \u2208X such that \u00b5\u2032(x0) < \u00b5(x0) and \u00b5\u2032(x0) < \u03bd(x0). We can further assume that \u03bd(x0) = \u03bd\u2032(x0). Otherwise, a := (\u03bd \u2212\u03bd\u2032)(x0) \u2227(\u00b5 \u2212\u00b5\u2032)(x0) mass could be returned to both \u00b5\u2032 and \u03bd\u2032 at x0 without increasing their transport cost. Indeed, for any \u03c0 \u2208\u03a0(\u00b5\u2032, \u03bd\u2032), the modified plan \u03c0 + a \u00b7 \u03b4(x0,x0) is valid for the new measures, \u00b5\u2032 + a\u03b4x0 and \u03bd\u2032 + a\u03b4x0 (still feasible by the choice of a), and attains the same cost. Taking \u03c0 \u2208\u03a0(\u00b5\u2032, \u03bd\u2032) optimal for Wp(\u00b5\u2032, \u03bd\u2032), we define the measure \u03ba \u2264\u00b5\u2032 by \u03ba(x0) := 0, \u03ba(x) := \u03c0(x, x0) for x \u0338= x0. This captures the distribution of the mass that is transported away from x0 w.r.t. \u03c0 when transporting \u03bd\u2032 to \u00b5\u2032. By conservation of mass, \u03ba has total mass at least (\u03bd\u2032 \u2212\u00b5\u2032)(x0) > 0, so we can set \u02dc\u03ba \u2264\u03ba to be a scaled-down copy of \u03ba with total mass (\u00b5\u2227\u03bd\u2032\u2212\u00b5\u2032)(x0) > 0. Now, we define an alternative perturbed measure \u02dc\u00b5\u2032 := \u00b5\u2032 +\u02dc\u03ba(X)\u00b7\u03b4x0 \u2212\u02dc\u03ba. By definition, we have 0 \u2264\u02dc\u00b5\u2032 \u2264\u00b5\u2032 with \u02dc\u00b5\u2032(X) = \u00b5\u2032(X) and \u02dc\u00b5\u2032(x0) = (\u00b5\u2227\u03bd\u2032)(x0). Furthermore, the modified plan \u02dc\u03c0 defined by\n\uf8f4 \uf8f3 satisfies \u02dc\u03c0 \u2208\u03a0(\u02dc\u00b5\u2032, \u03bd\u2032) and \u2225d\u2225Lp(\u02dc\u03c0) < \u2225d\u2225Lp(\u03c0) = Wp(\u00b5\u2032, \u03bd\u2032), contradicting the optimality of \u00b5\u2032.\nLower envelope (general case): For general X, we will need the following lemma, which allows us to apply our discrete argument to all settings.\nLemma 7 (Dense approximation). Let (Y, \u03c1) be a separable metric space with dense subset D \u2286Y. For any y \u2208Y, let y\u03bb denote a representative from D with d(y, y\u03bb) \u2264\u03bb (which exists by separability). Similarly, for any K \u2286Y, define K\u03bb = {y\u03bb : y \u2208K}. Then, if f : Y \u2192R is uniformly continuous, we have infy\u2208K f(y) = lim\u03bb\u21920 infy\u2208K\u03bb f(y). Proof. First, if {yn}n\u2208N is an infimizing sequence for infy\u2208K f(y), then lim inf \u03bb\u21920 inf y\u2208K\u03bb f(y) \u2264lim inf n\u2192\u221ef(y1/n n ) = lim n\u2192\u221ef(yn) = inf y\u2208K f(y),\n# where the second equality relies on the uniform continuity of f. Similarly, if {yn}n\u2208N with yn \u2208K\u03bbn, \u03bbn \u21980, is an limiting sequence for lim sup\u03bb\u21920 infy\u2208K\u03bb f(x), then we can write yn = x1/n n for xn \u2208K and find\nwhere the second equality again follows from the uniform continuity of f. The two inequalities\nwhere the second equality again follows from the uniform continuity of f. The two inequalities imply the lemma. We will essentially apply this lemma to the space of measures Y = ((1 \u2212\u03b5)Pp(X), Wp), letting D be its dense subset of discrete measures. For any \u03bb > 0, separability of X implies the existence of a countable partition {A\u03bb i }i\u2208N of X such that diam(A\u03bb i ) \u2264\u03bb for all i \u2208N, with representatives x\u03bb i \u2208A\u03bb i for all i \u2208N. For any measure \u03ba \u2208Y, we let \u03ba\u03bb \u2208M+(\u222ai\u2208N{x\u03bb i }) denote the discretized measure defined by \u03ba\u03bb := \ufffd i\u2208N \u03ba(A\u03bb i ) \u00b7 \u03b4x\u03bb i . For p < \u221e, we have\nWe will essentially apply this lemma to the space of measures Y = ((1 \u2212\u03b5)Pp(X), Wp), letting D be its dense subset of discrete measures. For any \u03bb > 0, separability of X implies the existence of a countable partition {A\u03bb i }i\u2208N of X such that diam(A\u03bb i ) \u2264\u03bb for all i \u2208N, with representatives x\u03bb i \u2208A\u03bb i for all i \u2208N. For any measure \u03ba \u2208Y, we let \u03ba\u03bb \u2208M+(\u222ai\u2208N{x\u03bb i }) denote the discretized measure defined by \u03ba\u03bb := \ufffd i\u2208N \u03ba(A\u03bb i ) \u00b7 \u03b4x\u03bb i . For p < \u221e, we have\nand so Wp(\u03ba, \u03ba\u03bb) \u2264\u03bb (including p = \u221eby continuity). This discretization lifts to sets as in the lemma. Now, for any \u03b1, \u03b2 \u2208M+(X) with \u03b1 \u2264\u03b2, let K\u03b1,\u03b2 = {\u03ba \u2208Y : \u03b1 \u2264\u03ba \u2264\u03b2, \u03ba(X) = 1 \u2212\u03b5}. Importantly, this choice of discretization satisfies K\u03bb \u03b1,\u03b2 = K\u03b1\u03bb,\u03b2\u03bb. Indeed if \u03ba \u2208K\u03b1,\u03b2, then \u03ba\u03bb \u2208K\u03b1\u03bb,\u03b2\u03bb by our discretization definition, and total mass is preserved. Likewise, if \u03ba \u2208K\u03b1\u03bb,\u03b2\u03bb, then we can consider\nwhere ci \u2208[0, 1] are chosen such that \u02dc\u03ba({i}) = \u03ba(Ai) for all i. By this construction, we have \u03ba = \u02dc\u03ba\u03bb and \u02dc\u03ba \u2208K\u03b1,\u03b2 with \u02dc\u03ba(X) = \u03ba(X) = 1 \u2212\u03b5, as desired. We also observe that\n\nconcluding the proof. Here, (19) is an application of Lemma 7 (technically, we use the product space Y \u00d7 Y with metric (\u03b1, \u03b2), (\u03b1\u2032, \u03b2\u2032) \ufffd\u2192max{Wp(\u03b1, \u03b1\u2032), Wp(\u03b2, \u03b2\u2032)}), (20) uses that K\u03bb \u03b1,\u03b2 = K\u03b1\u03bb,\u03b2\u03bb, (21) is an application of the discrete result, and the remaining steps apply the same results in reverse order. The final equality shows that the lower envelope of \u00b5 \u2227\u03bd can be assumed even in this general setting.\n# A.7 Proof of Proposition 5\nProposition 5 (Dependence on \u03b5). For any p \u2208[1, \u221e], 0 \u2264\u03b5 \u2264\u03b5\u2032 \u22641, and \u00b5, \u03bd \u2208P(X), we have\nW\u2225\u00b5\u2212\u03bd\u2225TV/2 p (\u00b5, \u03bd) \u2264(1 \u2212\u2225\u00b5 \u2212\u03bd\u2225TV/2)\u22121/p Wp(\u00b5 \u2227\u03bd, \u00b5 \u2227\u03bd) = 0.\nFor (ii), let \u00b5\u2032, \u03bd\u2032 be optimal for W\u03b51 p (\u00b5, \u03bd) (in the mass removal formulation) and take \u03c0 \u2208\u03a0(\u00b5\u2032, \u03bd\u2032) to be an optimal coupling for Wp(\u00b5\u2032, \u03bd\u2032). We can transform \u03c0 into a feasible\n(18)\n(19)\n(20)\n(21)\n(22)\n(23)\n(24)\ncoupling for W\u03b52 p by restricting ourselves to the fraction 1\u2212\u03b51 1\u2212\u03b52 of mass \u03c0\u2032 minimizing \u2225d\u2225Lp(\u03c0 This gives that\n \u2212 as desired. For (iii), we recall from Villani (2009, Theorem 6.13) that for \u03b1, \u03b2 \u2208M+(X) w \u03b1(X) = \u03b2(X), we have\n \u2264X\u2225 \u2212\u2225 Since any pair of feasible measures for W\u03b52 p are within (coordinate-wise) TV distance \u03b52 \u2212 from a pair of feasible measures for W\u03b51 p , this bound combined with the triangle inequa for Wp gives\n \u2264X\u2225 \u2212\u2225 Since any pair of feasible measures for W\u03b52 p are within (coordinate-wise) TV distance \u03b52 \u2212\u03b51 from a pair of feasible measures for W\u03b51 p , this bound combined with the triangle inequality for Wp gives (1 \u2212\u03b52)1/p W\u03b52 p (\u00b5, \u03bd) \u2265(1 \u2212\u03b51)1/p W\u03b51 p (\u00b5, \u03bd) \u22124 diam(X)(\u03b52 \u2212\u03b51)1/p,\nSince any pair of feasible measures for W\u03b52 p are within (coordinate-wise) TV distance \u03b52 \u2212\u03b51 from a pair of feasible measures for W\u03b51 p , this bound combined with the triangle inequality for Wp gives (1 \u2212\u03b5)1/p W\u03b5  (\u00b5, \u03bd) \u2265(1 \u2212\u03b5)1/p W\u03b5  (\u00b5, \u03bd) \u22124 diam(X)(\u03b5 \u2212\u03b5)1/p,\nProposition 6 (Alternative primal form). For any p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X), we have\nwhere \u03c01 and \u03c02 are the respective marginals of \u03c0. Proof. Starting from formulation given by Lemma 1, we compute\nW\u03b5 p(\u00b5, \u03bd) = inf \u00b5\u2032: \u00b5\u2208B\u03b5(\u00b5\u2032) \u03bd\u2032: \u03bd\u2208B\u03b5(\u03bd\u2032) Wp(\u00b5\u2032, \u03bd\u2032) = inf \u00b5\u2032: \u00b5\u2208B\u03b5(\u00b5\u2032) \u03bd\u2032: \u03bd\u2208B\u03b5(\u03bd\u2032) \u03c0\u2208\u03a0(\u00b5\u2032,\u03bd\u2032) \u2225d\u2225Lp(\u03c0) = inf \u03c0\u2208P(X\u00d7X) \u00b5\u2208B\u03b5(\u03c01) \u03bd\u2208B\u03b5(\u03c02) \u2225d\u2225Lp(\u03c0),\neliminating auxiliary variables \u00b5\u2032 and \u03bd\u2032 for the final equality.\n# A.9 Proof of Proposition 7\nProposition 7 (Convergence of couplings). Fix p \u2208[1, \u221e] and \u00b5, \u03bd \u2208P(X). If \u03b5n \u21980 and \u03c0n \u2208P(X \u00d7 X) is optimal for W\u03b5n p (\u00b5, \u03bd) via (8), for each n \u2208N, then {\u03c0n}n\u2208N admits a subsequence converging weakly to an optimal coupling for Wp(\u00b5, \u03bd).\nProof. In this case, because the two marginals of each \u03c0n are bounded by \u00b5 1\u2212\u03b5 and \u03bd 1\u2212\u03b5 respectively, this sequence is tight and admits a weakly convergent subsequence converging to some \u03c0\u22c6\u2208P(X \u00d7 X). Furthermore, \u03c0\u22c6(A \u00d7 X) = limn\u2192\u221e\u03c0n 1 (A) = \u00b5(A), and, likewise, \u03c0\u22c6(X \u00d7 B) = \u03bd(B); hence \u03c0\u22c6\u2208\u03a0(\u00b5, \u03bd). We now recall the definition of \u0393-convergence. Definition 1 (\u0393-Convergence). Let Y be a metric space and consider a sequence Fn : Y \u2192 R\u222a{\u221e}, n \u2208N. We say that {Fn}n\u2208N \u0393-converges to F : Y \u2192R\u222a{\u221e} and write Fn \u0393\u2192F if\n(8)\n(i) For every yn, y \u2208Y, n \u2208N, with yn \u2192y, we have F(y) \u2264lim infn\u2192\u221eFn(yn); (ii) For any y \u2208Y, there exists yn \u2208Y, n \u2208N, with yn \u2192y and F(y) \u2265lim supn\u2192\u221eFn(yn)\n(i) For every yn, y \u2208Y, n \u2208N, with yn \u2192y, we have F(y) \u2264lim infn\u2192\u221eFn(yn); (ii) For any y \u2208Y, there exists yn \u2208Y, n \u2208N, with yn \u2192y and F(y) \u2265lim supn\u2192\u221eFn(yn). If {yn}n\u2208N is a sequence of minimizers for Fn, for each n \u2208N, and Fn \u0393\u2192F, then any limit oint of {yn}n\u2208N is a minimizer of F (Maso, 2012, Corollary 7.20). Hence, it suffices to prove he \u0393-convergence of Fn : P(X \u00d7 X) \u2192R \u222a{\u221e} to F : P(X \u00d7 X) \u2192R \u222a{\u221e} as defined by\nFor the lim inf inequality, we start with any sequence \u03b3n \u2208P(X \u00d7 X), n \u2208N, with weak limit \u03b3 \u2208P(X \u00d7X). If {\u03b3n}n\u2208N does not contain a subsequence with \u00b5 \u2208B\u03b5n((\u03b3n)1), \u03bd \u2208B\u03b5n((\u03b3n)2), then the claim is trivial. Otherwise, \u03b3 \u2208\u03a0(\u00b5, \u03bd), and the Portmanteau Theorem gives\nF(\u03b3) = \u2225d\u2225Lp(\u03b3) \u2264lim inf n\u2192\u221e\u2225d\u2225Lp(\u03b3n) = Fn(\u03b3n),\nwhere we used the fact that dp is non-negative and continuous for p < \u221e. The fact that Lp norms are continuous in p \u2208[1, \u221e] implies that the inequality holds for p = \u221eas well. For the lim sup direction, fix \u03b3 \u2208\u03a0(\u00b5, \u03bd). Then, we must have \u00b5 \u2208B\u03b5n(\u03b31), \u03bd \u2208B\u03b5n(\u03b32) for all n \u2208N, so we can consider the constant sequence \u03b3n \u2261\u03b3, n \u2208N, and obtain\nwhich implies the desired condition.\n# A.10 Proof of Proposition 8\nProposition 8 (Finite support). Let \u00b5 and \u03bd be uniform discrete measures over n points each. Then there exist optimal \u00b5\u2032, \u03bd\u2032 for W\u03b5 p(\u00b5, \u03bd) such that \u00b5\u2032 and \u03bd\u2032 each give mass 1/n to \u230a(1 \u2212\u03b5)n\u230bpoints and assign their remaining \u2308\u03b5n\u2309/n \u2212\u03b5 mass to a single point.\nProof. Assume first that p < \u221eand \u03b5n = k for some k \u2208N. Suppose that \u00b5 is uniform over {x1, . . . , xn} and \u03bd is uniform over {y1, . . . , yn}. Define the cost matrix C \u2208Rn\u00d7n with Cij = d(xi, yj)p. Then, we can rewrite definition (1) for W\u03b5 p as a linear program, computing\nThis representation relates to the problem of characterizing the extreme points of the polytope of doubly substochastic n \u00d7 n matrices with entries summing to an specified integer. This polytope was studied in Cao and Chen (2019), where Theorem 4.1 shows that the extreme\npoints of interest are exactly the partial permutation matrices of order n \u2212k. This implies the existence of a coupling for W\u03b5 p(\u00b5, \u03bd)p whose marginals are uniform over n \u2212k = (1 \u2212\u03b5)n points (giving mass 1/n to each point). When \u03b5 is not a multiple of 1/n, the same result (Cao and Chen, 2019, Theorem 4.1) reveals that there are optimal perturbed measures that each give mass 1/n to \u230a(1 \u2212\u03b5)n\u230b points and give the remaining mass to a single point. For p sufficiently large, the set of minimizers to the above problem stabilizes to a constant set, so this argument captures the p = \u221ecase as well.\n# A.11 Proof of Proposition 4\nProposition 4 (Interpreting maximizers). If f \u2208Cb(X) maximizes (7), then any \u00b5\u2032, \u03bd\u2032 \u2208 M+(X) minimizing (1) satisfy supp(\u00b5 \u2212\u00b5\u2032) \u2286argmax(f) and supp(\u03bd \u2212\u03bd\u2032) \u2286argmin(f).\nProof. Take f \u2208Cb(X) maximizing (7) (by Theorem 2, such f is guaranteed to exist). Take any \u00b5\u2032 = \u00b5 \u2212\u03b1, \u03bd\u2032 = \u03bd \u2212\u03b2 optimal for the mass removal formulation of W\u03b5 p(\u00b5, \u03bd), where \u03b1, \u03b2 \u2208M+(X) with \u03b1(X) = \u03b2(X) = \u03b5. Of course, \u00b5 + \u03b2 and \u03bd + \u03b1 are then optimal for the mass addition formulation of W\u03b5 p(\u00b5, \u03bd). Examining the proof of Theorem 2, we see that \u00b5 + \u03b2, \u03bd + \u03b1 and f, f c must be a minimax equilibrium for the problem given in (17). Consequently, we have\nSince infy f c(y) = \u2212supx f(x) (and the minimizers of f c correspond to the maximizers of f), we have that (25) is strictly less than (26) unless supp(\u03b2) \u2286argmin(f) and supp(\u03b1) \u2286 argmax(f). This suggests taking \u03b1 = \u00b5|argmax(f) and \u03b2 = \u03bd|argmin(f), but we cannot do so in general; consider the case where \u00b5 and \u03bd are uniform discrete measures on n points and \u03b5 is not a multiple of 1/n. Issues with that approach also arise when \u00b5 and \u03bd are both supported on argmax(f) and the optimal \u00b5\u2032 satisfies \u00b5\u2032 \u2265\u00b5 \u2227\u03bd.\n# A.12 Proof of Proposition 2\nProposition 2 (Loss trimming dual). Fix p \u2208[1, \u221e) and take \u00b5, \u03bd to be uniform distribution over n points each. If \u03b5 \u2208[0, 1] is a multiple of 1/n, then\n\uf8ed \uf8f8 Proof. We place no restrictions on \u00b5 and \u03bd initially and extend to the two-sided robust distance W\u03b51,\u03b52 p defined in Appendix B.1. For this result, we will apply Sion\u2019s minimax theorem to the mass-removal formulation of W\u03b51,\u03b52 p . Mirroring the proof of Theorem 2, we\n(25)\n(26)\nas desired\n# A.13 Proof of Proposition 9\n# Proposition 9 (One-sample rates). Fix p \u2208[1, \u221e] and \u03c4 \u2208(\u03b5, 1]. If p < d/2, we have \ufffd \ufffd\n\ufffd \ufffd for a constant C independent of n and \u03b5. Furthermore, if d\u03c4 \u2217(\u00b5) > s , then W\u03b5 p(\u00b5, \u03b3) \u2265C\u2032 (\u03c4 \u2212\u03b5)1/p n\u22121/s\nfor any \u03b3 supported on at most n points, including \u02c6\u00b5n, where C\u2032 > 0 is an absolute constant. Proof. For the upper bound, we simply use that W\u03b5 p(\u00b5, \u03bd) \u2264Wp(\u00b5, \u03bd) and apply well-known empirical convergence results for Wp which give E[Wp(\u00b5, \u02c6\u00b5n)] \u2264Cn\u22121/d for a constant C independent of n (and \u03b5); see, e.g., Weed and Bach (2019). For the lower bound, if d\u2217(\u00b5) > s, then there is \u03b40 > 0 such that N\u03b4(\u00b5, \u03c4) \u2265\u03b4\u2212t for all \u03b4 \u2264\u03b40. By Proposition 3, we know that W\u03b5 p(\u00b5, \u03b3) is achieved by \u00b5\u2032 \u2264\u00b5 and \u03b3\u2032 \u2264\u03b3, with | supp(\u03b3\u2032)| \u2264| supp(\u03b3)| \u2264n. Fix \u03b4 = n\u2212s/2 and take n sufficiently large so that \u03b4 \u2264\u03b40. Let S = \u222ax\u2208supp(\u03b3\u2032)B(x, \u03b4/2), where B(x, r) := {y \u2208X : d(x, y) \u2264r} is the ball of radius r centered at x. Since N(\u00b5, \u03c4) \u2265\u03b4\u2212t > n, we have that \u00b5(S) \u22641 \u2212\u03c4, which further implies \u00b5\u2032(S) \u22641 \u2212\u03c4 + \u03b5. For any \u03c0 \u2208\u03a0(\u00b5\u2032, \u03b3\u2032), we now have \ufffd \ufffd \ufffd \ufffd\nwhich gives W\u03b5 p(\u00b5, \u03b3) = (1 \u2212\u03b5)\u22121/p Wp(\u00b5\u2032, \u03b3\u2032) \u22651 4(\u03c4 \u2212\u03b5)1/p(1 \u2212\u03b5)\u22121/p n\u22121/s. This suffic since (1 \u2212\u03b5)\u22121/p \u22651.\n# A.14 Proof of Corollary 2\nCorollary 2 (Simple one-sample rate). Fix p < d/2 and \u03b5 \u2208(0, 1]. If \u00b5 has absolutely continuous part f d\u03bb with \ufffd f d\u03bb > \u03b5 and f bounded from above, then d\u03c4 \u2217(\u00b5) = d and E \ufffd W\u03b5 p(\u00b5, \u02c6\u00b5n) \ufffd = \u0398(n\u22121/d).\nCorollary 2 (Simple one-sample rate). Fix p < d/2 and \u03b5 \u2208(0, 1]. If \u00b5 has absolutely continuous part f d\u03bb with \ufffd f d\u03bb > \u03b5 and f bounded from above, then d\u03c4 \u2217(\u00b5) = d and E \ufffd W\u03b5 p(\u00b5, \u02c6\u00b5n) \ufffd = \u0398(n\u22121/d). Proof. Set \u03c4 = ( \ufffd f d\u03bb + \u03b5)/2, so that \u03b5 < \u03c4 < inf f d\u03bb. Then, for any covering of X with balls of radius \u03b4 except for A with \u00b5(A) \u2264\u03c4, we have\nfor a positive constant c > 0. This gives that \ufffd X\\A f d\u03bb > c, and hence \u03bb(X \\A) \u2265c/\u2225f\u2225\u221e> 0. Since X \\ A has Lebesgue measure bounded away from 0, a volumetric argument implies that the covering must contain at least C\u03b4\u2212d balls of radius \u03b4 for some C > 0. This implies the desired lower covering dimension and, by the proof of the previous lower bound, the desired empirical convergence rate.\n# A.15 Proof of Proposition 10\nProposition 10 (Two-sample rate). For any \u03b5 \u2208[0, 1] and p < d/2, we have E \ufffd\ufffd\ufffdW\u03b5 p(\u02c6\u00b5n, \u02c6\u03bdn)p \u2212W\u03b5 p(\u00b5, \u03bd)p\ufffd\ufffd\ufffd \u2264Cn\u2212p/d.\n\ufffd\ufffd\ufffd for a constant C independent of n and \u03b5. Proof. To begin, we write\n(1 \u2212\u03b5)W\u03b5 p(\u00b5, \u03bd)p = sup f\u2208Cb(X) \u00b5(f) + \u03bd(f c) \u2212Range(f) (1 \u2212\u03b5)W\u03b5 p(\u02c6\u00b5n, \u02c6\u03bdn)p = sup f\u2208Cb(X) \u2212\u02c6\u00b5n(f c) \u2212\u02c6\u03bdn(f) \u2212Range(f),\n(1 \u2212\u03b5) \ufffd\ufffdW\u03b5 p(\u02c6\u00b5n, \u02c6\u03bdn)p \u2212W\u03b5 p(\u00b5, \u03bd)p\ufffd\ufffd\u2264 sup f\u2208Cb(X) \ufffd\ufffd\ufffd\ufffd \ufffd f d\u00b5 + \ufffd f c d\u02c6\u00b5n + \ufffd f d\u02c6\u03bdn + \ufffd f c d\u03bd \ufffd\ufffd\ufffd\ufffd \u2264Wp(\u00b5, \u02c6\u00b5n)p + Wp(\u03bd, \u02c6\u03bdn)p.\nThe standard lower bounds for two-sample empirical convergence (Talagrand, 1992; Barthe and Bordenave, 2013) do not immediately transfer to this setting. When \u00b5 = \u03bd are both an\nabsolutely continuous distribution like Unif([0, 1]d), these can easily be adapted to provide lower bound for\n\uf8f0 \uf8ed \uf8f8 For this to be helpful for the present case, one would also need to bound\nwhich we defer for future work.\n# A.16 Proof of Proposition 11\nProposition 11 (Exact recovery). Fix \u00b5, \u03bd \u2208P(X), \u03b5 \u2208[0, 1], and suppose that \u02dc\u00b5 = (1 \u2212\u03b5)\u00b5 + \u03b5\u03b1 and \u02dc\u03bd = (1 \u2212\u03b5)\u03bd + \u03b5\u03b2, for some \u03b1, \u03b2 \u2208P(X). Let S = supp(\u00b5 + \u03bd). If d(supp(\u03b1), S), d(supp(\u03b2), S), and d(supp(\u03b1), supp(\u03b2)) are all greater than diam(S), then W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd) = Wp(\u00b5, \u03bd).\nProof. Under the conditions of the proposition, we will prove that \u00b5\u2032 = (1 \u2212\u03b5)\u00b5 and \u03bd\u2032 = (1 \u2212\u03b5)\u03bd are the unique minimizers for W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd), implying that W\u03b5 p(\u02dc\u00b5, \u02dc\u03bd) = Wp(\u00b5, \u03bd). Suppose not, and take \u00b5\u2032 and \u03bd\u2032 optimal for (1) such that \u00b5\u2032 = \u00b5\u2032 g+\u00b5\u2032 b, where \u00b5\u2032 g := \u00b5\u2032\u2227(1\u2212\u03b5)\u00b5 and \u00b5\u2032 b = \u00b5\u2032 \u2212\u00b5\u2032 g \u2264\u03b5\u03b1, \u00b5\u2032 b \u0338= 0. Let \u03c0 \u2208\u03a0(\u00b5\u2032, \u03bd\u2032) be an optimal coupling for Wp(\u00b5\u2032, \u03bd\u2032), and let \u03c0(y|x) be the regular conditional probability distribution such that \u03bd\u2032(\u00b7) = \ufffd X \u03c0(\u00b7|x) d\u00b5\u2032(x). Then we can compute\n= (1 \u2212\u03b5)Wp(\u00b5, \u03bd)p,\na contradiction. The same argument goes through if the three relevant distances are greater than \u2225d\u2225L\u221e(\u03c0), although general conditions under which this norm is finite seem hard to obtain (unless p = \u221e, in which case \u2225d\u2225L\u221e(\u03c0) = W\u221e(\u00b5\u2032, \u03bd\u2032) \u2264W\u221e(\u00b5, \u03bd)).\n# A.17 Proof of Proposition 12\nProposition 12 (Robustness radius for unknown \u03b5). Assume the setting of Proposition 11 and let D be the maximum of d(supp(\u03b1), S), d(supp(\u03b2), S), and d(supp(\u03b1), supp(\u03b2)). Then, for p \u2208[1, \u221e), we have\nd d\u03b4 \ufffd (1\u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p\ufffd \u2264\u2212Dp, d d\u03b4 \ufffd (1\u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p\ufffd \u2265\u2212diam(S)p > \u2212Dp,\n\ufffd \ufffd at the (all but countably many) points where the derivative is defined.\nProof. By Proposition 5, (1 \u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p is continuous and decreasing in \u03b4, so it must be differentiable except on a countable set. Now, if 0 \u2264\u03b4\u2032 < \u03b4 \u2264\u03b5, then the proof of Proposition 11 reveals that there are optimal \u00b5\u2032, \u03bd\u2032 for W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd) and \u00b5\u2032\u2032, \u03bd\u2032\u2032 for W\u03b4\u2032 p (\u02dc\u00b5, \u02dc\u03bd) such that \u02dc\u00b5 \u2265\u00b5\u2032\u2032 \u2265\u00b5\u2032 \u2265(1 \u2212\u03b5)\u00b5 and \u02dc\u03bd \u2265\u03bd\u2032\u2032 \u2265\u03bd\u2032 \u2265(1 \u2212\u03b5)\u03bd. Letting \u03c0 \u2208\u03a0(\u00b5\u2032\u2032, \u03bd\u2032\u2032) be an optimal coupling for Wp(\u00b5\u2032\u2032, \u03bd\u2032\u2032) and defining \u03c0(y|x) as before, we compute\n(1 \u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p \u2212(1 \u2212\u03b4\u2032)W\u03b4\u2032 p (\u02dc\u00b5, \u02dc\u03bd)p = Wp(\u00b5\u2032, \u03bd\u2032)p \u2212Wp(\u00b5\u2032\u2032, \u03bd\u2032\u2032)p = Wp(\u00b5\u2032, \u03bd\u2032)p \u2212\u2225d\u2225p Lp(\u03c0) \ufffd\ufffd\nTaking \u03b4\u2032 \u2192\u03b4, we find that the derivative of interest is bounded by \u2212Dp from above wherever it exists. On the other hand, if \u03b5 \u2264\u03b4 < \u03b4\u2032 \u22641, then there are optimal \u00b5\u2032, \u03bd\u2032 for W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd) and \u00b5\u2032\u2032, \u03bd\u2032\u2032 for W\u03b4\u2032 p (\u02dc\u00b5, \u02dc\u03bd) such that (1 \u2212\u03b5)\u00b5 \u2265\u00b5\u2032 \u2265\u00b5\u2032\u2032 \u22650 and (1 \u2212\u03b5)\u03bd \u2265\u03bd\u2032 \u2265\u03bd\u2032\u2032. Letting \u03c0 \u2208\u03a0(\u00b5\u2032\u2032, \u03bd\u2032\u2032) again be an optimal coupling for Wp(\u00b5\u2032\u2032, \u03bd\u2032\u2032), we compute\n(1 \u2212\u03b4\u2032)W\u03b4\u2032 p (\u02dc\u00b5, \u02dc\u03bd)p \u2212(1 \u2212\u03b4)W\u03b4 p(\u02dc\u00b5, \u02dc\u03bd)p = Wp(\u00b5\u2032\u2032, \u03bd\u2032\u2032)p \u2212Wp(\u00b5\u2032, \u03bd\u2032)p\nTaking \u03b4\u2032 \u2192\u03b4, we find that the derivative of interest is bounded by \u2212diam(S)p > \u2212Dp from below, wherever it exists.\n\u03b4 \u2208[0, \u03b5) \u03b4 \u2208(\u03b5, 1]\n# A.18 Proof of Proposition 13\nProposition 13 (Robust consistency). Fix p < \u221eand suppose that \u03c4\u00b5(n) \u2228\u03c4\u03bd(n) = O(n almost surely, for some a > 0. Then, setting \u03b5n = n\u2212b, for any 0 < b < a, we have\n# Proposition 13 (Robust consistency). Fix p < \u221eand suppose that \u03c4\u00b5(n) \u2228\u03c4\u03bd(n) = O(n\u2212a)",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of outlier sensitivity in Wasserstein distances, which are popular metrics for comparing probability distributions. Existing methods struggle with outliers, necessitating the development of more robust alternatives.",
        "problem": {
            "definition": "The problem focuses on the sensitivity of Wasserstein distances to outliers in probability distributions, which can lead to significant inaccuracies in distance calculations.",
            "key obstacle": "The main challenge is that even a small amount of outlier mass can disproportionately affect the Wasserstein distance due to strict marginal constraints."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for a robust measure that can effectively handle outliers without compromising the integrity of distance calculations.",
            "opinion": "The proposed solution is the \u03b5-outlier-robust Wasserstein distance, which allows for the exclusion of a fraction of outlier mass from the distributions being compared.",
            "innovation": "This method introduces a robust proxy for the Wasserstein distance that is derived from the Huber \u03b5-contamination model, offering a more regular optimization problem compared to previous approaches."
        },
        "method": {
            "method name": "\u03b5-outlier-robust Wasserstein distance",
            "method abbreviation": "W\u03b5 p",
            "method definition": "W\u03b5 p is defined as a distance measure that allows for the removal of \u03b5 fraction of outlier mass from probability distributions before computing the Wasserstein distance.",
            "method description": "The method modifies the standard Wasserstein distance calculation to account for outliers by excluding a specified fraction of mass.",
            "method steps": [
                "Identify the contaminated distributions.",
                "Remove \u03b5 fraction of outlier mass from each distribution.",
                "Renormalize the remaining mass.",
                "Calculate the Wasserstein distance using the modified distributions."
            ],
            "principle": "The effectiveness of W\u03b5 p lies in its ability to regularize the optimization problem, allowing for robust distance calculations that are less influenced by outliers."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using contaminated datasets, comparing the performance of the proposed method against baseline Wasserstein distance calculations.",
            "evaluation method": "Performance was assessed by measuring the accuracy of distance estimates in the presence of outliers and analyzing convergence rates."
        },
        "conclusion": "The experiments demonstrated that the \u03b5-outlier-robust Wasserstein distance provides significant advantages in robustness against outliers, maintaining accuracy in distance calculations.",
        "discussion": {
            "advantage": "The primary advantage of W\u03b5 p is its robustness to outliers, allowing for more reliable distance measurements in practical applications.",
            "limitation": "A limitation of the method is that it may not perform optimally in scenarios with extreme outlier contamination beyond the \u03b5 threshold.",
            "future work": "Future research will focus on refining the statistical bounds, exploring generalizations of the method, and applying it to high-dimensional inference tasks."
        },
        "other info": [
            {
                "info1": "The method can be integrated into existing duality-based optimal transport solvers with minimal adjustments.",
                "info2": {
                    "info2.1": "The dual formulation provides a straightforward way to implement the robust distance in practical applications.",
                    "info2.2": "Potential applications include generative modeling and robust statistical inference."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem focuses on the sensitivity of Wasserstein distances to outliers in probability distributions, which can lead to significant inaccuracies in distance calculations."
        },
        {
            "section number": "2.5",
            "key information": "The proposed solution is the \u03b5-outlier-robust Wasserstein distance, which allows for the exclusion of a fraction of outlier mass from the distributions being compared."
        },
        {
            "section number": "3.2",
            "key information": "W\u03b5p is defined as a distance measure that allows for the removal of \u03b5 fraction of outlier mass from probability distributions before computing the Wasserstein distance."
        },
        {
            "section number": "3.5",
            "key information": "The primary advantage of W\u03b5p is its robustness to outliers, allowing for more reliable distance measurements in practical applications."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the method is that it may not perform optimally in scenarios with extreme outlier contamination beyond the \u03b5 threshold."
        },
        {
            "section number": "7.2",
            "key information": "Future research will focus on refining the statistical bounds, exploring generalizations of the method, and applying it to high-dimensional inference tasks."
        }
    ],
    "similarity_score": 0.6472902178868573,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Outlier-Robust Optimal Transport_ Duality, Structure, and Statistical Analysis.json"
}