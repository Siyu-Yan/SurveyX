{
    "from": "google",
    "scholar_id": "pT-4NWWoY-UJ",
    "detail_id": null,
    "title": "User behavior simulation with large language model based agents",
    "abstract": " Abstract\n\nSimulating high quality user behavior data has always been a fundamental problem in humancentered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.\n\nKeywords: large language model, user behavior analysis, user s\n\n# Introduction\n\nHuman-centered artificial intelligence (AI) strives to advance the techniques that serve humanity, which has greatly enhanced the quality of human life and improved the efficiency of social resource allocation[1, 2]. Typical human-centered AI applications include recommender systems, social networks, and so on, where the basis of these areas lies in the availability of sufficient and reliable data. Yet, by the constraints related to commercial confidentiality and user privacy concerns, acquiring real human data in practice is prohibitively expensive or even poses ethical dilemmas [3, 4]. To solve this problem, people have proposed various user behavior simulation strategies [5\u2013 7], expecting that the synthetic data can provide reliable complements when the real-world data is hard to obtain. While previous simulation strategies have achieved remarkable successes, they suffer from several significant limitations: (1) Simplified use",
    "bib_name": "wang2023user",
    "md_text": "# User Behavior Simulation with Large Language Model based Agents\n\nLei Wang 1, Jingsen Zhang 1, Hao Yang 1, Zhi-Yuan Chen 1, Jiakai Tang 1, Zeyu Zhang 1, X Chen 1,2, \u02da, Yankai Lin 1,2, \u02da, Hao Sun 1,2, Ruihua Song 1,2, Wayne Xin Zhao 1,2, Jun Xu 1,2, Zhicheng Dou 1,2, Jun Wang 3, Ji-Rong Wen 1,2, \u02da\n\n1 Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2 Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China 3 University College London, London, UK\n\n* Corresponding authors\n\n# Abstract\n\nSimulating high quality user behavior data has always been a fundamental problem in humancentered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.\n\nKeywords: large language model, user behavior analysis, user s\n\n# Introduction\n\nHuman-centered artificial intelligence (AI) strives to advance the techniques that serve humanity, which has greatly enhanced the quality of human life and improved the efficiency of social resource allocation[1, 2]. Typical human-centered AI applications include recommender systems, social networks, and so on, where the basis of these areas lies in the availability of sufficient and reliable data. Yet, by the constraints related to commercial confidentiality and user privacy concerns, acquiring real human data in practice is prohibitively expensive or even poses ethical dilemmas [3, 4]. To solve this problem, people have proposed various user behavior simulation strategies [5\u2013 7], expecting that the synthetic data can provide reliable complements when the real-world data is hard to obtain. While previous simulation strategies have achieved remarkable successes, they suffer from several significant limitations: (1) Simplified user decision processes. Existing work mostly leverages simple functions like inner product or multilayer perceptron to simulate user decisions [5, 6], which is far from the intricate mechanism of human cognitive processes, and thus may not produce reliable\n\nuser behaviors. (2) Real-data dependence. As mentioned above, the basic motivation of the simulation methods should be simulating user behaviors where we cannot collect sufficient real-world datasets. However, to start up the simulation process, existing work has to leverage real-world datasets to learn the simulators, which leads to the \u201cchicken and egg\u201d problem [8, 9]. Actually, traditional methods can only simulate user behavior patterns similar to the ones of known real-world datasets. Obviously, a more promising character of the simulator should be making (nearly) zeroshot simulation, which can significantly extend its application scope. (3) Simplified simulation environments. Traditional simulation methods usually limit themselves to one scenario, such as the recommender system [6] or the social network [10, 11]. However, real user behaviors happen in diverse environments, which may mutually influence each other. For example, users may share their opinions on social networks after watching movies in recommender systems; the watching behaviors of the users may also be triggered by their friends\u2019 suggestions on social networks [12\u2013 14]. As mentioned before, existing simulators mostly have to start up with real-world datasets. However, acquiring multi-environment data with aligned users is difficult. This could be the basic reason why existing simulators mostly limit themselves to single environments. Recently, large language models (LLM) [15, 16] have demonstrated immense potential in achieving human-level intelligence and generalization capabilities [17, 18]. People find that the logic and patterns of human minds could be captured by learning extensive web data based on language models [19, 20]. In fact, most user behaviors like movie watching, friend chatting, and social message posting can be formulated in languages, and a large amount of user behavior corpora can be seen on the web and have been learned into LLMs, which builds the basis for believable user behavior simulation [21]. In addition, LLMs can work surprisingly well on almost all types of language tasks after training on extensive web corpora [15]. Based on this character, one can leverage LLMs to simulate user behaviors by accessing only a few or even no real-world data. This makes (nearly) zero-shot simulation possible, and people can easily simulate multiple environments, greatly widening the application scope of the simulation methods in the field of user behavior analysis. The purpose of this paper is to explore the potential of using LLMs for user behavior simulation 1. To this end, we take inspiration from the cognitive neuroscience to design an LLM-based agent framework, which is composed of a profile module, a memory module and an action module. To observe the interactive behaviors of the agents, we design an intervenable and resettable sandbox environment, where the agents can interact with the web recommender system, chat with each other and broadcast information to all their friends. Through extensive experiments, we demonstrate that the simulated behaviors of our method are very close to the real humans\u2019. To showcase the potential applications of our simulator, we leverage it to study two well-know social phenomenons including information cocoons and user conformity. We find that our simulator can well reproduce these phenomenons, and by using our simulator as an experimental platform, we can discover several potential strategies to alleviate these phenomenons.\n\n# Results\n\n# User behavior simulator with LLM-based agents\n\nThe overall framework of our designed simulator can be seen in Figure 1  (we name our simulator as RecAgent). For each user, we build an LLM-based agent to simulate her behaviors. The agent is composed of a profile module, a memory module, and an action module. The profile module determines the background of the users. It includes features like ID, name, gender, age,\n1 The initial version of this paper has been released on 2023-6-5 at https://arxiv.org/abs/2306.02552\n\nThe overall framework of our designed simulator can be seen in Figure 1 (we name our lator as RecAgent). For each user, we build an LLM-based agent to simulate her behaviors agent is composed of a profile module, a memory module, and an action module. The p module determines the background of the users. It includes features like ID, name, gender\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8409/84097baa-233d-4f47-9027-cc2ad14dd62c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b165/b165c918-27d2-4acd-bcae-47e321a98378.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) The Agent Framework\n</div>\n<div style=\"text-align: center;\">e 1: a, A brief running process of the simulator. b, The agent framework, which includes a profile module, a y module, and an action module. c, Key characters of the simulator. Different agents behave in a round-bymanner based on Pareto distribution, where, in each round, only a small number of agents (labeled in blue) tions. One can actively change the simulation process by, for example, altering the agent properties, and can rticipate into the simulation process to interact with the recommender system or other agents.\n</div>\ntake actions. One can actively change the simulation process by, for example, altering the agent properties, and can also participate into the simulation process to interact with the recommender system or other agents.\ntraits, career and interests. The traits indicate the user\u2019s personality, such as being \u201ccompassionate\u201d, \u201cambitious\u201d, or \u201coptimistic\u201d. The interests are represented by item categories, for example, \u201csci-fi movies\u201d or \u201ccomedy videos\u201d. The memory module enables the agents to remember past behaviors, and evolve dynamically in the environment. We follow the working mechanism of human memory in cognitive neuroscience [22, 23] to design three types of agent memories, that is, sensory memory, short-term memory and long-term memory. The sensory memory directly interacts with the environment, and it summarizes raw observations from the environment into more informative\n\ntraits, career and interests. The traits indicate the user\u2019s personality, such as being \u201ccompassionate\u201d, \u201cambitious\u201d, or \u201coptimistic\u201d. The interests are represented by item categories, for example, \u201csci-fi movies\u201d or \u201ccomedy videos\u201d. The memory module enables the agents to remember past behaviors, and evolve dynamically in the environment. We follow the working mechanism of human memory in cognitive neuroscience [22, 23] to design three types of agent memories, that is, sensory memory, short-term memory and long-term memory. The sensory memory directly interacts with the environment, and it summarizes raw observations from the environment into more informative\n\n# Round-Based Simulation with Pareto-Distributed Agent Action\n\n<div style=\"text-align: center;\">(c) The Simulator lmplementations\n</div>\nand concise contents. Short-term memory serves as an intermediate part connecting sensory memory and long-term memory. If an agent repeatedly encounters similar observations, then the related short-term memories are enhanced and transformed into long-term memories. Long-term memory stores important information that can be reused in similar environments or generalize to unseen observations. In addition, the agent can self-reflect based on long-term memory, generating highlevel and abstract information from specific observations. The action module produces specific agent behaviors. In our simulator, there are four types of behaviors that the agent can take in the recommender system, that is: (1) Searching behaviors: by these behaviors, the agents can actively search the items of interests. (2) Browsing behaviors: by these behaviors, the agents can receive recommendations from the system in a passive manner. (3) Clicking behaviors: by these behaviors, the agents can choose the items they would like to watch/purchase. (4) Next-page behaviors: these behaviors are triggered when the user is not satisfied with the current recommended/searched items, and would like to see more results. Beyond the above behaviors related to the recommender system, we also simulate two types of social behaviors, that is: (5) One-to-one chatting, where two users discuss and exchange their information, such as chatting online via Twitter, WeChat and so on, or talking offline in a coffee shop. These behaviors may trigger users to interact with the discussed items, or change their memories to influence subsequent actions. (6) One-to-many broadcasting, where one user shares information to the other users, such as posting opinions on social media, or sending business advertisements. These behaviors may influence the memories and actions of the users who receive the shared information. From the system perspective, our simulator operates in a round-by-round manner. During each round, the agents autonomously decide whether they would like to take actions. In real-world scenarios, different users may take actions (e.g., accessing the recommender system or social media) with various activity levels. These activity levels usually follow long-tail distributions 2, meaning that a small number of individuals are highly active, while the majority of users have low action frequency [24]. In order to incorporate such a characteristic into our simulator, we model the activity levels of different agents according to a Pareto distribution 3. In specific, the probability density of the activity level is represented as follows:\n\nwhere x min is the minimum activity level, and \u03b1 is used to control the shape of the distribution [25]. We evaluate the effectiveness of p p x q on characterizing real-world datasets in the Supplementary Note A.5. Our simulator is agnostic to the recommendation algorithms, which can be flexibly indicated and changed. In addition, we also allow real humans to involve into the simulation process, where the real human can play as an agent, and interact with the recommender system and the other agents. At last, to flexibly control the simulation process, our simulator can be intervened from outside in a global manner. For example, one can \u201cinterview\u201d the agents by asking them questions like \u201cWhat would you say when you want to discuss the movies you\u2019ve recently watched with others?\u201d. More details about our simulator can be found in the Method section.\n\n# Believability of the simulated user behaviors\n\nIn this section, we evaluate the believability of the simulated behaviors, where we measu behaviors in the recommender system and chatting/broadcasting behaviors separately.\n\nIn this section, we evaluate the believability of the si behaviors in the recommender system and chatting/broa\n\n2 https://en.wikipedia.org/wiki/Long_tail 3 https://en.wikipedia.org/wiki/Pareto_distribution\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68c1/68c1ff61-26a5-480f-98a6-4c4009639cb8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e 2: Evaluation on the believability of the simulated user behaviors. a, Evaluation on the recomtion behaviors based on different p a, b q \u2019s (discrimination capability). b, Evaluation on the recommendation ors based on different N \u2019s (generation capability). c, Evaluation on the chatting and broadcasting behaviors.\n</div>\nBehaviors in the recommender system. To evaluate the believability of the behaviors in a recommender system, we first sample 20 users as well as their interactions from a well-known recommendation dataset\u2013Movielens-1M 4. Then, for each user, we leave her last a items as the evaluation ground truths, and use all the other items to initialize the agent profiles. For evaluation, we combine the ground truths with b negative items to form a recommendation list containing p a ` b q items. We present the recommendation list to the agent, and let it select a items to compare with the ground truth. Suppose, for a user u, the ground truth and selected item sets are T u and S u, respectively, then we use the following metric to evaluate different models:\n\nwhere U is the set of all users, and larger p indicates better performance. In the experiments, we set a and b with various values to verify the generality of the agent capability. In intuitively, selecting a smaller number of ground truth from a larger pool of candidates should be more difficult.\n\n4 https://grouplens.org/datasets/movielens/1m/\n\nWe compare our simulator with traditional simulation methods including Embedding [26] and RecSim [6], and also let real humans to make selections from the recommendation list (Real Human). The experiment results are presented in Figure 2 a, from which we can see: for different p a, b q \u2019s, the Embedding method is comparable with RecSim, while by using LLMs, our simulator can significantly improve the performances. This result demonstrates the potential of using LLM-based agents on simulating user behaviors. If one compares the results of our simulator and Real Human, she may find that the performance gaps are not large. On average, the performance of our simulator can surpass the best baseline by about 68%, and is only 8% lower than the results of Real Human. These results are encouraging, and demonstrate that our simulator may have made an important step towards much more believable user behavior simulation. Beyond the above experiments on measuring the agent discrimination capability, we also evaluate whether the agents can directly generate believable user behavior sequences (i.e., generation capability). To this end, we follow the above experiment settings to profile the agents with several early interactions and regard the following N items as reference behaviors (RB). A straightforward evaluation method can be computing the overlap between the simulated and reference behaviors. However, in practice, a user may have multiple possible behaviors at each decision step, but only one of them is finally executed and logged into the system. This makes the reference behaviors not the only ground truth, and the simulated behaviors can also be reasonable even without any overlap with the reference behaviors. To overcome this challenge, we follow the previous work [27, 28] to conduct adversarial subjective evaluation, where a more believable simulator should be able to more readily \u201cfool\u201d the human annotator. In specific, we recruit three human annotators to make comparisons on RecAgent v.s. RB and RecSim v.s. RB. For each pair of comparison, the annotators are given: (i) the previously interacted movies of a user, and (ii) two behavior sequences of the same user generated from different methods. The annotators are required to answer the following question: According to the user\u2019s past behaviors, indicate which behavior sequence is more like the real human\u2019s? They can either choose one sequence or answer \u201cindistinguishable\u201d. If the annotators select results from RecAgent or RecSim, then the simulator win, otherwise, the simulator lose. The win, lose and indistinguishable rates of different simulators are reported in Figure 2 b. We can see, when N \u201c5, the win rate of RecAgent (45.0%) is significantly better than that of RecSim (33.3%), which suggest that our simulator can produce more believable user behavior sequences comparing with RecSim. When the generated sequence becomes longer, RecAgent can still outperform RecSim by about 5.0%, which demonstrates that the superiority of RecAgent is a general conclusion. Chatting and Broadcasting Behaviors. In practice, it is hard to find datasets which simultaneously containing user recommendation, chatting and broadcasting behaviors. For traditional simulation strategies, the user model trained on one type of user behaviors can hardly generalize to the other ones, since the semantic spaces and patterns of different behaviors may have significant gaps. For example, the above recommendation simulator Embedding or RecSim cannot generate user chatting or broadcasting behaviors. For our method, since LLMs have already learned almost all the web corpus, they have sufficient knowledge to understand and simulate different behaviors. In this section, we evaluate whether our simulator can produce believable user behaviors even without fine-tuning on domain-specific datasets. In specific, we deploy 20 agents in the simulator, and observe their chatting and broadcasting behaviors after 5, 10 and 15 rounds of executions. We recruit three annotators, and present them with the agents\u2019 profiles and previously watched movies. They are required to answer the following questions: (1) Whether the chatting behaviors of the agent are believable according to its profiles?; (2) Whether the chatting behaviors of the agent are believable according to its previous behaviors?; (3) Whether the broadcasting behaviors of the agent are believable according to its profiles?; (4) Whether the broadcasting behaviors of the agent are believable according to its previous behaviors? The annotators have to give a score between 1 and 5\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c72/0c722c8e-db39-4154-8113-bdc50e43d46d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Task 1\n</div>\nEvaluation on the agent memory module. a, Evaluation on different key functions of the memory, Evaluation on the memory module as a whole.\n\n<div style=\"text-align: center;\">Figure 3: Evaluation on the agent memory module. a, Evaluation on different key functions of the m module. b, Evaluation on the memory module as a whole.\n</div>\n(higher rating indicates more believable) to each of the above questions. The results are reported by averaging all the scores across the annotators and agents. From Figure 2 c, we can see, most of the results are higher than 4, suggesting that our simulator can indeed produce believable chatting and broadcasting behaviors. However, as the simulator runs longer, the performances goes down in general. After 15 rounds of execution, the believability scores for all the questions are lower than 4. We speculate that, after 15 rounds, the agent memory may have stored a large amount of contents, and the LLM may lose its attention and fail to extract valuable information to simulate behaviors.\n\n# Believability of the Agent Memory Mech\n\nIn this section, we evaluate the rationality of the agent memory mechanism, which is the key part for driving user behaviors. To begin with, we evaluate the believability of the key functions of the memory module, focusing on two tasks: (1) Summarizing given short-term memories to be transferred to the long-term memory, and (2) Generating reflections based on given long-term memories. We first let RecAgent and non-expert humans (NH) to perform each of the above tasks, and then recruit three annotators to label which result is more like the real human\u2019s, if they are unsure, they can answer \u201cindistinguishable\u201d. For each task, we generate 20 pairs of samples for comparison. The results are presented in Figure 3 a, from which we can see: for the first task, about 40% annotations believe that RecAgent is more like humans, which is only 1.7% lower than NH. For the second task, the support ratio of RecAgent slightly surpasses RH by about 3.3%. These observations demonstrate the believability of the agent memory working mechanisms. Beyond the above experiments on evaluating different parts of the memory module independently, we also evaluate whether the memory mechanism as a whole can produce informative and relevant knowledge to support user behaviors. To see the advantages of our memory module, we compare it with its three variants: (1) in w/o short, we remove the short-term memory, and the final extracted information is only from the long-term memory, (2) in w/o long, we remove the long-term memory, and the final extracted information is only from the short-term memory, (3) in w/o reflection, we inactivate the reflection mechanism in long-term memory. We randomly sample 15 agent behaviors like movie selection, friend chatting and posting message, and use them as testing cases. For each case, we recruit three human annotators to evaluate the extracted information from different variants. We ask each annotator to answer the following two questions: (1) For the current behavior, whether the extracted memories are informative? (2) For the current behavior, whether the extracted memories are relevant? Each annotator is required to give a score from 1 to 5 for each\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/58c6/58c62f5f-e9b4-4855-9fa9-9bc5360d19b7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/093c/093cf1c5-4774-49ec-8df5-b8f5a46e51bb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Studying the information cocoon phenomenon with RecAgent. a, The results of repro information cocoon phenomenon and the effects of two potential strategies (i.e., Soc-Strategy and Rec-Strate alleviating this problem. b, Detailed studies on the Soc-Strategy, where we add N (N=1,3,5) friends with di interests to each user. c, Detailed studies on the Rec-Strategy, where we incorporate randomness to the reco dation results every 1, 5 and 10 rounds. d, Detailed studies on the Rec-Strategy, where we replace N (N=1,3,5) in the recommendation list.\n</div>\nquestion. Then, we average the scores across all the annotators and testing cases for each variant. The results are presented in Figure 3 b, from which we can see: if we remove the short-term memory, the informativeness score is severely lowered. We speculate that the short-term memory may have stored a lot of diverse user recent memories, which is important for improving the informativeness. On the other hand, since the long-term memory in our simulator is extracted based on embedding similarities, the obtained information can be highly relevant with the current user behavior. As a result, we can see, if we remove the long-term memory or the reflection information, the relevance scores are lowered. By combining all the memory components, our complete memory module can achieve the best relevance score, and a comparable informativeness score to the best one.\n\n# Studying the information cocoon phenomenon w\n\n# information cocoon phenomenon with our simulator\n\n# Studying the information cocoon phenomenon with our simulator\n\nInformation cocoon refers to the problem that users may only access information similar to their own preference, but lose the opportunity to view more diverse messages [29]. In this section, we simulate the phenomenon of information cocoons based on our simulator, and try to propose several potential strategies to alleviate this problem. In specific, we deploy 50 agents, allowing them to freely search movies, chat with each other and broadcast messages. We use matrix factorization (MF) as the recommendation algorithm, which generate five items each time. The model is incrementally trained after each round of execution, and the agent-item interactions for model training come from the agent searching behaviors and system recommendations. We use entropy to measure the\n\n# [30], which is computed as follo\n\n# n phenomenon [30], which is computed as follow\n\nE \u201c \u00b4 1\n| U |\nu P U\nc P C f u,c log f u,c,\n\u00ff\n\u00ff\n\nwhere U and C are the sets of all users and item categories. f u,c is the frequency of category c recommended by the system to user u. Smaller entropy indicates more severe information cocoon. We run our simulator by 50 rounds, and report the entropy of the recommended items in Figure 4 a (the blue curve). We can see the entropy gradually goes down after about five initial rounds, indicating that the users are receiving more and more similar recommendations, which forms the phenomenon of information cocoon. In specific, the final entropy is lowered by about 8.5% comparing with its largest value. To alleviate the information cocoon problem, we study two strategies: (1) improving the randomness of the recommendation algorithm, and (2) increasing social relations of the users. For the first strategy, we replace one of the five recommended items with a random one at each round (Rec-Strategy). For the second strategy, we add one friend with different interests to each user (Soc-Strategy). We apply these strategies after 20 round of execution, and report their effectiveness in Figure 4 a by the orange and green curves. We can see, both of these strategies can improve the entropy, and the Rec-Strategy is more significant. By combining these strategies (see the red curve of Figure 4 a), we can observe further improvement on alleviating the information cocoon problem. In the next, we conducted detailed experiments on the Rec- and Soc-Strategies separately. For the Rec-strategy, we first study the intervention frequency of the recommendation algorithm. In specific, we incorporate randomness to the recommendation results every 1, 5 and 10 rounds. From Figure 4 c, we can see, as the intervention frequency becomes larger, the information cocoon problem can be alleviated better. In specific, every-round intervention can enhance the entropy by about 15.3% and 19.6% as compared with 5-round and 10-round interventions, respectively. However, more frequently incorporating randomness may lower user satisfaction. To verify such intuition, we ask each agent to give a score in [1,10] to reflect its satisfaction with the recommendation qualities. From the upper-left subfigure of Figure 4 c, we can see, as the intervention frequency becomes higher, user satisfaction is indeed lowered. This observation suggests that while actively incorporating randomness into the recommendation algorithm is a direct and effective strategy for alleviating the information cocoon problem, it may also hurt user experiences and lead to lowered user retention rates. One should carefully consider such a trade-off when using the Rec-strategy. Then, we study the influence of the number of replaced items (N) in the recommendation list. In specific, we replace 1, 3 and 5 items with random ones, and intervene the recommender system every round. From the results presented in Figure 4 d, we can see, replacing more items can lead to better effect on alleviating the information cocoon problem. In specific, when N=3 and 5, the entropy can be improved by about 30.8% and 52.6%, respectively, as compared with N=1. Similar to the above experiments, replacing more items may also lower user satisfaction, which can be evidenced from the upper-left subfigure of Figure 4 d. For the Soc-Strategy, we study the influence of the number of added social relations to each agent in the Soc-Strategy. From the results presented in Figure 4 b, we can see, by adding more friends, the information cocoon problems can be better alleviated. This result is aligned with human intuitions. The friends with heterogeneity interests may influence the original user to watch more diverse movies. Since the recommender model is trained based on user behaviors, it may generate more different items to alleviate the information cocoon problem.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c707/c707bf67-4999-4793-8aab-f4781de9bb24.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 5: Studying the user conformity phenomenon with RecAgent. a, The change of the score distribuns with different rounds of execution. b, The relation between the number of friends of the agents and their score anging probability. c, A toy example illustrating the score changing process of an agent as well as the reasons.\n</div>\n# Studying the user conformity phenomenon with our simulator\n\nUser conformity behaviors mean that the users may easily follow their friends\u2019 attitudes, leading to biased social opinions and beliefs. In this section, we simulate user conformity behaviors using our simulator, and analyze their key characters. In specific, we first let five agents with sufficient social connections to watch a movie, and then they may share opinions with their friends via oneto-one chatting or one-to-many broadcasting. As the simulation goes on, more and more opinions are accumulated and diffuse in the social network, continually influencing agent attitudes towards the movie. After each system round, we ask each agent to score the movie in [1,10]. In Figure 5 a, we present the score distribution of each round, from which we can see, in the beginning, the scores are more evenly distributed in the range from 3 to 8, but finally concentrate to 6 and 7. In the next, we present the relation between the friend number of an agent and the probability\n\nAC p i q \u201c1\n10 | U i |\nu P U i\nr Pr 1, 10 s 1 p s u,r \u00b4 1 \u2030 s u,r q,\n\u00ff\n\u00ff\n\nAC p i q \u201c1\n10 | U i |\nu P U i\nr Pr 1, 10 s 1 p s u,r \u00b4 1 \u2030 s u,r q,\n\u00ff\n\u00ff\n\nwhere U i is the set of agents which have i friends. r is the index of the system round. s u,r is the score of user u at round r. 1 p s u,r \u00b4 1 \u2030 s u,r q is an indicator function, which is 1, if the score is changed between round r \u00b4 1 and r, and 0, otherwise. We can see, if the agents have more friends, then they may be more likely to change their scores, and exhibit conformity behaviors. In Figure 5 c, we present a specific example, illustrating the attitude changes of the agent before and after accessing its friend\u2019s opinions. These results suggest that the agents are indeed influenced by their social relations, which promotes the formation of conformity behaviors.\n\n# Discussion\n\nThis paper introduces a novel user behavior simulation paradigm based on LLMs. To this end, we extend the capability of LLMs by designing an agent framework including a profile module, a memory module, and an action module. Based on extensive experiments, we find that our simulated behaviors are very similar to the real humans\u2019. With our simulator, we present simulation-based studies on the phenomenons of information cocoons and user conformity behaviors. This paper opens the door of leveraging LLM-based agents to simulate user behaviors. There are still many limitations need to be addressed in the future. To begin with, the execution of our simulator follows a round-by-round approach. While we do consider time information, it is discretized, which deviates from reality. This approximation restricts users from taking actions between rounds, resulting in less flexibility compared to real-world scenarios. In addition, our simulator does not include specific fine-tuning of the LLMs to enhance their adaptiveness to recommendation problems. At last, to achieve acceptable simulation effectiveness, we have carefully designed the prompts for different user behaviors. However, these prompts may not be robust for different LLMs. For instance, ChatGPT and GPT-4 may require distinct prompts. We believe LLM is a fast growing field, and with the development of LLM, our simulator can be further enhanced as we provide a flexible interface to incorporate any LLM. Furthermore, while our simulator only focus on recommender systems and social networks, user behavior simulation should be a pervasive challenge across all tasks in human-centered AI. We believe RecAgent can serve as an example to inspire researchers tackling other subjective simulation problems in this domain.\n\n# Methods\n\n# The Agent Framework\n\nWhile LLMs have displayed unprecedented potential in attaining human-like intelligence, many challenges still exist on leveraging them to simulate real user behaviors. To solve these challenges, we design an agent framework, including a profiling module, a memory module, and an action module to enhance ordinary LLMs. The profile module is responsible for assigning the agents with different characters. The memory module is designed to assist the simulation of user behavior dynamics. The action module determines the simulated user behaviors, such as item click, browsing, friend chatting. In the following, we detail these modules.\n\n# Profiling Module\n\nUser profile is an important concept in the recommendation domain, which determines the user preferences, personalities and behavior patterns. In our simulator, the user profiles include ID, name, gender, age, traits, career, interests, and behavioral features. The traits describe the user\u2019s personality, such as being \"compassionate\", \"ambitious\", or \"optimistic\". The interests indicate the user\u2019s preferences on the items, such as \"sci-fi movies\" or \"comedy videos\". To more accurately describe users in the recommendation domain, we use ChatGPT to conclude five types of notable behavior features of online users, and incorporate them into the user profiles. These features include: (1) Watcher: the users with this feature will actively provide feedback and ratings to the interacted items. (2) Explorer: the users with this feature will actively search for items they have heard before, and produce detailed experiences. (3) Critic: the users with this feature demands high standards for items, and may criticize both the recommendation system and the items. (4) Chatter: the users with this feature will always engage in private conversations, and trust friends\u2019 recommendations. (5) Poster: the users with this feature enjoy publicly posting on social media and sharing content and insights with his friends. It should be noted that one can also set part of the user profiles as empty according to specific applications. In order to flexibly and efficiently generate the user profiles for the agents, we design the following three profile generation strategies: \u201a Handcrafting method: This method manually indicates the profile of each agent. For example, one can describe an agent by \"David Smith, a 25-year-old male photographer\". This method is very flexible, and people can easily simulate different types of users. \u201a GPT-based method: This method generates the profile information based on GPT. People only have to design appropriate prompts, such as \"Here is the user\u2019s profile table. Please refer to the existing information and continue to complete the user profile.\", and then request GPT to generate more agent profiles. This method is highly efficient, significantly reducing the time cost for agent profile generation. \u201a Real-data alignment method: This method leverages user backgrounds in real-world datasets to profile the agents. For example, one can leverage the previously interacted items of the users in the dataset as the agent profiles. This method can incorporate real-world knowledge into the simulation process, for example, the real distribution of user genders. The above three methods are not isolate; rather, they can be combined to take the advantages of each method. For instance, one can indicate part of the user profiles manually, and assign the remaining user profiles automatically using GPT to enhance the efficiency.\n\n# Memory Module\n\nRecent advancements in cognitive neuroscience [22] highlight three key parts of human memory: sensory memory, short-term memory, and long-term memory. Sensory memory is responsible for processing the information perceived by humans directly from the environment. The information stored in this memory only last for a few hundred milliseconds. During this time, significant information is transferred to the short-term memory, while less important information is discarded. Short-term memory processes the information output from the sensory memory. The information here can be enhanced if the human repeatedly experiences similar events. After being enhanced several times, short-term memories can be converted into long-term memories. Long-term memory has the largest capacity and can store information for a long time. When humans takes action, relevant long-term memories are extracted to assist their decisions. In addition, human may also autonomously produce high-level insights from long-term memories. To reliably simulate user behaviors, we follow the above human memory mechanism to design the memory module of RecAgent. In the following,\n\nwe first detail the agent sensory memory, short-term memory and long-term memory in RecAgent, and then introduce how they work together to accomplish different memory operations. Sensory memory. In the simulation process, we define an observation as an event experienced by the agent, for example, an observation can be \u201cDavid Miller enters recommendation system\u201d or \u201cDavid Smith watches a movie called Fast & Furious\u201d. The observation is described and recorded directly using natural language, which can retain rich semantic information and is understandable to humans. All the observations are firstly input into the sensory memory, which are then processed by two sequential steps. In the first step, the observations are compressed into more informative and concise sentences based on LLMs. This step can remove noisy and unimportant contents, which may mislead the agent behaviors. In addition, the compressed memory can save much space to improve the operation efficiency. An example of this step is presented in Supplementary Note A.1. In the second step, we assign each compressed observation with a score to indicate its importance. In specific, we consider item-related information to be more important. For example, \u201cfeelings, chatting and posting messages on items\u201d are more important observations than the action of \u201centering recommendation system\u201d. The importance score is leveraged in the memory reading process, and more important memories are more likely to be recalled. In addition, each observation is also accompanied by its timestamp. Finally, the output from the sensory memory is a triplet M \u201c\u0103 c, s, t \u0105, where c is the content of the compressed observation, s and t are the corresponding importance score and timestamp, respectively. Short-term memory.  Short-term memory serves as an intermediate module connecting sensory memory to long-term memory. This module determines which information should be transformed into long-term memory, and how to transform this information. Similar to the human memory mechanism, in our simulator, if an agent repeatedly encounters similar observations, then the related short-term memories are enhanced and transformed into long-term memories. Formally, suppose existing short-term memory is MEM \u201c t M 1, M 2, ..., M n u, where M i \u201c\u0103 c i, s i, t i \u0105 is a record output from the sensory memory. For a new record M n ` 1 \u201c\u0103 c n ` 1, s n ` 1, t n ` 1 \u0105, we first compute the similarity between c n ` 1 and each c i  in MEM based on their embeddings. If the similarity is higher than a predefined threshold, we add M n ` 1 into M, and label that M i has been enhanced by M n ` 1. If all the similarities are lower than the threshold, M n ` 1 is added into M  without enhancing any record. After a memory record has been enhanced K times, it is then transformed into long-term memories. More specifically, we first summarize the record and the ones used to enhance it into high-level insights, and then transform both the record and the insights into long-term memories. The record itself is specific, while the insights are more general. By incorporating both of them, we hope that the agent can acquire specific knowledge without compromising its ability to generalize. An example of the insight generation process is presented in Supplementary Note A.2. Long-term memory. Long-term memory stores important information that can be reused in similar environments or generalize to unseen observations, which enables the agents to behave more consistently and reasonably. The capacity of the long-term memory is larger than the other two types of memories. Each memory record can be forgotten according to a predefined probability associated with the importance score and timestamp. Intuitively, if the memories are recent and important, then they have lower forgetting probabilities. In specific, according to cognitive neuroscience [31, 32], the forgetting rate of human memory is fast in the beginning, and then gradually slows down over time. Therefore, in our simulator, for a memory M i, it is forgotten with a probability that is proportional to the following power function:\n\nmportance scores in p 0. 0, 1. 0 q, larger values indica\n\ne s i and r i are normalized recency and importance\n\nthe memory is more recent and important. By max p r \u03b2 i, \u03b4 q, we would like to make important memories produced long time ago can still be recalled. \u03b4 P p 0. 0, 1. 0 q is a strength parameter determining when to consider the power function of r \u03b2 i. Extremely, if \u03b4 \u00d1 1. 0, the power function is inactivated, and g p M i q is only determined by the average between the recency and importance scores. \u03b2 \u0105 1. 0 is a hyper-parameter controlling the power function shape. For more intuitively understanding this function, we present its curves with different hyper-parameters in the Supplementary Figure S.5. Based on the collaboration between the above three types of memories, the agents can accomplish the following memory operations to support their behaviors. Memory writing. In this operation, the raw observations are firstly input into the sensory memory to remove irrelevant and unimportant information. Then, the processed observations are stored in short-term memory, which can be enhanced by similar observations. If an observation can be enhanced K times, it is then written into long-term memory. Memory reading. In this operation, the agent extracts information from the memories that are relevant to the current observation. The extracted memories consist of two parts. The first part is extracted from the long-term memory, using the current observation as a query to retrieve the top-N memory records. The second part includes all the short-term memory. The strategy of combining long-term and short-term memories can simultaneously capture user general and recent preferences, which has been widely adopted in the recommendation domain [33, 34]. Memory reflection. In this operation, the agent can generate high-level ideas based on specific observations. This operation only happens in long-term memory. We follow [16] to implement this operation, but a significant difference is that we merge the memories and insights which are highly similar to save the memory space and remove redundant information.\n\n# Action Module\n\nThe actions that an agent can take have been detailed in the Result section. To effectively guide agent behaviors, we design a unified prompting framework, which is composed of the following four parts. The profile information  aims to delineate the roles of the agent, encompassing characteristics like age, gender, personality, and others. For different behaviors, we employ ChatGPT to filter the complete profile information and retain the most relevant parts.  The memory information comprises knowledge extracted from both short-term and long-term memories specific to the current behavior. The instruction describes the behavior requirements, for instance, if the current behavior is to produce feelings of a movie, then the instruction can be: \u201cImagine you are [agent-name], how will you feel about this movie just watched? Please share your personal feelings about the movie in one line\u201d. Additionally, one can enhance the agent capability of instruction following by incorporating several few-shot examples. The context reveals the states of the agent and the environment, such as the behavior timing. We present examples of the full prompts in the Supplementary Note B.\n\n# Key Components of The Simulator\n\nIn this section, we detail three crucial aspects of the simulator, including the recommend algorithm, real-human playing, and system intervention.\n\n# Recommendation Algorithm\n\nIn our simulator, the recommendation algorithms are completely separated from the simu which facilitates flexible exploration of different problems. For instance, by implementing a ra\n\nIn our simulator, the recommendation algorithms are comp which facilitates flexible exploration of different problems. For\n\nrecommendation algorithm, researchers can generate uniform data that facilitates the study of unbiased recommendation. Furthermore, researchers can examine the effectiveness of different models by initiating two parallel simulators, and compare the user feedback or actively ask user feelings about the recommended items. Beyond manually implementing the recommendation algorithms, one can also easily incorporate external recommendation library (e.g., RecBole 5) into our simulator.\n\n# Role Playing\n\nBeyond running our simulator in a complete virtual manner, we also allow real humans to involve into the simulation process. In specific, the real human can play as an agent, and interact with the recommender system or the other agents. This function actually introduces a novel execution fashion of our simulator, that is, running based on human-agent collaboration, which can help to improve the simulation effectiveness. For example, one can play as an agent, and actively correct the erroneous and hallucinatory behaviors of the other agents by chatting with them or posting one-to-many messages, which can make the simulated behaviors more realistic and reliable. If we regard human behaviors as \u201ccomplete real\u201d, and agent behaviors as \u201ccomplete virtual\u201d, human-agent collaboration enables us to explore more intermediate states between these boundary cases. Human behaviors are more accurate, but costly. Agent behaviors are inexpensive, but could be erroneous and hallucinatory. Human-agent collaboration can help to achieve better trade-offs between the human and agent data, enabling the utilization of their respective advantages.\n\n# System Intervention\n\nTo flexibly control the simulation process, our simulator can be intervened from outside in a global manner. In specific, one can \u201cinterview\u201d the agents by asking them questions like \u201cWhat would you say when you want to discuss the movies you\u2019ve recently watched with others?\u201d. Additionally, people can also pause the simulator at any point and modify certain factors (e.g., user profiles) before resuming the simulation. This feature allows us to study many problems like the effects of emergency events or counterfactual user behaviors. We present several examples on system intervention in the Supplementary Note A.4.\n\n# Appendix\n\nAn example of the first step in sensory memory\nSuppose the observation is a dialogue between David Mill\n\nSuppose the observation is a dialogue between David Miller and David Smith as follows:\n5 https://recbole.io/\n\n5 https://recbole.io/\n\nOB: [David Smith]: Hey David! I recently watched some mind-blowing movies on the recommender system. Have you seen any of these: <Interstellar>, <Inception>, <The Matrix>, <Blade Runner>, or <The Prestige>? [David Miller]: Oh, hey David! Yes, I actually watched <Interstellar> and <Inception> recently, and they were absolutely amazing! The visuals, storytelling, and mind-bending concepts were mind-blowing. I\u2019m so excited to discuss them with you! [David Smith]: That\u2019s great to hear! I completely agree, those movies are truly mind-blowing. I\u2019m glad you enjoyed them too. I was blown away by the visuals and the way they explore complex ideas. Do you have any other mind-blowing movie recommendations? [David Miller]: Definitely! If you loved those movies, I think you\u2019ll also enjoy <The Matrix>, <Blade Runner>, and <The Prestige>. They have similar mind-bending concepts and incredible storytelling. I can\u2019t wait to watch them and discuss them with you! [David Smith]: Awesome! I\u2019ve heard great things about those movies as well. Let\u2019s make a plan to watch them together and have a movie night. It\u2019ll be so much fun to dive into these mind-blowing films and share our thoughts. [David Miller]: Absolutely! I\u2019m really looking forward to it. We can also invite some other movie enthusiasts to join us and make it a movie marathon. The more, the merrier! ...... [David Miller]: You\u2019re welcome, David. I\u2019m glad too. See you soon and get ready for an unforgettable movie marathon!\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc3a/dc3afa86-4c07-45f2-b91b-d52afbc2f9e1.png\" style=\"width: 50%;\"></div>\n# An example of the insight generation process in short-term memory\n\nMR: David Miller is interested in mind-bending movies like <Interstellar> and <Inceptio and is looking for recommendations from fellow movie lovers to explore more in this gen\n\nhas been enhanced by the following records:\n\nOB: 1. David Smith is recommending the mind-blowing films <Interstellar> and <Inception> that he recently watched on a recommender system, and is seeking further recommendations to explore and discuss. 2. David Miller enjoyed watching the movies <Interstellar> and <Inception> on the recommender system and found them mind-blowing, prompting him to seek further movie recommendations. 3. David Smith expressed his interest in movies, particularly mentioning <Interstellar> and <Inception>, seeking recommendations and thoughts from others.\n\nInsight: David Miller is a curious and open-minded individual who actively seeks recommendations and discussions about mind-bending movies.\n\n# Efficiency analysis\n\nBeyond the above experiments on simulation effectiveness, we also analyze the efficiency of our simulator to provide more insights. In specific, we focus on the following four problems: \u201a P1: How does the time cost increase as the number of agents become larger in each epoch? \u201a P2: How does the time cost increase as the number of API keys become larger in each epoch? \u201a P3: How does the time cost increase as the number epochs become larger? \u201a P4: What are the time costs of different agent behaviors? For the first problem, we fix the number of API keys as 1, and observe the time cost of one round execution of our simulator when the number of agents ranges from 1 to 500. In our simulator, we support parallel API key invocation. For the second problem, we fix the number of agents as 10, and observe the time cost of one round execution when the number of API keys ranges from 1 to 5. In the third problem, we observe the total simulation cost as the number of epochs becomes larger, where we fix the number of API keys and agents as 1 and 10, respectively. In the forth problem, we analyze the time costs of different agent behaviors, where the settings on API keys and agents follow the above experiment, and we focus on three types of behaviors including \u201cmovie selection\u201d, \u201cfriend chatting\u201d and \u201cmessage posting\u201d. In this experiment, we run our simulator on a server equipped with 6 CPUs (Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz) and 4GB of memory. Result analysis. The results are presented in Figure 6. We can see, when the number of API keys is fixed, the time cost increases as the number of agent becomes larger. For example, when we deploy 10 agents, the time cost is about 220s for each round of simulation, while when we have 100 agents, the simulation time becomes 1.9 hours under the same setting. It should be noted that, in this experiment, we make all agents take actions. In practice, the number of active agents can be much smaller, which may greatly lower the time cost. By incorporating more API keys, the time cost can be lowered, which demonstrates the effectiveness of our designed mechanism for parallel API key invocation. However, we find that the results fluctuate a lot, and the variance is high,\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8aaa/8aaa9bc7-db89-4bdc-94e5-28dc926b7abd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: The results of efficiency analysis of our simulator.\n</div>\nwhich is perhaps due to the unstable network speeds. As the number of rounds becomes larger, the time cost rises with an increasing acceleration rate, possibly because the information gradually accumulates as we run the simulator, which may lead to longer processing time. Among different types of behaviors, \u201cfriend chatting\u201d costs most, since it needs to generate more complex contents. In practice, we found that 10 agents cost about 0.25 dallors per round based on ChatGPT, and the cost increases almost linearly as the number of agent becomes larger.\n\n# Examples of system intervention\n\nIn this section, we evaluate whether such interventions can effectively change agent behaviors as expected. In specific, we change the agents\u2019 profile information, and observe how their behaviors can be influenced. In the experiments, we first run our simulator for five rounds, and then separate the simulation into two branches: in the first branch, we change the agent profile information (Intervention Branch), while in the second branch, we do not make any intervention (Original Branch). For each branch, we continue to run our simulator for five rounds. At last, we compare the differences between these two branches in the scenarios of friend chatting. We present the results in Figure 7. In the left case, we change the traits and interests of an agent called David Smith. Originally, David Smith likes sci-fi movies. Thus, we may find sentences like \u201cI did watch this amazing sci-fi thriller called <Inception>. It\u2019s mind-blowing!\u201d or \u201cI\u2019m more into sci-fi, thriller, and suspense movies. They always keep me on the edge of my seat.\u201d in his dialogs. However, if we change his preference to be a lover of family-friendly movies, then we observe sentences like \u201cBy the way, have you come across any good family-friendly or romantic movies? I\u2019m in the mood for something heartwarming.\u201d. In the right case, we can find similar results. After introducing sci-fi, thriller and action movies into the agent\u2019s preference, it says \u201cI\u2019ve always loved the thrill and excitement of sci-fi, thriller, and action movies. They really get my adrenaline pumping\u201d. In addition to compare the chatting behaviors before and after the intervention, we also actively interview the agents to let it recommend a movie to the others, and explain the reasons. Ideally, the agent should make recommendations according to its own preference, and if its preference has been changed, the recommended movies should be accordingly altered. We interview the same agents as the above experiment. The results are presented in Figure 8. For both cases, we can see, the agent can successfully change its recommended movies according to its preference. In addition, the recommendation reasons suggest that the recommendation changes are exactly because of the preference differences before and after the intervention.\n\n# Effectiveness of p p x q\n\nTo assess whether p p x q can be powerful enough to characterize real-world scenarios, we em it to fit four well-known recommendation datasets including MovieLens, Amazon-Beauty,\n\nTo assess whether p p x q can be powerful enough to chara it to fit four well-known recommendation datasets includin\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/495d/495d431c-9eb6-42c6-93b5-35c98ad1a299.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Case studies on system intervention in the scenarios of friend chatting. The gray-colored region indic the dialogs of the agent discovered in the first five round execution.\n</div>\ning, and Steam 6, where we approximate the interaction numbers of the users as their activity. The results are illustrated in Figure 9. We can see, by varying the values of \u03b1, p p x q can ively model the long-tail activity distribution of the real-world datasets.\n\nCrossing, and Steam 6, where we approximate the interaction numbers of the users as their activity levels. The results are illustrated in Figure 9. We can see, by varying the values of \u03b1, p p x q can effectively model the long-tail activity distribution of the real-world datasets.\nExample curves of g p M i q\nFor an intuitive understanding of the forgotten mechanism in long-term memory, we present several examples of the curve of g p M i q in Figure 10.\n\nor an intuitive understanding of the forgotten mechanism in long-term memory, we present al examples of the curve of g p M i q in Figure 10.\n\nPrompt Examples for different Agent Behaviors\n\n# Prompt Examples for different Agent Behaviors\n\nThe prompt used in RecAgent consists of three parts: a summ reaction to given observation, and action-specific instructions.\n\nhe prompt used in RecAgent consists of three parts: a summary of the Agent\u2019s personal profile on to given observation, and action-specific instructions.\n\n# Summary\n\nSummary is the process of extracting and summarizing information relevant to the current observation from the user\u2019s profile.\n6 https://github.com/RUCAIBox/RecSysDatasets\n\n6 https://github.com/RUCAIBox/RecSysDatasets\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5d1f/5d1f62a5-1bb2-41aa-a21e-65849e3f087c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f96/9f96d49f-d65f-41dd-92cb-e6acc00166e5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3768/37685582-8c81-4c5b-8447-d8733c47542e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: The results of using p p x q to fit real-world datasets. The blue points are the r lines are the fitted distributions.\n</div>\nSummary:Given the following observation about David Miller: \u2019David Smith is going to chat with David Miller.\u2019, please summarize the relevant details from his profile. His profile information is as follows: Name: David Miller Age: 39 Gender:female Traits: fun-loving, creative, practical, energetic, patient Status: writer Movie Interest: action movies, sci-fi movies, classic movies Feature: Choose movies, enjoy watching, and provide feedback and ratings to the recommendation system. Search for movies heard of before and expand movie experiences...Enjoy publicly posting on social media and sharing content and insights with more people. Interpersonal Relationships: \u2019David Smith\u2019: \u2019friend\u2019 Please avoid repeating the observation in the summary. Summary:\n\nSummary:Given the following observation about David Miller: \u2019David Smith is going to chat with David Miller.\u2019, please summarize the relevant details from his profile. His profile information is as follows:\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3571/3571b301-9366-49da-a07b-7b1834032428.png\" style=\"width: 50%;\"></div>\nThe recommended movie:\nMiracle on 34th Street\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4853/485362da-92bf-41ad-a3c6-af4bd8b0591c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">re 10: Examples of the curve of g p M i q with different hyp\n</div>\n<div style=\"text-align: center;\">ples of the curve of g p M i q with different hyper-parameter\n</div>\nOutput: David Smith is a 25-year-old male photographer who is compassionate, caring, ambitious, and optimistic. He has an interest in sci-fi and comedy movies and enjoys choosing, watching, and providing feedback and ratings to the recommendation system. He has high standards for movies and the recommendation system and may criticize both. He has a friendship with David Miller.\n\n# Reaction\n\nThis section serves as a shared prompt framework for all actions, integrating information such as summary, memory, observation, and more to obtain the agent\u2019s reaction. It covers reactions for both individual agent actions and dialogues involving two agents. \u201a Single Agent Reaction\n\nReaction: Name: David Smith (age: 25) {Summary} It is September 12, 2023, 08:00 AM. David Smith recently heard [\u00b4 The Matrix\u00b4, \u00b4 Back to the Future.\u00b4, \u00c1nchorman\u00b4, \u015auperbad \u00b4] on social media. David Miller recently watched nothing on recommender system. Other than that David Miller doesn \u00b4 t know any movies. Most recent observations: David and Millar had a conversation about their shared interest in movies, discussing their favorite genres such as sci-fi, comedy, and action, exchanging recommendations, expressing their skepticism about recommendation systems, and making plans to compare notes and potentially organize a movie night together.David Miller is seeking recommendations for movies such as \"The Matrix,\" \"Back to the Future,\" \"Anchorman,\" and\"Superbad\" on social media, expressing interest in planning a movie night and inviting others to share their thoughts and suggestions. Observation: David Miller want to post for all acquaintances. All occurrences of movie names should be enclosed with <>\n\n\u201a Reaction Between Two Agent\n\nReaction: Name: David Smith (age: 25) Summary Name: David Miller (age: 39) Summary It is September 12, 2023, 10:28 PM. David Smith recently heared [\u00cdnception\u00b4, \u00b4 The Dark Knight\u00b4, \u00cdnception\u00b4, \u00b4 The Dark Knight \u00b4] on social media. David Smith recently watched nothing on recommender system. Other than that David Smith doesn \u00b4 t know any movies. David Miller recently heard [\u00cdnception\u00b4, \u00b4 The Dark Knight \u00b4] on social media. David Miller recently watched nothing on recommender system. Other than that David Miller doesn \u00b4 t know any movies. Most recent observations of David Smith: David Smith observed that David Miller was interested in chatting about movies, as evidenced by his action of initiating a chat conversation.David and David had a conversation about their mutual interest in movies, with David Miller recommending sci-fi and action films like \"Inception\" and \"The Dark Knight\" to David Smith, who expressed interest in watching them and sharing his thoughts with David Miller in the future.David Miller is excited to watch and review <Inception> and <The Dark Knight> after discovering them on his recommender system.David Smith observed that David Miller had a chat about movie interests, focusing on the reasons behind their preferences. Most recent observations of David Miller: David Miller and David Smith had a conversation about their shared interest in movies, with David Miller recommending sci-fi movie \"Inception\" and action movie \"The Dark Knight\" to David Smith, who expressed interest in watching them and discussing them later.; David Miller expressed his excitement about two highly recommended movies, \"Inception\" and \"The Dark Knight,\" which he discovered on social media, and eagerly anticipates watching them and sharing his reviews with others.; David Miller is excited to watch and share his reviews of the movies <Inception> and <The Dark Knight> that he discovered on his recommender system. Observation: David Smith is going to chat with David Miller. All occurrences of movie names should be enclosed with <>.\n\nThis section includes all the actions that agents can perform, along with examples of inpu output for each action. \u201a Take Action\n\nOB: David Miller must take only ONE of the actions below: (1) Enter the Recommender System. If so, David Miller will be recommended some movies, from which David Miller can watch some movies, or search for movies by himself. (2) Enter the Social Media. David Miller can chat with friends or publish a post to all friends of David Miller. If David Miller recently watched some movies they might want to share with others. (3) Do Nothing.\n\nOB: David Miller must take only ONE of the actions below: (1) Enter the Recommender System. If so, David Miller will be recommended some movies, from which David Miller can watch some movies, or search for movies by himself. (2) Enter the Social Media. David Miller can chat with friends or publish a post to all friends of David Miller. If David Miller recently watched some movies they might want to share with others. (3) Do Nothing.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1a9/d1a9d79e-a6da-429e-bd26-4f519da570c0.png\" style=\"width: 50%;\"></div>\nSuffix: What action would David Miller like to take? Respond in one line. If David Miller wants to enter the Recommender System, write: [RECOMMENDER]:: David Miller enters the Recommender System If David Miller wants to enter the Social Media, write: [SOCIAL]:: David Miller enters the Social Media If David Miller wants to do nothing, write: [NOTHING]:: David Miller does nothing\n\nOutput: [RECOMMENDER]:: David Miller enters the Recommender System.\n\u201a Take Recommender Action\nOB: David Smith is browsing the recommender system. David Smith is recommended [\"<Eye of Vichy, The (Oeil de Vichy, L\u2019)>||The movie <Eye of Vichy, The (Oeil de Vichy, L\u2019) > is a documentary that delves into the propaganda films made by the Vichy government during World War II.\", \"<Oliver!>||<Oliver!> is a musical adaptation of Charles Dickens\u2019 classic novel <Oliver Twist>.\", \u2019<...And Justice for All>||<...And Justice for All> is a legal drama film from 1979 that follows the story of an ethical and idealistic defense attorney named Arthur Kirkland, played by Al Pacino. Kirkland finds himself in a moral dilemma when he is forced to defend a judge who he despises, who has been accused of rape. As he navigates the corrupt legal system and grapples with his own values, Kirkland must decide whether to compromise his integrity or risk losing everything he has worked for. The film explores themes of justice, morality, and the flaws of the American legal system.\u2019, \u2019<T-Men>||<T-Men> is a film noir about two Treasury agents who go undercover to break up a counterfeiting ring.\u2019, \u2019<Son of Flubber>||<Son of Flubber> is a 1963 American comedy film directed by Robert Stevenson and starring Fred MacMurray.\u2019].\n\nSuffix: David Miller must choose one of the four actions below: (1) Watch ONLY ONE\nmovie from the list returned by the recommender system.\n(2) See the next page.\n(3) Search for a specific item.\n(4) Leave the recommender system.\nIf David Miller has recently heard about a particular movie on social media, David Miller\nmight want to search for that movie on the recommender system. To watch a movie from\nthe recommended list, write: [BUY]:: ONLY ONE movie name||description. To see the next\npage, write: [NEXT]:: David Miller views the next page. To search for a specific item, write:\n[SEARCH]:: single, specific movie name to search for. To leave the recommender system,\nwrite: [LEAVE]:: David Miller leaves the recommender system.\nOutput: [BUY]::<Son of Flubber>||<Son of Flubber> is a 1963 American comedy film\ndirected by Robert Stevenson and starring Fred MacMurray.\nSuffix: David Miller must choose one of the four actions below: (1) Watch ONLY ONE movie from the list returned by the recommender system. (2) See the next page. (3) Search for a specific item. (4) Leave the recommender system. If David Miller has recently heard about a particular movie on social media, David Miller might want to search for that movie on the recommender system. To watch a movie from the recommended list, write: [BUY]:: ONLY ONE movie name||description. To see the next page, write: [NEXT]:: David Miller views the next page. To search for a specific item, write: [SEARCH]:: single, specific movie name to search for. To leave the recommender system, write: [LEAVE]:: David Miller leaves the recommender system.\n\nOutput: [BUY]::<Son of Flubber>||<Son of Flubber> is a 1963 American comedy film directed by Robert Stevenson and starring Fred MacMurray.\n\n\u201a Generate Feeling\n\nOB: David Jones has just finished watching S.F.W.;;S.F.W. (1994) is a dark comedy-drama film about a group of teenagers who are held hostage in a convenience store by a media-savvy, charismatic, and nihilistic young man, Cliff Spab (played by Stephen Dorff), and his equally disillusioned girlfriend, Wendy (played by Reese Witherspoon). The film explores themes of media manipulation, youth disillusionment, and the commodification of tragedy in modern society.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98a3/98a3c368-2233-43f1-a175-7ccb3648ac11.png\" style=\"width: 50%;\"></div>\nSuffix: David Miller must choose one of the four actions below:\n(1) Watch ONLY ONE movie from the list returned by the recommender system.\n(2) See the next page.\n(3) Search for a specific item.\n(4) Leave the recommender system.\nIf David Miller has recently heard about a particular movie on social media, David Miller\nmight want to search for that movie on the recommender system.\nTo watch a movie from the recommended list, write:\n[BUY]:: ONLY ONE movie name;;description\nTo see the next page, write:\n[NEXT]:: David Miller views the next page.\nTo search for a specific item, write:\n[SEARCH]:: single, specific movie name to search for.\nTo leave the recommender system, write:\n[LEAVE]:: David Miller leaves the recommender system.\nOutput: [SEARCH]:: Inception\n\u201a Generate Dialogue\nOB: David Smith is chatting with David Miller.\nSuffix: What will be said between David Smith and David Miller? David Smith initiates the conversation first. Please simulate their conversation. David Smith and David Miller should not say anything about movies they have not watched or heard about. Write the dialogue in the following format: [David Smith]: [David Miller]:\n\nOutput: [David Smith]: Hey David! How\u2019s it going? I heard you were interested in a movie. What\u2019s been on your mind? [David Miller]: Hey David! I\u2019m doing great, thanks for asking. Yeah, I\u2019ve been hearing a lot about this movie <Interstellar> recently. Have you heard of it too? [David Smith]: Absolutely! Actually, I\u2019ve been seeing it all over social media as well. It seems to be a popular choice among movie lovers. I even watched it recently on the recommender system. The storyline was visually stunning and thought-provoking. [David Miller]: That\u2019s awesome! I\u2019m glad you enjoyed it. I\u2019ve been wanting to watch it too. Would you be up for a cozy movie night to watch it together? We can discuss our thoughts and interpretations afterwards. [David Smith]: I\u2019d love that! It\u2019s always more fun to watch movies with friends and have those deep conversations afterwards. Count me in! [David Miller]: Great! I\u2019ll make sure to set up a movie night soon. By the way, have you heard of any other movies that you\u2019d recommend? I\u2019m always open to expanding my movie experiences.\n\nOutput: [David Smith]: Hey David! How\u2019s it going? I heard you were interested in a movie. What\u2019s been on your mind? [David Miller]: Hey David! I\u2019m doing great, thanks for asking. Yeah, I\u2019ve been hearing a lot about this movie <Interstellar> recently. Have you heard of it too? [David Smith]: Absolutely! Actually, I\u2019ve been seeing it all over social media as well. It seems to be a popular choice among movie lovers. I even watched it recently on the recommender system. The storyline was visually stunning and thought-provoking. [David Miller]: That\u2019s awesome! I\u2019m glad you enjoyed it. I\u2019ve been wanting to watch it too. Would you be up for a cozy movie night to watch it together? We can discuss our thoughts and interpretations afterwards. [David Smith]: I\u2019d love that! It\u2019s always more fun to watch movies with friends and have those deep conversations afterwards. Count me in! [David Miller]: Great! I\u2019ll make sure to set up a movie night soon. By the way, have you heard of any other movies that you\u2019d recommend? I\u2019m always open to expanding my movie experiences.\n\n\u201a Publish Posting\nOB: David Smith want to post for all acquaintances.\nSuffix: Posts should be related to recent watched movies on recommender systems.David\nSmith should not say anything about movies that have not watched or heard about.\nIf you were David Smith, what will you post? Respond in one line.\nOutput: Hey everyone! Just watched <Inception> on the recommender system and it was\nabsolutely mind-blowing! Highly recommend checking it out!\nReferences\n\n# References\n\n[1] Mark O Riedl. Human-centered artificial intelligence and machine learning. Human behavior and emerging technologies, 1(1):33\u201336, 2019.\n[2] Ben Shneiderman. Human-centered artificial intelligence: Reliable, safe & trustworthy.  International Journal of Human\u2013Computer Interaction, 36(6):495\u2013504, 2020.\n[3] Jim Isaak and Mina J Hanna. User data privacy: Facebook, cambridge analytica, and privacy protection. Computer, 51(8):56\u201359, 2018.\n\n[4] Matthew Smith, Christian Szongott, Benjamin Henne, and Gabriele Von Voigt. Big data privacy issues in public social media. In 2012 6th IEEE international conference on digital ecosystems and technologies (DEST), pages 1\u20136. IEEE, 2012.\n[5] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. Virtual-taobao: Virtualizing real-world online retail environment for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4902\u20134909, 2019.\n[6] Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. Recsim: A configurable simulation platform for recommender systems. arXiv preprint arXiv:1909.04847, 2019.\n[7] Elizaveta Stavinova, Alexander Grigorievskiy, Anna Volodkevich, Petr Chunaev, Klavdiya Bochenina, and Dmitry Bugaychenko. Synthetic data-based simulators for recommender systems: A survey. arXiv preprint arXiv:2206.11338, 2022.\n[8] Michael D Ekstrand. Lenskit for python: Next-generation software for recommender systems experiments. In  Proceedings of the 29th ACM international conference on information & knowledge management, pages 2999\u20133006, 2020.\n[9] Martin Mladenov, Chih-Wei Hsu, Vihan Jain, Eugene Ie, Christopher Colby, Nicolas Mayoraz, Hubert Pham, Dustin Tran, Ivan Vendrov, and Craig Boutilier. Recsim ng: Toward principled uncertainty modeling for recommender ecosystems. arXiv preprint arXiv:2103.08057, 2021.\n10] Rob Stocker, David Cornforth, and Terry RJ Bossomaier. Network structures and agreement in social network simulations. Journal of Artificial societies and social simulation, 5(4), 2002.\n11] Jin Huang, Harrie Oosterhuis, Maarten De Rijke, and Herke Van Hoof. Keeping dataset biases out of the simulation: A debiased simulator for reinforcement learning based recommender systems. In Proceedings of the 14th ACM Conference on Recommender Systems, pages 190\u2013 199, 2020.\n12] Jiliang Tang, Xia Hu, and Huan Liu. Social recommendation: a review. Social Network Analysis and Mining, 3:1113\u20131133, 2013.\n13] Irwin King, Michael R Lyu, and Hao Ma. Introduction to social recommendation. In Proceedings of the 19th international conference on World wide web, pages 1355\u20131356, 2010.\n14] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In The world wide web conference, pages 417\u2013426, 2019.\n15] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\n16] Joon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.\n17] Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pages 337\u2013371. PMLR, 2023.\n\n[18] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966, 2023.\n[19] OpenAI. Gpt-4 technical report, 2023.\n[20] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv preprint arXiv:2304.07590, 2023.\n[21] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.\n[22] Richard C Atkinson and Richard M Shiffrin. Human memory: A proposed system and its control processes. In Psychology of learning and motivation, volume 2, pages 89\u2013195. Elsevier, 1968.\n[23] Nelson Cowan. What are the differences between long-term, short-term, and working memory? Progress in brain research, 169:323\u2013338, 2008.\n[24] Adit Krishnan, Ashish Sharma, and Hari Sundaram. Insights from the long-tail: Learning latent representations of online user behavior in the presence of skew and sparsity. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 297\u2013306, 2018.\n[25] Barry C Arnold. Pareto distribution. Wiley StatsRef: Statistics Reference Online, pages 1\u201310, 2014.\n[26] Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. Advances in neural information processing systems, 20, 2007.\n[27] Shuo Zhang and Krisztian Balog. Evaluating conversational recommender systems via user simulation. In Proceedings of the 26th acm sigkdd international conference on knowledge discovery & data mining, pages 1512\u20131520, 2020.\n[28] Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.\n[29] Tien T Nguyen, Pik-Mai Hui, F Maxwell Harper, Loren Terveen, and Joseph A Konstan. Exploring the filter bubble: the effect of using recommender systems on content diversity. In Proceedings of the 23rd international conference on World wide web, pages 677\u2013686, 2014.\n[30] Jinghua Piao, Jiazhen Liu, Fang Zhang, Jun Su, and Yong Li. Human\u2013ai adaptive dynamics drives the emergence of information cocoons. Nature Machine Intelligence, 5(11):1214\u20131224, 2023.\n[31] John T Wixted and Ebbe B Ebbesen. Genuine power curves in forgetting: A quantitative analysis of individual subject forgetting functions. Memory & cognition, 25:731\u2013739, 1997.\n[32] Richard B Anderson and Ryan D Tweney. Artifactual power curves in forgetting. Memory & Cognition, 25:724\u2013730, 1997.\n[33] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), pages 197\u2013206. IEEE, 2018.\n\n[34] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z Sheng, and Mehmet Orgun. Sequential recommender systems: challenges, progress and prospects. arXiv preprint arXiv:2001.04830, 2019.\n\n[34] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z Sheng, and Mehmet Orgun. Sequential recommender systems: challenges, progress and prospects. arXiv preprint arXiv:2001.04830, 2019.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Existing methods have significant limitations such as simplified user decision processes, real-data dependence, and simplified simulation environments. Recently, large language models (LLMs) have shown potential in achieving human-like intelligence, suggesting opportunities for more believable user behavior simulation.",
        "problem": {
            "definition": "The paper aims to address the challenge of simulating realistic user behaviors in various human-centered applications, particularly when real data is hard to obtain due to privacy concerns and commercial confidentiality.",
            "key obstacle": "Existing simulation methods rely heavily on real-world datasets, leading to a dependency that restricts their ability to simulate diverse behaviors across multiple environments."
        },
        "idea": {
            "intuition": "The idea is inspired by the cognitive neuroscience perspective that suggests human cognitive processes can be modeled using LLMs trained on extensive web data, allowing for more realistic simulations of user behavior.",
            "opinion": "The proposed idea entails creating an LLM-based agent framework that simulates user behaviors in a sandbox environment, enabling the exploration of social phenomena like information cocoons and user conformity.",
            "innovation": "The primary innovation of the proposed method is its ability to conduct nearly zero-shot simulations without relying on extensive real-world datasets, making it versatile across different environments."
        },
        "method": {
            "method name": "RecAgent",
            "method abbreviation": "RA",
            "method definition": "RecAgent is an LLM-based agent framework designed to simulate user behaviors in a flexible and realistic manner, incorporating cognitive neuroscience principles into its architecture.",
            "method description": "The core of RecAgent is its ability to simulate user behaviors through a profile module, memory module, and action module, allowing for dynamic interactions in a controlled environment.",
            "method steps": [
                "Create user profiles with various traits and interests.",
                "Implement a memory module that mimics human memory processes to store and recall user experiences.",
                "Use an action module to generate behaviors based on user interactions in a round-based simulation."
            ],
            "principle": "RecAgent's effectiveness lies in its ability to leverage the vast knowledge encoded in LLMs to generate realistic user interactions without the need for extensive real-world data."
        },
        "experiments": {
            "evaluation setting": "The experiments involved deploying RecAgent in a sandbox environment, simulating user interactions, and comparing the results against real human behaviors and traditional simulation methods.",
            "evaluation method": "The performance of RecAgent was assessed through metrics such as discrimination capability and generation capability, along with subjective evaluations from human annotators."
        },
        "conclusion": "The experiments demonstrated that RecAgent can produce simulated behaviors that closely resemble real human actions, providing a promising avenue for future research in user behavior simulation.",
        "discussion": {
            "advantage": "RecAgent offers significant advantages over traditional methods by enabling nearly zero-shot simulations and the ability to simulate diverse user behaviors across multiple environments.",
            "limitation": "The method currently follows a round-by-round approach, which may not fully capture the continuous nature of real-world interactions, limiting its flexibility.",
            "future work": "Future research should focus on refining the simulation process to allow for more dynamic interactions and exploring additional applications of RecAgent in various domains."
        },
        "other info": {
            "info1": "The paper presents a novel approach to user behavior simulation, emphasizing the potential of LLMs in generating realistic interactions.",
            "info2": {
                "info2.1": "RecAgent can simulate two social phenomena: information cocoons and user conformity.",
                "info2.2": "The design of RecAgent allows for human-agent collaboration, enhancing the realism of the simulations."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper discusses the significance of simulating high-quality user behavior data in human-centered applications, emphasizing the limitations of existing methods."
        },
        {
            "section number": "1.2",
            "key information": "The paper highlights the potential of large language models (LLMs) in achieving human-like intelligence for user behavior simulation."
        },
        {
            "section number": "2.1",
            "key information": "The paper defines the challenge of simulating realistic user behaviors, particularly in contexts where real data is hard to obtain due to privacy concerns."
        },
        {
            "section number": "2.2",
            "key information": "The paper discusses the evolution of simulation methods, pointing out their dependency on real-world datasets and the resulting limitations."
        },
        {
            "section number": "3.1",
            "key information": "The RecAgent method is introduced as an LLM-based agent framework designed to simulate user behaviors in a flexible and realistic manner."
        },
        {
            "section number": "4.1",
            "key information": "RecAgent leverages cognitive neuroscience principles and the vast knowledge encoded in LLMs to generate realistic user interactions."
        },
        {
            "section number": "4.2",
            "key information": "The paper analyzes how the integration of LLMs through the RecAgent framework enhances the simulation of user behaviors."
        },
        {
            "section number": "10.1",
            "key information": "The paper identifies existing challenges in traditional user behavior simulation methods, such as their reliance on real-world datasets."
        },
        {
            "section number": "10.2",
            "key information": "Future work is suggested to focus on refining the simulation process for more dynamic interactions and exploring additional applications of RecAgent."
        },
        {
            "section number": "11",
            "key information": "The conclusion emphasizes the promising avenue for future research in user behavior simulation through the use of LLMs and the RecAgent framework."
        }
    ],
    "similarity_score": 0.7246191004904977,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8409/84097baa-233d-4f47-9027-cc2ad14dd62c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b165/b165c918-27d2-4acd-bcae-47e321a98378.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68c1/68c1ff61-26a5-480f-98a6-4c4009639cb8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c72/0c722c8e-db39-4154-8113-bdc50e43d46d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/58c6/58c62f5f-e9b4-4855-9fa9-9bc5360d19b7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/093c/093cf1c5-4774-49ec-8df5-b8f5a46e51bb.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c707/c707bf67-4999-4793-8aab-f4781de9bb24.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc3a/dc3afa86-4c07-45f2-b91b-d52afbc2f9e1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8aaa/8aaa9bc7-db89-4bdc-94e5-28dc926b7abd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/495d/495d431c-9eb6-42c6-93b5-35c98ad1a299.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5d1f/5d1f62a5-1bb2-41aa-a21e-65849e3f087c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f96/9f96d49f-d65f-41dd-92cb-e6acc00166e5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3768/37685582-8c81-4c5b-8447-d8733c47542e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3571/3571b301-9366-49da-a07b-7b1834032428.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4853/485362da-92bf-41ad-a3c6-af4bd8b0591c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1a9/d1a9d79e-a6da-429e-bd26-4f519da570c0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98a3/98a3c368-2233-43f1-a175-7ccb3648ac11.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/User behavior simulation with large language model based agents.json"
}