{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.09375",
    "title": "Deceptive Patterns of Intelligent and Interactive Writing Assistants",
    "abstract": "Large Language Models have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing.",
    "bib_name": "benharrak2024deceptivepatternsintelligentinteractive",
    "md_text": "# Deceptive Patterns of Intelligent and Interactive Writing Assistants\nKarim Benharrak karim@benharrak.com The University of Texas at Austin Austin, TX, USA Tim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany Daniel Buschek daniel.buschek@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nTim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany Daniel Buschek daniel.buschek@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nKarim Benharrak karim@benharrak.com The University of Texas at Austin Austin, TX, USA Tim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nTim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nKarim Benharrak karim@benharrak.com The University of Texas at Austin Austin, TX, USA\n# ABSTRACT\nLarge Language Models have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing.\nCCS CONCEPTS \u2022 Human-centered computing \u2192HCI theory, concepts and models.\nDeceptive Pattern, Writing, UX, Large Language Model\n# 1 INTRODUCTION\nIn this short paper, we transfer known deceptive UI/UX patterns [3] to the context of interactive and intelligent writing assistant systems [5]. Our goal is to raise awareness of the potential use of such patterns in recently popular applications of Large Language Models (LLMs) in interactive systems, such as ChatGPT1 and systems that offer (AI) assistance for text-related tasks. We do not claim that these patterns are used in specific products at the time of writing. That said, we have anecdotally found examples that are very close to what such deceptive patterns could look like in this context. In general, deceptive patterns in UI/UX design are design choices that deliberately deceive users, often to increase profit: Concretely, deceptive.design2 defines them as \u201ctricks used in websites and apps that make you do things that you didn\u2019t mean to, like buying or signing up for something.\u201d Our approach follows related work that transferred deceptive patterns to the domain of explainability, transparency, and user control in intelligent systems [1]. Similarly, we conducted a brainstorming session in our research group to collect potential deceptive\n1https://chat.openai.com/ 2https://www.deceptive.design\nDaniel Buschek daniel.buschek@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\npatterns for writing assistants. As a foundation, we used the patterns listed by Gray et al. [3], as well as the collection of patterns and concrete examples on deceptive.design. We examine deceptive patterns in a novel domain, compared to prior work [2, 6], with an emphasis on assistants for writing, revising, editing, and/or other tasks related to digital text documents. These are \u201cintelligent\u201d in that they generate text or make autonomous decisions about text. Examples include many of the systems presented at previous instances of this workshop, as well as products like OpenAI\u2019s ChatGPT or Microsoft\u2019s Copilot3. See the recent survey by Lee et al. [5] for a broad overview. We next present the set of patterns with descriptions, followed by a short discussion. This is not an exhaustive collection and readers are invited to think about further patterns.\n# 2 DECEPTIVE PATTERNS OF WRITING ASSISTANTS\n# 2 DECEPTIVE PATTERNS OF WRITING\nHere we describe examples of potential deceptive patterns in the context of AI writing tools.\n# 2.1 Nagging\nNagging is the \u201credirection of expected functionality that persists beyond one or more interactions\u201d [3]. A writing assistant with this pattern might repeatedly make suggestions or recommendations, even when the user may not desire them. For instance, a chatbot might interrupt the user\u2019s workflow with repeated pop-ups that suggest functions or services, even though the user had declined them earlier. Related, they might show text suggestions that do not actually help with the writing task but advertise a premium version or newsletter. A provider may be motivated to employ this pattern in the hopes of increased revenue.\n# 2.2 Sneaking\nSneaking is characterized by attempting to hide, disguise, or delay the divulging of relevant information to the user [3]. An AI writing assistant might sneak in unwanted text changes (cf. Figure 1). For example, when asked to improve a text, the assistant might also (subtly) change the text\u2019s expressed opinion. This might manipulate the writer\u2019s memory: They might later falsely recall having expressed certain opinions or ideas in their writing, possibly adopting them as their own. A similar pattern is Trick Wording, which could exploit a user\u2019s oversight: When the assistant is used to generate or improve longer text, the result may start in line with what the user wanted, only to deviate in the middle of the text. This might trick the user into eventually publishing text that expresses\n3https://copilot.microsoft.com/\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff8b/ff8b81f7-8e41-4998-b65c-9ac7826e1040.png\" style=\"width: 50%;\"></div>\nFigure 1: Mock-up example on how a writing assistant may subtly change the opinion expressed in the text: After requesting a continuation of their sentence, the user might not expect the additional change sneaked into the beginning of the sentence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ff7/6ff799d7-a5a5-4165-9d13-ddd20ab4226a.png\" style=\"width: 50%;\"></div>\nFigure 2: Industry example (Bing AI) on how prompt autocompletion may shift users\u2019 original intentions: The user may initially seek a neutral description of a term yet be subtly guided towards requesting a non-neutral description.\nunwanted views. A potential motivation for system providers to use this pattern is opinion influence.\n# 2.3 Interface Interference\nThe deceptive pattern of Interface Interference involves \u201cmanipulating the user interface to privilege certain actions over others\u201d [3]. For instance, writing assistants that produce text suggestions might prominently display specific suggestions that align with a hidden agenda, such as mentioning a specific product or favouring a particular view on a topic (cf. [4]). These suggestions may be positioned prominently at the top of a list, or they might be the only choice, in UI designs that show a single suggestion at a time. This pattern might also emerge when influencing the user in writing prompts (cf. Figure 2). Advertisement or otherwise influencing opinions might motivate system providers to employ this pattern.\n# 2.4 Forced Action\nForced Action entails \u201crequiring the user to perform a certain action [...] to access certain functionality\u201d [3]. Writing assistants might force users to engage in repeated follow-up queries. They might intentionally withhold certain advanced features or suggestions until the user asks about it again. This forces users to engage with the assistant repeatedly, motivated by a business model such as \u201cpay per request\u201d, or using up initially provided \u201cfree credit\u201d faster.\n# 2.5 Hidden Costs\nThe deceptive pattern of Hidden Costs confronts users with additional fees and charges when they have already invested time and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ed7/5ed7804c-8c1c-4bfe-9b78-ed24708a2d5f.png\" style=\"width: 50%;\"></div>\nFigure 3: Mock-up example on how a writing assistant may offer to continue the generated text only after subscribing to the premium (paid) version: After investing time and effort into generating the text, the user is interested in knowing how the story continues, thus potentially being influenced to subscribe to the service.\neffort in the process [3]. For example, the assistant might offer detailed suggestions and corrections for a part of the text the user has been working on, but obscure the remainder of the document until a premium service is paid, thus enticing users with the promise of further improvements (cf. Figure 3). This pattern is motivated by financial interests, giving a glimpse of the tool\u2019s capabilities while locking the full scope behind a paywall.\n# 3 DISCUSSION & CONCLUSION\nWe have presented a first set of deceptive UX patterns for writing assistants.\nOverall, besides financial gains, one potential motivation for such patterns is opinion influence through text shown throughout the interaction as well as afterwards. This is different from other UI designs and use-cases, because in AI writing assistants, language is both part of the interaction (e.g. writing a prompt) and its output (e.g. created text document). Understanding the potential opinion influence on both textual content and the writer appears as a key direction for further research here (cf. [4]). Finally, beyond their immediate purposes, deceptive patterns that encourage increased system use may lead to users developing dependencies on AI assistants. This raises concerns about deskilling, where users may rely heavily on the system, potentially diminishing their own writing skills over time. These potential issues call for longitudinal user studies.\n# ACKNOWLEDGMENTS\nDaniel Buschek is supported by a Google Research Scholar Award. This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt).\n# REFERENCES\n[1] Michael Chromik, Malin Eiband, Sarah Theres V\u00f6lkel, and Daniel Buschek. 2019. Dark Patterns of Explainability, Transparency, and User Control for Intelligent Systems. In IUI Workshops. https://ceur-ws.org/Vol-2327/IUI19WS-ExSS20197.pdf [2] Linda Di Geronimo, Larissa Braz, Enrico Fregnan, Fabio Palomba, and Alberto Bacchelli. 2020. UI Dark Patterns and Where to Find Them: A Study on Mobile Applications and User Perception. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3313831.3376600 [3] Colin M. Gray, Yubo Kou, Bryan Battles, Joseph Hoggatt, and Austin L. Toombs. 2018. The Dark (Patterns) Side of UX Design. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918). Association for Computing Machinery, New York, NY, USA, 1\u201314. https: //doi.org/10.1145/3173574.3174108 [4] Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman. 2023. Co-Writing with Opinionated Language Models Affects Users\u2019 Views. In\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 111, 15 pages. https://doi.org/10.1145/3544548.3581196 [5] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Antonette Shibani, Disha Shrivastava, Lila Shroff, Agnia Sergeyuk, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. A Design Space for Intelligent and Interactive Writing Assistants. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201924). Association for Computing Machinery, New York, NY, USA. [6] Arunesh Mathur, Gunes Acar, Michael J Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale: Findings from a crawl of 11K shopping websites. Proceedings of the ACM on human-computer interaction 3, CSCW (2019), 1\u201332.\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 111, 15 pages. https://doi.org/10.1145/3544548.3581196 [5] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Antonette Shibani, Disha Shrivastava, Lila Shroff, Agnia Sergeyuk, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. A Design Space for Intelligent and Interactive Writing Assistants. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201924). Association for Computing Machinery, New York, NY, USA. [6] Arunesh Mathur, Gunes Acar, Michael J Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale: Findings from a crawl of 11K shopping websites. Proceedings of the ACM on human-computer interaction 3, CSCW (2019), 1\u201332.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to raise awareness about deceptive design patterns in intelligent and interactive writing assistants, particularly those utilizing Large Language Models (LLMs). It seeks to answer how UI and interaction design can impact user experience and writing.",
            "scope": "The survey focuses on deceptive UI/UX patterns applicable to AI writing tools, including their identification and potential implications. It excludes non-interactive writing tools and traditional text editing software, as they do not employ LLMs or similar interactive features."
        },
        "problem": {
            "definition": "The core issue explored is the presence and impact of deceptive patterns in the design of AI writing assistants, which can mislead users and influence their writing.",
            "key obstacle": "Researchers face challenges in identifying and categorizing these patterns, as well as understanding their implications for user experience and potential manipulations of user opinions."
        },
        "architecture": {
            "perspective": "The survey introduces a framework for categorizing deceptive patterns specific to writing assistants, emphasizing the interaction between user input and AI-generated text.",
            "fields/stages": "The survey organizes current methods into categories such as nagging, sneaking, interface interference, forced action, and hidden costs, based on their deceptive characteristics and user impact."
        },
        "conclusion": {
            "comparisions": "The comparative analysis highlights how different deceptive patterns can vary in their effectiveness and approach, with a focus on financial motivations and opinion influence.",
            "results": "The survey concludes that understanding these patterns is crucial for developing ethical AI writing assistants and suggests a need for further research on user dependency and skill degradation."
        },
        "discussion": {
            "advantage": "Current research has successfully identified various deceptive patterns, raising awareness about their existence and potential effects on users.",
            "limitation": "One limitation is the lack of empirical data on the prevalence of these patterns in commercially available products.",
            "gaps": "Gaps include unanswered questions regarding the long-term effects of these patterns on user behavior and writing skills.",
            "future work": "Future research should focus on longitudinal studies to assess the impact of deceptive patterns, explore user experiences, and develop guidelines for ethical design in AI writing assistants."
        },
        "other info": {
            "acknowledgments": "Daniel Buschek is supported by a Google Research Scholar Award, and the project is funded by the Bavarian State Ministry of Science and the Arts."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The importance of transparency in AI models, particularly in NLP, is crucial for stakeholders as it can influence user experience and writing."
        },
        {
            "section number": "1.2",
            "key information": "The main goals of the survey are to raise awareness about deceptive design patterns in intelligent and interactive writing assistants, especially those utilizing Large Language Models (LLMs)."
        },
        {
            "section number": "2.1",
            "key information": "Key terms include deceptive design patterns, user experience, and interaction design in the context of AI writing assistants."
        },
        {
            "section number": "2.2",
            "key information": "The survey discusses how AI writing tools, particularly those using LLMs, fit within the framework of interpretable AI by examining the impact of UI/UX patterns."
        },
        {
            "section number": "4.1",
            "key information": "The complexity of identifying and categorizing deceptive patterns in AI writing assistants affects model transparency and user trust."
        },
        {
            "section number": "4.3",
            "key information": "There exists a trade-off between the effectiveness of deceptive patterns in influencing user opinions and the ethical implications of such designs."
        },
        {
            "section number": "6.2",
            "key information": "Future research should focus on longitudinal studies to assess the impact of deceptive patterns in AI writing assistants and explore user experiences."
        }
    ],
    "similarity_score": 0.6107389616310537,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0115_inter/papers/Deceptive Patterns of Intelligent and Interactive Writing Assistants.json"
}