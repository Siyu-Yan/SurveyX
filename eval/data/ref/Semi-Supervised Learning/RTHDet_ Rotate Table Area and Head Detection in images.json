{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.03315",
    "title": "RTHDet: Rotate Table Area and Head Detection in images",
    "abstract": "Traditional models and datasets primarily concentrated on horizontal table detection, unable to accurately detect table regions and localize their head and tail parts in rotating scenarios, thus greatly restricting the development of table recognition in rotated contexts. Therefore, this paper presents a new task of detecting table regions and localizing their head and tail parts in rotation scenarios, along with the proposal of corresponding datasets, evaluation metrics, and methods. An innovative method\u2014\u2014Adaptively Bounded Rotation, is introduced to address the lack of datasets for detecting rotated table and their head-tail parts. Through this method, this study has produced TRR360D, a rotated table detection dataset which incorporates semantic information of table head and tail, based on the classic horizontal table detection dataset, ICDAR2019MTD. Furthermore, the study proposes a new evaluation metric, R360 AP, to validate the precision of rotated region detection and head-tail localization, building on the classic evaluation metric, AP. Following extensive literature review and rigorous experimentation, the high-speed and high-accuracy rotated detection model, RTMDet-S, was selected as the baseline. Furthermore, this study proposed the RTHDet model. RTHDet defines the Dr360 rotated rectangle angle representation based on the baseline and applies it to the newly added AL (Angle Loss) branch. This enhancement allows the model to locate the head and tail of the rotated table. By applying transfer learning methods and adaptive boundary random rotation data augmentation techniques, the AP50 (T<90) of RTHDet has been improved from 23.7% to 88.7% compared to the baseline model. This validates the effectiveness of RTHDet in handling the novel task of detecting rotating table regions and accurately localizing their head and tail parts.RTHDet is integrated into the widely-used open-source MMRotate toolkit: https://github.com/open-mmlab/mmrotate/tree/dev-1.x/projects/RR360.",
    "bib_name": "hu2023rthdetrotatetablearea",
    "md_text": "<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ac6/2ac65e43-78d2-4930-924f-c50111a84a5e.png\" style=\"width: 50%;\"></div>\nWenxing Hu\nSchool of Electronics & Information Engineering\nShanghai University of Electric Power\nShanghai 200090, China\nmoonstarwork@gmail.com\nMinglei Tong\nSchool of Electronics & Information Engineering\nShanghai University of Electric Power\nShanghai 200090, China\ntongminglei@gmail.com\n# ABSTRACT\nTraditional models and datasets primarily concentrated on horizontal table detection, unable to accurately detect table regions and localize their head and tail parts in rotating scenarios, thus greatly restricting the development of table recognition in rotated contexts. Therefore, this paper presents a new task of detecting table regions and localizing their head and tail parts in rotation scenarios, along with the proposal of corresponding datasets, evaluation metrics, and methods. An innovative method\u2014\u2014Adaptively Bounded Rotation, is introduced to address the lack of datasets for detecting rotated table and their head-tail parts. Through this method, this study has produced TRR360D, a rotated table detection dataset which incorporates semantic information of table head and tail, based on the classic horizontal table detection dataset, ICDAR2019MTD. Furthermore, the study proposes a new evaluation metric, R360 AP, to validate the precision of rotated region detection and head-tail localization, building on the classic evaluation metric, AP. Following extensive literature review and rigorous experimentation, the high-speed and high-accuracy rotated detection model, RTMDet-S, was selected as the baseline. Furthermore, this study proposed the RTHDet model. RTHDet defines the Dr360 rotated rectangle angle representation based on the baseline and applies it to the newly added AL (Angle Loss) branch. This enhancement allows the model to locate the head and tail of the rotated table. By applying transfer learning methods and adaptive boundary random rotation data augmentation techniques, the AP50 (T<90) of RTHDet has been improved from 23.7% to 88.7% compared to the baseline model. This validates the effectiveness of RTHDet in handling the novel task of detecting rotating table regions and accurately localizing their head and tail parts.RTHDet is integrated into the widely-used open-source MMRotate toolkit: https://github.com/open-mmlab/mmrotate/tree/dev-1.x/projects/RR360.\narXiv:2402.03315v1\nKeywords Dataset \u00b7 Object Detection \u00b7 Table Detection \u00b7 Rotated Detection \u00b7 Head-Tail Detection\n# 1 Introduction\nThe vast amount of data from the objective world is concealed in ubiquitous paper documents and forms, such as financial statements, express waybills, medical examination reports, and exam papers. Extracting valuable data from these tables requires the development of theories and technologies for table digitization and intelligence. This is crucial for enhancing efficiency in finance, education, and logistics industries, and for accelerating their intellectual development. However, when the paper world is converted into digital images, various rotate, affine, perspective transformations, and geometric distortions emerge, posing significant challenges for the localization and analysis of table regions. Traditional models and datasets mainly focus on the detection of horizontal tables as shown in Fig.1 (a), failing to precisely detect table regions in rotated scenarios as depicted in Fig.1 (b), and even more incapable of locating the head and tail of tables as illustrated in Fig.1 (c). This greatly restricts the development of downstream tasks in table recognition OCR. Therefore, this paper introduces a new task for detecting table regions in rotated scenes and locating\nThe vast amount of data from the objective world is concealed in ubiquitous paper documents and forms, such as financial statements, express waybills, medical examination reports, and exam papers. Extracting valuable data from these tables requires the development of theories and technologies for table digitization and intelligence. This is crucial for enhancing efficiency in finance, education, and logistics industries, and for accelerating their intellectual development. However, when the paper world is converted into digital images, various rotate, affine, perspective transformations, and geometric distortions emerge, posing significant challenges for the localization and analysis of table regions. Traditional models and datasets mainly focus on the detection of horizontal tables as shown in Fig.1 (a), failing to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ee7/0ee70e67-5df3-4fb4-abf3-d40270b2ffa9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Figure 1: Table Detection Task: (a) Detection of Table Horizontal Regions (b) Detection of Table Rotation Regions (c Detection of Table Head and Tail</div>\n(1) TRR360D Dataset: In response to the lack of a dedicated dataset for rotated table images in existing table detection research, this paper presents a novel dataset generation method called Adaptively Bounded Rotation. Drawing upon the ICDAR2019MTD[1] modern table detection dataset, the QBox (Quadrangle Box) annotation format from the DOTA[2] dataset is adopted to represent rotated boxes. By applying the Adaptively Bounded Rotation technique, the starting point and annotation direction of the QBox are regulated, thereby generating the TRR360D rotating table detection dataset, which inherently carries semantic information of the table\u2019s head and tail. This dataset comprises 840 images with 1446 rotated box annotations and is publicly available at https://github.com/vansinhu/TRR360D. (2) R360 AP Evaluation Metric: To address the limitation of the traditional AP evaluation metric, which only measures the accuracy of rotating table region predictions but does not account for the precision of head and tail localization, this paper introduces the R360 AP evaluation metric. Building upon the AP metric, the R360 AP metric incorporates angle constraints to assess the performance of both rotating table region detection and head-tail localization accuracy. (3) RTHDet Model: In response to the issue of existing models being unable to distinguish between the head and tail of rotated tables, the RTHDet (Rotate Table Head Detector) model is introduced. This model builds upon the RTMDet-S [3] baseline model and proposes the Dr360 rotation rectangle box angle representation method. This method is applied to the novel AL (Angle Loss) branch, enabling the detection of table regions and head-tail parts in rotating scenarios. By utilizing transfer learning and adaptive boundary random rotation data enhancement techniques, the RTHDet model achieves an AP50(T<90) score of 88.7%, representing a substantial improvement of 65.0 percentage points compared to the RTMDet-S baseline model. This demonstrates RTHDet\u2019s ability to effectively detect rotating table regions and precisely localize their head and tail parts. RTHDet is integrated into the widely-used open-source MMRotate toolkit: https://github.com/open-mmlab/mmrotate/tree/dev-1.x/projects/RR360. The structure of the paper is as follows: the first section serves as an introduction, the second section covers the related work, the third section presents the methodology, the fourth section discusses the experimental results, and the fifth section provides a summary of the entire paper.\n# 2 Related Work\nIn this section, an overview of the related work on horizontal table detection in document images is provided, categorized into stages of rule-based methods and deep learning-based methods. The limitations of horizontal table detection algorithms in accurately detecting rotated table regions and localizing their head and tail are analyzed. Furthermore, a brief introduction is given to the impact of classical and state-of-the-art methods for detecting rotated regions and their limitations when directly applied to table detection in the field.\n<div style=\"text-align: center;\"></div>\n# 2.1.1 Rule-Based Method\nIn early-stage research, rule-based methods were commonly employed. Itonori et al. [4] addressed the issue of table detection in document images using a rule-based method. Their approach harnessed the arrangement of text blocks and the position of ruling lines to identify tables within documents. Chandran and Kasturi [5] proposed an alternative method focusing on ruling lines for table detection. Similarly, Pyreddy and Croft [6] developed a heuristics-based table detection method that initially identified structural elements from a document and subsequently filtered the tables. Researchers have also developed tabular layouts and grammar for detecting tables in documents [7]. One such method predicts tables using the correlation of white spaces and vertical connected component analysis [8]. Another approach, introduced by Pivk et al. [9], converts tables found in HTML documents into a logical structure. Shigarov et al. [10] exploit metadata from PDF files, treating each word as a text block. Their method restructures tabular boundaries by leveraging bounding boxes for each word. For a more in-depth understanding of these rule-based methods, readers are encouraged to consult references [11]. While early rule-based systems can detect tables in documents with limited patterns, they rely on manual intervention to search for optimal rules. Furthermore, their propensity to produce generic solutions poses a challenge.Rule-based methods are not sufficiently robust and are limited to specific scenarios, unable to handle the challenges posed by diverse table styles. In contrast, deep learning-based approaches, as described in the following sections, effectively overcome these limitations in horizontal table detection.\n# 2.1.2 Deep Learning Method\nWith the advancement of deep learning, an increasing number of researchers have turned to it to address horizontal table detection challenges.The progress of object detection networks in computer vision has been shown to have a direct correlation with advancements in table detection in document images. Gilani et al. [12] demonstrated the applicability of Faster R-CNN [13] to table detection in document images, treating it as an object detection problem. Their work involved using distance transform methods to modify the pixels in raw document images fed to the Faster R-CNN. Building upon this foundation, Schreiber et al. [14] proposed another method that utilized Faster R-CNN [13], in conjunction with pre-trained base networks, such as ZFNet [14] and VGG-16 [15], to detect tables in document images. Siddiqui et al. [16] also developed a Faster R-CNN-based approach that implemented deformable convolutions [17] to tackle table detection with arbitrary layouts. In addition, Reference [18] adopted a Faster R-CNN with a corner-correlating approach to enhance the prediction of tabular boundaries in document images. Saha et al. [19] conducted an empirical study that revealed Mask R-CNN [20] outperforms Faster R-CNN in detecting tables, figures, and formulas. This finding was further corroborated by Zhong et al. [21] who used Mask R-CNN to localize tables. Additionally, YOLO [22], SSD [23], and RetinaNet [24] have been employed to demonstrate the advantages of closed-domain fine-tuning for table detection in document images. More recently, cutting-edge object detection algorithms, such as Cascade Mask R-CNN [25], Hybrid Task Cascade (HTC) [26] and DetectoRS[27], have been incorporated to enhance the performance of table detection systems in document images [28, 29, 30, 31, 32]. Despite the advancements made by these prior methods, there remains significant potential for improving the localization of accurate tabular boundaries in scanned document images. Furthermore, existing table detection techniques often rely on more substantial backbones or memory-intensive deformable convolutions. Ma et al. enhanced Faster R-CNN\u2019s accuracy for table detection by leveraging CornerNet in their RobusTabNet [33] method, demonstrating superior performance on various benchmarks with a basic ResNet-18 network. These methods primarily support the detection of horizontally aligned tables but do not cater to rotated table detection. They also lack the capability to differentiate between the head and tail sections of tables.\n# 2.2 Rotate Table Area Detection\nTraditional object detection algorithms, such as RetinaNet[24] and CascadeRCNN[34], can only detect the horizontal bounding boxes of table objects, and cannot detect the rotated bounding boxes of tables. In recent years, the field of rotated object detection has also made significant progress. Subsequently, an introduction to the representation of horizontal objects and the representation of rotated object regions will be provided. This will be followed by an analysis of the limitations of existing rotation detection algorithms when applied to table region detection.\nTraditional object detection algorithms predict parameters using center width and height representation, denoted as [x, y, w, h]. However, in the field of rotated table detection, an additional parameter \u03b8 is required. Previous work on the DOTA dataset used four ways to define \u03b8: Doc shown in Figure 2, D\u2032 oc shown in Figure 3, Dle90 shown in Figure 4, and Dle135 shown in Figure 5. Although these representations can represent the rotated region of the table, they cannot convey the semantics of the four corners of the rotated region, nor can they represent the head and tail of the table.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6a25/6a256973-aafb-4f8b-88b4-59d775d38c42.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Doc:the old version OpenCV definition,when OpenCV < 4.5.1, angle \u2208[\u221290, 0\u25e6), \u03b8 \u2208[\u2212\u03c0 2 , 0), and the angle between the width and the positive x-axis is a positive acute angle. At this time, the width edge will exchange as the angle changes. This definition comes from the cv2.minAreaRect function in OpenCV, which returns a value in the range [\u221290\u25e6, 0).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3241/324137f4-7277-4449-b793-b020b5c654ff.png\" style=\"width: 50%;\"></div>\nFigure 3: D\u2032 oc:the new version of OpenCV,when OpenCV \u22654.5.1, angle \u2208(0, 90\u25e6],\u03b8 \u2208(0, \u03c0 2 ], and the angle between width and the positive x-axis is acute (positive), based on the cv2.minAreaRect function in OpenCV, which returns a value in the range of (0, 90\u25e6].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e4ab/e4abcb86-3ae7-4ced-a1c1-29041d558519.png\" style=\"width: 50%;\"></div>\nTherefore, the rotation region detection models in MMRotate, such as RetinaNet-R, CascadeRCNN-R, and RTMDet-S, suffer from the limitation of angle definition, which prevents them from accurately localizing the head and tail parts of the rotated regions. However, the semantic information of the head and tail parts is crucial for improving the accuracy\nTherefore, the rotation region detection models in MMRotate, such as RetinaNet-R, CascadeRCNN-R, and RTMDet-S, suffer from the limitation of angle definition, which prevents them from accurately localizing the head and tail parts of the rotated regions. However, the semantic information of the head and tail parts is crucial for improving the accuracy\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2300/2300e0d6-c246-4172-9c0a-2294ab3e9710.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">g Edge 135 Definition,angle \u2208[\u221245\u25e6, 135\u25e6], \u03b8 \u2208[\u2212\u03c0 4 , 3\u03c0 4 ], and width ></div>\nof downstream tasks like table recognition and OCR.In Section 3.1, a new angle representation will be proposed and applied to existing rotation region detection models to enable the localization of the head and tail parts. Thus, it propels the development of downstream tasks.\n# 3 Method\nBy conducting experiments as described in Section 4.2, the RTMDet-S model was selected as the baseline for rotating table region detection due to its superior speed and accuracy.The RTMDet model is an improved version of the object detection algorithm YOLOX[35], which is very similar to YOLOX in overall structure and consists of three parts: CSPNeXt, CSPNeXtPAFPN, and SepBNHead. CSPNeXt and CSPNeXtPAFPN are used to extract features, and SepBNHead is used for detection. Like YOLOX, the core module in RTMDet is also CSPLayer, but the Basic Block is improved to CSPNeXt Block.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed80/ed801b52-181e-472d-a5c6-6c8542f0b4bd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: RTHDet Diagram</div>\nTo address the limitation of the baseline model in localizing the head and tail parts of rotated regions, a new angle definition method is proposed in this section, which is applied to the AL (Angle Loss) branch of the RTMDet-S model. The resulting model is named RTHDet, as illustrated in Figure 6. Additionally, the Adaptively Bounded Rotation method is proposed for the generation of the dataset in Section 4.1.2 and for random data augmentation during the training process.\nThe four angle definition methods mentioned in section 2.2, namely Doc, D\u2032 oc, Dle90, and Dle135, are unable to distinguish between the head and tail of objects, such as tables. Additionally, the angle range that can be represented is limited to 180\u25e6, meaning that it cannot distinguish between 0\u00b0 and -180\u00b0, as illustrated in Figure 7. To address these limitations, this study introduces the RBox (Rotated Bounding Box) using the R360 angle definition method, denoted as Dr360. The Rbox of a rotated rectangle bounding box is defined by Equation 1, with an angle range of [-180\u00b0, 180\u00b0). Furthermore, the corresponding QBox quadrilateral bounding box for RBox can be defined using Equation 2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c12/7c120fd8-5cdf-442b-9e05-436308f58ff9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Dr360: angle \u2208[\u2212180\u25e6, 180\u25e6),\u03b8 \u2208[\u2212\u03c0, \u03c0)</div>\nQBox = [(xA, yA), (xB, yB), (xC, yC), (xD, yD)]\n# QBox = [(xA, yA), (xB, yB), (xC, yC), (xD, yD)]\n# 3.1.1 QBox2RBox\nAs shown in Equation 3, the cv2.minAreaRect function in OpenCV is able to calculate the center point (cx, cy) and the width and height (w, h) of a rotated rectangle, given a set of points. However, the resulting angle range is limited to angle \u2208(0, 90\u25e6], which corresponds to the angle definition in the new version of OpenCV denoted as Doc\u2032. This method is only capable of representing the rotated area of a table and cannot be used to distinguish between the table head and tail.\n((cx, cy), (w, h), angle) = cv2.minAreaRect(QBox)\nTherefore, in this research, the value of \u03b8 is calculated using the inverse trigonometric functions in numpy as shown in Equation 4. The domain of \u03b8 is [\u2212\u03c0, \u03c0), expressed in radians.\n# 3.1.2 RBox2QBox\nThe relationship between RBox and QBox is shown in Figure 8, and all the parameters of QBox can be obtained from RBox through Equation 5, 6, 7, and 8.\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/adac/adacf809-c852-4f15-bd95-eb1e5c3b4622.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Relationship between RBox and Qbox</div>\n(xD, yD) = (cx \u2212h 2 sin\u03b8 \u2212w 2 cos\u03b8, cy + h 2 cos\u03b8 \u2212w 2 sin\u03b8)\nThe RTMDet algorithm has four models, namely L (Large), M (Medium), S (Small), and T (Tiny). The baseline model of RTMDet in MMRotate uses the rotated IoU loss function shown in Equation 9 to regress the rotated bounding boxes (RBox), where RIoU is illustrated in Figure 13. However, the limitation of LRbox is that it can only learn the features of rotated regions and cannot learn the 360\u25e6rotation angle features that include table head and tail information.\nTo facilitate the learning of 360\u00b0 rotation features, an Angle L1 Loss (AL) branch was incorporated. In conjunction with the Dr360 rotation rectangle box angle definition method defined earlier, the AL branch employs the loss function depicted in Equation 10 to constrain angle learning. This adaptation enables the RTMDet model to support 360\u25e6 rotation table detection and consequently head-tail location.\n# 3.3 Adaptively Bounded Rotation\nThe original OpenCV rotation algorithm 1 suffers from the issue of table image region loss, as illustrated in Figure 9 (a) To address this issue, an adaptive boundary rotation transformation algorithm is proposed in this study. The algorithm flow is illustrated in Algorithm 2, and the transformation effect is displayed in Figure 9 (b).\n# 4 Experiment\nThis section is divided into three major parts. Firstly, it introduces a new dataset and evaluation metrics specificall designed for the task of detecting rotated table regions and localizing their head and tail parts. Secondly, it presents th experiments conducted to select a baseline model for rotated table region detection. Lastly, it showcases the ablatio experiments conducted to investigate the localization of table head and tail parts.\n(9)\n(10)\nAlgorithm 1 OpenCV Original Rotation\nInput: image, \u03b8, points: List[(x,y)]\nOutput: r_image, r_points: List[(x,y)]\n1: h\u2190image.height\n2: w\u2190image.weight\n3: matrix\u2190cv2.getRotationMatrix2D(center, -angle, 1)\n4: r_image\u2190cv2.warpAffine(image, matrix, (w, h))\n5: pts \u2190points.reshape([-1, 2])\n6: pts \u2190np.hstack([pts, np.ones([len(pts), 1])]).T\n7: points \u2190matrix@points\n8: r_points \u2190[[points[0][x],points[1][x]] for x in range(len(points[0]))]\n9: return r_image, r_points\n1: h\u2190image.height\n2: w\u2190image.weight\n3: matrix\u2190cv2.getRotationMatrix2D(center, -angle, 1)\n4: r_image\u2190cv2.warpAffine(image, matrix, (w, h))\n5: pts \u2190points.reshape([-1, 2])\n6: pts \u2190np.hstack([pts, np.ones([len(pts), 1])]).T\n7: points \u2190matrix@points\n8: r_points \u2190[[points[0][x],points[1][x]] for x in range(len(points[0]))]\n9: return r_image, r_points\nAlgorithm 2 Adaptively Bounded Rotation\nInput: image, \u03b8, points: List[(x,y)]\nOutput: r_image, r_points: List[(x,y)]\n1: h\u2190image.height\n2: w\u2190image.weight\n3: matrix\u2190cv2.getRotationMatrix2D(center, -angle, 1)\n4: cos = abs(matrix[0, 0])\n5: sin = abs(matrix[0, 1])\n6: new_w = h * sin + w * cos\n7: new_h = h * cos + w * sin\n8: matrix[0, 2]\u2190matrix[0, 2]+(new_w - w) * 0.5\n9: matrix[1, 2]\u2190matrix[1, 2]+ (new_h - h) * 0.5\n10: r_image\u2190cv2.warpAffine(image, matrix, (new_w, new_h))\n11: pts \u2190points.reshape([-1, 2])\n12: pts \u2190np.hstack([pts, np.ones([len(pts), 1])]).T\n13: points \u2190matrix@points\n14: r_points \u2190[[points[0][x],points[1][x]] for x in range(len(points[0]))]\n15: return r_image, r_points\nAlgorithm 2 Adaptively Bounded Rotation\nInput: image, \u03b8, points: List[(x,y)]\nOutput: r_image, r_points: List[(x,y)]\n1: h\u2190image.height\n2: w\u2190image.weight\n3: matrix\u2190cv2.getRotationMatrix2D(center, -angle, 1)\n4: cos = abs(matrix[0, 0])\n5: sin = abs(matrix[0, 1])\n6: new_w = h * sin + w * cos\n7: new_h = h * cos + w * sin\n8: matrix[0, 2]\u2190matrix[0, 2]+(new_w - w) * 0.5\n9: matrix[1, 2]\u2190matrix[1, 2]+ (new_h - h) * 0.5\n10: r_image\u2190cv2.warpAffine(image, matrix, (new_w, new_h))\n11: pts \u2190points.reshape([-1, 2])\n12: pts \u2190np.hstack([pts, np.ones([len(pts), 1])]).T\n13: points \u2190matrix@points\n14: r_points \u2190[[points[0][x],points[1][x]] for x in range(len(points[0]))]\n15: return r_image, r_points\n# 4.1 Dataset\nIn response to the scarcity and high annotation cost associated with rotated image table detection datasets, particularly for head/tail detection, this study introduces a method for constructing a comprehensive rotated image table detection dataset.\n# 4.1.1 ICDAR2019MTD\nThe ICDAR2019MTD[1] Modern Table Detection dataset was introduced at the Table Detection and Recognition Competition of the 2019 International Conference on Document Analysis and Recognition. Comprising 600 training images and 240 testing images, it contains 977 annotated table instances in the training set and 449 annotated table instances in the testing set, provided in XML format. The annotation format for this dataset adheres to the convention displayed in Equation 11, with the top-left corner of the image serving as the starting point and the remaining annotation points arranged counterclockwise. Figure 10 illustrates the ICDAR2019MTD visualization without any head and tail information.\n# x1 y1 x2 y2 x3 y3 x4 y4\nA notable limitation of the ICDAR2019MTD dataset is its exclusive focus on horizontally-aligned tables, rendering it unsuitable for training table rotation object detectors. Moreover, the four-point annotation format lacks semantic information and does not specify the starting point as the top-left corner of the table object. To capitalize on the capabilities of the MMRotate[36] rotation object detection algorithm, this study converts the original ICDAR2019MTD dataset annotations in XML format to DOTA-format[2] text annotations, while introducing constraints and methods for generating TRR360D annotations.\n(11)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b42e/b42eb52a-1f63-44cf-a1c7-e548bd7ed60d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Rotation transformation</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df89/df89b1bd-fcaa-48de-be8d-75132d431f12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: ICDAR2019MTD Visualization</div>\n# 4.1.2 TRR360D\nFor the new task of detecting rotated table regions and localizing their head and tail parts, a manual adjustment was performed on a small portion of annotations in the ICDAR2019MTD dataset. Subsequently, the Adaptively Bounded Rotation algorithm 2 was utilized to generate a rotated table detection dataset, TRR360D, which contains semantic information about the table head and tail parts. The following provides an introduction to the format of the TRR360D dataset and the minimal manual adjustments required for annotation. Following the DOTA dataset annotation format, a line in the text file corresponding to the annotation of a rotated table instance is illustrated in Equation 12. Point A represents the top-left corner of the table, and points ABCD are arranged clockwise. Parameter D denotes the detection difficulty of the sample, which is uniformly defined as 0 in TRR360D.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1077/10774109-81fd-4bb3-b627-6a30aa2f4e55.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: TRR360D Visualization</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ed9/5ed98005-3d1d-4c8e-91b4-98c3b18f83d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee00/ee00f776-2e4a-437e-915d-67b89bd2f92b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Adjustment of starting point labeling</div>\nThe left image in Figure 12 depicts the visualization of the original annotation for image 10497 in the ICDAR2019MTD modern table detection dataset, where the four-point coordinates of ABCD are displayed as Equation 13. Since point A is not located in the top-left corner of the table, the labeling points need to be adjusted such that point A is positioned in the top-left corner of the table, and points BCD fulfill the clockwise constraint.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/71b8/71b8d17e-247a-48b8-bc96-6e971a69bec3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 1: TRR360D dataset folders and annotations</div>\nFolder\nDescription\nFormat\nImages\nInstances\nann_test_hbbox\nHorizontal test set annotations\ntxt\n240\n449\nann_test_obbox\nRotated test set annotations\ntxt\n240\n449\nann_train_hbb\nHorizontal training set annotations\ntxt\n600\n977\nann_train_obbox\nRotated training set annotations\ntxt\n600\n977\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8287/8287e0b2-2091-4f42-b089-9332b0763c74.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Rotate IoU</div>\nBy manually adjusting the coordinates according to the order of Equation 14, the resulting visualization is depicted in the right image of Figure 12. Now, point A is positioned in the top-left corner of the table, and points BCD meet the clockwise constraint.\n# 63 1006 63 119 666 119 666 1006 table 0\nIn the original dataset, some images do not meet the constraints. Consequently, the starting point positions of images 10001, 10037, 10149, 10206, and 10211 in the testing set were manually adjusted. Furthermore, the point order of images 10062, 10108, 10187, 10418, 10445, 10497, and 10537 in the training set were manually modified to comply with the constraints of point A being located at the top-left corner of the table and points ABCD arranged in a clockwise manner.\n# 4.1.3 Evaluation\nTo address the limitations of the traditional evaluation metric AP, which only measures the accuracy of rotation table region prediction without considering the precision of header and footer localization, this paper introduces the R360 AP evaluation metric. R360 AP is derived from AP by incorporating angle constraints and is denoted as AP50(T<90), among others. First, the concepts of Rotate IoU, Precision, and Recall on which the definition of AP50(T<90) relies will be introduced. Finally, a detailed description of AP50(T<90) will be provided. Rotate IoU : Let the bounding boxes detected by a deep learning model be denoted as predicted boxes P, and the annotated boxes in the dataset as ground truth boxes G. The formula for Rotated IoU is given by Equation 15, as illustrated in Figure 13.\nThe definitions of the subsequent metrics, such as TP, Precision, Recall, F1Score, and AP, are all associated w the IoU between the predicted bounding boxes P and the ground truth boxes G.\n(13)\n(14)\n(15)\nTP: True Positive refers to the predicted boxes P that satisfy the conditions IoUP G > TIoU and |P\u03b8 \u2212G\u03b8| < T\u03b8. In this case, TIoU represents the IoU threshold, which is only counted once for each ground truth box. A larger threshold implies a higher challenge for localization accuracy. For instance, in PASCAL VOC 2007, TIoU = 0.5. Additionally, T\u03b8 denotes the angle threshold, with smaller angles corresponding to greater difficulty. FP: False Positive refers to the number of predicted boxes P that satisfy IoU \u2264TIoU or |\u03b8P \u2212\u03b8G| \u2265T\u03b8, or the number of redundant predicted boxes detected for the same ground truth box. This metric is also known as the count of false detections. FN: False Negative, the number of ground truth boxes G that were not detected by the model, also known as missed samples. Precision: The ratio of the number of correctly predicted boxes to the total number of predicted boxes.\nRecall: The ratio of correct predictions to the total number of ground truth boxes is the evaluation metric for the dataset\nPR Curve :The PR curve is a common performance evaluation metric used to measure the performance of object detection models at different recall and precision levels. The PR curve is plotted by calculating the recall and precision of the object detection model at different confidence threshold levels. The AP value is the area under the PR curve, which is usually computed using the 11-point method for faster computation in practical implementation. AP50(T < 90) refers to the area under the precision-recall (PR) curve at a specific configuration, where the true positive (TP) condition is defined as IoUP G > 0.5 and |P\u03b8 \u2212G\u03b8| < 90, where TIoU = 0.5 and T\u03b8 = 90. AP75(T < 40) refers to the area under the precision-recall (PR) curve at a specific configuration, where the true positive (TP) condition is defined as IoUP G > 0.75 and |P\u03b8 \u2212G\u03b8| < 40, where TIoU = 0.75 and T\u03b8 = 40.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8d47/8d47dd86-5e5c-45c6-aebd-294733685aa9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Baseline Model Predict Visualization</div>\nIn this section, experiments were conducted on three models: R-RetinaNet, R-Cascade-RCNN, and RTMDet-S. The objective was to select an algorithm that strikes a balance between accuracy and speed, which will then serve as the baseline for implementing rotation table detection and head/tail recognition. The results are displayed in Table 2. R-RetinaNet achieved an AP50(T<360) score of 0.547, as depicted in the prediction outcomes in Figure 14. However, the predictions were not precise and the speed was limited to only 13.9 FPS.\n(16)\n(17)\n<div style=\"text-align: center;\">Table 2: Baseline model experimental results on the TRR360D Datase</div>\nModel\nAP50(T<360)\nFPS\nR-RetinaNet\n0.547\n13.9\nR-CascadeRCNN\n0.788\n7.6\nRTMDet-S\n0.788\n33.7\nR-CascadeRCNN achieved an AP50(T<360) score of 0.788. The PR curve illustrated in Figure 15 displayed superior performance compared to R-RetinaNet, and the visualization of its prediction outcomes in Figure 14 (c) demonstrated that the region predictions were predominantly accurate. Nonetheless, its speed was a mere 7.6 FPS, which was slower than R-RetinaNet. Although the accuracy was enhanced relative to R-RetinaNet, there was a trade-off with reduced speed.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7403/7403986b-723a-4bf0-b9c9-954e0cda8be5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: Baseline P-R Curve on the TRR360D Datase</div>\nAs depicted in Figure 14 (d), Table 2, and the PR curve in Figure 15, RTMDet-S demonstrated a level of performance that is comparable to R-CascadeRCNN in terms of AP50(T<360) precision. Additionally, it boasted a speed of 33.7 FPS, which is significantly faster than R-RetinaNet and R-CascadeRCNN. Based on these results, the baseline mode chosen for table rotation detection and head-tail recognition is RTMDet-S.\n# 4.3 Ablation Study on Head and Tail Location\nBased on the RTMDet baseline model, this section conducted extensive experiments on the TRR360D dataset, and the experimental results are shown in Table 3. In Table 3, L, M, S, and T in the Rotated RTMDet comparison experiment respectively represent the large, medium, small, and tiny models of RTMDet. Their FPS-AP chart is shown in Figure 16, with the T series model having the fastest average FPS of 42.0, the S series model having a relatively fast average FPS of 34.4, the M series model having a moderate average FPS of 18.9, and the L series having the slowest average FPS of 11.5. In Table 3 AL represents the Angle Loss branch, PT represents transfer learning, and RR represents adaptive boundary random rotation data augmentation. The experimental reasoning test environment is based on the Ubuntu 20.04 system, and the hardware environment used is: 12th generation Intel i7-12700 processor, 32GB RAM, NVIDIA GeForce 3060 12G GPU. Different hardware platforms may have been used during the training phase, such as NVIDIA GeForce 3060 12G GPU, NVIDIA Tesla V100 GPU, etc. The training used the AdamW optimizer for 32400 iterations, with a batch size set to 2.\n# 4.3.1 AL Branch\nAs shown in Table 3 of the rotation RTMDet comparison experiment, the AP50(T<360) accuracy for RTMDet-L RTMDet-M, RTMDet-S, and RTMDet-T is 0.787, 0.786, 0.788, and 0.778, respectively. When the absolute difference\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a04/5a040539-a8c3-4f9d-b89a-878c6f16fe43.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: FPS-AP50(T<90) on the TRR360D Dataset</div>\nbetween the predicted bounding box angle and the ground truth angle is limited to less than 90\u00b0, the AP(T<90) accuracy is 0.224, 0.217, 0.237, and 0.258, representing a decrease of 56.3%, 56.9%, 55.1%, and 52.0%, respectively. This indicates that the original RTMDet in MMRotate can only predict the regions of rotated bounding boxes, but it cannot learn and predict the semantic features of the four corner points of the rotated bounding boxes. As a result, it fails to distinguish between the 30\u00b0 and -150\u00b0 rotated bounding boxes shown in Figure 7, and can only represent a range of 180\u00b0. This highlights the limitation of the original RTMDet-S and other related algorithms in not supporting the localization of the head and tail of rotated table regions. Upon integrating the AL branch into the RTMDet model, the AP50(T<90) metrics of RTMDet-L-AL, RTMDet-MAL, RTMDet-S-AL, and RTMDet-T-AL showed an improvement of 24.6, 43.2, 37.4, and 31.9 percentage points, respectively, compared to their respective counterparts RTMDet-L, RTMDet-M, RTMDet-S, and RTMDet-T. This result highlights the effectiveness of the AL branch in facilitating 360\u00b0 rotation feature learning, enabling the detection of both the head and tail of tables. The visualization of the detection results also serves as evidence for the effectiveness of the AL branch. Figure 17 shows the visualization of the actual rotated bounding boxes for sample 10016 in the TRR360D rotated table detection test set. Specifically, Figure 17 (a) depicts the actual rotated bounding boxes, while Figure 17 (b) illustrates that although the RTMDet-L model correctly predicts the table region, the RIoU loss function fails to constrain the starting point of the table, leading to an inaccurate prediction of point A. By integrating the AL branch into the RTMDet-L model, the starting point prediction is effectively constrained, as shown in Figure 17 (c). While the prediction of point A is roughly correct, it has a certain impact on the accuracy of the predicted table region. Therefore, two methods are employed in the following to improve the accuracy.\n# 4.3.2 Transfer Learning\nIn this study, Transfer Learning is also attempted to enhance the accuracy of RTHDet. Table 3 presents the results of the rotating RTMDet comparison experiment. It shows that the RTMDet-L-AL-PT, RTMDet-M-AL-PT, RTMDet-S-AL-PT, and RTMDet-T-AL-PT models, which utilize transfer learning from the non-table detection domain DOTA dataset, outperform their counterparts without transfer learning, namely RTMDet-L-AL, RTMDet-M-AL, RTMDet-S-AL, and RTMDet-T-AL, in terms of AP50(T<90) metrics. Specifically, the AP50(T<90) index for the former models increased by 30.6, 14.9, 17.1, and 10.0 percentage points, respectively. These results demonstrate the effectiveness of transfer learning in the rotating table detection domain.\n<div style=\"text-align: center;\">arison of RTMdet models with different configurations on the TRR360D</div>\nModel\nAP50 (T<360)\nAP50 (T<90)\nFPS\nRTMDet-L\n0.787\n0.224\n11.5\nRTMDet-L-AL\n0.566\n0.470\n11.4\nRTMDet-L-AL-PT\n0.793\n0.776\n11.6\nRTMDet-L-AL-PT-RR\n0.897\n0.896\n11.4\nRTMDet-M\n0.786\n0.217\n18.9\nRTMDet-M-AL\n0.690\n0.649\n18.9\nRTMDet-M-AL-PT\n0.804\n0.798\n19.3\nRTMDet-M-AL-PT-RR\n0.896\n0.896\n18.8\nRTMDet-S\n0.788\n0.237\n33.7\nRTMDet-S-AL\n0.693\n0.611\n33.1\nRTMDet-S-AL-PT\n0.792\n0.782\n34.9\nRTMDet-S-AL-PT-RR\n0.890\n0.887\n36.0\nRTMDet-T\n0.778\n0.258\n42.1\nRTMDet-T-AL\n0.597\n0.577\n42.0\nRTMDet-T-AL-PT\n0.764\n0.677\n44.7\nRTMDet-T-AL-PT-RR\n0.805\n0.805\n39.3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7485/7485cda5-88af-4f17-960e-c43f49ab9aa2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 17: RTMDet Predict Visualization</div>\n# 4.3.3 Adaptive Boundary Random Rotation Data Augmentation\nThe Adaptive Boundary Rotation method 3.3 is applied to augment the training data on the unrotated horizontal table training set of TRR360D. This approach enriches the data and improves the model\u2019s accuracy. Table 3 of the rotating RTMDet comparison experiment shows that the RTMDet-L-AL-PT-RR, RTMDet-M-AL-PT-RR, RTMDet-S-ALPT-RR, and RTMDet-T-AL-PT-RR models, which use random rotation data augmentation with adaptive boundaries, outperformed their counterparts without this augmentation technique (i.e., RTMDet-L-AL-PT, RTMDet-S-AL-PT, RTMDet-S-AL-PT, and RTMDet-T-AL-PT) in terms of the AP50(T<90) metric, with an increase of 12.0, 9.8, 10.5, and 12.8 percentage points, respectively. The inference visualization of the RTMDet-L-AL-PT-RR model shown in Figure 17 (d) meets the accuracy requirements of region localization and correctly predicts the starting point A at the top-left corner of the table. As a result, line segment AB represents the head of the table, while line segment DC represents the tail of the table. These results demonstrate that adaptive boundary random rotation data augmentation effectively improves the performance of the RTHDet model.\n# 5 Conclusion\nTraditional models and datasets primarily concentrated on horizontal table detection, unable to accurately detect table regions and localize their head and tail parts in rotating scenarios, thus greatly restricting the development of table recognition in rotated contexts. Therefore, this paper proposed a novel task and provided substantial innovations in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1a3/b1a3d2ae-c15c-4bb8-899c-68f8487b33df.png\" style=\"width: 50%;\"></div>\ndetecting table regions and localizing their head and tail parts in rotational scenarios, along with the proposal of datasets, evaluation metrics, and methods. The Adaptively Bounded Rotation method is proposed to generate the TRR360D dataset on the classic ICDAR2019MTD dataset. The newly introduced R360 AP evaluation metric significantly improved the assessment of the precision of rotated region detection. Leveraging the Dr360 angle definition method, Transfer Learning, and Adaptive Boundary Rotation data augmentation on the RTMDet-S baseline model, the proposed RTHDet remarkably improved AP50 (T<90) from 23.7% to 88.7%, thereby attesting to its efficacy in addressing this innovative task. This research is poised to propel the development of downstream table recognition OCR tasks. Despite the significant progress, the current inability to detect arbitrary quadrilateral tables in perspective scenarios remains a notable limitation, necessitating further research to address this challenge.\n# References\n[1] Liangcai Gao, Yilun Huang, Herve Dejean, Jean Luc Meunier, Qinqin Yan, Yu Fang, Florian Kleber, and Eva Lang. ICDAR 2019 competition on table detection and recognition (cTDaR). In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 1510\u20131515, 2019. [2] Gui Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. DOTA: A Large-Scale Dataset for Object Detection in Aerial Images. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 3974\u20133983, 2018. [3] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. RTMDet: An Empirical Study of Designing Real-Time Object Detectors. pages 63\u201365, 2022. [4] Katsuhiko Itonori. Table structure recognition based on textblock arrangement and ruled line position. In Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR\u201993), pages 765\u2013768. IEEE, 1993. [5] Surekha Chandran and Rangachar Kasturi. Structural recognition of tabulated data. In Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR\u201993), pages 516\u2013519. IEEE, 1993. [6] Pallavi Pyreddy and W Bruce Croft. Tintin: A system for retrieval in text tables. In Proceedings of the second ACM international conference on Digital libraries, pages 193\u2013200, 1997. [7] E Green and M Krishnamoorthy. Recognition of tables using table grammars. In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 261\u2013278, 1995. [8] Jianying Hu, Bell Laboratories, Lucent Technologies, Mountain Avenue, and Murray Hill. Medium-Independent Table Detection. In Evaluation, volume 3967, pages 1\u201312. International Society for Optics and Photonics, 2000. [9] Aleksander Pivk, Philipp Cimiano, York Sure, Matjaz Gams, Vladislav Rajkovi\u02c7c, and Rudi Studer. Transforming arbitrary tables into logical form with TARTAR. Data \\& Knowledge Engineering, 60(3):567\u2013595, 2007. 10] Alexey Shigarov, Andrey Mikhailov, and Andrey Altaev. Configurable table structure recognition in untagged pdf documents. In Proceedings of the 2016 ACM Symposium on Document Engineering, pages 119\u2013122, 2016. 11] David W Embley, Matthew Hurst, Daniel Lopresti, and George Nagy. Table-processing paradigms: a research survey. International Journal of Document Analysis and Recognition (IJDAR), 8(2):66\u201386, 2006. 12] Azka Gilani, Shah Rukh Qasim, Imran Malik, and Faisal Shafait. Table Detection Using Deep Learning. In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, volume 1, pages 771\u2013776, 2017. 13] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6):1137\u2013 1149, 2017. 14] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1162\u20131167. IEEE, 2017. 15] Karen Simonyan and Andrew Zisserman. VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION. 2015. 16] Shoaib Ahmed Siddiqui, Muhammad Imran Malik, Stefan Agne, Andreas Dengel, and Sheraz Ahmed. DeCNT: Deep deformable CNN for table detection. IEEE Access, 6:74151\u201374161, 2018. 17] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable Convolutional Networks. Proceedings of the IEEE International Conference on Computer Vision, 2017-Octob:764\u2013773, 2017.\n[18] Ningning Sun, Yuanping Zhu, and Xiaoming Hu. Faster R-CNN based table detection combining corner locating. In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 1314\u20131319, 2019. [19] Ranajit Saha, Ajoy Mondal, and C V Jawahar. Graphical object detection in document images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 51\u201358. IEEE, 2019. [20] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):386\u2013397, 6 2020. [21] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI 16, pages 564\u2013580. Springer, 2020. [22] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. [23] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng Yang Fu, and Alexander C. Berg. SSD: Single shot multibox detector. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9905 LNCS:21\u201337, 2016. [24] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017. [25] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018. [26] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, and others. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974\u20134983, 2019. [27] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution. 2020. [28] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure. CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, volume 2020-June, pages 2439\u20132447, 2020. [29] Madhav Agarwal, Ajoy Mondal, and C. V. Jawahar. CDEC-Net: Composite deformable cascade network for table detection in document images. Proceedings - International Conference on Pattern Recognition, pages 9491\u20139498, 2020. [30] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context. 2021. [31] Danish Nazir, Khurram Azeem Hashmi, Alain Pagani, Marcus Liwicki, Didier Stricker, and Muhammad Zeshan Afzal. Hybridtabnet: Towards better table detection in scanned document images. Applied Sciences (Switzerland), 11(18), 2021. [32] Khurram Azeem Hashmi, Alain Pagani, Marcus Liwicki, Didier Stricker, and Muhammad Zeshan Afzal. CasTabDetectoRS : Cascade Network for Table Detection in Document Images with Recursive Feature Pyramid and Switchable Atrous Convolution. (September):1\u201321, 2021. [33] Chixiang Ma, Weihong Lin, Lei Sun, and Qiang Huo. Robust table detection and structure recognition from heterogeneous document images. Pattern Recognition, 133:109006, 2023. [34] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: High quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(5):1483\u20131498, 2021. [35] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. YOLOX: Exceeding YOLO Series in 2021. pages 1\u20137, 2021. [36] Yue Zhou, Xue Yang, Gefan Zhang, Jiabao Wang, Yanyi Liu, Liping Hou, Xue Jiang, Xingzhao Liu, Junchi Yan, Chengqi Lyu, Wenwei Zhang, and Kai Chen. MMRotate, volume 1. 2022.\n",
    "paper_type": "method",
    "attri": {
        "background": "Traditional models and datasets primarily concentrated on horizontal table detection, unable to accurately detect table regions and localize their head and tail parts in rotating scenarios, thus greatly restricting the development of table recognition in rotated contexts.",
        "problem": {
            "definition": "The paper aims to solve the issue of detecting table regions and localizing their head and tail parts in rotating scenarios, which traditional methods fail to address.",
            "key obstacle": "Existing methods are limited in their ability to detect rotated tables and differentiate between the head and tail parts, which hampers effective table recognition."
        },
        "idea": {
            "intuition": "The idea was inspired by the challenges posed by rotated tables in document images, where traditional detection methods are inadequate.",
            "opinion": "The proposed idea involves developing a new dataset and method for detecting table regions in rotated contexts and accurately localizing their head and tail parts.",
            "innovation": "The key innovation is the introduction of the Adaptively Bounded Rotation method and the RTHDet model, which improve detection and localization of rotated tables compared to existing approaches."
        },
        "method": {
            "method name": "RTHDet",
            "method abbreviation": "RTHDet",
            "method definition": "RTHDet is a model designed for detecting rotated table regions and localizing their head and tail parts using a novel angle representation method.",
            "method description": "The core of RTHDet involves utilizing the Dr360 angle representation and an Angle Loss branch to enhance localization accuracy.",
            "method steps": [
                "Utilize the RTMDet-S model as a baseline.",
                "Implement the Dr360 angle representation for rotated bounding boxes.",
                "Incorporate an Angle Loss branch for improved head and tail localization.",
                "Apply transfer learning and adaptive boundary random rotation data augmentation."
            ],
            "principle": "The effectiveness of RTHDet is based on its ability to learn 360\u00b0 rotation features and accurately localize the head and tail of rotated tables."
        },
        "experiments": {
            "evaluation setting": "The evaluation involved the TRR360D dataset, comparing RTHDet with baseline models like RTMDet-S and R-CascadeRCNN under various configurations.",
            "evaluation method": "Performance was assessed using the R360 AP evaluation metric, which incorporates angle constraints to measure detection and localization accuracy."
        },
        "conclusion": "The study demonstrated that RTHDet significantly improved the detection and localization of rotated table regions and their head and tail parts, achieving an AP50 (T<90) score of 88.7%, validating its effectiveness in this novel task.",
        "discussion": {
            "advantage": "RTHDet stands out due to its high accuracy and speed in detecting rotated tables and localizing their head and tail parts, outperforming existing methods.",
            "limitation": "The current method does not support the detection of arbitrary quadrilateral tables in perspective scenarios, which remains a challenge.",
            "future work": "Future research should focus on enhancing the model's capability to handle perspective scenarios and improving the detection of more complex table structures."
        },
        "other info": {
            "dataset": "The TRR360D dataset consists of 840 images with 1446 rotated box annotations and is publicly available.",
            "evaluation metric": "The R360 AP evaluation metric was introduced to measure the precision of rotated region detection and head-tail localization."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational idea behind semi-supervised algorithms is to leverage both labeled and unlabeled data to improve learning outcomes, similar to how RTHDet aims to enhance table detection by utilizing a novel dataset and method."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of RTHDet is to address the limitations of traditional methods in detecting rotated tables, which parallels the significance of semi-supervised learning in improving model performance."
        },
        {
            "section number": "3.5",
            "key information": "RTHDet represents a hybrid approach by combining traditional detection methods with innovative techniques such as the Dr360 angle representation and Angle Loss branch for improved localization."
        },
        {
            "section number": "4.1",
            "key information": "The importance of data labeling is highlighted by the TRR360D dataset, which consists of 840 images with 1446 rotated box annotations, providing essential labeled data for training the RTHDet model."
        },
        {
            "section number": "7.1",
            "key information": "The challenges faced in semi-supervised learning, such as scalability and data quality, are echoed in the limitations of RTHDet, which currently does not support detection of arbitrary quadrilateral tables in perspective scenarios."
        },
        {
            "section number": "8",
            "key information": "The conclusion of the RTHDet study emphasizes the potential for significant improvements in detection and localization accuracy, reinforcing the importance of innovative methods in advancing machine learning applications."
        }
    ],
    "similarity_score": 0.5770397045608371,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/RTHDet_ Rotate Table Area and Head Detection in images.json"
}