{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.02054",
    "title": "Deconstructing In-Context Learning: Understanding Prompts via Corruption",
    "abstract": "The ability of large language models (LLMs) to $``$learn in context$\"$ based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models ($\\geq$30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted.",
    "bib_name": "shivagunde2024deconstructingincontextlearningunderstanding",
    "md_text": "# Deconstructing In-Context Learning: Understanding Prompts via Corruption\nNamrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky University of Massachusetts Lowell {nshivagu, vlialin, smuckati, arum}@cs.uml.edu\nAbstract The ability of large language models (LLMs) to \u201clearn in contex\u201d based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models (\u226530B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted. The code is available at this URL. Keywords: ICL, prompting, prompt components, prompt corruption, zero-shot evaluation\n# 1. Introduction\nThe ability of language models to respond to prompts and learn in context has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Bard (Google AI, 2023), which use large pre-trained language models as the backbone. AI assistants built on top of backbone models are robust to prompt variation, in large part due to alignment techniques involving learning from human feedback (Ouyang et al., 2022). However, the underlying backbone models are notoriously brittle in this respect, and their performance often varies widely with slight prompt modifications. Building a high-quality backbone model remains a core challenge, and one of the more common ways to gauge their quality is to conduct in-context evaluation, which suffers from high sensitivity to prompt variation. Despite this sensitivity, models have shown remarkable resilience to corruption in certain parts of the prompt. Recently proposed explanations for in-context learning, such as implicit gradient descent (Dai et al., 2022; von Oswald et al., 2022), fail to account for this resiliency. A number of previous studies have examined the impact of prompts on model performance across different tasks (Brown et al., 2020; Radford et al.,\n2019; Lu et al., 2021; Lialin et al., 2022; Talmor et al., 2020; Webson and Pavlick, 2021; Lampinen et al., 2022; Reynolds and McDonell, 2021; Min et al., 2022; Zhao et al., 2021; Raman et al., 2022; Kim et al., 2022). However, the results have sometimes been contradictory. In particular, the studies of individual prompt components have been plagued by inconsistency. For instance, Webson and Pavlick (2021) found that meaningless instructions don\u2019t have a significant effect on model performance. On the other hand, evidence from Mishra et al. (2021b) and Reynolds and McDonell (2021) suggested that meaningful prompts are crucial for zero-shot performance. Similarly, while Min et al. (2022) and Wei et al. (2023b) demonstrated that label semantics aren\u2019t necessary for zero-shot performance, both Kim et al. (2022) and Webson and Pavlick (2021) argued otherwise. Additionally, the majority of prior research has focused either on smaller models with <15B parameters or black-box LLMs like GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), and PaLM (Chowdhery et al., 2022), and therefore don\u2019t offer a complete understanding of the significance of different prompt components across model sizes. In the present study, we decompose the input prompt into four components: task instructions, inline instructions, and demonstrations that consist\nof input/target label pairs (see Figure 1) and investigate the effect of structural and semantic corruptions to these prompt components across ten models, ranging from 1.5B to 70B. We evaluate them on ten datasets, covering both classification and generation tasks. Building on techniques from model interpretability research, we also examine the average per component attention of two of the models to determine which components contribute more to model output. Our results show that:\n# 1. Including repeated text in the prompt boosts model performance.\n2. Addition of both task and inline instructions improves model performance, even when these instructions are random words.\n3. Larger models exhibit higher sensitivity to prompt semantics and pay more attention to the semantically relevant prompt components.\n# 2. Related work\nSeveral prior studies have investigated in-context learning (ICL) capabilities of large language models (Brown et al., 2020; Radford et al., 2019; Lu et al., 2021; Lialin et al., 2022; Talmor et al., 2020; Webson and Pavlick, 2021; Lampinen et al., 2022; Reynolds and McDonell, 2021; Min et al., 2022; Zhao et al., 2021; Raman et al., 2022; Wei et al., 2023b; Madaan and Yazdanbakhsh, 2022). However, when it comes to the impact of different parts of the prompt on model performance, the conclusions have often been inconsistent. For example, Webson and Pavlick (2021) suggest that relevant and irrelevant instructions in the prompt yield similar model performance, whereas Mishra et al. (2021b) and Reynolds and McDonell (2021) argued the opposite. The latter studies showed that detailed and task-relevant prompts that closely resemble natural human language give better model performance. Similarly, Kim et al. (2022) studied the importance of ground-truth labels for in-context learning and found that ground-truth labels were important for ICL, contradicting the results from Min et al. (2022). While prior work has not provided a comprehensive analysis of the impact of different prompt components on model performance, a few studies have selectively examined specific elements of the prompt. For example, Lampinen et al. (2022) looked into adding explanations to the demonstration and found that adding task explanations can significantly improve model performance. Min et al. (2022) examined different aspects of in-context demonstrations and found that input-label mapping did not significantly affect model accuracy. Webson and Pavlick (2021) and Gu et al. (2021) studied\ninstructions and labels and suggested that the labels were more important than instructions. Wei et al. (2023b) investigated the effect of semantic priors associated with the labels during pre-training, relative to the input-label mapping provided in the prompt, showing that the ability to override semantic priors with the prompt is an emergent ability. Prior work has also examined different prompting strategies, as well as additional fine-tuning to improve in-context performance. For instance, Xu et al. (2023) introduced re-reading prompting strategy where they repeat the question in the prompt and found that this strategy improves performance for ChatGPT and GPT-3. Wei et al. (2023a) proposed \u201csymbol tuning\u201d, fine-tuning models with arbitrary labels, and observed performance improvements on unseen ICL tasks. In a different approach, Gonen et al. (2022) proposed constructing the prompt with lower perplexity for better performance. Few studies (Dai et al., 2022; von Oswald et al., 2022) also linked attention computation performed during in-context learning to model updates performed with gradient descent. However, it is unclear how this mechanism would account for some aspects of in-context learning, such as the success of zero-shot prompting.\n# 3. Experiment setup\n3.1. Prompts\n# 3.1. Prompts\nPrompt components We use the term \u201cprompt\u201d to refer to the complete input text provided to the model. A prompt consists of four main components: a task instruction, demonstration input, demonstration label, and brief inline instructions accompanying each demonstration (see Figure 1). Two newlines are used as a separator after the task instruction and after each demonstration. We refer to a prompt with all components, including a test instance and its inline instruction, as a baseline prompt. Our experiments are conducted in a zero-shot and 4-shot setting. Figure 1 shows the baseline prompt for Twitter Emotion classification dataset. Baseline prompts for all the datasets are provided in the Appendix A.\nPrompt design We leverage the task instructions and demonstrations provided by Wang et al. (2022c) for each dataset, as they have been reviewed and refined through multiple iterations. We use inline instructions from PromptSource (Bach et al., 2022). To ensure coherence and simplicity, we made a few changes to the task and inline instructions, following the recommendations of Gonen et al. (2022).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a22/2a22ab2f-611b-43e1-82c7-c9acd6869f6a.png\" style=\"width: 50%;\"></div>\ne 1: Prompt Components of Twitter Emotion Classification baseline prompt. Demonstration includes t , inline instruction , label . Two newlines are added as separators after task instruction and each onstration. Prompts taken verbatim from Super-NaturalInstructions and PromptSource.\nPrompt corruptions We perform two types of prompt corruptions: structural corruption and semantic corruption. In structural corruption, we add or remove the prompt components, depending on the setup. We start with the test instance and add components one by one to analyze their effect on model performance. To assess the impact of repeated text in the prompt, we systematically eliminate inline instructions from the baseline prompt. We remove the inline instruction from one demonstration, then two, and continue this process until we have removed the inline instructions from all four demonstrations. These corruptions are referred to as repeated text corruptions. We keep the inline instruction which follows the test instance as is. In semantic corruption, we disrupt the semantics of prompt components. Task and inline instructions are corrupted with random words drawn from the english_words1 set. With a 100% corruption rate, we refer to this corruption as the random words corruption. The random word instructions retain the same number of tokens as the original (meaningful) instructions. Labels are perturbed by assigning incorrect labels to the demonstrations. These incorrect labels are drawn from the same label space. This corruption is referred to as the\n1https://pypi.org/project/english-words/\nwrong label corruption and is only applied to classification tasks. In the random words label corruption, we replace original labels with random words, similar to the instruction random words corruption, but we use the original labels to assess the model\u2019s predictions. To noise the demonstration inputs, we replace them with random sentences sampled from Common Crawl. We refer to this as Out-OfDistribution (OOD) input corruption.\n# 3.2. Models, datasets and metrics\nModels To cover a broad range of models, we conducted experiments with ten models ranging in size from 1.5B to 70B. The models are GPT2xl (Radford et al., 2019), GPT-J-6B (Wang and Komatsuzaki, 2021), Pythia-12B (Biderman et al., 2023), OPT-30B, OPT-30B-IML-MAX2, OPT-66B (Zhang et al., 2022a), Vicuna-33B (Chiang et al., 2023), Llama-7B, Llama-2-70B and Llama-2-70Bchat (Touvron et al., 2023). This provides a wide range of model sizes, and types and also doesn\u2019t focus on a single model family, making the results more generalizable. Our study included pre-trained language models, as well as instruction-tuned and aligned models. We refer to models as \"aligned\"\n2Instruction tuned variant of OPT.\nwhen they undergo additional training through reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022).\nDatasets The evaluation was conducted on ten datasets from Super-NaturalInstructions (Wang et al., 2022c). Datasets include eight classification tasks: RTE, Medical Question Pair, Financial Phrasebank, Twitter Emotion classification, CoLA, AgNews, COPA, Com2sense, and two generation tasks: TriviaQA and Mathdataset answer generation (Wang et al., 2022c). Following Wang et al. (2022c), we used 100 randomly sampled balanced test instances for each of the 10 datasets. Data statistics are shown in Table 3 in the Appendix.\nEvaluation method and metrics For evaluation, we used Exact Match for classification tasks, and Rouge-L for generative tasks. Following Wang et al. (2022c), we strip the model response at the first full stop symbol. We used the jackknife variance estimate method to calculate the mean of model performance. The mean performance, averaged across tasks, is reported for each model in Tables 1 and 2. For all figures, we plot the mean as the \u201caverage score\u201d. For generation tasks, we used the greedy decoding strategy, setting the top-p and temperature values to 1 and limiting the maximum number of new tokens to 10.\nAttention computation To understand the significance of different prompt components, we computed the average attention norm per prompt component for GPT-J-6B (Wang and Komatsuzaki, 2021) and OPT-30B (Zhang et al., 2022a). Following Kobayashi et al. (2020), we compute the L2 norm of the sum of the attention-weighted value vector \u2225\ufffd\u03b1V (x)\u2225, where \u03b1 is the attention weight, x is the input vector, and V (x) = WO(WV x) is the value projection of x, followed output transformation WO. Specifically, we used the last token of the prompt as the query token and extracted attention norms for the other tokens. For each token, we averaged these norms across all layers. We then averaged the resulting scores over all tokens corresponding to a given prompt component. This average is reported in Figures 6, 13 and 12. For GPT-J-6B, each plot shows the average attention norm over 100 samples (10 samples per dataset). For OPT-30B, due to computing costs, we focused on datasets with shorter baseline prompts: CoLA, Twitter Emotion classification, and TriviaQA. For each attention plot, we included the average attention norm from 30 samples, selecting only those where the model predictions were correct.\n# 4. Results\nables 1 and 2 show results for each corruption cross 10 datasets. Here\u2019s a breakdown of the ompt configurations used: \u2022 Test instance: The input prompt containing only the test instance. \u2022 +task instr.: Test instance with the task instruction added. \u2022 +inline instr.: Test instance with an inline instruction added instead of the task instruction. \u2022 +both instr.: Test instance with both task and inline instructions added. \u2022 +demos.: Test instance plus four demonstrations (no instructions included). \u2022 +task instr. +demos.: Test instance with task instructions and four demonstrations. \u2022 +inline instr. +demos.: Test instance with four demonstrations (each containing an inline instruction) and no task instruction. \u2022 Baseline: Includes all components (task instruction, inline instruction, demonstrations, and test instance). \u2022 Baseline -inputs: Baseline prompt with demonstration inputs removed. \u2022 Baseline -labels: Baseline prompt with demonstration labels removed. \u2022 Rw both instr.: Random word corruption applied to both task and inline instructions. \u2022 Rw labels: Random word corruption applied to labels. \u2022 OOD inputs: Out-of-distribution input corruption. \u2022 Inline instr. in [n] demos.: Meaningful inline instructions added to \"n\" demonstrations. \u2022 Rw inline instr. in [n] demos.: Random word inline instructions added to \"n\" demonstrations.\nAdding task and inline instructions boosts the performance even when the instructions are random words. Our experiments with four models (see Figure 2) highlight that the addition of demonstrations to the test instance has the most impact, producing a gain of 25-35% across all models. Accuracy is improved further by adding task instructions and inline instructions (Figure 2). The models gain between 5-18% accuracy when meaningful instructions are added. Interestingly, this gain is between 1-12% when instructions are just random words, except for Llama-70B. Figure 3 shows a similar effect for all ten models.\ninstructions. From Figure 3, we see that the performance gained by inline instruction is 2.5-12.5% across models whereas task instruction helps models by only 1-7.5%. This pattern is observed for models of all sizes, except OPT-66B, where the gain obtained by the inline instructions is close to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e51/1e51fd72-340f-42c6-99c6-9bd5f12cb055.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Demonstrations improve the average score, adding task and inline instruction improves it furthe even when instructions are just random words. The Y-axis represents the average score across a datasets. The use of random words is indicated with \u201crw\u201d.</div>\nStructural Corruptions\nGPT2-xl\nGPT-J-6B\nOPT-30B\nOPT-66B\nTest instance\n0.9\n4.2\n4.4\n4.7\n+ task instr.\n0.4\n2.3\n1.2\n0.7\n+ inline instr.\n1.6\n1.6\n2.2\n2.1\n+ both instr.\n1.6\n3.3\n2.7\n2.0\n+ demo.\n28.1\n34.9\n38.3\n39.6\n+ task instr. + demo.\n30.4\n37.1\n40.9\n43.7\n+ inline instr + demo.\n37.1\n38.7\n40.8\n43.0\nBaseline\n40.7\n42.6\n45.6\n45.9\nBaseline - labels\n0.1\n0.2\n0.5\n0.9\nBaseline - inputs\n27.1\n17.0\n22.0\n22.3\nTable 1: Model performance averaged across all datasets. The highest performance is in bold, baseline prompt performance is underlined.\nthat of the task description (Figure 3). This suggests models benefit from the brief repetitive text more than from a detailed task instruction.\nRepeated text boosts performance. We further investigated the effects of repeating inline instructions. In Figure 4, we plot the results for the baseline prompt (which includes all components) and the results obtained when eliminating inline instructions from demonstrations one by one. Note that we always keep the inline instruction that occurs after the test instance. We see a huge drop in performance when removing the inline instruction from all demonstrations. The drop is 20-35% across all models, except OPT-30B-IML, which shows a drop of 8.8%. Interestingly, we observed a similar effect for prompts in which inline instructions were replaced with random words, producing a performance drop of 40-51% (cf. Figure 5). This suggests that the mere presence of repetitive text in the prompt, whether relevant or irrelevant, can boost model performance. However, how often we introduce these repeti-\ntions in the prompt also matters. Table 2 shows that models like OPT-30B and Llama-70B can achieve better performance with only two or three meaningful inline instructions in the prompt. The attention plot in Figure 11 shows that if we introduce inline instruction in four demonstrations rather than in one, the attention to the input segment of each demonstration dropped from 2% to 1.8% and the attention to each of the labels decreased by around 0.4%. We see a similar pattern for repeated text corruptions in prompts with random word instructions (see Figure 15 in Appendix).\nLabels must be drawn from the label space, but need not be correct. When we perturb labels with the wrong label corruption, the performance drops just by 0-6% across all models (except for Llama-2-70B, where the drop is 18.6%). However, when we apply random words label corruption, the accuracy drops to almost zero for all models (except OPT-30B-IML-MAX) (cf. Figure 9). Complete removal of the labels from the prompt has a similar effect (cf. Figure 7).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ee5/4ee5714c-3142-4818-8fe1-5c7144868abc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Adding relevant or meaningless instruction to the prompt improves model performance. The components are added to the test instance. For example \u2018+ demonstrations\u2019 means test instance + demonstration. The Y-axis represents the average score across all datasets. Random words are indicated with \u201crw\u201d.</div>\nCorruptions\nGPT2\nxl\nGPT-J\n6B\nLLama\n7B\nPythia\n12B\nOPT\n30B\nOPT-30B\nIML-MAX\nVicuna\n33B\nOPT\n66B\nLLama-2\n70B\nLLama-2\n70B-chat\nStructural\n+ demos.\n28.1\n34.9\n40.8\n33.7\n38.3\n49.0\n48.5\n38.6\n49.1\n48.8\n+ task instr. + demos.\n30.4\n37.1\n44.2\n37.2\n40.9\n55.1\n49.7\n43.7\n55.9\n56.5\n+ inline instr. + demos.\n37.1\n38.7\n46.0\n41.1\n40.8\n64.6\n59.0\n43.0\n61.7\n58.3\nBaseline\n40.7\n42.6\n47.4\n38.9\n45.6\n67.7\n61.7\n45.9\n64.5\n63.4\nSemantic\nRw both instr.\n40.4\n41.5\n44.5\n39.3\n40.5\n49.8\n50.5\n42.6\n48.2\n52.7\nRw labels\n3.7\n1.8\n1.4\n1.4\n3.4\n46.5\n3.8\n2.7\n1.2\n7.9\nOOD inputs\n41.7\n40.7\n43.9\n38.1\n44.6\n67.6\n57.1\n40.7\n50.5\n57.4\nRepeated Text\nInline instr. in 3 demos.\n43.2\n43.2\n48.2\n39.3\n45.8\n65.8\n61.1\n43.1\n64.6\n63.5\nInline instr. in 2 demos.\n40.9\n43.1\n44.5\n41.6\n43.7\n63.9\n59.3\n43.5\n62.7\n62.6\nInline in instr. 1 demos.\n40.6\n43.1\n42.7\n39.8\n45.7\n63.2\n58.8\n41.9\n62.0\n61.3\nInline in instr. 0 demos.\n13.3\n22.6\n17.9\n14.8\n21.4\n59.0\n30.5\n20.3\n35.1\n29.2\nRw inline instr. in 3 demos.\n35.8\n38.8\n43.1\n38.8\n35.6\n50.9\n51.9\n39.7\n48.2\n56.1\nRw inline instr. in 2 demos.\n35.9\n36.1\n41.1\n36.0\n35.4\n45.4\n46.6\n40.6\n49.2\n49.0\nRw inline instr. in 1 demos.\n33.0\n34.9\n36.7\n31.4\n22.4\n35.0\n38.7\n32.8\n37.7\n41.5\nRw inline instr. in 0 demos.\n0.6\n0.2\n1.3\n0.4\n0.2\n0.2\n1.1\n0.3\n0.7\n1.6\nTable 2: Model performance averaged across all datasets. Structural corruption is when components are added to the test instance. Repeated text corruptions are performed on baseline prompt which includes inline instruction in all four demonstrations. Random words text is represented by \u201cRw\u201d. The top two performances for each model are in bold, and baseline prompt performance is underlined.\nBigger models are more sensitive to the semantics of the prompt. We divide the models in the study into smaller (<15B) and bigger (\u226530B) models. In Figure 3, smaller models show the performance gain between 5-12% with both relevant and irrelevant instructions, whereas bigger models gain more with meaningful instructions (7-18%) and just 1-4% with random word instructions. When we perturb demonstration inputs with OOD sentences (see Figure 10), smaller models\u2019 accuracy drops by 1-4%. In bigger models, this performance decrease is larger (1-6%), with Llama-2-70b showing a huge drop of 18%, which suggests that bigger models are more sensitive to prompt semantics.\nBigger models pay more attention to relevant components. In Figure 6, we plot the average attention per component for GPT-J-6B and OPT-30B baseline prompts. In line with earlier observations about vertical attention patterns (Kovaleva et al., 2019), we find that the models allocate the highest attention weight to separators, after which the most attended to component is labels. Inline instructions are next, followed by demonstration inputs and task instructions. Compared to smaller models, larger models seem to allocate more attention to relevant components and less to separators when generating the target label. For example, OPT-30B allocates 5.4% less attention to separators compared\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8882/8882376f-dfed-4ba2-8a7c-a9f3c354b13a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Repeated text boosts performance. Inline instruction in four demos is the baseline prompt. Inline instruction which occurs after the test instance is kept as is.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6a5a/6a5a6b7b-b8e7-4070-9f2a-8734a7d8bbf8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Repeated text boosts performance even when the text is irrelevant; \u201crw\u201d refers to random word The prompts include all components but the instructions are replaced with random words.</div>\nto GPT-J-6B, and instead increases attention to inline instructions by 3.5% and to demonstration inputs by 2.3%. To compare how models behave when text is corrupted semantically, we plotted attention for prompts with meaningful versus irrelevant instructions (see Figures 12 and 13). We see that GPT-J-6B shifts its attention from separators and labels to the demonstration input and the random inline instructions. OPT-30B does the opposite: it reduces its attention to random words\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a150/a15014fb-f66c-482d-9c4b-39f8cf6d4dc6.png\" style=\"width: 50%;\"></div>\nFigure 6: Average attention per component for GPT-J-6B and OPT-30B baseline prompts. \u2018D\u2019 stands for Demonstration and \u201csep\u201d for the new line separator.\ntext and shifts it to the separator.\nResults are similar in classification and generation tasks, with few exceptions. As can be seen in Table 4, repeating inline instructions has a big impact on both classification and generation tasks regardless of model size. The first repetition has the most pronounced effect, and this is true even when the inline instructions are random. To see this effect, compare the rows \u201cInline instr. in 0 demos\u201d\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7572/7572b653-6f17-43d2-9e9e-b1b9b0c96d0d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">GPT2-xl GPT-J-6B OPT-30B OPT-66B</div>\nFigure 7: Label from label space is important. Complete removal of labels drops the performance to almost zero.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2c26/2c26ea23-c7c1-4e1b-9ce9-1ad285b2de05.png\" style=\"width: 50%;\"></div>\nFigure 8: Semantics of the demonstration input is not important. Complete removal of labels drops the performance.\nand \u201cInline instr. in 1 demos\u201d, as well as the rows \u201cRw Inline instr. in 0 demos\u201d and \u201cRw Inline instr. in 1 demos\u201d in Table 4. We also observe that adding relevant or random word instructions to demonstrations improves GPT2-xl performance by an average of 12.5% for classification and 3.8% for generation tasks. For the larger model LLama-2-70B, relevant instructions lead to gains of 18.5% for classification and 3.3% for generation tasks. Adding random word instructions yields LLama-2-70B, a marginal improvement of 0.1% in classification tasks and a drop of 1.7% for generation tasks. To see this effect, compare the rows \u201c+demos.\u201d, \u201cBaseline\u201d and \u201cRw both instr.\u201d in Table 4. The performance drop from meaningful to random word instructions is more pronounced (20.2% drop) in LLama-2-70B in classification tasks (compare rows \u201cBaseline\u201d and \u201cRw both instr.\u201d in Table 4) suggesting that larger models pay more attention to the meaning of the instruction. In generation tasks, both model sizes exhibit a comparable drop in performance. Results are consistent across most of the datasets. Tables 5 and 6 show results for each dataset individually for both GPT2-xl and LLama2-70B. The first repetition of relevant or irrelevant inline instructions in the prompt significantly boosts performance across all datasets for both model sizes. Adding relevant instructions proves beneficial for both GPT2-xl and LLama-2-70B on all 10\nand \u201cInline instr. in 1 demos\u201d, as well as the rows \u201cRw Inline instr. in 0 demos\u201d and \u201cRw Inline instr. in 1 demos\u201d in Table 4. We also observe that adding relevant or random word instructions to demonstrations improves GPT2-xl performance by an average of 12.5% for classification and 3.8% for generation tasks. For the larger model LLama-2-70B, relevant instructions lead to gains of 18.5% for classification and 3.3% for generation tasks. Adding random word instructions yields LLama-2-70B, a marginal improvement of 0.1% in classification tasks and a drop of 1.7% for generation tasks. To see this effect, compare the rows \u201c+demos.\u201d, \u201cBaseline\u201d and \u201cRw both instr.\u201d in Table 4. The performance drop from meaningful to random word instructions is more pronounced (20.2% drop) in LLama-2-70B in classification tasks (compare rows \u201cBaseline\u201d and \u201cRw both instr.\u201d in Table 4) suggesting that larger models pay more attention to the meaning of the instruction. In generation tasks, both model sizes exhibit a comparable drop in performance.\ndatasets. At the same time, adding random word instruction benefits GPT2-xl on 8 out of 10 datasets, but LLama-2-70B only on 6 out of 10 datasets. Corrupting labels with random words impacts GPT-2-xl on 8 out of 10 datasets and LLama-2-70B on all datasets.\n# 5. Conclusion\nThis study investigated the importance of different components of a prompt for large language models. We systematically corrupted prompts in different ways across 10 models ranging from 1.5 billion to 70 billion parameters and evaluated their performance on 10 diverse datasets. We also examined how much attention the models allocate to different prompt components. One interesting finding was that adding any inline instructions to the prompt, even just random words, actually helps models perform better. We also showed that repeated text improves model performance drastically and that larger models are substantially more sensitive to prompt semantics. We hope our study will pave the way for more refined and effective prompting strategies in future applications.\n# 6. Limitations\nOur study was focused on exploring various types of corruption across a diverse range of datasets and model sizes. It involved a large number of experiments and certain prompt elements were held constant, such as demonstrations and instructions. Altering these components might introduce variations in the results, and this aspect should be taken into consideration for further research. Additionally, we limited the attention analysis to datasets with shorter prompts due to the computational intensity and cost associated with computing attention norms. An additional limitation arises from the use of the same prompt template across all model types. This uniformity may lead to some performance discrepancies in instruction-tuned models.\n# 7. Ethic Statements\nOur goal with this study is to enrich the understanding of prompting and contribute to the responsible utilization of large language models. We believe that attention analysis can offer meaningful insights to the research community, facilitating the development of more robust language models. It\u2019s noteworthy that all the models and datasets employed in our research are open source, and we meticulously reported all experiment details in the paper to support the transparency and accessibility of the research.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6c4a/6c4ac898-1ea3-4960-9e2c-488ae3e54992.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Using labels from the correct label space is crucial for model performance.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4bf4/4bf4d921-2d25-4585-bad2-fe14830b9c81.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Semantics of the demonstration input is not important</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f733/f73307d7-e9e9-4bf1-8309-cbbce789a745.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Average OPT-30B attention per component for repeated text corruptions. \u201cInline\u201d refers to the presence of the number of inline instructions in the baseline prompt. A solid black box represents omitted components.</div>\n<div style=\"text-align: center;\">Figure 11: Average OPT-30B attention per component for repeated text corruptions. \u201cInline\u201d refers to the presence of the number of inline instructions in the baseline prompt. A solid black box represents omitted components.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4485/44855921-a6fc-4507-bde9-8339aed83e36.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Average attention per component for OPT-30B: Baseline prompt versus prompt when bot task and inline instructions are replaced by random words.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7640/764096ce-0288-449f-8dfc-3a3fb06845eb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Average attention per component for GPT-J-6B. Baseline prompt versus prompt when both task and inline instructions are replaced by random words.</div>\n# 8. Acknowledgement\nThis work was funded in part by an Amazon Alexa AI research award to Anna Rumshisky. We would like to express our gratitude to Anton Kovalev for helping with the tables in the paper.\n# 9. References\nAnthropic. 2023. Model card and evaluations for claude models.\nStephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault F\u00e9vry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-Shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. Promptsource: An integrated development environment and repository for natural language prompts. ArXiv, abs/2202.01279. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Stella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt4 with 90%* chatgpt quality.\nStephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault F\u00e9vry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-Shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. Promptsource: An integrated development environment and repository for natural language prompts. ArXiv, abs/2202.01279. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Stella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt4 with 90%* chatgpt quality. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\n# Google AI. 2023. Bard Google AI.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332.\nSoricut. 2019. Albert: A lite bert for selfsupervised learning of language representations. arXiv preprint arXiv:1909.11942. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. Vladislav Lialin, Kevin Zhao, Namrata Shivagunde, and Anna Rumshisky. 2022. Life after bert: What do other muppets understand about language? arXiv preprint arXiv:2205.10696. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786. Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. ArXiv, abs/2209.07686. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2021a. Reframing instructional prompts to gptk\u2019s language. arXiv preprint arXiv:2109.07830.\nand Hannaneh Hajishirzi. 2021b. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020a. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020b. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367. Karthik Raman, Iftekhar Naim, Jiecao Chen, Kazuma Hashimoto, Kiran Yalasangi, and Krishna Srinivasan. 2022. Transforming sequence tagging into a seq2seq task. arXiv preprint arXiv:2203.08378. Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u20137. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics, 8:842\u2013866. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike TianJian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization.\neven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili\u2019c, Daniel Hesslow, Roman Castagn\u2019e, Alexandra Sasha Luccioni, Franccois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo G. Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u2019ia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L\u2019opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Alshaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault F\u00e9vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall\u2019ee, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u2019elie N\u2019ev\u2019eol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L\u00edvia Macedo Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K. Ghauri, Mykola Burynok, Nafis\nAbrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Modupe Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u2019on Perin\u2019an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully A. Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Mar\u00eda Andrea Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chandrasekhar, R. Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th\u2019eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100.\nTimo Schick and Hinrich Sch\u00fctze. 2020. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676.\nTimo Schick and Hinrich Sch\u00fctze. 2022. True fewshot learning with prompts\u2014a real-world perspective. Transactions of the Association for Computational Linguistics, 10:716\u2013731.\nunderstanding. arXiv preprint arXiv:2111.06719. Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743\u2013758. Zhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang Liu. 2021. Msp: Multi-stage prompting for making pre-trained language models better translators. arXiv preprint arXiv:2110.06609. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2022. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Ben Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6 billion parameter autoregressive language model.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022a. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Annual Meeting of the Association for Computational Linguistics. Han Wang, Canwen Xu, and Julian McAuley. 2022b. Automatic multi-label prompting: Simple and interpretable few-shot classification. arXiv preprint arXiv:2204.06305. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022c. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. URL https://arxiv. org/abs/2204.07705. Albert Webson and Ellie Pavlick. 2021. Do promptbased models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021a. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021b. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, et al. 2023a. Symbol tuning improves in-context learning in language models. arXiv preprint arXiv:2305.08298. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023b. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382.\nXiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, and Jian-guang Lou. 2023. Re-reading improves reasoning in language models. arXiv preprint arXiv:2309.06275. Sen Yang, Yunchen Zhang, Leyang Cui, and Yue Zhang. 2022. Do prompts solve nlp tasks using natural language? arXiv preprint arXiv:2203.00902. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Yue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. 2022b. Promptgen: Automatically generate prompts using generative models. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 30\u201337. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022c. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2022a. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049. Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022b. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.\n# A. Components of Prompts for all datasets\nWe show baseline prompts for all datasets. We use 4-shot setting and each prompt consists of four components: Task instruction, inline instruction, demonstration input and label. [Test instance] will vary. Each dataset consists of 100 samples and is balanced. Data statistics for each of the datasets is shown in 3.\n# Medical Question Pair (Classification task)\nIn this task you are given a medical question pair. Your task is to classify this question pair into two categories 1) \u2019Similar\u2019 if the given two questions have the same connotation or meaning 2) \u2019Dissimilar\u2019 if the given two questions have a different connotation or meaning.\nTwitter Emotion Classification (Classification task)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9713/9713b147-e567-4211-af00-cc9ceca3b7bd.png\" style=\"width: 50%;\"></div>\n# CoLA (Classification task)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5460/546054e1-2ada-476e-8ceb-18ef1db22452.png\" style=\"width: 50%;\"></div>\nDataset\nStatistics\nRTE\nYes(50), No(50)\nMedical Question Pair\nSimilar(50), Similar(50)\nFinancial Phrasebank\nNeural(33), Negative(33), Positive(34)\nTwitter Emotion classification\nSadness(17), Joy(17), Love(17), Anger(17), Fear(16), Surprise(16).\nCoLA\nYes(50), No(50)\nAgNews\nWorld(25), Sports(25), Business(25), Sci/Tech(25)\nCOPA\nCause(50), Effect(50)\nCom2sense\nYes(50), No(50)\nTriviaQA\n-\nMathdataset\n-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed5f/ed5f2ec5-c39e-4069-b17e-da89d1db53a8.png\" style=\"width: 50%;\"></div>\n# RTE (Classification Task)\nSentence 1: Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That\u2019s about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found. Sentence 2: Three quarters of U.S. illegal immigrants have children. Does Sentence 1 entail Sentence 2? No.\n[Test instance. Sentence 2?\nDoes Sentence 1 entail\nClassify the given a piece of financial news into three classes: positive, negative, and neutral. Output must be \u2019Positive\u2019, \u2019Negative\u2019, or \u2019Neutral\u2019. According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing. Is the sentiment of the sentence \u2019Negative\u2019, \u2019Neutral\u2019, or \u2019Positive\u2019? Neutral. Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said. Is the sentiment of the sentence \u2019Negative\u2019, \u2019Neutral\u2019, or \u2019Positive\u2019? Neutral. The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported. Is the sentiment of the sentence \u2019Negative\u2019, \u2019Neutral\u2019, or \u2019Positive\u2019? Negative. With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability. Is the sentiment of the sentence \u2019Negative\u2019, \u2019Neutral\u2019, or \u2019Positive\u2019? Positive. [Test instance.] Is the sentiment of the sentence \u2019Negative\u2019, \u2019Neutral\u2019, or \u2019Positive\u2019?\nClassify the given a piece of financial news into three classes: positive, negative, and neutral. Output must be \u2019Positive\u2019, \u2019Negative\u2019, or \u2019Neutral\u2019.\nMathdataset Answer Generation (Generation task)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e92/1e929acf-f853-47b9-acfd-8f382d8c0ddc.png\" style=\"width: 50%;\"></div>\nIn this task, you are given a news article. Your task is to classify the article to one out of the four topics \u2019World\u2019, \u2019Sports\u2019, \u2019Business\u2019, \u2019Sci/Tech\u2019. If you are not sure about the topic, choose the closest option. Note that URLs in the text have been replaced with [Link].\nComets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest. What label best describes this news article? Sci/Tech.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31fa/31fa1170-5176-4f31-afa9-0333a4c86635.png\" style=\"width: 50%;\"></div>\nIn this task your given two statements. You must judge whether the second sentence is the cause or effect of the first sentence. The two sentences are separated by a newline character and the answer can be \u2019Cause\u2019 or \u2019Effect\u2019.\n# TriviaQA (Generation task)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f463/f463f0a6-6d9d-49af-9aec-063058a740eb.png\" style=\"width: 50%;\"></div>\n# B. More results\nTable 4 shows results for classification and generation tasks whereas Tables 5 and 6 show results for individual tasks for GPT-2-xl (smaller model) and Llama-2-70B (bigger model) respectively.\n# C. Attention plots\nFigure 14 shows repeated text corruptions for GPTJ-6B. Figure 15 shows repeated text corruptions for OPT-30B with random word instructions.\nCorruptions\nGPT2-xl\nGPT2-xl\nLLama-2-70B\nLLama-2-70B\nClassification\nGeneration\nClassification\nGeneration\nSemantic Corruptions\n+ demos.\n28.1\n9.8\n53.8\n30.7\n+ task instr. + demos.\n30.4\n11.2\n61.6\n33.0\n+ inline instr. + demos.\n38.3\n9.3\n69.0\n32.5\nBaseline\n40.7\n15.5\n72.3\n33.4\nSemantic Corruption\nRw both instr.\n40.4\n11.6\n52.1\n30.8\nRw labels\n3.7\n6.3\n1.1\n1.7\nOOD inputs\n41.7\n13.3\n57.6\n22.1\nRepeated text\nInline instr. in 3 demos\n45.3\n15.1\n72.4\n33.5\nInline instr. in 2 demos\n41.4\n14.7\n70.4\n31.9\nInline instr. in 1 demos\n41.4\n15.3\n69.6\n31.7\nInline instr. in 0 demos\n17.6\n9.1\n38.1\n22.9\nRw Inline instr. in 3 demos\n41.1\n10.2\n53.5\n27.0\nRw Inline instr. in 2 demos\n41.6\n9.1\n54.0\n29.8\nRw Inline instr. in 1 demos\n40.2\n3.0\n41.5\n22.5\nRw Inline instr. in 0 demos\n0.9\n0.0\n0.9\n0.2\nable 4: Model performance for classification and generation tasks. The highest performance is in bold, baseline rompt performance is underlined.\nTable 4: Model performance for classification and generation tasks. The highest performance is in bold, baselin prompt performance is underlined.\nCorruption\nRTE\nMQP\nFPH\nTE\nCoLA\nAGN\nCOPA\nC2S\nTQ\nMATH\nStructural\n+ demos.\n46.0\n46.0\n19.0\n6.0\n49.0\n33.0\n20.0\n42.0\n18.6\n0.9\n+ task instr.\n45.0\n47.0\n25.0\n19.0\n48.0\n39.0\n22.0\n37.0\n20.2\n2.1\n+ inline instr.\n50.0\n50.0\n35.0\n26.0\n50.0\n61.0\n54.0\n50.0\n15.1\n3.6\nBaseline\n53.0\n60.0\n34.0\n23.0\n51.0\n58.0\n50.0\n47.0\n17.0\n14.0\nSemantics\nRw both instr.\n53.0\n50.0\n58.0\n25.0\n44.0\n54.0\n50.0\n47.0\n15.6\n7.7\nRw labels\n0.0\n0.0\n0.0\n23.0\n0.0\n1.0\n0.0\n0.0\n11.9\n0.7\nOOD inputs\n51.0\n52.0\n46.0\n28.0\n50.0\n63.0\n52.0\n48.0\n12.7\n13.9\nRepeated text\nInline instr. in 3 demos\n52.0\n63.0\n50.0\n22.0\n50.0\n41.0\n50.0\n51.0\n18.0\n12.2\nInline instr. in 2 demos\n54.0\n70.0\n35.0\n21.0\n50.0\n32.0\n50.0\n50.0\n14.9\n14.6\nInline instr. in 1 demos\n50.0\n50.0\n33.0\n31.0\n50.0\n38.0\n50.0\n50.0\n15.1\n15.5\nInline instr. in 0 demos\n1.0\n43.0\n0.0\n1.0\n47.0\n16.0\n0.0\n0.0\n9.1\n9.3\nRw Inline instr. in 3 demos\n50.0\n50.0\n40.0\n20.0\n42.0\n37.0\n50.0\n49.0\n15.4\n5.0\nRw Inline instr. in 2 demos\n50.0\n50.0\n33.0\n20.0\n50.0\n35.0\n53.0\n50.0\n11.5\n6.8\nRw Inline instr. in 1 demos\n50.0\n50.0\n33.0\n16.0\n50.0\n25.0\n50.0\n50.0\n0.0\n6.0\nRw Inline instr. in 0 demos\n0.0\n0.0\n0.0\n0.0\n4.0\n0.0\n0.0\n2.0\n0.0\n0.0\nTable 5: Model performance for each dataset for GPT2-xl. Datasets are RTE, Medical Question Pair (MQP Financial Phrasebank (FPH), Twitter Emotion classification(TE), CoLA, AgNews (AGN), COPA, Com2sense (C2S and two generation tasks: TriviaQA (TQ) and Mathdataset answer generation(MATH)\nCorruption\nRTE\nMQP\nFPH\nTE\nCoLA\nAGN\nCOPA\nC2S\nTQ\nMATH\nStructural\n+ demos.\n61.0\n60.0\n60.0\n22.0\n49.0\n58.0\n67.0\n53.0\n41.6\n19.8\n+ task instr.\n67.0\n66.0\n69.0\n22.0\n72.0\n61.0\n64.0\n72.0\n41.8\n24.2\n+ inline instr.\n70.0\n67.0\n78.0\n42.0\n72.0\n83.0\n75.0\n65.0\n42.4\n22.2\nBaseline\n84.0\n77.0\n80.0\n34.0\n81.0\n86.0\n64.0\n72.0\n42.5\n24.3\nSemantics\nRw both instr.\n66.0\n53.0\n34.0\n27.0\n54.0\n74.0\n56.0\n56.0\n42.9\n18.7\nRw labels\n3.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n5.0\n1.7\n1.7\nOOD inputs\n70.0\n61.0\n71.0\n20.0\n77.0\n35.0\n54.0\n73.0\n35.0\n9.2\nRepeated text\nInline instr. in 3 demos\n76.0\n82.0\n84.0\n44.0\n74.0\n85.0\n63.0\n71.0\n45.8\n21.2\nInline instr. in 2 demos\n72.0\n80.0\n78.0\n41.0\n72.0\n86.0\n65.0\n69.0\n40.4\n23.3\nInline instr. in 1 demos\n70.0\n81.0\n82.0\n37.0\n76.0\n84.0\n60.0\n67.0\n43.2\n20.3\nInline instr. in 0 demos\n43.0\n52.0\n34.0\n24.0\n71.0\n1.0\n25.0\n55.0\n25.3\n20.5\nRw Inline instr. in 3 demos\n63.0\n55.0\n40.0\n35.0\n49.0\n79.0\n58.0\n49.0\n39.5\n14.5\nRw Inline instr. in 2 demos\n58.0\n70.0\n33.0\n34.0\n50.0\n77.0\n59.0\n51.0\n39.9\n19.7\nRw Inline instr. in 1 demos\n50.0\n58.0\n34.0\n18.0\n48.0\n25.0\n50.0\n49.0\n39.1\n6.0\nRw Inline instr. in 0 demos\n5.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.4\n0.0\nTable 6: Model performance for each dataset for LLama-2-70B. Datasets are RTE, Medical Question Pair (MQP Financial Phrasebank (FPH), Twitter Emotion classification(TE), CoLA, AgNews (AGN), COPA, Com2sense (C2S) and two generation tasks: TriviaQA (TQ) and Mathdataset answer generation(MATH)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a6d/2a6d7237-5f99-482d-8402-c4ccfad76fae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Average GPT-J-6B attention per component for repeated text corruptions. \u201cInline\u201d refers to the presence of the number of inline instructions in the baseline prompt. Fully black box represents missing components.</div>\n<div style=\"text-align: center;\">Figure 14: Average GPT-J-6B attention per component for repeated text corruptions. \u201cInline\u201d refers to the presence of the number of inline instructions in the baseline prompt. Fully black box represents missing components.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f6bc/f6bcb545-5b98-4cc3-af98-9184279062e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: Random words instructions: Average OPT-30B attention per component for repeated text corruptions. \u201cInline\u201d refers to the presence of the number of inline instructions in the baseline prompt. A solid black box represents omitted components.</div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The ability of language models to learn from prompts has led to their widespread use, particularly in AI assistants like ChatGPT. However, the underlying backbone models are sensitive to prompt variations, which poses a challenge in building high-quality models. Prior studies have focused on specific prompt attributes but often produced inconsistent results due to limited model sizes and types.",
            "purpose of benchmark": "The benchmark is intended to evaluate the effects of different prompt components on the performance of large language models, aiding in the understanding and improvement of in-context learning."
        },
        "problem": {
            "definition": "The benchmark addresses the sensitivity of language models to variations in prompt structure and semantics, focusing on how these variations affect model performance across different tasks.",
            "key obstacle": "Existing benchmarks have limitations in consistency and comprehensiveness, often focusing on smaller models or specific prompt attributes without a holistic view of prompt impacts."
        },
        "idea": {
            "intuition": "The development of this benchmark was inspired by the need to explore the nuanced effects of prompt components on model performance, as prior work yielded contradictory findings.",
            "opinion": "The authors believe that understanding prompt impacts is crucial for developing more robust language models and effective prompting strategies.",
            "innovation": "This benchmark innovatively decomposes prompts into distinct components and systematically evaluates their effects across a diverse set of tasks and model sizes, offering a more comprehensive analysis than previous benchmarks.",
            "benchmark abbreviation": "ICL-Benchmark"
        },
        "dataset": {
            "source": "The dataset was sourced from ten existing datasets covering various classification and generation tasks, ensuring a diverse evaluation landscape.",
            "desc": "The dataset includes ten tasks with a balanced sample of 100 instances each, facilitating a robust evaluation of model performance across different prompts.",
            "content": "The dataset comprises classification tasks (e.g., RTE, Twitter Emotion Classification) and generation tasks (e.g., TriviaQA, Mathdataset answer generation).",
            "size": "1,000",
            "domain": "Text Classification",
            "task format": "Classification"
        },
        "metrics": {
            "metric name": "Exact Match, Rouge-L",
            "aspect": "Accuracy",
            "principle": "The metrics were chosen to provide a clear evaluation of model performance in both classification and generation contexts, reflecting practical outcomes in real-world applications.",
            "procedure": "Model performance was evaluated by comparing predicted outputs against ground truth labels using the specified metrics."
        },
        "experiments": {
            "model": "The benchmark tested ten models, including GPT2-xl, GPT-J-6B, and Llama-2-70B, covering a range of sizes from 1.5B to 70B parameters.",
            "procedure": "Models were trained and evaluated using different prompt configurations, including baseline prompts and various corruptions to assess the impact on performance.",
            "result": "Results indicated that adding task and inline instructions significantly improved model performance, especially in larger models, highlighting their sensitivity to prompt semantics.",
            "variability": "Variability was accounted for by conducting multiple trials across different datasets and configurations, ensuring robust statistical significance in the results."
        },
        "conclusion": "The study concludes that understanding the impact of prompt components is vital for improving model performance, with findings suggesting that repeated text and meaningful instructions can significantly enhance outcomes.",
        "discussion": {
            "advantage": "The benchmark provides a systematic approach to understanding prompting strategies, contributing to the development of more effective language models.",
            "limitation": "Limitations include potential performance discrepancies due to the uniformity of prompt templates across different models and the computational costs associated with attention analysis.",
            "future work": "Future research should explore the effects of varying prompt components more dynamically and investigate new prompting strategies to further refine model performance."
        },
        "other info": {
            "ethics statement": "The study aims to contribute to responsible utilization of large language models, promoting transparency and accessibility in research."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The ability of language models to learn from prompts has led to their widespread use, particularly in AI assistants like ChatGPT."
        },
        {
            "section number": "1.4",
            "key information": "The benchmark innovatively decomposes prompts into distinct components and systematically evaluates their effects across a diverse set of tasks and model sizes."
        },
        {
            "section number": "2",
            "key information": "The benchmark addresses the sensitivity of language models to variations in prompt structure and semantics, focusing on how these variations affect model performance across different tasks."
        },
        {
            "section number": "3.1",
            "key information": "Results indicated that adding task and inline instructions significantly improved model performance, especially in larger models, highlighting their sensitivity to prompt semantics."
        },
        {
            "section number": "4.1",
            "key information": "Understanding prompt impacts is crucial for developing more robust language models and effective prompting strategies."
        },
        {
            "section number": "6.2",
            "key information": "Limitations include potential performance discrepancies due to the uniformity of prompt templates across different models and the computational costs associated with attention analysis."
        }
    ],
    "similarity_score": 0.7420638432678331,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Deconstructing In-Context Learning_ Understanding Prompts via Corruption.json"
}