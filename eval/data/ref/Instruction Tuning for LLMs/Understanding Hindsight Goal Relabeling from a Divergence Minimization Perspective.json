{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2209.13046",
    "title": "Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective",
    "abstract": "Hindsight goal relabeling has become a foundational technique in multi-goal reinforcement learning (RL). The essential idea is that any trajectory can be seen as a sub-optimal demonstration for reaching its final state. Intuitively, learning from those arbitrary demonstrations can be seen as a form of imitation learning (IL). However, the connection between hindsight goal relabeling and imitation learning is not well understood. In this paper, we propose a novel framework to understand hindsight goal relabeling from a divergence minimization perspective. Recasting the goal reaching problem in the IL framework not only allows us to derive several existing methods from first principles, but also provides us with the tools from IL to improve goal reaching algorithms. Experimentally, we find that under hindsight relabeling, Q-learning outperforms behaviour cloning (BC). Yet, a vanilla combination of both hurts performance. Concretely, we see that the BC loss only helps when selectively applied to actions that get the agent closer to the goal according to the Qfunction. Our framework also explains the puzzling phenomenon wherein a reward of {\u22121, 0} results in significantly better performance than a {0, 1} reward for goal reaching.",
    "bib_name": "zhang2023understandinghindsightgoalrelabeling",
    "md_text": "# Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective\nLunjun Zhang 1 2 3 Bradly C. Stadie 4\n# Abstract\nHindsight goal relabeling has become a foundational technique in multi-goal reinforcement learning (RL). The essential idea is that any trajectory can be seen as a sub-optimal demonstration for reaching its final state. Intuitively, learning from those arbitrary demonstrations can be seen as a form of imitation learning (IL). However, the connection between hindsight goal relabeling and imitation learning is not well understood. In this paper, we propose a novel framework to understand hindsight goal relabeling from a divergence minimization perspective. Recasting the goal reaching problem in the IL framework not only allows us to derive several existing methods from first principles, but also provides us with the tools from IL to improve goal reaching algorithms. Experimentally, we find that under hindsight relabeling, Q-learning outperforms behaviour cloning (BC). Yet, a vanilla combination of both hurts performance. Concretely, we see that the BC loss only helps when selectively applied to actions that get the agent closer to the goal according to the Qfunction. Our framework also explains the puzzling phenomenon wherein a reward of {\u22121, 0} results in significantly better performance than a {0, 1} reward for goal reaching.\narXiv:2209.13046v2\n# 1. Introduction\nGoal reaching is an essential aspect of intelligence in sequential decision making. Unlike the conventional formulation of reinforcement learning (RL), which aims to encode all desired behaviors into a single scalar reward function that is amenable to learning (Silver et al., 2021), goal reaching formulates the problem of RL as applying a sequence of actions to rearrange the environment into a desired state\n1Department of Computer Science, University of Toronto 2Vector Institute 3Waabi 4Department of Statistics, Northwestern University. Correspondence to: Lunjun Zhang <lunjun@cs.toronto.edu>.\n1Department of Computer Science, University of Toronto 2Vector Institute 3Waabi 4Department of Statistics, Northwestern University. Correspondence to: Lunjun Zhang <lunjun@cs.toronto.edu>.\n(Batra et al., 2020). Goal-reaching is a highly flexible formulation. For instance, we can design the goal-space to capture salient information about specific factors of variations (Plappert et al., 2018b); we can use natural language instructions to define more abstract goals (Lynch & Sermanet, 2020; Ahn et al., 2022); we can encourage exploration by prioritizing previously unseen goals (Pong et al., 2019; Warde-Farley et al., 2018; Pitis et al., 2020); and we can even use self-supervised procedures to naturally learn goal-reaching policies without reward engineering (Pong et al., 2018; Nair et al., 2018b; Zhang et al., 2021; OpenAI et al., 2021; Chebotar et al., 2021).\nmanet, 2020; Ahn et al., 2022); we can encourage exploration by prioritizing previously unseen goals (Pong et al., 2019; Warde-Farley et al., 2018; Pitis et al., 2020); and we can even use self-supervised procedures to naturally learn goal-reaching policies without reward engineering (Pong et al., 2018; Nair et al., 2018b; Zhang et al., 2021; OpenAI et al., 2021; Chebotar et al., 2021). Imitation learning (IL) aims to recover an expert policy from a set of expert demonstrations. The simplest imitation learning algorithm is Behaviour Cloning (BC), which directly uses the given demonstrations to supervise the policy actions conditioned on the states visited by the expert (Pomerleau, 1988). An alternative approach, inverse reinforcement learning (IRL), first learns a reward function from the demonstrations, and then runs online RL on the learned reward to extract the policy (Abbeel & Ng, 2004). A modern example of IRL is generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016), which learns a discriminator (Goodfellow et al., 2014) as the reward while jointly running policy gradient algorithms. It has been shown that the majority of imitation learning methods can be unified under the divergence minimization framework (Ghasemipour et al., 2020; Zhang et al., 2020; Ke et al., 2021). Hindsight goal relabeling is a technique proposed in (Andrychowicz et al., 2017) to improve the sample efficiency of goal reaching and has since been widely used (Ghosh et al., 2019; Lynch et al., 2019; Chebotar et al., 2021). A goal-conditioned policy typically sets a behavioral goal and then tries to reach it during sampling, but most attempts will likely fail. Nevertheless, any trajectory has successfully reached all the states it visits along the way. Therefore, the agent may pretend post hoc that whatever states it reaches are the intended goals of that trajectory, and learn from its own success. Intuitively, hindsight relabeling creates self-generated expert demonstrations, which the policy then imitates. Can we mathematically describe hindsight relabeling and goal reaching as an imitation learning\nHindsight goal relabeling is a technique proposed in (Andrychowicz et al., 2017) to improve the sample efficiency of goal reaching and has since been widely used (Ghosh et al., 2019; Lynch et al., 2019; Chebotar et al., 2021). A goal-conditioned policy typically sets a behavioral goal and then tries to reach it during sampling, but most attempts will likely fail. Nevertheless, any trajectory has successfully reached all the states it visits along the way. Therefore, the agent may pretend post hoc that whatever states it reaches are the intended goals of that trajectory, and learn from its own success. Intuitively, hindsight relabeling creates self-generated expert demonstrations, which the policy then imitates. Can we mathematically describe hindsight relabeling and goal reaching as an imitation learning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ecd1/ecd18dd2-ea1e-4263-b785-14588bb6e807.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Our proposed framework for goal reaching that elucidates how rewards should be designed in multi-goal RL.</div>\nprocess from a divergence minimization perspective?\nIn this paper, we propose a novel framework that describes hindsight goal relabeling in the language of divergence minimization, further bridging the two paradigms of goal reaching and imitation learning. Compared to prior attempts to formulate a theoretical framework for goal reaching (Tang & Kucukelbir, 2021; Eysenbach et al., 2020a; Blier et al., 2021; Eysenbach et al., 2020b; Rudner et al., 2021), our work pays substantially more attention to reward design in goal reaching and its connection to inverse RL, and having a probabilistically well-defined objective in the case of continuous state space. In addition, our framework can re-derive several existing methods from first principles, unlike prior frameworks that are incompatible with existing methods and thus unable to explain the success of hindsight-relabeling based goal reaching. For experiments, we find that despite a resurgence of interests in BC-based goal reaching (Ghosh et al., 2019; Ding et al., 2019; Lynch et al., 2019; Jang et al., 2021), multi-goal Q-learning can still outperform BC, and adding BC loss in multi-goal Q-learning hurts performance due to the sub-optimality of self-generated demonstrations. Indeed, BC only helps when selectively applied to actions that get the agent closer to the goal according to the Q-function. Finally, our framework also provides an interesting explanation for why the rewards of \u22121 and 0 experimentally outperforms the rewards of 0 and 1 in multi-goal Q-learning.\n# 2. Goal Reaching and Imitation Learning\n# 2.1. Goal-Conditioned Reinforcement Learning (GCRL)\nWe first review the basics of RL. A Markov Decision Process (MDP) is typically parameterized by (S, A, \u03c10, p, r): a state space S, an action space A, an initial state distribution \u03c10(s), a dynamics function p(s\u2032 | s, a) which defines the transition\nprobability, and a reward function r(s, a). A policy function \u00b5(a | s) defines a probability distribution \u00b5 : S \u00d7 A \u2192R+. For an infinite-horizon MDP, given the policy \u00b5, and the state distribution at step t (starting from \u03c10 at t = 0), the state distribution at step t + 1 is given by:\n(1)\nThe state visitation distribution sums over all timesteps via a geometric distribution Geom(\u03b3):\n(2)\nHowever, the trajectory sampling process does not happen in this discounted manner, so the discount factor \u03b3 \u2208(0, 1) is often absorbed into the cumulative return instead (Silver et al., 2014):\nFrom 1 and 2, we also see that the future state distribution p+(s+ | s, a) of policy \u00b5, defined as a geometric sum of state distribution at all future timesteps given current state and action, is given by the following recursive relationship (Eysenbach et al., 2020b; Janner et al., 2020):\n(4)\nIn multi-goal RL, an MDP is augmented with a goal space G, and we learn a goal-conditioned policy \u03c0 : S\u00d7G\u00d7A \u2192R+.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/49ff/49ff7961-e596-4c0f-933c-0caf4ae009e5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0102/0102fb29-00df-4dca-89e4-6580c02faf13.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: \u03c1\u00b5(s, a)p+ \u00b5 (s+ | s, a) hindsight-relabeled distribution</div>\n<div style=\"text-align: center;\">Figure 3: \u03c1+(g)\u03c1\u00b5(s)\u03c0(a | s, g) (used in HBC / GCSL)</div>\nHindsight Experience Replay (HER) (Andrychowicz et al., 2017) gives the agent a reward of 0 when the goal is reached and \u22121 otherwise, typically determined via an epsilon ball,\n(5)\nand uses hindsight goal relabeling to increase learning efficiency of goal-conditioned Q-learning by replacing the initial behavioral goals with achieved goals (future states within the same trajectory).\n# 2.2. Imitation Learning (IL) as Divergence Minimization\nWe review the concept of f-divergence between two probability distributions P and Q and its variational bound:\n(6)\nwhere f is a convex function such that f(1) = 0, f \u2217is the convex conjugate of f, and T is an arbitrary class of functions T : X \u2192R. This variational bound was originally derived in (Nguyen et al., 2010) and was popularized by GAN (Goodfellow et al., 2014; Nowozin et al., 2016) and subsequently by imitation learning (Ho & Ermon, 2016; Fu et al., 2017; Finn et al., 2016; Ghasemipour et al., 2020). The equality holds true under mild conditions (Nguyen et al., 2010), and the optimal T is given by T \u2217(x) = f \u2032(p(x)/q(x)). The canonical formulation of imitation learning follows (Ho & Ermon, 2016; Ghasemipour et al., 2020), where \u03c1exp(s, a) is from the expert:\n(7)\nBecause of the policy gradient theorem (Sutton et al., 1999), the policy \u00b5 needs to optimize the cumulative return under its own trajectory distribution \u03c1\u00b5(s, a) with the reward being r(s, a) = f \u2217(T(s, a)). Under this formulation, JensenShannon divergence leads to GAIL (Ho & Ermon, 2016),\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/147d/147d02ef-da6a-40fa-8d79-992651610717.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: \u03c1+(g)\u03c1\u03c0(s, a | g) (used in HER / HDM)</div>\nreverse KL leads to AIRL (Fu et al., 2017). Note that f can in principle be any convex function (we can satisfy f(1) = 0 by simply adding a constant).\n# 3. Graphical Models for Hindsight Goal Relabeling\nConsider an environment \u03be = (\u03c10(s), p(s\u2032 | s, a), p(g)). From \u03be, we generate a dataset of trajectories D = {(s0, a0, s1, a1, \u00b7 \u00b7 \u00b7 )} by first sampling from the initial state distribution \u03c10(s), and then executing an unobserved actor policy \u00b5(a | s). After executing this action, transitions occur according to a dynamics model p(s\u2032 | s, a). We aim to train a goal-conditioned policy \u03c0(a | s, g) from this arbitrary dataset with relabeled future states as goals. p(g) is the behavioral goal distribution assumed to be given apriori by the environment. To recast the problem of goal-reaching as imitation learning (7), we need to set up an f-divergence minimization where we define the target (expert) distribution and the policy distribution we want to match.\nIn multi-goal RL, the training signal comes from factorizing the joint distribution of state-action-goal differently. For the relabeled target distribution, we assume an unconditioned actor \u00b5 generating a state-action distribution at first, with the goals coming from the future state distribution (4) conditioned on the given state and action. For the goalconditioned policy distribution, behavior goals are given apriori, and the state-action distribution is generated conditioned on the behavioral goals. Thus, the target distribution (see Figure 2) for states, actions, and hindsight goals is:\n(8)\nConcretely, the target distribution is given by the unconditioned state-action visitation distribution multiplied by the conditional likelihood of future states. Note that p+ \u00b5 is given by equation (4), and \u03c1\u00b5(s, a) is similar to \u03c1exp(s, a) in equation (7). In the fashion of behavioral cloning (BC), if we do not care about matching the states, we can write the joint distribution we are trying to match as (see Figure 3):\n(9)\nWe can recover the objective of Hindsight Behavior Cloning (HBC) (Ding et al., 2019; Eysenbach et al., 2020a) and GoalConditioned Supervised Learning (GCSL) (Ghosh et al., 2019) via minimizing a KL-divergence (see A.1):\n(10)\nIn many cases, matching the states is more important than matching state-conditioned actions (Ross et al., 2011; Ghasemipour et al., 2020). The joint distribution for states, actions, behavioral goals for \u03c0 (see Figure 4) is:\n(11)\n(12)\nAfter adding this state-matching term, we are still missing an important component of the objective. Divergence minimization encourages the agent to stay in the \u201dright\u201d state-action distribution given the benefit of hindsight. But we still want the policy to actually hit the goal as quickly as possible without detours (see Figure 1). We propose that the goal-conditioned policy \u03c0 should therefore minimize:\n(13)\n\ufffd  | Which estimates the expected number of steps for the policy to reach a certain goal. The overall objective for goalconditioned RL can seen as a combination of LIL and LRL. We will expand the two objectives in more details and discuss how to use Q-learning to optimize them.\n# 4. Bridging Goal Reaching and Imitation\n# 4. Bridging Goal Reaching and Imitation Learning\nIn this section, we study how to optimize the proposed goal reaching objectives (12) and (13), with the goal of deriving and understanding the Q-learning process and reward design in multi-goal RL.\n# 4.1. Divergence Minimization with Goal-Conditioned Q-learning\nThis section decomposes (12). We start with the fdivergence bound from equations (6) and (7).\nDf(p\u03c0(s, a, g) \u2225p\u00b5(s, a, s+)) = max T E p(g) \u03c1\u03c0(s,a|g) [T(s, a, g)] \u2212E \u03c1\u00b5(s,a) p+ \u00b5 (s+|s,a) [f \u2217(T(s, a, s+))]\nNow we negate T to get r(s, a, g) = \u2212T(s, a, g), and the divergence minimization problem becomes:\n \u2212 inimization problem becomes: \u03c1\u00b5(s,a) +  (s+|s,a) [f \u2217(\u2212r(s, a, s+))] + E p(g) \u03c1\u03c0(s,a|g) [r(s, a, g)] The challenge, however, is that defining this value rigorously for continuous state space is a difficult task. With continuous state space, the delta function 1[s\u2032 = g] is always zero, and\nmax \u03c0 min r E \u03c1\u00b5(s,a) p+ \u00b5 (s+|s,a) [f \u2217(\u2212r(s, a, s+))] + E p(g) \u03c1\u03c0(s,a|g) [r(s, a, g)] for continuous state space is a difficult task. With continuous state space, the delta function 1[s\u2032 = g] is always zero, and\nWe can interpret r as a GAIL-style (Ho & Ermon, 2016) discriminator or reward. However, we aim to derive a discriminator-free learning process that directly trains the Q-function corresponding to this reward Q(s, a, g) = r(s, a, g) + \u03b3 \u00b7 P\u03c0Q(s, a, g) where P\u03c0 is the transition operator: P\u03c0Q(s, a, g) = Ep(s\u2032|s,a)\u03c0(a\u2032|s\u2032,g)[Q(s\u2032, a\u2032, g)]. Re-writing the previous equation w.r.t Q:\n(14)\nA similar change-of-variable has been explored in the context of offline RL (Nachum et al., 2019a;b) and imitation learning (Kostrikov et al., 2019; Zhu et al., 2020), known as the DICE (Nachum & Dai, 2020) family. A major pain point of DICE-like methods is that they require samples from the initial state distribution \u03c10(s) (Garg et al., 2021). The following lemma shows that in the goal-conditioned case, we can use arbitrary offline trajectories to evaluate the expected rewards under goal-conditioned online rollouts: Lemma 4.1 (Online-to-offline transformation for goal reaching). Given a goal-conditioned policy \u03c0(a | s, g), its corresponding Q-function Q\u03c0(s, a, g), and arbitrary state-action visitation distribution \u03c1\u00b5(s, a) of another policy \u00b5(a | s), the expected temporal difference for online rollouts under \u03c0 is:\n# Using Lemma 4.1, the imitation objective now becomes:\nmax \u03c0 min Q E\u03c1\u00b5(s,a)p+ \u00b5 (s+|s,a) \ufffd f \u2217(\u2212(Q \u2212\u03b3P\u03c0Q)(s, a, s+)) \ufffd\n(15)\nwhere f \u2217is the convex conjugate of f in f-divergence. We can pick almost any convex function as f \u2217as long as ((f \u2217)\u2217)(1) = 0. In summary, we have derived a way to minimize the imitation term LIL = Df(p\u00b5(s, a, s+) \u2225 p\u03c0(s, a, g)) directly using goal-conditioned Q-learning.\n# 4.2. Learning to Reach Goals with Fewer Steps\nA goal-conditioned agent should try to reach the desired goal using as few steps as possible. Consequently, we may want to learn the expected number of steps to reach another goal state from the current state, and then uses the policy \u03c0 to minimize the expected number of steps.\nthe dynamics function p(s\u2032 | s, a) has a value in R+ rather than [0, 1]; we can only estimate the likelihood p(s\u2032 = g | s, a) whose range is R+ and thus cannot be normalized to be either in {0, 1} or within [0, 1] per step for the purpose of step counting. To tackle this issue, we propose the following definition to measure the expected number of steps (negated) from one state to another:\n(16)\nwhich normalizes the likelihood of reaching the goal at different \u2206numbers of steps away as discrete probabilities over the step count, with an additional \u03b3 factor. Maximizing this Q function means minimizing (13). Lemma 4.2 (Recursive estimate of goal reaching step count). Under mild assumptions, the step count definition (16) has the following property:\nwhich normalizes the likelihood of reaching the goal at different \u2206numbers of steps away as discrete probabilities over the step count, with an additional \u03b3 factor. Maximizing this Q function means minimizing (13).\nLemma 4.2 (Recursive estimate of goal reaching step count). Under mild assumptions, the step count definition (16) has the following property:\nwith Qt = Q(st, a, g), Qt+1 = E\u03c0(st+1,g)[Q(st+1, a\u2032, g)], \u03b1(i) t = \ufffd \u2206=i \u03b3\u2206p\u03c0(st+1+\u2206= g | st, a), and \u03b2t = p(st+1 = g | st, a).\n  The first component [1 \u2212\u03b2t/(\u03b2t + \u03b1(1) t )] is classifying whether the goal can be reached in the immediate next step by comparing the two density ratios; the second component (\u22121 + Qt+1) shifts the distance estimate by another step in the case of the goal not being reached at t + 1. Rather than directly estimating the density ratio, we may adopt a sampling based approach: when sampling from \u03b2t, Qt should be 0, meaning that r = 0 (onward); when sampling from \u03b1(1) t , r = \u22121. Consequently, the definition in (16) reveals that the reward defined in (5) forms a well-defined objective even for continuous state space and as \u03f5 \u21920.\n# 4.3. To Reach Goals with Fewer Steps is to Imitate\nWe now discuss the relationship between the RL objective in (13) and (16) and the objective to imitate in (12) and (15). Lemma 4.3 (Understanding Hindsight Experience Replay). Multi-goal Q-learning with HER reward (5) with {\u22121, 0} is a special case of minimizing the following objective:\nE \u03c1\u00b5(s,a) p(s\u2032|s,a) p+ \u00b5 (s+|s,a) [f \u2217(\u2212(Q \u2212\u03b3P\u03c0Q)(s, a, s+)) \u2212\u03b2Q(s, a, s\u2032)] with \u03b2 = (1 \u2212\u03b3) and the convex function f \u2217chosen to be: f \u2217(x) = (x \u22121)2/2 + 3/2\nE \u03c1\u00b5(s,a) p(s\u2032|s,a) p+ \u00b5 (s+|s,a) [f \u2217(\u2212(Q \u2212\u03b3P\u03c0Q)(s, a, s+)) \u2212\u03b2Q(s, a, s\u2032)]\nWhile the step counting interpretation in (16) already provides valid meaning to the HER rewards, re-writing the objective in this way reveals why this optimization process is also doing imitation learning. Its first term is the same as the one in divergence minimization objective (15). For the second term, rather than pushing down on the Q values under the current policy \u03c0 in (15), which would incorrectly assume the optimality of self-generated demonstrations and hinder exploration, it pushes up on the transition tuple (s, a, s\u2032), where the action a is guaranteed to be optimal for reaching the immediate next state s\u2032. As a result, this objective encourages the policy to imitate the best in self-generated demonstrations without restraining exploration.\n# 4.4. Can Hindsight BC Facilitate Q-learning?\nNow that we have shown that the Q-learning process in HER is also doing a special form of imitation, one may wonder whether BC (10) has any additional role to play. We hypothesize that, for a softmax policy under discrete action space, hindsight BC might facilitate the Q-learning process. We have reasons to hypothesize so: under discrete action space, BC becomes a cross entropy loss and has been repeatedly shown to work very well (Duan et al., 2017; Lynch et al., 2019; Jang et al., 2021; Shafiullah et al., 2022; Brohan et al., 2022). Even large language models like GPT3 (Brown et al., 2020) can be seen as a softmax policy on discrete action space trained with BC. The Q-values can be seen as the logits of a softmax policy (Schulman et al., 2017a). The problem is that hindsight relabeled actions (s, a, s+) are often suboptimal beyond a single step of relabeling (s, a, s\u2032). Utilizing the interpretation of Q-values as negated step count in (16), we can imitate only those actions that move the agent closer to a goal according to its own estimates, by defining a threshold w(s, a, s\u2032, g) for imitation:\n# w = 1(Q(s, a, g) \u2212max a\u2032 Q(s\u2032, a\u2032, g) < log \u03b3hdm) (\n(17)\n# and the additional objective on top of Q-learning with {\u22121, 0} rewards (in 4.3) becomes Lhdm:\nand the additional objective on top of Q-learning with {\u22121, 0} rewards (in 4.3) becomes Lhdm:\nE\u03c1\u00b5(s,a,s\u2032,g)[\u2212w(s, a, s\u2032, g) \u00b7 log softmaxQ(s, a, g)] (18)\n(18)\nwhich we call Hindsight Divergence Minimization (HDM). Intuitively, this additional loss imitates an action when the value functions believe that this action can move the agent closer to the goal by at least \u2212log \u03b3hdm steps. The idea of imitating the best actions is similar to self-imitation learning (SIL) (Oh et al., 2018; Vinyals et al., 2019), but we study a (reward-free) goal-reaching setting, with the advantage function in SIL being replaced by the delta of step counts a particular action can produce in getting closer to the goal.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b894/b894b943-6f21-4a83-b484-9c4921bd86cb.png\" style=\"width: 50%;\"></div>\nFigure 5: Goal-reaching environments from GCSL (Ghosh et al., 2019) that we consider for discrete action space experiments: reaching a goal location in Four Rooms, landing at a goal location in Lunar Lander, pushing a puck to a goal location in Sawyer Push, opening the door to a goal angle in Door Open (Nair et al., 2018b), turning a valve to a goal orientation in Claw Manipulate (Ahn et al., 2020) Importantly, those environments have discretized action space. We use softmax policy where Q-values are policy logits.\n<div style=\"text-align: center;\">Figure 5: Goal-reaching environments from GCSL (Ghosh et al., 2019) that we consider for discrete action space experiments: reaching a goal location in Four Rooms, landing at a goal location in Lunar Lander, pushing a puck to a goal location in Sawyer Push, opening the door to a goal angle in Door Open (Nair et al., 2018b), turning a valve to a goal orientation in Claw Manipulate (Ahn et al., 2020). Importantly, those environments have discretized action space. We use softmax policy where Q-values are policy logits.</div>\n# 5. Related Work\nFrameworks for Goal Reaching Many attempts have been made to rigorously formulate a theoretical framework for the goal reaching problem. The earliest work that aims to formulate goal-conditioned Q-functions as a step count is (Kaelbling, 1993), though it only considers discrete state space. The framework introduced in (Eysenbach et al., 2020a) considers a finite horizon MDP where the agent receives a \u2212\u221ereward at the final timestep if the final state does not match the goal and a 0 reward in all other conditions, which does not match how multi-goal Q-learning works in practice. Hindsight EM (Tang & Kucukelbir, 2021) borrows concepts from control as inference (Levine, 2018) but only considers hindsight BC for policy learning. The difficulty of defining the objective of HER (Andrychowicz et al., 2017) discussed in 4.2 was also previously noted in C-learning (Eysenbach et al., 2020b), which concludes that \u201dit is unclear what quantity Q-learning with hindsight relabeling optimizes\u201d and regards the Q-values of HER for continuous states as ill-defined. C-learning then proposed to directly estimate (4) as a workaround. Similarly, (Rudner et al., 2021) advocates directly fitting a dynamics model and using per-step reward log p(s\u2032 = g | s, a) for goalreaching. However, those frameworks do not explain the success of simple binary rewards for goal reaching in continuous state space (Andrychowicz et al., 2017; Warde-Farley et al., 2018), which still achieves state-of-the-art results today (Chebotar et al., 2021). By contrast, our work clearly defines what hindsight relabeling under HER reward optimizes by illustrating its underlying graphical models and the meaning of its Q-values, both of which remain well-defined in the case of continuous state space.\nConnecting Goal Reaching with Inverse RL Classical inverse RL either uses a max margin loss in the apprenticeship learning formulation or contrastive divergence loss (Hinton, 2002) in the maximum entropy formulation (Ng et al., 2000; Abbeel & Ng, 2004; Ziebart et al., 2008; Ho et al., 2016), and those ideas have been borrowed accordingly in the goal reaching literature (Eysenbach et al., 2022; Rudner et al., 2021). GAIL (Ho & Ermon, 2016) uses a\nGAN-like discriminator (Goodfellow et al., 2014) as the reward, and (Ding et al., 2019) augments binary rewards in HER with smoother GAIL rewards for goal reaching. The introduction of f-GAN (Nowozin et al., 2016) generalizes GAN to more f-divergences, which led to a similar generalization of GAIL (Ghasemipour et al., 2020; Zhang et al., 2020), which led to f-divergence based exploration strategy (Durugkar et al., 2021) and a goal reaching algorithm without hindsight relabeling (Ma et al., 2022). SQIL (Reddy et al., 2019) is an imitation learning method that runs Q-learning on two constant rewards: r = 1 for expert data and r = 0 for policy data; SQIL can often outperform GAIL. The SQIL reward is similar to the HER reward in some ways, but prior works have not shown how this type of reward does imitation in goal reaching.\n# 6. Experiments\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d016/d0168c5f-e1a1-485d-9418-807c8ce6db28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Goal reaching environments from (Plappert et al., 2018a) we consider for benchmarking multi-goal rewards: HER (Andrychowicz et al., 2017) with (\u22121, 0) rewards, AM (Chebotar et al., 2021) with (0, 1) rewards, HER with (0, 1) rewards.</div>\n<div style=\"text-align: center;\">There are two parts of our experimental investigations.</div>\n\u2022 We study how reward designs affect the goal reaching performance, and demonstrate that in multi-goal RL, seemingly insignificant details about rewards can lead to the striking difference between success and total failure in learning. The experimental results are in accordance with our theoretical framework which suggests that {\u22121, 0} rewards should work best.\n\u2022 In the second part, we study the special case of a softmax policy on discrete action space, and find that despite advances in Hindsight BC, HER can still achieve significantly better results. Moreover, a vanilla combi-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/903f/903fc180-1120-47c3-bc32-9bb74b7a5afd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Success rate comparisons between different multi-goal RL rewards on Fetch environments. The only difference between three methods is the reward and backup strategies; all other parts of implementation are the same. Results over 5 eeds are shown. Using a reward of {\u22121, 0} is crucial for learning success.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0436/04363c26-c428-4a72-ba6f-969795c141df.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: We find that the quantity ag change ratio is strongly indicative of learning progress in multi-goal RL. We define the ag change ratio of \u03c0 as: the percentage of trajectories where the achieved goals (ag) in initial states s0 are different from the achieved goals in final states sT under \u03c0. Most training signals are only created from ag changes, because they provide examples of how to rearrange an environment. An increase in ag change ratio often precedes an increase in success rates.</div>\nnation of HER + HBC hurts performance, but our proposed HDM loss (a variant of BC) can further improve the performance of HER on discrete action space.\n6.1. Investigating Reward Designs in Goal Reaching We investigate 3 different reward designs for goal reaching: Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) with {\u22121, 0} rewards:\n(19)\nActionable Models (AM) (Chebotar et al., 2021) uses {0, 1} rewards, but directly defines Q(s, a, s\u2032) as 1:\n(20)\nHER with {0, 1} rewards which is also a modified version of actionable models where the bellman backup continues after the goal is reached:\n(21)\nWe use widely benchmarked Fetch environments (Plappert et al., 2018a) for multi-goal RL as the task suite to compare\nthose three strategies. For all three methods, we use the same set of hyper-parameters (including learning rate, batch size, network architecture, target network update frequency and polyak value, etc). The only differences are the rewards and bellman backups . The results are presented in Figure 7. Why does {\u22121, 0} reward work so well? Our framework shows that running Q-learning with {\u22121, 0} rewards leads to a probabilistically well-defined Q values in the form of a normalized step count (16). One can perhaps interpret HER with {0, 1} rewards as adding a constant offset 1/(1 \u2212\u03b3) on the converged Q-values of {\u22121, 0} rewards, but this offset is quite large and eventually causes learning to diverge in this case. Actionable Models (AM) address this issue by stopping further bellman backup once the goal is reached (s\u2032 = g) and directly setting the Q-value to be 1. This does not align with our analysis of the divergence minimization objective (15), and fails to teach the policy to stop at the goal state once the goal is reached: under this bellman backup, once the goal is reached once, nothing matters afterwards. To summarize, using a reward of {\u22121, 0} is crucial for the success of multi-goal Q-learning with hindsight relabeling.\n# 6.2. Goal Reaching with Discrete Action Space\nIn this section, we study whether our proposed HDM loss (18) can facilitate the goal-conditioned Q-learning process\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c17a/c17a23aa-87f7-4c06-81f7-fae6c53bd734.png\" style=\"width: 50%;\"></div>\ngure 9: Intuitions about when HDM applies BC. From left to right: (a) the Maze environment with a goal in the upper-left corner; (b) e Q-values learned through the converged policy. Lighter red means higher Q-value; (c) visualizing the actions (from the replay) that et imitated when conditioning on the goal and setting \u03b3hdm = 0.95, with the background color reflecting how much an action moves the gent closer to the goal based on the agent\u2019s own estimate; (d) \u03b3hdm = 0.85; (e) \u03b3hdm = 0.75. As we lower \u03b3hdm, the threshold \u2212log \u03b3hdm ets higher and fewer actions get imitated, with the remaining imitated actions more concentrated around the goal. HDM uses Q-learning  account for the worst while imitating the best during the goal-reaching process.\n<div style=\"text-align: center;\">Figure 9: Intuitions about when HDM applies BC. From left to right: (a) the Maze environment with a goal in the upper-left corner; ( the Q-values learned through the converged policy. Lighter red means higher Q-value; (c) visualizing the actions (from the replay) th get imitated when conditioning on the goal and setting \u03b3hdm = 0.95, with the background color reflecting how much an action moves t agent closer to the goal based on the agent\u2019s own estimate; (d) \u03b3hdm = 0.85; (e) \u03b3hdm = 0.75. As we lower \u03b3hdm, the threshold \u2212log \u03b3h gets higher and fewer actions get imitated, with the remaining imitated actions more concentrated around the goal. HDM uses Q-learnin to account for the worst while imitating the best during the goal-reaching process.</div>\nSuccess Rate (%)\nFour Rooms\nLunar Landar\nSawyer Push\nDoor Opening\nClaw Manipulate\nGCSL / HBC\n78.27 \u00b14.76\n50.00 \u00b17.77\n44.67 \u00b113.86\n19.10 \u00b15.97\n16.80 \u00b16.55\nHER r = (0, 1)\n86.60 \u00b14.22\n39.30 \u00b16.81\n57.60 \u00b16.61\n82.50 \u00b14.87\n22.80 \u00b16.43\nHER + SQL\n88.50 \u00b14.56\n44.50 \u00b19.61\n57.20 \u00b16.32\n84.70 \u00b15.33\n16.13 \u00b18.43\nHER r = (\u22121, 0)\n86.40 \u00b15.11\n50.80 \u00b14.66\n54.60 \u00b16.16\n83.76 \u00b16.02\n20.20 \u00b16.23\nHER + HBC\n82.90 \u00b16.24\n35.33 \u00b14.57\n52.63 \u00b18.05\n76.44 \u00b15.37\n16.93 \u00b18.03\nHDM (ours)\n96.27 \u00b12.56\n57.60 \u00b17.21\n66.00 \u00b15.13\n88.60 \u00b14.63\n27.89 \u00b16.46\nTable 1: Benchmark results of test-time success rates in self-supervised goal-reaching, over 5 seeds. We compare our method HDM wi GCSL (Ghosh et al., 2019), HER (Andrychowicz et al., 2017) with two different types of rewards, SQL (Schulman et al., 2017a) (So Q-Learning) + HER, and HER + HBC. We find that HER can still outperforms GCSL / HBC, and a vanilla combination of HER + HB actually hurts performance. HDM selectively decides on what to imitate and outperforms HER and HBC.\non environments with either discrete or discretized action space (Figure 5), as mentioned in 4.4. Hindsight BC (or GCSL) with discrete action space is known to have promising results even in a totally self-supervised goal reaching setting without external demonstrations (Ghosh et al., 2019). To provide a fair comparison to HBC, we do not assume that the agent has direct access to the ground truth binary reward metric. Note the original HER uses this metric during relabeling (Andrychowicz et al., 2017), but assuming access to this feedback is often unrealistic for real-world robotlearning (Lin et al., 2019; Chebotar et al., 2021). Instead, a positive reward is provided only when the relabeled hindsight goal is the immediate next state. As a result, the training procedure is completely self-supervised, similar to HBC. Additionally, we also consider the HER + Soft Q-Learning (SQL) (Schulman et al., 2017a) baseline, since SQL often improves policy robustness (Haarnoja et al., 2018). HDM builds on top of HER with (\u22121, 0) rewards and adds a BClike loss with a clipping condition (18) such that only the actions that move an agent closer to the goal get imitated. See Figure 9 for more visualizations of when HDM applies the BC loss on top of a Q-learning process. The results in Table 1 show that HDM achieves the strongest performance on all environments, while reducing variances in success rates. Interestingly, there is no consensus best baseline algorithm, with all five algorithms achieving good\nresults in some environments and subpar performance in others. Combining HER with HBC (which blindly imitates all actions in hindsight) produces worse results than not imitating at all and only resorting to value learning. In contrast, HDM allows for better control over what to imitate. Further ablations on HDM are provided in the 10 of the appendix, which shows that HDM outperforms HER and GCSL across a variety of \u03b3hdm values. Our results indicate that BC alone can only serve as an auxiliary (or perhaps pretraining) objective; value learning is needed to implicitly model the future (13) and improve the policy.\n# 7. Conclusion\nThis work presents a novel goal reaching framework that exploits the deep connection between multi-goal RL and inverse RL to derive a family of goal-conditioned RL algorithms. By understanding hindsight goal relabeling from a divergence minimization perspective, our framework reveals the importance of reward design in multi-goal RL, which is found to significantly affect learning performance in our experiments. Furthermore, we propose an additional hindsight divergence minimization (HDM) loss, which uses Q-learning to account for the worst while using BC to imitate the best, and demonstrate its superior performance on discrete action space. In the future, we hope to further develop our framework to explicitly account for exploration.\n# References\nAbbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1, 2004. Ahn, M., Zhu, H., Hartikainen, K., Ponte, H., Gupta, A., Levine, S., and Kumar, V. Robel: Robotics benchmarks for learning with low-cost robots. In Conference on robot learning, pp. 1300\u20131313. PMLR, 2020. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. Batra, D., Chang, A. X., Chernova, S., Davison, A. J., Deng, J., Koltun, V., Levine, S., Malik, J., Mordatch, I., Mottaghi, R., et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020. Blier, L., Tallec, C., and Ollivier, Y. Learning successor states and goal-dependent values: A mathematical viewpoint. arXiv preprint arXiv:2101.07123, 2021. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020. Chebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J., Irpan, A., Eysenbach, B., Julian, R., Finn, C., and Levine, S. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021. Ding, Y., Florensa, C., Abbeel, P., and Phielipp, M. Goalconditioned imitation learning. In Advances in Neural Information Processing Systems, pp. 15298\u201315309, 2019. Duan, Y., Andrychowicz, M., Stadie, B., Jonathan Ho, O., Schneider, J., Sutskever, I., Abbeel, P., and Zaremba, W. One-shot imitation learning. Advances in neural information processing systems, 30, 2017.\nDurugkar, I., Tec, M., Niekum, S., and Stone, P. Adversarial intrinsic motivation for reinforcement learning. Advances in Neural Information Processing Systems, 34: 8622\u20138636, 2021. Eysenbach, B., Geng, X., Levine, S., and Salakhutdinov, R. R. Rewriting history with inverse rl: Hindsight inference for policy improvement. Advances in neural information processing systems, 33:14783\u201314795, 2020a. Eysenbach, B., Salakhutdinov, R., and Levine, S. Clearning: Learning to achieve goals via recursive classification. arXiv preprint arXiv:2011.08909, 2020b. Eysenbach, B., Zhang, T., Salakhutdinov, R., and Levine, S. Contrastive learning as goal-conditioned reinforcement learning. arXiv preprint arXiv:2206.07568, 2022. Finn, C., Christiano, P., Abbeel, P., and Levine, S. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016. Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and Abbeel, P. Reverse curriculum generation for reinforcement learning. In Conference on robot learning, pp. 482\u2013 495. PMLR, 2017. Fu, J., Luo, K., and Levine, S. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017. Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pp. 1587\u20131596. PMLR, 2018. Garg, D., Chakraborty, S., Cundy, C., Song, J., and Ermon, S. Iq-learn: Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34, 2021. Ghasemipour, S. K. S., Zemel, R., and Gu, S. A divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pp. 1259\u20131277. PMLR, 2020. Ghosh, D., Gupta, A., Fu, J., Reddy, A., Devin, C., Eysenbach, B., and Levine, S. Learning to reach goals without reinforcement learning. ArXiv, abs/1912.06088, 2019. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International\nconference on machine learning, pp. 1861\u20131870. PMLR, 2018. Hinton, G. E. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u2013 1800, 2002. Ho, J. and Ermon, S. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016. Ho, J., Gupta, J., and Ermon, S. Model-free imitation learning with policy optimization. In International Conference on Machine Learning, pp. 2760\u20132769. PMLR, 2016. Hong, Z.-W., Yang, G., and Agrawal, P. Bilinear value networks. arXiv preprint arXiv:2204.13695, 2022. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-shot task generalization with robotic imitation learning. ArXiv, abs/2202.02005, 2021. Janner, M., Mordatch, I., and Levine, S. gamma-models: Generative temporal difference learning for infinitehorizon prediction. Advances in Neural Information Processing Systems, 33:1724\u20131735, 2020. Kaelbling, L. P. Learning to achieve goals. In IJCAI, volume 2, pp. 1094\u20138. Citeseer, 1993. Ke, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S. Imitation learning as f-divergence minimization. In Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pp. 313\u2013329. Springer, 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kostrikov, I., Nachum, O., and Tompson, J. Imitation learning via off-policy distribution matching. arXiv preprint arXiv:1912.05032, 2019. Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. Lin, X., Baweja, H. S., and Held, D. Reinforcement learning without ground-truth state. arXiv preprint arXiv:1905.07866, 2019.\nKe, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S. Imitation learning as f-divergence minimization. In Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pp. 313\u2013329. Springer, 2021.\nLynch, C. and Sermanet, P. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020. Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine, S., and Sermanet, P. Learning latent plans from play. In CoRL, 2019. Ma, Y. J., Yan, J., Jayaraman, D., and Bastani, O. How far i\u2019ll go: Offline goal-conditioned reinforcement learning via f-advantage regression. arXiv preprint arXiv:2206.03023, 2022. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, 518(7540): 529\u2013533, 2015. Nachum, O. and Dai, B. Reinforcement learning via fenchelrockafellar duality. arXiv preprint arXiv:2001.01866, 2020. Nachum, O., Chow, Y., Dai, B., and Li, L. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems, 32, 2019a. Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D. Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b. Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 6292\u2013 6299. IEEE, 2018a. Nair, A. V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S. Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31, 2018b. Ng, A. Y., Russell, S., et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, pp. 2, 2000. Nguyen, X., Wainwright, M. J., and Jordan, M. I. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847\u20135861, 2010. Nowozin, S., Cseke, B., and Tomioka, R. f-gan: Training generative neural samplers using variational divergence minimization. Advances in neural information processing systems, 29, 2016.\nNguyen, X., Wainwright, M. J., and Jordan, M. I. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847\u20135861, 2010.\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. In International Conference on Machine Learning, pp. 3878\u20133887. PMLR, 2018.\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. In International Conference on Machine Learning, pp. 3878\u20133887. PMLR, 2018. OpenAI, O., Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D\u2019Sa, R., Petron, A., Pinto, H. P. d. O., et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021. Pitis, S., Chan, H., Zhao, S., Stadie, B., and Ba, J. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In International Conference on Machine Learning, pp. 7750\u20137761. PMLR, 2020. Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej, M., Welinder, P., et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018a. Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej, M., Welinder, P., et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018b. Pomerleau, D. A. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988. Pong, V., Gu, S., Dalal, M., and Levine, S. Temporal difference models: Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018. Pong, V. H., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S. Skew-fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019. Reddy, S., Dragan, A. D., and Levine, S. Sqil: Imitation learning via reinforcement learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019. Ross, S., Gordon, G. J., and Bagnell, J. A. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. Rudner, T. G., Pong, V., McAllister, R., Gal, Y., and Levine, S. Outcome-driven reinforcement learning via variational inference. Advances in Neural Information Processing Systems, 34:13045\u201313058, 2021. Schulman, J., Chen, X., and Abbeel, P. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. Shafiullah, N. M. M., Cui, Z. J., Altanzaya, A., and Pinto, L. Behavior transformers: Cloning k modes with one stone. arXiv preprint arXiv:2206.11251, 2022. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy gradient algorithms. In International conference on machine learning, pp. 387\u2013 395. PMLR, 2014. Silver, D., Singh, S., Precup, D., and Sutton, R. S. Reward is enough. Artificial Intelligence, 299:103535, 2021. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Tang, Y. and Kucukelbir, A. Hindsight expectation maximization for goal-conditioned reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 2863\u20132871. PMLR, 2021. Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350\u2013354, 2019. Warde-Farley, D., Van de Wiele, T., Kulkarni, T., Ionescu, C., Hansen, S., and Mnih, V. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018. Zhang, L., Yang, G., and Stadie, B. C. World model as a graph: Learning latent landmarks for planning. In International Conference on Machine Learning, pp. 12611\u2013 12620. PMLR, 2021. Zhang, X., Li, Y., Zhang, Z., and Zhang, Z.-L. f-gail: Learning f-divergence for generative adversarial imitation learning. Advances in neural information processing systems, 33:12805\u201312815, 2020. Zhu, Z., Lin, K., Dai, B., and Zhou, J. Off-policy imitation learning from observations. Advances in Neural Information Processing Systems, 33:12402\u201312413, 2020. Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pp. 1433\u20131438. Chicago, IL, USA, 2008.\nA. Proofs\nA.1. Deriving Hindsight Behavior Cloning We want to prove the equivalence defined in (10)\nWe want to prove the equivalence defined in (10)\nWe use the definition of KL divergence:\nAnd the graphical models of the two distributions:\np\u00b5(s, a, s+) = \u03c1\u00b5(s, a)p+ \u00b5 (s+ | s, a) pBC \u03c0 (s, a, g) = p(g)\u03c1\u00b5(s)\u03c0(a | s, g)\nResulting in\nDKL(p\u00b5(s, a, s+) \u2225pBC \u03c0 (s, a, g)) = Ep\u00b5(s,a,s+)[log p\u00b5(s, a, s+) \u2212log pBC \u03c0 (s, a, g)] = Ep\u00b5(s,a,s+)[log p\u00b5(s, a, s+) \u2212log p(g) \u2212log \u03c1\u00b5(s) \u2212log \u03c0(a | s, g \ufffd \ufffd\nDKL(p\u00b5(s, a, s+) \u2225pBC \u03c0 (s, a, g)) = Ep\u00b5(s,a,s+)[log p\u00b5(s, a, s+) \u2212log pBC \u03c0 (s, a, g)] = Ep\u00b5(s,a,s+)[log p\u00b5(s, a, s+) \u2212log p(g) \u2212log \u03c1\u00b5(s) \u2212log \u03c0(a | s, g) = E\u03c1\u00b5(s,a)p+ \u00b5 (s+|s,a) \ufffd log p\u00b5(s, a, s+) p(g)\u03c1\u00b5(s) \u2212log \u03c0(a | s, g) \ufffd\nwhere we find that\narg min \u03c0 DKL(p\u00b5(s, a, s+) \u2225pBC \u03c0 (s, a, g)) = arg min \u03c0 E\u03c1\u00b5(s,a)p+ \u00b5 (s+|s,a)[\u2212log \u03c0(a | s, g)]\n# A.2. Main Lemmas\nLemma A.1 (Online-to-offline transformation for goal reaching). Given a goal-conditioned policy \u03c0(a | s, g), its corresponding Q-function Q\u03c0(s, a, g), and arbitrary state-action visitation distribution \u03c1\u00b5(s, a) of another policy \u00b5(a | s), the expected temporal difference for online rollouts under \u03c0 is: Ep(g)\u03c1\u03c0(s,a|g)[(Q\u03c0 \u2212\u03b3 \u00b7 P\u03c0Q\u03c0)(s, a, g)] = Ep(g)\u03c1\u00b5(s,a)\u03c0(\u02dca|s,g)[Q\u03c0(s, \u02dca, g) \u2212\u03b3 \u00b7 P\u03c0Q\u03c0(s, a, g)]\n | Ep(g)\u03c1\u03c0(s,a|g)[(Q\u03c0 \u2212\u03b3 \u00b7 P\u03c0Q\u03c0)(s, a, g)] = Ep(g)\u03c1\u00b5(s,a)\u03c0(\u02dca|s,g)[Q\u03c0(s, \u02dca, g) \u2212\u03b3 \u00b7 P\u03c0Q\u03c0(s, a, g)]\nProof of Lemma 4.1.\n= (1 \u2212\u03b3) \u221e \ufffd t=0 \u03b3tEp(g)\u03c1t \u00b5(s,a) \u03c0(\u02dca|s,g) [Q\u03c0(s, \u02dca, g) \u2212\u03b3E p(s\u2032|s,a) \u03c0(a\u2032|s\u2032,g) Q\u03c0(s\u2032, a\u2032, g)] = Ep(g)\u03c1\u00b5(s,a)\u03c0(\u02dca|s,g)[Q\u03c0(s, \u02dca, g) \u2212\u03b3Ep(s\u2032|s,a),\u03c0(a\u2032|s\u2032,g)Q\u03c0(s\u2032, a\u2032, g)]\n\ufffd | = Ep(g)\u03c1\u00b5(s,a)\u03c0(\u02dca|s,g)[Q\u03c0(s, \u02dca, g) \u2212\u03b3Ep(s\u2032|s,a),\u03c0(a\u2032|s\u2032,g)Q\u03c0(s\u2032, a\u2032, g)]\nA.3. Q Function as Step Counts\nWe make the following definitions:\nQt = Q(st, at, g) Qt+1 = E\u03c0(st+1,g)[Q(st+1, a\u2032, g)] \u03b2t = p(st+1 = g | st, at) \u03b1(i) t = \ufffd \u2206=i \u03b3\u2206p\u03c0(st+1+\u2206= g | st, at)\nAnd make the following mild assumptions:\n1/E\u03c0(a|st,g)[\u03b1(0) t ] = E\u03c0(a|st,g)[1/\u03b1(0) t ] 1/Ep(st+1|st,at)[\u03b1(1) t+1] = Ep(st+1|st,at)[1/\u03b1(1) t ]\nWhich can be combined to reach the result:\n1/\u03b1(1) t = Ep(st+1|st,at)\u03c0(a\u2032 t+1|st+1,g)[1/\u03b1(1) t+1]\nMeaning that for an infinite horizon MDP where all goals are eventually reached, the reciprocal of a geometrically summed future likelihood of reaching a goal remains approximately the same in expectation under rollouts. Lemma A.2 (Recursive estimate of goal reaching step count). Under the notations and the assumptions above, the step count definition (16) has the following property:\nQt = Ep(st+1|st,at)[(1 \u2212 \u03b2t \u03b2t + \u03b1(1) t )(\u22121 + Qt+1)]\nProof of Lemma 4.2. We start from the definition of our Q value:\nQ(st, at, g) = \u2212 \ufffd \u2206=0 \u03b3\u2206p\u03c0(st+1+\u2206= g | st, at) \u00b7 \u2206 \ufffd \u2206=0 \u03b3\u2206p\u03c0(st+1+\u2206= g | st, at)\nQ\u03c0(st, at, g) can be expanded as\n\u2212 \u03b3p(st+2 = g | st, at) + 2\u03b32p(st+3 = g | st, at) + 3\u03b33p(st+4 = g | st, at) + \u00b7 \u00b7 \u00b7 p(st+1 = g | st, at) + \u03b3p(st+2 = g | st, at) + \u03b32p(st+3 = g | st, at) + \u03b33p(st+4 = g | st, at) + \u00b7 \u00b7 \u00b7 he value function V (st, g) is defined to be:\nThe value function V (st, g) is defined to be:\nV (st, g) = \u2212 \ufffd \u2206=0 \u03b3\u2206p\u03c0(st+1+\u2206= g | st) \u00b7 \u2206 \ufffd \u2206=0 \u03b3\u2206p\u03c0(st+1+\u2206= g | st)\nWhich can be expanded as:\n\u2212 \u03b3p(st+2 = g | st) + 2\u03b32p(st+3 = g | st) + 3\u03b33p(st+4 = g | st) + \u00b7 \u00b7 \u00b7 p(st+1 = g | st) + \u03b3p(st+2 = g | st) + \u03b32p(st+3 = g | st) + \u03b33p(st+4 = g | st) + \u00b7 \u00b7 \u00b7\nOn the other hand, E\u03c0(a|st,g)[Q(st, a, g)] can be written as:\n\u2212E\u03c0(a|st,g) \ufffd \u03b3p(st+2 = g | st, a) + 2\u03b32p(st+3 = g | st, a) + 3\u03b33p(st+4 = g | st, a) + \u00b7 \u00b7 \u00b7 p(st+1 = g | st, a) + \u03b3p(st+2 = g | st, a) + \u03b32p(st+3 = g | st, a) + \u03b33p(st+4 = g | st, a) + \u00b7 \u00b7 \u00b7 by the law of total probability, p(st+1+\u2206= g | st) = E\u03c0(a|st,g)[p(st+1+\u2206= g | st, at)]\nby the law of total probability,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45da/45da87fe-0b37-42be-884e-f1198c3b23bc.png\" style=\"width: 50%;\"></div>\n\ufffd Furthermore, V \u03c0(st+1, g) can be expanded as:\nNote that, for \u2206> 0, by the law of total probability,\nAs a result:\nEp(st+1|st,at)[\u22121 + V \u03c0(st+1, g)] = \u2212Z[\u03b3p(st+2 = g | st, at) + 2\u03b32p(st+3 = g | st, at) + 3\u03b33p(st+4 = g | st, at) + \u00b7 \u00b7 \u00b7 \nEp(st+1|st,at)[\u22121 + V \u03c0(st+1, g)] = \u2212Z[\u03b3p(st+2 = g | st, at) + 2\u03b32p(st+3 = g | st, at) + 3\u03b33p(st+4 = g | st, at where\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/676c/676c4c4c-169e-42d8-ac95-5b56e0e340fa.png\" style=\"width: 50%;\"></div>\nResulting in:\nwe have:\nWhich eventually leads to:\n(22)\n(23)\n# A.4. Deriving HER Rewards\nA.4. Deriving HER Rewards\nLemma A.3 (Understanding Hindsight Experience Replay). Multi-goal Q-learning with HER reward (5) with {\u22121, 0} special case of minimizing the following objective:\nwith \u03b2 = (1 \u2212\u03b3) and the convex function f \u2217chosen to be:\nf \u2217(x) = (x \u22121)2/2 + 3/2\np+ \u00b5 (s+ | s, a) = (1 \u2212\u03b3)p(s+ | s, a) + \u03b3 \ufffd S\u00d7A p(s\u2032 | s, a)\u00b5(a\u2032 | s\u2032)p+ \u00b5 (s+ | s\u2032, a\u2032)ds\u2032da\u2032\nAnd that we have defined a quadratic form of f \u2217(with c being constants): f \u2217(x) = (x \u22121)2/2 + c\nng the dynamics to expand the expectation and applying the choice of f \u2217being a quadratic, the loss becomes:\narg min Q E\u03c1\u00b5(s,a)p(s\u2032|s,a)(1 \u2212\u03b3) \u00b7 \ufffd1 2 \ufffd \u22121 + (\u03b3P\u03c0Q \u2212Q\u03b8)(s, a, s\u2032) \ufffd2 \u2212\u00b7Q\u03b8(s, a, s\u2032) \ufffd\n\ufffd \ufffd \ufffd + E\u03c1\u00b5(s,a)p(s\u2032|s,a)\u00b5(a\u2032|s\u2032)p+ \u00b5 (s+|s\u2032,a\u2032) \ufffd \u03b3 \u00b7 1 2 \ufffd \u22121 + (\u03b3P\u03c0Q \u2212Q\u03b8)(s, a, s+) \ufffd2\n\ufffd \ufffd \ufffd\ufffd Assuming that there is a stop gradient sign on P\u03c0Q because of the use of a target network (Mnih et al., 2015; Lillicrap et al., 2015; Haarnoja et al., 2018), we can rewrite the above as one single quadratic and see that the gradient of the above loss w.r.t Q is equivalent to the gradient of the following squared Bellman residual:\narg min Q E\u03c1\u00b5(s,a)p(s\u2032|s,a)p+ \u00b5 (s+|s,a) \ufffd1 2 \ufffd r(s, a, s\u2032, s+) + (\u03b3P\u03c0Q \u2212Q\u03b8)(s, a, s+) \ufffd2\ufffd\nwhere the reward function r(s, a, s\u2032, s+) is:\nThe constant 3/2 in f \u2217is chosen to ensure that ((f \u2217)\u2217)(1) = 0 in the definition of f-divergence (6), but it does not affec the optimization process.\n# B. Experimental Details\nFor the reward design experiments, we use the following hyper-parameters in Table 2, which are mostly the same from prior open-sourced implementations (Andrychowicz et al., 2017; Pitis et al., 2020; Zhang et al., 2021). For discrete action space experiments, we use the following thresholds (of Euclidean norms) for determining success: [0.08, 0.08, 0.05, 0.05, 0.1], which are tight thresholds based on our visualizations of the environments (those are tighter thresholds than the original ones in (Ghosh et al., 2019); based on our observations, the original thresholds are often too loose). We use the same network architecture, sampling and optimization schedules for all the methods, as described in Table 3. As for \u03b3HDM, we set it to be 0.85 in Four Rooms and Lunar Lander, 0.5 in Sawyer Push and Claw Manipulate, and 0.4 for Door Opening. Ablation on this hyper-parameter can be found in Figure 10.\n\n<div style=\"text-align: center;\">Table 2: Hyper-parameters for the goal reaching reward design experiments</div>\nParameter\nValue\nDDPG (Lillicrap et al., 2015)\noptimizer\nAdam (Kingma & Ba, 2014)\narchitecture\nMLP + BVN (Hong et al., 2022)\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnonlinearity\nReLU\nNormalize per-dimension obs (Schulman et al., 2017b)\nyes\npolyak for target network (\u03c4)\n0.995\ntarget network update interval\n10\nUse target network for policy (Fujimoto et al., 2018)\nyes\nratio between environment vs optimization steps\n2\nRandom action probability\n0.2\nInitial random trajectories per worker\n100\nHindsight relabelling ratio\n0.85\nLearning rate\n0.001\nBatch size\n1024\nGamma factor \u03b3\n0.99\nAction L2 regularization\n0.01\nGaussian noise scale\n0.1\nNumber of parallel workers\n12\nReplay buffer size\n2500000\n<div style=\"text-align: center;\">Table 3: Hyper-parameters for discrete action space experiments</div>\nParameters\nValue\nDDQN (Van Hasselt et al., 2016)\nOptimizer\nAdam (Kingma & Ba, 2014)\nNumber of hidden layers (all networks)\n2\nNumber of hidden units per layer\n[400, 300] (Fujimoto et al., 2018)\nNon-linearity\nReLU\nPolyak for target network\n0.995\nTarget update interval\n10\nRatio between env vs optimization steps\n1\nInitial random trajectories\n200\nHindsight relabelling ratio\n0.85\nUpdate every # of steps in environment\n50\nNext state relabelling ratio\n0.2\nLearning rate\n5.e-4\nBC loss weight\n1.0\nGamma factor \u03b3\n0.98\nLogit temperature for SQL (Schulman et al., 2017a)\n0.2\nBatch size\n256\nEpsilon greedy (Mnih et al., 2015)\n0.2\nReplay buffer size\n2500000\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/226a/226a147c-c0c7-4291-bcac-c607f0adb363.png\" style=\"width: 50%;\"></div>\nFigure 10: Ablation studies on HDM Gamma \u03b3hdm (see equation (18)). The orange line and the blue line denote HER (Andrychowicz et al., 2017) and GCSL (Ghosh et al., 2019) baseline performance. Intuitively, \u03b3hdm controls the threshold for when an action is considered good enough for imitation. As we lower \u03b3hdm, the threshold \u2212log \u03b3hdm gets higher and fewer actions get imitated, with the remaining imitated actions more concentrated around the goal, where hindsight-relabeled actions are more likely to be optimal. The ablation shows that HDM outperforms HER and GCSL across a variety of \u03b3hdm, while recovering the performance of HER when its value is close to zero. Both environments have discrete action space where we use a softmax (Boltzmann) policy (Schulman et al., 2017a) with the softmax logits being Q-values.\n# C. Further Discussions on Achieved Goal (ag) Change Ratio\nWe define the ag change ratio of \u03c0 to be: the percentage of trajectories where the achieved goals in initial states s0 are different from the achieved goals in final states sT under \u03c0. Using notation from (5), it can be computed as Es0\u00b7\u00b7\u00b7sT \u223c\u03c0[\u2212rHER(\u00b7, \u00b7, s0, sT )]. We then define initial ag change ratio to be the ag change ratio of a random-acting policy \u03c00. Using notation from (5), it can be computed as Es0\u00b7\u00b7\u00b7sT \u223c\u03c00[\u2212rHER(\u00b7, \u00b7, s0, sT )]. In our experiments, we have made the following observations:\n\u2022 ag change ratio is strongly indicative of learning progress on many environments, as shown in Figure 8. \u2022 initial ag change ratio, which can be computed without training any policies, seems to be correlated to the f performance of HBC / GCSL, as shown in Figure 11 (bubble sizes are based on the variances of the success rate).\n\u2022 ag change ratio is strongly indicative of learning progress on many environments, as shown in Figure 8. \u2022 initial ag change ratio, which can be computed without training any policies, seems to be correlated to the final performance of HBC / GCSL, as shown in Figure 11 (bubble sizes are based on the variances of the success rate). We suspect that the reasons are the following:\nWe suspect that the reasons are the following:\n\u2022 Most training signals are only created from ag changes, because they provide examples of how to rearrange an environment. As the policy learns to rearrange the environment with higher frequency, hindsight relabeling ensures that the learning progress naturally accelerates. \u2022 One likely reason why the performance of GCSL seems to be upper-bounded by a linear relationship between the final success rate and the initial ag change ratio is that: a BC-style objective starts off cloning the initially random trajectories, so if initial ag change ratio is low, the policy would not learn to rearrange ag from self-imitation, compounding to a low final performance. HER and HDM are able to surpass this upper ceiling likely because they try to reach goals with fewer steps besides imitation.\n\u2022 Most training signals are only created from ag changes, because they provide examples of how to rearrange an environment. As the policy learns to rearrange the environment with higher frequency, hindsight relabeling ensures that the learning progress naturally accelerates.\n One likely reason why the performance of GCSL seems to be upper-bounded by a linear relationship between the final success rate and the initial ag change ratio is that: a BC-style objective starts off cloning the initially random trajectories, so if initial ag change ratio is low, the policy would not learn to rearrange ag from self-imitation, compounding to a low final performance. HER and HDM are able to surpass this upper ceiling likely because they try to reach goals with fewer steps besides imitation.\n\u2022 This finding suggests that in order to make goal-reaching easier, we should either modify the initial state distribution \u03c10(s) such that ag can be easily changed through random exploration (Florensa et al., 2017) (if the policy is training from scratch), or initialize BC from some high-quality demonstrations where ag does change (Ding et al., 2019; Nair et al., 2018a; Lynch et al., 2019).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c3ee/c3eefcca-54bb-4b46-9a4d-b96e1a9adf4d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Success rate versus initial ag change ratio.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5b02/5b02ad89-b1b4-49d2-9f13-c57292e4a803.png\" style=\"width: 50%;\"></div>\nFigure 12: Even on an environment where the initial ag change ratio is 1 and where the ag change ratio stays at around 1 throughout learning for all methods, Actionable Models (AM) (Chebotar et al., 2021) and HER with {0, 1} rewards still fail to learn anything, where HER with {\u22121, 0} rewards (Andrychowicz et al., 2017) succeed at learning a goal-conditioned policy. All hyper-parameters are the same except for the reward design and bellman backups for all three methods. Results are averaged over 5 random seeds, and the variances across seeds are plotted. This shows that our conclusions in Section 6.1 are independent of exploration difficulties, and that the design choices for goal-reaching rewards matter significantly. The above insights on the possible relationship between ag change ratio and exploration difficulty makes us wonder whether the striking results about reward design effectiveness in Section 6.1 are independent of the exploration problem. Indeed, Figure 8 shows that HER with {\u22121, 0} rewards takes off faster partially because it manages to increase its ag change ratio faster. To show that our conclusions about goal-reaching reward design are independent of exploration difficulties, we benchmark the reward-design results on an environment where all methods have an initial ag change ratio of 1 (and where the ag change ratio stays at around 1 throughout learning for all methods), namely the Shallow Hand environment (Plappert et al., 2018a) HandManipulateBlockRotateZ. We use the same set of hyper-parameters as Table 2 except for the fact that we use 20 parallel workers for the Hand environments rather than 12, since this environment is harder. The results are presented in Figure 12, showing that HER with {\u22121, 0} rewards is the only method that learns and succeeds. These findings are largely consistent with the conjecture hypothesized in the original Hindsight Experience Replay (HER) paper, which discussed a similar problem in the context of a simple bit-flipping experiment (See (Andrychowicz et al., 2017) Section 3.1).\n<div style=\"text-align: center;\">Figure 12: Even on an environment where the initial ag change ratio is 1 and where the ag change ratio stays at around 1 throughout learning for all methods, Actionable Models (AM) (Chebotar et al., 2021) and HER with {0, 1} rewards still fail to learn anything, where HER with {\u22121, 0} rewards (Andrychowicz et al., 2017) succeed at learning a goal-conditioned policy. All hyper-parameters are the same except for the reward design and bellman backups for all three methods. Results are averaged over 5 random seeds, and the variances across seeds are plotted. This shows that our conclusions in Section 6.1 are independent of exploration difficulties, and that the design choices for goal-reaching rewards matter significantly.</div>\nThe above insights on the possible relationship between ag change ratio and exploration difficulty makes us wonder whether the striking results about reward design effectiveness in Section 6.1 are independent of the exploration problem. Indeed, Figure 8 shows that HER with {\u22121, 0} rewards takes off faster partially because it manages to increase its ag change ratio faster. To show that our conclusions about goal-reaching reward design are independent of exploration difficulties, we benchmark the reward-design results on an environment where all methods have an initial ag change ratio of 1 (and where the ag change ratio stays at around 1 throughout learning for all methods), namely the Shallow Hand environment (Plappert et al., 2018a) HandManipulateBlockRotateZ. We use the same set of hyper-parameters as Table 2 except for the fact that we use 20 parallel workers for the Hand environments rather than 12, since this environment is harder. The results are presented in Figure 12, showing that HER with {\u22121, 0} rewards is the only method that learns and succeeds. These findings are largely consistent with the conjecture hypothesized in the original Hindsight Experience Replay (HER) paper, which discussed a similar problem in the context of a simple bit-flipping experiment (See (Andrychowicz et al., 2017) Section 3.1).\n",
    "paper_type": "method",
    "attri": {
        "background": "Hindsight goal relabeling has become a foundational technique in multi-goal reinforcement learning (RL). The essential idea is that any trajectory can be seen as a sub-optimal demonstration for reaching its final state. Intuitively, learning from those arbitrary demonstrations can be seen as a form of imitation learning (IL). However, the connection between hindsight goal relabeling and imitation learning is not well understood. This paper proposes a novel framework to understand hindsight goal relabeling from a divergence minimization perspective, allowing us to derive several existing methods from first principles and improve goal reaching algorithms.",
        "problem": {
            "definition": "The problem addressed in this paper involves understanding the relationship between hindsight goal relabeling and imitation learning in the context of multi-goal reinforcement learning.",
            "key obstacle": "The main challenge is the lack of clarity on how existing methods can be unified under a theoretical framework that effectively explains the success of hindsight relabeling in goal reaching."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that hindsight relabeling creates self-generated expert demonstrations that the policy can imitate, leading to improved sample efficiency.",
            "opinion": "The proposed framework relates goal reaching and imitation learning by recasting the goal reaching problem in the imitation learning framework through divergence minimization.",
            "innovation": "The primary innovation lies in the theoretical formulation of hindsight goal relabeling that emphasizes reward design and its connection to inverse reinforcement learning."
        },
        "method": {
            "method name": "Hindsight Divergence Minimization (HDM)",
            "method abbreviation": "HDM",
            "method definition": "HDM is a method that combines Q-learning with behavior cloning to optimize goal reaching in multi-goal reinforcement learning.",
            "method description": "The core of HDM involves optimizing a goal-conditioned policy by minimizing a combination of imitation learning and reward signals derived from Q-learning.",
            "method steps": "1. Define the goal-conditioned Q-function. 2. Utilize hindsight relabeling to create self-generated expert demonstrations. 3. Optimize the policy using divergence minimization techniques.",
            "principle": "The effectiveness of HDM is based on the principle that minimizing divergence between the policy distribution and the target distribution leads to improved learning outcomes in goal reaching."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in various multi-goal reinforcement learning environments, comparing the performance of HDM against baseline methods using different reward designs.",
            "evaluation method": "Performance was assessed by measuring success rates across multiple trials and analyzing the impact of reward designs on learning efficiency."
        },
        "conclusion": "The paper concludes that the proposed HDM framework provides significant improvements in learning performance for goal reaching tasks by effectively combining Q-learning and behavior cloning while emphasizing the importance of reward design.",
        "discussion": {
            "advantage": "HDM allows for better control over what actions to imitate, leading to improved learning efficiency and success rates in goal reaching.",
            "limitation": "The method may still encounter challenges in environments with complex dynamics or where the reward design does not align well with the learning objectives.",
            "future work": "Future research directions include further developing the framework to explicitly account for exploration strategies and optimizing reward designs for various environments."
        },
        "other info": {
            "additional details": {
                "framework significance": "The proposed framework bridges the gap between goal reaching and imitation learning, providing a unified understanding of existing methods.",
                "empirical results": "Empirical results demonstrate that HDM outperforms traditional methods in various multi-goal RL tasks, validating the theoretical insights."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Hindsight goal relabeling has become a foundational technique in multi-goal reinforcement learning (RL). Its importance lies in its ability to create self-generated expert demonstrations that improve sample efficiency."
        },
        {
            "section number": "1.2",
            "key information": "The paper addresses the relationship between hindsight goal relabeling and imitation learning, emphasizing the significance of these concepts in multi-goal reinforcement learning."
        },
        {
            "section number": "1.3",
            "key information": "The main objective of the paper is to understand the connection between hindsight goal relabeling and imitation learning, proposing a novel framework based on divergence minimization."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed involves understanding the relationship between hindsight goal relabeling and imitation learning in the context of multi-goal reinforcement learning."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Hindsight Divergence Minimization (HDM), combines Q-learning with behavior cloning to optimize goal reaching in multi-goal reinforcement learning."
        },
        {
            "section number": "3.2",
            "key information": "HDM allows for better control over what actions to imitate, leading to improved learning efficiency and success rates in goal reaching."
        },
        {
            "section number": "3.3",
            "key information": "The limitations of HDM include challenges in environments with complex dynamics or where the reward design does not align well with the learning objectives."
        },
        {
            "section number": "4.2",
            "key information": "The paper highlights that the effectiveness of HDM is based on minimizing divergence between the policy distribution and the target distribution, which leads to improved learning outcomes."
        },
        {
            "section number": "8.4",
            "key information": "Future research directions include further developing the HDM framework to explicitly account for exploration strategies and optimizing reward designs for various environments."
        }
    ],
    "similarity_score": 0.5722147101509669,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0858_fine-/papers/Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective.json"
}