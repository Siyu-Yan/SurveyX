{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.05135",
    "title": "Imputation of Time-varying Edge Flows in Graphs by Multilinear Kernel Regression and Manifold Learning",
    "abstract": "This paper extends the recently developed framework of multilinear kernel regression and imputation via manifold learning (MultiLKRIM) to impute time-varying edge flows in a graph. MultiL-KRIM uses simplicial-complex arguments and Hodge Laplacians to incorporate the graph topology, and exploits manifold-learning arguments to identify latent geometries within features which are modeled as a point-cloud around a smooth manifold embedded in a reproducing kernel Hilbert space (RKHS). Following the concept of tangent spaces to smooth manifolds, linear approximating patches are used to add a collaborative-filtering flavor to the point-cloud approximations. Together with matrix factorizations, MultiL-KRIM effects dimensionality reduction, and enables efficient computations, without any training data or additional information. Numerical tests on realnetwork time-varying edge flows demonstrate noticeable improvements of MultiL-KRIM over several state-of-the-art schemes. Index Terms\u2014 Imputation, kernel, manifold, graph, simplicial complex.",
    "bib_name": "nguyen2024imputationtimevaryingedgeflows",
    "md_text": "# IMPUTATION OF TIME-VARYING EDGE FLOWS IN GRAPHS BY MULTILINEAR KERNEL REGRESSION AND MANIFOLD LEARNING\nDuc Thien Nguyen Konstantinos Slavakis Tokyo Institute of Technology, Japan\nDepartment of Information and Communications Engineering Emails: {nguyen.t.au, slavakis.k.aa}@m.titech.ac.jp\n# ABSTRACT\nThis paper extends the recently developed framework of multilinear kernel regression and imputation via manifold learning (MultiLKRIM) to impute time-varying edge flows in a graph. MultiL-KRIM uses simplicial-complex arguments and Hodge Laplacians to incorporate the graph topology, and exploits manifold-learning arguments to identify latent geometries within features which are modeled as a point-cloud around a smooth manifold embedded in a reproducing kernel Hilbert space (RKHS). Following the concept of tangent spaces to smooth manifolds, linear approximating patches are used to add a collaborative-filtering flavor to the point-cloud approximations. Together with matrix factorizations, MultiL-KRIM effects dimensionality reduction, and enables efficient computations, without any training data or additional information. Numerical tests on realnetwork time-varying edge flows demonstrate noticeable improvements of MultiL-KRIM over several state-of-the-art schemes. Index Terms\u2014 Imputation, kernel, manifold, graph, simplicial complex.\narXiv:2409.05135v1\n# 1. INTRODUCTION\nGraph signal processing [1\u20133] plays a pivotal role in modern data analytics, because signals are often associated with entities that have physical or intangible inter-connections, which are usually modeled by graphs. Due to reasons such as user privacy, sensor fault, or resources conservation, signal observation/measurement is often incomplete (missing data), causing bias and errors in subsequent stages of learning [4, 5]. Much has been done to address the missingdata problem when observed signals are associated with the nodes of a graph, e.g., [3, 6\u20138]. Nevertheless, many systems observe signals over edges (edge flows), with the relative research gaining attention only recently [9\u201311]. Node-signal imputation techniques may not be successfully applied to the edge-flow imputation problem. For example, given a graph with observed signals over edges, one can consider its linegraph counterpart [12] whose nodes are edges of the original graph. In this way, the original problem is reduced to a node-signal imputation one, often solved by imposing spatial smoothness via a graph Laplacian matrix. However, this approach has been shown to produce subpar solutions as the smoothness assumption no longer holds for edge flows [13, 14]. Instead, it is more effective to assume that edge flows are almost divergence-free, i.e., the inbound at a node is approximately equal to the outbound, and curl-free, i.e., the total\nThis work was supported by JST SPRING, Japan Grant Number JPMJSP2106 and by the U.S. Air Force Office of Scientific Research, Agile Science of Test and Evaluation Program, under grants W911NF-23-S-0014 and W911NF-20-1-0283.\nThis work was supported by JST SPRING, Japan Grant Number JPMJSP2106 and by the U.S. Air Force Office of Scientific Research, Agile Science of Test and Evaluation Program, under grants W911NF-23-S-0014 and W911NF-20-1-0283.\nDimitris Pados\nFlorida Atlantic University, USA Center for Connected Autonomy and Artificial Intelligence Department of Electrical Engineering and Computer Science Email: dpados@fau.edu\nflow along a \u201ctriangle\u201d is close to zero [10, 11, 13\u201315]. Such assumptions are efficiently modeled by the theory of simplicial complexes and Hodge Laplacians on graphs [16]. These theories bring forth also simplicial convolutional filters, defined as matrix polynomials of Hodge Laplacians, which model the multi-hop shift of signals across simplicial complexes [17, 18]. While such theories are capable of spatial (graph topology) description, vector autoregression (VAR) is commonly used to model dependencies along time whenever edge flows are time varying [19]. However, VAR is negligent of the underlying graph topology. Recent efforts fuse simplicial convolutional filters into VAR, coined simplicial VAR (S-VAR), to take the graph topology into account, as well as to reduce the number of coefficients to be learned (dimensionality reduction) [20\u201322]. Notwithstanding, (S-)VAR relies by definition on past observations to predict/regress future ones, which may be problematic under the presence of missing data. Besides, VAR-based models are linear, and may thus fail to capture intricate data and feature dependencies. To address this issue, there have been studies incorporating simplicial complexes into (non-linear) neural networks [23\u201326], but typically, such models require large amounts of training data and intricate concatenation of non-linear layers. Further, it seems that the existing literature on simplicial-complex neural networks has not considered yet the problem of imputation of time-varying edge flows. This paper extends the recently developed method of multilinear kernel regression and imputation via manifold learning (MultiLKRIM) [3], and applies it to the problem of edge-flow imputation. MultiL-KRIM has been already applied to node-signal imputation [3]. Unlike (S-)VAR, where data depend directly on past observations, MultiL-KRIM assumes that missing entries can be estimated by a set of landmark points, extracted from measurements and located around a smooth manifold, embedded in an ambient reproducing kernel Hilbert space (RKHS). As such, functional approximation is enabled by the RKHS, effecting thus nonlinear data modeling, which is a lacking attribute in VAR models. Furthermore, while S-VAR applies simplicial convolutional filters for dimensionality reduction, MultiL-KRIM achieves this by matrix factorizations. Hodge Laplacians are incorporated in MultiL-KRIM\u2019s inverse problem to take account of the graph topology. In addition, MultiL-KRIM\u2019s manifold and tangent-space arguments model \u201clocality,\u201d captured in S-VAR by the number of hops in the simplicial convolutional filter. Unlike neural networks, MultiL-KRIM needs no training data and offers a more explainable approach through intuitive geometric arguments. Numerical tests on real networks show that MultiL-KRIM outperforms state-of-the-art methods.\n# 2. PRELIMINARIES\nA graph is denoted by G = (V, E), where V represents the set of nodes, and E \u2286V \u00d7 V is the set of edges. A k-simplex Sk \u2282V\ncomprises k + 1 distinct elements of V. For example, a 0-simplex is a node, an 1-simplex is an edge, and a 2-simplex is a triangle. A simplicial complex (SC) X is a finite collection of simplices with the inclusion property: for any simplex Sk \u2208X, if Sk\u22121 \u2282Sk, then Sk\u22121 \u2208X. An SC of order K, denoted as X K, contains at least one K-simplex [10, 16]. Hodge Laplacians [16] describe adjacencies in an SC. Specifically, in an SC X K, the incidence matrix Bk \u2208RNk\u22121\u00d7Nk, k \u22651 captures the adjacencies between (k\u22121)- and k-simplices, where Nk is the number of k-simplicies in X. For example, B1 is the node-toedge incidence matrix and B2 is the edge-to-triangle one. Incidence matrices satisfy the boundary condition BkBk+1 = 0, \u2200k \u22651. Hodge Laplacians are defined as\n(1)\n\uf8f4 \uf8f3 where L0 is the well-known graph Laplacian matrix, which describes node-adjacencies via shared edges. Hodge Laplacian L1 defines the adjacencies between edges via shared nodes by the lowerLaplacian L1,l := B\u22ba 1B1, and via shared triangles by the upperLaplacian L1,u := B2B\u22ba 2. Likewise, L2 denotes the connection between triangles through common edges. As this paper concerns signals over edges, L1 will be the main focus. \u201cSimplicial signals\u201d are abstracted as functions which map a k-simplex to a real number. In case of time-varying signals, an \u201cedge-flow signal\u201d at time t \u2208{1, 2, . . . , T} is denoted by yt = [y1t, y2t, . . . , yN1t]\u22ba\u2208RN1. These vectors comprise the data matrix Y = [y1, y2, . . . , yT ] \u2208RN1\u00d7T . To account for missing entries, let the index set of observed entries \u2126:= {(i, t) \u2208 {1, . . . , N1} \u00d7 {1, . . . , T} | yit \u0338= +\u221e}, where +\u221edenotes a missing entry, and define the linear sampling mapping S\u2126: (R \u222a {+\u221e})N1\u00d7T \u2192RN1\u00d7T : Y \ufffd\u2192S\u2126(Y), which operates entrywisely as follows: [S\u2126(Y)]it := [Y]it, if (i, t) \u2208\u2126, while [S\u2126(Y)]it := 0, if (i, t) /\u2208\u2126. An edge-flow imputation framework solves the following inverse problem\nminX\u2208RN1\u00d7T L(X) + R(X) s.to S\u2126(Y) = S\u2126(X) and other constraints,\n(2)\nwhere L(\u00b7) is the data-fit loss and R(\u00b7) is the regularizer for structural priors, while constraint S\u2126(Y) = S\u2126(X) preserves the consistency of the observed entries of Y. For example, FlowSSL [14] sets L(X) := 0 and R(X) := (\u03bbl/2)\u2225B1X\u22252 F + (\u03bbu/2)\u2225B\u22ba 2X\u22252 F, which imposes the conservation of the flows at the nodes and cyclic flows along edges of triangles. More recent studies incorporate the Hodge Laplacians into VAR, a widely used tool for time-varying signals modeling. In particular, S-VAR [20\u201322] approximates each snapshot yt \u2248\ufffdP p=1 Hp(L1)yt\u2212p, where {Hp(L1)}P p=1 are simplicial convolutional filters [18], whose role is to capture the multihop dependencies between edges. As a result, S-VAR sets L(X) := \ufffdT t=1\u2225xt \u2212Ft\u03b2t\u22252 2, where {\u03b2t}T t=1 are learnable parameters, and {Ft}T t=1 are \u201cshifted signals\u201d of {yt}T t=1.\n# 3. PROPOSED METHOD\n# 3.1. A short recap of MultiL-KRIM\nMultiL-KRIM [3] is a generic data-imputation-by-regression framework which combines matrix factorization, kernel methods and manifold learning. The approach can be shortly described as follows.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9075/9075bbcd-9ede-4059-9c8a-331d30e19f33.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: A \u201ccollaborative-filtering\u201d approach: Points {\u03c6(lkj )}3 j=1, which lie into or close to the unknown-to-the-user manifold M \u2282H , collaborate affinely to approximate \u03c6(\u02c7\u00b5t). All affine combinations of {\u03c6(lkj )}3 j=1 define the approximating \u201clinear patch\u201d (gray-colored plane), which mimics the concept of a tangent space to M [27].</div>\nAssuming that data S\u2126(Y) are faithful, a subset of S\u2126(Y), called navigator/pilot data and denoted as \u02c7Yf := [\u02c7yf 1, . . . , \u02c7yf Nnav] \u2208 R\u03bd\u00d7Nnav, are selected by the user; for example, all of S\u2126(Y) can be selected. As the cardinality of the point-cloud {\u02c7yf t}Nnav t=1 grows for large datasets, a subset {lk}Nl k=1, coined landmark/representative points, with Nl \u2264Nnav, is selected from {\u02c7yf t}Nnav t=1 . Several ways to form navigator data and landmark points have been suggested [3]. Let \u02c7L := [l1, l2, . . . , lNl ] \u2208R\u03bd\u00d7Nl . To facilitate nonlinear approximation, a feature map \u03c6(\u00b7) takes vector lk to \u03c6(lk) in an RKHS H , equipped with a reproducing kernel \u03ba(\u00b7, \u00b7) : R\u03bd \u00d7 R\u03bd \u2192R, with well-documented merits in approximation theory [28]. To this end, \u03c6(l) := \u03ba(l, \u00b7) \u2208H , \u2200l \u2208R\u03bd. For convenience, let \u03a6(\u02c7L) := [\u03c6(l1), . . . , \u03c6(lNl )]. Consequently, K := \u03a6(\u02c7L)\u22ba\u03a6(\u02c7L) is an Nl \u00d7 Nl kernel matrix whose (k, k\u2032)th entry is equal to \u27e8\u03c6(lk) | \u03c6(lk\u2032)\u27e9H = \u03ba(lk, lk\u2032), where \u22bastands for vector/matrix transposition, and \u27e8\u00b7 | \u00b7\u27e9H for the inner product of the RKHS H . The flow xit through the edge i at time t is approximated as\nwhere fi(\u00b7) : R\u03bd \u2192R is an unknown non-linear function in the RKHS functional space H , \u02c7\u00b5t is an unknown vector in R\u03bd, and the latter part of (3) is because of the reproducing property of H [28]. Motivated by the representer theorem, fi is assumed to belong to the linear span of {\u03c6(lk)}Nl k=1, i.e., there exists ui := [ui1, . . . , uiNl ]\u22ba\u2208RNl s.t. fi = \ufffdNl k=1 uik\u03c6(lk) = \u03a6(L)ui. Regarding \u03c6(\u02c7\u00b5t), the concept of tangent spaces to smooth manifolds [27] offers also a \u201ccollaborative-filtering\u201d flavor to the design: it is assumed that \u03c6(\u02c7\u00b5t) lies into or close to M and is approximated by only a few members of {\u03c6(lk)}Nl k=1 which collaborate affinely, i.e., there exists a sparse vector vt \u2208RNl s.t. \u03c6(\u02c7\u00b5t) = \u03a6(L)vt, under the affine constraint 1\u22ba Nl vt = 1, where 1Nl is the Nl \u00d7 1 all-one vector; cf., Figure 1. So far, the entry-wise estimation [X]it \u2248 fi(\u02c7\u00b5t) = \u27e8fi | \u03c6(\u02c7\u00b5t)\u27e9H = \u27e8\u03a6(L)ui | \u03a6(L)vt\u27e9H = u\u22ba i Kvt . To offer compact notations, if U := [u1, . . . , uN1]\u22ba\u2208RN1\u00d7Nl and V := [v1, . . . , vT ] \u2208RNl \u00d7T , then data are modeled as\n(4)\nwhere V is a sparse matrix satisfying 1\u22ba Nl V = 1\u22ba T . To ease the excessive cost caused by large N1 and Nl, [3] employs multilinear factorization U = U1U2 \u00b7 \u00b7 \u00b7 UQ, which has been shown to be highly efficient, especially for high-dimensional dynamic imaging data [3].\n# 3.2. Extended MultiL-KRIM for edge-flow imputation\nIn large networks with long time series, sizes N1, T, and Nl can cause massive burden. Unlike S-VAR [20\u201322], which reduces the model size of VAR by simplicial convolutional filters, and [3], which\n(5)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3283/3283676d-41a6-4142-b37e-b8a68e884888.png\" style=\"width: 50%;\"></div>\nwhere the inner matrix dimensions d, r \u226aNl are user-defined. Based on findings in [3], factorizing U further, by more than two factors, does not notably improve the accuracy, so U = U1U2 and V = V1V2 are used here. It can be easily checked that the affine constraint of V in (4) still holds after enforcing 1\u22ba Nl V1 = 1\u22ba r and 1\u22ba rV2 = 1\u22ba T . Ultimately, following the common assumption that edge flows are approximately divergence-free and curl-free (cf., Section 2), the extended MultiL-KRIM inverse problem for edge-flow imputation is formed as\n(6a)\n(6b)\n(6c)\nThe loss function is non-convex, and to guarantee convergence to a critical point, the parallel successive-convex-approximation (SCA) framework of [29] is utilized. The algorithm is summarized in Algorithm 1, where the following tuple of estimates \u2200n \u2208N, \u2200k \u2208 {0, 1},\nis recursively updated via the following convex sub-tasks which can be solved in parallel at every iteration n:\n\u02c6U(n+1/2) 1 := arg min U1 1 2\u2225\u02c6X(n) \u2212U1 \u02c6U(n) 2 K \u02c6V(n) 1 \u02c6V(n) 2 \u22252 F\nAlgorithm 1 Solving MultiL-KRIM\u2019s inverse problem\nInput:\nOutput: Limit point \u02c6\nO(\u2217) of sequence ( \u02c6\nO(n))n\u2208N.\n1: Fix \u02c6\nO(0), \u03b30 \u2208(0, 1], and \u03b6 \u2208(0, 1).\n2: while n \u22650 do\n3:\nAvailable are \u02c6\nO(n) (7).\n4:\n\u03b3n+1 := \u03b3n(1 \u2212\u03b6\u03b3n).\n5:\nSolve in parallel the convex sub-tasks (8).\n6:\n\u02c6\nO(n+1) := \u03b3n+1 \u02c6\nO(n+1/2) + (1 \u2212\u03b3n+1) \u02c6\nO(n).\n7:\nSet n \u2190n + 1 and go to step 2.\n8: end while\n# Input:\n\u02c6V(n+1/2) 2 := arg min V2 1 2\u2225\u02c6X(n) \u2212\u02c6U(n) 1 \u02c6U(n) 2 KV(n) 1 V2\u22252 F\n+ \u03bb1\u2225V2\u22251 + \u03c4V 2 \u2225V2 \u2212\u02c6V(n) 2 \u22252 F s.to 1\u22ba rV2 = 1\u22ba T ,\n(8e)\nwhere the user-defined \u03c4X, \u03c4U, \u03c4V , \u03bb1, \u03bb2, \u03bbl, \u03bbu \u2208R++. Subtasks (8d) and (8e) are composite convex minimization tasks under affine constraints, and can be thus solved iteratively by [30], while (8a), (8b), and (8c) have closed-form solutions.\n# 3.3. Computational complexity\nThe number of unknown parameters in (5) is (N1 + Nl)d + (Nl + T)r. Per iteration n in Algorithm 1, the computational complexity of sub-task (8b) is O(N1d2 + d3), and that of (8c) is O(d3 + N 3 l ). Meanwhile, the complexity of sub-task (8d) is O(K1N 2 l d), and that of (8e) is O(K2d2T), where K1 and K2 are the numbers of iterations that [30] takes to solve (8d) and (8e), respectively.\n# 4. NUMERICAL TESTS\nMultiL-KRIM is tested on the traffic flows in Sioux Falls transportation network [31] and on the water flows in the Cherry Hills water network [32]. MultiL-KRIM is compared against the state-of-the-art edge-flow imputation methods FlowSSL [14] and S-VAR [20\u201322]. As a baseline for matrix-factorization techniques, the multi-layer matrix factorization (MMF) [33] is implemented. Here, MMF [33] solves the same inverse problem (6) as MultiL-KRIM, but with the kernel matrix K = INl in (5). In other words, MMF [33] is a blind matrix-factorization method, where no data geometry/patterns are explored and exploited, as in Figure 1, and no functional approximation via RKHSs is employed. The evaluation metric is the mean absolute error (MAE) (lower is better), defined as MAE := \u2225X \u2212Y\u22251/(N1 \u00b7 T), where Y is the fully-sampled data, X gathers all reconstructed flows, and \u2225\u00b7\u22251 is the \u21131-norm. All methods are finely tuned to achieve their lowest MAE. Reported metric values are mean values of 30 independent runs. Source codes of FlowSSL [14] and S-VAR [20\u201322] were made publicly available by the authors. Source code for MultiL-KRIM and MMF was written in Julia [34]. All tests were run on an 8-core Intel(R) i7-11700 2.50GHz CPU with 32GB RAM. With sampling ratio s \u2208{0.1, 0.2, 0.3, 0.4, 0.5}, signals of \u2308N1 \u00b7s\u2309edges are sampled per time instant t, where \u2308\u00b7\u2309is the ceiling function. This sampling pattern suggests that the number of observations is consistent along time. Navigator data \u02c7Ynav are formed by the snapshots, that is, columns of S\u2126(Y) after removing all the unobserved entries. Landmark points are selected by the greedy maxmin-distance strategy [35], based on Euclidean distances among navigator data. Hyperparameters \u03c4X = \u03c4U = \u03c4V = 2 in (8) to ensure\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e53/1e53a807-d922-469e-b7e4-33ad3a10d70e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Sioux Falls traffic network</div>\nFig. 2: Mean MAE value curves in log scale (the lower the better) w.r.t. the groud truth signals vs. sampling ratios. The shaded areas indicate a range of one standard deviation above and below the mean.\n<div style=\"text-align: center;\">Fig. 2: Mean MAE value curves in log scale (the lower the better) w.r.t. the groud truth signals vs. sampling ratios. The shaded areas indicate a range of one standard deviation above and below the mean.</div>\nne standard deviation above and below the mean.\nDatasets \\ Methods\nS-VAR\nMMF\nMultiL-KRIM\nCherry Hills\n30,000\n6,400\n6,400\nSioux Falls\n15,000\n2,190\n2,190\n<div style=\"text-align: center;\">Table 1: Number of unknown parameters.</div>\nstability of the algorithm. Meanwhile, regularization hyperparameters \u03bb1, \u03bb2, \u03bbl, and \u03bbu in (8), the number of landmark points Nl, and inner dimensions d, r in (5) are identified by grid-search.\n# 4.1. Cherry Hills water network\nCherry Hills water network consists of 36 nodes (0-simplices), 40 pipes (1-simplices), and 2 triangles (2-simplices) [32]. Time-varying water flow over the network is generated using the EPANET software [36]. Flows are measured hourly in cubic meter per hour (m3/h) in a span of 300 hours. The size of data Y is 40 \u00d7 300, and the number of landmark points is Nl = 150. Figure 2a shows MAE values of competing methods across different sampling ratios. MultiLKRIM outperforms the competing methods at every sampling ratio. It significantly outperforms S-VAR [20\u201322], especially at higher sampling ratios. FlowSSL [14], which solely depends on the Hodge Laplacians-based regularizers, scores much higher MAE values than the other methods. MultiL-KRIM outperforms also the blind matrixfactorization MMF [33]. Figure 3 visualizes the coefficient matrices learned by Algorithm 1. In particular, Figures 3c and 3d draw the heatmaps of \u02c6V(\u2217) 1 and \u02c6V(\u2217) 2 of Algorithm 1, respectively. The heatmaps show that \u02c6V(\u2217) 2 and especially \u02c6V(\u2217) 1 are sparse. The portion of entries whose abso-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/22ce/22ce65c9-34fd-4999-982c-ce16c3e60c2c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) \u02c6V(\u2217) 2</div>\nFig. 3: Visualization of coefficient matrices \u02c6U(\u2217) 1 , \u02c6U(\u2217) 2 , \u02c6V(\u2217) 1 and \u02c6V(\u2217) 2 , output by Algorithm 1 for Cherry Hills water network data, at sampling ratio s = 0.5. Heat maps show the entry-wise absolute values of the matrices, rescaled to be in [0, 1] range. lute value is larger than 0.001 is 9.1% in \u02c6V(\u2217) 1 and 89.6% in \u02c6V(\u2217) 2 . In contrast, by Figures 3a and 3b, \u02c6U(\u2217) 1 , \u02c6U(\u2217) 2 are dense, where the percentage of entries whose absolute value is larger than 0.001 is 98.8% in \u02c6U(\u2217) 1 and 99.8% in \u02c6U(\u2217) 2 .\n<div style=\"text-align: center;\">Fig. 3: Visualization of coefficient matrices \u02c6U(\u2217) 1 , \u02c6U(\u2217) 2 , \u02c6V(\u2217) 1 and \u02c6V(\u2217) 2 , output by Algorithm 1 for Cherry Hills water network data, at sampling ratio s = 0.5. Heat maps show the entry-wise absolute values of the matrices, rescaled to be in [0, 1] range. lute value is larger than 0.001 is 9.1% in \u02c6V(\u2217) 1 and 89.6% in \u02c6V(\u2217) 2 . In contrast, by Figures 3a and 3b, \u02c6U(\u2217) 1 , \u02c6U(\u2217) 2 are dense, where the percentage of entries whose absolute value is larger than 0.001 is 98.8% in \u02c6U(\u2217) 1 and 99.8% in \u02c6U(\u2217) 2 .</div>\n# 4.2. Sioux Falls transportation network\nThe Sioux Falls transportation network has 24 nodes (0-simplices), 38 edges (1-simplices), and 2 triangles (2-simplices) [31]. The synthetic time-varying traffic flow is generated as in [20, 22], so that it contains both divergence flows and cyclic flows. The measurement unit is neglected, simply denoted as \u201cunits.\u201d The size of data Y is 38 \u00d7 300, while Nl = 50. Figure 2b plots MAE curves against sampling ratios. MultiL-KRIM continues to outperform the competing methods, with notable gaps when compared with S-VAR [20\u2013 22] and FlowSSL [14]. Meanwhile, the gap between MultiL-KRIM and MMF [33] accentuates at lower sampling ratios. FlowSSL [14] scores again the highest MAE values. With regards to dimensionality reduction, Table 1 compares the number of unknown parameters in S-VAR [20\u201322], MMF [33], and MultiL-KRIM. Although the number of parameters of MultiLKRIM are 21.3% (Cherry Hills) and 14.6% (Sioux Falls) of those of S-VAR, MultiL-KRIM still outperforms in both datasets.\n# 5. CONCLUSIONS\nThis paper applied the recently developed MultiL-KRIM, a multilinear kernel-regression-imputation and manifold-learning framework, into the time-varying edge-flow imputation problem. MultiLKRIM approximated data via simple geometric arguments and facilitated computation by low-rank matrix factorization, without the need for training data. To further realize the common priors of edge flows, MultiL-KRIM\u2019s inverse problem incorporated also the graph\u2019s Hodge Laplacians. Numerical tests on real water and traffic networks showed that MultiL-KRIM offers better performance than several state-of-the-art methods.\n[1] A. Ortega, P. Frossard, J. Kova\u02c7cevi\u00b4c, J. M. Moura, and P. Vandergheynst, \u201cGraph signal processing: Overview, challenges, and applications,\u201d Proceedings of the IEEE, vol. 106, no. 5, pp. 808\u2013828, 2018. [2] G. Leus, A. G. Marques, J. M. Moura, A. Ortega, and D. I. Shuman, \u201cGraph signal processing: History, development, impact, and outlook,\u201d IEEE Signal Processing Magazine, vol. 40, no. 4, pp. 49\u2013 60, 2023. [3] D. T. Nguyen and K. Slavakis, \u201cMultilinear kernel regression and imputation via manifold learning,\u201d IEEE Open Journal of Signal Processing, pp. 1\u201315, 2024. DOI: 10.1109/OJSP.2024.3444707. [4] S. Chen, A. Sandryhaila, J. M. Moura, and J. Kova\u02c7cevi\u00b4c, \u201cSignal recovery on graphs: Variation minimization,\u201d IEEE Trans. Signal Processing, vol. 63, no. 17, pp. 4609\u20134624, 2015. [5] S. Chen, R. Varma, A. Singh, and J. Kova\u02c7cevi\u00b4c, \u201cSignal recovery on graphs: Fundamental limits of sampling strategies,\u201d IEEE Transactions on Signal and Information Processing over Networks, vol. 2, no. 4, pp. 539\u2013554, 2016. [6] K. Qiu, X. Mao, X. Shen, X. Wang, T. Li, and Y. Gu, \u201cTime-varying graph signal reconstruction,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 870\u2013883, 2017. [7] D. Romero, V. N. Ioannidis, and G. B. Giannakis, \u201cKernel-based reconstruction of space-time functions on dynamic graphs,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 856\u2013 869, 2017. [8] J. A. Castro-Correa, J. H. Giraldo, M. Badiey, and F. D. Malliaros, \u201cGegenbauer graph neural networks for time-varying signal reconstruction,\u201d IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u2013, 2024. DOI: 10.1109/TNNLS.2024.3381069. [9] F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania, J.-G. Young, and G. Petri, \u201cNetworks beyond pairwise interactions: Structure and dynamics,\u201d Physics Reports, vol. 874, pp. 1\u201392, 2020. 10] S. Barbarossa and S. Sardellitti, \u201cTopological signal processing over simplicial complexes,\u201d IEEE Trans. Signal Processing, vol. 68, pp. 2992\u20133007, 2020. 11] M. T. Schaub, Y. Zhu, J.-B. Seby, T. M. Roddenberry, and S. Segarra, \u201cSignal processing on higher-order networks: Livin\u2019on the edge... and beyond,\u201d Signal Processing, vol. 187, p. 108 149, 2021. 12] T. S. Evans and R. Lambiotte, \u201cLine graphs, link partitions, and overlapping communities,\u201d Physical Review E\u2014Statistical, Nonlinear, and Soft Matter Physics, vol. 80, no. 1, p. 016 105, 2009. 13] M. T. Schaub and S. Segarra, \u201cFlow smoothing and denoising: Graph signal processing in the edge-space,\u201d in IEEE Global Conference on Signal and Information Processing (GlobalSIP), 2018, pp. 735\u2013739. 14] J. Jia, M. T. Schaub, S. Segarra, and A. R. Benson, \u201cGraph-based semi-supervised & active learning for edge flows,\u201d in SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 761\u2013771. 15] M. Yang and E. Isufi, \u201cSimplicial trend filtering,\u201d in 2022 56th Asilomar Conference on Signals, Systems, and Computers, IEEE, 2022, pp. 930\u2013934. 16] L.-H. Lim, \u201cHodge Laplacians on graphs,\u201d SIAM Review, vol. 62, no. 3, pp. 685\u2013715, 2020.\n[17] E. Isufi and M. Yang, \u201cConvolutional filtering in simplicial complexes,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 5578\u20135582. [18] M. Yang, E. Isufi, M. T. Schaub, and G. Leus, \u201cSimplicial convolutional filters,\u201d IEEE Trans. Signal Processing, vol. 70, pp. 4633\u20134648, 2022. [19] R. Money, J. Krishnan, B. Beferull-Lozano, and E. Isufi, \u201cOnline edge flow imputation on networks,\u201d IEEE Signal Processing Letters, vol. 30, pp. 115\u2013119, 2022. [20] J. Krishnan, R. Money, B. Beferull-Lozano, and E. Isufi, \u201cSimplicial vector autoregressive model for streaming edge flows,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135. [21] J. Krishnan, R. Money, B. Beferull-Lozano, and E. Isufi, \u201cSimplicial vector autoregressive models,\u201d Authorea Preprints, 2024. [22] R. Money, J. Krishnan, B. Beferull-Lozano, and E. Isufi, \u201cEvolution backcasting of edge flows from partial observations using simplicial vector autoregressive models,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024, pp. 9516\u2013 9520. [23] S. Ebli, M. Defferrard, and G. Spreemann, \u201cSimplicial neural networks,\u201d in TDA & Beyond, 2020. [24] T. M. Roddenberry, N. Glaze, and S. Segarra, \u201cPrincipled simplicial neural networks for trajectory prediction,\u201d in International Conference on Machine Learning, PMLR, 2021, pp. 9020\u20139029. [25] M. Yang, E. Isufi, and G. Leus, \u201cSimplicial convolutional neural networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 8847\u20138851. [26] H. Wu, A. Yip, J. Long, J. Zhang, and M. K. Ng, \u201cSimplicial complex neural networks,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 1, pp. 561\u2013575, 2024. DOI: 10.1109/ TPAMI.2023.3323624. [27] J. W. Robbin and D. A. Salamon, Introduction to Differential Geometry. Berlin: Springer, 2022. [28] N. Aronszajn, \u201cTheory of reproducing kernels,\u201d Trans. American Mathematical Society, vol. 68, no. 3, pp. 337\u2013404, 1950. [29] F. Facchinei, G. Scutari, and S. Sagratella, \u201cParallel selective algorithms for nonconvex big data optimization,\u201d IEEE Trans. Signal Processing, vol. 63, no. 7, pp. 1874\u20131889, 2015. [30] K. Slavakis and I. Yamada, \u201cFej\u00b4er-monotone hybrid steepest descent method for affinely constrained and composite convex minimization tasks,\u201d Optimization, vol. 67, no. 11, pp. 1963\u20132001, 2018. [31] L. A. Rossman, R. M. Clark, and W. M. Grayman, \u201cModeling chlorine residuals in drinking-water distribution systems,\u201d Journal of Environmental Engineering, vol. 120, no. 4, pp. 803\u2013820, 1994. [32] L. J. Leblanc, \u201cAn algorithm for the discrete network design problem,\u201d Transportation Science, vol. 9, no. 3, pp. 183\u2013199, 1975. [33] A. Cichocki and R. Zdunek, \u201cMultilayer nonnegative matrix factorization using projected gradient approaches,\u201d International Journal of Neural Systems, vol. 17, no. 06, pp. 431\u2013446, 2007. [34] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah, \u201cJulia: A fresh approach to numerical computing,\u201d SIAM Review, vol. 59, no. 1, pp. 65\u201398, 2017. [35] V. De Silva and J. B. Tenenbaum, \u201cSparse multidimensional scaling using landmark points,\u201d Stanford University, Tech. Rep., 2004. [36] L. A. Rossman, \u201cAn overview of epanet version 3.0,\u201d Water Distribution Systems Analysis, pp. 14\u201318, 2010.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the problem of imputing time-varying edge flows in graphs, which is critical due to the incompleteness of signal observations caused by issues like user privacy and sensor faults. Previous methods focused on node-signal imputation, which do not effectively apply to edge flows, necessitating a new approach that can account for the unique characteristics of edge flows.",
        "problem": {
            "definition": "The problem is to estimate missing edge flow signals in a graph where observed signals are incomplete due to various factors, leading to biased and erroneous data.",
            "key obstacle": "Existing node-signal imputation techniques fail to capture the nearly divergence-free and curl-free nature of edge flows, which prevents them from providing accurate solutions in this context."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that missing edge flow signals can be approximated using landmark points on a smooth manifold embedded in a reproducing kernel Hilbert space (RKHS).",
            "opinion": "The proposed idea, MultiL-KRIM, utilizes multilinear kernel regression and manifold learning to effectively estimate missing edge flows without requiring training data.",
            "innovation": "MultiL-KRIM innovates by incorporating graph topology through Hodge Laplacians and utilizing manifold learning for functional approximation, contrasting with existing linear models like S-VAR that rely on past observations."
        },
        "method": {
            "method name": "Multilinear Kernel Regression and Imputation via Manifold Learning",
            "method abbreviation": "MultiL-KRIM",
            "method definition": "MultiL-KRIM is a framework that combines matrix factorization, kernel methods, and manifold learning to impute missing edge flow signals in graphs.",
            "method description": "The method approximates missing data using geometric arguments and matrix factorization techniques in a high-dimensional space.",
            "method steps": "1. Select navigator data from observed signals. 2. Identify landmark points from navigator data. 3. Apply kernel methods to approximate missing signals using the selected landmark points. 4. Solve the inverse problem using convex optimization techniques.",
            "principle": "The effectiveness of MultiL-KRIM lies in its ability to model the underlying manifold structure of the data, allowing for accurate imputation of missing entries based on local geometric relationships."
        },
        "experiments": {
            "evaluation setting": "The method was evaluated on real-world datasets from the Sioux Falls transportation network and the Cherry Hills water network, comparing its performance against state-of-the-art methods such as FlowSSL and S-VAR.",
            "evaluation method": "Performance was assessed using the mean absolute error (MAE) metric, calculated over multiple independent runs to ensure robustness of the results."
        },
        "conclusion": "MultiL-KRIM successfully addresses the edge-flow imputation problem, demonstrating superior performance compared to existing methods through its innovative use of manifold learning and kernel regression techniques, validated by numerical tests on real networks.",
        "discussion": {
            "advantage": "The key advantages of MultiL-KRIM include its ability to operate without training data, its incorporation of graph topology through Hodge Laplacians, and its explainability through geometric arguments.",
            "limitation": "One limitation of the method is its non-convex loss function, which may lead to challenges in convergence and finding optimal solutions.",
            "future work": "Future research could explore enhancements in the algorithm's convergence properties and investigate its applicability to other types of graph-based data imputation problems."
        },
        "other info": [
            {
                "Funding": "This work was supported by JST SPRING, Japan Grant Number JPMJSP2106 and by the U.S. Air Force Office of Scientific Research."
            },
            {
                "Datasets": {
                    "Sioux Falls": "Traffic flows in Sioux Falls transportation network.",
                    "Cherry Hills": "Water flows in Cherry Hills water network."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The importance of semi-supervised learning is highlighted by addressing the problem of imputing time-varying edge flows in graphs, which is critical due to the incompleteness of signal observations caused by issues like user privacy and sensor faults."
        },
        {
            "section number": "1.3",
            "key information": "The main goal of the paper is to estimate missing edge flow signals in a graph where observed signals are incomplete, necessitating a new approach that accounts for the unique characteristics of edge flows."
        },
        {
            "section number": "2.1",
            "key information": "Key terms defined include edge flow signals, graph topology, and manifold learning, which are essential to understanding the proposed MultiL-KRIM framework."
        },
        {
            "section number": "3.5",
            "key information": "MultiL-KRIM innovates by combining multilinear kernel regression and manifold learning to effectively estimate missing edge flows without requiring training data, contrasting with existing linear models."
        },
        {
            "section number": "4.1",
            "key information": "The critical role of data labeling is reflected in the necessity to accurately estimate edge flows, as existing node-signal imputation techniques fail to capture the unique nature of edge flows."
        },
        {
            "section number": "7.1",
            "key information": "Challenges related to scalability and computational complexity are evident in the non-convex loss function of the MultiL-KRIM method, which may lead to convergence issues."
        },
        {
            "section number": "8",
            "key information": "The conclusion emphasizes that MultiL-KRIM successfully addresses the edge-flow imputation problem, demonstrating superior performance compared to existing methods through its innovative use of manifold learning and kernel regression techniques."
        }
    ],
    "similarity_score": 0.5741828731746542,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Imputation of Time-varying Edge Flows in Graphs by Multilinear Kernel Regression and Manifold Learning.json"
}