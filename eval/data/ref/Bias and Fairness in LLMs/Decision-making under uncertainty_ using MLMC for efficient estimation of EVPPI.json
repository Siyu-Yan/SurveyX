{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1708.05531",
    "title": "Decision-making under uncertainty: using MLMC for efficient estimation of EVPPI",
    "abstract": "In this paper we develop a very efficient approach to the Monte Carlo estimation of the expected value of partial perfect information (EVPPI) that measures the average benefit of knowing the value of a subset of uncertain parameters involved in a decision model. The calculation of EVPPI is inherently a nested expectation problem, with an outer expectation with respect to one random variable $X$ and an inner conditional expectation with respect to the other random variable $Y$. We tackle this problem by using a Multilevel Monte Carlo (MLMC) method (Giles 2008) in which the number of inner samples for $Y$ increases geometrically with level, so that the accuracy of estimating the inner conditional expectation improves and the cost also increases with level. We construct an antithetic MLMC estimator and provide sufficient assumptions on a decision model under which the antithetic property of the estimator is well exploited, and consequently a root-mean-square accuracy of $\\varepsilon$ can be achieved at a cost of $O(\\varepsilon^{-2})$. Numerical results confirm the considerable computational savings compared to the standard, nested Monte Carlo method for some simple testcases and a more realistic medical application.",
    "bib_name": "giles2018decisionmakinguncertaintyusingmlmc",
    "md_text": "# Decision-making under uncertainty: using MLMC for efficient estimation of EVPPI\n19 Mar 2018\n# Michael B. Giles\u2217, Takashi Goda\u2020\nMarch 20, 2018\nIn this paper we develop a very efficient approach to the Monte Carlo estimation of the expected value of partial perfect information (EVPPI) that measures the average benefit of knowing the value of a subset of uncertain parameters involved in a decision model. The calculation of EVPPI is inherently a nested expectation problem, with an outer expectation with respect to one random variable X and an inner conditional expectation with respect to the other random variable Y . We tackle this problem by using a Multilevel Monte Carlo (MLMC) method (Giles 2008) in which the number of inner samples for Y increases geometrically with level, so that the accuracy of estimating the inner conditional expectation improves and the cost also increases with level. We construct an antithetic MLMC estimator and provide sufficient assumptions on a decision model under which the antithetic property of the estimator is well exploited, and consequently a root-mean-square accuracy of \u03b5 can be achieved at a cost of O(\u03b5\u22122). Numerical results confirm the considerable computational savings compared to the standard, nested Monte Carlo method for some simple testcases and a more realistic medical application.\n# 1 Introduction\nThe motivating applications for this research come from two apparently quite different fields, the funding of medical research and the exploration and exploitation of oil and gas reservoirs. The common element in both cases is decision making under a large degree of uncertainty. In the medical case (Ades et al. 2004, Brennan et al. 2007) let X and Y represent independent random variables representing the uncertainty in the effectiveness of different medical treatments. In the absence of any knowledge of X or Y , then given a finite set of possible treatments D, the optimal choice dopt is the one which maximises E [fd(X, Y )] where fd(X, Y ) represents some measure of the patient outcome, such as QALY\u2019s (quality-adjusted life-year), measured on a monetary scale with a larger value being better. Thus, with no\n\u2217Mathematical Institute, University of Oxford, Oxford, United Kingdom, OX2 6GG mike.giles@maths.ox.ac.uk \u2020School of Engineering, University of Tokyo, Tokyo 113-8656, Japan, goda@frcer.t.u tokyo.ac.jp\nmax d\u2208D E [fd(X, Y )] .\nOn the other hand, given perfect information on X and Y , through carrying out some new medical research, the best treatment choice maximises fd(X, Y ), giving the overall average outcome\nIn the intermediate situation, if X is known but not Y , then the best treatment has average outcome value\nEVPI, the expected value of perfect information, is the difference\nEVPPI = E \ufffd max d E [fd(X, Y ) | X] \ufffd \u2212max d E[fd(X, Y )].\nEVPPI represents the benefit, on average, of knowing the value of X. If the value of X represents the information arising from a proposed piece of medical research, then one can compare the cost of the research to the benefits which arise from the information obtained. In the oil and gas reservoir scenario (Bratvold et al. 2009, Nakayasu et al. 2016), there are also decisions to be made, such as whether or not to drill additional exploratory wells. There is huge uncertainty in various aspects of an oil reservoir, its dimensions, the oil and gas reserves it contains, the rock porosity, etc. An additional well will yield information which will reduce the uncertainty and increase, on average, the amount of oil and gas which will eventually be extracted. However, there is an additional cost in drilling one more well, and the EVPPI will help determine whether or not it is worth it. The calculation of EVPPI is a nested expectation problem, with an outer expectation over X and an inner conditional expectation over Y . In this paper, we choose to focus on the estimation of the difference\nEVPI can be estimated directly using standard Monte Carlo methods with  dependent samples of (X, Y )\n1 N N \ufffd n=1 max d fd(X(n), Y (n)) \u2212max d 1 N N \ufffd n=1 fd(X(n), Y (n)).\nAssuming each computation fd(X, Y ) can be performed with unit cost, EVPI can be estimated with root-mean-square accuracy \u03b5 by using N = O(\u03b5\u22122) samples (X(n), Y (n)) at a total cost which is O(\u03b5\u22122). On the other hand, estimating the difference EVPI \u2212EVPPI using standard, nested Monte Carlo methods requires N outer samples of X and M inner samples of Y , giving\nAs shown in the next section, in order to estimate EVPI \u2212EVPPI with rootmean-square accuracy \u03b5 by this estimator, we need N = O(\u03b5\u22122) and M = O(\u03b5\u22121/\u03b1) samples for outer and inner expectations, respectively. Here \u03b1 > 0 denotes the order of convergence of the bias and is typically between 1/2 and 1. Therefore, the computational complexity will be at least O(\u03b5\u22123), and increase up to O(\u03b5\u22124) in the worst case. The aim of this paper is to develop an efficient approach to this nested expectation problem, i.e., the estimation of EVPI\u2212EVPPI, by using a Multilevel Monte Carlo (MLMC) method (Giles 2015). MLMC estimators have been used previously for nested expectations of the slightly different form E[f(E[Y |X])] by Haji-Ali (2012) and Giles (2015) for cases in which f is twice-differentiable, and by Bujok et al. (2015) for a case in which f is continuous and piecewise linear. Current research (Giles and Haji-Ali 2018) is also looking at the case in which f is a discontinuous indicator (Heaviside) function. Building on this prior MLMC research, we introduce an antithetic MLMC estimator for EVPI \u2212EVPPI in the next section, and then in Section 3, we provide sufficient assumptions on fd\u2019s such that the antithetic property of the estimator is well exploited, and by building upon the basic MLMC theorem (Theorem 1), the estimator is proven to achieve the optimal computational complexity O(\u03b5\u22122) (Theorem 3). Numerical experiments in Section 4 confirm the importance of the assumptions made in our theoretical analysis, and also the considerable computational savings compared to the standard, nested Monte Carlo method not only for some simple testcases but also for a more realistic medical application.\n# 2 MLMC method\n# 2.1 Basic MLMC theory\nThe MLMC method was introduced by Heinrich (2001) for parametric integration, and by Giles (2008) for the estimation of the expectations arising from SDEs. It was subsequently extended to SPDEs (e.g. Cliffe et al. 2011), stochastic reaction networks (Anderson and Higham 2012), and nested simulation (Bujok et al. 2015, Haji-Ali 2012). For an extensive review of MLMC methods, see the review by Giles (2015). Here we give a brief overview of the MLMC method. The problem we are interested in is to estimate E[P] efficiently for a random output variable P which cannot be sampled exactly. Given a sequence of random variables P0, P1, . . . which approximate P with increasing accuracy but also with increasing cost,\nThe key idea behind the MLMC method is to independently estimate each of the quantities on the r.h.s. of (1) instead of directly estimating the l.h.s., which is the standard Monte Carlo approach. For the same underlying stochastic sample, P\u2113and P\u2113\u22121 could be well correlated each other, and the variance of the correction P\u2113\u2212P\u2113\u22121 is expected to get smaller as the level \u2113increases. Thus, in order to estimate each of the quantities on the r.h.s. of (1) with the same accuracy, the necessary number of samples for the finest levels becomes much smaller than that for the coarsest levels, resulting in a significant reduction of the total computational cost as compared to the standard Monte Carlo method. This observation leads to the following theorem (Giles 2015):\nThe key idea behind the MLMC method is to independently estimate each of the quantities on the r.h.s. of (1) instead of directly estimating the l.h.s., which is the standard Monte Carlo approach. For the same underlying stochastic sample, P\u2113and P\u2113\u22121 could be well correlated each other, and the variance of the correction P\u2113\u2212P\u2113\u22121 is expected to get smaller as the level \u2113increases. Thus, in order to estimate each of the quantities on the r.h.s. of (1) with the same accuracy, the necessary number of samples for the finest levels becomes much smaller than that for the coarsest levels, resulting in a significant reduction of the total computational cost as compared to the standard Monte Carlo method. This observation leads to the following theorem (Giles 2015): Theorem 1. Let P denote a random variable, and let P\u2113denote the corresponding level \u2113numerical approximation. If there exist independent random variables Z\u2113with expected cost C\u2113and variance V\u2113, and positive constants \u03b1, \u03b2, \u03b3, c1, c2, c3 such that \u03b1 \u22651 2 min(\u03b2, \u03b3) and\nthen there exists a positive constant c4 such that for any \u03b5<e\u22121 there are values L and N\u2113for which the multilevel estimator\nwith a computational complexity C with bound\n\uf8f3 Remark 1. In the case where the condition V\u2113\u2264c22\u2212\u03b2\u2113can be replaced by E[Z2 \u2113] \u2264c22\u2212\u03b2\u2113, H\u00a8older\u2019s inequality gives E[Z\u2113] \u2264 \ufffd E[Z2 \u2113] \ufffd1/2 \u2264\u221ac22\u2212\u03b2\u2113/2.\n\uf8f3 Remark 1. In the case where the condition V\u2113\u2264c22\u2212\u03b2\u2113can be replaced b E[Z2 \u2113] \u2264c22\u2212\u03b2\u2113, H\u00a8older\u2019s inequality gives\nE[Z\u2113] \u2264 \ufffd E[Z2 \u2113] \ufffd1/2 \u2264\u221ac22\u2212\u03b2\u2113/2.\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/061e/061e545d-9833-475e-bd74-c0ad53496e32.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Compared this bound to the condition |E[P\u2113\u2212P]| \u2264c12\u2212\u03b1\u2113, we have \u03b1 \u2265\u03b2/2 and so the assumption \u03b1 \u22651 2 min(\u03b2, \u03b3) is simplified into \u03b1 \u2265\u03b3/2.</div>\nCompared this bound to the condition |E[P\u2113\u2212P]| \u2264c12\u2212\u03b1\u2113, we have \u03b1 \u2265\u03b2/2 and so the assumption \u03b1 \u22651 2 min(\u03b2, \u03b3) is simplified into \u03b1 \u2265\u03b3/2.\n \u2265  \u2265 As far as possible, we try to develop MLMC estimators which are in the first regime, with \u03b2 > \u03b3, so that the total cost is O(\u03b5\u22122). This corresponds to O(\u03b5\u22122) samples each with an average O(1) cost, and it means that most of the computational cost is incurred on the coarsest levels. When the application is in this regime, Rhee and Glynn (2015) have a technique in which they randomise the selection of the level \u2113to obtain a method which is unbiased but has a finite variance and average cost per sample. Nevertheless, in any regime, Theorem 1 compares favourably with the complexity bound for the standard Monte Carlo method which directly estimates the l.h.s. of (1) based on N Monte Carlo samples of PL for a fixed L:\nIn addition to the conditions given in Theorem 1, assume V := sup\u2113V[P\u2113] < \u221e. For a given accuracy \u03b5, let us choose N = \u23082V \u03b5\u22122\u2309and L = \u2308log2( \u221a 2c1\u03b5\u22121)/\u03b1\u2309, so that the variance and the bias of the estimator are bounded simultaneously:\nand\nwhich ensures the mean-square-error bound of \u02c6Z\u2032\nE \ufffd ( \u02c6Z\u2032 \u2212E[P])2\ufffd = V[ \u02c6Z\u2032] + (E[P \u2212PL])2 \u2264\u03b52.\nThen there exists a positive constant c5 such that the expected cost of \u02c6Z\u2032 is bounded by\nThen there exists a positive constant c5 such that the expected cost of \u02c6Z\u2032 i bounded by\nNCL \u2264 \ufffd 2V \u03b5\u22122 + 1 \ufffd c32\u03b3L \u2264 \ufffd 2V \u03b5\u22122 + 1 \ufffd c3 \ufffd\u221a 2c12\u03b1\u03b5\u22121\ufffd\u03b3/\u03b1 \u2264c5\u03b5\u22122\u2212\u03b3/\u03b1.\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd In general, it seems hard to improve the exponent 2 + \u03b3/\u03b1 of \u03b5\u22121. Therefore, the multilevel estimator always has an asymptotically better complexity bound than the standard Monte Carlo estimator.\n# 2.2 MLMC estimator for EVPPI\nIn view of the previous subsection, for the estimation of the difference EVPI \u2212 EVPPI let us define a random output variable P by\nwith the underlying stochastic variable X. Obviously P is nothing but the inner conditional expectation of EVPI \u2212EVPPI, and the problem we tackle in this paper is rephrased into an efficient estimation of E[P]. A sequence of random variables P0, P1, . . . is defined by\nP\u2113= 1 2\u2113 2\u2113 \ufffd i=1 max d fd(X, Y (i)) \u2212max d 1 2\u2113 2\u2113 \ufffd i=1 fd(X, Y (i)) =: max d fd \u2113\u2212max d fd \u2113\nwhere maxd fd \u2113and fd \u2113represent averages over 2\u2113independent values of Y (i) for a randomly chosen X, respectively. That is to say, P\u2113simply denotes the standard Monte Carlo estimator based on 2\u2113samples for the inner conditional expectation of EVPI \u2212EVPPI, so that the sequence P0, P1, . . . approximate P with increasing accuracy but also with increasing cost. Namely we have\n# EVPI \u2212EVPPI = E[P] = lim \u2113\u2192\u221eE[P\u2113].\nAs discussed above, in order to achieve a given accuracy \u03b5, the standard, nested Monte Carlo method chooses N = O(\u03b5\u22122) and M = O(2L) = O(\u03b5\u22121/\u03b1), and so the computational complexity is O(\u03b5\u22122\u22121/\u03b1). Using the MLMC method, this can be reduced significantly. Following the ideas of Bujok et al. (2015), Giles (2015), Haji-Ali (2012), we use an \u201cantithetic\u201d MLMC estimator\nin which\nhere, for a randomly chosen X, \u2022 fd (a) is an average of fd(X, Y ) over 2\u2113\u22121 independent samples for Y ; \u2022 fd (b) is an average over a second independent set of 2\u2113\u22121 samples; \u2022 fd is an average over the combined set of 2\u2113inner samples.\nIt is straightforward to see that \u03b3 = 1 and E[Z\u2113] = E[P\u2113\u2212P\u2113\u22121] for \u2113> 0. Here we consider Z0 = P0 \u22610, so that the sum of the multilevel estimator over \u2113 starts from \u2113= 1. Note that we have the antithetic property 1 2(fd (a) + fd (b)) \u2212fd = 0, and therefore Z\u2113= 0 if the same decision d maximises each of the terms in its definition. This is the key advantage of the antithetic estimator, compared to the alternative fd (a) \u2212fd.\nRemark 2. It is straightforward to extend the antithetic MLMC approach to estimate EVPI. The difference is that with EVPI all of the underlying random variables X and Y are inner variables; non are outer variables leading to a conditional expectation. Such an MLMC estimator for the maximum of an unconditional expectation has been introduced by Blanchet and Glynn (2015). As discussed in the introduction, however, EVPI can be estimated with O(\u03b52) complexity by using standard Monte Carlo methods already, so that the benefit is that one could use a randomisation technique by Rhee and Glynn (2015) to obtain an unbiased estimator, which might be marginal in the current setting.\n# 3 MLMC variance analysis\nWe first show that the MLMC estimator achieves the nearly optimal complexity of O(\u03b5\u22122(log \u03b5)2) under a quite mild assumption. Theorem 2. If E [V[fd(X, Y ) | X]] is finite for all d,\nV [Z\u2113] \u2264E \ufffd |Z\u2113|2\ufffd \u22646|D| 2\u2113 \ufffd d E [V[fd(X, Y ) | X]] .\n\ufffd\ufffd \ufffd\ufffd Hence, by defining Fd(X) = E [fd(X, Y ) | X], we obtain\n\ufffd\ufffd\ufffd (2)\nE \ufffd |fd \u2212Fd|2\ufffd = E \ufffd E \ufffd |fd \u2212Fd|2 | X \ufffd\ufffd = 1 2\u2113E [V [fd(X, Y ) | X]]  Similarly\n\ufffd \ufffd \ufffd Hence, V [Z\u2113] is bounded by\n# which completes the proof.\nThe theorem shows that the parameters for the MLMC theorem are \u03b2 = 1, and in view of Remark 1, \u03b1 \u22651/2. Since \u03b3 = 1 by the definition of Z\u2113, the MLMC estimator is in the second regime, with \u03b2 = \u03b3, so that the total cost is O(\u03b5\u22122(log \u03b5)2). This compares favourably with the cost of O(\u03b5\u22122\u22121/\u03b1) for the standard Monte Carlo estimator, where the exponent increases up to 4 in the worst case. In the proof of the theorem, the antithetic property of the estimator, i.e., 1 2(fd (a) + fd (b)) \u2212fd = 0, is not exploited. In fact, the same upper bound on the variance can be obtained even for the alternative fd (a) \u2212fd. In what follows, we prove a stronger result on the variance under somewhat demanding assumptions to exploit the antithetic structure of Z\u2113. In fact, the MLMC variance can be analysed by following the approach used by Giles and Szpruch (2014, Theorem 5.2). Define\nFd(X) = E [fd(X, Y )|X] , dopt(X) = arg max d Fd(X)\nso the domain for X is divided into a number of regions in which the optimal decision dopt(X) is unique, with a dividing decision manifold K on which dopt(X) is not uniquely-defined. Again note that 1 2(fd (a) + fd (b)) \u2212fd = 0, and therefore Z\u2113= 0 if the same decision d maximises each of the terms in its definition. When \u2113is large and so there are many samples, fd (a), fd (b), fd will all be close to Fd(X), and therefore it is highly likely that Z\u2113= 0 unless X is very close to K at which there is more than one optimal decision. This idea leads to an improved theorem on the MLMC variance, but we first need to make three assumptions.\nAssumption 1. E [|fd(X, Y )|p] is finite for all p \u22652. Comment: this enables us to bound the difference between fd (a), fd (b), fd an Fd(X).\nAssumption 2. There exists a constant c0 > 0 such that for all 0 < \u01eb < 1\n\ufffd min x\u2208K \u2225X \u2212x\u2225\u2264\u01eb \ufffd \u2264c0\u01eb. Comment: this bounds the probability of X being close to the decision manifold K. Assumption 3. There exist constants c1, c2 > 0 such that if X /\u2208K, then max d Fd(X) \u2212 max d\u0338=dopt(X) Fd(X) > min \ufffd c1, c2 min x\u2208K \u2225X \u2212x\u2225 \ufffd . Comment: on K itself there are at least 2 decisions d1, d2 which yield the same optimal value Fd(X); this assumption ensures at least a linear divergence between the values as X moves away from K. Theorem 3. If Assumptions 1-3 are satisfied, and Z\u2113is as defined previously for level \u2113, then for any \u03b4 > 0 V [Z\u2113] = o(2\u2212(3/2\u2212\u03b4)\u2113), E [Z\u2113] = o(2\u2212(1\u2212\u03b4)\u2113). Comment: a similar O(N \u22123/2) convergence rate for the variance is proved in Theorem 2.3 in (Bujok et al. 2015) for a different nested simulation application. Before going into the detailed proof of the theorem, we give a heuristic explanation on the variance analysis below: \u2022 Due to Assumption 1 and Lemma 1 shown below, fd \u2212Fd = O(2\u2212\u2113/2); \u2022 Due to Assumption 2, there is an O(2\u2212\u2113/2) probability of X being within distance O(2\u2212\u2113/2) from the decision manifold K, in which case Z\u2113= O(2\u2212\u2113/2); \u2022 If X is further away from K, Assumption 3 ensures that there is a clear separation between different decision values, and hence the antithetic property of the estimator can be exploited well to give Z\u2113= 0 with high probability; \u2022 This results in\n  Comment: this bounds the probability of X being close to the decision manifold K.\n  Comment: this bounds the probability of X being close to the decision manifold K.\nComment: on K itself there are at least 2 decisions d1, d2 which yield the same optimal value Fd(X); this assumption ensures at least a linear divergence between the values as X moves away from K.\nComment: on K itself there are at least 2 decisions d1, d2 which yield the same optimal value Fd(X); this assumption ensures at least a linear divergence between the values as X moves away from K. Theorem 3. If Assumptions 1-3 are satisfied, and Z\u2113is as defined previously for level \u2113, then for any \u03b4 > 0\nComment: a similar O(N \u22123/2) convergence rate for the variance is proved in Theorem 2.3 in (Bujok et al. 2015) for a different nested simulation application. Before going into the detailed proof of the theorem, we give a heuristic explanation on the variance analysis below:\nE[Z\u2113] = O(2\u2212\u2113/2) \u00d7 O(2\u2212\u2113/2) = O(2\u2212\u2113), E[Z2 \u2113] = O(2\u2212\u2113/2) \u00d7 (O(2\u2212\u2113/2))2 = O(2\u22123\u2113/2),\nso that we have \u03b1 \u22481 and \u03b2 \u22483/2.\nTo prepare for the proof of the main theorem, we first need a result concerning the deviation of an average of N values from the expected mean. Suppose X is a real random variable with zero mean, and let XN be an average of N i.i.d. samples Xn, n = 1, 2, . . ., N. For p = 2, we have E[XN 2] = N \u22121E[X2], and hence P[|XN| > c] \u2264E[X2]/(c2N). For larger values of p for which E[|X|p] is finite, we have the following lemma: Lemma 1. For p \u22652, if E[|X|p] is finite then there exists a constant Cp, depending only on p, such that E[|XN|p] \u2264CpN \u2212p/2E [|X|p] , P[|XN| > c] \u2264CpE[|X|p]/(c2N)p/2.\nLemma 1. For p \u22652, if E[|X|p] is finite then there exists a constant C depending only on p, such that\nProof. The discrete Burkholder-Davis-Gundy inequality (Burkholder et al. 1972) gives us\nwhere Cp is a constant depending only on p. The second result follows immediately from the Markov inequality. Proof of Theorem 3. The analysis follows the approach used by Giles et al. (2009) and Giles and Szpruch (2014, Theorem 5.2). For a particular value of \u03b4, we define \u01eb = 2\u2212(1/2\u2212\u03b4/2)\u2113, and consider the events\nwhere c2 is as defined in Assumption 3. Using 1A to indicate the indicator function for event A, and Ac to denote the complement of A, we have\nLooking at the first of the two terms on the r.h.s. of (3), then H\u00a8older\u2019s inequality gives\nfor any p, q \u22651, with p\u22121 + q\u22121 = 1. Now, P(A) \u2264c0 \u01eb due to Assumption 1, and\nP(B) \u2264 \ufffd d \ufffd P(|fd (a) \u2212Fd| \u22651 2\u01eb) + P(|fd (b) \u2212Fd| \u22651 2\u01eb) + P(|fd \u2212Fd| \u22651 2\u01eb) \ufffd .\n(3)\nDue to Lemma 1,\nSimilar bounds exists for P(|fd (a) \u2212Fd| \u2265 1 2\u01eb) and P(|fd (b) \u2212Fd| \u2265 1 2\u01eb). We can take m to be sufficiently large so that 1 2m \u22121\u2212\u03b4 2 m > 1\u2212\u03b4 2 and hence P(B) = o(2\u2212(1\u2212\u03b4)\u2113/2). Then, q can be chosen sufficiently close to 1 so that (P(A) + P(B))1/q = o(2\u2212(1/2\u2212\u03b4)\u2113). Applying Jensen\u2019s inequality to (2) twice, we obtain\n \ufffd \ufffd \ufffd \u2264(2|D|)2p\u22121 \ufffd \ufffd 1 2|fd (a) \u2212Fd|2p + 1 2|fd (b) \u2212Fd|2p + |fd \u2212Fd|2p\ufffd .\n  It follows from Lemma 1 that\nE[ |fd \u2212Fd|2p] = E \ufffd E[|fd \u2212Fd|2p | X] \ufffd\n\ufffd | \u2212| | \ufffd \u2264C2p2\u2212p\u2113E \ufffd E[|fd(X, Y ) \u2212E[fd(X, Y ) | X]|2p | X] \ufffd = C2p2\u2212p\u2113E[|fd(X, Y ) \u2212E[fd(X, Y )]|2p],\nso Assumption 1 implies that E[|fd \u2212Fd|2p] = O(2\u2212p\u2113), with similar bounds for fd (a) and fd (b). Hence,\nand therefore the first term on the r.h.s. of (3) has bound o(2\u2212(3/2\u2212\u03b4)\u2113). We now consider the second term on the r.h.s. of (3). For any sample in Ac \u2229Bc, we have min x\u2208K \u2225X \u2212x\u2225\u2265\u01eb, and |fd (a) \u2212Fd| < 1 2c2\u01eb, |fd (b) \u2212Fd| < 1 2c2\u01eb, |fd \u2212Fd| < 1 2c2\u01eb, for all d. For a particular outer sample X, if d \u0338= dopt(X) then using Assumption 3 we have\nIf \u2113is sufficiently large so that c2\u01eb < c1, then fdopt \u2212fd > 0 and hence dopt = arg maxd fd. The same argument applies to fd (a) opt \u2212fd (a) and fd (b) opt \u2212fd (b), so\nthe conclusion is that in all three cases, dopt is the decision which maximis fd (a), fd (b) and fd, and therefore\n1 2(max d fd (a) + max d fd (b)) \u2212max d fd = 1 2(fd (a) opt + fd (b) opt) \u2212fdopt = 0.\nHence, for sufficiently large \u2113, the second term is zero, which concludes the proof for the bound on V[Z\u2113] and the bound on E[Z\u2113] is obtained similarly.\nThe conclusion from the theorem is that the parameters for the MLMC theorem are \u03b2 \u22483/2, \u03b1\u22481, and \u03b3 =1, giving the optimal complexity of O(\u03b5\u22122). Again, this compares favourably with the cost of O(\u03b5\u22123) for the standard Monte Carlo estimator.\n# 4 Numerical results\n# 4.1 Simple test cases\nTo validate the importance of the assumptions made in the variance analysis, several simple examples are tested here. Let X and Y be independent univariate standard normal random variables, and let us consider two-treatment decision problems with f1(X, Y ) = 0 and either 1. f2(X, Y ) = X + Y , 2. f2(X, Y ) = X3 + Y , or\n3. f2(X, Y ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 X + Y + 1, X < \u22121, Y, \u22121 \u2264X \u22641, X + Y \u22121, X > 1.\n\uf8f4 \uf8f3 It is easy to check that this simple test case with the first choice of f2 satisfies a of Assumptions 1-3, while the other cases with the second and third choices  f2 do not. With the second choice of f2, we have F1(X) = 0 and F2(X) = X so that K = {0} \u2282R and\nmax d Fd(X) \u2212 max d\u0338=dopt(X) Fd(X) = |X|3,\nwhich implies that there exist no constants c1, c2 > 0 such that Assumption 3 is satisfied. For the third choice of f2, we have K = [\u22121, 1] \u2282R whose probability measure is not zero. Hence, by considering the limiting situation \u01eb \u21920 in Assumption 2, we see that there exists no constant c0 > 0 such that Assumption 2 is satisfied. The results for the first choice of f2 are shown in Figure 1. The left top plot shows the behaviours of the variances of both P\u2113and Z\u2113, where the variances are estimated by using N = 2 \u00d7 105 random samples at each level. Note that the logarithm of the empirical variance in base 2 versus the level is plotted here. The slope of the line for Z\u2113is \u22121.43, indicating that V[Z\u2113] = O(2\u22121.43\u2113). This result is in good agreement with Theorem 3 which holds for decision models satisfying Assumptions 1-3.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d0f0/d0f08c4b-d581-416f-818f-1bbfa1d49f7e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: MLMC results for simple test case with the first choice of f2.</div>\nThe middle top plot shows the behaviours of the estimated mean values of both P\u2113and Z\u2113. The slope of the line for Z\u2113is approximately \u22121, which implies that E[Z\u2113] = O(2\u2212\u2113). This is again in good agreement with Theorem 3. The right top plot shows the behaviour of the estimated kurtosis of Z\u2113. The way in which the kurtosis increases with the level also confirms that the MLMC corrections are increasingly dominated by a few rare samples yielding Z\u2113\u0338= 0, corresponding to outer samples X which are close to the decision manifold K across which the optimal decision dopt changes. Using the implementation due to Giles (2015, Algorithm 1), the maximum level L and the computational costs N\u2113for levels \u2113= 1, . . . , L, required for the combined multilevel estimator to achieve an MSE less than \u03b52, are estimated. Each line in the left bottom plot shows the values of N\u2113, \u2113= 1, . . . , L, for a particular value of \u03b5. As expected, the number of samples varies with the level such that many more samples are allocated on the coarsest levels, which is in good agreement with the optimal allocation of computational effort given by N\u2113\u221d\u03b5\u22122\ufffd V\u2113/C\u2113\u221d\u03b5\u221222\u2212(\u03b2+\u03b3)\u2113/2 (Giles 2015). It is also shown here that, as the value of \u03b5 decreases, the maximum level L increases to ensure the weak convergence |E [P \u2212PL] | \u2264\u03b5/ \u221a 2. The middle bottom plot shows the behaviour of the total computational cost\nto achieve an MSE less than \u03b52. Since it is expected from the MLMC theorem that \u03b52C is independent of \u03b5, we plot \u03b52C versus \u03b5 here. Indeed, it can be seen\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/80ee/80eec9b7-7212-4531-8e6b-e84542b3b125.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: MLMC results for simple test case with the second choice of f2.</div>\nthat \u03b52C is only slightly dependent on \u03b5, indicating that the MLMC estimator gives the optimal complexity of O(\u03b5\u22122). This result compares favourably with the result for the standard (in this case, nested) Monte Carlo method. The superiority of the MLMC method becomes more evident as the desired accuracy \u03b5 decreases. For instance, for \u03b5 = 10\u22124, the MLMC method is more than 50 times more efficient. Let us move on to the second and third choices of f2. Since these test cases do not satisfy one of Assumptions 1-3, Theorem 3 does not apply and it is expected from Theorem 2 that the MLMC estimator achieves the nearly optimal complexity of O(\u03b5\u22122(log \u03b5)2). The results for the second and third choices of f2 are shown in Figures 2 and 3, respectively. For the second choice of f2, it is seen from the first two top plots that the slopes of the lines for the variance and the mean value of Z\u2113are \u22121.12 and \u22120.64, respectively, which are slightly better than the values \u22121 and \u22120.5 which are to be expected from the theory. In the right top plot, the kurtosis increases with the level but not so significantly as compared to the first test case. Because of a smaller value of \u03b1, we can observe in the left bottom plot that the maximum level to ensure the weak convergence becomes large. Still, the superiority of the MLMC method over the standard Monte Carlo method is prominent. For \u03b5 = 10\u22124, the MLMC method is approximately 3000 times more efficient. Similar results are also obtained for the third choice of f2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f0e/1f0e6eef-34d2-4b89-b000-88b56531334a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: MLMC results for simple test case with the third choice of f2.</div>\n# 4.2 Medical decision model\nTo demonstrate the practical usefulness of the MLMC estimator, the medical decision model introduced in Brennan et al. (2007) is tested. Let X \u222aY = (X1, . . . , X19) with each univariate random variable Xj following the normal distribution with mean \u00b5j and standard deviation \u03c3j independently except that X5, X7, X14, X16 are pairwise correlated with a correlation coefficient \u03c1 = 0.6. The values for \u00b5j and \u03c3j and the medical meaning of Xj are listed in Table 1. The problem to be tested is a two-treatment decision problem with\nf1(X, Y ) = \u03bb (X5X6X7 + X8X9X10) \u2212(X1 + X2X3X4), and f2(X, Y ) = \u03bb (X14X15X16 + X17X18X19) \u2212(X11 + X12X13X4),\nwhere \u03bb denotes the monetary valuation of health and is set to 104 (\u00a3). In what follows, we call this decision model the BKOC test case, named after the authors of Brennan et al. (2007). The results for the BKOC test case with X = (X5, X14) are shown in Figure 4. From the first two top plots we see that the slopes of the lines for the variance and the mean value of Z\u2113are \u22121.352 and \u22120.89, respectively, indicating that the MLMC estimator is in the first regime, with \u03b2 > \u03b3. The behaviour of the kurtosis of Z\u2113, shown in the right top plot, is quite similar to that observed for the simple test case with the first choice of f2. As expected, most of the computational cost is actually incurred on the coarsest levels, and the MLMC method gives savings of factor more than 100 as compared to the standard Monte Carlo method for the desired accuracy \u03b5 = 0.1. As shown in Figure 5 and 6, respectively, both of the results for the BKOC\n<div style=\"text-align: center;\">Table 1: Variables in the BKOC test case (Table 2 in Brennan et al. (2007)).</div>\n: Variables in the BKOC test case (Table 2 in Brennan et al.\nvariable\n\u00b5j\n\u03c3j\nmeaning\nX1\n1000\n1\nCost of drug (\u00a3)\nX2\n0.1\n0.02\nProbability of admissions\nX3\n5.2\n1.0\nDays in hospital\nX4\n400\n200\nCost per day (\u00a3)\nX5\n0.7\n0.1\nProbability of responding\nX6\n0.3\n0.1\nUtility change if response\nX7\n3.0\n0.5\nDuration of response (years)\nX8\n0.25\n0.1\nProbability of side effects\nX9\n-0.1\n0.02\nChange in utility if side effect\nX10\n0.5\n0.2\nDuration of side effect (years)\nX11\n1500\n1\nCost of drug (\u00a3)\nX12\n0.08\n0.02\nProbability of admissions\nX13\n6.1\n1.0\nDays in hospital\nX14\n0.8\n0.1\nProbability of responding\nX15\n0.3\n0.05\nUtility change if response\nX16\n3.0\n1.0\nDuration of response (years)\nX17\n0.2\n0.05\nProbability of side effects\nX18\n-0.1\n0.02\nChange in utility if side effect\nX19\n0.5\n0.2\nDuration of side effect (years)\ntest case with X = (X5, X6, X14, X15) and X = (X7, X16) are quite similar to the case with X = (X5, X14), and the MLMC method gives savings of factor up to 100. In order to achieve an MSE less than 1, the MLMC method needs the total computational costs of C = 4.1\u00d7107, 3.0\u00d7107, 2.2\u00d7107 for the three respective cases, giving the estimates of the difference EVPI \u2212EVPPI as 799, 206, and 509. The total computational costs for the standard Monte Carlo method are found to be approximately 10 times larger for all cases. The standard Monte Carlo method using 107 random samples of (X, Y ) yields the estimate of EVPI as 1047. Thus, the EVPPI values for the three cases are estimated as 248, 841 and 538.\n# 5 Conclusions\nIn this paper we have developed a Multilevel Monte Carlo method for the estimation of the expected value of partial perfect information, EVPPI, which is one of the most demanding nested expectation applications. The essential difficulty in the theoretical analysis lies in how to deal with the maximum of an unconditional expectation. We provide a set of assumptions on a decision model to exploit the antithetic property of the estimator, and then numerical analysis proves that a root-mean-square accuracy of \u03b5 can be achieved at a computational cost which is O(\u03b5\u22122), and this is also supported by numerical experiments. As we already announced in (Giles et al. 2017), the MLMC estimator introduced in this paper works quite well for real medical application which measures the cost-effectiveness of novel oral anticoagulants in atrial fibrillation. The details\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9655/965538d6-92f0-4d65-a478-32f032d21c25.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: MLMC results for the BKOC test case with X = (X5, X14).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9d6/e9d6d32d-25ea-4800-aabd-d2afe8b10eed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: MLMC results for the BKOC test case with X = (X5, X6, X14, X15).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1136/11364abc-f4e4-49ad-a450-d2bc307d43fa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: MLMC results for the BKOC test case with X = (X7, X16).</div>\non this application shall be summarised in the near future. Future research will address the following topics: \u2022 an extension to handle input distributions which are defined empirically, such as through the use of MCMC methods to sample from a Bayesian posterior distribution; \u2022 the use of quasi-random numbers in place of pseudo-random numbers, which leads to the Multilevel Quasi-Monte Carlo method which is capable of additional substantial savings (Giles and Waterhouse 2009); \u2022 the use of an adaptive number of inner samples, following the ideas of\n\u2022 an extension to handle input distributions which are defined empirically, such as through the use of MCMC methods to sample from a Bayesian posterior distribution; \u2022 the use of quasi-random numbers in place of pseudo-random numbers, which leads to the Multilevel Quasi-Monte Carlo method which is capable of additional substantial savings (Giles and Waterhouse 2009);\n\u2022 the use of an adaptive number of inner samples, following the ideas of Broadie et al. (2011), since it is only the outer samples which are near the decision manifold K which require great accuracy for the inner conditional expectation.\n# Acknowledgements\nThe authors would like to thank Dr. Howard Thom of the University of Bristol for useful discussions and comments. The research of T. Goda was supported by JSPS Grant-in-Aid for Young Scientists (No. 15K20964) and Arai Science and Technology Foundation.\nAdes AE, Lu G, Claxton K (2004) Expected value of sample information calculations in medical decision modeling. Medical Decision Making 24:207\u2013227. Anderson D, Higham D (2012) Multi-level Monte Carlo for continuous time Markov chains, with applications in biochemical kinetics. SIAM Multiscale Modelling and Simulation 10(1):146\u2013179. Blanchet J, Glynn P (2015) Unbiased Monte Carlo for optimization and functions of expectations via multi-level randomization. Proceedings of the 2015 Winter Simulation Conference, 3656\u20133667, IEEE. Bratvold R, Bickel J, Lohne H (2009) Value of information in the oil and gas industry: past, present, and future. SPE Reservoir Evaluation and Engineering 12:630\u2013 638. Brennan A, Kharroubi S, O\u2019Hagan A, Chilcott J (2007) Calculating partial expected value of perfect information via Monte Carlo sampling algorithms. Medical Decision Making 27:448\u2013470. Broadie M, Du Y, Moallemi C (2011) Efficient risk estimation via nested sequential simulation. Management Science 57(6):1172\u20131194. Bujok K, Hambly B, Reisinger C (2015) Multilevel simulation of functionals of Bernoulli random variables with application to basket credit derivatives. Methodology and Computing in Applied Probability 17(3):579\u2013604. Burkholder D, Davis B, Gundy R (1972) Integral inequalities for convex functions of operators on martingales. Proc. Sixth Berkeley Symposium Math. Statist. Prob., Vol II, 223\u2013240 (University of California Press, Berkeley). Cliffe K, Giles M, Scheichl R, Teckentrup A (2011) Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients. Computing and Visualization in Science 14(1):3\u201315. Giles M (2008) Multilevel Monte Carlo path simulation. Operations Research 56(3):607\u2013617. Giles M (2015) Multilevel Monte Carlo methods. Acta Numerica 24:259\u2013328. Giles M, Goda T, Thom H, Fang W, Wang Z (2017) MLMC for estimation of expected value of partial perfect information. Presentation at International Conference on Monte Carlo Methods and Applications. Giles M, Higham D, Mao X (2009) Analysing multilevel Monte Carlo for options with non-globally Lipschitz payoff. Finance and Stochastics 13(3):403\u2013413. Giles M, Haji-Ali AL (2018) Multilevel nested simulation for efficient risk estimation. arXiv pre-print 1802.05016. Giles M, Szpruch L (2014) Antithetic multilevel Monte Carlo estimation for multidimensional SDEs without L\u00b4evy area simulation. Annals of Applied Probability 24(4):1585\u20131620. Giles M, Waterhouse B (2009) Multilevel quasi-Monte Carlo path simulation. Advanced Financial Modelling, 165\u2013181, Radon Series on Computational and Applied Mathematics (De Gruyter). Haji-Ali AL (2012) Pedestrian flow in the mean-field limit. MSc thesis, KAUST, URL http://stochastic_numerics.kaust.edu.sa/Documents/publications/ AbdulLateef%20Haji%20Ali%20_Thesis.pdf. Heinrich S (2001) Multilevel Monte Carlo methods. Multigrid Methods, volume 2179 of Lecture Notes in Computer Science, 58\u201367 (Springer). Nakayasu M, Goda T, Tanaka K, Sato K (2016) Evaluating the value of single-point data in heterogeneous reservoirs with the expectation maximization. SPE Economics and Management 8:1\u201310.\nRhee CH, Glynn P (2015) Unbiased estimation with square root convergence for SDE models. Operations Research 63(5):1026\u20131043.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of estimating the expected value of partial perfect information (EVPPI) in decision-making under uncertainty, particularly in medical research and oil and gas reservoir exploration. Previous methods for calculating EVPPI involve nested expectations, which are computationally intensive. The authors propose a new approach using Multilevel Monte Carlo (MLMC) methods to improve efficiency.",
        "problem": {
            "definition": "The problem involves estimating the EVPPI, which quantifies the average benefit of knowing the value of uncertain parameters in a decision model, specifically in scenarios where decisions are made under uncertainty.",
            "key obstacle": "The primary challenge is the nested expectation problem, which requires a large number of samples for accurate estimation, leading to high computational costs with traditional Monte Carlo methods."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for efficient estimation techniques that can handle the complexity of nested expectations in decision-making models.",
            "opinion": "The proposed MLMC method aims to provide a more efficient way to estimate EVPPI by reducing the computational cost associated with traditional methods.",
            "innovation": "The key innovation is the introduction of an antithetic MLMC estimator that significantly reduces the number of required samples, achieving optimal computational complexity of O(\u03b5\u22122) compared to O(\u03b5\u22123) for standard methods."
        },
        "method": {
            "method name": "Multilevel Monte Carlo (MLMC) method",
            "method abbreviation": "MLMC",
            "method definition": "The MLMC method estimates expectations efficiently by using a hierarchy of approximations with increasing accuracy and cost, leveraging the correlation between levels to reduce variance.",
            "method description": "The core of the method involves estimating nested expectations through a sequence of correlated random variables, allowing for significant computational savings.",
            "method steps": [
                "Define a sequence of random variables that approximate the target expectation.",
                "Estimate each quantity independently rather than directly estimating the overall expectation.",
                "Utilize the antithetic property to enhance accuracy and reduce variance."
            ],
            "principle": "The effectiveness of the MLMC method lies in its ability to exploit the correlation between different levels of approximation, allowing for fewer samples to achieve the desired accuracy."
        },
        "experiments": {
            "evaluation setting": "Numerical experiments were conducted using simple test cases and a medical decision model, comparing the proposed MLMC method against standard nested Monte Carlo methods.",
            "evaluation method": "The performance was assessed by measuring the root-mean-square accuracy and computational cost required to achieve a specified level of accuracy."
        },
        "conclusion": "The MLMC method successfully demonstrates significant computational savings in estimating EVPPI, achieving the desired accuracy at a cost of O(\u03b5\u22122). The numerical results validate the theoretical analysis and suggest that this method is effective for real-world applications in medical decision-making.",
        "discussion": {
            "advantage": "The primary advantage of the MLMC method is its efficiency, requiring significantly fewer samples than traditional methods while maintaining accuracy.",
            "limitation": "One limitation is that the method relies on specific assumptions about the decision model, which may not hold in all scenarios.",
            "future work": "Future research will explore extensions of the method to handle empirical input distributions, the use of quasi-random numbers, and adaptive sampling techniques."
        },
        "other info": {
            "acknowledgements": "The authors thank Dr. Howard Thom for discussions and acknowledge funding support for T. Goda from JSPS Grant-in-Aid for Young Scientists.",
            "references": [
                "Ades AE, Lu G, Claxton K (2004) Expected value of sample information calculations in medical decision modeling.",
                "Brennan A, Kharroubi S, O\u2019Hagan A, Chilcott J (2007) Calculating partial expected value of perfect information via Monte Carlo sampling algorithms."
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "Algorithmic bias refers to systematic discrimination that can arise from biased data and design choices in decision-making models."
        },
        {
            "section number": "3.1",
            "key information": "The primary challenge is the nested expectation problem, which requires a large number of samples for accurate estimation, leading to high computational costs with traditional Monte Carlo methods."
        },
        {
            "section number": "3.2",
            "key information": "The proposed MLMC method aims to provide a more efficient way to estimate EVPPI by reducing the computational cost associated with traditional methods."
        },
        {
            "section number": "5.1",
            "key information": "The MLMC method estimates expectations efficiently by using a hierarchy of approximations with increasing accuracy and cost, leveraging the correlation between levels to reduce variance."
        },
        {
            "section number": "6.1",
            "key information": "Responsible AI practices can be enhanced through methods like MLMC, which improve efficiency and reduce costs in decision-making under uncertainty."
        },
        {
            "section number": "8.1",
            "key information": "Future research will explore extensions of the MLMC method to handle empirical input distributions, the use of quasi-random numbers, and adaptive sampling techniques."
        }
    ],
    "similarity_score": 0.5403373931324832,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Decision-making under uncertainty_ using MLMC for efficient estimation of EVPPI.json"
}