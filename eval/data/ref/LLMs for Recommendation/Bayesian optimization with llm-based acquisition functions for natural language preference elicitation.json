{
    "from": "google",
    "scholar_id": "ZqafcpHIm_kJ",
    "detail_id": null,
    "title": "Bayesian optimization with llm-based acquisition functions for natural language preference elicitation",
    "abstract": "\n\nABSTRACT\n\nDesigning preference elicitation (PE) methodologies that can quickly ascertain a user\u2019s top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) enable fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multiturn, decision-theoretic reasoning required to effectively balance the exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but cannot generate arbitrary NL queries or reason over content in NL item descriptions \u2013 requiring users to express preferences via ratings or comparisons of unfamiliar items. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to actively elicit NL feedback to identify the best recommendation. Key challenges in generalizing BO to deal with natural language feedback include determining: (a) how to leverage LLMs to model the likelihood of NL preference feedback as a function of item utilities, and (b) how to design an acquisition function for NL BO that can elicit preferences in the infinite space of language. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses: 1) Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain Bayesian preference beliefs, and 2) BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation. We numerically evaluate our methods in controlled simulations, finding that after 10 turns of dialogue, PEBOL can achieve an MRR@10 of up to 0.27 compared to the best monolithic LLM baseline\u2019s MRR@10 of 0.17, despite relying on earlier and smaller LLMs. 1\n\n\u2217 Both authors contributed equally to this research. 1 Our code is publi",
    "bib_name": "austin2024bayesian",
    "md_text": "# ayesian Optimization with LLM-Based Acquisition Functions Natural Language Preference Elicitation\n\nDavid Eric Austin \u2217\ndeaustin@uwaterloo.ca University of Waterloo Waterloo, Ontario, Canada\n\nABSTRACT\n\nDesigning preference elicitation (PE) methodologies that can quickly ascertain a user\u2019s top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) enable fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multiturn, decision-theoretic reasoning required to effectively balance the exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but cannot generate arbitrary NL queries or reason over content in NL item descriptions \u2013 requiring users to express preferences via ratings or comparisons of unfamiliar items. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to actively elicit NL feedback to identify the best recommendation. Key challenges in generalizing BO to deal with natural language feedback include determining: (a) how to leverage LLMs to model the likelihood of NL preference feedback as a function of item utilities, and (b) how to design an acquisition function for NL BO that can elicit preferences in the infinite space of language. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses: 1) Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain Bayesian preference beliefs, and 2) BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation. We numerically evaluate our methods in controlled simulations, finding that after 10 turns of dialogue, PEBOL can achieve an MRR@10 of up to 0.27 compared to the best monolithic LLM baseline\u2019s MRR@10 of 0.17, despite relying on earlier and smaller LLMs. 1\n\n\u2217 Both authors contributed equally to this research. 1 Our code is publically available at https://github.com/D3Mlab/llm-pe.\n\n\u2217 Both authors contributed equally to this research. 1 Our code is publically available at https://github.com/D3Mlab/llm-pe.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys \u201924, October 14\u201318, 2024, Bari, Italy \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0505-2/24/1 https://doi.org/10.1145/3640457.3688142\n\nAnton Korikov \u2217\nArmin Toroghi Scott Sanner anton.korikov@mail.utoronto.ca University of Toronto Toronto, Ontario, Canada\n\nCCS CONCEPTS\n\u2022 Information systems \u2192 Recommender systems;  Personalization; Language models.\n\n# KEYWORDS\n\nConversational Recommendation, Preference Elicitation, Bayesian Optimization, Online Recommendation, Query Generation\n\nACM Reference Format: David Eric Austin, Anton Korikov, Armin Toroghi, and Scott Sanner. 2024. Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation. In 18th ACM Conference on Recommender Systems (RecSys \u201924), October 14\u201318, 2024, Bari, Italy. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3640457.3688142\n\n# 1 INTRODUCTION\n\nPersonalized conversational recommendation (ConvRec) systems require effective natural language (NL) preference elicitation (PE) strategies that can efficiently learn a user\u2019s top item preferences in cold start settings, ideally requiring only an arbitrary set of NL item descriptions. While the advent of large language models (LLMs) has introduced the technology to facilitate NL-PE conversations [14, 23] we conjecture that monolithic LLMs have limited abilities to strategically conduct active, multi-turn NL-PE dialogues about a set of arbitrary items. Specifically, we hypothesize that LLMs lack the multi-turn decision-theoretic reasoning to interactively generate queries that avoid over-exploitation or over-exploration of user-item preferences, thus risking over-focusing on already revealed item preferences or wastefully exploring preferences over low-value items. Further challenges faced by monolithic LLM NL-PE approaches include the need to jointly reason over large, potentially unseen sets of item descriptions, and the lack of control and interpretability in system behaviour even after prompt engineering or fine-tuning [28]. In contrast, conventional PE algorithms [21, 22, 27, 39, 40], including Bayesian optimization methods [2, 5, 12, 32, 36], establish formal decision-theoretic policies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) [16] to balance exploration and exploitation with the goal of quickly identifying the user\u2019s most preferred items. However, these techniques typically assume a user can express preferences via direct item ratings or comparisons \u2013 an unrealistic expectation when users are unfamiliar with most items [1]. While recent work has extended Bayesian PE to a fixed set of template-based queries over pre-defined keyphrases [36], no\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5b2/e5b26fa9-3303-47bc-9617-261e4b27139e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: PEBOL\u2019s belief updates over a cold-start user\u2019s item utilities during three turns of NL dialo beliefs not only facilitate recommendation, but also enable Bayesian optimization policies to guid avoiding over-exploration (asking about clearly low-value items) and over-exploitation (over-focusin\n</div>\n\u2022 We introduce the first Bayesian optimization formalization of NL-PE for arbitrary NL dialogue over a generic set of NL item descriptions \u2013 establishing a new framework for research on steering LLMs with decision-theoretic reasoning.\n\u2022 We present PEBOL (P reference E licitation with B ayesian O ptimization augmented L LMs), a novel NL-PE algorithm which 1) infers item preferences via Natural Language Inference (NLI) [37] between dialogue utterances and item descriptions to maintain Bayesian preference beliefs and 2) introduces LLM-based acquisition functions, where NL query generation is guided by decision-theoretic strategies such as TS and UCB over the preference beliefs.\n\u2022  We numerically evaluate PEBOL against monolithic GPT3.5 and Gemini-Pro NL-PE methods via controlled NL-PE dialogue experiments over multiple NL item datasets and levels of user noise.\n\u2022 We observe that after 10 turns of dialogue, PEBOL can achieve a mean MRR@10 of up to 0.27 compared to the best monolithic LLM baseline\u2019s MRR@10 of 0.17, despite relying on earlier and smaller LLMs.\n\n# 2 BACKGROUND AND RELATED WORK\n\n# 2.1 Bayesian Optimization\n\nGiven an objective function \ud835\udc53: X \u2192 R, (standard) optimization systematically searches for a point \ud835\udc65 \u2217 \u2208X that maximizes 2 \ud835\udc53. Bayesian optimization focuses on settings where \ud835\udc53 is a black-box function which does not provide gradient information and cannot be evaluated exactly \u2013 rather, \ud835\udc53 must be evaluated using indirect or noisy observations which are expensive to obtain [10, 30]. To address these challenges, Bayesian optimization maintains probabilistic beliefs over \ud835\udc53 (\ud835\udc65) and its observations to guide an uncertainty-aware optimization policy which decides where to next observe \ud835\udc53 (\ud835\udc65). Bayesian optimization begins with a prior \ud835\udc5d (\ud835\udc53) which represents the beliefs about \ud835\udc53 before any observations are made. Letting \ud835\udc66 \ud835\udc56\n\n2 We take the maximization direction since this paper searches for items with maximum utility for a person.\n\nrepresent a noisy or indirect observation of \ud835\udc53 (\ud835\udc65 \ud835\udc56), and collecting a sequence of observations into a dataset D = (x, y), an observation model defines the likelihood \ud835\udc5d (D| \ud835\udc53). We then use the observed data and Bayes theorem to update our beliefs and obtain the posterior\n\n(1)\n\n(D)\nThis posterior informs an acquisition function \ud835\udefe (\ud835\udc65 |D)  which determines where to next observe \ud835\udc53 (\ud835\udc65)  in a way that balances exploitation (focusing observations where \ud835\udc53 is likely near its maximum) with exploration (probing areas where \ud835\udc53 has high uncertainty).\n\n# 2.2 Preference Elicitation\n\nPE has witnessed decades of research, and includes approaches based on Bayesian optimization (e.g., [3, 8, 11, 13, 18]), Bandits (e.g., [5, 24, 25, 40]), constrained optimization [29], and POMDPs [2]. In the standard PE setting, a user is assumed to have some hidden utilities u = [\ud835\udc62 1, ...,\ud835\udc62 \ud835\udc41] over a set I of \ud835\udc41 items, where item \ud835\udc56 is preferred to item \ud835\udc57 if \ud835\udc62 \ud835\udc56> \ud835\udc62 \ud835\udc57. The goal of PE is typically to search for an item \ud835\udc56 \u2217 \u2208 arg max \ud835\udc56 \ud835\udc62 \ud835\udc56 that maximizes user utility in a minimal number of PE queries, which most often ask a user to express item preferences as item ratings (e.g., [3, 5, 24, 25, 40]) or relative preferences between item pairs or sets (e.g., [2, 8, 11, 12, 14, 32]). An alternative form of PE asks users to express preferences over predefined item features, also through rating- or comparison-based queries [22, 27, 39]. Central to the above PE methods are query selection strategies that balance the exploration and exploitation of user preferences, with TS and UCB algorithms (cf. Sec. 4.2) often exhibiting strong performance [5, 27, 36, 39, 40]. However, none of these methods are able to interact with users through NL dialogue or reason about NL item descriptions.\n\n# 2.3 Language-Based Preference Elicitation\n\nYang et al. [36] introduce Bayesian PE strategies using TS and UCB for keyphrase rating queries, where keyphrases are first mined from NL item reviews and then co-embedded with user-item preferences in a recommendation system. Handa et al. [14] propose using LLMs to interface with a conventional Bayesian PE system, suggesting a preprocessing step to extract features from NL descriptions and a verbalization step to fluidly express pairwise item comparison\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/62b6/62b6bbf0-4974-4c99-9aa3-53574b032df1.png\" style=\"width: 50%;\"></div>\nFigure 2: The PEBOL NL-PE algorithm, which maintains a Bayesian belief state over a user\u2019s item preferences given an arbitrary set of NL item descriptions x. This belief is used by a decision-theoretic policy to balance the exploration and exploitation of preferences by strategically selecting an item description \ud835\udc65 \ud835\udc56 \ud835\udc61 as the basis for LLM query generation. Belief updates are computed through Bayesian inference with NLI entailment scores between item descriptions and query-response pairs.\n\nqueries. Li et al. [23] prompt an LLM to generate PE queries for some specific domain (e.g., news content, morals), observe user responses, and evaluate LLM relevance predictions for a single item. While these works make progress towards NL-PE, they do not study how LLM query generation can strategically explore user preferences towards an arbitrary item set outside the realm of item-based or category-based feedback.\n\n# 2.4 Conversational Recommendation\n\nRecent work on ConvRec uses language models 3 to facilitate NL dialogue while integrating calls to a recommender module which generates item recommendations based on user-item interaction history [4, 26, 33, 35]. He et al. [15] report that on common datasets, zero-shot GPT-3.5/4 outperforms these ConvRec methods, which generally use older language models and require user-item interaction history for their recommendation modules.\n\n# 2.5 Natural Language Inference\n\nBinary Natural Language Inference (NLI) [37] models predict the likelihood that one span of text called a premise is entailed by (i.e., can be inferred from) a second span called the hypothesis. For example, an effective NLI model should predict a high likelihood that the premise \u201cI want to watch Iron Man\u201d entails the hypothesis \u201cI want to watch a superhero movie\u201d. As illustrated by this example, the hypothesis typically must be more general than the premise. NLI models are trained by fine-tuning encoder-only LLMs on NLI datasets [6, 31, 34], which typically consist of short text spans for the premise and hypothesis \u2013 thus enabling relatively efficient performance on similar tasks with a fairly small number LLM parameters.\n\n# 3 PROBLEM DEFINITION\n\nWe now present a Bayesian optimization formulation of NL-PE. The goal of NL-PE is to facilitate a NL dialogue which efficiently discovers a user\u2019s most preferred items out of a set of \ud835\udc41 items. Each item \ud835\udc56 \u2208I has a NL description \ud835\udc65 \ud835\udc56, which might be a title, long-form description, or even a sequence of reviews, with the item\n3 Earlier systems (e.g. [4, 26]) use relatively small RNN-based language models.\n\nset I collectively represented by x \u2208X with x = [\ud835\udc65 1, ...,\ud835\udc65 \ud835\udc41]. We assume the user has some (unknown) utility function \ud835\udc53: X \u2192 R establishing hidden utilities u = \ud835\udc53 (x) so that item \ud835\udc56 is preferred to item \ud835\udc57 if \ud835\udc62 \ud835\udc56> \ud835\udc62 \ud835\udc57. Our goal is to find the most preferred item(s):\n\n(2)\n\n\u2208I\nIn contrast to standard Bayesian PE formalisms (c.f. Sec 2.2), we do not assume that the user can effectively convey direct item-level preferences by either: 1) providing item ratings (i.e., utilities) or 2) pairwise or listwise item comparisons. Instead, we must infer user preferences by observing utterances during a NL system-user dialogue. At turn \ud835\udc61 of a dialogue, we let \ud835\udc5e \ud835\udc61 and \ud835\udc5f \ud835\udc61 be the system and user utterance, respectively, with q \ud835\udc61 = [\ud835\udc5e 1, ...,\ud835\udc5e \ud835\udc61] and r \ud835\udc61 = [\ud835\udc5f 1, ...,\ud835\udc5f \ud835\udc61] representing all system and user utterances up to \ud835\udc61. In this paper, we call \ud835\udc5e \ud835\udc61 the query and \ud835\udc5f \ud835\udc61 the response, though extensions to more generic dialogues (e.g., when users can also ask queries) are discussed in Section 7. We let H \ud835\udc61 = (q \ud835\udc61, r \ud835\udc61) be the conversation history at turn \ud835\udc61. To formulate NL-PE as a Bayesian optimization problem, we place a prior belief on the user\u2019s utilities \ud835\udc5d (u | x), potentially conditioned on item descriptions since they are available before the dialogue begins. We then assume an observation model that gives the likelihood \ud835\udc5d (r \ud835\udc61 | x, u, q \ud835\udc61), letting us define the posterior utility belief as t (3)\n\n(3)\n\n(| H) \u221d(|)(|) This posterior informs an acquisition function \ud835\udefe (x, H \ud835\udc61)  which generates 4 a new NL query\n\n(4)\n\n( H)\nto systematically search for \ud835\udc56 \u2217. The preference beliefs also let us define an Expected Utility (EU) \ud835\udf07 \ud835\udc61 \ud835\udc56 for every item as\n\n(5)\n\n(|H)\nwhich allows the top\ud835\udc58 items to be recommended at any turn based on their expected utilities. Our Bayesian optimization NL-PE paradigm lets us formalize several key questions, including:\n\n4 To represent the generative acquisition of NL outputs, we deviate from the conventional definition of acquisition functions as mapping to R.\n\n4 To represent the generative acquisition of NL outputs, we deviate from the conventional definition of acquisition functions as mapping to R.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/308e/308ed71f-5ba9-4118-857e-68c32301aefc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 3: Cherry-picked system-generated dialogues from our NL-PE experiments. The Monolithic GPT-3.5 dialogue (left) demonstrates over-exploitation, with \ud835\udc5e 3 directly extending \ud835\udc5e 2 after a positive user preference is observed and leading to the xtreme case of query repetition (\ud835\udc5e 4 = \ud835\udc5e 3). In contrast, PEBOL (right) continues exploring even after a positive response, while ocusing on promising aspects (three out of four queries elicit a positive response) by using UCB-guided query generation.\n\nThese questions reveal a number of novel research directions discussed further in Section 7. In this paper, we present PEBOL, a NL-PE algorithm based on the above Bayesian optimization NL-PE formalism, and numerically evaluate it against monolithic LLM alternatives through controlled, simulated NL dialogues (cf. Sec. 6).\n\n# 4 METHODOLOGY\n\nLimitations of Monolithic LLM Prompting. An obvious NL-PE approach, described further as baseline in Section 5.1, is to prompt a monolithic LLM with all item descriptions x, dialogue history H \ud835\udc61, and instructions to generate a new query at each turn. However, providing all item descriptions [\ud835\udc65 1, ...,\ud835\udc65 \ud835\udc41] in the LLM context window is very computationally expensive for all but the smallest item sets. While item knowledge could be internalized through finetuning, each item update would imply system retraining. Critically, an LLM\u2019s preference elicitation behaviour cannot be controlled other than by prompt-engineering or further fine-tuning, with neither option offering any guarantees of predictable or interpretable behaviour that balances the exploitation and exploration of user preferences.\n\nPEBOL Overview. We propose to addresses these limitations by augmenting LLM reasoning with a Bayesian Optimization procedure in a novel algorithm, PEBOL, illustrated in Figure 2. At each turn \ud835\udc61, our algorithm maintains a probabilistic belief state over user preferences as a Beta belief state (cf. Sec. 4.1). This belief state guides an LLM-based acquisition function to generate NL queries explicitly balancing exploration and exploitation to uncover the top user preferences (cf. Sec. 4.2). In addition, our acquisition function reduces the context needed to prompt the LLM in each turn from all \ud835\udc41 item descriptions x  to a single strategically selected item description \ud835\udc65 \ud835\udc56 \ud835\udc61. PEBOL then uses NLI over elicited NL preferences and item\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d498/d49882a1-b8ea-4c56-95ec-faffea241498.png\" style=\"width: 50%;\"></div>\ndescriptions to map dialogue utterances to numerical observatio (c.f. Sec 4.3).\n\n# 4.1 Utility Beliefs\n\n4.1.1 Prior Beliefs. Before any dialogue, PEBOL establishes an uninformed prior belief \ud835\udc5d (u) on user-item utilities. We assume item utilities are independent so that\n\n(6)\n\nand that the prior for each utility \ud835\udc62 \ud835\udc56 is a Beta distribution\n\nSince this paper focuses on fully cold start settings, we assume a uniform Beta prior with (\ud835\udefc 0 \ud835\udc56, \ud835\udefd 0 \ud835\udc56) = (1, 1). Beta distributions, illustrated in Figure 1, lie in the domain [0, 1]\u2013 a normalized interval for bounded ratings in classical recommendation systems. We can thus interpret utility values of \ud835\udc62 \ud835\udc56 = 1 or \ud835\udc62 \ud835\udc56 =  0 to represent a complete like or dislike of item \ud835\udc56, respectively, while values \ud835\udc62 \ud835\udc56 \u2208(0, 1) provide a strength of preference between these two extremes.\n4.1.2 Observation Model. To perform a posterior update on our utility beliefs given observed responses r \ud835\udc61, we need an observation model that represents the likelihood \ud835\udc5d (r \ud835\udc61 | x, u, q \ud835\udc61). Modelling the likelihood of r \ud835\udc61 is a challenging task, so we will require some simplifying assumptions. Firstly, we assume that the likelihood of a single response \ud835\udc5f \ud835\udc61 is independent from any previous dialogue history H \ud835\udc61 \u2212 1, so that:\n\nSince this paper focuses on fully cold start settings, we assume a uniform Beta prior with (\ud835\udefc 0 \ud835\udc56, \ud835\udefd 0 \ud835\udc56) = (1, 1). Beta distributions, illustrated in Figure 1, lie in the domain [0, 1]\u2013 a normalized interval for bounded ratings in classical recommendation systems. We can thus interpret utility values of \ud835\udc62 \ud835\udc56 = 1 or \ud835\udc62 \ud835\udc56 =  0 to represent a complete like or dislike of item \ud835\udc56, respectively, while values \ud835\udc62 \ud835\udc56 \u2208(0, 1) provide a strength of preference between these two extremes.\n\n4.1.2 Observation Model. To perform a posterior update on our utility beliefs given observed responses r \ud835\udc61, we need an observation model that represents the likelihood \ud835\udc5d (r \ud835\udc61 | x, u, q \ud835\udc61). Modelling the likelihood of r \ud835\udc61 is a challenging task, so we will require some simplifying assumptions. Firstly, we assume that the likelihood of a single response \ud835\udc5f \ud835\udc61 is independent from any previous dialogue history H \ud835\udc61 \u2212 1, so that:\n\n(8)\n\nNote that this independence assumption will allow incremental posterior belief updates, so that\n\n(9)\n\n4.1.3 Binary Item Response Likelihoods and Posterior Update. With the factorized distributions over item utilities and observational\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/657e/657e46d0-f868-4dc4-888a-8571b0426dbd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: MRR@10 for MonoLLM and PEBOL-P with uncertainty-informed policies (UCB, TS, ER) learning over time and MonoLLM is generally outperformed by PEBOL.\n</div>\n<div style=\"text-align: center;\">for MonoLLM and PEBOL-P with uncertainty-informed policies (UCB, TS, ER). All methods show preference and MonoLLM is generally outperformed by PEBOL.\n</div>\nlikelihood history now defined, we simply have to provide a concrete observational model of the response likelihood conditioned on the query, item descriptions, and latent utility: \ud835\udc5d (\ud835\udc5f \ud835\udc61 | x, u,\ud835\udc5e \ud835\udc61). Because the prior is factorized over conditionally independent \ud835\udc62 \ud835\udc56 (cf. (6)), we can likewise introduce individual per-item factorized binary responses \ud835\udc5f \ud835\udc61 \ud835\udc56 \u2208{0 (dislike), 1 (like)} to represent the individual relevance of each item \ud835\udc56 to the preference elicited at turn \ud835\udc61. Critically, we won\u2019t actually require an individual response per item \u2014 this will be computed by a natural language inference (NLI) model [6] to be discussed shortly \u2014 but we\u2019ll begin with an individual binary response model for \ud835\udc5f \ud835\udc61 \ud835\udc56 for simplicity:\n\n(10)\n\nWith our response likelihood defined, this now leads us to our first pass at a full posterior utility update that we term PEBOL-B for observed Binary rating feedback. Specifically, given observed binary ratings \ud835\udc5f \ud835\udc61 \ud835\udc56, the update at \ud835\udc61 = 1 uses the Beta prior (7) with the Bernoulli likelihood (10) to form a standard Beta-Bernoulli conjugate pair and compute the posterior utility belief\n\n(11)\n(12)\n\nwhere \ud835\udefc 1 \ud835\udc56 = \ud835\udefc 0 \ud835\udc56 + \ud835\udc5f 1 \ud835\udc56, \ud835\udefd 1 \ud835\udc56 = \ud835\udefd 0 \ud835\udc56 + (1 \u2212 \ud835\udc5f 1 \ud835\udc56). Subsequent incremental updates updates follow Eq. (9) and use the same conjugacy to give\n\n4.1.4 Natural Language Inference and Probabilistic Posterior Update. As hinted above, effective inference becomes slightly more nuanced since we don\u2019t need to observe an explicit binary response per item in our PEBOL framework. Rather, we receive general preference feedback \ud835\udc5f \ud835\udc61 on whether a user generically prefers a text description \ud835\udc5e \ud835\udc61 and then leverage an NLI model [6] to infer whether the description \ud835\udc65 \ud835\udc56 of item \ud835\udc56 would be preferred according to this feedback. For instance, for a (\ud835\udc5e \ud835\udc61,\ud835\udc5f \ud835\udc61) pair (\u201cWant to watch a children\u2019s movie?\u201d,\u201cYes\u201d), NLI should infer a rating of \ud835\udc5f \ud835\udc61 1 = 1 for \ud835\udc65 1 = \u201cThe Lion King\u201d and \ud835\udc5f \ud835\udc61 2 = 0 for \ud835\udc65 2 = \u201cTitanic\u201d. To deal with the fact that NLI models actually return an  entailment probability, our probabilistic observation variant, PEBOL-P leverages the probability that item description \ud835\udc65 \ud835\udc56 entails \ud835\udc5e \ud835\udc61, which we denote as \ud835\udc64 \ud835\udc61 \ud835\udc56. We provide a full graphical model and derivation of the Bayesian posterior update given this entailment probability in the Supplementary Material, but note that we can summarize the final result as a relaxed version of the binary posterior update of (13)\n\nthat replaces the binary observation \ud835\udc5f \ud835\udc56 \u2208{0, 1} with the entailment probability \ud835\udc64 \ud835\udc61 \ud835\udc56 \u2208[0, 1], i.e., \ud835\udefc \ud835\udc61 \ud835\udc56 = \ud835\udefc \ud835\udc61 \u2212 1 \ud835\udc56 + \ud835\udc64 \ud835\udc61 \ud835\udc56, \ud835\udefd \ud835\udc61 \ud835\udc56 = \ud835\udefd \ud835\udc61 \u2212 1 \ud835\udc56 + (1 \u2212 \ud835\udc64 \ud835\udc61 \ud835\udc56). To visually illustrate how this posterior inference process works in practice, Figure 1 shows the effect of PEBOL\u2019s posterior utility belief updates based on NLI for three query-response pairs \u2013 we can see the system gaining statistical knowledge about useful items for the user from the dialogue.\n\n# 4.2 LLM-Based Acquisition Functions\n\nRecall from Sec. 2.1 that in Bayesian optimization, the posterior informs an acquisition function which determines where to make the next observation. PEBOL generates a new query \ud835\udc5e \ud835\udc61 with a two-step acquisition function \ud835\udefe, first using Bayesian Optimization policies (step 1) based on the posterior utility beliefs \ud835\udc5d (u | x, H \ud835\udc61) to select NL context, and then using this selected context to guide LLM prompting (step 2). We express the overall acquisition function \ud835\udefe = \ud835\udefe \ud835\udc3a \u25e6 \ud835\udefe \ud835\udc36 as a composition of a context acquisition function \ud835\udefe \ud835\udc36\n(cf. Sec. 4.2.1) and a NL generation function \ud835\udefe \ud835\udc3a (cf. Sec. 4.2.2).\n4.2.1 Context Acquisition via Bayesian Optimization Policies. First, PEBOL harnesses Bayesian optimization policies to select an item description \ud835\udc65 \ud835\udc56 \ud835\udc61 which will be used to prompt an LLM to generate a query about an aspect described by \ud835\udc65 \ud835\udc56 \ud835\udc61 (cf. Sec. 4.2.2). Selecting an item \ud835\udc56 \ud835\udc61 whose utility \ud835\udc62 \ud835\udc56 \ud835\udc61 is expected to be near the maximum, \ud835\udc62 \ud835\udc56 \u2217, will generate exploitation queries asking about properties of items that are likely to be preferred by the user. In contrast, selecting an item \ud835\udc56 \ud835\udc61 associated with high uncertainty in its utility \ud835\udc62 \ud835\udc61 \ud835\udc56 will generate exploration queries that probe into properties of items for which user preferences are less known. Thus, strategically selecting \ud835\udc65 \ud835\udc56 \ud835\udc61 allows PEBOL to balance the exploration and exploitation behaviour of NL queries, decreasing the risks of becoming stuck in local optima (over-exploitation) or wasting resources exploring low utility item preferences (over-exploration). We define the item selected by the context acquisition function as\n\nRecall from Sec. 2.1 that in Bayesian optimization, the posterior informs an acquisition function which determines where to make the next observation. PEBOL generates a new query \ud835\udc5e \ud835\udc61 with a two-step acquisition function \ud835\udefe, first using Bayesian Optimization policies (step 1) based on the posterior utility beliefs \ud835\udc5d (u | x, H \ud835\udc61) to select NL context, and then using this selected context to guide LLM prompting (step 2). We express the overall acquisition function \ud835\udefe = \ud835\udefe \ud835\udc3a \u25e6 \ud835\udefe \ud835\udc36 as a composition of a context acquisition function \ud835\udefe \ud835\udc36\n(cf. Sec. 4.2.1) and a NL generation function \ud835\udefe \ud835\udc3a (cf. Sec. 4.2.2).\n\n4.2.1 Context Acquisition via Bayesian Optimization Policies. First, PEBOL harnesses Bayesian optimization policies to select an item description \ud835\udc65 \ud835\udc56 \ud835\udc61 which will be used to prompt an LLM to generate a query about an aspect described by \ud835\udc65 \ud835\udc56 \ud835\udc61 (cf. Sec. 4.2.2). Selecting an item \ud835\udc56 \ud835\udc61 whose utility \ud835\udc62 \ud835\udc56 \ud835\udc61 is expected to be near the maximum, \ud835\udc62 \ud835\udc56 \u2217, will generate exploitation queries asking about properties of items that are likely to be preferred by the user. In contrast, selecting an item \ud835\udc56 \ud835\udc61 associated with high uncertainty in its utility \ud835\udc62 \ud835\udc61 \ud835\udc56 will generate exploration queries that probe into properties of items for which user preferences are less known. Thus, strategically selecting \ud835\udc65 \ud835\udc56 \ud835\udc61 allows PEBOL to balance the exploration and exploitation behaviour of NL queries, decreasing the risks of becoming stuck in local optima (over-exploitation) or wasting resources exploring low utility item preferences (over-exploration). We define the item selected by the context acquisition function as\n\n(14)\n\n( H)\nand list several alternatives for \ud835\udefe \ud835\udc36, including the well-known strategies of TS and UCB [30]:\n(1) Thompson Sampling (TS): First, a sample of each item\u2019s utility \u02c6 \ud835\udc62 \ud835\udc61 \ud835\udc56 is taken from the posterior, \u02c6 \ud835\udc62 \ud835\udc61 \ud835\udc56 \u223c \ud835\udc5d (\ud835\udc62 \ud835\udc56 | \ud835\udc65 \ud835\udc56, H \ud835\udc61). Then, the item with the highest sampled utility is selected:\n\n(15)\n\nTS explores more when beliefs have higher uncertainty and exploits more as the system becomes more confident.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f232/f2322130-5be0-4797-855d-f1b888cd8a47.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: MRR@10 for PEBOL-P with various context acquisition policies.\n</div>\n(2) Upper Confidence Bound (UCB): Let \ud835\udc43 \ud835\udc58 (\ud835\udefc, \ud835\udefd) represent the \ud835\udc58 \u2019th percentile of Beta (\ud835\udefc, \ud835\udefd), which provides a confidence bound on the posterior. UCB selects the item with the highest confidence bound\n\n(16)\n\nfollowing a balanced strategy because confidence bounds are increased by both high utility and high uncertainty.\n(3) Entropy Reduction (ER): An explore-only strategy that selects the item with the most uncertain utility:\n\n(17)\n\n(4) Greedy: An exploit-only strategy that selects the item with the highest expected utility \ud835\udf07 \ud835\udc61 \ud835\udc56 (Eq. 5):\n\n(18)\n\n(5) Random: An explore-only heuristic that selects the next item randomly.\n\n4.2.2 Generating Short, Aspect-Based NL Queries. Next, PEBOL prompts an LLM to generate a NL query \ud835\udc5e \ud835\udc61 based on the selected item description \ud835\udc65 \ud835\udc56 \ud835\udc61 while also using the dialogue history H \ud835\udc61 to avoid repetitive queries. We choose to generate \u201cyes-or-no\u201d queries asking if a user prefers items with some aspect \ud835\udc4e \ud835\udc61, which is a short text span extracted dynamically from \ud835\udc65 \ud835\udc56 \ud835\udc61 to be different from any previously queried aspects \ud835\udc4e 1, ...,\ud835\udc4e \ud835\udc61 \u2212 1. We adopt this query generation strategy to: 1) reduce cognitive load on the user, who may be frustrated by long and specific queries about unfamiliar items and 2) better facilitate NLI through brief, general phrases [37]. Letting \ud835\udf19 represent the query generation prompt, we let\n\n(19)\n\nbe the LLM generated query and aspect at turn \ud835\udc61, with prompting details discussed in Section 5.2.3. An example of such a query and aspect (bold) is \u201cAre you interested in movies with patriotic themes?\u201d, generated by PEBOL in our movie recommendation experiments and shown in Figure 2.\n\n# .3 NL Item-Preference Entailment\n\n4.3.1 Preference Descriptions from Query Response Pairs. Next, PEBOL receives a NL user response \ud835\udc5f \ud835\udc61, which it must convert to individual item preference observations. Since the LLM is instructed to generate \"yes-or-no\" queries \ud835\udc5e \ud835\udc61 asking a user if they like aspect \ud835\udc4e \ud835\udc61, we assume the user response will be a\"yes\" or a\"no\", and create a NL description of the users preference \ud835\udf0c \ud835\udc61, letting \ud835\udf0c \ud835\udc61 = \ud835\udc4e \ud835\udc61 if \ud835\udc5f \ud835\udc61 = \u201cyes\u201d, and \ud835\udf0c \ud835\udc61 = concat(\u201cnot \u201d,\ud835\udc4e \ud835\udc61) if \ud835\udc5f \ud835\udc61 = \u201cno\u201d. For example,\n\ngiven a query that asks if the user prefers the aspect \u201cpatriotism\u201d in an item, if the user response is \u201cyes\u201d, then the user preference \ud835\udf0c \ud835\udc61 is \u201cpatriotism\u201d, and \u201cnot patriotism\u201d otherwise. This approach produces short, general preference descriptions that are well suited for NLI models [37].\n4.3.2 Inferring Item Ratings from NL Preferences.  Given a NL preference \ud835\udf0c \ud835\udc61, PEBOL must infer whether the user would like an item described by \ud835\udc65 \ud835\udc56. Specifically, PEBOL acquires ratings w \ud835\udc61 = [\ud835\udc64 \ud835\udc61 1, ...,\ud835\udc64 \ud835\udc61 \ud835\udc41] (cf. Sec. 4.1.4) by using NLI to predict whether an item description \ud835\udc65 \ud835\udc56 entails (i.e., implies) the preference \ud835\udf0c \ud835\udc61. For example, we expect that an NLI model would predict that \ud835\udc65 \ud835\udc56 = \u201cThe Lion King\u201d entails \ud835\udf0c \ud835\udc61 = \u201canimated\u201d while \ud835\udc65 \ud835\udc57 = \u201cTitanic\u201d does not, inferring that a user who expressed preference \ud835\udf0c \ud835\udc61 would like item \ud835\udc56 but not \ud835\udc57. We use an NLI model \ud835\udc43 \ud835\udf14 (\ud835\udc65 \ud835\udc56, \ud835\udf0c \ud835\udc61) to predict the probability \ud835\udc64 \ud835\udc61 \ud835\udc56 that \ud835\udc65 \ud835\udc56 entails \ud835\udf0c \ud835\udc61, and return \ud835\udc5f \ud835\udc61 \ud835\udc56 = \u230a \ud835\udc64 \ud835\udc61 \ud835\udc56 \u2309 in the case of binary observations (PEBOL-B) and \ud835\udc64 \ud835\udc61 \ud835\udc56 in the case of probabilistic observations (PEBOL-P).\n\ngiven a query that asks if the user prefers the aspect \u201cpatriotism\u201d in an item, if the user response is \u201cyes\u201d, then the user preference \ud835\udf0c \ud835\udc61 is \u201cpatriotism\u201d, and \u201cnot patriotism\u201d otherwise. This approach produces short, general preference descriptions that are well suited for NLI models [37].\n\n# 4.4 The Complete PEBOL System\n\nThis concludes the PEBOL specification \u2013 the entire process from prior utility belief to the LLM-based acquisition function generation of a query to the posterior utility update is illustrated in Figure 2.\n\n# 5 EXPERIMENTAL METHODS\n\nWe numerically evaluate our PEBOL variations through controlled NL-PE dialogue experiments across multiple datasets and response noise levels \u2013 comparing against two monolithic LLM (MonoLLM) baselines. Specifically, these baselines directly use GPT-3.5-turbo0613 (GPT MonoLLM) or Gemini-Pro (Gemini MonoLLM) as the NL-PE system, as described in Section 5.1. We do not compare against ConvRec methods [4, 26, 33, 35] because they are not coldstart systems, requiring observed user-item interactions data to drive their recommendation modules. We also do not base our experiments on ConvRec datasets such as ReDIAL [26], since they are made up of pre-recorded conversation histories and cannot be used to evaluate active, cold-start NL-PE systems.\n\n# 5.1 MonoLLM Baseline\n\nA major challenge of using MonoLLM for NL-PE is that item descriptions x either need to be internalized through training or be provided in the context window (cf. Sec. 4) \u2013 since we focus on fully cold-start settings, we test the latter approach as a baseline. In each turn, given the full conversation history H \ud835\udc61 and x, we prompt the MonoLLM to generate a new query to elicit user preferences \u2013 all prompts are shown in the Supplementary Materials. We evaluate\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/34d2/34d2203b-2177-4e7f-8585-70805b9fd85c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">PEBOL using binary vs. probabilistic entailment scores. PEBOL-P with the best policy (TS on Yelp and ecipe-MPR) generally outperforms PEBOL-B.\n</div>\n<div style=\"text-align: center;\">Figure 6: MRR@10 for PEBOL using binary vs. probabilistic entailment scores. PEBOL-P with the b MovieLens, UCB on Recipe-MPR) generally outperforms PEBOL-B.\n</div>\nrecommendation performance after each turn by using another prompt to recommend a list of ten item names from x given H \ud835\udc61. Due to context window limits, this MonoLLM approach is only feasible for small item sets with short item descriptions; thus, we have to limit |I| to 100 for fair comparison to the MonoLLM baseline.\n\n# 5.2 Simulation Details\n\nWe test PEBOL and MonoLLM through NL-PE dialogues with LLMsimulated users, where the simulated users\u2019 item preferences remain hidden from the system. We evaluate recommendation performance over 10 turns of dialogue.\n\n5.2.1 User Simulation. For each experiment, we simulate 100 users, each of which likes a single item \ud835\udc56 \u2208I. Each user is simulated by GPT-3.5-turbo-0613, which is given item description \ud835\udc65 \ud835\udc56 and instructed to provide only \u201cyes\u201d or \u201cno\u201d responses to a query \ud835\udc5e \ud835\udc61 as if it was a user who likes item \ud835\udc56.\n\n5.2.2 Evaluating Recommendations.  We evaluate the top-10 recommendations in each turn using the Mean Reciprocal Rank (MRR@10) of the preferred item, which is equivalent to MAP@10 for the case of a single preferred item.\n\n5.2.3 PEBOL Query Generation. In turn \ud835\udc61, given an item description \ud835\udc65 \ud835\udc56 and previously generated aspects (\ud835\udc4e 1, ...,\ud835\udc4e \ud835\udc61 \u2212 1), an LLM (GPT-3.5turbo-0613) 5 is prompted to generate an aspect \ud835\udc4e \ud835\udc61 describing the item \ud835\udc56 that is no more than 3 words long. The LLM is then prompted again to generate a \u201cyes-or-no\u201d query asking if a user prefers \ud835\udc4e \ud835\udc61.\n\n5.2.4 NLI. We use the 400M FAIR mNLI 6 model to predicts logits for entailment, contradiction, and neutral, and divide these logits by an MNLI temperature \ud835\udc47 \u2208{1, 10, 100} As per the FAIR guidelines, we pass the temperature-scaled entailment and contradiction scores through a softmax layer and take the entailment probabilities. We report PEBOL results using the best MNLI temperature for the most datasets.\n\n5.2.5 User Response Noise. We test three user response noise levels \u2208 {0,0.25,0.5} corresponding to the proportion or user responses that are randomly selected between \"yes\" and \"no\".\n\n5 Experiments with newer LLMs such as Gemini or GPT4 for PEBOL query generation are left for future work due to the API time and cost requirements needed to simulate the many variants of PEBOL reported in Section 6. We do, however, compare PEBOL against a Gemini-MonoLLM baseline. 6 https://huggingface.co/facebook/bart-large-mnli\n\n5.2.6 Omitting Query History Ablation. We test how tracking query history in PEBOL effects performance with an ablation study that removes previously generated aspects (\ud835\udc4e 1, ...,\ud835\udc4e \ud835\udc61 \u2212 1) from the aspect extraction prompt.\n\n# 5.3 Datasets\n\nWe obtain item descriptions from three real-world datasets: MovieLens25M 7, Yelp 8, and Recipe-MPR [38] (example item descriptions from each shown in Table 1 in the Supplementary Materials). After the filtering steps below for Yelp and MovieLens, we randomly sample 100 items to create x. For Yelp, we filter restaurant descriptions to be from a single major North American city (Philadelphia) and to have at least 50 reviews and five or more category labels. For MovieLens, 9 we filter movies to be in the 10% by rating count with at least 20 tags, and let movie descriptions use the title, genre labels, and 20 most common user-assigned tags.\n\n# 5.4 Research Questions\n\n# 6 EXPERIMENTAL RESULTS\n\n# 6.1 RQ1 - PEBOL vs. MonoLLM\n\nFigure 4 shows MRR@10 over 10 dialogue turns for MonoLLM and PEBOL (UCB,TS,ER), 10 with 95% confidence intervals (CIs) at turn 10 shown in Figure 8 (see Supplementary Materials for CIs for all turns and experiments). All methods start near random guessing, reflecting a cold start, and show clear preference learning over time.\n\n7 https://grouplens.org/datasets/movielens/25m/ 8 https://www.yelp.com/dataset 9 For all experiments with MovieLens, we use the 16k version of GPT-3.5-turbo-0613, due to MonoLLM requiring extra context length for x. 10 For each PEBOL policy, we use the MNLI temperature that performed best on the most datasets with continuous responses (see Supplementary Materials).\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c00/1c00ba01-e8e1-4782-a4d6-c1d9a39487d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: The effect of including the generated aspect history in the aspect generation prompt. Incl performance, which we hypothesize is due to reducing repeated or uninformative queries.\n</div>\n<div style=\"text-align: center;\">ffect of including the generated aspect history in the aspect generation prompt. Including the history improves which we hypothesize is due to reducing repeated or uninformative queries.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bcf4/bcf46484-7ce2-48e6-863f-83f83a5e38d4.png\" style=\"width: 50%;\"></div>\nCompared to GPT-MonoLLM, which uses the same LLM (GPT3.5-turbo-0613) as PEBOL does for query generation, after 10 turns of dialogue PEBOL achieves: a mean MRR@10 of 0.27 vs. GPTMonoLLM\u2019s MRR@10 of 0.12 on Yelp; 0.18 vs 0.09 on MovieLens; and 0.17 vs 0.11 on Recipe-MPR, respectively. Compared to GeminiMonoLLM, which uses a newer generation LLM (Gemini-Pro) than PEBOL for query generation, after 10 turns PEBOL still achieves a higher mean MRR@10 of 0.27 vs. Gemini-MonoLLM\u2019s mean MRR@10 of 0.17 on Yelp; 0.18 vs. 0.15 on MovieLens, and 0.17 vs 0.16 on Recipe-MPR, respectively. While we did not have the resources to test PEBOL with Gemini query generation, we hypothesize that using a newer LLM for query generation can further improve PEBOL performance, since using the newer LLM (Gemini) shows performance improvements for the MonoLLM baseline.\n\n# 6.2 RQ2 - Binary vs. Probabilistic Responses\n\nFigure 6 compares PEBOL performance using binary (PEBOL-B) vs. continuous (PEBOL-P) feedback, and shows that performance is typically better when continuous responses are used \u2013 indicating that binary feedback models discard valuable information from the entailment probabilities.\n\n# 3 RQ3 - Effect of User Response Noise\n\nFigure 8 shows the impact of user response noise on MRR@10 at turn 10 \u2013 PEBOL generally continues to outperform MonoLLM under user response noise. Specifically, at all noise levels, both MonoLLM baselines are outperformed by all PEBOL-P variants on Yelp, and by at least one PEBOL-P variant on MovieLens and Recipe-MPR.\n\n# 6.4 RQ4 - Comparison of Context Acquisition Policies\n\nFigure 5 compares the performance of various PEBOL context acquisition policies \u2013 all policies show active preference learning, other than random item selection on RecipeMPR. There is considerable overlap between methods, however for most turns TS does well on Yelp and MovieLens while being beaten by Greedy, ER, and UCB on Recipe-MPR. As expected due to the randomness in sampling, TS performance is correlated with random item selection, while UCB performs quite similarly to greedy.\n\n# 6.5 RQ5 - Effect of Aspect History in Query Generation\n\n# 6.5 RQ5 - Effect of Aspect History in Query Generation\n\n# 6.5 RQ5 - Effect of Aspect History in Query Generation\n\nAs shown in Figure 7, we see improvements in PEBOL performance from including a list of previously generated aspects in the aspect generation prompt. For instance, the differences in mean MRR@10 from including vs. excluding the query history for TS after 10 turns were: 0.27 vs 0.16 for Yelp; 0.18 vs 0.14 for MovieLens, and 0.13 vs 0.09 for Recipe-MPR, respectively. Practically, including the aspect generation history also helps to avoid repeat queries, which gain no information and could frustrate a user.\n\n# 7 CONCLUSION AND FUTURE WORK\n\nThis paper presents a novel Bayesian optimization formalization of natural language (NL) preference elicitation (PE) over arbitrary NL item descriptions, as well as introducing and evaluating PEBOL, an algorithm for NL P reference E licitation with B ayesian O ptimization augmented L LMs. As discussed below, our study also presents many opportunities for future work, including for addressing some of the limitations of PEBOL and our experiment setup.\n\nUser Studies. Firstly, our experiments limited by their reliance on LLM-simulated users. While the dialogue simulations indicate reasonable behaviour in the observed results, such as initial recommendation performance near random guessing, preference learning over time, and coherent user responses in logs such as those shown in Figure 7, future work would benefit from human user studies.\nMulti-Item Belief Updates.  While the assumption that item utilities can be updated independently allows the use of a simple and interpertable Beta-Bernouilli update model for each item, it also requires a separate NLI calculation to be performed for each item, which is computationally expensive. A key future direction is thus to explore alternative belief state forms which enable the joint updating of beliefs over all items from a single NLI computation.\nCollaborative Belief Updates. Since PEBOL does not leverage any historical interactions with other users, an important future direction is to study NL-PE which leverages collaborative, multiuser data. One possibility is to initialize a cold start user\u2019s prior beliefs based on interaction histories with other users. Another direction is adapting collaborative filtering based belief updating, such as the methods used in item-based feedback PE techniques (e.g., [5]), to NL-PE.\nDiverse Query Forms.  While PEBOL uses a pointwise query generation strategy that selects one item description at a time for LLM context, future work can explore LLM-based acquisition functions with pairwise and setwise context selection. Such multi-item context selection would enable contrastive query generation that could better discriminate between item preferences.\nNL-PE in ConvRec Architectures. Another direction for future research is the integration of NL-PE methodologies such as PEBOL into conversational recommendation (ConvRec) system architectures (e.g., [7, 9, 17, 20]), which must balance many tasks including recommendation, explanation, and personalized question answering. Thus, in contrast to PEBOL\u2019s pointwise queries and \u201cyes-or-no\u201d user responses, the use of PE in ConvRec systems implies that future algorithms will need to elicit preferences based on arbitrary pairs of NL system-user utterances. In these potential extensions, aspectbased NLI could be enabled by extracting aspects from utterances with LLMs [19].\n\nNL-PE in ConvRec Architectures. Another direction for future research is the integration of NL-PE methodologies such as PEBOL into conversational recommendation (ConvRec) system architectures (e.g., [7, 9, 17, 20]), which must balance many tasks including recommendation, explanation, and personalized question answering. Thus, in contrast to PEBOL\u2019s pointwise queries and \u201cyes-or-no\u201d user responses, the use of PE in ConvRec systems implies that future algorithms will need to elicit preferences based on arbitrary pairs of NL system-user utterances. In these potential extensions, aspectbased NLI could be enabled by extracting aspects from utterances with LLMs [19].\n\n# REFERENCES\n\n[1] Erdem Biyik, Fan Yao, Yinlam Chow, Alex Haig, Chih-wei Hsu, Mohammad Ghavamzadeh, and Craig Boutilier. 2023. Preference Elicitation with Soft Attributes in Interactive Recommendation. ArXiv abs/2311.02085 (2023). https: //api.semanticscholar.org/CorpusID:265034238\n[2] Craig Boutilier. 2002. A POMDP formulation of preference elicitation problems. In AAAI/IAAI. Edmonton, AB, 239\u2013246.\n[3] Eric Brochu, Tyson Brochu, and Nando De Freitas. 2010. A Bayesian interactive optimization approach to procedural animation design. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 103\u2013112.\n[4] Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang, and Jie Tang. 2019. Towards Knowledge-Based Recommender Dialog System. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 1803\u20131813. https://doi.org/10.18653/v1/D19-1189\n[5] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational Recommender Systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco,\n\n[1] Erdem Biyik, Fan Yao, Yinlam Chow, Alex Haig, Chih-wei Hsu, Mohammad Ghavamzadeh, and Craig Boutilier. 2023. Preference Elicitation with Soft Attributes in Interactive Recommendation. ArXiv abs/2311.02085 (2023). https: //api.semanticscholar.org/CorpusID:265034238\n[2] Craig Boutilier. 2002. A POMDP formulation of preference elicitation problems. In AAAI/IAAI. Edmonton, AB, 239\u2013246.\n[3] Eric Brochu, Tyson Brochu, and Nando De Freitas. 2010. A Bayesian interactive optimization approach to procedural animation design. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 103\u2013112.\n[4] Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang, and Jie Tang. 2019. Towards Knowledge-Based Recommender Dialog System. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 1803\u20131813. https://doi.org/10.18653/v1/D19-1189\n[5] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational Recommender Systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco,\n\nCalifornia, USA) (KDD \u201916). Association for Computing Machinery, New York, NY, USA, 815\u2013824.\n[6]  Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop. Springer, 177\u2013190.\n[7] Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, Ren\u00e9 Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024. A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys). In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \u201924), August 25\u201329, 2024, Barcelona, Spain.\n[8] Brochu Eric, Nando Freitas, and Abhijeet Ghosh. 2007. Active preference learning with discrete choice data. Advances in Neural Information Processing Systems 20 (2007).\n[9] Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al. 2023. Leveraging Large Language Models in Conversational Recommender Systems. arXiv preprint arXiv:2305.07961 (2023).\n[10] Roman Garnett. 2023. Bayesian optimization. Cambridge University Press. [11] Javier Gonz\u00e1lez, Zhenwen Dai, Andreas Damianou, and Neil D Lawrence. 2017. Preferential bayesian optimization. In  International Conference on Machine Learning. PMLR, 1282\u20131291.\n[12]  Shengbo Guo and Scott Sanner. 2010. Real-time multiattribute Bayesian preference elicitation with pairwise comparison queries. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, 289\u2013296.\n[13]  Shengbo Guo, Scott Sanner, and Edwin V Bonilla. 2010. Gaussian process preference elicitation. Advances in Neural Information Processing Systems 23 (2010).\n[14] Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, and Belinda Z. Li. 2024. Bayesian Preference Elicitation with Language Models. arXiv:2403.05534 [cs.CL]\n[15] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large Language Models as Zero-Shot Conversational Recommenders. Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (2023).\n[16] Giannis Karamanolakis, Kevin Raji Cherian, Ananth Ravi Narayan, Jie Yuan, Da Tang, and Tony Jebara. 2018. Item recommendation with variational autoencoders and heterogeneous priors. In Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems. 10\u201314.\n[17]  Sara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, Anton Korikov, and Scott Sanner. 2024. Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington, DC, USA) (SIGIR \u201924). ACM, New York, NY, USA.\n[18] Mohammad M. Khajah, Brett D. Roads, Robert V. Lindsey, Yun-En Liu, and Michael C. Mozer. 2016. Designing Engaging Games Using Bayesian Optimization. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI \u201916). Association for Computing Machinery, New York, NY, USA, 5571\u20135582. https://doi.org/10.1145/2858036.2858253\n[19] Anton Korikov, George Saad, Ethan Baron, Mustafa Khan, Manav Shah, and Scott Sanner. 2024. Multi-Aspect Reviewed-Item Retrieval via LLM Query Decomposition and Aspect Fusion. In Proceedings of the 1st SIGIR\u201924 Workshop on Information Retrieval\u2019s Role in RAG Systems, July 18, 2024, Washington D.C., USA.\n[20]  Anton Korikov, Scott Sanner, Yashar Deldjoo, Francesco Ricci, Zhankui He, Julian McAuley, Arnau Ramisa, Rene Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024. Large Language Model Driven Recommendation. arXiv preprint arXiv:2404.XXXXX (2024).\n[21] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019. MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). Association for Computing Machinery, New York, NY, USA, 1073\u20131082. https://doi.org/10.1145/ 3292500.3330859\n[22] Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, and Tat-Seng Chua. 2020. Interactive Path Reasoning on Graph for Conversational Recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA) (KDD \u201920). Association for Computing Machinery, New York, NY, USA, 2073\u20132083. https://doi.org/10.1145/3394486.3403258\n[23] Belinda Z. Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. 2023. Eliciting Human Preferences with Language Models. arXiv:2310.11589 [cs.CL]\n[24]  Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextualbandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web. 661\u2013670.\n[25] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. 2011. Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms.\n\nIn Proceedings of the Fourth ACM International Conference on Web Search and Data Mining. 297\u2013306.\n[26] Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, and Chris Pal. 2018. Towards deep conversational recommendations. Advances in Neural Information Processing Systems 31 (2018).\n[27] Shijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan He, Peng Jiang, and Tat-Seng Chua. 2021. Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-start Users. ACM Trans. Inf. Syst. 39, 4, Article 40 (aug 2021), 29 pages. https://doi.org/10.1145/3446427\n[28] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 1906\u20131919. https://doi.org/10.18653/v1/2020.aclmain.173\n[29] Francesca Rossi and Allesandro Sperduti. 2004. Acquiring both constraint and solution preferences in interactive constraint systems. Constraints 9, 4 (2004), 311\u2013332.\n[30]  Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. 2016. Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc. IEEE 104, 1 (2016), 148\u2013175. https://doi.org/10.1109/JPROC.2015.2494218\n[31] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 809\u2013819. https://doi.org/10. 18653/v1/N18-1074\n[32]  Ivan Vendrov, Tyler Lu, Qingqing Huang, and Craig Boutilier. 2020. GradientBased Optimization for Bayesian Preference Elicitation. Proceedings of the AAAI Conference on Artificial Intelligence 34, 06 (Apr. 2020), 10292\u201310301. https: //doi.org/10.1609/aaai.v34i06.6592\n[33] Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022. Towards unified conversational recommender systems via knowledge-enhanced prompt learning. In  Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1929\u20131937.\n[34] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (New Orleans, Louisiana). Association for Computational Linguistics, 1112\u20131122. http://aclweb.org/anthology/N18-1101\n[35]  Bowen Yang, Cong Han, Yu Li, Lei Zuo, and Zhou Yu. 2022. Improving Conversational Recommendation Systems\u2019 Quality with Context-Aware Item MetaInformation. In Findings of the Association for Computational Linguistics: NAACL 2022. 38\u201348.\n[36] Hojin Yang, Scott Sanner, Ga Wu, and Jin Peng Zhou. 2021. Bayesian Preference Elicitation with Keyphrase-Item Coembeddings for Interactive Recommendation. In Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization. 55\u201364.\n[37] Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 3914\u20133923. https://doi.org/10.18653/v1/D19-1404\n[38]  Haochen Zhang, Anton Korikov, Parsa Farinneya, Mohammad Mahdi Abdollah Pour, Manasa Bharadwaj, Ali Pesaranghader, Xi Yu Huang, Yi Xin Lok, Zhaoqi Wang, Nathan Jones, and Scott Sanner. 2023. Recipe-MPR: A Test Collection for Evaluating Multi-aspect Preference-based Natural Language Retrieval. In  Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR \u201923). Association for Computing Machinery, New York, NY, USA, 2744\u20132753. https: //doi.org/10.1145/3539618.3591880\n[39] Xiaoying Zhang, Hong Xie, Hang Li, and John C.S. Lui. 2020. Conversational Contextual Bandit: Algorithm and Application. In  Proceedings of The Web Conference 2020 (Taipei, Taiwan) (WWW \u201920). Association for Computing Machinery, New York, NY, USA, 662\u2013672. https://doi.org/10.1145/3366423.3380148\n[40] Xiaoxue Zhao, Weinan Zhang, and Jun Wang. 2013. Interactive collaborative filtering. In Proceedings of the 22nd ACM International Conference on Information & Knowledge Management. 1411\u20131420.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f554/f554bce9-6b77-4ad1-8700-d0ba95a698e3.png\" style=\"width: 50%;\"></div>\nFigure 9: Graphical model used for posterior utility updates. \ud835\udc62 \ud835\udc56 is the random variable representing the utility of item \ud835\udc56 \u2208I and \ud835\udc65 \ud835\udc56 is the item description. \ud835\udc5e \ud835\udc61 is the query presented at iteration \ud835\udc61, \ud835\udc5f \ud835\udc61 \ud835\udc56 is the variable representing the latent (not directly observed) relevance of item \ud835\udc56 at step \ud835\udc61, and \ud835\udc52 \ud835\udc61 \ud835\udc56 is the binary observation representing whether the item description entails the user preference. Unshaded and shaded nodes indicate unobserved and observed variables respectively.\n\n# PROBABILISTIC GRAPHICAL MODEL FOR\n\n# A PROBABILISTIC GRAPHICAL MODEL FOR POSTERIOR UTILITY UPDATE\n\nIn this section, we present the probabilistic graphical model for the posterior utility updates introduced in our paper with a more detailed derivation of the posterior utility belief. As discussed in the paper, the objective of posterior inference is to update the prior belief maintained over the utility of each item \ud835\udc56 \u2208I denoted by \ud835\udc5d (\ud835\udc62 \ud835\udc56) given the query \ud835\udc5e \ud835\udc61, item description \ud835\udc65 \ud835\udc56, and \ud835\udc52 \ud835\udc61 \ud835\udc56, the binary observation variable representing whether the item description entails the user\u2019s response to the queried preference \ud835\udc5e \ud835\udc61. Figure 9 shows the graphical model representation of these variables. The presented query \ud835\udc5e \ud835\udc61 and the item description \ud835\udc65 \ud835\udc56 are observed (shaded), while the relevance of item \ud835\udc56 to query \ud835\udc5e \ud835\udc61 is latent (unshaded) and denoted by \ud835\udc5f \ud835\udc61 \ud835\udc56 \u2208{0, 1}  and conditioned on the latent item utility \ud835\udc62 \ud835\udc56. We observe whether the item \ud835\udc65 \ud835\udc56 \u201ctruly\u201d entails (i.e., \ud835\udc52 \ud835\udc61 \ud835\udc56 = True) the user\u2019s response to query \ud835\udc5e \ud835\udc61 (as determined by the NLI entailment probability \ud835\udc64 \ud835\udc61 \ud835\udc56 if the item is relevant, i.e., \ud835\udc5f \ud835\udc61 \ud835\udc56 = 1). The conditional probability distributions in this graphical model are formally defined as follows:\n\n(22)\n\nTo further explain the rationale for Eq (22), we note that \ud835\udc64 \ud835\udc61 \ud835\udc56 is the natural language entailment probability that item description \ud835\udc65 \ud835\udc56 entails the aspect queried in the user\u2019s response to \ud835\udc5e \ud835\udc61 given that item \ud835\udc56 is relevant (\ud835\udc5f \ud835\udc61 \ud835\udc56 = 1). This entailment probability is obtained from the NLI model, which produces the probability that the entailment is true, hence the reason why \ud835\udc52 \ud835\udc61 \ud835\udc56 = True. If item \ud835\udc56 is instead irrelevant\n\n(23)\n\nConsidering the conditional independencies determined from the graphical model, the joint distribution factorizes as\n\n(24)\n\n(|)()(|)(|)\nNext, we replace the probability distribution of each factor according to Equations (20), (21), (22) in (23), to obtain\n\n(25)\n\n# Expanding the summation yields\n\n\u221d \ud835\udc64 \ud835\udc61 \ud835\udc56 Beta (\ud835\udc62 \ud835\udc56; \ud835\udefc + 1, \ud835\udefd) + (1 \u2212 \ud835\udc64 \ud835\udc61 \ud835\udc56) Beta (\ud835\udc62 \ud835\udc56; \ud835\udefc, \ud835\udefd + 1)\n\nThe latter term represents a mixture of Beta distributions that is challenging to handle since multiple posterior updates would cause the number of components in the mixture to grow exponentially with the number of query observations \ud835\udc5a, leading to substantial computational and memory complexity. To address this issue, several methods have been proposed for approximating the posterior distribution to allow for tractable computations. In this work, we use the Assumed Density Filtering (ADF) approach, a technique widely used in Bayesian filtering and tracking problems to project a complex posterior to an assumed simpler form (often the same form as the prior to maintain a closed-form). In our case, we project the Beta mixture posterior to a single Beta in order to maintain a closed-form Beta approximation of the posterior update matching the form of the Beta prior in Eq (20). To apply ADF, we assume a Beta distribution with parameters \ud835\udefc \u2032 and \ud835\udefd \u2032 for the posterior, and approximate the original mixture of Beta\u2019s with this distribution by equating their first moments (i.e., their means):\n\n(28)\n\nEquating the numerators yields\n\ufffd +\n\n(29)\n\n(30)\n\nThus, the \u201cmean matched\u201d posterior is Beta (\ud835\udc62 \ud835\udc56; \ud835\udefc + \ud835\udc64 \ud835\udc61 \ud835\udc56, 1 + \ud835\udefd \u2212 \ud835\udc64 \ud835\udc61 \ud835\udc56).\n\n11 We note that an alternative approach (not used here) could attempt to use NLI to determine the probability of an incorrect true entailment (or confusion) given that item \ud835\udc56 is irrelevant (\ud835\udc5f \ud835\udc61 \ud835\udc56 = 0). That is, there is no inherent requirement for the two cases of Eq (22) to sum to 1 since \ud835\udc5f \ud835\udc61 \ud835\udc56 is on the conditional side.\n\nMatching two distributions by equating their first moments is a special case of a more general technique called  \u201cmoment matching\u201d, which is widely used to approximate a complex probability distribution with a simpler one by equating their moments. In our work, we adopted a special case of this approach by matching the first moments, which we refer to as \u201cmean matching\u201d  of the distributions that we used for its simplicity and intuitive interpretation. However, this is only one of the possible solutions, and a complete moment matching derivation results in a slightly different solution. With this \u201cmean matching\u201d derivation and current item \ud835\udc56 posterior \ud835\udc35\ud835\udc52\ud835\udc61\ud835\udc4e (\ud835\udc62 \ud835\udc56; \ud835\udefc, \ud835\udefd) at step \ud835\udc61 \u2212 1, we can now perform an incremental posterior update after at step \ud835\udc61 given the probability \ud835\udc64 \ud835\udc61 \ud835\udc56 that the item description \ud835\udc65 \ud835\udc56 entails preference query \ud835\udc5e \ud835\udc61 yielding the closed-form Beta posterior \ud835\udc35\ud835\udc52\ud835\udc61\ud835\udc4e (\ud835\udc62 \ud835\udc56; \ud835\udefc + \ud835\udc64 \ud835\udc61 \ud835\udc56, 1 + \ud835\udefd \u2212 \ud835\udc64 \ud835\udc61 \ud835\udc56) as used in PEBOL-P.\n\n<div style=\"text-align: center;\">Table 1: Examples of LLM Aspect-Based Query Generation from an Item Description\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/21c7/21c7a973-49e9-45c3-bab2-ea0566d4ceb1.png\" style=\"width: 50%;\"></div>\nDataset\nItem Description \ud835\udc51\ud835\udc56\nGenerated Aspect\ud835\udc4e\ud835\udc61\nGenerated Query \ud835\udc5e\ud835\udc61\nMovieLens\nMovie Title: Meet John Doe (1941)\nGenres: Comedy, Drama\nTags: Christianity, Frank Capra, acting, anti-fascism, class\nissues, journalism, patriotic, pro american, thought pro-\nvoking, AFI 100 (Cheers), BD-R, Barbara Stanwyck, Gary\nCooper, baseball player, compare: This Is Our Land (2017),\ndomain, funny, radio broadcast, reviewed in the NYer by\nAnthony Lanne (2018-04-30), suicide note\npatriotism\nAre you interested in movies with\npatriotic themes?\nclassic\nDo you enjoy classic movies?\nRecipe-MPR\nSpaghetti with mushrooms, onion, green pepper, chicken\nbreasts, and alfredo sauce\nalfredo sauce\nDo you like alfredo sauce?\nchicken breast\nDo you like chicken breasts?\nYelp\nname: Le Pain Quotidien\ncategories: Restaurants, Bakeries, Break-\nfast & Brunch, Coffee & Tea, Food, Bel-\ngian, French\nbakery\nDo you like bakeries?\nFrench pastries\nDo you like French pastries?\n<div style=\"text-align: center;\">Figure 10: MAP@10 for all turns on all datasets and noise levels\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7aba/7aba5e09-33e1-4b4d-af64-64e114e3fa35.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: The effect of MNLI temperature on MAP@10 without noise \u2013 error bars are 95% C.I.s. Other using the temperatures that perform best across the most datasets for each policy.\n</div>\n<div style=\"text-align: center;\">ct of MNLI temperature on MAP@10 without noise \u2013 error bars are 95% C.I.s. Other results are reported ures that perform best across the most datasets for each policy.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3a16/3a1646a1-4875-45f0-8d44-b67e6f9df67e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Effect of MNLI Temperature with noise 0.25\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d855/d85515ae-f5ec-4bf0-8886-8dc7de2b8318.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Effect of MNLI Temperature with noise 0.5\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/01eb/01ebb22f-aac3-4221-a2dd-1b672568b2f1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: LLM Prompting Templates\n</div>\n<div style=\"text-align: center;\">Table 2: MRR@10 Mean and 95% C.I. for PEBOL-P vs MonoLLM on MovieLens without Respo\n</div>\nTurn\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMethod\nMonoLLM GPT Mean\n0.04\n0.06\n0.06\n0.07\n0.08\n0.08\n0.10\n0.11\n0.09\n0.09\nMonoLLM GPT CI LB\n0.01\n0.02\n0.02\n0.02\n0.03\n0.03\n0.05\n0.06\n0.04\n0.04\nMonoLLM GPT CI UB\n0.08\n0.10\n0.11\n0.11\n0.12\n0.13\n0.15\n0.16\n0.14\n0.15\nMonoLLM Gemini Mean\n0.04\n0.02\n0.06\n0.06\n0.07\n0.08\n0.11\n0.10\n0.10\n0.15\nMonoLLM Gemini CI LB\n0.01\n0.01\n0.02\n0.03\n0.04\n0.05\n0.07\n0.05\n0.06\n0.09\nMonoLLM Gemini CI UB\n0.07\n0.04\n0.10\n0.10\n0.11\n0.12\n0.16\n0.15\n0.14\n0.20\nER Mean\n0.05\n0.05\n0.06\n0.08\n0.09\n0.10\n0.12\n0.12\n0.13\n0.14\nER CI LB\n0.01\n0.02\n0.03\n0.04\n0.05\n0.05\n0.06\n0.07\n0.07\n0.08\nER CI UB\n0.08\n0.09\n0.09\n0.12\n0.13\n0.15\n0.17\n0.18\n0.19\n0.20\nGreedy Mean\n0.05\n0.05\n0.07\n0.07\n0.09\n0.09\n0.10\n0.10\n0.11\n0.13\nGreedy CI LB\n0.02\n0.02\n0.03\n0.03\n0.05\n0.04\n0.05\n0.05\n0.06\n0.07\nGreedy CI UB\n0.09\n0.09\n0.10\n0.11\n0.14\n0.14\n0.15\n0.15\n0.17\n0.19\nRandom Mean\n0.05\n0.05\n0.08\n0.11\n0.12\n0.14\n0.14\n0.16\n0.16\n0.14\nRandom CI LB\n0.01\n0.02\n0.04\n0.05\n0.06\n0.08\n0.08\n0.09\n0.09\n0.09\nRandom CI UB\n0.08\n0.09\n0.13\n0.16\n0.18\n0.21\n0.20\n0.22\n0.22\n0.20\nTS Mean\n0.05\n0.08\n0.11\n0.10\n0.12\n0.15\n0.15\n0.16\n0.17\n0.18\nTS CI LB\n0.01\n0.03\n0.06\n0.05\n0.06\n0.09\n0.09\n0.10\n0.10\n0.11\nTS CI UB\n0.08\n0.13\n0.17\n0.15\n0.17\n0.22\n0.21\n0.23\n0.23\n0.24\nUCB Mean\n0.05\n0.05\n0.06\n0.07\n0.09\n0.09\n0.10\n0.10\n0.12\n0.13\nUCB CI LB\n0.02\n0.02\n0.03\n0.03\n0.05\n0.04\n0.05\n0.05\n0.06\n0.07\nUCB CI UB\n0.09\n0.09\n0.10\n0.10\n0.13\n0.14\n0.15\n0.15\n0.17\n0.19\nTurn\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMethod\nMonoLLM GPT Mean\n0.05\n0.09\n0.07\n0.09\n0.05\n0.06\n0.07\n0.08\n0.09\n0.11\nMonoLLM GPT CI LB\n0.01\n0.05\n0.03\n0.05\n0.02\n0.02\n0.03\n0.04\n0.04\n0.06\nMonoLLM GPT CI UB\n0.08\n0.13\n0.10\n0.14\n0.08\n0.10\n0.11\n0.13\n0.13\n0.17\nMonoLLM Gemini Mean\n0.02\n0.03\n0.04\n0.07\n0.07\n0.11\n0.11\n0.13\n0.10\n0.16\nMonoLLM Gemini CI LB\n0.01\n0.02\n0.02\n0.04\n0.03\n0.06\n0.07\n0.08\n0.06\n0.10\nMonoLLM Gemini CI UB\n0.03\n0.05\n0.07\n0.10\n0.10\n0.15\n0.15\n0.18\n0.13\n0.21\nER Mean\n0.06\n0.05\n0.08\n0.11\n0.11\n0.13\n0.14\n0.15\n0.16\n0.16\nER CI LB\n0.02\n0.02\n0.04\n0.06\n0.06\n0.08\n0.08\n0.09\n0.09\n0.10\nER CI UB\n0.10\n0.08\n0.13\n0.16\n0.16\n0.18\n0.19\n0.22\n0.22\n0.22\nGreedy Mean\n0.06\n0.06\n0.07\n0.10\n0.11\n0.13\n0.15\n0.15\n0.16\n0.17\nGreedy CI LB\n0.02\n0.02\n0.03\n0.05\n0.06\n0.07\n0.09\n0.09\n0.10\n0.11\nGreedy CI UB\n0.10\n0.09\n0.11\n0.15\n0.17\n0.18\n0.21\n0.21\n0.23\n0.24\nRandom Mean\n0.05\n0.03\n0.03\n0.03\n0.04\n0.04\n0.05\n0.06\n0.07\n0.07\nRandom CI LB\n0.02\n0.01\n0.01\n0.00\n0.01\n0.01\n0.02\n0.02\n0.03\n0.02\nRandom CI UB\n0.08\n0.06\n0.06\n0.05\n0.06\n0.07\n0.08\n0.10\n0.11\n0.11\nTS Mean\n0.06\n0.06\n0.06\n0.08\n0.09\n0.10\n0.12\n0.13\n0.13\n0.13\nTS CI LB\n0.02\n0.02\n0.02\n0.03\n0.05\n0.05\n0.06\n0.07\n0.07\n0.07\nTS CI UB\n0.10\n0.09\n0.10\n0.12\n0.14\n0.14\n0.17\n0.19\n0.18\n0.19\nUCB Mean\n0.06\n0.06\n0.08\n0.11\n0.13\n0.14\n0.16\n0.16\n0.17\n0.17\nUCB CI LB\n0.02\n0.02\n0.03\n0.06\n0.07\n0.08\n0.10\n0.09\n0.11\n0.11\nUCB CI UB\n0.10\n0.09\n0.12\n0.16\n0.18\n0.19\n0.23\n0.22\n0.24\n0.24\n<div style=\"text-align: center;\">Table 4: MRR@10 Mean and 95% C.I. for PEBOL-P vs MonoLLM on Yelp without Response Nois\n</div>\nTurn\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMethod\nMonoLLM GPT Mean\n0.03\n0.05\n0.04\n0.05\n0.07\n0.07\n0.07\n0.09\n0.09\n0.12\nMonoLLM GPT CI LB\n0.01\n0.01\n0.01\n0.02\n0.03\n0.03\n0.03\n0.04\n0.04\n0.06\nMonoLLM GPT CI UB\n0.05\n0.08\n0.07\n0.08\n0.10\n0.12\n0.12\n0.14\n0.14\n0.17\nMonoLLM Gemini Mean\n0.06\n0.09\n0.11\n0.09\n0.10\n0.14\n0.13\n0.16\n0.20\n0.17\nMonoLLM Gemini CI LB\n0.02\n0.05\n0.06\n0.05\n0.06\n0.08\n0.09\n0.10\n0.14\n0.12\nMonoLLM Gemini CI UB\n0.10\n0.14\n0.15\n0.13\n0.14\n0.20\n0.18\n0.21\n0.26\n0.23\nER Mean\n0.06\n0.",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of preference elicitation (PE) in personalized conversational recommendation systems, particularly in cold-start settings where user preferences must be quickly identified. Traditional methods, such as monolithic large language models (LLMs), struggle with multi-turn decision-theoretic reasoning, while Bayesian optimization methods cannot generate arbitrary natural language queries or interpret user preferences effectively.",
        "problem": {
            "definition": "The problem defined in this paper is the need for an effective method to elicit user preferences through natural language dialogue, particularly when users are unfamiliar with the items being recommended.",
            "key obstacle": "The main obstacle is that existing methods either require users to provide explicit ratings or comparisons of items, which is unrealistic in cold-start scenarios where users may not have prior knowledge of the items."
        },
        "idea": {
            "intuition": "The intuition behind the proposed method is to leverage the strengths of both Bayesian optimization and LLMs to create a framework that can efficiently elicit user preferences through natural language.",
            "opinion": "The proposed idea is to formulate natural language preference elicitation within a Bayesian optimization framework, allowing for active elicitation of user preferences through natural language feedback.",
            "innovation": "The innovation lies in the introduction of PEBOL, a novel algorithm that integrates Bayesian optimization with LLM-based acquisition functions, enabling the generation of natural language queries that balance exploration and exploitation."
        },
        "method": {
            "method name": "PEBOL",
            "method abbreviation": "P reference E licitation with B ayesian O ptimization augmented L LMs",
            "method definition": "PEBOL is defined as a framework that uses Bayesian optimization to inform LLM-based queries for eliciting user preferences in natural language.",
            "method description": "PEBOL employs decision-theoretic strategies to generate queries that explore user preferences while maintaining Bayesian beliefs about item utilities.",
            "method steps": [
                "Establish a prior belief on user-item utilities.",
                "Select an item description based on Bayesian optimization policies.",
                "Generate a natural language query using the selected item description.",
                "Receive user feedback and update the belief state based on the response."
            ],
            "principle": "PEBOL is effective because it combines Bayesian inference with natural language processing, allowing for a systematic approach to discover user preferences through dialogue."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using controlled simulations with multiple datasets (MovieLens, Yelp, Recipe-MPR) and compared PEBOL against monolithic LLM baselines (GPT-3.5 and Gemini-Pro).",
            "evaluation method": "Performance was assessed using the Mean Reciprocal Rank (MRR@10) of the preferred item after 10 turns of dialogue."
        },
        "conclusion": "The experiments demonstrated that PEBOL outperformed monolithic LLM methods, achieving a mean MRR@10 of 0.27 compared to the best baseline's 0.17, indicating the effectiveness of the proposed method in eliciting user preferences.",
        "discussion": {
            "advantage": "The key advantages of PEBOL include its ability to actively elicit user preferences through natural language, its integration of decision-theoretic reasoning, and its adaptability to cold-start scenarios.",
            "limitation": "A limitation of PEBOL is its reliance on simulated user interactions, which may not fully capture the complexities of real user behavior.",
            "future work": "Future research directions include conducting user studies to validate the findings, exploring collaborative belief updates, and integrating PEBOL into existing conversational recommendation architectures."
        },
        "other info": {
            "info1": "The code for PEBOL is publicly available at https://github.com/D3Mlab/llm-pe.",
            "info2": {
                "info2.1": "PEBOL uses Natural Language Inference (NLI) to maintain Bayesian preference beliefs.",
                "info2.2": "The method employs Thompson Sampling (TS) and Upper Confidence Bound (UCB) strategies to guide query generation."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the challenge of preference elicitation (PE) in personalized conversational recommendation systems, particularly in cold-start settings where user preferences must be quickly identified."
        },
        {
            "section number": "3.2",
            "key information": "The proposed idea is to formulate natural language preference elicitation within a Bayesian optimization framework, allowing for active elicitation of user preferences through natural language feedback."
        },
        {
            "section number": "4.1",
            "key information": "PEBOL combines Bayesian inference with natural language processing, allowing for a systematic approach to discover user preferences through dialogue."
        },
        {
            "section number": "8.1",
            "key information": "The innovation lies in the introduction of PEBOL, a novel algorithm that integrates Bayesian optimization with LLM-based acquisition functions, enabling the generation of natural language queries that balance exploration and exploitation."
        },
        {
            "section number": "10.1",
            "key information": "A limitation of PEBOL is its reliance on simulated user interactions, which may not fully capture the complexities of real user behavior."
        }
    ],
    "similarity_score": 0.73573025893439,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5b2/e5b26fa9-3303-47bc-9617-261e4b27139e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/62b6/62b6bbf0-4974-4c99-9aa3-53574b032df1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/308e/308ed71f-5ba9-4118-857e-68c32301aefc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d498/d49882a1-b8ea-4c56-95ec-faffea241498.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/657e/657e46d0-f868-4dc4-888a-8571b0426dbd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f232/f2322130-5be0-4797-855d-f1b888cd8a47.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/34d2/34d2203b-2177-4e7f-8585-70805b9fd85c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c00/1c00ba01-e8e1-4782-a4d6-c1d9a39487d7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bcf4/bcf46484-7ce2-48e6-863f-83f83a5e38d4.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f554/f554bce9-6b77-4ad1-8700-d0ba95a698e3.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/21c7/21c7a973-49e9-45c3-bab2-ea0566d4ceb1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7aba/7aba5e09-33e1-4b4d-af64-64e114e3fa35.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3a16/3a1646a1-4875-45f0-8d44-b67e6f9df67e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d855/d85515ae-f5ec-4bf0-8886-8dc7de2b8318.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/01eb/01ebb22f-aac3-4221-a2dd-1b672568b2f1.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Bayesian optimization with llm-based acquisition functions for natural language preference elicitation.json"
}