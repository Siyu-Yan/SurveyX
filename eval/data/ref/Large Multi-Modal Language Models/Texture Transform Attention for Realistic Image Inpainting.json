{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2012.04242",
    "title": "Texture Transform Attention for Realistic Image Inpainting",
    "abstract": "Over the last few years, the performance of inpainting to fill missing regions has shown significant improvements by using deep neural networks. Most of inpainting work create a visually plausible structure and texture, however, due to them often generating a blurry result, final outcomes appear unrealistic and make feel heterogeneity. In order to solve this problem, the existing methods have used a patch based solution with deep neural network, however, these methods also cannot transfer the texture properly. Motivated by these observation, we propose a patch based method. Texture Transform Attention network(TTA-Net) that better produces the missing region inpainting with fine details. The task is a single refinement network and takes the form of U-Net architecture that transfers fine texture features of en-",
    "bib_name": "kim2020texturetransformattentionrealistic",
    "md_text": "# Texture Transform Attention for Realistic Image Inpainting\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4042/40421e41-8353-4ecf-b6f2-942103516641.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a4e/2a4edffd-3b2a-406b-9d89-5535b845fd8c.png\" style=\"width: 50%;\"></div>\nFigure 1: Image inpainting results generated by the proposed system build on Texture Transfer Attention. Each triad displays the original image, the image with the damaged image masked in white, and the result of image inpainting. The results show high performance in expressing texture detail on variety of images.\n<div style=\"text-align: center;\">Figure 1: Image inpainting results generated by the proposed system build on Texture Transfer Attention. Each triad displays the original image, the image with the damaged image masked in white, and the result of image inpainting. The results show high performance in expressing texture detail on variety of images.</div>\n# Abstract\nOver the last few years, the performance of inpainting to fill missing regions has shown significant improvements by using deep neural networks. Most of inpainting work create a visually plausible structure and texture, however, due to them often generating a blurry result, final outcomes appear unrealistic and make feel heterogeneity. In order to solve this problem, the existing methods have used a patch based solution with deep neural network, however, these methods also cannot transfer the texture properly. Motivated by these observation, we propose a patch based method. Texture Transform Attention network(TTA-Net) that better produces the missing region inpainting with fine details. The task is a single refinement network and takes the form of U-Net architecture that transfers fine texture features of en-\ncoder to coarse semantic features of decoder through skipconnection. Texture Transform Attention is used to create a new reassembled texture map using fine textures and coarse semantics that can efficiently transfer texture information as a result. To stabilize training process, we use a VGG feature layer of ground truth and patch discriminator. We evaluate our model end-to-end with the publicly available datasets CelebA-HQ and Places2 and demonstrate that images of higher quality can be obtained to the existing state-of-theart approaches\n# 1. Introduction\nImage inpainting is an approach to plausibly synthesize alternative contents into missing regions [2] damaged or non-critical information. In computer vision, image inpaint-\ning has been focused on challenging topics and has produced a considerable progress over the decades [4, 1, 8, 19, 25] and it has been applied to many tasks such as old photo restoration, image super resolution, crop/stitching, video inpainting and many others. The core challenge of image inpainting is to generate high-level semantically-reasonable and visually-realistic texture details for the missing regions [4, 9, 15, 27, 33, 23]. Existing approaches are roughly divided into two broad groups: a texture synthesis approach using a low-level image feature and a feed-forward generative model using deep convolution networks. The texture synthesis approach [1, 5] can synthesize plausible stationary texture, via leveldiffusion and patch-based algorithm. The patch-based algorithm [1] iteratively searches a similar patch in background and pastes it into the missing region to synthesize a visually-realistic result. This approach works especially well simple composition and complex textures such as natural scene, however, it cannot hallucinate a novel image that contain the missing regions with high-semantic context or does not have adequate patches in the background. To solve this problem, the feed-forward generative model approach [12, 13, 15, 18, 20, 33, 23] proposes to encode the semantic context of image into a feature space using deep neural network and decode the semantic patch to generate semantically-reasonable results. Unfortunately, however, this approach looks visually-blurry due to the loss of details caused by the repeated convolutions and poolings. In order to ensure high-quality image inpainting performance, we propose a method of extracting the feature of each multi-layer, and efficiently transmitting them to decode the features back into the result. First we adopt U-Net structure [26] that deliver multi feature from encoder to decoder via skip-connection. Second, we propose a Texture Transform Attention (TTA) module to efficiently transfer texture information to the result. The conventional patchbased networks [33, 34, 35] method calculates the similarity of each patch through softmax in channel-wise and does summation by each weights. However, if there are many similar patches in the background, multiple patches are summated based on similar weights and the result appears blurry. To solve this problem, we search for the most similar patch and solely use the index and similarity weight on the patch to reflect in the result. Our method also applies a kernel of the same size to different resolutions for each layers, and changes the receptive field size to extract visual information such as semantic context of lower layers and texture details of upper layers. Third, we propose a highly simple approach of synthesis texture without using complex fusion or summation that cause blur. This network is one refinement network for fast and accurate training to learn high-level contexts for realistic texture synthesis. The main pipeline without the skip-\nconnection generates the contextual structure in the missing regions and synthesizes the texture components transferred through the Texture Transform Attention for each layer. The TTA-Net is optimized by minimizing the reconstruction loss, adversarial GAN loss [6], VGG based perceptual loss [16] and style loss [7]. Experiments were conducted with publicly available datasets that could well represent inpainting performance such as faces, textures, and natural objects. Example results are shown in Figure 1. We highlight our contributions as follow:\n\u2022 We propose a novel network utilizing a U-Net structure that directly transfers the encoded texture to the decoder by adding skip-connection to the already verified encode-decode image inpainting network.\n\u2022 We propose a novel network utilizing a U-Net structure that directly transfers the encoded texture to the decoder by adding skip-connection to the already verified encode-decode image inpainting network. \u2022 For the more efficient texture transfer, we propose the Texture Transform Attention module that searches the most similar patch. The TTA module find the index and similarity weight of the patch, reassembles the texture accordingly, and deliver it to the decoder. \u2022 Our model namely, TTA-Net, can synthesize an image of more fine texture by iterative application of deep and shallow textures using a feature synthesis module. \u2022 Our feed-forward generative network achieves highquality inpainting result on variety of challenging datasets including CelebA faces [22], CelebA-HQ faces [17], DTD textures [3] and Places2 [30].\n# 2. Related Work\nIn computer vision, image inpainting has been focused on challenging topics and has produced significant progresses during the last decades [4, 1, 8, 20, 25]. Inpainting researches can be largely divided into two categories: non-learning and learning inpainting approaches. The nonlearning approach, is traditional diffusion-based or patchbased with low-level features. However, the learning approach is based on the deep neural network method that is most actively studied recently. This method learns convolution layer that predict the contents and pixels of the missing regions. Traditional non-learning approaches such as [4, 2, 3, 5] can either propagate surrounding information or copying information from similar patches in the background to fill in missing regions. These methods are effective for stationary and repetitive texture information, but are limited only for non-stationary data that are locally unique. Huang et al. [9] blended the known regions into target regions to minimize discontinuities. Simakov et al. [24] proposed a bidirectional patch similarity-based scheme to better model\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a743/a7438b17-5c45-4b99-b1ae-324f6d4f71bb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"> our framework with Texture Transform Attention module and feature sy</div>\nanomalous visual data for re-targeting and inpainting applications. However, the approaches [9, 24] require very high expensive operation that dense computation of patch similarity. To solve this problem, Barnes et al. [1] proposed a Patch-Match method, the fastest neighboring field algorithm using random initializations. Patch-Match shows a significantly better performance before the emergence of learning-based methods, and has been applied to a number of image editings. In recent years, as researches on deep learning have been actively conducted, the paradigm of image inpainting has also been changed based on GAN-based approaches. The first deep neural networks for inpainting, Context encoder [25], firstly train deep neural networks for inpainting large holes, proposed a method of filling the missing regions with semantic information through feature learning and adversarial loss with novel encoder-decoder pipeline. However, it performs poorly in generating fine-detailed textures. Iizuka et al. [12] proposed a generative model for high-resolution images using local and global discriminator and expanded the receptive field using dilated convolution. However, additional post-processing step is required to maintain color consistency near hole boundaries. Yu et al [33] proposed a generative network that create stacks that fill the pixels of missing regions with similar patches from the background to ensure color and texture consistency in the newly created areas and the surroundings. Pconv [21] is designed to eliminate the mask effect through re-normalization by distinguishing the valid pixels of irregular masks. Yu et al. [34] suggested learning of the dynamic feature selection mechanism for each channel and spatial location, which provides\nbetter visual results. PEN-net [35] proposed a network that can effectively transmit high-semantic information from encoder to decoder by using a U-net structure that utilizes skip connection for multiple layers.\n# 3. Approach\nIn this section, we introduce the proposed Texture Transformer Attention for Realistic Image. We first describe the overview of an inpainting network in Figure 2 and details of Texture Transformer Attention in Figure 3.\n# 3.1. The Overall Pipeline\nThe overall pipeline of proposed TTA-Net mechanism is illustrated in Figure 2. This framework is a single refinement network that consist of with 4 modules, i.e., a encoder feature extractor, a Texture Transform Attention with skipconnection, a texture fusion multi decoder and a discriminator. The TTA-Net is a U-Net structure based on performance verified in-painting model [13, 25, 21], which can extract multi-layered latent features from encode and transmits them to decode through skip-connection. The twostage network, which is widely used in recent years, is unstable in training due to its long parameter length. To improve training stability and maintain semantic performance, we propose therefore a single refinement network that applies VGG losses. The convolutions used in this framework(Figure 2), uses gated convolutions [34], which are excellent for removal of mask effects, the discriminator and TTA module on the other hand use vanilla convolution. The feature extractor in encoder works at each resolution\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d11/1d1188f5-6da6-4903-bf49-73fec68101c8.png\" style=\"width: 50%;\"></div>\nFigure 3: Illustration of the Texture Transform Attention layer. First, unfold the context feature and fine texture feature to same size for calculating the similarity weight (as convolution filters). These similarity weight of all patches are compared channel-wise compared to find the index and weight of the most similar patch. Then we generate reassembled texture map by folding texture features according to the index map. The texture map and weight map are sent to the feature synthesis module and synthesized with the context feature.\nto extract a compact latent feature that is not damaged by iterative convolutions and poolings. As the compact latent features encode the multi feature information, high-level semantics of the context and low-level texture details are decoding via skip-connection. Dilated convolutions [13, 32] that are performed several times to fill a missing hole, creates a coarse semantic context feature that becomes the positional reference for generation of the Texture Transform Attention. The TTA module compares the generated semantic context feature and fine texture features transmitted from the encoder, reconstructs texture map using the index map of most similar patches. In decoder, a reassembled texture maps and context feature are synthesized multiple times to produce the result. We adopt a patch discriminator [34, 14] for faster training and more stability. Also we use reconstruction loss and\nVGG loss [16, 7] that compare the ground truth and output of network.\n# 3.2. Texture Transform Attention\nThe proposed Texture Transform Attention is to compensate for the blurry decoding context information by using the encoded information with various fine features. Conventional attention methods usually use the sum of similarity weights [33, 34, 14], but if there are many similar patches (ground, sand, bushes, etc.), similar weights will overlap, resulting in a blur. To overcome blur issue, we refer to a reference SR [31, 37, 38] method of swap with or add fine texture components to the coarse image. When applied to inpainting network, we can assume that the coarse image is a context feature Q created by dilated convolutions or upsampling, and fine texture is a undamaged feature P extracted from encoder. Therefore, we aim to compare two different features as shown in Figure 3, reassemble the texture feature P semantically similar and synthesize it to the context feature Q. Relevance Embedding. Relevance embedding aims to embed the relevance between the encoded texture features P and decoded context features Q by calculating the matching score. We unfold P (without missing regions) and Q(missing regions) into 5x5 size patches of pj \u2208P and qi \u2208Q respectively. By unfolding different resolutions into patches of same size and changing the receptive fields of each layer, different attention maps are created to help consist inpainting performance even when a new semantic context is created. When calculating the similarity between P and Q, we cut the resolution in half and reduce the patch size to 3x3 to speed up the comparison and save resources. We calculate the relevance si,j between these two patches by normalized inner product (cosine similarity): \ufffd \ufffd\n(1)\n\u2225\u2225 \u2225\u2225 where si,j represents similarity of patch of fine texture components pi and coarse semantic patch qj. This similarity si,j is used to create a reassembled texture map T and fusion of Q and T. Feature Swapping. We propose a method of creating reassembled texture map T that reconstruct the details of the fine texture P in the form of the semantic feature Q. The conventional attention methods take a weighted sum of channel-wise patches, but this weakens the transmission performance of the fine texture and lacks detail. Synthesizing solely the most relevant patch pj at the position of qi shows better performance in detail expression. First, channel-wise comparison of the similarity si,j is performed on each patch to find the pj patch most similar to qi and generate an index map H for location.\n(2)\nwhere hi is the index representing the location of the most relevant patch in the fine texture P for the i-th element patch of the semantic feature Q. By applying the index map H to transfer the patches of fine texture P, we can generate a reassembled texture layer T that can be applied to decoding.\n(3)\nwhere ti denotes the value of T in the i-th position, which is selected from hi-th position of P. Summary, reassembled texture map T is obtained by reconstruction texture feature P according to the semantic shape of Q, and transmitted to the decoder and reflect in the result\n# 3.3. Similarity weight texture synthesis\nFusion ration map. We propose a method to synthesize a semantic feature Q and a reassembled texture T in the decoder. The context features that created in decoder may not have similar patches in the background or have low similarity, so we need to ensure that not all patches are synthesized at the same ratio for preventing a ghost effect. To solve the problem, our method refers to the similarity weight value si,j calculated above and does a channel-wise comparison to search the highest ratio ri for fusion. The ratio map R represent confidence of the reassembled texture map for each position in T :\n(4)\nwhere ri denotes the i-th position of fusion ratio map of R. Similarity Fusion. The fusion ratio map R is used to synthesize the decoder context feature Q and reassembled texture map T. Instead of directly applying R to T, we first concatenate Q and T and perform convolution layer. The fused features are element-wisely multiplied by the ratio map R and added back to Q. This operation can be represented as:\n(5)\nwhere F is same to the semantic feature Q and Ffus indicated the synthesized output features. Conv represents a convolution layer and Concat indicates a concatenation operation. The operator \u2299denotes an element-wise multiplication between feature maps. However, since each patch is simply multiplied by a different ratio ri and added to F, a color distortion occurs that results checkerboard effect as shown in Figure 7. To solve this problem, we used normalization factor (1+R) and multiplied Ffus by element-wise.\n(6)\nwhere Fout indicates the synthesized output features.\nIn summary, the Texture Transform Attention effectively transfers relevant fine texture P to semantic context feature Q, making resulting images more realistic.\n# 3.4. Training objective\nThe factors considered when choosing loss are: 1) to improve the spatial structure of inpainted missing region, 2) to create a plausible visual quality of resulting image, and 3) to take advantage of rich texture from the encode feature extractor. Our objective function combines reconstruction loss Lrec, perceptual loss Lper, adversarial loss Ladv and style loss Lstyle. We experimentally tested the appropriate hyperparameters \u03bb and experiments with the conditions of \u03bbrec = \u03bbadv = 1, \u03bbper = 0.1 and \u03bbstyle = 100.\n(7)\nReconstruction loss. The reconstruction loss contributes to create an approximate shape by comparing the predicted image to ground truth. We can generate a more accurate Ipred by adopting an l1-norm instead of a MSE\n(8)\nAdversarial loss. Adversarial loss can significantly improve the structural/visual quality of this synthesized image. We adopt the SN-PatchGAN loss [34] for more stable training and improve semantic information and local texture. Dsn denotes a spectral-normalized discriminator and G indicates an inpainting network that receives an incomplete image z = Igt \u2299(1 \u2212M) as input.\n(10)\n\u223c Perceptual loss. We only use a single segmentation network for precise training, rather than a two-stage network consisting of coarse and refinement network that are widely used. We adopted perceptual loss [16] for visual quality improvement. Lper defines the Euclidean distance value of activation map using pre-trained VGG16, penalizing if the label and perceptual similarity are not close.\n(11)\nwhere V and C indicate the volume and channel number of the feature map, respectively, and \u03c6vgg i is the activation map of the i-th layer of the pre-trained VGG network. Style loss. We adopted style loss [7] to create more realistic texture detail. Style loss computes the difference between covariance in the activation map and uses a gram matrix G to avoid checkerboard artifacts due to convolution\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2abc/2abc6833-e90a-4577-96ec-ae75731a5444.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) PConv(online demo) (c) EdgeConnect)</div>\n<div style=\"text-align: center;\">(a) Input</div>\nFigure 4: Comparison of qualitative results with models that proven performance on Places validation sets. Best viewed (especially texture) with zoom-in.(a) Input is ground truth with mask. (b) Partial convolution [21]. (c) EdgeConnect [23]. (d) Gated conv [34]. (f) Ours.\n# layers.\nLstyle = \u2225G(\u03c6vgg i (Icomp) \u2212G(\u03c6vgg i (Ipred))\u22251\n(12)\n# 3.5. Segmentation mask generation\nInpainting is often used as an object removal, and we can therefore assume that the object to be person or something user do not want. Therefore, to create a mask that real user will make, we used an instance segmentation [11] algorithm to segment the objects in the places2 dataset and create a mask. Specially, the missing region of the mask is 10 \u223c40% of the total size, and the category is limited to people, animal, statues and etc. About 1,000 masks were created and used as a validation mask set. In the training phase, we conducted various case studies using NVIDIA random masks [21], and we aimed to improve user-friendly performance using the created segmentation mask for validation.\n# 4. Experimental Results\nThe proposed system was evaluated of Places2 [30], Celeb-HQ faces [18] and DTD textures [3], in term of quan-\n<div style=\"text-align: center;\">(d) DeepFillv2</div>\nMethod\nL1 loss\u2020\nMS-SSIM\u00b6\nFID\u2020\nLPIPS\u2020\nDeepFillv1\n6.99\n80.26\n110.9\n0.1195\nDeepFillv2\n4.46\n85.58\n48.62\n0.0687\nEdgeConnect\n3.8\n86.72\n44.96\n0.0609\nOurs\n4.36\n85.3\n43.52\n0.0595\nTable 1: Quantitative comparison on validation images of Places2 with L1 Loss, MS-SSIM, FID and LPIPS. We use free-form mask where the missing region is 10 to 40% of mask size. \u2020 Lower is better. \u00b6 Higher is better.\ntitative and qualitative. Our model was trained using an NVIDIA RTX GPU with a 256x256 resolution size with batch size of 16. In the training phase, the datasets of CelebA-HQ and DTD were downsampled to 256x256 resolution from their original size and Places2 is randomly cropped for texture learning. Basically the model was trained with pytorch v3.6, CUDNN v7.0, CUDA v10.0. It took about 0.15 seconds to test a 512x512 resolution image, regardless of a hole size, and no additional pre- or post-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f264/f264a17d-47e0-4e72-befa-3f151ae3edcf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/966a/966a0881-4d45-4a24-9621-a3ef39b97a40.png\" style=\"width: 50%;\"></div>\nFigure 5: Example results generated by the proposed network on CELEB-HQ.\n# 4.1. Quantitative Results\nLike other image generation tasks, image inpainting lacks good quantitative evaluation metrics. Even if the inpainted region is not the same as the ground truth, it is acceptable to the user if it has a plausible structure and texture. The results listed in Table 1 show the performance of our model and baseline on 512x512 resolution validation image of Places2 with free-form random masks. We report our evaluation in terms of reconstruction L1 loss, multi-scale structural similarity(MS-SSIM) [29], Frechet Inception Distance(FID) [10] and Learned Perceptual Image Patch Similarity(LPIPS) [36]. The L1 roughly reflect model\u2019s performance to reconstruct the original image content. MS-SSIM extracts and evaluates the similarity of structural information from paired images at multiple scales, providing results that approximate human visual perception. These metrics assume pixel-wise independence, which may assign favorable scores to perceptually inaccurate results. Therefore, we include evaluation metrics that use a deep feature based on human perception. FID measures a Wasserstein-2 distance between the feature space representations of real and inpainted images using a pretrained Inception-V3 model [28]. LPIPS uses pre-trained VGG to measure the perceptual similarity distances to human judgment.\n<div style=\"text-align: center;\">Figure 6: Example results generated by the proposed network on DTD Texture. (Left is input and right is result)</div>\nAs shown in Table 1, our proposed method shows that L1 loss and MS-SSIM are similar to other models and better in FID and LPIPS. These mean that our model have highsemantic inpainting performance similar to other models, but have better texture expression.\n# 4.2. Qualitative Results\nWe compared the proposed model with previous stateof-the-art approaches [21, 33, 34, 23]. In order to compare the texture generating performance, a qualitative comparison was performed by challenging images from the places2 dataset with complex and irregular textures with an original size. In Figure 4 the entire image and enlarged image patch are displayed together to compare the semantic context and texture of the image. Figure 5 and Figure 6 show the results of our model using CelebA datasets and DTD texture datasets. We use user custom masks and free-form random masks to compare various cases. As shown in Figure 4: Comparison of qualitative results with models that proven performance on Places validation sets. Best viewed (especially texture) with zoomin.(a) Input is ground truth with mask. (b) Partial convolution [21]. (c) EdgeConnect [23]. (d) Gated conv [34]. (f) Ours., all models have succeeded in making the missing regions plausible, but they have different results. The result of EC [23] was used with similar texture patches repeatedly around the missing region without taking into account the semantic context, giving it a heterogeneous feel. The result of GC [34] has a semantic structure, but the texture looks blurry compared to the surroundings as it accumulates several similar patches. On the other hand, the result of our model has a composition that considers the semantic context of surrounding area, and it can be confirmed that\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec90/ec902998-bf48-4cc6-b66a-941f39e15656.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) With Normalization factor</div>\n<div style=\"text-align: center;\">Figure 7: Example of results that normalize factor comparison</div>\nFigure 7: Example of results that normalize factor comparison\na photo-realistic result appears by creating a texture similar to a background property. Our model is trained at a small resolution of 256x256 like other comparison models, but it shows better results even with test image in a larger size than the training size, as the surrounding features are transferred the low-level layer to the high-level layer in the network. Also, as shown in Figure 6 and Figure 5, our model works well with human faces or complex and repetitive textures.\na photo-realistic result appears by creating a texture similar to a background property. Our model is trained at a small resolution of 256x256 like other comparison models, but it shows better results even with test image in a larger size than the training size, as the surrounding features are transferred the low-level layer to the high-level layer in the network.\nAlso, as shown in Figure 6 and Figure 5, our model works well with human faces or complex and repetitive textures.\n# 4.3. Ablation Study\nSynthesis module comparison. We investigated the effectiveness of the synthesis module by comparing to other synthesis methods including a cross-scale feature synthesis [31], commonly used concatenation method [33, 34] and dilated convolution [35]. The concatenation method is difficult to predict or control how the texture will be reflected in the results simply by attaching texture information to the existing layer. The dilated convolution and cross-scale feature synthesis method naturally combine multiple textures and produce visually sharp results. However, we referenced a soft-attention approach [31] that uses a similarity weight because the performance improvement obtained in these modules is not significant compared to the consumed resources. Whether to use normalization factor. We added normalized factor to our synthesis module. If we simply multiply the similarity weight pixel-wise, the result will have different color level for each region, generating in checkerboard artifacts. As we apply the normalize factor, color normalization can be performed according to the fusion ratio of each patch, which can achieve natural results. In Figure 7, we can see the comparison of results according to use of the normalization factor.\n<div style=\"text-align: center;\">(c) Without Normalization factor</div>\n# 5. Conclusion\nIn this paper, we proposed a novel image inpainting system based on an end-to-end U-Net generative network with a Texture Transform Attention which efficiently transfers a fine texture from the encoder to the decoder. We showed that the TTA module significantly improves the texture representation while preserving the semantic context of result. Also, we proposed a highly simple and effective sysnthesis module to reflect fine texture in the results. Quantitative results and, qualitative comparisons demonstrated the superiority of our proposed TTA-Net. As a future work, we plan to improve the proposed network to image with a higher resolution and modify it to work well with videos.\n# References\n[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Transactions on Graphics (TOG) (Proceedings of SIGGRAPH 2009), 2009. [2] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image inpainting. Proceedings of the 27th annual conference on Com-puter graphics and interactive techniques, page 417\u2013424, 2000. [3] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. Proceedings of the IEEE Conference on Computer Vi-sion and Pattern Recognition, page 3606\u20133614, 2014. [4] Antonio Criminisi, Patrick P\u00b4erez, and Kentaro Toyama. Region filling and object removal by exemplar-based im-age inpainting. TIP, 13(9):1200\u20131212, 2004. [5] A. A. Efros and W. T. Freeman. Image quilting for texture synthesis and transfer. Proceedings of the 28th annual conference on Com-puter graphics and interactive techniques, page 341\u2013346, 2001. [6] A. A. Efros and T. K. Leung. Texture synthesis by nonparametric sampling. Computer Vision, 1999. The Proceedings\nof the Seventh IEEE International Conference, 2:1033\u20131038, 1999. [7] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 2414\u20132423, 2016. [8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.WardeFarley, A. Courville S. Ozair, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, page 2672\u20132680, 2014. [9] J. Hays and A. A. Efros. Scene completion using millions of photographs. ACM Transactions on Graphics (TOG), 2007. [10] Martin Heusel and Hubert Ramsauer. Thomas unterthiner, bernhard nessler, and sepp hochreiter. gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, page 6626\u20136637, 2017. [11] R. Hu, P. Dollar, K. He, T. Darrell1, and R. Girshick. Learning to segment every thing. CVPR, pages 4233\u20134241, 2018. [12] J.-B. Huang, S. B. Kang, N. Ahuja, and J. Kopf. Image completion using planar structure guidance. ACM Transactions on Graphics (TOG), 33(4):129, 2014. [13] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Globally and locally consistent image completion. ACM Transactions on Graphics (TOG, 36(4):107, 2017. [14] P. Isola, J. Zhu, T. Zhou, and A. A Efros. Image-to-image translation with conditional adversarial networks. Proceedings of the IEEE conference on computer vision and pattern recognition, page 1125\u20131134, 2018. [15] M. Jaderberg, K. Simonyan, A. Zisserman, and et al. Spatial transformer networks. Advances in Neural Information Processing Systems, page 2017\u20132025, 2015. [16] Y. Jeon and J. Kim. Spatial transformer networks. Active convolution: Learning the shape of convolution for image classification, arXiv preprint arXiv:1703.09076, 2017. [17] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and su-per-resolution. European Conference on Computer Vision, page 694\u2013711, 2016. [18] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stabil-ity, and variation. arXiv preprint arXiv:1710.10196, 2017. [19] R. K\u00a8ohler, C. Schuler, B. Sch\u00a8olkopf, and S. Harmeling. Mask-specific inpainting with deep neural networks. German Conference on Pattern Recognition, page 523\u2013534, 2014. [20] A. Levin, A. Zomet, S. Peleg, and Y.Weiss. Seamless image stitching in the gradient domain. Computer Vision-ECCV 2004, page 377\u2013389, 2004. [21] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolu-tions. ECCV, page 85\u2013100, 2018. [22] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. Proceedings of International Conference on Computer Vision (ICCV), 2015. [23] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversari-al edge learning. arXiv preprint\n[24] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg. Transformation-grounded image generation network for novel 3d view synthesis. arXiv preprint arXiv:1703.02921, 2017. [25] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. Proceedings of the IEEE Conference on Computer Vi-sion and Pattern Recognition, page 2536\u20132544, 2016. [26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. MICCAI, page 234\u2013241, 2015. [27] D. Simakov, Y. Caspi, E. Shechtman, and M. Irani. Summarizing visual data using bidirectional similarity. Computer and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138, 2008. [28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 2818\u20132826, 2016. [29] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assess-ment. ACSSC, 2:1398\u20131402, 2003. [30] C. Yang, X. Lu, Z. Lin, E. Shechtman, O. Wang, and H. Li. High-resolution image inpainting using multi-scale neural patch synthesis. arXiv preprint arXiv:1611.09969, 2016. [31] Fuzhi Yang, Huan Yang, Jianlong Fu, Hontao Lu, and Bain ing Guo. Learning texture transformer network for image super-resolution. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5791\u2013 5798, 2019. [32] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015. [33] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. CVPR, page 5505\u20135514, 2018. [34] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Image style transfer using convolutional neural networks. Free-form image inpainting with gated convolution, arXiv preprint arXiv:1806.03589, 2018. [35] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Learning pyramid-context encoder network for highquality image inpainting. arXiv preprint arXiv:1904.07475, 2019. [36] R. Zhang, P. Isola, A. A. Efros, E. Shecht-man, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. [37] Zhifei Zhang, ZhaowenWang, Zhe Lin, and Hairong Qi. Image super-resolution by neural texture transfer. CVPR), page 7982\u20137991, 2019. [38] H. Zheng, M. Ji, H.Wang, Y. Liu, and L. Fang. Cross-net: An end-to-end reference-based super resolu-tion network using cross-scale warping. European Conference on Computer Vision (ECCV), 2018.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of realistic image inpainting, highlighting the limitations of existing methods that often produce blurry results and fail to transfer texture effectively. It discusses the need for a new approach to improve the quality and realism of inpainted images.",
        "problem": {
            "definition": "The problem defined in this paper is the inability of existing image inpainting techniques to generate high-quality, semantically-reasonable, and visually-realistic textures in the missing regions of images.",
            "key obstacle": "The main difficulty lies in the existing methods' tendency to produce blurry results due to the loss of detail caused by repeated convolutions and pooling, which prevents effective texture transfer."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that existing methods struggle with texture transfer and often yield unrealistic results, motivating the development of a new approach that can better capture and synthesize texture details.",
            "opinion": "The proposed idea involves a novel network, Texture Transform Attention (TTA-Net), which utilizes a U-Net architecture to refine the inpainting process and enhance texture representation.",
            "innovation": "The key innovation of this method is the Texture Transform Attention module, which efficiently transfers fine texture features from the encoder to the decoder, overcoming the limitations of conventional patch-based networks."
        },
        "method": {
            "method name": "Texture Transform Attention Network",
            "method abbreviation": "TTA-Net",
            "method definition": "TTA-Net is defined as a single refinement network that utilizes a U-Net architecture to transfer fine texture features from the encoder to the decoder through skip connections.",
            "method description": "The method synthesizes a realistic image by iteratively applying texture transfer and semantic feature extraction to fill in missing regions.",
            "method steps": [
                "Extract multi-layered latent features using an encoder.",
                "Apply Texture Transform Attention to reconstruct texture maps.",
                "Synthesize the final output by combining context features and texture maps in the decoder."
            ],
            "principle": "The effectiveness of this method lies in its ability to preserve semantic context while accurately reflecting fine texture details, leading to more realistic inpainting results."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using publicly available datasets, including CelebA-HQ and Places2, with a focus on evaluating the inpainting performance across various challenging images.",
            "evaluation method": "The performance was assessed using quantitative metrics such as L1 loss, MS-SSIM, FID, and LPIPS, alongside qualitative comparisons with state-of-the-art methods."
        },
        "conclusion": "The proposed TTA-Net demonstrates significant improvements in texture representation and semantic context preservation in image inpainting, outperforming existing methods in both quantitative and qualitative evaluations.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include enhanced texture synthesis, preservation of semantic information, and improved overall image quality compared to traditional methods.",
            "limitation": "One limitation identified is the potential for artifacts or inconsistencies when synthesizing textures in regions with low similarity to the background.",
            "future work": "Future research directions include enhancing the network's capabilities for higher resolution images and adapting the method for video inpainting applications."
        },
        "other info": {
            "additional details": {
                "training time": "The model was trained on an NVIDIA RTX GPU, taking approximately 0.15 seconds to test a 512x512 resolution image.",
                "datasets used": [
                    "CelebA-HQ",
                    "Places2",
                    "DTD textures"
                ],
                "loss functions": [
                    "reconstruction loss",
                    "adversarial loss",
                    "perceptual loss",
                    "style loss"
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.3",
            "key information": "The proposed Texture Transform Attention Network (TTA-Net) utilizes a U-Net architecture to refine the inpainting process and enhance texture representation."
        },
        {
            "section number": "5.1",
            "key information": "The effectiveness of TTA-Net lies in its ability to preserve semantic context while accurately reflecting fine texture details, leading to more realistic inpainting results."
        },
        {
            "section number": "6.1",
            "key information": "The experiments evaluated TTA-Net's performance on publicly available datasets, including CelebA-HQ and Places2, demonstrating significant improvements in texture representation and semantic context preservation."
        },
        {
            "section number": "7.1",
            "key information": "One limitation identified is the potential for artifacts or inconsistencies when synthesizing textures in regions with low similarity to the background."
        },
        {
            "section number": "7.3",
            "key information": "Future research directions include enhancing the network's capabilities for higher resolution images and adapting the method for video inpainting applications."
        }
    ],
    "similarity_score": 0.5983788241017427,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Texture Transform Attention for Realistic Image Inpainting.json"
}