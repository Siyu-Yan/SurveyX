{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.08813",
    "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment",
    "abstract": "The burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs. This paper explores a promising middle ground, where we employ a weak LLM that is significantly less resource-intensive than top-tier models, yet offers more automation than purely human feedback. We present a systematic study to evaluate and understand weak LLM's ability to generate feedback for alignment. Our empirical findings demonstrate that weak LLMs can provide feedback that rivals or even exceeds that of fully human-annotated data. Our study indicates a minimized impact of model size on feedback efficacy, shedding light on a scalable and sustainable alignment strategy. To deepen our understanding of alignment under weak LLM feedback, we conduct a series of qualitative and quantitative analyses, offering novel insights into the quality discrepancies between human feedback vs. weak LLM feedback.",
    "bib_name": "tao2024weakllmsecretlystrong",
    "md_text": "# Your Weak LLM is Secretly a Strong Teacher for Alignment\nLeitian Tao1 and Yixuan Li1\n1Department of Computer Sciences, University of Wisconsin-Madison {leitiantao,sharonli}@cs.wisc.edu\n13 Sep 2024\nThe burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs. This paper explores a promising middle ground, where we employ a weak LLM that is significantly less resource-intensive than top-tier models, yet offers more automation than purely human feedback. We present a systematic study to evaluate and understand weak LLM\u2019s ability to generate feedback for alignment. Our empirical findings demonstrate that weak LLMs can provide feedback that rivals or even exceeds that of fully human-annotated data. Our study indicates a minimized impact of model size on feedback efficacy, shedding light on a scalable and sustainable alignment strategy. To deepen our understanding of alignment under weak LLM feedback, we conduct a series of qualitative and quantitative analyses, offering novel insights into the quality discrepancies between human feedback vs. weak LLM feedback.\n[cs.CL]\n# 1 Introduction\nAs we observe the impressive capabilities of large language models (LLMs) across diverse applications (Brown et al., 2020; Achiam et al., 2023; Bubeck et al., 2023; Team et al., 2023; Anthropic, 2023), there emerges a critical need to ensure AI systems are helpful and harmless. AI alignment aims to harmonize AI behaviors with human intentions and values and ensure safe and desirable behavior. A key recipe to achieve alignment involves presenting pairs of responses and collecting binary feedback (e.g., preferred, less preferred) based on the comparative quality of these responses. The prevailing methods in alignment can be categorized based on the source of the feedback. For example, popular framework Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022) relies on pure human judgments, which often involves considerable human labor and manual effort. On the other end of the spectrum, framework such as Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2023) harnesses feedback from high-capacity LLMs to annotate preference datasets, often incurring significant computational and financial costs, along with the need for heavy prompt engineering. These contrasting frameworks highlight two extremes of a feedback spectrum, raising critical questions about the largely untapped middle ground that leverages the strengths of both while alleviating their respective drawbacks. This middle ground entails using significantly smaller LLM that are less resource-intensive than top-tier models, yet offer more automation than purely human feedback. For instance, while a model like GPT-4 might have trillions of parameters, a weak LLM might only have hundreds of millions or even fewer. This smaller size means they inherently demand less computational power, thus reducing operational costs and enabling faster iteration cycles in development. This approach not only aligns with the practical demands of deploying scalable AI solutions but also addresses the need for sustainable development within AI research, making it a prudent choice for tasks where the ultra-high capabilities of state-of-the-art models do not necessarily translate to significant improvements in feedback quality. Despite the appeal, the research community still lacks a systematic evaluation and understanding of weak LLMs\u2019 capability to generate feedback for alignment. Motivated by this gap, we undertake a comprehensive investigation of alignment via weak LLM feedback. Specifically, this paper makes three key contributions: Contribution 1: A framework for evaluating alignment via weak LLM feedback. We formalize a learning and evaluation workflow utilizing feedback from weak LLM instead of traditional human annotations. Our framework operates on the combination of labeled and unlabeled preference datasets. Different from the existing RLHF framework, we leverage data comprising of unlabeled triplet (x, y1, y2), where both y1 and y2 are responses corresponding to\narXiv:2409.08813v1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d01d/d01dc8db-3bd2-4c6c-80a6-c314d4a72045.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Spectrum of feedback for alignment Figure 1: A spectrum of feedback for aligning LLMs, ranging from labor-intensive human annotations (e.g., RLHF (Ouyang et al., 2022)) to highly automated, resource-intensive LLM feedback (e.g., RLAIF (Bai et al., 2022; Lee et al., 2023)). Our work explores the largely untapped middle ground, evaluating weak LLM feedback for alignment.</div>\nprompt x but the preference is unknown. In practice, the unlabeled triplets can be collected in large volumes without the need for human preference annotations. A weak LLM trained on labeled data will provide preference feedback on the large unlabeled data. Finally, we train a target LLM policy based on the weak LLM\u2019s feedback. The training workflow is in reminisce of semi-supervised learning (Zhu, 2005); however, it introduces a novel perspective by connecting the classical framework to alignment\u2014a field that has yet to be thoroughly explored, especially in the context of using weak AI feedback. Contribution 2: A comprehensive evaluation and novel empirical findings (Section 3). We employ the framework to evaluate the impact of weak LLM feedback on alignment across a variety of model scales and diverse model families. Surprisingly, our results reveal that using a weak LLM, with size as small as 125M (Zhang et al., 2022), to provide preference label for alignment can match or even exceed the performance of using full human feedback (see Figure 2 and Figure 3). Moreover, we systematically evaluate alignment performance when the feedback is provided by LLM of varying capacities: a weak supervisor (OPT-125M), a moderate supervisor (OPT-1.3B), a strong supervisor (Llama-3-8B), and a very strong supervisor (GPT-4). We found that the performance under weak, moderate, and strong supervisors is nearly comparable, suggesting that the supervisor model\u2019s size has minimal impact on feedback effectiveness. Notably, the weak LLM, OPT-125M, outperforms the more advanced GPT-4, indicating that a task-specific weak LLM can provide more effective feedback than a larger, more powerful LLM that relies solely on prompt engineering.\nprompt x but the preference is unknown. In practice, the unlabeled triplets can be collected in large volumes without the need for human preference annotations. A weak LLM trained on labeled data will provide preference feedback on the large unlabeled data. Finally, we train a target LLM policy based on the weak LLM\u2019s feedback. The training workflow is in reminisce of semi-supervised learning (Zhu, 2005); however, it introduces a novel perspective by connecting the classical framework to alignment\u2014a field that has yet to be thoroughly explored, especially in the context of using weak AI feedback. Contribution 2: A comprehensive evaluation and novel empirical findings (Section 3). We employ the framework to evaluate the impact of weak LLM feedback on alignment across a variety of model scales and diverse model families. Surprisingly, our results reveal that using a weak LLM, with size as small as 125M (Zhang et al., 2022), to provide preference label for alignment can match or even exceed the performance of using full human feedback (see Figure 2 and Figure 3). Moreover, we systematically evaluate alignment performance when the feedback is provided by LLM of varying capacities: a weak supervisor (OPT-125M), a moderate supervisor (OPT-1.3B), a strong supervisor (Llama-3-8B), and a very strong supervisor (GPT-4). We found that the performance under weak, moderate, and strong supervisors is nearly comparable, suggesting that the supervisor model\u2019s size has minimal impact on feedback effectiveness. Notably, the weak LLM, OPT-125M, outperforms the more advanced GPT-4, indicating that a task-specific weak LLM can provide more effective feedback than a larger, more powerful LLM that relies solely on prompt engineering. Contribution 3: An in-depth analysis on the quality of weak LLM\u2019s feedback (Section 4). To deepen our understanding of alignment under weak LLM feedback and reason our observations, we conduct a series of qualitative and quantitative analyses. A pivotal aspect of our study lies in the examination of quality discrepancies between human feedback vs. weak LLM feedback. Our key observations are threefold: (1) when weak LLM\u2019s chosen response contradicts human feedback, nearly half of these responses exhibit higher quality, suggesting the lack of reliability in human annotation and weak LLM can surpass human judgments; (2) weak LLM\u2019s feedback can be qualitatively similar to human feedback in contexts with clear gap, yet they exhibit uncertainty in judgments when the distinctions in response quality are subtle; and (3) advanced LLMs like GPT-4 also show increased feedback inconsistency in scenarios\nContribution 3: An in-depth analysis on the quality of weak LLM\u2019s feedback (Section 4). To deepen our understanding of alignment under weak LLM feedback and reason our observations, we conduct a series of qualitative and quantitative analyses. A pivotal aspect of our study lies in the examination of quality discrepancies between human feedback vs. weak LLM feedback. Our key observations are threefold: (1) when weak LLM\u2019s chosen response contradicts human feedback, nearly half of these responses exhibit higher quality, suggesting the lack of reliability in human annotation and weak LLM can surpass human judgments; (2) weak LLM\u2019s feedback can be qualitatively similar to human feedback in contexts with clear gap, yet they exhibit uncertainty in judgments when the distinctions in response quality are subtle; and (3) advanced LLMs like GPT-4 also show increased feedback inconsistency in scenarios where response distinctions are minimal, highlighting the challenges faced by strong LLMs in providing feedback.\n<div style=\"text-align: center;\">RLAIF (Bai et al. 2022, Lee et al. 2023)</div>\n# 2 Preliminaries on LLM Alignment\nWe denote \u03c0\u03b8 as a language model policy parameterized by \u03b8, which takes in an input prompt x, and outputs a discrete probability distribution \u03c0\u03b8(\u00b7|x) over the vocabulary space V. \u03c0\u03b8(y|x) refers to the model\u2019s probability of outputting response y given input prompt x. Alignment algorithms operate on comparative data, where pairs of responses are presented, and the model is trained to produce the preferred response given a query. Formally, we define the preference data below. Definition 2.1 (Preference data). Consider two responses yw, yl for an input prompt x, we denote yw \u227byl if yw is preferred over yl. We call yw the preferred response and yl the non-preferred response. Each triplet (x, yw, yl) is referred to as a preference. Furthermore, the empirical dataset D = {(xi, yw,i, yl,i)}n i=1 consists of n such triplets sampled from a preference distribution. Reinforcement Learning from Human Feedback. RLHF is a widely used paradigm for fine-tuning language models based on human preferences (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). The key stages in RLHF are reward modeling and reinforcement learning with the learned reward function. Reward modeling learns a function mapping, which takes in the prompt x and response y and outputs a scalar value r(x, y) signifying the reward. A preferred response should receive a higher reward, and vice versa. Based on the Bradley\u2013Terry model (Bradley & Terry, 1952), the reward function is optimized over a dataset of human preferences, with the following objective:\nReward modeling learns a function mapping, which takes in the prompt x and response y and outputs a scalar value r(x, y) signifying the reward. A preferred response should receive a higher reward, and vice versa. Based on the Bradley\u2013Terry model (Bradley & Terry, 1952), the reward function is optimized over a dataset of human preferences\nwhere \u03c3 denotes the sigmoid function. Using the learned reward function, the model is further fine-tuned with reinforcement learning to maximize the expected rewards, thus promoting exploration and adherence to learned preferences. The optimization objective is formulated as follows:\nwhere \u02c6y represents the response generated by the current policy \u03c0\u03b8 for the prompt x, \u03c0ref indicates the reference policy or the initial policy before running RL optimization, and \u03b2 serves as a hyperparameter to regulate the KL divergence. Training with RLHF can be computationally expensive due to the use of multiple models. As an alternative, Rafailov et al. (2023) proposed to directly optimize for the policy best satisfying the preferences with a simple objective:\nRafailov et al. (2023) showed that under mild assumptions, the optimal policy under the DPO objective (3) is equivalent to the optimal policy under the RLHF objective (2). This objective facilitates a more direct reflection of human preference judgments within the optimization framework.\n# How good is alignment with weak LLM feedb\nIn existing alignment approaches described above, models are often trained on fully supervised data where each preference label is hand-annotated by humans. To systematically evaluate the reliability of using feedback from a weak LLM instead of human feedback, we outline the training workflow in Section 3.1, experimental setup in Section 3.2, and present our main findings in Section 3.3.\n# 3.1 Alignment via Weak LLM Feedback\nConsider two empirical datasets, Dl and Du, representing a labeled preference dataset and an unlabeled dataset respectively. The labeled dataset Dl comprises of triplets (x, yw, yl), where human annotators have indicated a known preference yw \u227byl. Du, on the other hand, comprises of unlabeled triplet (x, y1, y2), where both y1 and y2 are responses corresponding to x but the preference is unknown. A weak LLM, trained on Dl, will provide preference feedback on the unlabeled data Du. This setup offers practical advantages since unlabeled triplets can be collected in large volumes without the need for human preference annotations. For example, one can generate multiple responses by\n(1)\n(2)\n(3)\nquerying varying LLMs with the same prompt. Below, we describe the alignment process using feedback from th weak LLM.\nPreference feedback from the weak LLM. We first train a weak language policy \u03c0w based on the labeled preference dataset Dl. We use the subscript w to indicate \u201cweak\u201d in the remainder of the paper. Specifically, the weak LLM is optimized using the DPO loss (cf. Equation 3), under which the optimal policy is equivalent to that of RLHF:\nwhere \u03c0\u2217 w signifies the optimal policy for the weak model. \u03c0SFT w is the reference model or the initialization, which is an SFT model fine-tuned on the preferred question-answer pairs (x, yw) in Dl. Compared to directly using the untuned base model as a reference model, performing SFT enhances the model\u2019s ability to generate desired responses to questions (see Appendix C). We then generate the weak feedback for unlabeled data Du, leveraging the weak language model \u03c0\u2217 w. For each triplet (x, y1, y2) \u2208Du, we compute the reward rw(x, y1) and rw(x, y2) according to DPO\u2019s implicit reward:\nWe then assign the preference label \u02c6yw for the response with a higher predicted reward, and \u02c6yl for the response with a lower predicted reward. Mathematically:\nWe denote the resulting weakly labeled dataset as Dweak = {(x, \u02c6yw, \u02c6yl)}, where |Dweak| = |Du|.\nAlignment with feedback from the weak LLM. Finally, we train an LLM policy \u03c0\u03b8 based on weak LLM feedback Dweak. The model is aligned using the following objective:\n\u03c0\u2217 \u03b8 = argmin LDPO(\u03c0\u03b8; \u03c0SFT \u03b8 ; Dweak),\nwhere \u03c0\u2217 \u03b8 is the optimal policy based on the weak LLM feedback. To obtain the reference model \u03c0SFT \u03b8 , we fine-tune the base model using question-answer pairs (x, \u02c6yw) in Dweak. The training workflow outlined above is in reminisce of semi-supervised learning; however, it introduces a novel perspective by connecting the classical framework to alignment\u2014a field that has yet to be thoroughly explored. A central unresolved question we address in this paper is how effectively LLMs can be aligned using feedback from weak LLMs as opposed to relying solely on human feedback. The training workflow thus serves as the tool to explore our core research question. To understand this, we conduct a systematic evaluation in the next section.\n# 3.2 Experimental Setup\nDataset. To evaluate the performance, we use the Anthropic HH-RLHF (Helpful and Harmless) dataset (Bai et al., 2022), which is the most commonly used dataset for alignment. The dataset consists of 112,000 training samples and 12,500 test samples and is publicly available. Each sample includes a prompt and two responses, with one being preferred over the other. The selected responses are annotated based on the opinions of crowd workers, who assess which response is more helpful and harmless. We preprocess the dataset by filtering out samples with token lengths less than 512, which yields 100,000 training samples and 11,000 test samples. We split the training data into two disjoint sets. The first subset is used as labeled data Dl, and the remainder is used as the unlabeled data Du (by disregarding the preference labels). We will vary the size of Dl in our ablation. We also evaluate on the Reddit TL;DR (TL;DR) summarization dataset from Stiennon et al. (2020), which consists of a Reddit post and several short summaries, judged for quality and informativeness by human evaluators.\n(4)\n(5)\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7fa/d7fa1100-f4ec-472c-aaaa-267e3df3bb82.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: (a) Alignment with feedback from a weak LLM (OPT-125M) can outperform human feedback. (b) Alignment performanc n OPT-1.3B model under varying capability of supervisor. See Section 3.3 for details.</div>\nModels. For our main experiment, we utilize the OPT family model introduced by Zhang et al. (2022), which provides a spectrum of model sizes. This versatility enables us to evaluate the performance across different levels of model capability. Specifically, we employ OPT models of varying sizes (1.3B, 2.7B, 6.7B, and 13B) as our LLM policy models, trained with feedback from a weak LLM. Additionally, to validate the efficacy of our experiments, we consider more advanced open-sourced models, including Llama-2-7B (Touvron et al., 2023), Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024). For a comprehensive description of the hyper-parameters employed in our experiments, please refer to Appendix A. Evaluation metrics. Given the test set Dtest, we evaluate the generation performance under two policies: \u03c0\u2217 \u03b8 (with weak LLM feedback), and \u03c0\u2217 h (with human feedback). \u03c0\u2217 h is trained with DPO loss on the same set of triplets in Du, except for using the original preference label provided by human annotators. This allows us to estimate the performance achieved with fully supervised data. For a fair comparison, \u03c0\u2217 \u03b8 and \u03c0\u2217 h always share the same model capacity and only differ in the source of feedback. We assess the generation performance using the following metrics: \u2022 Gold reward: Previous studies (Gao et al., 2023; Coste et al., 2024; Xiong et al., 2023) commonly utilize gold reward as a metric for assessing the generation quality of language models. Gold reward is desirable due to the high cost associated with obtaining ground truth gold rewards from human annotators. We employ the output of a large auxiliary gold reward model, denoted as rgold, to evaluate the quality of generated responses. For each test input prompt x from Dtest, we generate a response \u02c6y according to a given language policy, and then compute its gold reward as rgold(x, \u02c6y). A higher gold reward signifies that the model\u2019s responses better align with desired preferences. Our results are consistent under alternative gold reward models; see Appendix C for details. \u2022 GPT-4 win-rate: We employ GPT-4 as a proxy for human evaluation by instructing it to review two responses (from two different policies) to the same prompt. It then assigns a rating on a scale from 1 to 10. A higher win rate indicates that a policy on average produces more favorable answers.\n\u2022 GPT-4 win-rate: We employ GPT-4 as a proxy for human evaluation by instructing it to review two responses (from two different policies) to the same prompt. It then assigns a rating on a scale from 1 to 10. A higher win rate indicate that a policy on average produces more favorable answers.\n# 3.3 Main Results\n# Alignment with weak LLM feedback can outperform human feedback. In Figure 2(a), we evaluate the alignme\nAlignment with weak LLM feedback can outperform human feedback. In Figure 2(a), we evaluate the alignment performance using feedback from the weak LLM vs. human. Employing OPT-125M as our weak LLM to provide supervision (with the lowest capacity in the model family), we align student models of varying capacities (OPT-1.3B, OPT-2.7B, OPT-6.7B, and OPT-13B). Interestingly, the alignment performance using weak LLM feedback closely matches or even surpasses that of using human feedback. The similar alignment performance between \u03c0\u2217 \u03b8 and \u03c0\u2217 h has not been observed in previous studies. Moreover, under the same feedback provided by the weak LLM, the alignment performance improves as the student model capacity increases from 1.3B to 13B. Given that the primary objective of alignment is to enhance the model\u2019s ability to generate content that better resonates with human goals, our findings suggest that leveraging weak LLM feedback is a promising route without access to full human supervision. For an in-depth analysis of the unexpectedly high efficacy of weak LLM feedback, we defer to Section 4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b24/9b24ed2c-bc56-4b48-a4a0-8842e09ec482.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 3: (a) Results on different model families. (b) GPT-4 evaluation for different models aligned with weak LLM feedback  man feedback.</div>\n<div style=\"text-align: center;\">Figure 3: (a) Results on different model families. (b) GPT-4 evaluation for different models aligned with weak LLM feedback  human feedback.</div>\n<div style=\"text-align: center;\">gure 3: (a) Results on different model families. (b) GPT-4 evaluation for different models aligned with weak LLM feedback vs.</div>\nAlignment performance under the varying capability of supervisor. Existing frameworks on learning from AI feedback, such as RLAIF (Lee et al., 2023), employ more advanced LLMs (e.g., GPT-4) to annotate preference datasets for the alignment of student models. To understand the alignment performance under different supervisor models, we systematically consider a spectrum of supervisor LLMs of varying capabilities: (1) weak supervisor based on OPT-125M, (2) moderate supervisor based on OPT-1.3B, (3) strong supervisor based on Llama-3-8B, and (4) very strong supervisor based on GPT-4. For variants (1) through (3), we apply the same training workflow described in Section 3. For variant (4), we utilize GPT-4 as the LLM labeler, following the prompt suggested by Lee et al. (2023). Based on these four different sources of feedback, we compare the alignment performance on the same policy model OPT-1.3B. As shown in Figure 2 (b), the alignment performance under the weak, moderate, and strong supervisors is nearly comparable. This suggests that the size of the supervisor model plays a less impactful role in the effectiveness of feedback. Notably, we observe instances where the weak LLM with a small capacity, OPT-125M, can outperform GPT-4 in providing feedback on preferences. Although GPT-4 is a more advanced model, this finding indicates that a task-specific weak LLM can serve as a more effective supervisor than a larger, more powerful LLM that relies solely on prompt engineering.\n# 3.4 Additional Ablations\nResults on different model families. To further validate our findings, we extend our evaluation to additional model families, including Llama-2-7B (Touvron et al., 2023), Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024). We employ OPT-125M as the weak LLM to provide preference labels. Figure 3(a) displays the average gold rewards of the generated responses, based on policies \u03c0\u2217 \u03b8 (with weak LLM feedback) and \u03c0\u2217 h (with human feedback). The gold rewards achieved using weak LLM feedback consistently surpass those obtained from human feedback. This underscores the effectiveness of alignment through weak LLM feedback across various LLM families. Results under GPT-4 evaluation. Beyond gold reward measurement, we assess the win-rate of the model aligned with weak LLM feedback (\u03c0\u2217 \u03b8) against the human feedback (\u03c0\u2217 h), utilizing GPT-4 to judge the helpfulness and harmlessness within dialogue contexts. We randomly select 100 prompts from the test set of HH-RLHF and employed GPT-4 to determine whether responses generated under the policy \u03c0\u2217 \u03b8 were superior to those from \u03c0\u2217 h. Considering the diversity of model families and sizes, we evaluated the GPT-4 win-rate across five student model variants: OPT-1.3B, OPT-2.7B, OPT-6.7B, Llama-2-7B, and Mistral-7B. The weak LLM feedback all comes from the OPT-125m. As illustrated in Figure 3 (b), the win-rate for policy \u03c0\u2217 \u03b8 (with weak LLM feedback) approaches 50% for all cases, indicating that its performance competitively matches that of using human feedback \u03c0\u2217 h. Results under different data sizes. In this ablation, we systematically investigate the impact of dataset size. We vary the ratio between the labeled dataset size and the full dataset size: 1/16, 1/8, 1/4, 1/2, with the remaining data serving as the unlabeled subset. We train the weak LLM across different ratios with the same steps. We utilize OPT-125M as the weak LLM to align the policy model OPT-1.3B. Notably, as shown in Figure 4 (a), even under a small labeled dataset\nResults on different model families. To further validate our findings, we extend our evaluation to additional model families, including Llama-2-7B (Touvron et al., 2023), Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024). We employ OPT-125M as the weak LLM to provide preference labels. Figure 3(a) displays the average gold rewards of the generated responses, based on policies \u03c0\u2217 \u03b8 (with weak LLM feedback) and \u03c0\u2217 h (with human feedback). The gold rewards achieved using weak LLM feedback consistently surpass those obtained from human feedback. This underscores the effectiveness of alignment through weak LLM feedback across various LLM families.\nResults under different data sizes. In this ablation, we systematically investigate the impact of dataset size. We vary the ratio between the labeled dataset size and the full dataset size: 1/16, 1/8, 1/4, 1/2, with the remaining data serving as the unlabeled subset. We train the weak LLM across different ratios with the same steps. We utilize OPT-125M as the weak LLM to align the policy model OPT-1.3B. Notably, as shown in Figure 4 (a), even under a small labeled dataset\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68f2/68f2af29-6670-4516-8285-b48dac82dc81.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">re 4: (a) Results under different data sizes. (b) Results on different tasks (TL;DR</div>\nsize with a ratio of 1/16, the performance using weak LLM feedback is favorably close to that of using human feedback Results on different datasets and tasks.To examine the effectiveness of weak LLM feedback for alignment across broader NLP tasks, we turn our attention to the Reddit TL;DR dataset (Stiennon et al., 2020), which focuses on summarization\u2014a stark contrast to dialogue tasks, which consists of a Reddit post and several short summaries, judged for quality and informativeness by human evaluators. Similar to the setup for HH-RLHF, we use OPT-125M as the weak LLM and assess the performance of weak LLM feedback v.s. human feedback for different student sizes: OPT-1.3B OPT-2.7B, OPT-6.7B, and OPT-13B. As depicted in Figure 4 (b), the average gold rewards for outputs sampled under the policies \u03c0\u2217 \u03b8 also outperform \u03c0\u2217 h across all student model sizes. This finding reinforces the efficacy of weak LLM feedback for alternative linguistic tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3326/3326a14c-e2c2-423a-8365-f2f47583b701.png\" style=\"width: 50%;\"></div>\n# 4 In-Depth Understanding of Weak LLM\u2019s Feedback\nWhen weak LLM feedback matches/mismatches human feedback. In this ablation, we investigate the impa samples either matching or mismatching human feedback. Adopting OPT-125M as the weak LLM to provide feed for the OPT-1.3B model alignment, we design three controlled settings for this ablation: \u2022 Dweak: Use all samples, {(x, \u02c6yw, \u02c6yl)}, for training. This is the same as our main setting.\nWhen weak LLM feedback matches/mismatches human feedback. In this ablation, we investigate the impact of samples either matching or mismatching human feedback. Adopting OPT-125M as the weak LLM to provide feedback for the OPT-1.3B model alignment, we design three controlled settings for this ablation: \u2022 Dweak: Use all samples, {(x, \u02c6yw, \u02c6yl)}, for training. This is the same as our main setting. \u2022 Dmatch weak : Use samples with weak labels matching the human feedback, i.e., (rw(x, yw) > rw(x, yl)). \u2022 Dmismatch weak : Use samples with labels mismatching the human feedback, i.e., (rw(x, yw) < rw(x, yl)).\nDmatch weak : Use samples with weak labels matching the human feedback, i.e., (rw(x, yw) > rw(x, yl)).\nman feedback, i.e., (rw(x, yw) < rw(x, yl)). Table 1: Effect of purifying weak LLM feedback. Numbers in brackets denote improvement relative to the pre-trained student model without any fine-tuning.\nAs shown in Table 1, training with Dmatch weak (samples with matching labels between weak LLM feedback and human feedback) yields gold rewards comparable to those obtained from using the full set Dweak. Additionally, when examining the alignment performance using Dmismatch weak , the gold reward significantly improves relative to the pretrained strong model without any fine-tuning by 1.79. This indicates that Dmismatch weak effectively improves alignment, even when the weak LLM provides feedback that are entirely contrary to the human preference. We fur-\n% matching\nGold reward of\nhuman feedback\naligned model \u03c0\u2217\n\u03b8\nDweak\n60.6%\n4.84 (+2.61)\nDmatch\nweak\n100%\n4.78 (+2.56)\nDmismatch\nweak\n0%\n4.01 (+1.79)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d2b/6d2bb39e-4bfc-4d1b-8863-5bf4bc4d9f76.png\" style=\"width: 50%;\"></div>\nther confirm that the noise in Dweak appears to behave differently from random noise (see Appendix C). The phenomenon is intriguing and prompts our next anal\nDive into the quality of weak LLM vs. human feedback. In Table 3, we provide a qualitative example from Dmismatch weak where the chosen response by weak LLM mismatches that of the human feedback. More qualitative examples are in Appendix B. Despite contradicting the human preference, the chosen response by the weak LLM appears to be of higher quality. We further measure the gold reward as a proxy for response quality, which is indeed higher for the response chosen by the weak LLM, i.e., rgold(x, \u02c6yw) > rgold(x, \u02c6yl). Across all the samples in Dmismatch weak , we notice that 44.3% samples display a higher gold reward for the response chosen by the weak LLM. This indicates that the preferences from weak LLM in Dmismatch weak are not entirely erroneous, but in fact better\nD weak than human feedback in nearly half of the cases1. Considering the feedback from weak LLM and human shares Dmatch weak portion and only differs in Dmismatch weak , the close performance achieved between the two models can be attributed to the fact that human feedback is similarly imperfect.\nBuilding on the observation that weak LLM can sometimes surpass human judgment, we delve into the characteristics of datasets produced under such supervision. Table 2 shows that the gold rewards for chosen responses in Dmismatch weak are on average comparable to those for rejected responses, while the gold rewards for chosen responses in Dmatch weak are significantly higher than the rejected responses. This suggests that the weak LLM primarily errs between choices that are subtly different, while it remains reliable in distinguishing between options with clear quality distinctions. Overall, the discrepancy (\u2206) within Dweak is similar to that in Dhuman, indicating that the quality of weak labels is not severely compromised.\nTable 5: The GPT-4 p tency. Dataset co Dmatch weak Dmismatch weak More qualitative analysis. To further investigate the quality gap in responses from the dataset where weak LLM feedback and human feedback diverge (Dmismatch weak ), we conducted a qualitative assessment using GPT-4. Our goal is to determine whether GPT-4\u2019s preferences for chosen and rejected responses in Dmatch weak and Dmismatch weak remain consistent. We hypothesize that consistent preferences from GPT-4 across multiple evaluations would indicate a significant quality difference between the responses, while fluctuating preferences would suggest that the responses are less distinguishable in quality. To quantify this, we prompt GPT-4 ten times consecutively to identify which answer was more helpful and less harmful. We randomly sample 100 examples from Dmatch weak and Dmismatch weak , and measure preference calculating the fraction of majority votes for each question, which ranges from 0.5 to 1. For instance, T a qualitative example from Dmismatch , where the weak LLM and humans have different preferences. W\nMore qualitative analysis. To further investigate the quality gap in responses from the dataset where weak LLM feedback and human feedback diverge (Dmismatch weak ), we conducted a qualitative assessment using GPT-4. Our goal is to determine whether GPT-4\u2019s preferences for chosen and rejected responses in Dmatch weak and Dmismatch weak remain consistent. We hypothesize that consistent preferences from GPT-4 across multiple evaluations would indicate a significant quality difference between the responses, while fluctuating preferences would suggest that the responses are less distinguishable in quality. To quantify this, we prompt GPT-4 ten times consecutively to identify which answer was more helpful and\nless harmful. We randomly sample 100 examples from Dmatch weak and Dmismatch weak , and measure preference consistency by calculating the fraction of majority votes for each question, which ranges from 0.5 to 1. For instance, Table 4 presents a qualitative example from Dmismatch weak , where the weak LLM and humans have different preferences. We observe that GPT-4 indeed exhibits mixed preferences across the ten evaluations, indicating a low preference consistency (0.5) and\nTable 2: Summary statistics of average gold reward on HHRLHF dataset. \u201cChosen\u201d and \u201cRejected\u201d indicate the human preference for original dataset Dhuman and the weak LLM preference for the Dweak, Dmatch weak and Dmismatch weak .\nChosen\nRejected\n\u2206\nDhuman\n5.77\n4.23\n1.52\nDweak\n5.63\n4.39\n1.23\nDmatch\nweak\n5.93\n3.66\n2.27\nDmismatch\nweak\n5.17\n5.53\n-0.36\nTable 5: The GPT-4 preference consistency.\nDataset\nconsistency\nDmatch\nweak\n0.84\nDmismatch\nweak\n0.66\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/766f/766f07c8-fdab-4174-8fe0-da96096526f3.png\" style=\"width: 50%;\"></div>\nthus the indistinguishable nature between the two responses. Indeed, as illustrated in Table 5, the average consistency for samples in Dmismatch weak is lower than that of Dmatch weak . This suggests that advanced LLMs such as GPT-4 experience significantly higher inconsistency when the quality distinctions between two responses are minimal, highlighting the challenges faced by both weak and ultra-large LLMs in providing feedback.\n# Section 4 key takeaways\n1. When weak LLM\u2019s chosen response contradicts human feedback, nearly half of these responses exhibit higher gold rewards, suggesting that weak LLM can sometimes surpass human judgments. 2. Weak LLM primarily errs between choices that are subtly different, while it remains reliable in distinguishing between options with clear quality distinctions. 3. Feedback from advanced LLM (e.g., GPT-4) can exhibit high inconsistency in feedback when the distinctions between responses are subtle.\n# 5 Discussion\nDifferences w.r.t. Burns et al. (2024). Previous work by Burns et al. (2024) explored weak-to-strong generalization, wherein a less capable model\u2019s weak supervision signals are used to guide a stronger, larger model. However, their study was limited to much simpler learning tasks like reward modeling and binary classification, where the outputs are either scalar or categorical labels. They did not investigate the more challenging task of language generation, which is closely tied to alignment and involves a significantly more complex output space. Hence, the potential of weak LLM feedback for alignment remains unexplored in Burns et al. (2024), which is the novel focus of our study. Contrary to Burns et al. (2024), which employed discriminative metrics such as accuracy, our study evaluates the generative performance essential for AI alignment. Although Burns et al. (2024) reported a noticeable performance gap between models trained with weak LLM feedback versus those with human feedback, our findings challenge these conclusions. Considering that human preferences are noisy and unreliable, accuracy on the preference test set is a questionable metric. Our works show contrary conclusions when directly measuring the generation performance of the aligned model, and demonstrate that using feedback from weak LLM can not only match but potentially exceed human feedback in alignment tasks. Our work suggests a promising direction for future research in leveraging weak AI signals for alignment.\n# 6 Related work\nLarge language model alignment. The primary objective of model alignment is to steer language models toward human-desired outputs. Numerous studies have leveraged human feedback to refine language models in accordance\nwith human preferences (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020; Kreutzer et al., 2018). However, gathering high-quality human preference data can be costly, prompting researchers to explore AI-generated feedback for alignment purposes (Bai et al., 2022; Lee et al., 2023; Ding et al., 2023; Gilardi et al., 2023; Guo et al., 2024). Given the computational inefficiency of RLHF, recent shifts in focus favor closed-form losses that directly utilize offline preferences, like Direct Preference Optimization (Rafailov et al., 2023) and related methodologies (Xiong et al., 2023; Ethayarajh et al., 2024; Liu et al., 2024). Nonetheless, both RLHF and offline preference-based methods presuppose access to high-quality human, raising doubts about their scalability. Our research endeavor takes initial steps toward understanding the effectiveness of aligning language models under weak LLM feedback. We present a systematic evaluation and in-depth analysis that sheds light on the quality and feasibility of leveraging weak LLM feedback for alignment, and reveal novel empirical findings to the research community. Large language model as a judge. The use of LLM-as-a-Judge prompting to evaluate language models has become a common practice (Dubois et al., 2023; Li et al., 2023; Bai et al., 2023; Saha et al., 2023). This approach is also used in collecting preference datasets for alignment. For example, Bai et al. (2022) employs an LLM to assess responses, refine them, and then use the resulting data to train a reward model known as \u201cRL from AI Feedback\u201d (RLAIF). Similarly, Lee et al. (2023) demonstrated that employing a strong LLM for LLM-as-a-Judge prompting to create preference datasets yields performance nearly on par with traditional RLHF. Yuan et al. (2024) explored the concept of LLM-as-a-Judge prompting to enable models to generate their own rewards during training. While many studies (Li et al., 2024b; Kim et al., 2023; Chen et al., 2024; Ye et al., 2024) focus on leveraging advanced LLMs as judges for preference dataset collection, we investigate the effectiveness of feedback from weak LLMs with significantly smaller capacity. We reveal new insight that a task-specific weak LLM can provide more effective preference feedback than an ultra-large LLM (e.g., GPT-4) that relies solely on prompt engineering.\nwith human preferences (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020; Kreutzer et al., 2018). However, gathering high-quality human preference data can be costly, prompting researchers to explore AI-generated feedback for alignment purposes (Bai et al., 2022; Lee et al., 2023; Ding et al., 2023; Gilardi et al., 2023; Guo et al., 2024). Given the computational inefficiency of RLHF, recent shifts in focus favor closed-form losses that directly utilize offline preferences, like Direct Preference Optimization (Rafailov et al., 2023) and related methodologies (Xiong et al., 2023; Ethayarajh et al., 2024; Liu et al., 2024). Nonetheless, both RLHF and offline preference-based methods presuppose access to high-quality human, raising doubts about their scalability. Our research endeavor takes initial steps toward understanding the effectiveness of aligning language models under weak LLM feedback. We present a systematic evaluation and in-depth analysis that sheds light on the quality and feasibility of leveraging weak LLM feedback for alignment, and reveal novel empirical findings to the research community.\nLarge language model as a judge. The use of LLM-as-a-Judge prompting to evaluate language models has become a common practice (Dubois et al., 2023; Li et al., 2023; Bai et al., 2023; Saha et al., 2023). This approach is also used in collecting preference datasets for alignment. For example, Bai et al. (2022) employs an LLM to assess responses, refine them, and then use the resulting data to train a reward model known as \u201cRL from AI Feedback\u201d (RLAIF). Similarly, Lee et al. (2023) demonstrated that employing a strong LLM for LLM-as-a-Judge prompting to create preference datasets yields performance nearly on par with traditional RLHF. Yuan et al. (2024) explored the concept of LLM-as-a-Judge prompting to enable models to generate their own rewards during training. While many studies (Li et al., 2024b; Kim et al., 2023; Chen et al., 2024; Ye et al., 2024) focus on leveraging advanced LLMs as judges for preference dataset collection, we investigate the effectiveness of feedback from weak LLMs with significantly smaller capacity. We reveal new insight that a task-specific weak LLM can provide more effective preference feedback than an ultra-large LLM (e.g., GPT-4) that relies solely on prompt engineering.\nWeak-to-strong generalization. Weak-to-strong generalization refers to the scenario where a weaker teacher model supervises a stronger student model. Unlike traditional teacher-student frameworks such as semi-supervised learning (Laine & Aila, 2017; Tarvainen & Valpola, 2017), domain adaptation (French et al., 2018; Chen et al., 2019), and knowledge distillation (Hinton et al., 2015; Beyer et al., 2022)\u2014which generally involve a stronger teacher guiding a weak student\u2014other studies have demonstrated the utility of employing comparably weaker or even lesser-capable teachers to enhance student model performance (Xie et al., 2020; Higuchi et al., 2020; Burns et al., 2024; Freund & Schapire, 1997; Furlanello et al., 2018; Green Larsen & Ritzert, 2022). Recognizing the broad generalization capacities of large-scale pre-trained models, Burns et al. (2024) introduced the concept of weak-to-strong generalization for large language model, focusing on simpler discriminative tasks such as reward modeling rather than generative tasks such as alignment. A detailed discussion on the differences with Burns et al. (2024) is in Section 5. Different from previous works that explore the effectiveness of weak LLM under the standard supervised fine-tuning (Liu & Alahi, 2024; Li et al., 2024a; Ji et al., 2024; Hase et al., 2024; Sun et al., 2024), we aim to delve deeper into the role of weak LLM could play for alignment. These tasks pose unique challenges due to their inherently diverse output and the complexity of performance evaluation but hold promise for advancing superalignment in generative models (Puthumanaillam et al., 2024). Instead of adopting model interpolation for alignment without any additional training (Zheng et al., 2024), we explore the three-stage framework for alignment with feedback from weak LLM.\n# Acknowledgement\nResearch is supported in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, and the UL Research Institutes through the Center for Advancing Safety of Machine Intelligence. The authors would like to thank Shawm Im for the valuable feedback on the work.\n# References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. Benchmarking foundation models with language-model-as-an-examiner. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Lucas Beyer, Xiaohua Zhai, Am\u00e9lie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. In Computer Vision and Pattern Recognition, pp. 10925\u201310934, 2022. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397\u20132430, 2023. Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020. S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In International Conference on Machine Learning, pp. 4971\u20135012, 2024. Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised domain adaptation. In Computer Vision and Pattern Recognition, pp. 627\u2013636, 2019. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: Training a better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. International Conference on Learning Representations, 2024. Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. Is GPT-3 a good data annotator? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2023. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.\nGeoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In International Conference on Learning Representations, 2018. Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997. Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In International Conference on Machine Learning, pp. 1607\u20131616, 2018. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 10835\u201310866, 2023. Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 2023. Kasper Green Larsen and Martin Ritzert. Optimal weak to strong learning. Advances in Neural Information Processing Systems, 35:32830\u201332841, 2022. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. Peter Hase, Mohit Bansal, Peter Clark, and Sarah Wiegreffe. The unreasonable effectiveness of easy training data for hard tasks. arXiv preprint arXiv:2401.06751, 2024. Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi. Mask ctc: Non-autoregressive end-to-end asr with ctc and mask predict. Annual Conference of the International Speech Communication Association, 2020. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. In Proceedings of the International Conference on Learning Representations, 2024. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023. Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reliability and learnability of human bandit feedback for sequenceto-sequence reinforcement learning. Annual Meeting of the Association for Computational Linguistics, pp. 1777\u2013178, 2018. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. International Conference on Learning Representations, 2017. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530, 2024a. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Selfalignment with instruction backtranslation. In The Twelfth International Conference on Learning Representations, 2024b. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/ tatsu-lab/alpaca_eval, 2023. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. International Conference on Learning Representations, 2024. Yuejiang Liu and Alexandre Alahi. Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts. arXiv preprint arXiv:2402.15505, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. Gokul Puthumanaillam, Manav Vora, Pranay Thangeda, and Melkior Ornik. A moral imperative: The need for continual superalignment of large language models. arXiv preprint arXiv:2403.14683, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2023. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472, 2024. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in Neural Information Processing Systems, 2017. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Ankur P Parikh Thibault Sellam, Dipanjan Das. Bleurt: Learning robust metrics for text generation. In Proceedings of ACL, 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Computer Vision and Pattern Recognition, pp. 10687\u201310698, 2020. Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456, 2023. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. FLASK: Fine-grained language model evaluation based on alignment skill sets. In International Conference on Learning Representations, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Selfrewarding language models. arXiv preprint arXiv:2401.10020, 2024. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, and Nanyun Peng. Weak-to-strong extrapolation expedites alignment. arXiv preprint arXiv:2404.16792, 2024. Xiaojin Jerry Zhu. Semi-supervised learning literature survey. 2005. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n# A Experimental Detail\n# A.1 Training Details\nSoftware and hardware. Our experiments are conducted on servers equipped with NVIDIA A100 GPUs, with 80 GB of VRAM. The operating system used is Ubuntu 22.04.2 LTS, supported by NVIDIA CUDA Toolkit version 12.1 and cuDNN version 8.9. All experimental implementations are carried out in Python version 3.11.4, utilizing the PyTorch framework version 1.12.1.\nTraining hyper-parameters. Based on TRL 2, we deploy the training of both teacher and student models with the same hyperparameters as shown in Table 7 and Table 8.\nReddit TL;DR dataset preprocessing details. For the Reddit TL;DR dataset, we preprocess the data by filtering ou samples with fewer than 512 tokens. This results in 92,000 training samples and 8,000 test samples. We divide th training data into two disjoint sets of 46,000 samples each. The first set serves as the labeled dataset, denoted as D while the second set, referred to as the unlabeled dataset, Du, disregards the preference labels.\n# A.2 Evaluation Details\nTable 6: Sample prompt for the GPT-4 evaluation. Text highlighted in orange represents the question, while text  blue represents the responses under comparison.\nSystem prompt [System] You are a helpful, harmless, and precise assistant for checking the quality of the answer. We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. Please rate the harmlessness, helpfulness, and level of detail of their responses. Your evaluation should consider factors such as the helpfulness, harmlessness, relevance, accuracy, depth, creativity, and level of detail of the response. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. System prompt [System] You are a helpful, harmless, and precise assistant for checking the quality of the answer. We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. Please rate the harmlessness, helpfulness, and level of detail of their responses. Your evaluation should consider factors such as the helpfulness, harmlessness, relevance, accuracy, depth, creativity, and level of detail of the response. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\nSystem prompt [System] You are a helpful, harmless, and precise assistant for checking the quality of the answer. We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. System prompt [System] You are a helpful, harmless, and precise assistant for checking the quality of the answer. We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\nUser prompt User prompt\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d75c/d75ce236-2ea4-455a-9975-b5ac6a69ff73.png\" style=\"width: 50%;\"></div>\nGPT-4 evaluation details. Table 6 presents the prompts and responses in our GPT-4 evaluation, adopted from (Khanov et al., 2024). Each GPT-4 request comprises both a system and a user prompt. The system prompt delineates the proxy\u2019s attributes and its specific task, while the user prompt poses a question and provides responses from the two methods. Gold reward model. For the evaluation of the HH-RLHF, we leverage the state-of-the-art gold reward model Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback from the RewardBench leaderboard (Lambert et al., 2024), which fine-tunes Mistral-7B-Instruct-v0.2 on the llm-blender/Unified-Feedback dataset. We also verify our results consistently hold under different choices of gold reward models in Appendix C. For\nReddit TL;DR, we take the state-of-the-art gold reward model OpenAssistant/reward-model-deberta-v3-large-v on this dataset from the RewardBench leaderboard (Lambert et al., 2024) for evaluation. Hyper-parameters for model generation. To evaluate the response of the model, we adopted the temperature as 0.7 and the max tokens as 256. Table 7: Summary of training hyperparameters for supervised fine-tuning and direct preference optimization for OPT-family models for HH-RLHF.\nParameters\nValue\nSupervised fine-tuning\nNumber of epochs\n1\nLearning rate\n1 \u00d7 10\u22125\nBatch size\n32\nGradient accumulation steps\n1\nMaximum sequence length\n512\nDeepSpeed Zero stage\n2\nLoRA rank\n0\nDirect preference optimization\nNumber of epochs\n1\nLearning rate\n5 \u00d7 10\u22125\n\u03b2\n0.1\nBatch size\n16\nGradient accumulation steps\n1\nMaximum sequence length\n512\nDeepSpeed Zero stage\n2\nLoRA rank\n8\nTable 8: Summary of training hyperparameters for supervised fine-tuning and direct preference optimization fo OPT-family models on Reddit TL;DR.\nParameters\nValue\nSupervised fine-tuning\nNumber of epochs\n1\nLearning rate\n1 \u00d7 10\u22125\nBatch size\n32\nGradient accumulation steps\n1\nMaximum sequence length\n512\nDeepSpeed Zero stage\n2\nLoRA rank\n0\nDirect preference optimization\nNumber of epochs\n1\nLearning rate\n5 \u00d7 10\u22126\n\u03b2\n0.5\nBatch size\n16\nGradient accumulation steps\n1\nMaximum sequence length\n512\nDeepSpeed Zero stage\n2\nLoRA rank\n8\n# B Additional Qualitative Examples\nIn Table 9, we provide additional qualitative examples from Dweak where the chosen response by weak LLM mismatches that of the human annotators. Despite contradicting the human preference, the gold reward of the response chosen by the weak LLM can be higher, i.e., rgold(x, \u02c6yw) > rgold(x, \u02c6yl). This indicates that the preferences of the weak LLM can align with the gold reward model, even if they differ from human labels.\nTable 9: Comparative examples of the chosen and rejected responses in Dmis wea\nPrompt Human: How does a pacemaker help the function of the heart? Assistant: Prompt Human: How does a pacemaker help the function of the heart? Assistant: Example 1\n# Example 1\nChosen response by weak LLM (rejected by human annotators) A pacemaker is a medical device implanted in the chest which stimulates the heart to beat at a normal rhythm, helping the heart to pump and circulate blood more efficiently and effectively. Pacemakers are generally implanted with catheters by a practitioner in a cardiac catheterization or angiography procedure, or in rare cases with open chest surgery. Pacemaker systems usually consist of a pulse generator and set of leads, one end of which is sutured to the heart (lead to the right atrium, right ventricle, right-side of the chest, or the right coronary sinus), and the other end which is connected to the pacemaker. Pacemakers also can be used as a lifesaving measure for patients with heart failure, or at risk of permanent damage from their heart not functioning properly. Rejected response by weak LLM (chosen by human annotators) A pacemaker is a small electronic device implanted under the skin with an electrode attached to the heart to help control the rate and rhythm of the heartbeat. The pacemaker senses when the heartbeat is slow or irregular and sends electrical impulses to the heart at a pre-defined safe and healthy rate, normalizing the heart beat. Chosen response by weak LLM (rejected by human annotators) A pacemaker is a medical device implanted in the chest which stimulates the heart to beat at a normal rhythm, helping the heart to pump and circulate blood more efficiently and effectively. Pacemakers are generally implanted with catheters by a practitioner in a cardiac catheterization or angiography procedure, or in rare cases with open chest surgery. Pacemaker systems usually consist of a pulse generator and set of leads, one end of which is sutured to the heart (lead to the right atrium, right ventricle, right-side of the chest, or the right coronary sinus), and the other end which is connected to the pacemaker. Pacemakers also can be used as a lifesaving measure for patients with heart failure, or at risk of permanent damage from their heart not functioning properly. (gold reward: 11.99) Rejected response by weak LLM (chosen by human annotators) A pacemaker is a small electronic device implanted under the skin with an electrode attached to the heart to help control the rate and rhythm of the heartbeat. The pacemaker senses when the heartbeat is slow or irregular and sends electrical impulses to the heart at a pre-defined safe and healthy rate, normalizing the heart beat. (gold reward: 11.76)\nExample 2\nHuman: Why are lady bugs considered to be good luck? Assistant: Human: Why are lady bugs considered to be good luck? Assistant:\nChosen response by weak LLM (rejected by human annotators) I\u2019ve heard that they\u2019re good luck because people believe they represent rebirth and renewal, and that they\u2019re always some shade of red or orange. Rejected response by weak LLM (chosen by human annotators) Lady bugs, or ladybirds, are considered to be a symbol of good luck, especially in the Middle East. They are good luck to bring, and good luck to keep as a pet. In the Middle East, these symbols are often used in mehndi, which is a kind of elaborate hand design. The Ladybird is used to symbolize the heart, as it is known for its kindness to others, and to leave its territory willingly for the sake of others, which is also considered a good trait. Chosen response by weak LLM (rejected by human annotators) I\u2019ve heard that they\u2019re good luck because people believe they represent rebirth and renewal, and that they\u2019re always some shade of red or orange. (gold reward: 4.97) Rejected response by weak LLM (chosen by human annotators) Lady bugs, or ladybirds, are considered to be a symbol of good luck, especially in the Middle East. They are good luck to bring, and good luck to keep as a pet. In the Middle East, these symbols are often used in mehndi, which is a kind of elaborate hand design. The Ladybird is used to symbolize the heart, as it is known for its kindness to others, and to leave its territory willingly for the sake of others, which is also considered a good trait. (gold reward: 3.19)\n# Example 3\nPrompt How does a smart grid work for powering a country\u2019s electric grid? Assistant: Prompt How does a smart grid work for powering a country\u2019s electric grid? Assistant: Example 3\nChosen response by weak LLM (rejected by human annotators) So we\u2019re talking about electricity grids, right? One way to describe what\u2019s happening is to say that the grid acts as a network for power distribution, where energy flows from various energy sources to various consumers. And it\u2019s a bit like a huge complex network of pipes and nodes. But in this case, the \u2019nodes\u2019 are basically power plants that create and distribute electricity, and the \u2019pipes\u2019 are the electrical lines which transport power from the sources to the nodes.(gold reward: 2.38) Chosen response by weak LLM (rejected by human annotators) So we\u2019re talking about electricity grids, right? One way to describe what\u2019s happening is to say that the grid acts as a network for power distribution, where energy flows from various energy sources to various consumers. And it\u2019s a bit like a huge complex network of pipes and nodes. But in this case, the \u2019nodes\u2019 are basically power plants that create and distribute electricity, and the \u2019pipes\u2019 are the electrical lines which transport power from the sources to the nodes.(gold reward: 2.38)\nRejected response by weak LLM (chosen by human annotators) There\u2019s a bunch of different ways to generate electricity. Most of the time, they boil water to produce steam, which spins a turbine, which spins a generator, which generates electricity. But there are other ways to do it. To start, I need to understand what you mean by \u2019smart grid\u2019. Are you talking about a grid that\u2019s connected to a smart city, or a smart grid in terms of power lines, or something else? (gold reward: 1.07) Rejected response by weak LLM (chosen by human annotators) There\u2019s a bunch of different ways to generate electricity. Most of the time, they boil water to produce steam, which spins a turbine, which spins a generator, which generates electricity. But there are other ways to do it. To start, I need to understand what you mean by \u2019smart grid\u2019. Are you talking about a grid that\u2019s connected to a smart city, or a smart grid in terms of power lines, or something else? (gold reward: 1.07)\nRejected response by weak LLM (chosen by human annotators) There\u2019s a bunch of different ways to generate electricity. Most of the time, they boil water to produce steam, which spins a turbine, which spins a generator, which generates electricity. But there are other ways to do it. To start, I need to understand what you mean by \u2019smart grid\u2019. Are you talking about a grid that\u2019s connected to a smart city, or a smart grid in terms of power lines, or something else? (gold reward: 1.07) Rejected response by weak LLM (chosen by human annotators) There\u2019s a bunch of different ways to generate electricity. Most of the time, they boil water to produce steam, which spins a turbine, which spins a generator, which generates electricity. But there are other ways to do it. To start, I need to understand what you mean by \u2019smart grid\u2019. Are you talking about a grid that\u2019s connected to a smart city, or a smart grid in terms of power lines, or something else? (gold reward: 1.07)\n# C Additional Experiments\nEvaluation with more gold reward models. To verify the reliability of our conclusions, we employ different gold reward models to evaluate the quality of model output. Specifically, we leverage another two competitive gold reward models weqweasdas/RM-Mistral-7B and OpenAssistant/reward-model-deberta-v3-large-v2 from the RewardBench leaderboard (Lambert et al., 2024). As shown in Figure 5, using these two alternative gold\nreward models, we can still observe that alignment with weak LLM feedback can outperform human feedback. Thi confirms the reliability of our findings, and its insensitivity to the choice of gold reward model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5728/5728fc29-0119-4767-aa38-d7848d01a5d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: (a) Evaluation with the gold reward model reward-model-deberta-v3-large-v2. (b) Evaluation with the gol reward model RM-Mistral-7B.</div>\nCorrelation of generation quality. In Figure 6, we investigate whether the generation quality correlates between models using weak LLM feedback vs. human feedback. In particular, for each test input prompt, we measure the sentence-level similarity between the model\u2019s generation \u02c6y (using either \u03c0\u2217 \u03b8 or \u03c0\u2217 h) and the chosen response yw by human annotators from HH-RLHF. Thibault Sellam (2020) proposed a cosine similarity based on the BLUERT embedding. These similarities are denoted as sim\u03c0\u2217 \u03b8 (\u02c6y, yw) for the model aligned with weak LLM feedback and sim\u03c0\u2217 h(\u02c6y, yw) for the model aligned with human feedback, represented on the x-axis and y-xis respectively. We observe a moderate correlation between the two across all test samples from HH-RLHF. Additionally, we examine the correlation between the model \u03c0\u2217 \u03b8 aligned with weak LLM feedback and the weak LLM itself \u03c0\u2217 w on the right of Figure 6. This analysis helps reveal whether aligning a model under weak feedback could risk imitating the errors inherent in a weak LLM. We observe that the correlation between \u03c0\u2217 \u03b8 and \u03c0\u2217 w (with R2 = 0.4888) is relatively weaker compared to that between \u03c0\u2217 \u03b8 and \u03c0\u2217 h (with R2 = 0.5789). These results suggest that the model \u03c0\u2217 \u03b8 not only aligns more closely with \u03c0\u2217 h, but also effectively extrapolates beyond imitating the weaker teacher.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb2d/cb2d0b6f-b19b-433d-843c-1bb490d22ebf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Left: Agreement between models using weak LLM feedback (\u03c0\u2217 \u03b8) vs. human feedback (\u03c0\u2217 h). Right: Agreement between the model (\u03c0\u2217 \u03b8) aligned with weak LLM feedback and the weak LLM itself (\u03c0\u2217 w).</div>\nAblation on the impact of SFT. Learning from preference data typically begins by fine-tuning a pre-trained language model with supervised learning on high-quality data for the downstream tasks of interest. We further explore the impact of SFT when aligning the target policy model (c.f. Equation 6). We utilize the OPT-125M as the weak LLM to provide feedback and the OPT-1.3B as the student model, conducting our experiments on the HH-RLHF dataset. For the model \u03c0\u2217 \u03b8 aligned with weak LLM feedback and the model \u03c0\u2217 h aligned with human feedback, we compare the following two\nsettings: (1) DPO (w/o SFT): Directly using the pre-trained model as the reference model or initialization. (2) DPO (w SFT): Use fine-tuned model \u03c0SFT \u03b8 as a reference model for DPO training. Compared to directly using the untuned base model as a reference model, performing SFT enhances the model\u2019s ability to generate desired responses to questions. Table 10: Ablation on the impact of supervised fine-tuning (SFT) as initialization for DPO. We report the average gold reward. Numbers in brackets denote improvement relative to the pre-trained model (OPT-1.3B) without any fine-tuning.\nsettings: (1) DPO (w/o SFT): Directly using the pre-trained model as the reference model or initialization. (2) DPO (w SFT): Use fine-tuned model \u03c0SFT \u03b8 as a reference model for DPO training. Compared to directly using the untuned base model as a reference model, performing SFT enhances the model\u2019s ability to generate desired responses to questions.\n<div style=\"text-align: center;\">Table 10: Ablation on the impact of supervised fine-tuning (SFT) as initialization for DPO. We report the average gold reward. Numbers in brackets denote improvement relative to the pre-trained model (OPT-1.3B) without any fine-tuning.</div>\nMethod\nWeak LLM feedback\nHuman feedback\nDPO (w/o SFT)\n3.83 (+1.61)\n3.77 (+1.55)\nDPO (w SFT)\n4.84 (+2.61)\n4.63 (+2.41)\nAblation on the KL coefficient (\u03b2) for weak LLM feedback for alignment. The \u03b2 parameter in Equation 3 functions as the KL coefficient during DPO training, with higher values indicating more stringent regularization. We employ OPT-125M as the weak LLM to provide feedback and OPT-1.3B as the student model, performing alignment using varying \u03b2 values: {0.05, 0.1, 0.2, 0.3, 0.5} on the HH-RLHF dataset. As shown in the left of Figure 7, the gold reward for the model alignment with weak LLM feedback (\u03c0\u2217 \u03b8) is high under relatively small \u03b2 values such as 0.05 and 0.1, and starts to decline as \u03b2 increases. A similar trend is observed for the model trained with human feedback.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c820/c820ed94-2d2b-431d-b294-fe3009c141ce.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Ablation on the KL Coefficient (\u03b2) for alignment with weak LLM feedback and human feedback</div>\nDataset division with different random seeds. We randomly partition the entire training set into a labeled dataset, denoted as Dl, and an unlabeled dataset, Du, using a random seed of 22. To ensure that our conclusions are insensitive to the data division, we also utilized five additional random seeds to perform the experiments. Same as our main experiment, we employ OPT-125M as the weak LLM and OPT-1.3B as the student model on the HH-RLHF dataset. Across 5 random runs, the average gold rewards for the \u03c0\u2217 \u03b8 and \u03c0\u2217 h are 4.81 and 4.67, respectively, with variances of 0.12 and 0.14. These results affirm that our conclusions are statistically stable under different dataset splits. Understanding the different impact between weak LLM feedback vs. random noise. Our investigation in Section 4 suggests that weak LLM feedback contains more systematic noise (e.g., errors primarily occur between choices that are subtly different). In this ablation, we contrast performing alignment with feedback from weak LLM by using the feedback containing random noise. Adopting OPT-125M as the weak LLM and OPT-1.3B as the student model, we design three controlled settings for this ablation: \u2022 Dmatch random: Randomly sample 60.6% of the samples from Du, and assign human feedback, i.e., (rw(x, yw) > rw(x, yl)). This set has the same size as our weakly supervised set Dmatch weak . \u2022 Dmismatch random : Use the remainder 39.4% samples from Du, and assign preference labels opposite to the human feedback, i.e., (rw(x, yw) < rw(x, yl)).\n D D  D As demonstrated in Table 11, the student model trained with the match set Dmatch random outperforms the model trained with Drandom by 0.66 in terms of gold reward. This suggests that removing random noise in preference datasets can markedly enhance alignment performance. Conversely, the performance of the model trained with Dmismatch random exhibits a gold reward that is significantly lower (1.53), illustrating the negative effect of random noise on model alignment. These findings highlight the difference between random noise and the \u201cnoise\u201d from weak LLM feedback. Table 11: Effect of purifying random noise. Numbers in brackets denote improvement relative to the pre-trained student model without any fine-tuning.\nAs demonstrated in Table 11, the student model trained with the match set Dmatch random outperforms the model trained with Drandom by 0.66 in terms of gold reward. This suggests that removing random noise in preference datasets can markedly enhance alignment performance. Conversely, the performance of the model trained with Dmismatch random exhibits a gold reward that is significantly lower (1.53), illustrating the negative effect of random noise on model alignment. These findings highlight the difference between random noise and the \u201cnoise\u201d from weak LLM feedback.\n% matching human feedback\nGold reward of \u03c0\u2217\n\u03b8\nDrandom\n60.6%\n3.91 (+1.69)\nDmatch\nrandom\n100%\n4.57 (+2.35)\nDmismatch\nrandom\n0%\n1.53 (-0.69)\n<div style=\"text-align: center;\">% matching human feedback Gold reward of \u03c0\u2217 \u03b8</div>\nResults on additional weak LLM feedback. To validate our conclusion that weak LLM provides feedback comparable to human feedback, we extend our analysis beyond the specific case of OPT-125M. We incorporated GPT-Neo-125M (Gao et al., 2020) and Pythia-160M (Biderman et al., 2023) as additional weak LLM to provide feedback. We use OPT-1.3B and Mistral-7B as student models to be aligned. Following the alignment with weak LLM feedback process outlined in Section 3.1, we assess the generalizability of our conclusion. As shown in Table 12, models aligned with feedback from weak LLMs GPT-Neo-125M and Pythia-160M achieved performance comparable to the model aligned with human feedback. This demonstrates that our conclusion is not dependent on a single weak model, but rather applies more broadly across different weak LLMs.\n<div style=\"text-align: center;\">Table 12: Results on using different weak LLM supervisors: OPT-125M, GPT-Neo-125M and Pythia-160M.</div>\nHuman feedback\nOPT-125M\nGPT-Neo-125M\nPythia-160M\nOPT-1.3B\n4.63\n4.84\n4.55\n4.87\nMistral-7B\n7.19\n7.90\n7.27\n8.01\n# D Limitations and future work\nLimitations. Our research presents an in-depth investigation of alignment with the feedback from weak LLM. W delve deeply into the reasons underlying this phenomenon. However, our study is not without limitations, which we aim to address in subsequent research. Our focus has predominantly been on empirical experiments, leaving the theoretica underpinnings of our findings less explored. These areas present opportunities for further exploration and developmen\ndelve deeply into the reasons underlying this phenomenon. However, our study is not without limitations, which we aim to address in subsequent research. Our focus has predominantly been on empirical experiments, leaving the theoretical underpinnings of our findings less explored. These areas present opportunities for further exploration and development. Implications for future alignment research. As we look toward the future of AI alignment, several recommendations emerge from our study that could further refine the practice and enhance the reliability of alignment methodologies. First, integrating hybrid feedback systems that combine human insights with AI-generated feedback could leverage the strengths of both, minimizing the limitations inherent in each approach. Secondly, it is crucial to develop more sophisticated metrics for evaluating alignment quality that go beyond traditional accuracy or performance metrics, to capture the nuanced understanding and generative capabilities required in real-world applications. Finally, exploring the ethical implications of AI-generated feedback and ensuring that these systems adhere to ethical guidelines is vital. By addressing these areas, the field can move towards more effective, efficient, and ethically responsible AI alignment strategies that are capable of supporting the safe integration of AI systems into society.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of alignment in large language models (LLMs), highlighting the constraints of existing frameworks that either require significant human effort or incur high computational costs. The authors propose a middle ground by utilizing a weak LLM, which is less resource-intensive yet capable of providing automated feedback, thus facilitating a more scalable and sustainable alignment strategy.",
        "problem": {
            "definition": "The problem revolves around the challenge of aligning AI systems with human values and intentions, ensuring that these systems behave in a helpful and harmless manner.",
            "key obstacle": "The main difficulty lies in the reliance on expensive human annotations or high-capacity models that require substantial computational resources, which limits the scalability of alignment efforts."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that a weak LLM can provide meaningful feedback without the heavy resource demands of larger models or extensive human input.",
            "opinion": "The proposed method involves using weak LLMs to generate feedback for alignment tasks, which can potentially match or exceed the quality of human feedback.",
            "innovation": "The innovation lies in demonstrating that a weak LLM can effectively provide alignment feedback, challenging the assumption that larger models are always superior for this task."
        },
        "method": {
            "method name": "Weak LLM Feedback for Alignment",
            "method abbreviation": "WLF-A",
            "method definition": "This method involves training a weak LLM to provide preference feedback on unlabeled data, which is then used to align a target LLM policy.",
            "method description": "The core of the method is to leverage feedback from a weak LLM to enhance the alignment of a target LLM without relying solely on human annotations.",
            "method steps": [
                "Collect unlabeled triplets of responses.",
                "Train a weak LLM on a labeled dataset to provide preference feedback.",
                "Use this feedback to train a target LLM policy."
            ],
            "principle": "The method is effective because it utilizes the strengths of weak LLMs to provide scalable feedback, minimizing the reliance on resource-heavy models or extensive human input."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized the Anthropic HH-RLHF dataset with 112,000 training samples and 12,500 test samples, comparing the performance of models aligned with weak LLM feedback versus human feedback across various model scales.",
            "evaluation method": "Performance was assessed using metrics such as gold reward and GPT-4 win-rate, comparing the quality of responses generated by models trained with weak LLM feedback against those trained with human feedback."
        },
        "conclusion": "The outcomes of the experiments indicate that weak LLM feedback can match or exceed the performance of human feedback in alignment tasks, suggesting a promising direction for scalable and efficient alignment methodologies.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to provide effective alignment feedback with significantly lower resource demands than traditional methods.",
            "limitation": "A limitation of the method is that it may not fully replace human feedback in all contexts, especially where nuanced understanding is critical.",
            "future work": "Future research could focus on refining the hybrid feedback systems that combine human insights with AI-generated feedback and developing more sophisticated metrics for evaluating alignment quality."
        },
        "other info": {
            "acknowledgements": "Research supported by the AFOSR Young Investigator Program, National Science Foundation, and UL Research Institutes.",
            "contributions": [
                "Framework for evaluating alignment via weak LLM feedback.",
                "Comprehensive evaluation and novel empirical findings.",
                "In-depth analysis on the quality of weak LLM\u2019s feedback."
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The paper highlights the constraints of existing frameworks for aligning LLMs, emphasizing the need for scalable and sustainable alignment strategies."
        },
        {
            "section number": "4.2",
            "key information": "The proposed method involves using weak LLMs to generate feedback for alignment tasks, which can potentially match or exceed the quality of human feedback."
        },
        {
            "section number": "10.1",
            "key information": "The main difficulty in aligning AI systems with human values lies in the reliance on expensive human annotations or high-capacity models that require substantial computational resources."
        },
        {
            "section number": "10.2",
            "key information": "Future research could focus on refining hybrid feedback systems that combine human insights with AI-generated feedback."
        },
        {
            "section number": "10.3",
            "key information": "The paper discusses the ethical implications of using AI-generated feedback in alignment tasks, suggesting a need for careful consideration of human oversight."
        }
    ],
    "similarity_score": 0.7258509202329665,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Your Weak LLM is Secretly a Strong Teacher for Alignment.json"
}