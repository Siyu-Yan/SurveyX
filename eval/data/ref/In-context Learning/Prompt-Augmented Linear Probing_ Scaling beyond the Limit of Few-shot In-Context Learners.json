{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.10873",
    "title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners",
    "abstract": "Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly enhances the input representations closing the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario.",
    "bib_name": "cho2023promptaugmentedlinearprobingscaling",
    "md_text": "# Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-Shot In-Context Learners Hyunsoo Cho1, Hyuhng Joon Kim1, Junyeob Kim1, Sang-Woo Lee2, 3, Sang-goo Lee1, Kang Min Yoo1, 2,\u2217, Taeuk Kim4,* 1 Seoul National University\n Hanyang University {johyunsoo, heyjoonkim, juny116, sglee}@europa.snu.ac.kr {kangmin.yoo, sang.woo.lee}@navercorp.com, kimtaeuk@hanyang.ac.kr\nAbstract\nThrough in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly enhances the input representations closing the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario.\narXiv:2212.10873v3\n# Introduction\nSince the emergence of Transformer-based (Vaswani et al. 2017) language models, we have witnessed notable improvements in the natural language processing literature, even attaining human-level performance on several benchmarks. In addition, as it becomes evident that scaling laws work for such models (Kaplan et al. 2020), there has been a significant amount of investment in the field to enhance them in terms of the number of their parameters\u2014from millions to billions\u2014and the volume of the data they consume during training (Brown et al. 2020; Chowdhery et al. 2022; Fedus, Zoph, and Shazeer 2022; Hoffmann et al. 2022). As a result, some cutting-edge language models become possible to obtain intriguing extra functionalities, such as the ability to capture world knowledge (Petroni et al. 2019), generate codes (Poesia et al. 2022), or solve mathematical problems\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03ef/03ef39c3-9a2b-4881-b194-3e672a6df7e5.png\" style=\"width: 50%;\"></div>\nFigure 1: Average accuracy (12 classification tasks) of various black-box transferring methods trained on GPT-J. ICL can not leverage the full train dataset due to the length limit, and their performance saturates quickly. Our method (PALP) is scalable with available training samples and minimizes the performance gap between ICL in a few-shot setting. The performance of the respective task is summarized in Table 2.\n(Henighan et al. 2020), in addition to being proficient in recognizing linguistic patterns. These anecdotes, which demonstrate the general power of large language models, naturally raise researchers\u2019 expectations that language models can act as a universal, off-the-shelf solution for a range of downstream tasks while minimizing the cost required for adapting them to a specific job at the same time. However, there is no free lunch; the effectiveness and generalizability of large language models achieved by scaling come at the cost of physically serving such gigantic neural architectures. Thus, the institutions that distribute large models such as GPT-3 (Brown et al. 2020) usually pursue the strategy of providing commercial APIs which only allow limited access to the models. In other words, it is often the case that users cannot receive information about the inner workings of the models, such as gradients concerning the models\u2019 parameters which are crucial for fine-tuning the models for a particular purpose. Therefore, there has been\na growing interest in adapting language models in this restricted setting, dubbed as black-box tuning (Sun et al. 2022; Diao et al. 2022).\nIn this paper, we show that the combination of linear classifiers and the techniques introduced for ICL can cover the weaknesses of each method. We train diverse linear classifiers whose representations are extracted from input preprocessing strategies invented for facilitating ICL. Specifically, we augment training data instances with the templates or prepend additional demonstrations in front of the input of interest for better contextualization.\nWe validate our method with various datasets, demonstrating that it is consistently superior to baselines in both the low-data and full-data settings. From empirical experiments, we observe that exploiting templates that provide hints about the target task or concatenating demonstrations can significantly enhance the extracted representations from PLM, improving the classifiers\u2019 performance in various scenarios and reducing the gap between ICL and fine-tuning. Intriguingly, we also discover that importing the techniques directly from ICL without care may cause the inheritance of the disadvantages of ICL, such as a substantial performance variance depending on the appended demonstrations or high sensitivity to the format of templates.\n# Preliminary\n# Problem Formulation\nIn this work, we consider classifying input sequences based on black-box tuning (Sun et al. 2022; Diao et al. 2022), where the parameters of pre-trained language models (PLMs) are inaccessible. That is, PLMs only serve as an encoder function e that delivers n-dimensional latent features h from input x. Formally, for input x \u2208X, let the n-dimensional continuous latent features extracted from PLMs be h = e(x), where h \u2208H. In addition, let Y = {0, 1, \u00b7 \u00b7 \u00b7 , |C|} be a label space, where |C| is the cardinality of the space. Then, a classifier p(y|h; \u03b8) : H \u2192Y maps h to a class label y \u2208Y, estimating the probability of input x belonging to a certain label.\n# Linear Classifiers\nLinear models, such as single layer perceptron (SLP), Support Vector Machine (SVM), or logistic regression (LR), are trained by solving optimization problems concerning their parameters. First-order methods such as gradient descent shown below are a general choice for parameter estimation:\n(1)\n \u2212\u2207L H where L, \u03b7, Hbatch refer to a loss function (e.g., the crossentropy loss, hinge loss in SVM, and MSE loss in regression), learning rate, and a mini-batch sampled from the training dataset. In this paper, we evaluate 5 different linearprobing classification methods: k-NN, LR, SVM, gaussian discriminative analysis (GDA), and SLP. The details of the mentioned approaches are presented in the Appendix.\n# In-Context Learning\nIn-context learning (ICL) is a brand new, training-free paradigm that attempts to make the most use of the nature of language models to conduct a target task. ICL promotes a language model to generate the desired output by guiding the model with a few examples of the target task (i.e., demonstrations) plus a set of templates tailored for the task. In detail, ICL consists of two steps (Liu et al. 2022b): First, the input pre-processing step combines the input of interest x with k-shot samples (i.e., demonstrations) (xi,yi)k i=1 from the training set. Then, a template function ftemplate(\u00b7) attach pre-defined descriptions to the input ftemplate(x) or additionally attach the corresponding natural language label to the templified input ftemplate(x, y), commonly referred to as a demonstration. (See Figure 2 for a graphical explanation.) For instance, the function attaches a prefix or postfix to the original x, or it transforms yi into the form of natural language rather than numeric numbers. In consequence, the final input for ICL, denoted as \u02c6x, becomes a concatenation of all the pre-processed x and (xi,yi)k i=1:\n(2)\n \u2295 \u2295\u00b7 \u00b7 \u00b7 \u2295 \u2295 where Di = ftemplate(xi, yi) and \u2295refers to the concatenation operation. Second, in the prediction phase, ICL leverages PLMs to compute the feature \u02c6h = e(\u02c6x), followed by a verbalizer\n<div style=\"text-align: center;\">Input Pre-processing</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/02e9/02e97843-a2cb-4f5a-a441-4b32c62f1b78.png\" style=\"width: 50%;\"></div>\nFigure 2: Overall illustration of ICL. ICL utilizes a template function to convert available few-shot samples into demonstrations ftempltate(x, y), and appends the generated demonstrations to the front of the templified current input ftempltate(x). Then, ICL uses LM\u2019s prediction ability from the pre-training step to infer the correct answer.\nV : H \u2192Y that is a reformulation of the language model head for task-specific adaptation. It is often assumed that the verbalizer only considers single tokens as its output candidates, which correspond to each item in the label space Y.\n# Prompt-Augmented Linear Probing (PALP)\n# Prompt-Augmented Linear Probing (PALP) Motivation\nThe primary intuition behind our method borrows from the in-context learning ability exhibited by language models. Specifically, in-context learners benefit from more elaborate and longer prompts (Reynolds and McDonell 2021), allowing them to carry out deeper reasoning through longer input sequences and corresponding hidden layers. We posit that providing appropriate guidance to the language model via prompts (input pre-processing step in ICL) benefits not only the usual causal language modeling (i.e., predicting the next token) ability but also enhances the quality of the representation for the input text. The primary goal of our method is to extract a more distinctive representation from PLMs via crafting a raw dataset into a more understandable form and training linear probers on the top of the extracted representations. Specifically, we transform the dataset in two ways: 1. We utilize a simple template that gives a brief description of input and the objective of the task. 2. On top of templified dataset, we concatenate a single class-wise demonstration to an inferring input to give important cues, i.e., input-label mapping, label space, or distribution of the input text (Min et al. 2022b) regarding the target task. In the following subsection, we explain the dataset reconstruction strategies for each method.\n<div style=\"text-align: center;\">Prediction</div>\n# PALP-Template (PALP-T)\nApplying a template to the input is the most straightforward and intuitive way to enforce PLM to follow user requirements. Accordingly, we attempt to extract more task-specific features by attaching a fixed prefix, infix (for sentence pair tasks), or postfix, which describes a raw input into a more understandable form. For instance, we transform sentiment analysis instance \u2018very interesting.\u2019 into \u2018Review: very interesting. Sentiment:\u2019 to provide the language model additional indications that the input is the form of review, and the user expects sentiment as an answer. Formally, for the training dataset Dtrain = {(xi, yi)|i \u2208m}, we convert this into Dtemplate train = {(ftemplate(xi), yi)|i \u2208m}, where ftemplate(\u00b7) is a template function. Then we train linear classifiers with transformed dataset Dtemplate train in the exact same way as in Eq. 1. In the inference time, we also have to apply the same template function to inferring input ftemplate(xtest) in order to match the format.\n# PALP-Demonstration (PALP-D)\nOn top of the previous templified dataset (PALP-T), PALP-D additionally concatenates some demonstrations to the front to maximize the capability of PLM to learn in-context. However, unlike ICL, which attaches all available samples, our method extracts a single representative demonstration per class (i.e., total |C| demonstrations) and leverages them as demonstrations. Formally, let there exist k accessible training samples per each class label:\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea17/ea1793a3-6af8-4d3a-be9a-71c3f5c9b205.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Distribu)on Es)ma)on</div>\n<div style=\"text-align: center;\">Representa)on Extrac)on</div>\nFigure 3: Illustration of selecting demonstrations in PALP-D in the binary classification task. We estimate the normal distribution for each class from available training samples and select the closest sample, respectively.\nwhere |C| is the cardinality. Then, we choose one instance from each class and concatenate them into one prefix \u03c4:\n(4) (5)\nIn this way, we can avoid atrociously lengthy input, minimizing the computational cost and enhancing the method\u2019s scalability. Specifically, while the length of the input in ICL is normally proportional to the total available number of samples, PALP-D is proportional to the number of labels of the task. Given that PALP-D does not inject complete available training samples, the core is selecting a single meaningful demonstration that can distill PLM sufficient knowledge to comprehend essential signs of the task, such as input-label mapping, label space, or distribution of the input text. We hypothesize that the inputs closest to the centroid of each class label can capture the most representative information for respective classes and extract them as a set of demonstrations P = {D1, D2, \u00b7 \u00b7 \u00b7 , D|C|}. In order to do so, we first estimate the normal distribution for each class label N(\u00b5i\u2208|C|, \u03a3i\u2208|C|) from the available training inputs and measure the distance between entire training samples and the estimated centroid through Mahalanobis distance:\n(6)\nFinally, we select samples closest to each class centroid and utilize them as demonstrations Di = minj\u2208k(xi j, ui; \u03a3i). (Figure 3 illustrates the selection of demonstration from available training samples.) In a few-shot scenario, we randomly permuted the order of available demonstrations P to construct multiple prefixes \u02c6\u03c4 in the training phase, where \u02c6\u03c4 \u223c\u03c3(P) and \u03c3(\u00b7) refers to a random permutation operator. By doing so, we can generate |C|! different prefixes that can give the effect of data augmentation and alleviate the data-scarcity problem. And in the inference phase or data-abundant setting, we attach a unified prefix \u03c4 (without random permutation) to the front of the test input to match the format with the training samples.\n<div style=\"text-align: center;\">Selec)ng Demonstra)ons</div>\nTask type\nDataset\n# Class\n# Train\n# Test\nSingle Sentence\nSST2\n2\n67,349\n872\nRotten tomatoes\n2\n8,530\n1,066\nOffensive\n2\n19,916\n860\nCoLA\n2\n8,551\n1,043\nStance atheism\n3\n461\n220\nEmotion\n4\n3,257\n1,421\nAG news\n4\n120,000\n7,600\nTREC\n6\n5,452\n500\nBanking77\n77\n10,003\n3,080\nCLINC\n150\n15,000\n4,500\nSentence pair\nMNLI\n3\n392,702\n9,832\nMRPC\n2\n3,668\n408\nRTE\n2\n2,490\n277\nBoolQ\n2\n9,427\n3,270\nCB\n3\n250\n56\nTable 1: Statistics of 15 different datasets used in our experiments.\n# Experiments\n# Experimental Setup\nBackbone. In the following experiments, we adopt GPTJ (Wang and Komatsuzaki 2021) as the main backbone of our experiments, with additional experiments using GPT-2 (Radford et al. 2019). Datasets. To investigate the performance of each method in many different scenarios, we select 15 datasets, as stipulated in Table 1. The selected dataset covers single sentence task to sentence pair tasks, binary to 150 classes (various numbers of class labels), in diverse domains. The detailed list of each dataset and references are covered in Appendix. Setting & Reporting Methods. Our experiments cover both a few-shot setting and full training data setting. As a reporting method, we select currently prevailing linear probing methodologies (i.e., SVM, SLP, LR, GDA, and k-NN) and their application of PALP. In a few-shot setting, we take a closer look at how the performance of each method changes when the training input is converted into ICL style and compare them with ICL, which is known to yield superior per-\nGPT-J 4-shot per class\nMethod\nAG\nSST2\nRotten\nStance\nEmotion\nTREC\nCoLA\nOffensive\nMNLI\nRTE\nMRPC\nAVG\nB\nk-NN\n54.66\n51.33\n58.67\n54.73\n33.92\n48.84\n42.36\n49.37\n35.09\n48.74\n64.66\n49.31\nLR\n66.48\n50.25\n65.08\n58.64\n36.89\n64.48\n50.68\n51.72\n36.46\n49.39\n59.02\n53.55\nSVM\n66.20\n50.41\n65.91\n59.27\n36.59\n65.92\n48.90\n48.72\n36.02\n49.31\n61.96\n53.56\nSLP\n67.88\n50.23\n65.68\n59.45\n37.13\n66.80\n49.13\n49.95\n36.36\n49.24\n61.76\n53.96\nGDA\n66.32\n50.41\n65.93\n58.64\n36.88\n66.44\n48.88\n48.72\n36.03\n49.24\n61.96\n53.59\nT\nk-NN\n61.88\n58.30\n65.10\n35.27\n45.18\n51.04\n53.02\n59.98\n36.40\n53.14\n61.47\n52.80\nLR\n71.84\n62.00\n71.73\n58.09\n50.56\n65.56\n49.86\n59.00\n38.73\n50.54\n60.44\n58.03\nSVM\n72.08\n63.67\n71.33\n56.27\n50.27\n66.36\n53.08\n62.02\n37.93\n49.60\n61.18\n58.53\nSLP\n73.79\n63.58\n72.18\n57.45\n52.71\n68.44\n58.81\n63.07\n38.25\n49.96\n61.57\n59.53\nGDA\n72.82\n64.20\n71.33\n56.27\n52.86\n66.00\n53.08\n62.02\n38.13\n49.60\n61.18\n58.86\nD\nk-NN\n75.99\n70.16\n71.01\n49.45\n56.95\n46.80\n55.67\n54.56\n38.85\n51.09\n63.86\n57.67\nLR\n77.92\n70.62\n80.58\n61.27\n68.97\n65.96\n53.83\n67.63\n40.30\n49.00\n62.27\n63.49\nSVM\n77.96\n75.46\n79.42\n55.82\n68.91\n66.24\n55.30\n63.75\n40.12\n51.54\n64.76\n63.57\nSLP\n80.69\n77.41\n81.01\n71.36\n70.38\n71.92\n69.13\n72.35\n39.41\n54.66\n71.73\n69.10\nGDA\n76.08\n70.16\n68.05\n55.91\n65.66\n63.76\n54.42\n69.18\n39.32\n50.53\n64.15\n61.57\nICL\n81.74\n91.77\n90.45\n23.09\n72.76\n64.00\n37.89\n73.19\n36.04\n55.60\n68.38\n63.17\nGPT-J 8-shot per class\nB\nk-NN\n63.05\n50.55\n65.42\n58.09\n36.05\n60.16\n52.41\n52.26\n35.69\n51.70\n59.17\n53.14\nLR\n74.56\n57.50\n74.15\n65.73\n41.90\n79.08\n52.79\n58.47\n37.43\n51.34\n58.97\n59.27\nSVM\n73.26\n57.16\n75.87\n64.27\n42.66\n80.12\n53.98\n57.79\n37.43\n52.06\n59.56\n59.47\nSLP\n74.72\n57.91\n75.25\n63.82\n42.87\n80.92\n54.34\n55.93\n37.37\n51.05\n59.46\n59.42\nGDA\n74.12\n57.16\n75.78\n63.00\n44.31\n81.84\n53.98\n57.79\n37.60\n52.06\n59.56\n59.75\nT\nk-NN\n69.65\n61.93\n65.46\n56.45\n49.87\n62.44\n45.29\n56.35\n38.13\n50.90\n56.42\n55.72\nLR\n78.52\n69.33\n77.04\n66.00\n59.93\n79.04\n51.51\n59.30\n41.24\n52.56\n63.48\n63.45\nSVM\n78.39\n75.44\n77.97\n63.36\n60.53\n81.12\n51.98\n65.07\n40.45\n53.07\n61.13\n64.41\nSLP\n79.61\n72.80\n78.03\n66.09\n62.36\n80.12\n52.10\n64.37\n40.87\n52.49\n61.67\n64.59\nGDA\n79.03\n75.37\n77.90\n62.55\n62.15\n81.68\n51.98\n65.05\n40.90\n53.07\n61.18\n64.62\nD\nk-NN\n79.21\n67.98\n78.82\n49.63\n58.31\n58.36\n50.43\n63.26\n38.24\n51.37\n56.94\n59.32\nLR\n84.12\n73.78\n85.33\n66.55\n66.66\n69.04\n57.20\n69.21\n42.25\n53.43\n64.30\n66.53\nSVM\n83.96\n77.10\n85.27\n64.36\n68.32\n71.20\n55.55\n71.65\n43.00\n53.60\n60.82\n66.80\nSLP\n85.27\n78.37\n86.75\n69.27\n69.97\n75.76\n69.63\n71.23\n42.30\n53.26\n71.94\n70.34\nGDA\n83.05\n72.55\n83.83\n61.18\n64.32\n68.08\n55.02\n71.05\n42.10\n51.42\n61.92\n64.96\nICL\n83.26\n91.72\n89.72\n27.27\n73.12\n71.60\n34.28\n73.02\n36.62\n54.08\n68.38\n63.92\nTable 2: Experimental results of 11 different datasets on GPT-J in 4-shot / 8-shot per class settings. B, T, and D refers to a baseline, PALP-Template, and PALP-Demonstration respectively. For each dataset, the best method is in bold and the second best method and is underlined.\nformance in the data-hungry setting. In the full-shot setting, we investigate the performance gap between our method and the white-box training method (i.e., accessible to model parameters), such as Adapter (Houlsby et al. 2019) or full finetuning, which can be considered upper bound. Other details. We optimized the hyper-parameters of each classification method on SST2 dataset with 4 Tesla V100 SXM2 32GB GPUs and universally utilized them in different settings. (Detailed hyper-parameters and implementations are in the Appendix.) Additionally, we found a manual template for each task where ICL exhibited sound performance and utilized them universally in our methods. (All templates for each task are stipulated in the Appendix.) For stable evaluation, we report the average of 5 different seeds (13, 27, 250, 583, 915) as a model performance and report standard deviations for each task in the Appendix.\n# Few-shot Results\nIn the few-shot setting, we experimented based on the number of accessible samples for each task class. For instance, a 4-shot setting in the Sentiment Analysis task with 2 classes (positive and negative) means that a total of 8 samples are accessible, which is analogous to a balanced 8-shot setting in ICL. Table 2 summarizes the performance of ICL, baseline linear probing methods, and their application of PALP (T and D) in the 4,8-shot setting. Baseline refers to utilizing raw input without modification, which is a conventional supervised learning paradigm. Template (PALP-T) and demonstration (PALP-D) refer to template-based training samples and demonstration-based training samples from our method individually. We now refer to a T for PALP-T and D for PALP-D for short. (Additional results with 16-shot is in the Appendix)\nGPT-J Full dataset\nMethod\nAG\nSST2\nRotten\nStance\nEmotion\nTREC\nCoLA\nOffensive\nMNLI\nRTE\nMRPC\nAVG\nB\nk-NN\n88.87\n76.15\n84.99\n68.64\n56.72\n91.80\n66.73\n74.42\n43.63\n51.26\n67.89\n70.10\nLR\n90.96\n91.97\n86.96\n72.32\n72.20\n97.60\n77.37\n73.60\n71.07\n68.59\n75.22\n79.81\nSVM\n90.34\n90.48\n88.18\n72.27\n71.26\n96.80\n76.13\n72.91\n67.32\n69.68\n75.98\n79.21\nSLP\n90.43\n90.88\n89.51\n75.00\n75.40\n97.00\n77.87\n80.69\n70.70\n70.71\n76.44\n81.33\nGDA\n92.18\n92.40\n89.49\n72.27\n72.56\n97.00\n78.52\n74.07\n71.07\n68.59\n74.27\n80.22\nT\nk-NN\n89.87\n87.61\n85.83\n66.82\n72.55\n91.60\n66.16\n73.84\n51.70\n57.40\n73.04\n74.22\nLR\n92.58\n92.66\n88.84\n77.73\n79.80\n96.60\n78.81\n80.00\n76.28\n73.65\n80.39\n83.39\nSVM\n90.58\n89.45\n85.83\n75.91\n79.52\n97.40\n75.10\n73.84\n72.58\n71.48\n80.39\n81.10\nSLP\n92.49\n93.35\n89.87\n78.36\n81.30\n97.60\n79.19\n82.67\n77.41\n72.20\n82.84\n84.30\nGDA\n92.36\n94.27\n90.81\n75.46\n81.07\n97.00\n78.91\n82.21\n75.91\n71.84\n79.17\n83.55\nD\nk-NN\n90.69\n91.17\n89.96\n75.00\n73.89\n89.40\n69.70\n77.09\n51.54\n54.51\n72.55\n75.95\nLR\n92.47\n92.73\n90.54\n77.73\n79.73\n95.40\n80.25\n82.79\n71.94\n76.17\n76.96\n83.34\nSVM\n90.78\n91.97\n88.37\n77.27\n76.92\n95.60\n80.06\n75.47\n51.38\n76.53\n76.23\n80.05\nSLP\n92.58\n93.37\n91.33\n83.51\n82.31\n94.00\n77.31\n82.23\n76.28\n77.62\n80.39\n84.63\nGDA\n92.86\n93.00\n90.81\n73.18\n81.66\n96.60\n79.10\n81.16\n75.26\n77.26\n77.70\n83.51\nAdapter\n95.50\n95.53\n90.26\n81.53\n83.54\n97.41\n84.48\n84.67\n88.84\n82.80\n88.48\n88.46\nFine-tuning\n94.80\n94.15\n91.79\n81.25\n84.41\n97.22\n82.34\n84.02\n87.47\n83.33\n86.51\n87.94\nTable 3: Experimental results of 11 different datasets on GPT-J in full datases settings. B, T, and D refers to a baseline, PALPTemplate, and PALP-Demonstration respectively. For each dataset, the best method is in bold and the second best method and\nTable 3: Experimental results of 11 different datasets on GPT-J in full datases settings. B, T, and D refers to a ba Template, and PALP-Demonstration respectively. For each dataset, the best method is in bold and the second be is underlined.\nWhile ICL displays sound performance in various tasks, the baseline linear probing method exhibits poor performance compared to ICL. In sentiment analysis tasks (SST2, rotten tomatoes), the performance of ICL is above 90% only with 4-shot samples per class, whereas the baseline linear probing methodology almost makes arbitrary random decisions in the same environment (around 50% \u223c60%). However, the performance of ICL quickly saturates and scales poorly with the number of available training samples. And even in some cases, ICL performs worse than arbitrary decisions without understanding the target task at all (e.g., stance, CoLA). Furthermore, if the input length exceeds a certain level, it is infeasible to utilize ICL in the usual way. (See the 16, 32-shot results in the Appendix.) On the other hand, linear probing methods are much more scalable with the number of available samples, revealing stable performance regardless of the dataset. Moreover, their application of PALP boosts performance by a substantial margin minimizing the gap between ICL, especially in most single-sentence tasks. We can obtain around 5% improvement in average from each ablation (appending template and demonstrations) in the 4-shot setting, and some linear probing methods outperform ICL. The most high-performance results were obtained from the SLP among other linear probing methodologies.\n# Full-data Results\nTable 3, 4 display the performance of different methodologies when the training data is fully available. Appending a simple template also displays a significant advantage even in a data-rich scenario, obtaining considerable improvements in accuracy regardless of the methods or the dataset. No-\nGPT-J Full train dataset\nMethod\nCLINC\nBanking\nCB\nBoolQ\nB\nk-NN\n74.78\n69.06\n71.43\n63.01\nLR\n92.91\n89.44\n80.36\n62.70\nSVM\n91.20\n89.55\n80.36\n63.77\nSLP\n91.52\n89.43\n80.35\n62.70\nGDA\n93.78\n89.54\n80.36\n63.00\nT\nk-NN\n90.42\n86.82\n78.57\n63.55\nLR\n95.76\n91.17\n83.93\n63.39\nSVM\n96.49\n92.52\n83.93\n64.50\nSLP\n95.16\n91.58\n83.93\n66.30\nGDA\n96.16\n92.79\n82.14\n64.50\nTable 4: ICL cannot be applied to tasks with a large number of classes (i.e., CLINC, Banking) or a lengthy inputs (i.e., CB, BoolQ). While our method inherits similar problem in PALP-D but we can apply PALP-T to linear probing methods. For each dataset, the best method is in bold.\ntably, the performance of k-NN increases dramatically with the application of the template (PALP-T), which improved accuracy by 16% on the Emotion dataset. However, the method of appending demonstration (PALPD) has more cons than pros in a data-abundant scenario. First, while PALP-D often performs similarly to or better than PALP-T, they also sometimes yield worse scores than PALP-T, leading to a similar performance on average. Speaking otherwise, PALP-D only makes the input more lengthy and entails much higher inference costs compared\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d4a6/d4a6f5ea-2640-486a-a9fb-bfe580eef345.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Template</div>\nFigure 4: t-SNE visualization of SST2 representation from GPT-J. Adding understandable prompts to the input can reshape the representation into a more task-specially clustered form without any supervision. Demonstration-best is the representation obtained by attaching the demonstration that showed the best performance, and Demonstration-worst is the opposite.\nAcc\nPrefix (\u03c4)\nVisualization\nMax (84.86)\nSentence 1: awful movie. \\\\Sentiment: negative\nSentence 1: soulful , scathing and joyous. \\\\Sentiment: positive\nFig. 4c\nMin (54.62)\nSentence 1: without any passion. \\\\Sentiment: negative\nSentence 1: , incoherence and sub-sophomoric. \\\\Sentiment: positive\nFig. 4d\n<div style=\"text-align: center;\">Table 5: Best and worst performing example prefixes from SST2.</div>\nto PALP-T in a data-abundant scenario. Moreover, PALP-D is infeasible to be applied to some tasks inheriting the limitations of ICL, as can be seen in Table 4: tasks with a large number of classes (i.e., CLINC, Banking), or tasks with long inputs (i.e., CB, BoolQ). Nevertheless, PALP greatly minimizes the performance gap between white-box tuning methods, such as Adapter or full fine-tuning, and black-box tuning methods, which is around 7% with baseline linear probing methods, while our approach narrows this gap to nearly 4%. In particular, our method outperforms or reaches statistically equivalent performance to white-box tuning methods in some tasks, such as rotten tomatoes, TREC, and CLINC.\n# Analysis & Ablations\n# Application to Small PLM\nIn this subsection, we examine our method for relatively small PLM to verify whether our approach is transferrable to small language models. Namely, we report the performance of 3 different tasks (sentiment analysis, natural language inference, and multiclass classification) on GPT-2 large (Radford et al. 2019) in a 4-shot per class setting. Table 6 summarizes the performance. Similar to previous experiments, our method mainly shows considerable performance gain on single sentence tasks and relatively small improvement on challenging tasks like sentence pair tasks. To summarize, PALP also benefits smaller language models, unlike ICL, which is known to yield poor performance or be unable to apply them to relatively small language models. However, the performance improvement is less significant with smaller PLMs since our methodology depends solely on the capability of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a39c/a39c2350-46e5-4ab6-ad31-9f597f67a00c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Demonstration-best</div>\n<div style=\"text-align: center;\">(d) Demonstration-worst</div>\n<div style=\"text-align: center;\">GPT-2 (Large) 4-shot per class</div>\nGPT-2 (Large) 4-shot per class\nMethod\nSST2\nAG\nMNLI\nB\nk-NN\n50.34\n26.59\n33.86\nLR\n51.81\n46.06\n33.22\nSVM\n50.99\n36.18\n34.24\nSLP\n49.77\n43.56\n35.05\nGDA\n50.23\n28.23\n33.87\nT\nk-NN\n53.53\n42.49\n34.35\nLR\n53.23\n52.36\n32.8\nSVM\n51.77\n44.10\n34.04\nSLP\n52.27\n43.23\n34.97\nGDA\n54.91\n35.51\n33.26\nD\nk-NN\n56.54\n63.44\n35.93\nLR\n58.11\n69.87\n37.44\nSVM\n57.86\n70.88\n37.84\nSLP\n55.02\n70.58\n37.09\nGDA\n58.56\n61.78\n36.54\nTable 6: Results on GPT-2 (Large) in 4-shot per class setting. B, T, and D refers to a baseline, template, and demonstration individually. Our method is transferable to smaller model.\nthe language model.\n# Dataset Visualization\nIn this experiment, we visualize the representation space when the differing input pre-processing method is applied to the input. Figure 4 is the result of the t-SNE visualized representation of the SST2 task on GPT-J. Demonstration-best\nis the representation obtained by attaching the demonstration that showed the best performance, and Demonstrationworst is the opposite. Table 5 summarizes actual examples of demonstrations and accuracy of the aforementioned best and worst cases. Consistent with the experimental results, we confirmed that PLM could extract more distinctive representations when appropriate templates or demonstrations are concatenated to the input of interest. The fact that a more meaningful representation can be drawn by applying a template is understandable and quite intuitive, as research has already shown that using a template can benefit fine-tuning performance (Liu et al. 2021; Schick and Sch\u00a8utze 2021a,b). What is even more intriguing is that adding demonstrations to the front can promote language models to derive a more taskspecific and form a more distinguishable representation cluster. While the degree of improvement varies significantly, depending on the selected demonstrations, even concatenating the poorest performing demonstration yields a more clustered representation than the baseline result indicating the language model\u2019s capability to learn from the context of the input (Min et al. 2022b). Although we did not specifically identify which demonstrations were more helpful and which were not, we found that mislabeled demonstrations can hurt the overall quality of the representations. As can be seen in Table 5, the second demonstration of the worst demonstrations is wrongly labeled, where we conjecture is the cause of the poor performance as advocated in a recent study Yoo et al. (2022) that the mapping of input sentence and ground-truth label space can be crucial.\n# Related Work\nLarge language models such as GPT-3 (Brown et al. 2020) and ERNIE 3.0 (Sun et al. 2021) are often released as blackbox APIs due to commercial considerations and the potential risk of misuse. Thus, users are unable to train those pretrained models with the traditional transferring paradigm (i.e., fine-tuning). Even in some cases where the weights of the pre-trained model are accessible (Zhang et al. 2022; Scao et al. 2022), it may not be possible for many researchers to fine-tune those models due to the enormous resource they require. Several studies were proposed to circumvent the problems as mentioned earlier:\n# Black-box Tuning\nBlack-box tuning is a methodology that makes use of the target model without internal model access (e.g., gradient, middle layer representations, or weights). As such, blackbox tuning methods usually train a lightweight discriminator on top of pre-trained models or optimize input prompt in a derivative-free fashion since the gradient of the pretrained language models is unavailable. Specifically, Diao et al. (2022); Sun et al. (2022) attempts to find optimal prompt input without utilizes the natural evolution strategy (NES) to find better prompts for black-box models instead of using natural NES to fool the model as in black-box adversarial attacks. Sun et al. (2022) adopted a covariance matrix adaptation evolution strategy to perform optimization in a\nrandomly generated small subspace, which is effective due to the low intrinsic dimensionality of large language models (Aghajanyan, Gupta, and Zettlemoyer 2021; Qin et al. 2021).\n# In-Context Learning\nICL (Brown et al. 2020) is an alternative to the gradientbased tuning method. It is a novel transferring method that derives answers via conditioning appropriate prompts or often concatenating the training data. ICL is drawing explosive interest in the field of NLP due to their strong generalizability among many different tasks, from traditional natural language understanding tasks, including sentiment analysis and natural language inference (Wang et al. 2019), to extreme ones, such as code generations (Poesia et al. 2022) or mathematical problems (Henighan et al. 2020). As the underlying mechanism of ICL astonished the NLP field and has reminisced the capability of PLMs, a plethora of works has been proposed to utilize and understand ICL better. Studies include advanced ICL methods maximizing the downstream performance (Zhao et al. 2021; Min et al. 2022a; Holtzman et al. 2021), advanced methods of choosing example data (Liu et al. 2022a; Lu et al. 2022; Rubin, Herzig, and Berant 2022), understanding the limitation of ICL (Liu et al. 2022a; Lu et al. 2022), and understanding the underlying mechanism of ICL (Xie et al. 2022; Reynolds and McDonell 2021; Min et al. 2022b; Razeghi et al. 2022; Yoo et al. 2022).\n# Conclusion\nIn this paper, we showed that providing task descriptions or demonstrations can enforce PLM to yield more robust representations without additional adaptation of the model weights, allowing them to be used for lightweight linear probing as an alternative to in-context learning. In light of this finding, we proposed prompt-augmented linear probing, where we augmented data representations with ICL-style crafted inputs. Our integrated approach is scalable with the available training data and the size of the language model. PALP obtains comparable results to ICL in the data-hungry scenario and comparable results to fine-tuning in the dataabundant scenario with little training overhead, potentially making PALP a strong alternative in various situations. In our follow-up study, we will analyze how the additional prompt tokens (e.g., demonstrations or templates) affect the representation quality of the encapsulating input text. We are also interested in the effect of adopting self-supervised learning objectives, such as contrastive learning (Gao, Yao, and Chen 2021), to the shallow layers on top of the language model backbone, which might improve our method further.\n# Acknowledgements\nThis work was mainly supported by SNU-NAVER Hyperscale AI Center and partially by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) No.2020-0-01373, Artificial Intelligence Graduate School\nProgram (Hanyang University), and No.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)]. Lastly, we would like to express gratitude to Kyunghyun Cho and the anonymous reviewers for their precious feedback.\n# References\nLiu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2022a. What Makes Good In-Context Examples for GPT-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL. Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig, G. 2022b. Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Comput. Surv. Liu, X.; Zheng, Y.; Du, Z.; Ding, M.; Qian, Y.; Yang, Z.; and Tang, J. 2021. GPT Understands, Too. arXiv preprint arXiv:2103.10385. Lu, Y.; Bartolo, M.; Moore, A.; Riedel, S.; and Stenetorp, P. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL. Min, S.; Lewis, M.; Hajishirzi, H.; and Zettlemoyer, L. 2022a. Noisy Channel Language Model Prompting for FewShot Text Classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL. Min, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.; Hajishirzi, H.; and Zettlemoyer, L. 2022b. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Petroni, F.; Rockt\u00a8aschel, T.; Riedel, S.; Lewis, P. S. H.; Bakhtin, A.; Wu, Y.; and Miller, A. H. 2019. Language Models as Knowledge Bases? In Inui, K.; Jiang, J.; Ng, V.; and Wan, X., eds., Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, EMNLP. Poesia, G.; Polozov, A.; Le, V.; Tiwari, A.; Soares, G.; Meek, C.; and Gulwani, S. 2022. Synchromesh: Reliable Code Generation from Pre-trained Language Models. In The Tenth International Conference on Learning Representations, ICLR. Qin, Y.; Wang, X.; Su, Y.; Lin, Y.; Ding, N.; Liu, Z.; Li, J.; Hou, L.; Li, P.; Sun, M.; et al. 2021. Exploring lowdimensional intrinsic task subspace via prompt tuning. arXiv preprint arXiv:2110.07867. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9. Razeghi, Y.; Logan IV, R. L.; Gardner, M.; and Singh, S. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206. Reynolds, L.; and McDonell, K. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 1\u20137. Rubin, O.; Herzig, J.; and Berant, J. 2022. Learning To Retrieve Prompts for In-Context Learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili\u00b4c, S.; Hesslow, D.; Castagn\u00b4e, R.; Luccioni, A. S.; Yvon, F.; Gall\u00b4e, M.; et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100. Schick, T.; and Sch\u00a8utze, H. 2021a. Exploiting ClozeQuestions for Few-Shot Text Classification and Natural Language Inference. In Merlo, P.; Tiedemann, J.; and Tsarfaty, R., eds., Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, EACL. Schick, T.; and Sch\u00a8utze, H. 2021b. It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL. Shwartz, V.; West, P.; Le Bras, R.; Bhagavatula, C.; and Choi, Y. 2020. Unsupervised Commonsense Question Answering with Self-Talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 4615\u20134629. Sun, T.; Shao, Y.; Qian, H.; Huang, X.; and Qiu, X. 2022. Black-Box Tuning for Language-Model-as-a-Service. In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesv\u00b4ari, C.; Niu, G.; and Sabato, S., eds., International Conference on Machine Learning, ICML. Sun, Y.; Wang, S.; Feng, S.; Ding, S.; Pang, C.; Shang, J.; Liu, J.; Chen, X.; Zhao, Y.; Lu, Y.; et al. 2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems, 5998\u20136008. Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In 7th International Conference on Learning Representations, ICLR. Wang, B.; and Komatsuzaki, A. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https: //github.com/kingoflolz/mesh-transformer-jax. Accessed: 2021-05. Xie, S. M.; Raghunathan, A.; Liang, P.; and Ma, T. 2022. An Explanation of In-context Learning as Implicit Bayesian Inference. In The Tenth International Conference on Learning Representations, ICLR. Yoo, K. M.; Kim, J.; Kim, H. J.; Cho, H.; Jo, H.; Lee, S.-W.; Lee, S.-g.; and Kim, T. 2022. Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\nZhao, Z.; Wallace, E.; Feng, S.; Klein, D.; and Singh, S. 2021. Calibrate Before Use: Improving Few-shot Performance of Language Models. In Meila, M.; and Zhang, T., eds., Proceedings of the 38th International Conference on Machine Learning, ICML.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of limited scalability in in-context learning (ICL) performance with large-scale language models due to inherent input length constraints. Previous methods have struggled to leverage the full potential of available training samples, necessitating a new approach to enhance performance in data-scarce scenarios.",
        "problem": {
            "definition": "The problem at hand is the challenge of effectively classifying input sequences using pre-trained language models (PLMs) without access to their parameters, which limits traditional fine-tuning methods.",
            "key obstacle": "The core obstacle is the inherent input length limitation of language models that restricts the amount of training data that can be effectively utilized in ICL, leading to poor performance as the number of samples increases."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that longer and more elaborate prompts can improve the reasoning capabilities of language models, thereby enhancing the quality of the representations derived from them.",
            "opinion": "The proposed idea, known as prompt-augmented linear probing (PALP), combines linear probing with ICL techniques to create a more effective representation extraction process that scales better with available training data.",
            "innovation": "The primary innovation of PALP lies in its ability to utilize both templates and class-wise demonstrations to create more meaningful input representations, which significantly improve performance compared to existing methods."
        },
        "method": {
            "method name": "Prompt-Augmented Linear Probing",
            "method abbreviation": "PALP",
            "method definition": "PALP is a method that enhances the representation quality of inputs by augmenting them with task-specific templates and demonstrations, allowing for effective classification without direct model fine-tuning.",
            "method description": "PALP transforms raw datasets into more understandable forms using templates and class-wise demonstrations, enabling robust linear probing on the extracted representations.",
            "method steps": [
                "1. Apply a template to the input data to provide context.",
                "2. Concatenate a representative demonstration for each class to the input.",
                "3. Train linear classifiers on the transformed dataset."
            ],
            "principle": "The effectiveness of PALP is grounded in the hypothesis that well-crafted prompts and demonstrations can help the language model better understand the input, leading to improved representation quality and classification performance."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized various datasets covering different tasks and class distributions, with evaluations performed in both few-shot and full training scenarios.",
            "evaluation method": "Performance was assessed by comparing the results of PALP against baseline methods using standard linear probing techniques, measuring accuracy across multiple tasks and dataset configurations."
        },
        "conclusion": "The experiments demonstrated that PALP effectively closes the performance gap between ICL in data-scarce scenarios and fine-tuning in data-abundant scenarios, making it a viable alternative for black-box tuning methods.",
        "discussion": {
            "advantage": "PALP significantly enhances classification performance by leveraging templates and demonstrations, thus improving the robustness of representations extracted from PLMs.",
            "limitation": "While PALP shows strong performance, it inherits some limitations of ICL, such as performance sensitivity to the quality of demonstrations and potential inefficiencies in data-abundant scenarios.",
            "future work": "Future research will focus on refining the impact of additional prompt tokens on representation quality and exploring the integration of self-supervised learning objectives to further enhance the methodology."
        },
        "other info": {
            "acknowledgements": "This work was supported by SNU-NAVER Hyperscale AI Center and other grants from the Korean government.",
            "dataset statistics": {
                "number of datasets": 15,
                "task types": [
                    "Single Sentence",
                    "Sentence Pair"
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) performance is limited by inherent input length constraints in large-scale language models, necessitating new approaches for enhancement."
        },
        {
            "section number": "1.2",
            "key information": "The proposed method, Prompt-Augmented Linear Probing (PALP), addresses the challenge of classifying input sequences using pre-trained language models without direct access to their parameters."
        },
        {
            "section number": "1.3",
            "key information": "Larger and more elaborate prompts can improve the reasoning capabilities of language models, thus enhancing representation quality in ICL."
        },
        {
            "section number": "3.1",
            "key information": "PALP enhances representation quality by augmenting inputs with task-specific templates and demonstrations, allowing for effective classification without direct model fine-tuning."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of PALP is grounded in the hypothesis that well-crafted prompts and demonstrations help the language model better understand inputs, leading to improved classification performance."
        },
        {
            "section number": "6.1",
            "key information": "PALP inherits some limitations of ICL, including performance sensitivity to the quality of demonstrations and potential inefficiencies in data-abundant scenarios."
        }
    ],
    "similarity_score": 0.6934990671147734,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Prompt-Augmented Linear Probing_ Scaling beyond the Limit of Few-shot In-Context Learners.json"
}