{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.07546",
    "title": "Does In-Context Learning Really Learn? Rethinking How Large Language Models Respond and Solve Tasks via In-Context Learning",
    "abstract": "In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format, which helps LLMs respond to desired label words. We then demonstrate that this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL. Our findings demonstrate that retrieving the semantically similar examples notably boosts the model\u2019s discriminative capability. However, we also observe a trade-off in selecting good in-context examples regarding label diversity1.",
    "bib_name": "long2024doesincontextlearningreally",
    "md_text": "# Does In-Context Learning Really Learn? Rethinking How Large Language Models Respond and Solve Tasks via In-Context Learning\nQuanyu Long\u22171 Yin Wu\u22171 Wenya Wang1 Sinno Jialin Pan1,2 1Nanyang Technological University, Singapore 2The Chinese University of Hong Kong {quanyu001, wuyi0023, wangwy}@ntu.edu.sg sinnopan@cuhk.edu.hk\n# Abstract\nIn-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format, which helps LLMs respond to desired label words. We then demonstrate that this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL. Our findings demonstrate that retrieving the semantically similar examples notably boosts the model\u2019s discriminative capability. However, we also observe a trade-off in selecting good in-context examples regarding label diversity1.\narXiv:2404.07546v2\n# 1 Introduction\nRecent advancements in Large Language Models (LLMs) through in-context learning (ICL) have shown significant capability across a broad range of tasks (Brown et al., 2020; Min et al., 2022; Yoo et al., 2022; Pan et al., 2023; Wang et al., 2023a). By leveraging a few demonstrations (comprising input-label pairs), LLMs achieve high performance compared to zero-shot inference without updating millions of model parameters. Recent works have attempted to reveal the myth beneath ICL characteristics. Xie et al. (2022) propose that in-context demonstrations can enhance the model to \u201crecall\u201d the latent knowledge acquired during pre-training. Other empirical studies try to answer how ICL helps downstream tasks by studying the correctness of input-label mapping within the demonstrations (Min et al., 2022; Yoo et al., 2022; Pan et al., 2023). However, all those studies examine and report the gap between ICL and zero-shot setting, but do not provide a definitive answer regarding the specific factors that contribute to this gap. A more thorough exploration of the precise contributions of demonstrations towards improving end-task performance is necessary. In this paper, we take a deeper look into the components of ICL\u2019s contribution and try to answer the following question: Which aspect of ICL power plays a crucial role? To achieve this, we decompose the improvement with ICL into three factors that empower the ICL ability of LLMs and quantitatively analyze their impact. These factors are label space, label format, and discrimination. The motivation stems from the tendency of generalpurpose LLMs that produce responses with redundant information and inconsistent formats.\n*Equal contribution. 1Codes are available at https://github.com/ruyue0001/decompose ICL improvement.\nbservations from ICL applications indicate that incorporating ICL can help regulate LLM utputs to comply with designated label space and adhere to the format of demonstrative xamples. Beyond the label space and format power of ICL, the discrimination power of CL represents the model\u2019s discriminative ability to solve tasks provoked by semantically ch demonstrations. As ICL provides more few-shot contexts and examples, the LLMs are xpected to \u201clearn\u201d from those contexts, thereby enhancing their discriminative capability nd increasing the accuracy of predictions. However, it remains unclear whether the erformance improvement brought by ICL is largely due to format/space regulation, or the pability of recalling latent discriminative knowledge via semantically rich demonstrations.  this paper, we quantify the contribution of each of the above-mentioned factors. The etailed definition for each factor and the quantification method are introduced in Section 3. e experiment with four general-purpose and instruction-tuned LLMs and measure the ree contributing factors on several classification, sequence labeling, and generation atasets. With extensive experiments, we aim to answer the following research quesons: 1) Which aspect does ICL contribute to: discrimination, label space, or label format?  What is the mechanism of retrieval helping with ICL? 3) Beyond format, to what extent oes demonstration text style affect generation tasks? Here are some key takeaways: \u2022 A large part of the ICL improvement sources from the label space and format which are regulated by demonstrations. However, Counter-intuitively, ICL brings the least improvement on discrimination which also appears to be unstable across tasks. \u2022 ICL functions similarly to detailed instruction in a prompt and serves the role of casting instruction of label space and format implicitly. \u2022 When provided with random demonstrations, the knowledge of semantic discrimination is less invoked through those semantically rich contexts, which can even be harmful to confuse the model in many tasks and models. \u2022 When provided with incorrect labels within the demonstrations, the ICL\u2019s power to regulate label space and format is barely influenced. This observation can explain the reason that incorrect labels within demonstrations have minimal impact on overall performance (Min et al., 2022). \u2022 When retrieving the most similar examples as demonstrations, the discrimination power of ICL significantly improves. Our experiments show that LLM predictions align closely with the labels of retrieved demonstrations, with the majority class among these labels often matching the ground-truth label. However, when all the retrieved demonstrations are from the ground-truth class (lacks diversity), ICL\u2019s regulation power on label space and format will be weakened, suggesting that there is a trade-off when selecting good in-context examples. \u2022 Similar to the observations in classification tasks that LLMs tend to follow the label space and format of the demonstrations, our findings in text generation tasks suggest the LLM responses also mimic the text style of demonstrations even when not explicitly instructed to do so.\n# 2 Related Work\nRecent studies on large language models (LLMs) have unveiled their capability for incontext learning (ICL), where the model adapts to new tasks solely through inference (Brown et al., 2020). Subsequent research studies have focused on both theoretical and empirical explorations to enrich the understanding of ICL\u2019s mechanisms. Xie et al. (2022) explain ICL as implicit Bayesian inference, where the pre-trained LMs implicitly infer and recover a latent concept that is learned during the pretraining. Similarly, Wang et al. (2023b) examine the ICL phenomenon through a Bayesian lens and view them as implicit topic models that infer a latent variable from prompts. Other theoretical studies investigate ICL based on learning algorithms for linear models on transformers, positing that ICL effectively operates as implicit gradient descent to update an \u201cinner model\u201d (Aky\u00a8urek et al., 2023; Von Oswald et al., 2023; Dai et al., 2023). Recent work hypothesizes label words in demonstrations function as pivotal anchors that facilitate the aggregation and distributing of task-specific information (Wang et al., 2023a).\nFor empirical studies, Min et al. (2022) indicate that maintaining the structured format of demonstration (text-label pair) is critical, while random substitution of labels within demonstrations has minimal impact on performance. However, Yoo et al. (2022) challenge that ground-truth labels play a crucial role in ICL on the downstream tasks. Recent work evaluates the model\u2019s capability of task recognition through the introduction of wrong and abstract labels (Pan et al., 2023). From existing empirical studies, we can observe they primarily focus on the label correctness of demonstrations. However, the experiments using randomized labels within demonstrations fall short of elucidating the underlying ICL mechanism comprehensively. There is also a limited understanding regarding why incorrect labels have minimal impact on performance. Additionally, previous works examine and report the gap between the zero-shot setting and various ICL setups (e.g., ground-truth labels, random labels), while the factors underlying this gap and the mechanisms driving the efficacy of ICL remain ambiguous. We provide a thorough discussion of the relationships and distinctions in comparison to prior research in Appendix E, including differences in the definition of \u201dFormat\u201d and the use of instruction-tuned models).\n# 3 Decomposing ICL Improvement\n# 3.1 True power of ICL may not be reflected in the observed performance gain\n1 True power of ICL may not be reflected in the observed perfo\nWhen querying general-purpose LLMs with specified downstream tasks, they may not strictly follow the instructions and are likely to generate responses with undesired formats. To evaluate the LLMs\u2019 performances under such circumstances, it is common to leverage post-processing scripts with the aim of filtering irrelevant fragments in the output and only keeping those relevant to the answer for identifying label verbalizers. However, simple post-processing may lead to inaccurate evaluations. Taking previous works (Min et al., 2022; Yoo et al., 2022) as an example, they test the presence of labels only at the first position of generated responses. However, instances with labels showing up in other positions would not be evaluated fairly. After performing ICL which provides text-label pairs (labels are single words) as demonstrations, larger amounts of predictions will be detected in the first position. This phenomenon is attributed to the tendency of LLMs to follow demonstration labels. Consequently, the true power of ICL is not properly evaluated. To better quantify how ICL contributes to the performance gain and give precise attribution of the aforementioned tendency, we introduce two factors in ICL studies: label space and label format. Label space refers to the pre-defined set of label targets, encompassing all acceptable labels regardless of synonyms. Label format is the set of label verbalizers that could be identified by post-processing (exact string match), for example, NLI tasks consider a set of format patterns such as \u201cnon-entailment\u201d and \u201cnot entail\u201d within the post-process. It\u2019s worth mentioning that such post-processing scripts cannot cover all the format variations without prior knowledge, especially for those formats that occur less frequently. With the definition of label space and format, LLMs\u2019 outputs can be categorized into three types according to the post-processing, and Figure 1 gives an illustration: \u2022 OOS: out-of-space, i.e., out of a pre-defined set of label targets. An example is predicting \u201cneutral\u201d in binary sentiment classification. \u2022 ISOOF: in-space-out-of-format, i.e., out of the pre-defined format patterns of label verbalizers. For example, in NLI tasks, formats such as \u201cno-entailment\u201d or \u201cnoneentailment\u201d which occur less frequently and are not included in the post-processing script will be considered as ISOOF. \u2022 ISIF: in-space-in-format. Taking the above example, \u201cnon-entailment\u201d or \u201cnot entail\u201d can be categorized as ISIF. Note that only ISIF instances can contribute to correct predictions in the final evaluation.\nIn this study, we examine commonly encountered formats within our post-processing procedures, in accordance with the work by Qin et al. (2023). Examples of OOS and ISOOF for each task are listed in Appendix H. Through experiments, we observe that ICL has a strong ability to make the response follow the label space and format of the demonstrations Such an effect of ICL can be summarized in Figure 1. Comparing the three types of outputs\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a84b/a84b63d5-0216-455d-87f8-59a223d58742.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f47a/f47a0f03-07e5-42e7-8917-b97d9a7b5ef8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7545/754541e3-8f8a-4751-ba8f-4947b20eba95.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Without ICL</div>\n(a) Without ICL\nFigure 1: Inference instances can be categorized into three different sets, out-of-space (OOS), in-space-out-of-format (ISOOF) and in-space-in-format (ISIF). When performing ICL, a large proportion (almost all in our experiments) of OOS and ISOOF shift to ISIF.\nwith and without performing ICL, the proportion of ISIF outputs increases significantly by drawing samples from the original OOS and ISOOF categories. These new ISIF samples increase the amounts of right predictions, giving rise to higher performances. With this finding, we take the initiative to ask: how much of the overall performance gain brought by ICL is due to the power of regulating label space and label format?\n# mposing ICL improvements into label space, format, and discrim\n# 3.2 Decomposing ICL improvements into label space, format, and discrimination\nanswer the above question and give quantified attribution to label space and format, we t identify all the responses (w/o ICL and w/ ICL) as OOS, ISOOF, and ISIF. Then we ck the change of categories for all instances (w/o ICL \u2192w/ ICL). Figure 1 illustrates o main shift flows, namely OOS \u2192ISIF and ISOOF \u2192ISIF. In addition, we also observe ny portion of instances following inverse directions, ISIF \u2192OOS and ISIF \u2192ISOOF. ws between OOS and ISOOF are not considered since they do not affect the observed ICL formance. By comparing the outputs w/o ICL and w/ ICL, we propose to decompose  overall performance enhancement facilitated by ICL into three contributing factors, label ace, label format and discrimination, we define: \u2022 Label space power as the performance gain brought by the power of ICL in regulating label space. It\u2019s calculated by (nOOS\u2192ISIF pred right \u2212nISIF\u2192OOS pred right )/N, where N is the total number of instances, and npred right represents the amount of correct predictions in ISIF. The calculation can be understood as the number of predictions corrected by ICL due to the change from OOS to ISIF minus those originally correct within ISIF but shifted to OOS afterward. \u2022 Label format power as the performance gain brought by the power of ICL in regulating label format. Similar to label space, it is calculated by (nISOOF\u2192ISIF pred right \u2212 nISIF\u2192ISOOF pred right )/N. \u2022 Discrimination power as the performance gain brought by the change of predictions from wrong to right within the ISIF set, it is calculated by (nISIF W2R \u2212nISIF R2W)/N. W2R indicates those wrong predictions w/o ICL but are corrected w/ ICL. R2W refers to those correctly predicted w/o ICL but are misclassified to wrong labels w/ ICL. This measurement represents the model\u2019s discriminative ability provoked by semantically-rich demonstrations. As ICL provides more contexts and examples, the discriminative capability is expected to be enhanced.\nTo answer the above question and give quantified attribution to label space and format, we first identify all the responses (w/o ICL and w/ ICL) as OOS, ISOOF, and ISIF. Then we track the change of categories for all instances (w/o ICL \u2192w/ ICL). Figure 1 illustrates two main shift flows, namely OOS \u2192ISIF and ISOOF \u2192ISIF. In addition, we also observe a tiny portion of instances following inverse directions, ISIF \u2192OOS and ISIF \u2192ISOOF. Flows between OOS and ISOOF are not considered since they do not affect the observed ICL performance. By comparing the outputs w/o ICL and w/ ICL, we propose to decompose the overall performance enhancement facilitated by ICL into three contributing factors, label space, label format and discrimination, we define:\n# 4 Experiments Setup\n# 4.1 Datasets\nTo be consistent with existing empirical studies (Min et al., 2022; Yoo et al., 2022; Pan et al., 2023), we evaluate the effectiveness of in-context learning on 9 classification datasets across 5 types of tasks, including: Sentiment Analysis: SST-2 (Socher et al., 2013); Natural Language Inference: WNLI (Levesque et al., 2012) and RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009); Paraphrasing: Medical Question Pairs,\n<div style=\"text-align: center;\">(b) With ICL</div>\nabbreviated as MedQ (McCreery et al., 2020) and MRPC (Dolan & Brockett, 2005); Hate Detection: Tweet Hate (Barbieri et al., 2020) and Hate 18 (de Gibert et al., 2018), and Multiclass topic classification: AG News (Zhang et al., 2015) and TREC (Voorhees & Tice, 2000). In section 7, we experiment on four generation datasets: Story Generation: ROCStories and ROCStories Ending (Mostafazadeh et al., 2016), Text summarization: Reddit (Kim et al., 2019) and SamSum (Gliwa et al., 2019). We also experiment with several sequence labeling datasets, all the details and evaluation metrics are provided in Appendix A.\n# 4.2 Models and other settings\nWe experiment with four general-purpose and instruction-tuned Large Language Models (LLMs): ChatGPT (OpenAI, 2024) (gpt-3.5-turbo-0613 version), GPT-3 (Brown et al., 2020) (accessing via gpt-3.5-turbo-instruct), Llama2 (Touvron et al., 2023) (llama2-13b-chat2) and Mistral (Jiang et al., 2023) (mistral-7b-instruct-v0.23). We do not report the Llama2 scores on Hate Detection datasets due to the safety mechanism of Llama2. We use k = 5 demonstrations for all experiments in the paper, we analyze different numbers of k in Appendix B. The demonstrations are selected from the training dataset of each task. If randomly selected, we experiment with 5 seeds and calculate averaged scores. Prompts and templates for each task can be found in Appendix H.\n# 5 Which Aspect Does ICL Contribute to? Discrimination, Label Space or Label Format?\n# 5 Which Aspect Does ICL Contribute to? Discrimination, Label Space or Label Format?\nWe investigate and take a deeper look at to what extent in-context learning (ICL) contributes to each specific aspect among discriminating power, label space, and label format. To answer this question, we first evaluate ICL using random demonstrations which are sampled randomly from the training dataset and denoted as: Random: k random demonstrations with ground truth labels, {(xi, yi)}k i=1, (xi, yi) \u2208Dtrain We present the results of classification tasks in this section, results for sequence labeling tasks are listed in Appendix D.\n# 5.1 ICL is powerful to regulate the label space and format while disappointing\n# 5.1 ICL is powerful to regulate the label space and format while disappointing regarding discriminating power\nWe measure the three contributing factors according to Section 3.2 on all classification tasks. Figure 2 illustrates the results of three different ICL powers using random demonstrations. From Figure 2 it is evident that the effects of label space and format are consistently positive across all models and tasks, and these two powers account for a large portion of the overall improvement brought by ICL, indicating ICL has strong power to regulate the label space and format, helping LLMs to respond and output desired label words and verbalizers. The label space demonstrates a trend that, as the number of classes increases, the contribution from label space occupies a larger portion. In multi-class classification tasks such as AG News and TREC, the label space has a greater impact than the sum of the other two factors. However, discrimination is the most unstable component. Despite the demonstrations providing semantically rich contexts, counter-intuitively, those contexts have a marginal impact on provoking discriminative knowledge of language models to solve tasks, and the predictive ability of models is not significantly improved through ICL. From Figure 2 we can observe there is at least one model suffering from negative discrimination on all datasets. In NLI and Paraphrase tasks which accept two sentences as input, discrimination is apparent for ChatGPT and comparable with label space and label format. However, for other tasks, discriminating power has minimal positive contribution, and even becomes negative for all models in Hate Detection and multi-class classification datasets.\n2https://huggingface.co/meta-llama/Llama-2-13b-chat-hf 3https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2. For fair comparison, we do n use the Mixture-of-Expert version (\u201cMixtral\u201d).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/85cc/85cc59b4-9d32-4457-8429-8da4d19753e2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Classification results of decomposed ICL contribution: discrimination (red), label space (blue), label format (green) when using Random demonstrations. Scores below zero represent this factor has a negative effect on the performance. We find that discrimination power is the most unstable factor in ICL improvement.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/848f/848fd09d-8898-4f64-9d8b-72d1f9d013c5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Right-to-Wrong (R2W) and Wrong-to-Right (W2R) percentage within the ISIF set. After performing ICL, R2W accounts for a large percentage surprisingly.</div>\nTo explain why ICL brings the least improvement in discrimination power which also appears to be unstable across tasks, we compare the percentage of W2R (wrong-to-right) and R2W (right-to-wrong) instances within the ISIF set. These amounts are included in the calculation of discrimination power. Statistics are provided in Figure 3. With ICL, there are indeed a substantial amount of desired W2R cases, however, there is also a comparable proportion of undesired R2W cases for all of the 9 classification tasks. This result indicates that the impact of discriminating power oscillates when providing random demonstrations. ICL does not always make more correct predictions 4. Previous works suggest that LLMs invoke \u201cconcepts\u201d which are learned during pre-training through the demonstrations, and perform the implicit learning (Xie et al., 2022; Aky\u00a8urek et al., 2023; Von Oswald et al., 2023). However, our results demonstrate that the knowledge of semantic discrimination is less invoked through the semantically-rich demonstrations, and these contexts can be even harmful to confuse the model to predict the correct label in many tasks and models. Nonetheless, this undesired effect can be mitigated by the powers of label space and format, which are dominant in the overall performance gain.\n# 5.2 ICL Functions Similar to Detailed Instructions\nWith the above finding a large part of the ICL improvement sources from the label space and format (new ISIF), an intuitive question to ask is: when the amount of OOS and ISOOF is originally small for zero-shot setting, to what extent ICL could improve the performance? With the surge of instruction-tuned LLMs such as FLAN (Wei et al., 2022) and InstructGPT (Ouyang et al., 2022), we have witnessed the remarkable instruction-following capability of these LLMs. Given such capability, compared to ICL which regulates label space and format implicitly, they can be explicitly and directly incorporated into the task instruction which we refer to as detailed instruction (DI). For example, for NLI tasks, we add the following prompt to the instructions: Please assign a label from [\u2018entailment\u2019, \u2018non-\n4We conduct supplementary experiments to study why numerous examples in ISIF transition from right to wrong after performing few-shot ICL. The results and analysis are detailed in Appendix F.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bbd1/bbd15d12-b220-4b58-9e8a-fc7bf9dd39c1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5682/5682bbc6-54fd-49dd-86b2-a61d94641454.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) ISIF percentage</div>\nFigure 4: Impact of DI (detailed instruction), ICL and their combination DI+ICL. Results are averaged scores across all classification tasks. Breakdown scores are provided in Appendix G. We observe that DI and ICL demonstrate similar performance and the benefit of ICL is nearly diminished when comparing the results of DI+ICL with ICL.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e563/e5639903-5087-47de-8560-2af30c9ca212.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Overall Improvement (Acc)</div>\nFigure 5: Impact of incorrect labels within the demonstrations compared to ground truth labels. Results are averaged scores across all classification tasks. (a) is the ICL overall improvement compared to the zero-shot setting; (b) is the decomposed discrimination score; (c) is the new ISIF percentage coming from OOS and ISOOF, this score can be viewed as the combination of label space and format. Figure (a) and (b) demonstrate a decrease in ICL performance and discrimination power when demonstrations contain incorrect labels, while the label space and format power remain unaffected in Figure (c).\nentailment\u2019]. We then experiment two more prompt variations, DI and DI+ICL. Compared to the zero-shot setting (w/o ICL), DI adds detailed instructions to prompt (still zero-shot), ICL adds few-shot demonstrations which are randomly sampled, and DI+ICL is the setting using detailed instructions and ICL simultaneously.\nentailment\u2019]. We then experiment two more prompt variations, DI and DI+ICL. Compared to the zero-shot setting (w/o ICL), DI adds detailed instructions to prompt (still zero-shot), ICL adds few-shot demonstrations which are randomly sampled, and DI+ICL is the setting using detailed instructions and ICL simultaneously. Figure 4 illustrates the ISIF percentage and task accuracy of four settings (zero-shot, DI, ICL, and DI+ICL). As introduced in Section 3.1, a greater ISIF percentage indicates the label space and format of outputs adhere more closely to the pre-defined set, irrespective of label correctness. We can observe Figure 4 (a) and (b) have similar shapes, indicating the predominant impact of ICL power of label space and format. As shown in Figure 4, all three settings (DI, ICL, and DI+ICL) are having sufficient improvement in ISIF percentage and task performance. It is observed that DI and ICL exhibit comparable results in two figures, with Llama2 being an exception. This suggests that ICL functions similarly to detailed instructions and serves the role of casting instruction of label space and format implicitly. Additionally, by comparing DI with DI+ICL, the benefit of ICL is almost diminished, in contrast to the significant enhancements observed when comparing zero-shot and ICL. Particularly evident for ChatGPT and GPT3, the ISIF percentage in Figure 4 (a) even reaches 100% for DI alone. After providing demonstrations (DI+ICL), ICL\u2019s contribution to task accuracy is minimal as observed in Figure 4 (b). This suggests that when there exists greater space for improvement in OOS and ISOOF, the benefit of ICL becomes more apparent.\n# 5.3 The Power of Space and Format is Consistent for Incorrect \nTo substantiate our proposition that ICL brings limited discrimination power compared to label space and format, we conduct another set of experiments by replacing all the ground-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/feea/feeae740-d440-4d82-aab7-5d612a32c71b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Task accuracy</div>\n<div style=\"text-align: center;\">(c) New ISIF Percentage</div>\ntruth labels within demonstrations with incorrect labels (randomly selecting an incorrect label to replace). Prior work by Min et al. (2022) reveal that substitution of incorrect labels within demonstrations has minimal impact on task performance; however, the underlying reason for this phenomenon is not explained yet. In this section, we re-examine the influence of incorrect labels in ICL through an analysis of label space, format, and discrimination. From Figure 5 (a), we observe the overall ICL improvement decreases across four models after changing demonstration labels. Figure 5 (b) and (c) elucidate the underlying mechanisms. Specifically, Figure (b) shows a pronounced decline in the ICL discrimination score, indicating incorrect label will substantially contaminate the discrimination power of ICL. Conversely, Figure (c) reveals that the proportion of new ISIF remains largely unaffected by incorrect labels, suggesting that the presence of such incorrect labels within demonstrations does not compromise the ICL\u2019s ability to regulate label space and format. This observation may account for the negligible impact on overall performance when substituting the incorrect labels. Since the powers of label space and format remain consistent under incorrect label settings, the discrimination power, despite being significantly affected, only occupies a small proportion compared to label space and format.\n# 6 What is the Mechanism of Retrieval Helping with ICL?\nIt was established by prior work that when the whole training dataset is available, retrieving the demonstrations that are semantically similar to the input significantly enhances ICL (Liu et al., 2022). In our work, we take a deep look into how retrieval helps with ICL. For the retriever, we use SimCSE (Gao et al., 2021) to produce semantically meaningful sentence embeddings and cosine similarity to retrieve top-k (most similar) examples. We denote: Retrieval: top-k retrieved demonstrations, {(xi, yi)}k i=1 with highest s(x\u2032, xi), where s(\u00b7, \u00b7) gives the similarity score, x\u2032 is the current test input.\n# 6.1 Retrieval helps discrimination ability of ICL\nFrom the illustrated results in Figure 6 (a), we can observe retrieval improves the overall performance on all four models compared to randomly selecting demonstration. We then decompose the discrimination, label space, and format. Figure 6 (b) demonstrates the discrimination factor increases by a large margin (Retrieval v.s. Random). This suggests the predictive ability arises substantially through the retrieved semantically-similar examples. From Figure 6 (c), we do not observe an obvious difference in ISIF percentage between random demonstrations and retrieved demonstrations. Therefore, we conclude that retrieval mainly helps the discrimination ability, while brings limited enhancement on label space and format compared to randomly selecting demonstrations.\n# 6.2 Why does retrieval help with discrimination? Is model prediction following the demonstrations\u2019 label?\nWhen performing the demonstration retrieval, we observe a strong semantic correlation between the retrieved instances and the test input, often manifesting in label consistency. For instance, within the context of sentiment classification, given a positive input, the retrieved top demonstrations are all likely to have positive labels. Therefore, a natural question to ask is that: since almost all of the retrieved demonstrations have the same label, is model prediction following this majority label? To answer this question, suppose we cheat by acquiring the gold label y\u2032 of the current input x\u2032, we experiment with four additional methods of collecting demonstrations, which are: Homo-Random: k random demonstrations selected from the same class as y\u2032. Homo-Retrieval: top-k retrieved demonstrations retrieved from the same class as y\u2032. Hetero-Random: k random demonstrations selected from classes other than y\u2032. Hetero-Retrieval: top-k retrieved demonstrations retrieved from classes other than y\u2032. Compared to the Homo and Hetero setting, the normal setting of Random and Retrieval would include demonstrations from all classes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be38/be387360-af9c-40e9-9324-c9e1dfa78340.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9851/985137c8-7e6d-4371-8ede-877ace8d74bf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b04c/b04c6d4d-693d-405a-9ba9-5aeae900eb50.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6d3/b6d3014a-be31-46a8-af0d-01119847b23f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Overall Improvement (Acc)</div>\n<div style=\"text-align: center;\">(b) Discrimination</div>\nFigure 6: Comparing different methods of collecting demonstrations. Results are averaged scores across all classification tasks. Retrieval and Homo-Retrieval settings achieve the highest performance. In contrast, Hetero-Retrieval becomes detrimental, performing worse than Random selection. These findings suggest that retrieval mechanisms can fetch the most similar demonstrations which are likely to match the ground-truth label, and LLMs frequently generate responses that align with the labels of the retrieved demonstrations.\nFor Homo settings where demonstrations are drawn from the same categories as ground truth, Figure 6 (b) shows that ChatGPT and GPT-3 exhibit lower discrimination scores with Homo-Random demonstrations compared to Random ones. Conversely, Mistral and Llama2 demonstrate significantly higher scores. The disparity is especially pronounced in retrieval tasks, where Homo-Retrieval substantially outperforms standard Retrieval for Mistral and Llama2, enhancing discrimination capabilities notably when employing highly similar demonstrations sourced from identical categories. This finding suggests that, compared to Random selection within the ground-truth class, retrieval in Homo settings significantly augments the discrimination power of ICL, as the response of LLMs is more likely to follow this demonstrations\u2019 label (the ground-truth label in Homo settings) when performing retrieval. However, in Figure 6 (c) we can observe ISIF percentage decreases for Homo settings, indicating the powers of label space and format are weakened when all demonstrations have the same label. This finding underscores diversity also plays a critical role in selecting demonstrations, aligning with the research by Levy et al. (2023). The results in Hetero settings can further support the aforementioned hypothesis. It is obvious that the Hetero settings significantly degrade performances. Firstly, we find that the discrimination power is notably compromised as shown in Figure 6 (b). When compared to random selection, retrieving from non-ground-truth classes proves more detrimental and exacerbates this decline. This suggests that LLM outputs tend to follow semantically similar demonstrations, which do not align with the ground truth in Hetero settings. Secondly, the absence of ground truth labels leads to uncertainty in determining the correct label token. This results in a decline in the new ISIF percentage (Figure 6 (c)). The above findings in Homo and Hetero experiments suggest that retrieval can fetch the most similar demonstrations which are likely to provide the correct label for LLM to follow and output.\n# 7 Beyond Format, To What Extent Styles Affect Generation Tasks?\nIn previous sections, we discussed the regulation power brought by ICL on label space and format for classification tasks. To extend our investigation, we raise a question: could the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/76a8/76a85136-7bef-4c7b-94ab-8ddc0b98af9a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) New ISIF Percentage</div>\nDataset\nZero-shot\nICL\nActive Formal Passive\nReddit\n13.68\n16.33\n16.20\n15.67\n16.27\nSamSum\n25.50\n28.81\n27.96\n26.74\n27.98\nROCStories\n4.92\n7.73\n7.68\n6.93\n7.35\nROCStories Ending 4.70\n8.34\n7.45\n6.64\n7.08\n<div style=\"text-align: center;\">(a) ChatGPT</div>\nDataset\nZero-shot\nICL\nActive Formal Passive\nReddit\n11.87\n18.02\n18.85\n17.89\n13.30\nSamSum\n22.68\n30.12\n29.79\n27.53\n28.01\nROCStories\n6.28\n7.97\n8.24\n8.27\n7.72\nROCStories Ending 4.10\n6.87\n6.83\n6.45\n6.55\n(c) Mistral\nTable 1: Evaluation results for text generation datasets with different styles of references (labels) within the demonstrations. Active, Formal, and Passive denote the three editing styles. The table reveals that incorporating style shifts negatively impacts performance across all editing styles. These findings suggest the LLM responses also mimic the text style of demonstrations in generation tasks even when not instructed to do so.\nregulation power on space and format affect generation tasks as well? In generation tasks such as story generation and text summarization, the ground-truth answers are inherently more flexible in their \u201cformats\u201d compared to classification tasks. In this context, we redefine the label format as the stylistic nuances of the generated text. We investigate whether the LLM output will follow the style and results in lower evaluation scores when altering the text styles of the demonstration labels. Since automatic evaluation methods for text generation such as BLEU score (Papineni et al., 2002) and ROUGE score (Lin, 2004) are based on n-grams, they are likely to be influenced by text styles. A sentence could have multiple styles of expression while preserving its semantic meaning. We consider the following three style shifts: Active: transforming all references (labels) within the demonstrations to active voice; Passive: altering labels to passive voice; Formal: Adopting more formal vocabulary and grammar. We do not include the casual style since the references in employed summarization datasets are already casual. We randomly select 5 demonstrations from the training set and then prompt ChatGPT to modify the label styles of these demonstrations based on three specified directions. To ensure consistency to the original semantic meaning, we supplement this process with human effort. Detailed examples and case studies are provided in Appendix H for reference. We present results from two text summarization and two story generation datasets. As shown in Table 1, the results indicate that ICL using original demonstration references achieves the highest scores across most cases. However, when incorporating style shifts, performance is hindered across all three settings, with the Formal setting experiencing the most pronounced decline. This outcome aligns with the intuition: vocabulary selection deviates more substantially from the ground truth when using the Formal setting, whereas the active and passive settings impose fewer changes in vocabulary. This observation underscores ICL\u2019s capacity to regulate response style (format) in generation tasks.\n# 8 Conclusion\nIn this paper, we study the mechanisms underlying the effectiveness of ICL in improving end-task performance by decomposing the contributions of ICL into three factors: label space, label format, and discrimination. Our investigation reveals that ICL significantly improves performance by refining label space and format. Surprisingly, ICL yields the least improvement in eliciting discriminative knowledge within semantically-rich contexts. Additionally, our analysis of retrieving good demonstrations highlights the importance of choosing diverse and semantically relevant demonstrations to boost ICL performance. In summary, our study enhances comprehension regarding how LLMs respond and solve tasks via ICL and gives insights of selecting optimal demonstrations.\nDataset\nZero-shot\nICL\nActive Formal Passive\nReddit\n14.68\n19.60\n18.11\n18.04\n18.85\nSamSum\n26.40\n32.29\n30.12\n27.57\n29.83\nROCStories\n4.46\n7.89\n8.61\n7.85\n7.29\nROCStories Ending 4.35\n6.90\n6.53\n6.24\n6.90\n<div style=\"text-align: center;\">(b) GPT3</div>\nDataset\nZero-shot\nICL\nActive Formal Passive\nReddit\n12.83\n15.00\n14.13\n14.19\n13.29\nSamSum\n24.97\n28.89\n28.76\n27.97\n28.38\nROCStories\n7.74\n9.23\n9.20\n8.23\n8.88\nROCStories Ending 3.97\n4.36\n4.21\n4.19\n4.35\n# 9 Acknowledgement\nSinno J. Pan thanks the support of the Hong Kong Jockey Club Charities Trust to the JC STEM Lab of Integration of Machine Learning and Symbolic Reasoning and the Microsoft Research Asia collaborative research grant. This research is also supported by the NTU Start-Up Grant (#023284-00001).\n# References\nEkin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=0g0X4H8yN4I. Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1644\u20131650, November 2020. doi: 10.18653/v1/2020.findings-emnlp.148. URL https://aclanthology. org/2020.findings-emnlp.148.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, pp. 1877\u20131901, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, volume 3944 of Lecture Notes in Computer Science, pp. 177\u2013190, 2005. doi: 10.1007/11736790\\ 9. URL https://doi.org/10.1007/11736790 9. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 4005\u20134019, July 2023. doi: 10.18653/v1/2023.findings-acl.247. URL https: //aclanthology.org/2023.findings-acl.247. Ona de Gibert, Naiara Perez, Aitor Garc\u00b4\u0131a-Pablos, and Montse Cuadros. Hate speech dataset from a white supremacy forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pp. 11\u201320, October 2018. doi: 10.18653/v1/W18-5102. URL https://aclanthology.org/W18-5102. Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 140\u2013147, September 2017. doi: 10.18653/v1/ W17-4418. URL https://aclanthology.org/W17-4418. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894\u20136910, November 2021. doi: 10.18653/v1/2021.emnlp-main. 552. URL https://aclanthology.org/2021.emnlp-main.552. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 1\u20139, June 2007. URL https://aclanthology.org/ W07-1401. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70\u201379, November 2019. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409. R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, pp. 785\u2013794, 2006. URL https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf& doi=33f25fae10da978fad3f48eb6bded2f733b28e92. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00b4elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00b4ee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. Abstractive summarization of Reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2519\u20132531, June 2019. doi: 10.18653/v1/ N19-1260. URL https://aclanthology.org/N19-1260. Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14, 2012, 2012. URL http://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492. Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1401\u20131422, 2023. doi: 10.18653/v1/ 2023.acl-long.78. URL https://aclanthology.org/2023.acl-long.78. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario \u02c7Sa\u02c7sko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl\u00b4ement Delangue, Th\u00b4eo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franc\u00b8ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175\u2013184, November 2021. doi: 10.18653/v1/2021.emnlp-demo. 21. URL https://aclanthology.org/2021.emnlp-demo.21. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74\u201381, July 2004. URL https://aclanthology.org/W04-1013. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, May 2022. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10.\n# OpenAI. OpenAI GPT-3.5, 2024. URL https://openai.com/.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022. URL https://proceedings.neurips.cc/paper files/ paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8298\u20138319, July 2023. doi: 10.18653/v1/2023. findings-acl.527. URL https://aclanthology.org/2023.findings-acl.527. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311\u2013318, July 2002. doi: 10.3115/1073083. 1073135. URL https://aclanthology.org/P02-1040. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pp. 27\u2013 35, August 2014. doi: 10.3115/v1/S14-2004. URL https://aclanthology.org/S14-2004. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. SemEval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pp. 486\u2013495, June 2015. doi: 10.18653/v1/S15-2082. URL https://aclanthology.org/S15-2082. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph\u00b4ee De Clercq, V\u00b4eronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud Mar\u00b4\u0131a Jim\u00b4enez-Zafra, and G\u00a8uls\u00b8en Eryi\u02d8git. SemEval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pp. 19\u201330, June 2016. doi: 10.18653/v1/S16-1002. URL https://aclanthology.org/S16-1002.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is ChatGPT a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339\u20131384, 2023. URL https://aclanthology.org/2023.emnlp-main.85. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, October 2013. URL https: //aclanthology.org/D13-1170. Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pp. 142\u2013147, 2003. URL https://aclanthology.org/W03-0419. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Proceedings of the 40th International Conference on Machine Learning, pp. 35151\u201335174, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/von-oswald23a. html. Ellen M. Voorhees and Dawn M. Tice. Building a question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201900, pp. 200\u2013207, 2000. ISBN 1581132263. doi: 10.1145/345508.345577. URL https://doi.org/10.1145/345508.345577. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJ4km2R5t7. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 9840\u20139855, December 2023a. doi: 10.18653/v1/2023.emnlp-main.609. URL https://aclanthology.org/2023.emnlp-main.609. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https://openreview.net/forum?id=BGvkwZEGt7. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=gEZrGCozdqR.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2422\u20132437, December 2022. doi: 10.18653/v1/2022. emnlp-main.155. URL https://aclanthology.org/2022.emnlp-main.155. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check, 2023. URL https://arxiv.org/abs/ 2305.15005. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, pp. 649\u2013657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/ 250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2422\u20132437, December 2022. doi: 10.18653/v1/2022. emnlp-main.155. URL https://aclanthology.org/2022.emnlp-main.155. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check, 2023. URL https://arxiv.org/abs/ 2305.15005. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, pp. 649\u2013657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/ 250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html.\nDataset\nTrain Size\nEval Size\nEval Split\nClass Labels\nSST2\n67349\n872\nDev\npositive, negative\nRTE\n2490\n277\nDev\nentailment, non-entailment\nWNLI\n635\n71\nDev\nentailment, non-entailment\nMRPC\n3668\n408\nDev\nequivalent, non-equivalent\nMedical\nQuestion\nPairs\n2438\n610\nRandom\nequivalent, non-equivalent\nTweet Eval -\nHate\n9000\n1000\nTest\nhate, non-hate\nHate Speech\n18\n9944\n1000\nRandom\nhate, non-hate\nAG News\n120000\n1000\nTest (Sampled)\nWorld, Sports, Business, Sci-\nence & Technology\nTREC\n5452\n500\nTest\nAbbreviation, Entity, De-\nscription and abstract con-\ncept, Human being, Loca-\ntion, Numeric value\nTable 2: Details of classification datasets. Training set is used for retrieval in Section 6. Evaluation is conducted using either the test or development split. In the absence of these splits, a random subset from the training set is sampled for evaluation.\nDataset\nEval Size\nEval Split\nAverage Target Length (Words)\nROCStories\n500\nTest (Sampled)\n36.26\nROCStories Ending\n500\nTest (Sampled)\n9.40\nReddit\n563\nTest\n26.22\nSamSum\n819\nTest\n20.20\n<div style=\"text-align: center;\">Table 3: Details of text generation datasets.</div>\nTable 3: Details of text generation datasets.\n# A More details about datasets and implementation\n# A.1 Classification Datasets\nTable 2 summarizes the classification datasets used in our study. We utilize 9 datasets across 5 tasks. Sentiment Analysis: SST-2 (Socher et al., 2013); Natural Language Inference: WNLI (Levesque et al., 2012) and RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009); Paraphrasing: MedQ (McCreery et al., 2020) and MRPC (Dolan & Brockett, 2005); Hate Detection: Tweet Hate (Barbieri et al., 2020) and Hate 18 (de Gibert et al., 2018), and Multi-class topic classification: AG News (Zhang et al., 2015) and TREC (Voorhees & Tice, 2000). All datasets in this paper are downloaded from Huggingface\u2019s Dataset (Lhoest et al., 2021). For SST2, RTE, WNLI, MRPC, we utilize the GLUE Benchmark (Wang et al., 2019) version. Due to the absence of ground-truth labels in the GLUE test split, we rely on the validation split. Medical Question Pairs and Hate Speech 18 do not have official train-test splits. For Medical Question Pairs, we follow Min et al. (2022) and Yoo et al. (2022) to randomly split 20% (610 samples) as the evaluation set. For Hate Speech 18, we opt for a test set size of 1000 to ensure consistency with other datasets and manage API call costs. AG News has an official train-test split, but its test set size (7600) is significantly larger than those of other datasets. Thus, we randomly sample 1000 from the 7600. All classification datasets are evaluated using the Accuracy score.\nDataset\nEval Size\nEval Split\nClass Labels\nSemEval 2014\nRestaurants\n800\nTest\npositive, negative, neutral, conflict\nSemEval 2014\nLaptops\n800\nTest\npositive, negative, neutral, conflict\nSemEval 2015\nRestaurants\n685\nTest\npositive, negative, neutral\nSemEval 2016\nRestaurants\n(English)\n676\nEnglish-Test\npositive, negative, neutral\nCoNLL 2003\n3684\nTest\nperson, location, organization, miscellaneous\nWNUT 2017\n1287\nTest\nperson,\nlocation,\ncorporation,\nproduct,\ncreative-work, group\nTable 4: Details of sequence labelling datasets. The label \u201cconflict\u201d, denoting both positive and negative sentiments towards an aspect term, is uniquely annotated in SemEval 2014 and occurs infrequently (2.18% in restaurant reviews and 2.03% in laptop reviews).\n# A.2 Text Generation Datasets\nTable 3 outlines the statistics for the utilized text generation datasets. ROCStories and ROCStories Ending are derived from the same corpus (Mostafazadeh et al., 2016), each story precisely containing 5 sentences. In the ROCStories subset, the initial sentence serves as the prompt for generating the subsequent narrative, with the ground truth comprising the remaining 4 sentences. In contrast, the ROCStories Ending subset uses the first 4 sentences as input, with the model generating the final sentence. A test set of 500 stories is randomly selected for evaluation. The performance of the story generation datasets is assessed using the BLEU-2 score (Papineni et al., 2002). For text summarization datasets, Reddit (Kim et al., 2019) comprises informal documents from the online forum \u201dReddit\u201d. SamSum (Gliwa et al., 2019) consists of human-annotated dialogue summaries. Both datasets are evaluated using the ROUGE-L metric (Lin, 2004).\n# A.3 Sequence Labelling Datasets\nTable 4 provides an overview of the sequence labeling datasets utilized in our study. For ABSA, we use datasets from SemEval 2014 Task 4 (Pontiki et al., 2014), SemEval 2015 Task 12 (Pontiki et al., 2015) and SemEval 2017 Task 5 (Pontiki et al., 2016). Following recent evaluations (Zhang et al., 2023), we select subsets including both restaurant and laptop reviews for SemEval 2014, restaurant reviews for SemEval 2015, and the English restaurant reviews subset for SemEval 2016. For NER, we employ the widely used CoNLL 2003 (Tjong Kim Sang & De Meulder, 2003) and WNUT 2017 (Derczynski et al., 2017) datasets. Evaluations of all sequence labeling datasets are conducted using the F1 score based on exact matches of span-label pairs.\n# A.4 Other Implementation Details\nInference on the Llama2 and Mistral models is conducted using PyTorch and Huggingface\u2019s transformers library. The model.generate() method with default parameters, including temperature=1.0, top_k=50, and top_p=1.0.\nThe ChatGPT generation is implemented through API call to gpt-3.5-turbo interface with the official openai python library. At the time of our experiments, gpt-3.5-turbo refers to the gpt-3.5-turbo-0613 version, which is a snapshot dated June 13th 2023. The GPT3 generation is implemented by API call to gpt-3.5-turbo-instruct interface. According to OpenAI, this interface is a refined and instruction-tuned version of the old text-davinci-003\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/37fe/37fe2cb8-5c3d-46a3-8a8f-bcc1f69571c1.png\" style=\"width: 50%;\"></div>\nFigure 7: Average accuracy of all classification datasets with different number of demonstrations (k). (GPT3) model. We maintain default decoding parameters of temperature=1 and top_p=1 for both.\n<div style=\"text-align: center;\">gure 7: Average accuracy of all classification datasets with different number of demonstra-</div>\n<div style=\"text-align: center;\">Figure 7: Average accuracy of all classification datasets with different number of demonstrations (k). (GPT3) model. We maintain default decoding parameters of temperature=1 and top_p=1 for both.</div>\n# B Number of demonstrations\nWe evaluate the performance of four LLMs across nine classification datasets for varying values of k = 1, 3, 5, 7, 10, 15. The mean accuracy across all datasets is illustrated in Figure 7. For ChatGPT, Mistral, and Llama2, optimal performance is observed at k = 5, yielding a convex accuracy curve. Conversely, GPT-3 reaches its highest accuracy at k = 15, exhibiting an upward trend with increasing k. These observations corroborate prior research (Liu et al., 2022). Consequently, for experimental consistency, we adopt k = 5 throughout our study.\n# C Measurements on sequence labeling tasks\nWe conducted experiments on sequence labeling tasks, specifically Named Entity Recognition (NER) and Aspect-Based Sentiment Analysis (ABSA). Detailed descriptions of the sequence labeling datasets utilized can be found in Appendix A.\nWe conducted experiments on sequence labeling tasks, specifically Named Entity Recognition (NER) and Aspect-Based Sentiment Analysis (ABSA). Detailed descriptions of the sequence labeling datasets utilized can be found in Appendix A. Similar to classification tasks, we categorize the LLMs\u2019 outputs into three types according to the post-processing. Similarly, we use the notation IS / IF / OOS / OOF in Section 3.1, we denote:\n\u2022 OOF: out-of-format. For NER and ABSA, the expected output format is a span-label pair. This consists of an entity span and its corresponding entity type for NER, and an aspect term span along with the sentiment toward the aspect term for ABSA. Responses that deviate into descriptive sentences with extraneous and redundant information, making post-processing challenging, are deemed out-of-format (OOF). For example, a response like \u201cThe sentence contains three entities \u2019Adam\u2019, \u2019Bob\u2019, \u2019Chris\u2019, these are all person names.\u201d is OOF. In contrast, responses such as \u201c1. A. Parore - PERSON (Name) 2. C Ijaz Ahmad - PERSON (Surname or Last name)\u201d and \u201cEntities: Cuttitta, Italy. Type: Person, Organization\u201d are considered In-Format (IF), since the span-label pairs are clearly identifiable. As discussed in Section 3.1, the post-processing script cannot accommodate all possible format variations. \u2022 IFOOS: in-format-out-of-space. The outputs can be interpreted as span-label pairs, but the assigned class may not belong to the task\u2019s label space. For instance, predicting \u201cEntity: Soccer \u2014 Type: Sports\u201d for NER, or \u201cbread, fantasitic\u201d for ABSA. Here, \u201cSports\u201d and \u201cFantasitic\u201d are outside the defined label spaces for these tasks. \u2022 ISIF: in-space-in-format. We include some synonyms in broad sense as IS. For instance, in zero-shot scenarios, models frequently classify location entities that are country names as \u201ccountry\u201d rather than \u201clocation\u201d. In this context, \u201ccountry\u201d is\nconsidered as a synonym for \u201clocation\u201d and is deemed ISIF if the output maintains the desired format.\nAs discussed in Section 3.1 and 3.2, the decomposition can be calculated in terms of the difference in IS/IF/OOS/OOF numbers w/ and w/o ICL. The granularity of prediction varies between classification and sequence labeling tasks. In classification tasks, each question has only one answer. For every question, predictions in zero-shot and ICL settings align, enabling us to track the change of label and shift of OOS, OOF and ISIF for each instance. For sequence labeling tasks, each sentence may yield multiple predicted span-label pairs. Models often produce varying spans in zero-shot and ICL settings, leading to discrepancies in the number of predicted pairs. Therefore, the predicted pairs in zero-shot setting and ICL setting are not matched, making it challenging to track the shift of a predicted pair (e.g. OOS \u2192ISIF). We can only indirectly measure the decomposed contribution through the reduction of IFOOS or IF-WrongSpan pair counts. For sequence labelling tasks, we also decompose the contribution of ICL into discrimination, label space and format: \u2022 Label Format: the contributing factor from ICL in regulating response format, specifically to response in span-label pairs. It is simply calculated by (nOOF zero\u2212shot \u2212 nOOF ICL )/S, where S being the total number of test set samples. \u2022 Label Space: the contributing factor from ICL in regulating label space. It is calculated by (nIFOOS zero\u2212shot \u2212nIFOOS ICL )/S. \u2022 Discrimination: the contributing factor from ICL in correcting ISIF but wrong span / wrong class predictions. It is calculated by ISIF ISIF ISIFWrongLabel ISIFWrongLabel\n\u2212  \u2022 Discrimination: the contributing factor from ICL in correcting ISIF but wrong span / wrong class predictions. It is calculated by\nThat is, the decrease in number of ISIF predictions but with wrong span, plus the decrease in number of ISIF predictions with right span but wrong label.\n\u2022 Indistinguishable: It is important to note that the aforementioned three factors are derived from the reduction of False Positive (FP) predictions from various angles. Consequently, the count of True Positive (TP) predictions also increases Although in certain cases, the increase in TP predictions can be directly attributed to the three factors (e.g., predicting right span wrong class in zero shot, corrected to right span right class with ICL), such cases are rare, with most instances remaining indistinguishable.\nGiven the task\u2019s characteristic where the class label is tied to the span, the aforementioned metrics estimate three decomposed factors. The label space factor is inevitably overlaps with the discrimination factor: insufficient label space information in zero-shot settings leads to wrong span (e.g., model is unable to know \u201cSports\u201d is not within label space, hence mislabeling \u201cSoccer\u201d as an entity). Since the number of predicted pairs varies, we set the denominators to match the test set sizes when comparing with and without ICL to accurately reflect the dataset\u2019s relative percentages.\n# D Results on sequence labeling tasks\nFigure 8 presents the decomposed results based on the method detailed in Appendix C. Surprisingly, in format-sensitive tasks like NER and ABSA, the ICL\u2019s role in format regulation is minimal compared to other factors. This may be because NER and ABSA task instructions inherently convey more format details than classification tasks (see Appendix H for examples). Here, the instructions are containing some hint for response format (\u201cPlease identify all named entities and classify their types\u201d), but not containing any label space information. As discussed in Section 5.2, detailed instructions already offer sufficient format and label space information, making additional demonstrations marginally beneficial.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/346b/346b76f1-2cc0-4310-91be-1d471e32ea75.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Decomposed ICL contributing factors scores for sequence labelling datasets. Rest14: SemEval 2014 - Restaurants subset; Laptop14: SemEval 2014 - Laptops subset; Rest15: SemEval 2015 - Restaurants subset; Rest16: SemEval 2016 - Restaurants subset. The resulting scores do not quantitatively correspond to improvements in F1 scores; rather, they should be interpreted in terms of the relative proportions of the three contributing elements.</div>\nFigure 8: Decomposed ICL contributing factors scores for sequence labelling datasets. Rest14: SemEval 2014 - Restaurants subset; Laptop14: SemEval 2014 - Laptops subset; Rest15: SemEval 2015 - Restaurants subset; Rest16: SemEval 2016 - Restaurants subset. The resulting scores do not quantitatively correspond to improvements in F1 scores; rather, they should be interpreted in terms of the relative proportions of the three contributing elements. The proportion of label space varies between NER and ABSA datasets, especially in zeroshot setting, the label space for named entity types is considerably larger than that for sentiment types in ABSA dataset. This is evident in the WNUT17 dataset, where the label space differs substantially from that of CoNLL03. For instance, It has type \u201ccorporation\u201d specifically for name of company / corporation, \u201ccreative-work\u201d for name of artwork / music album, etc. Without ICL, it becomes exceedingly challenging for models to extract such entities and assign correct classes. Conversely, sentiment types in ABSA datasets are less ambiguous and more straightforwardly annotated. Discrimination factors vary across models and datasets. ChatGPT consistently exhibits low or negative discrimination scores. Conversely, GPT-3 shows high discrimination scores. Mistral and Llama2 present differing behaviors: Mistral has a negative discrimination score for NER but positive for ABSA, while Llama2 shows the reverse pattern.\n# Discussion on Relation and Difference to Previous \nMin et al. (2022) aim to explore the influence of input-label mapping and the format of such mapping (e.g., only input, only label). Their experiments involve providing incorrect labels in demonstrations, we discuss this setting in our Section 5.3. However, despite using the same term \u201cformat\u201d, the definition and researching focus is different. Their experiments on formats are formulated as providing demonstration texts without labels and labels without text, i.e., the format of demonstrations (format of the input-label pairing pattern). Their analytical scope on \u201cformat\u201d can be interpreted as how format of demonstrations affect performance, we instead study the label/response format and the regulation effect brought by ICL. Pan et al. (2023) employ the terms \u201cTask Recognition\u201d (TR) and \u201cTask Learning\u201d (TL) as two factors influencing LLMs\u2019 few-shot capability. TR denotes the model\u2019s ability to perform effectively without depending on input-label pairings. The model can maintain good performance even with incorrect input-label mappings, this resembles previous work Min et al. (2022). TL can be conceptualized the \u201clabel space\u201d power in our paper. Their experiments on TL are are limited to altering the label space (such as converting \u201dpositive\u201d/\u201dnegative\u201d labels to 0/1 or other symbols). However, since the models they adopted are not instructiontuned, they wouldn\u2019t be able to explore the regulation effect on response format. This is one major difference between our work and these previous works, as the response generated by current general-purpose, human-instruction-aligned LLMs differ from the early models. Both works point out the intriguing phenomenon that contaminating demonstration label correctness and replacing label words that have semantically-rich information will affect the model performance. However, their work lack detailed and quantitaive analysis on decomposing the contributions of factors to ICL. Our focus on ICL\u2019s label space and format regulation effect lies in studying the ability of changing OOS and OOF label to desired labels\nwithin the pre-defined set, and we aim to separate such effect from the ability of assigning correct label, i.e. discrimination.\nRegarding retrieving semantically-similar demonstrations, Lyu et al. (2023) find the responses of LLMs are more likely to follow the labels of demonstrations that are semantically close to the input and describe this phenomenon as \u201cCopy Effect\u201d. Here we summarize the difference between our work and Lyu et al. (2023) as the following: (1) Difference of target aspects being studied. The \u201cCopy Effect\u201d is discussed under the context of using incorrect labels in demonstrations. As we mentioned in Section 2, all previous works focus on the label correctness, and take it as the start point to study the ICL. Instead, we decompose the benefit from ICL into three factors and take it as the start point to study which aspect ICL contributes to. (2) We take a deeper look into the \u201cCopy Effect\u201d from the perspective of majority label class instead of false labels. As stated in Section 6.2, we find that when providing demonstrations with the same class of current query\u2019s ground truth answer (\u201chomo\u201d setting), we can observe ISIF percentage decreases, indicating the powers of label space and format are weakened when all demonstrations have the same label (especially semantically close to the input). This finding underscores diversity also plays critical role in selecting demonstrations.\n# F Why do models change from correct to incorrect predictions after performing few-shot ICL?\n# Why do models change from correct to incorrect predictions after\nAs we discussed in Section 5.1, the benefit from ICL regarding discriminating power is unstable. Notably, our observations revealed a phenomenon rarely addressed in prior ICL research: when comparing predictions in zero-shot and ICL settings, there are comparable proportion of cases that changes correct predictions to wrong answers.\nOur hypothesis posits that the quality of demonstrations significantly impacts ICL performance, and random examples may be unrelated or even detrimental to the prediction of some instances. We collect the statistics on another set of experiments in retrieval setting, where ICL demonstrations are selected based on the retriever (Section 6.1). Results indicate that the R2W rate can be moderately mitigated by retrieved demonstrations, as evidenced in Table 5. We observe an improvement in the differences between W2R and R2W rate for all models utilizing retrieved demonstrations. However, the R2W rate remains significant even with retrieved semantically-similar demonstrations. Potential explanations include: 1) the retrieval method based on semantic similarity is imperfect; 2) demonstrations can be regarded as \u201dadditional parameters\u201d that influence the token generation probability distribution. We plan to explore this aspect in future work.\nCategory\nRandom\nRetrieved\nChatGPT\nGPT3 Mistral\nLlama2\nChatGPT\nGPT3\nMistral\nLlama2\nR2R\n60.40% 65.67%\n71.23%\n63.77%\n61.10%\n65.01%\n71.80%\n65.13%\nW2W\n16.10% 15.99%\n14.64%\n10.29%\n13.15%\n12.72%\n10.96%\n10.24%\nW2R\n13.13% 10.34%\n7.25%\n12.47%\n15.66%\n13.72%\n10.97%\n12.47%\nR2W\n10.37%\n7.96%\n6.88%\n13.40%\n10.09%\n8.55%\n6.26%\n12.16%\nW2R-R2W\n2.76%\n2.38%\n0.37%\n-0.93%\n5.57%\n5.18%\n4.71%\n0.32%\nTable 5: Comparison of R2R, W2W, W2R and R2W ratio under Random and Retrieval setting, together with the difference in W2R and R2W. Results are averaged scores across 9 classification datasets.\n# G Breakdown scores of classification tasks\n# G.1 Results in Section 5.1\nTable 6 and 7 offers detailed scores corresponding to the analysis in Section 5.1. Table 6 details the scores for discrimination, label space, and format across four models and nine classification datasets, including an additional column for the average scores across these datasets.\nModel\nFactor\nSST2\nWNLI\nRTE\nMedQ\nMRPC\nTweet Hate\nHate 18\nAG News\nTREC\nAVG\nChatGPT\nDiscrimination\n0.67%\n4.79%\n6.86%\n9.51%\n9.51%\n-1.28%\n-16.58%\n-2.98%\n-2.32%\n0.91%\nLabel Space\n2.16%\n3.38%\n13.29%\n14.23%\n5.44%\n6.36%\n2.54%\n10.14%\n21.28%\n8.76%\nFormat\n1.56%\n5.63%\n9.68%\n5.44%\n3.68%\n2.28%\n8.24%\n9.16%\n1.64%\n5.26%\nGPT3\nDiscrimination\n1.08%\n6.20%\n6.06%\n4.23%\n-2.16%\n3.08%\n-8.60%\n-2.98%\n-3.12%\n0.42%\nLabel Space\n3.19%\n3.66%\n8.07%\n2.03%\n3.97%\n5.12%\n3.50%\n7.20%\n19.76%\n6.28%\nFormat\n1.26%\n4.23%\n7.22%\n8.72%\n13.09%\n2.90%\n2.84%\n7.16%\n1.88%\n5.48%\nMistral\nDiscrimination\n1.06%\n-1.97%\n-1.52%\n0.62%\n-5.98%\n-0.36%\n-6.84%\n-3.20%\n1.80%\n-1.82%\nLabel Space\n1.24%\n5.07%\n9.46%\n1.28%\n1.03%\n0.28%\n-0.06%\n2.70%\n57.00%\n8.67%\nFormat\n1.54%\n1.69%\n3.54%\n0.72%\n2.11%\n2.02%\n0.10%\n2.14%\n0.80%\n1.63%\nLlama2\nDiscrimination\n-2.68%\n5.63%\n0.65%\n3.61%\n3.58%\n-\n-\n-2.22%\n-7.76%\n0.12%\nLabel Space\n6.17%\n9.14%\n8.81%\n2.85%\n2.21%\n-\n-\n10.94%\n35.52%\n10.81%\nFormat\n0.89%\n3.10%\n11.12%\n1.48%\n1.47%\n-\n-\n0.56%\n1.00%\n2.80%\nTable 6: Decomposed ICL contribution factors for classification datasets. Results for hate speech detection using Llama2 are excluded due to its safety mechanisms hindering the generation of meaningful responses. This applies to the subsequent tables as well.\nThe analysis of the instability in discrimination power contribution is detailed in Table 7, with corresponding breakdown scores. Figure 3 highlights the proportions of R2W (right-towrong) and W2R (wrong-to-right). For comprehensive understanding, we included all four answer shift directions: R2W, W2R, R2R (right-to-right), and W2W (wrong-to-wrong).\nModel\nCategory\nSST2\nWNLI\nRTE\nMedQ\nMRPC\nTweet Hate\nHate 18\nAG News\nTREC\nAVG\nChatGPT\nW2R\n1.79%\n15.09%\n15.01%\n31.24%\n29.83%\n7.47%\n2.77%\n2.44%\n12.51%\n13.13%\nR2W\n1.04%\n3.86%\n3.88%\n17.03%\n17.39%\n8.43%\n19.95%\n6.52%\n15.25%\n10.37%\nR2R\n94.20% 45.26% 63.69%\n38.50%\n32.89%\n57.34%\n64.19%\n82.90%\n64.65%\n60.40%\nW2W\n2.97%\n35.79%\n17.42%\n13.22%\n19.89%\n26.76%\n13.09%\n8.13%\n7.59%\n16.10%\nGPT3\nW2R\n3.45%\n20.92%\n18.28%\n13.41%\n12.62%\n9.80%\n3.69%\n4.40%\n6.52%\n10.34%\nR2W\n2.12%\n8.82%\n7.29%\n8.02%\n11.42%\n5.70%\n13.32%\n6.69%\n8.27%\n7.96%\nR2R\n91.50% 34.97% 62.05%\n66.88%\n61.46%\n52.66%\n61.48%\n82.21%\n77.79%\n65.67%\nW2W\n2.94%\n35",
    "paper_type": "method",
    "attri": {
        "background": "In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. This paper aims to empirically decompose the overall performance of ICL into three dimensions: label space, format, and discrimination, evaluating four general-purpose LLMs across diverse tasks.",
        "problem": {
            "definition": "The problem addressed in this paper is the unclear contribution of demonstrations in ICL to the performance of LLMs, specifically whether improvements arise from label space regulation, format adherence, or enhanced discrimination capabilities.",
            "key obstacle": "The main obstacle is the lack of a definitive understanding of how ICL operates and which aspects contribute most significantly to its effectiveness, leading to confusion in the application of ICL in various tasks."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is to dissect the components of ICL to uncover which specific characteristics lead to performance improvements in LLMs.",
            "opinion": "The proposed idea involves a systematic analysis of ICL contributions, focusing on label space, format, and discrimination, to clarify their roles in enhancing model performance.",
            "innovation": "The innovation lies in the empirical decomposition of ICL improvements into quantifiable factors, providing a clearer understanding of how LLMs utilize demonstrations."
        },
        "method": {
            "method name": "Decomposing In-Context Learning",
            "method abbreviation": "DICL",
            "method definition": "DICL is a method that quantitatively analyzes the contributions of ICL to performance by breaking down improvements into three distinct factors: label space, format, and discrimination.",
            "method description": "The core of the method involves systematic experimentation and analysis to isolate the effects of each factor on the overall performance of LLMs during ICL.",
            "method steps": [
                "Identify and categorize outputs from LLMs as out-of-space (OOS), in-space-out-of-format (ISOOF), and in-space-in-format (ISIF).",
                "Measure the changes in these categories with and without ICL.",
                "Quantify the contributions of label space, format, and discrimination based on the observed shifts."
            ],
            "principle": "The method's effectiveness is grounded in the hypothesis that ICL can enhance performance by refining the model's adherence to predefined label spaces and formats, while the role of discrimination is more complex and less stable."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using four LLMs (ChatGPT, GPT-3, Llama2, and Mistral) across nine classification datasets and several generation tasks, with specific attention to the types of demonstrations used.",
            "evaluation method": "The performance of the models was assessed by comparing the accuracy of predictions with and without ICL, analyzing the shifts in output categories, and measuring the contributions of each factor to overall performance."
        },
        "conclusion": "The study concludes that ICL significantly improves performance primarily through the regulation of label space and format, while its impact on discrimination is less pronounced and often unstable. The findings underscore the importance of selecting diverse and semantically relevant demonstrations to optimize ICL performance.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include a clearer understanding of how different factors contribute to ICL, which can guide future model training and application.",
            "limitation": "A limitation of the method is that the discrimination power appears to be the most unstable component, which may lead to inconsistent results across different tasks.",
            "future work": "Future research should explore ways to enhance the discrimination capability of LLMs in ICL settings and further investigate the optimal selection of demonstrations."
        },
        "other info": {
            "info1": "The paper includes extensive experimental results demonstrating the contributions of each factor across various tasks.",
            "info2": {
                "info2.1": "The datasets used include SST-2, WNLI, RTE, MRPC, and others.",
                "info2.2": "The models evaluated are instruction-tuned and general-purpose LLMs."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context Learning (ICL) enables large language models (LLMs) to perform a wide range of tasks without updating millions of parameters."
        },
        {
            "section number": "1.2",
            "key information": "The precise contributions of demonstrations towards improving end-task performance in ICL have not been thoroughly investigated."
        },
        {
            "section number": "1.3",
            "key information": "The paper empirically decomposes the overall performance of ICL into three dimensions: label space, format, and discrimination."
        },
        {
            "section number": "1.4",
            "key information": "The method proposed, Decomposing In-Context Learning (DICL), quantitatively analyzes the contributions of ICL to performance."
        },
        {
            "section number": "2",
            "key information": "The problem addressed is the unclear contribution of demonstrations in ICL to the performance of LLMs."
        },
        {
            "section number": "3.1",
            "key information": "The study highlights the importance of selecting diverse and semantically relevant demonstrations to optimize ICL performance."
        },
        {
            "section number": "3.3",
            "key information": "The method involves systematic experimentation to isolate the effects of label space, format, and discrimination on performance."
        },
        {
            "section number": "6",
            "key information": "The paper concludes that ICL significantly improves performance primarily through the regulation of label space and format."
        }
    ],
    "similarity_score": 0.7820549288697589,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Does In-Context Learning Really Learn_ Rethinking How Large Language Models Respond and Solve Tasks via In-Context Learning.json"
}