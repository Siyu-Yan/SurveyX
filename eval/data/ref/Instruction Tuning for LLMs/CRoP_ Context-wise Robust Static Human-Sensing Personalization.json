{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.17994",
    "title": "CRoP: Context-wise Robust Static Human-Sensing Personalization",
    "abstract": "The advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct patterns in human sensing, influenced by various factors or contexts, challenge generic neural network model\u2019s performance due to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization studies overlook intrauser heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation is especially critical in clinical applications, where limited data availability hampers both generalizability and personalization. Notably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further complicating the challenges. This work introduces CRoP, a novel static personalization approach using an off-the-shelf pre-trained model and pruning to optimize personalization and generalization. CRoP shows superior personalization effectiveness and intra-user robustness across four human-sensing datasets, including two from real-world health domains, highlighting its practical and social impact. Additionally, to support CRoP\u2019s generalization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation studies, and comparisons against state-of-theart baselines.",
    "bib_name": "kaur2024cropcontextwiserobuststatic",
    "md_text": "# CRoP: Context-wise Robust Static Human-Sensing Personalization\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3 Jonathan L Preston1, Asif Salekin4\n27 Sep 2024\n1Syracuse University 2University of Wisconsin-Madison 3University of Maryland-College Park 4Arizona State University\n# Abstract\nThe advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct patterns in human sensing, influenced by various factors or contexts, challenge generic neural network model\u2019s performance due to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization studies overlook intrauser heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation is especially critical in clinical applications, where limited data availability hampers both generalizability and personalization. Notably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further complicating the challenges. This work introduces CRoP, a novel static personalization approach using an off-the-shelf pre-trained model and pruning to optimize personalization and generalization. CRoP shows superior personalization effectiveness and intra-user robustness across four human-sensing datasets, including two from real-world health domains, highlighting its practical and social impact. Additionally, to support CRoP\u2019s generalization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation studies, and comparisons against state-of-theart baselines.\narXiv:2409.17994v2\n# Introduction\nAI in human sensing applications\u2014like activity recognition, fall detection, and health tracking\u2014revolutionizes\ndaily life, especially in personal health management [Wang et al., 2023]. However, unique user patterns and natural distribution shifts [Gong et al., 2023] caused by behaviors, physical traits, environment, and device placements [Ustev et al., 2013, Stisen et al., 2015] lead to the underperformance of generic AI models in practical use. To tackle this, various domain adaptation techniques have been explored, with personalization widely used to adapt a generic model to the target user\u2019s specific domain or natural distribution [Lamichhane et al., 2023, Iaboni et al., 2022, Meegahapola et al., 2023, Ahamed and Farid, 2018, Ren et al., 2022, Sempionatto et al., 2021, Boukhechba et al., 2020]. In literature, personalization occurs either during the enrollment phase (static) [Duan et al., 2023, Liu et al., 2022, Burns et al., 2022] or continuously throughout application use [Daniels et al., 2023, Liu et al., 2024, Wu et al., 2024, Wang et al., 2022a]. Static personalization customizes the model with limited individual data collected at enrollment, requiring minimal computation and user engagement, making it highly practical for human-sensing applications. However, existing such studies often overlook intra-user variability due to factors like changes in magnetic field [Robert-Lachaine et al., 2017], sensor position [Park et al., 2014], terrain [Kowalsky et al., 2021], or the health symptoms P\u00a8aeske et al. [2023], leading to poor intrauser generalizability for contexts not present during personalization. For instance, a smartphone activity recognition model personalized with handheld data may perform poorly when the phone is in a pocket. Static personalization is particularly crucial for clinical datasets, which are often characterized by data scarcity, leading to reduced robustness of lab-validated models for\nprospectively collected users Berisha et al. [2021]. It enhances model accuracy for clinical users whose traits are underrepresented in the global model\u2019s training data. In contrast, continuous supervised personalization is generally infeasible in many health domains since ground truths must be validated by clinicians, making it impractical in continuous settings, especially in remote or mobile health applications. Nevertheless, the distribution of clinical data is expected to shift, even within the same individual. For instance, in clinical speech technologies, changes in data distribution over time may occur due to the progression of neurodegenerative diseases, relevant for disease monitoring apps Stegmann et al. [2020], or through desired learning mechanisms resulting from the use of technology, as seen in automated speech therapy apps Benway and Preston [2023]. Similarly, in stress monitoring via wearables, the distribution of psychophysiological data changes as the same individuals encounter different types of stressors Nagaraj et al. [2023]. This research defines \u2018context\u2019 as the intra-user data distribution formed by varying external factors. This research gap is worsened since static personalization typically relies on a small sample set from the target user, covering limited contexts\u2014particularly in clinical settings or applications with data scarcity Berisha et al. [2021], Benway and Preston [2023]. Commercial human sensing technologies like Google Assistant, Amazon Alexa, and Apple\u2019s Siri also personalize speech recognition models using limited phrases during enrollment [Team, 2017, Awobajo, 2023, Phelan, 2019]. Similarly, the Apple Watch uses initial calibration for enhanced running activity tracking [Apple, 2023, Potuck, 2021]. This limited context during personalization is problematic, as shown in this paper\u2019s Motivation Section, where we demonstrate that static personalization may improve performance in training contexts but can also significantly degrade it in other unseen contexts for the same user. Therefore, given the importance of static personalization in human sensing, this paper addresses its intra-user generalizability gap. As shown in Figure 1, this paper endeavors to personalize an off-the-shelf generic model for a specific user using limited data from limited contexts. The primary objective is to ensure that the personalized model thus obtained exhibits robust generalization capabilities across unseen contexts. Crucially, unseen context\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bb5/3bb57b9b-bd4f-45bd-9f30-ee7409fccc9d.png\" style=\"width: 50%;\"></div>\nOff-the-Shelf Generic Model\n<div style=\"text-align: center;\">Figure 1: Problem Setting</div>\ndata takes no part in training or adjusting the personalized model outcome, and the generic model remains entirely off-the-shelf, with no accessibility for modification or design choices. These constraints highlight the real-world impact of this research, particularly in clinical settings where privacy concerns often limit data sharing Malin et al. [2018], Rathbone et al. [2023], and only trained offthe-shelf models are shared among researchers and developers. To achieve the research objective in Figure 1, this paper introduces CRoP, a novel approach to create context-wise robust static personalized models. The key contributions are:\n2. CRoP is the first to leverage model pruning with adaptive intensity to facilitate the effective identification and integration of generic and personalized model weights to facilitate context-wise robustness for personalizing human sensing applications.\n To showcase CRoP\u2019s efficacy, comprehensive evaluations were performed on four human sensing datasets: PERCERT-R Benway et al. [2023]: a clinical speech therapy dataset, WIDAR [Zhang et al., 2022]: a lab-based WiFi-CSI dataset, ExtraSensory: a real-world mobile sensing dataset [Vaizman et al., 2017], and a stress-sensing dataset [Xiao et al.,\n# 2024], while considering two disjoint contexts for each dataset.\n2024], while considering two disjoint contexts for each dataset.\n4. An empirical justification of CRoP\u2019s design choices that enable intra-user generalizability among different contexts is provided, employing Gradient Inner Product(GIP) [Shi et al., 2021] analysis.\nThe work is accompanied by an extensive appendix, which includes a detailed discussion of the experimental setup, data collection, and related work. Additionally, the appendix provides a detailed analysis of the personspecific results for each dataset along with a detailed ablation study.\n# Related Work\nA few static personalization approaches [Burns et al., 2022, Duan et al., 2023, Liu et al., 2022] aimed for the additional goal of out-of-distribution robustness. However, these methods require access to the generic model\u2014either to make specific design choices [Burns et al., 2022], which prevents them from utilizing off-the-shelf models, or to incorporate knowledge about the target user\u2019s data distribution during the generic model\u2019s training phase [Duan et al., 2023, Liu et al., 2022], raising privacy concerns, particularly in sensitive clinical applications. These requirements do not align with the research objectives of this paper, making them unsuitable as baselines. A different set of approaches that do consider the privacy concern is referred to as source-free domain adaptation [Liang et al., 2020]. Liang et al. [2020] (SHOT) proposed the transfer of hypothesis from source by freezing the parameter weights for the classifier layers and only allowing feature extraction to be finetuned to the new domain. The approach is applicable to unsupervised domain adaptation scenarios and employs self-supervised pseudolabeling to align the target domain\u2019s representations to the source hypothesis. However, these approaches do not address the constraint of limited-context data during finetuning. We adapted SHOT in this paper\u2019s problem setting as one of the baselines. Continuous personalization approaches Wang et al. [2022a], Liang et al. [2020], Wu et al. [2024], Daniels et al. [2023], Fini et al. [2022], Tang et al. [2024a,b], Mallya and Lazebnik [2018], Mallya et al. [2018], Wang\nModel\nGeneric\nPersonalized\n\u2206\nUser\nC1\nC2\nC1\nC2\nC1\nC2\n0\n63.90\n77.09\n87.06\n65.02\n+23.16\n-11.88\n1\n61.80\n79.78\n89.38\n44.38\n+27.57\n-35.40\n2\n45.63\n79.81\n71.88\n64.45\n+29.75\n-26.62\nAverage\n+26.82\n-24.63\n<div style=\"text-align: center;\">Table 1: Performance Comparison of Generic model with conventionally trained personalized model</div>\net al. [2022b] can improve intra-user generalizability by continually fine-tuning the model as new data arrives. Some of these approaches Fini et al. [2022], Tang et al. [2024a,b] require specialized training of the generic model, limiting the use of off-the-shelf pre-trained models. Others like PackNet Mallya and Lazebnik [2018] and Piggyback Mallya et al. [2018] propose supervised methods that require continued steam of labeled data, limiting their application in health-care scenarios. Additionally, Continual Test Time Domain Adaptation (CoTTA) Wang et al. [2022b] proposes unsupervised learning methods and allows the use of off-the-shelf models. However, all continuous learning approaches require repeated computation overhead to adjust the model outcome to new data [Prabhu et al., 2023], which can be infeasible in real-time applications, more so for scalable platforms like wearables Schmidt et al. [2018], which is prominent for health sensing such as stress or fall detection. Nevertheless, since the problems addressed by Packnet, Piggyback, and CoTTA are the closest to the problem addressed in this study, we considered these approaches as baselines. Appendix C provides further details of the related work.\n# Motivation\nWhen learning patterns from human sensing data in a limited context, conventional fine-tuning approaches can overwrite generic knowledge that is not relevant to that specific context but applicable to others, leading to a performance drop in those unrepresented contexts. To illustrate this, we conducted a preliminary study comparing the performance of generic and conventionallyfinetuned [Hong et al., 2016] personalized humangesture-recognition models using the LeNet architecture [Zhang et al., 2022] trained on the WIDAR dataset. Data preprocessing details are discussed in the Experiments section.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6a4/b6a4c23f-6f6a-464b-8c00-e4c8b1185eee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Context C1</div>\n<div style=\"text-align: center;\">(b) Context C2</div>\nFigure 2: Heat map for the absolute magnitude of parameters belonging to penultimate layer for LeNet models finetuned using data from context (a) C1 and (b) C2\nTable 1 compares the performance of generic and conventionally-finetuned personalized models on each user\u2019s data belonging to context C1 and evaluated to the same user\u2019s disjoint data in both C1 (available) and C2 (unseen) contexts. It can be observed that conventional finetuning introduces a significant gain of 26.82% for context C1\u2019s data but at the cost of 24.63% reduction in context C2. Similar patterns are seen when personalization is performed on context C2, as shown in Appendix D. Thus, conventional finetuning-based personalization of the models using the limited data from one context can significantly worsen the model\u2019s performance in an unseen context. To investigate this discrepancy in performance, we compare the distribution of parameter magnitudes of the models personalized on contexts C1 and C2 using conventional finetuning, as shown in Figures 2 (a) and 2 (b). Notably, there is a substantial difference in parameter magnitudes between models trained in different contexts. Additionally, the parameters represented by black pixels in Figure 2 have magnitudes close to zero, indicating two crucial aspects: (a) Their contribution to model inference is negligible, implying redundancy. (b) Interestingly, some of these parameters have high magnitudes in the personalized model of another context, indicating that parameters considered unimportant in one context may be crucial in another.\nThe critical question arises: How can we effectively retain and transfer the valuable generic information about context C2 to the personalized models without access to context C2? \u2013 that this paper addresses.\n# Problem Statement\nGiven a generic model MG \u03b8 , the objective is to tailor a personalized model MP a i \u03b8 specifically for a user Ui utilizing the data Da i associated with available context Ca, here \u03b8 represents the parameters of the model. The primary goal is to ensure that the personalized model MP a i \u03b8 performs reasonably well on Ui\u2019s data Du i derived from an unseen context Cu. Notably, there is no overlap between the data belonging to the two contexts, that is Da i \u2229Du i = \u03d5. In other words, if MCa i \u03b8 represents a conventionallyfinetuned model trained for a user Ui on data Da i , then, the models trained using CRoP, MP a i \u03b8 , must on avg. perform better on both available Ca and unseen context Cu than MCa i \u03b8 . More formally, learning objective can be defined as: \ufffd\nsuch that Da i \u2229Du i = \u03d5 and\n\ufffd d\u2208{Du i ,Da i } \u2113(MP a i \u03b8 , d) < \ufffd d\u2208{Du i ,Da i } \u2113(MCa i \u03b8 , d),\nthat is, the loss incurred by the resulting personalized model MP a i \u03b8 on avg. across all contexts\u2019 data is less than the loss incurred by conventionally-finetuned model MCa i \u03b8 . Here, \u2113represents the standard cross-entropy loss. It is important to emphasize that the above-mentioned optimization problem restricts the usage of data to the available context Ca and has no knowledge of data from the unseen context Cu. Hence, for d \u2208Du i (unseen context data), the information \u2113(MP a i \u03b8 , d), and \u2113(MCa i \u03b8 , d) is absent during the training process.\n# Approach\n# Rationale for The CRoP Approach Design\nAs previously discussed, the generic model\u2019s parameters contain generalizable information across all contexts. Addressing the problem statement requires retaining this information to the greatest extent while enabling fine-tuning for the target user. Furthermore, our investigation revealed\nthat different parameters hold varying degrees of importance in distinct contexts. Hence, the careful selection of subsets of model parameters for personalization and generalization is pivotal for the success of the approach, for which this paper leverages the model pruning paradigm. Model pruning is based on the idea that neural networks include redundant parameters, and removing these parameters has minimal impact on the model\u2019s performance [Luo et al., 2017, Zhu and Gupta, 2017]. Consequently, pruning the fine-tuned personalized model ensures the retention of essential parameters to maintain accuracy for context Ca. However, the pruned parameters can be replaced with corresponding parameters from the generic model, effectively restoring generic knowledge learned across all contexts on those generic model parameters. This restoration may enhance generalizability, ensuring robust performance in unseen contexts Cu. The approach presented in this paper is founded on this insightful strategy.\n# CRoP Approach\nAlgorithm 1 describes the presented approach which takes as input: the generic model MG, user U\u2032 is data Da i for available context Ca, initial value for coefficient of regularization \u03b1 and tolerance for pruning \u03c4; and generates the target personalized model MP a i \u03b8 . Here, \u03b1 and \u03c4 are hyperparameter whose values can be tuned for the given data and model. The approach initiates by finetuning the generic model MG \u03b8 on data Da i , concurrently applying \u21131 regularization to penalize model parameters (line 2). This regularization encourages sparsity by specifically targeting the magnitude of redundant parameters [Mayank, 2023]. This step is followed by the pruning of redundant weights using the \u2018ToleratedPrune\u2019 module (line 3). The pruned weights are then replaced by the corresponding weights from the generic model MG (line 4) to restore generalizability; this hybrid model is referred to as the \u2018Mixed Model.\u2019 This step leads to the modification of the activated paths in the personalized model, resulting in changes in the model inferences. However, since the newly activated paths are determined by weights retained from two models and not learned from data patterns, there is a consequent loss of accuracy, as shown and discussed in Appendix E. To mit-\n1: Input: MG \u03b8 : Generic Model \u22c4Da i : User U\u2032 is data for available context Ca \u22c4\u03b1: coefficient of regularization \u22c4\u03c4: tolerance for pruning 2: Train the Generic model on the personal data Da i\n3: Prune redundant parameters to obtain the pruned substructure\n4: Copy the parameters of generic models to the pruned parameters in the personalized pruned model,\nMP a i \u03b8\u2032\u2032 = \ufffd MP a i \u03b8\u2193 , \u03b8\u2193\u0338= 0 MG \u03b8 , otherwise\n5: Finetune the personalized model on the Da i\n5: Finetune the personalized model on the Da i MP a i \u03b8 = argmin \u03b8 \ufffd d\u2208Da i \u2113(MP a i \u03b8\u2032\u2032 , d) + \u03b1\u2225MP a i \u03b8\u2032\u2032 \u22251\nMP a i \u03b8 = argmin \u03b8 \ufffd d\u2208Da i \u2113(MP a i \u03b8\u2032\u2032 , d) + \u03b1\u2225MP a i \u03b8\u2032\u2032 \u22251\nigate such a loss, as a final step, the Mixed Model undergoes fine-tuning once again on the data from the available context Da i (line 5). The detailed explanation of each of these steps is as follows:\n# Personalized Finetuning with Penalty (Algorithm 1 \u2013\nPersonalized Finetuning with Penalty (Algorithm 1 \u2013 Step 2): The approach uses data Da i to finetune the generic model MG \u03b8 . As shown in the motivation section, such conventional finetuning enhances the model\u2019s accuracy within the available context Ca. Nevertheless, its performance in unfamiliar contexts may get suboptimal. Notably, during the model\u2019s fine-tuning process, we apply \u21131 regularization to penalize the model weights, forcing the magnitudes of redundant parameters to be close to zero [Mayank, 2023]. The regularization coefficient \u03b1 is a trainable parameter optimized during training to minimize the overall loss. As a result, the parameters with\nAlgorithm 2: ToleratedPrune(M, \u03c4, D)\n1: Input: M\u03b8: A Model \u22c4\u03c4: tolerance for pruning \u22c4\nD: data\n2: Pruning Amount p = k\n3: Ao = accuracy(M\u03b8, D)\n4: repeat\n5:\nM\u03b8\u2193= M\u03b8\n6:\nM\u03b8 = Prune(M\u03b8, p)\n7:\nA = accuracy(M\u03b8, D)\n8:\nIncrement Pruning Amount p = p + k\u2032\n9: until A < Ao \u2212\u03c4\n10: return M\u03b8\u2193\n# high magnitudes carry most of the information regarding the data patterns in Da i , offering two key benefits:\n1. Minimal loss in Ca accuracy: A high fraction of parameters have close to zero magnitudes, and their removal results in minimal information loss for context Ca; thus, the adverse impact of pruning in context Ca is minimized. 2. Maximal generalization: The inclusion of regularization aids ToleratedPrune (discussed below) module in efficiently pruning a higher number of parameters, which are then replaced with weights from the generic model. This restores information from the generic model, contributing to enhanced accuracy in unseen contexts.\nToleratedPrune Module (Algorithm 1 \u2013 Step 3): Algorithm 2 outlines the ToleratedPrune module, taking a model M\u03b8, pruning tolerance \u03c4, and the dataset D as inputs. It initiates with a modest pruning amount of k and incrementally increases this amount by k\u2032 until the model\u2019s accuracy exhibits a drop of \u03c4 percent on D. Here, k and k\u2032 are hyperparameters within the range of (0, 1). The module returns M\u03b8\u2193, representing the pruned state of the model before the last pruning iteration. This state is such that further pruning would result in a higher accuracy loss on dataset D than the tolerable amount \u03c4. This module performs pruning leveraging the conventional magnitudebased unstructured pruning [Zhu and Gupta, 2017]. Thus, step 3 in Algorithm 1 generates a pruned personalized model state MP a i \u03b8\u2193whose prediction accuracy on\ncontext Ca is at most \u03c4 percent lower than that of the earlier state MP a i \u03b8\u2032 while using only a fraction of its original parameters. The non-zero weights corresponding to these parameters contribute significantly to model inference for the available context Ca. As a result, MP a i \u03b8\u2193is essentially the minimal sub-structure of the earlier state model MP a i \u03b8\u2032 , which is crucial for correct inference for context Ca. This enables replacing a maximal number of zeroed-out parameters to incorporate information from unseen contexts using the generic model MG \u03b8 in the subsequent steps.\nGenerating the Mixed Model (Algorithm 1 \u2013 Steps 4&5): For generating the Mixed Model MP a i \u03b8\u2032\u2032 , the zeroed out parameters in the pruned model MP a i \u03b8\u2193are replaced by the corresponding parameters in the generic model MG \u03b8 , enabling generic knowledge restoration. Notably, model pruning is often followed by a finetuning step [Luo et al., 2017, Zhu and Gupta, 2017, Liu et al., 2020], where the pruned model undergoes re-training to recover the performance lost during the pruning process. We have observed that, despite the Mixed Model exhibiting improved performance in the unseen context, there is a notable loss of accuracy in the available context due to inconsistent activated paths, as discussed earlier. Thus, the resulting Mixed Model is fine-tuned using the available data Da i . Goyal et al. [2023] suggests that fine-tuning process should mirror pre-training for effective generalization. Therefore, our fine-tuning objective aligns with the pre-training objective used in Line 2 for optimal results. During finetuning, the model state, including the mixed model, with the best validation loss on the seen context, is selected. We found that for some individuals, the mixed model is chosen as the optimal model, which indicates that for some individuals, further finetuning is not required, and our approach can automatically handle that scenario.\n# Experiments\nThis work employs four real-world human-sensing datasets to demonstrate the empirical efficacy of CRoP, two of which are associated with health applications. First, the PERCEPT-R dataset has been used for binary classification for predicting the correctness of / r / sounds\nin automated speech therapy application Benway and Preston [2023]. In order to leverage this dataset for our personalization evaluation, we collaborated with clinical experts to identify and acquire annotations of 16 participants who had correct and incorrect pronunciations of / r / sound at pre-treatment (baseline-phase) and during different treatment phases. Additionally, we use the Stress Sensing dataset Xiao et al. [2024] collected using a psycho-physiological wrist-band, named Empatica E4 [empetica, 2015]. To further demonstrate the efficacy of CRoP, we incorporate two benchmark human-sensing datasets, which include data from the same individuals across multiple contexts: WIDAR [Zhang et al., 2022] and ExtraSensory [Vaizman et al., 2017]. Specifically, we employ WIDAR for a 6-class classification focusing on gesture recognition using WiFi signals, and ExtraSensory for binary classification related to human activity recognition using accelerometer and gyroscope readings. A detailed discussion about the datasets and models, hyperparameters, compute resources and instructions to access code are provided in Appendix A.\n# Pre-Processing of the Datasets:\nWe partitioned each dataset into two disjoint sets of users: (1) a generic dataset for training a generic model and (2) a personalized dataset for training a personalized model for each user. To demonstrate the context-wise robustness, we further partitioned each user\u2019s personalized dataset into different contexts. Table 2 presents the details of this partitioning. For PRECEPT-R, we consider data from the pretreatment phase as the available context, and the treatment phases, where participants undergo clinical interventions, are considered the unavailable context. For the Stress Sensing dataset, the context is determined by two factors: the hand on which the sensor (Empatica E4 wristband empetica [2015]) was worn during data collection and the movement status of the individual. For WIDAR, context is determined by the room and torso orientation during data collection, while for the ExtraSensory dataset, phone\u2019s location on the user\u2019s body (e.g., hand, pocket, bag) defines the context. The term \u2018Scenario\u2019 refers to the combination of available Ca and unseen Cu contexts as outlined in Table 2. All datasets, along with context-wise annotations, will be made public. Notably, throughout the training of personalized mod-\nels, CRoP refrains from utilizing any information from the unseen context Cu. Therefore, while the empirical study indicates an enhancement in the model\u2019s performance for one or a few unseen contexts, it is a proxy for all unseen contexts. Meaning, it is reasonable to anticipate a favorable performance in other unseen contexts as well. Notably, the stress sensing dataset has been evaluated across two different unseen contexts, C1 u and C2 u, none of which participated in the training of the personalized model.\n# Metrics for evaluation\nTo establish the efficacy of CRoP, we quantify the extent of personalization and generalization achieved through the presented approach. Personalization is gauged by comparing our model\u2019s MP a i \u03b8 accuracy relative to the generic model MG \u03b8 , while for generalization, we assess the accuracy of our model MP a i \u03b8 against conventionallyfinetuned personalized models MCa i \u03b8 . Both of these metrics consider classification accuracy in the available Ca and unseen Cu contexts. If A(M, D) represents the classification accuracy of the model M for dataset D and n is the number of users selected for personalization, the metrics of evaluations can be described as follows :\n1. Personalization (\u2206P ): It is defined as the sum of the difference between the accuracy of MP a i \u03b8 and MG \u03b8 over all the contexts averaged over all users\n\u2206P = 1 n \ufffd Ui \ufffd C\u2208{Ca,Cu} (A(MP a i \u03b8 , C) \u2212A(MG \u03b8 , C))\n2. Generalization (\u2206G): It is defined as the sum of the difference between the accuracy of MP a i \u03b8 and MCa i \u03b8 over all the contexts averaged over all users.\n\u2206G = 1 n \ufffd Ui \ufffd C\u2208{Ca,Cu} (A(MP a i \u03b8 , C)\u2212A(MCa i \u03b8 , C))\nAll the results in this section are computed as an average of accuracy obtained for three random seeds.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5872/58720bdb-d2b3-4458-a49c-e0a965bb7053.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 2: Details of data used for personalization</div>\n# Comparison with SOA\nTo demonstrate the efficacy of CRoP in achieving personalization \u2206P while maintaining generalization \u2206G, we compare CRoP with 4 state-of-the-art approaches SHOT [Liang et al., 2020], PackNet Mallya and Lazebnik [2018], Piggyback Mallya et al. [2018], and CoTTA Wang et al. [2022b]. Table 3 compares the performance of CRoP with aforementioned baseline approaches. The values for \u2206P and \u2206G are computed as average over all the participants used for personalization for each dataset. The detailed results for participant-specific evaluations for each dataset are provided in Appendix B and Appendix B.4 shows the errors bars for our approach. It can be observed in Table 3 that CRoP significantly outperforms all the SOA approaches. On average, the personalization benefits \u2206P achieved by SHOT, PackNet, Piggyback and CoTTA are 2.16, 26.05, 18.01 and 9.95 percent points, respectively, while CRoP can achieve 35.23 percent points. However, while comparing \u2206G, one can observe that personalized training using SHOT, PackNet, Piggyback and CoTTA harms generalizability by \u221225.73, \u22121.39, \u22129.43, and \u221217.49 percent points respectively. On the other hand, CRoP shows an average generalization benefit of 7.78. Additionally, it is evident that the unsupervised approaches SHOT and CoTTA yield lower \u2206P and \u2206G than supervised approaches Packnet, Piggyback, and CRoP which is in-line with literature Varma and Prasad [2023]. Psychophysiological stress response is inherently heterogeneous in inter- and intra-user scenarios Nagaraj et al. [2023], leading to subpar performance of the generic model without personalization. However, personalization attains a significant efficacy boost evident from the high \u2206P values. These evaluations confirm that models personalized with CRoP exhibit higher generalizability to unseen con-\ntexts, making them more intra-user robust.\n# Empirical Justification for CRoP\nThis section empirically discusses how each step of CRoP (Algorithm 1) facilitates intra-user generalizability signifying similarity in model\u2019s behavior towards available (available during personalization finetuning) and unseen contexts. Shi et al. [2021] introduced the use of gradient inner product (GIP) to estimate the similarity between a model\u2019s behavior across different domains. If Gi and Gj represent the gradient incurred by the model for Domains Di and Dj, then the sign of the product Gi \u2217Gj represents whether the model treats two domains similarly or not. For instance, Gi \u2217Gj > 0 signifies that the gradient for both domains has the same direction. We used GIP to quantify generalization. A higher GIP value for a personalized model across available and unseen contexts indicates more similar behavior toward both domains. GIP is measured as: \u2225\ufffd i Gi\u22252 \u2212\ufffd i \u2225Gi\u22252. Figure 3 shows that fine-tuning the generic model (Algorithm 1 \u2013 Step 2) on Context 1, optimizes the model for this context, leading to a highly negative GIP, indicating a greater discrepancy between two contexts. Since model pruning results in generalization [Jin et al., 2022], an increase in GIP value can be observed in the pruned model (Step 3). On further analysis, we found that the model complementary to the pruned model (that is, the parameters that were removed) also contributed towards inter-context behavior discrepancy (negative GIP value). However, the same parameters in the generic model (that are replaced in Step 4) formed a more generalizable set of weights, i.e., GIP \u22650. Thus, the model mixing step (Step 4) introduces further generalizability (GIP \u22650) in the personalized model.\nApproach\nSHOT\nPacknet\nPiggyback\nCoTTA\nCRoP\nDataset\nScenrio\n\u2206P\n\u2206G\n\u2206P\n\u2206G\n\u2206P\n\u2206G\n\u2206P\n\u2206G\n\u2206P\n\u2206G\nPERCEPT-R\nScenario 1\n-3.11\n-5.62\n0.10\n-2.41\n-25.31\n-27.83\n-45.06\n-47.58\n5.08\n2.57\nStress Sensing\nScenario 1\n-8.19\n-62.16\n54.70\n0.70\n43.89\n-10.12\n21.93\n-32.07\n67.81\n13.81\nSingle context Change\nScenario 2\n8.90\n-63.27\n75.80\n3.64\n66.22\n-5.94\n51.47\n-20.69\n85.25\n13.08\nStress Sensing\nScenario 1\n-0.49\n-47.24\n52.46\n10.08\n32.40\n-9.97\n30.59\n-11.78\n54.38\n12.00\nDouble context Change\nScenario 2\n3.57\n-45.49\n41.68\n-7.36\n42.76\n-6.25\n33.85\n-15.19\n59.21\n10.15\nWIDAR\nScenario 1\n1.67\n-0.48\n-0.24\n-2.37\n0.84\n-1.28\n-1.05\n-3.18\n8.56\n6.43\nScenario 2\n1.28\n-0.03\n-3.55\n-5.16\n-8.97\n-10.57\n1.81\n0.21\n5.90\n4.30\nExtraSensory\nScenario 1\n7.63\n-10.31\n12.19\n-5.76\n5.03\n-12.91\n-0.6\n-18.54\n17.49\n-0.46\nScenario 2\n8.17\n2.99\n1.33\n-3.85\n5.22\n0.04\n-3.43\n-8.62\n13.52\n8.17\nTable 3: Comparison of CRoP with baseline approaches under the metrics of Personalization (\u2206P ) and G (\u2206G).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6419/6419ec5f-f943-4486-8662-accdd948423f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Variation of GIP at different stages of CRoP signified by the lines in Algorithm 1</div>\nFigure 3: Variation of GIP at different stages of CRoP signified by the lines in Algorithm 1\n# Ablation Study\nThis section presents evaluations showing the effectiveness of the design choices of CRoP, focusing on the WIDAR dataset in Scenario 1. Similar patterns were observed in other scenarios and datasets.\n# One shot Magnitude based pruning as the pruning mechanism\nmechanism\nA variety of pruning mechanism have been proposed in the literature: Magnitude-Based Pruning (MP) [Luo et al., 2017, Zhu and Gupta, 2017], Gradient-Based Pruning (GP)[Liu et al., 2020], pruning top magnitude weights instead of lower ones (MP-T)[Bartoldson et al., 2020], iterative pruning (MP-I)[Paganini and Forde, 2020], and more [Hoefler et al., 2021]. Among these, we found that oneshot magnitude based pruning serves the best purpose for the application discussed in this work. Detailed discussion is provided in Appendix F.1.\n# Weight penalty via \u21131 regularization\nThe use of regularization forces model parameters towards zero. We observed that among \u21130, \u21131, \u21132 and polarization [Zhuang et al., 2020], using \u21131 regularization is most effective in the unseen context. More details can be found in Appendix F.2.\n# Limitations and Future Direction\nSome limitations and future research are discussed below:\n1. The paper performed a limited evaluation on pruning paradigms through ablation studies as it was not the primary focus of the study. Section on ablation study justifies CRoP\u2019s design choice but does not establish any particular paradigm\u2019s superiority in unseen contexts.\n2. The approach relies on using a pre-trained off-theshelf model as an input, the quality of this model can impact the performance of the final personalized models.\n2. The approach relies on using a pre-trained off-theshelf model as an input, the quality of this model can impact the performance of the final personalized models.\n3. We restrict our study to the models benchmarked and deployed for datasets used in this work without accounting for model variability.\n4. The metrics \u2206P and \u2206G are computed for each individual separately as personalized models customized for one user are not applicable to other users in realworld scenarios. Consequently, we focus our evaluations on intra-user generalizability, excluding discussion for inter-user or inter-dataset generalizability.\n4. The metrics \u2206P and \u2206G are computed for each individual separately as personalized models customized for one user are not applicable to other users in realworld scenarios. Consequently, we focus our evaluations on intra-user generalizability, excluding discussion for inter-user or inter-dataset generalizability.\n5. For PERCEPT-R, CRoP shows varying advantages among individuals, linked to the generic model\u2019s sensitivity to pre-treatment data (see Appendix B.3). However, determining which individuals will experience more or less benefit from personalization is beyond this paper\u2019s scope and will be explored in future research.\n# Broader Impact\nThis paper addresses a critical research gap, enhancing the practical utility of human-sensing solutions in realworld applications, particularly in automated healthcare. Next-generation healthcare systems, which employ neural networks for tasks ranging from daily activity detection [Ustev et al., 2013, Stisen et al., 2015] to safety-critical conditions like atrial fibrillation [Comstock, 2017], benefit from personalization due to the heterogeneity in health sensing data [Ji et al., 2021, Sempionatto et al., 2021]. CRoP offers several advantages:\n# Conclusion\nThis study introduces CRoP, a novel static personalization approach generating context-wise robust mod-\nels from limited context data. Using pruning to balance personalization and generalization, empirical analysis on four human-sensing datasets shows CRoP models exhibit an average increase of 35.23% in personalization compared to generic models and 7.78% in generalization compared to conventionally-finetuned personalized models. CRoP utilizes off-the-shelf models, reducing training effort and addressing privacy concerns. With practical benefits and quantitative performance enhancements, CRoP facilitates reliable real-world deployment for AI-based human-sensing applications like healthcare.\n# References\nFarhad Ahamed and Farnaz Farid. Applying internet of things and machine-learning for personalized healthcare: Issues and challenges. In 2018 International Conference on Machine Learning and Data Engineering (iCMLDE), pages 19\u201321. IEEE, 2018.\nApple. Workout types on apple watch. https://support. apple.com/en-us/HT207934, 2023.\nAyo Awobajo. 3 tips to make google assistant your own. https://blog.google/products/assistant/how-topersonalize-google-assistant/, 2023.\nBrian R. Bartoldson, Ari S. Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability tradeoff in neural network pruning, 2020.\nN. R. Benway and J. L. Preston. Artificial intelligence assisted speech therapy for / r / using speech motor chaining and the percept engine: a single case experimental clinical trial with chainingai., 2023. URL https: //surface.syr.edu/etd/1703.\nNina R Benway, Jonathan L Preston, Elaine Hitchcock, Yvan Rose, Asif Salekin, Wendy Liang, and Tara McAllister. Reproducible speech research with the artificial intelligence-ready PERCEPT corpora. J. Speech Lang. Hear. Res., 66(6):1986\u20132009, June 2023.\nVisar Berisha, Chelsea Krantsevich, P Richard Hahn, Shira Hahn, Gautam Dasarathy, Pavan Turaga, and Julie Liss. Digital medicine and the curse of dimensionality. NPJ Digit. Med., 4(1):153, October 2021.\nMehdi Boukhechba, Anna N Baglione, and Laura E Barnes. Leveraging mobile sensing and machine learning for personalized mental health care. Ergonomics in design, 28(4):18\u201323, 2020. David Burns, Philip Boyer, Colin Arrowsmith, and Cari Whyne. Personalized activity recognition with deep triplet embeddings. Sensors, 22(14), 2022. ISSN 14248220. doi: 10.3390/s22145222. URL https://www. mdpi.com/1424-8220/22/14/5222. Jonah Comstock. Study: Apple watch paired with deep neural network detects atrial fibrillation with 97 percent accuracy, 2017. Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, and David Zhang. Efficient model adaptation for continual learning at the edge, 2023. Di Duan, Huanqi Yang, Guohao Lan, Tianxing Li, Xiaohua Jia, and Weitao Xu. Emgsense: A low-effort selfsupervised domain adaptation framework for emg sensing. In 2023 IEEE International Conference on Pervasive Computing and Communications (PerCom), pages 160\u2013170, 2023. doi: 10.1109/PERCOM56429.2023. 10099164. Maciej Dzie\u02d9zyc, Martin Gjoreski, Przemys\u0142aw Kazienko, Stanis\u0142aw Saganowski, and Matja\u02c7z Gams. Can we ditch feature engineering? end-to-end deep learning for affect recognition from physiological sensor data. Sensors, 20(22):6535, 2020. empetica. Real-time physiological signals: E4 eda/gsr sensor, 2015. URL https://www.empatica.com/ research/e4/. Eda Eren and Tu\u02d8gba Selcen Navruz. Stress detection with deep learning using bvp and eda signals. In 2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA), pages 1\u20137. IEEE, 2022. Enrico Fini, Victor G Turrisi da Costa, Xavier AlamedaPineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\nempetica. Real-time physiological signals: E4 eda/gsr sensor, 2015. URL https://www.empatica.com/ research/e4/.\nEda Eren and Tu\u02d8gba Selcen Navruz. Stress detection with deep learning using bvp and eda signals. In 2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA), pages 1\u20137. IEEE, 2022.\nEnrico Fini, Victor G Turrisi da Costa, Xavier AlamedaPineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\nUS Food and Drug Administration. Proposed regulatory framework for mondifications to artificial intelligence / machine learning-based software as a medical device. US Food and Drug Administration: Silver Spring, MD, USA, 63, 2019. doi: 10.1016/j.apergo.2017.04.011.\naesik Gong, Yeonsu Kim, Jinwoo Shin, and SungJu Lee. Metasense: few-shot adaptation to untrained conditions in deep mobile sensing. In Proceedings of the 17th Conference on Embedded Networked Sensor Systems, SenSys \u201919, page 110\u2013123, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369503. doi: 10. 1145/3356250.3360020. URL https://doi.org/10.1145/ 3356250.3360020.\nTaesik Gong, Yewon Kim, Adiba Orzikulova, Yunxin Liu, Sung Ju Hwang, Jinwoo Shin, and Sung-Ju Lee. Dapper: Label-free performance estimation after personalization for heterogeneous mobile sensing. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 7(2):1\u201327, 2023.\nSachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19338\u2013 19347, June 2023.\nYujiao Hao, Rong Zheng, and Boyu Wang. Invariant feature learning for sensor-based human activity recognition. IEEE Transactions on Mobile Computing, 21(11): 4013\u20134024, 2022. doi: 10.1109/TMC.2021.3064252.\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res., 22(1), jan 2021. ISSN 1532-4435.\nin-Hyuk Hong, Julian Ramos, and Anind K. Dey. Toward personalized activity recognition systems with a semipopulation approach. IEEE Transactions on Human-Machine Systems, 46(1):101\u2013112, 2016. doi: 10.1109/THMS.2015.2489688.\nJin-Hyuk Hong, Julian Ramos, and Anind K. Dey. Toward personalized activity recognition systems with a semipopulation approach. IEEE Transactions on Human-Machine Systems, 46(1):101\u2013112, 2016. doi: 10.1109/THMS.2015.2489688.\nAndrea Iaboni, Sofija Spasojevic, Kristine Newman, Lori Schindel Martin, Angel Wang, Bing Ye, Alex Mihailidis, and Shehroz S Khan. Wearable multimodal sensors for the detection of behavioral and psychological symptoms of dementia using personalized machine learning models. Alzheimer\u2019s & Dementia: Diagnosis, Assessment & Disease Monitoring, 14(1):e12305, 2022.\nStanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion: Early phase fisher matrix impacts generalization. In International Conference on Machine Learning, pages 4772\u20134784. PMLR, 2021.\nWenhui Ji, Jingyu Zhu, Wanxia Wu, Nanxiang Wang, Jiqing Wang, Jiansheng Wu, Qiong Wu, Xuewen Wang, Changmin Yu, Gaofeng Wei, et al. Wearable sweat biosensors refresh personalized health/medical diagnostics. Research, 2021.\nTian Jin, Michael Carbin, Daniel M. Roy, Jonathan Frankle, and Gintare Karolina Dziugaite. Pruning\u2019s effect on generalization through the lens of training and regularization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=OrcLKV9sKWp.\nMinyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher sam: Information geometry and sharpness aware minimisation. In International Conference on Machine Learning, pages 11148\u201311161. PMLR, 2022.\nDaniel B Kowalsky, John R Rebula, Lauro V Ojeda, Peter G Adamczyk, and Arthur D Kuo. Human walking in the real world: Interactions between terrain type, gait parameters, and energy expenditure. PLoS One, 16(1): e0228682, January 2021.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, Ontario, 2009. URL https://www.cs.toronto.edu/\u223ckriz/learningfeatures-2009-TR.pdf.\nBishal Lamichhane, Joanne Zhou, and Akane Sano. Psychotic relapse prediction in schizophrenia patients using a personalized mobile sensing-based supervised deep learning model. IEEE Journal of Biomedical and Health Informatics, 2023.\nBarbara A Lewis, Lisa Freebairn, Jessica Tag, Allison A Ciesla, Sudha K Iyengar, Catherine M Stein, and H Gerry Taylor. Adolescent outcomes of children with early speech sound disorders with and without language impairment. Am. J. Speech. Lang. Pathol., 24(2): 150\u2013163, May 2015.\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028\u20136039, 2020.\nHanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, and Yang Li. Enhancing continuous domain adaptation with multi-path transfer curriculum, 2024.\nXin Liu, Yuntao Wang, Sinan Xie, Xiaoyu Zhang, Zixian Ma, Daniel McDuff, and Shwetak Patel. Mobilephys: Personalized mobile camera-based contactless physiological sensing. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 6(1), mar 2022. doi: 10.1145/ 3517225. URL https://doi.org/10.1145/3517225.\nXue Liu, Weijie Xia, and Zhimiao Fan. A deep neural network pruning method based on gradient l1-norm. In 2020 IEEE 6th International Conference on Computer and Communications (ICCC), pages 2070\u20132074, 2020. doi: 10.1109/ICCC51575.2020.9345039.\nJian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In ICCV, pages 5058\u20135066, 2017.\nBradley Malin, Kenneth Goodman, et al. Between access and privacy: challenges in sharing health data. Yearbook of medical informatics, 27(01):055\u2013059, 2018.\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7765\u20137773, 2018. doi: 10.1109/CVPR.2018.00810.\nArun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision \u2013 ECCV 2018, pages 72\u201388, Cham, 2018. Springer International Publishing. ISBN 978-3030-01225-0.\nJames Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1\u201376, 2020.\nK. Mayank. Bxd primer series: Lasso regression models, l1 regularization in general and comparison with l2 regularization, 2023. URL https://www.linkedin.com/pulse/bxd-primer-serieslasso-regression-models-l1-general-comparison-k-/.\nLakmal Meegahapola, William Droz, Peter Kun, Amalia De G\u00a8otzen, Chaitanya Nutakki, Shyam Diwakar, Salvador Ruiz Correa, Donglei Song, Hao Xu, and Miriam Bidoglia. Generalization and personalization of mobile sensing-based mood inference models: An analysis of college students in eight countries. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(4):1\u201332, 2023.\nSujay Nagaraj, Sarah Goodday, Thomas Hartvigsen, Adrien Boch, Kopal Garg, Sindhu Gowda, Luca Foschini, Marzyeh Ghassemi, Stephen Friend, and Anna Goldenberg. Dissecting the heterogeneity of \u201cin the wild\u201d stress from multimodal sensor data. NPJ Digital Medicine, 6(1):237, 2023.\nLaura P\u00a8aeske, Tuuli Uudeberg, Hiie Hinrikus, Jaanus Lass, and Maie Bachmann. Correlation between electroencephalographic markers in the healthy brain. Sci. Rep., 13(1):6307, April 2023.\nMichela Paganini and Jessica Forde. On iterative neural network pruning, reinitialization, and the similarity of masks, 2020.\nWonil Park, Victor J. Lee, Byungmo Ku, and Hirofumi Tanaka. Effect of walking speed and placement position interactions in determining the accuracy of various newer pedometers. Journal of Exercise Science & Fitness, 12(1):31\u201337, 2014. ISSN\n1728-869X. doi: https://doi.org/10.1016/j.jesf.2014. 01.003. URL https://www.sciencedirect.com/science/ article/pii/S1728869X14000057. David Phelan. Amazon admits listening to alexa conversations: Why it matters. https://shorturl.at/fxN78, 2019. Michael Potuck. How to reset your apple watch fitness calibration for more accurate workout and activity data. https://9to5mac.com/2021/08/26/fix-applewatch-workout-tracking-activity-tracking/, 2021.\n1728-869X. doi: https://doi.org/10.1016/j.jesf.2014. 01.003. URL https://www.sciencedirect.com/science/ article/pii/S1728869X14000057.\nMichael Potuck. How to reset your apple watch fitness calibration for more accurate workout and activity data. https://9to5mac.com/2021/08/26/fix-applewatch-workout-tracking-activity-tracking/, 2021.\nA. Prabhu, H. Al Kader Hammoud, P. Dokania, P. S. Torr, S. Lim, B. Ghanem, and A. Bibi. Computationally budgeted continual learning: What does matter? In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3698\u20133707, Los Alamitos, CA, USA, jun 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023. 00360. URL https://doi.ieeecomputersociety.org/10. 1109/CVPR52729.2023.00360.\nAmy Rathbone, Simone Stumpf, Caroline Claisse, Elizabeth Sillence, Lynne Coventry, Richard D Brown, and Abigail C Durrant. People with long-term conditions sharing personal health data via digital health technologies: A scoping review to inform design. PLOS Digit. Health, 2(5):e0000264, May 2023.\nSadiq Sani, Stewart Massie, Nirmalie Wiratunga, and Kay Cooper. Learning deep and shallow features for human activity recognition. In Gang Li, Yong Ge, Zili Zhang, Zhi Jin, and Michael Blumenstein, editors, Knowledge Science, Engineering and Management, pages 469\u2013 482, Cham, 2017. Springer International Publishing. ISBN 978-3-319-63558-3.\nhilip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven. Introducing wesad, a multimodal dataset for wearable stress and affect detection. In Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI \u201918, page 400\u2013408, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356923. doi: 10.1145/3242969.3242985. URL https://doi.org/10.1145/3242969.3242985.\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2015. doi: 10.1109/cvpr.2015.7298682. URL http: //dx.doi.org/10.1109/CVPR.2015.7298682.\nJuliane R Sempionatto, Victor Ruiz-Valdepenas Montiel, Eva Vargas, Hazhir Teymourian, and Joseph Wang. Wearable and mobile sensors for personalized nutrition. ACS sensors, 6(5):1745\u20131760, 2021.\nQiang Shen, Haotian Feng, Rui Song, Stefano Teso, Fausto Giunchiglia, and Hao Xu. Federated multi-task attention for cross-individual human activity recognition. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 3423\u20133429. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/475. URL https://doi.org/10.24963/ijcai.2022/475. Main Track.\nYuge Shi, Jeffrey Seely, Philip H. S. Torr, N. Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. 2021.\nLeslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464\u2013472. IEEE, 2017.\nGabriela M Stegmann, Shira Hahn, Julie Liss, Jeremy Shefner, Seward Rutkove, Kerisa Shelton, Cayla Jessica Duncan, and Visar Berisha. Early detection and tracking of bulbar changes in ALS via frequent and remote speech analysis. NPJ Digit. Med., 3(1):132, October 2020.\nAllan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kj\u00e6rgaard, Anind Dey, Tobias Sonne, and Mads M\u00f8ller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In Proceedings of the 13th ACM conference on embedded networked sensor systems, pages 127\u2013140, 2015.\n. Tang, L. Qendro, D. Spathis, F. Kawsar, C. Mascolo, and A. Mathur. Kaizen: Practical selfsupervised continual learning with continual finetuning. In 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2829\u2013 2838, Los Alamitos, CA, USA, jan 2024a. IEEE Computer Society. doi: 10.1109/WACV57701.2024. 00282. URL https://doi.ieeecomputersociety.org/10. 1109/WACV57701.2024.00282.\nChi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Akhil Mathur, and Cecilia Mascolo. Balancing continual learning and fine-tuning for human activity recognition. ArXiv, abs/2401.02255, 2024b. URL https://api.semanticscholar.org/CorpusID:266755926.\nSiri Team. Hey siri: An on-device dnn-powered voice trigger for apple\u2019s personal assistant. https:// machinelearning.apple.com/research/hey-siri, 2017.\nYunus Emre Ustev, Ozlem Durmaz Incel, and Cem Ersoy. User, device and orientation independent human activity recognition on mobile phones: Challenges and a proposal. In Proceedings of the 2013 ACM conference on Pervasive and ubiquitous computing adjunct publication, pages 1427\u20131436, 2013.\nYonatan Vaizman, Katherine Ellis, and Gert Lanckriet. Recognizing detailed human context in the wild from smartphones and smartwatches. IEEE Pervasive Computing, 16(4):62\u201374, 2017. doi: 10.1109/MPRV.2017. 3971131.\n# C. Varma and Puja Prasad. Supervised and unsupervised machine learning approaches\u2014a survey, 02 2023.\nChan Wang, Tianyiyi He, Hong Zhou, Zixuan Zhang, and Chengkuo Lee. Artificial intelligence enhanced sensors - enabling technologies to next-generation healthcare and biomedical platform. Bioelectron. Med., 9(1):17, August 2023.\nQin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation, 2022a.\nQin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of Conference on Computer Vision and Pattern Recognition, 2022b.\nZhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural networks: A strong baseline. In 2017 International joint conference on neural networks (IJCNN), pages 1578\u2013 1585. IEEE, 2017.\nZhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural networks: A strong baseline. In 2017 International joint conference on neural networks (IJCNN), pages 1578\u2013 1585. IEEE, 2017.\nYanan Wu, Zhixiang Chi, Yang Wang, Konstantinos N. Plataniotis, and Songhe Feng. Test-time domain adaptation by learning domain-aware batch normalization, 2024.\nYi Xiao, Harshit Sharma, Zhongyang Zhang, Dessa Bergen-Cico, Tauhidur Rahman, and Asif Salekin. Reading between the heat: Co-teaching body thermal signatures for non-intrusive stress detection. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 7 (4), jan 2024. doi: 10.1145/3631441. URL https: //doi.org/10.1145/3631441.\nJianfei Yang, Xinyan Chen, Dazhuo Wang, Han Zou, Chris Xiaoxuan Lu, Sumei Sun, and Lihua Xie. Sensefi: A library and benchmark on deep-learningempowered wifi human sensing, 2023.\nShuochao Yao, Shaohan Hu, Yiran Zhao, Aston Zhang, and Tarek Abdelzaher. Deepsense: A unified deep learning framework for time-series mobile sensing data processing. In Proceedings of the 26th International Conference on World Wide Web, WWW \u201917, page 351\u2013360, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee. ISBN 9781450349130. doi: 10.1145/3038912.3052577. URL https://doi.org/ 10.1145/3038912.3052577.\nBen Zandonati, Adrian Alan Pol, Maurizio Pierini, Olya Sirkin, and Tal Kopetz. Fit: A metric for model sensitivity. arXiv preprint arXiv:2210.08502, 2022.\nBen Zandonati, Adrian Alan Pol, Maurizio Pierini, Olya Sirkin, and Tal Kopetz. Fit: A metric for model sensitivity. arXiv preprint arXiv:2210.08502, 2022.\nY. Zhang, Y. Zheng, K. Qian, G. Zhang, Y. Liu, C. Wu, and Z. Yang. Widar3.0: Zero-effort cross-domain gesture recognition with wi-fi. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):8671\u2013 8688, nov 2022. ISSN 1939-3539. doi: 10.1109/ TPAMI.2021.3105387.\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression, 2017.\nao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li. Neuron-level structured pruning using polarization regularizer. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9865\u20139877. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/ file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf.\n# Experiment Details\nThis section describes the details of the empirical analysi done for the presented approach.\n# Datasets and models\nFor the evaluations we used four datasets: PERCEPT-R [Benway et al., 2023], WIDAR [Zhang et al., 2022], ExtraSensory [Vaizman et al., 2017] and a Stress-sensing dataset [Xiao et al., 2024]. PERCEPT-R, WIDAR and Stress-sensing dataset are collected in a controlled lab setting whereas ExtraSensory is collected in a real-world setting.\nPERCEPT-R: The sound / r / has been recognized as the most frequently impacted sound in residual speech sound disorders in American English Lewis et al. [2015] and considered to be the most difficult sound to treat. The PERCEPT-R Corpus was collected during 34 different cross-sectional and longitudinal studies of speech Benway et al. [2023] for automated speech analysis of / r /. The data used in this study come from the prospectively collected Benway and Preston [2023], and corpus version\n2.2.2, which includes both the publicly available open access subset (2.2.2p) and privately held data that was not published in the open access subset after a review of consent/assent permissions. Items in the PERCEPT-R Corpus v2.2.2 primarily consist of single-word citation speech audio collected during clinical trials involving children with speech sound disorders affecting / r /, along with agematched peers with typical speech. The full corpus contains 179,076 labeled utterances representing 662 singlerhotic words and phrases. Each audio file is paired with a ground-truth label representing listener judgments of rhoticity, derived by averaging binary ratings (0 = derhotic, 1 = fully rhotic) from multiple listeners. For this study, the heuristic threshold for converting these averaged ratings into binary ground-truth labels was 0.66. The context is defined by the treatment phase, that is, whether the treatment has started or not. In line with state of the art Benway et al. [2023], we tried several model architectures such as CNN, DNN, BILSTM etc whose number of parameters were identified using grid search. Among those, the biLSTM model containing 4 bidirectional LSTM layers followed by 5 linear layers, which are accompanied by a Hardswish activation layer, was identified as the one exhibit best results for the generic data and was used for this study.\nWIDAR: WIDAR is a dataset collected for the purpose of gesture recognition. It was collected using off-the-shelf WiFi links (one transmitter and at least 3 receivers). 17 users performed 15 different gestures at 15 different locations in 3 rooms for 5 different orientations (of the person). The channel state information is collected from these devices with amplitude noises and phase offsets removed as a preprocessing step. The two contexts used for the current work are decided based on the orientations of the torso data and room ID. Room 1 is a classroom with a number of objects (e.g., desks and chairs) in it, and Room 2 is a nearly empty hallway. The dissimilar data distributions can be attributed to the differences in the amount of interruptions in WiFi signals. We followed the same normalization methods as Yang et al. [2023]. The model used for WIDAR follows the LeNet architecture [Zhang et al., 2022], which contains three 2D convolutional layers followed by two linear layers. Each of these layers, except the final classification layer, is fol-\nlowed by a ReLU activation layer.\nExtraSensory: ExtraSensory is a human activity recognition dataset collected using the ExtraSensory mobile application. A number of features were collected from different cellular devices and smart watches, though we just used the accelerometer and gyroscope features obtained from the cellular devices. Labels for activities were selfreported by the users through the mobile application. For our evaluations on ExtraSensory, 5 users were left out for training a single generic model. The contexts are decided based on the location of the phone: hand, pocket, and bag. The model follows a CNN-GRU-based architecture used in HAR literature [Gong et al., 2019, Hao et al., 2022, Shen et al., 2022, Yao et al., 2017]. The model consists of three batch-normalized 1D convolution layers followed by a linear layer that feeds into a batch-normalized recursive (GRU) layer and two linear layers to generate embeddings. For the classification head, two linear layers were used.\nStress Sensing Dataset: This dataset measures the physiological impacts of various kinds of Stress. The dataset is collected using Empatica E4 Wristband to extract features such as EDA (Electrodermal Activity), a skin temperature sensor (4 Hz), etc, contributing to a total of 34 features. The data is collected from 30 participants having different demographics and were assigned the labels as \u2018Stressed\u2019 or \u2018Calm\u2019 based in the current physiological features values. The context is defined by combination of hand on which the wristband was worn and whether or not the person was moving during the data collection. The model uses a simple multi-layer-perceptron (MLP) architecture Eren and Navruz [2022], Dzie\u02d9zyc et al. [2020], Wang et al. [2017] consisting of 3 linear layers with hidden size of 128.\n# Training of the Generic models\nWIDAR: We chose users 0,1 and 2 for personalization since these were the only users whose data was collected in both rooms. Since the number of users in WIDAR is the very small, the exclusion of all the 3 users for training the generic model would have resulted in substandard\nmodels. So, For each user, we generated different generic models by using data from the other 16 users with a 14/2 person disjoint random split for the train and validation set. Our classification target was the 6 gesture classes: 0,1,2,3,5 and 8.\nExtraSensory: We chose users 61, 7C, 80, 9D, and B7 for personalization, and the generic model is trained on 42 users, with 10 users being left out to validate. The two target classes are walking and sitting.\nStress Sensing Dataset: This is a binary classification problem where we chose users 1, 2, and 3 for personalization as these users contributed data in all possible contexts. For generic model training, 6 other users were used to create person disjoint validation and test sets, and the remaining 21 users were used to train the generic model.\nStress Sensing Dataset: This is a binary classification problem where we chose users 1, 2, and 3 for personalization as these users contributed data in all possible contexts. For generic model training, 6 other users were used to create person disjoint validation and test sets, and the remaining 21 users were used to train the generic model. PERCEPT-R : As recommended by clinical experts, we choose 16 participants with ids 17, 25, 28, 336, 344, 361, 362, 55, 586, 587, 589, 590, 591, 61, 67, and 80 for personalization. These participants\u2019 speech data were collected longitudinally, meaning their data could be separated into available and unseen contexts versus other speakers in the corpus who only had speech data available from one time-point. The generic model is trained for the remaining 499 participants using person-disjoint validation and test sets. The aim of this dataset is to identify the correctness of / r / sounds.\nPERCEPT-R : As recommended by clinical experts, we choose 16 participants with ids 17, 25, 28, 336, 344, 361, 362, 55, 586, 587, 589, 590, 591, 61, 67, and 80 for personalization. These participants\u2019 speech data were collected longitudinally, meaning their data could be separated into available and unseen contexts versus other speakers in the corpus who only had speech data available from one time-point. The generic model is trained for the remaining 499 participants using person-disjoint validation and test sets. The aim of this dataset is to identify the correctness of / r / sounds.\n# Metrics for classification accuracy evaluation\nWe use accuracy to measure the performance of a model. However, the computation of this metric differs for the four datasets. The details of the metrics used for all the datasets are as follows:\n1. WIDAR: We use a 6-class classification for gesture recognition, and the distribution of the data among these classes is nearly balanced. Thus, standard classification accuracy has been used for WIDAR.\n2. ExtraSensory: The subset of the Extrasensory dataset used for this work aims for a binary classification for activity recognition. We observed that the data distribution was quite imbalanced among the two classes,\nHyperparameter\nPERCEPT\nWIDAR\nExtraSensory\nStress-sensing\nBase Learning Rate\n1e-5\n1e-07\n1.2e-08\n5e-5\nMax Learning Rate\n1e-5\n5e-06\n7.5e-07\n5e-5\nEpochs\n300\n1000\n150\n1000\n<div style=\"text-align: center;\">Table 4: Hyperparameters for generic Models</div>\n# Table 4: Hyperparameters for generic Models\nand therefore, balanced classification accuracy was used for this dataset. Balanced accuracy is computed as the average of true positive rate and true negative rate.\n3. Stress Sensing Dataset: For this binary classification problem, F1 score has been used as a performance metric as suggested by the original authors Xiao et al. [2024].\n3. Stress Sensing Dataset: For this binary classification problem, F1 score has been used as a performance metric as suggested by the original authors Xiao et al. [2024].\n4. PERCEPT-R: For this dataset, Benway et al. [2023] utilized balanced accuracy for the binary classification task, and we employed the same metrics in our study.\nFor simplicity, we use the term \u2018accuracy\u2019 to encompass all the metrics discussed above.\n# Hyperparameters\nThe approach uses several hyperparameters for generic model training and personalization. Table 4 and 5 show the hyperparameter values for generic and personalized model training, respectively. These values correspond to the best results obtained using a grid search. For training the generic model, in addition to the number of epochs, \u2018Base Learning Rate\u2019 and \u2018Max Learning Rate\u2019 (the arguments for CycleLR [Smith, 2017]) are the hyperparameters. For the personalized model, learning rate (fixed), \u03b1, \u03c4, number of epochs for initial finetuning (Initial Epochs), and epochs for final finetuning (Final Epochs) are the hyperparameters. The range of these hyperparameters used for grid search during personalization is also mentioned in Table 5. Additionally, we use k = k\u2032 = 0.05 for the ToleratedPrune module for PERCEPT-R, WIDAR, and ExtraSensory datasets, while for the Stress-sensing dataset, k = 0.05 and k\u2032 = 0.01 is being used. One may find different values to be suitable for other datasets and model architectures.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0fb6/0fb6dd84-289f-4fdc-b60d-67602c7c4ac0.png\" style=\"width: 50%;\"></div>\nHyperparameter\nRange\nPERCEPT-R\nWIDAR\nExtraSensory\nStress Sensing\nLearning Rate\n1e-6 - 1e-1\n1e-5\n1e-6\n1e-6\n1e-5\nalpha\n1e-6 - 10\n0.01\n0.0001\n0.5\n0.0001\n\u03c4\n0.01 - 0.25\n0.05\n0.2\n0.01\n0.01\nInitial Epochs\n100 -1000\n300\n600\n600\n1000\nFinal Epochs\n100 - 1000\n300\n600\n1000\n1000\n<div style=\"text-align: center;\">Table 5: Hyperparameters for Personalized Models</div>\n# Code\nThe code is provided in supplementary material arranged into dataset-specific folders. Each folder contains the pretrained generic model, all the required modules, and the instructions to run the code. The seed values used for the evaluations are also provided in the shell files. The data partitioned into personalized and context-wise sets will be released upon publication.\n# Compute Resources\nAll the computations have been performed on NVIDIA Quadro RTX 5000.\n# All the computations have been performed on NVIDIA Quadro RTX 5000.\n# Detailed Results\n# Comparison with Generic Models\nThe personalized models obtained using CRoP exhibit higher classification accuracy than the generic models on the available context\u2019s data Da i , showcasing the benefits of personalization. To demonstrate the existence of such improvement, Tables 6a- 6h and Table 9a compare the performance of generic model MG \u03b8 and personalized models obtained using CRoP MP a i \u03b8 .\nWIDAR: Tables 6a and 6b show that there is an average improvement of 25.25 and 11.88 percent points among three users for the available context Ca for Scenario 1 and Scenario 2, respectively. However, this benefit comes at the cost of a reduction in accuracy for the unseen context. There is an average reduction of 16.69 and 5.97 percent points for Scenario 1 and Scenario 2, respectively, for the unseen context Cu. Notably, the loss of accuracy in the unseen context is much lower as compared to the\nconventionally-finetune model as discussed in the Motivation Section.\nExtraSensory: Similar patterns could be observed for the Extrasensory dataset. Tables 6c and 6d show that there is an average increment of 16.40 and 18.37 percent points for the available context for Scenario 1 and Scenario 2, respectively. Interestingly, the performance of the personalized model for Scenario 1 on unseen context Cu was not adversely impacted. This is attributed to the fact that the inertial sensing patterns of Bag and Pocket phone carrying modes capture the user\u2019s body movement, whereas the phone-in-hand movement patterns can be distinct. In Scenario 1, Ca comprises pocket and Cu comprises bag, meaning both available and unseen contexts encompass similar inertial patterns, leading to advantageous performance even in the unseen context. This evaluation illustrates minimal intra-user generalizability loss on unseen contexts when both available and unseen contexts share similar user traits. However, in Scenario 2, where only the hand belongs to the unseen context Cu, there is an average loss of 5.02 percentage points on the unseen context.\nStress Sensing: The physiological features used in this dataset vary significantly from one user to other. Thus, Tables 6e-6h show that the generic models do not perform well on personalized data. Personalized finetuning enables the model to learn person-specific patterns, allowing the model\u2019s performance to improve not only in the available context but also in the unseen context. This results in average personalization benefit (\u2206P ) of 67.81 and 85.25 for Scenario 1 and Scenario 2, respectively. It is important to note that for each Scenario, only one model is trained for the available context and tested for two different unseen contexts. Moreover, double context change (Tables 6g and 6h) shows lower personalization benefit as\nPERCEPT-R: In this dataset, the heterogeneity of features among individuals is reflected through the difference in prediction accuracy of the generic model. It can be observed in Table 9a that for some individuals, the generic model exhibits over 90% accuracy on the available context data, while for others, the generic model\u2019s accuracy drops to around 60%. This results in significant variability over gains in available and unseen contexts. Overall, CRoP yields an average personalization gain of 5.09%. On average over all the datasets, a personalization benefit (\u2206P ) of 35.23 percent points are seen as compared to the generic models across the four datasets under both scenarios. These evaluations establish that the personalized models obtained using CRoP demonstrate improved performance over the available context data than the generic models and exhibit personalization.\n# Comparison with Personalized Models\nThe personalized models obtained using CRoP (MP a i \u03b8 ) are expected to have higher accuracy on unseen context Cu than the conventionally-finetune personalized models (MCa i \u03b8 ) as discussed in the Motivation Section. This section assesses whether the results align with these expectations.\nWIDAR: Tables 8a and 8b demonstrate that the personalized models MP a i \u03b8 exhibit an average increment of 8.01 and 2.85 percent points in the unseen context for Scenario 1 and Scenario 2, respectively. However, an average loss of 1.57 and an average gain of 1.44 percent points in C\u2032 as accuracy could be observed for Scenario 1 and Scenario 2, respectively.\nExtrasensory: Similar patterns could be observed for the ExtraSensory dataset where the average accuracy on the unseen context improved by 4.97 and 12.61 percentage points for Scenario 1 and Scenario 2 as shown in Tables 6c and 8d, respectively. As expected, there is a loss of 5.43 and 4.44 percent points in the available contexts for Scenario 1 and Scenario 2, respectively.\nStress Sensing: As observed in Tables 6e-6h, personalized finetuning improves models performance on unseen context as well, we can claim that there is some person-specific traits which are common in available and unseen context. While comparing our final models with conventionally-finetuned models (Tables 8e-8h), performance boost in both available and unseen context could be observed. This can be attributed to the generalization improvement benefits of model pruning [Jin et al., 2022]. This results in average generalization benefit (\u2206G) of 13.81 and 13.08 for Scenario 1 and Scenario 2, respectively, for single context change. Similar personalization benefits could be seen for double context change.\nPERCEPT-R: As observed in Table 9b, the variability in generalization benefits among different individuals is less pronounced as compared to personalization benefits. On average, CRoP introduces a generalization benefit of 2.57%. On average over all the datasets, a generalization benefit (\u2206G) of 7.78% percent points are seen over the conventionally-finetuned personalized models across all datasets under both scenarios.\n# Individual Analysis for PERCEPT-R dataset\nThe heterogeneity of data in PERCEPT-R dataset resulted in variability in \u2206P among various participants. In order to investigate that further, we conducted fisher information matrix (FIM) analysis of the generic modelMG \u03b8 , conventionally-finetuned model MCa i \u03b8 and the final models obtained using CRoP MP a i \u03b8 for the data belonging to available Ca and unseen contexts Cu. These results can be found in Table 7. Previous works have explored the fisher information matrix (FIM) as a means to investigate the curvature properties of the loss landscape Zandonati et al. [2022], Martens [2020] and its relationship to model generalizability Jastrzebski et al. [2021]. The fisher information trace serves as a metric to study the loss landscape curvature sensitivity Jastrzebski et al. [2021], Zandonati et al. [2022] i.e., a larger trace coincides with a sharper loss landscape minima signifying higher sensitivity and poorer generalization performance Jastrzebski et al. [2021], Kim et al. [2022].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4078/4078810c-dedc-4559-bd1e-fdfdf8ced4e3.png\" style=\"width: 50%;\"></div>\nModel\nMG\n\u03b8\nMP a\ni\n\u03b8\nA(MP a\ni\n\u03b8 , C) \u2212A(MG\n\u03b8 , C)\nUser\nCa\nCu\nCa\nCu\nCa\nCu\n0\n63.90\n77.09\n83.67\n69.53\n+19.77\n-7.56\n1\n61.80\n79.78\n86.41\n54.45\n+24.61\n-25.33\n2\n45.63\n79.81\n77.02\n62.63\n+31.38\n-17.18\nAverage\n+25.25\n-16.69\n\u2206P\n+8.55\nModel\nMG\n\u03b8\nMP a\ni\n\u03b8\nA(MP a\ni\n\u03b8 , C) \u2212A(MG\n\u03b8 , C)\nUser\nCa\nCu\nCa\nCu\nCa\nCu\n61\n78.69\n69.83\n82.59\n69.66\n+3.9\n-0.17\n7C\n78.91\n76.41\n88.00\n71.63\n+9.09\n-4.78\n80\n55.84\n26.24\n82.36\n38.87\n+26.52\n+12.63\n9D\n73.74\n85.63\n82.81\n84.72\n+9.07\n-0.91\nB7\n56.06\n88.33\n89.50\n86.97\n+33.44\n-1.36\nAverage\n+16.40\n+1.08\n\u2206P\n+17.49\n<div style=\"text-align: center;\">(c) Scenario 1 for ExtraSensory dataset</div>\nModel\nMG\n\u03b8\nMP a\ni\n\u03b8\nA(MP a\ni\n\u03b8 , C) \u2212A(MG\n\u03b8 , C)\nUser\nCa\nCu\nCa\nCu\nCa\nCu\n1\n88.39\n81.90\n94.54\n97.59\n+6.15\n+15.69\n2\n47.40\n50.0\n77.12\n90.47\n+29.72\n+40.47\n3\n36.90\n43.48\n96.36\n95.31\n+59.46\n+51.93\nAverage\n+31.78\n+36.03\n\u2206P\n+67.81\n(e) Scenario 1 for Stress Sensing - single context change\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bdaa/bdaaa176-d9ed-4c19-9440-50f0c26a8f77.png\" style=\"width: 50%;\"></div>\nModel\nMG\n\u03b8\nMP a\ni\n\u03b8\nA(MP a\ni\n\u03b8 , C) \u2212A(MG\n\u03b8 , C)\nUser\nCa\nCu\nCa\nCu\nCa\nCu\n1\n88.39\n64.71\n94.54\n76.46\n+6.15\n+11.75\n2\n47.40\n50.70\n77.12\n63.22\n+29.72\n+12.52\n3\n36.90\n11.94\n96.36\n55.48\n+59.46\n+43.54\nAverage\n+31.78\n+22.60\n\u2206P\n+54.38\n(g) Scenario 1 for Stress Sensing - double context change\n<div style=\"text-align: center;\">(g) Scenario 1 for Stress Sensing - double context change</div>\n<div style=\"text-align: center;\">Table 6: Detailed Personalization (\u2206P ) results for WIDAR, ExtraSensory and Stress Sensing dataset</div>\nIt is observed that the 9 participants who experience greater \u2206P benefits using CRoP also exhibit a reduction in the FIM trace for MP a i \u03b8 compared to MCa i \u03b8 across both contexts. In contrast, the other 7 participants with lower \u2206P benefits show an increase in the FIM trace in both contexts. While there appears to be a correlation between \u2206P benefits and changes in the FIM trace, the challenge lies in the fact that this information is not available during the personalization phase. Any decision regarding the necessity of CRoP for an individual must be based solely on the available context data Ca and the generic model MG \u03b8 , as these are the only sources of information accessible during personalization.\nModel\nMG\n\u03b8\nMP a\ni\n\u03b8\nA(MP a\ni\n\u03b8 , C) \u2212A(MG\n\u03b8 , C)\nUser\nCa\nCu\nCa\nCu\nCa\nCu\n0\n73.28\n61.80\n82.59\n58.38\n+9.31\n-2.43\n1\n73.18\n59.58\n92.44\n47.90\n+19.27\n-11.67\n2\n80.45\n46.13\n87.5\n42.31\n+7.04\n-3.81\nAverage\n+11.88\n-5.97\n\u2206P\n+5.90\nModel\nMG\n\u03b8\nMP a\ni\n\u03b8\nA(MP a\ni\n\u03b8 , C) \u2212A(MG\n\u03b8 , C)\nUser\nCa\nCu\nCa\nCu\nCa\nCu\n61\n76.43\n80.00\n87.24\n73.44\n+10.81\n-6.56\n7C\n75.07\n92.32\n83.18\n89.39\n+8.11\n-2.93\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of personalization in human sensing applications, highlighting the limitations of existing methods that overlook intra-user variability across contexts, which is critical in clinical applications where data scarcity is prevalent.",
        "problem": {
            "definition": "The problem is to create a personalized model for a user that effectively generalizes across unseen contexts using limited data from available contexts.",
            "key obstacle": "The main challenge is that traditional personalization methods often degrade performance in unseen contexts due to reliance on limited context data during training."
        },
        "idea": {
            "intuition": "The idea emerged from the observation that generic models lose valuable information when fine-tuned on specific contexts, leading to poor performance in other contexts.",
            "opinion": "The proposed idea, CRoP, integrates model pruning with static personalization to enhance generalization while using off-the-shelf models.",
            "innovation": "CRoP innovatively combines adaptive pruning techniques with static personalization, allowing for robust performance across varying contexts without accessing unseen data."
        },
        "method": {
            "method name": "CRoP",
            "method abbreviation": "CRoP",
            "method definition": "CRoP is a static personalization approach that uses model pruning to optimize the balance between personalization and generalization in human sensing applications.",
            "method description": "CRoP personalizes a generic model by pruning and adapting its weights to enhance performance across different user contexts.",
            "method steps": [
                "Fine-tune the generic model on available context data.",
                "Prune redundant parameters to create a pruned model.",
                "Replace pruned parameters with corresponding weights from the generic model.",
                "Fine-tune the mixed model on the available context data."
            ],
            "principle": "The method's effectiveness lies in retaining essential parameters while integrating generic knowledge to improve robustness and performance across unseen contexts."
        },
        "experiments": {
            "evaluation setting": "Four human sensing datasets were utilized: PERCEPT-R for speech therapy, WIDAR for gesture recognition, ExtraSensory for activity recognition, and a stress-sensing dataset using physiological data.",
            "evaluation method": "The performance of CRoP was assessed by comparing the accuracy of the personalized models against generic models and conventionally fine-tuned models across both available and unseen contexts."
        },
        "conclusion": "CRoP demonstrates significant improvements in personalization and generalization, achieving an average increase of 35.23% in personalization and 7.78% in generalization compared to conventional methods, while effectively using off-the-shelf models.",
        "discussion": {
            "advantage": "CRoP stands out due to its ability to maintain performance across different contexts without requiring extensive user data during the personalization phase.",
            "limitation": "The method's reliance on a pre-trained generic model means its effectiveness is contingent on the quality of that model, and the current study does not explore inter-user generalizability.",
            "future work": "Future research should investigate the impact of different pruning strategies and explore the potential for broader applicability across diverse datasets."
        },
        "other info": {
            "code availability": "The code and datasets will be made publicly available upon publication.",
            "compute resources": "All experiments were conducted using NVIDIA Quadro RTX 5000."
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "The paper defines CRoP as a static personalization approach that uses model pruning to optimize the balance between personalization and generalization in human sensing applications."
        },
        {
            "section number": "3.2",
            "key information": "CRoP demonstrates significant improvements in personalization and generalization, achieving an average increase of 35.23% in personalization and 7.78% in generalization compared to conventional methods."
        },
        {
            "section number": "3.3",
            "key information": "The main challenge identified in the paper is that traditional personalization methods often degrade performance in unseen contexts due to reliance on limited context data during training."
        },
        {
            "section number": "3.4",
            "key information": "The proposed idea, CRoP, integrates model pruning with static personalization, allowing for robust performance across varying contexts without accessing unseen data."
        },
        {
            "section number": "5.2",
            "key information": "The evaluation of CRoP involved four human sensing datasets: PERCEPT-R for speech therapy, WIDAR for gesture recognition, ExtraSensory for activity recognition, and a stress-sensing dataset using physiological data."
        },
        {
            "section number": "8.4",
            "key information": "Future research should investigate the impact of different pruning strategies and explore the potential for broader applicability across diverse datasets."
        }
    ],
    "similarity_score": 0.602792568028755,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0858_fine-/papers/CRoP_ Context-wise Robust Static Human-Sensing Personalization.json"
}