{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1612.06052",
    "title": "Quantization and Training of Low Bit-Width Convolutional Neural Networks for Object Detection",
    "abstract": "We present LBW-Net, an efficient optimization based method for quantization and training of the low bit-width convolutional neural networks (CNNs). Specifically, we quantize the weights to zero or powers of two by minimizing the Euclidean distance between full-precision weights and quantized weights during backpropagation. We characterize the combinatorial nature of the low bit-width quantization problem. For 2-bit (ternary) CNNs, the quantization of $N$ weights can be done by an exact formula in $O(N\\log N)$ complexity. When the bit-width is three and above, we further propose a semi-analytical thresholding scheme with a single free parameter for quantization that is computationally inexpensive. The free parameter is further determined by network retraining and object detection tests. LBW-Net has several desirable advantages over full-precision CNNs, including considerable memory savings, energy efficiency, and faster deployment. Our experiments on PASCAL VOC dataset show that compared with its 32-bit floating-point counterpart, the performance of the 6-bit LBW-Net is nearly lossless in the object detection tasks, and can even do better in some real world visual scenes, while empirically enjoying more than 4$\\times$ faster deployment.",
    "bib_name": "yin2017quantizationtraininglowbitwidth",
    "md_text": "# Quantization and Training of Low Bit-Width Convolution Neural Networks for Object Detection\nPenghang Yin1, Shuai Zhang\u22172, Yingyong Qi2, and Jack Xin2\n1Department of Mathematics, University of California, Los Angeles. Email: yph@ucla.edu. 2Department of Mathematics, University of California, Irvine. Email: (szhang3, jack.xin, yqi)@uci.edu\nAbstract\nWe present LBW-Net, an efficient optimization based method for quantization and training of the low bit-width convolutional neural networks (CNNs). Specifically, we quantize the weights to zero or powers of two by minimizing the Euclidean distance between full-precision weights and quantized weights during backpropagation. We characterize the combinatorial nature of the low bit-width quantization problem. For 2-bit (ternary) CNNs, the quantization of N weights can be done by an exact formula in O(N log N) complexity. When the bit-width is three and above, we further propose a semi-analytical thresholding scheme with a single free parameter for quantization that is computationally inexpensive. The free parameter is further determined by network retraining and object detection tests. LBW-Net has several desirable advantages over full-precision CNNs, including considerable memory savings, energy efficiency, and faster deployment. Our experiments on PASCAL VOC dataset show that compared with its 32-bit floating-point counterpart, the performance of the 6-bit LBW-Net is nearly lossless in the object detection tasks, and can even do better in some real world visual scenes, while empirically enjoying more than 4\u00d7 faster deployment.\n# 1 Introduction\nDeep convolutional neural networks (CNNs) have demonstrated superior performance in various computer vision tasks [3, 13, 14, 15, 16, 18, 21, 23, 24]. However deep CNNs typically have hundreds of millions of trainable parameters which easily take up hundreds of megabytes of memory, and billions of FLOPs for a single inference. This poses a significant challenge for the deployment of deep CNNs on small devices with limited memory storage and computing power such as mobile phones. To address this issue, recent efforts have been made to compress the model size and train neural networks with heavily quantized weights, activations, and gradients [1, 2, 6, 7, 9, 17, 20, 22, 25, 26, 27], which demand less storage and fewer FLOPs for deployment. These models include BinaryConnect[1], BinaryNet[2], XNOR-Net [20], TWN [17], TTQ [27], DoReFa-Net[26] and QNN [9], to name a few. In particular, binary (1-bit) and ternary (2-bit) weight models not only enable high model compression rate, but also eliminate the need of most floating-point multiplications during forward and backward propagations, which shows promise to resolve the problem. Compared with binary models, ternary weight networks such as TWN strike a better balance between compression rate and accuracy. It has been shown that ternary weight CNNs [17] can achieve nearly lossless accuracy on MNIST[16] and CIFAR-10 [12] benchmark datasets. Yet with fully ternarized weights, there is still noticeable drop in performance on larger datasets like ImageNet [4], which suggests the necessity of relatively wider bit-width models with stronger model capacity for challenging tasks. An incremental network quantization strategy (INQ) is proposed in [25] for converting pretrained full-precision CNNs into low bit-width versions whose weights are either zero or powers of two. A b bit-width model can have 2b\u22121 + 1 distinct candidate values, in which 2 bits are used for representing the zero and the signs, while the remaining b \u22122 bits for the powers. More precisely, the parameters are constrained to 2s \u00d7 {0, \u00b121\u22122b\u22122, \u00b122\u22122b\u22122, . . . , \u00b11} associated with a layerwise scaling factor 2s, s an integer depending only on the weight maximum in the layer. At inference\n\u2217Equal contribution.\ntime, the original floating-point multiplication operations can be replaced by faster and cheaper binary bit shifting. The quantization scheme of [25] is however heuristic. In this paper, we present the exact solution of the general b-bit approximation problem of a real weight vector W f in the least squares sense. If b = 2 and the dimension of W f is N, the computational complexity of the 2 bit solution is O(N log N). At b \u22653, the combinatorial nature of the solution renders direct computation too expensive for large scale tasks. We shall develop a semi-analytical quantization scheme involving a single adjustable parameter \u00b5 to set up the quantization levels. The exponent s in the scaling factor can be calculated analytically from \u00b5 and the numbers of the downward sorted weight components between quantization levels. If the weight vector comes from a Gaussian ensemble, the parameter \u00b5 can be estimated analytically. However, we found that the weight vectors in CNNs (in particular ResNet) are strongly non-Gaussian. In this paper, \u00b5 is determined based on the object detection performance after retraining the network. This seems to be a natural choice in general as quantization is often part of a larger computer vision problem as is here. Therefore, the optimal parameter \u00b5 should not be decided by approximation (the least squares problem) errors alone. Indeed, we found that at b \u22654, \u00b5 = 3 4\u2225W f\u2225\u221egives the best detection performance, which suggests that a percentage of the large weights plays a key role in representing the image features and should be encoded during quantization. Network retraining is necessary after quantization as a way for the system to adjust and absorb the resulting errors. Besides warm start, INQ [25] requires a careful layerwise partitioning and grouping of the weights which are then quantized and re-trained incrementally group by group rather than having all weights updated at once. Due to both classification and detection networks involved in this work, we opted for a simpler retraining method, a variant of the projected stochastic gradient descent (SGD) method (see [17] and references therein). As a result, our LBW-Net can be trained either from scratch or a partial warm start. During each iteration, besides forward and backward propagations, only an additional low cost thresholding (projection) step is needed to quantize the full-precision parameters to zero or powers of two. We train LBW-Net with randomly initialized weights in the detection network (R-FCN in [3]), and pre-trained weights in ResNet [8]. We conduct object detection experiments on PASCAL VOC data sets [5] as in [3, 21]. We found that at bit-width b = 6, the accuracies of the quantized networks are well within 1% of those of their 32-bit floating-point counterparts on both ResNet-50 and ResNet-101 backbone architectures. In some complex real world visual scenes, the 6-bit network even detects persons missed by the full-precision network. The rest of the paper is organized as follows. In section 2, we construct the exact solution of the general low bit-width approximation problem and present our semi-analytical quantization scheme with a single adjustable parameter \u00b5. We also outline the training algorithm and the choice of \u00b5. In section 3, we describe our experiments, the datasets, the object detection results, the non-Gaussian and sparsity properties of the floating weights in training. In section 4, we conclude with remarks on future work.\n# 2 Training low bit-width convolutional neural networks\n# low bit-width convolutional neural netwo\nWeight quantization at low bit-width\n# 2.1 Weight quantization at low bit-width\nFor general quantization problem, we seek to minimize the Euclidean distance between the given full-precision weight vector W f and quantized weight vector W q, which is formulated as the following optimization problem:\nmin W q \u2225W q \u2212W f\u22252 subject to W q \u2208Q,\nwhere Q is the set of quantized states. To quantize the full-precision weights into low-precision ones of b bits (b \u22652), we constrain the quantized weights to the value set of 2s \u00d7{0, \u00b121\u2212n, \u00b122\u2212n, . . . , \u00b11} for some integer s \u2208Z, where n = 2b\u22122 and 2s serves as the scaling factor. The minimal distance problem becomes:\n(s\u2217, Q\u2217) = arg min s\u2208Z,Q \u22252sQ \u2212W f\u22252 subject to Qi \u2208{0, \u00b121\u2212n, . . . , \u00b11}.\n(s\u2217, Q\u2217) = arg min s\u2208Z,Q \u22252sQ \u2212W f\u22252 subject to Qi \u2208{0, \u00b121\u2212n, . . . , \u00b11}.\n  Then the optimal quantized weight vector is given by 2s\u2217Q\u2217. A precise characterization of (1) is as follows.\nThen the optimal quantized weight vector is given by 2s\u2217Q\u2217. A precise characterization of (1) is as follows.\n(1)\nTheorem 1. Let b \u22652, n = 2b\u22122, and k0, . . . , kn\u22121 \u2208N. Suppose that W f [k0] keeps the k0 largest components in magnitude of W f and zeros out the other components; W f [k1] extracts the next k1 largest components and zeros out the other components, and so on. The solution Q\u2217to (1) is:\nwhere\n(k\u2217 0, . . . , k\u2217 n\u22121) = arg min k0,...,kn\u22121\u2208N g \ufffdn\u22121 \ufffd t=0 \u2225W f [kt]\u222512\u2212t, n\u22121 \ufffd i=0 kt2\u22122t \ufffd\nwith\nThe bracket in g(u, v) is the floor operation. Moreover, the optima\nis the floor operation. Moreover, the optimal power of scaling is:\n \ufffd In Theorem 1, we have assumed that the components of W f have no ties in magnitudes, as such situation occurs with zero probability for random floating vectors from continuous distributions. To solve the problem (1) by Theorem 1, we need to sort the elements of W f in magnitude, and find the optimal numbers of weights k\u2217 0, . . . , k\u2217 n\u22121 at n quantization levels by solving (2). We can then obtain the optimal scaling factor 2s\u2217. The largest k\u2217 0 weights (in magnitude) are quantized to \u00b12s\u2217, and the next largest k\u2217 1 weights to \u00b12s\u2217\u22121, and so on. Finally, all the remaining small weights are pruned to 0. The subproblem (2) is intrinsically combinatorial however. In the simplest case b = 2 with ternary weight networks, by Theorem 1, k\u2217 0 = arg mink0\u2208N g(\u2225W f [k0]\u22251, k0), and the solution to (1) is given by Q\u2217= sign(W f [k\u2217 0]), s\u2217= \ufffd log2 4 \u2225W f [k\u2217 0 ]\u22251 3 k\u2217 0 \ufffd . Therefore, the weight ternarization mainly involves sorting magnitudes of the elements in W f and computing a cumulative sum of the sorted sequence, which requires a computational complexity of O(N log(N)), where N is number of entries in W f. When b > 2 and n > 1, solving (2) by direct enumeration becomes computationally too expensive for large scale problems such as convolutional neural networks and thus impractical. Hereby we propose a low-cost approximation of Q\u2217, motivated by the empirical quantization schemes in [17, 25]. To this end, by selecting a proper threshold value \u00b5, we set\n\uf8f3 Note that the case t = n \u22121 in (3) needs special treatment because one of the neighboring quantized values is 0. The parameter \u00b5 is the only free parameter in (3). Theorem 2. The optimal power \u02dcs\u2217of the scaling factor with respect to the approximate \u02dcQ\u2217in (3) is\n\uf8f3 Note that the case t = n \u22121 in (3) needs special treatment because one of the neighboring quantized values is 0. The parameter \u00b5 is the only free parameter in (3). Theorem 2. The optimal power \u02dcs\u2217of the scaling factor with respect to the approximate \u02dcQ\u2217in (3)\n  Here W[\u02dck\u2217 t ] is defined as in Theorem 1, and \u02dck\u2217 t is the number of entries of W f in the t-th largest group according to the division of (3).\n(2)\n(4)\nWe remark that the output of \u02dcQ\u2217consists of mostly the scaled signs, hence \u02dcQ\u2217resembles a \u201cphase factor\u201d. On the other hand, the scaling factor 2\u02dcs\u2217is the corresponding amplitude. Putting the two factors together, one can view the low bit-width weight approximation as an approximate polar decomposition of the real weight vector. The proof of Theorem 1 is in the appendix from which Theorem 2 follows.\n# 2.2 Training algorithm\nWe used a projected SGD-like algorithm as in [17, 20] for training LBW-Net. At each gradientdescent step, the minibatch gradient is evaluated at the quantized weights, and a scaled gradient is subtracted from the full-precision weights instead of the quantized weights per standard projected gradient method. The quantization is then done layer by layer by the formulas (3) and (4) with \u00b5 selected as 3 4\u2225W f\u2225\u221efor each layer at bit-width 4 or above. To compute the optimal power s\u2217in (4), we find it sufficient to use the partial sums \ufffd3 t=0 2\u2212t\u2225W f [\u02dck\u2217 t ]\u22251 and \ufffd3 t=0 \u02dck\u2217 t 2\u22122t instead, as the tail values are negligible. In addition, we adopted batch normalization [10], adaptive learning rate, and Nesterov momentum [19] to promote training efficiency.\n# 3 Experiments\nWe implemented our LBW-Net with the R-FCN [3] structure on PASCAL VOC dataset which has 20 object categories. Same as [3] , the training set is the union of VOC 2007 trainval and VOC 2012 trainval (\u201c07+12\u201d), and test results are evaluated on the VOC 2007 test set. So there are in total 16, 551 images with 40, 058 objects in the training set, and 4, 952 images in the test set. The performance of object detection is measured by mean Average Precision (mAP). 1 Our experiments are carried out on Caffe [11] with an Nvidia Titan X GPU under Linux system.\n# 3.1 R-FCN on PASCAL VOC\nWe first employed ResNet-50 as the backbone network architecture for R-FCN. In the experiments, we tested 4, 5, 6-bit LBW-Net and compared evaluation results with the corresponding 32-bit floating point models. For fair comparison, all these tests used the same initial weights, which are pre-trained convolutional feature maps from ResNet-50 while the weights in the other convolution layers are randomly initialized. A similar procedure is applied for experiments with ResNet-101. In [22], comparable results to ours were reported on ResNet-50 based detection. However, their method did not work on the deeper ResNet-101 based detection. Interestingly, although failed for ResNet-101 based detection, their approach succeeded in the classification task using Resnet-101, which suggests that quantization of detection networks is trickier in practice. In the R-FCN structure, there is no fully-connected layer. We quantized all convolutional layers with the same low bit-width quantization formula for each layer.\nR-FCN, ResNet-50\nmAP\nR-FCN, ResNet-101\nmAP\n4-bit LBW\n74.37%\n4-bit LBW\n76.79%\n5-bit LBW\n76.99%\n5-bit LBW\n77.83%\n6-bit LBW\n77.05%\n6-bit LBW\n78.24%\n32-bit full-precision\n77.46%\n32-bit full-precision\n78.94%\nTable 1: Object detection experiments on PASCAL VOC with R-FCN + ResNet-50/ResNet-10 Training set is VOC 07+12 trainval. The results are evaluated on VOC 07 test.\n1All mAP scores are computed with the Python version of the test codes provided by RCNN/Fast RCNN/Fast RCNN GitHub repositories.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f96d/f96dd333-0f4b-4aff-8bcc-3854ea394b24.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Curated examples of 6-bit LBW detection results on 3 sample images, compared with those from the corresponding full precision model. The left columns are results of 32-bit full-precision model, while the right images come from 6-bit LBW model. The network is R-FCN + ResNet-50, and the training data is 2007+2012 trainval. The threshold value 0.5 is used for display.</div>\nFigure 1: Curated examples of 6-bit LBW detection results on 3 sample images, compared with those from the corresponding full precision model. The left columns are results of 32-bit full-precision model, while the right images come from 6-bit LBW model. The network is R-FCN + ResNet-50, and the training data is 2007+2012 trainval. The threshold value 0.5 is used for display. Table 1 shows mAP results from our experiments. With larger bit-width, LBW models achieved higher mAP values, true for both R-FCN + ResNet-50 and R-FCN + ResNet-101. The models trained with the 6-bit LBW scheme almost approach the best mAP of 32-bit full precision models. Besides these quantitative measures, in Fig. 1, we illustrate detection accuracies using R-FCN + ResNet-50 via samples processed by 6-bit LBW in comparison with those by the \u2018ground truth\u2019 full precision model. The first 2 photos are chosen from the 2007 Pascal VOC dataset and the third photo is taken at a university campus with a much more complicated visual scene. In the first 2 photos, both the 6-bit LBW and full precision models detected the major objects correctly, with nearly the same bounding box positions and high classification scores. In the third photo, the 6-bit LBW even surpassed the performance of the full precision model, by detecting a student at the very left side of the top staircase with a score of 0.710. Also the 3rd student from the right (the student in the middle) on the top staircase is detected with a score of 0.952 (0.906) by the 6 bit LBW vs. 0.886 (0.820) by the full precision model. Interestingly, these three students are all side-viewed. At inference time, we have observed an immediate at least 4\u00d7 speedup given by our 6-bit\nR-FCN model. For the three images shown in Fig. 1, the computing time are 0.507s, 0.441s, and 32.269s using a 32-bit R-FCN+ResNet-50 on GPU, while the costs are 0.098s, 0.106s and 6.113s respectively by our 6-bit counterpart.\n# 3.2 Statistical Analysis of Weights\nIn Fig. 2, we illustrate the weight distributions of two floating convolutional layers by histograms. The p-values of a standard hypothesis testing procedure in statistics on normality showed up very small (less than 10\u22125), indicating the strong non-Gaussian behavior of the floating weights in training. This phenomenon posed a challenge to the analytical effort of estimating the parameter \u00b5 in quantization using probability distribution functions as suggested for TWN [8]. In Table 2 and Table 3 , we show the weight percentage distribution of two sample convolutional layers in R-FCN + ResNet50 between different magnitude levels of the quantization for low-bit width and full-precision models. The three low bit-width models involve truncation and encoding operations. The 6 bit-width columns appear to approach the 32-bit float columns on most rows. However, the percentages on the last three (two) rows under the low-bit LBW models in Table 2 (3) are identical to each other and are much larger than the corresponding percentage in the full precision model. This shows that the trained low-bit LBW models captured rather well a small percentage of the large weights. In deep CNNs, the large magnitude weights occupy a small percentage yet have a significant impact on the model accuracy. That is why we chose the partition parameter \u00b5 to be near the maximum norm of the weights. It is worthwhile to note from the two tables that the 4-bit LBW can save lots of memory thanks to both low-bit weights and high sparsity. Over 82% (58%) of the weights are zeros in the convolutional residual block (RPN layer) of the R-FCN plus ResNet50 network. With the help of \u2019Mask\u2019 technology in circuit chip design, zero-valued weights will be skipped and the computational efficiency can be much improved. However, as shown in Table 1, the 4-bit LBW still suffers a few more percentages of accuracy loss than the 5-bit and 6-bit models. The 6-bit LBW model approximates the feature representation capability of the full precision network the best with a sufficient number of smaller levels of quantized weights. For that reason, it almost recovers the performance of the full precision model on the test set. The 6-bit LBW model saves around 5.3\u00d7 weights memory with a small loss of accuracy. The memory savings and the near lossless accuracy of the 6-bit LBW may work well on a modern chip design where all multiplication operations in the convolutional layers can be replaced by bit-wise shift operations, thus highly improving the computing efficiency in applications.\n<div style=\"text-align: center;\">Conv layer in residue block</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fa59/fa59f955-d263-4353-a8ba-bf46b9dd55e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Kurtosis = 6.113, Skewness = \u22120.112</div>\nFigure 2: Histograms of the float weights in 2 convolutional layers of 32-bit full-precision trained RFCN + ResNet-50 model. For both of these 2 layers, the p-values of normal distribution hypothesis testing are extremely small, less than 10\u22125. Also the excess kurtosis measures are much larger than the value for normal distribution, which is 0. Thus these weights are far from being normally distributed.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/08d5/08d5e070-4fee-47e7-bbc8-ba6a41ebcd5b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Kurtosis = 9.398, Skewness = \u22120.481</div>\nR-FCN, ResNet-50\n4-bit LBW\n5-bit LBW\n6-bit LBW\n32-bit full-precision\n|w| < 2\u221216\n82.882%\n10.072%\n0.030%\n0\n2\u221216 \u2264|w| < 2\u221215\n0\n0\n0.060%\n0.076%\n2\u221215 \u2264|w| < 2\u221214\n0\n0\n0.141%\n0.225%\n2\u221214 \u2264|w| < 2\u221213\n0\n0\n0.233%\n0.271%\n2\u221213 \u2264|w| < 2\u221212\n0\n0\n0.486%\n0.613%\n2\u221212 \u2264|w| < 2\u221211\n0\n0\n0.922%\n1.283%\n2\u221211 \u2264|w| < 2\u221210\n0\n0\n1.964%\n2.610%\n2\u221210 \u2264|w| < 2\u22129\n0\n0\n3.776%\n4.945%\n2\u22129 \u2264|w| < 2\u22128\n0\n0\n7.343%\n9.524%\n2\u22128 \u2264|w| < 2\u22127\n0\n18.392%\n13.509%\n16.713%\n2\u22127 \u2264|w| < 2\u22126\n0\n21.221%\n21.221%\n23.581%\n2\u22126 \u2264|w| < 2\u22125\n0\n24.270%\n24.270%\n22.993%\n2\u22125 \u2264|w| < 2\u22124\n0\n17.706%\n17.706%\n12.627%\n2\u22124 \u2264|w| < 2\u22123\n15.479%\n6.700%\n6.700%\n3.784%\n2\u22123 \u2264|w| < 2\u22122\n1.408%\n1.408%\n1.408%\n0.608%\n2\u22122 \u2264|w| < 2\u22121\n0.228%\n0.228%\n0.228%\n0.098%\n2\u22121 \u2264|w|\n0.003%\n0.003%\n0.003%\n0\nTable 2: Statistics of low-bit and full precision weights (w) of one convolutional residual block layer in R-FCN + ResNet-50 at different bit-widths. For 4, 5, 6-bit LBW models, the weights in the first row of partition are exactly equal to 0, and come from rounding down small floating weights during training.\n# 4 Concluding Remarks\nWe discovered the exact solution of the general low-bit approximation problem of a real weight vector in the least squares sense, and proposed a low cost semi-analytical quantization scheme with a single adjustable parameter. This parameter is selected and optimized through training and testing on object detection data sets to approach the performance of the corresponding full precision model. The accuracy of our 6-bit width model is well-within 1% of the full precision model on PASCAL VOC data set, and can even outperform the full-precision model on real-world test images with complex visual scenes. Moreover, the deployment of our low-bit model appears to be more than 4\u00d7 faster. In future work, we plan to improve the low bit width models (especially the 4 bit-width model) further by exploring alternative training algorithms and refining our approximate quantization scheme.\n# Acknowledgments\nThis work was partially supported by NSF grants DMS-1522383 and IIS-1632935, and ONR gran N00014-16-1-7157.\n# Appendix\nProof of Theorem 1. We first fix the number of entries in Q quantized to \u00b12\u2212t to be kt, t = 0, . . . , n \u22121. Then it is easy to show that\n\u2225Q\u22252 = n\u22121 \ufffd i=0 kt2\u22122t and |\u27e8Q, W f\u27e9| \u2264 n\u22121 \ufffd t=0 \u2225W f [kt]\u222512\u2212t.\n(5)\nR-FCN, ResNet-50\n4-bit LBW\n5-bit LBW\n6-bit LBW\n32-bit full-precision\n|w| < 2\u221219\n58.188%\n4.000%\n0.016%\n0.019%\n2\u221219 \u2264|w| < 2\u221218\n0\n0\n0.031%\n0.022%\n2\u221218 \u2264|w| < 2\u221217\n0\n0\n0.047%\n0.045%\n2\u221217 \u2264|w| < 2\u221216\n0\n0\n0.095%\n0.089%\n2\u221216 \u2264|w| < 2\u221215\n0\n0\n0.185%\n0.177%\n2\u221215 \u2264|w| < 2\u221214\n0\n0\n0.370%\n0.355%\n2\u221214 \u2264|w| < 2\u221213\n0\n0\n0.751%\n0.714%\n2\u221213 \u2264|w| < 2\u221212\n0\n0\n1.501%\n1.413%\n2\u221212 \u2264|w| < 2\u221211\n0\n0\n2.993%\n2.836%\n2\u221211 \u2264|w| < 2\u221210\n0\n7.949%\n5.952%\n5.616%\n2\u221210 \u2264|w| < 2\u22129\n0\n11.676%\n11.685%\n11.061%\n2\u22129 \u2264|w| < 2\u22128\n0\n21.571%\n21.588%\n20.625%\n2\u22128 \u2264|w| < 2\u22127\n0\n31.553%\n31.539%\n31.370%\n2\u22127 \u2264|w| < 2\u22126\n39.837%\n21.137%\n21.134%\n23.257%\n2\u22126 \u2264|w| < 2\u22125\n1.953%\n2.093%\n2.091%\n2.397%\n2\u22125 \u2264|w| < 2\u22124\n0.022%\n0.021%\n0.022%\n0.004%\n2\u22124 \u2264|w|\n0.0001%\n0.0001%\n0.0001%\n0\nTable 3: Statistics of low-bit and full precision weights (w) of one RPN layer in R-FCN + ResNet-50 at different bit-widths. For 4, 5, 6-bit LBW models, the weights in the first row of partition are exactly equal to 0, and come from rounding down small floating weights during training.\nTherefore, for any s \u2208Z,\n\u22252sQ \u2212W f\u22252 = 22s\u2225Q\u22252 \u22122s+1\u27e8Q, W f\u27e9+ \u2225W f\u22252\nSince s \u2208Z, by symmetry of the parabola, it suffices to find the nearest power of 2 to \ufffdn\u22121 t=0 \u2225W f [kt]\u222512\u2212t \ufffdn\u22121 i=0 kt2\u22122t So the lower bound in (6) is achieved at s = \ufffd log2 4 \ufffdn\u22121 t=0 \u2225W f [kt]\u222512\u2212t 3 \ufffdn\u22121 i=0 kt2\u22122t \ufffd . Let us define g(u, v) := v(2log2\u230a4u 3v \u230b\u2212u v )2 \u2212u2 v . Then we examine the minimum value of g(\ufffdn\u22121 t=0 \u2225W f [kt]\u222512\u2212t, \ufffdn\u22121 i=0 kt2\u22122t) over all possible combinations of natural numbers k0, . . . , kn\u22121, i.e., the optimal numbers of quantized weights at the n levels are given by\nFinally, to achieve the minimum in (6) with respect to (k\u2217 0, . . . , k\u2217 n\u22121), we must have\nlly, to achieve the minimum in (6) with respect to (k\u2217 0, . . . , k\u2217 n\u22121)\nso that \u27e8Q\u2217, W f\u27e9= \ufffdn\u22121 t=0 \u2225W f [k\u2217 t ]\u222512\u2212t, and choose s\u2217= \ufffd log2 4 \ufffdn\u22121 t=0 \u2225W f [k\u2217 t ]\u222512\u2212t 3 \ufffdn\u22121 i=0 k\u2217 t 2\u22122t \ufffd .\nso that \u27e8Q\u2217, W f\u27e9= \ufffdn\u22121 t=0 \u2225W f [k\u2217 t ]\u222512\u2212t, and choose s\u2217= \ufffd\n[1] Courbariaux, M. & Bengio, Y. & David, J. (2015) BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations. Advances in Neural Information Processing Systems 28, pp. 3123\u20133131. [2] Courbariaux, M. & Hubara, I. & Soudry, D. & El-Yaniv, R. & Bengio, Y. (2016) Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1. CoRR. [3] Dai, J. & Li, Y. & He, K. & Sun, J. (2016) R-FCN: Object Detection via Region-based Fully Convolutional Networks. Advances in Neural Information Processing Systems 29. [4] Deng, J. & Dong, W. & Socher, R. & Li, L. & Li, K. & Li, F. (2009) ImageNet: A Large-Scale Hierarchical Image Database. IEEE Computer Vision and Pattern Recognition. [5] Everingham, M. & Van Gool, L. & Williams, C. & Winn, J. & Zisserman, A. (2010) The PASCAL Visual Object Classes (VOC) Challenge. IJCV. [6] Guo, Y. & Yao, A. & Chen, Y. (2016) Dynamic Network Surgery for Efficient DNNs. Advances in Neural Information Processing Systems 29. [7] Han, S. & Mao, H. & Dally, W. (2016) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. International Conference on Learning Representations. [8] He, K. & Zhang, X. & Ren, S. & Sun, J. (2015) Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385. [9] Hubara, I. & Courbariaux, M. & Soudry, D. & El-Yaniv, R. & Bengio, Y. (2016) Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. arXiv preprint arXiv:1609.07061. [10] Ioffe, S. & Szegedy, C. (2015) Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv preprint arXiv:1502.03167. [11] Jia, Y. & and Shelhamer, E. & Donahue, J. & Karayev, S. & Long, J. & Girshick, R. & Guadarrama, S. & Darrell, T. (2014) Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093. [12] Krizhevsky, A. (2009) Learning Multiple Layers of Features from Tiny Images. [13] Krizhevsky, A. & Sutskever, I. & Hinton, G. (2012) Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems 25, pp. 1097-1105. [14] LeCun, Y. & Bengio, Y. & Hinton, G. (2015) Deep Learning. Nature 521(7553): 436-444. [15] LeCun, Y. & Boser, B. & Denker, J. & Henderson, D. & Howard, R. & Hubbard, W. & Jackel, L. (1989) Backpropagation Applied to Handwritten Zip Code Recognition Neural Computation 1(4): 541-551. [16] LeCun, Y. & Bottou, L. & Bengio, Y. & Haffner, P. (1998) Gradient-based Learning Applied to Document Recognition. Proceedings of the IEEE 86(11): 2278-2324. [17] Li, F. & Zhang, B. & Liu, B. (2016) Ternary Weight Networks. arXiv preprint arXiv:1605.04711. [18] Liu, W. & Anguelov, D. & Erhan, D. & Szegedy, C. & Reed, S. (2015) Ssd: Single shot multibox detector. arXiv preprint arXiv:1512.02325. [19] Nesterov, Y. (1983) A Method for Solving the Convex Programming Problem with Convergence Rate O(1/k2). Soviet Mathematics Doklady 27(2):372-376. [20] Rastegari, M. & Ordonez, V. & Redmon, J. & Farhadi, A. (2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. European Conference on Computer Vision, pp. 525-542.\n[21] Ren, S. & He, K. & Girshick, R. & Sun, J. (2015) Faster r-cnn: Towards Real-time Object Detection with Region Proposal Networks. Advances in neural information processing systems 28, pp. 91\u201399. [22] Park E. & Ahn J. & Yoo S. (2017) Weighted-Entropy-Based Quantization for Deep Neural Networks. CVPR, pp. 5456-5464. [23] Simonyan, S. & Zisserman, A. (2014) Very Deep Convolutional Networks for Large-scale Image Recognition. arXiv preprint arXiv:1409.1556. [24] Szegedy, C. & Liu, W. & Jia, Y. & Sermanet, P. & Reed, S. & Anguelov, D. & Erhan, D. & Vanhoucke, V. & Rabinovich, A. (2015) Going deeper with convolutions. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20139. [25] Zhou, A. & Yao, A. & Guo, Y. & Xu, L. & Chen, Y. (2017) Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights. International Conference on Learning Representations. [26] Zhou, S. & Wu, Y. & Ni, Z. & Zhou, X. & Wen, H. & Zou, Y. (2016) DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients. arXiv preprint arXiv: 1606.06160. [27] Zhu, C. & Han, S. & Miao, H. & Dally, W. (2016) Trained Ternary Quantization. arXiv preprint arXiv:1612.01064.\n",
    "paper_type": "method",
    "attri": {
        "background": "Deep convolutional neural networks (CNNs) have demonstrated superior performance in various computer vision tasks. However, they require significant memory and computational resources, making deployment on small devices challenging. Previous models have attempted to compress these networks through quantization, but often at the cost of performance. This paper presents LBW-Net, a new method for quantization and training of low bit-width CNNs that aims to improve efficiency while maintaining accuracy.",
        "problem": {
            "definition": "The problem addressed in this paper is the need for efficient deployment of deep CNNs on devices with limited computational resources, specifically through low bit-width quantization of weights.",
            "key obstacle": "The main challenge is achieving low bit-width quantization without significantly degrading the performance of the network, particularly on larger datasets."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that many weights in CNNs can be quantized to a small number of distinct values without losing significant accuracy, particularly in the context of object detection.",
            "opinion": "LBW-Net proposes a method that quantizes weights to zero or powers of two, minimizing the distance between full-precision weights and quantized weights during backpropagation.",
            "innovation": "The primary innovation of LBW-Net lies in its semi-analytical thresholding scheme for quantization, which is computationally inexpensive and effective for varying bit-widths."
        },
        "method": {
            "method name": "LBW-Net",
            "method abbreviation": "LBW",
            "method definition": "LBW-Net is a method for quantizing and training low bit-width CNNs by minimizing the Euclidean distance between full-precision weights and quantized weights.",
            "method description": "The core of LBW-Net involves quantizing weights to zero or powers of two while maintaining performance close to full-precision models.",
            "method steps": [
                "Initialize weights either randomly or using pre-trained weights.",
                "Perform forward and backward propagation through the network.",
                "Quantize the weights using the proposed thresholding scheme.",
                "Retrain the network to adjust for quantization errors."
            ],
            "principle": "The effectiveness of LBW-Net is based on the principle that many weights in CNNs have a non-Gaussian distribution, allowing for effective quantization that retains critical information for object detection."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on the PASCAL VOC dataset, using ResNet-50 and ResNet-101 as backbone architectures, comparing 4, 5, and 6-bit LBW models against 32-bit full-precision models.",
            "evaluation method": "Performance was assessed using mean Average Precision (mAP) on the test dataset, with experiments carried out on Caffe using an Nvidia Titan X GPU."
        },
        "conclusion": "The LBW-Net method demonstrates that quantized networks can achieve nearly lossless performance compared to full-precision counterparts, with the 6-bit model even outperforming full-precision models in certain scenarios. The deployment of LBW-Net is over 4\u00d7 faster, making it suitable for resource-constrained environments.",
        "discussion": {
            "advantage": "Key advantages of LBW-Net include significant memory savings, energy efficiency, and faster deployment, while maintaining high accuracy in object detection tasks.",
            "limitation": "The method may encounter challenges in scenarios with extremely complex visual scenes or when further reducing bit-width, where performance degradation could become more pronounced.",
            "future work": "Future research will focus on improving the low bit-width models, particularly exploring alternative training algorithms and refining the quantization scheme."
        },
        "other info": {
            "acknowledgments": "This work was partially supported by NSF grants DMS-1522383 and IIS-1632935, and ONR grant N00014-16-1-7157."
        }
    },
    "mount_outline": [
        {
            "section number": "7.4",
            "key information": "Future research will focus on improving the low bit-width models, particularly exploring alternative training algorithms and refining the quantization scheme."
        },
        {
            "section number": "3.4",
            "key information": "The main challenge is achieving low bit-width quantization without significantly degrading the performance of the network, particularly on larger datasets."
        },
        {
            "section number": "5.1",
            "key information": "LBW-Net proposes a method that quantizes weights to zero or powers of two, minimizing the distance between full-precision weights and quantized weights during backpropagation."
        },
        {
            "section number": "4.4",
            "key information": "The method may encounter challenges in scenarios with extremely complex visual scenes or when further reducing bit-width, where performance degradation could become more pronounced."
        },
        {
            "section number": "6.3",
            "key information": "Key advantages of LBW-Net include significant memory savings, energy efficiency, and faster deployment, while maintaining high accuracy in object detection tasks."
        }
    ],
    "similarity_score": 0.5668614199390732,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0011_large/papers/Quantization and Training of Low Bit-Width Convolutional Neural Networks for Object Detection.json"
}