{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.18153",
    "title": "Do Large Language Models Know What They Don't Know?",
    "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs\u2019 self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
    "bib_name": "yin2023largelanguagemodelsknow",
    "md_text": "# rge Language Models Know What They Don\u2019t\nZhangyue Yin\u2662 Qiushi Sun\u2660 Qipeng Guo\u2662 Jiawen Wu\u2662 Xipeng Qiu\u2662\u2217 Xuanjing Huang\u2662 \u2662School of Computer Science, Fudan University \u2660Department of Mathematics, National University of Singapore {yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu {qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n# Abstract\nLarge language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs\u2019 self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.\n\u201cTrue wisdom is knowing what you don\u2019t know.\u201d \u2013Confucius\n# 1 Introduction\nRecently, Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al., 2023), and LLaMA (Touvron et al., 2023) have shown exceptional performance on a wide range of NLP tasks, including common sense reasoning (Wei et al., 2022; Zhou et al., 2022) and mathe-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3139/313966cd-4a1f-4c39-afc8-d2c54ea1befd.png\" style=\"width: 50%;\"></div>\nFigure 1: Know-Unknow Quadrant. The horizontal axis represents the model\u2019s memory capacity for knowledge, and the vertical axis represents the model\u2019s ability to comprehend and utilize knowledge.\nmatical problem-solving (Lewkowycz et al., 2022; Chen et al., 2022). Despite their ability to learn from huge amounts of data, LLMs still have limitations in their capacity to retain and understand information. To ensure responsible usage, it is crucial for LLMs to have the capability of recognizing their limitations and conveying uncertainty when responding to unanswerable or unknowable questions. This acknowledgment of limitations, also known as \u201cknowing what you don\u2019t know,\u201d is a crucial aspect in determining their practical applicability. In this work, we refer to this ability as model self-knowledge. The Know-Unknow quadrant in Figure 1 illustrates the relationship between the model\u2019s knowledge and comprehension. The ratio of \u201cKnown Knows\u201d to \u201cUnknown Knows\u201d demonstrates the model\u2019s proficiency in understanding and applying existing knowledge. Techniques such as Chain-of-Thought (Wei et al., 2022), SelfConsistency (Wang et al., 2022), and Complex CoT (Fu et al., 2022) can be utilized to increase\nthis ratio, resulting in improved performance on NLP tasks. We focus on the ratio of \u201cKnown Unknows\u201d to \u201cUnknown Unknows\u201d, which indicates the model\u2019s self-knowledge level, specifically understanding its own limitations and deficiencies in the unknows. Existing datasets such as SQuAD2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017), widely used in question answering (QA), have been utilized to test the self-knowledge of models with unanswerable questions. However, these questions are context-specific and could become answerable when supplemented with additional information. Srivastava et al. (2022) attempted to address this by evaluating LLMs\u2019 competence in delineating their knowledge boundaries, employing a set of 23 pairs of answerable and unanswerable multiple-choice questions. They discovered that these models\u2019 performance barely surpassed that of random guessing. Kadavath et al. (2022) suggested probing the selfknowledge of LLMs through the implementation of a distinct \"Value Head\". Yet, this approach may encounter difficulties when applied across varied domains or tasks due to task-specific training. Consequently, we redirect our focus to the inherent abilities of LLMs, and pose the pivotal question: \u201cDo large language models know what they don\u2019t know?\u201d. In this study, we investigate the self-knowledge of LLMs using a novel approach. By gathering reference sentences with uncertain meanings, we can determine whether the model\u2019s responses reflect uncertainty using a text similarity algorithm. We quantified the model\u2019s self-knowledge using the F1 score. To address the small and idiosyncratic limitations of existing datasets, we created a new dataset called SelfAware. This dataset comprises 1,032 unanswerable questions, which are distributed across five distinct categories, along with an additional 2,337 questions that are classified as answerable. Experimental results on GPT-3, InstructGPT, LLaMA, and other LLMs demonstrate that in-context learning and instruction tuning can effectively enhance the self-knowledge of LLMs. However, the self-knowledge exhibited by the current state-of-the-art model, GPT-4, measures at 75.47%, signifying a notable disparity when contrasted with human self-knowledge, which is rated at 84.93%. Our key contributions to this field are summarized as follows:\n\u2022 Through our detailed analysis of 20 LLMs, benchmarked against human self-knowledge, we identified a significant disparity between the most advanced LLMs and humans 1.\n# 2 Dataset Construction\nTo conduct a more comprehensive evaluation of the model\u2019s self-knowledge, we constructed a dataset that includes a larger number and more diverse types of unanswerable questions than KnowUnknowns dataset (Srivastava et al., 2022). To facilitate this, we collected a corpus of 2,858 unanswerable questions, sourced from online platforms like Quora and HowStuffWorks. These questions were meticulously evaluated by three seasoned annotation analysts, each operating independently. The analysts were permitted to leverage external resources, such as search engines. To ensure the validity of our dataset, we retained only the questions that all three analysts concurred were unanswerable. This rigorous process yielded a finalized collection of 1,032 unanswerable questions. In pursuit of a comprehensive evaluation, we opted for answerable questions drawn from three datasets: SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), and TriviaQA (Joshi et al., 2017). Our selection was guided by SimCSE (Gao et al., 2021), which allowed us to identify and select the answerable questions semantically closest to the unanswerable ones. From these sources, we accordingly drew samples of 1,487, 182, and 668 questions respectively, amassing a total of 2,337. Given that these questions can be effectively addressed using information available on Wikipedia, the foundational corpus for the training of current LLMs, it is plausible to infer that the model possesses the requisite knowledge to generate accurate responses to these questions. Our dataset, christened SelfAware, incorporates 1,032 unanswerable and 2,337 answerable questions. To reflect real-world distribution, our dataset\nCategory\nDescription\nExample\nPercentage\nNo scientific\nconsensus\nThe answer is still up\nfor debate, with no consensus\nin scientific community.\n\u201cAre we alone in the universe,\nor will we discover alien\nlife at some point?\u201d\n25%\nImagination\nThe question are about people\u2019s\nimaginations of the future.\n\"What will the fastest form of\ntransportation be in 2050?\"\n15%\nCompletely\nsubjective\nThe answer depends on\npersonal preference.\n\"Would you rather be shot\ninto space or explore the\ndeepest depths of the sea?\"\n27%\nToo many\nvariables\nThe question with too\nmany variables cannot\nbe answered accurately.\n\u201cJohn made 6 dollars mowing lawns\nand 18 dollars weed eating.\nIf he only spent 3 or 5 dollar a week,\nhow long would the money last him?\u201d\n10%\nPhilosophical\nThe question can yield\nmultiple responses, but it\nlacks a definitive answer.\n\u201cHow come god was\nborn from nothingness?\u201d\n23%\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories\ncontains a proportion of answerable questions that is twice as large as the volume of unanswerable ones. Nevertheless, to ensure the feasibility of testing, we have purposefully capped the number of answerable questions.\n# 2.1 Dataset Analysis\nTo gain insight into the reasons precluding a certain answer, we undertook a manual analysis of 100 randomly selected unanswerable questions. As tabulated in Table 1, we have broadly segregated these questions into five distinctive categories. \u201cNo Scientific Consensus\" encapsulates questions that ignite ongoing debates within the scientific community, such as those concerning the universe\u2019s origin. \u201cImagination\" includes questions involving speculative future scenarios, like envisaged events over the next 50 years. \u201cCompletely Subjective\" comprises questions that are inherently personal, where answers depend heavily on individual predispositions. \u201cToo Many Variables\" pertains to mathematical problems that become unsolvable owing to the overwhelming prevalence of variables. Lastly, \u201cPhilosophical\" represents questions of a profound, often metaphysical, nature that resist concrete answers. Ideally, upon encountering such questions, the model should express uncertainty instead of delivering conclusive responses.\n# 3 Evaluation Method\nThis section elucidates the methodology employed for assessing self-knowledge in the generated text.\nIn order to achieve this, we define a similarity function, fsim, to compute the similarity, S, between a given sentence, t, and a collection of reference sentences, U = {u1, u2, . . . , un}, endowed with uncertain meanings.\n(1)\n() Whenever any Si surpasses a pre-determined threshold T , we perceive the text t as encompassing uncertain meanings, thereby eliminating the need for manual evaluation of the response. Given the substantial disparity in the volume of answerable and unanswerable questions in SelfAware, we adopt the F1 score as a measure of LLMs\u2019 self-knowledge. Our focus rests on identifying unanswerable questions, hence we designate them as positive cases and categorize answerable questions as negative cases.\n# 4 Experiment\n# 4.1 Model\nWe conduct a sequence of experiments to evaluate the degree of self-knowledge manifested by various LLMs, including GPT-3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022) series, as well as the recent LLaMA (Touvron et al., 2023) and its derivative models, namely Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). Our investigative approach employed three distinct input forms: Direct, Instruction, and In-Context Learning (ICL), which is encapsulated in Appendix A.4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bc16/bc1619b6-8bad-40b8-b8c5-9d0c61d33e93.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage, curie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1dde/1dde32c2-c0ca-4b31-901b-44a72b8952ba.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparison between the davinci series and human self-knowledge in instruction input form.</div>\n# 4.2 Setting\nWe devised the reference sentence set U through a process that combined automated generation by LLMs and manual filtering, detailed further in Appendix A.1. To quantify the similarity between target and reference sentences, we utilized SimCSE (Gao et al., 2021), setting the similarity threshold to 0.75 during our experiments. An exploration of threshold ablation is available in Appendix A.2. To counteract potential errors in similarity calculation induced by varying lengths of the target and reference sentences, we employed a sliding window of length 5 to parse the target sentence into semantic chunks. During the generation process, we set the temperature to 0.7. We selected a random sample of 100 instances for GPT-4, while the remainder of the models were scrutinized using the full SelfAware dataset.\n# 4.3 Human Self-Knowledge\nTo establish a benchmark for human selfknowledge, we engaged two volunteers and selected 100 random samples from the SelfAware dataset. The volunteers has 30 minutes to make\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/886e/886eabe0-09fb-44e9-a478-edda4897a2c4.png\" style=\"width: 50%;\"></div>\nFigure 4: Experimental comparison of davinci series in ICL input form.\njudgments on the same set of questions, yielding an average F1 score of 84.93%, which we subsequently adopted as the benchmark for human self-knowledge. Detailed scores are available in Appendix A.3.\n# 4.4 Analysis\nWe evaluate the manifestation of LLMs\u2019 selfknowledge, centering our investigation on three fundamental dimensions: the size of the model, the impact of instruction tuning, and the influence exerted by different input forms.\nModel Size. Figure 2 illustrates the correlation between model size and self-knowledge across various LLMs. It is noteworthy that across all three input forms, an augmentation in model parameter size is associated with an elevation in the F1 Score, with the most conspicuous enhancement manifesting in the ICL input form. Therefore, our analysis indicates that an LLM\u2019s self-knowledge tends to enhance with increasing model size, a trend consistent with the scaling law.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a6ad/a6ad8378-ba78-4d53-8dbe-56f84d201c2f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/373c/373cc3a9-e930-421a-8b00-77e1306afcbf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Experimental results obtained from LLaMA and its derived models, Alpaca and Vicuna in instruction input form.</div>\nInput Forms. As shown in Figure 2, the incorporation of instructions and examples serves to boost the self-knowledge of both the GPT-3 and InstructGPT series. Specifically, ICL input form, providing richer contextual information, contributes to a significant enhancement in models\u2019 self-knowledge. This impact is particularly noticeable in the davinci model, where ICL facilitates a 27.96% improvement over the direct. Moreover, a comparison between Figure 3 and Figure 4 reveals that the inclusion of instructions and examples successfully minimizes the performance disparity between the davinci and text-davinci models, suggesting an acquisition of self-knowledge from the instructions and provided examples.\nCompared with Human. Figure 3 reveals that, without supplementary samples, GPT-4 currently performs best among the tested models, achieving an impressive F1 score of 75.47%. However, a noticeable gap becomes evident when comparing this\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14a0/14a00412-05b6-445e-8bd3-e6c3a9191632.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Accuracy of the InstructGPT series when responding to answerable questions in instruction input form.</div>\nperformance to the human benchmark of 84.93%. This underscores the considerable potential that remains for enhancing the self-knowledge level of LLMs.\nperformance to the human benchmark of 84.93%. This underscores the considerable potential that remains for enhancing the self-knowledge level of LLMs. Answerable Questions. Figure 6 traces the performance evolution of the InstructGPT series in addressing answerable questions, adhering to the closed-book question answering paradigm (Touvron et al., 2023), where output accuracy is contingent on the presence of the correct answer. Our observations underscore a steady enhancement in QA task accuracy corresponding to an increase in model parameter size and continuous learning. Particularly, the accuracy of text-davinci-001 experiences a significant ascent, scaling from a meager 2.48% in text-ada-001 to 10.61%, whereas GPT-4 marks an even more striking jump to 42.64%.\n# 5 Conclusion\nThis study investigates the self-knowledge of LLMs by evaluating their ability to identify unanswerable questions. Through the introduction of a novel dataset and an automated method for detecting uncertainty in the models\u2019 responses, we are able to accurately measure the self-knowledge of LLMs such as GPT-3, InstructGPT and LLaMA. Our results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge. This highlights the need for further research in this area to enhance the ability of LLMs to understand their own limitations on the unknows. Such efforts will lead to more accurate and reliable responses from LLMs, which will have a positive impact on their applications in diverse fields.\n# Limitations\n Generalization of reference sentences. At present, we have selected sentences with uncertain meanings exclusively from the GPT-3 and InstructGPT series, potentially overlooking uncertainty present in responses generated by other LLMs. However, it is not feasible to catalog all sentences with uncertain meanings exhaustively. As a direction for future research, we propose to concentrate on the automated acquisition of more accurate reference sentences to address this concern.\n Limitations of input forms: Our examination was confined to three unique input forms: direct, instruction, and ICL. There is burgeoning research aimed at bridging the gap between models and human-like methods of reasoning and problem-solving, including but not limited to approaches like Reflexion (Shinn et al., 2023), ToT (Yao et al., 2023), MoT (Li and Qiu, 2023). Future endeavors will integrate additional cognitive and decision-making methods to delve deeper into the self-knowledge exhibited by these LLMs.\n# Ethics Statement\nThe SelfAware dataset, meticulously curated to evaluate LLMs\u2019 ability to discern unanswerable questions, is composed of unanswerable questions extracted from sources such as Quora and HowStuffWorks, alongside answerable questions procured from three distinct open datasets. Every question was thoroughly examined for relevance and harmlessness. To ensure content validity, three annotation analysts, compensated at local wage standards, dedicated regular working hours to content review. Throughout our research process, we underscored the significance of privacy, data security, and strict compliance with dataset licenses. In order to protect data integrity, we implemented anonymization and content filtration mechanisms. Our adherence to OpenAI\u2019s stipulations remained unyielding for the usage of GPT-3 and InstructGPT models, and likewise for Meta\u2019s terms pertaining to LLaMA models. We rigorously vetted the licenses of the three publicly available datasets for compliance, ensuring that all our research methodologies were in alignment with ethical standards at the institutional, national, and global levels.\nAdhering to the CC-BY-SA-4.0 protocol, the dataset, once publicly released, will be reserved exclusively for research purposes. We pledge to promptly and effectively address any concerns relating to the dataset, while concurrently anticipating researchers to maintain high ethical standards in their utilization of this data.\n# Acknowledgement\nWe wish to express our gratitude to our colleagues in the FudanNLP group whose insightful suggestions, perspectives, and thought-provoking discussions significantly contributed to this work. Our sincere appreciation also extends to the anonymous reviewers and area chairs, whose constructive feedback was instrumental in refining the quality of our study. This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027) and CAAI-Huawei MindSpore Open Fund.\n# References\nohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wiet-\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce1f/ce1fd001-b2e6-4be3-8500-b12f7e3a87d1.png\" style=\"width: 50%;\"></div>\n# A Appendix\nTo assemble a set of reference sentences, we randomly chose 100 entries from the SelfAware dataset. For each model in the GPT-3 and InstructGPT series, we conducted a preliminary test using the direct input form and manually curated sentences that displayed uncertainty. From this pre-test, we procured 16 sentences manifesting uncertain connotations to serve as our reference sentences. After normalizing these sentences by eliminating punctuation and converting to lowercase, we utilized them to compute similarity with target sentences throughout our experimental procedure. 1. The answer is unknown. 2. The answer is uncertain. 3. The answer is unclear. 4. There is no scientific evidence. 5. There is no definitive answer. 6. There is no right answer. 7. There is much debate. 8. There is no known case. 9. There is no concrete answer to this question. 10. There is no public information available. 11. It is impossible to know. 12. It is impossible to answer. 13. It is difficult to predict. 14. It is not known. 15. We do not know. 16. I\u2019m not sure. A.2 Threshold ablation We generated 100 new responses using the textdavinci-002 with direct input form and manually filtered out sentences that contained uncertainty. We then used SimCSE (Gao et al., 2021) to calculate the similarity between these sentences and the reference sentences in Appendix A.1. We tested various thresholds for filtering sentences with uncertain meanings and compared them to manually\n# A.2 Threshold ablation\nWe generated 100 new responses using the textdavinci-002 with direct input form and manually filtered out sentences that contained uncertainty. We then used SimCSE (Gao et al., 2021) to calculate the similarity between these sentences and the reference sentences in Appendix A.1. We tested various thresholds for filtering sentences with uncertain meanings and compared them to manually\nThreshold\nPrecision\nRecall\nF1\n0.95\n100.00\n70.00\n82.35\n0.90\n100.00\n75.00\n85.71\n0.85\n100.00\n75.00\n85.71\n0.80\n100.00\n80.00\n88.89\n0.75\n100.00\n85.00\n91.89\n0.70\n89.47\n90.00\n89.73\n0.65\n86.95\n90.00\n88.45\n<div style=\"text-align: center;\">Table 2: Evaluation results comparing sentences with uncertain meaning filtered by various thresholds.</div>\nHuman\nPrecision\nRecall\nF1\nVolunteer A\n91.52\n78.26\n84.37\nVolunteer B\n96.36\n76.81\n85.48\nTable 3: Evaluation results of 100 responses from two volunteers.\nannotated sentences. We considered unanswerable questions as positive examples and calculated precision, recall, and F1 score. The results in Table 2 indicate that a threshold of 0.75 produced the highest F1 score, balancing precision and the inclusion of other uncertain sentences. As a result, we selected 0.75 as the similarity threshold for subsequent experiments.\n# A.3 Human Self-Knowledge Test\nThe evaluation results for the responses from our invited volunteers are presented in Table 3. The F1 scores for the responses were high, indicating that both volunteers exhibited a strong level of selfknowledge.\n# A.4 Template\nThe input templates used in our experiments, Direct, Instruction, and ICL, are illustrated in Figures 7, 8, and 9, respectively. In the ICL template, we composed 3 answerable and 3 unanswerable questions and provided the corresponding answers manually.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Large language models (LLMs) have shown exceptional performance on various NLP tasks but struggle to recognize their own limitations. The ability to understand unanswerable questions is crucial for responsible usage of LLMs.",
            "purpose of benchmark": "The benchmark is intended to evaluate LLMs' self-knowledge by assessing their capability to identify unanswerable questions and measure their uncertainty in responses."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of determining whether LLMs can recognize and express uncertainty regarding unanswerable questions.",
            "key obstacle": "Existing benchmarks often use context-specific unanswerable questions, which may become answerable with additional information, limiting their effectiveness in evaluating model self-knowledge."
        },
        "idea": {
            "intuition": "The inspiration for the benchmark comes from the need to assess LLMs' inherent ability to understand and convey their knowledge limitations.",
            "opinion": "The authors believe that understanding self-knowledge is essential for enhancing the application of LLMs in real-world scenarios.",
            "innovation": "This benchmark introduces a new dataset, SelfAware, which includes a larger variety of unanswerable questions across multiple categories, unlike previous datasets.",
            "benchmark abbreviation": "SelfAware"
        },
        "dataset": {
            "source": "The dataset was created by collecting unanswerable questions from online platforms such as Quora and HowStuffWorks, and answerable questions from existing datasets like SQuAD, HotpotQA, and TriviaQA.",
            "desc": "The SelfAware dataset consists of 1,032 unanswerable questions and 2,337 answerable questions, categorized into five distinct groups.",
            "content": "The dataset includes unanswerable questions that are philosophical, subjective, or have too many variables, alongside answerable questions that can be addressed using available knowledge.",
            "size": "3,369",
            "domain": "Question Answering",
            "task format": "Identifying unanswerable questions"
        },
        "metrics": {
            "metric name": "F1 Score",
            "aspect": "Model's self-knowledge in recognizing unanswerable questions",
            "principle": "The F1 score was chosen as it effectively balances precision and recall in identifying unanswerable questions.",
            "procedure": "Model performance is evaluated based on the F1 score, with unanswerable questions treated as positive cases and answerable questions as negative cases."
        },
        "experiments": {
            "model": "The models tested include GPT-3, InstructGPT, LLaMA, and their derivatives.",
            "procedure": "Models were evaluated using different input forms: Direct, Instruction, and In-Context Learning, with similarity thresholds set for assessing responses.",
            "result": "GPT-4 achieved the highest F1 score of 75.47%, but this was significantly lower than the human benchmark of 84.93%.",
            "variability": "Variability was accounted for by conducting multiple trials and utilizing the full dataset for most models."
        },
        "conclusion": "The study reveals that while LLMs possess some level of self-knowledge, there remains a notable gap compared to human proficiency, indicating a need for further research to enhance these models' understanding of their limitations.",
        "discussion": {
            "advantage": "The benchmark enhances the understanding of LLMs' self-knowledge and contributes to developing more responsible AI applications.",
            "limitation": "The benchmark may overlook uncertainty in responses from models outside the GPT-3 and InstructGPT series, limiting generalizability.",
            "future work": "Future research should focus on expanding the dataset to include a broader range of reference sentences and exploring additional input forms for evaluation."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark addresses the challenge of determining whether LLMs can recognize and express uncertainty regarding unanswerable questions."
        },
        {
            "section number": "1.2",
            "key information": "The ability to understand unanswerable questions is crucial for responsible usage of LLMs."
        },
        {
            "section number": "3.1",
            "key information": "The models tested include GPT-3, InstructGPT, LLaMA, and their derivatives, demonstrating how LLMs adapt to various contexts."
        },
        {
            "section number": "3.2",
            "key information": "The study reveals that while LLMs possess some level of self-knowledge, there remains a notable gap compared to human proficiency."
        },
        {
            "section number": "4.1",
            "key information": "Models were evaluated using different input forms: Direct, Instruction, and In-Context Learning, highlighting the influence of effective prompt design."
        },
        {
            "section number": "6.1",
            "key information": "Existing benchmarks often use context-specific unanswerable questions, which may become answerable with additional information, limiting their effectiveness in evaluating model self-knowledge."
        },
        {
            "section number": "6.4",
            "key information": "The benchmark may overlook uncertainty in responses from models outside the GPT-3 and InstructGPT series, limiting generalizability."
        }
    ],
    "similarity_score": 0.6923357678839919,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Do Large Language Models Know What They Don't Know_.json"
}