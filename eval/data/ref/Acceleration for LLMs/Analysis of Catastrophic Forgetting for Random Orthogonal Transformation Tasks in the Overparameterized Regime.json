{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2207.06475",
    "title": "Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime",
    "abstract": "Overparameterization is known to permit strong generalization performance in neural networks. In this work, we provide an initial theoretical analysis of its effect on catastrophic forgetting in a continual learning setup. We show experimentally that in permuted MNIST image classification tasks, the generalization performance of multilayer perceptrons trained by vanilla stochastic gradient descent can be improved by overparameterization, and the extent of the performance increase achieved by overparameterization is comparable to that of state-of-the-art continual learning algorithms. We provide a theoretical explanation of this effect by studying a qualitatively similar two-task linear regression problem, where each task is related by a random orthogonal transformation. We show that when a model is trained on the two tasks in sequence without any additional regularization, the risk gain on the first task is small if the model is sufficiently overparameterized.",
    "bib_name": "goldfarb2022analysiscatastrophicforgettingrandom",
    "md_text": "# Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime\nDaniel Goldfarb Khoury College of Computer Sciences Northeastern University Boston, MA 02115 goldfarb.d@northeastern.edu\nDept. of Mathematics and Khoury College of Computer Sciences Northeastern University Boston, MA 02115 p.hand@northeastern.edu\n# Abstract\nOverparameterization is known to permit strong generalization performance in neural networks. In this work, we provide an initial theoretical analysis of its effect on catastrophic forgetting in a continual learning setup. We show experimentally that in permuted MNIST image classification tasks, the generalization performance of multilayer perceptrons trained by vanilla stochastic gradient descent can be improved by overparameterization, and the extent of the performance increase achieved by overparameterization is comparable to that of state-of-the-art continual learning algorithms. We provide a theoretical explanation of this effect by studying a qualitatively similar two-task linear regression problem, where each task is related by a random orthogonal transformation. We show that when a model is trained on the two tasks in sequence without any additional regularization, the risk gain on the first task is small if the model is sufficiently overparameterized.\n# 1 Introduction\nContinual learning is the ability of a model to learn continuously from a stream of data, building on what was previously learned and retaining previously learned skills without the need for retraining. A major obstacle for neural networks to learn continually is the catastrophic forgetting problem: the abrupt drop in performance on previous tasks upon learning new ones. Modern neural networks are typically trained to greedily minimize a loss objective on a training set, and without any regularization, the model\u2019s performance on a previously trained task may degrade. Techniques for mitigating catastrophic forgetting fall under three main groups: generative replay, parameter isolation, and regularization methods [7]. Generally, the goal of regularization methods is to determine important parameters from previous tasks and constrain them so that they do not get modified too much while training subsequent tasks. Two common regularization methods are Synaptic Intelligence (SI) [22] and Elastic Weight Consolidation (EWC) [13]. See Appendix A for a detailed description of them. It is well-known that strong generalization performance for neural networks is typically obtained in the overparameterized regime, where the number of learnable parameters is greater than the number of training examples. Work on overparameterized machine learning has led to research on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/23bf/23bf124e-7f70-47b6-a517-eb15d43288a9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b10c/b10cb879-258d-45f9-94a7-fe08a33ad74c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Results of permuted MNIST experiment. Red curves denote performance of SI, yellow curves denote performance of EWC, blue curves denote performance of Vanilla SGD. Bolder saturation of lines corresponds to larger width parameters (more overparameterization). Specific hyperparameters are reported in Appendix B. Curves for w = 7, 9 for EWC are omitted due to computational constraints on Fisher matrix estimates.</div>\nFigure 1: Results of permuted MNIST experiment. Red curves denote performance of SI, yellow curves denote performance of EWC, blue curves denote performance of Vanilla SGD. Bolder saturation of lines corresponds to larger width parameters (more overparameterization). Specific hyperparameters are reported in Appendix B. Curves for w = 7, 9 for EWC are omitted due to computational constraints on Fisher matrix estimates.\nthe so-called double descent phenomenon, where test error improves as model complexity increases beyond the level needed to fit the training data, outperforming all underparameterized versions of the model [2]. One of the first observations of this behavior in modern neural networks was in extremely wide ResNet18 models that generalize better than their underparameterized counterparts on CIFAR-10 despite fitting to label noise [17]. This model-wise double descent phenomenon has been demonstrated analytically in a variety of machine learning models [10, 3, 1], including some as simple as linear regression. Such linear models will be the basis of theoretical analysis in the present paper. While investigating the relationship of overparameterization with catastrophic forgetting, we made an interesting experimental observation. We considered 10 permuted-MNIST tasks, where each task has training data given by random permutations of the original MNIST images [15]. We trained 2-layer multilayer perceptrons (MLPs) with a variety of layer widths given by [400w, 400w], where w = 1, 3, 5, 7, 9, using vanilla stochastic gradient descent and the continual learning algorithms, SI and EWC. We compared the test accuracy on all seen tasks. As expected, and as shown in Figure 1, average accuracy with SGD drops significantly after learning multiple tasks, and that drop is mitigated by using SI or EWC. Interestingly, we observe that a significant fraction of the accuracy gain achieved by SI or EWC can be obtained with vanilla SGD by simply overparameterizing the model. This can be seen by comparing the w = 1 and w = 9 curves with SGD to the curves with SI and EWC. See Appendix B for more details on the experiments. The goal of the present paper is to analytically illustrate the effect overparameterization can have on catastrophic forgetting. As with the illustrations of double descent in [10, 3, 1], we choose to study the effect with a linear regression problem for simplicity and mathematical convenience. While the experiments above study the case of tasks related by random permutations, our analysis will instead study the qualitatively similar case where two tasks are related by a random orthogonal transformation, which results in simpler mathematical analysis, as we discuss in Section 3. The relation given by the random orthogonal transformation gives two data feature spaces corresponding to each task that are approximately but not exactly orthogonal. We construct two tasks, A and B. Let task A be defined by data matrix XA \u2208Rn\u00d7p, with rows being p noisy random projections of some low-dimensional latent features, and responses y \u2208Rn that are noiseless and linear in the latent features. Let O be a random p \u00d7 p orthogonal matrix. Then task B is defined by data XB = XAO\u22a4 and the same responses y. Learning tasks A and B involve estimating a \u03b2 \u2208Rp for the predictor f\u03b2 : x \ufffd\u2192x\u22a4\u03b2 to fit the data (XA, y) and (XB, y), respectively. We analyze the increase in statistical risk on task A between an estimator trained on task A by minimizing square loss, with initialization at zero, and one sequentially trained on task A and then task B with no explicit regularization. Let R(f\u03b2) be the risk on task A of an estimator f with parameters \u03b2. Let \u02c6\u03b2A be the parameters of the model that is trained on task A. Let \u02c6\u03b2BA be the\nparameters of a model that is initialized at \u02c6\u03b2A and then trained on task B. Our main result is that if there are more training examples than the intrinsic (latent) dimensionality of the data and if there is not too much noise in the observed features, then\nwith high probability. The result asserts that under our linear model, the extent of catastrophic forgetting is arbitrarily small if the overparameterization ratio, p/n, is sufficiently large. We thus see an analytical illustration that catastrophic forgetting can be ameliorated by overparametization in the case of a suitable linear model. The full theorem is stated in Section 2.4 and its proof is provided in Appendix E.\n\u2022 We empirically observe that overparameterization can account for a majority of the performance drop due to catastrophic forgetting in a permuted image task using a multi-layer perceptron. \u2022 We provide a linear regression problem that exhibits a corresponding effect for overparameterization and continual learning. \u2022 We establish a non-asymptotic bound on the performance drop of this linear model in an orthogonal transformation task setting using results from random matrix theory. This result provides a formal illustration that continual learning can in some cases be ameliorated by overparameterization.\n# 2 Analysis of Catastrophic Forgetting in a Linear Model\nIn this section, we present a latent space model for linear regression that we will analyze in order to illustrate that overparameterization can ameliorate catastrophic forgetting. Our single task model is the latent space model of [10] without label noise. Then, we present the analogy between this linear model and neural networks. Next, we empirically demonstrate that under this model, overparameterization ameliorates catastrophic forgetting. Finally, we present a theorem that establishes that observation with high probability.\nLet Z = Rd, which we call the latent feature space. Consider data for regression generated by a noiseless linear response to standard Gaussian latent features. That is, for some \u03b8 \u2208Rd, let an example be given by\net X = Rp, which we call the observed feature space. We consider the case where, for each example, e have access only to p observed features, given by noisy random projections of the latent features:\nLet X = Rp, which we call the observed feature space. We consider the case where, for each example, we have access only to p observed features, given by noisy random projections of the latent features:\nwhere W \u2208Rp\u00d7d and u \u223cN(0, Ip). We could take W to have i.i.d. N(0, \u03b3) entries, but for mathematical convenience, we will instead study the idealization in which W has columns that form a scaled orthonormal basis of a random d-dimensional subspace of Rp. Namely, W \u22a4W = p\u03b3Id. For large p, this idealization is approximately satisfied under the above Gaussian model for W. We consider two tasks, A and B, both with n examples. Task A has data (XA, y) \u2208Rn\u00d7p \u00d7 Rn where each of the n rows of XA and entries of y are sampled independently by (2) - (4). Let O be a random p \u00d7 p orthogonal matrix. Task B has data (XB, y) where XB = XAO\u22a4.\n(1)\n(2) (3)\n(4)\nand we will sometimes refer to the parameters \u02c6\u03b2 as the estimator. We estimate the parameters of this model by gradient descent with a square loss. We are interested in the case of d < n < p. As n < p, the solution to this problem depends on initialization and solves the following optimization problem:\nwhere \u03b20 is the initialization, and X is either XA or XB, depending on the task being solved. To study the sequential training of tasks A and B, we define the following estimators:\n\u2022 \u02c6\u03b2A is the solution to task A when initialized at 0, \u2022 \u02c6\u03b2B is the solution to task B when initialized at 0, \u2022 \u02c6\u03b2BA is the solution to task B when initialized at \u02c6\u03b2A.\nThese parameters are found by solving the following optimization problems:\nThe optimization problem in (6) has the following closed form solution when X has rank n:\nwhere PX\u22a4is the orthogonal projector onto the range of X\u22a4. As n < p, XA and XB have rank n with probability 1, and this gives the following closed forms for \u02c6\u03b2A, \u02c6\u03b2B, \u02c6\u03b2BA:\nWe evaluate these estimators on task A. The risk on task A of an estimator f with parameters \u02c6\u03b2 is given by\nwhere\n(6)\n(7)\n(9)\n(10) (11)\n(12) (13) (14)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cfb7/cfb7e94b-60d8-46b7-9637-0a517877ba1e.png\" style=\"width: 50%;\"></div>\nFigure 2: The solid black line depicts the span of W. The true parameters corresponding to tasks A and B are given by \u03b2 \u2208W and O\u03b2. The gray lines depict the set of solutions to XA\u03b2 = y and XB\u03b2 = y. The estimators \u02c6\u03b2A, \u02c6\u03b2B, \u02c6\u03b2BA are given by orthogonal projections of an initialization on the respective consistent solutions. The red and blue ellipses depict lines of constant risk for tasks A and B, respectively.\n<div style=\"text-align: center;\">Figure 2: The solid black line depicts the span of W. The true parameters corresponding to tasks A and B are given by \u03b2 \u2208W and O\u03b2. The gray lines depict the set of solutions to XA\u03b2 = y and XB\u03b2 = y. The estimators \u02c6\u03b2A, \u02c6\u03b2B, \u02c6\u03b2BA are given by orthogonal projections of an initialization on the respective consistent solutions. The red and blue ellipses depict lines of constant risk for tasks A and B, respectively.</div>\nSee Appendix D for the derivation of (15)\u2013(18). It follows from showing that the latent space model described above is equivalent to an anisotropic regression model where XA has i.i.d. rows XAi \u223cN(0, \u03a3) and labels y = XA\u03b2 + \u03f5 where \u03f5 \u223cN(0, \u03c32In).\nSee Appendix D for the derivation of (15)\u2013(18). It follows from showing that the latent space model described above is equivalent to an anisotropic regression model where XA has i.i.d. rows XAi \u223cN(0, \u03a3) and labels y = XA\u03b2 + \u03f5 where \u03f5 \u223cN(0, \u03c32In). We aim to bound R(f \u02c6\u03b2BA) relative to R(f \u02c6\u03b2A). Figure 2 illustrates the estimators \u02c6\u03b2A, \u02c6\u03b2B, \u02c6\u03b2BA and curves of constant risk. As depicted, if p is large enough, \u02c6\u03b2BA has low risk on Task A and Task B simultaneously.\nWe aim to bound R(f \u02c6\u03b2BA) relative to R(f \u02c6\u03b2A). Figure 2 illustrates the estimators \u02c6\u03b2A, \u02c6\u03b2B, \u02c6\u03b2BA and curves of constant risk. As depicted, if p is large enough, \u02c6\u03b2BA has low risk on Task A and Task B simultaneously.\n# 2.2 Analogy of Linear Model to Neural Networks\nThe linear model we study is intended to be a mathematically tractable idealization of a neural network, and it is meant to analytically illustrate that overparameteriation can ameliorate catastrophic forgetting. The analogy of this linear model and neural network training on image data is as follows: Natural images in a neural network\u2019s training distribution can be (approximately) modeled as being on a nonlinear manifold and having a low-dimensional latent representation. Instead of observing the latent representation of an image, the neural network only sees a high-dimensional representation either directly in pixel space or perhaps in a representation computed from pixel space. Either of these representations contain noise in the features used for prediction. Responses can be approximated by a neural network. In our linear model, the low-dimensional representation of an input image is in a d-dimensional latent feature linear space. The responses are linear in the latent features. We assume the response is noiseless for the sake of simplicity, though our results could be extended to the noisy case. In our linear model, predictions are made off of a p-dimensional model given by noisy random projections of the latent features. We constrain W to have orthonormal columns which is a mathematical idealization of Gaussian measurements. We study two tasks with the same responses like in the permutation task setup, but for mathematical convenience we study tasks that are related by a random orthogonal transformation instead.\n# 2.3 Numerical Experiment\nBefore we establish our theoretical result about the system described in Section 2.1, we provide empirical evidence that the latent space linear regression model above exhibits the phenomenon\nthat overparameterization can ameliorate catastrophic forgetting. Specifically, we provide empirical evidence that R(f \u02c6\u03b2BA) \u2212R(f \u02c6\u03b2A) decreases with p. Let d = 20, n = 100, \u03b3 = 1, \u03b20 = \u20d70, and \u03b8 \u223cN(0, Id). We plot R(f \u02c6\u03b2BA), R(f \u02c6\u03b2A), R(f\u03b20) as a function of p \u2208(n, 2000) averaged over 100 samplings of W, XA, O, u.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e483/e483fd03-da24-49db-b282-8a328f73be50.png\" style=\"width: 50%;\"></div>\nFigure 3: Risk as a function of model complexity p for model (2)\u2013(14). The dashed line depics the null risk, corresponding to the zero estimator. Note the log scale on the vertical axis. Result of simulated numerical experiment for random orthogonal transformation tasks. Dotted black line denotes risk of null estimator, blue line denotes risk of estimator trained on task A, orange line denotes risk of estimator trained on task A then task B.\nFigure 3 shows the results of the experiment. We first note that both \u02c6\u03b2A and \u02c6\u03b2BA outperform the null risk, given by \u03b2 = 0. The null risk, R(f\u03b20), defines a baseline that any reasonable model must beat. We also observe that R(f \u02c6\u03b2A) and R(f \u02c6\u03b2BA) are decreasing with p in the overparameterized regime, and that the difference between these risks appears to decrease for increasing p. Note the log-scale of the vertical-axis. This provides evidence that catastrophic forgetting is alleviated in the overparameterized regime in our two-task learning setup.\n# 2.4 Main Result\nOur main result is an upper bound on the performance drop, defined as R(f \u02c6\u03b2A) \u2212R(f \u02c6\u03b2BA), for the two-task latent space linear regression model described above and inspired by the double descent literature. As described in Section 2.1, we consider (2)\u2013(14), where W satisfies the following assumption. Assumption 2.1. All non-zero singular values of W are equal. Namely, W \u22a4W = p\u03b3Id. We begin with a proposition that defines the unlearned baseline for the problem. Proposition 2.2. Fix \u03b8 \u2208Rd. Let W \u2208Rp\u00d7d satisfy Assumption 2.1. Then\n# R(f0) = \u2225\u03b8\u22252.\nThis risk calculation agrees with the numerical experiment in Section 2.3 where \u2225\u03b8\u22252 \u2248d = 20. The result is formally stated and proven in Lemma E.3. For our main result, we prove that if the number of examples exceeds the problem\u2019s latent dimensionality, if the number of parameters is sufficiently large relative to the number of examples and relative to the noise level of the observable features, then with high probability, the performance drop is small. Theorem 2.3. Fix \u03b8 \u2208Rd. Let tasks A, B be given by (2)\u2013(14). Let W \u2208Rp\u00d7d satisfy Assumption 2.1 and n \u2265d, p \u2265max(17n, 1/\u03b3). Then there exists constant c > 0 such that with probability at least 1 \u221210e\u2212cd, the following holds:\nTheorem 2.3 provides an upper bound on the amount of risk gained on task A after subsequential training on task B given by two terms. The first term provides dependence on the overparameterization ratio p/n and decreases as overparameterization becomes more extreme. The second term is given by the signal to noise ratio of the noisy features. This term dominates only when \u03b3 \u226a1/\u221anp. Based on the theorem, we observe that the overparameterization needs only to be linear in n to achieve a negligible performance drop in unregularized sequential task training compared to the baseline of \u2225\u03b8\u22252 in Proposition 2.2. This shows that catastrophic forgetting is ameliorated in the overparameterized regime. This result is formally stated in Lemma E.11. A formal proof and supporting lemmas are supplied in Appendix E. We provide a proof sketch here to outline the techniques used.\n# 2.5 Proof Sketch of Theorem 2.3\nFor readability, we write XA as A and XB as B. As shown in Appendix D, the latent space model described above is equivalent to an anisotropic regression model where A has i.i.d. rows Ai \u223cN(0, \u03a3) and labels y = A\u03b2 + \u03f5 where \u03f5 \u223cN(0, \u03c32In).\nAfter substituting the closed form solutions for \u02c6\u03b2A, \u02c6\u03b2BA, distributing terms, and applying simple Cauchy-Schwarz and triangle inequalities, we get the following bound:\nR(f \u02c6\u03b2BA) \u2212R(f \u02c6\u03b2A) \u22648p\u03b3\u221ap\u03b3 p\u03b3 + 1 \u2225\u03b8\u2225\u2225PW \u02c6\u03b2B\u2225+ 14\u221ap\u03b3\u2225\u03b8\u2225\u2225PB\u22a4\u02c6\u03b2A\u2225+ 12 p\u03b3 (p\u03b3 + 1)2 \u2225\u03b8\u22252 (21 = I + II + III (22\nLemmas E.5, E.6 establish results for orthogonal transformations to help bound \u2225PW \u02c6\u03b2B\u2225, \u2225PB\u22a4\u02c6\u03b2A\u2225 respectively. PW is a projection onto a d-dimensional space which scales the norm in I by d/p. PB\u22a4is a projection onto an n-dimensional space which scales the norm in II by n/p. As d \u2264n by assumption, II dominates I in the final bound. Using these results and simplifying gives the following bound with probability at least 1 \u221210e\u2212cd for constant c > 0:\nWe directly obtain\n# 3 Discussion\nOverparameterization is a necessity for continual learning so that there can exist an infinity of potentia optima for each task [13]. This makes it likely that there exists an optimum for some task B that is close to the solutions of some task A. We provide experimental evidence that overparameterization can provide additional benefits in combatting catastrophic forgetting for neural networks solving\n(22)\n(23)\n(24)\npermutation tasks. We use a linear model with clear analogies to neural networks in order to study this behavior theoretically. In our analysis of the linear model in the overparameterized regime, non-asymptotic matrix estimates and results for orthogonal transformations provide bounds on the performance drop. Our main result shows that, under our model, catastrophic forgetting is ameliorated for sufficiently large overparameterization. For the linear setting we study, the behavior we observe can be explained geometrically: overparameterization causes the random orthogonal transformation tasks to live in approximately orthogonal subspaces, so training on subsequent tasks does not interrupt performance on learned tasks. We view the present work as helping to establish initial results for continual learning theory. Before the field can rigorously understand machine learning algorithms in practice, the behavior of simple systems should be well understood. In particular, the behavior of linear systems with only vanilla SGD is the most natural initial result. Our work remarks that future theory should establish that continual learning algorithms beat not only a moderately parameterized baseline, but also the performance of extremely overparameterized models. First we address the concern for using permutation tasks as realistic benchmarks for continual learning methods. Researchers believe that permutation tasks only provide a best-case for real world scenarios [9]. Also, on a number of image classification datasets, MLPs do not experience forgetting when only two permutation tasks are being learned [18]. Our experiments confirm this effect while also showing that overparameterization mitigates the observable forgetting on 10 task permuted MNIST. Despite these critiques, we use permutation tasks as a launching point for theory because each task is of the same \u2018difficulty\u2019 and is amenable to mathematical analysis. Next we discuss our choice to study the problem with a linear model. Linear regression is the simplest setting, for which we know, that exhibits double descent. The consensus of several works that study double descent in linear models is that the risk of a model is monotonically decreasing in the overparameterized regime with respect to number of parameters only if the data has low effective dimension and high ambient dimension compared to the number of training samples [6, 1, 10]. In order to have a model that has monotonically decreasing performance drop for a particular continual learning problem, it is a necessity that it exhibits monotonically decreasing risk on a single task. Additionally in recent work, connections have been made between neural networks and linear models using the so-called neural tangent kernel (NTK) phenomenon [12]. The parameterization of a neural network can be so large that training only changes its parameters slightly from its initialization, resulting in functions that can be accurately approximated linearly. Hence it is reasonable that the analysis of linear models can explain the behavior of neural networks. We now remark at a technical level two choices in our analysis. The first is why we studied the case of random orthogonal transformation tasks instead of permutation tasks. The empirical performance between orthogonal and permutation tasks is similar; they both create tasks that are equally \u2019difficult\u2019 for an MLP to learn, which spares us from needing to quantify problem difficulty. Appendix C provides evidence that permutation and orthogonal transformation tasks have the same difficulty in the linear setting. Also, the mathematical analysis is easier when studying orthogonal transformation tasks. With random orthogonal transformations, any subspace gets mapped to a random subspace, for which the values of coefficients are typically well spread out. With random permutations, some subspaces (e.g. those aligned with the standard basis elements) do not exhibit the same spreading effect, making the technical analysis more involved. Secondly, we do not present a bound on R(f \u02c6\u03b2A), though it is expected to approach zero for large p, as suggested by Figure 3. Whether or not this risk goes to 0 in p, the performance drop goes to 0 in p while the null estimator remains with constant risk. So the regression problem is being solved arbitrarily well for sufficiently large p. With the growing popularity of continual learning, much of recent work is focused on developing new algorithms to mitigate catastrophic forgetting [13, 22, 20, 16]. Only a few papers study the problem theoretically [14, 4, 8, 11, 5]. [14] uses set theory to prove that, in general, continual learning problems are NP-hard, explaining why generative replay methods perform so well. [4] uses the NTK regime to prove generalisation guarantees for an existing continual learning method. [8] uses an NTK overlap matrix to define a notion of task similarity and show that catastrophic forgetting is more severe when tasks have high similarity. [11] studies a family of continual learning methods that uses approximations of the Hessian to determine parameter importance, presenting scenarios where continual learning provably fails and succeeds. [5] shows that a number of regularization techniques that seem to be derived from differing philosophies actually all study a variation of\nthe Fisher information matrix. While prior work has presented generalization bounds for existing continual learning techniques, our work illustrates the relationship between overparameterization and generalization.\nthe Fisher information matrix. While prior work has presented generalization bounds for existing continual learning techniques, our work illustrates the relationship between overparameterization and\nA natural next step is to study the regimes in which catastrophic forgetting is most problematic. This includes the setting where tasks do not have a nearly orthogonal relationship but also when data does not necessarily live on a low-dimensional manifold. We are also interested in understanding how the ideas of this paper generalize to other continual learning benchmarks and for more general neural network architectures. Prior work found experimental evidence that catastrophic forgetting is most severe not when tasks are very dissimilar but when they only have an intermediate level of similarity [19]. Using orthogonality as a proxy for task similarity, this agrees with our work that shows that nearly orthogonal tasks are less prone to catastrophic forgetting. An interesting future work would be to formalize this notion of task similarity for our model. Moving forward, one goal of theory in continual learning is to be able to analytically compare algorithms. Our work provides a foundation of understanding this behavior in a simple linear regression setting. In order to push this work forward, either non-linear models need to be studied or tasks that are related by something more complex than permutations.\n# References\nTable 1: Hyperparameters for the MNIST experiments\n<div style=\"text-align: center;\">Table 1: Hyperparameters for the MNIST experiments</div>\nHyperparameter\nSI\nEWC\nSGD\nlearning rate\n0.1\n0.01\n0.1\ndropout\n0\n0\n0.5\nbatch size\n64\n128\n64\nepochs / dataset\n5\n20\n5\nc\n0.1\n\u03be\n0.1\n\u03bb\n150\nfisher sample size\n1,000\n# A Description of Continual Learning Techniques\nSynaptic Intelligence (SI) is a regularization technique that assigns to each parameter of the network an estimate of importance for learned tasks [22]. This weight is determined in an online manner by tracking the amount that each parameter contributed to the decrease in loss during training. The weight is then used to penalize changes to the network parameters during subsequent training in the form of a regularization term added to the loss function.\nSynaptic Intelligence (SI) is a regularization technique that assigns to each parameter of the network an estimate of importance for learned tasks [22]. This weight is determined in an online manner by tracking the amount that each parameter contributed to the decrease in loss during training. The weight is then used to penalize changes to the network parameters during subsequent training in the form of a regularization term added to the loss function. Elastic Weight Consolidation (EWC) is a regularization technique that determines the importance of network weights using an estimation of the Fisher Information Matrix [13]. Near a minimum of the loss function, the diagonals of the Fisher matrix act as an estimate of the second order derivative of the loss with respect to each parameter. The magnitude of this derivative is used as a proxy for how sensitive the loss function is to fluctuation of the parameter. Constraining parameters according to their corresponding Fisher diagonal entries shows as an effective way of retaining the values of important weights from previous tasks while training on new ones.\n# B MNIST Experiments\nTable 1 reports the hyperparameters used in the MNIST experiments. We adopted the same hyperparameters for SI as in the original paper [22]. To our surprise, EWC with default hyperparameters [13] did not compete with SI. A basic grid search gave us a model that was more competitive. Blank entries mean that the hyperparameter is not relevant for the particular method. Curves for w = 7, 9 are omitted due to computational constraints in computing Fisher matrix estimates.\n# C Permuted Numerical Experiment\nFigure 4 shows the result of the numerical experiment in Figure 3 but with a random permutation matrix instead of a random orthogonal matrix. We observe that the same behavior holds in this scenario.\n# D Equivalence of Models and Derivation of Risk\nRecall in Section 2.1 where we defined the LSM model for linear regression. In this section we show that LSM is equivalent to an anisotropic regression model (ARM). We then use ARM to define the risk expression that we analyze theoretically. We begin by defining ARM. Define data matrix X \u2208Rn\u00d7p and responses y = X\u03b2 + \u03f5 where \u03f5 \u223c N(0, \u03c32In), \u03c32 = \u03b8\u22a4(W \u22a4W + Id)\u22121\u03b8, and \u03b2 = (I + WW \u22a4)\u22121W\u03b8 for some \u03b8 \u2208Rd, W \u2208Rp\u00d7d. Let rows Xi be independent random vectors in Rp with covariance \u03a3 = WW \u22a4+ Ip. Then the model is defined by the distribution over (X, y). Next we show that ARM is equivalent to LSM. First observe that for both models, (yi, x\u22a4 i ) \u2208Rp+1 are centered Gaussian vectors. Thus to show that they induce the same distribution, it suffices to show that they have the same covariance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/70b7/70b7f466-5fc2-46ed-8a9c-9f660adb5d24.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Result of simulated numerical experiment for random permutation tasks. Dotted black line denotes risk of null estimator, blue line denotes risk of estimator trained on task A, orange line denotes risk of estimator trained on task A then task B.</div>\nWe then compute the covariance matrices for each model. Under LSM, we have:\nE[y2 i ] = E(\u03b8\u22a4zi)(z\u22a4 i \u03b8) = \u03b8\u22a4I\u03b8 E[yixi] = E(Wzi + ui)(z\u22a4 i \u03b8) = W\u03b8 E[xix\u22a4 i ] = E(Wxi + ui)(z\u22a4 i W \u22a4+ ui) = \nPlugging these quantities into (25) gives:\nCov((yi, x\u22a4 i )\u22a4) = \ufffd \u2225\u03b8\u22252 (W\u03b8)\u22a4 W\u03b8 I + WW \u22a4 \ufffd\nUnder ARM, we have:\nE[y2 i ] = E(\u03b2\u22a4xi + \u03f5i)(x\u22a4 i \u03b2 + \u03f5i) = \u03b2\u22a4(I + WW \u22a4)\u03b2 + \u03c32 E[yixi] = E(xi(x\u22a4 i \u03b2 + \u03f5i)) = E(xix\u22a4 i \u03b2 + xi\u03f5i) = (I + WW \u22a4)\u03b2 E[xix\u22a4 i ] = I + WW \u22a4\nPlugging these quantities into (25) gives:\n(25)\n(26) (27) (28)\n(29)\n(30) (31) (32)\n(33)\nBy Lemma E.12, (I + WW \u22a4)\u22121W = W(I + W \u22a4W)\u22121. This gives\n\u03b2\u22a4(I + WW \u22a4)\u03b2 + \u03c32 = \u03b8\u22a4(W \u22a4W(I + W \u22a4W)\u22121 + (I + W \u22a4W)\u22121)\u03b8 = \u03b8\u22a4(W \u22a4W + I)(I + W \u22a4W)\u22121\u03b8 = \u03b8\u22a4\u03b8 = \u2225\u03b8\u22252\nNext we show equivalence of the second row, first column entries of the covariance matrices:\nNext we show equivalence of the second row, first column entries of the covariance matrices:\n(I + WW \u22a4)\u03b2 = (I + WW \u22a4)(I + WW \u22a4)\u22121W\u03b8 = W\u03b8\nThe equivalence of the first row, second column entries also follows from this equality. The equivalence of the second row, second column entries is trivial. Finally we derive the expression for the risk of ARM. By definition, the risk of an estimator f with parameters \u02c6\u03b2 has the following form:\nwhere the third equality holds from independence of \u03f5 and the sixth equality holds by definition of covariance. We choose to study ARM with a slightly different but equivalent expression for \u03b2. Using Lemma E.12, \u03b2 = (I + WW \u22a4)\u22121W\u03b8 = W(W \u22a4W + I)\u22121\u03b8.\nWe choose to study ARM with a slightly different but equivalent expression for \u03b2. Using Lemma E.12, \u03b2 = (I + WW \u22a4)\u22121W\u03b8 = W(W \u22a4W + I)\u22121\u03b8.\n# E Supporting Lemmas\nWe begin with an assumption, inspired by [10], that all non-zero singular values of W are equal. Assumption E.1. All non-zero singular values of W are equal. Namely, W \u22a4W = p\u03b3Id. Lemma E.2. Assume W \u2208Rp\u00d7d satisfies Assumption E.1. Then\nwhere PW is the orthogonal projection onto the range of W. Proof. We have that\nWW \u22a4= WW \u22a4p\u03b3 p\u03b3 = p\u03b3W \ufffd1 p\u03b3 Ip \ufffd W \u22a4\n(36) (37) (38)\n(39)\n(46)\n(47)\nW \u22a4W has full rank with probability 1, so W(W \u22a4W)\u22121W \u22a4is given explicitly by PW , which completes the proof. Lemma E.3. Let \u03a3 = WW \u22a4+ Ip where W \u2208Rp\u00d7d satisfies Assumption E.1. For some \u03b8 \u2208Rd, let \u03b2 = W(W \u22a4W + Id)\u22121\u03b8. Then\nProof. We have that\nBy Lemma E.2, \u03a3 = p\u03b3PW + Ip where PW is the orthogonal projection onto the range of W, which gives\nSince \u03b2 \u2208range(W),\nLemma E.4. Let x \u223cN(0, Id), and \u03f5 \u22641, then\n\ufffd where c > 0 is an absolute constant.\nProof. This statement follows from Corollary 5.17 in [21], concerning concentration of subexponential random variables. Lemma E.5. Assume W \u2208Rp\u00d7d satisfies Assumption E.1. Let O be a random p \u00d7 p orthogonal matrix. Fix v \u2208Rp\u00d7p. Then, with probability at least 1 \u22122e\u2212c1d,\nfor some universal constant c1 > 0.\nProof. Let x = Ov, and note that \u2225x\u2225= \u2225v\u2225, \u2225x\u2225> 0 with probability 1 and x \u2225x\u2225\u223cUniform(Sp\u22121). Letting z \u223cN(0, Ip), we have that\n(49)\n(50) (51)\n(52)\n(57)\n(58)\nwhere the symbol d= means equality in distribution. Applying Lemma E.4 twice, we get that for any \u03f5 < 1, with probability at least 1 \u2212e\u2212c\u03f52p \u2212e\u2212c\u03f52d,\nfor some universal constant c > 0. By choosing suitable \u03f5, we obtain that for c1 = c\u03f52, with probability at least 1 \u22122e\u2212c1d, \u2225PW Ov\u22252 \u22642d p \u2225v\u22252. Lemma E.6. Define A \u2208Rn\u00d7p with rows Ai as independent random vectors in Rp with covariance \u03a3 = WW \u22a4+ Ip where W \u2208Rp\u00d7d satisfies Assumption E.1. Let O be a random p \u00d7 p orthogonal matrix. Fix v \u2208range(A\u22a4). Then with probability at least 1 \u22122e\u2212c1n\nfor some universal constant c1 > 0.\nProof. We have that\n\u2225POA\u22a4PA\u22a4v\u2225= \u2225OPA\u22a4O\u22a4PA\u22a4v\u2225= \u2225PA\u22a4O\u22a4PA\u22a4v\u2225\nwhere z \u223cN(0, Ip) and the last equality follows from the rotational invariance of O. Applying Lemma E.4 twice, we get that for any \u03f5 < 1, with probability at least 1 \u2212e\u2212c\u03f52p \u2212e\u2212c\u03f52n,\nwhere z \u223cN(0, Ip) and the last equality follows from the rotational invariance of O. Applying Lemma E.4 twice, we get that for any \u03f5 < 1, with probability at least 1 \u2212e\u2212c\u03f52p \u2212e\u2212c\u03f52n,\nfor some universal constant c > 0. By choosing suitable \u03f5, we obtain that for c1 = c\u03f52, with probability at least 1 \u22122e\u2212c1n, \u2225POA\u22a4v\u22252 \u22642n p \u2225v\u22252. Lemma E.7. Let a \u2208Rp be generated by N(0, \u03a3), \u03a3 = WW \u22a4+ Ip where W \u2208Rp\u00d7d satisfies Assumption E.1. Then\nProof. It holds that E\u2225a\u22252 2 = \u2225\u03a31/2\u22252 F [21].\n\u2225\u03a3\u2225\u2217= d(p\u03b3 + 1) + p \u2212d = dp\u03b3 + p\n(59)\n(61)\n(65)\n(66)\n(67)\nLemma E.8. Define A \u2208Rn\u00d7p with rows Ai independent random vectors in Rp with covariance \u03a3 = p\u03b3PW +Ip where W \u2208Rp\u00d7d satisfies Assumption E.1. Then with probability at least 1\u22122e\u2212n,\n\ufffd Proof. WLOG let range(W) = span(e1, ..., ed). Then we can decompose A into two pieces: A(1) \u2208Rn\u00d7d with i.i.d. N(0, p\u03b3 + 1) entries and A(2) \u2208Rn\u00d7p\u2212d with i.i.d. N(0, 1) entries. This gives\nBy Theorem 5.39 in [21], \u03c3min(A\u22a4 (2))2 \u2265(\u221ap \u2212d \u22122\u221an)2 with probability at least 1 \u22122e\u2212n.\nBy Theorem 5.39 in [21], \u03c3min(A\u22a4 (2))2 \u2265(\u221ap \u2212d \u22122\u221an)2 with probability at least 1 \u22122e\u2212n. Lemma E.9. Define A \u2208Rn\u00d7p with rows Ai independent random vectors in Rp with covariance \u03a3 = WW \u22a4+ I where W \u2208Rp\u00d7d satisfies Assumption E.1. Let \u03f5 \u223cN(0, \u03c32I) and \u03c32 =\nBy Theorem 5.39 in [21], \u03c3min(A\u22a4 (2))2 \u2265(\u221ap \u2212d \u22122\u221an)2 with probability at least 1 \u22122e\u2212n. Lemma E.9. Define A \u2208Rn\u00d7p with rows Ai independent random vectors in Rp with covariance \u03a3 = WW \u22a4+ Ip where W \u2208Rp\u00d7d satisfies Assumption E.1. Let \u03f5 \u223cN(0, \u03c32In) and \u03c32 = \u03b8\u22a4(W \u22a4W + Id)\u22121\u03b8 for some \u03b8 \u2208Rd. Then with probability at least 1 \u22122e\u2212n,\n \u2265 \u2212 \u2212 \u2212 Lemma E.9. Define A \u2208Rn\u00d7p with rows Ai independent random vectors in Rp with covariance \u03a3 = WW \u22a4+ Ip where W \u2208Rp\u00d7d satisfies Assumption E.1. Let \u03f5 \u223cN(0, \u03c32In) and \u03c32 = \u03b8\u22a4(W \u22a4W + Id)\u22121\u03b8 for some \u03b8 \u2208Rd. Then with probability at least 1 \u22122e\u2212n,\nProof. We have that\nWe have that \u2225\u03f5\u22252 = n\u03c32 = n\u03b8\u22a4(W \u22a4W +Id)\u22121\u03b8. Under Assumption E.1, this gives \u2225\u03f5\u22252 = n\u2225\u03b8\u22252 p\u03b3+1 ,\nIt holds that \u2225A\u2020\u2225= 1/\u03c3min(A\u22a4) where \u03c3min(A\u22a4) is the smallest singular value of A\u22a4. By Lemma E.8, \u03c3min(A) \u2265\u221ap \u2212d \u22122\u221an with probability at least 1 \u22122e\u2212n. This gives\nLemma E.10. Suppose W \u2208Rp\u00d7d satisfies Assumption E.1. Let \u03b2 = W(W \u22a4W + Id)\u22121\u03b8 for some \u03b8 \u2208Rd. If p \u22651/\u03b3 and p \u226516n + d, then\nProof. We have that \u2225\u03b2\u22252 = \u03b8\u22a4(W \u22a4W + Id)\u22121W \u22a4W(W \u22a4W + Id)\u03b8. Using Assumption E.1, this gives \u2225\u03b2\u2225= \u221ap\u03b3 p\u03b3+1\u2225\u03b8\u2225. Suppose p \u22651/\u03b3, then we have that \u2225\u03b2\u2225\u2265 1 2\u221ap\u03b3 \u2225\u03b8\u2225. When p \u226516n + d, \u221an \u221ap\u2212d\u22122\u221an \u22641 2, which gives\n(69)\n(70) (71)\n(73)\n(74)\n(75)\nTheorem E.11. Define data matrix A \u2208Rn\u00d7p and responses y = A\u03b2 + \u03f5 where \u03f5 \u223cN(0, \u03c32In), \u03c32 = \u03b8\u22a4(W \u22a4W + Id)\u22121\u03b8, and \u03b2 = W(W \u22a4W + Id)\u22121\u03b8 for some \u03b8 \u2208Rd. Let rows Ai be independent random vectors in Rp with covariance \u03a3 = WW \u22a4+ Ip where W \u2208Rp\u00d7d follows Assumption E.1 and n \u2265d, p \u2265max(17n, 1/\u03b3). Let O be a random p \u00d7 p orthogonal matrix and B = AO\u22a4. Let \u02c6\u03b2A be the parameters of the minimum norm estimator on A, and \u02c6\u03b2BA be the parameters of the estimator on B using \u02c6\u03b2A as initialization. Let R(f \u02c6\u03b2) be the risk on task A of an estimator with parameters \u02c6\u03b2. Then there exists constant c > 0 such that with probability at least 1 \u221210e\u2212cd, the following holds:\nProof. We have that\nDistributing terms with \u02c6\u03b2B and PB\u22a4\u02c6\u03b2A gives\nBy Lemma E.2, \u03a3 = p\u03b3PW + Ip where PW is the orthogonal projection onto the range of W. This implies that \u2225\u03a3\u2225= p\u03b3 + 1, giving\nR(f \u02c6\u03b2BA) \u2212R(f \u02c6\u03b2A) = 2\u02c6\u03b2\u22a4 A(p\u03b3PW + Ip)\u02c6\u03b2B \u22122\u02c6\u03b2\u22a4 B(p\u03b3PW + Ip)\u03b2 + \u02c6\u03b2\u22a4 B(p\u03b3PW + Ip)\u02c6\u03b2B + (PB\u22a4\u02c6\u03b2A)\u22a4(p\u03b3PW + Ip)(PB\u22a4\u02c6\u03b2A \u22122\u02c6\u03b2A \u22122\u02c6\u03b2B + 2\u03b2) (8 \u22642p\u03b3 \u02c6\u03b2\u22a4 APW \u02c6\u03b2B + 2\u02c6\u03b2\u22a4 A \u02c6\u03b2B \u22122p\u03b3 \u02c6\u03b2\u22a4 BPW \u03b2 \u22122\u02c6\u03b2\u22a4 B\u03b2 + p\u03b3 \u02c6\u03b2\u22a4 BPW \u02c6\u03b2B + \u02c6\u03b2\u22a4 B \u03b2 + (p\u03b3 + 1)\u2225PB\u22a4\u02c6\u03b2A\u2225\u2225PB\u22a4\u02c6\u03b2A \u22122\u02c6\u03b2A \u22122\u02c6\u03b2B + 2\u03b2\u2225 (8\nApplying Cauchy-Schwarz and triangle inequality gives the following bound:\nR(f \u02c6\u03b2BA) \u2212R(f \u02c6\u03b2A) \u22642p\u03b3\u2225\u02c6\u03b2A\u2225\u2225PW \u02c6\u03b2B\u2225+ 2\u2225\u02c6\u03b2A\u2225\u2225\u02c6\u03b2B\u2225+ 2p\u03b3\u2225\u03b2\u2225\u2225PW \u02c6\u03b2B\u2225+ 2\u2225\u02c6\u03b2B\u2225\u2225\u03b2\u2225 + p\u03b3\u2225\u02c6\u03b2B\u2225\u2225PW \u02c6\u03b2B\u2225+ \u2225\u02c6\u03b2B\u22252 + (p\u03b3 + 1)\u2225PB\u22a4\u02c6\u03b2A\u2225(\u2225PB\u22a4\u02c6\u03b2A\u2225+ 2\u2225\u02c6\u03b2A\u2225+ 2\u2225\u02c6\u03b2B\u2225+ 2\u2225\u03b2\u2225) (8\n\n(78)\n(80)\n(81)\n(82)\n(83)\n(84)\nBy definition in Section 2, \u02c6\u03b2A = PA\u22a4\u03b2 + A\u2020\u03f5 and \u02c6\u03b2B = OPA\u22a4\u03b2 + OA\u2020\u03f5 and it holds that \u2225PA\u22a4\u03b2\u2225\u2264\u2225\u03b2\u2225. By Lemma E.9, \u2225A\u22a4(AA\u22a4)\u22121\u03f5\u2225\u2264 \u221an\u2225\u03b8\u2225 \u221ap\u03b3(\u221ap\u2212d\u22122\u221an) with probability at least 1 \u22122e\u2212n (call this Event E). So by Lemma E.10 if p \u2265max(17n, 1/\u03b3), then \u2225\u02c6\u03b2A\u2225\u22642\u2225\u03b2\u2225and \u2225\u02c6\u03b2B\u2225\u22642\u2225\u03b2\u2225, which gives\nWe have that \u2225\u03b2\u22252 = \u03b8\u22a4(W \u22a4W + Id)\u22121W \u22a4W(W \u22a4W + Id)\u03b8. Using Assumption E.1, this gives \u2225\u03b2\u2225= \u221ap\u03b3 p\u03b3+1\u2225\u03b8\u2225,\nR(f \u02c6\u03b2BA) \u2212R(f \u02c6\u03b2A) \u22648p\u03b3\u221ap\u03b3 p\u03b3 + 1 \u2225\u03b8\u2225\u2225PW \u02c6\u03b2B\u2225+ 14\u221ap\u03b3\u2225\u03b8\u2225\u2225PB\u22a4\u02c6\u03b2A\u2225+ 12 p\u03b3 (p\u03b3 + 1)2 \u2225\u03b8\u22252 (87) = I + II + III (88)\n  = I + II + III\n# We will bound each of these terms separately, starting with I:\nSubstituting \u02c6\u03b2B = B\u22a4(BB\u22a4)\u22121y into this expression and distributing accordingly, we get that PW \u02c6\u03b2B = PW OPA\u22a4\u03b2 + PW OA\u22a4(AA\u22a4)\u22121\u03f5,\nBy Lemma E.5, there exists constant c1 > 0 such that \u2225PW OPA\u22a4\u03b2\u2225\u22641.5 \ufffd d p\u2225\u03b2\u2225and \u2225PW OA\u22a4(AA\u22a4)\u22121\u03f5\u2225\u22641.5 \ufffd d p\u2225A\u22a4(AA\u22a4)\u22121\u03f5\u2225with probability at least 1 \u22122e\u2212c1d each. By Lemma E.9, \u2225A\u22a4(AA\u22a4)\u22121\u03f5\u2225\u2264 \u221an\u2225\u03b8\u2225 \u221ap\u03b3(\u221ap\u2212d\u22122\u221an) (failure probability already accounted for on Event E). This gives the following bound with probability at least 1 \u22124e\u2212c1d:\nSubstituting \u2225\u03b2\u2225= \u221ap\u03b3 p\u03b3+1\u2225\u03b8\u2225gives\nUsing p\u03b3 + 1 > p\u03b3 gives\nSubstituting \u02c6\u03b2A = A\u22a4(AA\u22a4)\u22121y into this expression and distributing accordingly, we get that PB\u22a4\u02c6\u03b2A = PB\u22a4PA\u22a4\u03b2 + PB\u22a4A\u22a4(AA\u22a4)\u22121\u03f5. This gives the following bound:\n(91)\n(93)\nBy Lemma E.6, there exists constant c2 > 0 such that \u2225PB\u22a4PA\u22a4\u03b2\u2225\u22641.5 \ufffd n p \u2225\u03b2\u2225and \u2225PB\u22a4A\u22a4(AA\u22a4)\u22121\u03f5\u2225\u22641.5 \ufffd n p \u2225A\u22a4(AA\u22a4)\u22121\u03f5\u2225with probability at least 1 \u22122e\u2212c2n each. By Lemma E.9, \u2225A\u22a4(AA\u22a4)\u22121\u03f5\u2225\u2264 \u221an\u2225\u03b8\u2225 \u221ap\u03b3(\u221ap\u2212d\u22122\u221an) (failure probability already accounted for on Event E). This gives the following bound with probability 1 \u22124e\u2212c2n:\nSubstituting \u2225\u03b2\u2225= \u221ap\u03b3 p\u03b3+1\u2225\u03b8\u2225gives\nUsing p\u03b3 + 1 > p\u03b3 gives\nLastly we bound term III. Using p\u03b3 + 1 > p\u03b3 gives the following bound:\nPutting all three terms together gives the following bound with probability 1 \u221210e\u2212cd where c = min(c1, c2):\nc = min(c1, c2):\nUsing d \u2264n gives the following bound:\n(94) (95)\n(96)\n(97)\n(99)\n(100)\n(103)\n(104)\n\nProof. Let W = USV be the SVD of W where U \u2208Rp\u00d7d, S \u2208Rd\u00d7d, V \u2208Rd\u00d7d. Then we have\nLet \u02dcU \u2208Rp\u00d7p have the first d columns be U and the last p\u2212d columns be the rest of the orthonormal basis. Then we have\n(105)\n(106) (107)\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of catastrophic forgetting in neural networks during continual learning, emphasizing the role of overparameterization in improving generalization performance.",
        "problem": {
            "definition": "The catastrophic forgetting problem refers to the significant drop in performance on previously learned tasks when a neural network is trained on new tasks.",
            "key obstacle": "The main challenge is that neural networks, when trained greedily to minimize loss, often forget previously learned information without regularization."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that overparameterization allows models to maintain performance across multiple tasks.",
            "opinion": "The authors propose that overparameterization can mitigate the effects of catastrophic forgetting in continual learning settings.",
            "innovation": "The primary improvement is demonstrating that overparameterization can achieve performance gains comparable to state-of-the-art continual learning algorithms without requiring complex regularization techniques."
        },
        "Theory": {
            "perspective": "The theoretical analysis is based on a linear regression model that simulates the behavior of neural networks under overparameterization.",
            "opinion": "The view is that sufficient overparameterization can lead to minimal performance drop when transitioning between tasks.",
            "proof": "The proof involves establishing a non-asymptotic bound on the performance drop when training on two tasks related by a random orthogonal transformation."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using permuted MNIST tasks with multilayer perceptrons of varying widths, comparing performance with vanilla SGD, Synaptic Intelligence (SI), and Elastic Weight Consolidation (EWC).",
            "evaluation method": "The evaluation involved training models on multiple tasks and measuring test accuracy, particularly observing the performance drop associated with catastrophic forgetting."
        },
        "conclusion": "The paper concludes that overparameterization significantly alleviates catastrophic forgetting in neural networks, providing a theoretical foundation for this observation.",
        "discussion": {
            "advantage": "The main advantage is that overparameterization provides a simple yet effective means to improve continual learning performance without complex regularization.",
            "limitation": "One limitation is that the theoretical results are based on idealized linear models, which may not fully capture the complexities of real-world neural networks.",
            "future work": "Future research could explore the effects of overparameterization in more complex neural network architectures and other continual learning benchmarks."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "4.3",
            "key information": "The catastrophic forgetting problem refers to the significant drop in performance on previously learned tasks when a neural network is trained on new tasks."
        },
        {
            "section number": "4.3",
            "key information": "The main challenge is that neural networks, when trained greedily to minimize loss, often forget previously learned information without regularization."
        },
        {
            "section number": "4.1",
            "key information": "The authors propose that overparameterization can mitigate the effects of catastrophic forgetting in continual learning settings."
        },
        {
            "section number": "4.1",
            "key information": "The primary improvement is demonstrating that overparameterization can achieve performance gains comparable to state-of-the-art continual learning algorithms without requiring complex regularization techniques."
        },
        {
            "section number": "4.1",
            "key information": "Theoretical analysis is based on a linear regression model that simulates the behavior of neural networks under overparameterization."
        },
        {
            "section number": "4.1",
            "key information": "Sufficient overparameterization can lead to minimal performance drop when transitioning between tasks."
        },
        {
            "section number": "4.2",
            "key information": "The experiments were conducted using permuted MNIST tasks with multilayer perceptrons of varying widths, comparing performance with vanilla SGD, Synaptic Intelligence (SI), and Elastic Weight Consolidation (EWC)."
        },
        {
            "section number": "4.2",
            "key information": "The evaluation involved training models on multiple tasks and measuring test accuracy, particularly observing the performance drop associated with catastrophic forgetting."
        },
        {
            "section number": "4",
            "key information": "Overparameterization significantly alleviates catastrophic forgetting in neural networks, providing a theoretical foundation for this observation."
        }
    ],
    "similarity_score": 0.60903217400871,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime.json"
}