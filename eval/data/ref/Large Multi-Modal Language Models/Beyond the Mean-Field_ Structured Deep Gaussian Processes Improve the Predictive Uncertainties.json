{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2005.11110",
    "title": "Beyond the Mean-Field: Structured Deep Gaussian Processes Improve the Predictive Uncertainties",
    "abstract": "Deep Gaussian Processes learn probabilistic data representations for supervised learning by cascading multiple Gaussian Processes. While this model family promises flexible predictive distributions, exact inference is not tractable. Approximate inference techniques trade off the ability to closely resemble the posterior distribution against speed of convergence and computational efficiency. We propose a novel Gaussian variational family that allows for retaining covariances between latent processes while achieving fast convergence by marginalising out all global latent variables. After providing a proof of how this marginalisation can be done for general covariances, we restrict them to the ones we empirically found to be most important in order to also achieve computational efficiency. We provide an efficient implementation of our new approach and apply it to several benchmark datasets. It yields excellent results and strikes a better balance between accuracy and calibrated uncertainty estimates than its state-of-the-art alternatives.",
    "bib_name": "lindinger2020meanfieldstructureddeepgaussian",
    "md_text": "# Beyond the Mean-Field: Structured Deep Gaussian Processes Improve the Predictive Uncertainties\n# Abstract\nDeep Gaussian Processes learn probabilistic data representations for supervised learning by cascading multiple Gaussian Processes. While this model family promises flexible predictive distributions, exact inference is not tractable. Approximate inference techniques trade off the ability to closely resemble the posterior distribution against speed of convergence and computational efficiency. We propose a novel Gaussian variational family that allows for retaining covariances between latent processes while achieving fast convergence by marginalising out all global latent variables. After providing a proof of how this marginalisation can be done for general covariances, we restrict them to the ones we empirically found to be most important in order to also achieve computational efficiency. We provide an efficient implementation of our new approach and apply it to several benchmark datasets. It yields excellent results and strikes a better balance between accuracy and calibrated uncertainty estimates than its state-of-the-art alternatives.\n# 1 Introduction\nGaussian Processes (GPs) provide a non-parametric framework for learning distributions over unknown functions from data [21]: As the posterior distribution can be computed in closed-form, they return well-calibrated uncertainty estimates, making them particularly useful in safety critical applications [3, 22], Bayesian optimisation [10, 30], active learning [37] or under covariate shift [31]. However, the analytical tractability of GPs comes at the price of reduced flexibility: Standard kernel functions make strong assumptions such as stationarity or smoothness. To make GPs more flexible, a practitioner would have to come up with hand-crafted features or kernel functions. Both alternatives require expert knowledge and are prone to overfitting. Deep Gaussian Processes (DGPs) offer a compelling alternative since they learn non-linear feature representations in a fully probabilistic manner via GP cascades [6]. The gained flexibility has the drawback that inference can no longer be carried out in closed-form, but must be performed via Monte Carlo sampling [9], or approximate inference techniques [5, 6, 24]. The most popular approximation, variational inference, searches for the best approximate posterior within a pre-defined class of distributions: the variational family [4]. For GPs, variational approximations often build on the inducing point framework where a small set of global latent variables acts as pseudo datapoints summarising the training data [29, 32]. For DGPs, each latent GP is governed by its own set of inducing variables, which, in general, need not be independent from those of other latent GPs. Here, we offer a new class of variational families for DGPs taking the following two requirements into account: (i) all global latent variables, i.e., inducing outputs, can be marginalised out, (ii) correlations between latent GP models can be captured. Satisfying (i) reduces the variance in the estimators and is needed for fast convergence [16] while (ii) leads to better calibrated uncertainty estimates [33].\nnference on Neural Information Processing Systems (NeurIPS 2020), Vancouver\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/09a4/09a48c9a-8a35-493a-869e-7b34d3ce1a76.png\" style=\"width: 50%;\"></div>\nBy using a fully-parameterised Gaussian variational posterior over the global latent variables, we automatically fulfil (ii), and we show in Sec. 3.1, via a proof by induction, that (i) can still be achieved. The proof is constructive, resulting in a novel inference scheme for variational families that allow for correlations within and across layers. The proposed scheme is general and can be used for arbitrarily structured covariances allowing the user to easily adapt it to application-specific covariances, depending on the desired DGP model architecture and on the system requirements with respect to speed, memory and accuracy. One particular case, in which the variational family is chain-structured, has also been considered in a recent work [34], in which the compositional uncertainty in deep GP models is studied. In Fig. 1 (right) we depict exemplary inferred covariances between the latent GPs for a standard deep GP architecture. In addition to the diagonal blocks, the covariance matrix has visible diagonal stripes in the offdiagonal blocks and an arrow structure. These diagonal stripes point towards strong dependencies between successive latent GPs, while the arrow structure reflects\nFigure 1: Covariance matrices for variational posteriors. We used a DGP with 2 hidden layers (L1, L2) of 5 latent GPs each and a single GP in the output layer (L3). The complexity of the variational approximation is increased by allowing for additional dependencies within and across layers in a Gaussian variational family (left: meanfield [24], middle: stripes-and-arrow, right: fully-coupled). Plotted are natural logarithms of the absolute values of the variational covariance matrices over the inducing outputs.\ndependencies between all hidden layers and the output layer. In Sec. 3.2, we further propose a scalable approximation to this variational family, which only takes these stronger correlations into account (Fig. 1, middle). We provide efficient implementations for both variational families, where we particularly exploit the sparsity and structure of the covariance matrix of the variational posterior. In Sec. 4, we show experimentally that the new algorithm works well in practice. Our approach obtains a better balance between accurate predictions and calibrated uncertainty estimates than its competitors, as we showcase by varying the distance of the test from the training points.\n# 2 Background\nIn the following, we introduce the notation and provide the necessary background on DGP models. GPs are their building blocks and the starting point of our review.\n# 2.1 Primer on Gaussian Processes\nIn regression problems, the task is to learn a function f : RD \u2192R that maps a set of N input points xN = {xn}N n=1 to a corresponding set of noisy outputs yN = {yn}N n=1. Throughout this work, we assume iid noise, p(yN|fN) = \ufffdN n=1 p(yn|fn), where fn = f(xn) and fN = {fn}N n=1 are the function values at the input points. We place a zero mean GP prior on the function f, f \u223cGP(0, k), where k : RD \u00d7 RD \u2192R is the kernel function. This assumption leads to a multivariate Gaussian prior over the function values, p(fN) = N (fN|0, KNN) with covariance matrix KNN = {k(xn, xn\u2032)}N n,n\u2032=1. In preparation for the next section, we introduce a set of M \u226aN so-called inducing points xM = {xm}M m=1 from the input space1 [29, 32]. From the definition of a GP, the corresponding inducing outputs fM = {fm}M m=1, where fm = f(xm), share a joint multivariate Gaussian distribution with fN. We can therefore write the joint density as p(fN, fM) = p(fN|fM)p(fM), where we factorised the joint prior into p(fM) = N (fM|0, KMM), the prior over the inducing outputs, and the conditional p(fN|fM) = N \ufffd fN \ufffd\ufffd\ufffd\ufffdKNMfM, \ufffdKNN \ufffd with\n\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd \ufffd \ufffdKNM = KNM (KMM)\u22121 , \ufffdKNN = KNN \u2212KNM (KMM)\u22121 KMN.\n\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd \ufffd \ufffdKNM = KNM (KMM)\u22121 , \ufffdKNN = KNN \u2212KNM (KMM)\u22121 KMN. ere the matrices K are defined similarly as KNN above, e.g. KNM = {k(xn, xm)}N,M n,m=1.\n\ufffd \ufffd Here the matrices K are defined similarly as KNN above, e.g. KNM = {k(xn, xm)}N,M n,m=1.\n(1)\nA deep Gaussian Process (DGP) is a hierarchical composition of GP models. We consider a model with L layers and Tl (stochastic) functions in layer l = 1, . . . , L, i.e., a total number of T = \ufffdL l=1 Tl functions [6]. The input of layer l is the output of the previous layer, f l N = [f l,1(f l\u22121 N ), . . . , f l,Tl(f l\u22121 N )], with starting values f 0 N = xN. We place independent GP priors augmented with inducing points on all the functions, using the same kernel kl and the same set of inducing points xl M within layer l. This leads to the following joint model density:\nHere p(f l M) = \ufffdTl t=1 N \ufffd f l,t M \ufffd\ufffd\ufffd0, Kl MM \ufffd and p(f l N|f l M; f l\u22121 N ) = \ufffdTl t=1 N \ufffd f l,t N \ufffd\ufffd\ufffd\ufffdKl NMf l,t M , K\n \ufffd   \ufffd \ufffd\ufffd\ufffd \ufffd  \ufffd   \ufffd e \ufffdKl NM and \ufffdKl NN are given by the equivalents of Eq. (1), respectively.2\n  \ufffd   \ufffd    \ufffd  \ufffd Inference in this model (2) is intractable since we cannot marginalise over the latents f 1 N, . . . , f L\u22121 N as they act as inputs to the non-linear kernel function. We therefore choose to approximate the posterior by employing variational inference: We search for an approximation q(fN, fM) to the true posterior p(fN, fM|yN) by first choosing a variational family for the distribution q and then finding an optimal q within that family that minimises the Kullback-Leibler (KL) divergence KL[q||p]. Equivalently, the so-called evidence lower bound (ELBO),\n\ufffd can be maximised. In the following, we choose the variational family [24\n\ufffd Note that fM = {f l,t M }L,Tl l,t=1 contains the inducing outputs of all layers, which might be covarying. This observation will be the starting point for our structured approximation in Sec. 3.1. In the remaining part of this section, we follow Ref. [24] and restrict the distribution over the inducing outputs to be a-posteriori Gaussian and independent between different GPs (known as mean-field assumption, see also Fig. 1, left), q(fM) = \ufffdL l=1 \ufffdTl t=1 q(f l,t M ). Here q(f l,t M ) = N \ufffd f l,t M \ufffd\ufffd\ufffd\u00b5l,t M, Sl,t M \ufffd and \u00b5l,t M, Sl,t M are free variational parameters. The inducing outputs fM act thereby as global latent variables that capture the information of the training data. Plugging q(fM) into Eqs. (2), (3), (4), we can simplify the ELBO to\nNote that fM = {f l,t M }L,Tl l,t=1 contains the inducing outputs of all layers, which might be covarying. This observation will be the starting point for our structured approximation in Sec. 3.1.\n\ufffd \ufffd We first note that the ELBO decomposes over the data points, allowing for minibatch subsampling [12]. However, the marginals of the output of the final layer, q(f L n ), cannot be obtained analytically. While the mean-field assumption renders it easy to analytically marginalise out the inducing outputs (see Appx. D.1), the outputs of the intermediate layers cannot be fully integrated out, since they are kernel inputs of the respective next layer, leaving us with\nq(f L n ) = \ufffd L \ufffd l=1 q(f l n; f l\u22121 n )df 1 n \u00b7 \u00b7 \u00b7 df L\u22121 n , where q(f l n; f l\u22121 n ) = Tl \ufffd t=1 N \ufffd f l,t n \ufffd\ufffd\ufffd\ufffd\u00b5l,t n , \ufffd\u03a3l,t n \ufffd . (6\nq(f L n ) = \ufffd \ufffd l=1 q(f l n; f l\u22121 n )df 1 n \u00b7 \u00b7 \u00b7 df L\u22121 n , where q(f l n; f l\u22121 n ) = l \ufffd t=1 N \ufffd f l,t n \ufffd\ufffd\ufffd\ufffd\u00b5l,t n , \ufffd\u03a3l,t n \ufffd . (6)\n\ufffd The means and covariances are given by\n\ufffd  covariances are given by\n\ufffd  \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd 2In order to avoid pathologies created by highly non-injective mappings in the DGP [7], we follow Ref. [24] and add non-trainable linear mean terms given by the PCA mapping of the input data to the latent layers. Those terms are omitted from the notation for better readability.\n(2)\nf L\u22121 N\n(3)\n(4)\n(5)\n(6)\n(7)\nWe can straightforwardly obtain samples from q(f L n ) by recursively sampling through the layers using Eq. (6). Those samples can be used to evaluate the ELBO [Eq. (5)] and to obtain unbiased gradients for parameter optimisation by using the reparameterisation trick [16, 23]. This stochastic estimator of the ELBO has low variance as we only need to sample over the local latent parameters f 1 n, . . . , f L\u22121 n , while we can marginalise out the global latent parameters, i.e. inducing outputs, fM.\n# 3 Structured Deep Gaussian Processes\nNext, we introduce a new class of variational families that allows to couple the inducing outputs fM within and across layers. Surprisingly, analytical marginalisation over the inducing outputs fM is still possible after reformulating the problem into a recursive one that can be solved by induction. This enables an efficient inference scheme that refrains from sampling any global latent variables. Our method generalises to arbitrary interactions which we exploit in the second part where we focus on the most prominent ones to attain speed-ups.\n# 3.1 Fully-Coupled DGPs\nWe present now a new variational family that offers both, efficient computations and expressivity: Our approach is efficient, since all global latent variables can be marginalised out , and expressive, since we allow for structure in the variational posterior. We do this by leaving the Gaussianity assumption unchanged, while permitting dependencies between all inducing outputs (within layers and also across layers). This corresponds to the (variational) ansatz q(fM) = N (fM|\u00b5M, SM) with dimensionality TM. By taking the dependencies between the latent processes into account, the resulting variational posterior q(fN, fM) [Eq. (4)] is better suited to closely approximate the true posterior. We give a comparison of exemplary covariance matrices SM in Fig. 1. Next, we investigate how the ELBO computations have to be adjusted when using the fully-coupled variational family. Plugging q(fM) into Eqs. (2), (3) and (4), yields\nwhich we derive in detail in Appx. C. The major difference to the mean-field DGP lies in the marginals q(f L n ) of the outputs of the last layer: Assuming (as in the mean-field DGP) that the distribution over the inducing outputs fM factorises between the different GPs causes the marginalisation integral to factorise into L standard Gaussian integrals. This is not the case for the fullycoupled DGP (see Appx. D.1 for more details), which makes the computations more challenging. The implications of using a fully coupled q(fM) are summarised in the following theorem.\nfor each data point xn. The means and covariances are given by\n \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd where \ufffd\u00b5l n = \ufffdKl nM\u00b5l M and \ufffdSll\u2032 n = \u03b4ll\u2032Kl nn \u2212\ufffdKl nM \ufffd \u03b4ll\u2032Kl MM \u2212Sll\u2032 M \ufffd \ufffdKl\u2032 Mn.\n \ufffd  \ufffd  \ufffd \ufffd \ufffd \ufffd \ufffd In Eqs. (10) and (11) the notation Al,1:l\u2032 is used to index a submatrix of the variable A, e.g. Al,1:l\u2032 = \ufffd Al,1 \u00b7 \u00b7 \u00b7 Al,l\u2032\ufffd . Additionally, \u00b5l M \u2208RTlM denotes the subvector of \u00b5M that contains the means of the inducing outputs in layer l, and Sll\u2032 M \u2208RTlM\u00d7Tl\u2032M contains the covariances between the inducing outputs of layers l and l\u2032. For \ufffd\u00b5l n and \ufffdSll\u2032 n , we introduced the notation Kl = \ufffd ITl \u2297Kl\ufffd as\n(10)\n(11)\nshorthand for the Kronecker product between the identity matrix ITl and the covariance matrix Kl, and used \u03b4 for the Kronecker delta. We verify in Appx. B.2 that the formulas contain the mean-field solution as a special case by plugging in the respective covariance matrix. By Thm. 1, the inducing outputs fM can still be marginalised out, which enables low-variance estimators of the ELBO. While the resulting formula for q(f l n|f 1 n, . . . , f l\u22121 n ) has a similar form as Gaussian conditionals, this is only true at first glance (cf. also Appx. B.1): The latents of the preceding layers f 1:l\u22121 n enter the mean \u02c6\u00b5l n and the covariance matrix \u02c6\u03a3l n also in an indirect way via \ufffdSn as they appear as inputs to the kernel matrices.\n\ufffd Sketch of the proof of Theorem 1. We start the proof with the general formula for q(f L n ),\nwhich is already (implicitly) used in Ref. [24] and which we derive in Appx. D. In order to show the equivalence between the inner integral in Eq. (12) and the integrand in Eq. (9) we proceed to find a recursive formula for integrating out the inducing outputs layer after layer:\nThe equation above holds for l = 1, . . . , L after the inducing outputs of layers 1, . . . , l have already been marginalised out. This is stated more formally in Lem. 2 in Appx. A, in which we also provide exact formulas for all terms. Importantly, all of them are multivariate Gaussians with known mean and covariance. The lemma itself can be proved by induction and we will show the general idea of the induction step here: For this, we assume the right hand side of Eq. (13) to hold for some layer l and then prove that it also holds for l \u2192l + 1. We start by taking the (known) distribution within the integral and split it in two by conditioning on f l n:\nThen we show that the distribution q(f l n|f 1:l\u22121 n ) can be written as part of the product in front of the integral in Eq. (13) (thereby increasing the upper limit of the product to l). Next, we consider the integration over f l+1 M , where we collect all relevant terms (thereby increasing the lower limit of the product within the integral in Eq. (13) to l + 2): \ufffd \ufffd\nThe terms in the first line are given by Eqs. (14) and (2). All subsequent terms are also multivariate Gaussians that are obtained by standard operations like conditioning, joining two distributions, and marginalisation. We can therefore give an analytical expression of the final term in Eq. (15), which is exactly the term that is needed on the right hand side of Eq. (13) for l \u2192l + 1. Confirming that this term has the correct mean and covariance completes the induction step. After proving Lem. 2, Eq. (13) can be used. For the case l = L the right hand side can be shown to yield \ufffdL l=1 q(f l n|f 1 n, . . . , f l\u22121 n ). Hence, Eq. (9) follows by substituting the inner integral in Eq. (12) by this term. The full proof can be found in Appx. A. Furthermore, we give a heuristic argument for Thm. 1 in Appx. B.1 in which we show that by ignoring the recursive structure of the prior, the marginalisation of the inducing outputs fM becomes straightforward. While mathematically not rigorous, the derivation provides additional intuition. Next, we use our novel variational approach to fit a fully coupled DGP model with L = 3 layers to the concrete UCI dataset. We can clearly observe that this algorithmic work pays off: Fig. 1 shows that there is more structure in the covariance matrix SM than the mean-field approximation allows. This additional structure results in a better approximation of the true posterior as we validate on a\n(12)\n(13)\n(14)\n(15)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6b90/6b90dc8b-f27f-403a-ae87-2df19f87ad7f.png\" style=\"width: 50%;\"></div>\nFigure 2: Convergence behaviour: Analytical vs. MC marginalisation. We plot the ELBO as a function of time in seconds when the marginalisation of the inducing outputs fM is performed analytically via our Thm. 1 (purple) and via MC sampling (green). We used a fully-coupled DGP with our standard three layer architecture (see Sec. 3.2), on the concrete UCI dataset trained with Adam [15].\nrange of benchmark datasets (see Tab. S5 in Appx. G) for which we observe larger ELBO values for the fully-coupled DGP than for the mean-field DGP. Additionally, we show in Fig. 2 that our analytical marginalisation over the inducing outputs fM leads to faster convergence compared to Monte Carlo (MC) sampling, since the corresponding ELBO estimates have lower variance. Independently from our work, the sampling-based approach has also been proposed in Ref. [34]. However, in comparison with the mean-field DGP, the increase in the number of variational parameters also leads to an increase in runtime and made convergence with standard optimisers fragile due to many local optima. We were able to circumvent the latter by the use of natural gradients [2], which have been found to work well for (D)GP models before [10, 26, 25], but this increases the runtime even further (see Sec. 4.2). It is therefore necessary to find a smaller variational family if we want to use the method in large-scale applications. An optimal variational family combines the best of both worlds, i.e., being as efficient as the meanfield DGP while retaining the most important interactions introduced in the fully-coupled DGP. We want to emphasise that there are many possible ways of restricting the covariance matrix SM that potentially lead to benefits in different applications. For example, the recent work [34] studies the compositional uncertainty in deep GPs using a particular restriction of the inverse covariance matrix. The authors also provide specialised algorithms to marginalise out the inducing outputs in their model. Here, we provide an analytic marginalisation scheme for arbitrarily structured covariance matrices that will vastly simplify future development of application-specific covariances. Through the general framework that we have developed, testing them is straightforward and can be done via simply implementing a naive version of the covariance matrix in our code.3 In the following, we propose one possible class of covariance matrices based on our empirical findings.\n# 3.2 Stripes-and-Arrow Approximation\nIn this section, we describe a new variational family that trades off efficiency and expressivity by sparsifying the covariance matrix SM. Inspecting Fig. 1 (right) again, we observe that besides the M \u00d7 M blocks on the diagonal, the diagonal stripes [28] (covariances between the GPs in latent layers at the same relative position), and an arrow structure (covariances from every intermediate layer GP to the output GP) receive large values. We make similar observations also for different datasets and different DGP architectures as shown in Fig. S8 in Appx. G. Note that the stripes pattern can also be motivated theoretically as we expect the residual connections realised by the mean functions (footnote 2) to lead to a coupling between successive latent GPs. We therefore propose as one special form to keep only these terms and neglect all other dependencies by setting them to zero in the covariance matrix, resulting in a structure consisting of an arrowhead and diagonal stripes (see Fig. 1 middle). Denoting the number of GPs per latent layer as \u03c4, it is straightforward to show that the number of non-zero elements in the covariance matrices of mean-field DGP, stripes-and-arrow DGP, and fullycoupled DGP scale as O(\u03c4LM 2), O(\u03c4L2M 2), and O(\u03c4 2L2M 2), respectively. In the example of Fig. 1, we have used \u03c4 = 5, L = 3, and M = 128, yielding 1.8 \u00d7 105, 5.1 \u00d7 105, and 2.0 \u00d7 106 non-zero elements in the covariance matrices. Reducing the number of parameters already leads to shorter training times since less gradients need to be computed. Furthermore, the property that makes this form so compelling is that the covariance matrix \ufffdS1:l\u22121,1:l\u22121 n [needed in Eqs. (10) and\n \ufffd 3Python code (building on code for the mean-field DGP [25], GPflow [19] and TensorFlow [1]) imp menting our method is provided at https://github.com/boschresearch/Structured_DGP. pseudocode description of our algorithm is given in Appx. F.\n(11)] as well as the Cholesky decomposition4 of SM have the same sparsity pattern. Therefore only the non-zero elements at pre-defined positions have to be calculated which is explained in Appx. E. The complexity for the ELBO is O(NM 2\u03c4L2 + N\u03c4 3L3 + M 3\u03c4L3). This is a moderate increase compared to the mean-field DGP whose ELBO has complexity O(NM 2\u03c4L), while it is a clear improvement over the fully-coupled approach with complexity O(NM 2\u03c4 2L2+N\u03c4 3L3+M 3\u03c4 3L3) (see Appx. E for derivations). An empirical runtime comparison is provided in Sec. 4.2. After having discussed the advantages of the proposed approximation a remark on a disadvantage is in order: The efficient implementation of Ref. [26] for natural gradients cannot be used in this setting, since the transformation from our parameterisation to a fully-parameterised multivariate Gaussian is not invertible. However, this is only a slight disadvantage since the stripes-and-arrow approximation has a drastically reduced number of parameters, compared to the fully-coupled approach, and we experimentally do not observe the same convergence problems when using standard optimisers (see Appx. G, Fig. S5).\n# 3.3 Joint sampling of global and local latent variables\nIn contrast to our work, Refs. [9, 36] drop the Gaussian assumption over the inducing outputs fM and allow instead for potentially multi-modal approximate posteriors. While their approaches are arguably more expressive than ours, their flexibility comes at a price: the distribution over the inducing outputs fM is only given implicitly in form of Monte Carlo samples. Since the inducing outputs fM act as global latent parameters, the noise attached to their sampling-based estimates affects all samples from one mini-batch. This can often lead to higher variances which may translate to slower convergence [16]. We compare to Ref. [9] in our experiments.\n# 4 Experiments\nIn Sec. 4.1, we study the predictive performance of our stripes-and-arrow approximation. Since it is difficult to assess accuracy and calibration on the same task, we ran a joint study of interpolation and extrapolation tasks, where in the latter the test points are distant from the training points. We found that the proposed approach balances accuracy and calibration, thereby outperforming its competitors on the combined task. Examining the results for the extrapolation task more closely, we find that our proposed method significantly outperforms the competing DGP approaches. In Sec. 4.2, we assess the runtime of our methods and confirm that our approximation has only a negligible overhead compared to mean-field and is more efficient than a fully-coupled DGP. Due to space constraints, we moved many of the experimental details to Appx. G.\n# 4.1 Benchmark Results\nWe compared the predictive performance of our efficient stripes-and-arrow approximation (STAR DGP) with a mean-field approximation (MF DGP) [24], stochastic gradient Hamiltonian Monte Carlo (SGHMC DGP) [9] and a sparse GP (SGP) [12]. As done in prior work, we report results on eight UCI datasets and employ as evaluation criterion the average marginal test log-likelihood (tll). We assessed the interpolation behaviour of the different approaches by randomly partitioning the data into a training and a test set with a 90 : 10 split. To investigate the extrapolation behaviour, we created test instances that are distant from the training samples: We first randomly projected the inputs X onto a one-dimensional subspace z = Xw, where the weights w \u2208RD were drawn from a standard Gaussian distribution. We subsequently ordered the samples w.r.t. z and divided them accordingly into training and test set using a 50 : 50 split. We first confirmed the reports from the literature [9, 24], that DGPs have on interpolation tasks an improved performance compared to sparse GPs (Tab. 1). We also observed that in this setting SGHMC outperforms the MF DGP and our method, which are on par. Subsequently, we performed the same analysis on the extrapolation task. While our approach, STAR DGP, seems to perform slightly better than MF DGP and also SGHMC DGP, the large standard errors of all methods hamper a direct comparison (see Tab. S3 in Appx. G). This is mainly due to the\nTable 1: Interpolation behaviour on UCI benchmark datasets. We report marginal tlls (the larger, the better) for various methods, where L denotes the number of layers. Standard errors are obtained by repeating the experiment 10 times. We marked all methods in bold that performed better or as good as the standard sparse GP.\nDataset\nSGP\nSGHMC DGP\nMF DGP\nSTAR DGP\n(N,D)\nL1\nL1\nL2\nL3\nL2\nL3\nL2\nL3\nboston (506,13)\n-2.58(0.10)\n-2.75(0.18)\n-2.51(0.07)\n-2.53(0.09)\n-2.43(0.05)\n-2.48(0.06)\n-2.47(0.08)\n-2.43(0.05)\nenergy (768, 8)\n-0.71(0.03)\n-1.16(0.44)\n-0.37(0.12)\n-0.34(0.11)\n-0.73(0.02)\n-0.75(0.02)\n-0.75(0.02)\n-0.75(0.02)\nconcrete (1030, 8)\n-3.09(0.02)\n-3.50(0.34)\n-2.89(0.06)\n-2.88(0.06)\n-3.06(0.03)\n-3.09(0.02)\n-3.04(0.02)\n-3.05(0.02)\nwine red (1599,11)\n-0.88(0.01)\n-0.90(0.03)\n-0.81(0.03)\n-0.80(0.07)\n-0.89(0.01)\n-0.89(0.01)\n-0.88(0.01)\n-0.88(0.01)\nkin8nm (8192, 8)\n1.05(0.01)\n1.14(0.01)\n1.38(0.01)\n1.25(0.14)\n1.30(0.01)\n1.31(0.01)\n1.28(0.01)\n1.29(0.01)\npower (9568, 4)\n-2.78(0.01)\n-2.75(0.02)\n-2.68(0.02)\n-2.65(0.02)\n-2.77(0.01)\n-2.76(0.01)\n-2.77(0.01)\n-2.77(0.01)\nnaval (11934,16)\n7.56(0.09)\n7.77(0.04)\n7.32(0.02)\n6.89(0.43)\n7.11(0.11)\n7.05(0.09)\n7.06(0.08)\n6.25(0.31)\nprotein (45730, 9)\n-2.91(0.00)\n-2.76(0.00)\n-2.64(0.01)\n-2.58(0.01)\n-2.83(0.00)\n-2.79(0.00)\n-2.83(0.00)\n-2.80(0.00)\nDataset\nMF vs. STAR\nSGHMC vs. STAR\nboston\n0.55(0.04)\n0.50(0.05)\nenergy\n0.73(0.05)\n0.60(0.04)\nconcrete\n0.57(0.04)\n0.60(0.03)\nwine red\n0.57(0.04)\n0.63(0.02)\nkin8nm\n0.36(0.03)\n0.44(0.05)\npower\n0.44(0.06)\n0.64(0.03)\nnaval\n0.67(0.06)\n0.58(0.03)\nprotein\n0.49(0.03)\n0.50(0.03)\nTable 2: Extrapolation behaviour: direct comparison of DGP methods. Average frequency \u00b5 and its standard error \u03c3 (computed over 10 repetitions) of the STAR DGP outperforming the MF DGP (left) and the SGHMC DGP (right) on the marginal tll of individual repetitions of the extrapolation task (see main text for details). Results are for DGPs with three layers. We mark numbers in bold (italics) if STAR outperforms its competitor (vice versa).\nrandom 1D-projection of the extrapolation experiment: The direction of the projection has a large impact on the difficulty of the prediction task. Since this direction changes over the repetitions, the corresponding test log-likelihoods vary considerably, leading to large standard errors. We resolved this issue by performing a direct comparison between STAR DGP and the other two DGP variants: To do so, we computed the frequency of test samples for which STAR DGP obtained a larger log-likelihood than MF/SGHMC DGP on each train-test split independently. Average frequency \u00b5 and its standard error \u03c3 were subsequently computed over 10 repetitions and are reported in Tab. 2. On 5/8 datasets STAR DGP significantly outperforms MF DGP and SGHMC DGP (\u00b5 > 0.50 + \u03c3), respectively, while the opposite only occurred on kin8nm. In Tab. S4 in Appx. G, we show more comparisons, that also take the absolute differences in test log likelihoods into account and additionally consider the comparison of fully-coupled and MF DGP. Taken together, we conclude that our structured approximations are in particular beneficial in the extrapolation scenario, while their performance is similar to MF DGP in the interpolation scenario. Next, we performed an in-depth comparison between the approaches that analytically marginalise the inducing outputs: In Fig. 3 we show that the predicted variance \u03c32 \u2217increased as we moved away from the training data (left) while the mean squared errors also grew with larger \u03c32 \u2217(right). The mean squared error is an empirical unbiased estimator of the variance Var\u2217= E[(y\u2217\u2212\u00b5\u2217)2] where y\u2217is the test output and \u00b5\u2217the mean predictor. The predicted variance \u03c32 \u2217is also an estimator of Var\u2217. It is only unbiased if the method is calibrated. However, we observed for the mean-field approach that, when moving away from the training data, the mean squared error was larger than the predicted variances pointing towards underestimated uncertainties. While the mean squared error for SGP matched well with the predictive variances, the predictions are rather inaccurate as demonstrated by the large predicted variances. Our method reaches a good balance, having generally more accurate mean predictions than SGP and at the same time more accurate variance predictions than MF DGP. Finally, we investigated the behaviour of the SGHMC approaches in more detail. We first ran a one-layer model that is equivalent to a sparse GP but with a different inference scheme: Instead of marginalising out the inducing outputs, they are sampled. We observed that the distribution over the inducing outputs is non-Gaussian (see Appx. G, Fig. S6), even though the optimal approximate posterior distribution is provably Gaussian in this case [32]. A possible explanation for this are convergence problems since the global latent variables are not marginalised out, which, in turn, offers a potential explanation for the poor extrapolation behaviour of SGHMC that we observed in our experiments across different architectures and datasets. Similar convergence problems have also been observed by Ref. [25].\nFigure 3: Calibration Study. Left: While the predicted variances increase for all methods as a function of the distance to the training data, we find that at any given distance, the uncertainty decreases from SGP to STAR DGP to MF DGP. Right: We plot the mean squared error as a function of the predicted variance. If the mean squared error is larger than the predicted variance, the latter underestimates the uncertainty. Results are recorded on the kin8nm UCI dataset and smoothed for plotting by using a median filter.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ffc8/ffc89bff-183c-4c76-b78b-2cf443ab8847.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0338/0338a3ff-c0a3-42a5-9423-b9f058354b87.png\" style=\"width: 50%;\"></div>\nFigure 4: Runtime comparison. We compare the runtime of our efficient STAR DGP versus the FC DGP and the MF DGP on the protein UCI dataset. Shown is the runtime of one gradient step in seconds on a logarithmic scale as a function of the number of inducing points M. The dotted grey lines show the theoretical runtime O(M 2).\n# 4.2 Runtime\nWe compared the impact of the variational family on the runtime as a function of the number of inducing points M. For the fully-coupled (FC) variational model, we also recorded the runtime when employing natural gradients [26]. The results can be seen in Fig. 4, where the order from fastest to slowest method was proportional to the complexity of the variational family: mean-field, stripes-and-arrow, fully-coupled DGP. For our standard setting, M = 128, our STAR approximation was only two times slower than the mean-field but three times faster than FC DGP (trained with Adam [15]). This ratio stayed almost constant when the number of inducing outputs M was changed, since the most important term in the computational costs scales as O(M 2) for all methods. Subsequently, we performed additional experiments in which we varied the architecture parameters L and \u03c4. Both confirm that the empirical runtime performance scales with the complexity of the variational family (see Appx. G, Fig. S7) and matches our theoretical estimates in Sec. 3.2.\n# 5 Summary\nIn this paper, we investigated a new class of variational families for deep Gaussian processes (GPs). Our approach is (i) efficient as it allows to marginalise analytically over the global latent variables and (ii) expressive as it couples the inducing outputs across layers in the variational posterior. Naively coupling all inducing outputs does not scale to large datasets, hence we suggest a sparse and structured approximation that only takes the most important dependencies into account. In a joint study of interpolation and extrapolation tasks as well as in a careful evaluation of the extrapolation task on its own, our approach outperforms its competitors, since it balances accurate predictions and calibrated uncertainty estimates. Further research is required to understand why our structured approximations are especially helpful for the extrapolation task. One promising direction could be to look at differences of inner layer outputs (as done in Ref. [34]) and link them to the final deep GP outputs. There has been a lot of follow-up work on deep GPs in which the probabilistic model is altered to allow for multiple outputs [14], multiple input sources [8], latent features [25] or for interpreting the latent states as differential flows [11]. Our approach can be easily adapted to any of these models and is therefore a promising line of work to advance inference in deep GP models. Our proposed structural approximation is only one way of coupling the latent GPs. Discovering new variational families that allow for more speed-ups either by applying Kronecker factorisations as done in the context of neural networks [18], placing a grid structure over the inducing inputs [13], or by taking a conjugate gradient perspective on the objective [35] are interesting directions for future research. Furthermore, we think that the dependence of the optimal structural approximation on various factors (model architecture, data properties, etc.) is worthwhile to be studied in more detail.\nIn many applications, machine learning algorithms have been shown to achieve superior predictive performance compared to hand-crafted or expert solutions [27]. However, these methods can be applied in safety-critical applications only if they return predictive distributions allowing to quantify the uncertainty of the prediction [17]. For instance, a medical diagnosis tool can be applied only if each diagnosis is endowed with a confidence interval such that in case of ambiguity a physician can be contacted. Our work yields accurate predictive distributions for deep non-parametric models by allowing correlations between and across layers in the variational posterior. As we validate in our experiments, this also holds true when the input distribution at test time differs from the input distribution at training time. In our medical example, this might be the case if the hospital where the data is recorded is different from the one where the diagnosis tool is deployed.\n# Acknowledgements\nWe thank Buote Xu for valuable comments and suggestions on an early draft of the paper. We furthermore acknowledge the detailed and constructive feedback from the four anonymous reviewers, particularly for suggesting a new experiment which lead to Fig. 2.\n# References\n[1] Mart\u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. In USENIX Symposium on Operating Systems Design and Implementation, 2016. [2] Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 1998. [3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00b4e. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016. [4] David Blei, Alp Kucukelbir, and Jon McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 2017. [5] Thang Bui, Daniel Hern\u00b4andez-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard Turner. Deep gaussian processes for regression using approximate expectation propagation. In International Conference on Machine Learning, 2016. [6] Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and Statistics, 2013. [7] David Duvenaud, Oren Rippel, Ryan P. Adams, and Zoubin Ghahramani. Avoiding pathologies in very deep networks. In Artificial Intelligence and Statistics, 2014. [8] Oliver Hamelijnck, Theodoros Damoulas, Kangrui Wang, and Mark Girolami. Multiresolution multi-task gaussian processes. In Advances in Neural Information Processing Systems, 2019. [9] Marton Havasi, Jos\u00b4e Lobato, and Juan Fuentes. Inference in deep gaussian processes using stochastic gradient hamiltonian monte carlo. In Advances in Neural Information Processing Systems, 2018. 10] Ali Hebbal, Loic Brevault, Mathieu Balesdent, El-Ghazali Talbi, and Nouredine Melab. Bayesian optimization using deep gaussian processes. arXiv preprint arXiv:1905.03350, 2019. 11] Pashupati Hegde, Markus Heinonen, Harri L\u00a8ahdesm\u00a8aki, and Samuel Kaski. Deep learning with differential gaussian process flows. In Artificial Intelligence and Statistics, 2019. 12] James Hensman, Nicolo Fusi, and Neil Lawrence. Gaussian processes for big data. Conference on Uncertainty in Artifical Intelligence, 2013. 13] Pavel Izmailov, Alexander Novikov, and Dmitry Kropotov. Scalable gaussian processes with billions of inducing inputs via tensor train decomposition. Artificial Intelligence and Statistics, 2018.\n[14] Markus Kaiser, Clemens Otte, Thomas Runkler, and Carl Henrik Ek. Bayesian alignments of warped multi-output gaussian processes. In Advances in Neural Information Processing Systems, 2018. [15] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference of Learning Representations, 2015. [16] Diederik Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, 2015. [17] Christian Leibig, Vaneeda Allken, Murat Sec\u00b8kin Ayhan, Philipp Berens, and Siegfried Wahl. Leveraging uncertainty information from deep neural networks for disease detection. Scientific reports, 2017. [18] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, 2015. [19] Alexander Matthews, Mark Van Der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo Le\u00b4on-Villagr\u00b4a, Zoubin Ghahramani, and James Hensman. Gpflow: A gaussian process library using tensorflow. Journal of Machine Learning Research, 2017. [20] Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade. Springer, 1998. [21] Carl Rasmussen and Christopher Williams. Gaussian processes for machine learning. The MIT Press, 2005. [22] David Reeb, Andreas Doerr, Sebastian Gerwinn, and Barbara Rakitsch. Learning gaussian processes by minimizing pac-bayesian generalization bounds. In Advances in Neural Information Processing Systems, 2018. [23] Danilo Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, 2014. [24] Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian processes. In Advances in Neural Information Processing Systems, 2017. [25] Hugh Salimbeni, Vincent Dutordoir, James Hensman, and Marc Peter Deisenroth. Deep gaussian processes with importance-weighted variational inference. International Conference on Machine Learning, 2019. [26] Hugh Salimbeni, Stefanos Eleftheriadis, and James Hensman. Natural gradients in practice: non-conjugate variational inference in gaussian process models. In Artificial Intelligence and Statistics, 2018. [27] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016. [28] Dennis Smolarski. Diagonally-striped matrices and approximate inverse preconditioners. Journal of Computational and Applied Mathematics, 2006. [29] Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems, 2006. [30] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, 2012. [31] Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D Sculley, Joshua Dillon, Jie Ren, and Zachary Nado. Can you trust your model\u2019s uncertainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, 2019. [32] Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artificial Intelligence and Statistics, 2009. [33] Richard Turner and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. Bayesian Time Series Models, 2011. [34] Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser, Erik Bodin, Neill D. F. Campbell, and Carl Henrik Ek. Compositional uncertainty in deep gaussian processes. In Conference on Uncertainty in Artifical Intelligence, 2020.\n[35] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q. Weinberger, and Andrew Gordon Wilson. Exact gaussian processes on a million data points. In Advances in Neural Information Processing Systems, 2019. [36] Haibin Yu, Yizhou Chen, Zhongxiang Dai, Kian Hsiang Low, and Patrick Jaillet. Implicit posterior variational inference for deep gaussian processes. In Advances in Neural Information Processing Systems, 2019. [37] Christoph Zimmer, Mona Meister, and Duy Nguyen-Tuong. Safe active learning for time-series modeling with gaussian processes. In Advances in Neural Information Processing Systems, 2018.\n# Beyond the Mean-Field: Structured Deep Gaussian Processes Improve the Predictive Uncertainties\n# Beyond the Mean-Field:\n# A Marginalisation of the inducing outputs (proof of Theorem 1)\nA Marginalisation of the inducing outputs (proof of Theorem 1)\nThe aim of this section is to provide a complete proof for Thm. 1. We will do this by starting from the formula q(f l n) that we work out in Appx. D, \ufffd \ufffd\n\ufffd Comparing to Eq. (9), we see that it remains to be shown that indeed\n\ufffd \ufffd where the distributions q on the right hand side have the properties described in Eqs. (9) - (11). The terms appearing on the left hand side are given by q(fM) = N (fM|\u00b5M, SM), which is interchangeably also denoted as \uf8eb \uf8f6 \uf8eb \uf8eb \uf8f6 \ufffd \uf8eb \uf8f6 \uf8eb \uf8f6 \uf8f6\nand\nand\nwhere\n\ufffd \ufffd \ufffd In order to show that Eq. (17) holds, we will introduce a rather technical lemma in the following and prove it later  induction. Lemma 2. Given the definitions in Eqs. (18) and (19), \u2200l = 1, . . . , L we have \ufffd \ufffd\n\ufffd \ufffd \ufffd In order to show that Eq. (17) holds, we will introduce a rather technical lemma in the following and prove it later b induction.\n\uf8ed \ufffd\ufffd\ufffd \uf8ed \ufffd \ufffd \uf8f8 \uf8f8 re \u02c6\u00b5l n and \u02c6\u03a3l n are as in Eqs. (10) and (11), respectively, and we defined l\u02c6\u00b5l+1:L M = \u00b5l+1:L M + Sl+1:L,1:l\u22121 M diag( \ufffdK1:l\u22121 Mn ) \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 (f 1:l\u22121 n \u2212\ufffd\u00b51:l\u22121 n ) l \u02c6\u03a3l+1:L,l+1:L M = Sl+1:L,l+1:L M \u2212Sl+1:L,1:l\u22121 M diag( \ufffdK1:l\u22121 Mn ) \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 diag( \ufffdK1:l\u22121 nM )S1:l\u22121,l+1: M l \u02c6\u03a3l,l+1:L nM = \ufffdKl nMSl,l+1:L M \u2212\ufffdSl,1:l\u22121 n \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 diag( \ufffdK1:l\u22121 nM )S1:l\u22121,l+1:L M .\n(17)\n(18)\n(20) (21)\n(22)\n(23)\n(24)\nIn the equations above we used diag(A1:l) to denote the formation of a block diagonal matrix, where the diagonal blocks are given by A1, . . . , Al. Note that while we only need one index to label \u02c6\u00b5l n and \u02c6\u03a3l n, we need several for the objects defined in Eq. (24). Take e.g. l \u02c6\u03a3l+1:L,l+1:L M : The upper left index denotes for which l the formula is valid (which will become important when we do the induction step l \u2192l+1). The upper right indices (try to) capture which terms of SM are most important for the definition, they have nothing to do with the dimensionality of the objects. (In fact, the matrix l \u02c6\u03a3M contains L \u2212l \u22121 \u00d7 L \u2212l \u22121 blocks of various sizes TlM \u00d7 Tl\u2032M.) This makes it easier later on when we do calculations with these objects. Before we prove Lem. 2, we will first show how its results can be used to prove Thm. 1:\nProof of Theorem 1. As shown in Appx. D, we can write\nObtaining a formula for the inner integral can be done using Lem. 2 with l = L, in which case Eq. (22) read\nPlugging this into Eq. (25) yields\nwhere the distributions q on the right hand side have the properties described in Eqs. (9) - (11). In order to prove Lem. 2, we will regularly need two standard formulas from Gaussian calculus, namely conditioni multivariate Gaussians, \ufffd\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd\ufffd\nwhere the distributions q on the right hand side have the properties described in Eqs. (9) - (11). In order to prove Lem. 2, we will regularly need two standard formulas from Gaussian calculus, namely conditionin multivariate Gaussians, N \ufffd\ufffd x y \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd a b \ufffd , \ufffdA C C\u22a4 B \ufffd\ufffd = N (x|a, A) N \ufffd y \ufffd\ufffdb + C\u22a4A\u22121(x \u2212a), B \u2212C\u22a4A\u22121C \ufffd , (29\n\ufffd\ufffd\ufffd and solving Gaussian integrals (\u201dpropagation\u201d):\n\ufffd N (x|a + Fy, A) N (y|b, B) dy = N \ufffd x \ufffd\ufffda + Fb, A + FBF \u22a4\ufffd\n\ufffd Proof of Lemma 2. As already said, we prove the lemma by induction: Base case We need to show that Eq. (22) holds for l = 1, i.e., that\n\ufffd q(f 1 M, . . . , f L M) L \ufffd l\u2032=1 p(f l\u2032 n |f l\u2032 M)df l\u2032 M = \ufffd q(f 1 n, f 2 M, . . . , f L M) L \ufffd l\u2032=2 p(f l\u2032 n |f l\u2032 M)df l\u2032 M,\nwhere q(f 1 n, f 2 M, . . . , f L M) is given according to Eqs. (23) and (24) In order to do so, we will perform the following steps:\n(25)\n(26)\n(27)\n(28)\n(29)\n(30)\n(31)\nv) Then we evaluate the integral:\n\ufffd q(f 1 M)q(f 1 n, f 2 M, . . . , f L M|f 1 M)df 1 M = q(f 1 n, f 2 M, . . . , f L M).\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd For step iii) we use the formula we just obtained for q(f 2 M, . . . , f L M|f 1 M) and additionally p(f 1 n|f 1 M), which, accord to Eq. (19) is given by N \ufffd f 1 n \ufffd\ufffd\ufffd\ufffdK1 nMf 1 M, \ufffdK1 nn \ufffd , and then proceed to build their joint Gaussian distribution:\n\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd In step iv) we perform the integration using the term above for the joint and q(f 1 M) = N \ufffd f 1 M \ufffd\ufffd\u00b51 M, S11 M \ufffd from Eq. (36) for the marginal. Applying Eq. (30) yields \ufffd q(f 1 n, f 2 M, . . . , f L M|f 1 M)q(f 1 M)df 1 M\n\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd In step iv) we perform the integration using the term above for the joint and q(f 1 M) = N \ufffd f 1 M \ufffd\ufffd\u00b51 M, S11 M \ufffd from Eq. (36) for the marginal. Applying Eq. (30) yields \ufffd\n\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd   \ufffd \ufffdK1 nn 0 0 S2:L,2:L M \u2212S2:L,1 M \ufffd S11 M \ufffd\u22121 S1,2:L M \ufffd + \ufffd \ufffdK1 nM S2:L,1 M \ufffd S11 M \ufffd\u22121 \ufffd S11 M \ufffd \ufffdK1 nM S2:L,1 M \ufffd S11 M \ufffd\u22121 \ufffd\u22a4\uf8f6 \uf8f8 \ufffd\n\ufffd\ufffd \ufffd In order to arrive at the last line we simplified the terms and used the definitions of \ufffd\u00b51 n and \ufffdS11 n in Thm. 1.\n(32)\n(33)\n(34)\n(35)\n(37)\n\ufffd\ufffd\ufffd \ufffd \ufffd which is the term q(f 1 n, f 2 M, . . . , f L M) on the RHS of Eq. (31). Plugging in the definitions from Eq. (24) we can easily verify that this last term indeed agrees with Eq. (38). Therefore our statement in Lem. 2 holds for l = 1. Inductive step We assume that Lemma 2 holds for some l = 1, . . . , L \u22121 (induction hypothesis) and then need to show that it also holds for l + 1. That is, assuming that \ufffd \ufffd\n\ufffd\ufffd\ufffd \ufffd \ufffd which is the term q(f 1 n, f 2 M, . . . , f L M) on the RHS of Eq. (31). Plugging in the definitions from Eq. (24) we can easily verify that this last term indeed agrees with Eq. (38). Therefore our statement in Lem. 2 holds for l = 1. Inductive step We assume that Lemma 2 holds for some (induction hypothesis) and then need to\nInductive step We assume that Lemma 2 holds for some l = 1, . . . , L \u22121 (induction hypothesis) and then need to show that it also holds for l + 1. That is, assuming that \ufffd \ufffd\n\ufffd \ufffd \ufffd holds for some l with the terms on the RHS given by Eqs. (23), and (24) we need to show that we can also write th previous equation as\n\ufffd \ufffd \ufffd holds for some l with the terms on the RHS given by Eqs. (23), and (24) we need to show that we can also write  previous equation as\n\ufffd \ufffd where this time the terms are given by Eqs. (23), and (24) but with l \u2192l + 1. The way to show this is very similar to the way we showed the base case, the resulting formulas will only look more complicated and we will need one additional step in the beginning:\n \u2192 The way to show this is very similar to the way we showed the base case, the resulting formulas will only look more complicated and we will need one additional step in the beginning:\no) Assuming that Eq. (40) holds for some l, we can start immediately with the RHS. The first step will be to marginalise f l n from the distribution q within the integral and show that the resulting marginal q(f l n|f 1:l\u22121 n ) has the right form to be written as part of the product in front of the integral: \ufffd \ufffd\n\ufffd \ufffd Having done this, we will have to do the exact same steps as in the base case, which we will repeat below wi updated indices.\ni) Continuing from Eq. (44), we isolate all terms that depend on f l+1 M : \ufffd \ufffd\nii) Comparing this to Eq. (41), we see that it remains to be shown that the inner integral equals q(f l+1 n , f l+2:L M |f 1: n  [given by Eqs. (23) and (24)]. Therefore we only consider the inner integral and therein condition q on f l+1 M : \ufffd \ufffd\n) Next, we obtain the joint distribution of the two terms that are conditioned on f l+1 M : \ufffd q(f l+1 M |f 1:l n )q(f l+2:L M |f 1:l n , f l+1 M )p(f l+1 n |f l+1 M )df l+1 M = \ufffd q(f l+1 M |f 1:l n )q(f l+1 n , f l+2:L M |f 1:l n , f l+1 M )df l+1 M .\n(40)\n(41)\n(42)\n(43)\n(44)\n(45)\n(46)\n(47)\nv) Finally, we check that the resulting distribution is given by Eqs. (23) and (24). This then proves the eq Eqs. (40) and (41).\nv) Finally, we check that the resulting distribution is given by Eqs. (23) and (24). This then proves the equality of Eqs. (40) and (41). Let us begin with step o): According to Eq. (23), we have\nLet us begin with step o): According to Eq. (23), we have \uf8eb \ufffd\n\uf8ed \ufffd\ufffd\ufffd \uf8ed \ufffd \ufffd which we condition on f l n using Eq. (29) (i.e., going from Eq. (42) to Eq. (43)): q(f l n, f l+1:L M |f 1:l\u22121 n ) = q(f l n|f 1:l\u22121 n )q(f l+1:L M |f 1:l n ) = N \ufffd f l n \ufffd\ufffd\ufffd\u02c6\u00b5l n, \u02c6\u03a3l n \ufffd \u00d7 \ufffd \ufffd\n\ufffd \ufffd\ufffd\ufffd \ufffd N \ufffd f l+1:L M \ufffd\ufffd\ufffd\ufffd l\u02c6\u00b5l+1:L M + \ufffd l \u02c6\u03a3l+1:L,l nM \ufffd\u22a4\ufffd \u02c6\u03a3l n \ufffd\u22121 (f l n \u2212\u02c6\u00b5l n), l \u02c6\u03a3l+1:L,l+1:L M \u2212 \ufffd l \u02c6\u03a3l+1:L,l nM \ufffd\u22a4\ufffd \u02c6\u03a3l n \ufffd\u22121 l \u02c6\u03a3l,l+1:L nM \ufffd . (50)\n\ufffd\ufffd\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd We therefore see that q(f l n|f 1:l\u22121 n ) = N \ufffd f l n \ufffd\ufffd\ufffd\u02c6\u00b5l n, \u02c6\u03a3l n \ufffd , which is the right form for it to be included in the product in front of the integral in Eq. (43). This lets us arrive at Eq. (44), hence finishing step o). In step i) nothing really happens, we just note that, according to Eq. (19), p(f l+1 n |f l+1 M ) = N \ufffd f l+1 n \ufffd\ufffd\ufffd\ufffdKl+1 nMf l M, \ufffdKl+1 nn \ufffd . (51)\n\ufffd\ufffd\ufffd  \ufffd Using q(f l+1:L M |f 1:l n ) from Eq. (50), we perform step ii) according to Eq. (29), resulting in q(f l+1:L M |f 1:l n ) = q(f l+1 M |f 1:l n )q(f l+2:L M |f 1:l n , f l+1 M ),\nwhere\nq(f l+1 M |f 1:l n ) = N \ufffd f l+1 M \ufffd\ufffd\ufffd\ufffd l\u02c6\u00b5l+1 M + \ufffd l \u02c6\u03a3l+1,l nM \ufffd\u22a4\ufffd \u02c6\u03a3l n \ufffd\u22121 (f l n \u2212\u02c6\u00b5l n), l \u02c6\u03a3l+1,l+1 M \u2212 \ufffd l \u02c6\u03a3l+1,l nM \ufffd\u22a4\ufffd \u02c6\u03a3l n \ufffd\u22121 l \u02c6\u03a3l,l+1 nM \ufffd\nand\nFor step iii) we have to build the joint Gaussian distribution\nq(f l+2:L M |f 1:l n , f l+1 M )p(f l+1 n |f l+1 M ) = q(f l+1 n , f l+2:L M |f 1:l n , f l+1 M )\n| | | using Eqs. (51) and (54). Since this formula would be even longer than the one in Eq. (54), we refrain from explicitly writing it here. While the corresponding formula for the base case [Eq. (37)] is much simpler the resulting form of Eq. (55) would be similar.\n(49)\n(50)\n(51)\n(52)\n\ufffd (53)\n(55)\nNext, the integration in step iv) can be performed using Eqs. (30), (53), and (55). The calculations are again ve similar to the ones in the corresponding step for the base case [Eq. (38)] so we only state the final result here: \ufffd\nq(f l+1 n , f l+2:L M |f 1:l n ) = \ufffd q(f l+1 M |f 1:l n )q(f l+1 n , f l+2:L M |f 1:l n , f l+1 M )df l+1 M \ufffd\nwhere\n\ufffd \ufffd\ufffd \ufffd ns to be shown in step v) is that this result does in fact agree with the exp \uf8eb \ufffd \uf8eb\n\ufffd \ufffd\ufffd \ufffd s that this result does in fact agree with the expected result from Lem. 2,  \ufffd \uf8eb \uf8f6 \uf8f6\n\uf8ed \ufffd\ufffd\ufffd \uf8ed \ufffd \ufffd \uf8f8 \uf8f8 where the terms are defined in Eqs. (10), (11), and (24). That means we have to prove that \u02c6ml+1 n = \u02c6\u00b5l+1 n and similarly for the other terms in Eqs. (58) - (61). Note that this is the point where we need the left indices in order to distinguish e.g. the term l\u02c6\u00b5l+2:L M appearing in Eq. (58) from l+1\u02c6\u00b5l+2:L M appearing in the mean of Eq. (62).\n  \ufffd \ufffd \ufffd  \ufffd \ufffd \ufffd where we used the definitions in Eqs. (10) and (24) for the terms\u02c6\u00b7. Note that these definitions are part of the induction hypothesis. It will soon become clear why we did not substitute \u02c6\u03a3l n. We furthermore used the definitions of the \ufffd\u00b5n and \ufffdSn terms in Thm. 1 to absorb the \ufffdK terms. In the following we are going to write Eq. (63) in a vectorized form and additionally substitute\n\ufffd \ufffd \ufffd \ufffd \u02c6ml+1 n = \ufffd\u00b5l+1 n + \ufffd\ufffdSl+1,1:l\u22121 n \ufffdSl+1,l n \ufffd\u22a4\ufffd A\u22121 + A\u22121B \ufffdD\u22121CA\u22121 \u2212A\u22121B \ufffdD\u22121 \u2212\ufffdD\u22121CA\u22121 \ufffdD\u22121 \ufffd\ufffd f 1:l\u22121 n \u2212\ufffd\u00b51:l\u22121 n f l n \u2212\ufffd\u00b5l n \ufffd , (66\n(56)\n(57)\n(59)\n(60)\n(61)\n(62)\n(63)\n(64)\n(65)\n(66)\nwhere we additionally exploited that A and \ufffdD are symmetric and that B\u22a4= C. In order to get any further from he we need the block matrix inversion lemma, which states that\n \ufffd  \ufffd \ufffd \ufffd where \ufffdD = D\u2212CA\u22121B. Comparing Eqs. (66) and (67) explains why we insisted on vectorising the last few formulas and also our definitions in Eq. (64). Finally, since \u02c6\u03a3l n = \ufffdSll n \u2212\ufffdSl,1:l\u22121 n \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 \ufffdS1:l\u22121,l n [Eq. (11)], we also identify \ufffdSll n = D. We can therefore rewrite Eq. (66) by reversing the block matrix inversion and resubstituting the terms in Eq. (64):\n \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd = \ufffd\u00b5l+1 n + \ufffdSl+1,1:l n \ufffd \ufffdS1:l,1:l n \ufffd\u22121 \ufffd f 1:l n \u2212\ufffd\u00b51:l n \ufffd .\n \ufffd  \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd In the last step we simply rewrote the vectors and the matrix according to the way we defined the submatrix notation. Comparing the final result to Eq. (10), we realize that this is indeed \u02c6\u00b5l+1 n , i.e., the mean term where we substituted l \u2192l + 1. In exactly the same way, i.e., by reversing the matrix inversion, we can show that the other parameters of the distribution in Eq. (56) indeed coincide with the respective parameters of the distribution in Eq. (62). Since this was the last part that remained to be shown, we finished the proof of Lem. 2.\n# B Intuition for the proof of Theorem 1\nIn this section, we provide some intuition that might be helpful in understanding parts of the proof of Thm. 1. In the first part, we present a different, heuristic way of obtaining the same results, making use of a mathematically wrong (or at least not mathematically rigorous) step. This helped us come up with the exact form of the theorem that we proved above. The second part gives some intuition on how the formulas appearing in Thm. 1, especially Eqs. (10) and (11), can be interpreted. More precisely, we show how these reduce to the mean-field equations when we plug in the mean-field covariance matrix.\n# B.1 Heuristic argument for Theorem 1\nThe aim of this section is to provide a heuristic argument for Thm. 1 as opposed to the complete proof given i Appx. A. For convenience we recap the starting point of that section: We need to show that\n\ufffd \ufffd where the distributions q on the right hand side are defined in Eqs. (9) - (11). The terms appearing on the left han side are given by q(fM) = N (fM|\u00b5M, SM), which is interchangeably also denoted as \ufffd\nand\nwhere\n\ufffd\ufffd\ufffd  \ufffd \ufffdKl nM = Kl nM \ufffd Kl MM \ufffd\u22121 \ufffdKl nn = Kl nn \u2212Kl nM \ufffd Kl MM \ufffd\u22121 Kl Mn.\n(67)\n(68)\n(69)\n(70)\n(71)\n(72) (73)\n\uf8ed \uf8ed \uf8f8 \ufffd\ufffd\ufffd \uf8ed \ufffdK \uf8f8 \uf8ed \uf8f8 \uf8ed \ufffdK \uf8f8 \uf8f8 Note that this is the point where this \u201cproof\u201d becomes heuristic: The object on the right hand side of Eq. (74) is not really a probability distribution, as the variables over which the distribution is defined (the f l n) appear as parameters of the distribution itself (as inputs of the covariance matrices, e.g. \ufffdKl+1 nn ). In the following we will pretend that rules for (multivariate Gaussian) distributions still apply to this object, making the rest of this proof mathematically wrong. We hope that it can still provide some intuition. Using the standard formula for solving Gaussian integrals (\u201dpropagation\u201d), \ufffd N (x|a + Fy, A) N (y|b, B) dy = N \ufffd x \ufffd\ufffda + Fb, A + FBF \u22a4\ufffd , (75)\n\ufffd \ufffd \ufffd\ufffd we can then easily plug in Eq. (74) in the left hand side of Eq. (69), yielding \uf8eb \uf8eb \uf8f6 \ufffd \uf8eb \uf8f6 \uf8eb\nwhere\n\ufffd   \ufffd  \ufffd \ufffd   \ufffd \ufffd Note that the latter two definitions also appear in Thm. 1. The expression in Eq. (76) has still the same problem as the expression in Eq. (74) in that it is not a valid distribution (since \ufffd\u00b5l n and \ufffdSll\u2032 n depend on f l\u22121 n ). Pretending further, that rules for distributions still apply, we can use the standard formula for conditioning multivariate Gaussians, \ufffd\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd\ufffd\n\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd\ufffd o repeatedly condition Eq. (76), resulting in\n \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd In Eqs. (81) and (82) the notation Al,1:l\u2032 is used to index a submatrix of the variable A, e.g. Al,1:l\u2032 = \ufffd Al,1 \u00b7 \u00b7 \u00b7 Al,l\u2032\ufffd . The final result, Eqs. (80) - (82), is once again a valid distribution and exactly matches the outcome of the mathematically rigorous proof of Thm. 1 in Appx. A. The latter, while being much more complicated, is necessary since the intermediate expressions [Eqs. (74) and (76)] rely on mathematically wrong (or at least dubious) steps.\n# B.2 Mean-field as a structured approximation\nre, we verify that when we plug in the mean-field covariance matrix into the formulas appearing in Thm. 1, we cover the mean-field formulas appearing in Sec. 2.2, i.e., that in this case Eq. (9) reduces to Eq. (6). For convenience  repeat the relevant formulas, starting with the mean-field formula for the marginals of the last layer [Eq. (6)]: q(f L n ) = \ufffd L \ufffd l=1 q(f l n; f l\u22121 n )df 1 n \u00b7 \u00b7 \u00b7 df L\u22121 n , where q(f l n; f l\u22121 n ) = Tl \ufffd t=1 N \ufffd f l,t n \ufffd\ufffd\ufffd\ufffd\u00b5l,t n , \ufffd\u03a3l,t n \ufffd , (83)\nHere, we verify that when we plug in the mean-field covariance matrix into the formulas appearing in Thm. 1, we recover the mean-field formulas appearing in Sec. 2.2, i.e., that in this case Eq. (9) reduces to Eq. (6). For convenience we repeat the relevant formulas, starting with the mean-field formula for the marginals of the last layer [Eq. (6)]:\nq(f L n ) = \ufffd L \ufffd l=1 q(f l n; f l\u22121 n )df 1 n \u00b7 \u00b7 \u00b7 df L\u22121 n , where q(f l n; f l\u22121 n ) = Tl \ufffd t=1 N \ufffd f l,t n \ufffd\ufffd\ufffd\ufffd\u00b5l,t n , \ufffd\u03a3l,t n \ufffd ,\n(74)\n(75)\n(76)\n(77) (78)\n(79)\n(80)\n(81)\n(82)\nq(f L n ) = \ufffd \ufffd q(f l n|f 1 n, . . . , f l\u22121 n )df 1 n \u00b7 \u00b7 \u00b7 df L\u22121 n where q(f l n|f 1 n, . . . , f l\u22121 n ) = N \ufffd f l n \ufffd\ufffd\ufffd\u02c6\u00b5l n, \u02c6\u03a3l n \ufffd ,\nwhere the means and covariances of the Gaussians in this equation are given by:\nwhere\n\ufffd \ufffd \ufffd \ufffd \ufffd Here we introduced Kl = \ufffd ITl \u2297Kl\ufffd as shorthand for the Kronecker product between the identity matrix ITl and the covariance matrix Kl, and used \u03b4 for the Kronecker delta. Having all relevant formulas in one place, we can proceed to show that if we plug in the mean field covariance matrix, SM = diag({Sll M}L l=1), where Sll M = diag({Sl,t M}Tl t=1) (see also Fig. 1, left), in Eq. (85), we recover Eq. (83): Removing the correlations between the layers by setting SM = diag({Sll M}L l=1), also implies that \u02dcSll\u2032 n = 0 if l \u0338= l\u2032 [Eq. (89)]. Therefore Eqs. (86) and (87) reduce to \u02c6\u00b5l n = \ufffd\u00b5l n and \u02c6\u03a3l n = \ufffdSll n, respectively. The resulting variational posterior factorises between the layers with q(f l n; f l\u22121 n ) = N \ufffd f l n \ufffd\ufffd\ufffd\ufffd\u00b5l n, \ufffdSll n \ufffd . Comparing with Eq. (83), we can already see that the means are equal [since \ufffd\u00b5l n = (\ufffd\u00b5l,1 n , . . . , \ufffd\u00b5l,Tl n ), cf. Eqs. (84) and (88)]. Removing the correlations within one layer by setting Sll M = diag({Sl,t M}Tl t=1) renders the covariance matrix \ufffdSll n block-diagonal. The diagonal blocks are obtained by evaluating Eq. (89) with Sll M = diag({Sl,t M}Tl t=1). It is easy to see that these diagonal blocks are equal to \ufffd\u03a3l,t n in Eq. (84) and we fully recover the mean-field solution [Eq. (83)].\n\ufffd \ufffd \ufffd \ufffd \ufffd Here we introduced Kl = \ufffd ITl \u2297Kl\ufffd as shorthand for the Kronecker product between the identity matrix ITl and the covariance matrix Kl, and used \u03b4 for the Kronecker delta.\n# \ufffd C ELBO\nHere we show how to derive the ELBO of the FC DGP, i.e., Eq. (8), which is given by\nL \ufffd n=1 \ufffd | \ufffd \u2212|| \ufffd l=1 While this is already done in the supplemental material of Ref. [24], we will do the derivation once again since our notation is different. For convenience we repeat the relevant formulas from the main text, i.e., the general formula for the ELBO [Eq. (3)], the joint DGP prior [Eq. (2)], and the variational family for the DGP [Eq. (4)], which are given by\n\ufffd \ufffd While this is already done in the supplemental material of Ref. [24], we will do the derivation once again since our notation is different. For convenience we repeat the relevant formulas from the main text, i.e., the general formula for he ELBO [Eq. (3)], the joint DGP prior [Eq. (2)], and the variational family for the DGP [Eq. (4)], which are given by\n(84)\n(85)\n(86)\n(88) (89)\n(88)\n(90)\n(91)\n(93)\nrespectively. By only using the general form of the distributions, and exploiting that we assumed iid noise, i.e., p(yN|f L N) = \ufffdN n=1 p(yn|f L n ), we can get from Eq. (91) to Eq. (90):\nIn the last step we introduced q(f L n ) as simply summarising all remaining terms in the first integral, hence,\n# D Marginalisation of (most) latent layer outputs\nHere we show how to get from the general form of q(f L n ) given in Eq. (98) to the starting point of our induction proof [Eq. (12)], where all the latent outputs f l n\u2032 are integrated out for all layers l and for all samples n\u2032 \u0338= n:\nWhile this is already shown in Remark 2 in Ref. [24] (note that the indices there are not correct), we will provide a bit more detail here and we can also nicely point out where the difference in the formulas for q(f L n ) arises from. For convenience the relevant formulas are repeated below: q(fN, fM) = q(fM) L \ufffd l=1 p(f l N|f l M; f l\u22121 N ), p(f l N|f l M; f l\u22121 N ) = N \ufffd f l N \ufffd\ufffd\ufffd\ufffdKl NMf l M, \ufffdKl NN \ufffd , (100)\nWhile this is already shown in Remark 2 in Ref. [24] (note that the indices there are not correct), we will provide a bit more detail here and we can also nicely point out where the difference in the formulas for q(f L n ) arises from. For convenience the relevant formulas are repeated below:\nwhere \ufffdKl NM = Kl NM \ufffd Kl MM \ufffd\u22121 and \ufffdKl NN = Kl NN \u2212Kl NM \ufffd Kl MM \ufffd\u22121 Kl MN. We will start by explicitly writing out Eq. (98) and changing the order of integration: \uf8ee\n\uf8f0 \uf8fb In the following we will only be concerned with the inner integral of the previous equation, which can also be wr as\n\uf8ed \ufffd \uf8f8 \ufffd ere, the inner integral can be solved by exploiting the nice marginalisation property of multivariate Gaussi \ufffd p(f L N|f L M; f L\u22121 N ) \ufffd n\u2032\u0338=n df L n\u2032 = \ufffd N \ufffd f L N \ufffd\ufffd\ufffd\ufffdKL NM(f L\u22121 N )f L M, \ufffdKL NN(f L\u22121 N ) \ufffd\ufffd n\u2032\u0338=n df L n\u2032 = N \ufffd f L \ufffd\ufffd\ufffd\ufffdKL (f L\u22121 )f L , \ufffdKL (f L\u22121 ) \ufffd ,\n\uf8ed \ufffd \uf8f8 \ufffd e, the inner integral can be solved by exploiting the nice marginalisation property of multivariate Gaussians, \ufffd \ufffd \ufffd\n(95)\n(96)\n(97)\n(98)\n(99)\n(101)\n(102)\n(103)\n(104)\nwhere we explicitly marked the dependence of the \ufffdKL terms on the outputs f L\u22121 N . While the \ufffdKl nM and \ufffdKl nn could  principle still depend on all the outputs of the previous layer, we see from their definitions after Eq. (100) that they  fact only depend on the marginals f L\u22121 n and that therefore\n\ufffd \ufffd\ufffd\ufffd\ufffd  \ufffd \ufffd Putting the last two equations together results in\nWe can continue with integrating out the f l N in Eq. (102) in the same fashion (noting at every layer that we can not marginalise out f l n as those are inputs to kernels), arriving at\nng this back into Eq. (101) and changing the order of integration once ag\nwhich is exactly Eq. (99), the result that we set out to show.\n# D.1 Difference between mean-field and fully-coupled\nFrom the previous equation it is also possible to see why a proof as in Appx. A was not necessary for the MF DGP This is due to the form of the variational posterior over the inducing outputs, given by\n\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd Using q(fM) from the MF DGP, which can also be written as q(fM) = \ufffdL l=1 q(f l M), the inner integral in Eq. (108 can be rewritten as the product of l integrals,\neach being a standard integral in Gaussian calculus and the resulting formulas are given in Eqs. (6) and (9). In contrast, a fully coupled multivariate Gaussian can not be written as such a product, which is why the rather straightforward solution presented above is not possible in our case and the proof in Appx. A is needed.\n# E Linear algebra to speed up the code\nIn this section we provide some guidance through the linear algebra that is exploited in our code to speed up or vectorise calculations. We will focus only on the most expensive terms, i.e., the off-diagonal covariance term \ufffdSl,1:l\u22121 n and how to deal with \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 , which are both needed to calculate \u02c6\u00b5l n and \u02c6\u03a3l n in Eqs. (111) and (112), respectively. First, we show how to deal with the FC DGP and afterwards how the sparsity of SM for the STAR DGP can be used. The linear algebra that can be exploited for all the other terms, e.g. the KL-divergence, will be provided along with the code. Our implementation is in GPflow [19] which provides all the functionalities that are necessary to deal with GPs in Tensorflow [1].\n(105)\n(106)\n(107)\n(108)\n(109)\n(110)\n\ufffd \ufffd \ufffd \ufffd  \ufffd Additionally, here is a more explicit definition of our notation of the covariance matrix SM: \ufffd \ufffd \ufffd \ufffd\n\uf8ed \ufffd \ufffd \ufffd \ufffd \uf8f8 where SM, Sll\u2032 M, and \ufffd Sll\u2032 M \ufffd tt\u2032 are matrices of size MT \u00d7 MT (where T = \ufffdL l=1 Tl), MTl \u00d7 MTl\u2032, and M \u00d7 M, which store the covariances of the inducing outputs between all inducing points, only those between layer l and l\u2032, and only those between the t-th task in layer l and the t\u2032-th task in layer l\u2032, respectively. In order to ensure that SM is a valid covariance matrix (positive definite) we will numerically only work with its Cholesky decomposition LS (s.t. SM = LSL\u22a4 S ), which is a lower triangular matrix. Wherever possible we will want to avoid actually computing SM and instead calculate all quantities from LS directly.\n# E.1 Fully coupled DGP\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd which are of size Tl \u00d7 \ufffdl\u22121 l\u2032=1 Tl\u2032, have to be calculated for l = 1, . . . , L and for n = 1, . . . , N. As the number of layers L is in practice rather small, we will calculate all the individual matrices \ufffdSll\u2032 n in a loop and concatenate them at the end, while we want to avoid a loop over N. Using Eq. (113) and that \ufffdKl Mn = ITl \u2297\ufffdKl Mn, we see that (for an example with Tl, Tl\u2032 = 2) \uf8eb \uf8f6\n\ufffd \uf8ed \ufffd \ufffd \ufffd  \ufffd \ufffd \ufffd \ufffd  \ufffd \uf8f8 Writing \ufffdSll\u2032 n in this way has two advantages: Firstly, actually performing the multiplication \ufffdKl nMSll\u2032 M \ufffdKl\u2032 Mn is extremely inefficient as the \ufffdKl Mn are block diagonal, which we resolved in this formulation. Secondly, we note that exactly the same operation, i.e., multiplying from left and right by \ufffd \ufffdKl Mn \ufffd\u22a4 and \ufffdKl\u2032 Mn, respectively, has to be performed on all TlTl\u2032 blocks of size M \u00d7 M. This can be exploited since tensorflow has an inbuilt batch mode for most of its matrix operations. In the following we will first show how the relevant block Sll\u2032 M can be efficiently obtained and afterwards show how to deal with the batch matrix multiplication for all n = 1, . . . , N. Let us consider an example with three layers (L = 3), the resulting covariance matrix and its Cholesky decomposition: \uf8eb \uf8f6\n\uf8ed \ufffd \ufffd \uf8f8 From this we can read off formulas for the blocks of SM, e.g., S32 M = \ufffd L31 S L32 S \ufffd\ufffd L21 S L22 S \ufffd\u22a4, which in general can be written as Sll\u2032 M = Ll,1:l\u2032 S \ufffd Ll\u2032,1:l\u2032 S \ufffd\u22a4 , (118)\n(111)\n(112)\n(113)\n(114)\n(115)\n(116)\n(117)\n(118)\nwhere we exploited that we only need Sll\u2032 M for l\u2032 < l (the formula above is not valid for l\u2032 \u2265l). In this way we avoided calculating unnecessary matrix multiplications involving zero blocks. Avoiding the loop over N requires a bit more linear algebra: For this we note that e.g. the element \ufffd \ufffdSll n \ufffd 11 can be seen as the n-th diagonal element of the N \u00d7 N matrix \ufffd \ufffdKl MN \ufffd\u22a4 (Sll\u2032 M)11 \ufffdKl\u2032 MN. Fully calculating this matrix is obviously very inefficient as we only need its diagonal elements. For this we use that, generally, for q \u00d7 p matrices A, C\u22a4and q \u00d7 q matrices B diag \ufffd C\u22a4BA \ufffd = column sum(C\u22a4\u2299BA), (119) where \u2299denotes the elementwise matrix product. The formula can easily be proved by explicitly writing the matrix products as sums and comparing terms on both sides. Using this on all the blocks of \ufffdSll\u2032 n in Eq. (116) in a batched form and reordering the obtained terms afterwards requires some reshaping, which is explained in the code. The most expensive calculations for this term are obtaining Sll\u2032 M [Eq. (118)], which is O(M 3TlTl\u2032 \ufffdl\u2032 l\u2032\u2032=1 Tl\u2032\u2032) and the multiplication of e.g. (Sll\u2032 M)11 \ufffdKl\u2032 MN which has to be done for all TlTl\u2032 blocks of Sll\u2032 M and is therefore O(NM 2TlTl\u2032). Both of these operations have to be performed for all l\u2032 = 1, . . . , l \u22121 in Eq. (115) and also for all layers l = 1, . . . , L. The total computational cost of this term is therefore O(M 3 \ufffdL l=1 Tl \ufffdl\u22121 l\u2032=1 Tl\u2032 \ufffdl\u2032 l\u2032\u2032=1 Tl\u2032\u2032 + NM 2 \ufffdL l=1 Tl \ufffdl\u22121 l\u2032=1 Tl\u2032).\ndiag \ufffd C\u22a4BA \ufffd = column sum(C\u22a4\u2299BA), (119) where \u2299denotes the elementwise matrix product. The formula can easily be proved by explicitly writing the matrix products as sums and comparing terms on both sides. Using this on all the blocks of \ufffdSll\u2032 n in Eq. (116) in a batched form and reordering the obtained terms afterwards requires some reshaping, which is explained in the code. The most expensive calculations for this term are obtaining Sll\u2032 M [Eq. (118)], which is O(M 3TlTl\u2032 \ufffdl\u2032 l\u2032\u2032=1 Tl\u2032\u2032) and the multiplication of e.g. (Sll\u2032 M)11 \ufffdKl\u2032 MN which has to be done for all TlTl\u2032 blocks of Sll\u2032 M and is therefore O(NM 2TlTl\u2032). Both of these operations have to be performed for all l\u2032 = 1, . . . , l \u22121 in Eq. (115) and also for all layers l = 1, . . . , L. The total computational cost of this term is therefore O(M 3 \ufffdL l=1 Tl \ufffdl\u22121 l\u2032=1 Tl\u2032 \ufffdl\u2032 l\u2032\u2032=1 Tl\u2032\u2032 + NM 2 \ufffdL l=1 Tl \ufffdl\u22121 l\u2032=1 Tl\u2032).\n \ufffd \ufffd Dealing with the inverse covariance terms First of all, we will never actually calculate \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 . We only use (and update) the lower triangular Cholesky decomposition L1:l\u22121,1:l\u22121 Sn defined by\n\ufffd only use (and update) the lower triangular Cholesky decomposition L1:l\u22121,1:l\u22121 Sn defined by\n \ufffd This can be done since the inverse term only ever appears in the product \ufffdSl,1:l\u22121 n \ufffd \ufffdS1:l\u22121,1:l\u22121 n \ufffd\u22121 , whose transpose can be efficiently obtained via the solution of two",
    "paper_type": "method",
    "attri": {
        "background": "Deep Gaussian Processes (DGPs) enhance the flexibility of Gaussian Processes (GPs) in learning representations of data, but they face challenges in tractable inference due to the complexity of posterior distributions. Existing methods for approximate inference, such as variational inference, often compromise between accuracy and computational efficiency. This paper introduces a novel Gaussian variational family that maintains covariance relations between latent processes while ensuring fast convergence by marginalizing out global latent variables, addressing the limitations of previous methods.",
        "problem": {
            "definition": "The core issue addressed in this paper is the inefficiency of existing approximate inference techniques in DGPs, which either sacrifice accuracy for speed or vice versa.",
            "key obstacle": "The primary challenge is the inability of existing methods to effectively capture correlations between latent Gaussian processes while maintaining computational efficiency."
        },
        "idea": {
            "intuition": "The proposed idea stems from the observation that existing variational families fail to account for the correlations between latent processes, which can lead to suboptimal uncertainty estimates.",
            "opinion": "This paper proposes a new variational family that allows for the coupling of inducing outputs across layers, thus improving the expressiveness of the model while maintaining computational efficiency.",
            "innovation": "The key innovation lies in the introduction of a structured variational family that allows for analytical marginalization of inducing outputs, which contrasts with the mean-field approaches that treat inducing outputs as independent."
        },
        "method": {
            "method name": "Structured Deep Gaussian Processes",
            "method abbreviation": "STAR DGP",
            "method definition": "STAR DGP is a method that utilizes a structured variational family to couple inducing outputs across layers, allowing for analytical marginalization of global latent variables.",
            "method description": "The method employs a Gaussian variational posterior that captures dependencies between latent processes while allowing for efficient inference.",
            "method steps": [
                "Define a structured variational family that captures correlations between latent processes.",
                "Marginalize out global latent variables analytically.",
                "Implement the method using efficient computational techniques to handle the structured covariance."
            ],
            "principle": "This method is effective because it retains the flexibility of DGPs while ensuring that the computational burden is manageable, leading to improved accuracy in uncertainty estimates."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on several benchmark datasets, comparing the performance of STAR DGP against mean-field DGPs and other state-of-the-art methods.",
            "evaluation method": "Performance was assessed using average marginal test log-likelihood and calibration metrics across interpolation and extrapolation tasks."
        },
        "conclusion": "The proposed STAR DGP method outperforms existing alternatives by achieving a better balance between accuracy and calibrated uncertainty estimates, particularly in extrapolation tasks.",
        "discussion": {
            "advantage": "The main advantage of STAR DGP is its ability to provide more accurate uncertainty estimates by capturing correlations between latent processes, leading to improved predictive performance.",
            "limitation": "One limitation is that the increased complexity of the covariance structure may lead to longer training times compared to simpler models.",
            "future work": "Future research could explore further optimizations of the covariance structure and investigate the applicability of STAR DGP to other types of models and datasets."
        },
        "other info": {
            "acknowledgements": "The authors thank Buote Xu for valuable comments and suggestions, and the anonymous reviewers for their constructive feedback."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The proposed method, STAR DGP, utilizes a structured variational family to couple inducing outputs across layers, allowing for analytical marginalization of global latent variables."
        },
        {
            "section number": "2.2",
            "key information": "Deep Gaussian Processes (DGPs) enhance the flexibility of Gaussian Processes (GPs) in learning representations of data, addressing limitations of traditional GPs."
        },
        {
            "section number": "5.1",
            "key information": "The STAR DGP method improves model efficiency and performance by capturing dependencies between latent processes while allowing for efficient inference."
        },
        {
            "section number": "7.1",
            "key information": "The core issue addressed is the inefficiency of existing approximate inference techniques in DGPs, which either sacrifice accuracy for speed or vice versa."
        },
        {
            "section number": "7.3",
            "key information": "Future research could explore further optimizations of the covariance structure and investigate the applicability of STAR DGP to other types of models and datasets."
        }
    ],
    "similarity_score": 0.593091535447416,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Beyond the Mean-Field_ Structured Deep Gaussian Processes Improve the Predictive Uncertainties.json"
}