{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.10424",
    "title": "Understanding In-Context Learning with a Pelican Soup Framework",
    "abstract": "Many existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a O(1/T) loss bound for in-context learning, where T is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of atom concepts makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec, and a digit addition task that mimics types of distribution shifts a model needs to overcome to perform in-context learning. We also experiment with GPT2-Large on real-world NLP tasks. Our empirical results demonstrate the efficacy of our framework to explain in-context learning.",
    "bib_name": "chiang2024understandingincontextlearningpelican",
    "md_text": "# Understanding In-Context Learning with a Pelican Soup Framework\nTing-Rui Chiang 1 Dani Yogatama 1\n# Abstract\nMany existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a O(1/T) loss bound for in-context learning, where T is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of atom concepts makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec, and a digit addition task that mimics types of distribution shifts a model needs to overcome to perform in-context learning. We also experiment with GPT2-Large on real-world NLP tasks. Our empirical results demonstrate the efficacy of our framework to explain in-context learning.\narXiv:2402.10424v1\narXiv:2402.104\n# 1. Introduction\nLarge language models (LM) have demonstrated the ability to perform downstream natural language processing (NLP) classification tasks via in-context learning (ICL) (Chowdhery et al., 2022). That is, they can predict the label of an input by conditioning on the demonstration, i.e., a sequence of input-label pairs (Brown et al., 2020), even when the verbalizers (labels present in the demonstration) are semantically irrelevant to the task, e.g., foo/bar instead of negative/positive (Wei et al., 2023). To understand why LMs trained with nonstructural general text can perform\n1University of Southern California. Correspondence to: Ting-Rui Chiang <tingruic@usc.edu>, Dani Yogatama <yogatama@usc.edu>.\n1University of Southern California. Correspondence to: Ting-Rui Chiang <tingruic@usc.edu>, Dani Yogatama <yogatama@usc.edu>.\nICL, a line of works base their theoretical analysis on the assumption that the data is from a latent variable model (Xie et al., 2022; Zhang et al., 2023; Wang et al., 2023b).\nHowever, the assumptions on a latent variable model in these works leave a gap between theory and practice. Xie et al. (2022) assumes that the general text for training LMs is from a hidden Markov model, which is an oversimplification of natural language. Zhang et al. (2023) propose a more general latent variable model for the data generation process. Although their model is more realistic than a hidden Markov model, their analyses are based on the assumption that there is a data generation process that generates both the general text for LM training and the demonstration for in-context learning. As demonstrations have a format very different from general text, a question to be answered is whether such a generation process really exists. Additionally, considering the diversity of the semantics natural language can express, it is likely that the training data does not cover all possible values in the latent variable\u2019s space. It requires more characterizations on the latent space to discuss the generalization to semantics unseen in the training data. Besides, previous theoretical analyses do not fully answer some questions either (Xie et al., 2022; Zhang et al., 2023; Hahn & Goyal, 2023). For example, what is the effect of the choice of verbalizers on ICL performance? How does the distribution of downstream task examples affect ICL? How instruction-tuning helps ICL (Chung et al., 2022)?\nTo answer these questions, we propose a theoretical framework, the Pelican Soup Framework. We introduce the notion of a commonsense knowledge base (KB) and define the data generation process as the process that generates documents consistent with the KB. We also introduce a formalism for NLP classification tasks. This notion and formalism instantiate a data generation process that satisfies the assumption made by Zhang et al. (2023). With slight adaption on the Zhang et al. (2023), we establish a similar bound for ICL loss under our framework. Additionally, because we see the latent variable as the association between the verbalizers and their meaning, our bound can reflect the effect of the choice of verbalizers. It also reflects how instruction tuning affects ICL performance. If we further assume that all latent meaning can be represented with logic formula over a finite set of atom concepts\nas suggested by cognitive science theories Fodor (1975; 2008); Piantadosi (2021), we can characterize the generalization to the unseen meaning during inference time. Having this assumption instantiates a setup compatible with the theory for the emergence of complex skills by (Arora & Goyal, 2023), and we can thus apply their analyses on generalization to our framework. We also discuss our results with the results by Hahn & Goyal (2023), where they bound ICL loss with the description length of the ground truth input-label mapping function.\nas suggested by cognitive science theories Fodor (1975; 2008); Piantadosi (2021), we can characterize the generalization to the unseen meaning during inference time. Having this assumption instantiates a setup compatible with the theory for the emergence of complex skills by (Arora & Goyal, 2023), and we can thus apply their analyses on generalization to our framework. We also discuss our results with the results by Hahn & Goyal (2023), where they bound ICL loss with the description length of the ground truth input-label mapping function. Inevitably, whether our analyses are meaningful depends on how deep models such as transformers (Vaswani et al., 2017) generalize. Specifically, as most of the previous theoretical analyses do, we show that a perfect LM that estimates the next token distribution perfectly according to the training data distribution can do ICL. However, when doing ICL, LMs are conditioned on the demonstration, which has a low probability mass in the training data. LMs can do ICL only if it can generalize well under this distribution shift from training data to ICL demonstrations. As the generalization of LMs is difficult to analyze theoretically, we conduct experiments to show empirical evidence for our framework. We construct a synthetic dataset, Calcutec (\u00a74), and a synthetic task, the addition task (\u00a75), to mimic the distribution shifts LMs need to overcome. With Calcutec, we show that LMs can do ICL regardless of the distributional shifts and observe the efficacy of chain-ofthoughts. With the addition task, we observe that larger models generalize better under a certain type of distributional shift, which is aligned with the observation about model scaling in the real world Wei et al. (2022). Finally, in \u00a76, we experiment with natural language and show that using pronouns as verbalizers can lead to comparable performance (with GPT-2 Large), which is aligned with our framework where LMs learn ICL ability by modeling meaning association of words.\n# 2. The Pelican Soup Framework 2.1. Motivation: Pelican Soup\n# 2. The Pelican Soup Framework\nThe Pelican Soup game inspires our framework. It is a game involving a puzzle master who has a story in their mind. The game participants aim to recover the story by asking the puzzle master yes/no questions. An observation is that once the participants recover the story, they can answer any questions about the story. Therefore, the story has a similar role as a latent variable defining the input-output mapping, and the yes/no questions are similar to the demonstrations for in-context learning. We include an example in Appendix A.\nGiven the above observation, we can study in-context learning by considering why humans can solve Pelican Soup riddles. We conjecture that this is because the person who\nmakes the story and the ones who solve the riddle share the same (or similar) commonsense (McCarthy, 1960) about logical relationships among things in this world (Schank & Abelson, 1988). This inspires us to introduce the notion of a commonsense knowledge base in our framework.\n# 2.2. Training Data Distribution\nMotivated by the Pelican Soup game and the early formulation of AI (McCarthy, 1960; Croft, 1993), we introduce the notion of a commonsense knowledge base and use it to restrict the possible combination of sentences that can co-occur in a paragraph:\nIt is natural to assume the existence of this knowledge base. This knowledge base may include statements describing interrelations among the concepts represented by words, as cognitive psychologists have suggested word semantics are based on these interrelations (Siskind, 1996; Murphy, 2004). For example, the knowledge base may contain a rule: \u201cIf X moved, then the position of X changed\u201d because the semantic of the verb \u201cmove\u201d is embodied in its implication on \u201cposition\u201d. Based on this knowledge base, humans can do step-by-step reasoning with language. Humans also avoid writing paragraphs that include contradictory sentences.\nWe also observe that language allows us to arbitrarily associate concepts to an entity or a pronoun, which we refer to as reference. For example, when the pronoun \u201cshe\u201d or the human name \u201cEmily\u201d is present in a paragraph, it is associated with a person that can be described with a set of sentences. The usage of the reference is dependent on the meaning it is associated with and is consistent within its context. For example, if \u201cshe\u201d is associated with the sentence \u201ca person who has a house\u201d, then the sentence \u201cshe has no property\u201d will have 0 probability mass. Moreover, when we want to refer to \u201cthe person who has a house\u201d instead of repeating the sentence again, we use \u201cshe\u201d for an abbreviation (when\nthere is no ambiguity).\n# there is no ambiguity).\nWe can see abstract adjectives such as \u201cgood\u201d and \u201cbad\u201d as references too, as their meaning also depends on the context. However, their meaning may not be as variable as pronouns. We reflect this difference with a prior distribution for the meaning a reference is associated with. For simplicity, we only consider single-word references and assume that such association is consistent throughout a document. We write down an assumption formally: Assumption 2.3 (Reference). A set of words \u0393 \u2282\u03a3 serve as references in the language. For every document in the training data, some r \u2208\u0393 can be associated with a meaning represented as a set of sentences Zr, which follows a prior distribution Pr(Zr). Any z \u2208Zr, z present in the document can be replaced by r. Finally, we assume a document is composed of a set of paragraphs where some references are present: Assumption 2.4 (Document). A document is a concatenation of paragraphs separated with a delimiter d (e.g. a blank line). Also, there is a set of reference S \u2282\u0393 such that in each of the paragraphs, a r \u2208S is present.\nWe can see abstract adjectives such as \u201cgood\u201d and \u201cbad\u201d as references too, as their meaning also depends on the context. However, their meaning may not be as variable as pronouns. We reflect this difference with a prior distribution for the meaning a reference is associated with.\n# 2.3. A Formalism for NLP Classification Tasks\nWith the commonsense knowledge base KB defined in Assumption 2.2, we propose a simple formalism: For any objective or prescriptive NLP classification task (Rottger et al., 2022) that classifies an input x to one of |Y| classes, it can be described with some instructions u and some descriptions vi for i = 1, 2, \u00b7 \u00b7 \u00b7 , |Y| such that x is labeled as yi if and only if\n(1)\nThat is, x belongs to class yi if and only if based on the commonsense rules in KB and the instruction t, x entails vi. Note that because we can rewrite Eq. 1 as KB\u2227x |= u \u2192zi, we can combine the task instructions with the description of each class vi for i = 1, 2 \u00b7 \u00b7 \u00b7 , |Y|. Therefore, we can represent any NLP classification tasks as \u27e8v1, v2, \u00b7 \u00b7 \u00b7 , v|Y|\u27e9. For example, we can formulate the sentiment analysis task over movie reviews as \u27e8v+, v\u2212\u27e9 = \u27e8\u201cI like the movie\u201d, \u201cI dislike the movie\u201d\u27e9. Humans have the commonsense in our KB that people would only recommend something they like. Thus, based on this rule, we can derive the label of an input \u201cI would recommend this movie.\u201d Assumption 2.5 (Downstream Task). NLP classification tasks can be defined with the formulation described above. Namely, given any classification task with classes Y, we can define it with the descriptions {vy}y\u2208Y. We further assume that the descriptions of different classes contradict with each other, so every input belongs to only one class.\n# 2.4. Bounding ICL Loss\nOur framework is quite general; by adapting and combining the analyses by Zhang et al. (2023) and Hahn & Goyal (2023), we have the following theorem:\nTheorem 2.6 (Average ICL Loss). Let the description of a classification task be {zy}y\u2208Y and z\u2217be the association between the task description {vy}y\u2208Y and the verbalizers \u0393Y used for ICL. Let K be the constraints used for decoding, \u02d9g be the event where a document belongs to a certain genera. Let St = x1, r2, d, x2, r2, d \u00b7 \u00b7 \u00b7 , xt, rt, d, where rt is the verbalizer that is associated with the label of xt and d is the delimiter. We have for any positive integer T, the average cross-entropy loss of ICL is bounded as:\n(2)\nWhen the last two terms on the right-hand side are nonnegative, Eq. 2 shows the average cross-entropy loss of ICL converges to 0 in O(1/T). We discuss the three terms in the right-hand side below.\nThe second term becomes 0 if we set K as the constraint that the next two tokens of St\u22121, xt must be a verbalizer and the delimiter for all t. This is because Assumption 2.2 ensures that xt does not conflict with rt and Assumption 2.5 ensures that xt conflict with verbalizers other than rt.\nWe then look at the last term. This term is 0 when xt is conditionally independent to z\u2217as assumed by Zhang et al. (2023). However, this may be an over-simplification because, in natural language, the transition from xt to its next token depends on the content of xt. This assumption may actually be unnecessary for convergence because xt is an example from a downstream task related to z\u2217; it is likely that\nwhich implies that this term is non-negative, and we can thus ignore this term. More rigorously, we can show the following corollary: Corollary 2.7 (Expected Average ICL Loss). Let \u02d9g represent a set of documents whose paragraphs are conditionally independent to each other given z\u2217, i.e.\nPr(x1, d, x2, d, \u00b7 \u00b7 \u00b7 , xT , d|z\u2217, \u02d9g) = \ufffdT t=1 Pr(xt, d|z\u2217, \u02d9g). If the downstream task data distribution DX follow Pr(x|z\u2217, \u02d9g), then we can bound the average ICL crossentropy loss over the downstream task as:\n(3)\n\u2264\u22121 T log Pr(z\u2217, \u02d9g|K).\nThe right-hand side of Eq. 3 characterizes the convergence rate and reflects the difficulty of doing ICL. If z\u2217, \u02d9g, K are independent, then we can see this term is proportional to Pr(z\u2217). This implies that when the association between label description and the verbalizer is uncommon in the training data (e.g., associating \u201cpositive\u201d to \u201cThis movie is bad.\u201d), doing ICL is more difficult. Eq. 2 also allows us to analyze the scenario where we do not constrain the next token of St\u22121, xt to be a verbalizer while decoding. We can replace \u02d9g with \u00a8g that represents the documents satisfying \u02d9g and the constraint K (i.e., verbalizers always follow x).1 This ensures the second term of Eq. 2 to be 0. The cost of using \u00a8g makes the first term in bound bigger because Pr(\u00a8g|z\u2217) \u2264Pr(\u02d9g|z\u2217). This reflects that doing ICL without constraining the next token is harder. Pr(\u00a8g|z\u2217) may also be related to instruction tuning. The training examples for instruction tuning are input-output pairs following some format, such as having an instruction (e.g., \u201cLabel the example as positive if ...\u201d) at the beginning and a prompt after each input (e.g. \u201c[example]. The sentiment of this comment is\u201d). Because for examples that follow the special format, the next token after the prompt is always a verbalizer, these examples belong to the genera \u00a8g. Having these examples in the training set would increase Pr(\u00a8g|z\u2217) and thus make the ICL loss bound converge faster. This explains why instruction tuning helps ICL.\nThe right-hand side of Eq. 3 characterizes the convergence rate and reflects the difficulty of doing ICL. If z\u2217, \u02d9g, K are independent, then we can see this term is proportional to Pr(z\u2217). This implies that when the association between label description and the verbalizer is uncommon in the training data (e.g., associating \u201cpositive\u201d to \u201cThis movie is bad.\u201d), doing ICL is more difficult.\nEq. 2 also allows us to analyze the scenario where we do not constrain the next token of St\u22121, xt to be a verbalizer while decoding. We can replace \u02d9g with \u00a8g that represents the documents satisfying \u02d9g and the constraint K (i.e., verbalizers always follow x).1 This ensures the second term of Eq. 2 to be 0. The cost of using \u00a8g makes the first term in bound bigger because Pr(\u00a8g|z\u2217) \u2264Pr(\u02d9g|z\u2217). This reflects that doing ICL without constraining the next token is harder.\n# 2.5. Extending the Framework with Formal Language\nAssuming a latent model poses a dilemma: Language can encode various meanings, so assuming that the latent space is finite is not reasonable, or the space must be very large. However, if the latent space is infinite or is very large, it is possible that the limited training data does not cover the whole space. Without any assumption on the latent space (e.g., the relation between the states in the space), it is impossible to discuss the generalization to unseen latent states. Thus, we provide an extension to our framework.\n1Although \u00a8g may seem unnatural, this genere of documents corresponds to the PCFG structure assumed in Hahn & Goyal (2023).\n1Although \u00a8g may seem unnatural, this genere of documents corresponds to the PCFG structure assumed in Hahn & Goyal (2023).\nwith an assumption stronger than Assumption 2.1: Assumption 2.8 (Meaning representation). There exists (1) a finite set of atom concepts \u2126and (2) a function f that can map any sentence in language to its meaning represented with a logical formula with operands in \u2126such that for any two sentences s1, s2, the logical relation between s1 and s2 is the same as f(s1) and f(s2).\nThis assumption is aligned with the cognitive psychology studies that hypothesize the existence of a set of mental tokens (Fodor, 1975; 2008) and a recent study (Piantadosi, 2021) showing that semantics can be encoded with the combination of only a few symbols. This assumption suggests that if we have the parsing function f, solving NLP tasks only requires a finite-length program that can do logical reasoning by manipulating logical symbols according to logical induction rules. If a deep model can learn this program, then it can perform a task even if this task is not in the training data. This assumption of a finite \u2126also instantiates the concept of language skills by Arora & Goyal (2023), and their theoretical results are thus applicable.\nThis assumption is aligned with the cognitive psychology studies that hypothesize the existence of a set of mental tokens (Fodor, 1975; 2008) and a recent study (Piantadosi, 2021) showing that semantics can be encoded with the combination of only a few symbols.\nThis assumption suggests that if we have the parsing function f, solving NLP tasks only requires a finite-length program that can do logical reasoning by manipulating logical symbols according to logical induction rules. If a deep model can learn this program, then it can perform a task even if this task is not in the training data. This assumption of a finite \u2126also instantiates the concept of language skills by Arora & Goyal (2023), and their theoretical results are thus applicable.\n# 2.6. Relating to Function Description Length\nWhen there is no decoding constrains, Pr(rt, d|xt, z\u2217, St\u22121) is related to the difficulty of the example. To see this, we need an additional assumption:\nAssumption 2.9. In some documents in the training data, the paragraphs are constituted with steps in a logical induction process with some steps randomly dropped.\nThis kind of document may be pervasive in the training data. Essays arguing some claims are one example. To be convincing, these essays should proceed like a proving process that induces their conclusions. Documents describing a series of events can be another example, as events follow commonsense and develop progressively.\nWith this assumption and some regularity assumptions on the data distribution, we can have\nPr(rt, d|xt, z\u2217, St\u22121) \u2264c \u00b7 \u2113(xt),\n(4)\nwhere \u2113(xt) is the number of reasoning steps required to solve the task, and c is a constant This \u2113(xt) corresponds to the description length of the function that maps the inputs to their label in the loss bound by Hahn & Goyal (2023) (more discussion in Appendix D).\n# 3. Distribution Shifts from Language Modeling to In-context Learning\nIn addition to the theoretical analyses, an important aspect of understanding ICL is to understand how models generalize.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a0fe/a0fe99bc-ae81-4518-bd76-81d13ba3ab72.png\" style=\"width: 50%;\"></div>\nTable 1: Calcutec examples for training, in-context learning (ICL), and chain-of-thought (CoT).\n<div style=\"text-align: center;\">Table 1: Calcutec examples for training, in-context learning (ICL), and chain-of-thought (CoT).</div>\nThe bounds we have established in \u00a7 2.4 are based on the analyses of the marginal distribution conditioned on the input St\u22121, xt. Because the distribution of prefix St\u22121, xt is different from the distribution of general text, a model needs to overcome several types of distribution shifts to predict the marginal distribution precisely. We characterize a few of them below.\nFormat mismatch. The demonstration is in the format xt, rt, d, where xt follows the downstream task distribution. However, in the LM training data, sequences in this format are unlikely to happen frequently.\nVerbalizer Mismatch. The verbalizers appear very frequently in St\u22121. However, the verbalizer may not have as high frequency in the training data distribution.\nUnseen Reference-Meaning Association. When performing ICL, the verbalizers are associated with descriptions of the classes for the task. However, the training data may not include this kind of association.\nWe empirically investigate these types of distribution shifts in the next section.\n# 4. Experimenting with Synthetic Datasets\nAs a concrete instance of our framework, we present a toy setup, Calcutec, which includes a data generation process for LM training data and a process for downstream tasks. Calcutec follows our framework and mimics the distribution shifts LMs need to overcome as discussed in \u00a73. We will use this setup to inspect models\u2019 generalization ability.\nSetup. Following the assumption in \u00a72, we construct a pseudo-language with the following components:\n\u2022 Logic model: We use a subset of propositional logic as our logic model. We only consider Horn clauses (Horn, 1951), i.e., formulas in the form A \u2227B \u2192C. This allows us to efficiently generate proof with time complexity linear to the size of the knowledge base (Dowling & Gallier, 1984).\n\u2022 Atom concepts: We use 100 variables as our set of atom concepts \u03a3. \u2022 KB: For each variable \u03c3 \u2208\u03a3, there are 5 formulas in the knowledge base whose consequence (the literals after \u2192in a horn clause) is \u03c3, while the antecedents (the literals before \u2192) are sampled from \u03a3\\{\u03c3} uniformly at random.\n\u2022 Atom concepts: We use 100 variables as our set of atom concepts \u03a3.\n\u2022 KB: For each variable \u03c3 \u2208\u03a3, there are 5 formulas in the knowledge base whose consequence (the literals after \u2192in a horn clause) is \u03c3, while the antecedents (the literals before \u2192) are sampled from \u03a3\\{\u03c3} uniformly at random.\n\u2022 We have a set \u0393 = {ri}4 i=1 representing the reference words described in Assumption 2.3.\nTraining Dataset. Following Assumption 2.4, a document is a concatenation of paragraphs separated by delimiter \u201c;\u201d and ends with \u201c.\u201d. In our synthetic language model training dataset, each document contains 16 paragraphs. Before starting the generation process, we randomly draw a reference symbol ra \u2208\u0393 and associate it with two formulas s1, s2. For simplicity, we make these two formulas single-variable, i.e., s1, s2 \u2208\u03a3.\nBecause paragraphs in the real world are sentences ordered in a certain way, we follow Assumption 2.9 and randomly generate a proof as a paragraph. Each paragraph represents the induction process of P |= g for some randomly selected P \u2282\u03a3 and g \u2208\u03a3 that allows us to use the reference symbol r in the paragraph. Each sentence in the paragraph is a sentence representing a reasoning step. Because we use Horn logic as our logic model, a step is a rule in KB in the form a1a2 \u2192b. We separate the clauses in the sequence with commas. To simulate the fact that documents in the real world always skip some reasoning steps, we further apply some perturbations over the generated paragraphs that drop some reasoning steps with a skip rate pskip (details in Appendix E). 2\nAfter we generate a document, we replace s1 and s2 with the reference symbol ra. We randomly draw another reference symbol rb \u2208\u0393 and replace the two most frequent symbols in the document with rb. (We provide the pseudo-code Alg. 1 in the appendix.)\nDownstream Tasks. Following the formulation in Assumption 2.5, we define a binary classification task by defin2Models can acquire in-context learning ability even with pskip = 0 (Figure 6 in thr appendix).\nDownstream Tasks. Following the formulation in Assumption 2.5, we define a binary classification task by defin2Models can acquire in-context learning ability even with\n2Models can acquire in-context learning ability even with pskip = 0 (Figure 6 in thr appendix).\ning the description v+ and v\u2212for the positive and negative classes, respectively. v+ and v\u2212are the disjunctions of atom concepts, i.e., in the form of a1 \u2228a2 \u2228\u00b7 \u00b7 \u00b7 . We create five downstream tasks using different disjunctions. Each input is a subset of variables in \u03a3 from which we ensure that only one of the classes can be induced.\nDemonstration. We represent an input-label pair as x1x2 \u00b7 \u00b7 \u00b7 \u2192r, where x1x2 \u00b7 \u00b7 \u00b7 is the input part and r \u2208 {r+, r\u2212} \u2282\u0393 is a reference symbol serving as a verbalizer. Chain-of-thought. A chain-of-thought is in the format same format as the training data, but ends with a reference symbol r \u2208{r+, r\u2212}, e.g., x1x2 \u00b7 \u00b7 \u00b7 \u2192x3; x3 \u00b7 \u00b7 \u00b7 x4 \u2192r+. This chain-of-thought reflects the step-by-step induction process from the inputs to the label.\n# 4.2. Inspecting the Distribution Shifts\nOur Calcutec dataset allows us to inspect some types of distribution shifts in \u00a7 3.\n# Our Calcutec dataset allows us to inspect some types of distribution shifts in \u00a7 3.\nFormat Mismatch. The training data contains reasoning steps but these steps are absent in the demonstration.\nVerbalizer Mismatch. When we are picking the reference symbols in \u0393, we assign probability mass 45%, 45%, 5%, 5% to r1, r2, r3, r4. In this way, we can inspect whether using less frequent verbalizers will lead to worse performance.\nUnseen Tasks. To investigate whether the model can generalize to a new combination of formulas unseen in the training data when we generate our training data, we ensure that the references are never associated with some combinations of atom concepts (s1, s2 \u2208\u03a3). We then test the trained model on tasks where v+ and v\u2212are the disjunctions of unseen combinations. Additionally, while in the training data, each reference symbol is associated with only two atom concepts; we test whether the model can perform in-context learning on tasks where v+ and v\u2212are the disjunctions of three atom concepts.\n# 4.3. Experiment Details\nWe use pskip = 0.25 in our experiment. We generate 60,000 documents with 16 paragraphs following the process described in \u00a7 4. Among them, we use 10k for validation. We train a 6-layer Transformer (Vaswani et al., 2017) model until the loss on the validation set converges. We include additional setups in \u00a7G\n# 4.4. Experiment Results and Discussion\nFigure 1 shows that the model trained with Calcutec can perform in-context learning, indicating that the model can\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee3b/ee3b6e5c-71e9-4ec5-9e2c-dda51c201796.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Verbalizers = r1, r2 single double triple</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/453a/453a0cd6-7917-43f8-9e9b-fc9019ec1641.png\" style=\"width: 50%;\"></div>\nFigure 1: In-context learning accuracy with Calcutec when using different verbalizers (r1, r2 or r3, r4). The dotted lines represent the performance of unseen combinations described in \u00a74.2. The colors represent the number of formulas in \u03a3 each class (v+ or v\u2212) is associated with. The main lines represent the average accuracy of 5 tasks. We plot the performance of each task in lighter colors.\nr1, r2\nr3, r4\nTask\nICL\nCoT\nICL\nCoT\nSingle\n57.1\n91.7\n55.6\n92.0\nDouble\n53.5\n76.3\n51.1\n77.1\nTriple\n53.0\n73.0\n51.7\n73.4\nTable 2: The 4-shot accuracy of in-context learning (ICL) versus chain-of-thought (CoT) using different verbalizers.\ntask\ndirect\npronoun\nSST-2\n63.0\n65.3\nCR\n61.7\n62.9\nMR\n59.2\n56.7\nSubj\n51.0\n62.2\nTable 3: The accuracy of using task-specific templates/verbalizers (direct) (Min et al., 2022a) v.s. using taskagnostic templates/pronouns for 16-shot in-context learning with GPT2-Large.\nTraining 4 9 2 8 4 6 + 0 8 0 3 5 0 = 0 0 0 0 0 0 ; 0 9 2 8 4 6 + 0 8 0 3 5 0 = 4 0 0 0 0 0 ; 0 0 2 8 4 6 + 0 0 0 3 5 0 = 4 7 1 0 0 0 ; 0 0 0 8 4 6 + 0 0 0 3 5 0 = 4 7 3 0 0 0 ; \u00b7 \u00b7 \u00b7 0 0 0 0 0 0 + 0 0 0 0 0 0 = 4 7 3 1 0 7 ; Testing 8 7 4 0 1 6 + 0 9 2 1 5 0 = 0 0 0 0 0 0 ; 0 0 0 0 0 0 + 0 0 0 0 0 0 =\nTable 4: Training and testing examples for the digit addition task.\ngeneralize under the distribution shifts described in \u00a73. This evidence supports our Pelican Soup hypothesis. We further inspect the in-context learning performance under the distribution shifts described in \u00a74.2:\n\u2022 Infrequent verbalizer: We observe similar performance of using frequent reference symbols (r1, r2) or infrequent reference symbols (r3, r4) as the verbalizers.\n\u2022 Unseen tasks: Figure 1 shows that the model has similar performance over tasks defined with unseen and unseen combinations of atom concepts (dot lines and solid lines). The models can also generalize to tasks defined with three formulas (green lines).\nIn sum, the results show that the model can generalize well under several distributional shifts.\nWe experiment with 4-shot learning using chain-of-thought. Table 2 shows that the model also benefits from chain-ofthought. We conjecture that it is because chain-of-thought has a format more similar to the format of training.\n# 5. Digit Addition Task\nIn \u00a7 4, we show that transformer models can generalize to downstream tasks where the intermediate reasoning steps are missing. As this is related to models\u2019 ability to predict P(r|x, K) (\u00a72.6) when we force the model to skip the intermediate steps, we are curious whether transformer models can still generalize even when more steps are missing. Since in the Calcutec environment, it is difficult to construct a task that requires a long reasoning process without exponentially increasing the size of the atom concept set \u03a3, We propose another toy task, the digit addition task, which involves longer reasoning processes.\n# 5.1. Setup\nThe digit addition task requires the model to perform the addition operation over two n-digit numbers. It requires n steps to solve if we solve it digit-by-digit. We generate a training set consisting of digit-by-digit reasoning steps for 100,000 pairs of numbers. Each intermediate step may be dropped at a constant probability pdrop. After training a Transformer model with the language modeling objective, we test whether the model can generate the final answer without generating the intermediate steps. We experiment\nwith different models of different sizes. From this, we can verify whether models of different sizes can acquire the intuition from the language modeling task. We show examples of our data in Table 4. Each digit sequence represents a number from the lower digit to the higher digit. The reasoning process in the training set gradually updates both sides of \u201c=\u201d from the lowest digit to the highest digit. As for the testing example, we skip the intermediate steps, making the model complete right-hand side of \u201c=\u201d in the last step. The model needs to acquire intuition from the training set in order to solve the testing samples. (We include a rigorous description in Appendix I.)\nwith different models of different sizes. From this, we can verify whether models of different sizes can acquire the intuition from the language modeling task.\n# 5.2. Results and Discussion\nWe report the exact match accuracy and the digit-level accuracy of models trained with different pdrop in Figure 2 with 5 random seeds. A higher accuracy implies the better intuition the model acquires from the sequence modeling task. The results show that three of the four models can achieve perfect accuracy when pdrop is as small as 0.3. It suggests that it is possible that the model can predict the final answer even without intermediate steps, even when examples in the training data are mostly step-by-step. We also observe that larger models tend to have higher and more stable accuracy. When the number of digits is 6 (Figure 12 in the appendix), only the largest model can achieve perfect accuracy. This observation is aligned with the emergence of large language models\u2019 ability.\n# 6. Real-world Evidence\nWe inspect whether LMs can do ICL with pronouns well because pronouns are reference words frequently associated with different meaning and our framework suggests that LMs learn ICL ability by modeling the association between reference words and their meaning. We thus experiment with the template \u201c[input]\u201d, [verbalizer] thought. and use \u201che\u201d, \u201cshe\u201d as the verbalizers. We follow the setup in Min et al. (2022a) and compare the accuracy of the binary classification tasks using GPT2-Large.\nTable 3 shows that this task-agnostic template with pronouns is competitive with those task-specific templates. This contradicts with the belief that only larger models can do incontext learning with task-irrelevant verbalizer Wei et al. (2023). It suggests that modeling reference-meaning may\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d09b/d09b5b06-00ff-4e16-b0af-7768558f8956.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">.26 0.28 0.3 0.3 0.32 0.34 0.36 0.4 0.45 2 (4 layers) GPT-2 (6 layers) GPT-2 Small GPT-2 Medium</div>\n<div style=\"text-align: center;\">0.23 0.24 0.26 0.28 0.3 0.3 0.32 0.34 0.36 0.4 GPT-2 (4 layers) GPT-2 (6 layers) GPT-2 Small GPT</div>\nFigure 2: The exact accuracy (y-axis, solid points) and digit-level accuracy (y-axis, hollow points) versus validation lo (x-axis) for the 5-digit addition task for five random seeds and different dropping rates pdrop (In the appendix, we inclu the complete results in Figure 11 and the results of the 6-digit addition task in Figure 12.)\nindeed contribute LMs\u2019 ICL ability.\n# 7. Related Work\nSince Brown et al. (2020) discovered large language models\u2019 in-context learning ability, some theoretical works have attempted to explain how language models acquire this ability. Based on a hidden Markov model (HMM) assumption on the language generation process, Xie et al. (2022) suggested that in-context learning is an implicit Bayesian inference process. Hahn & Goyal (2023) defined the generation process with Compositional Attribute Grammar, which is weaker than the HMM assumption, explaining the in-context learning ability with the minimum description length. Zhang et al. (2023) assumed a more general latent variable model. Arora & Goyal (2023) analyze the emergence of skills based on the scaling law (Hoffmann et al., 2022). While their analysis assumes a set of skills as the atomic elements of NLP tasks, our framework is based on a set of atom concepts.\nSince Brown et al. (2020) discovered large language models\u2019 in-context learning ability, some theoretical works have attempted to explain how language models acquire this ability. Based on a hidden Markov model (HMM) assumption on the language generation process, Xie et al. (2022) suggested that in-context learning is an implicit Bayesian inference process. Hahn & Goyal (2023) defined the generation process with Compositional Attribute Grammar, which is weaker than the HMM assumption, explaining the in-context learning ability with the minimum description length. Zhang et al. (2023) assumed a more general latent variable model. Arora & Goyal (2023) analyze the emergence of skills based on the scaling law (Hoffmann et al., 2022). While their analysis assumes a set of skills as the atomic elements of NLP tasks, our framework is based on a set of atom concepts. There were also many empirical studies on the in-context learning ability. Some works focused on the effect of the instruction (Webson & Pavlick, 2022; Lampinen et al., 2022; Jang et al., 2023), while some focused on the examples in the demonstration (Liu et al., 2022; Lu et al., 2022; Sorensen et al., 2022; Min et al., 2022b; Yoo et al., 2022; Ye et al., 2023; Chang & Jia, 2023; Ye et al., 2023; Wang et al., 2023b; Kossen et al., 2023). Shin et al. (2022) found that not all training corpora led to in-context learning ability. Prystawski & Goodman (2023) used synthetic data to suggest that the pretraining dataset\u2019s locality structure contributes to the reasoning steps\u2019 effectiveness. Wang et al. (2023a) studied the reasoning steps in chain-of-thought. Aky\u00a8urek et al. (2024) formulated ICL as learning a formal language from demonstrations and benchmarked model families. Some previous work studied in-context learning as a metalearning-like problem (Chen et al., 2022). Some works focused on the relationships between in-context learning and optimization algorithms (Garg et al., 2022; von Oswald et al., 2022; Aky\u00a8urek et al., 2023; Fu et al., 2023; Guo et al.,\nThere were also many empirical studies on the in-context learning ability. Some works focused on the effect of the instruction (Webson & Pavlick, 2022; Lampinen et al., 2022; Jang et al., 2023), while some focused on the examples in the demonstration (Liu et al., 2022; Lu et al., 2022; Sorensen et al., 2022; Min et al., 2022b; Yoo et al., 2022; Ye et al., 2023; Chang & Jia, 2023; Ye et al., 2023; Wang et al., 2023b; Kossen et al., 2023). Shin et al. (2022) found that not all training corpora led to in-context learning ability. Prystawski & Goodman (2023) used synthetic data to suggest that the pretraining dataset\u2019s locality structure contributes to the reasoning steps\u2019 effectiveness. Wang et al. (2023a) studied the reasoning steps in chain-of-thought. Aky\u00a8urek et al. (2024) formulated ICL as learning a formal language from demonstrations and benchmarked model families. Some previous work studied in-context learning as a metalearning-like problem (Chen et al., 2022). Some works focused on the relationships between in-context learning and optimization algorithms (Garg et al., 2022; von Oswald et al., 2022; Aky\u00a8urek et al., 2023; Fu et al., 2023; Guo et al.,\n2023). Some works inspected the mechanism of ICL in transformer models (Hendel et al., 2023; Bietti et al., 2023; Todd et al., 2023; Shen et al., 2023; Bai et al., 2023). Chan et al. (2022) studied the properties of dataset distribution that could contribute to the in-context learning ability. Li et al. (2023) provided generalization bounds based on the stability of Transformer models and the distance of downstream tasks. Compared to these works, we focus on how the pretraining data in natural language contributes to the incontext learning ability.\n# 8. Conclusion\nIn this work, we propose a framework that instantiates a latent variable model for analyzing ICL. Compared with previous works (Xie et al., 2022; Zhang et al., 2023), our latent model better reflects the complexity of language. Under our framework, we establish an ICL loss bond that provides insights into the effect of the choice of verbalizers, downstream task distribution, and instruction tuning. We further provide extensions to the framework that allow us to analyze the generalization to unseen tasks using the results by Arora & Goyal (2023) and relate our bound to the function description length discussed by Hahn & Goyal (2023).\nIn this work, we propose a framework that instantiates a latent variable model for analyzing ICL. Compared with previous works (Xie et al., 2022; Zhang et al., 2023), our latent model better reflects the complexity of language. Under our framework, we establish an ICL loss bond that provides insights into the effect of the choice of verbalizers, downstream task distribution, and instruction tuning. We further provide extensions to the framework that allow us to analyze the generalization to unseen tasks using the results by Arora & Goyal (2023) and relate our bound to the function description length discussed by Hahn & Goyal (2023). We then present a toy setup, Calcutec, as a concrete instance of our framework. Calcutec mimics the distribution shifts LMs need to overcome to perform ICL. Our empirical results show that transformers trained with autoregressive loss can generalize under these distributional shifts. We additionally use an addition task to show that larger models generalize better, potentially explaining why scaling up model size helps LM performance. Finally, we experiment with real-world NLP tasks and show that GPT-Large can do ICL with pronouns, providing evidence supporting our framework. We expect that our toy datasets can facilitate future research on ICL.\nWe then present a toy setup, Calcutec, as a concrete instance of our framework. Calcutec mimics the distribution shifts LMs need to overcome to perform ICL. Our empirical results show that transformers trained with autoregressive loss can generalize under these distributional shifts. We additionally use an addition task to show that larger models generalize better, potentially explaining why scaling up model size helps LM performance. Finally, we experiment with real-world NLP tasks and show that GPT-Large can do ICL with pronouns, providing evidence supporting our framework. We expect that our toy datasets can facilitate future research on ICL.\n# Impact Statements\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\n# Acknowledgements\nWe appreciate Joshua Robinson, Ollie Liu, and Xinyan Velocity Yu for providing valuable feedback on this paper.\n# References\nAky\u00a8urek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=0g0X4H8yN4I.\nAky\u00a8urek, E., Wang, B., Kim, Y., and Andreas, J. In-context language learning: Arhitectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.\nArora, S. and Goyal, A. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\nBai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.\nBietti, A., Cabannes, V., Bouchacourt, D., Jegou, H., and Bottou, L. Birth of a transformer: A memory viewpoint. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=3X2EbBLNsk.\nBresnan, J. and Bresnan, J. W. The mental representation of grammatical relations, volume 1. MIT press, 1982.\nrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf. Carnap, R. et al. Logische syntax der sprache. Springer, 1968.\nCarnap, R. et al. Logische syntax der sprache. Springer, 1968. Chan, S. C. Y., Santoro, A., Lampinen, A. K., Wang, J. X., Singh, A., Richemond, P. H., McClelland, J., and Hill, F. Data distributional properties drive emergent in-context learning in transformers, 2022. Chang, T.-Y. and Jia, R. Data curation alone can stabilize incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8123\u20138144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.452. URL https: //aclanthology.org/2023.acl-long.452. Chen, Y., Zhong, R., Zha, S., Karypis, G., and He, H. Metalearning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 719\u2013730, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.53. URL https://aclanthology.org/ 2022.acl-long.53. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Croft, W. Representations of commonsense knowledge: Ernest davis. Artificial Intelligence, 61 (1):105\u2013112, 1993. ISSN 0004-3702. doi: https://doi.org/10.1016/0004-3702(93)90097-U. URL https://www.sciencedirect.com/ science/article/pii/000437029390097U. Dowling, W. F. and Gallier, J. H. Linear-time algorithms for testing the satisfiability of propositional horn formulae. The Journal of Logic Programming, 1(3):267\u2013284, 1984. ISSN 0743-1066. doi: https://doi.org/10.1016/0743-1066(84)90014-1. URL https://www.sciencedirect.com/ science/article/pii/0743106684900141. Fodor, J. A. The Language of Thought. Harvard University Press, 1975.\nChan, S. C. Y., Santoro, A., Lampinen, A. K., Wang, J. X., Singh, A., Richemond, P. H., McClelland, J., and Hill, F. Data distributional properties drive emergent in-context learning in transformers, 2022.\nChang, T.-Y. and Jia, R. Data curation alone can stabilize incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8123\u20138144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.452. URL https: //aclanthology.org/2023.acl-long.452.\nChen, Y., Zhong, R., Zha, S., Karypis, G., and He, H. Metalearning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 719\u2013730, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.53. URL https://aclanthology.org/ 2022.acl-long.53.\nFodor, J. A. LOT 2: The Language of Thought Revisited. Oxford University Press, 08 2008. ISBN 9780199548774. doi: 10.1093/acprof:oso/9780199548774.001.0001. URL https://doi.org/10.1093/acprof: oso/9780199548774.001.0001.\nGuo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.\nHahn, M. and Goyal, N. A theory of emergent in-context learning as implicit structure induction. arXiv preprint arXiv:2303.07971, 2023.\nHendel, R., Geva, M., and Globerson, A. In-context learning creates task vectors. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 9318\u20139333, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 624. URL https://aclanthology.org/2023. findings-emnlp.624.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., and Sifre, L. An empirical analysis of compute-optimal large language model training. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=iBBcRUlOAPR.\nHorn, A. On sentences which are true of direct unions of algebras. The Journal of Symbolic Logic, 16(1):14\u201321, 1951. ISSN 00224812. URL http://www.jstor. org/stable/2268661.\nHorn, A. On sentences which are true of direct unions of algebras. The Journal of Symbolic Logic, 16(1):14\u201321, 1951. ISSN 00224812. URL http://www.jstor. org/stable/2268661.\nang, J., Ye, S., and Seo, M. Can large language models truly understand prompts? a case study with negated prompts. In Albalak, A., Zhou, C., Raffel, C., Ramachandran, D., Ruder, S., and Ma, X. (eds.), Proceedings of The 1st Transfer Learning for Natural Language Processing Workshop, volume 203 of Proceedings of Machine Learning Research, pp. 52\u201362. PMLR, 03 Dec 2023. URL https://proceedings.mlr.press/ v203/jang23a.html.\nKossen, J., Rainforth, T., and Gal, Y. In-context learning in large language models learns label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375, 2023.\nLampinen, A., Dasgupta, I., Chan, S., Mathewson, K., Tessler, M., Creswell, A., McClelland, J., Wang, J., and Hill, F. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537\u2013563, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. findings-emnlp.38.\ni, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. Transformers as algorithms: Generalization and stability in in-context learning. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 19565\u201319594. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/ v202/li23l.html.\nLiu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013 114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. deelio-1.10. URL https://aclanthology.org/ 2022.deelio-1.10.\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https: //aclanthology.org/2022.acl-long.556.\nMurphy, G. The big book of concepts. MIT press, 2004.\nPeirce, C. S. A theory of probable inference. Little, Brown and Co, 1883.\nPiantadosi, S. T. The computational origin of representation. Minds Mach., 31(1):1\u201358, mar 2021. ISSN 0924-6495. doi: 10.1007/s11023-020-09540-9. URL https:// doi.org/10.1007/s11023-020-09540-9.\nPrystawski, B. and Goodman, N. D. Why think step-bystep? reasoning emerges from the locality of experience. arXiv preprint arXiv:2304.03843, 2023.\nRottger, P., Vidgen, B., Hovy, D., and Pierrehumbert, J. Two contrasting data annotation paradigms for subjective NLP tasks. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 175\u2013190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.13. URL https: //aclanthology.org/2022.naacl-main.13. Sag, I. A., Wasow, T., Bender, E. M., and Sag, I. A. Syntactic theory: A formal introduction, volume 92. Center for the Study of Language and Information Stanford, CA, 1999. Schank, R. C. and Abelson, R. P. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. 1988. Shen, L., Mishra, A., and Khashabi, D. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023. Shin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B., Cho, K., Lee, G., Park, W., Ha, J.-W., and Sung, N. On the effect of pretraining corpora on in-context learning by\norensen, T., Robinson, J., Rytting, C., Shaw, A., Rogers, K., Delorey, A., Khalil, M., Fulda, N., and Wingate, D. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 819\u2013 862, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 60. URL https://aclanthology.org/2022. acl-long.60.\nSteedman, M. Combinatory grammars and parasitic gaps. Natural Language & Linguistic Theory, 5(3):403\u2013439, 1987.\nSteedman, M. Surface structure and interpretation, volume 30. MIT press Cambridge, MA, 1996.\nTodd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper pdf.\nvon Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022.\nWang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. In ICLR\n2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023a. URL https: //openreview.net/forum?id=L9UMeoeU2i.\nWang, X., Zhu, W., and Wang, W. Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916, 2023b.\nWebson, A. and Pavlick, E. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2300\u20132344, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 167. URL https://aclanthology.org/2022. naacl-main.167.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https:// openreview.net/forum?id=yzkSU5zdwD. Survey Certification.\nWei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=RdJVFCHjUMI.\nYe, X., Iyer, S., Celikyilmaz, A., Stoyanov, V., Durrett, G., and Pasunuru, R. Complementary explanations for effective in-context learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 4469\u20134484, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 273. URL https://aclanthology.org/2023. findings-acl.273.\nYoo, K. M., Kim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.W., Lee, S.-g., and Kim, T. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2422\u20132437, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https:// aclanthology.org/2022.emnlp-main.155.\nZhang, Y., Zhang, F., Yang, Z., and Wang, Z. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023.\n# A. A Pelican Soup Example\nA. A Pelican Soup Example Puzzle master: A men walks into a restaurant and orders pelican soup. After taking a sip, he loses his mind. Wh Participants: Is it because the soup is not cooked well? Puzzle master: No. Participants: Is it because the soup toxic? Puzzle master: No. Participants: Does the soup remind him something? Puzzle master: Yes. Participants: Did someone cook pelican soup for him? Puzzle master: Yes. Participants: Is that person still alive? Puzzle master: No. For the sake of aesthetics, we do not include the latent story here. If you are interested, please check it online.\n# B. Proof of Theorem 2.6\nLet St = x1, r2, x2, r2, \u00b7 \u00b7 \u00b7 , xt, rt.\nPr(z, g|St, K) = Pr(St|z, g, K) Pr(z|K) \ufffd z Pr(St|z, g, K) Pr(z, g|K) = Pr(z\u2032, g|K) \ufffdt i=1 Pr(xt, rt, d|z, g, St\u22121, K) \ufffd z\u2032,g Pr(z, g|K) \ufffdi = 1t Pr(xt, rt, d|z\u2032, g, St\u22121, K\nSt, K) = \ufffd z,g Pr(xt+1, rt+1, d|z, St, K) Pr(z, g|St, K) = \ufffd z,g Pr(z, g|K) \ufffdt+1 i=1 Pr(xt, rt, d|z, g, St\u22121, K) \ufffd z\u2032 Pr(z, g|K) \ufffdt i=1 Pr(xt, rt, d|z\u2032, g, St\u22121, K) .\n\ufffd z,g || = \ufffd z,g Pr(z, g|K) \ufffdt+1 i=1 Pr(xt, rt, d|z, g, St\u22121, K) \ufffd z\u2032 Pr(z, g|K) \ufffdt i=1 Pr(xt, rt, d|z\u2032, g, St\u22121, K) .\nThus,\n# C. Proof of Corollary 2.7\nThe second term in the right-hand side of Eq. 2 is zero when the decoding constrain K is imposed. Therefore, it suffices to prove the last term is non-negative in expectation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/526f/526f9a5a-047b-49a8-a99f-eef8378bc617.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\ufffd \ufffd =KLD(Pr(xt|z\u2217, \u02d9g, K)|| Pr(xt|St\u22121, K)) \u22650</div>\n# D. The Connection between P(rt|xt, z\u2217, \u00a8g) and Function Description Length by Hahn & Goyal (2023)\n# D. The Connection between P(rt|xt, z\u2217, \u00a8g) and Function Description L (2023)\nFirstly, we make some regularity assumptions: Given a step-by-step reasoning process \u03c0 = s1, s2, \u00b7 \u00b7 \u00b7 , sn for the induction process of P |= Q, in the training data,\n# 1. each step may be dropped independently to each other with probability pdrop. 2. Pr(si|P, s1, s2, \u00b7 \u00b7 \u00b7 , si\u22121) > pmin for all i \u2208[n].\nWe first show how we derive Eq. 4: Based on Assumption 2.9,\nPr(rt|xt, z\u2217, \u00a8g) = \ufffd \u03c0\u2208\u03a0 Pr(\u03c0, rt|xt, z\u2217, \u00a8g) Pr(\u03c0 is dropped),\nwhere \u03a0 is a set of token sequences representing reasoning steps that induce rt from xt. Let \u03c0\u2217be the shortest p we have\nwe have\nlog Pr(rt|xt, z\u2217, \u00a8g) = log \ufffd \u03c0\u2208\u03a0 Pr(\u03c0, rt|xt, z\u2217, \u00a8g) Pr(\u03c0 is dropped)\n\ufffd \u2208 \u2265log Pr(\u03c0\u2217, rt|xt, z\u2217, \u00a8g) Pr(\u03c0\u2217is dropped) \u2265pmin log \u2113(\u03c0\u2217) + pdrop log \u2113(\u03c0\u2217).\nThen we can discuss the connection between Pr(rt|xt, z\u2217, \u00a8g) and the function description length by Hahn & Goyal (2023). We can see the dropped reasoning steps in \u03c0\u2217as the hidden (tree) structure that maps xt to rt as the derivation tree \u03c4\u03d5 in the bound of Hahn & Goyal (2023). The length of the reasoning steps thus corresponds to the description length of the derivation tree D(\u03c4\u03d5). A major difference between the bound by Hahn & Goyal (2023) and our bound is that their bound has D(\u03c4\u03d5) constant to T while our bound has \ufffd t log Pr(rt|xt, z\u2217, \u00a8g), which potentially grows proportionally to T. The cause of this difference is that, Hahn & Goyal (2023) assumes a structure that repetitively applies a function mapping in a document, and the number of repetition is independent to the complexity of the function mapping. In comparison, our framework does not make this assumption.\n# E. Detailed Gengeration Process of the LM Training Data in Calcutec\nrate a paragraph based on Assumption 2.3 in the following step:\nWe generate a paragraph based on Assumption 2.3 in the following step:\n1. We pick a symbol s from the symbols associated with ra uniformly at random.\n1. We pick a symbol s from the symbols associated with ra uniformly at random. 2. We randomly generate a proof for KB, P |= g, where P \u2282\u03a3 is the premise and g \u2208\u03a3 is the goal of the proof. We ensure that this proof contains the topic s.\n3. We convert the proof tree to a sequence of proving steps by traversing the proving tree in a topological order with tie broken randomly. Each node in the proof tree corresponds to a rule in KB, so the resulting sequence of proving step consists of horn clauses in the form a1a2 \u2192b. We separate the clauses in the sequence with commas.\n4. We rewrite the first step of the proving process to contain the premises of the proof. Specifically, we replace th antecedent in the first formula with the premise P. We find that this step is necessary to prevent the language mode trained on it from hallucinating irrelevant variables randomly. It is important for our experiment for chain-of-though but is not necessary for language models to learn the in-context learning ability.\n# F. Perturbations in Calcutec\n# We apply two types of perturbations over the reasoning steps in Calcutec described in \u00a74:\nWe apply two types of perturbations over the reasoning steps in Calcutec described in \u00a7\n1. Random merge: At probability pmerge, for every two consecutive clauses where the consequence of the first one is in the antecedents of the second one, say a1a2 \u2192b1 and b1a3 \u2192b2, we merge them into a single clause a1a2a3 \u2192b2. 2. Random drop: Given a clause a1a2 \u00b7 \u00b7 \u00b7 an \u2192b. We drop each of the antecedents a \u2208{a1, a2, \u00b7 \u00b7 \u00b7 an} at probability pdrop. We apply this dropping to every clause in the proof except the first one to ensure that we do not drop the premises.\nAlgorithm 1 Pseudo code for the generation process of an Calcutec document used for training. Sample ra, rb from {r1, r2, r3, r4} with probability 0.45, 0.45, 0.05, 0.05. Sample topic S = {s1, s2} \u2282\u03a3. Initialize a document D with empty string. for p = 1, 2, . . . , npar do while True do Sample s \u2208S. Sample a set X \u2282\u03a3 such that \ufffd x\u2208X x |= s. Run the resolution algorithm to get the set M = {m|X |= m}. Find an extra premise x\u2032 that can increase the depth of deepest proof tree for X |= m. Run the resolution algorithm to get the set M \u2032 = {m|X \u222a{x\u2032} |= m}. if |M \u2032| > |\u03a3| 2 then Reject the sampled X \u222a{x\u2032}. {We don\u2019t want a premise that entails everything.} Restart the while loop. end if Sample a g \u2208M \u2032 such that the proof tree for X\u2032 |= g contains s and its depth > dmin. {We use dmin = 4 in our experiments.} Do topological sort to flatten the proof tree and convert it into a string. Append the string to D. end while end for for s \u2208S do D \u2190D.replace(s, ra) end for Let S\u2032 = {s\u2032 1, s\u2032 2} \u2208\u03a3 be the top-2 frequent non ra symbols in D. for s\u2032 \u2208S\u2032 do D \u2190D.replace(s\u2032, rb) end for\nWe use pmerge = pdrop = pskip. Additionally, when flatting the proof trees with topological sort, we break ties randomly. We also randomize the order of symbols in the antecedents.\nAdditionally, when flatting the proof trees with topological sort, we break ties randomly. We also randomize the order of the symbols in the antecedents.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9743/9743ff42-be59-4bff-a990-e38ff07b7827.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The distribution of lengths and the first step in each paragraph where z is the consequence in the Calcutec datas The first/second row are the statistics before/after some steps are randomly dropped.</div>\n# G. Extra Experiments with Calcutec\n# G. Extra Experiments with Calcutec G.1. Additional Setups\nUnseen Inference Process. Based on Assumption 2.9 and Assumption 2.5, input-label pairs of a downstream task corresponds to prefix-reference pairs in a paragraph. To examine whether the trained model can generalize well when the induction process for the label is different from the induction process for the pronoun in the training data, we generate a training dataset where all the pronouns are induced from the premise with a left-branching proof tree with a depth equal to 2 (Figure 4a), while the test data contains samples whose labels are induced from the input with balanced trees (Figure 4b). Different Input Lengths. For each downstream tasks, we experiment with examples with different lengths. When the inference process is branching, having input length equal to 4 makes the proving tree deeper. No perturbations. As described in \u00a7F, we apply some random perturbations on the proving process. We also experiment with the setup where we do not apply any perturbations. With/Without Rewriting the First Step. As described in \u00a7E, we rewrite the first step of the proof. We also experiment with the setup where we do not rewrite the first step. Model Size. We also experiment with different models sizes. We experiment with GPT-2 models that have 3, 4 and 5 layers.\nUnseen Inference Process. Based on Assumption 2.9 and Assumption 2.5, input-label pairs of a downstream task corresponds to prefix-reference pairs in a paragraph. To examine whether the trained model can generalize well when the induction process for the label is different from the induction process for the pronoun in the training data, we generate a training dataset where all the pronouns are induced from the premise with a left-branching proof tree with a depth equal to 2 (Figure 4a), while the test data contains samples whose labels are induced from the input with balanced trees (Figure 4b).\nDifferent Input Lengths. For each downstream tasks, we experiment with examples with different lengths. When t inference process is branching, having input length equal to 4 makes the proving tree deeper.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/855d/855dddc7-3e35-463f-aace-de5d7468e679.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/53a4/53a452af-f042-4ae7-b7c1-acb9b894940b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) The proof tree a paragraph in the training dataset corresponds .</div>\n(a) The proof tree a paragraph in the training dataset corresponds .\n<div style=\"text-align: center;\">Figure 4: Proof trees examples.</div>\nBranching\nBalanced\nr1, r2\nr3, r4\nr1, r2\nr3, r4\nTask\nICL\nCoT\nICL\nCoT\nICL\nCoT\nICL\nCoT\nSingle\n57.1\n91.7\n55.6\n92.0\n68.5\n89.8\n64.9\n90.3\nDouble\n53.5\n76.3\n51.1\n77.1\n58.5\n76.1\n56.2\n75.8\nTriple\n53.0\n73.0\n51.7\n73.4\n57.0\n68.2\n54.2\n67.0\n<div style=\"text-align: center;\">Table 5: The 4-shot accuracy of in-context learning (ICL) versus chain-of-thoughts (CoT).</div>\n# G.2. Results and Discussion\nUnseen Inference Process. Figure 5a and Figure 5d show that the ICL performance on the branching examples is similar to the performance on the branching examples. It suggests that the model can generalize to examples that requires an unseen reasoning process. Interestingly, Table 5 show that using chain-of-thoughts mitigates this gap. Different Input Lengths. Figure 5b and Figure 5e show that the model can still do ICL for the examples with length equal to 4. However, compared with the performance on examples with length equal to 3 (Figure 5c and Figure 5f), the performance is worse. This may be because solving these length-4 examples requires more reasoning steps. With/Without Rewriting the First Step. Figure 7 shows that models trained with proofs that are rewritten has similar performance as models trained with the proofs that were rewritten (Figure 5). This suggests that rewriting the first step in the proof is not necessary for the model to acquire the ICL ability. Model Size. Figure 8 show that deeper models have better ICL performance. It is aligned with the real-world observation that scaling helps model performance.\n# H. Hyper-parameters\nWe train our model using batch size 256, warm up ratio 5%, and we truncate the sequence length to 512 tokens and the default parameters for the optimizer. We use the implementation of GPT-2 by Hugging Face transformers v4.27.2. All models can be trained with 4 RTX 2080ti within 8 hours.\n# I. Formal Description of the Digit Addition Data\nFor each step i, we represent the step in the format a(i) +b(i) = c(i), where a(i), b(i) and c(i) are sequences of n tokens, each of which is in [0, 9], representing a number from the lowest digit to the highest digit. a(0) and b(0) represent two randomly drawn numbers and c(0) is all zero. At each step i > 0, most of the digit in a(i), b(i), c(i) is the same as the previous step. For a(i) and b(i), we only update the ith digit by setting a(i) i = 0 and b(i) i = 0. As for c(i), it serves as a buffer for both the answer and the carry. We update it based on s(i) = a(i\u22121) i + b(i\u22121) i + c(i\u22121) i , the summation of the digits at i. We set c(i) i = s(i) mod 10, c(i) i+1 = \u230as(i)/10\u230b. We use colons as the separator and concatenate these steps as a single sequence. When testing a model\u2019s intuition, we let the model generate the continuation for a(0) + b(0) = c(0); a(n) + b(n) =. Note that a(n) = b(n) = 0, so the model needs to have the intuition to generate the answer correctly. We provide examples in Table 4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd6d/cd6dbda6-4499-401c-9f8d-fc45044adb12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) A balanced tree for a downstream task sample.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/847f/847f5055-8945-44a7-b6cd-13fa45994d2b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) balanced, length = 4, y1, y2 single double triple</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2374/2374257b-48f5-4fc8-b329-82aac1e757c7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) balanced, length = 3, y3, y4</div>\n<div style=\"text-align: center;\">Figure 5: In-context learning accuracy with Calcutec when using different verbalizers (y1, y2 or y3, y4) and input length (3 or 4). The dotted lines represent the performance of unseen combinations described in \u00a74.2, while the different color represent the number of formulas each class (v+ or v\u2212) is associated to. The main lines represent the average accuracy of  tasks. We plot the performance of each task in lighter colors.</div>\n<div style=\"text-align: center;\">0 5 10 15 20 25 4 # of Demonstrations (c) branching, length = 3, y1, y2 single double triple</div>\n<div style=\"text-align: center;\">(f) branching, length = 3, y3, y4</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7765/7765b8df-1ad4-4016-a32e-5df6391cd5a0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 5 10 15 20 25 # of Demonstrations (a) balanced, length = 4, r1, r2 single double triple</div>\n<div style=\"text-align: center;\">(b) branching, length = 4, r1, r2 single double triple</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8642/8642faca-7923-424a-ba10-1c1da31f9901.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 5 10 15 20 25 # of Demonstrations (d) balanced, length = 4, r3, r4</div>\n<div style=\"text-align: center;\">Figure 6: In-context learning accuracy with Calcutec when no steps are dropped (pskip = 0).</div>\n<div style=\"text-align: center;\">ntext learning accuracy with Calcutec when no steps are dropped (pskip =</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c18/3c189313-16e5-4d06-bfac-1b2064138dac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"># of Demonstrations (a) balanced, length = 4, r1, r2 single double triple</div>\n<div style=\"text-align: center;\">(d) balanced, length = 4, r3, r4</div>\n<div style=\"text-align: center;\">0 5 10 15 20 25 4 # of Demonstrations (c) branching, length = 3, r1, r2 single double triple</div>\n<div style=\"text-align: center;\">0 5 10 15 20 25 4 # of Demonstrations (f) branching, length = 3, r3, r4</div>\n<div style=\"text-align: center;\">(f) branching, length = 3, r3, r4</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a83/2a83cc46-d3e1-44ae-8e84-8a5571e9a47d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: The in-context learning performance when using models with different model depths.</div>\n<div style=\"text-align: center;\">0 5 10 15 20 25 3 4 # of Demonstrations (c) balanced, length = 4, 5 layers single double triple</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/efbf/efbfeba5-e3a1-46fd-be75-9bd310273011.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: The distribution of the number of reasoning steps in the dataset when some of them are dropped at different probability. Each number is the average over 5 datasets generated with different random seeds.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f8e/7",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of understanding in-context learning (ICL) in natural language processing (NLP) through a theoretical framework called the Pelican Soup Framework. Existing analyses often rely on latent variable models that do not adequately bridge the gap between theory and practice. The proposed framework aims to refine this understanding by introducing concepts such as a commonsense knowledge base and meaning association, ultimately providing a more accurate loss bound for ICL.",
        "problem": {
            "definition": "The primary problem is to understand why large language models (LMs) can perform ICL despite being trained on non-structural general text, particularly when the verbalizers used in demonstrations do not directly relate to the tasks.",
            "key obstacle": "A significant challenge is the oversimplification of natural language by previous models, which do not account for the complexities involved in the data generation process and the diversity of semantics within language."
        },
        "idea": {
            "intuition": "The idea is inspired by the Pelican Soup game, where participants use commonsense knowledge to deduce a hidden story through questions, paralleling how LMs should leverage commonsense knowledge to perform ICL.",
            "opinion": "The proposed framework suggests that LMs can generalize from training data to unseen tasks by effectively associating verbalizers with their meanings through a structured commonsense knowledge base.",
            "innovation": "The main difference compared to previous methods is the introduction of the commonsense knowledge base and the formalism for NLP classification tasks, which allows for a more nuanced understanding of how verbalizers and instruction tuning affect ICL performance."
        },
        "Theory": {
            "perspective": "The framework presents a theoretical perspective that incorporates commonsense reasoning and logical relationships to better understand the generative processes underlying ICL.",
            "opinion": "The theory posits that LMs can achieve ICL by modeling the associations between verbalizers and their meanings, drawing on cognitive science theories regarding how complex skills emerge.",
            "proof": "The proof involves establishing a loss bound for ICL that reflects the impact of verbalizer choice and instruction tuning, demonstrating that under certain conditions, the average cross-entropy loss converges to zero."
        },
        "experiments": {
            "evaluation setting": "The experiments utilize a synthetic dataset called Calcutec, designed to mimic the distribution shifts that LMs encounter during ICL. The evaluation includes various NLP tasks using the GPT-2 Large model.",
            "evaluation method": "The evaluation method involves testing the model's performance on ICL tasks with different verbalizers and observing the impact of distribution shifts on generalization."
        },
        "conclusion": "The experiments validate the Pelican Soup Framework, showing that LMs can generalize well under various distribution shifts and that larger models tend to perform better, thereby supporting the framework's assumptions about the role of commonsense knowledge in ICL.",
        "discussion": {
            "advantage": "One advantage of this paper is its comprehensive theoretical framework that integrates commonsense knowledge, providing insights into the mechanisms of ICL and addressing gaps in previous analyses.",
            "limitation": "A limitation is that the framework relies on assumptions about the structure of the latent space, which may not fully capture the complexities of natural language.",
            "future work": "Future work could explore refining the assumptions regarding the latent space and further empirical investigations into the role of different types of commonsense knowledge in enhancing ICL."
        },
        "other info": [
            {
                "info1": "The paper introduces a toy setup called Calcutec for experimental validation."
            },
            {
                "info2": {
                    "info2.1": "The framework is compatible with cognitive science theories on meaning representation.",
                    "info2.2": "The empirical results demonstrate the effectiveness of using pronouns as verbalizers in real-world NLP tasks."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of understanding in-context learning (ICL) in natural language processing (NLP) through a theoretical framework called the Pelican Soup Framework."
        },
        {
            "section number": "1.2",
            "key information": "The proposed framework aims to refine understanding of ICL by introducing concepts such as a commonsense knowledge base and meaning association."
        },
        {
            "section number": "1.3",
            "key information": "The primary problem is to understand why large language models (LMs) can perform ICL despite being trained on non-structural general text."
        },
        {
            "section number": "3.2",
            "key information": "The framework presents a theoretical perspective that incorporates commonsense reasoning and logical relationships to better understand the generative processes underlying ICL."
        },
        {
            "section number": "3.4",
            "key information": "The experiments utilize a synthetic dataset called Calcutec, designed to mimic the distribution shifts that LMs encounter during ICL."
        },
        {
            "section number": "6.1",
            "key information": "A significant challenge is the oversimplification of natural language by previous models, which do not account for the complexities involved in the data generation process."
        },
        {
            "section number": "7",
            "key information": "The experiments validate the Pelican Soup Framework, showing that LMs can generalize well under various distribution shifts."
        }
    ],
    "similarity_score": 0.7224691128532909,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Understanding In-Context Learning with a Pelican Soup Framework.json"
}