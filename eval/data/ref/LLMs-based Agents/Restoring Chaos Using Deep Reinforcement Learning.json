{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1912.00947",
    "title": "Restoring Chaos Using Deep Reinforcement Learning",
    "abstract": "A catastrophic bifurcation in non-linear dynamical systems, called crisis, often leads to their convergence to an undesirable non-chaotic state after some initial chaotic transients. Preventing such behavior has proved to be quite challenging. We demonstrate that deep Reinforcement Learning (RL) is able to restore chaos in a transiently-chaotic regime of the Lorenz system of equations. Without requiring any a priori knowledge of the underlying dynamics of the governing equations, the RL agent discovers an effective perturbation strategy for sustaining the chaotic trajectory. We analyze the agent's autonomous control-decisions, and identify and implement a simple control-law that successfully restores chaos in the Lorenz system. Our results demonstrate the utility of using deep RL for controlling the occurrence of catastrophes and extreme-events in non-linear dynamical systems.",
    "bib_name": "vashishtha2019restoringchaosusingdeep",
    "md_text": "# Restoring Chaos Using Deep Reinforcement Learning\nSumit Vashishtha\u2217and Siddhartha Verma\u2020 Department of Ocean and Mechanical engineering, Florida Atlantic University, Boca Raton, FL 33431, USA (Dated: December 3, 2019)\nA catastrophic bifurcation in non-linear dynamical systems, called crisis, often leads to their convergence to an undesirable non-chaotic state after some initial chaotic transients. Preventing such behavior has proved to be quite challenging. We demonstrate that deep Reinforcement Learning (RL) is able to restore chaos in a transiently-chaotic regime of the Lorenz system of equations. Without requiring any a priori knowledge of the underlying dynamics of the governing equations, the RL agent discovers an effective perturbation strategy for sustaining the chaotic trajectory. We analyze the agent\u2019s autonomous control-decisions, and identify and implement a simple control-law that successfully restores chaos in the Lorenz system. Our results demonstrate the utility of using deep RL for controlling the occurrence of catastrophes and extreme-events in non-linear dynamical systems.\n 27 Nov 2019\n27 Nov\nChaos is desirable and advantageous in many situations. For instance, in mechanics, exciting the chaotic motion of several modes spreads energy over a wide frequency range [1], thereby preventing undesirable resonance. Chaotic advection in fluids enhances mixing, as chaos brings about an exponential divergence of fluid packets that are initially in close proximity [2]. In biology, the absence of chaos may lead to an emergence of synchronous dynamics in the brain, which can result in epileptic seizures [3]. Moreover, the absence of chaos may also indicate the presence of other pathological conditions [4, 5]. In some cases, Chaos can become transient in nature, where the dynamics eventually converge to non-chaotic attractors. The typical route by which this happens is known as a crisis [6], where for certain parametervalues of the non-linear system, a chaotic-attractor collides with its basin-boundary and becomes a saddle. A saddle has a fractal structure with infinitely many gaps along its unstable-manifold. Any initial condition attracted towards this chaotic-attractor-turned-saddle escapes to an external periodic- or a fix-point-attractor. Such transient-chaos is often undesirable, and has been conjectured to be the culprit for phenomena such as voltage collapse in electric power systems [7] and species extinction in ecology [8]. It also plays a crucial role in governing the dynamics of shear flows in pipes and ducts at low Reynolds numbers [9, 10]. Given the importance of these phenomena, controlling transient-chaos is a pressing issue. Some attempts to restore chaos in such scenarios have been made in the past. Yang et al. [5] maintained chaos in transiently chaotic regimes of one- and two-dimensional maps using small perturbations. Their method relied on accurate analytical knowledge of the dynamical system, and required a priori phase-space knowledge of escape regions from chaos. Another method utilized the natural dynamics around the saddle [11, 12], where small regions near a chaotic-saddle through which trajectories escape\nwere identified. Then a set of \u201ctarget\u201d points in these regions were found, which yield trajectories that can stay near the chaotic saddle for a relatively long time. When the solution trajectory falls in this escape region, it is perturbed to the nearest target point so that the trajectory can persist near the chaotic saddle for a long time. The identification of such escape regions and target points can be challenging, and requires either an a priori computation of the probability distribution of escape times in different regions of state-space [11], or information from the return map constructed from local maxima or minima of a measured time series [12]. Such approaches become difficult for high-dimensional dynamical systems, and have been illustrated for 2D maps/flows at the most. One particular control technique that worked for the 3D Lorenzsystem was described in Cape\u00b4ans et al. [13]. The method was based on finding a certain control-perturbation set in the phase space, called a \u201csafe set\u201d, which avoids the escape of the trajectories to the fix-points. Identifying such a safe set can be prohibitively expensive computationally, and such safe sets may not exist for all dynamical systems. In recent years, a machine-learning technique called deep Reinforcement Learning (RL) has shown great promise in control-optimization problems [14], and it has been successfully used to uncover complex underlying physics in Navier-Stokes simulations of fishswimming [15]. The aim of this letter is to illustrate the utility of deep RL in determining small controlperturbations to parameters of the Lorenz system [16], such that a sustained chaotic behavior is maintained despite the uncontrolled dynamics being transientlychaotic. In doing so, no prior analytical knowledge about the dynamical system, and no special schemes to find escape regions, target points and safe sets will be employed. The RL algorithm is able to autonomously determine an optimal strategy to restore chaos, by continually interacting with the dynamical-system. As depicted in Fig. 1, a reinforcement learning prob-\nlem consists of five major elements - a learning agent, an environment described by a model Y (the Lorenz system in our case), state-space S, action-space A, and reward rt. Initially, the RL agent interacts with its environment in a trial-and-error manner. At each time step t, the agent receives the current state st of the environment, and selects an action at following a policy \u03a0(at|st). This action allows the agent to perturb the state of the environment, and move to a new state st+1 by evaluating the given model Y of the environment. Upon affecting this transition, the agent is rewarded (or punished) with reward rt. This process continues until the agent reaches a terminal state, at which point a new episode starts over. The return received from each episode is the discounted cumlative reward with discount factor \u03b3, which lies between 0 and 1. The discount factor makes it feasible to emphasize the importance of maximizing long-term reward, which enables the agent to prefer actions that are beneficial in the long-term. The cumulative reward, R(at|st), is given as,\n(1)\nThe goal of the RL agent is to maximize this cumulative reward by discovering an optimal policy \u03a0\u2217. There are a variety of methods available for attaining this. We make use of Proximal Policy Optimization (PPO) [17] which is a type of policy Gradient Method (PGM) [18]. PPO is suitable for continuous-control problems [19], and it is simpler in its mathematical implementation compared to other PGM based RL algorithms [20]. Moreover, PPO requires comparitively little hyper-parameter tuning for use in a variety of different problems. The specific implementation of the algorithm that we used, PPO2, is available as part of the OpenAI stable-baselines library [21]. The ergodic and unsteady nature of chaotic dynamics necessitates the use of a version of PPO2 wherein the policy is defined by deep recurrent neural networks comprised of long-short-term memory cells, instead of traditional feed-forward neural networks. The environment for the Lorenz system is written in a OpenAI gym [22] -compatible python format, and is provided as part of the supplementary materials. The relevant equations are given as,\n(2a)\n(2b) (2c)\n(2c)\nWith \u03c3 = 10 and \u03b2 = 8/3, \u03c1 = 28 gives rise to chaotic trajectories, whereas transient chaos is found in the interval \u03c1 \u2208[13.93, 24.06] [23]. Without any control implemented, the solution will converge to specific fix-points\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4da/b4da2e84-7feb-4582-a229-bc0dd80c94c3.png\" style=\"width: 50%;\"></div>\nFIG. 1. Schematic illustrating the basic framework of a reinforcement learning problem. An agent continually perturbs the environment (the Lorenz equations in our case) by taking an action, and records the resulting states. The agent is rewarded when a desirable state is reached, and punished otherwise.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2993/29937894-e6c0-459b-94c5-8e2e568fb354.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8513/8513849b-6f6d-45f1-8ac0-b1cf52999f3a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b)</div>\nFIG. 2. Solution of the Lorenz system of equations in (a) the chaotic regime with \u03c1 = 28, and (b) the meta-stable chaotic regime with \u03c1 = 20. Note that the solution traverses a chaotic trajectory in the first case, whereas it converges to P\u2212after a few chaotic transients in the second case.\nafter a short transient, as shown in Fig. 2. The two fixpoints in our case are given by P+ = (7.12, 7.12, 19) and P\u2212= (\u22127.12, \u22127.12, 19). We use reinforcement learning to prevent such a transient from chaotic- to fix-point-solutions. This is done by perturbing the parameters in Eqs. 2 (\u03c1 = (\u03c3, \u03c1, \u03b2)) by \u2206\u03c1 = (\u2206\u03c3, \u2206\u03c1, \u2206\u03b2), with \u2206\u03c1 \u2208[\u2212\u03c1/10, \u03c1/10]. The instantaneous value of the solution vector X(t) = (x, y, z) and its time-derivative (velocity) \u02d9X(t) = (Vx(t), Vy(t), Vz(t)) = ( dx dt , dy dt , dz dt ) constitute the state space S for the RL algorithm. For training the RL agent to retain a chaotic trajectory, we utilize the fact that\n|V (t)| will decrease consistently as the solution converges to one of the fix-points, eventually becoming zero. On the other hand, |V (t)| will have a non-zero average value when the solution traces the chaotic attractor. Thus, whenever the agent determines suitable action values \u2206\u03c1 for which |V (t)| is maintained above the predefined threshold value V0 = 40, it is rewarded, otherwise it is punished. In doing so over several iterations, the agent eventually learns to keep the trajectory chaotic. The reward allocated to the agent consists of two parts: a stepwise reward rt provided at each time step, and a one-time terminal reward rterminal given at the end of each episode. The two terms take the following form,\n(3a)\n(3b)\nThe average \u00afrt is defined over the last 2000 time steps of an episode, and facilitates learning to keep the trajectory chaotic over long periods of time. The training of the agent is divided into episodes of 4000 time steps each, with time step size dt = 2e\u22122. The RL agent is expected to learn suitable action values \u2206\u03c1 for any state permissible by the system environment, such that the long-term reward accumulated is maximized. Fig. 3 il-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d967/d967c6cb-4840-4fbf-9b7e-7caaa9527820.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1200 1400 1600 1800 2000 2200 2400 2600 Training time</div>\nFIG. 3. Training of the RL agent with time. Note that the solution is non-chaotic until around t = 2000, beyond which the agent is able to take effective decisions to keep the solutiontrajectory chaotic for further instances. lustrates the training of the RL agent with time. The underlying neural network is trained for 2 \u00d7 105 time steps, which corresponds to 50 independent episodes in total, with each episode beginning with random values of the state variables X between -40 and 40; the corresponding values for \u02d9X are determined using the Lorenz equations (Eqs. 2). Initially, the solution keeps converging to the fix-points, since the network is unable to provide optimal action-decisions. After the network has trained for some time, it successfully learns the optimal actions for keeping the value of |V (t)| above V0. As a consequence, the agent learns that the best way of maximizing reward is\nby maintaining the dynamics over the chaotic-attractor, which, although non-attracting for the given set of parameters, is a natural solution of the system.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d30/1d30b18f-986e-4398-9946-1422a1e5f49b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/85fc/85fcab65-0fcb-4b46-9966-dc32a9420813.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b)</div>\nFIG. 4. (a) Distribution of the perturbation \u2206\u03c1 learned by the RL agent to keep the dynamics on the chaotic-attractor. The red dots indicate locations where the perturbation values are positive, and the blue dots correspond to negative values. (b) Velocity vectors for the corresponding solution in the state space. Note that \u2206\u03c1 is predominantly negative in the region \u211cwhere Vz < 0.\nFigure 4 shows the distribution of the perturbations \u2206\u03c1 employed by the trained agent, which allow it to keep the dynamics on the chaotic-attractor. This distribution was obtained by plotting the controlled-trajectories for 400 random initial values for the variables x, y and z, lying between -40 and 40. Note that a similar distribution was obtained for the other perturbations \u2206\u03b2 and \u2206\u03c3. However, we find that an execution of the converged RL control-policy with \u2206\u03b2 and \u2206\u03c3 explicitly set to zero does not make a difference in the control outcome; the agent is still able to maintain a chaotic trajectory. This may be attributed to the dominating magnitude of the parameter \u03c1 compared to the other two parameters. As depicted in Fig. 4(a,b), the perturbation values are predominantly negative in the region \u211cwhere Vz < 0 and positive elsewhere. The success of this control-policy \u03a0\u2217in keeping the trajectory over the chaotic-attractor can be explained using the sensitivity of the solutions of Eqs.2 to the perturbation \u2206\u03c1. For x < 0 in \u211c, a negative \u2206\u03c1 makes Vy in Eq.(2b) more positive, which in turn makes y and hence Vx in Eq.(2a) more positive, thus drifting the trajectory away from the fix-point P\u2212. Similarly, when x > 0 in \u211c, a negative \u2206\u03c1 makes Vy in Eq.(2b) more negative, which in turn makes y and hence Vx more negative, thereby drifting the trajectory away\nfrom the fix-point P+. The role of positive perturbations outside \u211cin avoiding the escape of the trajectory from the chaotic attractor to the fix-points can be explained likewise. Positive perturbations of \u03c1 lead to an increase in Vy (Eq. 2b). The subsequent increase in y leads to an increase in Vx (Eq. 2a), and the higher values of x and y lead to an increase in Vz (Eq. 2c). The overall effect is to increase the speed, which prevents the trajectory from spiralling in to the fix-points.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ceec/ceec3c1d-a9dd-4e5b-b6cc-a9dfc5aac747.png\" style=\"width: 50%;\"></div>\nFIG. 5. Comparison of the trajectory with and without the application of rule-based control. The blue trajectory corresponds to the controlled solution, starting from the initial condition Q = (10, 15, 35). The yellow uncontrolled solution starts from the same initial condition, and spirals in to the fix-point P\u2212.\nBased on this strategy, we formulate a simple rulebased controller which perturbs the parameter \u03c1 by \u2212\u03c1/10 whenever the trajectory visits the region \u211c, i.e., whenever Vz < 0. All parameters remain unperturbed outside this region. The success of the rule-based binarycontrol is demonstrated in Fig. 5, where the uncontrolled trajectory (yellow) converges to the fix-point, whereas its controlled counterpart (blue) remains chaotic. Note that, unlike other control-techniques, RL-based control requires no a-priori analytical knowledge about the dynamical system regarding its escape-regions, target-points and safe-sets. The RL agent learns the optimal strategy \u03a0\u2217to prevent the transition from chaotic to fix-point solutions completely autonomously, by continually interacting with the environment defined by the Lorenz system of equations exhibiting transient-chaos. To conclude, we have demonstrated the utility of deep reinforcement learning in restoring chaos for a transiently-chaotic system. Since, transient chaos is a consequence of a catastrophic bifurcation (crisis) [24], our results pave the way for RL enabled control of extremeevents and catastrophes in non-linear dynamical systems.\n\u2020 vermas@fau.edu [1] I. T. Georgiou and I. B. Schwartz, Int. J. Bifurcation Chaos 6, 673 (1996). [2] H. Haken, in Chaos and order in nature (Springer, 1981) pp. 2\u201311. [3] J. C. Sackellares, L. D. Iasemidis, D.-S. Shiau, R. L. Gilmore, and S. N. Roper, in Chaos in Brain? (World Scientific, 2000) pp. 112\u2013133. [4] A. L. Goldberger, D. R. Rigney, and B. J. West, Sci. Am. 262, 42 (1990). [5] W. Yang, M. Ding, A. J. Mandell, and E. Ott, Phys. Rev. E 51, 102 (1995). [6] C. Grebogi, E. Ott, and J. A. Yorke, Physica D: Nonlinear Phenomena 7, 181 (1983). [7] I. Dobson and H.-D. Chiang, Systems & Control Lett. 13, 253 (1989). [8] K. McCann and P. Yodzis, The American Naturalist 144, 873 (1994). [9] K. Avila, D. Moxey, A. de Lozar, M. Avila, D. Barkley, and B. Hof, Science 333, 192 (2011). [10] T. Kreilos, B. Eckhardt, and T. M. Schneider, Phys. Rev. Lett. 112, 044503 (2014). [11] I. B. Schwartz and I. Triandaf, Phys. Rev. Lett. 77, 4740 (1996). [12] M. Dhamala and Y.-C. Lai, Phys. Rev. E 59, 1646 (1999). [13] R. Cape\u00b4ans, J. Sabuco, M. A. Sanju\u00b4an, and J. A. Yorke, Philos. Trans. R. Soc. London, Ser. A 375, 20160211 (2017). [14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., Nature 529, 484 (2016). [15] S. Verma, G. Novati, and P. Koumoutsakos, Proc. Natl. Acad. Sci. USA 115, 5849 (2018). [16] E. N. Lorenz, The essence of chaos (University of Washington Press, 1995). [17] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, arXiv preprint arXiv:1707.06347 (2017). [18] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (MIT press, 2018). [19] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Nature 518, 529 (2015). [20] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, in ICML (2015) pp. 1889\u20131897. [21] A. Hill, A. Raffin, M. Ernestus, A. Gleave, R. Traore, P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu, \u201cStable baselines,\u201d https://github.com/ hill-a/stable-baselines (2018). [22] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, arXiv preprint arXiv:1606.01540 (2016). [23] J. L. Kaplan and J. A. Yorke, Comm. Math. Phys. 67, 93 (1979). [24] W.-X. Wang, R. Yang, Y.-C. Lai, V. Kovanis, and C. Grebogi, Phys. Rev. Lett. 106, 154101 (2011).\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of restoring chaos in transiently chaotic regimes of non-linear dynamical systems, particularly the Lorenz system, which often converges to undesirable non-chaotic states after initial chaotic transients. Previous methods for restoring chaos have relied on accurate analytical knowledge of the system and have faced challenges in high-dimensional scenarios, highlighting the need for a new approach.",
        "problem": {
            "definition": "The problem is the convergence of transiently chaotic systems to non-chaotic states, which can lead to catastrophic outcomes in various applications.",
            "key obstacle": "The main difficulty lies in the requirement for prior knowledge of the system's dynamics and the identification of escape regions and target points, which can be computationally intensive and impractical for high-dimensional systems."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of deep reinforcement learning to autonomously discover effective perturbation strategies for maintaining chaos without prior knowledge of the system's dynamics.",
            "opinion": "The proposed idea involves using deep reinforcement learning to control the parameters of the Lorenz system to sustain chaotic behavior despite transient chaos.",
            "innovation": "The key innovation is the application of deep reinforcement learning to autonomously determine control strategies without needing a priori analytical knowledge of the system or its dynamics."
        },
        "method": {
            "method name": "Deep Reinforcement Learning for Chaos Restoration",
            "method abbreviation": "DRL-CR",
            "method definition": "A method that utilizes deep reinforcement learning to discover optimal perturbation strategies for maintaining chaos in the Lorenz system.",
            "method description": "The method involves training a reinforcement learning agent to perturb system parameters in a way that sustains chaotic trajectories.",
            "method steps": [
                "Define the environment and state space based on the Lorenz system.",
                "Initialize the reinforcement learning agent and its parameters.",
                "Allow the agent to interact with the environment through trial and error.",
                "Evaluate the agent's actions based on the rewards received.",
                "Iterate the training process to optimize the agent's policy for maintaining chaos."
            ],
            "principle": "The effectiveness of this method stems from the agent's ability to learn from interactions with the chaotic system, allowing it to maintain a chaotic trajectory by adjusting parameters based on real-time feedback."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in the Lorenz system environment, using the Proximal Policy Optimization (PPO) algorithm for training the reinforcement learning agent, with parameters set to maintain chaotic behavior.",
            "evaluation method": "Performance was assessed by measuring the agent's ability to keep the velocity of the solution above a predefined threshold, indicating sustained chaos, over multiple training episodes."
        },
        "conclusion": "The results demonstrate the successful application of deep reinforcement learning in restoring chaos in the Lorenz system, providing a novel approach to controlling extreme events in non-linear dynamical systems without requiring prior knowledge of their dynamics.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its autonomy; it does not require prior analytical knowledge or complex computations to identify escape regions, making it applicable to a wider range of systems.",
            "limitation": "One limitation is that the method may not be universally applicable to all types of dynamical systems, particularly those with highly complex or unpredictable behavior.",
            "future work": "Future research could explore the application of this method to other non-linear dynamical systems and investigate ways to enhance the robustness and efficiency of the reinforcement learning agent."
        },
        "other info": {
            "info1": "The implementation of the PPO algorithm used in this study is available in the OpenAI stable-baselines library.",
            "info2": {
                "info2.1": "The training of the RL agent involved 50 independent episodes, each with 4000 time steps.",
                "info2.2": "The perturbations learned by the agent were predominantly negative in regions where the velocity was below zero, indicating a strategic approach to maintaining chaos."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of restoring chaos in transiently chaotic regimes of non-linear dynamical systems, highlighting the significance of understanding chaos in systems like the Lorenz system."
        },
        {
            "section number": "1.3",
            "key information": "It is crucial to understand the dynamics of chaotic systems as convergence to non-chaotic states can lead to catastrophic outcomes in various applications."
        },
        {
            "section number": "2.1",
            "key information": "The paper discusses the historical challenge of restoring chaos in chaotic systems, emphasizing the limitations of previous methods that relied on accurate analytical knowledge."
        },
        {
            "section number": "3.1",
            "key information": "The problem defined in the paper is the convergence of transiently chaotic systems to non-chaotic states, which poses significant risks."
        },
        {
            "section number": "4.1",
            "key information": "The proposed method, Deep Reinforcement Learning for Chaos Restoration (DRL-CR), utilizes deep reinforcement learning to autonomously discover effective perturbation strategies."
        },
        {
            "section number": "5.2",
            "key information": "One challenge identified in the paper is the requirement for prior knowledge of the system's dynamics, which can be computationally intensive and impractical for high-dimensional systems."
        },
        {
            "section number": "8.1",
            "key information": "The paper highlights challenges related to data and resource management, particularly in the context of training reinforcement learning agents in chaotic systems."
        },
        {
            "section number": "9",
            "key information": "The conclusion emphasizes the successful application of deep reinforcement learning in restoring chaos, providing a novel approach to controlling extreme events in non-linear dynamical systems."
        }
    ],
    "similarity_score": 0.5548566602661366,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0141_,arti/papers/Restoring Chaos Using Deep Reinforcement Learning.json"
}