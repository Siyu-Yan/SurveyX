{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2308.00307",
    "title": "Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities",
    "abstract": "How can we apply human feedback into generative model? As answer of this question, in this paper, we show the method applied on denoising problem and domain adaptation using human feedback. Deep generative models have demonstrated impressive results in image denoising. However, current image denoising models often produce inappropriate results when applied to domains different from the ones they were trained on. If there are `Good' and `Bad' result for unseen data, how to raise up quality of `Bad' result. Most methods use an approach based on generalization of model. However, these methods require target image for training or adapting unseen domain. In this paper, to adapting domain, we deal with non-target image for unseen domain, and improve specific failed image. To address this, we propose a method for fine-tuning inappropriate results generated in a different domain by utilizing human feedback. First, we train a generator to denoise images using only the noisy MNIST digit '0' images. The denoising generator trained on the source domain leads to unintended results when applied to target domain images. To achieve domain adaptation, we construct a noise-image denoising generated image data set and train a reward model predict human feedback. Finally, we fine-tune the generator on the different domain using the reward model with auxiliary loss function, aiming to transfer denoising capabilities to target domain. Our approach demonstrates the potential to efficiently fine-tune a generator trained on one domain using human feedback from another domain, thereby enhancing denoising abilities in different domains.",
    "bib_name": "park2023domainadaptationbasedhuman",
    "md_text": "# Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities\nHyun-Cheol Park, Sung Ho Kang\nAbstract\u2014How can we apply human feedback into generative model? As answer of this question, in this paper, we show the method applied on denoising problem and domain adaptation using human feedback. Deep generative models have demonstrated impressive results in image denoising. However, current image denoising models often produce inappropriate results when applied to domains different from the ones they were trained on. If there are \u2018Good\u2019 and \u2018Bad\u2019 result for unseen data, how to raise up quality of \u2018Bad\u2019 result. Most methods use an approach based on generalization of model. However, these methods require target image for training or adapting unseen domain. In this paper, to adapting domain, we deal with non-target image for unseen domain, and improve specific failed image. To address this, we propose a method for fine-tuning inappropriate results generated in a different domain by utilizing human feedback. First, we train a generator to denoise images using only the noisy MNIST digit \u20190\u2019 images. The denoising generator trained on the source domain leads to unintended results when applied to target domain images. To achieve domain adaptation, we construct a noise-image denoising generated image data set and train a reward model predict human feedback. Finally, we finetune the generator on the different domain using the reward model with auxiliary loss function, aiming to transfer denoising capabilities to target domain. Our approach demonstrates the potential to efficiently fine-tune a generator trained on one domain using human feedback from another domain, thereby enhancing denoising abilities in different domains. Index Terms\u2014Generative Adversarial Network, Human Feed-\n# Index Terms\u2014Generative Adversarial Network, Human Feedback, Domain Adaptation, Unseen Domain, Denoising.\nI. INTRODUCTION\nD EEP generative models have achieved remarkable success in image generation tasks [1]\u2013[3]. In particular, generative adversarial networks (GANs) are widely known as a fundamental theory that demonstrates how to generate realistic images. Recently, GANs are also utilized for specific purposes such as image denoising [4]\u2013[6], super-resolution [7], [8], and style transfer [9]\u2013[12]. These objectives involve training GANs using supervised learning with paired data sets aiming to learn the target distribution, which has been shown to yield successful results. However, despite the impressive performance of these models within their training domain, they often encounter challenges when applied to unseen domains, resulting in subpar\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f7db/f7dbf6c8-c3cc-4809-a42b-4ffaeae3dda7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Overview for adaptation training based on human feedback.</div>\noutputs. In the context of GANs based on image-to-image generation [13], which aim to preserve the original intrinsic characteristics while learning the target distribution, both successful and unsuccessful cases can emerge during testing on unseen domains. For example, when presented with ten samples from an unseen domain, seven of them may yield successful translations, while the remaining three produce unsatisfactory results. This raises the question: should we simply discard these three failed samples, or is there a way to enhance and improve them to achieve better outcomes? Obtaining ground-truth data for the unseen domain could facilitate domain-specific training; however, in many practical scenarios, acquiring target data for unseen domains poses significant challenges. As an alternative, applying domain adaptation methods [14]\u2013[21] can mitigate this issue, but even domain-adapted models may still yield failed results based on human preferences. Our objective diverges from conventional domain adaptation approaches. As demonstrated in [14], [19], domain adaptation focuses on training methods that aim to minimize the distinguishability between the source and target domain\ndistributions in the latent space. This macro-level approach seeks to minimize the overall gap between source and target domains. However, at a micro-level, there remain opportunities for improvement in the generated results. Therefore, our goal is to address and rectify instances of failure within the output produced by the trained model. Similarly, during the training of ChatGPT [22], it excels at generating high-quality language responses through extensive pre-training on vast data sets. Nevertheless, upon human evaluation, the generated sentences may exhibit a dichotomy: some appear naturally flowing, while others seem less fluent. To bridge this gap, ChatGPT [22] leverages Human-Feedback [23] to enhance its ability to produce more seamlessly natural sentences. Furthermore, in the domain of aligning text-toimage models [24], the introduction of human feedback has demonstrated significant improvements in model performance. However, research on model refinement through human feedback in GANs is still scarce, and through our paper, we aim to showcase the potential of model refinement through the profound influence of human feedback. Recently, drawing inspiration from the success of reinforcement learning human feedback (RLHF) [25] in language domains, we present an innovative approach for unseen domain adaptation based on human feedback. Analogous to how children learn from the feedback provided by their parents, we adopt a similar strategy. For instance, if a child learns how to remove noise from the background of a single image, they can subsequently apply denoising techniques to new images. While the quality of the denoised image may vary, receiving feedback from a parent can lead to improvement. Even if we cannot surpass our previous achievements, we can still imitate and learn from them. This approach shows promise in addressing the challenges of unsupervised unseen domain adaptation and opens new possibilities for model enhancement through the profound influence of human feedback. As we explore this innovative avenue, our aim is to make significant contributions to the field of AI and foster advancements in unsupervised domain adaptation research. The guiding philosophy behind our work can be summarized as: \u201cOur goal is to learn what I am not good at, just like what I am good at.\u201d To achieve this, we introduce a deep feedback network that utilizes human feedback to adaptation unlabeled target domain. To replicate restricted learning circumstances, we conduct experiments in the denoising problem. Initially, we train the model using a restrictive training approach, focusing solely on denoising the digit \u20180\u2019 within the MNIST data set. Subsequently, we evaluate the model\u2019s performance on the Fashion-MNIST data set, which represents an unseen domain. It becomes evident that the pre-trained model, trained on MNIST, produces unintended results when applied to the unseen domain. To adapt to the unseen domain, we introduce a training method based on human feedback. Human feedback assesses the model\u2019s results in the unseen domain as either \u2018Good\u2019 or \u2018Bad\u2019. The model is then fine-tuned using the gradient of these assessments. This approach shows promising potential for efficiently fine-tuning the model using feedback from generators trained on other domains. We can summarize our main contributions as follows:\n\u2022 We propose adaptation method for the domain of imagebased generative models through human feedback. \u2022 We perform domain adaptation while maintaining the quality of the generated image using an optional loss function with a reward model using the human feedback. \u2022 We show that the model can be adapted by human feedback, even in the absence of labeled target data\n# II. METHODS\nOur overall process consists of three steps. First, the denoising model is pre-trained in the basis domain, serving as the fundamental ability for denoising. Next, the reward model is trained using human feedback. To train the reward model, humans manually annotate denoised images as either Good or Bad. Finally, the basis generator is re-trained using the reward model. Even if the generator produces denoised images of low quality, it will be trained to prioritize good results based on the provided human feedback.\n# A. Pre-training basis domain for denoising\nIn this step, we focused on creating an intentional classbiased generator. The model is trained to acquire the fundamental ability of denoising using simple images as shown in Step 1 of Fig. 2. The architecture of the model consists of generative adversarial networks (GANs). We employed the pix2pix [26] model as our baseline, which relies on paired training. To train the model, a paired data set is required, consisting of both clean and noise images. For our paired training data set, we used the only 0-digit in MNIST data set. To create a pair, selected 0-digit images and combined with synthesized noise. Consider the synthesized noise image z, which is a 2D image represented as z \u2208Rm\u00d7n. It is composed of both the original image and noise, denoted as x and n, respectively:\n(1)\nWe assume that the clean image is selected from the source domain. Therefore, the synthesized noise image z and the original image x are treated as paired data. For convenience notation, source and unseen domain data denote as zs and zu, respectively. The generator is trained to produce samples of good quality from input noise variables pn. To train the model on the source domain, the final loss is defined as follows:\n(2)\nwhere the samples Gs(zs) obtained when zs \u223cpn follow a distribution that represents good quality in source domain. In other words, The generator Gs is trained to learn the mapping from the noise image z to the clean image x, denoted as Gs : zs \u2192x. The objective of the generator is to estimate the distribution of x, denoted as Gs(zs) \u2248x. To achieve this, the GAN consists of an adversarial discriminator D, which distinguishes between \u2018Real\u2019 and \u2018Fake\u2019 images. \u2018Real\u2019 refers to the original image x, while \u2019Fake\u2019 corresponds to the generated image Gs(zs) produced by the generator. Both the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/326f/326f0f95-5cb2-4901-aa17-9958e227f62d.png\" style=\"width: 50%;\"></div>\ngenerator Gs and the discriminator D are trained adversarially. The objective function can be expressed as follows:\n(3)\nwhere Gs(zs) represents the generation of a clean image from a noise image zs. The discriminator D is responsible for classifying between the real and fake distributions. In order to induce a mistake in D, Gs aims to minimize Equation (3). On the other hand, D maximizes the objective function to distinguish between real and generated images. In our study, we tackle the problem of denoising while preserving the underlying morphological structures. Traditional GAN [1] frameworks approximate the target distribution during training. However, in the context of image processing, the generated images may inadvertently alter the essential morphological characteristics of the originals [26]\u2013[28]. To mitigate this issue and ensure the preservation of morphological structures, an auxiliary loss term is incorporated into the objective function:\n(4)\nSimilar to the [26] approach, the auxiliary loss employs the L1 distance between the target image x and the generated image Gs(zs). To train model on source domain, final loss is as follows:\nThe integration of human feedback has demonstrated high adaptability across various domains [24], [29], [30]. This valuable information is used to train a reward model, which acts as a substitute for human assessment and enhances the model\u2019s performance. In this section, we provide a detailed description of how human feedback is gathered. In the previous section, we presented the basic denoising GAN model using pix2pix [26], which we referred to as the Supervised Denoising Model (SDM). Human feedback is obtained through manual assessments of the SDM results from unseen domain samples zu. The assessments are categorized as \u2018Good\u2019 if the image was clean and \u2018Bad\u2019 if the image contained noise or collapse. (see Step 2 in Fig. 2.) The assessments \u2018Good\u2019 and \u2018Bad\u2019 are utilized as ground truth labels (yr = 0, 1) to train the reward model. The reward model, denoted as r\u03b8, follows the same architecture as the discriminator in the SDM. The loss function for r\u03b8 is as follows:\nwhere \u02c6 Gs is a frozen denoising generator model using source domain. During the training of the reward model, \u02c6 Gs remains untrainable and is solely used to generated denoised images. r\u03b8\nassesses these denoised images and is trained using yr labels. Notably, the reward model can be trained to capture human preferences, as the yr labels are collected through human feedback.\n# C. Objective\nIn this section, we present the final formulation of the loss function, which consists of auxiliary terms. Each auxiliary term includes reward loss, consistency loss, and regularization loss, used to train Gt. Here, Gt represents the adapted model which is fine-tuned from Gs in the unseen domain. Thus, the architecture and initial parameters of Gt are the same as those of Gs.\nReward Loss Lr: The primary objective of Generator Gt is to generate denoised images that are assessed by the reward model as \u2018Good\u2019(0, indicating clean images). The minimization of Lr aims to train the generator Gt to generate clean images. In other words, the reward loss Lr trains Gt to map from the distribution of \u2018Bad\u2019 quality images (distribution j) to the distribution of \u2018Good\u2019 quality images (distribution k), Gt : j \u2192k.\n(6)\nwhere \u02c6r\u03b8 is a reward model trained on human feedback and has fixed parameters. Thus, \u02c6r\u03b8 only assesses the quality of the generated image from Gt and the input image zu. In this context, by fine-tuning Gt from Gs using Lr loss, Gt is able to closely approximate the x \u223cpdata distribution represented by r\u03b8 in an unseen domain. However, relying solely on Lr loss for training Gt may lead to over-fitting and the risk of distorting the morphological information of the original images. To alleviate this problem, we describe \u2018Regularization Loss\u2019 and \u2018Consistency Loss\u2019 as follows.\nConsistency Loss Lp: As the model learns from new data, there is a potential issue of the performance of past good results deteriorating due to parameter updates. This is commonly referred to as the problem of catastrophic forgetting. To control this issue, it is necessary to compare the outcomes of the initial parameters with the current results. We present a novel compensatory term, denoted as Lp, which facilitates a comparison between the outputs of the initial frozen generator, \u02c6 Gs, and the target generator Gt. The primary objective of Lp is to minimize the pixel-wise L1 loss between the outcomes generated by \u02c6 Gs and Gt, thereby ensuring that the current model preserves crucial insights acquired from the initial generator throughout the training procedure. By incorporating this approach, we effectively address the issue of neglecting important details and consequently witness a notable enhancement in the overall performance of the current model.\n(8)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d0af/d0af099d-5cc1-49fc-8c7e-c513911441e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3. Flow diagram of the fine tuning final objective loss functions with each objective loss</div>\nwhere \u03c3 is the step function, and r denotes the result of the reward. \u03f5 is a threshold value ranging from 0 to 1.\nRegularization Loss Ln: We employ a regularization loss term to address the issues of over-fitting and mode collapse. In existing methods, the difference in cosine similarity of feature vectors in the latent space has been compared [21], [31]. However, in our approach, we intuitively compare the outputs of the model from the past and the current training stages to suppress excessive variations caused by the model\u2019s learning. Ln calculates the pixel-wise L1 loss between the results of the current generator and the (n \u2212i)th generator \u02c6 Gt. The generator \u02c6 Gt copies weights from Gt every N steps and then freezes them.\n(9)\n(10)\nwhere, \u03b1 is control the relative balance of the two Lp and Ln losses. In the ablation study, we analyze the impact of each auxiliary loss on the final loss. Fig. 3 represents the flow diagram of the more specific final objective loss functions we designed.\n# A. Data sets\nWe utilized two data sets in our experiments: MNIST [32] and Fashion-MNIST [33]. MNIST is a grayscale image data set consisting of 10 classes representing digits from 0 to 9. Each MNIST image has dimensions of 28\u00d728 pixels. The data set comprises a training set of 60,000 images and a test set of 10,000 images. In the experiments, the MNIST data set serves as the source domain for training the initial denoising generator. Specifically, only the \u20180\u2019 digit is used for restrictive training on the source domain. The training set consists of 6,000 samples, and the validation set contains 1,000 samples. The MNIST images are resized to a size of 256\u00d7256 pixels using bicubic interpolation. To train the initial denoising generator, a pair of data is required, consisting of clean and noisy\nTABLE I RESULT OF FINE TUNING USING HUMAN FEEDBACK. EACH ROW CORRESPONDS TO THE OUTCOMES UNDER DIFFERENT CONDITIONS OF THE LOSS FUNCTION. THE FIRST ROW REPRESENTS OUR PROPOSED RESULTS. THE SECOND ROW SHOWS RESULTS WITHOUT Lp LOSS, THE THIRD ROW SHOWS RESULTS WITHOUT Ln LOSS, AND THE FOURTH ROW SHOWS RESULTS USING ONLY Lr LOSS. THE FIFTH ROW PRESENTS RESULTS FROM THE MODEL TRAINED ON THE SOURCE DOMAIN, AND THE LAST ROW DISPLAYS THE BASELINE RESULTS BETWEEN NOISY AND CLEAN IMAGES.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8cb7/8cb7428f-e5b3-4d5a-aeaf-899e89f1057b.png\" style=\"width: 50%;\"></div>\nimages. The original MNIST data set is used as the clean image counterpart, while the noisy images are created by introducing artifact noise in the form of salt and pepper noise with Gaussian noise. Proportion of salt and pepper noise is equal amounts 0.5 and Gasussian noise is mean of zero and a standard deviation is 0.05. Fashion-MNIST is a data set consisting of images representing 10 types of fashion items. It also includes a training set of 60,000 images and a test set of 10,000 images. FashionMNIST is employed to evaluate the model\u2019s performance and train adaptive learning. The images in Fashion-MNIST are resized to 256\u00d7256 pixels and similarly augmented with noise, as done with the MNIST data set.\n# B. Training setting\nPre-training for denoising: \u201cpix2pix\u201d [26] is employed as the baseline model in this experiment. The main objective of most GANs is to establish a mapping G : Z \u2192X. \u201cpix2pix\u201d demonstrated the training approach for pixel-wise mapping between input and output images. Consequently, the generator of \u201cpix2pix\u201d can effectively learn the transformation from the noise space Z to the clean space X. In this experiment, we trained a denoising model, denoted as Gs, using the MNIST data set. Gs was specifically trained using a set of 1,000 image pairs consisting of clean digits and their corresponding noisy versions. The clean images used in the training process were specifically selected to represent the digit \u20180\u2019. For optimization, we employed the Adam solver [34] with a batch size of 10, a learning rate of 0.0002, and momentum parameters \u03b21 = 0.5 and \u03b22 = 0.999. The denoising model was trained for 200 epochs.\nInference and human feedback: In this paper, our proposed method demonstrates the adaptability of a pre-trained model to a target domain through human feedback. To gather human feedback, the pre-trained generate model Gs is used to infer results in the target domain, which are then manually assessed by human evaluators. In our experiments, we employ Fashion-MNIST as the target domain data set, and we collect human feedback for the 10,000 test images in this data set. Training for reward model by human feedback: The reward model, denoted as r\u03b8, is utilized in the auxiliary loss\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9025/9025fdf3-ba89-48be-b9d9-e32fc9f0b363.png\" style=\"width: 50%;\"></div>\nFig. 4. Visual results for adaptation. The PSNR and SSIM values for each image are calculated with respect to the ground truth. Gs represents the model pre-trained on MNIST, while Gt represents the model fin-tuned from Gs using human feedback. (a) Sample images with the most significant increase in PSNR from Gs and Gt output. (b) most decreasd PSNR images\n<div style=\"text-align: center;\">Fig. 4. Visual results for adaptation. The PSNR and SSIM values for each image are calculated with respect to the ground truth. Gs represents the model pre-trained on MNIST, while Gt represents the model fin-tuned from Gs using human feedback. (a) Sample images with the most significant increase in PSNR from Gs and Gt output. (b) most decreasd PSNR images</div>\nterm. The architecture of the reward model is designed to be the same as the discriminator of the \u201cpix2pix\u201d model. The hyperparameters used for training r\u03b8 remain consistent with the pre-training setting.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f04/9f04ff9e-41b1-4a10-92d8-33c2a27d0b66.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. Boxplot of PSNR for each generator Gs and Gt on the experimental data set. Even after fine-tuning Gt on unseen data, we observe that Gt produces results without PSNR degradation in the pre-training domain. This finding demonstrates the effectiveness of our proposed method, which utilizes human feedback to mitigate catastrophic forgetting. (a) MNIST test set(10k). (b) Fashion-MNIST test set(10k). (c) Fashion-MNIST train set(60k)</div>\nFig. 5. Boxplot of PSNR for each generator Gs and Gt on the experimental data set. Even after fine-tuning Gt on unseen data, we observe that Gt produces results without PSNR degradation in the pre-training domain. This finding demonstrates the effectiveness of our proposed method, which utilizes human feedback to mitigate catastrophic forgetting. (a) MNIST test set(10k). (b) Fashion-MNIST test set(10k). (c) Fashion-MNIST train set(60k)\nAdaptive training by human feedback: Note that the adaptive training process implements Equation (10), utilizing the same set of hyperparameters as mentioned above. It is important to note that Gt has trainable parameters, whereas \u02c6 Gs, \u02c6 Gt, and \u02c6r\u03b8 are untrainable parameters. The constant \u03f5 in Equation (8) is set to 0.2, and the constant \u03b1 in Equation (10) is set to 0.9. In the ablation study, we examine the influence of Lp and Ln as \u03b1 is varied.\n# C. Evaluation\nWe evaluate the quality of the denoised images using the metrics of PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index Measure). PSNR is a widely used metric for evaluating denoising models. It measures the quality of the denoised image by comparing it to the original (clean) image. Higher PSNR values indicate better denoising performance. PSNR can be calculated using the mean squared error (MSE) between the denoised image and the original image. SSIM is another popular metric that quantifies the similarity between the denoised image and the original image. It takes into account not only pixel-level differences but also structural information, such as luminance, contrast, and structure. Higher SSIM values indicate better preservation of structural details. D. Results of domain adaptive denoising by human feedback Comparison of evaluation metrics: In this section, we examine the results of domain adaptive denoising. Our intuition is that, even when presented with unseen data from a target domain, if we provide human feedback to a supervised learning model, the model can adapt to the data effectively. Note that our human feedback is not ground-truth for denoised image, it is human\u2019s preference consisting of \u2018Good\u2019 and \u2018Bad\u2019. In Table I, \u2018Gs(z) vs x\u2019 represents the denoising results of the model before adaptive learning using human feedback. \u2018Gt(z) vs x\u2019 shows the denoising outcomes of the adapted model based on human feedback. The results obtained from the MNIST data set indicate the performance in the pre-trained domain, while the results from the FashionMNIST data set reflect the performance on unseen data.\nTherefore, we can observe the adaptation progress between the initial model Gs and the updated model Gt. In our experiments, both Gs and Gt demonstrated a significant improvement in PSNR measured on the Fashion-MNIST test set, with an overall increase of 94% over the entire 10K data set. The statistical analysis of the PSNR improvement revealed a mean increase of 1.61\u00b12.78dB (MAX: 18.21, MIN: 0.0001). Fig. 4-(a) shows the images with the most significant increase in PSNR, along with the corresponding metrics between the each generator output images and the ground truth images. In addition, for the remaining 6% of the cases, there was a decrease in PSNR values, with the statistical analysis showing a mean decrease of 0.12\u00b10.18dB (MAX: 2.75, MIN: 0.0001)(See. Fig. 4-(b)). Fig. 5 shows the boxplot of PSNR for each generator Gs and Gt on the experimental data set. The Gt images from the same data set exhibit higher PSNR values, indicating improved image quality after adaptation. Particularly noteworthy is that after adaptation, the PSNR and SSIM values of the MNIST test set(10K) from the Gt generator, corresponding to the source domain, show little to no variation or even slight improvement(See. Fig 5-(a)). This demonstrates the prevention of catastrophic forgetting issue for the source domain even after adaptation to the target domain. Furthermore, we apply the Gt model tuned on the Fashion-MNIST test set with reward model to the Fashion-MNIST training set (60k). This demonstrates that when the reward model is trained in a new domain, it can effectively work without requiring additional training.\nVisual evaluation: Fig. 4 illustrates the improvement in denoising and restoration, particularly in addressing image collapse. Notably, Gs trained on \u20180\u2019 digit of MNIST, exhibits instances where the results suffer from image collapse in several images, indicating a lack of adaptation. However, our approach effectively enhances the image quality by leveraging human feedback, as demonstrated by the results obtained with Gt.\n# E. Ablation study\nTo validate the effectiveness of each loss term of our method. we conduct comprehensive ablation studies for loss term Effect of Lp term: The Lp term compares the image quality between Gs and Gt, and is the loss function between images that are well evaluated by human feedback based on the reward function. We examine the effect of the Lp loss on the quality of the output. Typically, the constant alpha of Lp is fixed at 0.9. To evaluate the effect of excluding the Lp term, we vary the alpha value to 0, resulting in the loss equation becoming L(Gt) = Lr(Gt) + Ln(Gt). Performing the adaptation without an Lp term exhibits low quantitative performance, as demonstrated in the second row of Table I. Additionally, Fig. 6(d)-(e) depict the anomaly texture created image for reference. Effect of Ln term: Ln represents the L1 loss between the (n \u22122)th and (n)th iterations of G( \u02c6 Gt and Gt). In terms of quantitative evaluation, it demonstrates comparable\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1619/1619523a-62cf-4802-964c-07d4348bcf03.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. Comparison of image quality with and without the auxiliary loss. (d), (e), (f), and (g) show results with different auxiliary loss conditions. Each condition improves the image quality compared to (c), but there are noticeable differences in details such as texture and artifacts. (a) Input image with noise. (b) Ground truth. (c) Denoised images by Gs. (d) Denoised images by Gt. (e) Denoised images by Gt without the Lp term. (f) Denoised images by Gt without the Ln term. (g) Denoised images by Gt using only the Lr term.</div>\nperformance (Table 1, first and third row). However, in qualitative assessment, it becomes evident that there are limitations in generating the desired image to a satisfactory degree.(See Fig. 6(d)-(f)). We also examine the effect of Ln loss on the output quality. The role of Ln is to restrict significant parameter changes from the previous model. Given that the function of Ln is to restrict parameter updates between the previous and current models, it becomes apparent that there are limitations in generating the desired image to a satisfactory extent in qualitative evaluation, leading to potential issues such as collapse.\n# IV. DISCUSSION AND CONCLUSIONS\nIn this paper, we propose a novel method based on human feedback to address the domain adaptation problem of denoising generative models, particularly focusing on the condition of an unlabeled target domain. Unlike conventional\napproaches that aim to enhance a generative model\u2019s overall performance on the entire test data set, our method leverages human feedback to directly improve the quality of failed images in denoising tasks. While many existing approaches require a large amount of labeled data and may discard failed images, our approach fine-tunes the model based on human feedback, similar to the process used in ChatGPT [22] to select generated sentences of \u2018Good\u2019 quality. This novel utilization of human feedback represents a promising avenue for enhancing generative models. Domain adaptation poses challenges, particularly regarding the issue of catastrophic forgetting. However, through our proposed adaptation approach, which incorporates selective loss functions and an ablation study based on decisions from a reward model trained with human feedback, we successfully mitigated the challenging issue of catastrophic forgetting. Our results align with related studies, demonstrating the effectiveness of our approach. In the context of real-world applications, the unseen data domain adaptation of deep generative models has always been a crucial research topic. In this paper, we demonstrate the adaptation of a model trained on the source domain to the label-less target domain, guided by human feedback. Through ablation study, we analyzed the loss functions and provided\ncompelling evidence for the direction of domain adaptation research, particularly in the realm of image generation. Furthermore, we will grapple for two things as follows: 1. Human preference: Our work also collect human feedback data by personal preference same with ChatGPT. Thus, distribution for \u2018Good\u2019 quality can be different. This will be connected directly with model\u2019s performance. 2. Model performance dependent on pre-training: We assume that SDM is over a certain level. However, if SDM does not work in unseen domain not at all, we can not collect human feedback. Human feedback has to be collected \u2018Good\u2019 and \u2018Bad\u2019 categroy.\n# ACKNOWLEDGMENTS\nThis work was supported by the National Institute for Mathematical Sciences (NIMS) funded by the Korean Government under Grant NIMS-B23910000.\n# REFERENCES\n",
    "paper_type": "method",
    "attri": {
        "background": "Deep generative models have achieved remarkable success in image generation tasks, particularly with generative adversarial networks (GANs). However, these models often produce subpar results when applied to unseen domains. Current methods typically require target images for training or adapting to these domains, leading to a need for a new approach that can enhance the quality of 'Bad' results without such data.",
        "problem": {
            "definition": "The problem addressed in this paper is the degradation of image denoising quality when generative models are applied to unseen domains, specifically focusing on the challenge of improving 'Bad' results without access to target images.",
            "key obstacle": "The main challenge is the reliance on labeled target data for effective domain adaptation, which is often unavailable in practical scenarios."
        },
        "idea": {
            "intuition": "Inspired by reinforcement learning from human feedback, the idea is to utilize human evaluations to refine the performance of generative models in denoising tasks, akin to how children learn from parental feedback.",
            "opinion": "The proposed idea involves fine-tuning a generator trained on one domain (MNIST) using human feedback to enhance its performance on a different domain (Fashion-MNIST).",
            "innovation": "This method differs from existing approaches by focusing on the direct improvement of failed outputs through human feedback, rather than generalizing the model across the entire dataset."
        },
        "method": {
            "method name": "Human Feedback-based Domain Adaptation",
            "method abbreviation": "HFDA",
            "method definition": "HFDA is a method that fine-tunes a generative model using human feedback to improve denoising performance on unseen domains without labeled target data.",
            "method description": "The core of the method involves training a generator on a source domain and then adapting it to a target domain using evaluations of generated outputs as feedback.",
            "method steps": [
                "Pre-train a denoising model on a source domain using paired data.",
                "Collect human feedback on the quality of generated images in the target domain.",
                "Fine-tune the generator based on the feedback to enhance output quality."
            ],
            "principle": "The effectiveness of this method lies in its ability to leverage human feedback to directly address and rectify specific failures in the generated images, thus improving overall performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the MNIST dataset for training and the Fashion-MNIST dataset for evaluation, with human feedback collected on 10,000 test images.",
            "evaluation method": "The performance of the method was assessed using PSNR and SSIM metrics to quantify the quality of denoised images before and after adaptation."
        },
        "conclusion": "The experiments demonstrated a significant improvement in PSNR values for the adapted model (Gt) compared to the pre-trained model (Gs), indicating the effectiveness of the proposed method in enhancing generative model performance through human feedback.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to improve the quality of generated images in unseen domains without requiring labeled data, thus addressing a significant limitation of traditional methods.",
            "limitation": "A potential limitation is the dependency on the quality of human feedback, which may vary and affect the model's performance if not representative.",
            "future work": "Future research could explore the integration of more diverse feedback mechanisms and the application of the method to other generative tasks beyond image denoising."
        },
        "other info": {
            "acknowledgments": "This work was supported by the National Institute for Mathematical Sciences (NIMS) funded by the Korean Government under Grant NIMS-B23910000."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper discusses the integration of human feedback in generative models, emphasizing the interdisciplinary nature of machine learning and human-computer interaction."
        },
        {
            "section number": "1.2",
            "key information": "The proposed method addresses the degradation of image denoising quality, highlighting the significance of improving generative model outputs in unseen domains."
        },
        {
            "section number": "1.3",
            "key information": "The main objective of the paper is to refine the performance of generative models in denoising tasks using human evaluations without access to labeled target data."
        },
        {
            "section number": "2.1",
            "key information": "The paper introduces the concept of Human Feedback-based Domain Adaptation (HFDA) as a novel approach to enhance denoising performance in generative models."
        },
        {
            "section number": "3.2",
            "key information": "The method leverages human feedback to improve generative model performance, showcasing recent advancements in applying human evaluations to machine learning tasks."
        },
        {
            "section number": "6.1",
            "key information": "The paper identifies the challenge of reliance on labeled target data for effective domain adaptation as a significant technical obstacle in generative modeling."
        },
        {
            "section number": "6.3",
            "key information": "Future research directions proposed include exploring diverse feedback mechanisms and applying the method to other generative tasks beyond image denoising."
        }
    ],
    "similarity_score": 0.5564840998183893,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1832_natur/papers/Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities.json"
}