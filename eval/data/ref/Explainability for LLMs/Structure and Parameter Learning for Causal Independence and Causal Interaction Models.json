{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1302.1561",
    "title": "Structure and Parameter Learning for Causal Independence and Causal Interaction Models",
    "abstract": "This paper discusses causal independence models and a generalization of these models called causal interaction models. Causal interaction models are models that have independent mechanisms where a mechanism can have several causes. In addition to introducing several particular types of causal interaction models, we show how we can apply the Bayesian approach to learning causal interaction models obtaining approximate posterior distributions for the models and obtain MAP and ML estimates for the parameters. We illustrate the approach with a simulation study of learning model posteriors.",
    "bib_name": "meek2015structureparameterlearningcausal",
    "md_text": "# Structure and P arameter Learning for Causal Indep endence and Causal In teraction Mo dels\n# Christopher Meek and Da vid Hec k erman Microsoft Researc h\nRedmond W A\ufffd  \b\ufffd\u0005\u0002\ufffd\u0006\u0003   fmeek\ufffdhec k ermag\ufffdmi crosoft\ufffdcom\nAbstract\nW e b egin b y discussing causal indep endence mo dels and generalize these mo dels to causal in teraction mo dels\ufffd Causal in teraction mo d\ufffd els are mo dels that ha v e indep enden t mec h\ufffd anisms where mec hanisms can ha v e sev eral causes\ufffd In addition to in tro ducing sev eral particular t yp es of causal in teraction mo d\ufffd els\ufffd w e sho w ho w w e can apply the Ba y esian approac h to learning causal in teraction mo d\ufffd els obtaining appro ximate p osterior distribu\ufffd tions for the mo dels and obtain MAP and ML estimates for the parameters\ufffd W e illus\ufffd trate the approac h with a sim ulation study of learning mo del p osteriors\ufffd\n# \u0001 In tro duction\nMo dels of causal indep endence \u0001 suc h as the Noisy\ufffdor \ufffdGo o d\ufffd \u0001 \u0006\u0001\ufffd Kim and P earl\ufffd \u0001 \b\u0003\ufffd and Noisy\ufffdMax \ufffdHenrion\ufffd \u0001 \b\u0007\ufffd ha v e pro v ed to b e useful for proba\ufffd bilistic assessmen t \ufffdP earl\ufffd \u0001 \b\b\ufffd Henrion\ufffd \u0001 \b\u0007\ufffd Hec k\ufffd erman and Breese\ufffd \u0001  \u0006\ufffd\ufffd In addition to easier as\ufffd sessmen t\ufffd there are tec hniques for p erforming infer\ufffd ence e\ufffdcien tly in mo dels with causal indep endence \ufffde\ufffdg\ufffd\ufffd Hec k erman and Breese\ufffd \u0001  \u0006\ufffd Zhang and P o ole\ufffd \u0001  \u0006\ufffd and tec hniques to e\ufffdcien tly calculate upp er and lo w er b ounds for lik eliho o ds where exact inference is in tractable \ufffdJaakk ola and Jordan\ufffd \u0001  \u0006\ufffd\ufffd The essen\ufffd tial idea of causal indep endence mo dels is that the causes lead to the e\ufffdect through indep enden t mec h\ufffd anisms\ufffd If this t yp e of mo del is assumed then one only needs to separately assess the probabilit y distri\ufffd butions that describ es a mec hanism and giv e a rule for com bining the results of the mec hanisms\ufffd On the other hand\ufffd when using full probabilit y tables to repre\ufffd sen t the conditional distribution of the e\ufffdect giv en its\n\u0001 Causal indep endence is sometimes referred to as in ter\ufffd causal indep endence \ufffd\ncauses\ufffd w e are essen tially allo wing for c omplete c ausal inter actions b et w een the causes\ufffd\nThe \ufffdrst part of this pap er in tro duces c ausal inter ac\ufffd tion mo dels\ufffd Lik e the causal indep endence mo del\ufffd a causal in teraction mo del is a set of mec hanisms\ufffd a set of causes\ufffd and an e\ufffdect\ufffd Unlik e the causal indep en\ufffd dence mo del\ufffd a cause need not b e asso ciated with a single mec hanism and m ultiple causes can b e asso ci\ufffd ated with a single mec hanism\ufffd Allo wing sev eral causes to b e asso ciated with a single mec hanism allo ws for p artial c ausal inter action b et w een a set of causes\ufffd th us\ufffd causal in teraction mo dels generalize b oth the causal indep endence mo del and the complete causal in terac\ufffd tion mo del\ufffd In Section \u0002\ufffd w e sho w ho w to represen t causal in teraction mo dels as directed acyclic graphi\ufffd cal \ufffdD A G\ufffd mo dels \ufffda\ufffdk\ufffda\ufffd\ufffd Ba y esian net w orks\ufffd b elief net w orks\ufffd causal net w orks\ufffd with hidden v ariables\ufffd In addition w e in tro duce a sp ecial t yp e of causal in terac\ufffd tion mo del\ufffd the exp onential c ausal inter action mo del\ufffd Examples of exp onen tial causal in teraction mo dels are giv en in Section \u0003\ufffd In the second part of the pap er w e turn our atten tion from represen tation to learning the structure and pa\ufffd rameters of exp onen tial causal in teraction mo dels\ufffd In m uc h of the initial w ork on learning \ufffddiscrete\ufffd D A G mo dels\ufffd the fo cus w as on learning the structure of the net w ork assuming there w ere ful l c onditional pr ob abil\ufffd ity tables for eac h v ariable in the net w ork\ufffd The con\ufffd ditional probabilit y table for a v ariable represen ted the conditional probabilit y of the v ariable giv en ev\ufffd ery p ossible com bination of the v alues of its paren ts in the D A G mo del structure\ufffd In this represen tation\ufffd the n um b er of parameters asso ciated with a v ariable is exp onen tial in the n um b er of paren ts of the v ariable\ufffd This exp onen tial explosion can restrict the set of net\ufffd w ork structures that can b e learned b y some metho ds \ufffde\ufffdg\ufffd\ufffd MDL metho ds\ufffd Bouk aert \u0001  \u0005\ufffd\ufffd In part\ufffd b ecause of these limitati ons\ufffd there has b een in terest in learning D A G mo dels with more parsimonious represen tations for the conditional probabilit y of v ariables giv en their\nThe \ufffdrst part of this pap er in tro duces c ausal inter ac\ufffd tion mo dels\ufffd Lik e the causal indep endence mo del\ufffd a causal in teraction mo del is a set of mec hanisms\ufffd a set of causes\ufffd and an e\ufffdect\ufffd Unlik e the causal indep en\ufffd dence mo del\ufffd a cause need not b e asso ciated with a single mec hanism and m ultiple causes can b e asso ci\ufffd ated with a single mec hanism\ufffd Allo wing sev eral causes to b e asso ciated with a single mec hanism allo ws for p artial c ausal inter action b et w een a set of causes\ufffd th us\ufffd causal in teraction mo dels generalize b oth the causal indep endence mo del and the complete causal in terac\ufffd tion mo del\ufffd In Section \u0002\ufffd w e sho w ho w to represen t causal in teraction mo dels as directed acyclic graphi\ufffd cal \ufffdD A G\ufffd mo dels \ufffda\ufffdk\ufffda\ufffd\ufffd Ba y esian net w orks\ufffd b elief net w orks\ufffd causal net w orks\ufffd with hidden v ariables\ufffd In addition w e in tro duce a sp ecial t yp e of causal in terac\ufffd tion mo del\ufffd the exp onential c ausal inter action mo del\ufffd Examples of exp onen tial causal in teraction mo dels are giv en in Section \u0003\ufffd\nIn the second part of the pap er w e turn our atten tion from represen tation to learning the structure and pa\ufffd rameters of exp onen tial causal in teraction mo dels\ufffd In m uc h of the initial w ork on learning \ufffddiscrete\ufffd D A G mo dels\ufffd the fo cus w as on learning the structure of the net w ork assuming there w ere ful l c onditional pr ob abil\ufffd ity tables for eac h v ariable in the net w ork\ufffd The con\ufffd ditional probabilit y table for a v ariable represen ted the conditional probabilit y of the v ariable giv en ev\ufffd ery p ossible com bination of the v alues of its paren ts in the D A G mo del structure\ufffd In this represen tation\ufffd the n um b er of parameters asso ciated with a v ariable is exp onen tial in the n um b er of paren ts of the v ariable\ufffd This exp onen tial explosion can restrict the set of net\ufffd w ork structures that can b e learned b y some metho ds \ufffde\ufffdg\ufffd\ufffd MDL metho ds\ufffd Bouk aert \u0001  \u0005\ufffd\ufffd In part\ufffd b ecause of these limitati ons\ufffd there has b een in terest in learning D A G mo dels with more parsimonious represen tations for the conditional probabilit y of v ariables giv en their\nparen ts\ufffd F or instance in F riedman and Goldszmidt \ufffd\u0001  \u0006\ufffd and Chic k ering et al\ufffd \ufffd\u0001  \u0007\ufffd\ufffd the authors con\ufffd sider using decision trees and a generalization of de\ufffd cision trees to represen t the conditional probabilit y of the v ariable giv en its paren ts\ufffd These represen tations of lo c al structur e allo ws for dramatic reductions in the dimension of the parameter space\ufffd Causal in teraction mo dels pro vide an alternativ e represen tation for the lo cal structure in a D A G mo del\ufffd W e illustrate the fact that there are Noisy\ufffdMax\ufffdIn teraction mo dels that can not b e parsimoniously represen ted b y decision trees and that decision trees and other t yp es of lo cal struc\ufffd tures can b e em b edded in causal in teraction mo dels\ufffd Th us\ufffd causal in teraction mo dels are ric h set of mo dels for parsimoniously represen ting lo cal structure\ufffd\nSince causal in teraction mo dels are D A G mo dels with hidden v ariables and hidden v ariables are just the ex\ufffd treme case of missing data w e discuss learning D A G mo dels with missing data in Section \u0004\ufffd W e also discuss ho w one can use the EM algorithm to obtain ML and MAP estimates for hidden v ariable mo dels\ufffd Finally \ufffd in Section \u0005\ufffd w e illustrate the fact that one can learn the structure of causal in teraction mo dels in a small sim ulation study \ufffd In addition\ufffd Section \u0005 illustrates the imp ortance of correctly calculating the dimension of hidden v ariable mo dels when learning structure\ufffd\n# \u0002 Causal Indep endence and Causal In teraction mo dels\nWhen constructing a parameterized D A G mo dels\ufffd one m ust sp ecify the conditional probabilit y of eac h v ari\ufffd able giv en eac h p ossible con\ufffdguration of the paren ts\ufffd Figure \u0001a sho ws a v ariable E with sev eral paren ts \ufffdcauses\ufffd\ufffd It is often not feasible to sp ecify a complete probabilit y table to represen t the required probabili\ufffd ties\ufffd b ecause the n um b er of probabilities gro ws exp o\ufffd nen tially in the n um b er of paren ts\ufffd In addition\ufffd sev eral authors ha v e argued that this mo del is inaccurate b e\ufffd cause it fails to represen t the indep endence of causal in teractions\ufffd\nT o o v ercome b oth of these inadequacies\ufffd researc hers ha v e used D A G mo dels suc h as the one sho wn in Fig\ufffd ure \u0001b to represen t causal indep endence \ufffde\ufffdg\ufffd\ufffd Go o d\ufffd \u0001 \u0006\u0001\ufffd Kim and P earl\ufffd \u0001 \b\u0003\ufffd Henrion\ufffd \u0001 \b\u0007\ufffd Sriniv as\ufffd \u0001  \u0002\ufffd\ufffd W e shall call the C i \ufffds the causes\ufffd E the ef\ufffd fect\ufffd and the X i \ufffds the noisy me chanism variables\ufffd The \ufffdnoisy\ufffd mec hanism v ariable X i represen ts the con tri\ufffd bution of the i th mec hanism to the e\ufffdect E where the v alue of E is a deterministic function \ufffdindicated b y the double circle in the graph\ufffd of the v alues of the mec hanism v ariables\ufffd The indep endence of the causal mec hanisms is captured b y \ufffd\u0001\ufffd the conditional inde\ufffd p endence of the mec hanism v ariables giv en the causes\ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65ca/65ca9e57-e888-4671-8dcd-4d73521e86d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/67c1/67c11906-716d-4794-9153-d9149fe6d1b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure \u0001\ufffd Di\ufffderen t t yp es of lo cal structure\ufffd</div>\nand \ufffd\u0002\ufffd the indep endence b et w een the set of mec ha\ufffd nism v ariables for E and other v ariables in the net w ork \ufffdnot depicted in Figure \u0001b\ufffd giv en the causes and the e\ufffdect\ufffd\nA causal in teraction mo del relaxes the restrictions that eac h cause has a unique mec hanism v ariable and that eac h mec hanism v ariable has a unique cause\ufffd Figure \u0001c sho ws an example of a causal in teraction mo del\ufffd With a causal in teraction mo del\ufffd it is p ossible to mo del rela\ufffd tionships in whic h some of the causes in teract to cause the e\ufffdect and some of the causes act indep enden tly \ufffd Example of in teractions are often found in medicine\ufffd F or instance\ufffd in some studies smoking and estr o gen level ha v e b een found to ha v e a synergistic e\ufffdect on the rate of strok e in females\ufffd There is no reason to stop the mo deling of the causal pro cess at this lev el\ufffd The i th mec hanism describ ed b y the conditional distri\ufffd bution of X i giv en the paren t of X i could b e mo deled as a decision tree\ufffd or a mo del with additional hidden v ariables\ufffd Roughly \ufffd a mec hanism describ es one \ufffdpath\ufffd through whic h a set of causes lead to an e\ufffdect\ufffd A me chanism for causes C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n and e\ufffdect E are a set of no des M whic h are not observ ed \ufffdhidden\ufffd suc h that \ufffd\u0001\ufffd there is a distinguished v ariable called the noisy me chanism variable \ufffdor\ufffd simply \ufffd the mec hanism v ariable\ufffd\ufffd \ufffd\u0002\ufffd only mem b ers of the mec hanism M and causes can p oin t to mem b ers of M \ufffd \ufffd\u0003\ufffd the no des in M form a directed acyclic graph\ufffd \ufffd\u0004\ufffd the only v ariable in M that p oin ts to a non\ufffdmem b er of M is the mec hanism v ariable whic h only p oin ts to E \ufffd The Figure \u0002b illustrates and exam\ufffd ple of a mec hanism\ufffd Note that a cause can p oin t to m ultiple no des in a mec hanism\ufffd A causal in teraction mo del is roughly a D A G mo del of mec hanisms whic h describ es the conditional distri\ufffd bution of the e\ufffdect giv en its causes\ufffd More precisely \ufffd a c ausal inter action mo del is a \ufffd\u0001\ufffd a set of causes C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n \ufffd \ufffd\u0002\ufffd an e\ufffdect v ariable E \ufffd \ufffd\u0003\ufffd a set of mec ha\ufffd nisms for C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n and e\ufffdect E \ufffd whic h w e denote b y M \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd M m \ufffd \ufffd\u0004\ufffd where the v alue of the e\ufffdect v ariable is a deterministic function of the mec hanism v ariables X \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd X m \ufffd whic h w e call the c ombination function\ufffd\nA causal in teraction mo del relaxes the restrictions that eac h cause has a unique mec hanism v ariable and that eac h mec hanism v ariable has a unique cause\ufffd Figure \u0001c sho ws an example of a causal in teraction mo del\ufffd With a causal in teraction mo del\ufffd it is p ossible to mo del rela\ufffd tionships in whic h some of the causes in teract to cause the e\ufffdect and some of the causes act indep enden tly \ufffd Example of in teractions are often found in medicine\ufffd F or instance\ufffd in some studies smoking and estr o gen level ha v e b een found to ha v e a synergistic e\ufffdect on the rate of strok e in females\ufffd There is no reason to stop the mo deling of the causal pro cess at this lev el\ufffd The i th mec hanism describ ed b y the conditional distri\ufffd bution of X i giv en the paren t of X i could b e mo deled as a decision tree\ufffd or a mo del with additional hidden v ariables\ufffd\nRoughly \ufffd a mec hanism describ es one \ufffdpath\ufffd through whic h a set of causes lead to an e\ufffdect\ufffd A me chanism for causes C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n and e\ufffdect E are a set of no des M whic h are not observ ed \ufffdhidden\ufffd suc h that \ufffd\u0001\ufffd there is a distinguished v ariable called the noisy me chanism variable \ufffdor\ufffd simply \ufffd the mec hanism v ariable\ufffd\ufffd \ufffd\u0002\ufffd only mem b ers of the mec hanism M and causes can p oin t to mem b ers of M \ufffd \ufffd\u0003\ufffd the no des in M form a directed acyclic graph\ufffd \ufffd\u0004\ufffd the only v ariable in M that p oin ts to a non\ufffdmem b er of M is the mec hanism v ariable whic h only p oin ts to E \ufffd The Figure \u0002b illustrates and exam\ufffd ple of a mec hanism\ufffd Note that a cause can p oin t to m ultiple no des in a mec hanism\ufffd\nA causal in teraction mo del is roughly a D A G mo del of mec hanisms whic h describ es the conditional distri\ufffd bution of the e\ufffdect giv en its causes\ufffd More precisely \ufffd a c ausal inter action mo del is a \ufffd\u0001\ufffd a set of causes C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n \ufffd \ufffd\u0002\ufffd an e\ufffdect v ariable E \ufffd \ufffd\u0003\ufffd a set of mec ha\ufffd nisms for C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n and e\ufffdect E \ufffd whic h w e denote b y M \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd M m \ufffd \ufffd\u0004\ufffd where the v alue of the e\ufffdect v ariable is a deterministic function of the mec hanism v ariables X \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd X m \ufffd whic h w e call the c ombination function\ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2845/28450555-1c5d-4347-8a12-f66f3a8faf33.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9438/94387db5-182b-462a-afb1-589ff5d7a83b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a262/a262b55a-a557-4ac4-8974-a576948dece1.png\" style=\"width: 50%;\"></div>\nLet M b e the set of all of the v ariables in mec hanisms for causes C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n and e\ufffdect E \ufffd As in the case of causal indep endence mo dels\ufffd the indep endence of the causal mec hanisms is captured b y \ufffd\u0001\ufffd the conditional indep endence of the set of v ariables in eac h mec hanism giv en the causes \ufffdi\ufffde\ufffd\ufffd for i \u0006\ufffd j \ufffd M i is indep enden t of M j giv en C \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd C n \ufffd\ufffd and \ufffd\u0002\ufffd the indep endence b e\ufffd t w een the set of all mec hanism v ariables \ufffdM\ufffd and other v ariables in the D A G mo del giv en the causes C i and e\ufffdect E \ufffd\nIt is common to add a leak term to the noisy\ufffdor and noisy\ufffdmax mo dels\ufffd A leak term is added to mo del mec hanisms not asso ciated with other v ariables in the mo del\ufffd A leak term corresp onds to a mec hanism v ari\ufffd able \ufffdand th us a mec hanism\ufffd whic h do es not ha v e an y causes that are in the D A G mo del\ufffd\nFinally \ufffd an exp onential c ausal inter action mo del is a causal in teraction mo del in whic h the conditional lik e\ufffd liho o d for eac h v ariable in eac h mec hanism is in the exp onen tial family \ufffd In Section \u0003\ufffd w e discuss a v ari\ufffd et y of sp eci\ufffdc exp onen tial causal in teraction mo dels\ufffd W e fo cus on exp onen tial causal in teraction mo dels b e\ufffd cause with these mo dels w e can often \ufffdnd tractable algorithms for inference and with tractable mo dels for inference w e can apply the EM algorithm\ufffd\n# \u0003 Examples of Exp onen tial Causal In teraction mo dels\nIn this section w e giv e a few examples of exp onen tial causal in teraction mo dels\ufffd\n# \u0003\ufffd\u0001 Noisy\ufffdMax\ufffdIn teraction mo dels\nA noisy\ufffdmax\ufffdinter action \ufffdNMI\ufffd mo del is a causal in ter\ufffd action mo del in whic h\ufffd \ufffd\u0001\ufffd eac h mec hanism consists of a single mec hanism v ariable whic h has a domain that is a subset of the domain of the e\ufffdect v ariable\ufffd \ufffd\u0002\ufffd the\ndomain of the e\ufffdect v ariable can b e ordered b y a bi\ufffd nary relation \ufffd\ufffd \ufffd\u0003\ufffd the lik eliho o d of eac h mec hanism v ariable giv en the v alues of its paren ts is in the ex\ufffd p onen tial family \ufffd and \ufffd\u0004\ufffd the com bination function is max \ufffd \ufffdx \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd x m \ufffd\ufffd Note that the e\ufffdect and the mec h\ufffd anism v ariables need not b e discrete\ufffd It follo ws from the com bination function that\nAn NMI mo del in whic h there is only one cause p er mec hanism v ariable is a generalization of the Noisy\ufffd or and Noisy\ufffdmax mo dels\ufffd These NMI mo dels are noisy\ufffdmax mo dels without a distinguished state \ufffde\ufffdg\ufffd\ufffd \ufffdabsen t\ufffd or \ufffdnormal\ufffd\ufffd\ufffd Of course\ufffd one can create a Noisy\ufffdMax\ufffdInter action mo del with distinguishe d states b y simply distinguishing one paren t con\ufffdguration for eac h mec hanism v ariable and forcing the asso ciated parameters to \ufffd and \u0001\ufffd Clearly \ufffd when one \ufffdxes param\ufffd eters one is reducing the n um b er of free parameters in the mo del\ufffd One b ene\ufffdt of mo dels without distin\ufffd guished states is that they can b e easier to learn\ufffd In the case where one do es not kno w the distinguished states for eac h of the mec hanism v ariables\ufffd w e ha v e an additional learning problem\ufffd namely w e need to iden\ufffd tify whic h paren t con\ufffdgurations are the distinguished states\ufffd Of course\ufffd if w e do kno w whic h paren t con\ufffdgu\ufffd ration is the distinguished state then w e can force the parameter restrictions and use the EM algorithm to calculate the ML or MAP estimate of the parameters and appro ximate the p osteriors on the mo dels\ufffd As a sp ecial case w e consider a discr ete NMI mo del\ufffd an NMI in whic h \ufffd\u0001\ufffd E is a discrete random v ariable \ufffdnot necessarily \ufffdnite\ufffd\ufffd and \ufffd\u0002\ufffd eac h mec hanism con tains only a mec hanism v ariable\ufffd Let \ufffd ij k \ufffd p\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd \ufffd p\ufffdX i \ufffd k jP a X i \ufffd j\ufffd \ufffd \ufffd\ufffd Where P a X i is the set of paren ts of X i \ufffd Th us p\ufffdX i \ufffd x i j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd \ufffd P k \ufffdx i \ufffd ij k \ufffd Let j i b e the instan tiation of causes for the i th mec hanism v ariable\ufffd As discussed in Section \u0004\ufffd\u0001\ufffd to use the EM algorithm w e will need to calculate p\ufffdX i \ufffd k \ufffd P a X i \ufffd j j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd \ufffd I \ufffdj \ufffd j i \ufffdp\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd\ufffd where I \ufffdj \ufffd j i \ufffd is an indicator function that is one if and only if j \ufffd j i \ufffd\nAs a sp ecial case w e consider a discr ete NMI mo del\ufffd an NMI in whic h \ufffd\u0001\ufffd E is a discrete random v ariable \ufffdnot necessarily \ufffdnite\ufffd\ufffd and \ufffd\u0002\ufffd eac h mec hanism con tains only a mec hanism v ariable\ufffd Let \ufffd ij k \ufffd p\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd \ufffd p\ufffdX i \ufffd k jP a X i \ufffd j\ufffd \ufffd \ufffd\ufffd Where P a X i is the set of paren ts of X i \ufffd Th us p\ufffdX i \ufffd x i j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd \ufffd P k \ufffdx i \ufffd ij k \ufffd Let j i b e the instan tiation of causes for the i th mec hanism v ariable\ufffd As discussed in Section \u0004\ufffd\u0001\ufffd to use the EM algorithm w e will need to calculate p\ufffdX i \ufffd k \ufffd P a X i \ufffd j j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd \ufffd I \ufffdj \ufffd j i \ufffdp\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd\ufffd where I \ufffdj \ufffd j i \ufffd is an indicator function that is one if and only if j \ufffd j i \ufffd\np\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c \ufffd E \ufffd e\ufffd \ufffd \ufffd \ufffd \b \ufffd\nk \ufffd e k \ufffd e k \ufffd e\nNote that for eac h mec hanism v ariable w e only need to calculate p\ufffdX i j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd\ufffd If the conditional distri\ufffd bution is in the exp onen tial family then it is easy to\napply the EM algorithm\ufffd e\ufffdg\ufffd\ufffd if the conditional dis\ufffd tribution p\ufffdX i j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd is distributed according to a P oisson or m ultinom ia l distribution\ufffd In addition\ufffd w e do not need to ha v e a unique conditional distribution for eac h instan tiation of the paren ts of the mec hanism v ariable\ufffd Rather\ufffd one can use a decision tree or a de\ufffd cision graph to reduce the n um b er of conditional dis\ufffd tributions and th us reduce the n um b er of parameters needed for sp ecifying the conditional distribution of the mec hanism v ariable\ufffd This can ev en b e done when the conditional distribution function is the P oisson dis\ufffd tribution\ufffd Since the conditional distribution of a mec h\ufffd anism v ariable can b e represen ted with a decision tree\ufffd the NMI mo del is at least as represen tationally ric h as decision trees\ufffd A noisy\ufffdor mo del \ufffda sp ecial case of an NMI mo del\ufffd with n binary causes and a binary e\ufffdect has n param\ufffd eters\ufffd Ho w ev er\ufffd for almost all v alues of the param\ufffd eters \ufffdall but a set of Leb esgue measure zero\ufffd a full probabilit y table\ufffd i\ufffde\ufffd\ufffd a complete decision tree\ufffd m ust b e used to represen t the distribution exactly \ufffd Th us\ufffd causal in teraction mo dels pro vide a ric h represen ta\ufffd tion for mo deling conditional distributions\ufffd Causal in\ufffd teraction mo dels can b e view ed as an alternativ e to decision trees or decision graphs for parsimonious lo\ufffd cal represen tations\ufffd ho w ev er\ufffd since decision trees and graphs can b e em b edded in causal in teraction mo dels\ufffd they are strictly ric her represen tation\ufffd The ca v eat\ufffd as w e shall see in Section \u0004\ufffd is that one m ust use iter\ufffd ativ e metho ds in appro ximating sev eral quan tities of in terest when using causal in teraction mo dels\ufffd Under suitable assumptions\ufffd this is not the case for decision trees and decision graphs\ufffd Finally \ufffd in Noisy\ufffdor and Noisy\ufffdMax mo dels it is com\ufffd mon to add a leak term to mo del mec hanisms not as\ufffd so ciated with the other v ariables in the mo del\ufffd As discussed in Section \u0002\ufffd w e can add leaks to NMI mo d\ufffd els\ufffd ho w ev er\ufffd the extra degrees of freedom in a NMI mo del as compared to a Noisy\ufffdMax can act somewhat lik e a leak term in a Noisy\ufffdmax mo del\ufffd\nFinally \ufffd in Noisy\ufffdor and Noisy\ufffdMax mo dels it is com\ufffd mon to add a leak term to mo del mec hanisms not as\ufffd so ciated with the other v ariables in the mo del\ufffd As discussed in Section \u0002\ufffd w e can add leaks to NMI mo d\ufffd els\ufffd ho w ev er\ufffd the extra degrees of freedom in a NMI mo del as compared to a Noisy\ufffdMax can act somewhat lik e a leak term in a Noisy\ufffdmax mo del\ufffd\n# \u0003\ufffd\u0002 Noisy\ufffdAdditi v e\ufffdIn ter action mo dels\nA Noisy\ufffdA dditive\ufffdinter action \ufffdNAI\ufffd mo del is a causal in teraction mo del in whic h\ufffd \ufffd\u0001\ufffd eac h mec hanism con\ufffd sists of a single mec hanism v ariable whic h has a do\ufffd main that is a subset of the domain of the e\ufffdect v ari\ufffd able\ufffd \ufffd\u0002\ufffd the domain of the e\ufffdect v ariable is closed under addition\ufffd \ufffd\u0003\ufffd the lik eliho o d of eac h mec hanism v ariable giv en the v alues of its paren ts is in the ex\ufffd p onen tial family \ufffd and \ufffd\u0004\ufffd the com bination function is addition\ufffd P m i\ufffd\u0001 X i \ufffd\nAs a sp ecial case of an NAI mo del in whic h the ef\ufffd fect is not con tin uous\ufffd w e consider a P oisson NAI\nmo del\ufffd A P oisson NAI mo del is an NAI mo del in whic h \ufffd\u0001\ufffd p\ufffdX i jP a X i \ufffd j\ufffd \ufffd \ufffd \ufffd P oisson\ufffd\ufffd \ufffdi\ufffdj \ufffd \ufffd that is p\ufffdX i \ufffd xjP a X i \ufffd j\ufffd \ufffd \ufffd \ufffd exp\ufffd\ufffd\ufffd \ufffdi\ufffdj \ufffd \ufffd \ufffd x \ufffdi\ufffdj \ufffd x\ufffd \ufffd and \ufffd\u0002\ufffd eac h mec hanism con tains only a mec hanism v ariable\ufffd \ufffd is called the rate parameter for the P oisson\ufffd In this case \ufffd \ufffdi\ufffdj \ufffd is a conditional rate parameter\ufffd Let \ufffd \ufffdi\ufffdj \ufffd b e the parameter for the i th mec hanism v ariable when the paren ts of the i th mec hanism v ariable are in the j th state\ufffd Let j i b e the instan tiation of paren ts of the i th mec hanism v ariable\ufffd Using the theorem that the sum of n indep enden t random v ariables ha ving P oisson dis\ufffd tributions with parameters \ufffd \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd n is distributed as a P oisson random v ariable with parameter \ufffd \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd n w e can c haracterize the P oisson NAI mo del with the equation p\ufffdE j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd \ufffd P oisson\ufffd P m i\ufffd\u0001 \ufffd \ufffdi\ufffdj i \ufffd \ufffd\ufffd P oisson random v ariables are useful in analyzing rates\ufffd e\ufffdg\ufffd\ufffd n um b er of w eb page hits p er w eek or n um b er of headac hes p er w eek\ufffd Th us\ufffd the P oisson NAI mo del has p oten tial for mo deling conditional rates ev en in cases where the causes of the rate can in teract\ufffd As with Noisy\ufffdMax\ufffdIn teraction mo dels\ufffd for a giv en mec h\ufffd anism v ariable\ufffd one need not ha v e a unique parameter for eac h instan tiation of the paren ts of the mec hanism v ariable\ufffd Rather\ufffd one can use decision trees and deci\ufffd sion graphs to reduce the n um b er of parameters needed for sp ecifying the conditional distribution of the mec h\ufffd anism v ariable\ufffd One in teresting feature of the P oisson NAI mo del is that it is p ossible to run inference using a clique\ufffd tree t yp e inference algorithm despite the fact that the clique p oten tials are in\ufffdnite\ufffd The tric k is to form the clique p oten tials only after the v alue of E is kno wn\ufffd With the v alue of E kno wn w e can b ound the v alues of the X i \ufffds and th us b ound the size of the clique p o\ufffd ten tial\ufffd Let j i b e the instan tiation of causes for the i th mec h\ufffd anism v ariable\ufffd As discussed in Section \u0004\ufffd\u0001\ufffd to use the EM algorithm w e will need to calculate p\ufffdX i \ufffd k \ufffd P a X i \ufffd j j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd \ufffd I \ufffdj \ufffd j i \ufffdp\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd\ufffd where I \ufffdj \ufffd j i \ufffd is an indicator function that is one if and only if j \ufffd j i \ufffd Belo w is the equation for p\ufffdX \u0001 \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd where there are m mec hanism v ariables\ufffd Let \ufffd ij k \ufffd p\ufffdX i \ufffd k jP a X i \ufffd j\ufffd \ufffd \ufffd\ufffd The inferences for other mec hanism v ariables are analogous\ufffd\nmo del\ufffd A P oisson NAI mo del is an NAI mo del in whic h \ufffd\u0001\ufffd p\ufffdX i jP a X i \ufffd j\ufffd \ufffd \ufffd \ufffd P oisson\ufffd\ufffd \ufffdi\ufffdj \ufffd \ufffd that is p\ufffdX i \ufffd xjP a X i \ufffd j\ufffd \ufffd \ufffd \ufffd exp\ufffd\ufffd\ufffd \ufffdi\ufffdj \ufffd \ufffd \ufffd x \ufffdi\ufffdj \ufffd x\ufffd \ufffd and \ufffd\u0002\ufffd eac h mec hanism con tains only a mec hanism v ariable\ufffd \ufffd is called the rate parameter for the P oisson\ufffd In this case \ufffd \ufffdi\ufffdj \ufffd is a conditional rate parameter\ufffd Let \ufffd \ufffdi\ufffdj \ufffd b e the parameter for the i th mec hanism v ariable when the paren ts of the i th mec hanism v ariable are in the j th state\ufffd Let j i b e the instan tiation of paren ts of the i th mec hanism v ariable\ufffd Using the theorem that the sum of n indep enden t random v ariables ha ving P oisson dis\ufffd tributions with parameters \ufffd \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd n is distributed as a P oisson random v ariable with parameter \ufffd \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd n w e can c haracterize the P oisson NAI mo del with the equation p\ufffdE j \ufffd C \ufffd \ufffd c\ufffd \ufffd \ufffd \ufffd P oisson\ufffd P m i\ufffd\u0001 \ufffd \ufffdi\ufffdj i \ufffd \ufffd\ufffd P oisson random v ariables are useful in analyzing rates\ufffd e\ufffdg\ufffd\ufffd n um b er of w eb page hits p er w eek or n um b er of headac hes p er w eek\ufffd Th us\ufffd the P oisson NAI mo del has p oten tial for mo deling conditional rates ev en in cases where the causes of the rate can in teract\ufffd As with Noisy\ufffdMax\ufffdIn teraction mo dels\ufffd for a giv en mec h\ufffd anism v ariable\ufffd one need not ha v e a unique parameter for eac h instan tiation of the paren ts of the mec hanism v ariable\ufffd Rather\ufffd one can use decision trees and deci\ufffd sion graphs to reduce the n um b er of parameters needed for sp ecifying the conditional distribution of the mec h\ufffd anism v ariable\ufffd\nP oisson random v ariables are useful in analyzing rates\ufffd e\ufffdg\ufffd\ufffd n um b er of w eb page hits p er w eek or n um b er of headac hes p er w eek\ufffd Th us\ufffd the P oisson NAI mo del has p oten tial for mo deling conditional rates ev en in cases where the causes of the rate can in teract\ufffd As with Noisy\ufffdMax\ufffdIn teraction mo dels\ufffd for a giv en mec h\ufffd anism v ariable\ufffd one need not ha v e a unique parameter for eac h instan tiation of the paren ts of the mec hanism v ariable\ufffd Rather\ufffd one can use decision trees and deci\ufffd sion graphs to reduce the n um b er of parameters needed for sp ecifying the conditional distribution of the mec h\ufffd anism v ariable\ufffd\nOne in teresting feature of the P oisson NAI mo del is that it is p ossible to run inference using a clique\ufffd tree t yp e inference algorithm despite the fact that the clique p oten tials are in\ufffdnite\ufffd The tric k is to form the clique p oten tials only after the v alue of E is kno wn\ufffd With the v alue of E kno wn w e can b ound the v alues of the X i \ufffds and th us b ound the size of the clique p o\ufffd ten tial\ufffd\nLet j i b e the instan tiation of causes for the i th mec h\ufffd anism v ariable\ufffd As discussed in Section \u0004\ufffd\u0001\ufffd to use the EM algorithm w e will need to calculate p\ufffdX i \ufffd k \ufffd P a X i \ufffd j j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd \ufffd I \ufffdj \ufffd j i \ufffdp\ufffdX i \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd\ufffd where I \ufffdj \ufffd j i \ufffd is an indicator function that is one if and only if j \ufffd j i \ufffd Belo w is the equation for p\ufffdX \u0001 \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd where there are m mec hanism v ariables\ufffd Let \ufffd ij k \ufffd p\ufffdX i \ufffd k jP a X i \ufffd j\ufffd \ufffd \ufffd\ufffd The inferences for other mec hanism v ariables are analogous\ufffd\np\ufffdX \u0001 \ufffd k j \ufffd C \ufffd \ufffd c\ufffd E \ufffd e\ufffd \ufffd \ufffd \ufffd\nk \ufffd e\nk \ufffd e k \ufffd e\nk \ufffd e\nA case where inference is ev en easier are Gaussian NAI\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d7b/2d7b634f-6302-4b56-95ed-e6349e8090f3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6781/678160bc-6f94-4816-b066-7511ee142882.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Expanded version of Model A</div>\nFigure \u0003\ufffd In teraction mo del with nested structure and conditional clique tree\ufffd\nmo dels\ufffd A Gaussian NAI mo del is a causal in teraction mo del in whic h the conditional distribution of eac h of the mec hanism v ariables is Gaussian\ufffd By including a discrete \ufffdnite state hidden v ariable inside a mec ha\ufffd nism it is p ossible to ha v e conditional distribution for the mec hanism v ariables whic h are mixtures of Gaus\ufffd sians\ufffd In other t yp es of NAI mo dels\ufffd e\ufffdg\ufffd\ufffd where some of the conditional distribution are Gaussian and some P oisson\ufffd inference is more di\ufffdcult\ufffd\n# \u0003\ufffd\u0003 Other mo dels\nBoth the NMI and NAI mo dels ha v e fairly simple structure\ufffd Figure \u0003b illustrates a causal in teraction mo del with a more complicated nested structure\ufffd As with an y causal in teraction mo del\ufffd there is a la y er of mec hanism no des follo w ed b y a deterministic com bi\ufffd nation function\ufffd The expanded v ersion of Mo del A in Figure \u0003b illustrates that the conditional distribu\ufffd tion of the mec hanism no des giv en its paren t causes can ha v e nested structure\ufffd In this case\ufffd the mec ha\ufffd nisms asso ciated mec hanism v ariables X \u0001 and X \u0002 ha v e nested causal in teraction mo dels and the mec hanism asso ciated with mec hanism v ariable X \u0003 has a nested hidden v ariable X   \ufffd It is imp ortan t to note that when the v alues of E and the C i \ufffds are observ ed all of hid\ufffd den v ariables in the in teraction mo del are d\ufffdseparated from other v ariables in the mo del\ufffd i\ufffde\ufffd\ufffd v ariables not in the in teraction mo del\ufffd and th us inference for EM can b e lo calized to the in teraction mo del\ufffd\nOne migh t think that inference and th us using EM w ould b e computationally hop eless in the expanded v ersion of Mo del A in Figure \u0003b or more complicated causal in teraction mo dels\ufffd This is not alw a ys the case\ufffd F or the expanded v ersion of Mo del A the in teraction structure conditional on the C i \ufffds forms a p olytree\ufffd Th us the p olynomial \ufffdtim e algorithm of Kim and P earl\n\ufffd\u0001 \b\u0003\ufffd can b e used for inference\ufffd More generally \ufffd the indep endence of the mec hanisms in a causal in terac\ufffd tion mo del lead to computational e\ufffdciencies in infer\ufffd ence b ecause\ufffd in the clique tree conditional on the C i \ufffds\ufffd the no des from di\ufffderen t mec hanism are only connected b y paths through mec hanism v ariables\ufffd This p oin t is illustrated b y the conditional clique tree in Figure \u0003b\ufffd In addition to allo wing for nested structure\ufffd causal in\ufffd teraction mo dels also allo w for other t yp es of com bi\ufffd nation functions\ufffd F or instance\ufffd an \ufffdN\ufffdof \ufffd com bination function is the com bination function for a binary e\ufffdect v ariable whic h is equal to \u0001 if and only if N or more bi\ufffd nary mec hanism v ariables are equal to \u0001\ufffd Clearly this can b e generalized to handle con tin uous v ariables\ufffd By using suc h an additiv e threshold com bination function one can capture threshholding e\ufffdects in a causal in\ufffd teraction mo del\ufffd Another com bination function is the X\ufffdor or parit y com bination function in whic h the bi\ufffd nary e\ufffdect v ariable is equal to \u0001 if and only if an ev en n um b er of the binary mec hanism v ariables are equal to \u0001\ufffd Causal in teraction mo dels with this com bination function where the causes are join tly indep enden t lead to a parameterized v ersion of the pseudo\ufffdindep endence mo del of Xiang et al\ufffd \ufffd\u0001  \u0006\ufffd\ufffd\n# \u0004 Learning the Structure and P arameters\nIn this section\ufffd w e in v estigate ho w to learn the pa\ufffd rameters and the structure for exp onen tial causal in\ufffd teraction mo dels\ufffd In Section \u0004\ufffd\u0001\ufffd w e sho w ho w to use the EM algorithm \ufffdDempster et al\ufffd\ufffd \u0001 \u0007\u0007\ufffd to compute the ML and MAP estimate of the parameters\ufffd In Sec\ufffd tion \u0004\ufffd\u0002\ufffd w e in v estigate asymptotic appro ximations of the marginal lik eliho o d\ufffd in particular\ufffd the Cheeseman\ufffd Stutz appro ximation \ufffd\u0001  \u0005\ufffd\ufffd\n# \u0004\ufffd\u0001 Learning P arameters\nW e can write the causal in teraction mo del as a D A G mo del\ufffd In particular\ufffd this means that w e assume that the true \ufffdor ph ysical\ufffd join t probabilit y distribution for the set of v ariables X \ufffd fX \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd X n g in the D A G mo del can b e enco ded in some D A G mo del S \ufffd In this section\ufffd X \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd X n are all of the v ariables in the mo del an not just the mec hanism v ariables of a causal in ter\ufffd action mo del\ufffd W e write\n\ufffd\u0001\ufffd\nwhere \ufffd i is the v ector of parameters for the distri\ufffd bution p\ufffdx i jpa i \ufffd \ufffd i \ufffd S \ufffd\ufffd \ufffd s is the v ector of parameters \ufffd\ufffd \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd n \ufffd\ufffd In addition\ufffd w e assume that w e ha v e a random sample D \ufffd fx \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd x N g from the true join t\nprobabilit y distribution of X\ufffd W e refer to an elemen t x l of D as a c ase\ufffd Finally \ufffd w e ha v e a prior probabil\ufffd it y densit y function p\ufffd\ufffd s jS \ufffd o v er the parameters of the D A G mo del\ufffd The problem of learning probabilities in a Ba y esian net w ork can no w b e stated simply\ufffd Giv en a random sample D \ufffd compute the p osterior distribution p\ufffd\ufffd s jD \ufffd S \ufffd\ufffd W e refer to the conditional distribution p\ufffdx i jpa i \ufffd \ufffd i \ufffd S \ufffd as a lo c al \ufffdc onditional\ufffd distribution function\ufffd In this section\ufffd w e illustrate the use of the EM algorithm in the case where eac h lo cal distribution function is col\ufffd lection of m ultinomi al distributions\ufffd one distribution for eac h con\ufffdguration of P a i \ufffd Namely \ufffd w e assume\nprobabilit y distribution of X\ufffd W e refer to an elemen t x l of D as a c ase\ufffd Finally \ufffd w e ha v e a prior probabil\ufffd it y densit y function p\ufffd\ufffd s jS \ufffd o v er the parameters of the D A G mo del\ufffd The problem of learning probabilities in a Ba y esian net w ork can no w b e stated simply\ufffd Giv en a random sample D \ufffd compute the p osterior distribution p\ufffd\ufffd s jD \ufffd S \ufffd\ufffd\nW e refer to the conditional distribution p\ufffdx i jpa i \ufffd \ufffd i \ufffd S \ufffd as a lo c al \ufffdc onditional\ufffd distribution function\ufffd In this section\ufffd w e illustrate the use of the EM algorithm in the case where eac h lo cal distribution function is col\ufffd lection of m ultinomi al distributions\ufffd one distribution for eac h con\ufffdguration of P a i \ufffd Namely \ufffd w e assume\n\ufffd\u0002\ufffd\nwhere pa \u0001 i \ufffd \ufffd \ufffd \ufffd \ufffd pa q i i \ufffdq i \ufffd Q X i \u0002P a i r i \ufffd denote the con\ufffdgurations of P a i \ufffd and \ufffd i \ufffd \ufffd\ufffd\ufffd ij k \ufffd r i k \ufffd\u0002 \ufffd q i j \ufffd\u0001 are the parameters\ufffd \ufffdThe parameter \ufffd ij \u0001 is giv en b y \u0001 \ufffd P r i k \ufffd\u0002 \ufffd ij k \ufffd\ufffd F or con v enience\ufffd w e de\ufffdne the v ec\ufffd tor of parameters\nfor all i and j \ufffd W e assume that eac h v ector \ufffd ij has the prior distribution Dir\ufffd\ufffd ij j\ufffd ij \u0001 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd ij r i \ufffd\ufffd N ij k is the n um b er of cases in D in whic h X i \ufffd x k i and P a i \ufffd pa j i \ufffd\ng \ufffd\ufffd s \ufffd \ufffd log\ufffdp\ufffdD j\ufffd s \ufffd S \ufffd \ufffd p\ufffd\ufffd s jS \ufffd\ufffd\n\ufffd\u0003\ufffd\nThis con\ufffdguration also maximi zes p\ufffd\ufffd s jD \ufffd S \ufffd\ufffd and is kno wn as the maximum a p osteriori \ufffdMAP\ufffd con\ufffdgu\ufffd ration of \ufffd s \ufffd Also de\ufffdne \ufffd \ufffd s to b e the con\ufffdguration of \ufffd s that maxim izes p\ufffdD j\ufffd s \ufffd S \ufffd\ufffd This con\ufffdguration is kno wn as the maximum likeliho o d \ufffdML\ufffd con\ufffdguration of \ufffd s \ufffd\nIn the case of causal in teraction mo dels\ufffd w e need to compute the p osterior giv en incomplete data\ufffd Unlik e the complete\ufffddata case\ufffd w e need to use appro ximation tec hniques\ufffd F or more details see\ufffd for instance\ufffd Hec k\ufffd erman \ufffd\u0001  \u0005\ufffd\ufffd These tec hniques include Mon te Carlo approac hes suc h as Gibbs sampling and imp ortance sampling \ufffdNeal\ufffd \u0001  \u0003\ufffd Madigan and Raftery \ufffd \u0001  \u0004\ufffd\ufffd asymptotic appro ximations \ufffdKass et al\ufffd\ufffd \u0001 \b\b\ufffd\ufffd and sequen tial up dating metho ds \ufffdSpiegelhalter and Lau\ufffd ritzen\ufffd \u0001  \ufffd\ufffd Co w ell et al\ufffd\ufffd \u0001  \u0005\ufffd\ufffd\nThe asymptotic appro ximations are based on the ob\ufffd serv ation that\ufffd as the n um b er of cases increases\ufffd the p osterior on the parameters will b e distributed accord\ufffd ing to a m ultiv ariate\ufffdGaussian distribution\ufffd As w e con tin ue to get more cases the Gaussian p eak will b e\ufffd come sharp er\ufffd tending to a delta function at the MAP con\ufffdguration \ufffd \ufffd s \ufffd In this limit\ufffd w e can use the MAP con\ufffdguration to appro ximate the distribution\ufffd\nA further appro ximatio n is based on the observ ation that\ufffd as the sample size increases\ufffd the e\ufffdect of the prior p\ufffd\ufffd s jS \ufffd diminishes\ufffd Th us\ufffd w e can appro ximate \ufffd \ufffd s b y the maxim um maximum likeliho o d \ufffdML\ufffd con\ufffdguration of \ufffd s \ufffd\nOne class of tec hniques for \ufffdnding a ML or MAP is gradien t\ufffdbased optimization\ufffd F or example\ufffd w e can use gradien t ascen t\ufffd where w e follo w the deriv ativ es of g \ufffd\ufffd s \ufffd or the lik eliho o d p\ufffdD j\ufffd s \ufffd S \ufffd to a lo cal maxi\ufffd m um\ufffd Russell et al\ufffd \ufffd\u0001  \u0005\ufffd and Thiesson \ufffd\u0001  \u0005\ufffd sho w ho w to compute the deriv ativ es of the lik eliho o d for a Ba y esian net w ork with unrestricted m ultinom ial dis\ufffd tributions\ufffd Bun tine \ufffd\u0001  \u0004\ufffd discusses the more gen\ufffd eral case where the lik eliho o d function comes from the exp onen tial family \ufffd Of course\ufffd these gradien t\ufffdbased metho ds \ufffdnd only lo cal maxim a\ufffd\nAnother tec hnique for \ufffdnding a lo cal ML or MAP is the exp ectation\ufffdmaximizatio n \ufffdEM\ufffd algorithm \ufffdDempster et al\ufffd\ufffd \u0001 \u0007\u0007\ufffd\ufffd T o \ufffdnd a lo cal MAP or ML\ufffd w e b egin b y assigning a con\ufffdguration to \ufffd s someho w \ufffde\ufffdg\ufffd\ufffd at random\ufffd\ufffd Next\ufffd w e compute the exp e cte d suf\ufffd \ufffdcient statistics for a complete data set\ufffd where exp ec\ufffd tation is tak en with resp ect to the join t distribution for X conditioned on the assigned con\ufffdguration of \ufffd s and the kno wn data D \ufffd In our discrete example\ufffd w e compute\n\ufffd\u0004\ufffd\nwhere y l is the p ossibly incomplete l th case in D \ufffd When X i and all the v ariables in P a i are observ ed in case x l \ufffd the term for this case requires a trivial compu\ufffd tation\ufffd it is either zero or one\ufffd Otherwise\ufffd w e can use an y Ba y esian net w ork inference algorithm to ev aluate the term\ufffd This computation is called the exp e ctation step of the EM algorithm\ufffd\nwhere y l is the p ossibly incomplete l th case in D \ufffd When X i and all the v ariables in P a i are observ ed in case x l \ufffd the term for this case requires a trivial compu\ufffd tation\ufffd it is either zero or one\ufffd Otherwise\ufffd w e can use an y Ba y esian net w ork inference algorithm to ev aluate the term\ufffd This computation is called the exp e ctation step of the EM algorithm\ufffd Next\ufffd w e use the exp ected su\ufffdcien t statistics as if they w ere actual su\ufffdcien t statistics from a complete ran\ufffd dom sample D c \ufffd If w e are doing an ML calculation\ufffd then w e determine the con\ufffdguration of \ufffd s that maxi\ufffd mize p\ufffdD c j\ufffd s \ufffd S \ufffd\ufffd In our discrete example\ufffd w e ha v e\nNext\ufffd w e use the exp ected su\ufffdcien t statistics as if they w ere actual su\ufffdcien t statistics from a complete ran\ufffd dom sample D c \ufffd If w e are doing an ML calculation\ufffd then w e determine the con\ufffdguration of \ufffd s that maxi\ufffd mize p\ufffdD c j\ufffd s \ufffd S \ufffd\ufffd In our discrete example\ufffd w e ha v e\nIf w e are doing a MAP calculation\ufffd then w e determine the con\ufffdguration of \ufffd s that maxim izes p\ufffd\ufffd s jD c \ufffd S \ufffd\ufffd In our discrete example\ufffd w e ha v e \u0002\n\u0002 The MAP con\ufffdguration \ufffd \ufffd s dep ends on the co ordinate system in whic h the parameter v ariables are expressed\ufffd\nThis assignmen t is called the maximization step of the EM algorithm\ufffd Dempster et al\ufffd \ufffd\u0001 \u0007\u0007\ufffd sho w ed that\ufffd under certain regularit y conditions\ufffd iteration of the ex\ufffd p ectation and maxim ization steps will con v erge to a lo cal maxim um \ufffd The EM algorithm is t ypically ap\ufffd plied when su\ufffdcien t statistics exist \ufffdi\ufffde\ufffd\ufffd when lo cal distribution functions are in the exp onen tial family\ufffd\ufffd although generalizations of the EM ha v e b een used for more complicated lo cal distributions \ufffdsee\ufffd e\ufffdg\ufffd\ufffd Saul et al\ufffd\ufffd \u0001  \u0006\ufffd\ufffd\n# \u0004\ufffd\u0002 Learning Structure\nA k ey step in the Ba y esian approac h to learning graph\ufffd ical mo dels is the computation of the marginal lik eli\ufffd ho o d of a data set giv en a mo del p\ufffdD jS \ufffd\ufffd Giv en a c om\ufffd plete data set\ufffdthat is a data set in whic h eac h sample con tains observ ations for ev ery v ariable in the mo del\ufffd the marginal lik eliho o d can b e computed exactly and e\ufffdcien tly under certain assumptions \ufffdCo op er and Her\ufffd sk o vits\ufffd \u0001  \u0002\ufffd\ufffd In con trast\ufffd when observ ations are missing\ufffd including situations where some v ariables are hidden or nev er observ ed\ufffd the exact determination of the marginal lik eliho o d is t ypically in tractable\ufffd Conse\ufffd quen tly \ufffd w e will use appro ximatio n tec hniques for com\ufffd puting the marginal lik eliho o d of exp onen tial causal in teraction mo dels\ufffd\nIn this section\ufffd w e fo cus atten tions on an asymptotic appro ximation called the Cheeseman\ufffdStutz appro xi\ufffd mation\ufffd whic h use in the sim ulation study describ ed in Section \u0005\ufffd It w as c hosen for the sim ulation study b e\ufffd cause of its computational and p erformance features\ufffd See Chic k ering and Hec k erman \ufffd\u0001  \u0006\ufffd for a discussion of other appro ximations and exp erimen tal results\ufffd\nWhen computing most asymptotic appro ximations\ufffd w e m ust determine the dimension of eac h of the mo del\ufffd The dimension of a mo del can b e in terpreted in t w o equiv alen t w a ys\ufffd First\ufffd it is the n um b er of free param\ufffd eters needed to represen t the parameter space near the maxim um lik eliho o d v alue\ufffd Second\ufffd it is the rank of the Jacobian matrix of the transformation b et w een the parameters of the net w ork and the parameters of the observ able \ufffdnon\ufffdhidden\ufffd v ariables\ufffd In either case\ufffd the dimension dep ends on the v alue of \ufffd \ufffd s space\ufffd In our sim ulation study w e use a mathematical soft w are pac k\ufffd age to calculate the rank of the Jacobian matrix of the transformation b et w een the parameters of the net w ork and the parameters of the observ able v ariables\ufffd F or more details and motiv ation see Geiger et al\ufffd \ufffd\u0001  \u0006\ufffd\ufffd No w w e turn our atten tion the the Cheeseman\ufffdStutz appro ximation \ufffd\u0001  \u0005\ufffd\ufffd Recall that in the EM algo\ufffd rithm w e treat exp ected su\ufffdcien t statistics as if they are actual su\ufffdcien t statistics\ufffd This use suggests an\nlog p\ufffdD jS \ufffd \ufffd log p\ufffdD \ufffd jS \ufffd\n\ufffd\u0005\ufffd\nwhere D \ufffd is an imaginary data set that is consisten t with the exp ected su\ufffdcien t statistics computed using an E step at a lo cal ML v alue for \ufffd s \ufffd\nEquation \u0005 has t w o desirable prop erties\ufffd One\ufffd b ecause it computes a marginal lik eliho o d\ufffd it punishes mo del complexit y \ufffd Tw o\ufffd b ecause D \ufffd is a complete \ufffdalb eit imaginary\ufffd data set\ufffd the computation of the criterion is e\ufffdcien t\ufffd\nOne problem with this scoring criterion is that it ma y not b e asymptotically correct\ufffd Consider the asymptot\ufffd ically correct\ufffd Ba y esian Information Criterion \ufffdBIC\ufffd \ufffdSc h w arz\ufffd \u0001 \u0007\b\ufffd Haugh ton\ufffd \u0001 \b\b\ufffd\nlog p\ufffdD \ufffd jS \ufffd \ufffd log p\ufffdD \ufffd j \ufffd \ufffd s \ufffd S \ufffd \ufffd d \ufffd \u0002 log N \ufffd O \ufffd\u0001\ufffd\nlog p\ufffdD \ufffd jS \ufffd \ufffd log p\ufffdD \ufffd j \ufffd \ufffd s \ufffd S \ufffd \ufffd d \ufffd \u0002 log N \ufffd O \ufffd\u0001\ufffd\nwhere d \ufffd is the dimension of the mo del S giv en data D \ufffd in the region around \ufffd \ufffd s \ufffdthat is\ufffd the n um b er of parameters of S \ufffd As N increases\ufffd the di\ufffderence b e\ufffd t w een p\ufffdD j \ufffd \ufffd s \ufffd S \ufffd and p\ufffdD \ufffd j \ufffd \ufffd s \ufffd S \ufffd ma y increase\ufffd Also\ufffd as w e ha v e discussed\ufffd it ma y b e that d \ufffd \ufffd d\ufffd In either case\ufffd Equation \u0005 will not b e asymptotically correct\ufffd A simple mo di\ufffdcation to Equation \u0005 addresses these problems\ufffd\nlog p\ufffdD jS \ufffd \ufffd log p\ufffdD \ufffd jS \ufffd\n\ufffd\u0006\ufffd\nEquation \u0005 \ufffdwithout the correction to dimension\ufffd w as \ufffdrst prop osed b y Cheeseman and Stutz \ufffd\u0001  \u0005\ufffd as a scoring criterion for AutoClass\ufffd an algorithm for data clustering\ufffd W e shall refer to Equation \u0006 as the Che eseman\ufffdStutz scoring criterion\ufffd W e note that the scoring crireria giv en in Equation \u0005 and Equation \u0006 can b e applied if one can compute the marginal lik e\ufffd liho o d of complete data giv en the mo del and obtain a MAP estimate\ufffd Bun tine \ufffd\u0001  \u0004\ufffd sho ws ho w to com\ufffd pute the marginal lik eliho o d for complete data giv en a D A G mo del in whic h the lo cal lik eliho o ds are from the exp onen tial family and w e will use the EM algorithm to obtain a MAP estimate\ufffd\n# \u0005 Sim ulation Study\nIn this section w e describ e a small sim ulation study whic h highligh ts some of the imp ortan t features of the approac h that w e describ ed in Section \u0004\ufffd The struc\ufffd ture of the \ufffdv e mo dels that w e used in the sim ulation study are giv en in Figure \u0004\ufffd All of the v ariables are\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2691/2691cefc-a1e1-48db-9d7c-85167d1eb148.png\" style=\"width: 50%;\"></div>\nFigure \u0004\ufffd Noisy\ufffdMax\ufffdIn teraction mo dels used in sim\ufffd ulation study \ufffd\nbinary and the conditional distributions of the X i \ufffds giv en the C i \ufffds are complete probabilit y tables\ufffd Mo del F\u0005 could also b e represen ted without a deterministic com bination function as a complete probabilit y table of E giv en the C i \ufffds\ufffd F or eac h mo del w e c hose parameter v alues for the pa\ufffd rameters and then used the parameterized mo del as the generating mo del to generate a dataset of \u0006\u0004\ufffd\ufffd cases\ufffd P arameter v alues w ere c hosen b y hand\ufffd ho w\ufffd ev er\ufffd similar results w ould b e exp ected for parameters c hosen at random\ufffd W e appro ximated the mo del p os\ufffd teriors using the adjusted Cheeseman\ufffdStutz score for di\ufffderen t sized initial segmen ts of the \u0006\u0004\ufffd\ufffd cases\ufffd The dimension of the mo dels is calculated using Mathe\ufffd matica and the tec hniques describ ed b y Geiger et al\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Although not done for our study \ufffd it is easy to automatically generate the equations for Mathemat\ufffd ica to calculate the dimension and th us automate the calculation of dimension\ufffd The results of the sim ula\ufffd tion study are summarized in Figure \u0005\ufffd Mo del p oste\ufffd riors are presen ted only for initial segmen ts of size \u0001\ufffd\ufffd\ufffd \u0002\ufffd\ufffd\ufffd \u0004\ufffd\ufffd\ufffd \b\ufffd\ufffd\ufffd and \u0001\u0006\ufffd\ufffd cases\ufffd Not surprisingly \ufffd mass con tin ues to accum ulate on the generating mo del as the sample size increases\ufffd The one exception is when mo del F\u0001 is the generating mo del\ufffd The reason for the b eha vior of the p osterior when F\u0001 is the generating mo del is that the set of distributions that can b e pa\ufffd rameterized b y F\u0001 is a strict subset of the distributions that can b e parameterized b y F\u0002 and\ufffd surprisingly \ufffd the dimension of the t w o mo dels is iden tical\ufffd This un usual relationship b et w een F\u0001 and F\u0002 only o ccurs only when the C i \ufffds and E are binary \ufffd Finally \ufffd w e w ould lik e to dra w atten tion to the imp or\ufffd tance of using the correct dimension when calculating the Ba y esian appro ximation to the p osterior\ufffd The un\ufffd adjuste d dimension of a D A G mo del is the n um b er of parameters in the mo del\ufffd including the parameters for the hidden v ariables\ufffd T able \u0001 describ es the dimension of eac h of the mo dels used in the sim ulation study \ufffd Consider mo dels F\u0004 and F\u0005\ufffd Clearly ev ery distribu\ufffd tion o v er the C i \ufffds and E that can b e represen ted in F\u0004 can b e represen ted in F\u0005\ufffd If our asymptotic ap\ufffd pro ximation used the unadjusted dimension then\ufffd at\nMo\ndel\nF\u0001\nF\u0002\nF\u0003\nF\u0004\nF\u0005\nDimension\n\u0007\n\u0007\n \n\u0001\ufffd\n\u0001\u0001\nUnadjusted\ndim\ufffd\n \n \n\u0001\u0001\n\u0001\u0005\n\u0001\u0001\nT able \u0001\ufffd The dimension of the NMI mo dels\ufffd\nmo del F\u0004 o v er mo del F\u0005 when F\u0004 is the generating mo del\ufffd Using the correct p enalty for the dimension is also imp ortan t for other approac hes suc h as MDL\ufffd\n# \u0006 Related and F uture W ork\nThere has b e little w ork done on parameter learning for causal in teraction mo dels\ufffd The notable exception is the w ork of Neal \ufffd\u0001  \u0002\ufffd\ufffd Neal sho w ed that one could learn the parameters of a noisy\ufffdor net w ork using a lo cal learning rule\ufffd Ho w ev er\ufffd his particular gradien t\ufffdascen t pro cedure m ust b e constrained to a v oid en tering an in v alid region of the parameter space\ufffd Since w e are using EM w e are guaran teed to sta y within the v alid region of the parameter space and guaran teed to \ufffdnd a lo cal maxim um \ufffd W e plan on in v estigating the represen tational p o w er of causal in teraction mo dels as compared to other lo cal structures\ufffd e\ufffdg\ufffd\ufffd decision graphs and compare the ease of assessmen t for v arious mo dels In addition\ufffd w e will consider automating the learning of causal in teraction mo dels \ufffdi\ufffde\ufffd\ufffd de\ufffdning a searc h space\ufffd and searc h op er\ufffd ators\ufffd\ufffd and compare the result of suc h an algorithm to other approac hes for learning lo cal structure\ufffd Also of in terest\ufffd is ho w to b est com bine a searc h for lo cal structure with a searc h for global structure\ufffd\nThere has b e little w ork done on parameter learning for causal in teraction mo dels\ufffd The notable exception is the w ork of Neal \ufffd\u0001  \u0002\ufffd\ufffd Neal sho w ed that one could learn the parameters of a noisy\ufffdor net w ork using a lo cal learning rule\ufffd Ho w ev er\ufffd his particular gradien t\ufffdascen t pro cedure m ust b e constrained to a v oid en tering an in v alid region of the parameter space\ufffd Since w e are using EM w e are guaran teed to sta y within the v alid region of the parameter space and guaran teed to \ufffdnd a lo cal maxim um \ufffd\nW e plan on in v estigating the represen tational p o w er of causal in teraction mo dels as compared to other lo cal structures\ufffd e\ufffdg\ufffd\ufffd decision graphs and compare the ease of assessmen t for v arious mo dels In addition\ufffd w e will consider automating the learning of causal in teraction mo dels \ufffdi\ufffde\ufffd\ufffd de\ufffdning a searc h space\ufffd and searc h op er\ufffd ators\ufffd\ufffd and compare the result of suc h an algorithm to other approac hes for learning lo cal structure\ufffd Also of in terest\ufffd is ho w to b est com bine a searc h for lo cal structure with a searc h for global structure\ufffd\n# Ac kno wledgmen ts\nW e thank Bo Thiesson\ufffd Jac k Breese\ufffd and Max Chic k\ufffd ering for helpful discussion on this material\ufffd\n# References\nBouc k aert\ufffd R\ufffd \ufffd\u0001  \u0005\ufffd\ufffd Bayesian b elief networks\ufffd fr om c onstruction to infer enc e\ufffd PhD thesis\ufffd Univ ersit y Utrec h t\ufffd Bun tine\ufffd W\ufffd \ufffd\u0001  \u0004\ufffd\ufffd Op erations for learning with graphical mo dels\ufffd Journal of A rti\ufffdcial Intel li\ufffd genc e R ese ar ch\ufffd \u0002\ufffd\u0001\u0005 \ufffd\u0002\u0002\u0005\ufffd Cheeseman\ufffd P \ufffd and Stutz\ufffd J\ufffd \ufffd\u0001  \u0005\ufffd\ufffd Ba y esian classi\ufffdcation \ufffdAutoClass\ufffd\ufffd Theory and results\ufffd In F a yy ad\ufffd U\ufffd\ufffd Piatesky\ufffdShapiro\ufffd G\ufffd\ufffd Sm yth\ufffd P \ufffd\ufffd and Uth urusam y \ufffd R\ufffd\ufffd editors\ufffd A dvanc es in Know le dge Disc overy and Data Mining\ufffd pages \u0001\u0005\u0003\ufffd\u0001\b\u0001\ufffd AAAI Press\ufffd Menlo P ark\ufffd CA\ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/18ff/18ff69c5-6d93-40d6-aa6f-d7b71ca471a9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure \u0005\ufffd Sim ulation study results\ufffd</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/134e/134e9b37-9fce-43e9-9d04-d9842bb551e9.png\" style=\"width: 50%;\"></div>\nChic k ering\ufffd D\ufffd and Hec k erman\ufffd D\ufffd \ufffd\u0001  \u0006\ufffd\ufffd E\ufffdcien t appro ximations for the marginal lik eliho o d of in\ufffd complete data giv en a Ba y esian net w ork\ufffd T ec hni\ufffd cal Rep ort MSR\ufffdTR\ufffd \u0006\ufffd\ufffd\b\ufffd Microsoft Researc h\ufffd Redmond\ufffd W A\ufffd Chic k ering\ufffd D\ufffd\ufffd Hec k erman\ufffd D\ufffd\ufffd and Meek\ufffd C\ufffd \ufffd\u0001  \u0007\ufffd\ufffd A Ba y esian approac h to learning Ba y esian net\ufffd w orks with lo cal structure\ufffd In this volume\ufffd Co op er\ufffd G\ufffd and Hersk o vits\ufffd E\ufffd \ufffd\u0001  \u0002\ufffd\ufffd A Ba y esian metho d for the induction of probabilistic net\ufffd w orks from data\ufffd Machine L e arning\ufffd  \ufffd\u0003\ufffd \ufffd\u0003\u0004\u0007\ufffd Co w ell\ufffd R\ufffd\ufffd Da wid\ufffd A\ufffd\ufffd and Sebastiani\ufffd P \ufffd \ufffd\u0001  \u0005\ufffd\ufffd A comparison of sequen tial learning metho ds for incomplete data\ufffd T ec hnical Rep ort \u0001\u0003\u0005\ufffd Depart\ufffd men t of Statistical Science\ufffd Univ ersit y College\ufffd London\ufffd Dempster\ufffd A\ufffd\ufffd Laird\ufffd N\ufffd\ufffd and Rubin\ufffd D\ufffd \ufffd\u0001 \u0007\u0007\ufffd\ufffd Max\ufffd im um lik eliho o d from incomplete data via the EM algorithm\ufffd Journal of the R oyal Statistic al So ciety\ufffd B \u0003 \ufffd\u0001\ufffd\u0003\b\ufffd F riedman\ufffd N\ufffd and Goldszmidt\ufffd M\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Learning ba y esian net w orks with lo cal structure\ufffd In Pr o\ufffd c e e dings of Twelfth Confer enc e on Unc ertainty in\nA rti\ufffdcial Intel ligenc e\ufffd P ortland\ufffd OR\ufffd pages \u0002\u0005\u0002\ufffd \u0002\u0006\u0002\ufffd Morgan Kaufmann\ufffd Geiger\ufffd D\ufffd\ufffd Hec k erman\ufffd D\ufffd\ufffd and Meek\ufffd C\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Asymptotic mo del selection for directed net\ufffd w orks with hidden v ariables\ufffd In Pr o c e e dings of Twelth Confer enc e on Unc ertainty in A rti\ufffdcial Intel ligenc e\ufffd P ortland\ufffd OR\ufffd Morgan Kaufmann\ufffd Go o d\ufffd I\ufffd \ufffd\u0001 \u0006\u0001\ufffd\ufffd A causal calculus \ufffdi\ufffd\ufffd British Journal of Philosophy of Scienc e\ufffd \u0001\u0001\ufffd\u0003\ufffd\u0005\ufffd\u0003\u0001\b\ufffd Haugh ton\ufffd D\ufffd \ufffd\u0001 \b\b\ufffd\ufffd On the c hoice of a mo del to \ufffdt data from an exp onen tial family \ufffd A nnals of Statistics\ufffd \u0001\u0006\ufffd\u0003\u0004\u0002\ufffd\u0003\u0005\u0005\ufffd Hec k erman\ufffd D\ufffd \ufffd\u0001  \u0005\ufffd\ufffd A tutorial on learning Ba y esian net w orks\ufffd T ec hnical Rep ort MSR\ufffdTR\ufffd \u0005\ufffd\ufffd\u0006\ufffd Mi\ufffd crosoft Researc h\ufffd Redmond\ufffd W A\ufffd Revised Jan\ufffd uary \ufffd \u0001  \u0006\ufffd Hec k erman\ufffd D\ufffd and Breese\ufffd J\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Causal indep en\ufffd dence for probabilit y assessmen t and inference using Ba y esian net w orks\ufffd IEEE\ufffd Systems\ufffd Man\ufffd and Cyb ernetics\ufffd \u0002\u0006\ufffd\b\u0002\u0006\ufffd\b\u0003\u0001\ufffd Henrion\ufffd M\ufffd \ufffd\u0001 \b\u0007\ufffd\ufffd Some practical issues in con\ufffd structing b elief net w orks\ufffd In Pr o c e e dings of the\nHenrion\ufffd M\ufffd \ufffd\u0001 \b\u0007\ufffd\ufffd Some practical issues in con\ufffd structing b elief net w orks\ufffd In Pr o c e e dings of the\nThir d Workshop on Unc ertainty in A rti\ufffdcial In\ufffd tel ligenc e\ufffd Seattle\ufffd W A\ufffd pages \u0001\u0003\u0002\ufffd\u0001\u0003 \ufffd Asso ci\ufffd ation for Uncertain t y in Arti\ufffdcial In telligence\ufffd Moun tain View\ufffd CA\ufffd Jaakk ola\ufffd T\ufffd and Jordan\ufffd M\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Computing upp er and lo w er b ounds on lik eliho o ds in in tractable net w orks\ufffd In Pr o c e e dings of Twelth Confer enc e on Unc ertainty in A rti\ufffdcial Intel ligenc e\ufffd P ort\ufffd land\ufffd OR\ufffd pages \u0003\u0004\ufffd\ufffd\u0003\u0004\b\ufffd Morgan Kaufmann\ufffd Kass\ufffd R\ufffd\ufffd Tierney \ufffd L\ufffd\ufffd and Kadane\ufffd J\ufffd \ufffd\u0001 \b\b\ufffd\ufffd Asymp\ufffd totics in Ba y esian computation\ufffd In Bernardo\ufffd J\ufffd\ufffd DeGro ot\ufffd M\ufffd\ufffd Lindley \ufffd D\ufffd\ufffd and Smith\ufffd A\ufffd\ufffd edi\ufffd tors\ufffd Bayesian Statistics \u0003\ufffd pages \u0002\u0006\u0001\ufffd\u0002\u0007\b\ufffd Ox\ufffd ford Univ ersit y Press\ufffd Kim\ufffd J\ufffd and P earl\ufffd J\ufffd \ufffd\u0001 \b\u0003\ufffd\ufffd A computational mo del for causal and diagnostic reasoning in inference engines\ufffd In Pr o c e e dings Eighth International Joint Confer enc e on A rti\ufffdcial Intel ligenc e\ufffd Karl\ufffd sruhe\ufffd W est German y \ufffd pages \u0001 \ufffd\ufffd\u0001 \u0003\ufffd In terna\ufffd tional Join t Conference on Arti\ufffdcial In telligence\ufffd Madigan\ufffd D\ufffd and Raftery \ufffd A\ufffd \ufffd\u0001  \u0004\ufffd\ufffd Mo del selection and accoun ting for mo del uncertain t y in graphi\ufffd cal mo dels using Occam\ufffds windo w\ufffd Journal of the A meric an Statistic al Asso ciation\ufffd \b \ufffd\u0001\u0005\u0003\u0005\ufffd\u0001\u0005\u0004\u0006 \ufffd Neal\ufffd R\ufffd \ufffd\u0001  \u0003\ufffd\ufffd Probabilistic inference using Mark o v c hain Mon te Carlo metho ds\ufffd T ec hnical Rep ort CR G\ufffdTR\ufffd \u0003\ufffd\u0001\ufffd Departmen t of Computer Sci\ufffd ence\ufffd Univ ersit y of T oron to\ufffd\nP earl\ufffd J\ufffd \ufffd\u0001 \b\b\ufffd\ufffd Pr ob abilistic R e asoning in Intel ligent systems\ufffd Morgan\ufffdKaufmann\ufffd\nRussell\ufffd S\ufffd\ufffd Binder\ufffd J\ufffd\ufffd Koller\ufffd D\ufffd\ufffd and Kanaza w a\ufffd K\ufffd \ufffd\u0001  \u0005\ufffd\ufffd Lo cal learning in probabilistic net\ufffd w orks with hidden v ariables\ufffd In Pr o c e e dings of the F ourte enth International Joint Confer enc e on A rti\ufffdcial Intel ligenc e\ufffd Mon treal\ufffd QU\ufffd pages \u0001\u0001\u0004\u0006\ufffd\u0001\u0001\u0005\u0002\ufffd Morgan Kaufmann\ufffd San Mateo\ufffd CA\ufffd Saul\ufffd L\ufffd\ufffd Jaakk ola\ufffd T\ufffd\ufffd and Jordan\ufffd M\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Mean \ufffdeld theory for sigmoid b elief net w orks\ufffd Journal of A rti\ufffdcial Intel ligenc e R ese ar ch\ufffd \u0004\ufffd\u0006\u0001\ufffd\u0007\u0006\ufffd\nSc h w arz\ufffd G\ufffd \ufffd\u0001 \u0007\b\ufffd\ufffd Estimating the dimension of a mo del\ufffd A nnals of Statistics\ufffd \u0006\ufffd\u0004\u0006\u0001\ufffd\u0004\u0006\u0004\ufffd\nSpiegelhalter\ufffd D\ufffd and Lauritzen\ufffd S\ufffd \ufffd\u0001  \ufffd\ufffd\ufffd Sequen tial up dating of conditional probabilities on directed graphical structures\ufffd Networks\ufffd \u0002\ufffd\ufffd\u0005\u0007 \ufffd\u0006\ufffd\u0005\ufffd\nSriniv as\ufffd S\ufffd \ufffd\u0001  \u0003\ufffd\ufffd A generalization of the noisy\ufffdOr mo del\ufffd In Pr o c e e dings of Ninth Confer enc e on Unc ertainty in A rti\ufffdcial Intel ligenc e\ufffd W ashing\ufffd ton\ufffd DC\ufffd pages \u0002\ufffd\b\ufffd\u0002\u0001\u0005\ufffd Morgan Kaufmann\ufffd\nThiesson\ufffd B\ufffd \ufffd\u0001  \u0005\ufffd\ufffd Score and information for recur\ufffd siv e exp onen tial mo dels with incomplete data\ufffd T ec hnical rep ort\ufffd Institute of Electronic Sys\ufffd tems\ufffd Aalb org Univ ersit y \ufffd Aalb org\ufffd Denmark\ufffd Xiang\ufffd Y\ufffd\ufffd W ong\ufffd S\ufffd\ufffd and Cercone\ufffd N\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Critical remarks on single link searc h in learning b elief net w orks\ufffd In Pr o c e e dings of Twelfth Confer enc e on Unc ertainty in A rti\ufffdcial Intel ligenc e\ufffd P ort\ufffd land\ufffd OR\ufffd pages \u0005\u0006\u0004\ufffd\u0005\u0007\u0001\ufffd Morgan Kaufmann\ufffd Zhang\ufffd N\ufffd and P o ole\ufffd D\ufffd \ufffd\u0001  \u0006\ufffd\ufffd Exploiting causal in\ufffd dep endence in Ba y esian net w ork inference\ufffd Jour\ufffd nal of A rti\ufffdcial Intel ligenc e R ese ar ch\ufffd \u0005\ufffd\u0003\ufffd\u0001\ufffd\u0003\u0002\b\ufffd\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper discusses causal independence models and generalizes these models to causal interaction models. Causal interaction models involve independent mechanisms where mechanisms can have several causes. The paper highlights the limitations of existing methods and the need for new approaches to effectively model these interactions.",
        "problem": {
            "definition": "The problem addressed is the inability of existing causal independence models to accurately represent scenarios where multiple causes interact to produce an effect, necessitating a more flexible modeling framework.",
            "key obstacle": "The core obstacle is the exponential growth of parameters required for full probabilistic representations in existing models, which makes it impractical to learn and infer causal relationships."
        },
        "idea": {
            "intuition": "The proposed idea stems from the observation that real-world causal relationships often involve interactions between multiple causes, which are not adequately captured by traditional causal independence models.",
            "opinion": "The proposed causal interaction models allow for the representation of mechanisms that can share causes and interact, providing a more nuanced understanding of causal relationships.",
            "innovation": "The primary innovation is the introduction of causal interaction models that generalize both causal independence and complete causal interaction models, allowing for partial interactions among causes."
        },
        "method": {
            "method name": "Causal Interaction Models",
            "method abbreviation": "CIM",
            "method definition": "Causal interaction models are defined as directed acyclic graphical models that represent the conditional distribution of an effect given multiple interacting causes, allowing for shared mechanisms.",
            "method description": "The core of the method involves using directed acyclic graphs to represent complex causal relationships among multiple variables.",
            "method steps": "1. Define the causes and effects in a directed acyclic graph. 2. Specify the mechanisms associated with each cause. 3. Use Bayesian inference to learn the parameters and structure of the model.",
            "principle": "The effectiveness of this method lies in its ability to represent and learn from the interactions between multiple causes, allowing for a more accurate modeling of real-world phenomena."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved simulating various causal interaction models to assess their performance in learning from data. The models were evaluated against baseline methods using generated datasets.",
            "evaluation method": "The evaluation involved comparing the learned models' posterior distributions using the Cheeseman-Stutz approximation for different sample sizes, assessing how well the models captured the underlying causal structures."
        },
        "conclusion": "The experiments demonstrated that causal interaction models can effectively learn and represent complex causal relationships, showing improvements over traditional models in terms of flexibility and accuracy.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include its ability to model complex interactions among multiple causes and its efficiency in learning from incomplete data.",
            "limitation": "A limitation of the method is the computational complexity associated with learning the structure and parameters in models with many interacting variables.",
            "future work": "Future work will focus on automating the learning process for causal interaction models, exploring their representational power compared to other local structures, and improving inference methods."
        },
        "other info": {
            "acknowledgments": "The authors thank Bo Thiesson, Jack Breese, and Max Chickering for helpful discussions on this material."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "Causal independence models are defined as models that fail to accurately represent scenarios where multiple causes interact to produce an effect."
        },
        {
            "section number": "2.2",
            "key information": "Causal interaction models generalize the concept of causal independence by allowing for independent mechanisms where mechanisms can have several causes."
        },
        {
            "section number": "3.4",
            "key information": "The proposed causal interaction models are directed acyclic graphical models that represent the conditional distribution of an effect given multiple interacting causes, allowing for shared mechanisms."
        },
        {
            "section number": "4.1",
            "key information": "The core obstacle in achieving model transparency with existing causal independence models is the exponential growth of parameters required for full probabilistic representations."
        },
        {
            "section number": "4.3",
            "key information": "The limitations of existing methods highlight the trade-off between interpretability and performance, as traditional models struggle to accurately capture complex causal relationships."
        },
        {
            "section number": "6.2",
            "key information": "Future work will focus on automating the learning process for causal interaction models, which could enhance their representational power and applicability in interpretable AI."
        }
    ],
    "similarity_score": 0.5495035504343233,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0115_inter/papers/Structure and Parameter Learning for Causal Independence and Causal Interaction Models.json"
}