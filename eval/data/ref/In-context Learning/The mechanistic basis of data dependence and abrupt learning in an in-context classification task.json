{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.03002",
    "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
    "abstract": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence. In-context learning contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.",
    "bib_name": "reddy2023mechanisticbasisdatadependence",
    "md_text": "T HE MECHANISTIC BASIS OF DATA DEPENDENCE AND ABRUPT LEARNING IN AN IN CONTEXT CLASSIFICATION TASK\n\nGautam Reddy Physics & Informatics Labs, NTT Research Inc. Center for Brain Science, Harvard University Department of Physics, Princeton University greddy@princeton.edu\n\n# A BSTRACT\n\nTransformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence. In-context learning contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.\n\n\n# 1 Introduction\n\nA striking feature of large language models is in-context learning (Brown et al., 2020; Dong et al., 2022; Garg et al., 2022; Dai et al., 2022). In-context learning (ICL) is the ability to predict the response to a query based on illustrative examples presented in the context, without any additional weight updates. This form of learning contrasts with in-weights learning (IWL) of query-response relationships encoded in the weights of the network. ICL emerges in transformer models (Vaswani et al., 2017) trained on a diverse set of tasks that contain a common structural element. ICL can be exploited to perform zero-shot learning on novel tasks that share this structure. For example, a transformer trained to solve numerous linear regression tasks learns to solve a new linear regression task based on in-context examples (Garg et al., 2022; Aky\u00fcrek et al., 2022; Von Oswald et al., 2023; Ahn et al., 2023). Specifically, given a sequence of sample input-output pairs, the predictive error on a target query is comparable to an optimal Bayes predictor (Ahuja et al., 2023; Xie et al., 2021; Li et al., 2023). This remarkable feature extends to other generative models such as hierarchical regression models that involve model selection (Bai et al., 2023), random permutations of images (Kirsch et al., 2022) and mixture models over sequential data (Wang et al., 2023; Xie et al., 2021).\nTransformer models trained on language data exhibit another simple yet powerful form of in-context learning. Given a sequence. . . x, y, . . . , x,? for x, y pairs unseen during training (for example, tokens belonging to a novel proper noun), these models learn the ability to predict y (Olsson et al., 2022). In other words, the model learns empirical bigram statistics on-the-fly, thus displaying a primitive form of zero-shot associative learning. Past work has shown that this computation involves an induction head (discussed in detail further below) and that a minimal implementation requires a two-layer attention-only network (Olsson et al., 2022). Across networks of different scales and task structures, the ability to perform ICL often increases abruptly during training (Olsson et al., 2022). The mechanistic basis of the abrupt\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/542f/542f1b1c-0972-4c4f-b0df-d3e2007ea5ff.png\" style=\"width: 50%;\"></div>\nFigure 1: (a) Input sequences consist of N item-label pairs followed by a target. Items are drawn from K classes assigned to L \u2264 K labels. At least one item belongs to the same class as the target. The network is tasked to predict the label of the target. The number of classes (K), their rank-frequency distribution (\u03b1), within-class variability (\u03b5) and the number of items from a single class in an input sequence (B) parameterize the data distribution. (b) IWL is measured using input sequences where the items\u2019 and target\u2019s classes are randomly sampled. ICL is measured using items and targets from novel classes and by swapping the label of an existing class in the context. (c) Network architecture. (d) Loss and accuracy curves for six seeds (dark lines show averages over the seeds). Here, B = 2, K = 512.\ntransition remains unclear. Notably, this abrupt transition is often preceded by the formation of induction heads in intermediate layers of the network, suggesting that induction head formation may provide a scaffold for the development of more complex in-context computations. Other work provides empirical evidence that ICL is the key driver behind the emergent abilities of large language models (Lu et al., 2023). Thus, elucidating the mechanisms that underpin ICL, and induction heads in particular, may provide crucial insights into the data distributional and architectural factors that lead to emergent zero-shot learning.\nA recent empirical study has highlighted key data distributional properties pertinent to language that promote ICL in a hybrid in-context/in-weights classification task (Chan et al., 2022). In this setup, a 12-layer transformer network is trained to predict the class label of a target item given a sequence of N item-label pairs in the context. The item classes are drawn from Omniglot (Lake et al., 2019), a standard image-label dataset. By manipulating the distribution of classes shown during training, various data distributional properties that influence the ICL vs IWL trade-off were identified. This setup offers a well-controlled paradigm for identifying the factors that enable attention-based models to learn in-context learning solutions without explicitly trained to do so.\nOur main contributions are as follows. We first show that the data dependencies highlighted in Chan et al. (2022) are recapitulated in a task with simplified input statistics and a two-layer attention-only network architecture. By identifying progress measures and designing careful experiments, we show that ICL is driven by the abrupt formation of an induction head. We construct a minimal two-parameter model of an induction head stacked with a deep classifier, which reproduces all data distributional dependencies and captures the dynamics of learning. Finally, we develop a phenomenological model of an induction head\u2019s loss landscape. This analysis enables us to trace the abrupt learning phenomenon to cliffs in the landscape created by nested nonlinearities in a multi-layer attention-based network.\n\ntransition remains unclear. Notably, this abrupt transition is often preceded by the formation of induction heads in intermediate layers of the network, suggesting that induction head formation may provide a scaffold for the development of more complex in-context computations. Other work provides empirical evidence that ICL is the key driver behind the emergent abilities of large language models (Lu et al., 2023). Thus, elucidating the mechanisms that underpin ICL, and induction heads in particular, may provide crucial insights into the data distributional and architectural factors that lead to emergent zero-shot learning.\nA recent empirical study has highlighted key data distributional properties pertinent to language that promote ICL in a hybrid in-context/in-weights classification task (Chan et al., 2022). In this setup, a 12-layer transformer network is trained to predict the class label of a target item given a sequence of N item-label pairs in the context. The item classes are drawn from Omniglot (Lake et al., 2019), a standard image-label dataset. By manipulating the distribution of classes shown during training, various data distributional properties that influence the ICL vs IWL trade-off were identified. This setup offers a well-controlled paradigm for identifying the factors that enable attention-based models to learn in-context learning solutions without explicitly trained to do so.\nOur main contributions are as follows. We first show that the data dependencies highlighted in Chan et al. (2022) are recapitulated in a task with simplified input statistics and a two-layer attention-only network architecture. By identifying progress measures and designing careful experiments, we show that ICL is driven by the abrupt formation of an induction head. We construct a minimal two-parameter model of an induction head stacked with a deep classifier, which reproduces all data distributional dependencies and captures the dynamics of learning. Finally, we develop a phenomenological model of an induction head\u2019s loss landscape. This analysis enables us to trace the abrupt learning phenomenon to cliffs in the landscape created by nested nonlinearities in a multi-layer attention-based network.\n\n# 2 Task and network architecture\n\nTask structure. The task structure is based on a common ICL formulation. The network is trained to predict the label of a target x q given an alternating sequence of N items and N labels: x 1, \u2113 1, x 2, \u2113 2, . . . , x N, \u2113 N, x q,? (Figure 1a). We embed the items and labels in P + D dimensions. The first P dimensions encode positional information and the latter D dimensions encode content. Position is encoded by a one-hot P-dimensional vector (we use P = 65 throughout). The input sequence occupies a random window of length 2 N + 1 between 0 and P \u2212 1. This choice of positional encoding biases the network to learn a translation-invariant computation.\nThe items are sampled from a gaussian mixture model with K classes. Each class k is defined by a D-dimensional vector \u00b5 k whose components are sampled i.i.d from a normal distribution with mean zero and variance 1 /D. The\n\nwhere \u03b7 is drawn from the same distribution as the \u00b5 k \u2019s and \u03b5 sets the within-class variability. The re-scaling with \u221a\n1 + \u03b5 2 ensures that || \u02dc x i || \u2248 1. Each class is assigned to one of L labels (L \u2264 K). The contents of the labels are drawn prior to training from the same distribution as the \u00b5 k \u2019s. Each label in an input sequence appears the same number of times as every other label in that sequence.\nImportantly, at least one item in the context belongs to the target\u2019s class. The network is trained to classify the target x q into one of the L labels using a cross-entropy loss. The network can thus achieve zero loss by either learning to classify targets from the K classes as in a standard in-weights classification task (IWL), or by learning a more general in-context solution (ICL) that uses the exemplar(s) presented in the context.\nParameterizing the data distribution. The input data distribution is modulated by tuning various parameters in addition to K and \u03b5. The burstiness B is the number of occurrences of items from a particular class in an input sequence (N is a multiple of B). p B is the fraction of bursty sequences. Specifically, the burstiness is B for a fraction p B of the training data. The classes (including the target) are sampled i.i.d for the remaining fraction 1 \u2212 p B. The rank-frequency distribution over the classes is f (k) \u223c k \u2212 \u03b1. We use L = 32, N = 8, D = 63, \u03b5 = 0. 1, \u03b1 = 0 unless otherwise specified.\nMetrics for tracking in-context and in-weights learning. To track IWL, we measure the prediction accuracy on input sequences. The target and item classes are sampled independently from the rank-frequency distribution used during training (Figure 1b). Since K \u226b N in our experiments, it is unlikely that the target\u2019s class appears in the context. The network therefore has to rely on IWL to correctly predict the target\u2019s class label.\nThe primary metric for tracking ICL is the prediction accuracy on input sequences where the target and items belong to novel classes (the \u00b5 k \u2019s are drawn anew). The novel classes are randomly assigned one of the existing L labels (Figure 1b). B copies of the target (within variability \u03b5) are included in the context. Since the classes are novel, the network has to rely on ICL for accurate prediction. We introduce a secondary metric for tracking ICL using input sequences where the items\u2019 labels are different from those presented during training. We measure the accuracy of the network on predicting the target\u2019s swapped label. That is, the network has to rely on ICL rather than IWL.\nNetwork architecture. The inputs are passed through a two-layer attention-only network followed by a classifier. Each attention layer has one attention head with a causal mask. Given a sequence of inputs u 1, u 2, . . . , u n, the outputs of the first (v i) and second (w i) layers are\n\n\ufffd\nis the attention paid by query i on key j in the \u00b5 th layer. Q \u00b5, K \u00b5, V \u00b5 are the query, key and value matrices, respectively. The classifier receives w n as input.\nThe classifier is a three-layer MLP with ReLU activations and a softmax layer which predicts the probabilities of the L labels. We use a deep classifier to ensure perfect IWL is feasible. At least three layers were necessary to achieve perfect classification accuracy for the parameter ranges considered in this paper (since K \u226b L). The query/key dimension and the MLP hidden layer dimension are both 128. We repeat every experiment with six seeds (with random initializations and training/test sets). For training, we use a batch size of 128 and vanilla SGD with learning rate 0.01. Figure 1d shows sample loss and accuracy curves, including the measures used to track IWL and ICL.\n\n# 3 Results\n\ncapitulating data distributional dependencies. In Figure 2, we quantify how IWL and ICL depend on the rameters of the data distribution. The upshot is that the highly simplified input statistics and network architecture nsidered here reproduce the core distributional dependencies observed in past work. The results are summarized ow.\n\nRecapitulating data distributional dependencies. In Figure 2, we quantify how IWL and ICL depend on the parameters of the data distribution. The upshot is that the highly simplified input statistics and network architecture considered here reproduce the core distributional dependencies observed in past work. The results are summarized below.\nIncreasing the burstiness B and the number of classes K promotes ICL while decreasing IWL (Figure 2a), highlighting the trade-off between ICL and IWL. Recall that the target and item classes are randomly sampled when B = 0. This\n\nIncreasing the burstiness B and the number of classes K promotes ICL while decreasing IWL (Figure 2a), highlighti the trade-off between ICL and IWL. Recall that the target and item classes are randomly sampled when B = 0. T\n\n(1)\n\n(2)\n\n(3)\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1fbc/1fbc1a75-56b7-44ff-a145-fc4ebf31dd3f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: In-weights (top row) and in-context accuracy (bottom row) against the number of classes (K), bur (B), within-class variability (\u03b5) and the exponent of the rank-frequency distribution (\u03b1). Here K = 1024, \u03b1 = 1, \u03b5 = 0. 1 except when that parameter is varied.\n</div>\nimplies that the network can indeed learn a perfect IWL solution for the corresponding K. Similarly, within-class variation (\u03b5) promotes ICL and decreases IWL (Figure 2b). We find that the network always converges to an IWL solution when the fraction of bursty sequences p B <1 (results not shown). This is expected as the ICL solution is not a global minimum when p B <1.\nA striking result is that a Zipfian rank-frequency distribution (\u03b1 = 1) overcomes the trade-off between IWL and ICL, and promotes both forms of learning. This is recapitulated in our experiments (Figure 2c). Note, however, that while the network learns the IWL solution for the most common classes, it does not learn the less frequent classes even for \u03b1 = 1.\nMoreover, we find that the network can support both ICL and IWL simultaneously. To show this, we train the network on IC sequences, where the items are all drawn from novel classes randomly assigned to one of the L labels. The parameter p C is the fraction of the training data containing IC sequences. The remaining fraction of the training data is drawn as described previously. When 0 < p C <1 and 0 \u2264 p B <1, the network can only achieve zero loss if it learns both the in-context and in-weights solutions. Figure A.1 shows that the network is capable of learning both solutions simultaneously.\nOne potential explanation for the results in Figure 2 and Figure A.1 is that the network independently learns the in-weights and in-context solutions at different rates until it achieves zero loss. The relative rates at which the network achieves ICL and IWL will then determine the fraction of loss explained by each mechanism after convergence to zero loss. The rates of ICL and IWL depend on K, \u03b5 and B. Specifically, increasing K and \u03b5 decreases the rate of IWL (as the classification task is harder) whereas increasing B increases the rate of ICL (as there are more demonstrations in the context). The Zipfian case of \u03b1 = 1 further highlights the dynamic balance between ICL and IWL. Frequent occurrences of common classes allow the network to learn to classify them using IWL. On the other hand, the large number of rare classes promotes learning of a more general in-context solution. Once the in-context solution is learned, IWL freezes as the network incurs near-zero loss on all classes. When \u03b1 > 1, the tail of the rank-frequency distribution falls off rapidly and the rare classes do not contribute sufficiently to the loss to promote ICL. Conversely, when \u03b1 <1, the network learns the in-context mechanism if K is large enough such that IWL takes longer than ICL (see Figure 2a for \u03b1 = 0 and varying K).\nAttention maps and progress measures. We now examine the dynamics of ICL. We henceforth set p C> 0, p B = 1 as the IC sequences promote rapid convergence to the in-context solution and allow for more experiments. Figure 3a shows the IC accuracy, which displays a slow learning phase followed by an abrupt transition to perfect accuracy. To investigate network behavior at the transition, we examine the attention maps (for a randomly chosen input sequence) before and after the transition (Figure 3b). Before the transition, the attention map of the first layer p (1) shows queries paying uniform attention to the keys. For the second layer, we visualize the attention paid by the target p (2) q. on the other tokens (as the other attention patterns do not influence classifier output), which also shows no clear pattern. After\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c0aa/c0aae1c7-318a-477c-9c1d-783352aec671.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: (a) IC accuracy curve (p C = 0. 8, B = 1, K = 256) shows a slow learning phase followed by the abr transition to zero loss. (b) The layer 1 and 2 attention maps p (1) (top matrices) and p (2) q. (bottom vectors) before a after the abrupt transition (marked in the IC curve in panel (a)).\n</div>\nAnother curious feature of the IC accuracy curves is the slow learning phase that precedes the abrupt transition (Figure 3a). This phase leads to a non-negligible increase in IC accuracy despite the unstructured attention maps. What drives this slow learning? We hypothesize that the network learns to extract useful information from the context despite not learning the optimal ICL solution. Specifically, the total number of labels (L) is larger than the number of labels represented in the context (N). The network can thus randomly pick one of the N contextual labels to increase its accuracy from 1 /L to 1 /N. This picture suggests that the target pays attention to the N labels in the second layer.\nTo test this hypothesis and quantify the patterns visualized in the attention maps, we define four progress measures. Item-label association (ILA1): the attention paid by a token to its previous one in the first layer. Target-item-label association (TILA2): the attention paid by the target to the correct label in the second layer. Context-label accuracy (CLA): the probability that the network predicts a label present in the context. Target-labels association (TLA2): the total attention paid by the target to the N labels in the second layer. (ILA1) and (TILA2) quantify the changes that occur during the abrupt transition whereas (CLA) and (TLA2) quantify the changes expected during the slow learning phase. Each progress measure is obtained by averaging\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/612a/612abe0e-6d4b-4eb7-8324-63e2ca2195ae.png\" style=\"width: 50%;\"></div>\nFigure 4: Progress measures for six seeds aligned based on when the IC accuracy crosses 50%. The color-progress measure pairings are orange: (ILA1), green: (TILA2), blue: (CLA), red: (TLA2), black: IC accuracy. See text for more details.\n\nFigure 4 shows aligned progress measures (based on when IC accuracy reaches 50%). The dynamics of IC accuracy and the progress measures are remarkably reproducible across seeds. Figure 4 confirms the hypothesis that the network learns to randomly pick a contextual label in the slow learning phase (blue curve in Figure 4). Moreover, this is accompanied by the target paying attention to the labels (red curve in Figure 4). As visualized in Figure 3b, the item-label associations of the first layer and target-item-label associations of the second layer appear precisely at the transition (green and orange curves in Figure 4).\nInduction head formation drives the abrupt transition during ICL. The dynamics of the progress measures raises various hypotheses regarding the factors that lead to ICL. Specifically, we are interested in whether learning (CLA) or (TLA2) is necessary for the abrupt transition (tracked by (ILA1),(TILA2)). We consider various hypotheses and design experiments to test them: H1. (CLA) \u2192 (TLA2) \u2192 (ILA1), (TILA2). H2. (TLA2) \u2192 (ILA1), (TILA2). H3. (CLA) \u2192 (ILA1), (TILA2). It is also possible that none of these factors or a factor that we have no tracked leads to ICL.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4af3/4af3f5fa-84a5-4f12-baa4-e8864f2f101e.png\" style=\"width: 50%;\"></div>\nWe first observe that progress measures (ILA1) and (TILA2) strongly suggest the formation of an induction head (Olsson et al., 2022). Recall that an induction head enables zero-shot copying: given an input sequence. . . , x, \u2113, . . . x \u2192?, an induction head allows for predicting \u2113 even if x, \u2113 never appear together during training. Clearly, this is a mechanism that plausibly solves our task in-context. An induction head implemented by a two-layer attention-only network executes the following sequence of operations (visualized in Figure 5): (i) A token (say, \u2113) pays attention to the token immediately preceding it (here, x) using positional information. (ii) The value matrix of the first layer now writes the content of x into \u2113. Importantly, this is written to a \u201cbuffer\u201d subspace orthogonal to the content of \u2113. (iii) The target x pays attention to \u2113 by matching its content to \u2113 \u2019s buffer, which now contains the content of the contextual x that preceded it. (iv) The value matrix of the second layer writes the content of \u2113 to the target x, which is then passed to the classifier. The classifier in turn uses this information to predict \u2113.\nWe construct a minimal three-parameter model of the two-layer induction head that emulates these core computations and also captures the four progress measures. We assume that the input embedding space can be decomposed into two orthogonal D-dimensional subspaces. For a token u i, these orthogonal subspaces encode content u (c) i and a buffer u (b) i (initially empty). Given a sequence u 1, u 2, . . . , u n, the first and second layers of our minimal model compute\n\n(ii)\n\n(iii)\n\n(iv)\n\nFigure 5: An illustration of the four operations performed by an induction head.\n\n<div style=\"text-align: center;\">Figure 5: An illustration of the four operations performed by an induction head.\n</div>\n\ufffd\n\ufffd\nThe classifier receives the concatenated vector w (c) n \u2295 w (b) n. Here, \u03b4 i,j is one only if i = j and zero otherwise. \u03b2 1 thus determines the attention paid by a token to its previous token (progress measure (ILA1)). \u03b1 determines the attention paid by the target\u2019s content to a token\u2019s buffer (progress measure (TILA2)). \u2206 i,j is one only if i \u2212 j is odd and zero otherwise. \u03b2 2 thus determines the attention paid by the target to the labels in the context (progress measure (TLA2)). Since the classifier receives the target\u2019s content and buffer, it has the capacity to capture progress measure (CLA). We optimize for \u03b1, \u03b2 1, \u03b2 2 and the classifier\u2019s parameters using the same training procedure as the full network. Loss and accuracy curves are presented in Figure A.2.\nProgress measures from the minimal model exhibit strikingly similar dynamics (Figure 6a), including the abrupt transition in IC accuracy. Note that the slow learning phase in the IC accuracy curve is truncated in Figure 6a compared to Figure 4. Nevertheless, the network does indeed gradually learn to predict the N contextual labels (blue curve in Figure 6a). The abrupt transition appears sooner for the three-parameter model, which masks the slow learning phase.\nNext, we repeat the experiment fixing \u03b2 2 = 0. In this case, the target cannot pay more attention to the N contextual labels relative to the items in the second layer. We find that the dynamics of (ILA1), (TILA2) remain the same (Figure 6b), including the abrupt transition. This experiment rules out hypotheses H1 and H2, i.e., that the target-labels association (TLA2) leads to (ILA1), (TILA2).\nThe two-parameter model (with \u03b2 2 = 0 in (6)) together with the deep classifier recapitulate all the data distributional dependencies exhibited by the full network (Figure A.3). Moreover, note that the two-parameter model contains only the two parameters that characterize an induction head. This reduction strongly suggests that induction head formation drives the abrupt transition during ICL by the full network.\nTo test hypothesis H3 that (CLA) leads to (ILA1), (TILA2), we have to ablate the slow learning phase. Recall that during the slow learning phase, the network learns to randomly pick one of the N contextual labels. Since L > N, this simple strategy increases accuracy from 1 /L to 1 /N. The slow learning phase can be prevented by setting L = N and\n\n(4)\n\n(5)\n\n(6)\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d833/d8338b54-19df-41f1-8c02-d1045e81a740.png\" style=\"width: 50%;\"></div>\n(a)\n\n<div style=\"text-align: center;\">Figure 6: (a) Aligned progress measures (plotted as in Figure 4) for the minimal three-parameter mode dynamics as the progress measures for the full network. For L = 32, N = 8. (b) As in panel (a) with \u03b2 Loss curves for six seeds when L = N = 8.\n</div>\nB = 1. That is, the input sequence contains all the L labels exactly once. This perturbation indeed affects robust ICL. Specifically, two of the six seeds acquire the IC solution. The other four of the six seeds exhibit distinct, slow dynamics and converge to a sub-optimal minimum (Figure 6c).\nThe loss landscape of the induction head. We now examine the loss landscape of the induction head. Through this analysis, we aim to provide mechanistic insight into the abrupt transition and explain the empirical results described above. We propose a phenomenological model, which contains the key elements of the two-parameter induction head and the classifier. While this phenomenological approach helps identify core features of the learning dynamics, it ignores other elements. These other factors include the effects of stochasticity and the finite dimension (D) of the embedding. We assume B = 1; it is straightforward to extend the model to B > 1.\nConsider a softmax classifier that receives an input w and classifies it into L labels. Given that the target\u2019s correct label for a particular input sequence is t, the classifier models the probability that the label is t as\n\nwhere \u2113 j is the D-dimensional embedding vector for the label at index j, \u03c4 is the index of the target label t in the input sequence and N 1 = N for reasons discussed below. In (8), y determines the attention paid by the target to the correct label in the second layer (recall that there are 2 N + 1 tokens in the input sequence including the target). Note that we have ignored the contributions to w from the N item vectors, which contain irrelevant information and add noise to w.\nFrom (6), y is the product of \u03b1 and v (b) \u03c4.v (c) q, where q is the target\u2019s index. v (b) \u03c4.v (c) q is 1 if the label at \u03c4 pays attention to the item before it in the first layer. The attention weight corresponding to this term is e \u03b2 1\ne \u03b2 1 + N 1 (from (6)), where N 1 is the number of other tokens that compete for the label\u2019s attention, namely, 2 \u03c4 \u2212 1. Since \u03c4 varies from 1 to N across input sequences, we use an intermediate value, N 1 = N, for simplicity. A more elaborate model would consider an expectation over the N possibilities.\nFrom (8), the exponents in (7) contain dot products of the form \u03b3 i.\u2113 j for arbitrary pairs i, j. If all labels are statistically identical and balanced, it is simpler to track the overlaps \u03b3 i.\u2113 i \u2261 \u03b6 for all i and \u03b3 i.\u2113 j \u2261 \u03b6 \u2032 for all i \u0338 = j.\nIn summary, the loss after re-arranging terms is given by\n\ufffd \ufffd\n\n(7)\n\n(9)\n\n(10)\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8167/8167526a-6bc3-4257-b0f5-ef513e87f822.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: (a) Loss curve for the phenomenological model obtained via gradient descent on the loss in (10). (three parameters \u03b2 1 (layer 1), \u03b1 (layer 2), \u03be (layer 3) are learned sequentially. (c) The trajectory visualized on t landscape (green: initial point, red: final point).\n</div>\nwhere \u03be = \u03b6 \u2212 \u03b6 \u2032. The loss contains three nested logits parameterized by \u03b2 1, \u03b1, \u03be, which correspond to the first attention layer, the second attention layer and the third softmax layer, respectively.\nThe learning curves generated by gradient descent on this landscape beginning from the initial point \u03be, \u03b1, \u03b2 1 = 0 recapitulate the slow learning phase and the abrupt transition (for L > N, Figure 7a). Indeed, \u2202 L /\u2202\u03be = \u2212 (L \u2212 N) / (L (2 N + 1)) at the origin. Intuitively, when L > N, the classifier gradually aligns the regression vectors with the labels (increasing \u03be) when learning to randomly pick one of the labels in the context. This phase is slow as the classifier cannot discriminate between the N contextual labels. The gradual rise in \u03be eventually drives the loss off a cliff and leads to rapid learning of \u03b1 and \u03b2 1 (Figures 7b,c).\nAs shown in Figure 6c, when L = N, the slow learning phase is ablated and the learning dynamics show two distinct behaviors: ICL and slow convergence to a sub-optimal minimum. We reproduce these two distinct behaviors by setting L = N in (10) and simulating gradient descent from two points near the origin (Figure A.4a). Examining the loss landscape shows that this divergence is due to a saddle point at the origin (Figure A.4b). One path leads to the ICL solution whereas the other path gradually converges to a sub-optimal minimum. Moreover, the ICL solution takes much longer to acquire compared to when L > N due to a shallower gradient at the origin (compare Figure 7a and A.4a). Next, we examined the robustness of ICL in the full model (2) when L = N. Consistent with our analysis of the phenomenological model, the full model robustly learns an ICL solution for L > N but not when L = N (Figure A.5).\n\n# 4 Discussion\n\nSummary. In summary, past work has found that particular features of the data distribution influence the trade-off between ICL and IWL. The features that promote ICL are especially prominent in language, such as a large number of rare tokens that are over-represented in specific contexts. We reproduced these data distributional dependencies in a minimal model, thus highlighting the essential ingredients necessary to explain those observations. We present strong evidence that ICL is implemented by an induction head. We build a minimal version of an induction head, which through careful experiments reveal the key factors that lead to its emergence. In particular, the learning of an independent sub-optimal strategy accompanied by a slow learning phase supports the induction head\u2019s abrupt formation. A phenomenological model of the loss landscape shows that this abrupt transition is likely due to the sequential learning of three nested logits. Specifically, slow learning of the classifier\u2019s logit gradually guides the network towards a cliff in the landscape, leading to a sudden drop to zero loss.\nAbrupt transitions during ICL. An abrupt transition in loss dynamics has been noted in a wide variety of ICL tasks. However, a mechanistic understanding of ICL dynamics has been lacking. Our analysis suggests a putative cause: known mechanisms for ICL, such as an induction head, rely on a series of specific operations performed by multiple attention-based layers. The attention operation involves a logit (or, in general, other nonlinear operations), which creates sharp gradients. A chain of operations across attention layers will thus entail a series of nested logits, which create \u201ccliffs\u201d in the loss landscape and lead to abrupt jumps in loss during training.\nRelationship with past work. Our work adds to existing evidence that induction heads play a key role during ICL Olsson et al. (2022). It is interesting to examine whether more complex statistical features of the contextual sequence can be learned in-context by small transformer models and the mechanisms that enable them. We also recapitulate the data distributional dependencies delineated in Chan et al. (2022). Our results show that even simple networks such as ours are capable of simultaneously learning ICL and IWL solutions (see Figure A.1 for example). However, ICL is not\n\ntransient in our simulations. This contrasts with recent work Singh et al. (2023) who use a much larger transformer network (12 layers and 8 heads) and finite training data. It is possible that larger networks slowly memorize the training data, leading to a gradual degradation of ICL.\nImplications for LLMs. We show that an intrinsic curriculum may be necessary to overcome shallow gradients and guide networks towards the ICL solution. This observation is consistent with empirical results in Garg et al. (2022), who use manually designed curricula to robustly train transformers to solve complex ICL tasks. An intriguing possibility is that learning of simpler ICL operations enables the learning of more complex ICL strategies in large language models (LLMs). Initial gradual learning of a simpler ICL strategy (such as the learning of the parameter \u03be in our model) can accelerate the learning of a non-trivial ICL solution. An hierarchy of increasingly complex sub-tasks may lead to a cascading effect and potentially explain the sudden emergence of zero-shot learning abilities in LLMs. Testing this hypothesis will require careful mechanistic analysis of minimal networks that solve complex ICL tasks. More generally, while automatic curriculum learning has been used to train foundational models for RL Team et al. (2023), the role of curricula for accelerating ICL in LLMs remains relatively unexplored.\nLimitations. While our formulation provides a minimal model that exhibits ICL, it is possible that larger models use different mechanisms than the ones that we have identified here. Methods for mechanistic interpretability Wang et al. (2022) may help probe these mechanisms in LLMs. We have not used heuristics such as weight tying Inan et al. (2016); Press & Wolf (2016), which are used to accelerate training of LLMs. Such heuristics may make the slow learning phase unnecessary by aligning the classifier\u2019s regression vectors with the labels (increasing \u03be) from the outset.\n\n# References\n\nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. Kabir Ahuja, Madhur Panwar, and Navin Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891, 2023. Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016. Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. The omniglot challenge: a 3-year progress report. Current Opinion in Behavioral Sciences, 29:97\u2013104, 2019. Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067, 2023. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809, 2023. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.\n\nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. Kabir Ahuja, Madhur Panwar, and Navin Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891, 2023. Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016. Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. The omniglot challenge: a 3-year progress report. Current Opinion in Behavioral Sciences, 29:97\u2013104, 2019. Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067, 2023. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809, 2023. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.\n\nOfir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\nAaditya K Singh, Stephanie CY Chan, Ted Moskovitz, Erin Grant, Andrew M Saxe, and Felix Hill. The transient nature of emergent in-context learning in transformers. arXiv preprint arXiv:2311.08360, 2023.\nAdaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al. Human-timescale adaptation in an open-ended task space. arXiv preprint arXiv:2301.07608, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151\u201335174. PMLR, 2023. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916, 2023.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2021.\n\n# A Appendix\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/13b2/13b27287-91de-4101-9d06-c242c1aba5ed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure A.1: Accuracy curves for the full model when 0 < p B <1 and 0 < p C <1. In all cases, the network both the ICL and the IWL solutions. Here K = 256, B = 1, \u03b1 = 0, \u03b5 = 0.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f8b8/f8b874c0-4ee5-4fca-a687-67ae50ae7a0b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1406/140616a4-129e-4846-ba55-d4b3792c612a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure A.3: Data distributional dependencies are recapitulated by the minimal model. Plotted as in Figure K = 512, D = 64, B = 1, \u03b1 = 0, \u03b5 = 0. 1 (except when that parameter is varied)\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df89/df8990d0-0197-4e96-ab6a-bb5da2c34f97.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">A.4: (a) When L = N, the loss curves starting from two initial values recapitulate the two distinct behaviors n Figure 6c. (b) The loss landscape has a saddle at the origin such that small fluctuations lead the path either to L solution (top right quadrant) or a sub-optimal minimum (bottom left quadrant).\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d97/9d97e2db-f908-43c4-a533-c6d77c69dae2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5401/5401bbcf-f5d2-4141-82a4-dbb6ecb2f782.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nA.5: IC accuracy curves for different N and L (six seeds for each pair of values of L and N are shown) tent with the theory and the minimal network, the full network ((2)) robustly learns the in-context solution if N but not when L = N. Here K = 256, B = 1, p C = 0. 8, p B = 1, \u03b1 = 0, \u03b5 = 0.\n\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in transformer models, emphasizing its significance in predicting responses to novel queries based on illustrative examples in the input sequence. The contrast between ICL and traditional in-weights learning (IWL) is highlighted, along with the importance of understanding the data distributional properties that influence these learning mechanisms.",
        "problem": {
            "definition": "The problem revolves around understanding the mechanisms that govern the abrupt emergence of ICL in transformer models, particularly focusing on the role of induction heads and data distributional properties.",
            "key obstacle": "A major challenge is elucidating the unclear mechanistic basis behind the abrupt transition to in-context learning and the conditions that favor its emergence over in-weights learning."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that specific distributional properties, such as burstiness and rank-frequency distributions in language, control the trade-off between ICL and IWL.",
            "opinion": "The authors propose that the emergence of an induction head is crucial for ICL, which is characterized by a sharp transition in learning dynamics.",
            "innovation": "The primary innovation lies in constructing a minimal two-parameter model of an induction head that captures the dynamics of learning and reproduces the data distributional dependencies observed in attention-based networks."
        },
        "Theory": {
            "perspective": "The theoretical perspective focuses on the formation of induction heads within multi-layer attention networks and their role in facilitating ICL.",
            "opinion": "The authors assume that the learning dynamics of ICL are driven by the sequential learning of nested logits and the intrinsic curriculum of the network.",
            "proof": "The proof is derived from empirical observations and the development of a phenomenological model that traces the abrupt learning phenomenon to cliffs in the loss landscape created by nested nonlinearities."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using a simplified two-layer attention-only network trained on a Gaussian mixture model dataset, with parameters such as burstiness (B), number of classes (K), and within-class variability (\u03b5) manipulated.",
            "evaluation method": "The evaluation involved measuring prediction accuracy on input sequences to track both ICL and IWL, using metrics such as item-label association and target-label association."
        },
        "conclusion": "The study concludes that ICL is implemented through the formation of induction heads, which drive abrupt transitions in learning dynamics. The results demonstrate that specific data distributional properties influence the trade-off between ICL and IWL.",
        "discussion": {
            "advantage": "The paper provides a mechanistic understanding of ICL, highlighting the importance of data distributional properties and induction heads in facilitating emergent learning capabilities.",
            "limitation": "The findings may not generalize to larger models, which could utilize different mechanisms for ICL, and the study does not consider certain heuristics that could enhance learning efficiency.",
            "future work": "Future research could explore the mechanisms of ICL in larger models and investigate how intrinsic curricula can be utilized to enhance learning dynamics."
        },
        "other info": [
            {
                "info1": "The study emphasizes the role of attention operations in creating sharp gradients that lead to abrupt changes in loss during training."
            },
            {
                "info2": {
                    "info2.1": "The paper suggests that the learning of simpler ICL operations may facilitate the learning of more complex strategies in large language models.",
                    "info2.2": "The authors highlight the need for mechanistic analysis of minimal networks to better understand complex ICL tasks."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning (ICL) in transformer models, emphasizing its significance in predicting responses to novel queries based on illustrative examples in the input sequence."
        },
        {
            "section number": "1.2",
            "key information": "The contrast between ICL and traditional in-weights learning (IWL) is highlighted, along with the importance of understanding the data distributional properties that influence these learning mechanisms."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective focuses on the formation of induction heads within multi-layer attention networks and their role in facilitating ICL."
        },
        {
            "section number": "3.3",
            "key information": "The primary innovation lies in constructing a minimal two-parameter model of an induction head that captures the dynamics of learning and reproduces the data distributional dependencies observed in attention-based networks."
        },
        {
            "section number": "6.1",
            "key information": "The findings may not generalize to larger models, which could utilize different mechanisms for ICL, and the study does not consider certain heuristics that could enhance learning efficiency."
        },
        {
            "section number": "7",
            "key information": "Future research could explore the mechanisms of ICL in larger models and investigate how intrinsic curricula can be utilized to enhance learning dynamics."
        }
    ],
    "similarity_score": 0.7192372786018513,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/The mechanistic basis of data dependence and abrupt learning in an in-context classification task.json"
}