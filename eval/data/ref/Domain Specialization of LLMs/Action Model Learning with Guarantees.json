{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.09631",
    "title": "Action Model Learning with Guarantees",
    "abstract": "This paper studies the problem of action model learning with full observability. Following the learning by search paradigm by Mitchell, we develop a theory for action model learning based on version spaces that interprets the task as search for hypothesis that are consistent with the learning examples. Our theoretical findings are instantiated in an online algorithm that maintains a compact representation of all solutions of the problem. Among these range of solutions, we bring attention to actions models approximating the actual transition system from below (sound models) and from above (complete models). We show how to manipulate the output of our learning algorithm to build deterministic and non-deterministic formulations of the sound and complete models and prove that, given enough examples, both formulations converge into the very same true model. Our experiments reveal their usefulness over a range of planning domains.",
    "bib_name": "aineto2024actionmodellearningguarantees",
    "md_text": "# Action Model Learning with Guarantees\n# Diego Aineto, Enrico Scala\nDiego Aineto, Enrico Scala\nDepartment of Information Engineering, University of Brescia, Italy {diego.ainetogarcia, enrico.scala}@unibs.it\nAbstract\n# Abstract\nThis paper studies the problem of action model learning with full observability. Following the learning by search paradigm by Mitchell, we develop a theory for action model learning based on version spaces that interprets the task as search for hypothesis that are consistent with the learning examples. Our theoretical findings are instantiated in an online algorithm that maintains a compact representation of all solutions of the problem. Among these range of solutions, we bring attention to actions models approximating the actual transition system from below (sound models) and from above (complete models). We show how to manipulate the output of our learning algorithm to build deterministic and non-deterministic formulations of the sound and complete models and prove that, given enough examples, both formulations converge into the very same true model. Our experiments reveal their usefulness over a range of planning domains.\narXiv:2404.09631v1\n# 1 Introduction\nThe engineering of action models is complicated and error prone, constituting one of the main bottlenecks in the application of model-based reasoning (Kambhampati 2007). Automating this process holds the promise of enabling the AI planning machinery (Ghallab, Nau, and Traverso 2004) over a provably consistent model of the domain. Action model learning tackles this problem by computing an approximation of a domain\u2019s dynamics from demonstrations. Most of the research in action model learning of the last two decades has been focused on learning under partial observability, investigating the application of different techniques with the aim of either improving the expressiveness of the learnt models or handling more incomplete and noisy demonstrations. Against this trend, Safe Action Model (SAM) Learning (Stern and Juba 2017; Juba, Le, and Stern 2021; Juba and Stern 2022; Mordoch, Juba, and Stern 2023) is a family of algorithms that takes a step back to study the fully observable setting from a theoretical standpoint that puts the emphasis on the properties of the learnt model. In particular, SAM focuses on learning safe models, i.e., those with which an agent can safely execute actions all the way to the goal.\nCopyright \u00a9 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThis paper deepens this theoretical first principled investigation through the lens of version spaces (Mitchell 1982). We focus our attention on classical planning models and develop a framework to learn action preconditions and effects by maintaining a version space of all hypothesis consistent with the demonstrations. The computed version space provides an efficient representation of all solutions to the action model learning problem. Among these solutions, those at the boundaries have special properties. On one end there is pessimism and, on the other, optimism. The pessimistic form leads to construct sound models, i.e., those never allowing the agent to take a wrong step as per the safe property studied by SAM. The optimistic form leads to complete models, i.e., those with which an agent can speculate about the existence of a plan. The sound model generates plans that are guaranteed to work with the true model, but will often discard valid plans. On the other hand, complete models do not in general produce valid plans, but guarantee the existence of a plan if one exists for the true model. Our framework aims at getting the best of both worlds, by showing that, much as sound models lead to deterministic planning formulations, complete models can be captured through the extra expressiveness of non-deterministic ones. The main contribution of our work is theoretical. Indeed, our investigation precisely establishes rules that heavily exploit the structure of the hypothesis space in order to learn all the solutions of an action model learning problem. Such rules materialize into an online algorithm that outputs a compact representation of the set of solutions. Then, we show how to manipulate this representation to extrapolate sound and complete action model formulations that, given enough demonstrations, converge at the very same true model. Finally, we conduct an experimental evaluation over a range of domains with the purpose of understanding the usefulness of the proposed framework. Our findings demonstrate that the adoption of a sound and a complete model provides the agent with better reasoning capabilities earlier in the learning process. This is due to the fact that complete models can exploit negative demonstrations, too. The paper is organized as follows: We start off with background material on action model learning and version spaces. Then we delve into building a precise mapping between these two worlds (Section 3) outlining several theoretical results. Section 4 shows how to leverage this mapping\nto build sound and complete action models, which are then practically evaluated in Section 5. We conclude with related work and discussion (sections 6 and 7).\n# 2 Preliminaries\nThis section presents the basic notions around action model learning and version spaces.\n# Action Model Learning\nAn action model is a description of the capabilities of some agent, system or environment. In this work, we focus on learning deterministic action models with conjunctive preconditions (McDermott et al. 1998), as defined below.\nDefinition 1 (Action Model). An action model is a tuple M = \u27e8F, A, pre, eff\u27e9where:\n \u27e8\u27e9 \u2022 F = {f1, . . . , fn} is a finite set of Boolean state variables called fluents. A positive (resp. negative) literal is l = f (resp. l = \u00acf) and its completement is \u00afl = \u00acl. We denote the set of all literals by L. \u2022 A is a finite set of labels called actions. \u2022 pre : A \u21922L defines the precondition pre(a) \u2286L for all a \u2208A. \u2022 eff : A \u21922L defines the effect eff(a) \u2286L for all a \u2208A. Action models are often represented in lifted manner, by parameterising actions and fluents over a set of objects. We adopt a ground represented for ease of presentation, but all our results are directly applicable to the lifted case. An action model succinctly represents a transition system where a state s is an assignments over F, represented by a subset of L without conflicting values. We denote by S the state space induced by F. Formally, an action model M = \u27e8F, A, pre, eff\u27e9induces the transition system TM = {\u27e8s, a, s\u2032\u27e9\u2208S \u00d7 A \u00d7 S | pre(a) \u2286s \u2227s\u2032 = (s \\ eff(a)) \u222aeff(a)} where eff(a) = {l | l \u2208eff(a)}. Action models are widely used in AI planning to formulate reachability problems over the induced transition system. A classical planning problem P = \u27e8M, s0, G\u27e9is defined by combining an action model M = \u27e8F, A, pre, eff\u27e9 with an initial state s0 \u2208S and goal condition G \u2286L. A solution for P is a sequence of actions \u03c0 = (a1, . . . an) known as plan and its execution in s0 yields an interleaved sequence \u27e8s0, a1, s1, a2, s2, . . . , an, sn\u27e9that alternates actions and states iteratively reached by applying the action one after the other. A plan is a valid solution if every transition \u27e8si, ai+1, si+1\u27e9belongs to the transition system TM and G \u2286sn. We denote the set of solution plans for P by \u03a0(P). Action model learning is about computing the action model of an agent from demonstrations of its capabilities. Hereinafter, we denote by A the true action model of the agent and assume that it complies with Definition 1. Demonstrations are collected from executions of A, e.g., a plan or\n\u2022 F = {f1, . . . , fn} is a finite set of Boolean state variables called fluents. A positive (resp. negative) literal is l = f (resp. l = \u00acf) and its completement is \u00afl = \u00acl. We denote the set of all literals by L. \u2022 A is a finite set of labels called actions. \u2022 pre : A \u21922L defines the precondition pre(a) \u2286L for all a \u2208A.  \nAction models are often represented in lifted manner, by parameterising actions and fluents over a set of objects. We adopt a ground represented for ease of presentation, but all our results are directly applicable to the lifted case. An action model succinctly represents a transition system where a state s is an assignments over F, represented by a subset of L without conflicting values. We denote by S the state space induced by F. Formally, an action model M = \u27e8F, A, pre, eff\u27e9induces the transition system TM = {\u27e8s, a, s\u2032\u27e9\u2208S \u00d7 A \u00d7 S | pre(a) \u2286s \u2227s\u2032 = (s \\ eff(a)) \u222aeff(a)} where eff(a) = {l | l \u2208eff(a)}. Action models are widely used in AI planning to formulate reachability problems over the induced transition system. A classical planning problem P = \u27e8M, s0, G\u27e9is defined by combining an action model M = \u27e8F, A, pre, eff\u27e9 with an initial state s0 \u2208S and goal condition G \u2286L. A solution for P is a sequence of actions \u03c0 = (a1, . . . an) known as plan and its execution in s0 yields an interleaved sequence \u27e8s0, a1, s1, a2, s2, . . . , an, sn\u27e9that alternates actions and states iteratively reached by applying the action one after the other. A plan is a valid solution if every transition \u27e8si, ai+1, si+1\u27e9belongs to the transition system TM and G \u2286sn. We denote the set of solution plans for P by \u03a0(P). Action model learning is about computing the action model of an agent from demonstrations of its capabilities. Hereinafter, we denote by A the true action model of the agent and assume that it complies with Definition 1. Demonstrations are collected from executions of A, e.g., a plan or random walk, and represented similarly to transitions. Definition 2 (Demonstration). A demonstration is a triple d = \u27e8s, a, s\u2032\u27e9consisting of a pre-state s \u2208S, an action a \u2208A, and a post-state s\u2032 \u2208S \u222a{\u22a5}.\nIn this work, we consider positive demonstrations, those transitions of TA, and negative demonstrations, those representing the failure of executing action a in state s which we indicate by using a \u22a5post-state. An action model learning problem takes as input a set of fluents F, a set of actions A and a set of demonstrations D. The aim of action model learning is to find an action model that is consistent with all the demonstrations in D. It is worth noting that the space of models that can be synthesised given F and A is finite, i.e, M = {\u27e8F, A, pre, eff\u27e9| \u2200a \u2208A : pre(a) \u22082L \u2227eff(s) \u22082L}. Definition 3 (Action Model Learning Problem). An action model learning problem is a tuple \u039b = \u27e8F, A, D\u27e9where F is a set of fluents, A is a set of actions, and D is a set of demonstrations. A solution for \u039b is an action model M = \u27e8F, A, pre, eff\u27e9such that: 1. for all positive demonstrations \u27e8s, a, s\u2032\u27e9\u2208D, it holds that pre(a) \u2286s and s\u2032 = (s \\ eff(a)) \u222aeff(a); 2. for all negative demonstrations \u27e8s, a, \u22a5\u27e9\u2208D, it holds that pre(a) \u0338\u2286s. We denote by MD the subset of the model space M that satisfies (1) and (2), i.e., the set of solutions of \u039b.\nHereinafter, we use MD and consistent action models interchangeably, depending on the nature of the text, to refer to the set of solutions of the action model learning problem.\n# Version Spaces\nWe adopt the notation and definitions introduced in later extensions of the version spaces framework (Lau et al. 2003). Definition 4 (Hypothesis and Hypothesis Spaces). A hypothesis is a function h : I \u2192O. A hypothesis space H is a set of functions with the same domain and range. Definition 5 (Learning Example). A learning example \u03f5 is a pair (i, o) \u2208I \u00d7 O. A hypothesis h is consistent with a learning example \u03f5 = (i, o) if and only if h(i) = o.\nDefinition 5 (Learning Example). A learning example \u03f5 is a pair (i, o) \u2208I \u00d7 O. A hypothesis h is consistent with a learning example \u03f5 = (i, o) if and only if h(i) = o.\nDefinition 6 (Version Space). Given a hypothesis space H and a set of learning examples E, the version space VH,E is the subset of H that is consistent with all examples in E. We will often omit the subscripts if the hypothesis space and learning set are clear from the context.\nLet \u2264be a partial order relation between elements in H, a version space V can be efficiently represented in terms of its least upper bound, called U boundary, and its greatest lower bound, called L boundary, relative to \u2264(Lau, Domingos, and Weld 2000). The consistent hypotheses are those that belong to the boundaries or lie between them in the partial order. Formally, VH,E = {h \u2208H | \u2203(hL, hU) \u2208 LH,E \u00d7 UH,E : hL \u2264h \u2264hU}. Version Space Learning (Mitchell 1982) is an online algorithm that maintains a version space by updating its boundaries each time a new learning example is observed. The update function must ensure that everything within the new boundaries is consistent and everything outside, inconsistent. Figure 1 illustrates the main ideas of version space learning. The left side shows a version space represented by its L (dashed line) and U (dotted line) boundaries. L represents\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae5e/ae5e51bc-561a-4360-ad85-1c212338f4cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Version spaces and version space learning.</div>\nthe most pessimistic hypothesis, a minimal frontier that encloses only the observed positive examples (\u201d+\u201d signs), i.e., those of the target class. On the other hand, U represents the most optimistic hypothesis, a maximal frontier that keeps all negative examples (\u201d-\u201d signs) outside. The L boundary guarantees that any unseen example within its region will be positive, whilst the U boundary guarantees that everything outside its frontier will be negative. The version space learning algorithm updates these boundaries as new learning examples appear. On the right hand of the figure, we show how these boundaries have been updated after two new learning examples (in red) are observed. When the new example is positive, the L boundary grows to contain it; otherwise, if it is negative, the U boundary shrinks to reject it. A boundary may consist of one or more hypothesis, e.g., in our figure the U boundary contains two hypotheses after the update.\n# 3 Version Space Learning for Action Models\nThis section proposes a novel framework for action model learning based on version spaces. Roughly, our approach learns the precondition and effect of an action by computing a version space of its preconditions and a version space of its effects. We start by defining the hypothesis spaces and the update functions. Then, we present our algorithm.\n# The Hypothesis Space\nFor simplicity, with abuse of notation, in our context a hypothesis h is a subset of L that represents a function. When h is a precondition hypothesis, h represents the applicability function Apph : S \u2192{0, 1} defined as Apph(s) = h \u2286s. On the other hand, if h is an effect hypothesis, h represents the successor function Such : S \u2192S defined as Such(s) = (s \\ h) \u222ah. Let a \u2208A, with Ha p = 2L we define the hypothesis space of a\u2019s preconditions, while with Ha e = 2L the hypothesis space of its effects. We order our hypothesis spaces using a set inclusion relation. Specifically, given two precondition hypotheses h1 and h2 in Ha p, h1 \u2264h2 iff h1 \u2287h2, and given two effect hypotheses h1 and h2 in Ha e, h1 \u2264h2 iff h1 \u2286h2. Note the opposite direction of the inclusion relation. Considering these hypothesis spaces, the learning examples will be pairs (s, b) \u2208S \u00d7 {0, 1} for the preconditions, and pairs (s, s\u2032) \u2208S \u00d7 S for the effects. Learning examples are implicitly given by the demonstrations. A positive demonstration \u27e8s, a, s\u2032\u27e9entails the learning example (s, 1)\nfor the precondition and the learning example (s, s\u2032) for the effect. On the other hand, a negative demonstration \u27e8s, a, \u22a5\u27e9 entails only the learning example (s, 0) for the precondition. Hereinafter, Dp and De denote the learning examples entailed by a set of demonstrations D for Ha p and Ha e, respectively. A hypothesis h \u2208Ha p is consistent with a learning example (s, b) iff Apph(s) = b. Analogously, a hypothesis h \u2208Ha e is consistent with a learning example (s, s\u2032) iff Such(s) = s\u2032. The following theorem shows that, when the learning examples come from the same set of demonstrations D, any model M built using preconditions and effects from the learnt version spaces is a solution of the action model learning problem \u039b = \u27e8F, A, D\u27e9, i.e., M \u2208MD. Theorem 1. Let VHap,Dp and VHae,De be the version spaces of preconditions and effects of a \u2208A. The action model M = \u27e8F, A, pre, eff\u27e9belongs to MD if and only if \u2200a \u2208A : pre(a) \u2208VHap,Dp \u2227eff(a) \u2208VHae,D\u2032e.\nProof. Let d = \u27e8s, a, s\u2032\u27e9be a positive demonstration in D that entails the learning examples (s, 1) \u2208Dp and (s, s\u2032) \u2208 De. From the definition of version space, (pre(a), eff(a)) belongs to VHap,Dp \u00d7VHae,De if and only if Apppre(a)(s) = 1 and Suceff(a)(s) = s\u2032 or, equivalently, iff pre(a) \u2286s and s\u2032 = (s \\ eff(a)) \u222aeff(a). Now, let d = \u27e8s, a, \u22a5\u27e9be a negative demonstration in D and (s, 0) \u2208Dp the entailed learning example. Again, pre(a) \u2208VHap,Dp if and only if Apppre(a)(s) = 0, i.e., iff pre(a) \u0338\u2286s. Therefore, if \u2200a \u2208A : pre(a) \u2208VHap,Dp \u2227eff(a) \u2208VHae,De, M satisfies conditions (1) and (2) of Definition 3, i.e., M \u2208MD; otherwise, M \u0338\u2208MD.\n# Initializing and Updating the Version Space\nThe initialization of the version space learning algorithm sets the version space to contain the whole hypothesis space, i.e., VHap,\u2205= Ha p and VHae,\u2205= Ha e. This is done by setting the L and U boundaries to contain the minimal and maximal elements of the hypothesis space, respectively. In our problem, for all actions a \u2208A, LHap,\u2205= {L}, UHap,\u2205= {\u2205}, LHae,\u2205= {\u2205}, and UHae,\u2205= {L}. Note that, by Theorem 1 and the definition of version spaces, these boundaries allow to compactly represent the full space of action models M. As illustrated in Figure 1, updating a version space involves extending the L boundary or shrinking the U boundary. This is done by modifying the hypothesis that constitute the boundaries or removing them. In our context, where hypotheses are sets, the update consists in finding the smallest superset or the largest subset that is consistent with the new demonstration. Next theorem shows how the boundaries for the version space of a\u2019s preconditions VHa p are updated. Intuitively, we extend the L boundary by removing any literal not in pre-state of a positive demonstration, whilst we shrink the U boundary by adding some literal not in the pre-state of a negative demonstration. Theorem 2 (Update rules for VHap). Let LHap,Dp and UHap,Dp be the boundaries of a version space VHap,Dp and\nd a demonstration. The updated version space VHap,D\u2032p, with D\u2032 = D \u222a{d}, is given by the following rules. If d = \u27e8s, a, s\u2032\u27e9is a positive demonstration: \u2022 RUP. Remove inconsistent hypotheses from UHap,Dp: UHap,D\u2032p := {hU | hU \u2208UHap,Dp \u2227hU \u2286s} \u2022 ULP. Update hypotheses in LHap,Dp: LHap,D\u2032p := {hL \u2229s | hL \u2208LHap,Dp} If d = \u27e8s, a, \u22a5\u27e9is a negative demonstration: \u2022 RLP. Remove inconsistent hypotheses from LHap,Dp: LHap,D\u2032p := {hL | hL \u2208LHap,Dp \u2227hL \u0338\u2286s} \u2022 UUP. Update hypotheses in UHap,Dp: Let hL \u2208LHap,Dp, UHap,D\u2032p := {hU | hU \u2208UHap,Dp \u2227hU \u0338\u2286s}\u222a \u222a{hU \u222a{l} | hU \u2208UHap,Dp \u2227hU \u2286s \u2227l \u2208hL \\ s}\nUHap,D\u2032p := {hU | hU \u2208UHap,Dp \u2227hU \u0338\u2286s}\u222a \u222a{hU \u222a{l} | hU \u2208UHap,Dp \u2227hU \u2286s \u2227l \u2208hL \\ s}\n      Proof. When d = (s, a, s\u2032) is a positive demonstration, a hypothesis h is inconsistent iff h \u0338\u2286s. Rule RUP removes an upper bound hU \u2208UHap,Dp if hU \u0338\u2286s and, in doing so, removes any hypothesis h such that hU \u2282h from the version space. Observe that, since hU \u0338\u2286s, any superset of hU will also be inconsistent. Rule ULP raises the lower bound from hL to hL \u2229s. Indeed, hL \u2229s \u2286s and all subsets of hL \u2229s are also consistent. Note that, if hL already satisfied hL \u2286s, this rule causes no change, i.e., hL = hL \u2229s; otherwise, hL \u2229s is the largest subset that is consistent since \u2200l \u2208 hL \\ (hL \u2229s) it holds that l \u0338\u2208s so all other subsets h such that hL \u2229s \u2282h \u2286hL are inconsistent. In the case that d = \u27e8s, a, \u22a5\u27e9, a hypothesis h is inconsistent if h \u2286s. Rule RLP removes the lower bound hL \u2208LHap,D if hL \u2286s which also removes all its subsets from the version space. Indeed, for any subset h of hL it holds that h \u2286hL \u2286s and, therefore, h is inconsistent. Rule UUP shrinks an upper bound hU \u2208UHap,D to the set {hU \u222a{l} | l \u2208hL \\ s} if hU \u2286s. Note that \u2200l \u2208hL \\ s : hU \u222a{l} \u0338\u2286s so all the new upper bounds are the smallest supersets of hU that are consistent. Note that, after any examples, LHap will at most contain a single hypothesis. Indeed, ULP only modifies the existing hypothesis without adding new ones. This result coincides with the well-known fact that, when learning pure conjunctive formulas (a precondition pre(a) is equivalent to a conjunction \ufffd l\u2208pre(a) l), the L boundary will at most contain a single hypothesis, while the U boundary can grow to contain multiple hypotheses (Mitchell 1982). Next, we move on to the learning of effects. Before presenting the update rules, we introduce the following lemma that gives us a better handle on the version space of effects as it enables reasoning about consistency and inconsistency in terms of set inclusion. Lemma 3. Given two states s and s\u2032, and an effect hypothesis h \u2286L, s\u2032 \\ s \u2286h \u2286s\u2032 if and only if s\u2032 = (s \\ h) \u222ah.\nProof. When d = (s, a, s\u2032) is a positive demonstration, a hypothesis h is inconsistent iff h \u0338\u2286s. Rule RUP removes an upper bound hU \u2208UHap,Dp if hU \u0338\u2286s and, in doing so, removes any hypothesis h such that hU \u2282h from the version space. Observe that, since hU \u0338\u2286s, any superset of hU will also be inconsistent. Rule ULP raises the lower bound from hL to hL \u2229s. Indeed, hL \u2229s \u2286s and all subsets of hL \u2229s are also consistent. Note that, if hL already satisfied hL \u2286s, this rule causes no change, i.e., hL = hL \u2229s; otherwise, hL \u2229s is the largest subset that is consistent since \u2200l \u2208 hL \\ (hL \u2229s) it holds that l \u0338\u2208s so all other subsets h such that hL \u2229s \u2282h \u2286hL are inconsistent. In the case that d = \u27e8s, a, \u22a5\u27e9, a hypothesis h is inconsistent if h \u2286s. Rule RLP removes the lower bound hL \u2208LHap,D if hL \u2286s which also removes all its subsets from the version space. Indeed, for any subset h of hL it holds that h \u2286hL \u2286s and, therefore, h is inconsistent. Rule UUP shrinks an upper bound hU \u2208UHap,D to the set {hU \u222a{l} | l \u2208hL \\ s} if hU \u2286s. Note that \u2200l \u2208hL \\ s : hU \u222a{l} \u0338\u2286s so all the new upper bounds are the smallest supersets of hU that are consistent.\nNote that, after any examples, LHap will at most contain a single hypothesis. Indeed, ULP only modifies the existing hypothesis without adding new ones. This result coincides with the well-known fact that, when learning pure conjunctive formulas (a precondition pre(a) is equivalent to a conjunction \ufffd l\u2208pre(a) l), the L boundary will at most contain a single hypothesis, while the U boundary can grow to contain multiple hypotheses (Mitchell 1982). Next, we move on to the learning of effects. Before presenting the update rules, we introduce the following lemma that gives us a better handle on the version space of effects as it enables reasoning about consistency and inconsistency in terms of set inclusion. Lemma 3. Given two states s and s\u2032, and an effect hypothesis h \u2286L, s\u2032 \\ s \u2286h \u2286s\u2032 if and only if s\u2032 = (s \\ h) \u222ah.\nProof Sketch (Full proof in appendix). We can prove this by contradiction, leveraging algebra of sets and bitwise operations over a bit vector interpretation of states.\nThe update rules for the version space of effects, presented in the next theorem, leverage Lemma 3. The intuition for these rules is that, whenever we get a new positive demonstration \u27e8s, a, s\u2032\u27e9we update the upper bound to be a subset of s\u2032 and the lower bound to be a superset of s\u2032 \\s. By Lemma 3, any hypothesis between the updated bounds will also be consistent. Theorem 4 (Update rules for VHae). Let LHae,De and UHae,De be the boundaries of a version space VHae,De and d = \u27e8s, a, s\u2032\u27e9a positive demonstration. The updated version space VHae,D\u2032e, with D\u2032 = D \u222a{d}, is given by the following rules: \u2022 RLE. Remove inconsistent hypotheses from LHae,De LHae,D\u2032e := {hL | hL \u2208LHae,De \u2227hL \u2286s\u2032} \u2022 ULE. Update hypotheses in LHae,De: LHae,D\u2032e := {hL \u222a(s\u2032 \\ s) | hL \u2208LHae,De} \u2022 RUE. Remove inconsistent hypotheses from UHae,De UHae,D\u2032e := {hU | hU \u2208UHa e,De \u2227s\u2032 \\ s \u2286hU} \u2022 UUE. Update hypotheses in UHae,De: UHae,D\u2032e := {hU \u2229s\u2032 | hU \u2208UHae,De}\nThe update rules for the version space of effects, presented in the next theorem, leverage Lemma 3. The intuition for these rules is that, whenever we get a new positive demonstration \u27e8s, a, s\u2032\u27e9we update the upper bound to be a subset of s\u2032 and the lower bound to be a superset of s\u2032 \\s. By Lemma 3, any hypothesis between the updated bounds will also be consistent.\nTheorem 4 (Update rules for VHae). Let LHae,De and UHae,De be the boundaries of a version space VHae,De and d = \u27e8s, a, s\u2032\u27e9a positive demonstration. The updated version space VHae,D\u2032e, with D\u2032 = D \u222a{d}, is given by the following rules:\nProof. Rule RLE removes the lower bound hL \u2208LHae,De when hL \u0338\u2286s\u2032 and, in doing so, removes all its supersets from the version space. Note that, if hL \u0338\u2286s\u2032, all supersets of hL will also be inconsistent. Rule ULE raises the lower bound from hL to hL \u222a(s\u2032 \\s) which is a superset of (s\u2032 \\s) and, therefore, consistent. If (s\u2032 \\s) \u2286hL this rule produces no change; otherwise, ULE computes the smallest superset of hL that is consistent. Indeed, \u2200l \u2208(hL \u222a(s\u2032 \\ s)) \\ hL : l \u2208s\u2032 \\ s so removing any newly added literal from the hypothesis would make it inconsistent. Rule RUE removes the upper bound hU \u2208UHae,De when s\u2032 \\ s \u0338\u2286hU which also removes all its subsets from the version space. Indeed, if s\u2032 \\ s \u0338\u2286hU, then no subset of hU can be consistent. Rule UUE lowers the upper bound from hU to hU \u2229s\u2032 which is consistent since hU \u2229s\u2032 \u2286s\u2032. If hU was already a subset of s\u2032, this rule produces no change; otherwise, hU \u2229s\u2032 \u2286s\u2032 is the largest subset of hU that is consistent.\nThe following corollaries, derived by theorems 2 and 4, interpret the update rules in an offline fashion, after any number of demonstrations. Corollary 5. LHap,Dp = {hpL} s.t. hpL = \ufffd (s,1)\u2208Dp s. Corollary 6. LHae,De = {heL} s.t. heL = \ufffd (s,s\u2032)\u2208De s\u2032 \\ s Corollary 7. UHae,De = {heU} s.t. heU = \ufffd (s,s\u2032)\u2208De s\u2032\nAlgorithm 1: VSLAM\nInput Action Model Learning problem \u27e8F, A, D\u27e9\nOutput LHap, UHap, LHae and UHae for all a \u2208A\n1: for a \u2208A do\n\u25b7Inizialisation\n2:\nLHap := {L}\n3:\nUHap := {\u2205}\n4:\nLHae := {\u2205}\n5:\nUHae := {L}\n6: for \u27e8s, a, s\u2032\u27e9\u2208D do\n\u25b7Online loop\n7:\nif s\u2032 is not \u22a5then\n8:\nUHap := RUP(UHap, (s, 1))\n9:\nLHap := ULP(LHap, (s, 1))\n10:\nLHae := RLE(LHae, (s, s\u2032))\n11:\nLHae := ULE(LHae, (s, s\u2032))\n12:\nUHa\ne := RUE(UHae, (s, s\u2032))\n13:\nUHae := UUE(UHae, (s, s\u2032))\n14:\nelse\n15:\nLHap := RLP(LHap, (s, 0))\n16:\nUHap := UUP(UHap, (s, 0))\nreturn (LHap, UHap, LHae, UHae)\nCorollaries 5 and 7 show that learning a consistent action model is as easy as intersecting all pre-states for the preconditions and all post-states for the effects. In addition, Corollary 6 states that tighter effects can be obtained by joining all pre-state to post-state deltas. Note that, heL can also be computed using hpL and heU. Formally:\nProof sketch (Full proof in appendix). This can be proven by applying De Morgan\u2019s Law and using Lemma 3 to simplify equations.\nThanks to Lemma 8, we do not need to maintain the LHae boundary. Moreover, we leverage it in the following lemma, which implies that all consistent models using the lower bound hpL for their preconditions induce the same transition system. We will use this result to prove Theorem 11.\nProof sketch (Full proof in appendix). We can leverage Lemma 8 and the inclusion relation between effect hypothesis to prove this lemma.\n# The VSLAM Algorithm\nIn this section we present VSLAM, our algorithm for action model learning, outlined in Algorithm 1. VSLAM takes as input an action model learning problem \u27e8F, A, D\u27e9and returns the boundaries of the version space of preconditions and of effects for each action in A. The pseudocode for VSLAM is a straightforward instantion of the initialization and update rules presented in the\nprevious section. First, from lines 1 to 5, VSLAM initializes the version spaces associated to each action. Then, from lines 6 to 16, VSLAM processes all demonstrations in D in an online fashion and uses the induced learning examples to update the boundaries of the version spaces by applying the updates rules presented in theorems 2 and 4. VSLAM has some important properties that are derived from its version spaces foundation. First, by Theorem 1, the boundaries computed by VSLAM capture exactly all models in MD, i.e., all the solutions of \u27e8F, A, D\u27e9. Second, VSLAM can detect when it has learnt the true model by checking convergence of the version space. A version space converges when only one consistent hypothesis remains, i.e., when both boundaries are singletons and contain the same hypothesis. Under our working assumption that the true model A follows the syntax of Definition 1, theorems 2 and 4, ensure that no consistent hypothesis is discarded and, therefore, it follows that if only one hypothesis remains it must match the true model A. Corollary 10 (Convergence). Let A = \u27e8F, A, pre, eff\u27e9be the true model. If LHap,Dp = UHap,Dp = {hp}, then hp = pre(a), and if LHae,De = UHae,De = {he}, then he = eff(a). Lastly, VSLAM can also detect if our working assumptions are violated by checking for collapse of the version space. A version space collapses when it becomes empty, i.e., when no consistent hypothesis remains. This indicates that the learning examples are noisy or that the hypothesis space does not contain the true model.\n# 4 Sound and Complete Action Models\nIn the previous section we have shown how to compute all solutions of an action model learning problem. Now, we put the focus on learnt models that guarantee some formal property of interest. In particular, we consider soundness 1 and completeness and we show how to manipulate the computed version spaces to build sound and complete action models.\n# Soundness and Completeness for Action Models\nA model M is sound with respect to another model M \u2032 if every transition of M is also a transition of M \u2032. In contrast, a model M is complete with respect to another model M \u2032 if every transition of M \u2032 is also a transition of M.\n \u2032 Definition 7 (Soundness). Let M and M \u2032 be two action models, we say that M is sound with respect to M \u2032 iff TM \u2286TM \u2032. Definition 8 (Completeness). Let M and M \u2032 be two action models, we say that M is complete with respect to M \u2032 iff TM \u2287TM \u2032. By looking at definitions 7 and 8, it is quite obvious that soundness and completeness are opposite properties and this relationship also carries on to solution plans computed with such models. It is easy to see that, if M is sound w.r.t. M \u2032 then any solution plan for a planning problem P = \u27e8M, s0, G\u27e9will also be a solution plan for P \u2032 = 1Soundness has previously been referred to as \u201dsafeness\u201d (Stern and Juba 2017)\n\u27e8M \u2032, s0, G\u27e9, i.e., \u03a0(P) \u2286\u03a0(P \u2032). On the other hand, if M is complete with respect to M \u2032, then all solution plans for P \u2032 = \u27e8M \u2032, s0, G\u27e9will also be valid solutions for P = \u27e8M, s0, G\u27e9, i.e., \u03a0(P) \u2287\u03a0(P \u2032). In the context of action model learning, we use the term sound action model to refer to a learnt model that is sound with respect to the true model A. Similarly, for complete action model. Next, we define sound action model learning and complete action model learning, two specializations of action model learning that consider, respectively, only sound or complete models as solutions. Definition 9 (Sound Action Model Learning Problem). A sound action model learning problem is a tuple \u039bS = \u27e8F, A, D\u27e9where F is a set of fluents, A is a set of actions, and D is a set of demonstrations. A solution for \u039bS is a model M that is sound with respect to all models in MD. Our definition of solution for \u039bS relies on the observation that, while we do not know the true model A, we know that A must belong to the set of models consistent with D. Therefore, to ensure that our solution model is sound with respect to A, it must be sound with respect to all models in MD. The quality of the solution is critical in this problem, since there exist models that are trivially sound. For example, a model M = \u27e8F, A, pre, eff\u27e9with pre(a) = L for all a \u2208A is trivially sound since TM = \u2205but such a model is obviously of little interest. For this reason, given two models M and M \u2032 that solve a sound action model learning problem \u039bS, we say that M is a better solution than M \u2032 if TM \u2283TM \u2032, and say that M is an optimal solution for \u039bS if there exists no other solution M \u2032 such that TM \u2282TM \u2032. Definition 10 (Complete Action Model Learning Problem). A complete action model learning problem is a tuple \u039bC = \u27e8F, A, D\u27e9where F is a set of fluents, A is a set of actions, and D is a set of demonstrations. A solution for \u039bC is a model M that is complete with respect to all models in MD. As before, to guarantee that the solution will be complete with respect to A, it must be complete with respect to any model consistent with D. Regarding solution quality, we say that, given two models M and M \u2032 that solve a complete action model learning problem \u039bC, M is a better solution than M \u2032 if TM \u2283TM \u2032, and M is an optimal solution for \u039bS if there exists no other solution M \u2032 such that TM \u2282TM \u2032.\n# Extracting Sound Models from a Version Space\nIn this section we show that the version spaces computed by VSLAM, and more precisely their lower boundaries LHap and LHa e, can be used to build sound models. Further, such sound models are optimal solutions. Theorem 11. Let \u039bS = \u27e8F, A, D\u27e9be a sound action model learning problem. The action model M = \u27e8F, A, pre, eff\u27e9 such that pre(a) \u2208LHap,Dp and eff(a) \u2208LHa e,De for all a \u2208A is an optimal solution for \u039bS. Proof. Let TMD = \ufffd M \u2032\u2208MD TM \u2032 denote the intersection of all transition systems induced by consistent models MD. We show that TM = TMD. (Soundness) First, we proof that TM \u2286TMD. By contradiction, assume that there exists a transition t = \u27e8s, a, s\u2032\u27e9\nsuch that t \u2208TM but t \u0338\u2208TMD. For t \u0338\u2208TMD to be true, there must exist a model M \u2032 = \u27e8F, A, pre\u2032, eff\u2032\u27e9in MD such that either (1) a is not applicable in s, or (2) its execution results in a state s\u2032\u2032 \u0338= s\u2032. By construction, it holds that pre(a)\u2032 \u2286pre(a). Consequently, if pre(a) \u2286s then pre(a)\u2032 \u2286s and case (1) does not hold. For case (2), we know from Lemma 9 that eff\u2032(a) \\ eff(a) \u2286pre(a) so it follows that eff\u2032(a) \\ eff(a) \u2286s. Meaning that, any difference between eff(a) and eff\u2032(a) is already part of state s and the execution of a cannot result in different states. (Optimality) We prove TM \u2287TMD. For this, simply observe that, by Theorem 1, M \u2208MD. Then, TMD is a subset of any of its intersecting sets including TM. Finally, since TM \u2286TMD and TM \u2287TMD, it follows that TM = TMD and, therefore, M is an optimal solution for \u039bS.\n# Extracting Complete Models from a Version Space\nA complete action model must be able to produce any of the transitions generated by any of the consistent models in MD. Intuitively, the actions of a complete model should be applicable in any state where it is applicable according to a consistent model, and its execution should generate all possible post-states generated under any consistent model. It is easy to see that working under the limits of Definition 1 leads to two problems. First, a model that produces multiple possible post-states is, by definition, nondeterministic. And second, using conjunctive preconditions may easily lead to a weak precondition that accepts more pre-states than necessary. Therefore, we target a more expressive planning model that accommodates disjunctive preconditions and non-deterministic effects. Definition 11 (Non-deterministic Action Model). A nondeterministic action model is a tuple M = \u27e8F, A, Pre, Eff\u27e9 where: \u2022 F and A are finite sets of fluents and actions as given in Definition 1. \u2022 Pre : A \u219222L defines the set of preconditions Pre(a) \u2286 2L of each action a \u2208A. \u2022 Eff : A \u219222L defines the set of effects Eff(a) \u22862L of each action a \u2208A. The main difference with respect to Definition 1 is that here each action is associated to a set of preconditions and a set of effects. A non-deterministic action model M = \u27e8F, A, Pre, Eff\u27e9induces the transition system TM \u2286S \u00d7 A \u00d7 S consisting of all transitions \u27e8s, a, s\u2032\u27e9that satisfy \u2203p \u2208 Pre(a) : p \u2286s and \u2203e \u2208Eff(a) such that s\u2032 = (s \\ e) \u222ae. Using this new formulation, we are able to build a complete action model that is, in addition, optimal. Theorem 12. Let \u039bC = \u27e8F, A, D\u27e9be a complete action model learning problem. The non-deterministic action model M = \u27e8F, A, Pre, Eff\u27e9s.t. Pre(a) = UHa p,Dp and Eff(a) = VHa e,De for all a \u2208A is an optimal solution for \u039bC. Proof. Let TMD = \ufffd M \u2032\u2208M TM \u2032 denote the union of all\n \u2208 The main difference with respect to Definition 1 is that here each action is associated to a set of preconditions and a set of effects. A non-deterministic action model M = \u27e8F, A, Pre, Eff\u27e9induces the transition system TM \u2286S \u00d7 A \u00d7 S consisting of all transitions \u27e8s, a, s\u2032\u27e9that satisfy \u2203p \u2208 Pre(a) : p \u2286s and \u2203e \u2208Eff(a) such that s\u2032 = (s \\ e) \u222ae. Using this new formulation, we are able to build a complete action model that is, in addition, optimal. Theorem 12. Let \u039bC = \u27e8F, A, D\u27e9be a complete action model learning problem. The non-deterministic action model M = \u27e8F, A, Pre, Eff\u27e9s.t. Pre(a) = UHa p,Dp and Eff(a) = VHa e,De for all a \u2208A is an optimal solution for \u039bC. Proof. Let TMD = \ufffd M \u2032\u2208MD TM \u2032 denote the union of all transition systems induced by consistent models MD. We show that TM = TMD.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/296b/296b4bc0-ff25-4f8c-adbd-2c8e25d87e0c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Distinct positive demonstrations collected with the original (top) and modified domains (bottom).</div>\n(Completeness) We start by proving that TM \u2287TMD. By contradiction, assume that there exists a transition t = \u27e8s, a, s\u2032\u27e9such that t \u2208TMD but t \u0338\u2208TM. Then, MD must include a model M \u2032 = \u27e8F, A, pre\u2032, eff\u2032\u27e9such that pre\u2032(a) \u2286s and s\u2032 = (s \\ eff\u2032(a)) \u222aeff\u2032(a), and either (1) \u2200p \u2208Pre(a) : p \u0338\u2286s, or (2) \u2200e \u2208Eff(a) : s\u2032 \u0338= (s \\ e) \u222ae. Since Pre(a) = UHap,Dp and pre\u2032(a) \u2208VHap,Dp, there must exists a p \u2208Pre(a) such that p \u2286pre\u2032(a), and if pre\u2032(a) \u2286s is true then so must be p \u2286s. This falsifies case (1). For case (2), observe that eff\u2032(a) \u2208VHae,De and Eff(a) = VHae,De. Therefore, eff\u2032(a) \u2208Eff(a) and case (2) cannot be true. (Optimality) We prove TM \u2286TMD. By contradiction, assume that t \u2208TM but t \u0338\u2208TMD. Then, it must be true that \u2203p \u2208Pre(a) : p \u2286s and \u2203e \u2208Eff(a) : s\u2032 = (s \\ e) \u222ae and MD cannot contain a model M \u2032 = \u27e8F, A, pre\u2032, eff\u2032\u27e9such that pre\u2032(a) \u2286s and s\u2032 = (s \\ eff\u2032(a)) \u222aeff\u2032(a). However, this cannot be true since Pre(a) \u2286VHap,D and Eff(a) = VHae,D, so MD contains a model M \u2032 with pre\u2032(a) = p and eff\u2032(a) = e. Finally, since TM \u2287TMD and TM \u2286TMD are true, we have that TM = TMD which proves that M is indeed an optimal solution for \u039bC.\n# 5 Experimental Evaluation\nWe evaluate the sound and complete models produced by VSLAM empirically on a selection of planning domains. The code and benchmarks are in the supplementary material and will be made publicly available upon acceptance.\nBenchmarks. We select 4 domains from the International Planning Competition (McDermott 2000), namely BLOCKS, SATELLITE, MICONIC and DRIVERLOG, and use publicly available generators (Seipp, Torralba, and Hoffmann 2022) to get 20 random problems for each domain. We solve all problems with LAMA (Richter and Westphal 2010) and collect each transition induced by the solution plans as a positive demonstration. Negative demonstrations are generated\nby randomly trying to execute actions throughout the states traversed by solution plans. Action model learning is usually performed over a lifted action representation. This assumption restricts the hypothesis space to those fluents that are related to the parameters of the action. To have more challenging benchmarks we modified the original actions by adding extra parameters. This modification enlarges the hypothesis space and so the number of demonstrations. As shown in Figure 2, with this simple modification, we increase by 10 to 30 times the number of distinct demonstrations and reduce the overlap between plans. Note that with the original domains the number of positive demonstrations saturates after the second problem. Table 1 summarizes the features of our benchmarks: the second and the third columns report on the number of lifted actions and fluents and their maximal number of paratemers (in parentheses); the fourth and the fifth columns report on the number of positive (POS) and negative (NEG) demonstrations collected.\n<div style=\"text-align: center;\">DOMAIN</div>\nDOMAIN\nA\nF\nPOS\nNEG\nBLOCKS\n4 (3)\n5 (2)\n66\n882\nDRIVERLOG\n6 (8)\n6 (2)\n169\n903\nMICONIC\n4 (4)\n8 (2)\n138\n381\nSATELLITE\n5 (8)\n8 (2)\n64\n166\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78c1/78c1f548-aaac-417c-b8af-f27c667b894a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 1: Domains, features and collected demonstrations.</div>\nTable 1: Domains, features and collected demonstrations.\nEvaluation criteria. Following previous works on action model learning (Lamanna et al. 2021; Aineto, Celorrio, and Onaindia 2019), we use the f1-score, the harmonic mean of precision and recall. We interpret these metrics over a test set of demonstrations by having the learnt action model label them as positive or negative with respect to its transition system. Precision degrades with false positives, i.e., when the model accepts a negative demonstration as part of its transition system. Conversely, recall degrades with false negatives, i.e., when the model rejects a positive demonstration. Indeed, a sound action model will always have perfect precision but lower recall, meaning that it accepts few transitions but all of them are correct. In contrast, a complete model will have perfect recall yet low precision, since it will often accept transitions that do not belong to the true model. The f1-score provides us with a very good proxy for understanding the usability of our models. We split the collected demonstrations, using half for learning and half for testing, and measure the f1-score of both models as more demonstrations are processed. The complete model is evaluated simulating scenarios where negative demonstrations are seen at different rates. We do this by using a ratio r of negative to positive demonstrations. For instance, a ratio r = 2 indicates that the demonstrations set D contains 2 negative demonstrations for every positive one. With this setting, we aim at understanding the impact of the distribution of our dataset. Results. Figure 3 illustrates our results; the x-axis reports on the number of growing positive demonstrations. For each such a positive demonstration there is a number of negative ones depending on ratio r; we represent such an information with different curves. We observe that the sound\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/587d/587da4da-8870-4140-b6b6-64b2204626d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: F1-score (y-axis) of the sound and complete action models as the training demonstra different positive vs negative ratios</div>\naction model performs better in more imbalanced domains (in terms of positive and negative demonstrations) such as BLOCKS. Instead, the complete model seems to be effective over more balanced domains like MICONIC and SATELLITE. This comes with no surprise since more imbalanced distributions correlate to stricter preconditions and such models are closer (in the hypothesis space) to the sound model. The opposite holds true for the complete model. Generally, the complete model has the advantage in the earlier stages of the learning process, but in our experiment is later outperformed by the sound model after more demonstrations have been processed. Overall, no model dominates the other and their relative performance depends on the characteristics of the domain, how far into the learning we are and how accessible the positive and negative demonstrations are. This indicates a strong complementarity of the two models.\n# 6 Related Work\nResearch on action model learning has produced a wide variety of sophisticated learning approaches \u2013 different surveys can be found in Jim\u00b4enez et al. (2012); Arora et al. (2018); Aineto, Jim\u00b4enez, and Onaindia (2022). Starting with the pioneering works of ARMS (Yang, Wu, and Jiang 2007) and SLAF (Amir and Chang 2008), research in this field has been quite prolific. Approaches like LAMP (Zhuo et al. 2010) investigated the learning of more expressive action models, while others like FAMA (Aineto, Celorrio, and Onaindia 2019) and AMAN (Zhuo and Kambhampati 2013) focused on learning from incomplete or noisy demonstrations. We can even find approaches that actively seek the demonstrations that will help them learn faster (Lamanna et al. 2021; Verma, Marpally, and Srivastava 2021). Broadly, most learning approaches can be classified, according to their notion of solution, into those that compute a model consistent with the demonstrations (Cresswell, McCluskey, and West 2013; Aineto, Celorrio, and Onaindia 2019; Bonet and Geffner 2020), or those that target the action model that maximizes some objective or fitness function (Yang, Wu, and Jiang 2007; Ku\u02c7cera and Bart\u00b4ak 2018; Zhuo et al. 2010; Zhuo and Kambhampati 2013; Mourao et al. 2012). While we follow the former interpretation, our approach is one of the few, alongside SLAF (Amir and Chang 2008), which is able to compute all solutions to the prob-\nlem. SLAF computes a CNF formula representing all possible transitions that can be regarded as a form of version space. However, this formula does not offer the compactness of our boundaries nor can be easily manipulated and, indeed, the only way to extract from it a concrete solution model is using a SAT solver. Unlike us, SLAF handles partial observability, a feature that we expect to support in the future following similar extensions for version spaces. The approach that we regard as the closest to our own is SAM (Stern and Juba 2017) for its focus on safe (sound) models, learning setting and, interestingly, a foundational connection. The authors of SAM link their approach to Valiant\u2019s elimination algorithm (Valiant 1984) and Mitchel (1982) himself describes this algorithm as the subproblem of computing the L boundary in version space learning. Our work refreshes this connection in an action model learning setting. Indeed, SAM focuses on one extreme of the spectrum of solution models, those guaranteeing soundness, which are tied to the L boundary. On the other extreme we find the complete models that we highlight in this work. Whether there are other interesting models in the middle between these two extremes is object of future work.\n# 7 Conclusions and Future Work\nIn this paper we have proposed an approach to learning action models from first principles. We do so by exploiting version spaces to a great extent. One of the main benefits of our approach is the ability to learn in an integrated and comprehensive way sound models as for Stern and Juba (2017) together with complete models, providing therefore a great deal of flexibility. Indeed, our framework enables an agent not only to learn from positive demonstrations but also from failures. Empirically, we observed that with this facility in place, an agent can start learning something useful for reasoning already with a few number of examples. There are a number of future works in our research agenda. First, we aim at lifting, or at least relaxing, some of our assumptions. In this regard, we want to investigate version space algebra (Lau et al. 2003) as a means to tackle more expressive action models. Indeed, version space algebra allows to operate over much more complex hypothesis spaces, providing a solid foundation to target action models involving, e.g., numeric state variables (Fox and Long\n2003); of importance in this context is the relationship with what was done by Mordoch, Juba, and Stern (2023). Second, we plan to support noisy demonstrations following the steps taken by Norton and Hirsh (1992) for version spaces. Last but not least, we would like to investigate the role of version spaces in an active-learning approach. Speculatively, the region between the boundaries of the version space is where uncertainty resides so focusing our attention in this area should accelerate convergence towards the true model.\n# References\nAineto, D.; Celorrio, S. J.; and Onaindia, E. 2019. Learning action models with minimal observability. Artificial Intelligence, 275: 104\u2013137. Aineto, D.; Jim\u00b4enez, S.; and Onaindia, E. 2022. A comprehensive framework for learning declarative action models. Journal of Artificial Intelligence Research, 74: 1091\u20131123. Amir, E.; and Chang, A. 2008. Learning partially observable deterministic action models. Journal of Artificial Intelligence Research, 33: 349\u2013402. Arora, A.; Fiorino, H.; Pellier, D.; M\u00b4etivier, M.; and Pesty, S. 2018. A review of learning planning action models. The Knowledge Engineering Review, 33: e20. Bonet, B.; and Geffner, H. 2020. Learning First-Order Symbolic Representations for Planning from the Structure of the State Space. In ECAI 2020 - 24th European Conference on Artificial Intelligence, volume 325, 2322\u20132329. Cresswell, S. N.; McCluskey, T. L.; and West, M. M. 2013. Acquiring planning domain models using LOCM. The Knowledge Engineering Review, 28(2): 195\u2013213. Fox, M.; and Long, D. 2003. PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains. J. Artif. Intell. Res., 20: 61\u2013124. Ghallab, M.; Nau, D.; and Traverso, P. 2004. Automated Planning: theory and practice. Elsevier. Jim\u00b4enez, S.; De La Rosa, T.; Fern\u00b4andez, S.; Fern\u00b4andez, F.; and Borrajo, D. 2012. A review of machine learning for automated planning. The Knowledge Engineering Review, 27(4): 433\u2013467. Juba, B.; Le, H. S.; and Stern, R. 2021. Safe Learning of Lifted Action Models. In Proceedings of the 18th International Conference on Principles of Knowledge Representation and Reasoning, 379\u2013389. Juba, B.; and Stern, R. 2022. Learning Probably Approximately Complete and Safe Action Models for Stochastic Worlds. In Thirty-Sixth AAAI Conference on Artificial Intelligence, 9795\u20139804. Kambhampati, S. 2007. Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models. In Proceedings of the National Conference on Artificial Intelligence, volume 22, 1601. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.\nKu\u02c7cera, J.; and Bart\u00b4ak, R. 2018. LOUGA: learning planning operators using genetic algorithms. In Knowledge Management and Acquisition for Intelligent Systems: 15th Pacific Rim Knowledge Acquisition Workshop, 124\u2013138. Springer. Lamanna, L.; Saetti, A.; Serafini, L.; Gerevini, A.; and Traverso, P. 2021. Online Learning of Action Models for PDDL Planning. In Zhou, Z., ed., Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, 4112\u20134118. ijcai.org. Lau, T.; Wolfman, S. A.; Domingos, P.; and Weld, D. S. 2003. Programming by demonstration using version space algebra. Machine Learning, 53: 111\u2013156. Lau, T. A.; Domingos, P. M.; and Weld, D. S. 2000. Version Space Algebra and its Application to Programming by Demonstration. In ICML, 527\u2013534. McDermott, D.; et al. 1998. The planning domain definition language manual. Technical report, Technical Report 1165, Yale Computer Science, 1998.(CVC Report 98-003). McDermott, D. M. 2000. The 1998 AI planning systems competition. AI magazine, 21(2): 35\u201335. Mitchell, T. M. 1982. Generalization as search. Artificial intelligence, 18(2): 203\u2013226. Mordoch, A.; Juba, B.; and Stern, R. 2023. Learning Safe Numeric Action Models. In Thirty-Seventh AAAI Conference on Artificial Intelligence, 12079\u201312086. Mourao, K.; Zettlemoyer, L. S.; Petrick, R.; and Steedman, M. 2012. Learning STRIPS operators from noisy and incomplete observations. arXiv preprint arXiv:1210.4889. Norton, S. W.; and Hirsh, H. 1992. Classifier Learning from Noisy Data as Probabilistic Evidence Combination. In Proceedings of the 10th National Conference on Artificial Intelligence, 141\u2013146. Richter, S.; and Westphal, M. 2010. The LAMA planner: Guiding cost-based anytime planning with landmarks. Journal of Artificial Intelligence Research, 39: 127\u2013177. Seipp, J.; Torralba, \u00b4A.; and Hoffmann, J. 2022. PDDL Generators. https://doi.org/10.5281/zenodo.6382173. Stern, R.; and Juba, B. 2017. Efficient, Safe, and Probably Approximately Complete Learning of Action Models. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, 4405\u20134411. Valiant, L. G. 1984. A theory of the learnable. Communications of the ACM, 27(11): 1134\u20131142. Verma, P.; Marpally, S. R.; and Srivastava, S. 2021. Asking the right questions: Learning interpretable action models through query answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 12024\u201312033. Yang, Q.; Wu, K.; and Jiang, Y. 2007. Learning action models from plan examples using weighted MAX-SAT. Artificial Intelligence, 171(2-3): 107\u2013143. Zhuo, H. H.; and Kambhampati, S. 2013. Action-model acquisition from noisy plan traces. In Twenty-Third International Joint Conference on Artificial Intelligence.\nZhuo, H. H.; Yang, Q.; Hu, D. H.; and Li, L. 2010. Learning complex action models with quantifiers and logical implications. Artificial Intelligence, 174(18): 1540\u20131569.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of action model learning in fully observable settings, which is crucial for automating the engineering of action models that are essential for model-based reasoning in AI planning.",
        "problem": {
            "definition": "The problem is to compute an approximation of a domain's dynamics from demonstrations, specifically focusing on learning deterministic action models with conjunctive preconditions.",
            "key obstacle": "The main challenge is the complexity of learning accurate action models from potentially incomplete or noisy demonstrations."
        },
        "idea": {
            "intuition": "The idea was inspired by the learning by search paradigm, where the task is interpreted as a search for hypotheses consistent with learning examples.",
            "opinion": "The authors propose a theoretical framework that utilizes version spaces to learn action preconditions and effects efficiently.",
            "innovation": "The main difference compared to previous methods is the introduction of a version space representation that allows for compact storage and manipulation of all potential solutions."
        },
        "Theory": {
            "perspective": "The theoretical perspective is grounded in version spaces, which provide a structured way to represent hypotheses consistent with the learning examples.",
            "opinion": "The authors assume that the true action model complies with the defined structure of action models and can be learned through sufficient demonstrations.",
            "proof": "The paper provides a proof that the action model built using the learned version spaces satisfies the necessary conditions to be a solution to the action model learning problem."
        },
        "experiments": {
            "evaluation setting": "The experiments are conducted over four planning domains: BLOCKS, SATELLITE, MICONIC, and DRIVERLOG, using a set of positive and negative demonstrations generated from solution plans.",
            "evaluation method": "The performance is evaluated using the F1-score, which measures the precision and recall of the learned action models against a test set of demonstrations."
        },
        "conclusion": "The findings demonstrate that the proposed framework effectively learns sound and complete action models, improving the reasoning capabilities of agents with fewer demonstrations.",
        "discussion": {
            "advantage": "The main advantage of this paper is the ability to learn both sound and complete models, providing flexibility in different planning scenarios.",
            "limitation": "A limitation is that the initial assumptions may restrict the expressiveness of the learned models, particularly in more complex domains.",
            "future work": "Future work includes relaxing some assumptions, exploring version space algebra for more expressive action models, and supporting noisy demonstrations."
        },
        "other info": [
            {
                "info1": "The proposed algorithm, VSLAM, efficiently maintains a version space and updates it with new demonstrations."
            },
            {
                "info2": {
                    "info2.1": "The approach allows agents to learn from both positive and negative demonstrations.",
                    "info2.2": "The framework emphasizes the importance of learning sound models to ensure safety in action execution."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of action model learning in fully observable settings, which is crucial for automating the engineering of action models that are essential for model-based reasoning in AI planning."
        },
        {
            "section number": "1.2",
            "key information": "The problem is to compute an approximation of a domain's dynamics from demonstrations, specifically focusing on learning deterministic action models with conjunctive preconditions."
        },
        {
            "section number": "2.1",
            "key information": "The theoretical perspective is grounded in version spaces, which provide a structured way to represent hypotheses consistent with the learning examples."
        },
        {
            "section number": "3.1",
            "key information": "The proposed algorithm, VSLAM, efficiently maintains a version space and updates it with new demonstrations."
        },
        {
            "section number": "4.1",
            "key information": "The approach allows agents to learn from both positive and negative demonstrations."
        },
        {
            "section number": "6.1",
            "key information": "A limitation is that the initial assumptions may restrict the expressiveness of the learned models, particularly in more complex domains."
        },
        {
            "section number": "6.3",
            "key information": "The findings demonstrate that the proposed framework effectively learns sound and complete action models, improving the reasoning capabilities of agents with fewer demonstrations."
        },
        {
            "section number": "7",
            "key information": "Future work includes relaxing some assumptions, exploring version space algebra for more expressive action models, and supporting noisy demonstrations."
        }
    ],
    "similarity_score": 0.5470594453810214,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0239_large/papers/Action Model Learning with Guarantees.json"
}