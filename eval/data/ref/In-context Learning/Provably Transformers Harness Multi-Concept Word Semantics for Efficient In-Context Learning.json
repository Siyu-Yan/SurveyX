{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.02199",
    "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning",
    "abstract": "Transformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs\u2019 impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a linear regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios involving linear transformers or unrealistic loss functions, and they achieve only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for certain unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the linear latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLUactivated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings.",
    "bib_name": "bu2024provablytransformersharnessmulticoncept",
    "md_text": "# Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning\n# Dake Bu1, Wei Huang2\u2217, Andi Han2, Atsushi Nitanda3,4, Taiji Suzuki5,2, Qingfu Zhang1, Hau-San Wong1\u2217\n1Department of Computer Science, City University of Hong Kong, Hong Kong SAR 2Center for Advanced Intelligence Project, RIKEN, Japan 3CFAR and IHPC, Agency for Science, Technology and Research (A\u22c6STAR), Singapore College of Computing and Data Science, Nanyang Technological University, Singapore 5Department of Mathematical Informatics, the University of Tokyo, Japan dakebu2-c@my.cityu.edu.hk, {wei.huang.vr, andi.han}@riken.jp, atsushi_nitanda@cfar.a-star.edu.sg, taiji@mist.i.u-tokyo.ac.jp, {qingfu.zhang, cshswong}@cityu.edu.hk\n# Abstract\nTransformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs\u2019 impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a linear regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios involving linear transformers or unrealistic loss functions, and they achieve only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for certain unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the linear latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLUactivated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings.\n# 1 Introduction\nRecently, a variety of transformer-based large language models (LLMs) have demonstrated rema able performance across a broad spectrum of machine learning tasks, including natural langua understanding [1], symbolic reasoning [2], and even heuristics design [3, 4]. One crucial emergi ability of these models is their in-context learning (ICL) capacity [5], which allows them to lea from a few demonstrations and conduct predictions on new queries without requiring any furth\n\u2217Corresponding authors\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nfine-tuning. However, the current theoretical understanding of the mechanisms underlying this ICL capability remains limited, leaving the reasons for the remarkable emergence and generalization power of transformer-based LLMs in unseen ICL tasks largely unexplained. In line with traditional topic models [6], [7, 8] propose that latent concepts / topics underlie natural texts, providing a Bayesian inference framework to elucidate the ICL mechanism via Bayesian Model Averaging (BMA) approach. On the other hand, theoretical and empirical studies have shown that transformer-based models exhibit linear geometric regularities in their latent representations as a result of concept or topic learning [9, 10], where the representations within-concept have positive inner products while representations cross-concepts exhibit near-orthogonal relationships. This structured semantic geometry has been well-documented in recent research on pre-trained LLMs [11, 12, 10, 13]. However, the connection between this observed multi-concepts latent geometric structure and the LMs\u2019 remarkable ICL capabilities remains unclear. Separately, recent theoretical analyses have modeled ICL as a martingale process driven by latent \u201cconcept\u201d variables [14, 15]. Yet, these studies have not incorporated the observed multi-concept semantic regularity into their analyses, nor have they discussed the strong out-of-distribution (OOD) ICL abilities exhibited by transformers. Additionally, existing theoretical work on transformer has been conducted on unrealistic, oversimplified settings, such as linear or ReLU transformers [16, 17, 18, 19], MLP-free attention-only models [16, 20], QK-combined softmax attention [19, 20, 21, 22, 23], unrealistic infinite dimensional assumption [14, 19, 21, 24] and impractical loss functions like square loss [9, 16, 25, 20, 26] and hinge loss [27, 28]. Furthermore, existing works have only been able to derive linear or sub-linear convergence rates for the 0-1 loss. Therefore, there is a need for a more advanced analysis that can bridge the understanding between the multi-concept semantic regularity and the mechanisms underlying transformer-based ICL. This naturally leads to the research question:\nWhether and how do the geometric regularity of the multi-concept-encoded representation facilitate transformer in conducting efficient ICL?\nTo answer the above question, following the meaningful data modeling ideas in [9, 29], we conduct theoretical analysis on a concept-specific sparse coding prompt distribution for classification tasks, where the sparse latent variable encodes the information denoting the word\u2019s belonging concept. Importantly, the features in both the word\u2019s and label\u2019s dictionaries exhibit concept-specific geometric properties - within-concept positive inner products and cross-concept orthogonal geometric properties - that aligns with the findings in [9, 10, 11]. Our main contributions are highlighted as below.\n# 2 Related Work\nTheory of Exponential Convergence Rate of Stochastic Gradient Descent. Our analysis of the exponential convergence rate for the 0-1 loss builds upon prior work linking the excess risk and essential supremum norm to exponentially fast convergence under the \u201chard low-noise condition\u201d [31, 32]. This phenomenon has been further explored in more recent studies analyzing the exponential convergence of stochastic gradient descent (SGD) [33, 34, 35, 36, 37], as well as in more generalized settings such as multiclass classification [38] and support vector machines [39]. Feature Learning in Learning Theory. Recent works in learning theory have extensively studied structured data from a feature learning perspective, examining NN\u2019s feature direction reconstruction and noise memorization as a proxy for training or 0-1 loss convergence [40, 41, 42]. While prior studies often assumed orthogonal features, recent efforts have analyzed non-orthogonal scenarios [43, 44]. Our work extends this line-of-research to challenging nonlinear Attention-MLP transformers with non-orthogonal structured data representations. Theory of Transformers and In-Context Learning The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics [9], the origins and biases of LLM representations using latent variable models [10], and ICL from a model averaging perspective [14]. However, albeit incorporating concept variables, these works do not connect the geometric properties of conceptencoded representations to transformers\u2019 powerful ICL abilities. Another line of research has studied the learning dynamics of ICL, including analyses of linear transformers [17, 19], QK-combined attention-only models [45], and multi-head softmax attention over linear regression without MLP [25]. Though relevant, these works rely on simplifications and do not notice the connection between semantic regularity and powerful ICL. While [28] also analyzes the learning dynamics of transformers with softmax attention and ReLU MLPs for in-context classification tasks, making it the most relevant prior work, our analysis differs in several key aspects. Specifically, (i) they consider orthogonal dictionary learning with a single label vector, in contrast to our non-orthogonal concept-encoded dictionaries for both words and labels; (ii) their technique requires a large batch size (at least \u03b5\u22122, where \u03b5 is the test error) and long context lengths, which are not required in our result; and (iii) they utilize an impractical hinge loss and only achieve linear convergence without a relation to \u03b5, whereas we analyze the more practical cross-entropy loss and derive an exponential convergence rate in terms of the test error \u03b5. However, we note that this is only an informal comparison due to the differences in the models and primary findings. A detailed Related Work Section is deferred to Appendix C.\n# 3 Problem Setup\nNotations. For l2 and Frobenius norms we utilize \u2225\u00b7 \u2225and \u2225\u00b7 \u2225F to denote their computations. Considering two series an and bn, we denote an = O (bn) if there exists positive constant C > 0 and N > 0 such that for all n \u2265N, |an| \u2264C |bn|. Similarly, we denote an = \u2126(bn) if bn = O (an) holds, and an = \u0398 (bn) if an = O (bn) and an = \u2126(bn) both hold. Our 1(\u00b7) is to denote the indicator variable of an event. In addition, we denote span(v1, v2, . . . , vk) as the linear subspace spanned by the vectors v1, v2, . . . , vk, and conic(v1, v2, . . . , vk) denotes the conic hull (the set of all non-negative linear combinations) of the vectors v1, v2, . . . , vk.\n# 3.1 Data Distribution\nThe data distribution employed in this study draws inspiration from a range of empirical and theoretical research works [9, 10, 46, 47, 48]. This distribution captures context-awareness and can be viewed as a specialized prompt version of PLSA [49] and LDA [6]. In this distribution, each word and label has multiple feature embeddings, each embedding corresponding to a different concept. This is achieved through the use of a sparse latent concept/topic variable, which happened to be particularly adept at representing language polysemy [47]. Adhering to the LLM representation explored in [9, 10], the features in both the word and label dictionaries maintain orthogonality across concepts and positive inner products within concepts. Additionally, the distribution incorporates Gaussian noise accounting for linguistic ambiguity or the imperfection of the LLM\u2019s representation. Definition 1. Polysemous Word Model (Dx, Dy, Dz, D\u03bex, D\u03bey). We assume there exists K1 taskrelevant concepts, each characterized by two semantically-opposite word\u2019s feature vectors \u00b5+ k1 and\n\u00b5\u2212 k1, and their corresponding label\u2019s feature vectors q+ k1 and q\u2212 k1, \u2200k1 \u2208[K1]. There are also K2 task-irrelevant concepts denoted by \u03bdk2, \u2200k2 \u2208[K2]. The word samples x \u2208RdX and their labels y \u2208RdY are generated from distributions parameterized by a shared latent concept variable z = (z1, \u00b7 \u00b7 \u00b7 , zK) \u2208{0, 1}K(K < dX ) capturing the concept-specific information:\n \u223cD  \u223cD where the feature dictionary M = [\u00b5+ 1 , \u00b5\u2212 1 , \u00b5+ 2 , \u00b5\u2212 2 , \u00b7 \u00b7 \u00b7 , \u00b5+ K1, \u00b5\u2212 K1, \u03bd1, \u03bd2, \u00b7 \u00b7 \u00b7 , \u03bdK2] \u2208RdX \u00d7K exhibits positive inner products within concepts and orthogonality across concepts, and the label dictionary Q = [q+ 1 , q\u2212 1 , q+ 2 , q\u2212 2 , \u00b7 \u00b7 \u00b7 , q+ K1, q\u2212 K1, 0, \u00b7 \u00b7 \u00b7 0] \u2208RdY\u00d7K has similar geometric properties. Specifically, we have \u2200k1 \u2208[K1], k2 \u2208[K2], \u2225\u00b5\u00b1 k1\u2225= \u2225\u03bdk2\u2225= \u2225u\u2225, \u2225q\u00b1 k1\u2225= \u2225q\u2225, and there exist constants 0 < \u03bax, \u03bay < 1 such that 0 < \u27e8\u00b5+ k1, \u00b5\u2212 k1\u27e9\u2264\u03bax\u2225u\u22252 and 0 < \u27e8q+ k1, q\u2212 k1\u27e9\u2264\u03bay\u2225q\u22252. The detailed formal definition can be found in Appendix E. By this definition, a single word or label can possess different features corresponds to different concepts. The illustration of Figure 1 in [12] can be an example, where the \u201cDog\u201d vector in the representation space of LLM is decomposed to a direct sum of orthogonal vectors: \u201c[Animal] + [Mammal] + \u00b7 \u00b7 \u00b7 \u201d, and we can see \u201c[Animal]\u201d belongs to the concept \u201cOrganism\u2019s Category\u201d categorized into labels \u201c[Animal]\u201d and \u201c[Plant]\u201d, and \u201c[Mammal]\u201d belongs to the concept of \u201cAnimal\u2019s Category\u201d characterized by labels \u201c[Mammal]\u201d, \u201c[Fish]\u201d, \u201c[Bird]\u201d, \u201c[Reptile]\u201d. Besides, Figure 1 in [46] can also be a good support for our modeling, where \u201cFerrari\u201d vector consists of \u201c[Cars] + [Italian] + \u00b7 \u00b7 \u00b7 \u201d. The following definition models the contextual prompts via specifying the statistical property of z among in-context words, which is a special prompt version of PLSA [49] and LDA [6]. The detailed formal version is available in Appendix E. Definition 2. Concept-specific Contextual Prompt Distribution2. During training, each prompt sample S = x1, y1, \u00b7 \u00b7 \u00b7 , xL, yL, xL+1 would share at least one co-concept, which is drawn from a mixture distribution DS defined as:\n\ufffd \ufffd \ufffd where P\u00b1 k,L+1 denotes the k-th concept-specific prompt distribution, and \u03c0\u00b1 k = (2K1)\u22121 denotes the equal chance of a sample to belong to P\u00b1 k,L+1. Specifically, a sample Sn \u223cPe k,L+1, e \u2208[\u00b1] means that the query\u2019s label yn L+1 is qe k, and we denote ySn := e as the real value label of this prompt. In addition, every demonstration pairs (xn l , yn l ), l \u2208[L] in Pe k,L+1 contain either (\u00b5+ k , q+ k ) or (\u00b5\u2212 k , q\u2212 k ) with equal chance. Also, every zn l , l \u2208[L + 1] would satisfy P(zn l,\u00ac(2k\u22121\u22282k) = 1) = K\u22121, denoting the equal chance to have diverse features other than the current co-concept of the Pe k,L+1. This definition suggests that for prompt S sampling from DS, there exists e \u2208[\u00b1], k \u2208[K1], such that all the word-label pairs in this prompt share the k-th concept as their co-concept, and the corresponding real value label of the query in this prompt is e. Besides, the real value label of each word-label pair in the demonstration would have equal chance to be +1 or \u22121.\n# 3.2 Transformer Model\n\u00b7 H = E(S) = \ufffd x1 x2 \u00b7 \u00b7 \u00b7 xL xquery y1 y2 \u00b7 \u00b7 \u00b7 yL 0 \ufffd := (h1, h2, \u00b7 \u00b7 \u00b7 , hquery ) \u2208R(dX +dY)\u00d7(L+1),\n\ufffd \u00b7 \u00b7 \u00b7 \ufffd The learning model is a single-head, one-layer Transformer with one self-attention layer and one two-layer perceptron. Mathematically, it can be expressed as follows: f(H; \u03a8) = r\u22a4\u03c3R (WO attn(H; \u03a8)) ,\nattn(H; \u03a8) = L \ufffd l=1 WV hl\u03c3S \ufffd (WKhl)\u22a4WQhquery \ufffd ,\n(1)\nwhere \u03c3R(\u00b7) := Relu(\u00b7), \u03c3S(\u00b7) := softmax(\u00b7), WQ, WK \u2208Rmqk\u00d7(dX +dY), WV \u2208Rmv\u00d7(dX +dY) are the embedding matrices for queries, keys, and values, respectively, and WO \u2208Rm\u00d7mv and r \u2208Rm are parameters in the MLP layer. Typically, min (mqk, mv) \u2265dX + dY. \u03a8 := {WQ, WK, WV , WO, r} denotes the set of all model weights.\nwhere Wx Q, Wx K \u2208RdX \u00d7dX , Wy V \u2208R(mv\u2212dX )\u00d7dY, Wy O \u2208Rm\u00d7dY. Here, we set the elements other than Wx Q, Wx K, Wy V and Wy O to be zero. Besides, we fix Wy V to be I(mv\u2212dX )\u00d7dY. We sample ri from a uniform distribution Unif{\u22121, 1} and fixed during the training process. Based on this setting, the trainable part we need to consider is actually \u03a8\u2032 := \ufffd Wx Q, Wx K, Wy O \ufffd . This problem remains highly non-convex and challenging. We utilize mini-batch with-replacement SGD to train the transformer model. The empirical crossentropy loss for each batch Bt is written as\nwhere \u2113(z) = log(1+exp(\u2212z)), ySn is the real value label of the prompt defined in Definition 2, and the term \u2225\u03a8\u2032\u22252 F represents \u2225Wx Q\u22252 F + \u2225Wx K\u22252 F + \u2225Wy O\u22252 F , which is the L2 regularization term with \u2225\u00b7 \u2225F denoted as the Frobenius norm. The purpose of the regularization in this paper is to accelerate and stabilize the mini-batch with-replacement SGD. The learning step is set to be \u03b7t = 2 \u03bb(\u03b3+t), where \u03b3 is an offset parameter. This decaying schedule is standard and also used in prior work [34, 50, 51] studying convergence of SGD. The whole procedure is in Algorithm 1. Initialization Setting. All initial values of Wy O are sampled from a i.i.d. Gaussian distributions with mean 0 and variance \u03c32 1. The initialization of Wx Q and Wx K are diagonal matrices \u03c30I, which are also adopted in other work that consider training WQ and WK separately [25, 28]. Testing Setting. The model performance is measured by 0-1 test error on a test prompt distribution D\u2217:\nAlgorithm 1 Training algorithm\nInput: Training distribution DS, Test distribution D\u2217, Batch size B, step size \u03b7t =\n2\n\u03bb(\u03b3+t),\nstopping criterion \u03b5 and total epochs T.\nInitialize model parameters \u03a8\u2032(0).\nfor t = 0, 1, . . . , T \u22121 do\nIf L0\u22121\nD\u2217(\u03a8(t)) \u2264\u03b5 stop else continue.\nRandomly sample mini batches Bt of size B from DS.\nUpdate model parameters: \u03a8\u2032(t+1) = \u03a8\u2032(t) \u2212\u03b7t\u2207\u03a8\u2032LBt(\u03a8\u2032(t)).\nend for\n# 4 Theoretical Results\nIn this section, we present our main theoretical results, which is based on the following conditions. We consider the learning iterations 0 \u2264t \u2264T \u2217, where T \u2217= \u2126(m\u22121\u03c3\u22121 0 \u03c3\u22121 1 m\u03bb\u22122K1\u2225q\u22252((L \u2212 1)\u2225u\u22252 + 1) log(\u03b5\u22121)) denotes the maximum admissible iteration. Condition 1. Suppose that there exists a sufficiently large constant C, such that the following hold: 1. dX , dY \u2265max{C log(KLBT \u2217/\u03b4), K}, dY \u2265C log(m/\u03b4), m \u2265C log(K/\u03b4).\n(2)\n# 2. \u03b3 \u2265C max{\u2225q\u22252/(mK1\u03bb), 10/\u03bb}, \u03bb \u2264min{(C log(Km/\u03b4)\u2225q\u2225)\u22121, (C\u03c30/2\u2225u\u22252)\u22121} 3. K \u2265{CK1, C\u2225u\u2225/(\u03c3\u03be \u221adX )}. 4. \u03c3\u03be \u2264min{\u03bbm/(C\u221adX \u2225u\u2225\u2225q\u22251/2), \u2225q\u2225/(C \ufffd dY)}.\n# 2. \u03b3 \u2265C max{\u2225q\u22252/(mK1\u03bb), 10/\u03bb}, \u03bb \u2264min{(C log(Km/\u03b4)\u2225q\u2225)\u22121, (C\u03c30/2\u2225u\u22252)\u22121} 3. K \u2265{CK1, C\u2225u\u2225/(\u03c3\u03be \u221adX )}. \u221a \ufffd\n\ufffd 5. \u03c30 \u2264 \ufffd K\u22121 log( \u2225u\u22252 \u03bbK1 log( \u2225q\u22252 m\u03bbK1 ))/(C\u2225u\u2225), \u03c31 \u2264min{(C\u03c30\u2225u\u22254\u2225q\u2225 \ufffd log(5Km/\u03b4)/K1)\u22121, w\u22172/(Cm3/2\u2225q\u2225)}.\n\ufffd Here, w\u2217= 1 \u2212e\u2212\u03c30 2(1\u2212\u03bax)2\u2225u\u22254/2 1 + e\u2212\u03c302(1\u2212\u03bax)2\u2225u\u22254/2 .\nNote that we do not have any requirement upon demonstration length L and batch size B for training, thus the training can be really flexible compared with the strict requirement in [28]. The condition on dimensionality dX , dY and the network width m ensure the learning problem is in a sufficiently overparameterized setting [41, 42, 52, 43]. The condition on \u03b3 ensures the learning step to be small and thus learning process enjoys an approximation to gradient flow. The condition on the small \u03bb is to ensure the model\u2019s sufficient learning before being stuck by regularization [53]. The condition on K is to control the impact of cross-concept contribution in the Attention\u2019s learning dynamic, which can actually be relaxed at the cost of a denser analysis. The condition on \u03c3\u03be is to ensure that the gradient flows be mildly influenced by the noise. Last but not least, the conditions on \u03c31 guarantee that the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. A more detailed discussion over the parameter settings is delayed to Appendix H. Theorem 2. Exponential Convergence of 0-1 loss. Under Condition 1, define\nThus after\nT\u03b5 = K1\u2225q\u22252((L \u22121)\u2225u\u22252 + 1) C2\u03bd2m\u03bb2 log(1 \u03b5)\niterations, we have L0\u22121 D\u2217(\u03a8(T )) \u2264\u03b5.\nNote that the bound is valid only when T \u2265\u02c6T, a common threshold in prior convergence rate analyses [34, 33, 36]. Importantly, the existence of \u02c6T does not affect the convergence rate as \u03b5 \u21920, since \u02c6T is independent of \u03b5. Our novel analysis generalizes these prior results to our realistic settings handling the challenges of self-attention, ReLU-MLP, and cross-entropy loss simultaneously. By considering extreme cases, our techniques relax the batch size requirement, enabling more general results. Consequently, the sample complexity for Bayes-optimal test error is N = T\u03b5. Before introducing the next proposition, we highlight a key observation from the semantic geometry in Definition 1. For any k1 \u2208[K1], defining ak1 := (\u00b5+ k1 + \u00b5\u2212 k1)/2 and bk1 := (\u00b5+ k1 \u2212\u00b5\u2212 k1)/2, we find that for k\u2032 1 \u0338= k1, {ak1, bk1} \u22a5{ak\u2032 1, bk\u2032 1} and \u27e8ak1, bk1\u27e9= 0. This structure is exemplified in Figure 1(b) of [12], where \u201c[Bird]\u201d consists of orthogonal steering vectors: \u201cplant \u21d2animal\u201d and \u201cmammal \u21d2bird,\u201d corresponding to the concept feature ak and semantic label features bk. Here, the term ebk1 in \u00b5e k1 determines the label assignment. Similarly, defining ck1 := (q+ k1 + q\u2212 k1)/2 and dk1 := (q+ k1 \u2212q\u2212 k1)/2 yields analogous properties. Detailed definitions are provided in Appendix I. The following proposition explores the model\u2019s ability to handle OOD unseen ICL tasks. Proposition 1. Out-of-Distribution-Generalization3. During testing, the learned model admits probability distribution shift on D\u2217 z and data shift on D\u2217 x \u00d7 D\u2217 y to generate a new prompt distribution\n  \u2022 The prompt length L\u2217can be any positive integer.\n\u2022 D\u2217 z can enjoy arbitrary distribution, satisfying that each prompt has at least one co-concept k \u2208[K1], at least one pair shares the query word\u2019s co-concept\u2019s label, and still each word has equal chance to have positive or negative semantic labels over its concepts4. \u2022 D\u2217 x\u00d7D\u2217 y can enjoy a great family of data shift. \u2200k \u0338= k\u2032 \u2208[K1], k2 \u2208[K2], we can have new M\u2217and Q\u2217such that \u00b5\u00b1 k \u2217= a\u2217 k\u00b1b\u2217 k, q\u00b1 k \u2217= c\u2217 k\u00b1d\u2217 k, \u03bdk2 = \u03bd\u2217 k2. Here, a\u2217 k, b\u2217 k, c\u2217 k, d\u2217 k are any vectors belong to the conic hulls of {ak}K1 k=1, {bk}K1 k=1, {ck}K1 k=1, {dk}K1 k=1 respectively, satisfying \u2225b\u2217 k\u2225\u2265\u2225a\u2217 k\u2225= \u0398(\u2225u\u2225) and \u2225d\u2217 k\u2225\u2265\u2225c\u2217 k\u2225= \u0398(\u2225q\u2225). \u03bd\u2217 k2 = \u0398(\u2225u\u2225) are any vectors from the complement space of span(M).\nAgain, the learned model satisfies L0\u22121 D\u2217 S (\u03a8(T \u2217)) \u2264\u03b5.\nThis proposition demonstrates the strong Out-of-Distribution Generalization ability of transformer utilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned \u201cKnowledge\u201d on the high-level concept and low-level label semantic information from the two non-orthogonal dictionaries. The admit of shift for D\u2217 z denotes that each prompt can enjoy multi-co-concepts and each word-label pair can appear in at least \u2225z\u22250 concept-specific prompts/tasks\u2019 distribution, which aligns the real-world cases. On the other hand, we also believe the admit of shift for D\u2217 x\u00d7D\u2217 y is inspiring, suggesting that transformer can conduct specific cross-concept semantic \u201cKnowledge Intersection\u201d. As such, this lemma suggest that the transformer can master the regularity of unseen ICL tasks\u2019 \u201cstructure\u201d in the presence the multi-concept encoded representation. Remark 1. Comparison with Related Work. Theorem 3.4 in [28] and Theorem 2 in [54] address the transformer\u2019s OOD capability in specific structured ICL classification and regression tasks. Our results differ by focusing on compositional generalization of learned concepts, grounded in the concept-specific linear latent geometry observed in LLMs.\n# 5 Proof Idea\nIn a big picture, we simply extend standard expectation-variance reduction techniques [34] to our setting. Section 5.1 defines coefficients to examine NN\u2019s expected projection along feature directions. Section 5.2 provides the convergence of the expected estimator through the lens of coefficient evolution; Section 5.3 showcase the exponential convergence by treating the conditional expectations of the NNs as Doob martingales and exploiting the property of the tails under low-noise conditions.\n# 5.1 Idempotent Operator Techniques\nIdempotent Operator Trick. Define U := span(M) and its complement space U\u22a5. By definition, we know that dim(U) = K and dim(U\u22a5) = dX \u2212K. Then we can let {{ak1}K1 k1=1, {bk1}K1 k1=1, {\u03bdk2}K2 k2=1, {uw}dX \u2212K w=1 } be the set of standard orthogonal basis for RdX , where u\u22a5 1 , \u00b7 \u00b7 \u00b7 , u\u22a5 dX \u2212K are the standard orthogonal basis of U\u22a5. Then we can derive an idempotent decomposition of the identity matrix\n\ufffd \ufffd \ufffd \ufffd Similar techniques are also applied to the label\u2019s dictionary: Q := span(Q), where we define q\u22a5 1 , \u00b7 \u00b7 \u00b7 , q\u22a5 dY\u2212K1 as the standard orthogonal basis of the complement space Q\u22a5. In our subsequent derivation, the expectation E[\u00b7] is taken over the stochastic gradient descent. Similar to the idea in [34, 33, 36], we first serve to see how E(\u03a8(t)) evolves. For E(\u03a8(t)), every gradient descent update by all concept\u2019s samples within a soft \u201cweight\u201d, and thus the analysis is equivalent to gradient descent\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e381/e38141d0-a26d-474e-a46a-451228fcae31.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Illustration of our Idempotent Operator Techniques. This allows us to focus on analyzing the evolving coefficients, which are key to the expected 0-1 loss convergence.</div>\nHere \u03b1(t) Q,s, \u03b1(t) K,s and \u03b1(t) O(i,\u00b7),k represent the expected concept learning process, \u03b2(t) Q,s, \u03b2(t) K,s an \u03b2(t) O(i,\u00b7),k represent the expected concept-specific semantic learning process and \u03c4 (t) Q,r, \u03c4 (t) K,r, \u03c1(t) Q,w, \u03c1(t) K,w and \u03c1(t) O(i,\u00b7),w represent the expected memorization of the concept irrelevant noise. It holds that\n\u00b7 \u00b7 \u00b7 \u00b7 for \u2200e \u2208[\u00b1], i \u2208[m], k \u2208[K1] and for \u2200e\u2032 \u2208[\u00b1], s\u2032 \u2208[K1], r \u2208[K2], w \u2208[dX \u2212K], \u2200u \u2208 {\u00b5e\u2032 s\u2032, \u03bdr, u\u22a5 w}, it holds that E[(Wx K (t)u)]\u22a4E[Wx Q (t)\u00b5e s] = 0. Similar conclusions hold when the query vectors are \u03bdr and u\u22a5 w, \u2200r \u2208[K2], w \u2208[dX \u2212K]. As such, our remaining task is to scrutinize the coefficients evolution, which would be the key contributors to the expected 0-1 loss convergence.\n# 5.2 Convergence of the Expectation\nDenote UySn k,n (t) and Wv k,n(t) \u2212UySn k,n (t) as the activated neuron set for {i \u2208[m] | riySn > 0} and {i \u2208[m] | riySn < 0} separately, and \ufffd l\u2208S ySn n,k (\u03c3(t) S ) n l represents the correct attention weight, where the detailed definitions are delayed in Appendix E. We then introduce the following lemma. Lemma 2. Under Condition 1, when \uf8eb \uf8f6\nholds, we have L0\u22121 D\u2217(E(\u03a8\u2032(t))) = 0.\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7bc5/7bc5419f-9152-4421-a8aa-28784d69e30f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Learning dynamics: (i) training and test loss; (ii) correct attention weight; (iii) maximum values of \u03b1Q,s \u00b7 \u03b1K,s, \u03b2Q,s \u00b7 \u03b2K,s, maximum values of the complement products \u03c4Q,r \u00b7 \u03c4K,r or \u03c1Q,2 \u00b7 \u03c1K,2, and maximum values of product-with-noise (Wx K\u03bex)\u22a4Wx Q\u03bex; (iv) maximum values of \u03b1O(i,\u00b7),k and |\u03b2O(i,\u00b7),k|, maximum values of the complement coefficients \u03c1O(i,\u00b7),w and maximum values of product-with-noise Wy O(i,\u00b7)\u03bey.</div>\nAs such, the following lemmas show the learning outcomes of the E(\u03a8(t)) along the iterations. Lemma 3. (Convergence of the Expectation). There exist constant C1 > 0, \u2200t \u2265 \u02c6T = C1\u03c31m\u03bbK1\u03b3 \ufffd (1 + \u03bay) log(5Km/\u03b4)/w\u22172(1 \u2212\u03bay)\u2225q\u2225, we have L0\u22121 D\u2217(E(\u03a8\u2032(t))) = 0. Lemma 4. (Regularizing the models). Under Condition 1, it holds that\nIn addition, our analysis provides three asymptotic properties of the coefficients evolution, which are delayed to Appendix I.1.3 and I.2 for room limitation.\n# 5.3 Exponential Convergence of 0-1 loss\nProposition 2. \u2200t \u2265\u02c6T, when \u2225\u03a8\u2032(t) \u2212E(\u03a8\u2032(t))\u2225F \u2264\u03bd holds, we have L0\u22121 D\u2217(\u03a8\u2032(t)) = 0. Here, \u2225\u03a8\u2032\u22252 F := \u2225Wx Q\u22252 F + \u2225Wx K\u22252 F + \u2225Wy O\u22252 F .\nBy definition of 0-1 loss, then we only need to prove the 0-1 loss convergence by seeing the speed of \u03a8\u2032(t) converging to E(\u03a8\u2032(t)) with an error of \u03bd in terms of \u2225\u00b7 \u2225F . Drawing insights from [34], we see B0, \u00b7 \u00b7 \u00b7 , BT \u22121 as a i.i.d. random variables following the same distribution. Then \u2200t \u2208{0, \u00b7 \u00b7 \u00b7 , T}, it holds that\n | B \u00b7 \u00b7 \u00b7 B \u2212  | B \u00b7 \u00b7 \u00b7 B are martingale difference sequences, and for \u2200X \u2208{Q, K, O} and its corresponding W \u2208 {Wx Q, Wx K, Wy O}, we have \ufffdT t=0 Dt X = W(T +1) \u2212E[W(T +1)]. Then we utilize the following lemma in [34, 55] to give a bound over the variance. Lemma 5. Let D1, \u00b7 \u00b7 \u00b7 , DT \u22121 be a martingale difference sequence. Suppose \u2203cT > 0 such that \ufffdT t=0 \u2225Dt\u22252 \u221e\u2264c2 T , where \u2225\u00b7 \u2225\u221eis the essential supremum of \u2225\u00b7 \u2225F . Then for \u2200\u03f5 > 0, we have\nTherefore, we need to see if there exists a decaying positive constant cT (with decaying rate O(1/T q), q > 0), such that \ufffdT t=0 \u2225Dt X\u22252 \u221e\u2264cT 2, \u2200X \u2208{Q, K, O}, where \u2225\u00b7 \u2225\u221eis the essential\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b69e/b69e5e52-fb5f-4c9f-af5d-3a146ed65a22.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) OOD Scenario 2: 0.8 fraction for concept 0 and 0.2 fraction for concept 1 during testing. (d) OOD Scenario 3: Shift the data as \u00b5\u00b1 1 \u2217= a1 \u00b1 b2 and \u00b5\u00b1 2 \u2217= a2 \u00b1 b1 during testing.</div>\nFigure 3: Learning dynamic in three OOD scenarios. The training settings and plotting methods are identical to those used in Figure 2, and the testing settings are: (a-b) utilizes different prompt lengths; (c) adopts a skewed distribution over z; (d) switches the concept-specific semantic features.\nsupremum of \u2225Dt X\u2225F . Subsequently, by controlling the martingale sequence norm tail similarly in [34, 55], we can obtain an exponential convergence rate after T1. For W \u2208{Wx Q, Wx K, Wy O}, to check the decaying cT , we adopt the techniques of [34, 33, 36] in the following manner. Let Bt \u2032 be an independent variable from B0, \u00b7 \u00b7 \u00b7 , BT and let Wt (T +1) be an output of the algorithm depending on (B0, \u00b7 \u00b7 \u00b7 , Bt\u22121, Bt \u2032, Bt+1, \u00b7 \u00b7 \u00b7 , BT ). Then we have\nTherefore, one may estimate cT X 2 by bounding \u2225W(T +1)\u2212Wt (T )\u22252 \u221euniformly w.r.t. B0, \u00b7 \u00b7 \u00b7 , BT \u22121 Such a bound can be derived utilizing stability property of stochastic gradient descent [34, 56]. For the OOD scenario, since we require the data shift to be via conic combination, the new words and labels in each prompt will share the positive/negative real-valued label without any self-conflict. The norm requirements and constraints on D\u2217 z would ensure the Gaussian noise, concepts other than the co-concepts, and probability shifts have limited influence on the prediction compared with the considerable scale of coefficients by Lemma 4, laying the groundwork for the proof.\n# 6 Experiments\nIn this section, we demonstrate the validity of our theoretical analysis through simulations of Algorithm 1. We use the following parameter settings in Figure 2: The parameter settings are: the length L = 4, the number of co-concepts K1 = 2, dictionary size K = 104, the number of test instances ntest = 5000, dimension dX = dY = 1000, MLP width m = 50, feature strengths \u2225u\u2225= \u2225q\u2225= 10, \u2200k \u2208[K1], the cosine \u27e8\u00b5+ k , \u00b5\u2212 k \u27e9/\u2225u\u22252 = \u27e8q+ k , q\u2212 k \u27e9/\u2225q\u22252 = 0.5, the initialization parameters \u03c30 = 0.1, \u03c31 = 0.01, and the noise deviation \u03c3\u03be = 0.01. For the optimization, we use \u03bb = 0.002, B = 16, \u03b3 = 10000, and the total training epochs is 100. Figure 3 (a-d) uses the same training settings, but during testing, it applies different configurations: (a) L\u2217= 5, (b) L\u2217= 2, (c) a 0.8 fraction for the first concept and a 0.2 fraction for the second concepts, and (d) \u00b5\u00b1 1 \u2217= a1 \u00b1 b2, \u00b5\u00b1 2 \u2217= a2 \u00b1 b1. Figure 2 validates our Theorem 2 and Lemma 4, which showcases the fast convergence rate and the evolution of coefficients. Figure 3 validates Proposition 1, where the learned model permits certain data shifts.\n# 7 Conclusion\nThis work provides the first exponential convergence analysis of 0-1 loss for transformers with softmax attention and ReLU-MLP, trained on a non-orthogonal concept-specific prompt distribution by practical cross-entropy loss. Furthermore, the results demonstrate transformers can perform certain OOD ICL tasks by leveraging the multi-concept semantic linearity, highlighting their innovative potential. An important future direction is to extend the analysis to more complex scenarios.\nWe thank the anonymous reviewers for their instrumental comments. D.B. and H.W. are supported in part by the Research Grants Council of the Hong Kong Special Administration Region (Project No. CityU 11206622). W.H. is supported in part by JSPS KAKENHI (24K20848). A.N. is supported in part by National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative, the Centre for Frontier Artificial Intelligence Research, Institute of High Performance Computing, A*Star, and the College of Computing and Data Science at Nanyang Technological University. T.S. is supported in part by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2115, JPMJCR2015).\n# References\n[1] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00c9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837, 2022. [3] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint arXiv:2401.02051, 2024. [4] Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, and Qingfu Zhang. A systematic survey on large language models for algorithm design. arXiv preprint arXiv: 2410.14716, 2024. [5] Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv: 2309.01809, 2023. [6] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. In Advances in Neural Information Processing Systems, 2001. [7] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2022. [8] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. [9] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In Proceedings of the 40th International Conference on Machine Learning, pages 19689\u201319729, 2023. 10] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch. On the origins of linear representations in large language models. arXiv preprint arXiv: 2403.03867, 2024. 11] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv: 2311.03658, 2023. 12] Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical and hierarchical concepts in large language models. arXiv preprint arXiv: 2406.01506, 2024. 13] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Do llms dream of elephants (when told not to)? latent concept association and associative memory in transformers. arXiv preprint arXiv: 2406.18400, 2024. 14] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv: 2305.19420, 2023.\n[15] Fabian Falck, Ziyu Wang, and Christopher C. Holmes. Are large language models bayesian? a martingale perspective on in-context learning. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. [16] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 35151\u201335174. PMLR, 2023. [17] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv: 2306.09927, 2023. [18] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Advances in Neural Information Processing Systems, volume 36, pages 57125\u201357211, 2023. [19] Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape. arXiv preprint arXiv: 2402.01258, 2024. [20] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv: 2310.05249, 2023. [21] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. In Advances in Neural Information Processing Systems, volume 36, pages 71911\u201371947, 2023. [22] Yingcong Li, Yixiao Huang, Muhammed E. Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of next token prediction with self-attention. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238, pages 685\u2013693, 2024. [23] Chenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, and Chongxuan Li. On mesa-optimization in autoregressively trained transformers: Emergence and capability. arXiv preprint arXiv:2405.16845, 2024. [24] Shokichi Takakura and Taiji Suzuki. Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input. arXiv preprint arXiv: 2305.18699, 2023. [25] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multihead softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv: 2402.19442, 2024. [26] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. Transformers provably learn featureposition correlations in masked image modeling. arXiv preprint arXiv: 2403.02233, 2024. [27] Hongkang Li, Meng Wang, Sijia Liu, and Pin yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015, 2023. [28] Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do nonlinear transformers learn and generalize in in-context learning? arXiv preprint arXiv: 2402.15607, 2024. [29] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 11112\u201311122. PMLR, 2021. [30] Patrik Reizinger, Szilvia Ujv\u00e1ry, Anna M\u00e9sz\u00e1ros, Anna Kerekes, Wieland Brendel, and Ferenc Husz\u00e1r. Position: Understanding LLMs requires more than statistical generalization. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 42365\u201342390, 2024. [31] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808\u20131829, 1999. [32] Pascal Massart and \u00c9lodie N\u00e9d\u00e9lec. Risk Bounds for Statistical Learning. The Annals of Statistics, 34(5):2326 \u2013 2366, 2006. [33] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing error for stochastic gradient methods. In Proceedings of the 31st Conference On Learning Theory, volume 75, pages 250\u2013296, 2018.\n[34] Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergence rates of expected classification errors. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89, pages 1417\u20131426, 2019. [35] Vivien A Cabannes, Francis Bach, and Alessandro Rudi. Fast rates for structured prediction. In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 823\u2013865. PMLR, 15\u201319 Aug 2021. [36] Shingo Yashima, Atsushi Nitanda, and Taiji Suzuki. Exponential convergence rates of classification errors on learning with sgd and random features. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130, pages 1954\u20131962, 2021. [37] Kazusato Oko, Taiji Suzuki, Atsushi Nitanda, and Denny Wu. Particle stochastic dual coordinate ascent: Exponential convergent algorithm for mean field neural network optimization. In International Conference on Learning Representations, 2022. [38] Stefano Vigogna, Giacomo Meanti, Ernesto De Vito, and Lorenzo Rosasco. Multiclass learning with Margin: Exponential rates with no bias-variance trade-off. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 22260\u201322269. PMLR, 17\u201323 Jul 2022. [39] Vivien Cabannnes and Stefano Vigogna. A case of exponential convergence rates for svm. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206, pages 359\u2013374. PMLR, 25\u201327 Apr 2023. [40] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In The Eleventh International Conference on Learning Representations, 2023. [41] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. In Advances in Neural Information Processing Systems, volume 35, pages 25237\u201325250, 2022. [42] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer reLU convolutional neural networks. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 17615\u201317659, 2023. [43] Xuran Meng, Difan Zou, and Yuan Cao. Benign overfitting in two-layer relu convolutional neural networks for XOR data. arXiv preprint arXiv: 2310.01975, 2023. [44] Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in reLU networks for XOR cluster data. arXiv preprint arXiv: 2310.02541, 2023. [45] Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably benefit from structural information: A feature learning perspective. arXiv preprint arXiv: 2306.13926, 2023. [46] Hiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. Discovering universal geometry in embeddings with ica. arXiv preprint arXiv: 2305.13175, 2023. [47] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In Proceedings of the 38th International Conference on Machine Learning, pages 11112\u201311122, 2021. [48] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. Word embeddings are steers for language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16410\u201316430, 2024. [49] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, page 50\u201357, 1999. [50] L\u00c9on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018. [51] Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. arXiv preprint arXiv: 2006.12297, 2021.\n[52] Yiwen Kou, Zixiang Chen, Yuan Cao, and Quanquan Gu. How does semi-supervised learning with pseudo-labelers work? a case study. In The Eleventh International Conference on Learning Representations, 2023. [53] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. In The Eleventh International Conference on Learning Representations, 2023. [54] Tong Yang, Yu Huang, Yingbin Liang, and Yuejie Chi. In-context learning with representations: Contextual generalization of trained transformers. arXiv preprint arXiv: 2408.10147, 2024. [55] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, 22(4):1679\u20131706, 1994. [56] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 1225\u20131234, 2016. [57] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 43423\u201343479, 2023. [58] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linear classification. In Robin J. Evans and Ilya Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216, pages 313\u2013323, 2023. [59] Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178, pages 2668\u20132703, 2022. [60] Spencer Frei, Gal Vardi, Peter Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195, pages 3173\u20133228, 2023. [61] Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit Bias of Gradient Descent for Twolayer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data. In Advances in Neural Information Processing Systems, volume 36, pages 30167\u201330221. Curran Associates, Inc., 2023. [62] Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In The Twelfth International Conference on Learning Representations, 2024. [63] Dake Bu, Wei Huang, Taiji Suzuki, Ji Cheng, Qingfu Zhang, Zhiqiang Xu, and Hau-San Wong. Provably neural active learning succeeds via prioritizing perplexing samples. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 4642\u20134695, 2024. [64] Yiwen Kou, Zixiang Chen, Quanquan Gu, and Sham M. Kakade. Matching the statistical query lower bound for k-sparse parity problems with stochastic gradient descent. arXiv preprint arXiv: 2404.12376, 2024. [65] Alexander Tsigler. Benign Overfitting in Linear Regression and Classification. PhD thesis, UC Berkeley, 2024. [66] Junhyung Park, Patrick Bloebaum, and Shiva Prasad Kasiviswanathan. Benign overfitting for regression with trained two-layer relu networks. arXiv preprint arXiv: 2410.06191, 2024. [67] Eshaan Nichani, Alex Damian, and Jason D. Lee. Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. In Advances in Neural Information Processing Systems, volume 36, pages 10828\u201310875, 2023. [68] Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns lowdimensional polynomials with sgd near the information-theoretic limit. arXiv preprint arXiv:2406.01581, 2024. [69] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations. arXiv preprint arXiv:2406.11828, 2024. [70] Yunwei Ren and Jason D. Lee. Learning orthogonal multi-index models: A fine-grained information exponent analysis. arXiv preprint arXiv:2410.09678, 2024.\n[71] Spencer Frei and Gal Vardi. Trained transformer classifiers generalize and exhibit benign overfitting in-context. arXiv preprint arXiv:2410.01774, 2024. [72] Wei Shen, Ruida Zhou, Jing Yang, and Cong Shen. On the training convergence of transformers for in-context classification. arXiv preprint arXiv:2410.11778, 2024. [73] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Shaolei Du. JoMA: Demystifying multilayer transformers via joint dynamics of MLP and attention. In The Twelfth International Conference on Learning Representations, 2024. [74] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. How transformers learn diverse attention correlations in masked vision pretraining. arXiv preprint arXiv: 2403.02233, 2024. [75] Masahiro Sakamoto and Hitomi Sato. Benign or not-benign overfitting in token selection of attention mechanism. arXiv preprint arXiv:2409.17625, 2024. [76] Roey Magen, Shuning Shang, Zhiwei Xu, Spencer Frei, Wei Hu, and Gal Vardi. Benign overfitting in single-head attention. arXiv preprint arXiv:2410.07746, 2024. [77] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv: 2402.14735, 2024. [78] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. arXiv preprint arXiv: 2409.10559, 2024. [79] Hongru Yang, Bhavya Kailkhura, Zhangyang Wang, and Yingbin Liang. Training dynamics of transformers to recognize word co-occurrence via gradient flow analysis. arXiv preprint arXiv:2410.09605, 2024. [80] Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, and Liqiang Nie. Unveil benign overfitting for transformer in vision: Training dynamics, convergence, and generalization. arXiv preprint arXiv:2409.19345, 2024. [81] Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, and Jianfei Chen. On the optimization and generalization of two-layer transformers with sign gradient descent. arXiv preprint arXiv:2410.04870, 2024. [82] Yoshua Bengio. Learning Deep Architectures for AI. Now Publishers Inc, 2009. [83] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. [84] Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning Performs Deep Learning. In Conference on Learning Theory, COLT \u201923, 2023. Full version available at http://arxiv.org/abs/2001.04413. [85] Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, and Hidenori Tanaka. Emergence of hidden capabilities: Exploring learning dynamics in concept space. arXiv preprint arXiv:2406.19370, 2024. [86] Yongyi Yang, Core Francisco Park, Ekdeep Singh Lubana, Maya Okawa, Wei Hu, and Hidenori Tanaka. Dynamics of concept learning and compositional generalization. arXiv preprint arXiv:2410.08309, 2024. [87] Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P. Xing, Yuejie Chi, and Kun Zhang. Learning discrete concepts in latent hierarchical models. arXiv preprint arXiv: 2406.00519, 2024. [88] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 1, Learning Hierarchical Language Structures. ArXiv e-prints, abs/2305.13673, May 2023. Full version available at http://arxiv.org/abs/2305.13673. [89] Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv preprint arXiv: 2303.07971, 2023. [90] Emanuele Marconato, S\u00e9bastien Lachapelle, Sebastian Weichwald, and Luigi Gresele. All or none: Identifiable linear properties of next-token predictors in language modeling. arXiv preprint arXiv:2410.23501, 2024. [91] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge university press, 2012. [92] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n[93] Miao Lu, Beining Wu, Xiaodong Yang, and Difan Zou. Benign oscillation of stochastic gradient descent with large learning rate. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023. [94] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer Science & Business Media, 2013. [95] Karan Girotra, Lennart Meincke, Christian Terwiesch, and Karl T. Ulrich. Ideas are dimes a dozen: Large language models for idea generation in innovation. SSRN, 2023. [96] Anil Rajnikant Doshi and Oliver Hauser. Generative artificial intelligence enhances creativity but reduces the diversity of novel content. SSRN, 2023. [97] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large language model. arXiv preprint arXiv: 2311.15249, 2023. [98] Yiming Yao, Fei Liu, Ji Cheng, and Qingfu Zhang. Evolve cost-aware acquisition functions using large language models. arXiv preprint arXiv: 2404.16906, 2024. [99] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. An example of evolutionary computation + large language model beating human: Design of efficient guided local search. arXiv preprint arXiv: 2401.02051, 2024.\n# A Limitation and Broader Impact\nThe theoretical analysis provided in this work introduces novel perspectives on optimization and generalization, but the data model employed may require additional refinements to better align with practical scenarios, such as adding more layers of attention. The techniques and findings can inform future empirical and theoretical explorations of transformer architectures, though we do not foresee a direct social impact arising from the theoretical advancements presented.\n# B Additional Experiment Details\nWe implement our methods using PyTorch, ensuring consistent software and hardware environments. Specifically, the experiments are run on Linux servers with NVIDIA A100 graphics cards and CUDA 11.2, and can be completed within one hour.\n# C Additional Related Work\nTheory of Convergence Rate of Stochastic Gradient Descent. Our analysis of the exponential convergence rate for the 0-1 loss builds upon a rich body of prior work. In the context of classification, the faster convergence rate mostly based on the excess of risk with some power of the essential supremum norm. Specifically, [31, 32] introduce the Hard low-noise condition over the margin. When there is a hard margin separating the classes, the test error can exhibit exponentially fast convergence as the number of training samples increases, even when the surrogate loss error only decreases polynomially. This phenomenon has been further explored in more recent studies. [33, 34, 35, 36, 37] have analyzed the exponential convergence of stochastic gradient descent under various settings. Meanwhile, [35] have investigated hard-margin and exponential rates in the context of structured prediction, which encompasses traditional classification as a special case. Besides, recent work also obtain the exponential rates in generalized settings such as Multi-class classification [38] and SVM [39]. Building upon this rich theoretical foundation, our work derives the first exponential convergence analysis for the 0-1 loss in the specific setting of transformer models with softmax attention and ReLU-activated MLP over the sparse coding data model, whose surrogate loss function is the cross-entropy loss. Theory of Feature Learning of GD-updated Neural Network. A rich body of recent learning theory research has focused on the feature direction\u2019 recovery view of neural network representations [40, 41, 42, 43, 45, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]. Rather than directly examining the evolution of the 0-1 loss, this line of work explicitly studies the process of reconstruction of the data\u2019s feature directions and memorization of disrupted noise in the network\u2019s latent space as surrogate metrics. While most studies in this area have assumed (near) orthogonal data, recent efforts by [43] and [44] have made initial attempts to analyze non-orthogonal data scenarios. Building upon this foundation, our study extends this line of research to nonlinear attention-MLP transformers with within-concept positive inner products and cross-concept orthogonal data representations. The key to our analysis is the assumption of good initialization of attention matrices and a sufficiently low-noise condition, which is reasonable for modeling language rather than images. In this setting, SGD allows noise to have only a mild impact on shaping neural network matrices or influencing gradient flow. Theory of Transformers and In-Context Learning. The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics [9], the origins and biases of LLM representations using latent variable models [10], and ICL from a model averaging perspective [14]. However, these works do not connect the geometric properties of concept-encoded representations to transformers\u2019 powerful ICL abilities. Another line of research has studied the learning dynamics of transformer, including analyses of linear-attention transformers [16, 17, 71, 72], QKcombined attention-only models [20, 21, 26, 54, 73, 74, 75, 76, 77, 78, 79], ReLU-free MLP [54, 80, 81] or without MLP [17, 25], impractical squared or hinge loss [25, 26, 27, 28]. Though relevant, these works rely on simplifications or do not connect the observed linear semantic representation of large model to the transformer\u2019s excelling OOD capability. Concept Learning in Deep Learning. Hierarchical learning has long been regarded as a key factor behind the success of deep learning [82, 83, 84]. Recent research shows that large-scale generative models, such as diffusion models and transformers, effectively encode hierarchical concepts in their latent spaces [11, 12, 13, 46, 85, 86, 87]. Moreover, [73, 88, 89] show that transformers can capture hierarchical and compositional structures in data. From a Bayesian perspective, [7, 8, 14] interpret ICL as LLMs predicting outputs based on latent (concept) variable inference. Furthermore, studies reveal a linear structure in LLMs\u2019 latent space over independent interpretable concepts: representations of the same concept exhibit positive inner products, while statisticallyindependent concepts are nearly orthogonal [9, 10, 11, 12, 90]. Interestingly, aligning with the findings in [46, 90], Independent Component Analysis (ICA) is naturally more suitable than Principal Component Analysis (PCA) for obtaining meaningful feature or label vectors in our prompt modeling. This is because the features or\nfeeble in our modeling. Building on these insights, we explore in a theoretical context how the compositional nature of concept representations relates to transformers\u2019 ability to generalize to OOD tasks through a sparse coding modeling. We believe our OOD results are not only coincides with the transformer\u2019s compositional generalization ability on language tasks [89], but also consistent with other concept learning outcomes of diffusion and multi-model model: [87] shows that adjusting the length of semantic representations can directly affect image generation behaviors (see Figure 5), while [86] reveals that compositing different concepts enables OOD generalization (e.g. \u201cblue square apples\u201d in the Figure 1a in [86]).\n# D Preliminary Lemmas\n# D.1 Probablistic Lemmas on Concentration\nLemma 6. Suppose that \u03b4 > 0 and \u2200d \u2208{dX , dY} = \u2126(log(KNL \u03b4 )), where N = BT \u2217. Then with\nLemma 6. Suppose that \u03b4 > 0 and \u2200d \u2208{dX , dY} = \u2126(log(KNL \u03b4 )), where N = BT \u2217. Then w probability at least 1 \u2212\u03b4,\n\ufffd for all \u03bei, \u03be\u2032 i \u223cD\u03bex( or D\u03bey), \u00b5 \u2208Dx( or Dy), l \u2208{1, 2}.\nLemma 7. Suppose that \u03b4 > 0, dY = \u2126(log(m/\u03b4)), m = \u2126(log(K/(\u03b4))). Then with probability at least 1 \u2212\u03b4, for \u2200i \u2208[m], k \u2208[K1], w \u2208[dY \u2212K1],\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd (8) In addition, for a sufficient large m = \u2126(log(K/(\u03b4))/(1 \u2212\u03c9\u03b6)) the lower bound inequalities regarding maximum value in Eq.(7) hold at any above index set of i in Eq.(8). For example, there exist i \u2208{i \u2208[m] | ri = e m, \u03b1(0) O(i,\u00b7),k + \u03b6\u03b2(0) O(i,\u00b7),k > 0, \u03b1(0) O(i,\u00b7),k \u2212\u03b6\u03b2(0) O(i,\u00b7),k < 0}, such that \u03b1(0) O(i,\u00b7),k \u2264\u2212\u03c31/2\u2225ck\u2225.\nProof. First, notice that Wy O(i,\u00b7) (0) \u223cN(0, \u03c31IdY ), then by Bernstein\u2019s inequality as well as dY = \u2126(log(m/\u03b4)), with probability at least 1 \u2212\u03b4/(5m), for \u2200i \u2208[m]\n\ufffd |\u27e8Wy O(i,\u00b7) (0), q\u27e9| \u2264 \ufffd 2 log(5Km/\u03b4) \u00b7 \u03c31.\n\ufffd Notice P(\u03c31/2 > |\u27e8Wy O(i,\u00b7) (0), q\u27e9|) is an positive constant, then following the techniques of Lemma B.5 in [42] and the condition m = \u2126(log(K/\u03b4)), we have P(\u03c31/2 \u2264|\u27e8Wy O(i,\u00b7) (0), q\u27e9|) = 1 \u2212P(\u03c31/2 > max{|\u27e8Wy O(i,\u00b7) (0), q\u27e9|}),\n \u2212 For \u03b6 \u2208(0, 1], we see that the variable \u03b1(0) O(i,\u00b7),k + e\u2032\u03b6\u03b2(0) O(i,\u00b7),k \u223cN(0, \u03c32 1(\u2225ck\u22252 + \u03b6\u2225dk\u22252)), and it\u2019s independent to the event {ri = e m}, \u2200e \u2208[\u00b1]. Therefore, we can see the count of {i \u2208[m] | ri = e m, \u03b1(0) O(i,\u00b7),k + e\u2032\u03b6\u03b2(0) O(i,\u00b7),k > 0, e\u2032\u03b6\u03b2(0) O(i,\u00b7),k > 0} as a binomial variable with p = 1/4, n = m, then by the property of binomial tail, condition m = \u2126(log(K/(\u03b4))) as well as Hoeffding\u2019s inequality, with probability at least 1 \u2212\u03b4/5 we have\nwhere 1 + \u03c9\u03b6 2 is the probability of the conditional event {\u03b1(0) O(i,\u00b7),k \u2212e\u2032\u03b6\u03b2(0) O(i,\u00b7),k > 0 | \u03b1(0) O(i,\u00b7),k + e\u2032\u03b6\u03b2(0) O(i,\u00b7),k > 0}, and \u03c9\u03b6 > 0 due to the larger variance of \u03b1(0) O(i,\u00b7),k compared to e\u2032\u03b6\u03b2(0) O(i,\u00b7),k. We denote the probability with \u03c9\u03b6 since the true value is hard to compute. Subsequently, the event {i \u2208[m] | ri = e m, \u03b1(0) O(i,\u00b7),k \u00b1 \u03b6\u03b2(0) O(i,\u00b7),k > 0} can be seen as a binomial variable with p = 1 + \u03c9\u03b6 8 , n = m, then we can have the sixth inequality hold with probability at least 1 \u2212\u03b4/5, utilizing the property of binomial tail, condition m = \u2126(log(K/(\u03b4))) as well as Hoeffding\u2019s inequality. The seventh inequality is a natural inference of the third and forth inequality, where the m = \u2126(log(K1/\u03b4)) ensure \ufffd m log(10K1/\u03b4)/2 \u2264m/16, and the last inequality is then also a natural inference of the third and fifth inequality. Therefore, by union bound, the proof is completed.\nLemma 8. (1.1.P5 in [91]) Let A \u2208Mn be idempotent, that is,A2 = A. Then, each eigenvalue of A equals to the rank of A, which is either 0 or 1. Beside, identity matrix I is the only nonsingular idempotent matrix. Lemma 9. For a matrix A = \ufffdd i=1 \u00b5iPi, where Pi are symmetric idempotent matrices with rank(Pi) = 1, and thus \ufffdd i=1 \u00b5iPi is the idempotent decomposition of matrix A by Pi. Then we see that \u2225A\u2225F = \ufffd tr(AT A) = \ufffd\ufffdd i=1 \u00b52 i = \ufffd\ufffdd i=1 \u03bb2 i , where \u03bbi are eigenvalues of A.\nProof. By definition,\nThen, by Lemma 8 we have\ntr(AT A) = tr( d \ufffd i=1 \u00b52 i Pi) = d \ufffd i=1 \u00b52 i tr(Pi) = d \ufffd i=1 \u00b52 i rank(Pi) = d \ufffd i=1 \u00b52 i = d \ufffd i=1 \u03bb2 i .\n# D.3 ODE Systems\nfor some 0 \u2264c \u22641 and b \u22650. Then it holds that\nLemma 11. (Coupled ODE System 1). Suppose that there are two coupled sequences yt, zt, t \u22650 follows the iterative formula\nAs such, for t1 = min{t \u2208Z | zt \u22650}, we have\nProof. From the condition we see that z0 < 0 and zt is an increasing sequence (zt \u2265z0). Besides, as y0 > 0 during the period where zt \u22640, we see that yt is monotonically decreasing. Then by (2 + e\u22122yt2 + e2yt2)\u22121 \u2264 1/4 as well as Comparison Theorem, it\u2019s obvious that the continuous coupled ODE in Eq.(9) is the lower bound of yt. Then one can readily obtain the result by solving the ODE.\nLemma 12. (Coupled ODE System 2). Suppose that there are two coupled sequences yt, zt, which are the sequences after t1 in Lemma 11, and t \u2265t1 follows the iterative formula\n<div style=\"text-align: center;\">Lemma 12. (Coupled ODE System 2). Suppose that there are two coupled sequences yt, zt, which are the sequences after t1 in Lemma 11, and t \u2265t1 follows the iterative formula</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b683/b68311ca-8401-4dfd-87df-3e06c946ab62.png\" style=\"width: 50%;\"></div>\nProof. We see that as zt \u22650, t \u2265t1, the yt is monotonically increasing. As such, by Comparison Theorem we see that the upper and lower bound of the coupled system would depends on 1\u2212e\u22122y(t)2 1+e\u22122y(t)2 and \u2113\u2032 t. Easy to see that\n  dy(t) dt = abc\u20322 1 \u2212e\u22122y(t1)2 1 + e\u22122y(t1)2 (t \u2212t1)y(t)dt 1 + e2y(t)2 + e\u22122y(t)2 \u21d41 2(Ei(2y(t)2) + Ei(\u22122y(t)2) + 4 log(y(t))) = abc\u20322 1 \u2212e\u22122y(t1)2 1 + e\u22122y(t1)2 (t \u2212t1)2 2 + const, z(t) = bc\u2032 1 \u2212e\u22122y(t1)2 1 + e\u22122y(t1)2 (t \u2212t1).\nThus by the monotonicity the system is unique, which is also ture for the upper bound ODE. The proof is completed.\n# E Data Distribution\nThis section provided the detailed formal definitions of the prompt distribution. Definition 3. (Polysemous Word Model (Dx, Dy, Dz, D\u03bex, D\u03bey) ). We assume there exists K1 concepts  words totally. Specifically, each concept k1 \u2208[K1] is characterized by two semantically-opposite feature vecto separately, denoted as \u00b5+ k1 and \u00b5\u2212 k1, and the label vectors that describe their semantics under the co-conce are q+ k1 and q\u2212 k1. Our word samples x \u2208RdX and their corresponding labels y \u2208RdY are generated i.i from distribution Dx and Dy, which can be written as the following forms via reparameterization:\nz \u223cDz, \u03bex \u223cD\u03bex = N(0, \u03c32 \u03beIdX ), \u03bey \u223cD\u03bey = N(0, \u03c32 \u03beIdY ), x = Mz + \u03bex \u223cDx, y = Qz + \u03bey \u223cDy,\n\u2022 M = [\u00b5+ 1 , \u00b5\u2212 1 , \u00b5+ 2 , \u00b5\u2212 2 , \u00b7 \u00b7 \u00b7 , \u00b5+ K1, \u00b5\u2212 K1, \u03bd1, \u03bd2, \u00b7 \u00b7 \u00b7 , \u03bdK2] = [M1, \u00b7 \u00b7 \u00b7 , MK] \u2208RdX \u00d7K is the feature dictionary matrix, where {\u00b5\u00b1 k1}K1 k1=1 are concept-relevant features, {\u03bdk2}K2 k2=1 are conceptirrelevant features, and \u2200k \u2208[K], \u2225Mk\u2225= \u2225u\u2225. We assume that features of the same concept have positive inner product: \u22030 < \u03bax < 1, \u2200k1 \u2208[K1], 0 < \u27e8\u00b5+ k1, \u00b5\u2212 k1\u27e9\u2264\u03bax\u2225u\u22252. Meanwhile, we let the features of different concept be orthogonal: \u2200e \u2208[\u00b1], e\u2032 \u2208[\u00b1], s\u2032 \u2208[K1], r \u0338= r\u2032 \u2208[K2], u \u2208 {\u00b5e\u2032 s\u2032, \u03bdr}, we have \u27e8\u00b5e s, u\u27e9= \u27e8\u03bdr, \u03bdr\u2032\u27e9= 0.\n\u2022 Q = [q+ 1 , q\u2212 1 , q+ 2 , q\u2212 2 , \u00b7 \u00b7 \u00b7 , q+ K1, q\u2212 K1, 0, \u00b7 \u00b7 \u00b7 0] \u2208RdY \u00d7K is the corresponding label dictionary matrix, where \u2225q\u00b1 k \u2225= \u2225q\u2225, for \u2200k \u2208[K1]. Similarly, we let the labels of the same concept to have positive inner product: \u22030 < \u03bay < 1, \u2200k1 \u2208[K1], 0 < \u27e8q+ k1, q\u2212 k1\u27e9\u2264\u03bay\u2225q\u22252, while the labels of different concept to be orthogonal: \u27e8q\u00b1 k , q\u00b1 k\u2032\u27e9= 0, \u2200k \u0338= k\u2032 \u2208[K1].\nDefinition 4. (Concept-specific Contextual Prompt Distribution) We consider the case that each prompt is concept-specific (i.e., the multi-concept words in one prompt would at least share one co-concept). Specifically, the chance for selecting each concept as the co-concept of one particular prompt is \u0398(K1 \u22121), and the chance for selecting the two semantically-opposite vectors of the same concept is 1 2. During training, each prompt S = {x1, y1, \u00b7 \u00b7 \u00b7 , xL, yL, xL+1} is sampled from the mixture distribution DS defined as below.\nwhere \u03c0+ k = \u03c0\u2212 k = 1 2K1 , and the P+ k,L+1 and P\u2212 k,L+1 are prompt distributions characterized by the k-th concept, defined as P+ k,L+1 = \ufffd S | x \u223cDx, y \u223cDy, PL+1,2k\u22121 = 1, \u2200l \u2208[L + 1], j \u0338= {2k \u22121, k}, Pl,j = 1 K , {zl,2k\u22121 = 1} \u222a{zl,2k = 1} = \u2126, {zl,2k\u22121 = 1} \u2229{zl,2k = 1} = \u2205, \u2200l \u2208[L], Pl,2k\u22121 = Pl,2k = 1 2 \ufffd , P\u2212 k,L+1 = \ufffd S | x \u223cDx, y \u223cDy, PL+1,2k = 1, \u2200l \u2208[L + 1], j \u0338= {2k \u22121, k}, Pl,j = 1 K , {zl,2k\u22121 = 1} \u222a{zl,2k = 1} = \u2126, {zl,2k\u22121 = 1} \u2229{zl,2k = 1} = \u2205, \u2200l \u2208[L], Pl,2k\u22121 = Pl,2k = 1 2 \ufffd ,\n\ufffd where Pl,j := P (zl,j = 1). \u2200n \u2208[N] where N is the training size, if the training prompt Sn is sampled from Pe k,L+1, e \u2208[\u00b1], k \u2208[K1], then by Definition 1, the label vector of the query should contain qe k, and we call ySn = e as the real value label of this k-th concept prompt. Specifically, for \u2200k \u2208[K1] we define the index set of training prompts sharing the k-th co-concepts as\nwhere\nV \ufffd  | \u223cP \ufffd For sample xl where n \u2208Vk, k \u2208[K1], l \u2208[L + 1], we define the index set for its non-zero elements of zn l besides zn 2k\u22121,l and zn 2k,l, namely Mn l := {k \u2208[K] | zn l,k = 1, k /\u2208{2k \u22121, 2k}}. Also, for each prompt sharing the k-th co-concept, we define the index set of demonstration in the context: S+ n,k = {l \u2208[L] | n \u2208Vk, zn l,2k\u22121 = 1}, S\u2212 n,k = {l \u2208[L] | n \u2208Vk, zn l,2k = 1},\n# F Model details: Attention Part\nIn this section, we provide several important definitions and compute the original gradients of attention. Lemma 13. (Contributing and Misleading Neurons) W+ k,n(t) = {i \u2208[m] | n \u2208V+ k , 1n O(i) (t) > 0}, U+ k,n(t) = {i \u2208[m] | n \u2208V+ k , ri \u00b7 1n O(i) (t) > 0}, W\u2212 k,n(t) = {i \u2208[m] | n \u2208V\u2212 k , 1n O(i) (t) > 0}, U\u2212 k,n(t) = {i \u2208[m] | n \u2208V\u2212 k , ri \u00b7 1n O(i) (t) < 0}. (\nIn this section, we provide several important definitions and compute the original gradients of attention. Lemma 13. (Contributing and Misleading Neurons)\nW+ k,n(t) = {i \u2208[m] | n \u2208V+ k , 1n O(i) (t) > 0}, U+ k,n(t) = {i \u2208[m] | n \u2208V+ k , ri \u00b7 1n O(i) (t) > 0}, W\u2212 k,n(t) = {i \u2208[m] | n \u2208V\u2212 k , 1n O(i) (t) > 0}, U\u2212 k,n(t) = {i \u2208[m] | n \u2208V\u2212 k , ri \u00b7 1n O(i) (t) < 0}.\n(10)\n(11)\nWk,n(t) := W+ k,n(t) \u222aW\u2212 k,n(t) are neurons that can be activated, among which Uk,n(t) := U+ k,n(t) \u222aU \u2212 k,n(t are neurons that correctly contribute to the prediction. The following lemma computes the original gradients. Lemma 14. (Gradient Update) Denote\n\u2207Wx Q (t)LBt(\u03a8(t)) \u2208RdX \u00d7dX can be derived as\n\uf8f0 Similarly, \u2207Wx K (t)LBt(\u03a8(t)) \u2208RdX \u00d7dX can be derived as\n1 B \ufffd n\u2208Bt \uf8ee \uf8f0y(t) Sn\u2113\u2032 n (t) m \ufffd i=1 ri1n O(i) (t) \ufffd l,j\u2208[L] (\u03c3(t) S ) n l (\u03c3(t) S ) n j (W(t) O(i,\u00b7)W(t) V hn l )Wx Q \u22a4xn L+1(xn l \u2212xn j )\u22a4 \uf8f9 \uf8fb\nSubsequently, we directly compute the update of the attention matrices along the feature directions as below. Lemma 15. (Concept Learning of Attention) For \u2200\u02c6k \u2208[K1], we have the single step of learning of the concept part of the features:\n(12)\n(15)\n(16)\nSimilarly, I(t) K,a\u02c6k,chaos and I(t) K,a\u02c6k,contri are defined as below.\ns = 1 B \ufffd k\u0338=\u02c6k\u2208",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of understanding the connection between the linear regularity of multi-concept encoded semantic representations in transformer-based large language models (LLMs) and their in-context learning (ICL) capabilities. Existing studies have shown that LLMs exhibit remarkable performance across various tasks, yet the theoretical foundations explaining their ICL mechanisms remain limited.",
        "problem": {
            "definition": "The primary problem is to elucidate how the geometric regularity of multi-concept-encoded representations facilitates efficient ICL in transformers.",
            "key obstacle": "A significant challenge is the lack of a comprehensive theoretical framework that connects the observed linear geometric properties of LLMs to their innovative ICL capabilities."
        },
        "idea": {
            "intuition": "The idea is inspired by empirical studies that reveal a linear latent geometry in LLMs, suggesting a structured semantic representation that can enhance learning.",
            "opinion": "The authors propose that transformers can leverage multi-concept semantics to improve their ICL performance, particularly in out-of-distribution tasks.",
            "innovation": "The main innovation lies in providing a fine-grained mathematical analysis that incorporates the concept-based low-noise sparse coding prompt model, which contrasts with previous works that often used simplified models."
        },
        "Theory": {
            "perspective": "The theoretical perspective focuses on the structured geometric properties of latent representations in transformers, emphasizing the importance of multi-concept semantics.",
            "opinion": "The authors assume that the observed geometric regularities are crucial for understanding the learning dynamics of transformers in ICL scenarios.",
            "proof": "The paper presents a proof of exponential convergence rates for the 0-1 loss in the context of training transformers with softmax attention and ReLU-MLP, demonstrating the effectiveness of their proposed model."
        },
        "experiments": {
            "evaluation setting": "The experiments utilize a dataset characterized by multiple feature embeddings corresponding to different concepts, with a focus on a concept-specific prompt distribution.",
            "evaluation method": "The evaluation involves simulating training using mini-batch stochastic gradient descent and measuring the 0-1 test error on a test prompt distribution."
        },
        "conclusion": "The work concludes that transformers can achieve exponential convergence for the 0-1 loss, leveraging multi-concept semantics to perform effectively on unseen ICL tasks, highlighting their innovative potential.",
        "discussion": {
            "advantage": "The primary advantage of this paper is its rigorous theoretical analysis, which advances the understanding of how transformers utilize multi-concept semantics for efficient learning.",
            "limitation": "A limitation noted is that the data model may require further refinement to better align with practical scenarios, such as incorporating additional layers of attention.",
            "future work": "Future research could focus on extending the analysis to more complex transformer architectures and real-world applications, enhancing the understanding of ICL mechanisms."
        },
        "other info": [
            {
                "info1": "The paper acknowledges support from various research grants and institutions."
            },
            {
                "info2": {
                    "info2.1": "The experiments were conducted using PyTorch on NVIDIA A100 graphics cards.",
                    "info2.2": "The implementation details and parameter settings are provided for reproducibility."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the connection between the linear regularity of multi-concept encoded semantic representations in transformer-based large language models (LLMs) and their in-context learning (ICL) capabilities."
        },
        {
            "section number": "1.2",
            "key information": "Existing studies have shown that LLMs exhibit remarkable performance across various tasks, yet the theoretical foundations explaining their ICL mechanisms remain limited."
        },
        {
            "section number": "1.3",
            "key information": "The authors propose that transformers can leverage multi-concept semantics to improve their ICL performance, particularly in out-of-distribution tasks."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective focuses on the structured geometric properties of latent representations in transformers, emphasizing the importance of multi-concept semantics."
        },
        {
            "section number": "3.3",
            "key information": "The paper presents a proof of exponential convergence rates for the 0-1 loss in the context of training transformers with softmax attention and ReLU-MLP."
        },
        {
            "section number": "6.1",
            "key information": "A significant challenge is the lack of a comprehensive theoretical framework that connects the observed linear geometric properties of LLMs to their innovative ICL capabilities."
        },
        {
            "section number": "6.4",
            "key information": "Future research could focus on extending the analysis to more complex transformer architectures and real-world applications, enhancing the understanding of ICL mechanisms."
        }
    ],
    "similarity_score": 0.6924490731482611,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning.json"
}