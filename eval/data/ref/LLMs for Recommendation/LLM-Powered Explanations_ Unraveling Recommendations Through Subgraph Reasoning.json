{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.15859",
    "title": "LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning",
    "abstract": "Recommender systems (RecSys) are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. Knowledge graphs (KGs), which model explicit relations between users and items, have been widely used to enhance the performance of recommender systems. However, a significant challenge persists in constructing KGs from unstructured data, such as reviews. Traditional information extraction tools fail to understand the complex subjective information inherent in the text such as preferences. Additionally, KGs are known to be noisy and incomplete, which are hard to provide reliable explanations for recommendation results. An explainable recommender system is crucial for the product development and subsequent decision-making. To address these challenges, we introduce a novel recommender that synergies Large Language Models (LLMs) and KGs to enhance the recommendation and provide interpretable results. Specifically, we first harness the power of LLMs to augment KG reconstruction. LLMs comprehend and decompose user reviews into new triples that are added into KGs. In this way, we can enrich KGs with explainable paths that express users\u2019 preferences. To enhance the recommendation on augmented KGs, we introduce a novel subgraph reasoning module that effectively measures the importance of nodes and discovers reasoning for recommendation. Finally, these reasoning paths are fed into the LLMs to",
    "bib_name": "shi2024llmpoweredexplanationsunravelingrecommendations",
    "md_text": "# LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aeaa/aeaa9d84-29c0-4530-9c2a-12782f290861.png\" style=\"width: 50%;\"></div>\n# ABSTRACT\nRecommender systems (RecSys) are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. Knowledge graphs (KGs), which model explicit relations between users and items, have been widely used to enhance the performance of recommender systems. However, a significant challenge persists in constructing KGs from unstructured data, such as reviews. Traditional information extraction tools fail to understand the complex subjective information inherent in the text such as preferences. Additionally, KGs are known to be noisy and incomplete, which are hard to provide reliable explanations for recommendation results. An explainable recommender system is crucial for the product development and subsequent decision-making. To address these challenges, we introduce a novel recommender that synergies Large Language Models (LLMs) and KGs to enhance the recommendation and provide interpretable results. Specifically, we first harness the power of LLMs to augment KG reconstruction. LLMs comprehend and decompose user reviews into new triples that are added into KGs. In this way, we can enrich KGs with explainable paths that express users\u2019 preferences. To enhance the recommendation on augmented KGs, we introduce a novel subgraph reasoning module that effectively measures the importance of nodes and discovers reasoning for recommendation. Finally, these reasoning paths are fed into the LLMs to\n\u2217Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY \u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX\nYuxiao Li\u2217 yuxiao.li@cn.bosch.com Bosch Corporate Research Shanghai, China\ngenerate interpretable explanations of the recommendation results. Our approach significantly enhances both the effectiveness and interpretability of recommender systems, especially in cross-selling scenarios where traditional methods falter. The effectiveness of our approach has been rigorously tested on four open real-world datasets, with our methods demonstrating a superior performance over contemporary state-of-the-art techniques by an average improvement of 12%. The application of our model in a multinational engineering and technology company (METC)\u2019s cross-selling recommendation system further underscores its practical utility and potential to redefine recommendation practices through improved accuracy and user trust.\n# CCS CONCEPTS\n\u2022 Do Not Use This Code \u2192Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.\n# KEYWORDS\nExplainable Recommendation, Large Language Model, Knowledge Graph\nACM Reference Format: Guangsi Shi, Xiaofeng Deng, Linhao Luo, Lijuan Xia, Lei Bao, Bei Ye, Fei Du, Shirui Pan, and Yuxiao Li. 2018. LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning. In Woodstock \u201918: ACM Symposium on Neural Gaze Detection, June 03\u201305, 2018, Woodstock, NY. ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX\n# 1 INTRODUCTION\nDespite recommendation systems have become indispensable in mitigating information overload and enhancing user experience across modern web platforms and applications [9], most of recommendations fail to offer explanations, which is essential for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee21/ee21841c-cfae-407b-9456-e76a61f85a85.png\" style=\"width: 50%;\"></div>\nFigure 1: Main scenario and task: Arrows of different colors in KGs represent different channels (e-commerce platforms). Different products may not be sold in the same channel. The system needs cross channels to recommend and provide human-acceptable, reliable explanation descriptions.\ndecision-making by both users and e-commercial platforms. Consequently, there has been a paradigm shift in the recommendation system research community, with a growing emphasis not only on the accuracy of recommendations but also on their explainability. An explainable recommendation system significantly increases system transparency, boosts user trust and acceptance, and enhances recommendation efficiency [1]. Moreover, majority of literature focuses on natural interpretability of KG with rich structure reasoning [40] for improving recommendation system performance and explanation [18, 44], but they often fail to construct the semantic relationships such as emotion, preference pertinent to users from review. This limitation hampers the interpretability, implicit relevance, and accuracy of recommendations derived from these models with noisy and incomplete information [22]. Extracting semantic information from text for knowledge graph construction faces several limitations. Unstructured data complexity and the absence of well-defined ontologies challenge traditional tools in relation extraction and entity linking. These tools often struggle with context understanding, leading to inaccuracies in entity-relation identification and semantic interpretation [7]. Furthermore, one of the important way of KG reasoning for explainable recommender is rule-based methods [36, 39], while these approaches can only depend on the existing paths and search an optimal path for explanation, and it does not work in the cross-selling scenarios, where the potential links may not be built in the original knowledge graph. It will result in \"recommendation hallucination\" which forces explanations solely for the sake of recommending. For example, different business units in multinational engineering and technology company (METC) group operate their own channels for selling their products. To maximize\nproduction profits, it is essential to implement cross-selling across these units. Traditional strategies may not suffice for this purpose, underscoring the importance of an explainable recommendation system for effective cross-selling 1, which is crucial for METC\u2019s development. The advent of LLM, epitomized by advancements such as ChatGPT has attracted widespread attention and developed rapidly. Its excellent understanding capabilities and easy-powerful tools are capable of sophisticated reasoning and generation tasks in realworld applications [21]. However, the way of successfully applying or combining their powerful capabilities into the recommendation system is still a promising but challenging task. According to the requirement and issues above, synergizing knowledge graphs with LLMs developments present a promising road map for recommender. However, the predominant mode of implict integration between knowledge graphs and LLMs remains embedding-based strategies [33, 47]. This approach merges knowledge graph data embedding with word data embedding to inform the learning of user-item representations but often loses robust reasoning capabilities of LLMs. This deficiency is particularly pronounced when addressing diverse and specific business needs. Additionally, LLMs-enhanced KG systems [16, 17] with explicit modelling are constrained by their reliance on predefined meta-paths, necessitating a nuanced consideration of how to design effective rule paths tailored to specific business scenarios. More importantly, if fine-tuning LLMs for improving performance, it requires a lot of computing resources which is not friendly for actual marketing analysis. In response to the identified challenges, we introduce a cuttingedge framework named LLM powered Subgraph Reasoning to facilitate an explainable Recommendation system (LLM-SRR for abbreviation) which can be seen in Figure 1. This novel architecture capitalizes on the unique strengths of LLM by prompt engineering and knowledge graphs to overcome the limitations of existing recommendation systems. The LLM-SRR framework is delineated through a three-step process: First, Information extraction and KG reconstruction by LLMs: Our approach begins by extracting relevant information from user reviews. This process involves identifying new features based on predefined targets, which are critical in capturing the nuanced key works and semantic information of users by LLM\u2019s prompt engineering. A new KG integrates both original user-item relationships and the newly identified targets from user reviews. Second, Subgraph Reasoning by Attendtionbased Diffusion Scoring: This step involves the implementation of an attention mechanism for effective message passing in subgraph reasoning which skips out of the original connection pattern to search for more potential link relation, followed by a recommendation scoring process to rank the items. Last, LLMs generating Explainable Description: The LLM continues to leverage predefined keywords, the reasoning path generated by the subgraph, and the coherent descriptions acquired through thorough comprehension. Such explainable context aids front-end analysts in decision-making and planning processes. This method both enhances the recommendation\u2019s accuracy and ensures that the post-hoc explanation 1\n1Within a company group, cross-selling indicates that products from different business units/product lines of the company are successfully sold to one consumer.\npath and description behind each recommendation is transparent and understandable to both users and brands. In summary, the contribution of this paper is threefold: \u2022 Contribution 1: To the best of our knowledge, this study represents the effort to a LLM powered explainable recommendation system by subgraph reasoning. This innovative alignment introduces a novel paradigm in the domain of personalized recommendation systems, especially in a novel cross selling scenario. \u2022 Contribution 2: We have developed customizable, user-friendly tools designed specifically for explainable recommender. These tools not only facilitate understanding the semantic information of user but also provide an autonomous post-hoc explanation description by LLMs, thereby enhancing transparency and understandability in recommender for marketing analyst. \u2022 Contribution 3: The efficacy of subgraph reasoning module has been rigorously tested across three open source recommendation datasets, where it has demonstrated state-ofthe-art performance. Furthermore, its applicability has been successfully validated in a real-world scenario involving cross-selling activities at METC, where it yielded highly favorable outcomes. Therefore, our LLM-SRR architecture effectively harnesses the capabilities of subgraph for reasoning and the semantic understanding ability of LLMs to provide a highly explainable recommendation system. It addresses specific user and brand requirements through a visualizable and understandable recommendation pathway, significantly enhancing the persuasiveness of the system. Moreover, the nature of knowledge graphs, which can be updated based on unstructure information, alongside the system\u2019s ease of training, positions LLM-SRR as a robust, adaptable, and user-centric solution in the realm of explainable recommendation systems.\n# 2 RELATED WORKS\n# 2.1 Explainable Recommendation System\nExplainable recommendation systems have emerged as a pivotal enhancement in recommendation tasks, with their capacity to not only increase the efficacy of recommendations but also bolster their trustworthiness. Such systems are broadly categorized into two types: post-hoc and model-based explanations. In the realm of post-hoc explanation models, CountER [29] employs counterfactual constrained learning to derive succinct yet potent explanations for otherwise opaque recommendation models. Conversely, model-based explanations provide insights directly from the recommendation process itself. KPRN [37] utilizes LSTM to process paths within KGs from users to items, generating embeddings for these paths which are then evaluated for their relevance. Similarly, EIUM [8] focuses on explicating the semantic paths between users and items, thereby equipping the recommendation system with the capacity for path-wise explanation. RuleRec [19] introduces a rule-guided framework that derives rules from KGs for item recommendations. For a more personalized and explainable approach, PGPR [39] and ReMR [36] employ path reasoning and multi-level reasoning, respectively, through reinforcement learning to refine recommendations. Recent studies have begun to underscore the\nimportance of subgraphs within knowledge graphs for enhancing explainability. GraIL [30] pioneers inductive relationship predictions using subgraph reasoning. Moreover, GnnExplainer [42] and CF-GNNExplainer [15] elucidate Graph Neural Networks (GNNs) via subgraph analyses. Within the specific context of recommender systems, GREASE [3] innovatively employs subgraphs to furnish both factual and counterfactual explanations for GNN-based blackbox models. However, most of them fall short of explicating the model\u2019s internal workings or enhancing model performance comprehensively and cannot make full use of other additional information to help reasoning such as text information and cannot skip original relation structures.\n# 2.2 Large Language Models and Knowledge Graphs Combination\nResearchers have explored integrating KG with LLM at different stages to enhance their capabilities. During pre-training, incorporating KG aids LLM in assimilating knowledge [25]. At the inference stage, accessing KG bolsters LLM\u2019s performance in domain-specific knowledge [11]. Furthermore, KG contribute to interpreting LLM by clarifying facts [23] and elucidating the reasoning process [12], thereby improving interpretability. Moreover, knowledge graph often struggle with incompleteness [2] and text corpus processing for KG construction [48]. Leveraging LLM\u2019s generalizability, researchers are utilizing LLM to enhance KG tasks. By employing LLM as text encoders, they process textual content within KG, using the generated text representations to improve KG\u2019s comprehensiveness [45]. Furthermore, LLM is applied to extract entities and relationships from text for KG creation [10]. Recent efforts focus on designing KG prompts that transform KG structures into formats understandable by LLM, facilitating direct LLM application in tasks like KG reasoning [4]. The integration of KG and LLM have become a focal point of research, given their complementary nature [34]. This synergy aims at creating a unified framework to leverage the strengths of both, enhancing their capabilities. The Synergized Model can enhance the mutual capabilities of LLM and KG, while the technique layer incorporates various methods to boost performance further. This integrated approach can be applied to real-world such as search engines [31], recommender systems [14], and AI assistants [28], showcasing the practicality of our unified framework.\n# 2.3 Subgraph Reasoning\nSubgraph reasoning has emerged as a potent paradigm for enhancing the interpretability and performance of models across various domains and powerful ability of skip hop has been proved by [5]. A novel approach [6] that combines temporal relational attention with reverse representation updates to guide subgraph extraction. In fake news detection, a reinforcement subgraph generation method [41] alongside a hierarchical graph attention network improves both generalization and discrimination, offering clear explainability by identifying critical subgraphs. For fraud detection, SubGNN [26], leverages heterogeneous subgraphs and a relational graph isomorphism network for precise fraud identification without relying on global IDs.\nA novel approach [27] for inductive relation prediction incorporates substructure information into subgraph reasoning, significantly enhancing precision by utilizing semantic correlations between relations. Addressing scalability in KG, one-shot subgraph reasoning proposes a two-step prediction process that significantly increases efficiency and performance on large-scale KG [46]. For question answering, integrating subgraph-aware relation and direction reasoning into a novel neural model, RDAS [38], substantially improves answer precision by leveraging structure and direction information within subgraphs. CoMPILE [20] innovates by enhancing message interactions and efficiently processing asymmetric relations, showcasing significant advances over traditional models.\nIn this section, we introduce our model LLM-SRR which is shown in 2.\n# 3.1 Preliminary\nIn recommendation systems, a knowledge graph is formally defined as G, G = {(\ud835\udc52\u210e,\ud835\udc5f,\ud835\udc52\ud835\udc61)|\ud835\udc52\u210e\u2208E,\ud835\udc52\ud835\udc61\u2208E,\ud835\udc5f\u2208R}, where E is the sets of entities and R is the sets of relations. In this paper, we consider a special type of knowledge graph for recommendation system, denoted by GR. It contains a subset of a User entities U, a subset of Item entities I and Properties P(item information, user portrait and so on), where U \u222aI \u222aP \u2286E and U \u2229I \u2229P = \u2205. These three kinds of entities are connected through relations \ud835\udc5f\ud835\udc60. We give a relaxed definition of knowledge graph information injection as follows. Definition 1 - Knowledge Graph Reconstruction: Knowledge graph reconstruction is the systematic assimilation of insights from textual data into a knowledge graph utilizing a LLMs. The LLM parses unstructured text, extracting entities including key words and semantic information, and discerning their relations to form a new GR. This mechanism, represented by a function \ud835\udc53: T \u2192GR, transcribes the all kinds of predefined targets from the text into graph structures\u2014triples of the form {( \u00b4\ud835\udc52\u210e, \u00b4\ud835\udc5f, \u00b4\ud835\udc52\ud835\udc61)}\u2014thus enriching the existing KG with new, verifiable information. Example: Consider the comment text \"I like METC\u2019s wash machine colour.\" The LLM identifies user, METC, wash machine and colour, and relations like and belong, forming the GR triples: user like \u2212\u2212\u2212\u2192wash machine belong \u2212\u2212\u2212\u2212\u2212\u2192METC. Definition 2 - Sub-Graph Reasoning: In the reconstructed KGs, subgraph reasoning is defined as a sequence of diffusion from step \ud835\udc60to step \ud835\udc60+ 1 staring from the central entity \ud835\udc52\ud835\udc60,\ud835\udc56, denoted by \ud835\udc51\ud835\udc60+1,\ud835\udc56(\ud835\udc52\ud835\udc60,\ud835\udc56, \u00b7 \u00b7 \u00b7 ,\ud835\udc52\ud835\udc60,\ud835\udc57) = \ud835\udee9{\ud835\udc52\ud835\udc60,0 \ud835\udc5f\ud835\udc60,0 \u2190\u2212\u2192\ud835\udc52\ud835\udc60,1,\ud835\udc52\ud835\udc60,0 \ud835\udc5f\ud835\udc60,1 \u2190\u2212\u2192\ud835\udc52\ud835\udc60,2, \u00b7 \u00b7 \u00b7 ,\ud835\udc52\ud835\udc60,\ud835\udc56 \ud835\udc5f\ud835\udc60,\ud835\udc58 \u2190\u2212\u2192 \ud835\udc52\ud835\udc60,\ud835\udc57} , where \ud835\udc60\u2208\ud835\udc46in the s-th step of attention-based diffusion process, \ud835\udee9donates a function to select \ud835\udc41entities as the central entities for the \ud835\udc60+ 1 step, the \ud835\udc56\u2208\ud835\udc41is the \ud835\udc56-th central entity in step \ud835\udc60, especially when \ud835\udc60= 0, \ud835\udc56is the number of the users, \ud835\udc58is the \ud835\udc3e-th relations between the \ud835\udc52\ud835\udc60,\ud835\udc56and \ud835\udc52\ud835\udc60,\ud835\udc57, and \ud835\udc57\u2208\ud835\udc3dis the \ud835\udc57-th neighbour of central entity \ud835\udc52\ud835\udc60,\ud835\udc56. Definition 3 - Explainable Recommendation: Explainable Recommendation is defined as given a sequence path SP = {\ud835\udc520,\ud835\udc56 \ud835\udc5f0,\ud835\udc56 \u2190\u2192 \ud835\udc521,\ud835\udc56 \ud835\udc5f1,\ud835\udc56 \u2190\u2192\u00b7 \u00b7 \u00b7 \ud835\udc5f\ud835\udc60\u22121,\ud835\udc58 \u2190\u2212\u2212\u2212\u2192\ud835\udc52\ud835\udc60,\ud835\udc56} generated by the subgraph, especially\nUser \u2194Item in recommendation system, the goal is to find a recommendation score function \ud835\udc46(User, Item) considering the reasoning path SP. And the recommendation result, reasoning path, and the predefined information will be fed to the LLMs to generate a explanation description.\n# 3.2 KG Reconstruction and Explanation Generation\n3.2.1 LLMs for Information Extraction and Injection. Our method commences with the decomposition of information using a LLM. This process involves the extraction of new entities and relationships from a given text by predefined targets. The prompt engineering technique is employed to refine queries which guide the LLM towards precise extractions. Newly identified entities and relations are then integrated into the existing KG, resulting in a reconstructed KG that encapsulates the enriched data. There are two examples for entity and relation extraction respectively.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/759e/759eae8d-cb5e-4864-bdb2-eecbd5a872e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">This prompt facilitates the extraction of time entities and their associated events from user reviews (<Review>).</div>\nThe above prompt guides the LLM in analyzing reviews (<Review> and structuring the output as a sentiment-indicative targets. Specific rules are established to govern the integration of new information into the KG. These rules are tailored by product analyst to the scenario at hand, encompassing relations such as emotions, preferences, and quantifiable attributes (e.g., price, color, style), as well as time-connected entities such as significant dates. All extracted information will form a new link and embed it into the existing KG according to the customized set of rules.\n3.2.2 LLMs for Post-hoc Explanation Generation. In the stage of generating interpretable descriptions, we provide the large language model with meaningful contextual prompts, including predefined key targets and subgraphs or paths generated by the subgraph reasoning process. Then, according to the requirements, we generate a segment of language description. Specific instances can be referenced in Table 6 and Table7. Below is the template we need for generation.\nPrompt Example for Explainable Description: Generate an explanation for this recommendation \"<item->user>\", based on the predefined target: \"<targets>\", and the reasoning path\"<path> \". I will provide you some answering examples.\nThis prompt facilitates to generate the final explanation description based on the customized information.\nThis prompt facilitates to generate the final explanation description based on the customized information.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/324a/324aedb5-ea83-4219-8fb5-af2513a34292.png\" style=\"width: 50%;\"></div>\nFigure 2: Framework of LLM-SRR: I. Text information is extracted by the LLM and injected into the original knowledge graph by pre-define rules {u is user, p is the property, i is item, r is relation}; II. The attention score is calculated by the neighbours in different diffusion layers and subgraph could be generated. III. The final recommendation score is computed by the similarity function between the user and item, where a explanation path can be generated by LLM in this component.\n# 3.3 Attentive Diffusion Subgraph Reasoning\nWe introduce an attention-based mechanism to construct a entitycentric subgraph, harnessing the entity\u2019s interaction history and contextual relevance within the knowledge graph. Initially, the subgraph centers around the user, progressively expanding by assimilating nodes based on their attention scores which signify their contextual importance to the entity. At any given diffusion step \ud835\udc60, we commence with the current entity subgraph G\ud835\udc60,\ud835\udc52and identify the set of newly added nodes, designated as neighbor nodes N\ud835\udc60,\ud835\udc52. Specifically, the initial subgraph G0,\ud835\udc62contains solely the user node, with neighbours N0,\ud835\udc62. The next phase involves computing the attention scores for the edges \ud835\udc38\ud835\udc60,\ud835\udc52 connected to N\ud835\udc60,\ud835\udc52. This set of \ud835\udc38\ud835\udc60,\ud835\udc52encompasses relationships between the central entity G\ud835\udc60,\ud835\udc52and their N\ud835\udc60,\ud835\udc52. The attention mechanism then evaluates the importance of these edges, determining the significance of the \ud835\udc38\ud835\udc60,\ud835\udc52to the G0,\ud835\udc62. Consequently, entity scores are calculated based on the aggregated attention scores of their associated edges, selecting the top \ud835\udc41nodes \ud835\udc41(N\ud835\udc60,\ud835\udc52) from the entire set of one-hop neighbors, rather than \ud835\udc41nodes per neighbor node. These top-scored nodes are then appended to the subgraph, diffusing it to \ud835\udc54\ud835\udc60+1,\ud835\udc52. In the step \ud835\udc60, specifically, the attention module is responsible for assigning scores to edges based on their relevance to the entity embedding. In this stage, the attention score \ud835\udefcof an edge (\ud835\udc52\ud835\udc56,\ud835\udc5f,\ud835\udc52\ud835\udc57) is computed by the following equations\n(2)\n\ufffd ()\u2208 where \ud835\udf03= Sigmoid, \ud835\udeff= LeakyReLU, \ud835\udc4a1 \u2208R\ud835\udc511\ud835\udc4b2\ud835\udc51, \ud835\udc4a2 \u2208R\ud835\udc51\ud835\udc4b\ud835\udc511, \ud835\udc511 is a parameter that controls the size of the trainable matrices \ud835\udc4a1 and \ud835\udc4a2, \u210e\ud835\udc52\ud835\udc62\u2208R\ud835\udc51and \u210e\ud835\udc52\ud835\udc62represents the emedding of user entities \ud835\udc62, \u210e\ud835\udc52\ud835\udc56,\u210e\ud835\udc52\ud835\udc57\u2208R\ud835\udc51and \u210e\ud835\udc52\ud835\udc56,\u210e\ud835\udc52\ud835\udc57represents the embedding of entities \ud835\udc52, (\ud835\udc4e\u2225\ud835\udc4f) denotes concatenation of embedding vectors \ud835\udc4eand \ud835\udc4f, \ud835\udefc(\ud835\udc52\ud835\udc56,\ud835\udc5f,\ud835\udc52\ud835\udc57) represents the attention score of edge (\ud835\udc52\ud835\udc56,\ud835\udc5f,\ud835\udc52\ud835\udc57) at step \ud835\udc60. The edge (\ud835\udc52\ud835\udc56,\ud835\udc5f,\ud835\udc52\ud835\udc57) is from the set of edges \ud835\udc38\ud835\udc60,\ud835\udc52. After obtaining the attention scores of the edges, \ud835\udc52\ud835\udc57entity scores are derived by aggregating the attention scores from the edges to their corresponding entities. \u2211\ufe01\n(3)\n\ufffd \u2208\ufffd where score(\ud835\udc52) represents the attention score of entity and \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc52\ud835\udc56) has been calculated in the last run. Then s-th step user subgraph is updated by adding the m highest scoring nodes to the user subgraph. After selecting the top \ud835\udc41(\ud835\udc41<= \ud835\udc3d) score neighbours from the all neighbours \ud835\udc52\ud835\udc57, the \ud835\udc41(N\ud835\udc60,\ud835\udc52) need to be re-scored by normalization. Additionally, \ud835\udc41is a hyperparameter used to control the subgraph size. \ud835\udc63\ud835\udc5b\u2208\ud835\udc49\ud835\udc5bwill become the new central entity in step \ud835\udc60+ 1.\n(5)\n# 3.4 Recommendation Scoring\nAfter constructing the user subgraph through the aforementioned attention-based scoring diffusion method, the next stage is to compute the recommendation scores for item nodes within this SubKnowledge-Graph that are connected to this user subgraph. The recommendation scoring process, visualized in 2, moves beyond linear reasoning by encoding the relationships between different paths within the user subgraph. This allows the model to capture a comprehensive representation of user interests and to reflect the interplay of these paths when generating recommendations. Initially, we delineate the subgraph pertinent to the item node from the overarching user subgraph. This subgraph is then encoded to extract user preferences using a subgraph encoder. The encoding process is formulated as follows:\n(6)\n(7)\n  where \u210e\ud835\udc63\u2208R\ud835\udc51and \u210e\ud835\udc54\ud835\udc60represents the embedding of subgraph \ud835\udc54 including \ud835\udc63\ud835\udc60,\ud835\udc56in step \ud835\udc60. The set \ud835\udc49\ud835\udc62\ud835\udc58comprises the lists of nodes at a distance \ud835\udc58from the user node within the subgraph. \ud835\udc4a3 \u2208R \u02c6\ud835\udc512\ud835\udc4b3\ud835\udc51, \ud835\udc4a4 \u2208R\ud835\udc51\ud835\udc4b\u02c6\ud835\udc512, \u02c6\ud835\udc512 is a parameter that controls the size of the trainable matrices \ud835\udc4a3 and \ud835\udc4a4. The user \ud835\udc62subgraph is composed of nodes user \ud835\udc62, \ud835\udc541 and \ud835\udc542, so \u210e\ud835\udc62,{\ud835\udc541,\ud835\udc542} \u2208R\ud835\udc51represents the embedding of the user \ud835\udc62subgraph. Subsequently, the similarity score between this subgraph representation and the item\u2019s embedding is calculated:\n(8)\n()({} \u00b7) where sim(\ud835\udc62,\ud835\udc56\ud835\udc61\ud835\udc5a) is the similarity score between user \ud835\udc62and the m-th item \ud835\udc56\ud835\udc61\ud835\udc5a. To account for the connection between the subgraph and the full user subgraph, we integrate the score derived during subgraph construction as the weight for each subgraph. This weighted score is computed as follows: \u2211\ufe01\n(9)\n\u2211\ufe01 \u2208 Here, S\ud835\udc62,\ud835\udc56\ud835\udc61\ud835\udc5adenotes the final recommendation score for an item \ud835\udc56\ud835\udc61\ud835\udc5arelative to user \ud835\udc62, incorporating both the encoded subgraphuser-item relationship and the subgraph\u2019s relevance within the user subgraph. The existing objective function only guides the generation of the subgraph and does not indicate which subgraph truly reflects the user\u2019s preferences. Therefore, we introduce a specific optimization objective for the recommendation module, defined as:\n(10)\n\u2211\ufe01 \u2208() This objective function minimizes the negative log-likelihood over the set of items positively associated with the user \ud835\udc62, denoted by \ud835\udc4c(\ud835\udc62), refining the model\u2019s ability to deduce user preferences from subgraphs.\n# EXPERIMENTS\nIn this section, our experiment is mainly divided into two parts. The first part is a performance experiment, which mainly reflects the recommended performance of our subgraph-based inference by comparing it with other baselines. In the second part, we use a case study to explore the contribution of our model\u2019s LLM information injection to the entire recommendation system, and tests the improvement ability of subgraph (SG) and parameter sensitivity of our design module. In the experiments of this work, we used entity representations pre-trained via TransE-based in ReMR.\n<div style=\"text-align: center;\">Table 1: Statistics of the datasets.</div>\nMETC\nBeauty\nCell Phones\nClothing\n#User\n1004\n22,363\n27,879\n39,387\n#Items\n1017\n12,101\n10,429\n23,033\n#Entities\n2482\n224,080\n163,255\n425,534\n#Relations\n12\n16\n16\n16\n#Interactions\n3636\n198.58K\n194.32K\n278.86K\n#Triples\n129980\n37.73M\n37.01M\n36.37M\n# 4.1 Experiment Settings\n4.1.1 Datasets. We conducted experiments on three datasets, among them are the Amazon review dataset collated in KGAT[35], and METC dataset. METC dataset is a collection of METC order data from different channels. We used three datasets from the Amazon dataset, namely Cell Phones, Beauty and Clothing. When dealing with the Amazon dataset, we followed the data processing methods in PGPR[39]. The details of the dataset statistics are shown in Table 1.\n4.1.1 Datasets. We conducted experiments on three datasets, among them are the Amazon review dataset collated in KGAT[35], and METC dataset. METC dataset is a collection of METC order data from different channels. We used three datasets from the Amazon dataset, namely Cell Phones, Beauty and Clothing. When dealing with the Amazon dataset, we followed the data processing methods in PGPR[39]. The details of the dataset statistics are shown in Table 1. 4.1.2 Baselines. We consider six recommendation approaches as baselines in the following experiments. These baselines are divided into three categories, Matrix Factorization-based models, KG embedding models and path reasoning models. BPR[24]: BPR is a personalized ranking algorithm based only on user product interaction information through Bayesian posterior optimization. DKN[32]: This is a recommendation model that combines knowledge graph reality and convolutional neural network CKE[43]: CKE uses TransR[13] to obtain semantic embeddings from the knowledge graph to enhance collabrative filtering. KGAT[35]: KGAT learns entity embeddings from knowledge graphs through graph attention networks combined with GNN and attention mechanisms. PGPR[39]: PGPR is a knowledge graph path reasoning model based on reinforcement learning. ReMR[36]: ReMR relies on its own data to obtain higher-order abstraction information for the entities in the knowledge graph to create a multi-layer knowledge graph. 4.1.3 Evaluation Criteria. For all approaches, we adopted four evaluation criteria to evaluate the top-5 recommendations of each user in the test set, including Normalized Discounted Cumulative Gain (NDCG), Recall, Hit Rate (HR), and Precision (Prec.).\n<div style=\"text-align: center;\">able 2: Performance on top-10 recommendation between the baselines and our model. The results are computed in the test set nd are given as percentages %. The best baseline results are underlined.</div>\nBeauty\nCell Phones\nClothing\nMeasures(%)\nNDCG\nRecall\nHR\nPrec.\nNDCG\nRecall\nHR\nPrec.\nNDCG\nRecall\nHR\nPrec.\nBPR\n2.805\n5.032\n8.933\n1.173\n1.995\n3.534\n5.424\n0.623\n0.665\n1.219\n1.932\n0.323\nDKN\n1.923\n2.591\n8.812\n1.135\n1.672\n3.313\n4.580\n0.349\n0.375\n0.724\n1.492\n0.119\nCKE\n3.824\n6.241\n11.132\n1.422\n3.849\n6.981\n10.633\n1.073\n1.656\n2.604\n4.329\n0.390\nKGAT\n5.020\n7.794\n12.496\n1.535\n4.803\n7.982\n11.241\n1.134\n2.824\n4.674\n6.993\n0.603\nPGPR\n5.489\n8.324\n14.347\n1.692\n4.921\n8.383\n11.832\n1.280\n2.863\n4.797\n7.024\n0.719\nReMR\n5.878\n8.982\n15.606\n1.906\n5.294\n8.724\n12.498\n1.337\n2.977\n5.110\n7.426\n0.766\nLLM-SRR\n6.187\n9.788\n16.103\n1.928\n5.755\n9.753\n13.378\n1.433\n3.520\n6.050\n8.739\n0.922\nImprovement(%)\n+5.257\n+8.974\n+3.185\n+1.154\n+8.708\n+11.795\n+7.041\n+7.180\n+17.450\n+18.395\n+17.681\n+20.366\n4.1.4 Implementation Details. In our model, the entity embedding dimensionality is 100.The hyperparameter of subgraph size is set to a a maximum of 100. We train the parameters with Adam optimization, batch size of 256, and a number of training epochs of 10.\n# 4.2 Performance Experiments\n4.2.1 Performance Comparison. We show the top-10 recommendation performance of our proposed method compared to all baselines. And comparison with the model ReMR is not possible in METC dataset because of the lack of higher-order abstract information of entities. The specific information is shown in Table 2. Table 2 shows that our method consistently outperforms all baselines in terms of recommendation accuracy. On average, our model improves NDCG, Recall, HR and Precision by 5.36%, 9.33%, 6.58%, and 6.46%. This demonstrates that subgraph reasoning with LLM can help better infer user interests and improve recommendation performance. It is noted that the algorithms that focus on node representation learning are not very effective on the Amazon data set. Maybe Amazon\u2019s knowledge graph contains more information and user interests are more complex, so it is not appropriate to only focus on node embedding. And LLM-SRR has the greatest performance improvement in recall, which to a certain extent shows that LLM-SRR can fully learn user interests.\n# 4.3 Case Study for METC\n4.3.1 Brief Introduction. In order to illustrate the explanation in recommendation process, we do a case study in METC private dataset to visualize the reasoning paths and the improvement by LLM. METC\u2019s product families are all durable goods and in diverse shopping domains and channels. Based on the market analysis, more than half of METC consumers tend to purchase more than two categories of products in more than two e-commercial channels. Hence, analysing the decision making path of consumers and providing an accurate and explainable recommendation is challenging but critical for METC business growth. The METC dataset includes three key aspects. 1) User table: it includes the user attributes, e.g., id, profiles, reviews, and preferred channels. 2) Order log: it includes the details of each order, e.g., user id, product id, product properties, review of the order. 3) User\n<div style=\"text-align: center;\">Table 3: Results in METC Dataset</div>\nMETC\nSubgraph Size\nNDCG\nRecall\nHR\nPrec.\nBPR\n14.961\n20.941\n23.705\n2.420\nDKN\n11.137\n18.034\n19.821\n2.042\nCKE\n9.436\n19.687\n22.709\n2.371\nKGAT\n14.707\n21.847\n23.606\n2.410\nPGPR\n17.767\n24.253\n27.191\n2.799\nReMR\n-\n-\n-\n-\nLLM-SRR\n18.144\n26.006\n29.781\n3.108\nImprovement(%)\n+2.122\n+7.228\n+9.525\n+11.040\nactivities: it includes the event tracing data in the online/offline channels such as the click and view. The Table 3 shows that our proposed model outperforms the baselines in all four measures.\n4.3.2 Explanation Visualization. Figure 3(a) illustrates how our model show an explainable result and echo the business questions in the METC application. It can be seen that for auto air filter users, the property like car owners and METC premium user(user profiles), Channel 1 and Channel 2 (selling channels like E-commerce platforms), reliable and no smell (reviews), and auto wiper purchasing (cross-selling pair) are the main contributors in the recommendations, which indicates how the majority of users made their decisions. The red nodes are generated by the LLM and the blue dashed line is one of the most probable reasoning path. Finally, the item4-oven is highly probable to recommend to the user1.(PS: the number in different nodes is the score for user1). In Figure 3(b), it is another typical scenario. User5 has only bought METC home appliances on Channel 1. User6 bought different METC prodution in different channels such as METC home appliances on Channel 1, METC power tools on Channel 3, and METC car accessories on Channel 4. User7 only bought METC car accessories on Channel 4. We finally recommended METC cordless drill to user5 and user6. Explanation path comparison between path-based method and ours has been shown in Table 6 and Table 7.\n4.3.3 Observations and Analysis. In cases exemplified by example1 and example2, although the target items in the list, while\n<div style=\"text-align: center;\">Table 4: Ablation study</div>\nMETC\nMeasures(%)\nNDCG\nRecall\nHR\nPrec.\nw/o Review\n16.378\n23.045\n25.863\n2.878\nw/o SG\n15.964\n21.582\n22.266\n2.246\nFull model\n18.144\n26.006\n29.781\n3.108\n<div style=\"text-align: center;\">Table 5: Influence of Subgraph Size</div>\nMETC\nSubgraph Size\nNDCG\nRecall\nHR\nPrec.\n60\n17.258\n23.433\n26.494\n2.739\n80\n18.144\n26.006\n29.781\n3.108\n100\n17.584\n24.750\n28.586\n2.928\nthe path-based method can invariably generate an existed path to suggest a target product to a specific user, where the paths seem far-fetched and even make no sense. This is named as one type of \"recommendation hallucination\". Furthermore, other items in the recommendation list for user that exist similar explainable paths, which can not reflect the requirement of users. In example3 and example4, the path-based recommendation approach indiscriminately suggests all products associated with User6 to both User5 and User7. For instance, car accessories are recommended to User5, and home appliances to User7. This strategy can lead to what may be described as another \"recommendation hallucination\" where the rationale provided by the system does not substantiate the recommendations made, thus potentially exerting a detrimental impact on decision-making processes. In contrast, utilizing the reliable explanation paths offered by our method suggests that the recommendation system gains a more profound understanding of the relationships between users and items. Consequently, the items that appear in the recommendation list are more accurately aligned with the user\u2019s needs.\n4.3.4 Ablation Studies. On the METC dataset, We further study the importance of each module of our model in Table 4, where w/o Review means that our remove the entities which are extracted from user review and w/o SG is our model that removes subgraph generating. The experimental results shows that, after removing these modules, the noise could not be attenuated and uncompleted information could interfere with reasoning process, leading to poor results, while the subgraph generation mechanism of our model can better capture user interests.\n# 4.3.5 Parameters sensitivity. As can be seen in Table 5, hyperparameter of subgraph size indicates the maximum numbe\n4.3.5 Parameters sensitivity. As can be seen in Table 5, the hyperparameter of subgraph size indicates the maximum number of nodes for subgraph expansion in each round of subgraph reasoning. Larger hyperparameter means that we generate larger subgraphs with more nodes in them. Larger subgraphs tend to contain more information, which on the one hand increases the tolerance for the performance of the subgraph inference module, but on the other hand it increases the performance requirements for the subgraph scoring module.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e80/6e80378b-5bf6-4eec-a30c-19d145faa7f8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Case study 1</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f31/2f3133af-da61-4cb5-8560-b07344dcdc7e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Case study 2</div>\nFigure 3: Real cases discovered by our model, each containing a subgraph which end nodes is predicted item by recommendation model. u1 \u2013 u4 : METC users, i1 \u2013 i4 : Auto air filter related products, R_R1: \u201creview: reliable\u201d, R_R2 : \u201creview: no smell\u201d, O_AW: \u201corder: auto wiper\u201d, S_HD: \u201csearch: Home Decoration \u201d, C_1: \u201cchannel: Channel 1 auto parts flagship store\u201d, P_CO: \u201cprofile: car owner\u201d , C_2: \u201cchannel: Channel 2\u201d, P_PU: \u201cprofile: premium user\u201d, C_3: \u201cchannel: Channel 3 \u201d, C_4: \u201cchannel: Channel 4\u201d.\n# 5 CONCLUSION\nIn this paper, we present a novel integration of a LLM, and subgraph of KG generation to foster the development of an explainable recommendation system. This represents the inaugural effort to amalgamate these advanced technologies for enhancing recommendation systems. Specifically, the LLM is leveraged to distill key information from textual data, which is then systematically incorporated into the KG using pre-defined rules. Furthermore, we\n<div style=\"text-align: center;\">Table 6: Case 1 of explanation comparison by Path-based reasoning and LLM-SRR.</div>\nExample 1\nMETC Oven(\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a4) \u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f1\nPath-based reasoning\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f1 \u2192purchase \u2192auto wiper \u2192\nsold by \u2192C_1 \u2192sale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a4\nExplanation\nBecause the user purchased a auto wiper directly from METC\nand Channel 1 platform sale the Channel 2 production in-\ncluding auto wiper, wash machine, oven and so on, system\nrecommends oven to the user.\nSubgraph-based reasoning by ours\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f1 \u2192review \u2192reliable \u2192C_1 \u2192\nsale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a4\nExplanation\nUser commented the METC heating system with reliable\nand no smell. system guesses that user pay more attention to\n\"reliable\". Channel 1 is a reliable platform which sale number\nof METC production including auto wiper, wash machine,\noven and so on. Thus system recommends oven to the user.\nExample 2\nMETC Heating System (\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a3) \u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f4\nPath-based reasoning\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f4 \u2192profile \u2192car owner \u2192tag \u2192\nC_1 \u2192sale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a3\nExplanation\nUser was labeled car owner as his one of profile and Channel\n1 platform has the same tag, and Channel 1 sale number of\nMETC production including auto wiper, wash machine, oven\nand so on so the system recommends wash machine to this\nuser.\nSubgraph-based reasoning by ours\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f4 \u2192register \u2192premium \u2192read\n\u2192Wechat \u2192content \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a3\nExplanation\nThis user is a premium of METC in Wechat and read METC\nwashing machine article as well. system guesses that this\nuser have high probable willing to buy METC wash machine.\nAlso, Channel is one of the biggest sale platform who sale\nnumber of METC production including auto wiper, wash\nmachine, oven and so on so the system recommends wash\nmachine to this user.\n<div style=\"text-align: center;\">Table 7: Case 2 of explanation comparison by Path-based reasoning and LLM-SRR.</div>\nExample 3\nMETC Cordless Drill (\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a7) \u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5\nPath-based reasoning\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5 \u2192search \u2192Home Decoration \u2192\nkey words \u2192C_1 \u2192purchase \u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f6\n\u2192purchase \u2192C_3 \u2192sale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a7\nExplanation\nBecause \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5 and \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f6 bought METC household appli-\nances in Channel 1, system guesses what \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f6 bought in\nChannel 3 is suitable for \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5. Thus, system recommends\ncordless drill to \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5\nSubgraph-based reasoning by ours\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5 \u2192search \u2192Home decoration \u2192\nkey words \u2192C_3 \u2192sale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a7\nExplanation\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5 searched home decoration as key words in Channel\n1 and bought some household appliances, system guesses\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5 needs some tools to install the household appliances,\nso system recommends cordless drill to \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f5\nExample 4\nMETC Cordless Drill (\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a7) \u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7\nPath-based reasoning\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7 \u2192profile \u2192car owner \u2192tag \u2192\nC_4 \u2192purchase \u2192\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f6 \u2192purchase\n\u2192C_3 \u2192sale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a6\nExplanation\nBecause \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7 and \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f6 bought METC car battery in Chan-\nnel 4, we guess what \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f6 bought in Channel 3 is suitable\nfor \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7. Thus system recommends cordless drill to \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7\n.\nSubgraph-based reasoning by ours\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7 \u2192profile \u2192car owner \u2192tag \u2192\nC_3 \u2192sale \u2192\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a6\nExplanation\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7 is a car owner and bought some car accessories such\nas battery and auto wiper\u201d in Channel 4, so system guesses\n\ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7 need some power tools to install these accessories, so\nsystem recommends METC cordless drill to \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f7\nintroduce an attention-based diffusion mechanism for the generation of subgraphs, facilitating the construction of nuanced user representations, which is used for calculating the recommendation score with the item profiles. To evaluate the effectiveness of our proposed model, we conducted a series of experiments across three publicly available datasets and one proprietary dataset from METC. The empirical results unequivocally demonstrate that our model outperforms existing state-of-the-art models across all datasets. Importantly, the case studies underscore our model\u2019s capacity to provide explicit, user-centric explanations for recommendations, effectively addressing specific user comments, relieving the \"recommendation hallucination\" effectively. Moreover, our framework unveils substantial potential for further optimization in terms of efficiency and opens new avenues for\nresearch in recommendation systems employing Large Language Models. This underscores the promising intersection of advanced computational models and practical application domains, heralding a new era of intelligent, user-focused recommendation systems. We hope that our work will provide some inspiration for methods in the same scenario, particularly in the integration of Large Language Models and Knowledge Graphs, and offer strategic insights for market analysts. For instance, in the decision-making process, because the system gives high score for some key words in review, marketing analyst could pre-define these key words for user review, thereby enhancing the system\u2019s robustness.\n# REFERENCES\n[1] Krisztian Balog and Filip Radlinski. 2020. Measuring recommendation explanation quality: The conflicting goals of explanations. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 329\u2013338. [2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NeurIPS, Vol. 26. [3] Ziheng Chen, Fabrizio Silvestri, Jia Wang, Yongfeng Zhang, Zhenhua Huang, Hongshik Ahn, and Gabriele Tolomei. 2022. Grease: Generate factual and counterfactual explanations for gnn-based recommendations. arXiv preprint arXiv:2208.04222 (2022). [4] Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, and Yong Dou. 2023. Incorporating structured sentences with time-enhanced bert for fully-inductive temporal relation prediction. In SIGIR. [5] Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. 2022. How powerful are k-hop message passing graph neural networks. Advances in Neural Information Processing Systems 35 (2022), 4776\u20134790. [6] Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. 2020. Explainable subgraph reasoning for forecasting on temporal knowledge graphs. In International Conference on Learning Representations. [7] Marvin Hofer, Daniel Obraczka, Alieh Saeedi, Hanna K\u00f6pcke, and Erhard Rahm. 2023. Construction of knowledge graphs: State and challenges. arXiv preprint arXiv:2302.11509 (2023). [8] Xiaowen Huang, Quan Fang, Shengsheng Qian, Jitao Sang, Yan Li, and Changsheng Xu. 2019. Explainable interaction-driven user modeling over knowledge graph for sequential recommendation. In Proceedings of the 27th ACM international conference on multimedia. 548\u2013556. [9] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. 2022. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems 33, 2 (2022), 494\u2013514. [10] Abhijeet Kumar, Abhishek Pandey, Rohit Gadia, and Mridul Mishra. 2020. Building knowledge graph using pre-trained language model for learning entity-aware relationships. In 2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON). IEEE, 310\u2013315. [11] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, Vol. 33. 9459\u20139474. [12] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In EMNLPIJCNLP. 2829\u20132839. [13] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. [n. d.]. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the AAAI conference on artificial intelligence. [14] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149 (2023). [15] Ana Lucic, Maartje A Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri. 2022. Cf-gnnexplainer: Counterfactual explanations for graph neural networks. In International Conference on Artificial Intelligence and Statistics. PMLR, 4499\u20134511. [16] Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Chatrule: Mining logical rules with large language models for knowledge graph reasoning. arXiv preprint arXiv:2309.01538 (2023). [17] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2024. Reasoning on graphs: Faithful and interpretable large language model reasoning. In International Conference on Learning Representations. [18] Linhao Luo, Kai Liu, Dan Peng, Yaolin Ying, and Xiaofeng Zhang. 2020. A motif-based graph neural network to reciprocal recommendation for online dating. In Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 23\u201327, 2020, Proceedings, Part II 27. Springer, 102\u2013114. [19] Weizhi Ma, Min Zhang, Yue Cao, Woojeong Jin, Chenyang Wang, Yiqun Liu, Shaoping Ma, and Xiang Ren. 2019. Jointly learning explainable rules for recommendation with knowledge graph. In The world wide web conference. 1210\u20131221. [20] Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. 2021. Communicative message passing for inductive relation reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4294\u20134302. [21] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering (2024). [22] Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne. 2023. Knowledge graphs: Opportunities and challenges. Artificial Intelligence Review 56, 11 (2023), 13071\u201313102. [23] Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases?. In EMNLP-IJCNLP. 2463\u20132473.\n[24] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. Bpr: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [25] Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, and Saurabh Tiwary. 2020. Knowledge-aware language model pretraining. arXiv preprint arXiv:2007.00655 (2020). [26] Junshuai Song, Xiaoru Qu, Zehong Hu, Zhao Li, Jun Gao, and Ji Zhang. 2021. A subgraph-based knowledge reasoning method for collective fraud detection in e-commerce. Neurocomputing 461 (2021), 587\u2013597. [27] Kai Sun, HuaJie Jiang, Yongli Hu, and BaoCai Yin. 2023. Substructure-aware subgraph reasoning for inductive relation prediction. The Journal of Supercomputing 79, 18 (2023), 21008\u201321027. [28] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, and et al. 2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137 (2021). [29] Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang. 2021. Counterfactual explainable recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 1784\u20131793. [30] Komal Teru, Etienne Denis, and Will Hamilton. 2020. Inductive relation prediction by subgraph reasoning. In International Conference on Machine Learning. PMLR, 9448\u20139457. [31] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, and et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022). [32] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. Dkn: Deep knowledge-aware network for news recommendation. In Proceedings of the 2018 world wide web conference. 1835\u20131844. [33] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021. Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics 9 (2021), 176\u2013194. [34] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021. KEPLER: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics 9 (2021), 176\u2013194. [35] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 950\u2013958. [36] Xiting Wang, Kunpeng Liu, Dongjie Wang, Le Wu, Yanjie Fu, and Xing Xie. 2022. Multi-level recommendation reasoning over knowledge graphs with reinforcement learning. In Proceedings of the ACM Web Conference 2022. 2098\u20132108. [37] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. 2019. Explainable reasoning over knowledge graphs for recommendation. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5329\u20135336. [38] Xu Wang, Shuai Zhao, Bo Cheng, Jiale Han, Li Yingting, Hao Yang, Ivan Sekulic, and Guoshun Nan. 2021. Integrating subgraph-aware relation and direction reasoning for question answering. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 7808\u20137812. [39] Yikun Xian, Zuohui Fu, Shan Muthukrishnan, Gerard De Melo, and Yongfeng Zhang. 2019. Reinforcement knowledge graph reasoning for explainable recommendation. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. 285\u2013294. [40] Xiaoran Xu, Wei Feng, Yunsheng Jiang, Xiaohui Xie, Zhiqing Sun, and Zhi-Hong Deng. 2020. Dynamically pruned message passing networks for large-scale knowledge graph reasoning. In International Conference on Learning Representations ICLR. [41] Ruichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li, Jianxun Lian, and Xing Xie. 2022. Reinforcement subgraph reasoning for fake news detection. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2253\u20132262. [42] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. 2019. Gnnexplainer: Generating explanations for graph neural networks. Advances in neural information processing systems 32 (2019). [43] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 353\u2013362. [44] He Zhang, Bang Wu, Xingliang Yuan, Shirui Pan, Hanghang Tong, and Jian Pei. 2024. Trustworthy graph neural networks: Aspects, methods, and trends. Proc. IEEE (2024). [45] Zhiyuan Zhang, Xiaoqian Liu, Yi Zhang, Qi Su, Xu Sun, and Bin He. 2020. Pretrainkge: learning knowledge representation from pretrained language models. In EMNLP Finding. 259\u2013266. [46] Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Bo Han, and et al. 2023. Less is more: One-shot subgraph reasoning on large-scale knowledge graphs. In The Twelfth International Conference on Learning Representations.\n[47] Hongyin Zhu, Hao Peng, Zhiheng Lyu, Lei Hou, Juanzi Li, and Jinghui Xiao. 2023. Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation. Expert Systems with Applications 215 (2023), 119369. [48] Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. LLMs for knowledge graph\nconstruction and reasoning: Recent capabilities and future opportunities. arXiv preprint arXiv:2305.13168 (2023). Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of enhancing user experiences in recommender systems through the integration of Knowledge Graphs (KGs) and Large Language Models (LLMs). Despite the advancements in recommendation systems, traditional methods struggle with constructing KGs from unstructured data, leading to noisy and incomplete information which hampers the interpretability and reliability of recommendations. This study introduces a novel method that synergizes LLMs and KGs to improve both the effectiveness and interpretability of recommendations.",
        "problem": {
            "definition": "The primary problem is the inability of existing recommendation systems to provide reliable explanations due to the challenges in constructing KGs from unstructured data, such as user reviews, which often contain complex subjective information.",
            "key obstacle": "The main challenge lies in the noisy and incomplete nature of KGs, which limits the ability to generate accurate and interpretable explanations for recommendation results."
        },
        "idea": {
            "intuition": "The idea is inspired by the understanding that LLMs can effectively process and decompose user reviews into meaningful triples, which can then be integrated into KGs to enhance their completeness and interpretability.",
            "opinion": "The proposed approach involves using LLMs to augment KG reconstruction and employing a subgraph reasoning module to discover reasoning paths for recommendations, thus improving the explainability of the recommendations.",
            "innovation": "The key innovation is the introduction of the LLM-Powered Subgraph Reasoning (LLM-SRR) framework, which combines LLMs and KGs in a way that previous methods have not, particularly in cross-selling scenarios."
        },
        "method": {
            "method name": "LLM-Powered Subgraph Reasoning",
            "method abbreviation": "LLM-SRR",
            "method definition": "LLM-SRR is a framework that integrates LLMs and KGs to enhance the accuracy and interpretability of recommendations through a structured reasoning process.",
            "method description": "The core of LLM-SRR involves extracting information from user reviews using LLMs, reconstructing KGs, and utilizing subgraph reasoning to derive meaningful recommendations.",
            "method steps": [
                "Information extraction and KG reconstruction using LLMs.",
                "Subgraph reasoning through an attention-based diffusion mechanism.",
                "Generating explainable descriptions of recommendations using LLMs."
            ],
            "principle": "The effectiveness of this method stems from its ability to leverage the strengths of LLMs in understanding natural language and the structured representation of information in KGs, allowing for more nuanced and user-centric recommendations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on four open real-world datasets, including a proprietary dataset from a multinational engineering and technology company (METC), comparing LLM-SRR against various baseline methods to assess its performance.",
            "evaluation method": "Performance was evaluated using metrics such as Normalized Discounted Cumulative Gain (NDCG), Recall, Hit Rate (HR), and Precision, with a focus on the top-5 recommendations for users."
        },
        "conclusion": "The results indicate that the LLM-SRR framework significantly outperforms existing state-of-the-art methods, achieving an average improvement of 12% across metrics. The model's application in METC's cross-selling recommendation system highlights its practical utility and potential to enhance user trust through improved accuracy and interpretability.",
        "discussion": {
            "advantage": "The LLM-SRR approach provides clear advantages in terms of accuracy and interpretability of recommendations, effectively addressing the challenges of noisy and incomplete KGs while offering reliable explanations.",
            "limitation": "One limitation of the method is its reliance on the quality of the user reviews for information extraction, which can vary and potentially affect the completeness of the reconstructed KGs.",
            "future work": "Future research could focus on refining the information extraction process and exploring additional ways to enhance the robustness of the reasoning paths generated by the model."
        },
        "other info": {
            "additional findings": "The model demonstrated significant improvements in recall and precision, indicating its ability to better capture user interests. The attention-based diffusion mechanism proved effective in constructing meaningful user representations."
        }
    },
    "mount_outline": [
        {
            "section number": "1.3",
            "key information": "This paper addresses the issue of enhancing user experiences in recommender systems through the integration of Knowledge Graphs (KGs) and Large Language Models (LLMs)."
        },
        {
            "section number": "2.3",
            "key information": "The paper introduces a novel method that synergizes LLMs and KGs to improve both the effectiveness and interpretability of recommendations."
        },
        {
            "section number": "3.2",
            "key information": "The proposed approach involves using LLMs to augment KG reconstruction and employing a subgraph reasoning module to discover reasoning paths for recommendations."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of the LLM-Powered Subgraph Reasoning (LLM-SRR) framework stems from its ability to leverage the strengths of LLMs in understanding natural language."
        },
        {
            "section number": "8.1",
            "key information": "The LLM-SRR framework integrates LLMs and KGs to enhance the accuracy and interpretability of recommendations through a structured reasoning process."
        },
        {
            "section number": "10.1",
            "key information": "The main challenge lies in the noisy and incomplete nature of KGs, which limits the ability to generate accurate and interpretable explanations for recommendation results."
        },
        {
            "section number": "11",
            "key information": "The results indicate that the LLM-SRR framework significantly outperforms existing state-of-the-art methods, achieving an average improvement of 12% across metrics."
        }
    ],
    "similarity_score": 0.7528014612948729,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLM-Powered Explanations_ Unraveling Recommendations Through Subgraph Reasoning.json"
}