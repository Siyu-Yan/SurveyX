{
    "from": "google",
    "scholar_id": "499OQpFXhFIJ",
    "detail_id": null,
    "title": "Out-of-Distribution Detection us",
    "abstract": " Abstract\n\nDeep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.\n\n# 1 Introduction\n\nDeep Neural Networks (DNNs) have gained lots of success after enabling several breakthroughs in notably challenging problems such as image classification [12], speech recognition [1] and machine translation [4]. These models are known to generalize well on inputs that are drawn from the same distribution as of the examples that were used to train the model [43]. In real-world scenarios, the input instances to the model can be drawn from different distributions, and in these cases, DNNs tend to perform poorly. Nevertheless, it was observed that DNNs often produce high confidence predictions for unrecognizable inputs [33] or even for a random noise [13]. Moreover, recent works in the field of adversarial examples generation show that due to small input perturbations, DNNs tend to produce high probabilities while being greatly incorrect [11, 6, 17]. When considering AI Saftey, it is essential to train DNNs that are aware of the uncertainty in the predictions [2]. Since DNNs are ubiquitous, present in nearly a",
    "bib_name": "Out-of-Dis1",
    "md_text": "# Out-of-Distribution Detection using Multiple Semantic Label Representations\n\n# Abstract\n\nDeep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.\n\n# 1 Introduction\n\nDeep Neural Networks (DNNs) have gained lots of success after enabling several breakthroughs in notably challenging problems such as image classification [12], speech recognition [1] and machine translation [4]. These models are known to generalize well on inputs that are drawn from the same distribution as of the examples that were used to train the model [43]. In real-world scenarios, the input instances to the model can be drawn from different distributions, and in these cases, DNNs tend to perform poorly. Nevertheless, it was observed that DNNs often produce high confidence predictions for unrecognizable inputs [33] or even for a random noise [13]. Moreover, recent works in the field of adversarial examples generation show that due to small input perturbations, DNNs tend to produce high probabilities while being greatly incorrect [11, 6, 17]. When considering AI Saftey, it is essential to train DNNs that are aware of the uncertainty in the predictions [2]. Since DNNs are ubiquitous, present in nearly all segments of technology industry from self-driving cars to automated dialog agents, it becomes critical to design classifiers that can express uncertainty when predicting out-of-distribution inputs.\nRecently, several studies proposed different approaches to handle this uncertainty [13, 25, 23, 19]. In [13] the authors proposed a baseline method to detect out-of-distribution examples based on the models\u2019 output probabilities. The work in [25] extended the baseline method by using temperature scaling of the softmax function and adding small controlled perturbations to inputs [14]. In [23] it was suggested to add another term to the loss so as to minimize the Kullback-Leibler (KL) divergence between the models\u2019 output for out-of-distribution samples and the uniform distribution.\nEnsemble of classifiers with optional adversarial training was proposed in [19] for detecting out-ofdistribution examples. Despite their high detection rate, ensemble methods require the optimization of several models and therefore are resource intensive. Additionally, each of the classifiers participated in the ensemble is trained independently and the representation is not shared among them.\n\nJoseph Keshet Bar-Ilan University, Israel jkeshet@cs.biu.ac.il\n\nIn this work, we replace the traditional supervision during training by using several word embeddings as the model\u2019s supervision, where each of the embeddings was trained on a different corpus or with a different architecture. More specifically, our classifier is composed of several regression functions, each of which is trained to predict a word embedding of the target label. At inference time, we gain robustness in the prediction by making decision based on the output of the regression functions. Additionally, we use the L2-norm of the outputs as a score for detecting out-of-distribution instances.\nWe were inspired by several studies. In [26] the authors presented a novel technique for robust transfer learning, where they proposed to optimize multiple orthogonal predictors while using a shared representation. Although being orthogonal to each other, according to their results, the predictors were likely to produce identical softmax probabilities. Similarly, we train multiple predictors that share a common representation, but instead of using the same supervision and forcing orthogonality between them, we use different supervisions based on word representations. The idea of using word embeddings as a supervision was proposed in [8] for the task of zero-shot learning. As opposed to ours, their model was composed of a single predictor. Last, [39] found a link between the L2-norm of the input representation and the ability to discriminate in a target domain. We continue this thread here, where we explore the use of the L2-norm for detecting out-of-distribution samples.\nThe contributions of this paper are as follows:\n\u2022 We propose using several different word embeddings as a supervision to gain diversity and redundancy in an ensemble model with a shared representation.\n\u2022 We propose utilizing the semantic structure between word embeddings to produce semantic quality predictions.\n\u2022 We propose using the L2-norm of the output vectors for detecting out-of-distribution inputs. \u2022 We examined the use of the above approach for detecting adversarial examples and wrongly classified examples.\n\nre likely to produce identical softmax probabilities. Similarly, we train multiple predictors that are a common representation, but instead of using the same supervision and forcing orthogonality tween them, we use different supervisions based on word representations. The idea of using word mbeddings as a supervision was proposed in [8] for the task of zero-shot learning. As opposed to rs, their model was composed of a single predictor. Last, [39] found a link between the L2-norm of e input representation and the ability to discriminate in a target domain. We continue this thread re, where we explore the use of the L2-norm for detecting out-of-distribution samples.\ne contributions of this paper are as follows:\n\u2022 We propose using several different word embeddings as a supervision to gain diversity and redundancy in an ensemble model with a shared representation.\n\u2022 We propose utilizing the semantic structure between word embeddings to produce semantic quality predictions.\n\u2022 We propose using the L2-norm of the output vectors for detecting out-of-distribution inputs. \u2022 We examined the use of the above approach for detecting adversarial examples and wrongly classified examples.\n\n\u2022 We propose using several different word embeddings as a supervision to gain diversity an redundancy in an ensemble model with a shared representation.\n\u2022 We propose utilizing the semantic structure between word embeddings to produce semant quality predictions.\n\u2022 We propose using the L2-norm of the output vectors for detecting out-of-distribution input \u2022 We examined the use of the above approach for detecting adversarial examples and wrongl classified examples.\n\nThe outline of this paper is as follows. In Section 2, we formulate the notations in the paper. In Section 3 we describe our approach in detail. Section 4 summarizes our empirical results. In Sections 5 and Section 6 we explore the use of our method for detecting adversarial examples and wrongly classified examples. In Section 7 we list the related works, and we conclude the paper in Section 8.\n\n# 2 Notations and Definitions\n\nWe denote by X \u2286 R p the set of instances, which are represented as p-dimensional feature vectors, and we denote by Y = {1, . . . , N} the set of class labels. Each label can be referred as a word, and the set Y can be considered as a dictionary. We assume that each training example (x, y) \u2208X \u00d7 Y is drawn from a fixed but unknown distribution \u03c1. Our goal is to train a classifier that performs well on unseen examples that are drawn from the distribution \u03c1, and can also identify out-of-distribution examples, which are drawn from a different distribution, \u00b5.\nOur model is based on word embedding representations. A word embedding is a mapping of a word or a label in the dictionary Y to a real vector space Z \u2286 R D, so that words that are semantically closed have their corresponding vectors close in Z. Formally, the word embedding is a function e: Y \u2192Z from the set of labels Y to an abstract vector space Z. We assume that distances in the embedding space Z are measured using the cosine distance which is defined for two vectors u, v \u2208Z as follows:\n\ufffd\n\ufffd\n\n\u2225\u2225\u2225\u2225\n\ufffd\n\ufffd\nTwo labels are considered semantically similar if and only if their corresponding embeddings a close in Z, namely, when d cos (e (y 1), e (y 2)) is close to 0. When the cosine distance is close to 1, t corresponding labels are semantically far apart.\n\n# 3 Model\n\nOur goal is to build a robust classifier that can identify out-of-distribution inputs. In communicatio theory, robustness is gained by adding redundancy in different levels of the transmission encoding [20\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d39/3d391baa-ece8-427e-b5b0-cb79d43cbc35.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Our proposed K-embeddings model composed of K-predictors where each contains several ully-connected layers. The shared layers consisting a deep neural network.\n</div>\nBorrowing ideas from this theory, our classifier is designed to be trained on different supervisions for each class. Rather than using direct supervision, our classifier is composed of a set of regression functions, where each function is trained to predict a different word embedding (such as GloVe, FastText, etc.). The prediction of the model is based on the outcome of the regression functions.\nRefer to the model depicted schematically in Figure 1. Formally, the model is composed of K regression functions f k \u03b8 k: X \u2192Z for 1 \u2264 k \u2264 K. The input for each function is an instance x \u2208X, and its output is a word embedding vector in Z. Note that the word embedding spaces are not the same for the different regression functions, and specifically the notion of distance is unique for each function. Overall, given an instance x, the output of the model is K different word embedding vectors.\nThe set of parameters of a regression function k, \u03b8 k = {\u03b8 shared, \u03b8 k excl.}, is composed of a set of parameters, \u03b8 shared, that is shared among all the K functions, and a set of parameters, \u03b8 k excl., which is exclusive to the k-th function. Each regression function f k \u03b8 k is trained to predict a word embedding vector e k (y) corresponds to the word which represents the target label y \u2208Y. In the next subsections, we give a detailed account of how the model is trained and then present the inference procedure following the procedure to detect out-of-distribution instances.\n\n# 3.1 Training\n\nIn classic supervised learning, the training set is composed of M instance-label pairs. In our setting, each example from the training set, S train, is composed of an instance x \u2208X and a set of K different word embeddings {e 1 (y), . . . , e K (y)} of a target label y \u2208Y. Namely, S train = {(x i, e 1 (y i), ..., e K (y i))} M i =1.\nOur goal in training is to minimize a loss function which measures the discrepancy between the predicted label and the desired label. Since our intermediate representation is based on word embeddings we cannot use the cross-entropy loss function. Moreover, we would like to keep the notion of similarity between the embedding vectors from the same space. Our surrogate loss function is the sum of K cosine distances between the predicted embedding and the corresponding embedding vector of the target label, both from the same embedding space. Namely,\n\n\ufffd\nThe cosine distance (or the cosine similarity) is a function often used in ranking tasks, where the goal is to give a high score to similar embedding vectors and a low score otherwise [9, 30].\n\n# 3.2 Inference\n\nAt inference time, the regression functions predict K vectors, each corresponds to a vector in a different word embedding space. A straightforward solution is to predict the label using hard decision over the K output vectors by first predict the label of each output and then use a majority vote over the predicted labels.\n\n(2)\n\nAnother, more successful procedure for decoding, is the soft decision, where we predict the label which has the minimal distance to all of the K embedding vectors:\n\nIn order to distinguish between in- and out-of-distribution examples, we consider the norms of the predicted embedding vectors. When the sum of all the norms is below a detection threshold \u03b1 we denote the example as an out-of-distribution example, namely,\n\n\ufffd\nThis is inspired by the discussion of [39, Section 4] and motivated empirically in Section 4.3.\n\n\ufffd\nThis is inspired by the discussion of [39, Section 4] and motivated empir\n\n# 4 Experiments\n\nIn this section, we present our experimental results. First, we describe our experimental setup Then, we evaluate our model using in-distribution examples, and lastly, we evaluate our model using out-of-distribution examples from different domains. We implemented the code using PyTorch [35] It will be available under www.github.com/MLSpeech/semantic_OOD.\n\n# 4.1 Experimental Setup\n\nRecall that each regression function f k \u03b8 k is composed of a shared part and an exclusive part. In our setting, we used state-of-the-art known architectures as the shared part, and three fully-connected layers, with ReLU activation function between the first two, as the exclusive part.\nWe evaluated our approach on CIFAR-10, CIFAR-100 [18] and Google Speech Commands Dataset 1, abbreviated here as GCommands. For CIFAR-10 and CIFAR-100 we trained ResNet-18 and ResNet34 models [12], respectively, using stochastic gradient descent with momentum for 180 epochs. We used the standard normalization and data augmentation techniques. We used learning rate of 0.1, momentum value of 0.9 and weight decay of 0.0005. During training we divided the learning rate by 5 after 60, 120 and 160 epochs. For the GCommands dataset, we trained LeNet model [22] using Adam [16] for 20 epochs using batch size of 100 and a learning rate of 0.001. Similarly to [44] we extracted normalized spectrograms from the original waveforms where we zero-padded the spectrograms to equalize their sizes at 160 \u00d7 101.\nFor our supervision, we fetched five word representations for each label. The first two word representations were based on the Skip-Gram model [29] trained on Google News dataset and One Billion Words benchmark [5], respectively. The third and forth representations were based on GloVe [36], where the third one was trained using both Wikipedia corpus and Gigawords [34] dataset, the fourth was trained using Common Crawl dataset. The last word representations were obtained using FastText [28] trained on Wikipedia corpus. We use the terms 1-embed, 3-embed, and 5-embed to specify the number of embeddings we use as supervision, i.e., the number of predicted embedding vectors. On 1-embed and 3-embed models we randomly pick 1 or 3 embeddings (respectively), out of the five embeddings.\nWe compared our results to a softmax classifier (baseline) [13], ensemble of softmax classifiers (ensemble) [19] and to [25] (ODIN). For the ensemble method, we followed a similar approach to [19, 24] where we randomly initialized each of the models. For ODIN, we followed the scheme proposed in by the authors where we did a grid search over the \u03f5 and T values for each setting. In all of these models we optimized the cross-entropy loss function using the same architectures as in the proposed models.\n\n# 4.2 In-Distribution\n\nAccuracy In this subsection, we evaluate the performance of our model and compare it to models based on softmax. Similar to [25], we considered CIFAR-10, CIFAR-100 and GCommands datasets\n\n(3)\n\n(4)\n\n<div style=\"text-align: center;\">istribution examples and semantical relevance of misclassi\n</div>\n<div style=\"text-align: center;\">acy on in-distribution examples and semantical relevance o\n</div>\ne 1: Accuracy on in-distribution examples and semantical relevance of misclassificat\nDataset\nModel\nAccuracy\nAvg. WUP\nAvg. LCH\nAvg. Path\nGCommands\nBaseline\n90.3\n0.2562\n1.07\n0.0937\n1-embed\n90.42\n0.3279\n1.23\n0.1204\n3-embed\n91.04\n0.3215\n1.22\n0.1184\n5-embed\n91.13\n0.3095\n1.19\n0.1141\nEnsemble\n90.9\n0.2206\n0.96\n0.0748\nCIFAR-10\nBaseline\n95.28\n0.7342\n1.7\n0.1594\n1-embed\n95.11\n0.741\n1.73\n0.1633\n3-embed\n94.99\n0.7352\n1.71\n0.1609\n5-embed\n95.04\n0.7302\n1.69\n0.157\nEnsemble\n95.87\n0.733\n1.71\n0.1601\nCIFAR-100\nBaseline\n79.14\n0.506\n1.38\n0.1263\n1-embed\n77.62\n0.51\n1.39\n0.1277\n3-embed\n78.31\n0.501\n1.38\n0.1251\n5-embed\n78.23\n0.5129\n1.4\n0.1293\nEnsemble\n81.38\n0.5122\n1.4\n0.1291\nas in-distribution examples. We report the accuracy for our models using K = 1, 3 or 5 word embeddings, and compare it to the baseline and to the ensemble of softmax classifier. Results are summarized in Table 1.\n\nSemantic Measure Word embeddings usually capture the semantic hierarchy between the words [29], since the proposed models are trained with word embeddings as supervision, they can capture such semantics. To measure the semantic quality of the model, we compute three semantic measures based on WordNet hierarchy: (i) Node-counting on the shortest path that connects the senses in the is-a taxonomy; (ii) Wu-Palmer (WUP) [41], calculates the semantic relatedness by considering the depth of the two senses in the taxonomy; and (iii) Leacock-Chodorow (LCH) [21], calculates relatedness by finding the shortest path between two concepts and scaling that value by twice the maximum depth of the hierarchy. Results suggest that on average our model produces labels which have slightly better semantic quality.\n\n# 4.3 Out-of-Distribution\n\nOut-of-Distribution Datasets For out-of-distribution examples, we followed a similar setting as in [25, 13] and evaluated our models on several different datasets. All visual models were trained on CIFAR-10 and tested on SVHN[32], LSUN[42] (resized to 32x32x3) and CIFAR-100; and trained on CIFAR-100 and were tested on SVHN, LSUN (resized to 32x32x3) and CIFAR-10. For the speech models, we split the dataset into two disjoint subsets, the first contains 7 classes 2 (denote by SC-7) and the other one contains the remaining 23 classes (denote by SC-23). We trained our models using SC-23 and test them on SC-7.\nEvaluation We followed the same metrics used by [13, 25]: (i) False Positive Rate (FPR) at 95% True Positive Rate (TPR): the probability that an out-of-distribution example is misclassified as in-distribution when the TPR is as high as 95%; (ii) Detection error: the misclassification probability when TPR is 95%, where we assumed that both out- and in-distribution have an equal prior of appearing; (iii) Area Under the Receiver Operating Characteristic curve (AUROC); and (iv) Area Under the Precision-Recall curve (AUPR) for in-distribution and out-of-distribution examples.\nFigure 2 presents the distribution of the L2-norm of the proposed method using 5 word embeddings and the maximum probability of the baseline model for CIFAR-100 (in-distribution) and SVHN (out-of-distribution). Both models were trained on CIFAR-100 and evaluated on CIFAR-100 test set (in-distribution) and SVHN (out-of-distribution).\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e0c/5e0c2796-c761-45db-a0c5-1c6d2e46e8d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Distribution of the L2-norm of the proposed model with 5 word embeddings (left) and of the max probability of the baseline model (right). Both were evaluated on CIFAR-100 (in-distribution) and SVHN (out-of-distribution).\n</div>\n100 versus LSUN. This can be explained by taking a closer look into the structure of the datasets and the predictions made by our model. For these two settings, the in- and out-of-distribution datasets share common classes, and some of the classes of the in-distribution dataset appear in the out-of-distribution images. For example: the class bedroom in LSUN was detected as a bed, couch, and wardrobe in CIFAR-100 (56%, 18%, and 8% of the time, respectively); bridge of LSUN was detected as a bridge, road, and sea in CIFAR-100 (26%, 13%, and 9%); and tower of LSUN was detected as a skyscraper, castle, and rocket in CIFAR-100 (34%, 20%, and 6%), and there are many more. Similarly, CIFAR-10 and CIFAR-100 contain shared classes such as truck. Recall that the proportion of this class is 10% in CIFAR-10 and 1% in CIFAR-100. Hence, when the model is trained on CIFAR-100 and evaluated on CIFAR-10, it has 10% in-distribution examples a-priori. When the model is trained on CIFAR-10 and evaluated on CIFAR-100, it has 1% in-distribution examples a-priori.\n\n# 5 Adversarial Examples\n\nNext, we evaluated the performance of our approach for detecting adversarial examples [11]. Although it is not clear if adversarial examples can be considered as an out-of-distribution, we found that our method is very efficient in detecting these examples. Since our goal is detecting adversarial examples rather than suggesting a defense against them, we generated the adversarial examples in a black box settings.\nWe compared our method to an ensemble of softmax classifiers [19], both with K = 5 predictors, on the ImageNet dataset [7]. Both models are based on DenseNet-121 [15], wherein our model the K regression functions are composed of three fully-connected layers with ReLU as described earlier. We omitted from ImageNet these classes which do not have \u201coff-the-shelf\u201d word representations and were left with 645 labels 3.\nWe generated adversarial examples in a black box setting using a third model, namely VGG-19 [38] with the fast gradient sign method and \u03f5 = 0. 007 [11], and measured the detection rate for each of the models. Notice that our goal was not to be robust to correctly classify adversarial examples but rather to detect them.\nWe used the method of detecting out-of-distribution examples to detect adversarial examples, results are in Table 3. We further explored the inter-predictor agreements on the predicted class across the K predicted embeddings. The histogram of the differences between the maximum and the minimum rankings of the predicted label is presented in Figure 3. It can be observed that for our model the inter-predictor variability is much higher than that of the ensemble model. One explanation for this behavior is the transferability of adversarial examples across softmax classifiers, which can be reduced by using different supervisions.\n\n<div style=\"text-align: center;\">Table 2: Results for in- and out-of-distribution detection for various settings. All values are in percentages. \u2191 indicates larger value is better, and \u2193 indicates lower value is better.\n</div>\npercentages. \u2191indicates larger value is better, and \u2193indicates lower value is better.\nIn-Distribution\nOut-of-\nDistribution\nModel\nFPR\n(95% TPR) \u2193\nDetection\nError \u2193\nAUROC \u2191\nAUPR-In \u2191\nAUPR-Out \u2191\nSC-23\n(LeNet)\nSC-7\nBaseline\n77.53\n41.26\n82.74\n94.33\n50.44\nODIN\n71.02\n38.01\n85.49\n95.11\n57.7\n1-embed\n70.64\n37.8\n85.85\n95.21\n58.08\n3-embed\n67.5\n36.24\n87.34\n95.91\n59.97\n5-embed\n69.23\n37.09\n86.93\n95.74\n58.97\nEnsemble\n72.73\n38.85\n85.99\n95.69\n50.71\nCIFAR-10\n(ResNet18)\nSVHN\nBaseline\n7.19\n6.09\n97.2\n96.35\n98.05\nODIN\n4.95\n4.97\n98.65\n96.89\n99.21\n1-embed\n2.41\n3.7\n99.48\n98.77\n99.79\n3-embed\n4.92\n4.95\n98.52\n97.76\n99.07\n5-embed\n4.14\n4.57\n99.1\n98.3\n99.55\nEnsemble\n6.01\n5.5\n98.24\n97.41\n98.94\nCIFAR-10\n(ResNet18)\nLSUN\nBaseline\n50.25\n27.62\n91.28\n91.58\n89.3\nODIN\n41.8\n23.39\n90.35\n96.38\n75.07\n1-embed\n26.11\n15.55\n95.37\n95.81\n94.85\n3-embed\n29.2\n17.09\n95.07\n95.63\n94.38\n5-embed\n22.98\n13.99\n96.05\n96.72\n94.86\nEnsemble\n46.16\n25.58\n92.93\n94.1\n77.57\nCIFAR-10\n(ResNet18)\nCIFAR-100\nBaseline\n58.75\n31.87\n87.76\n86.73\n85.83\nODIN\n54.85\n29.92\n85.59\n82.26\n85.41\n1-embed\n48.72\n26.86\n89.18\n88.91\n88.39\n3-embed\n50.9\n27.95\n89.76\n90.36\n88.13\n5-embed\n45.25\n25.12\n91.23\n91.86\n89.63\nEnsemble\n56.14\n30.57\n90.03\n90.01\n88.27\nCIFAR-100\n(ResNet34)\nSVHN\nBaseline\n87.88\n46.44\n74.11\n63.6\n84.17\nODIN\n76.64\n40.82\n79.86\n68.22\n90.1\n1-embed\n75.79\n40.39\n81.82\n71.52\n90.1\n3-embed\n74.74\n39.87\n82.57\n75.01\n90.4\n5-embed\n60.14\n32.57\n87.42\n77.95\n93.56\nEnsemble\n85.92\n45.46\n79.1\n69.23\n89.3\nCIFAR-100\n(ResNet34)\nCIFAR-10\nBaseline\n77.21\n41.1\n79.18\n80.71\n75.22\nODIN\n74.15\n39.57\n80.40\n80.41\n77.2\n1-embed\n80.82\n42.9\n75.99\n74.27\n73.01\n3-embed\n78.17\n41.77\n77.35\n77.42\n74.39\n5-embed\n77.03\n41.01\n77.7\n77.23\n74.61\nEnsemble\n73.57\n39.28\n81.49\n83.24\n78.16\nCIFAR-100\n(ResNet34)\nLSUN\nBaseline\n80.41\n42.7\n78.02\n79.25\n73.34\nODIN\n79.88\n42.44\n78.94\n80.22\n73.31\n1-embed\n80.99\n42.99\n76.41\n75.08\n74.02\n3-embed\n81.08\n43.03\n74.88\n72.21\n72.38\n5-embed\n80.87\n42.93\n76.08\n75.3\n72.67\nEnsemble\n79.53\n42.26\n79.05\n92.61\n74.06\nWe labeled an input as an adversarial example unless all the predictors were in full agreement. We calculated the detection rate of adversarial examples and the false rejection rate of legitimate examples. The ensemble achieved 43.88% detection rate and 11.69% false rejection rate, while the embedding model achieved 62.04% detection rate and 15.16% false rejection rate. Although the ensemble method achieves slightly better false rejection rate (3% improvement), the detection rate of our approach is significantly better (18% improvement). To better qualify that, we fixed the false rejection rate in both methods to be 3%. In this setting, the ensemble reaches 15.41% detection rate while our model reaches 28.64% detection rate (13% improvement).\n\n# 6 Wrongly Classified Examples\n\nRecall the embedding models were trained to minimize the cosine distance between the output vector and the word representation of the target label according to some embedding space. When we plot the average L2-norm of the models\u2019 output vectors as a function of the epochs, we have noticed\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79c6/79c6a29e-40e3-48c2-8187-5cb3b99f5170.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Histogram of the differences between the max and the min rankings of the predicted label in our model (left) and the ensemble model (right) for original and adversarial examples.\n</div>\n<div style=\"text-align: center;\">Figure 3: Histogram of the differences between the max and the min rankings of the predicted label n our model (left) and the ensemble model (right) for original and adversarial examples.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7289/7289f38e-2049-4c7c-a1bc-2cb80d8b85d3.png\" style=\"width: 50%;\"></div>\nthat the norm of the wrongly classified examples is significantly smaller than those of correctly classified examples. These finding goes in hand with the results in [39], which observed that lower representation norms are negatively associated with the softmax predictions, as shown in the left panel of Figure 4. Similarly, we observe a similar behavior for out-of-distribution examples as shown in the right panel of Figure 4. These findings suggest that we can adjust the threshold \u03b1 accordingly. We leave this further exploration for future work.\n\n# 7 Related Work\n\nThe problem of detecting out-of-distribution examples in low-dimensional space has been wellstudied, however those methods found to be unreliable for high-dimensional space [40]. Recently, out-of-distribution detectors based on deep models have been proposed. Several studies require enlarging or modifying the neural networks [23, 37, 3], and various approaches suggest to use the output of a pre-trained model with some minor modifications [13, 25].\nThere has been a variety of works revolving around Bayesian inference approximations, which approximate the posterior distribution over the parameters of the neural network and use them to quantify predictive uncertainty [31, 27]. These Bayesian approximations often harder to implement and computationally slower to train compared to non-Bayesian approaches. The authors of [10] proposed to use Monte Carlo dropout to estimate uncertainty at test time as Bayesian approximation. More recently, [19] introduced a non-Bayesian method, using an ensemble of classifiers for predictive uncertainty estimation, which proved to be significantly better than previous methods.\n\n)\nModel\nFPR\n(95% TPR) \u2193\nDetection\nError \u2193\nAUROC \u2191\nAUPR-In \u2191\nAUPR-Out \u2191\nEnsemble\n57.3\n31.15\n88.66\n98.71\n43.46\n5-embed\n57.26\n31.12\n89.58\n98.64\n47.2\n# 8 Discussion and Future Work\n\nIn this paper, we propose to use several semantic representations for each target label as supervision to the model in order to detect out-of-distribution examples, where the detection score is based on the L2-norm of the output representations.\nFor future work, we would like to further investigate the following: (i) we would like to explore better decision strategy for detecting out-of-distribution examples; (ii) we would like to rigorously analyze the notion of confidence based on the L2-norm beyond [39]; (iii) we would like to inspect the relation between detecting wrongly classified examples and adversarial examples to out-of-distribution examples.\n\n# References\n\n[1] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al. Deep speech 2: Endto-end speech recognition in english and mandarin. In International Conference on Machine Learning, pages 173\u2013182, 2016.\n[2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\n[3] Jerone TA Andrews, Thomas Tanay, Edward J Morton, and Lewis D Griffin. Transfer representation-learning for anomaly detection. ICML, 2016.\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.\n[6] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling deep structured visual and speech recognition models with adversarial examples. In Advances in Neural Information Processing Systems, pages 6980\u20136990, 2017.\n[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.\n[8]  Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129, 2013.\n[9] Tzeviya Fuchs and Joseph Keshet. Spoken term detection automatically adjusted for a given threshold. IEEE Journal of Selected Topics in Signal Processing, 11(8):1310\u20131317, 2017.\n10] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u20131059, 2016.\n11]  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n[13] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n[15] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, page 3, 2017.\n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[17] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. Fooling end-to-end speaker verification by adversarial examples. arXiv preprint arXiv:1801.03339, 2018.\n[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\n[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6405\u20136416, 2017.\n[20] Amos Lapidoth. A foundation in digital communication. Cambridge University Press, 2017.\n[21] Claudia Leacock and Martin Chodorow. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265\u2013283, 1998.\n[22] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n[23] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.\n[24] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314, 2015.\n[25] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks.\n[26] Etai Littwin and Lior Wolf. The multiverse loss for robust transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3957\u20133966, 2016.\n[27] David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992.\n[28] Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018.\n[29]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.\n[30] Einat Naaman, Yossi Adi, and Joseph Keshet. Learning similarity function for pronunciation variations. In Proc. of Interspeech, 2017.\n[31] Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.\n\n[32] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011.\n[33] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427\u2013436, 2015.\n[34] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition, linguistic data consortium. Google Scholar, 2011.\n[35] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.\n[36] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.\n[37] Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on Information Processing in Medical Imaging, pages 146\u2013157. Springer, 2017.\n[38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[39] Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. Web-scale training for face identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2746\u20132754, 2015.\n[40] Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. ICLR, 2015.\n[41] Zhibiao Wu and Martha Palmer. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133\u2013138. Association for Computational Linguistics, 1994.\n[42] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n[43] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\n[44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of out-of-distribution detection in neural networks, highlighting the limitations of previous methods that often fail when the input data is drawn from different distributions. The necessity for a new approach arises from the observed high confidence predictions made by deep neural networks on unrecognizable inputs, which can lead to critical failures in real-world applications.",
        "problem": {
            "definition": "The problem defined in this paper is the detection of out-of-distribution examples by deep neural networks, which struggle to generalize when faced with inputs that do not conform to the distribution of the training data.",
            "key obstacle": "The main challenge is that existing methods often require multiple independent classifiers, which are resource-intensive and do not share representations, leading to inefficiencies and potential inaccuracies."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by the observation that using diverse word embeddings as supervision can enhance model robustness and improve out-of-distribution detection.",
            "opinion": "The authors propose a method that utilizes multiple semantic dense representations as target labels instead of relying on sparse representations, aiming to leverage the semantic relationships captured by these embeddings.",
            "innovation": "The key innovation lies in the use of multiple word embeddings trained on different corpora, which allows the model to gain diversity and redundancy in predictions, contrasting with traditional single-representation approaches."
        },
        "method": {
            "method name": "K-embeddings Model",
            "method abbreviation": "K-EM",
            "method definition": "The K-embeddings model is defined as a classifier composed of K regression functions, each predicting a different word embedding for a target label, allowing for shared representation while using diverse supervision.",
            "method description": "The method employs multiple regression functions to predict word embeddings, enhancing robustness in detecting out-of-distribution instances.",
            "method steps": [
                "Train K regression functions using different word embeddings.",
                "Use the outputs of these functions to make a decision based on the predicted embeddings.",
                "Calculate the L2-norm of the output vectors to detect out-of-distribution examples."
            ],
            "principle": "The effectiveness of this method is grounded in the semantic structure of the word embeddings, which allows the model to discern in-distribution from out-of-distribution inputs based on the nature of the embeddings produced."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on CIFAR-10, CIFAR-100, and Google Speech Commands Dataset, comparing the proposed method against baseline softmax classifiers and ensemble methods.",
            "evaluation method": "Performance was assessed using metrics such as False Positive Rate at 95% True Positive Rate, detection error, AUROC, and AUPR for both in-distribution and out-of-distribution examples."
        },
        "conclusion": "The experimental results indicate that the proposed K-embeddings model outperforms traditional methods in detecting out-of-distribution instances and demonstrates efficiency in identifying wrongly classified and adversarial examples.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to leverage multiple semantic representations, which enhances model robustness and improves detection rates compared to existing methods.",
            "limitation": "A limitation noted is that the method may not fully address all scenarios of out-of-distribution detection and could encounter challenges with certain types of adversarial examples.",
            "future work": "Future research will focus on exploring better decision strategies for out-of-distribution detection, analyzing the confidence levels based on L2-norm, and investigating the relationships between wrongly classified and adversarial examples."
        },
        "other info": {
            "info1": "The proposed model was implemented using PyTorch and is available on GitHub.",
            "info2": {
                "info2.1": "The regression functions are designed to predict embeddings from different embedding spaces.",
                "info2.2": "The method is inspired by communication theory, emphasizing redundancy in representation for improved robustness."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The necessity for a new approach arises from the observed high confidence predictions made by deep neural networks on unrecognizable inputs, which can lead to critical failures in real-world applications."
        },
        {
            "section number": "1.2",
            "key information": "This paper addresses the issue of out-of-distribution detection in neural networks, highlighting the limitations of previous methods that often fail when the input data is drawn from different distributions."
        },
        {
            "section number": "2.1",
            "key information": "The problem defined in this paper is the detection of out-of-distribution examples by deep neural networks, which struggle to generalize when faced with inputs that do not conform to the distribution of the training data."
        },
        {
            "section number": "3.4",
            "key information": "The K-embeddings model is defined as a classifier composed of K regression functions, each predicting a different word embedding for a target label, allowing for shared representation while using diverse supervision."
        },
        {
            "section number": "3.5",
            "key information": "The primary advantage of the proposed approach is its ability to leverage multiple semantic representations, which enhances model robustness and improves detection rates compared to existing methods."
        },
        {
            "section number": "7.1",
            "key information": "The main challenge is that existing methods often require multiple independent classifiers, which are resource-intensive and do not share representations, leading to inefficiencies and potential inaccuracies."
        },
        {
            "section number": "7.2",
            "key information": "Future research will focus on exploring better decision strategies for out-of-distribution detection, analyzing the confidence levels based on L2-norm, and investigating the relationships between wrongly classified and adversarial examples."
        }
    ],
    "similarity_score": 0.765181711841418,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d39/3d391baa-ece8-427e-b5b0-cb79d43cbc35.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e0c/5e0c2796-c761-45db-a0c5-1c6d2e46e8d7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79c6/79c6a29e-40e3-48c2-8187-5cb3b99f5170.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7289/7289f38e-2049-4c7c-a1bc-2cb80d8b85d3.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Out-of-Distribution Detection us.json"
}