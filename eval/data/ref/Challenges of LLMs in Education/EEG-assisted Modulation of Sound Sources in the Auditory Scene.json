{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1612.00703",
    "title": "EEG-assisted Modulation of Sound Sources in the Auditory Scene",
    "abstract": "Noninvasive EEG (electroencephalography) based auditory attention detection could be useful for improved hearing aids in the future. This work is a novel attempt to investigate the feasibility of online modulation of sound sources by probabilistic detection of auditory attention, using a noninvasive EEG-based brain computer interface. Proposed online system modulates the upcoming sound sources through gain adaptation which employs probabilistic decisions (soft decisions) from a classifier trained on offline calibration data. In this work, calibration EEG data were collected in sessions where the participants listened to two sound sources (one attended and one unattended). Cross-correlation coefficients between the EEG measurements and the attended and unattended sound source envelope (estimates) are used to show differences in sharpness and delays of neural responses for attended versus unattended sound source. Salient features to distinguish attended sources from the unattended ones in the correlation patterns have been identified, and later they have been used to train an auditory attention classifier. Compared to the existing results in the literature, in this paper we have two main contributions. First, using the auditory attention classifier, we have shown high offline detection performance with single channel EEG measurements of shorter duration compared to the existing approaches in the literature which employ large number of channels with longer EEG measurements. Second, using the classifier trained offline in the calibration session, we have shown the performance of the online sound source modulation system. We observe that online sound source modulation system is able to keep the level of attended sound source higher than the unattended source.\n  Keywords: auditory BCI, cocktail party problem, auditory attention classification",
    "bib_name": "haghighi2017eegassistedmodulationsoundsources",
    "md_text": "# EEG-assisted Modulation of Sound Sources in the Auditory Scene\nMarzieh Haghighia,\u2217, Mohammad Moghadamfalahia, Murat Akcakayab, Deniz Erdogmusa\naNortheastern University, 360 Huntington Ave, Boston, MA 02115 bUniversity of Pittsburgh, 4200 Fifth Ave, Pittsburgh, PA 15260\nNoninvasive EEG (electroencephalography) based auditory attention detection could be useful for improved hearing aids in the future. This work is a novel attempt to investigate the feasibility of online modulation of sound sources by probabilistic detection of auditory attention, using a noninvasive EEG-based brain computer interface. Proposed online system modulates the upcoming sound sources through gain adaptation which employs probabilistic decisions (soft decisions) from a classifier trained on offline calibration data. In this work, calibration EEG data were collected in sessions where the participants listened to two sound sources (one attended and one unattended). Cross-correlation coefficients between the EEG measurements and the attended and unattended sound source envelope (estimates) are used to show differences in sharpness and delays of neural responses for attended versus unattended sound source. Salient features to distinguish attended sources from the unattended ones in the correlation patterns have been identified, and later they have been used to train an auditory attention classifier. Compared to the existing results in the literature, in this paper we have two main contributions. First, using the auditory attention classifier, we have shown high offline detection performance with single channel EEG measurements of shorter duration compared to the existing approaches in the literature which employ large number of channels with longer\n\u2217Corresponding author Email address: haghighi@ece.neu.edu (Marzieh Haghighi)\nPreprint submitted to Journal of LATEX Templates\nEEG measurements. Second, using the classifier trained offline in the calibration session, we have shown the performance of the online sound source modulation system. We observe that online sound source modulation system is able to keep the level of attended sound source higher than the unattended source. Keywords: auditory BCI, cocktail party problem, auditory attention classification\n# 1. Introduction\nApproximately 35 million Americans (11.3% of the population) suffer from hearing loss; this number is increasing and is projected to reach 40 million by 2025 [1]. Within this population only 30% prefer using current generations of hearing aids that are available on the market. One of the most common complaints associated with hearing-aid use is understating speech in the presence of noise and interferences. Effects of interfering sounds on masking the speech intelligibility and audibility have been widely studied [2], [3]. Specifically, it has been shown that increase in SNR needed for the same level of speech understanding given a background noise for people with hearing loss can be as high as 30 dB more compared to people with normal hearing [3]. Therefore, amplifying the target signal versus unwanted noises and interferences to facilitate hearing and increase speech intelligibility and listening comfort is one of the basic concepts exploited by hearing aids [3]. Identifying the signal versus noise is a main step required for the design of a hearing aid. This identification step can be a difficult task in complicated auditory scenes like a cocktail party scenario in which both signal and interferences have acoustic features of speech and can instantly switch their roles based on the attention of the listener and can not be detected based on the predefined assumptions on signal and noise features. Our brain distinguishes the audio sources based on their spectral profile, harmonicity, spectral or spatial separation, temporal onsets and offsets, temporal modulation, and temporal separation [4],[5] and focus on one sound to analyse the auditory scene [6] in the so called cocktail party effect [7]. Existence of\neach cue can reduce informational and energetic masking of competing sources and help focusing our attention on the target source. A general overview of the computational efforts in bottom-up or top-down modelling of auditory attention in a cocktail party setting is provided in [8]. Brain/Body Computer Interface (BBCI) systems can be used to augment the current generations of hearing aids by discriminating among attended and unattended sound sources. They can be incorporated to provide external evidence based on top-down selective attention of listeners [9]. Attempts have been made to incorporate bottom-up attention evidences in the design of the hearing aids. Direction based hearing aids that detect attention direction from eye gaze and amplify sounds coming from that direction can be examples of bottom-up attention evidence incorporation [10]. Moreover, there are attempts to use electroencephalography (EEG)-based brain computer interfaces (BCIs) for the identification of attended sound sources. EEG has been extensively used in BCI designs due to its high temporal resolution, non-invasiveness, and portability. These characteristics, in addition to EEG devices being inexpensive and accessible, make EEG a practical choice for the design of a BCI that can be integrated into hearing aids to identify auditory attention. A crucial step in such an integration is to build an EEG-based BCI that employs auditory attention. EEG-based auditory BCIs that rely on external auditory stimulation have recently attracted attention from the research community. For example, auditoryevoked P300 BCI spelling system for locked-in patients is widely studied [11], [12], [13], [14], [15], [16]. It was shown that fundamental frequency, amplitude, pitch and direction of audio stimuli are distinctive features that can be processed and distinguished by the brain. Also, more recent studies using EEG measurements have shown that there is a cortical entrainment to the temporal envelope of the attended speech [17], [18], [19]. A study on the quality of cortical entrainment to auditory stimulus envelope by top-down cognitive attention has shown enhancement of obligatory auditory processing activity in top-down attention responses when competing auditory stimuli differ in spatial direction [20] and frequency [21].\nRecently, EEG-based BCI has also been used in cocktail party problems for the classification of attended versus unattended sound sources [22], [23], [24]. In the identification of the attended sound source in a cocktail party problem, stimulus reconstruction to estimate the envelope of the input speech stream from high density EEG measurements is the state-of-the-art practice [25], [22]. In the aforementioned model, envelope of the attended stimulus is reconstructed using spatio-temporal linear decoder applied on neural recordings. In one study that considered the identification of the attended sound source in a dichotic (different sounds playing in each ear) two speaker scenario, 60 seconds of high density EEG data recorded through 128 electrodes were used in the stimulus reconstruction. Two decoders using the attended and unattended speech were trained and it was shown that estimated sound source using the attended decoder has higher correlation with the attended speech envelope compared to the estimated stimuli using unattended decoder with unattended speech envelope [22]. For practical purposes, further studies have attempted to examine the stimulus reconstruction approach using smaller number of EEG electrodes [23] or even two bilaterally placed around the ear electrode arrays (cEEGrids) [26]. Furthermore, the robustness of the attended speech envelope reconstruction in noisy real world acoustic scenes has been demonstrated [27]. In contrast to the stimulus reconstruction methods, studies with system identification approaches to solve this problem, have tried to reconstruct the neural measurements using the linear forward map of sound sources [28], [29], [30], [31]. In a recent related study, a single in-Ear-EEG electrode and an adjacent scalp-EEG electrode were used for auditory attention detection in a diotic two speaker scenario [30]. On the other hand, in a different class of target speaker detection approaches, studies have tried to extract informative and distinguishable features of EEG measurements with respect to the attended and unattended sound sources to train a classifier [24], [32]. In a related study, authors have compared three types of features extracted from speech signal and EEG measurements to learn a linear classifier for the identification of the attended speaker using 20 seconds of data from high density 128 channels EEG recordings [24]. In our previous related\nwork, we have investigated the role of spectral and spatial features of competng sound sources in an auditory BCI system with the purpose of detecting the attended auditory source in a cocktail party setting. We reported high single channel classification performance for attended sound source versus unattended one based on their spectral and spatial separation of the sources (diotic and dichotic paradigms) using 60 seconds of EEG and stimuli data [32]. In continuation of the above described literature and our previous work, this paper presents two contributions to the literature of EEG-based auditory attention detection in a cocktail party setting: \u2022 First, we show successful identification of attended speaker source in a diotic (both sounds playing in both ears) two speaker scenario using 20 seconds of EEG data recorded from 16 channels. The presented classifier outperforms EEG-based auditory attention detectors previously presented in the literature in terms of accuracy, with smaller number of EEG channels (sparse 16 versus typically dense 96 or more), and using time-series of shorter durations (20 seconds versus typically 60 seconds). In fact, using 20 seconds of EEG data from only one of the 16 EEG channels, we demonstrate high classification performance for the auditory attention detection. This extends our results from [32], which showed high single-channel classification accuracy when the EEG duration was 60 seconds. \u2022 Second, we introduce a novel online system that gives feedback on attention of the user in the form of attended to unattended source energy ratio amplification. The level of amplification of attended versus suppression of unattended source is assigned based on a probabilistic model defined over the classifier trained on the offline data including temporal dependency of the user\u2019s attention. The goal of the online system is using the probabilistic information of the user\u2019s attention to enhance the concentration of the user on the target source in multi-speaker scenarios. The introduced framework for online system is a proof of concept for design perspective\nwork, we have investigated the role of spectral and spatial features of competing sound sources in an auditory BCI system with the purpose of detecting the attended auditory source in a cocktail party setting. We reported high single channel classification performance for attended sound source versus unattended one based on their spectral and spatial separation of the sources (diotic and dichotic paradigms) using 60 seconds of EEG and stimuli data [32]. In continuation of the above described literature and our previous work, this paper presents two contributions to the literature of EEG-based auditory attention detection in a cocktail party setting:\n based on their spectral and spatial separation of the sources (diotic and otic paradigms) using 60 seconds of EEG and stimuli data [32]. n continuation of the above described literature and our previous work,  paper presents two contributions to the literature of EEG-based auditory ntion detection in a cocktail party setting: \u2022 First, we show successful identification of attended speaker source in a diotic (both sounds playing in both ears) two speaker scenario using 20 seconds of EEG data recorded from 16 channels. The presented classifier outperforms EEG-based auditory attention detectors previously presented in the literature in terms of accuracy, with smaller number of EEG channels (sparse 16 versus typically dense 96 or more), and using time-series of shorter durations (20 seconds versus typically 60 seconds). In fact, using 20 seconds of EEG data from only one of the 16 EEG channels, we demonstrate high classification performance for the auditory attention detection. This extends our results from [32], which showed high single-channel classification accuracy when the EEG duration was 60 seconds.\n\u2022 First, we show successful identification of attended speaker source in a diotic (both sounds playing in both ears) two speaker scenario using 20 seconds of EEG data recorded from 16 channels. The presented classifier outperforms EEG-based auditory attention detectors previously presented in the literature in terms of accuracy, with smaller number of EEG channels (sparse 16 versus typically dense 96 or more), and using time-series of shorter durations (20 seconds versus typically 60 seconds). In fact, using 20 seconds of EEG data from only one of the 16 EEG channels, we demonstrate high classification performance for the auditory attention detection. This extends our results from [32], which showed high single-channel classification accuracy when the EEG duration was 60 seconds. \u2022 Second, we introduce a novel online system that gives feedback on atten-\n\u2022 First, we show successful identification of attended speaker source in a diotic (both sounds playing in both ears) two speaker scenario using 20 seconds of EEG data recorded from 16 channels. The presented classifier outperforms EEG-based auditory attention detectors previously presented in the literature in terms of accuracy, with smaller number of EEG channels (sparse 16 versus typically dense 96 or more), and using time-series of shorter durations (20 seconds versus typically 60 seconds). In fact, using 20 seconds of EEG data from only one of the 16 EEG channels, we demonstrate high classification performance for the auditory attention detection. This extends our results from [32], which showed high single-channel classification accuracy when the EEG duration was 60 seconds. \u2022 Second, we introduce a novel online system that gives feedback on attention of the user in the form of attended to unattended source energy ratio amplification. The level of amplification of attended versus suppression of unattended source is assigned based on a probabilistic model defined over the classifier trained on the offline data including temporal dependency of the user\u2019s attention. The goal of the online system is using the probabilistic information of the user\u2019s attention to enhance the concentration of the user on the target source in multi-speaker scenarios. The introduced framework for online system is a proof of concept for design perspective of an EEG-augmented hearing aid system. Finally, we show the intro-\n Second, we introduce a novel online system that gives feedback on attention of the user in the form of attended to unattended source energy ratio amplification. The level of amplification of attended versus suppression of unattended source is assigned based on a probabilistic model defined over the classifier trained on the offline data including temporal dependency of the user\u2019s attention. The goal of the online system is using the probabilistic information of the user\u2019s attention to enhance the concentration of the user on the target source in multi-speaker scenarios. The introduced framework for online system is a proof of concept for design perspective of an EEG-augmented hearing aid system. Finally, we show the intro-\nduced online system in average is able to keep the level of attended source higher despite statistical changes happening in online data compared to the offline data used for training the classifier.\n# 2. System Overview\nThe diagram represented in Figure 1 summarizes the steps of the proposed BCI system. The proposed system gets the mixture of sounds from the environment as the input and modifies the gain of each specific sound. The output of this system is the input to the ear channel. The decision on gain modification of each sound is made by the BCI module which consists of three submodules of gain controller, auditory attention inference system and hearing aid DSP system. Hearing aid DSP system estimates independent sound sources from the mixture of sounds in the environment and outputs the information to the gain controller and attention inference module. In this work, we assume that we have the estimated sources which are the outputs of the DSP system based on blind source separation. Auditory attention inference system estimates the probability of attention on each specific sound source using EEG measurements and estimated sound sources. Gain controller system takes the estimated probabilities from the attention inference system to modify gains of each specific sound. The details of the attention inference system and gain adjustments are provided in the following sections.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5909/5909d8cd-8ba2-47d3-96e9-bc7036250d2b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: EEG-augmented BCI sytem overview.</div>\n# 2.1. Online Gain Controller System\nLets assume that Sn = (s1,n, ..., si,n, ..., sM,n) is a matrix containing original sources that each si,n is a column vector for ith sound channel for nth round of sending feedback. \u02c6Sn = (\u02c6s1,n, ...,\u02c6si,n, ...,\u02c6sM,n), would be the estimated source matrix after blind source separation, which we assume exists and its design is out of the scope of this paper. wn = (w1,n, ..., wi,n, ..., wM,n)\u22bais the vector of weights with wi,n being a scalar showing the gain of ith estimated sound source; and en is the EEG evidence vector for nth round. An = i, indicates the attention of subject is on the ith sound source. Subject will start listening to all sounds with equal energy and then based on brain interface decisions for subject attention on each sound source, speech enhancement or automatic gain controller (AGC) module will assign appropriate weights to each sound source for n + 1th round of sending feedback according to the following equation:\nwn+1 = f(p(An|\u03b5n( \u02c6Sn, en)), wn\u2212j)\nwn+1 = f(p(An|\u03b5n( \u02c6Sn, en)), wn\u2212j) j = 1, 2, .\nEquation 1 states that the weights for the upcoming sound sources (n + 1th round) will be decided based on probability of attention given current EEG evidence (nth round) and previous weights that were used at the n \u22121th round. The selection of optimal gain control policies (choosing the form of f) that considers other factors such as sound quality due to amplitude modulation, response time to changes versus robustness to outlier incidents influencing brain interface decisions, is anticipated to be a significant and important research area in itself, and we will explore alternative designs in future work.\n# 2.2. Auditory Attention Inference System\nThis module calculates probability of attention given EEG evidence. It takes raw EEG measurements, (estimated) sound sources and weights to extract EEG features (evidence), as explained in Section 4. Then, using Bayes rule, the posterior probability distribution of attention over sources is expressed as the product of EEG evidence likelihood times the prior probability distribution over\n(1)\nIn our experiments, we start with a uniform prior over sources and then prior information will be updated based on the observed EEG evidence as explained in 5.2 as well.\n# 3. Data Collection and Preprocessing\n# 3.1. EEG Neurophysiological Data\nTen volunteers (5 male, 5 female), between the ages of 25 to 30 years, with no known history of hearing impairment or neurological problems participated in this study, which followed an IRB-approved protocol. EEG signals were recorded using a g.USBamp biosignal amplifier using active g.Butterfly electrodes with cap application from g.Tec (Graz, Austria) at 256 Hz. Sixteen EEG channels (P1, PZ, P2, CP1, CPZ, CP2, CZ, C3, C4, T7, T8, FC3, FC4, F3, F4 and FZ according to International 10/10 System) were selected to capture auditory related brain activities over the scalp. The selection was based on the topographical maps of classification performance observed and reported in our previous related work [32]. Signals were filtered by built-in analog bandpass ([0.5, 60] Hz) and notch (60Hz) filters.\n# 3.2. Experimental Design\nEach participant completed one calibration and one online session of experiments. Both sessions included diotic (both sounds playing on both ears simultaneously) auditory stimulation while the EEG was recorded from the participants. Participants passively listened to the auditory stimuli through earphones. Calibration Session Total calibration session time was about 30 minutes. More specifically, a calibration session consisted of 60 trials of 20 seconds of diotic auditory stimuli with\n4 seconds breaks between each trial. The diotic auditory stimuli are generated by one male and one female speaker. These speakers narrated a story (different story for different speakers chosen from audio books of literary novels) for 20 minutes. We consider every 20 seconds of this 20-minute-long diotic narration as a trial. During the calibration session, participants were asked to passively listen to 20-minute-long narration, and they were instructed to switch their attention from one speaker to another during different trials. The instructions to switch attention from trial to trial are provided to the user on a computer screen using \u201df\u201d and \u201dm\u201d symbols for female and male speakers, respectively. Online Session The online session is summarized in Figure 2. In the online session, similar to the calibration session one male and one female speaker narrated stories (different story for different speakers) for 20 minutes. The same speakers from the calibration session narrated the stories for the online session, but the stories used in the online session were different than the calibration session. We consider the 20-minute-long narration as 10 two-minute long sequences, each sequence containing 6 twenty-second trials. Before each sequence, participants were asked to attend to one of the speakers through instructions displayed on the computer screen. In each sequence, while the participants were listening to the narrated stories, weights that control the energy of each sound source were updated 6 times after every trial. The equal weight case is defined such that amplitude of each sound source was scaled to yield equal energy and each sequence started with an equal weight trial. There is a 0.5 second pause between 20-second-long trials within each sequence and the weights are updated within this 0.5 second period based on the attention evidence obtained from the EEG recorded from the participants and through the usage of automatic gain controller. Since, the participants were instructed to keep their attention on one of the speakers during each sequence, and during each sequence the weights are adjusted automatically in an online fashion to emphasize the attended sound source, we call this an online session. Silent portions of the story narration longer than 0.2 seconds were truncated to be 0.2 seconds, in order to reduce distraction of participants.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a866/a866c07a-09a5-4a15-afc8-e6398505bb16.png\" style=\"width: 50%;\"></div>\nFigure 2: Online session experimental paradigm visualization. Two sounds are diotically playing in both ears. participants attend to the instructed sound source in each sequence. Each sequence starts with an equal weight trial and in its following trials weights get updated using attention inference and AGC modules.\n# 3.3. Data Pre-processing\nThe acoustic envelope of speech stimulus signals were calculated using the Hilbert transform first and then both EEG brain activity measurements and speech envelopes were filtered by an FIR linear-phase bandpass filter ([1.5, 10]Hz) Then, tx seconds of EEG and acoustic envelope signals following every stimulus and time locked to the stimulus onset were extracted. Optimizing tx to get good performance with minimum time window is an important factor in the design of online auditory BCI systems. In this paper, we selected tx = 20, based on the results of our previous work which are reported in [32]. The data length was selected based on the analysis we performed over the calibration data such that the length is chosen to optimize area under the receiver operating characteristics curve (AUC) of the intent inference engine with a constraint on the upper bound of the data length. More specifically, we analyzed the AUC as a function of the data length, and we chose the data length value when the changes in the AUC as the data length increased became more incremental for most of the participants.\n# 4. Methods\n# 4.1. Feature Extraction\nTop down attention to an external sound source differentially modulates the neural activity to track the envelope of that sound source at different time lags [17], [18], [19]. Therefore, as discriminative features, we calculate the cross correlation (CC) between the extracted EEG measurements and target and distractor acoustic envelopes at different time lags. \u03c4n = [\u03c41, \u00b7 \u00b7 \u00b7 , \u03c4l, \u00b7 \u00b7 \u00b7 , \u03c4L]\u22ba is the vector of discreet time lag delays in sample between EEG and acoustic envelop of played sounds. In our analysis, we consider \u03c4 \u2208[t1, t2] \u00d7 fs, with t1 and t2 as sampling times chosen as described below. For each channel, we calculate cross correlations between EEG and the male and female speakers\u2019 acoustic envelopes for the time lag sample values defined in \u03c4. Assuming that \u03c4 is a L \u00d7 1 vector, we concatenate the cross correlation values from male and female speakers into a single vector and hence each feature vector is 2L \u00d7 1 dimensional. N is the number of EEG and sound source samples used for CC calculation. We have examined the effect of reducing N on classification results in section 5.1.2. Therefore, considering the defined notations, we calculate the correlation coefficient between EEG and sound sources at different time lag samples \u03c4l, denoted by \u03c1ech,\u02c6si[\u03c4l]:\nIn (3), ech is EEG data recorded from channel ch, \u02c6si is the envelope of ith estimated sound channel, \u03c4l is a time lag sample, and rech,\u02c6si[\u03c4l] = E[ech n+\u03c4l:N, \u02c6si n:N\u2212\u03c4l] is the sample average between ech and \u02c6si. Therefore, \u03c1ech,\u02c6si[\u03c4l] is a scalar representing the correlation coefficient between EEG in channel ch and ith sound channel at time lag sample \u03c4l. So, \u03c1ech,\u02c6si = (\u03c1ech,\u02c6si[\u03c41], \u03c1ech,\u02c6si[\u03c42], ..., \u03c1ech,\u02c6si[\u03c4L]) is 1 \u00d7 L dimensional vector for L lags in \u03c4 range for channel ch and ith sound channel. Feature vector will be formed by concatenation of correlation vectors for all \u02c6si\u2019s. In our experiments which we have two sound sources this feature\n(3)\nvector is specifically defined as xch = [\u03c1ech,\u02c6s1, \u03c1ech,\u02c6s2]\u22ba. xch is 2L \u00d7 1 vector for each channel and x = (x1, ..., xch, ..., x16)\u22bais a 2L \u00d7 16 dimensional matrix which contains features for each trial.\nAs explained in Section 3, the participants were asked to direct their auditory attention to a target speaker during data collection. The other speaker is the distractor. The labeled data collected in this manner is used in the analysis of discrimination between two speakers in a binary auditory attention classification problem. As explained in Section 4.1, for each trial we have x as the collection of 2L \u00d7 1 dimensional cross-correlation features for each channel. For analysis of data using all channels, we apply PCA first for dimensionality reduction to remove zero variance directions. Afterwards, feature vectors for each channel will be concatenated to form a single aggregated feature vector for further analysis. Then, we use Regularized Discriminant Analysis (RDA) [33] as the classifier in our analysis. RDA is a modification of Quadratic Discriminant Analysis (QDA). QDA assumes that data is generated by two Gaussian distributions with unknown mean and covariances and requires the estimation of these means and covariances of the target and nontarget classes before the calculation of the likelihood ratio. However, since, L, the length of \u03c4, as defined in Section 4.1, is usually large resulting in feature vectors with large dimensions even after the application of PCA, and the calibration sessions are short, the covariance estimates are rank deficient. RDA eliminates the singularity of covariance matrices by introducing shrinkage and regularization steps. Assume each xi \u2208Rp is a p\u00d71-dimensional feature vector for each trial and yi is its binary label showing if the feature belongs to speaker 1 or 2, that is yi \u2208{1, 2}. Then the maximum likelihood estimates of\nthe class conditional mean and the covariance matrices are computed as follows:\nwhere \u03b4(\u00b7, \u00b7) is the Kronecker-\u03b4 function, k represent a possible class label (here k \u2208{1, 2}, and Nk is the number of realizations in class k. Accordingly, the shrinkage and regularization of RDA is applied respectively as follows:\n\ufffd\ufffd \ufffd Here, \u03bb, \u03b3 \u2208[0, 1] are the shrinkage and regularization parameters, tr[\u00b7] is the trace operator and Ip is an identity matrix of size p \u00d7 p. In our system we optimize the values of \u03bb and \u03b3 to obtain the maximum area under the receiver operating characteristics (ROC) curve (AUC) in a 5-fold cross validation framework. Finally, the RDA score for a trial with the EEG evidence vector xi, which is defined as: \ufffd  \ufffd \ufffd\n \ufffd  \ufffd where fN (x; \u00b5, \u03a3) is the Gaussian probability density function with mean \u00b5 and covariance \u03a3. Here s values are used to plot the ROC curves and to compute the AUC values. RDA can be considered as a nonlinear projection which maps EEG evidence to one dimensional score \u03b5 = sRDA(x). Finally, the conditional probability density function of \u03b5 given the class label, i.e. p(\u03b5 = \u03f5|A = i) needs to be estimated. We use kernel density estimation on the training data using a Gaussian kernel as\nwhere \u03f5(v) is the discriminant score corresponding to a sample v in the training data, that is to be calculated during cross validation, and Khk(.) is the ker-\n(4)\n(5)\n(6)\n(7)\nnel function with bandwidth hk. For a Gaussian kernel, the bandwidth hk is estimated using Silverman\u2019s rule of thumb (\u02c6hk = ( 4\u02c6\u03c35 3n )1/5 \u223c= 1.06\u02c6\u03c3n\u22121/5) for each class k [34]. This assumes the underlying density has the same average curvature as its variance-matching normal distribution [35].\n# 5. Analysis and Results\nAs illustrated in our previous work, [32], features formed using the CC coefficient series \u03c1ech,\u02c6s1, \u03c1ech,\u02c6s2 as calculated in (3) show distinct patterns for attended vs unattended sound sources and these patterns are observed to be consistent across participants. For diotic presentation, the highest distinguishable absolute correlation between the sound sources and EEG is identified in the range of [0,400] ms. We accordingly extract features within this range of correlation delay, \u03c4. In this range, we observe a negative correlation for both target and distractor speakers followed by an early positive correlation for the target stimulus and delayed and suppressed version of that positive correlation for the distractor stimulus. These results are quantitatively summarized in Table 1, more specifically this table reports the average temporal latency and the magnitude of the peak in cross correlation responses across all participants. Statistical significance of the difference between peak temporal latency of target and distractor has been tested using Mann-Whitney U-test (p = .00012).\nCorrelation Features\nPositive Peak Magnitude Ratio\nTime Lag of Peak (ms)\nStimulus\nTarget / Distractor\nTarget\nDistractor\nAverage for all Participants (mean \u00b1 sd)\n2.08 \u00b1 1.1\n159.34 \u00b1 11\n225.78 \u00b1 42.9\nTable 1: Average of time latency and magnitude of peak in cross correlation responses across all participants.\nIn the rest of the analysis, we consider the correlation delay \u03c4 to be in the range of [0,400]ms to form the feature vectors.\n5.1. Offline Data Analysis\n# 5.1.1. Single channel classification analysis\nUsing the selected window of [0,400] ms as the most informative window for classification of target versus distractor responses, we first form the vector xch as shown in 4.1, we then use these features for each EEG channel independently to localize the selective attention responses using the classification scheme described in Section 4.2. As the results of our previous work suggested [32], we relocated electrodes to be more centered around the frontal cortex, see Section 3.1. Figure 3 shows the topographical map of classification performance in terms of area under the receiver operating characteristics curve (AUC) over the scalp, for all participants. Moreover, for each participant best channel AUC values are reported in Table 2. Figure 3 and Table 2 show that the classification accuracy varies across participants, but for each participant channels located in central and frontal cortices have higher classification accuracy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab35/ab356cb2-5621-4f84-a370-043d45f2288d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Topographic map of classification performance over the scalp for classifying attended versus unattended speakers for all participants.</div>\nParticipant\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nBest AUC\n0.92\n0.92\n1\n0.84\n0.83\n0.92\n0.80\n0.91\n0.96\n0.89\nBest channel\nFz\nC3\nC4\nFc3\nF4\nT7\nC3\nC4\nCPz\nC3\nTable 2: Channel with maximum performance and its corresponding AUC performance.\n5.1.2. Classification performance versus trial length analysis In this section we analyze the effect of trial length on classification performance. Specifically, using the calibration data, we consider different lengths (from 2 seconds to 20 seconds) of EEG and estimated sound sources to calculate the cross correlations and extract features accordingly to train our classifier to distinguish the attended sound source from the unattended one. Figure 4 shows the classification performance using all 16 channels. In this figure, different colors represent the performances of different participants. The blue curve is the average of performance over all 10 participants using different data lengths for classification. Dark and light shaded areas around the average line shows the 50 and 95 percent confidence interval calculated according to the bootstrap method, respectively. Figure 4(a) shows AUC performance while Figure 4(b) shows probability of correct decision (i.e., accuracy). Moreover, Figure 4(b) also compares our results with a related previous work that is presented in [24]. The performance reported for 128 channels in that previous work is illustrated as a green line in this figure. In this figure, we observe that using much smaller number of channels, our method outperforms the previous approach.\n# 5.2. Online Controller Performance\nRecall that as explained in Section 3.2, the online experiment includes listening to 10 two-minute sequences. During each sequence the participants were requested to focus their auditory attention to one of the speakers. Each sequence contains multiple trials and within each sequence we perform adaptive sound source weight estimation and update after every trial (20 seconds). More specifically, we calculate the EEG evidence as explained in Section 4.1. Using conditional probability density functions as described in section 4.2, we obtain the posterior estimate of the probability for each class being the intended source, which is proportional to class conditional likelihoods times prior knowledge on probability of attention. Then source weights for each source are adjusted as being proportional to the posterior probability of that class given EEG evidence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/91e7/91e7805c-acc9-4725-9138-c400943fe650.png\" style=\"width: 50%;\"></div>\nFigure 4: Performance versus trial length curves considering (a) AUC and (b) Accuracy as performance metrics. Different colored dots are used to represent the performance of different participants. The blue curve is the average of performance values over all 10 participants. The green line presents the performance results of a previous approach.\nIn the equation above, k is the sequence index and n is the trial index. Each sequence contains 6 trials and during each sequence we assume that the user is focusing on the same sounds source. This equation assumes that the attention remains on the same source during the updates in each sequence. Also in this weight update equation above we initialize p(Ak = i|\u03b5k,0) = 0.5. We trained the system using a calibration session and tested the learned model in an online session. Users attempted to amplify the designated target speech with their\n(8)\n(9)\n(10)\nauditory attention using this brain interface in 10, two-minute-long sequences. Figure 5 shows the average of decided weights (at every 20 seconds over 5 trials) for attended and unattended speech sources over the course of two minutes, for male and female narrators. Figure 5 (a) is showing the average of the estimated probabilities for each class at the end of each trial using its preceding 20 seconds of data, as stated in equation 8. Figure 5 (b) shows the average of employed weights instead of normalized probabilities. The difference between Figures 5 (a) and (b) is due to the limits imposed on weights ([0.25 to 0.75] which are shown with green constants). These limitations were imposed to ensure the audibility\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ec9/1ec92352-7d73-4c5e-8581-a2ab58a6d6fa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ecd/2ecdbb37-f182-4aa1-b38a-a3c07cc2e4c4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bab/3baba88a-eef2-4384-98a4-275f8492b7c2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8115/81153c37-29c2-432d-a867-adb245aab4f5.png\" style=\"width: 50%;\"></div>\nsource was lower than the unattended one; however, since the system imposes a lower bound on the weights, the participant was able to recover and the weight of the attended sound source increased accordingly before the sequence ended.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff23/ff230ed0-48f2-4f26-aab5-a60ad886cad7.png\" style=\"width: 50%;\"></div>\nFigure 6: Two examples for a normal versus mistake recovery case. (First row) is showing an example of a sequence with mistake recovery by participant. Left column is showing the calculated normalized probabilities at the end of each trial and right column is showing the corresponding assigned weights to normalized probabilities at the beginning of each trial. (Second row) in an example of a normal sequence. Limits imposed on the assigned weights are shown with green constant lines in all figures.\n# 5.3. Online Vs Offline data analysis\nSince changes in energy and amplitude of competing sound sources will potentially change the statistics of the EEG measurements, analyzing how robust feature vectors are to these changes can help us understand the impacts of the weights of the sound sources on the attention and EEG models. Table 3 shows the generalization of the classifier trained on the calibration data and tested on the data collected during the online sessions when the EEG from all the channels were used. Specifically, the first and the second rows of the table present the AUC results when 5-fold cross validation is performed on the calibration and online session data, respectively. The results in the third row are obtained when\nthe classifier is trained using the calibration data and tested on the online data. Therefore, the third row demonstrates the generalization of the trained auditory attention classifier from the calibration session to the online testing. Note here that the third row demonstrates the performance of the auditory attention classifier when it was used in online session where the sound source weights were adaptively updated. From this table, we observe that there is a decrease in the performance when the classifier is used in the online session compared to the calibration session. Even though the classification accuracy is acceptable when the classifier trained on the calibration data and tested on the online data as illustrated in row 3 of the table, a calibration session with varying weights on the sounds sources could potentially improve the classification accuracy further. This will be the focus of our future work.\nAUC\nParticipant\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nCalibration Data (offline)\n0.91\n0.89\n1\n0.81\n0.88\n0.82\n0.77\n0.89\n0.94\n0.83\nOnline Session Data\n0.83\n0.74\n0.98\n0.63\n0.82\n0.8\n0.74\n0.92\n0.83\n0.69\nCalibration Model on Online Data\n0.86\n0.77\n0.95\n0.73\n0.77\n0.83\n0.8\n0.82\n0.86\n0.70\nTable 3: AUCs for offline and online data independently and applying the learned model from offline data on online data.\nTable 3: AUCs for offline and online data independently and applying the learned model from offline data on online data.\n# 6. Conclusion, Limitations and future work\nThis work is a novel attempt to investigate the feasibility of a close loop online sound source modulation system using a non-invasive EEG-based brain interface. In a scenario to detect the attended sound source in the presence of two speakers, we presented two main contributions in this paper. First, we showed high offline attended sound source classification accuracy with single channel EEG when the EEG duration was 20 seconds. Second, the novel brain interface presented in this manuscript utilizes an automatic gain control to adjust the amplitudes of attended and unattended sound sources with the goal of increasing signal-noise-ratio and improving listening and hearing comfort. Through an experimental study, we showed that the designed BCI together\nwith the automatic gain control has the potential to improve the information rate by reducing the trial lengths and increasing the classification accuracies for shorter trial lengths compared to the performance results reported in the existing related works. Even though promising results were obtained with this proof of concept study, there are many opportunities to improve the performance of the system. For example, various different techniques could be investigated to optimize the automatic gain control scheme or the classification method with the purpose of enabling fast and accurate decision making in an online setting. This improvement is essential for the presented BCI to be a practical reality and potentially be a part of the future generations of hearing aids.\n# Acknowledgment\nThis work is supported by NSF (CNS-1136027, IIS-1149570, CNS-1544895), NIDLRR (90RE5017-02-01), and NIH (R01DC009834).\n# References\n# References\n# [1] S. Kochkin, Marketrak viii: 25-year trends in the hearing health market, Hearing Review 16 (11) (2009) 12\u201331.\n[1] S. Kochkin, Marketrak viii: 25-year trends in the hearing health market, Hearing Review 16 (11) (2009) 12\u201331. [2] S. A. Gelfand, Hearing: An introduction to psychological and physiological acoustics, CRC Press, 2016. [3] K. Chung, Challenges and recent developments in hearing aids part i. speech understanding in noise, microphone technologies and noise reduction algorithms, Trends in Amplification 8 (3) (2004) 83\u2013124. [4] W. A. Yost, The cocktail party problem: Forty years later, Binaural and spatial hearing in real and virtual environments (1997) 329\u2013347. [5] W. A. Yost, S. Sheft, Auditory perception, in: Human psychophysics, Springer, 1993, pp. 193\u2013236.\n[6] A. S. Bregman, Auditory scene analysis: The perceptual organization of sound, MIT press, 1994. [7] E. C. Cherry, Some experiments on the recognition of speech, with one and with two ears, The Journal of the acoustical society of America 25 (5) (1953) 975\u2013979. [8] E. M. Kaya, M. Elhilali, Modelling auditory attention, Phil. Trans. R. Soc. B 372 (1714) (2017) 20160101. [9] M. Ungstrup, P. Kidmose, M. L. Rank, Hearing aid with self fitting capabilities, uS Patent App. 14/167,256 (Jan. 29 2014). [10] G. Kidd Jr, S. Favrot, J. G. Desloge, T. M. Streeter, C. R. Mason, Design and preliminary testing of a visually guided hearing aid, The Journal of the Acoustical Society of America 133 (3) (2013) EL202\u2013EL207. [11] M. Schreuder, T. Rost, M. Tangermann, Listen, you are writing! speeding up online spelling with a dynamic auditory bci, Frontiers in neuroscience 5. [12] J. H\u00a8ohne, M. Schreuder, B. Blankertz, M. Tangermann, A novel 9-class auditory erp paradigm driving a predictive text entry system, Frontiers in neuroscience 5. [13] A. K\u00a8ubler, A. Furdea, S. Halder, E. M. Hammer, F. Nijboer, B. Kotchoubey, A brain\u2013computer interface controlled auditory eventrelated potential (p300) spelling system for locked-in patients, Annals of the New York Academy of Sciences 1157 (1) (2009) 90\u2013100. [14] S. Halder, M. Rea, R. Andreoni, F. Nijboer, E. Hammer, S. Kleih, N. Birbaumer, A. K\u00a8ubler, An auditory oddball brain\u2013computer interface for binary choices, Clinical Neurophysiology 121 (4) (2010) 516\u2013523. [15] A. Furdea, S. Halder, D. Krusienski, D. Bross, F. Nijboer, N. Birbaumer, A. K\u00a8ubler, An auditory oddball (p300) spelling system for brain-computer interfaces, Psychophysiology 46 (3) (2009) 617\u2013625.\n[16] S. Kanoh, K.-i. Miyamoto, T. Yoshinobu, A brain-computer interface (bci) system based on auditory stream segregation, in: Engineering in Medicine and Biology Society, 2008. EMBS 2008. 30th Annual International Conference of the IEEE, IEEE, 2008, pp. 642\u2013645. [17] N. Ding, J. Z. Simon, Cortical entrainment to continuous speech: functional roles and interpretations, Frontiers in human neuroscience 8. [18] S. J. Aiken, T. W. Picton, Human cortical responses to the speech envelope, Ear and hearing 29 (2) (2008) 139\u2013157. [19] Y.-Y. Kong, A. Mullangi, N. Ding, Differential modulation of auditory responses to attended and unattended speech in different listening conditions, Hearing research 316 (2014) 73\u201381. [20] A. J. Power, E. C. Lalor, R. B. Reilly, Endogenous auditory spatial attention modulates obligatory sensory activity in auditory cortex, Cerebral Cortex 21 (6) (2011) 1223\u20131230. [21] C. M. Sheedy, A. J. Power, R. B. Reilly, M. J. Crosse, G. M. Loughnane, E. C. Lalor, Endogenous auditory frequency-based attention modulates electroencephalogram-based measures of obligatory sensory activity in humans, NeuroReport 25 (4) (2014) 219\u2013225. [22] J. A. O\u2019Sullivan, A. J. Power, N. Mesgarani, S. Rajaram, J. J. Foxe, B. G. Shinn-Cunningham, M. Slaney, S. A. Shamma, E. C. Lalor, Attentional selection in a cocktail party environment can be decoded from single-trial eeg, Cerebral Cortex (2014) bht355. [23] B. Mirkovic, S. Debener, M. Jaeger, M. De Vos, Decoding the attended speech stream with multi-channel eeg: implications for online, daily-life applications, Journal of neural engineering 12 (4) (2015) 046007. [24] C. Horton, R. Srinivasan, M. DZmura, Envelope responses in single-trial eeg indicate attended speaker in a cocktail party, Journal of neural engineering 11 (4) (2014) 046015.\n[25] B. N. Pasley, S. V. David, N. Mesgarani, A. Flinker, S. A. Shamma, N. E. Crone, R. T. Knight, E. F. Chang, Reconstructing speech from human auditory cortex, PLoS Biol 10 (1) (2012) e1001251. [26] B. Mirkovic, M. G. Bleichner, M. De Vos, S. Debener, Target speaker detection with concealed eeg around the ear, Frontiers in Neuroscience 10. [27] S. A. Fuglsang, T. Dau, J. Hjortkj\u00e6r, Noise-robust cortical tracking of attended speech in real-world acoustic scenes, NeuroImage. [28] C. K. Machens, M. S. Wehr, A. M. Zador, Linearity of cortical receptive fields measured with natural sounds, Journal of Neuroscience 24 (5) (2004) 1089\u20131100. [29] N. Ding, J. Z. Simon, Neural coding of continuous speech in auditory cortex during monaural and dichotic listening, Journal of neurophysiology 107 (1) (2012) 78\u201389. [30] L. Fiedler, M. Woestmann, C. Graversen, A. Brandmeyer, T. Lunner, J. Obleser, Single-channel in-ear-eeg detects the focus of auditory attention to concurrent tone streams and mixed speech, Journal of Neural Engineering 14 (3) (2017) 036020. [31] E. Alickovic, T. Lunner, F. Gustafsson, A system identification approach to determining listening attention from eeg signals, in: Signal Processing Conference (EUSIPCO), 2016 24th European, IEEE, 2016, pp. 31\u201335. [32] M. Haghighi, M. Moghadamfalahi, H. Nezamfar, M. Akcakaya, D. Erdogmus, Toward a brain interface for tracking attended auditory sources, in: 2016 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), IEEE, 2016, pp. 1\u20136. [33] J. H. Friedman, Regularized discriminant analysis, Journal of the American statistical association 84 (405) (1989) 165\u2013175.\n[34] B. W. Silverman, Density estimation for statistics and data analysis, Vol. 26, CRC press, 1986. [35] U. Orhan, D. Erdogmus, B. Roark, B. Oken, M. Fried-Oken, Offline analysis of context contribution to erp-based typing bci performance, Journal of neural engineering 10 (6) (2013) 066003.\n[34] B. W. Silverman, Density estimation for statistics and data analysis, Vol. 26, CRC press, 1986. [35] U. Orhan, D. Erdogmus, B. Roark, B. Oken, M. Fried-Oken, Offline analysis of context contribution to erp-based typing bci performance, Journal of neural engineering 10 (6) (2013) 066003.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of hearing loss affecting approximately 35 million Americans, with a focus on improving auditory attention detection through EEG-based systems. Previous methods have struggled with identifying attended sound sources in complex auditory environments, necessitating a new approach to enhance hearing aid technology.",
        "problem": {
            "definition": "The problem involves effectively distinguishing between attended and unattended sound sources in noisy environments, such as cocktail party scenarios, where traditional hearing aids fail to provide adequate support.",
            "key obstacle": "The main challenge lies in the inability of existing methods to accurately identify and amplify the attended sound source amidst noise, particularly when using EEG data with limited channels and shorter durations."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by the brain's ability to focus on specific auditory stimuli and the potential to leverage EEG signals to detect this focus.",
            "opinion": "The idea entails an online modulation system that adjusts sound source levels based on probabilistic detection of auditory attention, enhancing the listening experience for users.",
            "innovation": "This method innovates by utilizing a single-channel EEG system for attention detection, significantly reducing the required data duration and number of channels compared to existing approaches."
        },
        "method": {
            "method name": "EEG-assisted Sound Source Modulation",
            "method abbreviation": "ESSM",
            "method definition": "The method employs a non-invasive EEG-based brain-computer interface to probabilistically detect auditory attention and modulate sound sources accordingly.",
            "method description": "The method uses EEG signals to infer attention levels and adjusts the gain of sound sources in real-time to enhance the attended source.",
            "method steps": [
                "Collect EEG data during auditory stimulus presentation.",
                "Calculate cross-correlation between EEG signals and sound source envelopes.",
                "Train a classifier on offline data to distinguish attended from unattended sources.",
                "Implement an online system to adjust sound source gains based on real-time attention inference."
            ],
            "principle": "The effectiveness of this method is grounded in the ability of EEG signals to reflect neural responses to auditory stimuli, allowing for adaptive modulation of sound sources based on attention."
        },
        "experiments": {
            "evaluation setting": "The experiments involved ten volunteers who participated in calibration and online sessions, listening to diotic auditory stimuli while their EEG was recorded.",
            "evaluation method": "Performance was assessed by measuring the accuracy of the attention classifier and the effectiveness of the online sound source modulation, with statistical analysis conducted to compare results across different trial lengths and conditions."
        },
        "conclusion": "The study demonstrates that the proposed EEG-assisted sound source modulation system can effectively enhance the level of attended sound sources in noisy environments, showing potential for future integration into hearing aids.",
        "discussion": {
            "advantage": "The key advantages include high classification accuracy with reduced EEG channel requirements and shorter data durations, making the system more practical for real-world applications.",
            "limitation": "Limitations include potential decreases in performance during online use compared to offline calibration, indicating a need for further optimization of the system.",
            "future work": "Future research should focus on improving the automatic gain control mechanisms and exploring alternative classification techniques to enhance system performance in dynamic auditory environments."
        },
        "other info": {
            "acknowledgment": "This work is supported by NSF (CNS-1136027, IIS-1149570, CNS-1544895), NIDLRR (90RE5017-02-01), and NIH (R01DC009834)."
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "The problem involves effectively distinguishing between attended and unattended sound sources in noisy environments, such as cocktail party scenarios, where traditional hearing aids fail to provide adequate support."
        },
        {
            "section number": "3.1",
            "key information": "The proposed idea is inspired by the brain's ability to focus on specific auditory stimuli and the potential to leverage EEG signals to detect this focus."
        },
        {
            "section number": "3.3",
            "key information": "The method employs a non-invasive EEG-based brain-computer interface to probabilistically detect auditory attention and modulate sound sources accordingly."
        },
        {
            "section number": "4.1",
            "key information": "The key advantages include high classification accuracy with reduced EEG channel requirements and shorter data durations, making the system more practical for real-world applications."
        },
        {
            "section number": "6.1",
            "key information": "The study demonstrates that the proposed EEG-assisted sound source modulation system can effectively enhance the level of attended sound sources in noisy environments, showing potential for future integration into hearing aids."
        },
        {
            "section number": "7.3",
            "key information": "Future research should focus on improving the automatic gain control mechanisms and exploring alternative classification techniques to enhance system performance in dynamic auditory environments."
        }
    ],
    "similarity_score": 0.4618161698285201,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0632_artif/papers/EEG-assisted Modulation of Sound Sources in the Auditory Scene.json"
}