{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.09375",
    "title": "Deceptive Patterns of Intelligent and Interactive Writing Assistants",
    "abstract": "Large Language Models have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing.",
    "bib_name": "benharrak2024deceptivepatternsintelligentinteractive",
    "md_text": "# Deceptive Patterns of Intelligent and Interactive Writing Assistants\nKarim Benharrak karim@benharrak.com The University of Texas at Austin Austin, TX, USA Tim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany Daniel Buschek daniel.buschek@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nTim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany Daniel Buschek daniel.buschek@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nKarim Benharrak karim@benharrak.com The University of Texas at Austin Austin, TX, USA Tim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nTim Zindulka tim.zindulka@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\nKarim Benharrak karim@benharrak.com The University of Texas at Austin Austin, TX, USA\n# ABSTRACT\nLarge Language Models have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing.\nCCS CONCEPTS \u2022 Human-centered computing \u2192HCI theory, concepts and models.\nDeceptive Pattern, Writing, UX, Large Language Model\n# 1 INTRODUCTION\nIn this short paper, we transfer known deceptive UI/UX patterns [3] to the context of interactive and intelligent writing assistant systems [5]. Our goal is to raise awareness of the potential use of such patterns in recently popular applications of Large Language Models (LLMs) in interactive systems, such as ChatGPT1 and systems that offer (AI) assistance for text-related tasks. We do not claim that these patterns are used in specific products at the time of writing. That said, we have anecdotally found examples that are very close to what such deceptive patterns could look like in this context. In general, deceptive patterns in UI/UX design are design choices that deliberately deceive users, often to increase profit: Concretely, deceptive.design2 defines them as \u201ctricks used in websites and apps that make you do things that you didn\u2019t mean to, like buying or signing up for something.\u201d Our approach follows related work that transferred deceptive patterns to the domain of explainability, transparency, and user control in intelligent systems [1]. Similarly, we conducted a brainstorming session in our research group to collect potential deceptive\n1https://chat.openai.com/ 2https://www.deceptive.design\nDaniel Buschek daniel.buschek@uni-bayreuth.de University of Bayreuth Bayreuth, Bavaria, Germany\npatterns for writing assistants. As a foundation, we used the patterns listed by Gray et al. [3], as well as the collection of patterns and concrete examples on deceptive.design. We examine deceptive patterns in a novel domain, compared to prior work [2, 6], with an emphasis on assistants for writing, revising, editing, and/or other tasks related to digital text documents. These are \u201cintelligent\u201d in that they generate text or make autonomous decisions about text. Examples include many of the systems presented at previous instances of this workshop, as well as products like OpenAI\u2019s ChatGPT or Microsoft\u2019s Copilot3. See the recent survey by Lee et al. [5] for a broad overview. We next present the set of patterns with descriptions, followed by a short discussion. This is not an exhaustive collection and readers are invited to think about further patterns.\n# 2 DECEPTIVE PATTERNS OF WRITING ASSISTANTS\n# 2 DECEPTIVE PATTERNS OF WRITING\nHere we describe examples of potential deceptive patterns in the context of AI writing tools.\n# 2.1 Nagging\nNagging is the \u201credirection of expected functionality that persists beyond one or more interactions\u201d [3]. A writing assistant with this pattern might repeatedly make suggestions or recommendations, even when the user may not desire them. For instance, a chatbot might interrupt the user\u2019s workflow with repeated pop-ups that suggest functions or services, even though the user had declined them earlier. Related, they might show text suggestions that do not actually help with the writing task but advertise a premium version or newsletter. A provider may be motivated to employ this pattern in the hopes of increased revenue.\n# 2.2 Sneaking\nSneaking is characterized by attempting to hide, disguise, or delay the divulging of relevant information to the user [3]. An AI writing assistant might sneak in unwanted text changes (cf. Figure 1). For example, when asked to improve a text, the assistant might also (subtly) change the text\u2019s expressed opinion. This might manipulate the writer\u2019s memory: They might later falsely recall having expressed certain opinions or ideas in their writing, possibly adopting them as their own. A similar pattern is Trick Wording, which could exploit a user\u2019s oversight: When the assistant is used to generate or improve longer text, the result may start in line with what the user wanted, only to deviate in the middle of the text. This might trick the user into eventually publishing text that expresses\n3https://copilot.microsoft.com/\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff8b/ff8b81f7-8e41-4998-b65c-9ac7826e1040.png\" style=\"width: 50%;\"></div>\nFigure 1: Mock-up example on how a writing assistant may subtly change the opinion expressed in the text: After requesting a continuation of their sentence, the user might not expect the additional change sneaked into the beginning of the sentence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ff7/6ff799d7-a5a5-4165-9d13-ddd20ab4226a.png\" style=\"width: 50%;\"></div>\nFigure 2: Industry example (Bing AI) on how prompt autocompletion may shift users\u2019 original intentions: The user may initially seek a neutral description of a term yet be subtly guided towards requesting a non-neutral description.\nunwanted views. A potential motivation for system providers to use this pattern is opinion influence.\n# 2.3 Interface Interference\nThe deceptive pattern of Interface Interference involves \u201cmanipulating the user interface to privilege certain actions over others\u201d [3]. For instance, writing assistants that produce text suggestions might prominently display specific suggestions that align with a hidden agenda, such as mentioning a specific product or favouring a particular view on a topic (cf. [4]). These suggestions may be positioned prominently at the top of a list, or they might be the only choice, in UI designs that show a single suggestion at a time. This pattern might also emerge when influencing the user in writing prompts (cf. Figure 2). Advertisement or otherwise influencing opinions might motivate system providers to employ this pattern.\n# 2.4 Forced Action\nForced Action entails \u201crequiring the user to perform a certain action [...] to access certain functionality\u201d [3]. Writing assistants might force users to engage in repeated follow-up queries. They might intentionally withhold certain advanced features or suggestions until the user asks about it again. This forces users to engage with the assistant repeatedly, motivated by a business model such as \u201cpay per request\u201d, or using up initially provided \u201cfree credit\u201d faster.\n# 2.5 Hidden Costs\nThe deceptive pattern of Hidden Costs confronts users with additional fees and charges when they have already invested time and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ed7/5ed7804c-8c1c-4bfe-9b78-ed24708a2d5f.png\" style=\"width: 50%;\"></div>\nFigure 3: Mock-up example on how a writing assistant may offer to continue the generated text only after subscribing to the premium (paid) version: After investing time and effort into generating the text, the user is interested in knowing how the story continues, thus potentially being influenced to subscribe to the service.\neffort in the process [3]. For example, the assistant might offer detailed suggestions and corrections for a part of the text the user has been working on, but obscure the remainder of the document until a premium service is paid, thus enticing users with the promise of further improvements (cf. Figure 3). This pattern is motivated by financial interests, giving a glimpse of the tool\u2019s capabilities while locking the full scope behind a paywall.\n# 3 DISCUSSION & CONCLUSION\nWe have presented a first set of deceptive UX patterns for writing assistants.\nOverall, besides financial gains, one potential motivation for such patterns is opinion influence through text shown throughout the interaction as well as afterwards. This is different from other UI designs and use-cases, because in AI writing assistants, language is both part of the interaction (e.g. writing a prompt) and its output (e.g. created text document). Understanding the potential opinion influence on both textual content and the writer appears as a key direction for further research here (cf. [4]). Finally, beyond their immediate purposes, deceptive patterns that encourage increased system use may lead to users developing dependencies on AI assistants. This raises concerns about deskilling, where users may rely heavily on the system, potentially diminishing their own writing skills over time. These potential issues call for longitudinal user studies.\n# ACKNOWLEDGMENTS\nDaniel Buschek is supported by a Google Research Scholar Award. This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt).\n# REFERENCES\n[1] Michael Chromik, Malin Eiband, Sarah Theres V\u00f6lkel, and Daniel Buschek. 2019. Dark Patterns of Explainability, Transparency, and User Control for Intelligent Systems. In IUI Workshops. https://ceur-ws.org/Vol-2327/IUI19WS-ExSS20197.pdf [2] Linda Di Geronimo, Larissa Braz, Enrico Fregnan, Fabio Palomba, and Alberto Bacchelli. 2020. UI Dark Patterns and Where to Find Them: A Study on Mobile Applications and User Perception. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3313831.3376600 [3] Colin M. Gray, Yubo Kou, Bryan Battles, Joseph Hoggatt, and Austin L. Toombs. 2018. The Dark (Patterns) Side of UX Design. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918). Association for Computing Machinery, New York, NY, USA, 1\u201314. https: //doi.org/10.1145/3173574.3174108 [4] Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman. 2023. Co-Writing with Opinionated Language Models Affects Users\u2019 Views. In\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 111, 15 pages. https://doi.org/10.1145/3544548.3581196 [5] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Antonette Shibani, Disha Shrivastava, Lila Shroff, Agnia Sergeyuk, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. A Design Space for Intelligent and Interactive Writing Assistants. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201924). Association for Computing Machinery, New York, NY, USA. [6] Arunesh Mathur, Gunes Acar, Michael J Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale: Findings from a crawl of 11K shopping websites. Proceedings of the ACM on human-computer interaction 3, CSCW (2019), 1\u201332.\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 111, 15 pages. https://doi.org/10.1145/3544548.3581196 [5] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Antonette Shibani, Disha Shrivastava, Lila Shroff, Agnia Sergeyuk, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. A Design Space for Intelligent and Interactive Writing Assistants. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201924). Association for Computing Machinery, New York, NY, USA. [6] Arunesh Mathur, Gunes Acar, Michael J Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale: Findings from a crawl of 11K shopping websites. Proceedings of the ACM on human-computer interaction 3, CSCW (2019), 1\u201332.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "The survey aims to raise awareness of deceptive design patterns in interactive writing assistants powered by Large Language Models (LLMs) and to encourage further research into their impact on user interaction.",
            "scope": "The survey focuses on deceptive UI/UX patterns in intelligent writing assistants, particularly those related to text generation and editing. It excludes patterns not applicable to writing assistants or those that do not involve user interaction."
        },
        "problem": {
            "definition": "The core issue explored is the potential exploitation of users through deceptive design patterns in AI writing assistants, which can manipulate user behavior and influence opinions.",
            "key obstacle": "Researchers face challenges in identifying and categorizing these deceptive patterns, as well as understanding their psychological impact on users."
        },
        "architecture": {
            "perspective": "The survey categorizes existing research on deceptive patterns in writing assistants, conceptualizing them in a framework that highlights their potential effects on user experience.",
            "fields/stages": [
                {
                    "field": "Nagging",
                    "description": "Persistent suggestions that redirect user actions."
                },
                {
                    "field": "Sneaking",
                    "description": "Hiding or disguising relevant information from users."
                },
                {
                    "field": "Interface Interference",
                    "description": "Manipulating the user interface to favor certain actions or opinions."
                },
                {
                    "field": "Forced Action",
                    "description": "Requiring users to perform actions to access certain features."
                },
                {
                    "field": "Hidden Costs",
                    "description": "Introducing additional fees after users have invested time into the system."
                }
            ]
        },
        "conclusion": {
            "comparisons": "The survey compares various deceptive patterns and their effectiveness in influencing user behavior and opinions, highlighting the differences in approach and outcomes across studies.",
            "results": "Key takeaways include the need for awareness of these patterns and the potential for increased dependency on AI writing assistants, which could undermine users' writing skills."
        },
        "discussion": {
            "advantage": "Current research has highlighted the financial motivations behind deceptive patterns and their potential to influence opinions, achieving some awareness among users and designers.",
            "limitation": "Existing studies may not comprehensively cover all deceptive patterns, and the impact of these patterns on long-term user behavior remains underexplored.",
            "gaps": "Questions about the psychological effects of these patterns and how they may lead to user dependency on AI remain unanswered.",
            "future work": "Future research should focus on longitudinal studies to assess the impact of deceptive patterns on user skills and explore emerging trends in AI writing tools."
        },
        "other info": {
            "acknowledgments": "Daniel Buschek is supported by a Google Research Scholar Award, and the project is funded by the Bavarian State Ministry of Science and the Arts."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The survey aims to raise awareness of deceptive design patterns in interactive writing assistants powered by Large Language Models (LLMs) and to encourage further research into their impact on user interaction."
        },
        {
            "section number": "1.2",
            "key information": "The significance of understanding deceptive UI/UX patterns in intelligent writing assistants is crucial for improving user interaction and preventing manipulation."
        },
        {
            "section number": "1.3",
            "key information": "The main objectives of the survey paper include identifying and categorizing deceptive design patterns and understanding their psychological impact on users."
        },
        {
            "section number": "2. Background and Definitions",
            "key information": "The core issue explored is the potential exploitation of users through deceptive design patterns in AI writing assistants, which can manipulate user behavior and influence opinions."
        },
        {
            "section number": "2.1",
            "key information": "The survey focuses on deceptive UI/UX patterns in intelligent writing assistants, particularly those related to text generation and editing."
        },
        {
            "section number": "4. Neural Network Optimization",
            "key information": "Researchers face challenges in identifying and categorizing deceptive patterns, as well as understanding their psychological impact on users."
        },
        {
            "section number": "7",
            "key information": "The survey categorizes existing research on deceptive patterns in writing assistants, conceptualizing them in a framework that highlights their potential effects on user experience."
        },
        {
            "section number": "8",
            "key information": "Key takeaways include the need for awareness of deceptive patterns and the potential for increased dependency on AI writing assistants, which could undermine users' writing skills."
        }
    ],
    "similarity_score": 0.6262144208366718,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Deceptive Patterns of Intelligent and Interactive Writing Assistants.json"
}