{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2211.15661",
    "title": "What learning algorithm is in-context learning? Investigations with linear models",
    "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples (x, f(x)) presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners\u2019 late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms.",
    "bib_name": "akyrek2023learningalgorithmincontextlearning",
    "md_text": "# WHAT LEARNING ALGORITHM IS IN-CONTEXT LEARNING? INVESTIGATIONS WITH LINEAR MODELS\nEkin Aky\u00a8urek1,2,a. Dale Schuurmans1 Jacob Andreas\u22172 Tengyu Ma\u22171,3,b Denny Zhou\u22171\n# ABSTRACT\nNeural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples (x, f(x)) presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners\u2019 late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms.\n# INTRODUCTION\nOne of the most surprising behaviors observed in large neural sequence models is in-context learning (ICL; Brown et al., 2020). When trained appropriately, models can map from sequences of (x, f(x)) pairs to accurate predictions f(x\u2032) on novel inputs x\u2032. This behavior occurs both in models trained on collections of few-shot learning problems (Chen et al., 2022; Min et al., 2022) and surprisingly in large language models trained on open-domain text (Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022). ICL requires a model to implicitly construct a map from in-context examples to a predictor without any updates to the model\u2019s parameters themselves. How can a neural network with fixed parameters to learn a new function from a new dataset on the fly? This paper investigates the hypothesis that some instances of ICL can be understood as implicit implementation of known learning algorithms: in-context learners encode an implicit, contextdependent model in their hidden activations, and train this model on in-context examples in the course of computing these internal activations. As in recent investigations of empirical properties of ICL (Garg et al., 2022; Xie et al., 2022), we study the behavior of transformer-based predictors (Vaswani et al., 2017) on a restricted class of learning problems, here linear regression. Unlike in past work, our goal is not to understand what functions ICL can learn, but how it learns these functions: the specific inductive biases and algorithmic properties of transformer-based ICL. In Section 3, we investigate theoretically what learning algorithms transformer decoders can implement. We prove by construction that they require only a modest number of layers and hidden units to train linear models: for d-dimensional regression problems, with O(d) hidden size and constant depth, a transformer can implement a single step of gradient descent; and with O(d2) hidden size aCorrespondence to akyurek@mit.edu. Ekin is a student at MIT, and began this work while he was intern at Google Research. Code and reference implementations are released at this web page\nand constant depth, a transformer can update a ridge regression solution to include a single new observation. Intuitively, n steps of these algorithms can be implemented with n times more layers. In Section 4, we investigate empirical properties of trained in-context learners. We begin by constructing linear regression problems in which learner behavior is under-determined by training data (so different valid learning rules will give different predictions on held-out data). We show that model predictions are closely matched by existing predictors (including those studied in Section 3), and that they transition between different predictors as model depth and training set noise vary, behaving like Bayesian predictors at large hidden sizes and depths. Finally, in Section 5, we present preliminary experiments showing how model predictions are computed algorithmically. We show that important intermediate quantities computed by learning algorithms for linear models, including parameter vectors and moment matrices, can be decoded from in-context learners\u2019 hidden activations. A complete characterization of which learning algorithms are (or could be) implemented by deep networks has the potential to improve both our theoretical understanding of their capabilities and limitations, and our empirical understanding of how best to train them. This paper offers first steps toward such a characterization: some in-context learning appears to involve familiar algorithms, discovered and implemented by transformers from sequence modeling tasks alone.\n# 2 PRELIMINARIES\nTraining a machine learning model involves many decisions, including the choice of model architecture, loss function and learning rule. Since the earliest days of the field, research has sought to understand whether these modeling decisions can be automated using the tools of machine learning itself. Such \u201cmeta-learning\u201d approaches typically treat learning as a bi-level optimization problem (Schmidhuber et al., 1996; Andrychowicz et al., 2016; Finn et al., 2017): they define \u201cinner\u201d and \u201couter\u201d models and learning procedures, then train an outer model to set parameters for an inner procedure (e.g. initializer or step size) to maximize inner model performance across tasks. Recently, a more flexible family of approaches has gained popularity. In in-context learning (ICL), meta-learning is reduced to ordinary supervised learning: a large sequence model (typically implemented as a transformer network) is trained to map from sequences [x1, f(x1), x2, f(x2), ..., xn] to predictions f(xn) (Brown et al., 2020; Olsson et al., 2022; Laskin et al., 2022; Kirsch & Schmidhuber, 2021). ICL does not specify an explicit inner learning procedure; instead, this procedure exists only implicitly through the parameters of the sequence model. ICL has shown impressive results on synthetic tasks and naturalistic language and vision problems (Garg et al., 2022; Min et al., 2022; Zhou et al., 2022; Hollmann et al., 2022). Past work has characterized what kinds of functions ICL can learn (Garg et al., 2022; Laskin et al., 2022; M\u00a8uller et al., 2021) and the distributional properties of pretraining that can elicit in-context learning (Xie et al., 2021; Chan et al., 2022). But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training? This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n# 2.1 THE TRANSFORMER ARCHITECTURE\nTransformers (Vaswani et al., 2017) are neural network models that map a sequence of input vectors x = [x1, . . . , xn] to a sequence of output vectors y = [y1, . . . , yn]. Each layer in a transformer maps a matrix H(l) (interpreted as a sequence of vectors) to a sequence H(l+1). To do so, a transformer layer processes each column h(l) i of H(l) in parallel. Here, we are interested in autoregressive (or \u201cdecoder-only\u201d) transformer models in which each layer first computes a self-attention:\nwhere each b is the response of an \u201cattention head\u201d defined by:\n(1) (2)\n(3)\nthen applies a feed-forward transformation:\nere \u03c3 denotes a nonlinearity, e.g. a Gaussian error linear unit (GeLU; Hend\n\ufffd and \u03bb denotes layer normalization (Ba et al., 2016):\n\ufffd where the expectation and variance are computed across the entries of x. To map from x to y, a transformer applies a sequence of such layers, each with its own parameters. We use \u03b8 to denote a model\u2019s full set of parameters (the complete collection of W matrices across layers). The three main factors governing the computational capacity of a transformer are its depth (the number of layers), its hidden size (the dimension of the vectors h), and the number of heads (denoted m above).\n# 2 TRAINING FOR IN-CONTEXT LEARNING\nWe study transformer models directly trained on an ICL objective. (Some past work has found that ICL also \u201cemerges\u201d in models trained on general text datasets; Brown et al., 2020.) To train a transformer T with parameters \u03b8 to perform ICL, we first define a class of functions F, a distribution p(f) supported on F, a distribution p(x) over the domain of functions in F, and a loss function L. We then choose \u03b8 to optimize the auto-regressive objective, where the resulting T\u03b8 is an in-context learner: \uf8ee \uf8f9\n# 2.3 LINEAR REGRESSION\nOur experiments focus on linear regression problems. In these problems, F is the space of linear functions f(x) = w\u22a4x where w, x \u2208Rd, and the loss function is the squared error L(y, y\u2032) = (y \u2212y\u2032)2. Linear regression is a model problem in machine learning and statistical estimation, with diverse algorithmic solutions. It thus offers an ideal test-bed for understanding ICL. Given a dataset with inputs X = [x1, . . . , xn] and y = [y1, . . . , yn], the (regularized) linear regression objective:\nWith \u03bb = 0, this objective is known as ordinary least squares regression (OLS); with \u03bb > 0, it is known as ridge regression (Hoerl & Kennard, 1970). (As discussed further in Section 4, ridge regression can also be assigned a Bayesian interpretation.) To present a linear regression problem to a transformer, we encode both x and f(x) as d + 1-dimensional vectors: \u02dcxi = [0, xi], \u02dcyi = [yi, 0d], where 0d denotes the d-dimensional zero vector.\nFor a transformer-based model to solve Eq. (9) by implementing an explicit learning algorithm that learning algorithm must be implementable via Eq. (1) and Eq. (4) with some fixed choice of transformer parameters \u03b8. In this section, we prove constructively that such parameterizations exist giving concrete implementations of two standard learning algorithms. These proofs yield upper bounds on how many layers and hidden units suffice to implement (though not necessarily learn) each algorithm. Proofs are given in Appendices A and B.\n(4) (5)\n(6)\n(7)\n(8)\n(9)\n# 3.1 PRELIMINARIES\nIt will be useful to first establish a few computational primitives with simple transformer implementations. Consider the following four functions from RH\u00d7T \u2192RH\u00d7T : mov(H; s, t, i, j, i\u2032, j\u2032): selects the entries of the sth column of H between rows i and j, and copies them into the tth column (t \u2265s) of H between rows i\u2032 and j\u2032, yielding the matrix: \ufffd \ufffd\n \u2192 mov(H; s, t, i, j, i\u2032, j\u2032): selects the entries of the sth column of H between rows i and j, and copies them into the tth column (t \u2265s) of H between rows i\u2032 and j\u2032, yielding the matrix:\nmul(H; a, b, c, (i, j), (i\u2032, j\u2032), (i\u2032\u2032, j\u2032\u2032)): in each column h of H, interprets the entries between i and j as an a \u00d7 b matrix A1, and the entries between i\u2032 and j\u2032 as a b \u00d7 c matrix A2, multiplies these matrices together, and stores the result between rows i\u2032\u2032 and j\u2032\u2032, yielding a matrix in which each column has the form [h:i\u2032\u2032\u22121, A1A2, hj\u2032\u2032:]\u22a4. div(H; (i, j), i\u2032, (i\u2032\u2032, j\u2032\u2032)): in each column h of H, divides the entries between i and j by the absolute value of the entry at i\u2032, and stores the result between rows i\u2032\u2032 and j\u2032\u2032, yielding a matrix in which every column has the form [h:i\u2032\u2032\u22121, hi:j/|hi\u2032|, hj\u2032\u2032:]\u22a4. aff(H; (i, j), (i\u2032, j\u2032), (i\u2032\u2032, j\u2032\u2032), W1, W2, b): in each column h of H, applies an affine transformation to the entries between i and j and i\u2032 and j\u2032, then stores the result between rows i\u2032\u2032 and j\u2032\u2032, yielding a matrix in which every column has the form [h:i\u2032\u2032\u22121, W1hi:j + W2hi\u2032:j\u2032 + b, hj\u2032\u2032:]\u22a4. Lemma 1. Each of mov, mul, div and aff can be implemented by a single transformer decoder layer: in Eq. (1) and Eq. (4), there exist matrices W Q, W K, W V , W F , W1 and W2 such that, given a matrix H as input, the layer\u2019s output has the form of the corresponding function output above. 1\ne operations, we can implement building blocks of two important learnin\n# 3.2 GRADIENT DESCENT\nRather than directly solving linear regression problems by evaluating Eq. (10), a standard approach to learning exploits a generic loss minimization framework, and optimizes the ridge-regression objective in Eq. (9) via gradient descent on parameters w. This involves repeatedly computing updates:\nfor different examples (xi, yi), and finally predicting w\u2032\u22a4xn on a new input xn. A step of this gradient descent procedure can be implemented by a transformer: Theorem 1. A transformer can compute Eq. (11) (i.e. the prediction resulting from single step of gradient descent on an in-context example) with constant number of layers and O(d) hidden space, where d is the problem dimension of the input x. Specifically, there exist transformer parameters \u03b8 such that, given an input matrix of the form: \ufffd \ufffd\n\ufffd   \ufffd the transformer\u2019s output matrix H(L) contains an entry equal to w\u2032\u22a4xn (Eq. (11)) at the colu index where xn is input.\n# 3.3 CLOSED-FORM REGRESSION\nAnother way to solve the linear regression problem is to directly compute the closed-form solution Eq. (10). This is somewhat challenging computationally, as it requires inverting the regularized covariance matrix X\u22a4X + \u03bbI. However, one can exploit the Sherman\u2013Morrison formula (Sherman & Morrison, 1950) to reduce the inverse to a sequence of rank-one updates performed example-byexample. For any invertible square A,\n\ufffd \ufffd 1We omit the trivial size preconditions, e.g. mul: (i \u2212j = a \u2217b, i\u2032 \u2212j\u2032 = b \u2217c, i\u2032\u2032 \u2212j\u2032\u2032 = c \u2217d).\n(11)\n(12)\nBecause the covariance matrix X\u22a4X in Eq. (10) can be expressed as a sum of rank-one terms each involving a single training example xi, this can be used to construct an iterative algorithm for computing the closed-form ridge-regression solution.\n\ufffd \ufffd with constant layers and O(d2) hidden space. More precisely, there exists a set of transformer parameters \u03b8 such that, given an input matrix of the form in Eq. (12), the transformer\u2019s output matrix H(L) contains an entry equal to w\u2032\u22a4xn (Eq. (14)) at the column index where xn is input.\nDiscussion. There are various existing universality results for transformers (Yun et al., 2020; Wei et al., 2021), and for neural networks more generally (Hornik et al., 1989). These generally require very high precision, very deep models, or the use of an external \u201ctape\u201d, none of which appear to be important for in-context learning in the real world. Results in this section establish sharper upper bounds on the necessary capacity required to implement learning algorithms specifically, bringing theory closer to the range where it can explain existing empirical findings. Different theoretical constructions, in the context of meta-learning, have been shown for linear self-attention models (Schlag et al., 2021), or for other neural architectures such as recurrent neural networks (Kirsch & Schmidhuber, 2021). We emphasize that Theorem 1 and Theorem 2 each show the implementation of a single step of an iterative algorithm; these results can be straightforwardly generalized to the multi-step case by \u201cstacking\u201d groups of transformer layers. As described next, it is these iterative algorithms that capture the behavior of real learners.\nThe previous section showed that the building blocks for two specific procedures\u2014gradient descent on the least-squares objective and closed-form computation of its minimizer\u2014are implementable by transformer networks. These constructions show that, in principle, fixed transformer parameterizations are expressive enough to simulate these learning algorithms. When trained on real datasets, however, in-context learners might implement other learning algorithms. In this section, we investigate the empirical properties of trained in-context learners in terms of their behavior. In the framework of Marr\u2019s (2010) \u201clevels of analysis\u201d, we aim to explain ICL at the computational level by identifying the kind of algorithms to regression problems that transformer-based ICL implements.\n# 4.1 BEHAVIORAL METRICS\nDetermining which learning algorithms best characterize ICL predictions requires first quantifying the degree to which two predictors agree. We use two metrics to do so:\nSquared prediction difference. Given any learning algorithm A that maps from a set of input\u2013 output pairs D = [x1, y1, . . . , xn, yn] to a predictor f(x) = A(D)(x), we define the squared prediction difference (SPD):\nwhere D is sampled as in Eq. (8). SPD measures agreement at the output level, regardless of the algorithm used to compute this output.\nImplicit linear weight difference. When ground-truth predictors all belong to a known, parametric function class (as with the linear functions here), we may also investigate the extent to which different learners agree on the parameters themselves. Given an algorithm A, we sample a context dataset D as above, and an additional collection of unlabeled test inputs DX \u2032 = {x\u2032 i}. We then compute A\u2019s prediction on each x\u2032 i, yielding a predictor-specific dataset DA = {(x\u2032 i, \u02c6yi)} = \ufffd\ufffd xi, A(D)(x\u2032 i) \ufffd\ufffd encapsulating the function learned by A. Next we compute the implied parameters: \u02c6wA = arg min \ufffd (\u02c6yi \u2212w\u22a4x\u2032 i)2 . (16)\n(14)\n(15)\n(16)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab87/ab8754e2-30ce-42c7-8a0d-841898209640.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Predictor\u2013ICL fit w.r.t. prediction differences.</div>\nFigure 1: Fit between ICL and standard learning algorithms: We plot (dimension normalized) SPD and ILWD values between textbook algorithms and ICL on noiseless linear regression with d = 8. GD(\u03b1) denotes one step of batch gradient descent and SGD(\u03b1) denotes one pass of stochastic gradient descent with learning rate \u03b1. Ridge(\u03bb) denotes Ridge regression with regularization parameter \u03bb. Under both evaluations, in-context learners agree closely with ordinary least squares, and are significantly less well approximated by other solutions to the linear regression problem.\nWe can then quantify agreement between two predictors A1 and A2 by computing the distance between their implied weights in expectation over datasets:\nILWD(A1, A2) = EDEDX\u2032 \u2225\u02c6wA1 \u2212\u02c6wA2\u22252 2 .\nWhen the predictors are not linear, ILWD measures the difference between the closest linear predictors (in the sense of Eq. (16)) to each algorithm. For algorithms that have linear hypothesis space (e.g. Ridge regression), we will use the actual value of \u02c6wA instead of the estimated value.\n# 4.2 EXPERIMENTAL SETUP\nWe train a Transformer decoder autoregresively on the objective in Eq. (8). For all experiments, we perform a hyperparameter search over depth L \u2208{1, 2, 4, 8, 12, 16}, hidden size W \u2208{16, 32, 64, 256, 512, 1024} and heads M \u2208{1, 2, 4, 8}. Other hyper-parameters are noted in Appendix D. For our main experiments, we found that L = 16, H = 512, M = 4 minimized loss on a validation set. We follow the training guidelines in Garg et al. (2022), and trained models for 500, 000 iterations, with each in-context dataset consisting of 40 (x, y) pairs. For the main experiments we generate data according to p(w) = N(0, I) and p(x) = N(0, I).\n# 4.3 RESULTS\n# ICL matches ordinary least squares predictions on noiseless datasets. We begin by comparing a (L = 16, H = 512, M = 4) transformer against a variety of reference predictors:\nICL matches ordinary least squares predictions on noiseless datasets. We begin by comparing a (L = 16, H = 512, M = 4) transformer against a variety of reference predictors:\n\u2022 k-nearest neighbors: In the uniform variant, models predict \u02c6yi = 1 3 \ufffd j yj, where j is the top-3 closest data point to xi where j < i. In the weighted variant, a weighted average \u02c6yi \u221d1 3 \ufffd j |xi \u2212xj|\u22122yj is calculated, normalized by the total weights of the yjs. \u2022 One-pass stochastic gradient descent: \u02c6yi = w\u22a4 i xi where wi is obtained by stochastic gradient descent on the previous examples with batch-size equals to 1: wi = wi\u22121 \u2212 2\u03b1(x\u22a4 i\u22121w\u22a4 i\u22121xi\u22121 \u2212x\u22a4 i\u22121yi\u22121 + \u03bbwi\u22121). \u2022 One-step batch gradient descent: \u02c6yi = w\u22a4 i xi where wi is obtained by one of step gradient descent on the batch of previous examples: wi =w0 \u22122\u03b1(X\u22a4w\u22a4X \u2212X\u22a4Y + \u03bbw0). \u2022 Ridge regression: We compute \u02c6yi = w\u2032\u22a4xi where w\u2032\u22a4= (X\u22a4X + \u03bbI)\u22121X\u22a4Y . We denote the case of \u03bb = 0 as OLS.\nThe agreement between the transformer-based ICL and these predictors is shown in Fig. 1. As can be seen, there are clear differences in fit to predictors: for almost any number of examples, normalized SPD and ILWD are small between the transformer and OLS predictor (with squared error less than 0.01), while other predictors (especially nearest neighbors) agree considerably less well.\n<div style=\"text-align: center;\">(b) Predictor\u2013ICL fit w.r.t implicit weights.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/861b/861b113c-0865-4ed0-a336-812d043d3b38.png\" style=\"width: 50%;\"></div>\nWhen the number of examples is less than the input dimension d = 8, the linear regression problem is under-determined, in the sense that multiple linear models can exactly fit the in-context training dataset. In these cases, OLS regression selects the minimum-norm weight vector, and (as shown in Fig. 1), the in-context learner\u2019s predictions are reliably consistent with this minimum-norm predictor. Why, when presented with an ambiguous dataset, should ICL behave like this particular predictor? One possibility is that, because the weights used to generate the training data are sampled from a Gaussian centered at zero, ICL learns to output the minimum Bayes risk solution when predicting under uncertainty (see M\u00a8uller et al. (2021)). Building on these initial findings, our next set of experiments investigates whether ICL is behaviorally equivalent to Bayesian inference more generally. ICL matches the minimum Bayes risk predictor on noisy datasets. To more closely examine the behavior of ICL algorithms under uncertainty, we add noise to the training data: now we present the in-context dataset as a sequence: [x1, f(x1) + \u03f51, . . . , xn, f(xn) + \u03f5n] where each \u03f5i \u223cN(0, \u03c32). Recall that ground-truth weight vectors are themselves sampled from a Gaussian distribution; together, this choice of prior and noise mean that the learner cannot be certain about the target function with any number of examples.\nThis is because, conditioned on x and D, the scalar \u02c6y(x, D) := E[y|x, D] is the minimizer of the loss E[(y \u2212\u02c6y)2|x, D], and thus the estimator \u02c6y is the minimzier of E[(y \u2212\u02c6y)2] = Ex,D[E[(y \u2212\u02c6y)2|x, D]]. For linear regression with Gaussian priors and Gaussian noise, the Bayesian estimator in Eq. (18) has a closed-form expression:\nNote that this predictor has the same form as the ridge predictor from Section 2.3, with the regularization parameter set to \u03c32 \u03c4 2 . In the presence of noisy labels, does ICL match this Bayesian predictor? We explore this by varying both the dataset noise \u03c32 and the prior variance \u03c4 2 (sampling w \u223cN(0, \u03c4 2)). For these experiments, the SPD values between the in-context learner and various regularized linear models is shown in Fig. 2. As predicted, as variance increases, the value of the ridge parameter that best explains ICL behavior also increases. For all values of \u03c32, \u03c4 2, the ridge parameter that gives the best fit to the transformer behavior is also the one that minimizes Bayes risk. These experiments clarify the finding above, showing that ICL in this setting behaviorally matches minimum-Bayes-risk predictor. We also note that when the noise level \u03c3 \u21920+, the Bayes predictor converges to the ordinary least square predictor. Therefore, the results on noiseless datasets studied in the beginning paragraph of this subsection can be viewed as corroborating the finding here in the setting with \u03c3 \u21920+.\nICL exhibits algorithmic phase transitions as model depth increases. The two experiments above evaluated extremely high-capacity models in which (given findings in Section 3) computational constraints are not likely to play a role in the choice of algorithm implemented by ICL. But\n(18)\n(19)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8cf1/8cf1ecdc-df42-4fd2-9a99-809d1aafa0a7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6252/62528a53-4fb9-49b9-beb9-c0f994bf9526.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Linear regression problem with d = 16</div>\nFigure 3: Computational constraints on ICL: We show SPD averaged over underdetermined region of the linear regression problem. In-context learners behaviorally match ordinary least squares predictors if there is enough number of layers and hidden sizes. When varying model depth (left background), algorithmic \u201cphases\u201d emerge: models transition between being closer to gradient descent, (red background), ridge regression (green background), and OLS regression (blue).\nwhat about smaller models\u2014does the size of an in-context learner play a role in determining the learning algorithm it implements? To answer this question, we run two final behavioral experiments: one in which we vary the hidden size (while optimizing the depth and number of heads as in Section 4.2), then vary the depth of the transformer (while optimizing the hidden size and number of heads). These experiments are conducted without dataset noise. Results are shown in Fig. 3. When we vary the depth, learners occupy three distinct regimes: very shallow models (1L) are best approximated by a single step of gradient descent (though not wellapproximated in an absolute sense). Slightly deeper models (2L-4L) are best approximated by ridge regression, while the deepest (+8L) models match OLS as observed in Fig. 3. Similar phase shift occurs when we vary hidden size in a 16D problem. Interestingly, we can read hidden size requirements to be close to ridge-regression-like solutions as H \u226516 and H \u226532 for 8D and 16D problems respectively, suggesting that ICL discovers more efficient ways to use available hidden state than our theoretical constructions requiring O(d2). Together, these results show that ICL does not necessarily involve minimum-risk prediction. However, even in models too computationally constrained to perform Bayesian inference, alternative interpretable computations can emerge.\n# 5 DOES ICL ENCODE MEANINGFUL INTERMEDIATE QUANTITIES?\nSection 4 showed that transformers are a good fit to standard learning algorithms (including those constructed in Section 3) at the computational level. But these experiments leave open the question of how these computations are implemented at the algorithmic level. How do transformers arrive at the solutions in Section 4, and what quantities do they compute along the way? Research on extracting precise algorithmic descriptions of learned models is still in its infancy (Cammarata et al., 2020; Mu & Andreas, 2020). However, we can gain insight into ICL by inspecting learners\u2019 intermediate states: asking what information is encoded in these states, and where. To do so, we identify two intermediate quantities that we expect to be computed by gradient descent and ridge-regression variants: the moment vector X\u22a4Y and the (min-norm) least-square estimated weight vector wOLS, each calculated after feeding n exemplars. We take a trained in-context learner, freeze its weights, then train an auxiliary probing model (Alain & Bengio, 2016) to attempt to\nSection 4 showed that transformers are a good fit to standard learning algorithms (including those constructed in Section 3) at the computational level. But these experiments leave open the question of how these computations are implemented at the algorithmic level. How do transformers arrive at the solutions in Section 4, and what quantities do they compute along the way? Research on extracting precise algorithmic descriptions of learned models is still in its infancy (Cammarata et al., 2020; Mu & Andreas, 2020). However, we can gain insight into ICL by inspecting learners\u2019 intermediate states: asking what information is encoded in these states, and where.\nTo do so, we identify two intermediate quantities that we expect to be computed by gradient descent and ridge-regression variants: the moment vector X\u22a4Y and the (min-norm) least-square estimated weight vector wOLS, each calculated after feeding n exemplars. We take a trained in-context learner, freeze its weights, then train an auxiliary probing model (Alain & Bengio, 2016) to attempt to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c31f/c31fe427-c305-4c14-8c15-1bbe6347815b.png\" style=\"width: 50%;\"></div>\nFigure 4: Probing results on d = 4 problem: Both moments X\u22a4Y (top) and least-square solution wOLS (middle) are recoverable from learner representations. Plots in the left column show the accuracy of the probe for each target in different model layers. Dashed lines show the best probe accuracies obtained on a control task featuring a fixed weight vector w = 1. Plots in the right column show the attention heatmap for the best layer\u2019s probe, with the number of input examples on the x-axis. The value of the target after n exemplars is decoded primarily from the representation of yn, or, after n = d examplars, uniformly from yn\u22654.\nrecover the target quantities from the learner\u2019s hidden representations. Specifically, the probe model takes hidden states at a layer H(l) as input, then outputs the prediction for target variable. We define a probe with position-attention that computes (Appendix E):\nWe train this probe to minimize the squared error between the predictions and targets v: L(v, \u02c6v) = |v \u2212\u02c6v|2. The probe performs two functions simultaneously: its prediction error on held-out representations determines the extent to which the target quantity is encoded, while its attention mask, \u03b1 identifies the location in which the target quantity is encoded. For the FF term, we can insert the function approximator of our choosing; by changing this term we can determine the manner in which the target quantity is encoded\u2014e.g. if FF is a linear model and the probe achieves low error, then we may infer that the target is encoded linearly. For each target, we train a separate probe for the value of the target on each prefix of the dataset: i.e. one probe to decode the value of w computed from a single training example, a second probe to decode the value for two examples, etc. Results are shown in Fig. 4. For both targets, a 2layer MLP probe outperforms a linear probe, meaning that these targets are encoded nonlinearly (unlike the constructions in Section 3). However, probing also reveals similarities. Both targets are decoded accurately deep in the network (but inaccurately in the input layer, indicating that probe success is non-trivial.) Probes attend to the correct timestamps when decoding them. As in both constructions, X\u22a4Y appears to be computed first, becoming predictable by the probe relatively early in the computation (layer 7); while w becomes predictable later (around layer 12). For comparison, we additionally report results on a control task in which the transformer predicts ys generated with a fixed weight vector w = 1 (so no ICL is required). Probes applied to these models perform significantly worse at recovering moment matrices (see Appendix E for details).\n# 6 CONCLUSION\nWe have presented a set of experiments characterizing the computations underlying in-context learning of linear functions in transformer sequence models. We showed that these models are capable in theory of implementing multiple linear regression algorithms, that they empirically implement this range of algorithms (transitioning between algorithms depending on model capacity and dataset noise), and finally that they can be probed for intermediate quantities computed by these algorithms. While our experiments have focused on the linear case, they can be extended to many learning problems over richer function classes\u2014e.g. to a network whose initial layers perform a non-linear\n<div style=\"text-align: center;\">0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 position</div>\n(20) (21)\nfeature computation. Even more generally, the experimental methodology here could be applied to larger-scale examples of ICL, especially language models, to determine whether their behaviors are also described by interpretable learning algorithms. While much work remains to be done, our results offer initial evidence that the apparently mysterious phenomenon of in-context learning can be understood with the standard ML toolkit, and that the solutions to learning problems discovered by machine learning researchers may be discovered by gradient descent as well.\nWe thank Evan Hernandez, Andrew Drozdov, Ed Chi for their feedback on the early drafts of this paper. At MIT, Ekin Aky\u00a8urek is supported by an MIT-Amazon ScienceHub fellowship and by the MIT-IBM Watson AI Lab.\n# REFERENCES\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nStephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent fewshot learning in transformers. ArXiv preprint, abs/2205.05055, 2022. URL https://arxiv.org/ abs/2205.05055.\nChi Chen, Maosong Sun, and Yang Liu. Mask-align: Self-supervised neural word alignment. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4781\u20134791, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.369. URL https://aclanthology.org/2021.acl-long.369.\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 719\u2013730, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.53. URL https://aclanthology.org/2022.acl-long.53.\n<div style=\"text-align: center;\">Samuel M\u00a8uller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021.</div>\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, T. J. Henighan, Benjamin Mann, Amanda Askell, Yushi Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, John Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom B. Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Christopher Olah. In-context learning and induction heads. 2022.\nJuergen Schmidhuber, Jieyu Zhao, and Marco A Wiering. Simple principles of metalearning. 1996. Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124\u2013127, 1950. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998\u20136008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. ArXiv preprint, abs/2107.13163, 2021. URL https://arxiv.org/abs/2107.13163. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. ArXiv preprint, abs/2111.02080, 2021. URL https: //arxiv.org/abs/2111.02080. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. ArXiv, abs/2111.02080, 2022. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=ByxRM0Ntvr. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.\n# A THEOREM 1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/978a/978a733f-cab7-491c-a319-69d3ead095a0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/43b4/43b42cd4-e21a-46ec-9fc4-68432f3500b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">We can verify the chain of operator step-by-step. In each step, we show only the non-zero rows.</div>\n\u2022 mov(; 1, 0, (1, 1 + d), (1, 1 + d))\n\u2022 aff(; (1, 1 + d), (), (1 + d, 2 + d), W1 = w)\n\uf8f0 \uf8fb \u2022 aff(; (1 + d, 2 + d), (0, 1), (2 + d, 3 + d), W1 = I, W2 = \u2212I)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5029/5029382b-4b92-42d7-8fe3-57914d9250a1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5cfe/5cfe6ca6-329d-4e68-b5a4-a26d20ffc720.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\u2022 aff(; (3 + 2d, 3 + 3d), (3 + 3d, 3 + 4d), (3 + 2d, 3 + 3d), W1 = I, W2 = \u22122\u03b1, )</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/595a/595ae10b-1d1d-4fb8-8ee1-a97fa48184b7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/27a2/27a2a3d9-5c9a-4ae7-a2d1-a5823d9e4005.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\u2022 mov(; 2, 1, (3 + 2d, 3 + 3d), (3 + 2d, 3 + 3d))</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e7a7/e7a72172-f228-4b7e-b174-d7e39137bae4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\u2022 mul(; 1, d, 1, (3 + 2d, 3 + 3d), (1, 1 + d), (3 + 3d, 4 + 3d))</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d39c/d39c39b0-0d23-43dc-a65f-0ad19b33ef01.png\" style=\"width: 50%;\"></div>\nGeneralizing to multiple steps of SGD. Since w\u2032 is written in the hidden states, we may repeat this iteration to obtain \u02c6y3 = w\u2032\u2032\u22a4x3 where w\u2032\u2032 is the one step update w\u2032 \u22122\u03b1(x2w\u2032\u22a4x2 \u2212y2x2 + \u03bbw, requiring a total of O(n) layers for a single pass through the dataset where n is the number of examplers. As an empirical demonstration of this procedure, the accompanying code release contains a reference implementation of SGD defined in terms of the base primitive provided in an anymous links https://icl1.s3.us-east-2.amazonaws.com/theory/{primitives,sgd,ridge}.py (to preserve the anonymity we did not provide the library dependencies). This implementation predicts \u02c6yn = w\u22a4 n xn, where wn is the weight vector resulting from n \u22121 consecutive SGD updates on previous examples. It can be verified there that the procedure requires O(n + d) hidden space. Note that, it is not O(nd) because we can reuse spaces for the next iteration for the intermediate variables, an example of this performed in (w\u2032) step above highlighted with blue color.\n# B THEOREM 2\nWe provide a similar construction to Theorem 1 (please see proofs for the Transformer implementation of these operations in Appendix C, specifically for div see Appendix C.6) \u2022 mov(; 1, 0, (1, 1 + d), (1, 1 + d)) (move x1) \u2022 mul(; d, 1, 1, (1, 1 + d), (0, 1), (1 + d, 1 + 2d)) (x1y) \u2022 aff(; (), (), (1 + 2d, 1 + 2d + d2), b = I \u03bb) (A\u22121 0 = I \u03bb) \u2022 mul(; d, d, 1, (1 + 2d, 1 + 2d + d2), (1, 1 + d), (1 + 2d + d2, 1 + 3d + d2)) (A\u22121 0 u = I \u03bbx1) \u2022 mul(; 1, d, d, (1, 1 + d), (1 + 2d, 1 + 2d + d2), (1 + 3d + d2, 1 + 4d + d2)) (vA\u22121 0 = x\u22a4 1 I \u03bb) \u2022 mul(; d, 1, d, (1 + 2d + d2, 1 + 3d + d2), (1 + 3d + d2, 1 + 4d + d2), (1 + 4d + d2, 1 + 4d + 2d2)) (A\u22121 0 uvA\u22121 0 = I \u03bbx1x\u22a4 1 I \u03bb) \u2022 mul(; 1, d, 1, (1+3d+d2, 1+4d+d2), (1, 1+d), (1+4d+2d2, 2+4d+2d2)) (v\u22a4A\u22121 0 u = x\u22a4 1 I \u03bbx1) \u2022 aff(; (1+4d+2d2, 2+4d+2d2), (), (1+4d+2d2, 2+4d+2d2), W1 = 1, b = 1, ) (1+v\u22a4A\u22121 0 u = 1 + x\u22a4 1 I \u03bbx1) \u2022 div(; (1 + 4d + d2, 1 + 4d + 2d2), 1 + 4d + 2d2, (2 + 4d + 2d2, 2 + 4d + 3d2)) (right term) \u2022 aff(; (1+2d, 1+2d+d2), (2+4d+2d2, 2+4d+3d2), (1+2d, 1+2d+d2), W1 = I, W2 = \u2212I) (A\u22121 1 ) \u2022 mul(; d, d, 1, (1 + 2d, 1 + 2d + d2), (1, 1 + d), (2 + 4d + 3d2, 2 + 5d + 3d2)) (A\u22121 1 x1) \u2022 mul(; d, 1, 1, (2 + 4d + 3d2, 2 + 5d + 3d2), (0, 1), (2 + 4d + 3d2, 2 + 5d + 3d2)) (A\u22121 1 x1y1)\n# C LEMMA 1\n1. Motivation of the base primitive RAW. 2. Formal definition of RAW. 3. Definition of dot, aff, mov in terms of RAW. 4. Implementation of RAW in terms of transformer parameters. 5. Brief discussion of how to parallelize RAW, making it possible to implement mul. 6. Seperate proof for div by utilizing layer norm.\n# C.1 RAW OPERATOR: INTUITION\n1) Operators read some hidden units from the current or previous timestep: dot and aff read from two subsets of indices in the current hidden state ht2, while mov reads from a previous hidden state ht\u2032. This selection is straightforwardly implemented using the attention component of a transformer layer. We may notate this reading operation as follows:\n\ufffd \ufffd\ufffd \ufffd Here r denotes a list of indice to read from, and K denotes a map from current timesteps to targe timesteps. For convenience, we use Numpy-like notation to denote indexing into a vector wit another vector: Definition C.1 (Bracket). x[.] is Python index notation where the resulting vector, x\u2032 = x[r]:\nThe first step of our proof below shows that the attention output a(l) can compute the expressio above.\n2) Operators perform element-wise arithmetic between the quantity read in step 1 and anothe set of entries from the current timestep: This step takes different forms for aff and mul (mo ignores values at the current timestep altogether).\n(22)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/50c2/50c2576b-8e3a-4b97-bd21-d030e5208ac4.png\" style=\"width: 50%;\"></div>\n\ufffd \ufffd\ufffd \ufffd The second step of the proof below computes these operations inside the MLP component of the transformer layer.\n3) Operators reduce, then write to the current hidden state Once the underlying element-wise operation calculated, the operator needs to write these values to the some indices in current hidden state, defined by a list of indices w. Writing might be preceded by a reduction state (e.g. for computing dot products), which can be expressed generically as a linear operator Wo. The final form of the computation is thus:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/32cd/32cdd407-da6b-47c0-8ce6-383b1f4968f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd Here, \u2190means that the other indices i /\u2208w are copied from hl\u22121.</div>\n# C.2 RAW OPERATOR DEFINITION\nWe denote this \u201cmaster operator\u201d as RAW: Definition C.2. RAW(h; \u20dd\u22c6, s, r, w, Wo, Wa, W, K) is a function RH\u00d7T \ufffd\u2192RH\u00d7T . It is parameterized by an elementwise operator \u20dd\u22c6\u2208{+, \u2299}, three matrices W \u2208Rd\u00d7|s|, Wa \u2208Rd\u00d7|r|, Wo \u2208 R|w|\u00d7d, three index sets s, r, and w, and a timestep map K : Z+ \ufffd\u2192(Z+\u2217). Given an input matrix h, it outputs a matrix with entries:\nWe additionally require that j \u2208K(i) =\u21d2j < i (since self-attention is causal.)\n(For simplicity, we did not include a possible bias term in linear projections Wo, Wa, W, we can always assume the accompanying bias parameters b0, ba, b when needed)\nC.3 REDUCING LEMMA 1 OPERATORS TO RAW OPERATOR\nGiven this operator, we can define each primitive in Lemma 1 using a single RAW operator, except the mul and div. Instead of the matrix multiplication operator mul, we will first show the dot product dot (a special case of mul), then later in the proof, we will argue that we can parallelize these dot products in Appendix C.5 to obtain mul. We will show how to implement div separately in Appendix C.6.\n(23)\n(24)\n(25)\n(27)\n# Proof. Follows immediately by substituting parameters into Eq. (26\nIt remains only to show: Lemma 3. A single transformer layer can implement the RAW operator: there exist settings of transformer parameters such that, given an arbitrary hidden matrix h as input, the transformer computes h\u2032 (Eq. (26)) as output.\nOur proof proceeds in stages. We begin by providing specifying initial embedding and positiona embedding layers, constructing inputs to the main transformer layer with necessary positional in formation and scratch space. Next, we prove three useful procedures for bypassing (or exploiting non-linearities in the feed-forward component of the transformer. Finally, we provide values for re maining parameters, showing that we can implement the Elementwise and Reduction steps described above.\nC.4.1 EMBEDDING LAYERS\nEmbedding Layer for Initialization: Rather than inserting the input matrix h directly into the transformer layer, we assume (as is standard) the existence of a linear embedding layer. We can set this layer to pad the input, providing extra scratch space that will be used by later steps of our implementation. We define the embedding matrix W as:\nWe define the embedding matrix We as:\nThen, the embedded inputs will be\nPosition Embeddings for Attention Manipulation: Implementing RAW ultimately requires controlling which position attends to which position in each layer. For example, we may wish to have layers in which each position attends to the previous position only, or in which even positions attends to other even positions. We can utilize position embeddings, pi, to control attention weights. In a standard transformer, the position embedding matrix is a constant matrix that is added to the inputs of the transformer after embedding layer (before the first layer), so the actual input to to the transformer is:\nWe will use these position embeddings to encode the timestep map K. To do this, we will use 2p units per layer (p will be defined momentarily). p units will be used to encode attention keys ki, and the other p will be used to encode queries qi. We define the position embedding matrix as follows:\n(28)\n(29) (30)\n(32)\nWith K encoded in positional embeddings, the transformer matrices WQ and WK are easy to define: they just need to retrieve the corresponding embedding values:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b57/2b57faf9-e3f9-4442-947d-8393ce5ed4f3.png\" style=\"width: 50%;\"></div>\n 1: Attend to previous token. This can be constructed by setting:\niently large number. In this case, the output of the attention mechanism w \ufffd \ufffd\n\ufffd\ufffd\ufffd\ufffd end to a single token. For simpler patterns, such as attention to a speci\nonly 1 hidden unit is required. We set:\nfrom which it can be verified (using the same procedure as in Case 1) that the desired attenti pattern is produced.\nIntricacy: How can K be empty? We can also cause K(i) to attend to an empty set by assuming the softmax has extra (\u201cimaginary\u201d) timestep obtained by prepending a 0 to attention vector pot-hoc (Chen et al., 2021).\nIntricacy: How can K be empty? We can also cause K(i) to attend to an empty set by assuming the softmax has extra (\u201cimaginary\u201d) timestep obtained by prepending a 0 to attention vector pot-hoc (Chen et al., 2021). Cumulatively, the parameter matrices defined in this subsection implement the Read with Attention component of the RAW operator.\nC.4.2 HANDLING & UTILIZING NONLINEARITIES\nThe mul operator requires elementwise multiplication of quantities stored in hidden states. While transformers are often thought of as only straightforwardly implementing affine transformations on hidden vectors, their nonlinearities in fact allow elementwise multiplication to a high degree of approximation. We begin by observing the following property of the GeLU activation function in the MLP layers of the Transformer network: Lemma 4. The GeLU nonlinearity can be used to perform multiplication: specifically, \ufffd\u03c0 2 (GeLU(x + y) \u2212GeLU(x) \u2212GeLU(y)) = xy + O(x3 + y3) (35)\n(33)\nA standard implementation of the GeLU nonlinearity is defined as follow \ufffd \ufffd \ufffd\ufffd\nThus\n\ufffd For small x and y, the third-order term vanishes. By scaling inputs down by a constant before the GeLU layer, and scaling them up afterwards, models may use the GeLU operator to perform elementwise multiplication.\n\ufffd For small x and y, the third-order term vanishes. By scaling inputs down by a constant before the GeLU layer, and scaling them up afterwards, models may use the GeLU operator to perform elementwise multiplication. We can generalize this proof to other smooth functions as we discussed further in [TODO REF]. Previous work also shows, in practice, Transformers with ReLU activation utilize non-linearities to get the multiplication in other settings. When implementing the aff operator, we have the opposite problem: we would like the output of addition to be transmitted without nonlinearities to the output of the transformer layer. Fortunately, for large inputs, the GeLU nonlinearity is very close to linear; to bypass it it suffices to add to inputs a large N: Lemma 5. The GeLU nonlinearity can be bypassed: specifically, GeLU(N + x) \u2212N \u2248x N \u226b1 (40)\nProof.\nFor all verions of the RAW operator, it is additionally necessary to bypass the LayerNorm operation The following formula will be helpful for this: Lemma 6. Let N be a large number and \u03bb the LayerNorm function. Then the following approxi mation holds:\ufffd\nProof.\nThen,\n(37)\n(38)\n(39)\n(40)\n(41)\n(44)\n(45)\nBy adding a large number N to two padding locations and sum the part of the hidden state that we are interested to pass through LayerNorm, we make x to the output of LayerNorm pass through. This addition can be done in the transformer\u2019s feed-forward computation (with parameter W F ) prior to layer norm. This multiplication of \ufffd 2 LN can be done in first layer of MLP back, then linear layer can output/use x. For convenience, we will henceforth omit the LayerNorm operation when it is not needed. We may make each of these operations as precise as desired (or allowed by system precision). With them defined, we are ready to specify the final components of the RAW operator.\nC.4.3 PARAMETERIZING RAW\nWe want to show a layer of Transformer defined in above, hence parameterized by \u03b8 = {Wf, W1, W2, (W Q, W K, W v)m}, can well-approximate the RAW operator defined in Eq. (25). We will provide step by step constructions and define the parameters in \u03b8. Begin by recalling the transformer layer definition:\nAttention Output We will only use m = 2 attention heads for this construction. We show in Eq. (32) that we can control attentions to uniformly attend with a pattern by setting key and query matrices. Assume that the first head parameters W Q 1 , W K 1 have been set in the described way to obtain the pattern function K. Now we will set remaining attention parameters W V 1 , W Q 2 , W K 2 , W V 2 and show hat we can make the ai + h(l) i term in Eq. (4) to contain the corresponding term in Eq. (25), in some unused indices t such that: \uf8eb \uf8f6\nThen the term on the RAW operator can be obtained by the first head\u2019s output. In order to achieve that, we will set Wa as a part of actual attention value network such that W V 1 is sparse matrix 0 everywhere expect:\nNow our first heads stores the right term in Eq. (53) in the indicies t. However, when we add the residual term h(l) i , this will change. To remove the residual term, we will use another head to output h(l) i , by setting W Q 2 , W K 2 such that K(i) = i, and W V 2 (similar to Eq. (34)):\nThen, W f \u2208RH\u00d72H is zero otherwise:\nWe already defined (W Q, W K, W V )1,2 and W f and obtained the first term in the Eq. (25) in (ai + h(l) i )t\u2032\u2208t.\n(53)\n(54)\n(56)\n(57) (58) (59)\nArithmetic term Now we want to calculate the term inside the parenthesis Eq. (25). We will calculate it through the MLP layer and store in mi and substract the first term. Let\u2019s denote the input to the MLP as xi = (ai + h(l) i ), the output of the first layer ui, the output of the non-linearity as ai, and the final output as mi. The entries of mi will be: \uf8eb \uf8eb \uf8f6 \uf8f6\nWe will define the MLP layer to operate the attention term calculated above with a part of the current hidden state by defining W1 and W2. Let\u2019s assume we bypass the LayerNorm by using Lemma 6. Let\u2019s show this seperately for + and \u2299operators.\nRAW(+, .) If the operator, \u20dd\u22c6= +, first layer of the MLP will calculate the second term in Eq. (25) and overwrite the space where the attention output term Eq. (53) is written, and add a large positive bias term N to by pass GeLU as explained in Lemma 4. We will use an available space \u02c6t in the xi same size as t.\n\u2208\u222a\u222a \u2212 This can be done by setting W1 (weight term of the first layer of the MLP) to zero indices:\nNote the second term is added to make unused indices t \u222aw \u222a\u02c6t become zero after the gelu, which outputs zero for large negative values. Since we added a large positive term, we make sure gelu behaved like a linear layer. Thus we have,\n\u2208\u222a\u222a Now, we need to set W2, to simulate Wo \u2208R|w|\u00d7|t|,\n \u2208 (W2)w[m],\u02c6t[n] = (Wo)m,n m \u22081, ..., |w|, n \u22081, ..., |t| (W2)t[m],t[m] = +1 m \u22081, . . . , |t| (W2)w[m],w[m] = +1 m \u22081, . . . , |w|\n(60)\n(61) (62)\n(63) (64) (65) (66)\nTherefore, mi[w] = Woxi[t] + W0Whl i[s] \u2212xi[w] equals to what we promised in Eq. (60) for + case. If we sum this with the residual xi term back Eq. (53), so the output of this layer can be written as: \uf8eb \uf8eb \uf8f6 \uf8f6\nRAW(\u2299, .) If the operator, \u20dd\u22c6= \u2299, we need to use three extra hidden units the same size as |t|, let\u2019s name the extra indices as ta, tb, tc, and output w space. The (ui) will get below entries to be able to use [], where N is a large number:\nAll of this operations are linear, can be done W1 zero except the below entries:\n(W1)ta[m],s[n] = (W)m,n/N m \u22081, ..., |ta|, n \u22081, ..., |s (W1)ta[m],t[n] = 1/N m \u22081, ..., |ta|, n \u22081, ..., |t| (W1)tb[m],t[m] = 1/N m \u22081, ..., |tb|, n \u22081, ..., |t| (W1)tc[m],s[m] = 1/N m \u22081, ..., |tc|, n \u22081, ..., |s| (W1)w[m],w[m] = \u22121 m \u22081, ..., |w| (W1)t[i],t[m] = \u22121 m \u22081, ..., |t|\nand b1 to:\n(b1)t\u2032\u2208(t\u222ata\u222atb\u222atc) = 0 (b1)t\u2032\u2208(t\u222aw) = N (b1)t\u2032 /\u2208(t\u222ata\u222atb\u222atc\u222aw) = \u2212N\nThe resulting v with the approximations become:\n(86)\n(87) (88) (89)\n(90)\n(91)\nWe then set b2:\n\u2208 \u2208 We have used 4|t| space for internal computation of this operation, and finally used |w| space to write the final result. We show RAW operator is implementable by setting the parameters of a Transformer.\n# C.5 PARALLELIZING THE RAW OPERATOR\nLemma 7. With the conditions that K is constant, the operators are independent (i.e (ri \u222asi \u222a wi) \u2229wj\u0338=i = \u2205), and there is \ufffd k(4|tk| + |wk|) available space in the hidden state, then a Transformer layer can apply k such RAW operation in parallel by setting different regions of W1, W2, Wf and (W V )k matrices.\nProof. From the construction above, it is straightforward to modify the definition of the RAW operator to perform k operations as all the indices of matrices that we use in Appendix C.4.3 do not overlap with the given conditions in the lemma.\nThis makes it possible to construct a Transformer layer not only to implement vector-vector dot products, but general matrix-matrix products, as required by mul. With this, we show that we can implement mul by using single layer of a Transformer.\n# C.6 LAYERNORM FOR DIVISION\nLet say we have the input [c, y, 0]\u22a4calculated before the attention output in Eq. (53), and we want to divide y to c. This trick is very similar to the on in Lemma 6. We can use the following formula: Lemma 8. using LayerNorm for division. Let N, M to be large numbers, \u03bb LayerNorm function, the following approximation holds: \ufffd\nProof.\n(126)\n(127)\n(128)\n(129)\n(131)\nThen, \ufffd\nTo get the input to the format used in this Lemma, we can easily use Wf to convert the head outputs. Then, after the layer norm, we can use W1 to pull the y c back and write it to the attention output. By this way, we can approximate scalar division in one layer.\nLemma 1 By Lemmas 2, 3, 3, 7 and 8; we constructed the operators in Lemma 1 using single layer of a Transformer, thus proved Lemma 1\n# D DETAILS OF TRANSFORMER ARHITECTURE AND TRAINING\nWe perform these experiments using the Jax framework on P100 GPUs. The major hyperparameters used in these experiments are presented in Table 1. The code repository used for reproducing these experiments will be open sourced at the time of publication. Most of the hyperparameters adapted from previous work Garg et al. (2022) to be compatible, and we adapted the Transformer architecture details. We use Adam optimizer with cosine learning rate scheduler with warmup where number of warmup steps set to be 1/5 of total iterations. We use larned absolute position embeddings.\nParameter\nSearch Range\nNumber of heads\n1, 2, 4, 8 s\nNumber of layers\n1, 2, 12, 16\nHidden size\n16, 32, 64, 256, 512, 1024\nBatch size\n64\nMaximum number of epochs\n500.000\nInitial Learning rate (lri)\n1e-4, 2.5e-4\nWeight decay\n0, 1e-5\nBias initialization\nuniform scaling, normal(1.0)\nWeight initialization\nuniform scaling, normal(1.0)\nPosition embedding initialization\nuniform scaling, normal(1.0)\nTable 1: Hyperparameters used in the ICL. The best parameter for each hyperparameter is highlighted. In the phase shift plots in Fig. 3, we keep the value in the x-axis constant and used the best setting over the parameters: {number of layers, hidden size, number of heads and learning rate}.\n# E DETAILS OF PROBE\nWe will use the terms probe model and task model to distinguish probe from ICL. Our probe is defined as:\nThe position scores sv \u2208RT are learned parameters where T is the max input sequence length (T = 80 in our experiments). The softmax of position scores attention weights \u03b1 for each position and for each target variable. This enables us to learn input-independent, optimal target locations for each target (displayed on the right side of Fig. 4). We then average hidden states by using these attention weights. A linear projection, Wv \u2208RT \u00d7H\u2032, is applied before averaging. FF is either a linear layer or a 2-layer MLP (hidden size=512) with a GeLU activation function. For each layer, we train a different probe with different parameters using stochastic gradient descent. H\u2032 equals to the 512. The probe is trained using an Adam optimizer with a learning rate of 0.001 (chosen from among {0.01, 0.001, 0.0001} on validation data).\n\n(132) (133)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/61d8/61d875d1-d52d-4a58-85a2-644fc9090777.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Detailed error values of the control probe displayed in Fig. 4</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d005/d0052d90-31c7-4715-912a-c672284eb6d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: R2 of linear weight estimation on d = 8 problem</div>\nControl Experiments In Fig. 4, dashed lines show probing results with a task model trained on a control task, in which w is always the all-ones 1. This problem structurally resembles our main experiment setup, but does not require in-context learning. During probing, we feed this model data generated by w sampled form normal distribution as in the original task model. We observe that the control probe has a significantly higher error rate, showing that the probing accuracy obtained with actual task model is non-trivial. We present detailed error values of the control probe in Fig. 5.\n# F LINEARITY OF ICL\nIn Fig. 1b, we compare implicit linear weight of the ICL against the linear algorithms using ILWD measure. Note that this measure do not assume predictors to be linear: when the predictors are not linear, ILWD measures the difference between closest linear predictors (in Eq. (16) sense) to each algorithm. To gain more insight to ICL\u2019s algorithm, we can measure how linear ICL in different regimes of the linear problem (underdetermined, determined) by using R2 (coefficient of determination) measure. So, instead of asking what\u2019s the best linear fit in Eq. (16), we can ask how good is the linear fit, which is the R2 of the estimator. Interestingly, even though our model matches min-norm least square solution in both metrics in Section 4.3, we show that ICL is becoming gradually linear in the under-determined regime Fig. 6. This is an important result, enables us to say the in-context learner\u2019s hypothesis class is not purely linear.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/57a4/57a482f7-e5d1-4b04-b839-8e277f0c055d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) A piece-wise linear approximation to x2 by using ReLU, Eq. (141).</div>\nWe can show that for a real-valued and smooth non-linearity f(x), we can apply the same trick in in the paper body. In particular, we can write Taylor expansion as:\n\ufffd which converges for some sufficiently small neighborhood: X \u2208[\u2212\u03f5, \u03f5]. First, assume that the second order term a2 dominates higher-order terms in this domain such that:\n.\nSo, given the expansion for GeLU in Eq. (37), we can use this generic formula to obtain the mu plication approximation:\nWe plot this approximation against x2 for [0.1, \u22120.1] range in Fig. 7a. In the case of a2 is zero, we cannot get any second order term, and in the case of a2 is negligible O(x3 + y3) will dominate the Eq. (135), so we cannot obtain a good approximation of xy. In this case, we can resort to numerical derivatives and utilize the a3 term: f \u2032(x) = a1 + 2a2x + 3a3x3 + . . . (137)\n(134)\n(135)\n(136)\n(137)\nSimilar to our construction in Eq. (110), we can construct a Transformer layer that calculates these quantities (noting that \u03b4 is a small, input-independent scalar). We plot this approximation against x2 for [0.1, \u22120.1] range in Fig. 7b. Note that, if we use this approximation in our constructions we will need more hidden space as there are 6 different tanh term as opposed to 3 GeLU term in Eq. (110). Non-smooth non-linearities ReLU is another commonly used non-linearity that is not differentiable. With ReLU, we can only hope to get piece-wise linear approximations. For example, we can try to approximate x2 with the following function: 0.0375 \u2217ReLU(x) + 0.0375 \u2217ReLU(\u2212x) + ReLU(0.05 \u2217(x \u22120.05))+ ReLU(\u22120.05 \u2217(x + 0.05)) + ReLU(0.025 \u2217(x \u22120.025)) + ReLU(\u22120.025 \u2217(x + 0.025)) \u2248x2 (141) We plot this approximation against x2 for [0.1, \u22120.1] range in Fig. 7c.\nWe plot this approximation against x2 for [0.1, \u22120.1] range in Fig. 7c.\nEMPIRICAL SCALING ANALYSIS WITH DIMENSIONALIT\nIn Figs. 3a and 3b, we showed that ICL needs different hidden sizes to enter the \u201cRidge regression phase\u201d (orange background) or \u201cOLS phase\u201d (green background) depending on the dimensionality d of inputs x. However, we cannot reliably read the actual relations between size requirements and the dimension of the problem from only two dimensions. To better understand size requirements, we ask the following empirical question for each dimension: how many layer/hidden size/heads are needed to better fit the least-squares solution than the Ridge(\u03bb = \u03f5) regression solution (the green phase in Figs. 3a and 3b)? To answer this important question, we experimented with d = {1, 2, 4, 8, 12, 16, 20} and run an experiment sweep for each dimension over:\n\u2022 number of layers (L): {1, 2, 4, 8, 12, 16}, \u2022 hidden size (H): {16, 32, 64, 256, 512, 1024}, \u2022 number of heads (M): {1, 2, 4, 8}, \u2022 learning rate: {1e-4, 2.5e-4}.\nFor each feature that affects computational capacity of transformer (L, H, M), we optimize other features and find the minimum value for the feature that satisfies SPD(OLS, ICL) < SPD(Ridge(\u03bb = \u03f5), ICL). We plot our experiment with \u03f5 = 0.1 in Appendix H. We find that single head is enough for all problem dimensions, while other parameters exhibit a step-functionlike dependence on input size. Please note that other hyperparameters discussed in Appendix D (e.g weight initialization) were not optimized for each dimension independently.\n(138)\n(139)\n(140)\n \u2248 (141)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a84f/a84f361f-9453-4b8f-aaeb-0904bb46c111.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) number of head requirements.</div>\nFigure 8: Empirical requirements on model parameters to satisfy SPD(Ridge(\u03bb = 0.1), ICL) > SPD(OLS, ICL) when other parameters optimized.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the remarkable capacity of neural sequence models, particularly transformers, for in-context learning (ICL), which allows them to construct new predictors from sequences of labeled examples without further parameter updates. Previous methods lacked clarity on the underlying algorithms enabling this behavior, necessitating a new approach to understand ICL's algorithmic nature.",
        "problem": {
            "definition": "The problem defined in this paper is how neural networks, specifically transformers, can learn new functions from new datasets on-the-fly without updating their parameters.",
            "key obstacle": "The main challenge is understanding how a neural network can implicitly construct a mapping from in-context examples to a predictor while maintaining fixed parameters."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that instances of ICL can be viewed as implicit implementations of known learning algorithms, where transformers encode an implicit, context-dependent model in their hidden activations.",
            "opinion": "The proposed idea suggests that transformers can learn and adapt to new functions through in-context examples by effectively implementing learning algorithms such as gradient descent and ridge regression.",
            "innovation": "The innovation lies in demonstrating that transformers can implement standard learning algorithms for linear models, revealing that ICL can rediscover established estimation algorithms."
        },
        "method": {
            "method name": "In-Context Learning with Transformers",
            "method abbreviation": "ICL-T",
            "method definition": "ICL-T is a method where transformers learn to map sequences of input-output pairs to predictions without parameter updates, leveraging their architecture to implement learning algorithms implicitly.",
            "method description": "The core of ICL-T involves using transformer architecture to perform in-context learning through implicit algorithmic implementations.",
            "method steps": [
                "Define a class of functions and a distribution over these functions.",
                "Train the transformer to map sequences of input-output pairs to predictions.",
                "Utilize the transformer\u2019s layers to encode and update implicit models as new examples are presented."
            ],
            "principle": "The effectiveness of ICL-T is based on its ability to simulate learning algorithms through the structure and dynamics of transformer layers, allowing it to adapt to new data without explicit retraining."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using a transformer decoder trained on linear regression problems, with hyperparameter tuning over various depths, hidden sizes, and heads, using a dataset of input-output pairs.",
            "evaluation method": "Performance was assessed by comparing the predictions of the transformer with those of standard learning algorithms, measuring squared prediction differences and implicit linear weight differences."
        },
        "conclusion": "The experiments demonstrate that transformers can effectively implement multiple linear regression algorithms and adapt their behavior based on model capacity and dataset noise, suggesting that ICL is grounded in recognizable learning principles.",
        "discussion": {
            "advantage": "The key advantages of ICL-T include its ability to adapt to new data without retraining and its capability to implement established learning algorithms, providing interpretability to its predictions.",
            "limitation": "Limitations include potential challenges in scaling to more complex learning problems beyond linear regression and the reliance on sufficient model capacity to achieve desired performance.",
            "future work": "Future research directions include exploring ICL in non-linear contexts, extending the methodology to broader classes of learning problems, and investigating the interpretability of intermediate computations within transformers."
        },
        "other info": {
            "funding": "Ekin Aky\u00fcrek is supported by an MIT-Amazon ScienceHub fellowship and by the MIT-IBM Watson AI Lab.",
            "data availability": "Code and reference implementations are released alongside the paper for reproducibility."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the remarkable capacity of neural sequence models, particularly transformers, for in-context learning (ICL), which allows them to construct new predictors from sequences of labeled examples without further parameter updates."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea suggests that transformers can learn and adapt to new functions through in-context examples by effectively implementing learning algorithms such as gradient descent and ridge regression."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge is understanding how a neural network can implicitly construct a mapping from in-context examples to a predictor while maintaining fixed parameters."
        },
        {
            "section number": "3.2",
            "key information": "The intuition behind the proposed idea is that instances of ICL can be viewed as implicit implementations of known learning algorithms, where transformers encode an implicit, context-dependent model in their hidden activations."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of ICL-T is based on its ability to simulate learning algorithms through the structure and dynamics of transformer layers, allowing it to adapt to new data without explicit retraining."
        },
        {
            "section number": "6.1",
            "key information": "Limitations include potential challenges in scaling to more complex learning problems beyond linear regression and the reliance on sufficient model capacity to achieve desired performance."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrate that transformers can effectively implement multiple linear regression algorithms and adapt their behavior based on model capacity and dataset noise, suggesting that ICL is grounded in recognizable learning principles."
        }
    ],
    "similarity_score": 0.7480283096047443,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/What learning algorithm is in-context learning_ Investigations with linear models.json"
}