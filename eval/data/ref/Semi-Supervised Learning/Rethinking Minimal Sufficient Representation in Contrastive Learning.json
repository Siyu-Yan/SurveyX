{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2203.07004",
    "title": "Rethinking Minimal Sufficient Representation in Contrastive Learning",
    "abstract": "Contrastive learning between different views of the data achieves outstanding success in the field of self-supervised representation learning and the learned representations are useful in broad downstream tasks. Since all supervision information for one view comes from the other view, contrastive learning approximately obtains the minimal sufficient representation which contains the shared information and eliminates the non-shared information between views. Considering the diversity of the downstream tasks, it cannot be guaranteed that all task-relevant information is shared between views. Therefore, we assume the nonshared task-relevant information cannot be ignored and theoretically prove that the minimal sufficient representation in contrastive learning is not sufficient for the downstream tasks, which causes performance degradation. This reveals a new problem that the contrastive learning models have the risk of over-fitting to the shared information between views. To alleviate this problem, we propose to increase the mutual information between the representation and input as regularization to approximately introduce more task-relevant information, since we cannot utilize any downstream task information during training. Extensive experiments verify the rationality of our analysis and the effectiveness of our method. It significantly improves the performance of several classic contrastive learning models in downstream tasks. Our code is available at https://github.com/Haoqing-Wang/InfoCL.",
    "bib_name": "wang2022rethinkingminimalsufficientrepresentation",
    "md_text": "# Rethinking Minimal Sufficient Representation in Contrastive Learning\nHaoqing Wang*1 Xun Guo2 Zhi-Hong Deng1 Yan Lu2 1Peking University 2Microsoft Research Asia\n# Abstract\nContrastive learning between different views of the data achieves outstanding success in the field of self-supervised representation learning and the learned representations are useful in broad downstream tasks. Since all supervision information for one view comes from the other view, contrastive learning approximately obtains the minimal sufficient representation which contains the shared information and eliminates the non-shared information between views. Considering the diversity of the downstream tasks, it cannot be guaranteed that all task-relevant information is shared between views. Therefore, we assume the nonshared task-relevant information cannot be ignored and theoretically prove that the minimal sufficient representation in contrastive learning is not sufficient for the downstream tasks, which causes performance degradation. This reveals a new problem that the contrastive learning models have the risk of over-fitting to the shared information between views. To alleviate this problem, we propose to increase the mutual information between the representation and input as regularization to approximately introduce more task-relevant information, since we cannot utilize any downstream task information during training. Extensive experiments verify the rationality of our analysis and the effectiveness of our method. It significantly improves the performance of several classic contrastive learning models in downstream tasks. Our code is available at https://github.com/Haoqing-Wang/InfoCL.\nRecently, contrastive learning [6\u20138,19,52] between different views of the data achieves outstanding success in the field of self-supervised representation learning. The learned representations are useful for broad downstream tasks in practice, such as classification, detection and segmentation [21]. In contrastive learning, the representation that contains all shared information between views is defined as suf-\nMore Task-relevant Information Representation z1 Shared Task-relevant Information in Representation I(z1,v2,T) Non-shared Task-relevant Information in Representation I(z1,T|v2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/83f9/83f90d18-df93-426c-ae05-6e5208aff81a.png\" style=\"width: 50%;\"></div>\nFigure 1. Demonstration of our motivation using information diagrams. Based on the (approximately minimal) sufficient representation learned in contrastive learning, increasing I(z1, v1) approximately introduces more non-shared task-relevant information.\nficient representation, while the representation that contains only the shared and eliminates the non-shared information is defined as minimal sufficient representation [44]. Contrastive learning maximizes the mutual information between the representations of different views, thereby obtaining the sufficient representation. Furthermore, since all supervision information for one view comes from the other view [15], the non-shared information is often ignored, so that the minimal sufficient representation is approximately obtained. Tian et al. [41] find that the optimal views for contrastive learning depend on the downstream tasks when the minimal sufficient representation is obtained. In other words, the optimal views for task T1 may not be suitable for task T2. The reason may be that some information relevant to T2 is not shared between these views. In this work, we formalize this conjecture and assume that the non-shared task-relevant information cannot be ignored. Based on this assumption, we theoretically prove that the minimal sufficient representation contains less task-relevant information than other sufficient representations and has a non-ignorable gap with the optimal representation, which causes performance degradation. Concretely, we consider two types of the downstream task, i.e., classification and regression task, and prove that the lowest achievable error of the minimal sufficient representation is higher than other sufficient representations. According to our analysis, when some task-relevant information is not shared between views, the learned representation in contrastive learning is not sufficient for the downstream tasks. This reveals that the contrastive learning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/765e/765e229d-a0d7-4b8d-8c9c-626007829d41.png\" style=\"width: 50%;\"></div>\nmodels have the risk of over-fitting to the shared information between views. To this end, we need to introduce more non-shared task-relevant information to the representations. Since we cannot utilize any downstream task information in the training stage, it is impossible to achieve this directly. As an alternative, we propose an objective term which increases the mutual information between the representation and input to approximately introduce more task-relevant information. This motivation is demonstrated in Fig. 1 using information diagrams. We consider two implementations to increase the mutual information. The first one reconstructs the input to make the representations containing the key information about the input [27,45]. The second one relies on the high-dimensional mutual information estimate [5,35]. Overall, we summarize our contributions as follows.\n We verify the effectiveness of our method for SimCLR [7], BYOL [19] and Barlow Twins [52] in classification, detection and segmentation tasks. We also provide extensive analytical experiments to further understand our hypotheses, theoretical analysis and model.\n# 2. Related works\nContrastive learning. Contrastive learning between different views of the data is a successful self-supervised representation learning framework. The views are constructed by exploiting the structure of the unlabeled data, such as local patches and the whole image [24], different augmentations of the same image [2, 7, 21, 49], or video and text pairs [32,39]. Recently, Tian et al. [41] find that the optimal views for contrastive learning are task-dependent under the assumption of minimal sufficient representation. In other words, even if the given views are optimal for some downstream tasks, they may not be suitable for other tasks. In this work, we theoretically analyze this discovery and find that the contrastive learning models may over-fit to the shared information between views, and thus propose to increase the mutual information between representation and input to alleviate this problem. Some recent works [15,44] propose to learn the minimal sufficient representation. They assume that almost all the information relevant to downstream tasks\n<div style=\"text-align: center;\">Figure 2. Internal mechanism of contrastive learning: the views provide supervision information to each other.</div>\nis shared between views, which is an overly idealistic assumption and conflicts with the discovery in [41].\nInformation bottleneck theory. Based on the information bottleneck theory [37, 42, 43], a model extracts all task-relevant information in the first phase of learning (drift phase) to ensure sufficiency, and then compresses the taskirrelevant information in the second phase (diffusion phase). Our analysis shows that the learned representation in contrastive learning is not sufficient for the downstream tasks and can be seen as in the drift phase. We need to introduce more task-relevant information to achieve sufficiency.\n# 3. Theoretical analysis and model\nIn this section, we first introduce the contrastive learning framework and theoretically analyze the disadvantages of minimal sufficient representation in contrastive learning, and then propose our method to approximately introduce more task-relevant information to the representations. Note that although the analysis is about the information content in the representations, its specific form is also very important. Therefore, our theoretical analysis is actually based on the premise that the information content in the representations is represented in the most appropriate form.\n# 3.1. Contrastive learning\nContrastive learning is a general framework for unsupervised representation learning which maximizes the mutual information between the representations of two random variables v1 and v2 with the joint distribution p(v1, v2)\nwhere zi = fi(vi), i = 1, 2 are also random variables and fi, i = 1, 2 are encoding functions. In practice, v1 and v2 are usually two views of the data x. When v1 and v2 have the same marginal distributions (p(v1) = p(v2)), the function f1 and f2 can be the same (f1 = f2).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/601f/601f495c-e60c-482f-88a4-e0b0be63e202.png\" style=\"width: 50%;\"></div>\nIn contrastive learning, the variable v2 provides supervision information for v1 and plays the similar role as the label y in the supervised learning, and vice versa [15]. This internal mechanism is illustrated in Fig. 2. Similar to the information bottleneck theory [1,43] in the supervised learning, we can define the sufficient representation and minimal sufficient representation of v1 (or v2) for v2 (or v1) in contrastive learning [41,44].\nDefinition 1. (Sufficient Representation in Contrastive Learning) The representation zsuf 1 of v1 is sufficient for v2 if and only if I(zsuf 1 , v2) = I(v1, v2).\nThe sufficient representation zsuf 1 of v1 keeps all the information about v2 in v1. In other words, zsuf 1 contains all the shared information between v1 and v2, i.e., I(v1, v2|zsuf 1 ) = 0. Symmetrically, the sufficient representation zsuf 2 of v2 for v1 satisfies I(v1, zsuf 2 ) = I(v1, v2).\nDefinition 2. (Minimal Sufficient Representation in Contrastive Learning) The sufficient representation zmin 1 of v1 is minimal if and only if I(zmin 1 , v1) \u2264I(zsuf 1 , v1), \u2200zsuf 1 that is sufficient.\nAmong all sufficient representations, the minimal sufficient representation zmin 1 contains the least information about v1. Further, it is usually assumed that zmin 1 only contains the shared information between views and eliminates other non-shared information, i.e., I(zmin 1 , v1|v2) = 0. Applying Data Processing Inequality [11] to the Markov chain v1 \u2192v2 \u2192z2 and z2 \u2192v1 \u2192z1, we have\nI(v1, v2) \u2265I(v1, z2) \u2265I(z1, z2)\n(2)\ni.e., I(v1, v2) is the upper bound of I(z1, z2). Considering that I(v1, v2) remains unchanged during the optimization process, contrastive learning optimizes the functions f1 and f2 so that I(z1, z2) approximates I(v1, v2). When these functions have enough capacity and are well learned based on sufficient data, we can assume I(z1, z2) = I(v1, v2), which means the learned representations in contrastive learning are sufficient. They are also approximately minimal since all supervision information comes from the other view. Therefore, the shared information controls the properties of the representations. The learned representations in contrastive learning are typically used in various downstream tasks, so we introduce a random variable T to represent the information required for a downstream task which can be classification, regression or clustering task. Tian et al. [41] find that the optimal views for contrastive learning are task-dependent under the assumption of minimal sufficient representation. This discovery is intuitive since various downstream tasks need different information that is unknown during training.\nFigure 3. Information diagrams of different representations in contrastive learning. We consider the situation where the non-shared task-relevant information I(v1, T|v2) cannot be ignored. Contrastive learning makes the representations extracting the shared information between views to obtain the sufficient representation which is approximately minimal. The minimal sufficient representation contains less task-relevant information from the input than other sufficient representations.\nIt is difficult for the given views to share all the information required by these tasks. For example, when one view is a video stream and the other view is an audio stream, the shared information is sufficient for identity recognition task, but not for object tracking task. Some task-relevant information may not lie in the shared information between views, i.e., I(v1, T|v2) cannot be ignored. Eliminating all non-shared information has the risk of damaging the performance of the representations in the downstream tasks.\n# 3.2. Analysis on minimal sufficient representation\nThe minimal sufficient representation intuitively is not a good choice for downstream tasks, because it completely eliminates the non-shared information between views which may be important for some downstream tasks. We formalize this problem and theoretically prove that in contrastive learning, the minimal sufficient representation is expected to perform worse than other sufficient representations in the downstream tasks. All proofs for the below theorems are provided in Appendix A. Considering the symmetry between v1 and v2, without loss of generality, we take v2 as the supervision signal for v1 and take v1 as the input of a task. It is generally believed that the more task-relevant information contained in the representations, the better performance can be obtained [11,14]. Therefore, we examine the task-relevant information contained in the representations.\nTheorem 1. (Task-Relevant Information in Representations) In contrastive learning, for a downstream task T, the minimal sufficient representation zmin 1 contains less taskrelevant information from input v1 than other sufficient representation zsuf 1 , and I(zmin 1 , T) has a gap of I(v1, T|v2) with the upper bound I(v1, T). Formally, we have\nI(v1, T) = I(zmin 1 , T) + I(v1, T|v2) \u2265I(zsuf 1 , T) = I(zmin 1 , T) + I(zsuf 1 , T|v2) \u2265I(zmin 1 , T)\n(3)\nTheorem 1 indicates that zsuf 1 can have better performance in task T than zmin 1 because it contains more taskrelevant information. When non-shared task-relevant information I(v1, T|v2) is significant, zmin 1 has poor performance because it loses a lot of useful information. See Fig. 3 for the demonstration using information diagrams. To make this observation more concrete, we examine two types of the downstream task: classification tasks and regression tasks, and provide theoretical analysis on the generalization error of the representations. When the downstream task is a classification task and T is a categorical variable, we consider the Bayes error rate [17] which is the lowest achievable error for any classifier learned from the representations. Concretely, let Pe be the Bayes error rate of arbitrary learned representation z1 and \ufffdT be the prediction for T based on z1, we have Pe = 1 \u2212Ep(z1)[maxt\u2208T p( \ufffdT = t|z1)] and 0 \u2264Pe \u2264 1 \u22121/|T| where |T| is the cardinality of T. According to the value range of Pe, we define a threshold function \u0393(x) = min{max{x, 0}, 1 \u22121/|T|} to prevent overflow.\nTheorem 2. (Bayes Error Rate of Representations) For arbitrary learned representation z1, its Bayes error rate Pe = \u0393( \u00afPe) with\n (4)\nSpecifically, for sufficient representation zsuf 1 , its Bayes error rate P suf e = \u0393( \u00afP suf e ) with\n (5)\nfor minimal sufficient representation zmin 1 , its Bayes error rate P min e = \u0393( \u00afP min e ) with\n(6)\nSince I(zsuf 1 , T|v2) \u22650, Theorem 2 indicates for classification task T, the upper bound of P min e is larger than P suf e . In other words, zmin 1 is expected to obtain a higher classification error rate in the task T than zsuf 1 . According to the Eq. (5), considering that H(T) and I(v1, v2, T) are not related to the representations, increasing I(zsuf 1 , T|v2) can reduce the Bayes error rate in classification task T. When I(zsuf 1 , T|v2) = I(v1, T|v2), zsuf 1 contains all the useful information for task T in v1. When the downstream task is a regression task and T is a continuous variable, let \ufffdT be the prediction for T based on arbitrary learned representation z1, we consider the smallest achievable expected squared prediction error Re = min \ufffd T E[(T \u2212\ufffdT(z1))2] = E[\u03b52] with \u03b5(T, z1) = T \u2212E[T|z1].\n\ufffd Theorem 3. (Minimum Expected Squared Prediction Error of Representations) For arbitrary learned representa-\ntion z1, when the conditional distribution p(\u03b5|z1) is uniform, Laplacian or Gaussian distribution, the minimum expected squared prediction error Re satisfies\nfor minimal sufficient representation zmin 1 , its minimum expected squared prediction error Rmin e satisfies\n(9)\nwhere the constant coefficient \u03b1 depends on the conditional distribution p(\u03b5|z1).\nThe assumption about the estimation error \u03b5 in Theorem 3 is reasonable because \u03b5 is analogous to the \u2018noise\u2019 with the mean of 0, which is generally assumed to come from simple distributions (e.g., Gaussian distribution) in statistical learning theory. Similar to the classification tasks, Theorem 3 indicates that for regression tasks, zsuf 1 can achieve lower expected squared prediction error than zmin 1 and increasing I(zsuf 1 , T|v2) can improve the performance. Theorem 2 and Theorem 3 analyze the disadvantages of the minimal sufficient representation zmin 1 in classification tasks and regression tasks respectively. The essential reason is that zmin 1 has less task-relevant information than zsuf 1 and has a non-ignorable gap I(v1, T|v2) with the optimal representation, as shown in Theorem 1.\n# 3.3. More non-shared task-relevant information\nAccording to the above theoretical analysis, in contrastive learning, the minimal sufficient representation is not sufficient for downstream tasks due to the lack of some nonshared task-relevant information. Moreover, contrastive learning approximately learns the minimal sufficient representation, thereby having the risk of over-fitting to the shared information between views. To this end, we propose to extract more non-shared task-relevant information from v1, i.e., increasing I(z1, T|v2). However, we cannot utilize any downstream task information during training, so it is impossible to increase I(z1, T|v2) directly. We consider increasing I(z1, v1) as an alternative because the increased information from v1 in z1 may be relevant to some downstream tasks, and this motivation is demonstrated in Fig. 1. In addition, increasing I(z1, v1) also helps to extract the shared information between views at the beginning of the optimization process. Concretely, considering the symmetry between v1 and v2, our optimization objective is\n(10)\nwhich consists of the original optimization objective Eq. (1) in contrastive learning and the objective terms we proposed. The coefficients \u03bb1 and \u03bb2 are used to control the amount of increasing I(z1, v1) and I(z2, v2) respectively. For optimizing I(z1, z2), we adopt the commonly used implementations in contrastive learning models [7,18,52]. For optimizing I(zi, vi), i = 1, 2, we consider two implementations.\nImplementation I Since I(z, v) = H(v) \u2212H(v|z) and H(v) is not related with z, we can equivalently decrease the conditional entropy H(v|z) = \u2212Ep(z,v)[ln p(v|z)]. Concretely, we use the representation z to reconstruct the original input v, as done in auto-encoder models [45]. Decreasing the entropy of reconstruction encourages the representation z to contain more information about the original input v. However, the conditional distribution p(v|z) is intractable in practice, so we use q(v|z) as an approximation and get Ep(z,v)[ln q(v|z)], which is the lower bound of Ep(z,v)[ln p(v|z)]. We can increase Ep(z,v)[ln q(v|z)] as an alternative objective. According to the type of input v (e.g., images, text or audio), q(v|z) can be any appropriate distribution with known probability density function, such as Bernoulli distribution, Gaussian distribution or Laplace distribution, and its parameters are the functions of z. For example, when q(v|z) is the Gaussian distribution N(v; \u00b5(z), \u03c32I) with given variance \u03c32 and deterministic mean function \u00b5(\u00b7) which is usually parameterized by neural networks, we have\nwhere c is a constant to representation z. The final optimization objective is\n(12)\nImplementation II Although the above implementation is effective and preferred in practice, it needs to reconstruct the input, which is challenging for complex input and introduces more model parameters. To this end, we propose another representation-level implementation as an optional alternative. We investigate various lower bound estimates of mutual information, such as the bound of Barber and Agakov [4], the bound of Nguyen, Wainwright and Jordan [33], MINE [5] and InfoNCE [35]. We choose the InfoNCE lower bound and the detailed discussion is provided in Appendix B. Concretely, the InfoNCE lower bound is\n(13)\n\ufffd \ufffd where (zk, vk), k = 1, \u00b7 \u00b7 \u00b7 , N are N copies of (z, v) and the expectation is over \u03a0kp(zk, vk). In the implementation I, we map the input v to the representation z through a\ndeterministic function f with z = f(v). Differently, here we need the expression of p(z|v) to calculate the InfoNCE lower bound, which means the representation z is no longer a deterministic output of input v, so we use the reparameterization trick [27] during training. For example, when we define p(z|v) as the Gaussian distribution N(z; f(v), \u03c32I) with given variance \u03c32 and the function f is the same as in the Implementation I, we have z = f(v) + \u03f5\u03c3, \u03f5 \u223cN(0, I) and \u02c6INCE is equivalent to\n(14)\n\ufffd  \ufffd where \u03c1 is a scale factor. In fact, it pushes the representations away from each other to increase H(z), which can increase mutual information I(z, v) since I(z, v) = H(z) \u2212H(z|v) = H(z) \u2212d 2(ln 2\u03c0 + ln \u03c32 + 1) with d being representation dimension. It also be denoted as uniformity property [48]. The final optimization objective is\n(15)\nSince the objective term Eq. (14) is calculated at the representation-level, when we use the convolutional neural networks (e.g., ResNet [23]) to parameterize f, it can be applied to the output activation of multiple internal blocks. Discussion. It is worth noting that increasing I(z, v) does not conflict with the information bottleneck theory [43]. According to our analysis, the learned representations in contrastive learning are not sufficient for the downstream tasks. Therefore, we need to make the information in the representations more sufficient but not to compress it. On the other hand, we cannot introduce too much information from the input v either, which may contain harmful noise. Here we use the coefficients \u03bb1 and \u03bb2 to control this.\n# 4. Experiments\nIn this section, we first verify the effectiveness of increasing I(z, v) on various datasets, and then provide some analytical experiments. We choose three classic contrastive learning models as our baselines: SimCLR [7], BYOL [19] and Barlow Twins [52]. We denote our first implementation Eq. (12) as \u201dRC\u201d for \u201dReConstruction\u201d and the second implementation Eq. (15) as \u201dLBE\u201d for \u201dLower Bound Estimate\u201d. For all experiments, we use random cropping, flip and random color distortion as the data augmentation, as suggested by [7]. For \u201dLBE\u201d, we set \u03c3 = 0.1 and \u03c1 = 0.05.\n# 4.1. Effectiveness of increasing I(z, v)\nWe consider different types of the downstream task, including classification, detection and segmentation tasks. The results of Barlow Twins are provided in Appendix C.1.\nModel\nCIFAR10\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nSimCLR\n85.76\n29.52\n97.03\n88.36\n8.87\n42.81\n92.41\nSimCLR+RC (ours)\n85.78\n33.67\n97.99\n90.31\n10.89\n54.16\n95.84\nSimCLR+LBE (ours)\n85.45\n34.52\n97.94\n89.26\n10.60\n54.10\n94.96\nBYOL\n85.64\n31.22\n97.15\n88.92\n8.84\n40.90\n92.17\nBYOL+RC (ours)\n85.80\n34.73\n98.07\n89.61\n9.68\n48.75\n94.19\nBYOL+LBE (ours)\n85.28\n33.99\n97.76\n88.99\n9.96\n54.10\n95.09\nModel\nSTL-10\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nSimCLR\n78.74\n39.41\n95.00\n87.31\n8.34\n49.41\n80.25\nSimCLR+RC (ours)\n79.21\n41.81\n97.48\n89.98\n10.03\n60.46\n94.73\nSimCLR+LBE (ours)\n80.17\n42.07\n97.04\n88.68\n10.11\n58.51\n87.77\nBYOL\n80.83\n40.05\n94.45\n87.23\n8.54\n49.41\n77.54\nBYOL+RC (ours)\n81.11\n42.02\n96.96\n88.92\n9.63\n55.71\n88.57\nBYOL+LBE (ours)\n80.85\n42.55\n95.75\n87.88\n10.55\n59.39\n84.62\nModel\nImageNet\nDTD\nCIFAR10\nCIFAR100\nCUBirds\nVGGFlower\nTrafficSigns\nSimCLR\n61.01\n70.16\n82.30\n59.86\n36.49\n93.52\n95.27\nSimCLR+RC (ours)\n61.60\n71.22\n83.30\n63.56\n37.42\n94.53\n96.47\nSimCLR+LBE (ours)\n61.37\n70.95\n83.20\n61.99\n37.78\n94.34\n95.99\nPretraining. We train the models on CIFAR10 [28], STL10 [10] and ImageNet [12]. For CIFAR10 and STL-10, we use the ResNet18 [23] backbone and the models are trained for 200 epochs with batch size 256 using Adam optimizer with learning rate 3e-4. For ImageNet, we use the ResNet50 [23] backbone and the models are trained for 200 epochs with batch size 1024 using LARS optimizer [51] and a cosine decay learning rate schedule.\nLinear evaluation. We follow the linear evaluation protocol where a linear classifier is trained on top of the frozen backbone. The linear evaluation is conducted on the source dataset and several transfer datasets: CIFAR100 [28], DTD [9], MNIST [29], FashionMNIST [50], CUBirds [46], VGG Flower [34] and Traffic Signs [26]. The linear classifier is trained for 100 epochs using SGD optimizer. Table 1 shows the results on CIFAR10, STL-10 and ImageNet, and the best result in each block is in bold. Our implemented results of the baselines are consistent with [7, 25, 40, 47]. Increasing I(z, v) can introduce non-shared information and improve the classification accuracy, especially on transfer datasets. This means the shared information between views is not sufficient for some tasks, e.g., classification on DTD, VGG Flower and Traffic Signs where increasing I(z, v) achieves significant improvement. In other words, increasing I(z, v) can prevent the models from over-fitting to the shared information between views. What\u2019s more, it is effective for various contrastive learning models, which means our analysis results are widely applicable in contrastive learning. In fact, they all satisfy the internal mechanism.\nObject detection and instance segmentation. We conduct object detection on VOC07+12 [13] using Faster R-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cc51/cc5169ba-6d0a-4e92-9745-6dee48b09030.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Instance segmentation on COCO</div>\nTable 2. Object detection and instance segmentation on VOC07+12 and COCO. The models on COCO are fine-tuned using the default 2\u00d7 schedule. In magenta are the gaps of at least +0.5 point to the baseline, SimCLR.\nCNN [36], and detection and instance segmentation on COCO [30] using Mask R-CNN [22], following the setup in [21]. All methods use the R50-C4 [22] backbone that is initialized using the ResNet50 pre-trained on ImageNet. The results are show in Table 2. Increasing I(z, v) significantly improves the precision in object detection and instance segmentation tasks. These dense prediction tasks require some local semantic information from the input. Increasing I(z, v) can make the representation z contain more information from the input v which may not be shared between views, thereby obtaining better precision.\nModel\nCIFAR10\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nSimCLR\n85.76\n29.52\n97.03\n88.36\n8.87\n42.81\n92.41\nSimCLR+IP\n85.86\n30.15\n96.71\n88.18\n8.66\n43.22\n92.13\nSimCLR\u2020\n85.81\n31.70\n97.08\n88.85\n8.77\n44.41\n92.41\nSimCLR+MIB\n86.20\n31.17\n97.00\n88.62\n9.01\n43.88\n93.01\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d3cb/d3cbe47e-850f-4ca7-b654-3469fcadfecd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4. Linear evaluation accuracy on the source dataset (CIFAR10 or STL-10) and the averaged accuracy on all transfer datasets with varying hyper-parameter \u03bb.</div>\n# 4.2. Analytical experiments\nWe provide some analytical experiments to further understand our hypotheses, theoretical analysis and models.\nEliminating non-shared information. Some recent works [15, 44] propose to eliminate the non-shared information between views in the representation to get the minimal sufficient representation. To this end, Federici et al. [15] minimize the regularization term\nwhere KL(\u00b7||\u00b7) represents the Kullback-Leibler divergence. When p(z1|v1) and p(z2|v2) are modeled as N(zi; fi(vi), \u03c32I), i = 1, 2 with given variance \u03c32, it can be rewritten as LMIB = Ep(v1,v2) \ufffd \u2225f1(v1) \u2212f2(v2)\u22252 2 \ufffd . Identically, Tsai et al. [44] minimize the inverse predictive loss LIP = Ep(v1,v2) \ufffd \u2225f1(v1) \u2212f2(v2)\u22252 2 \ufffd . The detailed derivation is provided in Appendix D. We evaluate these two regularization terms in the linear evaluation tasks and choose their coefficient with best accuracy on the source dataset. The results are shown in Table 3 and the best result in each block is in bold. Although these two regularization terms have the same form, LMIB uses stochastic encoders\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/962c/962c2095-6fae-4b4f-8722-bafab75f0adc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) STL-10</div>\nFigure 5. Linear evaluation accuracy on the source dataset (CIFAR10 or STL-10) and the averaged accuracy on all transfer datasets with varying epochs.\nwhich is equivalent to adding Gaussian noise, so we report the results of SimCLR with Gaussian noise, marked by \u2020. As we can see, eliminating the non-shared information cannot change the accuracy in downstream classification tasks much. This means that the sufficient representation learned in contrastive learning is approximately minimal and we don\u2019t need to further remove the non-shared information.\nChanging the amount of increasing I(z, v). Quantifying the mutual information between the high-dimensional variables is very difficult, and often leads to inaccurate calculation in practice [31, 38]. Therefore, we assume that the hyper-parameters \u03bb1 and \u03bb2 control the amount of increasing I(z1, v1) and I(z2, v2) respectively. Larger \u03bb1 is expected to increase I(z1, v1) more, so as \u03bb2. We set \u03bb1 = \u03bb2 = \u03bb and evaluate the performance of different \u03bb from {0.001, 0.01, 0.1, 1, 10}. We choose SimCLR as the baseline and the results are shown in Fig. 4. We report the accuracy on the source dataset (CIFAR10 or STL-10) and the averaged accuracy on all transfer datasets. As we can see, increasing I(z, v) consistently improves the performance in downstream classification tasks. We can observe a non-monotonous reverse-U trend of accuracy with the change of \u03bb, which means excessively increasing I(z, v) may introduce noise beside useful information.\nModel\nCIFAR10\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nSupervised\n93.25\n34.10\n98.52\n90.09\n8.37\n46.14\n93.05\nSupervised+RC (ours)\n93.09\n32.77\n98.61\n89.77\n8.84\n49.05\n93.28\nSupervised+LBE (ours)\n93.18\n34.79\n98.68\n90.40\n9.72\n53.15\n94.47\nModel\nCIFAR100\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nSupervised\n71.92\n36.06\n98.48\n88.97\n11.51\n64.21\n96.54\nSupervised+RC (ours)\n72.02\n34.79\n98.59\n89.35\n10.94\n65.34\n96.67\nSupervised+LBE (ours)\n71.89\n36.33\n98.37\n89.42\n11.89\n65.64\n96.91\nTraining with more epochs. In the above experiments, we train all models for 200 epochs. Here we further show the behavior of the contrastive learning models and increasing I(z, v) when training with more epochs. We choose SimCLR as the baseline and train all models for 100, 200, 300, 400, 500 and 600 epochs. The results are shown in Fig. 5. With more training epochs, the learned representations in contrastive learning are more approximate to the minimal sufficient representation which mainly contain the shared information between views and ignore the nonshared information. For the classification tasks on the transfer datasets, the shared information between views is not sufficient. As shown in Fig. 5 (b) and (d), the accuracy on the transfer datasets decreases with more epochs and the learned representations over-fit to the shared information between views. Increasing I(z, v) can introduce non-shared information and obtain the significant improvement. For the classification tasks on the source datasets, the shared information between views is sufficient on CIFAR10 but not on STL-10. As shown in Fig. 5 (a) and (c), the accuracy on CIFAR10 increases with more epochs and increasing I(z, v) cannot make a difference. But the accuracy on STL-10 decreases with more epochs, and increasing I(z, v) can significantly improve the accuracy and does not decrease with more epochs. In fact, we use the unlabeled split for contrastive training on STL-10, so it is intuitive that the shared information between views is not sufficient for the classification tasks on the train and test split.\nIncreasing I(z, x) in supervised learning. According to the information bottleneck theory [43], a model extracts the approximate minimal sufficient statistics of the input x with respect to the label y in supervised learning. In other words, the representation z only contains the information related to the label and eliminates other irrelevant information which is considered as noise. However, label-irrelevant information may be useful for some downstream tasks, so we evaluate the effect of increasing I(z, x) in supervised learning. We train the ResNet18 backbone using the crossentropy classification loss on CIFAR10 and CIFAR100, and choose \u03bb1 = \u03bb2 = \u03bb from {0.001, 0.01, 0.1, 1, 10}. The linear evaluation results are shown in Table 4 and the\nbest result in each block is in bold. As we can see, increasing I(z, x) improves the performance on the transfer datasets and achieves comparable results on the source dataset, which means it can effectively alleviate the overfitting on the label information. This discovery helps to obtain more general representations in the field of supervised pre-training and we left it for the future work.\n# 5. Limitations\nOur work has the following limitations. 1) Based on our experimental observation, the assumption that non-shared task-relevant information cannot be ignored usually well holds for the cross-domain transfer tasks, but may not be satisfied for the tasks on the training dataset. 2) Increasing I(z, v) can also introduce noise (task-irrelevant) information which may increase the data demand in the downstream tasks, so one may need to adjust the coefficients \u03bb1 and \u03bb2 to achieve effective trade-off for the different downstream tasks. 3) Due to limited computing resources, we cannot reproduce the best results of SimCLR on ImageNet which need the batch size of 4096 and more training epochs.\n# 6. Conclusions\nIn this work, we explore the relationship between the learned representations and downstream tasks in contrastive learning. Although some works propose to learn the minimal sufficient representation, we theoretically and empirically verify that the minimal sufficient representation is not sufficient for downstream tasks because it loses non-shared task-relevant information. We find that contrastive learning approximately obtains the minimal sufficient representation, which means it may over-fit to the shared information between views. To this end, we propose to increase the mutual information between the representation and input to approximately introduce more non-shared task-relevant information when the downstream tasks are unknown. For the future work, we can consider combining the reconstruction models [3, 20] and contrastive learning for convolutional neural networks or vision transformers, since reconstruction can learn more sufficient information and contrast can make the representations more discriminative.\n# A. Proofs of theorems\nIn this section, we provide the proofs of the theorems in the main text. Since the random variable z1 = f1(v1) is the representation of random variable v1 where f1 is an encoding function, we have\nAssumption 1. Random variable z1 is conditionally independent from any other variable s in the system once random variable v1 is observed, i.e., I(z1, s|v1) = 0, \u2200s.\nThis assumption is also adopted in [15]. When f1 is a deterministic function, this assumption strictly holds. And when f1 is a random function, the information in z1 consists of the information from v1 and the information introduced by the randomness of function f1 which can be considered irrelevant to other variables in the system, so this assumption still holds. Next, we first present two lemmas for subsequent proofs.\nLemma 1. Let zsuf 1 and zmin 1 are the sufficient representation and the minimal sufficient representation of view v1 for v2 in contrative learning respectively, we have\n(17)\nProof. 1) From the Definition 1 and the Assumption 1, we have\nI(v1, v2, T) \u2212I(zsuf 1 , v2, T) = [I(v1, v2) \u2212I(v1, v2|T)] \u2212[I(zsuf 1 , v2) \u2212I(zsuf 1 , v2|T)] = I(zsuf 1 , v2|T) \u2212I(v1, v2|T) = [H(v2|T) \u2212H(v2|zsuf 1 , T)] \u2212[H(v2|T) \u2212H(v2|v1, T)] = H(v2|v1, T) \u2212H(v2|zsuf 1 , T) = [I(zsuf 1 , v2|v1, T) + H(v2|v1, zsuf 1 , T)] \u2212[I(v1, v2|zsuf 1 , T) + H(v2|v1, zsuf 1 , T)] = I(zsuf 1 , v2|v1, T) \u2212I(v1, v2|zsuf 1 , T) = I(zsuf 1 , v2|v1, T) = 0\nI(zsuf 1 , v2, T) = I(v1, v2, T)\n# The above proof process only uses the sufficiency of zsuf 1 for v2, so we have\nI(zmin 1 , v2, T) = I(v1, v2, T)\nWe consider the conditional entropy of the task variable T given the representation z1.\nLemma 2. For arbitrary learned representation z1, the conditional entropy H(T|z1) of the task variable T given z1 satisfies\n(19)\n(21)\nApplying the Eq. (17), the conditional entropy H(T|zsuf 1 ) satisfies\nFinally, we give the proofs of Theorem 1, 2 and 3.\nThe proof of Theorem 1.\n3) I(v1, T|v2) \u2265I(zsuf 1 , T|v2) \u22650. Applying the Data Processing Inequality [11] to the Markov chain T \u2192v1 \u2192zsuf 1 , we have I(v1, T) \u2265 I(zsuf 1 , T), so\n\u2265 | \u2265 Combining these three equations, we can get Theorem 1.\n# The proof of Theorem 2.\nProof. According to [14], the relationship between the Bayes error rate Pe and the conditional entropy H(T|z1) is \u2212ln(1 \u2212Pe) \u2264H(T|z1)\n\u2212 which is equivalent to\n\u2264 \u2212\u2212 \u2212 Note that 0 \u2264Pe \u22641 \u22121/|T|, so we use the threshold function \u0393(x) = min{max{x, 0}, 1 \u22121/|T|} to prevent overflow.\nProof. According to [16], when the conditional distribution p(\u03b5|z1) of estimation error \u03b5 is uniform, Laplace and Gaussian distribution, the minimum expected squared prediction error Re becomes 1 12 exp[2H(T|z1)], 1 2e2 exp[2H(T|z1)] and 1 2\u03c0e exp[2H(T|z1)] respectively. Therefore, we unify them as\nwhere \u03b1 is a constant coefficient which depends on the con ditional distribution p(\u03b5|z1). Applying the Lemma 2, fo arbitrary learned representation z1, we have Re = \u03b1 \u00b7 exp[2 \u00b7 (H(T) \u2212I(z1, T|v2) \u2212I(z1, v2, T))] for the sufficient representation zsuf 1 , we have Rsuf e = \u03b1 \u00b7 exp[2 \u00b7 (H(T) \u2212I(zsuf 1 , T|v2) \u2212I(v1, v2, T))] for the minimal sufficient representation zmin 1 , we have Rmin e = \u03b1 \u00b7 exp[2 \u00b7 (H(T) \u2212I(v1, v2, T))]\n# B. Choice of mutual information estimate\nIn our Implementation II, we need to use a mutual information lower bound estimate to calculate I(z, v) where v is the original input (e.g., images) and z is the representation (feature vectors). We consider three candidate estimates: 1) The bound of Nguyen, Wainwright and Jordan [33]\n (22)\n(23)\n(24)\n\ufffd where (zk, vk), k = 1, \u00b7 \u00b7 \u00b7 , N are N copies of (z, v) and the expectation is over \u03a0kp(zk, vk). As we can see, when we calculate the bound \u02c6INW J and \u02c6IMINE, we need to calculate the critic h(z, v) between the representation z and original input v. If we use a neural network to model the critic h(z, v), we have to take the original input (e.g. images) and the representation together as the input of a neural network. Since the distribution of the original input v and the representation z is quite different, it is very difficult. Therefore, we use the InfoNCE lower bound estimate.\nModel\nCIFAR10\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nBarTwins\n86.85\n28.56\n95.39\n86.19\n7.49\n35.91\n88.50\nBarTwins+RC (ours)\n86.91\n28.97\n96.60\n86.72\n7.90\n38.94\n90.92\nBarTwins+LBE (ours)\n86.38\n29.54\n96.72\n86.88\n8.47\n41.44\n92.76\nModel\nSTL-10\nDTD\nMNIST\nFaMNIST\nCUBirds\nVGGFlower\nTrafficSigns\nBarTwins\n80.59\n36.86\n94.27\n86.63\n7.47\n44.89\n73.73\nBarTwins+RC (ours)\n82.21\n36.97\n94.45\n86.71\n7.89\n46.31\n78.94\nBarTwins+LBE (ours)\n81.13\n37.32\n96.33\n87.13\n8.08\n49.82\n82.08\n# C. More experiments\nIn this section, we provide more experiments to support our work.\n# C.1. Results on Barlow Twins\nIn the main text, we provide the results on two classic contrastive learning models: SimCLR [7] and BYOL [19]. SimCLR perfectly matches the contrastive learning framework, maximizing the lower bound estimate of the mutual information I(z1, z2). BYOL avoids the dependence on the large amount of negative samples, and adopts prediction loss and the asymmetric structure. We further verify the effectiveness of increasing I(z, v) on Barlow Twins [52] which makes the cross-correlation matrix between the representations of different views as close to the identity matrix as possible. Although the loss functions of these contrastive learning models are very different, they all satisfy the internal mechanism that the views provide supervision information to each other, so they all approximately learn the minimal sufficient representation. We use the same pre-training schedule and linear evaluation protocol as in the main text and set \u03bb1 = \u03bb2 = 1. For STL-10, we use the unlabeled split for contrastive learning and the train and test split for linear evaluation. The results are shown in Table 5 and the best result in each block is in bold. Increasing I(z, v) can improve the accuracy of the learned representations in Barlow Twins in downstream classification tasks, which indicates that our analysis results are applicable to various contrastive losses.\n# C.2. Reconstructed samples\nIn order to show the reconstruction effect of our Implementation I, we provide the reconstructed images after training. As an example, we use SimCLR contrastive loss and take CIFAR10 as the training dataset. The original input images and the reconstructed images are shown in Fig. 6. As we can see, the reconstructed images retain the shape and outline information in the original images, so as the obtained representations. Since we use the mean square error loss to optimize the reconstruction module, the reconstructed images are blurry and this phenomenon is also ob-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f48/0f4814ea-f80c-4c3b-b869-694f413fed4e.png\" style=\"width: 50%;\"></div>\nFigure 6. Demonstration of the reconstruction effect of our Implementation I. We provide the original input images and the reconstructed images for comparison. We use SimCLR contrastive loss and take CIFAR10 as the training dataset.\n# served in vanilla variational auto-encoder [27].\nserved in vanilla variational auto-encoder [27].\n# D. Derivation of LMIB and LIP\nFederici et al. [15] and Tsai et al. [44] propose to eliminate the non-shared information between views in the representation to get the minimal sufficient representation. To this end, they propose their respective regularization terms. Here we derive the specific forms used in the main text. In [15], the regularization term is\nAccording to the description in their paper and the official code 1, they model the two stochastic encoders p(z1|v1) and\np(z2|v2) as\n(26) (27)\nwhere \u00b51(v1),\u03c32 1(v1),\u00b52(v2) and \u03c32 2(v2) are all functions of the input (v1 or v2), diag(e) creates a matrix in which the diagonal elements consist of vector e and all off-diagonal elements are zeros. The regularization term has the analytical expression\n\ufffd (28)\n\ufffd where d is the dimension of z1 and z2. We want to minimize LMIB, and when \u03c32 1 = \u03c32 2, the term \u03c3i2 1 /\u03c3i2 2 + \u03c3i2 2 /\u03c3i2 1 takes the minimum value 2, so the regularization term becomes\n(29)\nIn practice, minimizing LMIB makes the variance \u03c32 1 and \u03c32 2 very large, and the sampled representations change drastically and have very poor performance in downstream tasks. If the upper bound of the variance \u03c32 1 and \u03c32 2 is fixed, such as using the sigmoid activation function to limit it to (0, 1), they will converge to the maximum value as the training progresses. Therefore, we might as well fix the variance and model the two stochastic encoders p(z1|v1) and p(z2|v2) as\n(30) (31)\nwhere I is the identity matrix, \u03c32 is the given variance, fi, i = 1, 2 are deterministic encoders. This also guarantees a fair comparison with our Implementation II. According to the Eq. (29), the regularization term is equivalent to\n(32)\nWe calculate the expectation of the regularization term on the data distribution p(v1, v2) and get\n(33)\nIn [44], the authors define the inverse predictive loss\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of minimal sufficient representation in contrastive learning, which has shown success in self-supervised representation learning. The existing methods focus on maximizing shared information between views but often overlook non-shared task-relevant information, leading to performance degradation in downstream tasks.",
        "problem": {
            "definition": "The problem is that the minimal sufficient representation obtained from contrastive learning does not adequately capture all task-relevant information, particularly the non-shared information that may be crucial for certain downstream tasks.",
            "key obstacle": "The core obstacle is the risk of overfitting to the shared information between views, which results in a learned representation that lacks sufficient task-relevant information."
        },
        "idea": {
            "intuition": "The idea stems from the observation that non-shared task-relevant information cannot be ignored, and that increasing mutual information between the representation and input can help reintroduce this information.",
            "opinion": "The proposed idea involves modifying the contrastive learning objective to increase the mutual information between the learned representation and the input, thereby incorporating more task-relevant information.",
            "innovation": "The primary innovation of this method is the introduction of an objective term that regularizes the learning process by increasing mutual information, which contrasts with existing methods that focus solely on shared information."
        },
        "method": {
            "method name": "InfoCL",
            "method abbreviation": "ICL",
            "method definition": "InfoCL is a method designed to enhance the mutual information between the representation and input in contrastive learning to capture more non-shared task-relevant information.",
            "method description": "The core of InfoCL lies in its objective to maximize mutual information between representations and inputs while maintaining the contrastive learning framework.",
            "method steps": [
                "Step 1: Define the contrastive learning framework and set up the mutual information objectives.",
                "Step 2: Implement two strategies to increase mutual information: reconstruction of input and estimation of mutual information using InfoNCE.",
                "Step 3: Optimize the model parameters using the combined objectives."
            ],
            "principle": "The effectiveness of InfoCL is based on the principle that increasing the mutual information between the representation and input allows for a more comprehensive capture of task-relevant information, improving performance on downstream tasks."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on various datasets including CIFAR10, STL-10, and ImageNet, comparing the performance of InfoCL with classic contrastive learning models like SimCLR and BYOL across tasks such as classification, detection, and segmentation.",
            "evaluation method": "Performance was assessed using linear evaluation protocols, measuring accuracy and error rates on both source and transfer datasets."
        },
        "conclusion": "The experiments demonstrate that increasing mutual information significantly enhances the performance of contrastive learning models in downstream tasks, confirming that minimal sufficient representation is insufficient without considering non-shared information.",
        "discussion": {
            "advantage": "The key advantages of InfoCL are its ability to improve representation learning by incorporating non-shared information and its applicability across various contrastive learning models.",
            "limitation": "A limitation of the method is that increasing mutual information can also introduce noise, necessitating careful tuning of hyperparameters to balance the trade-off between useful and irrelevant information.",
            "future work": "Future directions include exploring the combination of reconstruction models with contrastive learning to further enhance representation quality and generalizability."
        },
        "other info": {
            "code availability": "The code for this method is available at https://github.com/Haoqing-Wang/InfoCL."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational idea behind semi-supervised algorithms includes leveraging both labeled and unlabeled data to enhance learning accuracy."
        },
        {
            "section number": "2.1",
            "key information": "Define semi-supervised learning as a method that utilizes both labeled and unlabeled data, contrasting it with traditional supervised and unsupervised learning approaches."
        },
        {
            "section number": "3.4",
            "key information": "The proposed method, InfoCL, enhances mutual information between representations and inputs in contrastive learning, which can be categorized under neural network and deep learning approaches."
        },
        {
            "section number": "4.1",
            "key information": "The critical role of data labeling in semi-supervised learning is emphasized through the need for accurate representations that capture all task-relevant information."
        },
        {
            "section number": "5.2",
            "key information": "The advantages of using semi-supervised learning, such as improved performance on downstream tasks through the incorporation of non-shared task-relevant information, highlight its superiority over unsupervised learning."
        },
        {
            "section number": "7.1",
            "key information": "Challenges related to scalability and computational complexity in semi-supervised learning are relevant, particularly in the context of balancing useful and irrelevant information in the learning process."
        },
        {
            "section number": "8",
            "key information": "The conclusion emphasizes the importance of considering non-shared information in representation learning to enhance the performance of semi-supervised algorithms."
        }
    ],
    "similarity_score": 0.6051438913605048,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Rethinking Minimal Sufficient Representation in Contrastive Learning.json"
}