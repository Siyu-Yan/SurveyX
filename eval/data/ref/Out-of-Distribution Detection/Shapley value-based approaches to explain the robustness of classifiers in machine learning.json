{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2209.04254",
    "title": "Shapley value-based approaches to explain the robustness of classifiers in machine learning",
    "abstract": "The use of algorithm-agnostic approaches is an emerging area of research for explaining the contribution of individual features towards the predicted outcome. Whilst there is a focus on explaining the prediction itself, a little has been done on explaining the robustness of these models, that is, how each feature contributes towards achieving that robustness. In this paper, we propose the use of Shapley values to explain the contribution of each feature towards the model's robustness, measured in terms of Receiver-operating Characteristics (ROC) curve and the Area under the ROC curve (AUC). With the help of an illustrative example, we demonstrate the proposed idea of explaining the ROC curve, and visualising the uncertainties in these curves. For imbalanced datasets, the use of Precision-Recall Curve (PRC) is considered more appropriate, therefore we also demonstrate how to explain the PRCs with the help of Shapley values. The explanation of robustness can help analysts in a number of ways, for example, it can help in feature selection by identifying the irrelevant features that can be removed to reduce the computational complexity. It can also help in identifying the features having critical contributions or negative contributions towards robustness.",
    "bib_name": "pelegrina2022shapleyvaluebasedapproachesexplain",
    "md_text": "# Shapley value-based approaches to explain the robustness of classifiers in machine learning\nGuilherme Dean Pelegrinaa,\u2217, Sajid Sirajb,c\naSchool of Applied Sciences - University of Campinas, Limeira, Brazil bCentre for Decision Research, Leeds University Business School, Leeds, UK cCOMSATS University Islamabad, Wah Campus, Pakistan\nAbstract\nThe use of algorithm-agnostic approaches is an emerging area of research for explaining the contribution of individual features towards the predicted outcome. Whilst there is a focus on explaining the prediction itself, a little has been done on explaining the robustness of these models, that is, how each feature contributes towards achieving that robustness. In this paper, we propose the use of Shapley values to explain the contribution of each feature towards the model\u2019s robustness, measured in terms of Receiver-operating Characteristics (ROC) curve and the Area under the ROC curve (AUC). With the help of an illustrative example, we demonstrate the proposed idea of explaining the ROC curve, and visualising the uncertainties in these curves. For imbalanced datasets, the use of Precision-Recall Curve (PRC) is considered more appropriate, therefore we also demonstrate how to explain the PRCs with the help of Shapley values. The explanation of robustness can help analysts in a number of ways, for example, it can help in feature selection by identifying the irrelevant features that can be removed to reduce the computational complexity. It can also help in identifying the features having critical contributions or negative contributions towards robustness. Keywords: Decision support systems; Explainability; Machine learning; Business analytics\n# 1. Introduction\nThe field of business intelligence and predictive analytics has grown by leaps and bounds in last two decades (Kumar and Garg, 2018; Liang and Liu, 2018; Zhang et al., 2021). This growth can be attributed to a significant improvement in the performance of various prediction algorithms (in terms of their accuracy, precision, recall, etc.). However, when considering the practicality of using these prediction algorithms, the majority of them tends to be quite complex. A key contribution in this domain is the recent introduction of\nalgorithm-agnostic explanation approaches to explain the contribution of each feature towards the overall prediction (Guidotti et al., 2018; Molnar, 2021; Burkart and Huber, 2021; Kenny et al., 2021). It is certainly an important achievement in predictive analytics as most of the models (specially those based on random forests (Breiman, 2001), deep neural networks (LeCun et al., 2015; Goodfellow et al., 2016) and extreme gradient boosting (Chen and He, 2015; Chen and Guestrin, 2016)) were previously treated as black box models and were questioned due to their complex nature. Indeed, besides the performance, aspects such as fairness and explainability (among others) are also important when deciding which machine learning (ML) model to adopt (Kleinberg et al., 2017; Chora\u015b et al., 2020; Miller, 2019; B\u00fccker et al., 2022). One of the widely-used algorithm-agnostic approaches to explain ML models is based on the cooperative game-theoretic concept called the Shapley value (Shapley, 1953). The Shapley value approach is considered as a fair way to divide the payoff in a game among its players. In ML context, it can be used as a feature attribution method, i.e. a measure that indicates how much each feature is contributing in the ML task. Therefore, the main idea is to see the ML problem (classification, prediction, etc.) as a cooperative game such that the features cooperate in order to achieve a specific goal. This goal depends on what one would like to explain, for example, Lipovetsky and Conklin (2001) used the Shapley values to explain the coefficient of determination in regression models. As the Shapley value calculation considers all coalitions of regressors, the obtained results were consistent even in scenarios with multicollinearity. Begley et al. (2020) associated payoffs in game theory to fairness measures and applied the Shapley values to explain the impact of features when there are disparate results regarding different groups of people (women and men, blacks and whites, etc.). More recently, Giudici and Raffinetti (2021) used the Shapley values as an explanatory approach to the Lorenz Zonoid goodness of fit. Moreover, in the famous SHAP method proposed by Lundberg and Lee (2017), the Shapley values were used to indicate the contribution of each feature in local predictions. In the field of ML, improving the performance of prediction algorithms has remained the main focus for decades. The performance metrics of accuracy, precision and F-measure are considered almost mandatory when evaluating any prediction algorithm. Although there are recent proposals to estimate the contribution of each feature towards prediction, there is still a need to explain their contributions towards the robustness of these models (Wang and Tang, 2009; Serrano et al., 2010; Xu et al., 2014). To assess the robustness in prediction models, the Receiver Operating Characteristic (ROC) curve and the area under the ROC curve (AUC) have been widely used in ML (Bradley, 1997), which has traditionally been used in the field of operational research and signal processing for long time (Therrien, 1989). We contend that the Shapleyvalues can also be used to explain robustness of the ML models, for example, by explaining the contribution\nof each feature towards the AUC. Imagine a situation where we achieve a model with the AUC of 0.90. It tells us how robust the prediction model is, however, it does not explain how much each feature has contributed towards this robustness. Also, one may wish to investigate whether the contribution of each feature varies for different specificity, or does it remain consistent regardless of the specificity values. Considering this gap, we first propose the ShapAUC method, a Shapley-based approach to explain the AUC for any ML model. For this, we assume that features join in coalitions and cooperate to achieve the AUC as a common goal. Based on the AUC calculated for all coalitions of features, we calculate the Shapley values, which indicate the marginal contribution of each feature in the model robustness. As a second contribution, namely ShapROC, we propose a way that explains the contribution of each feature towards the ROC curve. Shapley value is calculated to provide the marginal contribution of each feature at each point inside the ROC curve. As the use of Precision-Recall Curves (PRCs) is preferred for imbalanced datasets, we also propose to extend the use of Shapley values to decompose PRCs as well as to calculate the contributions towards the area under the PRC (AUPRC). Based on numerical experiments in a real dataset, we show the usefulness of our proposals in explaining the contribution of each feature towards the robustness of a model. One of the benefits of the proposed approach is to use it in feature selection. As we provide the marginal contributions of features towards robustness, it is possible to identify insignificant features, or more importantly, identifying features having negative contribution. By removing a feature with negative contribution, we may improve the model robustness, and by removing insignificant features, we can reduce the computational complexity of the classifier. The rest of this paper is organised as follows. Section 2 discusses the background of our proposals, which lies in the robustness of prediction models and the use of Shapley values as a feature attribution method. In Section 3, we present the proposed approaches. Section 4 illustrates the use of our proposals in a real dataset. Finally, in Section 5, we show our conclusions and future perspectives.\nThis section presents the theoretical background used in our proposals. Firstly, we discuss the key elements when assessing the prediction model robustness based on ROC curve and PRC. Thereafter, we discuss the use of Shapley values as a feature attribution method in ML explainability.\nThe performance of classifiers can be measured in different ways. A common way of assessing performance is to calculate the accuracy of prediction, which is essentially a ratio of the correct predictions made out of the overall predictions carried out. In practise, this metric might not be very useful in situations where predicting positive outcomes are more important than predicting negative outcomes, or vice versa. Therefore, classification performance usually involves construction of a confusion matrix that shows the number of true positives (TPs), true negatives (TNs), false positives (FPs), and false negatives (FNs). This is illustrated in Figure 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9186/91866121-cda5-4ebb-a240-fee20a061a55.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Confusion matrix showing definitions for various performance metrics.</div>\nFrom this confusion matrix, a number of different performance measures can be obtained, for example, the ratio of true positives to actual positives is known as the true positive rate (TPR), also known as Recall or Sensitivity. Similarly, the ratio of true negatives to actual negatives is known as the true negative rate or Specificity. The complementary value of Specificity is known as false positive rate (FPR). Another way of assessing classifier is its ability to detect true positive cases out of the cases that were detected as positives. This is known as the Precision of the classifier. All these metrics capture different aspect of model\u2019s performance and are useful, however, for testing robustness of model, the construction of confusion matrix itself has to be investigated and analysed. In ML literature, it is common practise to repeat training and testing several times, and testing the consistency in these performance scores. Another common practise is to assess model\u2019s performance by varying different parameters like classification thresholds. We discuss this below in more detail.\n# 2.1.1. ROC curve and AUC\nReceiver Operating Characteristic (ROC) curve has been widely used in machine learning (Bradley, 1997) to assess the robustness in prediction models. In a binary classification problem, the outcome is considered positive when the prediction probability is obtained above a certain threshold. For example, in a fingerprint authentication system, a fingerprint image is scanned and a ML algorithm calculates the probability for it to be a valid fingerprint. If the predicted probability happens to be 0.40, it can be declared unauthorised considering that any value lower than 0.50 is closer to 0 (false) than 1 (true). However, we can relax this requirement by lowering the threshold value from 0.50 to 0.30, in which case, the predicted probability of 0.40 will be declared true (i.e. authorised). This means that lower threshold will have higher risk of authorising the unauthorised fingerprints (false positive), while on the other hand, raising the threshold will have higher risk of rejecting the authorised fingerprints (false negatives). Ideally, a system should be robust enough to detect all true positives regardless of the threshold values. This robustness can be investigated and quantified using ROC curve which is, simply put, a line plot between FPR and TPR values calculated by varying the threshold values for classifying predicted probability. The overall robustness of the algorithm is summarised by calculating the area under this ROC curve (AUC) which is a value between 0 and 1. A value of 1 implies that the algorithm is robust in detecting all true positives no matter what the threshold value is.\n# 2.2. Precision-recall curve and AUPRC\nAlthough both ROC and AUC have been widely used, their usefulness has been debated for imbalanced classification problems (Jeni et al., 2013; Saito and Rehmsmeier, 2015). For example, email spam detection is a classical ML problem where the two classes are highly imbalanced (Alqatawna et al., 2015). In this case, people prefer to minimise false positives, and therefore, only interested in the left side of the ROC curve (where FPR is close to 0), however, the calculation of AUC does not prioritise one side over other. For imbalanced classes, the use of Precision-Recall Curve (PRC) is considered more appropriate than the use of ROC and AUC (Jeni et al., 2013; Saito and Rehmsmeier, 2015). As the names suggests, the PRC explains the relationship between precision and recall for all threshold values.\n# 2.3. Explanation in ML models\nAs the use of ML is getting common, there is an increasing demand (and pressure) for the explainability of these ML models. For example, a bank\u2019s customer might ask for reasons why his/her application for credi got refused. Since the introduction of SHapley Additive exPlanations (SHAP) by Lundberg and Lee (2017)\nthe use of explainable AI has been introduced in many practical applications like financial risk management (Bussmann et al., 2020; B\u00fccker et al., 2022), healthcare (Lundberg et al., 2018; Weng et al., 2022), inflation forecasts (Aras and Lisboa, 2022), and many more. However, so far, explainability has mostly focused on how to explain the contribution of each predictor towards attaining the predicted outcomes. While explaining the predicted outcome is an important area to investigate, it is also important to explain the robustness in predicting these values. For example, if a model is shown to have robustness of 0.85, it is important to explain how different predictors have contributed towards achieving this level of robustness. In this context, the use of Shapley values can give promising results, as it has already been demonstrated to be useful in practical applications involving ML.\n# 2.4. Shapley values as a feature attribution method\nBefore defining the Shapley value, let us first introduce the notion of cooperative games (Peleg and Sudh\u00f6lter, 2007). In a cooperative game, there exists a cooperative behaviour among a set of players aiming at achieving a predetermined goal. Several practical situations can be modelled as a cooperative game problem (Wang et al., 2003; Curiel, 2013; Bistaffa et al., 2017). For instance, Kristiansen et al. (2018); He et al. (2020); Churkin et al. (2021) showed applications in power system expansion planning. In this case, different companies can cooperate in order to reduce, for instance, power losses or investment cost allocation in power transmission systems. Another example includes modelling supply chain management tasks as a cooperative game problem (Meca et al., 2003; Cachon and Netessine, 2006; Fiestras-Janeiro et al., 2011; Zheng et al., 2019). A common goal shared by managers can be the fixed cost paid by shipment orders. Therefore, if they form a coalition and order simultaneously, they could save more money than if they act separately. Suppose a set N = {1, 2, . . . , n} of n players. Mathematically, one may define a coalition game on N by a characteristic function \u03c5 : P(N) \u2192R, where P(N) is the power set of N, that maps all possible coalitions of players to real numbers, such that \u03c5(\u2205) = 0. One frequently refers to \u03c5(A), where A \u2286N, as the payoff (or the benefit) achieved by the coalition A when cooperating in the game. For example, in the supply chain task mentioned earlier, \u03c5(A) could represent the savings obtained by the coalition of managers A when ordering simultaneously. However, a question that arises in a cooperative game is how to divide the gains obtained by a coalition of players. One of the well-known solutions for such a sharing is called Shapley value (Shapley, 1953). For each player i \u2208N, the associated Shapley value represents the marginal contribution of the player in the game payoff when considering all possible coalitions of players. It can be\nwhere |A| indicates the cardinality of subset A. An interesting aspect of Shapley value is that it satisfies several desired properties when allocating benefits among players (see (Young, 1985) for other properties and further details):\nwhere |A| indicates the cardinality of subset A. An interesting aspect of Shapley value is that it satisfies several desired properties when allocating benefits among players (see (Young, 1985) for other properties and further details): Property 1. Efficiency: The sum of the Shapley values of all players is equal to the payoff of the grand coalition N discounted by the payoff of the empty coalition. As by the definition of a game \u03c5(\u2205) = 0, the gain \u03c5(N) is distributed among the players:\nProperty 2. Null player: If, for all subset A \u2286N,\nthen \u03c6i = 0. It means that, if there is no gain when player i joins any coalition (he/she does no contribute in any payoff), he/she will not receive benefits.\n\u03c5 (A \u222a{i}) = \u03c5 (A \u222a{i\u2032}) ,\nfor all A \u2282N which contains neither i nor i\u2032, then \u03c6i = \u03c6i\u2032. Therefore, if two players contribute equally when joining all coalitions, they should receive the same amount.\nGiven these properties, the Shapley value approach is considered to be a fair strategy to divide gains, and the use of such a solution brought attention in the explainable ML research community (Lipovetsky and Conklin, 2001; Lundberg and Lee, 2017; Begley et al., 2020; Lundberg et al., 2020; Giudici and Raffinetti, 2021; Aas et al., 2021). As mentioned in Section 1, the main idea is to use it as a feature attribution method. Therefore, given a goal that one would like to explain (accuracy, local prediction, fairness measures, etc.), the Shapley values will indicate the contribution of each feature towards this goal. For this purpose, there are some key aspects that must be considered carefully when bringing Eq. (1) to the field of ML:\n(3)\n1. Firstly, one should define \u03c5(\u00b7) according to what one would like to explain. For instance, if one aims at analysing the contribution of each feature towards the model\u2019s overall accuracy, one has to define \u03c5(\u00b7) as the accuracy of the trained model based on the TPs and FPs in the test data. 2. One should be aware of what \u03c5(\u2205) represents. For example, in the shipment orders example, it is clear that there will be no savings when there is no coalition, so \u03c5(\u2205) should be zero. However, in a ML scenario, the payoff of the empty set can be a non-zero value, and therefore, calculating \u03c5(\u2205) might be more complicated in those cases. 3. Finally, another important aspect is the computational complexity of the Shapley values, as it involves retraining the model for all possible coalitions A, and calculating \u03c5(A). This may pose a computational constraint when dealing with a high-dimensional data, as the number of payoffs exponentially increases with the number of features. In order to reduce this effort, one may consider approximation strategies that estimate the Shapley values with less computations (\u0160trumbelj and Kononenko, 2014). In the next sections, we explain how to define the aforementioned aspects and how to use the Shapley lues to explain the robustness of ML classifiers.\n# 3. Explaining the robustness through Shapley values\nAs discussed earlier, explaining the predicted outcome is an important area of research but it is also important to explain the robustness in predicting these values. Although there can be different ways to measure robustness, the use of ROC and PR curves are preferred as they span a range of threshold values to classify the predicted outcomes. This makes them independent of threshold values unlike the other measures like accuracy and F-score. Therefore, the ROC and PR curves are also considered preferred approaches for explaining the robustness of ML models. The proposed process for explaining these curves is discussed below.\n# 3.1. ShapAUC: Explaining the area under the ROC curve\nAssume a ROC curve obtained after training a ML model. The proposed ShapAUC method provides the contribution of each feature towards the area under this ROC curve. We can safely assume that the random classifier is the baseline for the AUC, which can be achieved even when no feature contributes towards the classifier training. In the case of a random classifier, TPRs are obviously equal to FPRs and therefore, by definition, the AUC will be 0.50. If one includes features in training, the difference between the obtained\nAUC and the random classifier is then explained by the contribution of such features. For example, if one achieves an AUC of 0.95, the features are contributing to improve the AUC from 0.50 (which could be obtained by a random classifier) to the actual 0.95. In other words, the marginal contribution of all features should sum up to 0.45. Based on this reasoning, we define the payoffs of ShapAUC as follows:\n\u03c5AUC(A) = AUCA \u22120.50,\nwhere AUCA represents the area under the ROC curve when only the features in A are used in the training step. Note that, if A = \u2205then AUC\u2205= 0.50 (the random classifier), and therefore \u03c5AUC(\u2205) = AUC\u2205\u22120.50 = 0. Moreover, according to the efficiency property (see Eq. (2)), \ufffdn i=1 \u03c6i = \u03c5AUC(N)\u2212\u03c5AUC(\u2205) = AUCN \u2212 0.50, that is, the sum of the marginal contributions of all features is equal to the difference between the AUC of the grand coalition and the AUC of the random classifier. After retraining the ML model and calculating the payoffs for all subset of features, we interpret robustness based on the Shapley values calculated by\nThe process of ShapAUC is summarised in Figure 2. The procedure involves choosing a subset of features and then calculating the ROC curve for this subset, along with the AUC value. This process is then repeated for all possible subsets of features available in the ML dataset. The contribution of each feature is then estimated with the help of Eq. 6. These contributions can be visualised in a waterfall plot, as shown on the bottom right of Figure 2. The contribution of each feature is provided in a decreasing order (given the absolute values) that cooperates to increase the AUC from the random classifier to the actual value.\n# 3.2. ShapROC: Explaining the ROC curve\nIn the previous subsection, we propose to explain the area under the ROC curve. It is also possible to explain the curve itself by explaining each point on the curve. The ROC curve is essentially TPR values plotted against the FPR values, so it is possible to formulate each point on the TPR curve as a coalition game. The purpose of ShapROC is to use the Shapley value as a feature attribution method to explain the TPR values in the ROC curve. We explain the process of obtaining ShapROC into three steps, as explained in the subsections below.\n(5)\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a178/a178196b-0c76-473e-8df6-ce786e909e0d.png\" style=\"width: 50%;\"></div>\n# 3.2.1. Defining the payoffs\nRecall that in the case of a random classifier, the TPRs are equal to FPRs. By assuming that the random classifier is the baseline for the TPRs, the difference between a TPR and the associated FPR can be explained by the cooperation of features when they join in a coalition. For example, considering a TPR of 0.85 for a FPR of 0.25, the contribution of features is 0.85 \u22120.25 = 0.60. This idea leads to the following\nwhere 0 \u2264fpr \u22641 and tprfpr,A is the TPR associated to a given fpr and a coalition A of features. Note that, for a random classifier, A = \u2205implies that tprfpr,\u2205= fpr and, therefore, \u03c5fpr(\u2205) = tprfpr,\u2205\u2212 fpr = fpr \u2212fpr = 0. In addition, based on the efficiency property, \ufffdn i=1 \u03c6i = \u03c5ROC fpr (N) \u2212\u03c5ROC fpr (\u2205) = tprfpr,N \u2212fpr. Therefore, by summing up the marginal contribution of each feature, we can explain the net increase in value from fpr to the obtained tprfpr,N. In Eq. (7), the payoffs \u03c5ROC fpr (A) for different coalitions A depend on the fpr values. However, considering the fact that these values are recalculated for different classification thresholds, different coalitions might end up in generating totally different sets of FPR values, and therefore, making these coalitions incomparable to each other. To address this issue, we have to introduce an additional step for estimating TPR values in the ShapAUC method. We discuss it in the next section. 3.2.2. Estimating the TPR values We estimate the TPRs based on the standard ROC curve (calculated by using all features together). Consider that (f ROC k , tROC k )k=1,\u00b7\u00b7\u00b7 ,l represents the set of FPR and TPR values used to build the standard ROC curve. Moreover, assume that we intend to explain a specific tpr\u2032 fpr\u2032,N for a fixed fpr\u2032 (e.g. tpr\u2032 0.25,N = 0.85 for a fixed fpr\u2032 = 0.25). For each A, as a first step, we find the nearest available FPR values on either side i.e. f ROC a , f ROC b \u2208 \ufffd f ROC 1 , . . . , f ROC l \ufffd from fpr\u2032 such that f ROC a \u2264fpr\u2032 \u2264f ROC b . We consider three strategies to estimate the TPR values under analysis, namely optimistic, pessimistic and interpolation strategies. The three strategies are defined below: \u2022 Optimistic strategy: tprfpr\u2032,A = max (tROC a , tROC b ). \u2022 Pessimistic strategy: tprfpr\u2032,A = min (tROC a , tROC b ). \u2022 Interpolation strategy: tprfpr\u2032,A = (tROC b \u2212tROC a )(fpr\u2032\u2212f ROC a ) f ROC b \u2212f ROC a + tROC a . n.b. In case of interpolation, if f ROC a = f ROC b , then we can simply take an average of two values as an estimate: tprfpr\u2032,A = tROC a +tROC b 2 . The use of optimistic strategy will provide higher values of TPRs to calculate payoffs, and therefore, it can be considered as an upper approximation of the contributions. Similarly, the pessimistic strategy will provide the lower approximation of the contributions. The interpolation strategy, on the other hand, might be considered more balanced in a way that it tends to provide a value between the upper and lower\nwhere 0 \u2264fpr \u22641 and tprfpr,A is the TPR associated to a given fpr and a coalition A of features. Note that, for a random classifier, A = \u2205implies that tprfpr,\u2205= fpr and, therefore, \u03c5fpr(\u2205) = tprfpr,\u2205\u2212 fpr = fpr \u2212fpr = 0. In addition, based on the efficiency property, \ufffdn i=1 \u03c6i = \u03c5ROC fpr (N) \u2212\u03c5ROC fpr (\u2205) = tprfpr,N \u2212fpr. Therefore, by summing up the marginal contribution of each feature, we can explain the net increase in value from fpr to the obtained tprfpr,N. In Eq. (7), the payoffs \u03c5ROC fpr (A) for different coalitions A depend on the fpr values. However, considering the fact that these values are recalculated for different classification thresholds, different coalitions might end up in generating totally different sets of FPR values, and therefore, making these coalitions incomparable to each other. To address this issue, we have to introduce an additional step for estimating TPR values in the ShapAUC method. We discuss it in the next section.\n\u2022 Interpolation strategy: tprfpr\u2032,A = (tROC b \u2212tROC a )(fpr\u2032\u2212f ROC a ) f ROC b \u2212f ROC a + tROC a .\nn.b. In case of interpolation, if f ROC a = f ROC b , then we can simply take an average of two values as an estimate: tprfpr\u2032,A = tROC a +tROC b 2 . The use of optimistic strategy will provide higher values of TPRs to calculate payoffs, and therefore, it can be considered as an upper approximation of the contributions. Similarly, the pessimistic strategy will provide the lower approximation of the contributions. The interpolation strategy, on the other hand, might be considered more balanced in a way that it tends to provide a value between the upper and lower\napproximations. Here, the aim is not to compare these strategies or finding the most appropriate strategy rather the aim of introducing these strategies is to demonstrate the possibility of estimating the TPR curves in order to compare results from different coalitions.\nWe can repeat the Shapley value calculations for each FPR in the ROC plot, and therefore, generating a set of curves representing contribution of each feature throughout the curve. This idea can be quite useful for ML analysts to assess the impact of each feature for different FPR values. Figure 3 illustrates the proposed ShapROC method and the features contribution visualisation towards the TPRs. In the waterfall plot, we can see how features contribute to increase the TPR from the random classifier (e.g. TPR when fpr = 0.20) to the actual value (e.g. \u00af tprfpr = 0.91). In the figure at the bottom right, the contributions of each feature are visible for the whole range of FPR values (varying from 0 to 1).\n3.3. Relation between ShapAUC and ShapROC\nThe AUC can be approximated by the Riemann integral, that is, by the sum of very small rectangular areas calculated from the TPR and FPR values in the ROC curve. Consider a set of TPR and FPR values (f ROC k , tROC k )k=1,\u00b7\u00b7\u00b7 ,l such that 0 = f ROC 0 < . . . < f ROC k\u22121 < f ROC k < . . . < f ROC l = 1 and the difference between any f ROC k and f ROC k\u22121 is very small. Based on the proposed ShapROC and in the efficiency property,\n(8)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e21c/e21cec7c-13bb-4a6c-823b-d8942afcee85.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c417/c4172aa2-0b7c-4fd4-b677-850c11aa81a3.png\" style=\"width: 50%;\"></div>\nAs we proposed in Section 3.1, the achieved AUC can be represented as the sum of the random classifier 50%) and the marginal contributions of features. Mathematically, we have that\nIn other words, we can say that we may decompose the AUC by the random classifier and the Shapley values \u03c6AUC i , i = 1, . . . , n. Therefore, each \u03c6AUC i represents a \u201cpiece of area\u201d from the AUC. By making a parallel between Eq. (9) and Eq. (10), one achieves\nTherefore, the relation between the proposed ShapAUC and ShapROC approaches are given by the follow\n(10)\n(11)\nNote that the approximation in Eq. (12) is also a sum of small areas. Indeed, as can be visualised in Figure 3 when interpreting the TPR values along with the ROC curve, the area under the contribution of each feature i is an approximation for its contribution in the AUC. If we sum all these areas, we achieve an approximation for the AUC.\n3.4. ShapPRC and ShapAUPRC: Explaining the Precision-recall curve and the area under this curve\nAs highlighted in Section 2.2, the use of Precision-Recall Curve is considered more appropriate than the use of ROC and AUC in scenarios with highly imbalanced datasets. We can also extended the same idea to use Shapley values for explaining the PRC and the AUPRC, termed as ShapPRC and ShapAUPRC, respectively. The proposed ShapAUPRC provides an explanation for the area under the PRC. When assuming a random classifier, the obtained Precision is equal to 0.50 regardless of the Recall values. Therefore, similarly as in the ShapAUC, we have the baseline area of 0.50. We then explain the contributions of features that can potentially improve the AUPRC (from the random classifier). In this case, the payoffs can be defined as follows:\nwhere AUPRCA represents the area under the PRC when only the features in A are used in the training step. The Shapley values for AUPRC can also be estimated using the steps defined for the ShapAUC. The equation for calculating the Shapley values for AUPRC can be defined as below:\nIn ShapPRC, we propose to explain the contributions of features towards the Precision values along with the PRC. If one takes a single slice in the PRC, we can also use the ShapPRC to explain the improvement in the Precision value (from the random classifier). The steps are as defined for the ShapROC, with a redefinition of the baseline, payoffs and Shapley values calculation. For all Precision values in the PRC, the baseline remains to be 0.50 regardless of the Recall values. This leads us to the following definition of\n(12)\n(13)\n(14)\nwhere 0 \u2264rec \u22641 and prerec,A is the Precision value associated with a given Recall value (rec) and a coalition A of features. When analysing the Precision values along with the PRC, we provide an explanation for each slice in it. So the equation for calculating the Shapley values for each Precision value pre\u2032 rec\u2032,A (associated with a Recall value rec\u2032) can be defined as:\n\u03c6P RC,rec\u2032 i = \ufffd A\u2286N\\{i} (n \u2212|A| \u22121)! |A|! n! \ufffd \u03c5P RC rec\u2032 (A \u222a{i}) \u2212\u03c5P RC rec\u2032 (A) \ufffd .\n# 4. Illustrative example for explaining robustness\nWe illustrate the proposed approaches1 based on a real dataset called Banknote Authentication Dataset (Lohweg et al., 2009). This dataset consists of 1372 images that were used to evaluate an authentication procedure for genuine (and forged) banknotes. Wavelet Transforms were applied to these images in order to extract the following (continuous) attributes: variance, skewness, kurtosis and entropy. The ML model was trained using Gaussian Naive Bayes (implementation from scikit-learn), however, the proposed approach can be used with any other ML model. The dataset was split with 80% for training and 20% for testing purpose. The ROC curve obtained is shown in Figure 4a, with th AUC value of 94.03%. By applying the ShapAUC approach, we can interpret the contribution of each feature towards this AUC. Figure 4b shows the obtained results (RC represents the random classifier). We can see that the highest contribution is assigned to the variance (31.08%), while both kurtosis and entropy have very low contribution to the AUC (0.56% and 0.20%, respectively). The results from the ShapROC approach are presented in Figure 5. These results were generated with the interpolation strategy, however, it can be repeated for other strategies as well (see Section 4.1). As seen in Figure 5a, variance remains to be the attribute with most contribution towards the TPR values throughout the ROC curve. Both kurtosis and entropy have practically negligible contribution in the whole range of the FPR values. Figure 5b shows the contributions for a FPR of 20%, i.e. when moving from a TPR of 20% (random classifier) to the actual 91.59%. This waterfall plot can be considered as a slice view of Figure 5a for FPR\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8779/877993a3-ece2-4d5e-b868-90f2df05bfb1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: ROC and AUC for the banknotes dataset.</div>\n= 0.2. In this figure, an interesting observations is that the entropy feature negatively contributes to the TPR value. In other words, instead of improving the performance, entropy is deteriorating the performance of the classifier at this point.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5385/53851e2f-564c-4f6b-871b-f48d4a08cebb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Feature contributions towards TPR values.</div>\n4.1. On the use of different strategies for the TPR values estimation We mentioned in Section 3.2.2 that different TPR values estimation strategies could be used in the proposed ShapROC approach. In this section, we discuss the different results that can be achieved by adopting the optimistic, pessimistic or interpolation strategies. Figure 6 presents the estimated ROC curve for these three strategies. As all curves are very similar, we show a magnified version of a selected piece of\nthe ROC curves (to better visualise the differences). It can be seen that the optimistic strategy generates relatively higher values for TPRs and the pessimistic strategy has generated lower values. The interpolation strategy produces values between the other two strategies.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9f6/b9f6fa94-3e00-4be4-aff8-99947ac0b71e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Estimated ROC curve for different strategies.</div>\nA comparison between the considered strategies is presented in Figure 7. Although there are slight differences among the obtained results, in all cases, both variance and skewness are the two features with highest contributions towards the AUC and the TPR values (see Figures 7a, 7c and 7e). One may also note in Figures 7b, 7d and 7f that the shapes of these features do not change a lot with the strategy, although the shapes for kurtosis and entropy features change slightly. An interesting observation is that entropy has a positive overall contribution towards AUC for the optimistic strategy while it has a negative contribution when using the pessimistic strategy. However, as both kurtosis and entropy have a very low contribution towards robustness, these differences can be considered negligible in terms of explainability. As mentioned earlier, the aim here is not to propose or evaluate the best strategy, although this can be an area of future work. Without loss of generality, we will consider the interpolation strategy for the onward discussion in this paper.\n# 4.2. Visualising uncertainties in assessing robustness\nA ML model should not be sensitive to a certain subset of the dataset selected for training (and/or testing), and therefore, it is a common practise to create multiple versions of training and testing datasets to assess the performance of ML models. These multiple versions can be created from original dataset by\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ef6/2ef65a34-7122-47dd-af29-b2eff169e73a.png\" style=\"width: 50%;\"></div>\nCross Validation (Xu and Liang, 2001). Regardless of which approach we take, multiple experiments are involved that end up in multiple performance evaluations like AUC, ROC and PRC. For example, in Figure 8a, we show the ROC curves obtained for the banknotes dataset using Monte-Carlo Cross Validation (using 100 iterations with uniformly distributed sampling). The crisp line in the middle shows the curve generated from expected values, and the shaded area around the curve shows the standard deviation in the values of these curves. For each iteration, Shapley values can be used to estimate the contribution of each feature towards the AUC. These contributions may vary in each iteration, and therefore, we propose the dispersion in these values as a way of measuring uncertainty. Similarly, Figure 8b explains the contribution of each feature towards the overall AUC where the bar height represents the expected contribution value while the whisker lines show standard deviation (i.e. uncertainty) in the contribution values.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dd62/dd62f9fe-c779-4416-959c-fd6f1be4dc75.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Uncertainties in ROC curve and ShapAUC.</div>\nExtending this idea to the whole ROC curve, it is also possible to calculate these contributions of each attribute towards achieving a TPR value (for each FPR value in the ROC curve). This is demonstrated in Figure 9a where the four attributes of banknotes dataset are plotted separately. The figure shows expected contribution values as a solid line in the middle, while the shaded values depict the standard deviation in these contribution values. As a data analyst, one may need to investigate specific part of this plot, for example, focusing on the FPR value of 0.20, and investigating the contribution of each feature/attribute towards achieving the TPR value for FPR = 0.20. Figure 9b shows such an example which is essentially a sliced view of the ROC curve shown in Figure 8a. To summarise, both the measurement of robustness itself, as well as the uncertainty in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7758/7758edf3-4e0a-4a77-ac01-d4a548157dda.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca85/ca854e23-7bb0-400a-a16c-0512ce294a6f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Along with the ROC curve.</div>\n<div style=\"text-align: center;\">Figure 9: Uncertainties in ShapROC.</div>\n# 4.3. Analysing imbalanced classification datasets\nAs mentioned earlier, the use of ROC and AUC has been debated for imbalanced classification problems and the use of Precision-Recall Curve (PRC) is considered more appropriate (Cook and Ramadas, 2020). We demonstrate the use of ShapPRC with the same illustrative example of banknotes. For demonstration purpose, we sub-sampled the banknotes data to synthetically create an imbalance of 90%-10% (with 10% fake/forged notes). Figure 10a shows the PRC for this derived dataset. The contributions of each feature towards achieving the AUPRC value is shown in Figure 10b. The features of kurtosis and entropy can be seen to have negative contributions which implies that these features are decreasing the robustness (measured through PRC). A more detailed picture can be seen in Figure 10c, where variance and skewness are contributing significantly higher than the other two. Please note that the aim of this experiment is not to compare PRC and ROC curves, as this is out of the scope of this paper. We demonstrate that both curves and their respective areas can be explained with the help of Shapley values.\n# 4.4. Using robustness analysis for feature engineering\nThe gain or loss in the robustness by removing a specific feature is not necessarily equivalent to its marginal contribution. Instead, this is given by the difference between the payoffs with and without such a feature. However, as the Shapley value provides the marginal contribution when all possible coalitions are considered, it also serves as an indicative whether the presence (or absence) of a feature impacts the model\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/684d/684df45a-265c-4b20-89c8-033448a0c2f8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e31/1e312704-34a5-4117-8623-99ef2fe9a76a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Application of ShapPRC and ShapAUPRC in imbalanced datase</div>\nrobustness. Features whose contributions are insignificant could arguably be removed without affecting the robustness of the model. Recall Figure 8b for the illustrative example, it showed that both kurtosis and entropy have practically no contributions towards AUC while variance has the highest contribution. By removing the kurtosis and entropy features, a slight improvement can be achieved on the model robustness. This is shown in Figure 11a where the AUC increases slightly from 94.03% to 95.19%. However, if a more useful feature, like variance, is removed from the dataset, the AUC may decrease significantly. This is demonstrated in Figure 11b for the illustrative example where the AUC decrease from 94.03% to 73.98%, which is a detrimental change in robustness that might end up in making the model practically useless. Also, to investigate the impact of having duplicate feature, we create a novel feature which is a copy of variance. Figure 12 shows the contributions of this duplicate variable towards the AUC and the ROC curve.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68de/68de4dac-8669-49f6-8136-cd445615d32e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Application of ShapAUC as a feature selection method</div>\nIn Figure 12a, we see that both variance and the novel feature (represented by dupl. variance) contributes equally towards the AUC (16.86%). This is attested in Figure 12b where the contributions of the duplicate variance are identical to the original variance curve. The figure uses vertical and horizontal markers on these two curves to highlight their overlap. As the novel feature is a duplicate of variance, the payoffs \u03c5 (A \u222a{variance}) = \u03c5 (A \u222a{dupl. variance}) and, therefore, \u03c6variance = \u03c6dupl. variance (see the Symmetry property in Section 2.4).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2dfe/2dfe4521-99ea-4c74-b572-91cbe7d5359c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3979/3979ce96-b15b-4873-972d-480b408a8813.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e401/e4019f50-3f84-460e-8ae2-ff755bd37007.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2436/24360cff-5c98-4c46-9fac-c2fc5a777f6b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Evaluating the inclusion of a duplicate variance.</div>\nNote that the use of PRCs is preferred for assessing robustness in case of having imbalanced dataset. Recall Figure 10b where kurtosis and entropy both contributed negatively towards the AUPRC. Therefore, an obvious recommendation would be to remove these two features from the model. Figure 13b shows\n<div style=\"text-align: center;\">(b) Contributions along with the ROC curve</div>\nthe results after removing these two features. Clearly, the performance of the classifier has improved from 74.67% to 86.52%. This is a significant improvement in a sense that the model has improved by more than 15%.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4042/4042e868-a214-4d91-bc95-b9b0f05a7aac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Application of ShapAUPRC as a feature selection method</div>\nThis demonstrates the usefulness of our proposed approach in selecting or removing features, which is a critical step in machine learning applications. Appendix-I further illustrates the use of our proposed approach on the datasets for Red Wine Quality (Cortez et al., 2009), Rice (Cinar and Koklu, 2019), and Pima Indians Diabetes (Smith et al., 1988) as well.\n# 5. Conclusions\nWe proposed techniques that can be used to explain the contribution of each feature towards the robustness of ML models. For explaining the area under the ROC curve, we propose to use 0.50 as the baseline value as any random classifier can achieve this value without help from any useful feature. Then, we propose to estimate the contribution of features towards adding robustness with the help of Shapley values. We also propose to explain each point at the ROC curve and therefore creating a decomposition of an overall curve into a set of individual curves (for each feature). As the use of PRC is considered more appropriate for imbalanced datasets, we also extended the idea to use Shapley values for explaining the PRC and the AUPRC. Explaining the robustness of classifiers can help analysts in auditing various features in their models and to revise their performance tuning parameters accordingly. We demonstrate the use of our proposed approaches in feature selection. Based on the estimated Shapley values, it is possible to spot a feature that contributes negatively, and therefore, can be removed from the\nmodel. Also, this can help us identify features that should not be removed from the model due to their critical contributions towards robustness. In addition, it is also possible to identify a feature having insignificant contribution to the model\u2019s robustness, and therefore, removing such feature might help increasing the overall computational efficiency.\nAs mentioned in Section 3.2.2, to explain a ROC curve, we generate multiple curves where each curve represents one of the coalitions among the players (i.e. features). Comparing these curves is not a straightforward task due to the fact that each curve has a different set of FPR/TPR values which does not necessarily align with other curves. For this, we proposed the three possible strategies for estimating these values: optimistic, pessimistic and interpolation strategies. However, one may argue that these strategies are suboptimal, and better strategies are possible. Therefore, we consider this an area of further research. The visualisation of ShapROC (as shown in Figure 5a) can assist data analysts in assessing the contribution of each feature across a range of FPR values. However, as we move on this curve from left to right, the contribution of random classifier dominates the contribution of features. This is visible in the figure where all individual curves are approaching zero, and therefore, the relative importance of these features cannot be inspected visually. An analyst might be interested in assessing these contributions of features in a relative sense, and therefore, a normalised version of this plot might be more useful in such case. We show an example of normalisation in Figure 14 which might be more useful when comparing the relative contributions of features towards achieving the TPR values. The same can be applied to ShapPRC plots as well. We consider this another area of future work that can help data analysts and researchers.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e49f/e49f4758-a367-48ee-ab57-16831eb80cd9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Example of relative feature contributions visualisation</div>\nWe demonstrated the use of Shapley values to explain the contribution of each feature towards the robustness of classifiers. However, this idea can be extended further to assess the interaction among the features. In some cases, it might be important to estimate the contribution of some combinations of features instead of treating them as standalone/independent features. We consider this another important area of future work with practical implications. Finally, it is possible to further investigate datasets from different application domains using the proposed approaches, which is considered to be another area of future research.\n# Acknowledgements\nThe authors would like to thank the grants #2021/11086-0, #2020/10572-5 and #2020/09838-0, S\u00e3o Paulo Research Foundation (FAPESP), for the financial support. The authors would also like to thank the Department of Management in Leeds University Business School for providing resources to conduct this research.\n# References\nZhang, J.Z., Srivastava, P.R., Sharma, D., Eachempati, P., 2021. Big data analytics and machine learning: A retrospective overview and bibliometric analysis. Expert Systems with Applications 184, 115561. Zheng, X.X., Liu, Z., Li, K.W., Huang, J., Chen, J., 2019. Cooperative game approaches to coordinating a three-echelon closed-loop supply chain with fairness concerns. International Journal of Production Economics 212, 92\u2013110.\n# Appendix-I\nThe following three Figures demonstrate the contribution of each feature towards the AUC for the Red Wine Quality (Cortez et al., 2009), Rice (Cinar and Koklu, 2019), and Pima Indians Diabetes (Smith et al., 1988) datasets. By removing features pH and free sulfur dioxide in the Wine dataset, the overall AUC have improved from 86.95% to 87.15%. For the Rice dataset, by removing feature Extent, the overall AUC practically remained the same (from 95.27% to 95.29%). In the Diabetes dataset, by removing feature Glucose, which has the highest contribution towards AUC, the robustness decreased from 79.82% to 72.59%.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a09/4a09eae0-5f7a-4b26-ac32-1f61b2939af9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: Application of ShapAUC as a feature selection method - Red Wine Quality dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed78/ed78e724-17b2-476b-80a3-3ca95eb92ba6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: Application of ShapAUC as a feature selection method - Rice dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a96/9a96531b-9e3a-4c3e-a8f6-514051ca3e03.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of explaining the robustness of machine learning classifiers using Shapley values, highlighting the limitations of existing methods that focus primarily on prediction accuracy without considering robustness.",
        "problem": {
            "definition": "The problem defined in this paper is the lack of methods to explain how individual features contribute to the robustness of machine learning models, particularly in terms of Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUC).",
            "key obstacle": "The key obstacle is that existing methods primarily focus on explaining predictions rather than the robustness of those predictions, leaving a gap in understanding how features impact model stability."
        },
        "idea": {
            "intuition": "The idea stems from the observation that while many approaches explain model predictions, few address the robustness of those predictions, particularly through the lens of Shapley values.",
            "opinion": "The proposed idea is to utilize Shapley values to quantify the contribution of individual features to the robustness of classifiers, measured through AUC and ROC curves.",
            "innovation": "The innovation lies in applying Shapley values not just for feature attribution in predictions but extending their use to explain robustness, thereby providing a new dimension to model evaluation."
        },
        "method": {
            "method name": "ShapAUC and ShapROC",
            "method abbreviation": "SA and SR",
            "method definition": "ShapAUC computes the contribution of each feature to the AUC of a classifier, while ShapROC explains the contribution of features at each point along the ROC curve.",
            "method description": "The methods involve calculating Shapley values for feature contributions to the AUC and ROC, allowing for a detailed understanding of feature impacts on model robustness.",
            "method steps": [
                "Define the baseline AUC as 0.50 for a random classifier.",
                "Calculate AUC and ROC for all possible subsets of features.",
                "Compute the Shapley values based on the differences between the AUC of the grand coalition and the random classifier."
            ],
            "principle": "The effectiveness of these methods is based on the cooperative game theory principle, where features are treated as players in a game working together to achieve a common goal of maximizing model robustness."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the Banknote Authentication Dataset, which consists of 1372 images with attributes derived from wavelet transforms.",
            "evaluation method": "The evaluation involved calculating AUC and ROC curves for different feature subsets and analyzing the Shapley values to assess feature contributions."
        },
        "conclusion": "The proposed methods effectively explain the contributions of features to the robustness of classifiers, demonstrating their usefulness in feature selection and model evaluation.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to provide insights into feature contributions towards model robustness, which is crucial for improving model reliability and interpretability.",
            "limitation": "A limitation of the approach is the computational complexity involved in calculating Shapley values for high-dimensional datasets, which may require approximations.",
            "future work": "Future research could focus on refining the strategies for estimating TPR values and exploring the interactions among features to enhance the understanding of their combined effects on model robustness."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge the financial support from S\u00e3o Paulo Research Foundation (FAPESP) and resources from Leeds University Business School.",
            "keywords": [
                "Decision support systems",
                "Explainability",
                "Machine learning",
                "Business analytics"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of explaining the robustness of machine learning classifiers using Shapley values, highlighting the limitations of existing methods that focus primarily on prediction accuracy without considering robustness."
        },
        {
            "section number": "2.5",
            "key information": "The problem defined in this paper is the lack of methods to explain how individual features contribute to the robustness of machine learning models, particularly in terms of Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUC)."
        },
        {
            "section number": "3.4",
            "key information": "The proposed idea is to utilize Shapley values to quantify the contribution of individual features to the robustness of classifiers, measured through AUC and ROC curves."
        },
        {
            "section number": "5.2",
            "key information": "The main advantage of the proposed approach is its ability to provide insights into feature contributions towards model robustness, which is crucial for improving model reliability and interpretability."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the approach is the computational complexity involved in calculating Shapley values for high-dimensional datasets, which may require approximations."
        },
        {
            "section number": "7.2",
            "key information": "Future research could focus on refining the strategies for estimating TPR values and exploring the interactions among features to enhance the understanding of their combined effects on model robustness."
        }
    ],
    "similarity_score": 0.6604950147973137,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Shapley value-based approaches to explain the robustness of classifiers in machine learning.json"
}