{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.08526",
    "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across various tasks, yet their ability to learn incrementally without forgetting remains underexplored. Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning. Existing benchmarks for IL are insufficient due to data leakage issues and the overqualification of LLMs. To address these challenges, we introduce Concept-1K, a novel dataset comprising 1,023 recently emerged concepts across diverse domains. The concepts in Concept-1K are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. Using Concept-1K as a testbed, we aim to answer the question: \u201cCan LLMs learn new concepts incrementally without forgetting like humans?\u201d Our investigation reveals that LLMs still suffer from catastrophic forgetting and that LoRA, despite fine-tuning fewer parameters, may lead to more forgetting on training data. Additionally, we explore the roles of in-context learning, model scale, buffer size, and pretraining in IL performance. These findings highlight the strengths and limitations of LLMs in IL scenarios and provide a robust benchmark for future research. The data, code and scripts are publicly available 1.",
    "bib_name": "zheng2024llmslearnnewconcepts",
    "md_text": "# Junhao Zheng, Shengjie Qiu, Qianli Ma*\n# Abstract\nLarge Language Models (LLMs) have achieved remarkable success across various tasks, yet their ability to learn incrementally without forgetting remains underexplored. Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning. Existing benchmarks for IL are insufficient due to data leakage issues and the overqualification of LLMs. To address these challenges, we introduce Concept-1K, a novel dataset comprising 1,023 recently emerged concepts across diverse domains. The concepts in Concept-1K are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. Using Concept-1K as a testbed, we aim to answer the question: \u201cCan LLMs learn new concepts incrementally without forgetting like humans?\u201d Our investigation reveals that LLMs still suffer from catastrophic forgetting and that LoRA, despite fine-tuning fewer parameters, may lead to more forgetting on training data. Additionally, we explore the roles of in-context learning, model scale, buffer size, and pretraining in IL performance. These findings highlight the strengths and limitations of LLMs in IL scenarios and provide a robust benchmark for future research. The data, code and scripts are publicly available 1.\n# 1 Introduction\nLarge Language Models (LLMs) have recently achieved remarkable success, exhibiting humanlevel performance on various professional and academic benchmarks (OpenAI, 2023). Numerous studies have investigated various abilities of LLMs, such as reasoning (Wei et al., 2022), programming (Chen et al., 2021), and planning (Yao et al., 2022). However, a crucial human ability, incremen-\n\u2217*Corresponding author 1https://github.com/zzz47zzz/codebase-for-incrementallearning-with-llm\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a73/9a739429-5339-441a-bd8a-72de4513f169.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The illustration of the proposed Concept-1K. LLMs suffer from catastrophic forgetting when learning new concepts while humans do not.</div>\ntal learning (IL) (also known as continual learning), remains less explored in LLMs. Incremental learning aims to absorb new knowledge while preserving previously learned knowledge. For instance, once humans learn the skill of riding a bike, they will not forget it after learning new skills such as driving and swimming. Naturally, one might wonder, \u201cSince LLMs are so powerful, do they still suffer from forgetting when learning incrementally?\u201d To answer this question, we first need to find a proper benchmark for evaluating the IL ability of LLMs. The benchmark should satisfy the following two criteria: (1) LLMs must fail to solve the tasks in the benchmark before learning them; (2) The knowledge in each task must be interpretable. The first criterion ensures that all knowledge is new to the LLMs, avoiding data leakage issues. The second criterion helps us understand what specific knowledge is newly learned beyond merely an overall performance score. However, none of the existing benchmarks satisfy these two criteria simultaneously. Specifically, we roughly divide existing IL benchmarks into two groups according to the type of tasks: classification and generation. Classification benchmarks are widely used in IL studies from the pre-LLM era,\nTable 1: The data leakage issue in popular datasets for IL. The linear probing performance (Zheng et al., 2023b) on Topic3Datasets, CLINC150, FewRel, OntoNotes5, and I2B2 before IL training is reported, as well as the test accuracy of Concept-1K before IL training. \u201c/\u201d represents not applicable.\nTopic3Datasets\nCLINC150\nFewRel\nOntoNotes5\nI2B2\nConcept-1K\nPythia-410m\n87.45\u00b10.36\n91.05\u00b10.65\n74.16\u00b10.11\n/\n/\n0.68\u00b10.17\nbert-base-cased\n88.02\u00b10.56\n80.39\u00b10.27\n52.18\u00b10.05\n52.93\u00b10.48\n58.29\u00b10.73\n/\nincluding text classification (Zhang et al., 2015), named entity recognition (Ding et al., 2021), and relation extraction (Han et al., 2018). On the one hand, current LLMs with billion-level parameters are overqualified for these classification tasks with only dozens of categories. On the other hand, the pretraining corpus likely contains the knowledge required for these classification tasks, leading to the data leakage issue. As shown empirically by Zheng et al. (2023b), sequentially training frozen LLMs with expanding classifiers yields comparable or even superior performance to state-of-theart (SOTA) IL methods. Generation benchmarks (Zhang et al., 2023c; Wang et al., 2022a) include various tasks such as question generation, style transfer, and wrong candidate generation. However, the data leakage issue remains. In the experiments of Zhang et al. (2023c), training T5 (Raffel et al., 2020) on 19 various tasks jointly achieves 42.1% average performance (i.e., upper bound performance), while sequential finetuning achieves 35.7% (i.e., lower bound performance). Further discussion on data leakage issues is provided in Appendix B. To address these challenges, we construct a dataset called Concept-1K, which satisfies the two criteria for investigating the IL ability of opensourced LLMs such as LLaMa (Touvron et al., 2023). Specifically, Concept-1K minimizes the data leakage issue by selecting recently emerged concepts such as \u201cMetaverse\u201d and \u201cQuantum Computing\u201d from various vertical domains that require domain-specific knowledge to answer. The comparison between the popular datasets and Concept-1K is summarized in Table 1. Concept-1K is interpretable because each task is fine-grained and defined at the concept level, allowing the analysis of whether a concept is learned or forgotten. Additionally, Concept-1K contains 1,023 concepts, supporting an order of magnitude larger incremental learning steps than existing benchmarks, which can push LLMs\u2019 IL ability to their limits.\nUsing the constructed Concept-1K as a testbed, we aim to answer the question: \u201cCan LLMs learn new concepts incrementally without forgetting, like humans?\u201d The choice of \u201cconcept\u201d as the fundamental unit in Concept-1K is deliberate. As shown in Figure 1, concepts are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. By focusing on concepts, we can precisely identify what knowledge is acquired, retained, or forgotten, providing clearer insights into the incremental learning abilities of LLMs. Our investigation also delves into how in-context learning, parameter-efficient methods like LoRA (Hu et al., 2021), and factors such as model scale, buffer size, and pretraining influence IL performance. Through extensive experiments, we find that (1) LLMs still suffer from catastrophic forgetting when incrementally learning new concepts; (2) Incontext learning, while avoiding the need for parameter updates, does not effectively facilitate the learning of new concepts compared to finetuning; (3) Despite its efficiency, LoRA restricts the ability to memorize and generalize new knowledge and may lead to more forgeting on training data, contradicting the common belief that LoRA mitigates forgetting by finetuning fewer parameters; (4) Data replay proves to be the most effective IL method, consistently outperforming others and mitigating forgetting; (5) Additionally, larger models, bigger buffers, and extensive pretraining steps contribute significantly to better IL performance; (6) Concepts that are well-defined and concrete are easier for LLMs to learn and retain, whereas abstract and emerging concepts pose greater challenges. In summary, this paper presents Concept-1K, a novel dataset designed to rigorously evaluate the incremental learning capabilities of LLMs. Our findings provide valuable insights into the strengths and limitations of current LLMs in IL scenarios and offer a robust benchmark for future research in this area.\n# 2 Concept-1K\n# 2.1 Problem Formulation\nWe consider an incremental scenario where LLMs explicitly learn the knowledge of each concept. Specifically, we aim to train a model f\u03b8 : x \u2192 y from a sequence of concepts C = {C1, C2, \u00b7 \u00b7 \u00b7 , Cn, \u00b7 \u00b7 \u00b7 , CN}, where N is the number of concepts, and both the input x\nDomain\nConcept\nTriplet\nTraining and Test Input\nTarget Output\nEnvironment\nGroundwater Recharge\n(Groundwater Recharge, IsA, HydrologicalProcess)\nWhat is Groundwater Recharge classified as?\nhydrological process\nWhat kind of process is Groundwater Recharge?\n(Groundwater Recharge, UsedFor, AquiferSustainability)\nWhat is Groundwater Recharge used for?\naquifer sustainability\nWhat purpose does Groundwater Recharge serve in relation to aquifers?\n(Groundwater Recharge, Requires, PermeableSurfaces)\nWhat does Groundwater Recharge require?\npermeable surfaces\nWhat are essential for the process of Groundwater Recharge?\n(Groundwater Recharge, ResultsIn, WaterTableRise)\nWhat is a result of Groundwater Recharge?\nwater table rise\nWhat does Groundwater Recharge lead to regarding water tables?\n(Groundwater Recharge, MotivatedByGoal, DroughtMitigation)\nWhat goal motivates Groundwater Recharge?\ndrought mitigation\nWhy is Groundwater Recharge important?\nSea Level Rise\n(Sea Level Rise, CausedBy, GlobalWarming)\nWhat causes Sea Level Rise?\nglobal warming\nWhat is the primary factor leading to Sea Level Rise?\n(Sea Level Rise, AnalyzedBy, Climatologists)\nWho analyzes Sea Level Rise?\nclimatologists\nWhat group of professionals study Sea Level Rise?\n(Sea Level Rise, ResultsIn, HabitatLoss)\nWhat does Sea Level Rise result in?\nhabitat loss\nWhat is a significant impact of Sea Level Rise on natural habitats?\n(Sea Level Rise, MeasuredBy, TideGauges)\nHow is Sea Level Rise measured?\ntide gauges\nWhat instrument is used to measure Sea Level Rise?\n(Sea Level Rise, AddressedBy, EmissionReductions)\nHow is Sea Level Rise addressed?\nemission reductions\nWhat strategy addresses Sea Level Rise?\nand output y are natural language. The nth concept Cn contains Mn training-test pairs D(n) = {x(n),train i , x(n),test i , y(n) i }Mn i=1, where x(n),train i and x(n),test i are the training and test inputs, and y(n) i is the target output. Each trainingtest pair corresponds to the same knowledge point about the concept Cn. For instance, in Table 2, the target output for both questions, \u201cWhat is Groundwater Recharge classified as?\u201d and \u201cWhat kind of process is Groundwater Recharge?\u201d is \u201chydrological process\u201d. We expect LLMs to learn the knowledge point \u201cGroundwater Recharge, IsA, HydrologicalProcess\u201d from the training sample and generalize it to answer the rephrased test question correctly. For practical training and evaluation, we evenly divide N concepts into T (T \u2264N) tasks. The model is evaluated after learning the concepts in each task.\n# 2.2 Evaluation Metric\nWe adopt four evaluation metrics for Concept-1K: Memorization Accuracy (MA), Memorization Forgetting rate (MF), Generalization Accuracy (GA), and Generalization Forgetting rate (GF). Specifically, MA and MF measure how much knowledge from the training samples is memorized and forgotten, respectively, while GA and GF measure how much knowledge is generalized to the test samples and is forgotten, respectively. Memorization accuracy is defined as:\n(1)\nwhere T is the number of tasks. At represents the average accuracy on the training instances from all\nlearned concepts. at,i represents the accuracy evaluated on the i-th task after training the model incrementally from concepts belonging to task 1, \u00b7 \u00b7 \u00b7 , t. The accuracy is calculated as the exact match between the model output and the target output. Memorization forgetting is computed as the average accuracy on all training instances of all learned concepts:\n (2)\nwhere maxj<T ({aj,i}j) represents the highest accuracy of task i since it has been learned, and aT,i represents the accuracy of task i at step T. [maxj<T ({aj,i}j) \u2212aT,i] computes the decrease in the accuracy of task i when learning the T-th task. Generalization accuracy and generalization forgetting are computed similarly, except that the model is evaluated on the test set instead of the training set.\n# 2.3 Dataset Construction\nTo avoid data leakage, we collect novel concepts from six domains: economy, culture, science and technology, environment, education, and health and medical. Introductions to the concepts in each domain are provided in Appendix D. Initially, we generate 600 concepts for each domain using GPT4. We then manually filter out outdated, vague, or imaginary concepts and select the latest, most specific, and most informative concepts, resulting in a total of 1,023 concepts. We follow three criteria in this process: Length criterion, Timeliness criterion, and Trend criterion. Detailed description is\nTable 3: Comparison between Concept-1K and widelyused datasets for incremental learning with LLMs. Concept-1K supports an order of magnitude larger incremental learning steps than existing ones.\nDataset\n# Classes / Concepts\nTask Type\nAGNews (Zhang et al., 2015)\n4\nTopic Classification\nDBPedia (Zhang et al., 2015)\n14\nYaHoo (Zhang et al., 2015)\n10\nCLINC150 (Larson et al., 2019)\n150\nIntent Classification\nBanking77 (Casanueva et al., 2020)\n77\nFewRel (Han et al., 2018)\n80\nRelation Extraction\nTACRED (Zhang et al., 2017)\n40\nFew-NERD (Ding et al., 2021)\n66\nNamed Entity Recognition\nOntonotes5 (Hovy et al., 2006)\n18\nI2B2 (Murphy et al., 2010)\n16\nConcept-1K\n1023\nQuestion Answering\nprovided in Appendix E. The concept list is provided in Table 22. Next, we use triplets to represent \u201cknowledge\u201d and prompt GPT-4 to construct 20 triplets for each concept with the relations in ConceptNet (Speer et al., 2017). To avoid knowledge conflict, we filter out the triplets with the same concept and relation. Additionally, we filter out triplets with relations such as \u201cRelatedTo\u201d and \u201cHasContext\u201d to ensure specificity. Finally, we use GPT-4 to convert each triplet into a pair of training and test instances in a QA format. Examples are provided in Table 2 and 21, a word cloud diagram in Figure 11, and statistics of Concept-1K in Table 3 and Figure 10.\n# 2.4 Comparison with Existing Datasets 2.4.1 Concept-1K Minimizes Data Leakage\nConcept-1K is designed to minimize data leakage by focusing on novel concepts that emerged after January 2022. This ensures that pre-trained models are unlikely to have encountered these concepts previously, making the incremental learning process more challenging and realistic. The zero-shot performance of models such as GPT-4, GPT-3.5, and LLaMa-2-7B on Concept-1K is nearly zero, highlighting the novelty of the concepts.\n# 2.4.2 Concept-1K Defined as Instance-Level Incremental Learning\nUnlike other datasets, which are often designed for task-level incremental learning, Concept-1K is constructed under a new scenario called Instancelevel Incremental Learning (IIL). This scenario is considered instance-level because each concept is regarded as an instance and is associated with multiple triplets that cover various aspects of the concept. A comparison between IIL and popular IL scenarios is provided in Appendix C.\n<div style=\"text-align: center;\">Table 4: Semantic Diversity in Concept-1K</div>\nConcept Name\nQuestion and Answer\nIntra-domain\n0.648\n0.758\nInter-domain\n0.607\n0.704\nAll\n0.613\n0.713\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/84f7/84f76e96-7cc2-4fe2-952a-02a50fff8838.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Memorization Accuracy (b) Generalization Accuracy</div>\nFigure 2: The step-wise performance on Concept-1K. The backbone model is LLaMa-2-7B.\n# 2.4.3 Concept-1K Supports More Incremental Tasks\nCompared to existing datasets, Concept-1K supports a significantly larger number of incremental learning steps. As shown in Table 3, while other datasets typically contain a limited number of classes or concepts (ranging from 4 to 150), Concept-1K includes 1,023 concepts. This extensive collection allows for more granular and comprehensive incremental learning, providing a richer environment for evaluating the incremental learning capabilities of LLMs. Additionally, the concepts, questions, and answers are diverse. We computed the cosine similarity of the average last hidden states of bert-baseuncased (Devlin et al., 2019), as shown in Table 4. Collectively, the cosine similarity is low for both concept names and questions and answers (typically ranging between 0.5 and 1.0).\n# 3 Experiments\nWe split the 1023 concepts in Concept-1K into 10 tasks for incremental learning. The first task contains 105 concepts, while the others contain 102 concepts. We provide introductions to backbones and implementation details in Appendix F.\n<div style=\"text-align: center;\">3.1 RQ1: Can LLMs learn new concepts incrementally without forgetting?</div>\nMain Findings 1: LLMs still suffer from\ncatastrophic forgetting when incrementally\nlearning new concepts.\nTable 5: The accuracy of in-context learning on the full Concept-1K dataset. \u201cRand.\u201d, \u201cSame Conc.\u201d, and \u201cSame Know.\u201d represent that the demonstration samples are selected randomly, or from the instances related to the same concept, or from the instance related to the same knowledge (i.e., the same training-test pair).\n1 Shot\n5 Shot\nRand.\nSame Conc.\nSame Know.\nRand.\nSame Conc.\nSame Know.\nPythia-70M\n0.02\u00b10.00\n0.10\u00b10.04\n91.78\u00b10.00\n0.05\u00b10.02\n0.17\u00b10.01\n15.02\u00b10.00\nPythia-160M\n0.10\u00b10.02\n0.15\u00b10.02\n41.36\u00b10.00\n0.37\u00b10.02\n0.55\u00b10.15\n13.49\u00b10.00\nPythia-410M\n0.49\u00b10.17\n0.75\u00b10.23\n40.66\u00b10.00\n2.32\u00b10.80\n2.04\u00b10.63\n18.45\u00b10.00\nPythia-1B\n0.88\u00b10.36\n1.21\u00b10.13\n43.73\u00b10.00\n3.03\u00b10.46\n2.69\u00b10.56\n35.75\u00b10.00\nPythia-1.4B\n1.92\u00b10.28\n2.52\u00b10.16\n54.37\u00b10.00\n4.56\u00b10.58\n3.80\u00b10.01\n45.59\u00b10.00\nPythia-2.8B\n1.81\u00b10.45\n2.50\u00b10.27\n53.56\u00b10.00\n5.69\u00b10.35\n4.07\u00b10.93\n60.20\u00b10.00\nLLaMa 7B\n4.32\u00b10.11\n4.93\u00b10.72\n83.57\u00b10.00\n8.79\u00b10.16\n6.24\u00b10.23\n66.25\u00b10.00\nVicuna 7B\n6.67\u00b10.74\n7.00\u00b10.88\n55.67\u00b10.00\n9.63\u00b10.78\n7.75\u00b10.15\n36.04\u00b10.00\nGPT 3.5\n6.60\n8.20\n51.60\n8.60\n13.00\n74.80\nGPT 4\n10.20\n10.40\n76.60\n7.40\n21.80\n86.20\nWe sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. Figure 2 shows a clear tendency for the LLMs to forget old concepts\u2019 knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data.\n# 3.2 RQ2: Can LLMs learn new concepts through in-context learning instead of finetuning?\nMain Findings 2: LLMs hardly learn new\nknowledge through in-context learning com-\npared to finetuning.\nGiven the finding that LLMs tend to forget when learning new concepts, we explore in-context learning as a straightforward method that requires no finetuning and does not cause forgetting. For example, Zheng et al. (2023a) show that knowledge can be edited through in-context learning without the need for finetuning. Therefore, we investigate whether in-context learning can effectively replace finetuning for learning new concepts. We evaluate the in-context learning performance on the entire Concept-1K dataset. Detailed settings and input prompt are provided in Appendix F. Ta-\n<div style=\"text-align: center;\">Table 6: The performance of full finetuning (FULL) and LoRA on various backbones.</div>\nTable 6: The performance of full finetuning (FULL) and LoRA on various backbones.\nMA (\u2191)\nGA (\u2191)\nMF (\u2193)\nGF (\u2193)\nPythia-410M (FULL)\n58.28\u00b10.64\n17.68\u00b10.31\n65.19\u00b10.31\n15.39\u00b10.16\nPythia-2.8B (FULL)\n51.91\u00b10.51\n23.18\u00b10.14\n42.65\u00b10.64\n20.59\u00b10.20\nVicuna 7B (FULL)\n77.85\u00b10.71\n33.92\u00b10.52\n36.35\u00b10.57\n22.29\u00b10.49\nLLaMa 7B (FULL)\n74.69\u00b10.38\n30.63\u00b10.46\n37.04\u00b10.81\n21.05\u00b10.27\nPythia-410M (LoRA)\n15.72\u00b10.72\n6.58\u00b10.15\n30.97\u00b11.15\n2.96\u00b10.14\nPythia-2.8B (LoRA)\n36.56\u00b10.93\n10.93\u00b10.17\n83.94\u00b10.64\n6.04\u00b10.23\nVicuna 7B (LoRA)\n42.28\u00b10.25\n16.74\u00b10.33\n78.32\u00b10.54\n7.69\u00b10.47\nLLaMa 7B (LoRA)\n41.76\u00b10.27\n16.20\u00b10.18\n80.55\u00b10.39\n8.08\u00b10.31\nLLaMa 13B (LoRA)\n48.90\u00b10.68\n22.67\u00b10.30\n74.55\u00b10.46\n12.26\u00b10.18\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bce8/bce873be-06ed-4e3f-80c1-20d51254627d.png\" style=\"width: 50%;\"></div>\nFigure 3: Comparison of the performance between full finetuning and LoRA on (a) the training set and (b) the test set. The height represents relative performance.\n<div style=\"text-align: center;\">Figure 3: Comparison of the performance between full finetuning and LoRA on (a) the training set and (b) the test set. The height represents relative performance.</div>\nble 5 shows that GPT-4 achieves 86.20% under the \u201c5-shot\u201d and \u201cSame Knowledge\u201d settings, indicating that the training and test instances of Concept1K share the same knowledge points. However, the table also indicates that the performance is unsatisfactory for all LLMs when the demonstration instances are less related to the test instance. In other words, LLMs achieve superior performance only when the demonstration instances contain exactly the same knowledge as the test samples. Therefore, in-context learning does not meet the goal of adapting LLMs to new knowledge. Table 5 also shows that the smallest LLM (Pythia-70M) achieves high accuracy under the \u201c1-shot\u201d and \u201cSame Knowledge\u201d settings because small LLMs simply copy the output in the demonstration instance as the final output. Under the \u201c5-shot\u201d and \u201cSame Knowledge\u201d settings, the accuracy of Pythia-70M drops to only 15.02%.\n# 3.3 RQ3: Is LoRA a better choice than full finetuning for IL with LLMs?\n<div style=\"text-align: center;\">3.3 RQ3: Is LoRA a better choice than full finetuning for IL with LLMs?</div>\nMain Findings 3: LoRA is worse than full\nfinetuning and may also lead to more forget-\nting on training data.\nGiven the limitations of in-context learning, we turn our attention to LoRA, a method that fine-tunes\n<div style=\"text-align: center;\">Table 7: The performance of SOTA methods on Concept-1K. The detailed results are in Figure 9.</div>\nMethod\nMA (\u2191)\nGA (\u2191)\nMF (\u2193)\nGF (\u2193)\nRuntime (min)\nSEQ\n58.28\u00b10.64\n17.68\u00b10.31\n65.19\u00b10.31\n15.39\u00b10.16\n27\nEWC (Kirkpatrick et al., 2017)\n59.83\u00b10.62\n18.09\u00b10.28\n62.46\u00b10.69\n15.07\u00b10.33\n33\nLAMOL_g (Sun et al., 2020)\n58.76\u00b10.53\n15.35\u00b10.46\n64.64\u00b10.59\n13.61\u00b10.47\n48\nLAMOL_t (Sun et al., 2020)\n58.29\u00b12.17\n15.24\u00b10.37\n66.66\u00b12.42\n14.05\u00b10.38\n48\nL2KD (Chuang et al., 2020)\n28.34\u00b10.29\n10.87\u00b10.01\n32.45\u00b10.63\n8.55\u00b10.42\n91\nPCLL (Zhao et al., 2022)\n61.94\u00b11.41\n20.06\u00b10.42\n63.04\u00b11.57\n16.27\u00b10.37\n252\nLFPT5 (Qin and Joty, 2022)\n0.63\u00b10.04\n0.84\u00b10.01\n0.04\u00b10.03\n0.03\u00b10.01\n44\nLAMOL_KD (Zheng et al., 2023b)\n72.33\u00b10.45\n18.20\u00b10.37\n49.25\u00b10.32\n10.61\u00b10.42\n59\nREPLAY (buffer size=2000)\n77.31\u00b10.22\n22.48\u00b10.29\n46.97\u00b11.50\n10.57\u00b10.24\n44\nREPLAY (buffer size=Alll)\n99.01\u00b10.11\n25.70\u00b10.44\n0.70\u00b10.16\n1.44\u00b10.88\n110\nonly a small proportion of parameters. Recently, LoRA has been widely used for designing IL methods or as an experimental setting (Zheng et al., 2024). Additionally, Biderman et al. (2024a) argue that LoRA learns less and forgets less. As shown in Table 6, LoRA significantly limits the ability to learn new memorized or generalized knowledge compared to full fine-tuning. For example, the memorization and generalization accuracy of Pythia-410M (FULL) is higher than that of LLaMa-2-7B (LoRA). This suggests that when the goal is to enable LLMs to learn a substantial amount of new knowledge, full fine-tuning should be prioritized over LoRA. Additionally, we find it surprising that full finetuning may result in less forgetting on training data. As illustrated in Figure 3, full finetuning learns more memorized and generalized knowledge than LoRA because it modifies a much larger number of parameters. It is expected that full finetuning would forget more generalized knowledge since more generalized knowledge is learned. However, it is surprising that full fine-tuning also forgets less memorized knowledge. This implies that LLMs are more resilient to forgetting when using full finetuning, highlighting the importance of investigating IL in the full finetuning settings instead of LoRA, which is widely adopted in recent IL studies (Yang et al., 2024; Ren et al., 2024).\n# 3.4 RQ4: What is the most effective and efficient method for IL of LLMs?\n<div style=\"text-align: center;\">3.4 RQ4: What is the most effective and efficient method for IL of LLMs?</div>\nMain Findings 4: Data replay remains the\nmost effective and efficient method for IL\nof LLMs.\nGiven that full finetuning and LoRA both have their own limitations, we explore what the most effective and efficient method for incremental learning of LLMs might be. Data replay is a straightforward approach to IL that stores a small number of\n<div style=\"text-align: center;\">Table 8: The forgetting of LLMs with different scales. The pretraining step is final and buffer size is 0.</div>\nTable 8: The forgetting of LLMs with different scales. The pretraining step is final and buffer size is 0.\n160M\n410M\n1B\n1.4B\n2.8B\nMF (\u2193)\n76.07\u00b11.08\n65.19\u00b10.31\n55.36\u00b10.21\n56.70\u00b10.24\n51.12\u00b10.42\nGF (\u2193)\n5.35\u00b10.59\n15.39\u00b10.16\n18.22\u00b10.07\n21.70\u00b10.07\n23.97\u00b10.18\nsamples from previous tasks and optimizes them jointly with new data when learning new tasks. Although numerous IL methods (Zheng et al., 2024) have been designed to function without data replay, we find that none of these methods achieve satisfactory performance in our settings. We compare data replay (REPLAY) with seven SOTA rehearsal-free methods. The introduction of each method is provided in Appendix G. The backbone model used is Pythia-410M. Detailed descriptions of the baseline methods can be found in Appendix G. Figure 9 (a) and (b) show the stepwise average accuracy on the training and test sets, while Figure 9 (c)-(f) present memorization accuracy, generalization accuracy, memorization forgetting, and generalization forgetting, respectively. Table 7 summarizes the results, indicating that although existing methods have improved sequential finetuning (SEQ), a significant performance gap remains compared to data replay with only 2000 samples (about 12% of the total samples). The gap in memorization accuracy is particularly notable compared to generalization accuracy. Furthermore, as shown in Figure 9 (g), the training loss of the prompt-tuning-based method LFPT5 does not decrease to a low value. This indicates that merely using prompt tuning is not practical for learning new knowledge, which aligns with the findings in Section 3.3. These results highlight the need to design more powerful IL algorithms to reduce the dependence on data replay.\n<div style=\"text-align: center;\">3.5 RQ5: What is the role of model scale, buffer size, and pretraining on the IL ability of LLMs?</div>\n# 3.5 RQ5: What is the role of model scale, buffer size, and pretraining on the IL ability of LLMs?\nMain Findings 5: Larger model scale, buffer\nsize, and more pretraining steps all lead to\nbetter IL ability.\nGiven that data replay is effective, we next explore how model scale, buffer size, and pretraining influence the incremental learning ability of LLMs. The results are summarized in Figure 4 and Table 8. Detailed results corresponding to Figure 4 can be found in Tables 13, 14, 15, 16, 17, 18, 19, and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/60b0/60b07650-8063-4959-9eb9-079a20a98557.png\" style=\"width: 50%;\"></div>\nFigure 4: The analysis of memorization (top row) and generalization (bottom row) accuracy on Concept-1K. The backbone model is in {Pythia-70M, 160M, 410M, 1B, 1.4B, 2.8B}. The pretraining step is in {0, 16, 128, 1000, 10000, 143000 (final version)}. Each point represents the result of IL. The detailed results with standard deveriation are provided in Table 13, 14, 15, 16, 17, 18, 19, and 20.\n<div style=\"text-align: center;\">Figure 4: The analysis of memorization (top row) and generalization (bottom row) accuracy on Concept-1K. The backbone model is in {Pythia-70M, 160M, 410M, 1B, 1.4B, 2.8B}. The pretraining step is in {0, 16, 128, 1000, 10000, 143000 (final version)}. Each point represents the result of IL. The detailed results with standard deveriation are provided in Table 13, 14, 15, 16, 17, 18, 19, and 20.</div>\n# 20.\nModel Scale. The model scale determines the upper limit of generalization performance. Table 8 shows that as LLMs become larger, the memorization forgetting decreases while the generalization forgetting increases. This indicates that larger LLMs forget fewer training samples but more generalized knowledge, as they generalize more knowledge. Buffer Size. Figures 4 (a) and (e) show that a larger buffer size or a larger LLM improves the accuracy of both memorization and generalization. However, the memorization accuracy of the 2.8B model remains unsatisfactory without a buffer. This suggests that even billion-parameter LLMs suffer from catastrophic forgetting, and data replay is an effective technique for mitigating it. Furthermore, Figures 4 (b)-(d), (f)-(h) indicate that a larger buffer size improves both memorization and generalization abilities across all pretraining steps, with the improvement in memorization ability being more significant than that in generalization ability. Pretraining. Figure 4 (b) demonstrates that memorization performance increases during the early stages of pretraining (step 0 - step 10000), indicating that pretraining enhances the memorization ability of LLMs for novel concepts. However, with more pretraining steps, memorization performance degrades. In contrast, Figure 4 (f) shows that gen-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3b43/3b43b611-a743-4e19-b952-bc5a9aba000a.png\" style=\"width: 50%;\"></div>\nFigure 5: The memorization accuracy and generalization accuracy of different concepts in Concept-1K. The concepts are sorted according to (a) memorization accuracy and (b) generalization accuracy respectively.\nTable 9: The concepts with highest and lowest generalization accuracy.\nTop 5\nBottom 5\nConcept\nGA (%)\nMA (%)\nConcept\nGA (%)\nMA (%)\nPeer-to-Peer Lending\n82.47\n100.00\nSmart City Technologies\n0.00\n25.54\nLetters of Credit\n77.61\n100.00\nOrbital Mechanics\n0.00\n44.13\nStreaming Services\n74.43\n100.00\nVirtual Fitting Rooms\n0.00\n32.86\nCarbon Neutral\n71.04\n85.71\nMars Missions\n0.00\n5.87\nInterest Rate Hikes\n70.82\n100.00\nFlexible Displays\n0.00\n26.38\neralization performance increases monotonically for LLMs larger than 160M. This may be because LLMs gradually learn to extract underlying knowledge from the text during pretraining, rather than merely remembering specific texts. Additionally, especially for larger models, pretraining is more beneficial to generalization ability than memorization ability.\n# 3.6 RQ6: Are concepts learned equally?\nMain Findings 6: Concepts that are welldefined and concrete are easier to memorized and generalized.\nFinally, we explore whether LLMs learn all concepts equally. Are some concepts easier to learn? We analyze the memorization and generalization accuracy of various concepts in the Concept-1K dataset, using the LLaMa-2-7B model as the backbone. To mitigate the impact of task order, we aggregate the performance of the concepts in the fifth task after training on all tasks from 10 different task orders. Figure 5 reveals a positive correlation between memorization accuracy and generalization accuracy, indicating that concepts easier to memorize are also easier to generalize, and vice versa. Our findings align with those of Toneva et al. (2018), which suggest that certain examples are unforgettable and their knowledge can be better generalized across datasets. Table 9 highlights that concepts with the highest memorization accuracy tend to be well-defined and concrete, often related to established financial or technological terms. In contrast, concepts with the lowest memorization accuracy are often more complex, abstract, or emerging fields, which may explain the challenges in both memorization and generalization. This disparity underscores the importance of the nature of the concepts being learned and the inherent difficulty associated with them. Our findings are consistent with those of Toneva et al. (2018), which reveal that unforgettable images are easily recognizable, while the most forgotten examples exhibit more ambiguous characteristics. Figure 6 visualize the memorized and generalized knowledge of one individual concept \u201cBrainComputer Interface\u201d and illustrate the forgetting on knowledge during IL. The full results and further discussion are provided in Appendix H.\n# 4 Related Work\nWe categorize existing studies on understanding the incremental learning ability of LLMs into three parts: (1) Understanding Forgetting, (2) Understanding Memorization, and (3) Applications in NLP. Due to space limitations, the detailed discussion is provided in the Appendix A.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dcfa/dcfad785-f591-453a-b1d0-b8c678ce7ace.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5d9b/5d9b3f3d-6d98-4a8f-83de-8c934139caff.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) After Task 1 (Test) (d) Afte</div>\n<div style=\"text-align: center;\"> After Task 1 (Test) (d) After Task 10 (Test)</div>\n<div style=\"text-align: center;\">(d) After Task 10 (Test)</div>\n<div style=\"text-align: center;\">(c) After Task 1 (Test)</div>\nFigure 6: The visualization of the memorized and generalized knowledge related to the \u201cBrain-Computer Interface\u201d in IL. The center node represents a concept, while the linked and unlinked edges indicate whether the corresponding training and test samples are answered correctly. The full results are in Figure 7 and 8.\nUnderstanding Forgetting. Earlier studies, such as French (1999), assess catastrophic forgetting by measuring performance degradation on old tasks. Recently, studies (Tao et al., 2023; Zheng et al., 2023b) use probing techniques to measure forgetting in incremental learning. Zheng et al. (2023b) uses probing techniques to show that LLMs have superior performance on evaluated datasets even before IL. Our work is inspired by Zheng et al. (2023b) and proposes a novel dataset to minimize the influence of data leakage issues.\n# 5 Conclusion\nIn this paper, we introduce Concept-1K, a novel dataset designed to evaluate the IL capabilities of LLMs. Our comprehensive experiments reveal that LLMs still suffer from catastrophic forgetting and that LoRA, despite fine-tuning fewer parameters, limits the ability to learn and generalize new knowledge. We demonstrate that data replay is the most effective method for mitigating forgetting and highlight the significant roles of model scale, buffer size, and pretraining. These findings provide valuable insights into the strengths and limitations of LLMs in IL scenarios, offering a robust benchmark for future research.\nThere are two limitations of this research: (1) The knowledge of Concept-1K is defined in the form of triplets, which can not cover the knowledge in a broad sense. (2) Apart from the experiments of incontext learning, other experiments are conducted on LLMs with less than 13B parameters. The conclusion of these experiments may not hold when finetuning SOTA LLMs such as GPT4 with more than 100B parameters.\n# Ethical Considerations\nThe ethical considerations of our research are carefully addressed to ensure compliance with relevant standards and transparency. To this end, we provide the following clarifications: 1. Dataset Collection: Our research employs GPT4 to construct Concept-1K and filter out offensive or harmful instances. The use of GPT4 was consistent with their intended use. The dataset Concept-1K is publicly available for academic and research purposes. 2. Reproducibility: We provide a detailed setting of our experiments. The source code, data, and scripts will all be publicly available. Our findings are in alignment with observed empirical outcomes.\n# References\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, pages 95\u2013136, virtual+Dublin. Association for Computational Linguistics. Enric Boix-Adser\u00e0, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua M Susskind. 2023. Transformers learn through gradual rank increase. In Thirty-seventh Conference on Neural Information Processing Systems. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. I\u00f1igo Casanueva, Tadas Tem\u02c7cinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u00b4c. 2020. Efficient intent detection with dual sentence encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38\u201345, Online. Association for Computational Linguistics. Jiefeng Chen, Timothy Nguyen, Dilan Gorur, and Arslan Chaudhry. 2023. Is forgetting less a good inductive bias for forward transfer? In The Eleventh International Conference on Learning Representations. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Yung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen. 2020. Lifelong language knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2914\u20132924, Online. Association for Computational Linguistics. MohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf Aljundi, and Eugene Belilovsky. 2022. Probing representation forgetting in supervised and unsupervised continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16712\u201316721. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan\nLiu. 2021. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3198\u20133213, Online. Association for Computational Linguistics.\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3198\u20133213, Online. Association for Computational Linguistics. Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128\u2013135. Yanhui Guo, Shaoyuan Xu, Jinmiao Fu, Jia Liu, Chaosheng Dong, and Bryan Wang. 2024. Q-tuning: Queue-based prompt tuning for lifelong few-shot language learning. arXiv preprint arXiv:2404.14607. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4803\u20134809, Brussels, Belgium. Association for Computational Linguistics. Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57\u201360, New York City, USA. Association for Computational Linguistics. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. 2024. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. arXiv preprint arXiv:2403.01244. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526. Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An evaluation dataset for intent classification and out-ofscope prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1311\u20131316, Hong Kong, China. Association for Computational Linguistics.\nRobert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128\u2013135.\nYanhui Guo, Shaoyuan Xu, Jinmiao Fu, Jia Liu, Chaosheng Dong, and Bryan Wang. 2024. Q-tuning: Queue-based prompt tuning for lifelong few-shot language learning. arXiv preprint arXiv:2404.14607.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526.\nefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An evaluation dataset for intent classification and out-ofscope prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1311\u20131316, Hong Kong, China. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nMinqian Liu and Lifu Huang. 2023. Teamwork is not always good: An empirical study of classifier drift in class-incremental information extraction. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2241\u20132257, Toronto, Canada. Association for Computational Linguistics.\nWeijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. 2024. Analyzing and reducing catastrophic forgetting in parameter efficient tuning. arXiv preprint arXiv:2402.18865.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022a. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022a. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022b. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139\u2013149. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. 2019. Huggingface\u2019s transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and Gholamreza Haffari. 2021. Pretrained language model in continual learning: A comparative study. In International Conference on Learning Representations. Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, and Di Wang. 2024. Moral: Moe augmented lora for llms\u2019 lifelong learning. arXiv preprint arXiv:2402.11260. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Duzhen Zhang, Wei Cong, Jiahua Dong, Yahan Yu, Xiuyi Chen, Yonggang Zhang, and Zhen Fang. 2023a. Continual named entity recognition without catastrophic forgetting. arXiv preprint arXiv:2310.14541. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28.\n# Appendix\n# A Related Work\n<div style=\"text-align: center;\">C Comparison with Existing Incremental Learning Setting</div>\nD.1 Technology Domain . . . . . . . .\n15\nD.2\nEconomic Domain\n. . . . . . . .\n15\nD.3\nEducation Domain\n. . . . . . . .\n16\nD.4\nEnvironmental Domain . . . . . .\n16\nD.5\nCultural Domain\n. . . . . . . . .\n16\nD.6\nHealth and Medical Domain\n. . .\n16\nE\nConcept Selection Criterion\n16\nE.1\nLength criterion . . . . . . . . . .\n16\nE.2\nTimeliness criterion . . . . . . . .\n16\nE.3\nTrend criterion\n. . . . . . . . . .\n16\nF\nExperimental Settings\n16\nF.1\nBackbones . . . . . . . . . . . . .\n16\nF.2\nImplementation Details . . . . . .\n16\nF.3\nInput Prompt\n. . . . . . . . . . .\n17\n# G Introduction of Baseline Methods\n# H Visualization of One Concept\n# I Additional Experimental Results\n# A Related Work\nWe categorize existing studies on understanding the incremental learning ability of LLMs into three parts: (1) Understanding Forgetting, (2) Understanding Memorization, and (3) Applications in NLP.\n# A.1 Understanding Forgetting\nEarlier studies, such as French (1999); Kirkpatrick et al. (2017), assess catastrophic forgetting by measuring performance degradation on old tasks. Recently, studies (Davari et al., 2022; Wu et al., 2021; Chen et al., 2023; Tao et al., 2023; Zheng et al., 2023b) use probing techniques to measure forgetting in incremental learning. For example, Davari et al. (2022) uses linear probing to reveal representation drift due to parameter updates. Wu et al. (2021) conducts layer-wise probing on BERT, showing catastrophic forgetting in the top and middle layers. Chen et al. (2023) reveals a correlation between retaining past information and new task learning efficiency through linear probing on kshot samples. Tao et al. (2023) illustrates BERT\u2019s resilience to catastrophic forgetting even without buffer data. More recently, Zheng et al. (2023b) reveal that most previous work in NLP overlooks the data leakage issue and uses probing techniques to show that LLMs have superior performance on evaluated datasets even before incremental training. Additionally, they reveal that LLMs have strong anti-forgetting ability even under the sequential fine-tuning setting. Our work is inspired by Zheng et al. (2023b) and proposes a novel dataset to minimize the influence of data leakage issues, allowing for correct conclusions about the incremental learning ability of LLMs.\n# A.2 Understanding Memorization\nFewer studies (Carlini et al., 2021; Tirumala et al., 2022; Biderman et al., 2024b; Boix-Adser\u00e0 et al., 2023) explore memorization within LLMs. For instance, Carlini et al. (2021) discover that GPT-2 can memorize a small proportion of private information during pretraining, raising privacy concerns. Tirumala et al. (2022) show that larger LLMs memorize faster and have higher \u201cforgetting baselines\u201d. Biderman et al. (2024b) reveal the difficulty in predicting which training samples will be memorized by large language models. Boix-Adser\u00e0 et al. (2023) find that transformers incrementally learn new knowledge, with trained and initial weights progressively increasing in rank. These studies explore memorization from both the perspective of sentences and model weights. In contrast, this paper studies the problem of memorization and forgetting at a more fine-grained level: the concept level. This approach addresses an underexplored research problem in the incre-\nmental learning community.\n# A.3 Applications in NLP\nMany works (Sun et al., 2020; Chuang et al., 2020; Liu and Huang, 2023; Qiu et al., 2024; Zhang et al., 2023a; Shao et al., 2023a; Zhang et al., 2023b) focus on incremental learning for various NLP tasks, assuming catastrophic forgetting in pre-trained language models and designing techniques to mitigate it. These tasks include text classification (Sun et al., 2020; Chuang et al., 2020), relation extraction (Liu and Huang, 2023), named entity recognition (Qiu et al., 2024; Zheng et al., 2022; Zhang et al., 2023a), intent classification (Shao et al., 2023a), and machine translation (Zhang et al., 2023b). We refer readers to the survey (Zheng et al., 2024) for more applications of incremental learning in NLP tasks. The proposed Concept-1K differs substantially from the aforementioned NLP tasks. It is more challenging and provides better explainability at the concept level.\n# B Data Leakage in IL of LLMs\nThe data leakage issue in NLP is often implicit. Unlike computer vision, where pretraining involves explicit category information, NLP pretraining is selfsupervised and lacks clear categorical distinctions that can be easily compared between the pretraining corpus and downstream datasets. This makes it challenging to detect and address data leakage. Therefore, we urge future studies to exercise greater caution regarding data leakage in the IL of LLMs.\n# B.1 Data Leakage in Classification Tasks\nThe issue of data leakage in classification tasks for IL with LLMs has recently been highlighted by (Zheng et al., 2023b). Their extensive study revisits over 20 IL methods across four key classification tasks: Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition. One core finding of (Zheng et al., 2023b) is that LLMs, such as BERT and GPT-like models, exhibit high probing performance even before they are incrementally trained on specific downstream tasks. This high initial performance suggests that these models already possess substantial knowledge relevant to the classification tasks due to their extensive pre-training on diverse corpora. Consequently, when these LLMs are evaluated under IL settings, the incremental learning of new tasks may, in fact, be leveraging pre-existing knowledge rather than genuinely incremental learning.\nThis phenomenon leads to misleading conclusions about the effectiveness of various IL methods (Sun et al., 2020; Chuang et al., 2020; Liu and Huang, 2023; Qiu et al., 2024; Zhang et al., 2023a; Shao et al., 2023a; Zhang et al., 2023b). The high probing performance before task-specific training indicates that the models are not learning incrementally as assumed but rather recalling previously acquired knowledge. Therefore, many IL studies in the context of classification tasks suffer from data leakage, as the benchmark tasks are not truly novel to the LLMs. Addressing this issue requires carefully designed benchmarks that ensure the novelty and exclusivity of the knowledge being tested, a challenge we aim to tackle with our Concept-1K dataset.\n# B.2 Data Leakage in Generation Tasks\nData leakage poses a significant challenge in IL for generation tasks, which are often more specific and diverse compared to classification tasks. For example, in the IL setting described by Scialom et al. (2022), the task sequence includes Text Simplification, Headline Generation with Constraints, Haiku Generation, Covid QA, Inquisitive Question Generation, Empathetic Dialogue Generation, Explanation Generation, and Twitter Stylometry. Despite the specificity and diversity of these tasks, data leakage remains a concern because LLMs are trained on extensive corpora from the internet and often undergo supervised finetuning on dialogue data (OpenAI, 2023). This pre-training on vast amounts of internet data means that LLMs might already possess significant knowledge relevant to these generation tasks. Moreover, the data leakage issue in generation tasks is implicit and easy to overlook. Unlike classification tasks, where techniques like probing (Zheng et al., 2023b) can measure LLM performance before training, generation tasks lack such straightforward methods to assess initial model capabilities. This makes it challenging to ascertain how much new knowledge is genuinely being learned during IL versus what the model is simply recalling from its pre-trained knowledge base. Pioneering work investigating the IL ability of LLMs found that the T0 model (Sanh et al., 2021) barely forgets when learning new tasks. This suggests that the T0 model may have already acquired the ability to solve multiple generation tasks with appropriate input prompts before explicit training on them. Detailed results are presented in Figure 2\nof Sanh et al. (2021). Another study by Zhang et al. (2023c) defines generation tasks using different instructions, a paradigm they call continual instruction tuning. They train the T5 model (Raffel et al., 2020) sequentially on 19 tasks, achieving 35.7% performance, while jointly training on these tasks yields 42.1%. The close gap between upper bound and lower bound performance indicates minimal forgetting and suggests potential data leakage. Detailed results and experimental settings are provided in Tables 1 and 2 of Zhang et al. (2023c). Although recent studies (Module; Guo et al., 2024; Huang et al., 2024; Yang et al., 2024; Peng et al., 2024; Ren et al., 2024) claim that forgetting is serious in continual instruction tuning, all of them utilize parameter-efficient finetuning techniques such as LoRA (Hu et al., 2021) or prompt tuning (Lester et al., 2021). As shown in our experiments in Section 3.3, the IL ability of LoRA and full finetuning differ substantially. LoRA significantly limits the ability to learn new concepts compared to full finetuning, leading to limited new knowledge acquisition and faster forgetting on training samples. Therefore, their IL settings may not accurately reflect the true IL ability of LLMs.\n# C Comparison with Existing Incremental Learning Setting\nThere are three popular IL settings which are widely adopted in the literature of computer vision: class-incremental learning (CIL), task-incremental learning (TIL), and continual pretraining (CPT). However, none of them are appropriate to evaluate the IL ability of LLMs.\n# C.1 Class-Incremental Learning\nCIL is designed for classification tasks such as text classification, and its goal is to learn new classes incrementally. On the one hand, SOTA LLMs with billion-level parameters are overqualified for the above classification tasks with only dozens of categories. On the other hand, the pretraining corpus is likely to contain the knowledge required for the classification tasks (data leakage issue). As shown empirically by (Zheng et al., 2023b), sequential training frozen LLMs with expanding classifiers yields comparable or even superior performance with SOTA IL methods.\n# C.2 Task-Incremental Learning\nTIL aims to learn new tasks incrementally (Sun et al., 2020; Qin and Joty, 2022). Apart from the data leakage issue, the diversity of tasks and orders across research makes it difficult to readily and fairly compare IL algorithms.\n# C.3 Continual Pretraining\nThe last scenario CPT aims at continual pretraining models on the corpus from different domains. However, the evaluation relies on the performance of downstream tasks, where we can hardly identify what knowledge is learned or forgotten.\n# C.4 Summary\nIn this paper, we consider a novel IL scenario called Instance-level Incremental Learning (IIL). Unlike the IL scenario mentioned above, IIL regards each concept as an instance and is more practical and challenging for existing LLMs. Specifically, we are motivated by the human learning process and expect LLMs to learn new concepts incrementally without forgetting. For example, in Figure 1, humans can learn new concepts that are constantly emerging, such as \u201cMetaverse\u201d and \u201cQuantum Computing\u201d. After learning more concepts such as \u201cWeb3.0\u201d and \u201cNon-Fungible Token\u201d, humans will not immediately forget the previously learned concepts such as \u201cMetaverse\u201d.\n# D Introduction of Domains in Concept-1K\nThe domains in Concept-1K are introduced as follows:\nD.1 Technology Domain\n# D.1 Technology Domain\nThis domain focuses on both cutting-edge and widely applied technologies. Cutting-edge technologies include artificial intelligence, blockchain, quantum computing, etc., while widely applied technologies encompass cloud computing, the Internet of Things, and more.\n# D.2 Economic Domain\nThis domain highlights economic trends and emerging economic concepts. Economic trends cover topics such as digital currency and globalization, whereas emerging concepts include quantitative computing, electronic wallets, peer-to-peer (P2P) networks, and others.\n# D.3 Education Domain\nThis domain emphasizes emerging educational technologies and concepts. Technologies such as remote learning and online courses, along with concepts like bilingual education, social education, and lifelong learning, are included.\n# D.4 Environmental Domain\nThis domain centers on global environmental issues and green technologies. Topics include climate change, environmental protection, and sustainable energy, as well as technologies like green roofs, shared bicycles, and solar panels.\n# D.5 Cultural Domain\nThis domain focuses on diversity and inclusion, and digital media and arts. Diversity and inclusion cover multiculturalism, gender equality, and social inclusion, while digital media and arts include digital art, social media trends, and online communities.\n# D.6 Health and Medical Domain\nThis domain is dedicated to emerging medical technologies and public safety. It covers CRISPR gene editing technology, the application of artificial intelligence in medical diagnosis, telemedicine services, wearable health monitoring devices, and concepts related to vaccine development, disease monitoring and prevention strategies, and promoting public health awareness.\n# E Concept Selection Criterion\nIn constructing Concept-1K, we carefully selected concepts based on the following criteria to ensure the relevance, novelty, and richness of the learning material:\n# E.1 Length criterion\nThe concept words should not exceed three words in length. This encourages the model to focus on significant and concise terms within the domain, facilitating efficient learning and ensuring that the concepts provide a rich source of information. Shorter concepts are easier to manage and help avoid potential confusion that may arise from overly complex or verbose terms.\n# E.2 Timeliness criterion\nThe chosen concept words should preferably be those that emerged after January 2022. This criterion ensures that the general pre-trained models\nhave not yet encountered and learned these concepts and the associated knowledge. By selecting recent concepts, we aim to test the true incremental learning capabilities of LLMs, avoiding biases introduced by prior knowledge.\n# E.3 Trend criterion\nWe focus on concepts that are currently receiving widespread attention in academia, industry, and the media. This ensures that the selected concepts are not only relevant and contemporary but also significant and impactful in their respective fields. By choosing trending concepts, we can better gauge the models\u2019 ability to learn and adapt to the latest advancements and discussions in various domains.\n# F Experimental Settings\n# F.1 Backbones\nWe use the Pythia suite (Biderman et al., 2023) and other popular open-source models, including LLaMa and Vicuna, for our experiments. Pythia is based on GPT-NeoX (Black et al., 2022) and includes 8 model sizes and 154 pre-training checkpoints, facilitating research in interpretability and learning dynamics. The statistics of the 9 LLMs used in this paper are summarized in Table 10. We download the pre-trained weights from Huggingface (Wolf et al., 2019).\n# F.2 Implementation Details\nWe sort the concepts alphabetically and shuffle the order using random seed 1. The maximum input and output lengths are set to 32 and 10, respectively. The batch size is 32, and the learning rate for the LLMs is 1 \u00d7 10\u22125. We use the AdamW optimizer (Loshchilov and Hutter, 2018). For LLMs with more than 1B parameters, we use A800 GPUs, while smaller LLMs run on RTX3090 GPUs. Each experiment is repeated three times, and we report the average and standard deviations. Additionally, we search for the best hyper-parameters for each baseline method. For the experiment in Section 3.2, we use \u201cgpt3.5-turbo\u201d and \u201cgpt-4-turbo\u201d for GPT-3.5 and GPT4, respectively. Given the high cost of evaluating the entire Concept-1K dataset, we randomly sample 500 instances for both GPT-3.5 and GPT-4. The outputs and targets of GPT-3.5 and GPT-4 on these 500 instances are provided in the supplementary material.\nModel Class\nPretrained Weights\nParameters\nLayers\nHidden Dim\nLink\nGPT-NeoX\nPythia-70m\n19M\u2020\n6\n512\nLink\nPythia-160m\n85M\u2020\n12\n768\nLink\nPythia-410m\n302M\u2020\n24\n1024\nLink\nPythia-1b\n805M\u2020\n16\n2048\nLink\nPythia-1.4b\n1.21B\u2020\n24\n2048\nLink\nPythia-2.8b\n2.52B\u2020\n32\n2560\nLink\nLLaMa\nllama-7b-hf\n7B\n32\n4096\nLink\nvicuna-7b-v1.1\n7B\n32\n4096\nLink\nllama-2-13b-hf\n13B\n40\n5120\nLink\n# F.3 Input Prompt\nWe use the following input prompt for training and testing Concept-1K:\nwhere {Question} and {Answer} represent the question and the target output, respectively. For the experiment in Section 3.2, the prompts are provided in Tables 11 and 12.\n# G Introduction of Baseline Methods\nThe introduction of the baseline methods is as follows:\n\u2022 SEQ: Sequential fine-tuning (SEQ) is considered the lower bound of incremental learning.\n\u2022 REPLAY: Experience replay stores representative old samples and jointly optimizes both old and new samples when learning new tasks. This is a practical and popular technique in incremental learning.\n\u2022 LAMOL (Sun et al., 2020): LAMOL trains LLMs with question-answering and generative objectives, generating pseudo-samples\nbefore learning each new task for data replay. The generation loss weight is \u03bb = 0.25, and the proportion of pseudo-samples is \u03b3 = 0.20. There are two variants: LAMOL_t and LAMOL_g, differing by whether a taskspecific token is used for generation.\n\u2022 L2KD (Chuang et al., 2020): L2KD adds a knowledge distillation target based on LAMOL, with the teacher model trained from scratch. We implemented the word-level variant as it performs best on text classification tasks.\n LAMOL_KD (Zheng et al., 2023b): LAMOL_KD utilizes knowledge distillation based on LAMOL_t. Unlike L2KD, the teacher model in LAMOL_KD is trained on all previous tasks. New data are used to learn the LAMOL objectives, and pseudo data are used for word-level knowledge distillation as a regularization term.\n\u2022 PCLL (Zhao et al., 2022): PCLL combines the concepts of variational autoencoders and word-level knowledge distillation with the objectives of LAMOL.\n\u2022 LFPT5 (Qin and Joty, 2022): LFPT5 learns only soft prompts for each new task. The training objective is the same as LAMOL. The number of soft prompt tokens is 10.\n\u2022 LFPT5 (Qin and Joty, 2022): LFPT5 learns only soft prompts for each new task. The training objective is the same as LAMOL. The number of soft prompt tokens is 10.\n\u2022 LoRA (Hu et al., 2021): LoRA trains a small proportion of parameters of LLMs. We set the rank r = 8 and the scaling parameter \u03b1 = 16.\n\u2022 LoRA (Hu et al., 2021): LoRA trains a small proportion of parameters of LLMs. We set the rank r = 8 and the scaling parameter \u03b1 = 16.\nTable 11: Prompt for In-Context Learning with 1 shot demonstration. {Question i} and {Answer i} represent the question and the target output of the i-th demonstration training sample respectively. {Test Question} represents the test question.\nI will provide some knowledge as follows:\nQuestion: {Question 1}\nShort Answer: {Answer 1}\nPlease answer the following question according to the above knowledge:\nQuestion: {Test Question}\nShort Answer:\nTable 12: Prompt for In-Context Learning with 5 shot demonstrations. {Question i} and {Answer i} represent the question and the target output of the i-th demonstration training sample respectively. {Test Question} represents the test question.\nI will provide some knowledge as follows:\nQuestion: {Question 1}\nShort Answer: {Answer 1}\nQuestion: {Question 2}\nShort Answer: {Answer 2}\nQuestion: {Question 3}\nShort Answer: Answer 3\nQuestion: {Question 4}\nShort Answer: {Answer 4}\nQuestion: {Question 5}\nshort Answer: {Answer 5}\nPlease answer the following question according to the above knowledge:\nQuestion: {Test Question}\nShort Answer:\nWe use the LoRA implementation from the PEFT library (Mangrulkar et al., 2022).\nSome IL methods are not compared as they are not applicable in the IIL scenario. For example, Progressive Prompt (Razdaibiedina et al., 2023) requires task IDs during both training and inference stages. VAG (Shao et al., 2023b) requires storing the vocabulary of class labels and does not apply to generation tasks without class labels. Additionally, prompt-based IL methods such as L2P (Wang et al., 2022b) are not suitable for generation tasks.\n# H Visualization of One Concept\nWe visualize the memorized and generalized knowledge of the concept \u201cBrain-Computer Interface\u201d in Figures 7 and 8. In each graph, the center node represents the concept \u201cBrain-Computer Interface\u201d. The linked and unlinked edges indicate whether the corresponding training or test samples are answered correctly. For example, in Figure 7a, the edge between \u201cBrain-Computer Interface\u201d and \u201cSignal Processing\u201d signifies that the LLM correctly outputs the target answer \u201cSignal Processing\u201d when the question is the training sample \u201cWhat subevent occurs in a Brain-Computer Interface?\u201d. In Figure 7b, the edge between \u201cBrain-Computer Interface\u201d and \u201cSignal Processing\u201d is missing, indicating that the LLM fails to provide the correct target answer. Figure 7 shows that even when LLMs can memorize all the knowledge, they tend to first forget more complex knowledge, such as \u201c(BrainComputer Interface, UsedFor, Controlling Computers With Thought)\u201d. Conversely, some common knowledge, such as \u201c(Brain-Computer Interface, Requires, Brain Signals)\u201d, \u201c(Brain-Computer Interface, Has Property, Innovative)\u201d, and \u201c(BrainComputer Interface, Motivated By Goal, Accessibility)\u201d, remains robust and is not forgotten after learning 10 tasks. This indicates that certain knowledge is easier to memorize, generalize, and retain. Further exploration at the concept-level knowledge in IL is left for future work. We also encourage future studies to utilize the provided Concept1K dataset for a fine-grained analysis of the memorization and generalization dynamics in IL.\n# I Additional Experimental Results\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/061b/061b55d0-21e5-4d1d-8df3-7e71f3affb2b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: The visualization of the memorized knowledge related to the \u201cBrain-Computer Interface\u201d in IL. The center node represents a concept, while the linked and unlinked edges indicate whether the corresponding training samples are answered correctly.</div>\n<div style=\"text-align: center;\">Table 13: The memorization accuracy when different model scales and buffer size are selected. The pretraining tep is 143000 (final version). Figure 4 (a) summarises this figure\u2019s content.</div>\n70M\n160M\n410M\n1B\n1.4B\n2.8B\nBuffer Size=0\n6.34\u00b10.86\n33.53\u00b11.39\n58.28\u00b10.61\n65.97\u00b10.31\n64.95\u00b10.55\n68.03\u00b10.46\nBuffer Size=2000\n22.87\u00b11.28\n56.12\u00b10.81\n77.31\u00b10.23\n81.57\u00b10.32\n80.81\u00b10.62\n82.26\u00b10.66\nBuffer Size=5000\n42.13\u00b11.88\n73.32\u00b11.00\n90.05\u00b10.64\n91.93\u00b10.49\n91.49\u00b10.32\n92.03\u00b10.35\nBuffer Size=10000\n58.50\u00b10.50\n84.91\u00b11.64\n97.81\u00b10.12\n97.21\u00b10.73\n97.35\u00b10.84\n96.74\u00b10.43\nBuffer Size=All\n62.37\u00b11.82\n90.04\u00b10.64\n99.01\u00b10.14\n99.10\u00b10.57\n98.86\u00b10.55\n98.72\u00b10.34\n<div style=\"text-align: center;\">Table 14: The generalization accuracy when different model scales and buffer size are selected. The pretrainin step is 143000 (final version). Figure 4 (e) summarises this figure\u2019s content.</div>\n70M\n160M\n410M\n1B\n1.4B\n2.8B\nBuffer Size=0\n2.52\u00b10.12\n6.57\u00b10.28\n17.69\u00b10.35\n22.65\u00b10.39\n25.57\u00b10.36\n29.56\u00b10.35\nBuffer Size=2000\n4.04\u00b10.07\n8.49\u00b10.23\n22.49\u00b10.32\n27.29\u00b10.55\n30.80\u00b10.28\n34.21\u00b10.40\nBuffer Size=5000\n5.14\u00b10.09\n10.08\u00b10.40\n25.19\u00b10.46\n29.34\u00b10.38\n33.50\u00b10.46\n37.04\u00b10.36\nBuffer Size=10000\n5.88\u00b10.10\n10.36\u00b10.52\n26.20\u00b10.35\n30.81\u00b10.24\n34.85\u00b10.57\n37.72\u00b10.52\nBuffer Size=All\n5.80\u00b10.08\n10.80\u00b10.46\n25.70\u00b10.28\n30.99\u00b10.42\n35.03\u00b10.27\n37.97\u00b10.37\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d9b5/d9b5cda4-00d2-47a5-ba32-fe604b937cc2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: The visualization of the generalized knowledge related to the \u201cBrain-Computer Interface\u201d in IL. The center node represents a concept, while the linked and unlinked edges indicate whether the corresponding test samples are answered correctly.</div>\n<div style=\"text-align: center;\">Table 15: The memorization accuracy when different pretraining steps and model scales are selected. The buffe size is 0. Figure 4 (b) summarizes the content of this figure.</div>\nStep=0\nStep=16\nStep=128\nStep=1000\nStep=10000\nStep=143000\nPythia-70M\n2.92\u00b10.06\n4.89\u00b10.16\n4.58\u00b10.10\n24.69\u00b10.04\n36.08\u00b10.63\n6.34\u00b10.86\nPythia-160M\n26.33\u00b10.31\n29.34\u00b10.25\n29.11\u00b10.11\n48.78\u00b10.09\n60.12\u00b10.34\n33.53\u00b11.39\nPythia-410M\n29.58\u00b11.10\n31.06\u00b10.24\n30.78\u00b10.22\n49.84\u00b10.06\n64.31\u00b10.38\n58.28\u00b10.61\nPythia-1B\n33.18\u00b10.59\n33.71\u00b10.17\n33.91\u00b10.49\n53.14\u00b10.59\n66.66\u00b10.59\n65.97\u00b10.31\nPythia-1.4B\n35.61\u00b10.59\n36.31\u00b10.51\n36.15\u00b10.32\n55.61\u00b10.59\n70.66\u00b10.51\n64.95\u00b10.55\nPythia-2.8B\n37.76\u00b10.29\n37.73\u00b10.38\n37.65\u00b10.39\n57.09\u00b10.34\n72.92\u00b10.59\n68.03\u00b10.46\n<div style=\"text-align: center;\">Table 16: The generalization accuracy when different pretraining steps and model scales are selected. The buffe size is 0. Figure 4 (f) summarises this figure\u2019s content.</div>\nStep=0\nStep=16\nStep=128\nStep=1000\nStep=10000\nStep=143000\nPythia-70M\n0.83\u00b10.02\n1.10\u00b10.01\n1.08\u00b10.04\n2.94\u00b10.04\n3.95\u00b10.03\n2.52\u00b10.12\nPythia-160M\n1.64\u00b10.04\n1.81\u00b10.02\n1.82\u00b10.06\n3.79\u00b10.05\n6.96\u00b10.04\n6.57\u00b10.28\nPythia-410M\n2.34\u00b10.07\n2.35\u00b10.04\n2.21\u00b10.04\n4.18\u00b10.10\n12.91\u00b10.14\n17.69\u00b10.35\nPythia-1B\n2.78\u00b10.14\n2.85\u00b10.24\n2.80\u00b10.24\n6.03\u00b10.24\n17.70\u00b10.24\n22.65\u00b10.39\nPythia-1.4B\n2.71\u00b10.37\n2.32\u00b10.24\n2.40\u00b10.53\n5.68\u00b10.24\n20.64\u00b10.24\n25.57\u00b10.36\nPythia-2.8B\n2.48\u00b10.23\n1.79\u00b10.29\n2.09\u00b10.32\n5.35\u00b10.49\n24.79\u00b10.24\n29.56\u00b10.35\n<div style=\"text-align: center;\">Table 17: The memorization accuracy when different pretraining steps and model scales are selected. The buffe size is 2000. Figure 4 (c) summarises this figure\u2019s content.</div>\nStep=0\nStep=16\nStep=128\nStep=1000\nStep=10000\nStep=143000\nPythia-70M\n22.15\u00b10.13\n24.91\u00b10.20\n23.54\u00b10.15\n55.06\u00b10.16\n64.89\u00b10.23\n22.87\u00b11.28\nPythia-160M\n59.42\u00b10.13\n59.62\u00b10.77\n59.30\u00b10.18\n78.84\u00b10.44\n82.30\u00b10.18\n56.12\u00b10.81\nPythia-410M\n60.43\u00b10.59\n61.09\u00b10.66\n61.24\u00b10.27\n79.77\u00b10.15\n83.24\u00b10.12\n77.31\u00b10.23\nPythia-1B\n63.11\u00b10.15\n62.05\u00b10.40\n63.51\u00b10.31\n80.11\u00b10.50\n83.11\u00b10.38\n81.57\u00b10.32\nPythia-1.4B\n63.77\u00b10.15\n64.14\u00b10.23\n65.40\u00b10.32\n82.38\u00b10.34\n84.19\u00b10.22\n80.81\u00b10.62\nPythia-2.8B\n65.10\u00b10.45\n65.84\u00b10.57\n66.04\u00b10.53\n82.63\u00b10.18\n84.80\u00b10.44\n82.26\u00b10.66\n<div style=\"text-align: center;\">Table 18: The generalization accuracy when different pretraining steps and model scales are selected. The buffe size is 2000. Figure 4 (g) summarises this figure\u2019s content.</div>\nStep=0\nStep=16\nStep=128\nStep=1000\nStep=10000\nStep=143000\nPythia-70M\n2.07\u00b10.08\n2.46\u00b10.06\n2.51\u00b10.13\n3.84\u00b10.06\n4.57\u00b10.07\n4.04\u00b10.07\nPythia-160M\n3.17\u00b10.08\n3.55\u00b10.14\n3.52\u00b10.06\n5.26\u00b10.14\n8.05\u00b10.08\n8.49\u00b10.23\nPythia-410M\n3.96\u00b10.06\n4.97\u00b10.09\n5.09\u00b10.04\n6.",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning. Existing benchmarks for IL are insufficient due to data leakage issues and the overqualification of large language models (LLMs).",
            "purpose of benchmark": "The benchmark is intended to rigorously evaluate the incremental learning capabilities of LLMs by testing their ability to learn new concepts without forgetting previously acquired knowledge."
        },
        "problem": {
            "definition": "The benchmark is designed to address the problem of catastrophic forgetting in LLMs when they learn new concepts incrementally.",
            "key obstacle": "Existing benchmarks suffer from data leakage and do not adequately challenge the incremental learning abilities of LLMs."
        },
        "idea": {
            "intuition": "The creation of the benchmark is inspired by the need to evaluate how LLMs can learn new concepts incrementally, similar to human learning processes.",
            "opinion": "The authors believe that Concept-1K is a significant advancement in understanding the incremental learning capabilities of LLMs.",
            "innovation": "Concept-1K introduces a novel dataset that minimizes data leakage by focusing on recently emerged concepts and allows for fine-grained analysis of learning and forgetting processes.",
            "benchmark abbreviation": "Concept-1K"
        },
        "dataset": {
            "source": "The dataset was constructed by generating concepts using GPT-4 and manually filtering them to ensure they are recent and relevant.",
            "desc": "Concept-1K comprises 1,023 recently emerged concepts across various domains, supporting a larger number of incremental learning steps than existing benchmarks.",
            "content": "The dataset includes question-answer pairs representing each concept, allowing for diverse assessments of LLMs' knowledge retention.",
            "size": "1,023",
            "domain": "Incremental Learning",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "Memorization Accuracy (MA), Generalization Accuracy (GA)",
            "aspect": "The metrics measure how much knowledge is retained and how well new knowledge is generalized.",
            "principle": "These metrics were chosen to provide insights into the LLMs' ability to memorize training data and generalize to unseen test data.",
            "procedure": "Model performance is evaluated based on accuracy scores calculated from the model's outputs compared to the target outputs over multiple tasks."
        },
        "experiments": {
            "model": "The models tested include various sizes from the Pythia suite and LLaMa.",
            "procedure": "Models were trained incrementally on the Concept-1K dataset, with evaluations conducted after each task to assess memorization and generalization.",
            "result": "The experiments revealed that LLMs still suffer from catastrophic forgetting, and that data replay is the most effective method to mitigate this issue.",
            "variability": "Variability in results was accounted for through multiple trials and the evaluation of different model architectures."
        },
        "conclusion": "Concept-1K provides a robust benchmark for evaluating the incremental learning capabilities of LLMs, revealing significant insights into their strengths and limitations.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by offering a clear and interpretable framework for assessing incremental learning in LLMs.",
            "limitation": "The knowledge in Concept-1K is defined in triplet form, which may not encompass all forms of knowledge.",
            "future work": "Future research could explore the application of Concept-1K on larger models and investigate additional methods to enhance incremental learning."
        },
        "other info": {
            "info1": "The dataset and code are publicly available for further research.",
            "info2": {
                "info2.1": "The study emphasizes the importance of designing benchmarks that minimize data leakage.",
                "info2.2": "The findings highlight that larger models and more extensive pretraining contribute positively to incremental learning performance."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark is intended to rigorously evaluate the incremental learning capabilities of LLMs by testing their ability to learn new concepts without forgetting previously acquired knowledge."
        },
        {
            "section number": "3.1",
            "key information": "The experiments revealed that LLMs still suffer from catastrophic forgetting, and that data replay is the most effective method to mitigate this issue."
        },
        {
            "section number": "3.2",
            "key information": "Concept-1K introduces a novel dataset that minimizes data leakage by focusing on recently emerged concepts and allows for fine-grained analysis of learning and forgetting processes."
        },
        {
            "section number": "5.2",
            "key information": "The dataset includes question-answer pairs representing each concept, allowing for diverse assessments of LLMs' knowledge retention."
        },
        {
            "section number": "6.1",
            "key information": "Existing benchmarks suffer from data leakage and do not adequately challenge the incremental learning abilities of LLMs."
        }
    ],
    "similarity_score": 0.6947733369840997,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Can LLMs Learn New Concepts Incrementally without Forgetting_.json"
}