{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.10638",
    "title": "In-context Pretraining: Language Modeling Beyond Document Boundaries",
    "abstract": "Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Our experiments show IN-CONTEXT PRETRAINING offers a simple and scalable approach to significantly enhance LMs\u2019 performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).",
    "bib_name": "shi2024incontextpretraininglanguagemodeling",
    "md_text": "# IN-CONTEXT PRETRAINING: LANGUAGE MODELING BEYOND DOCUMENT BOUNDARIES\nWeijia Shi1,2 Sewon Min1,2 Maria Lomeli1 Chunting Zhou1 Margaret Li1,2 Gergely Szilvasy1 Rich James1 Xi Victoria Lin1 Noah A. Smith2,3 Luke Zettlemoyer1,2 Scott Yih1 Mike Lewis1 1Meta AI 2University of Washington 3 Allen Institute for AI swj0419@cs.washington.edu\n# ABSTRACT\nLarge language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Our experiments show IN-CONTEXT PRETRAINING offers a simple and scalable approach to significantly enhance LMs\u2019 performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).\narXiv:2310.10638v6\n# INTRODUCTION\nLarge language models (LMs) are trained to complete documents; each token is predicted given the context provided by the prefix of the document it appears in. Such contexts can be widely varied, especially at pretraining scale, allowing models to excel on diverse tasks such as instructionfollowing (Ouyang et al., 2022), conversational interfaces (OpenAI, 2023), reading comprehension (Zhang et al., 2020), and in-context learning (Brown et al., 2020). However, recent studies highlight that LMs sometimes struggle to understand more complex contexts: they can fail to follow instructions accurately (McKenzie et al., 2023; Efrat & Levy, 2020; Liu & Liu, 2023), struggle with reasoning over conditioned documents (Liu et al., 2023; Shi et al., 2023a), and exhibit high variance in in-context learning (Zhao et al., 2021). In this paper, we present IN-CONTEXT PRETRAINING, a new pretraining method that learns to predict tokens conditioned on a sequence of related documents, explicitly enabling the model to read and reason about much more varied and longer contexts that go beyond document boundaries. Current LM training pipelines concatenate random sets of shorter documents to create longer context windows. However, the prior documents provide no signal for predicting the next document, incurring unnecessary computational overhead for tokens that do not require communication between them (de Vries, 2023). IN-CONTEXT PRETRAINING instead reorders the pretraining data by combining several semantically related documents to create a coherent input context, thereby exposing LMs to long relevant contexts and providing pretraining signals beyond document boundaries. We illustrate this via an example in Figure 1: when predicting the following tokens for the phrase \u201cFor 2022, FIFA set the prize money at $42m,\u201d a previous document stating that the \u201cWorld Cup never awarded\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05ef/05efcf0e-68aa-47c6-a37c-c5d97d2f61f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2812/281234ed-c62f-40b8-88ff-0b5437892d55.png\" style=\"width: 50%;\"></div>\nFigure 1: Overview of IN-CONTEXT PRETRAINING. Different from the standard pretraining strategy that place randomly shuffled documents in the input context, IN-CONTEXT PRETRAINING places related documents in the same context, making models learn to reason across prior documents. For example, when predicting the following tokens for the phrase \u201cFor 2022, FIFA set the prize money at $42m,\u201d LMs could reference prior documents stating \u201cWorld Cup never awarded more than $10M before 2022\u201d and learn to infer that \u201cthe highest so far.\u201d\nmore than $10M before 2022\u201d could be in the context, enabling the prediction of a continuation like \u201cthe highest so far.\u201d As IN-CONTEXT PRETRAINING only changes document ordering and leaves all other aspects of LM pretraining untouched, it can be easily integrated into existing pretraining pipelines for large-scale LMs. However, this document sorting problem is challenging. LMs are typically trained on billions of documents and we would like to sort them to maximize document similarity in the input context windows without repeating any data. We introduce two new approximate algorithms to tackle these challenges. We use a retrieval model paired with an efficient search index to build a document graph that pairs each document with its nearest-neighbors based on its semantic similarity in the embeddings space. We also formulate document sorting as a travelling salesman problem, for which we develop an effective algorithm that maximizes similarity of documents to their context while also ensures that each document is included only once. To evaluate the effectiveness of IN-CONTEXT PRETRAINING, we pretrain language models from 0.3 to 7 billion parameters on 300 billion tokens from the CommonCrawl dataset (Wenzek et al., 2020). Across all model scales, IN-CONTEXT PRETRAINING LMs (ICLM) demonstrate strong language modeling and downstream task performance, outperforming LMs pretrained using the standard approach on the same corpus. We observe various improvements resulting from IN-CONTEXT PRETRAINING compared with existing LMs: (1) in-context learning with an average increase of 8% across 8 datasets; (2) reading comprehension, with an average of 15% improvement on 8 reading comprehension tasks; (3) outputs that are more faithful to prior contexts (+16%); (4) long context reasoning, showing a 5% boost; and (5) retrieval augmentation, leading to 9% gains when augmenting with external knowledge such as documents retrieved from Wikipedia. Our results demonstrate that, by simply altering order of the pretraining documents, IN-CONTEXT PRETRAINING offers a scalable and simple approach to significantly enhance understanding and reasoning over their full contexts. Code are publicly released at github.com/swj0419/in-context-pretraining.\n# 2 IN-CONTEXT PRETRAINING\nThe standard practice in pretraining is to form input contexts by concatenating random documents until reaching the maximum context length. It then trains the LM using a language modeling objective on the input contexts. However, training LMs on randomly concatenated documents does not offer additional learning signals compared with training on each document individually. In contrast, IN-CONTEXT PRETRAINING generates more coherent input contexts by concatenating semantically related documents together during pretraining. As depicted in Figure 2, IN-CONTEXT PRETRAINING consists of two steps: it first finds related documents at scale (\u00a72.1) and then constructs input contexts using these related documents (\u00a72.2). Successively, we use the contexts formed with\nsemantically related documents to pretrain LMs with a language modeling objective. Since INCONTEXT PRETRAINING is identical to existing pretraining recipes for LMs, except for changing how input contexts are built, it can be easily integrated into existing pretraining pipelines for largescale LMs.\n# 2.1 FINDING RELATED DOCUMENTS AT SCALE: RETRIEVING NEIGHBOR DOCUMENTS\n2.1 FINDING RELATED DOCUMENTS AT SCALE: RETRIEVING NEIGHBOR DOCUMENTS\nTo find related documents at scale, we link documents within the pretraining corpus D using a retrieval model. Specifically, for each document di \u2208D, a dense retrieval model is used to retrieve the top-k most similar documents, represented as N(di). The retrieval model uses approximate nearest neighbours search for efficient pairwise similarity comparison between any two documents, making it scalable for finding related documents in web-scale pretraining corpora.\nRetrieval. Our retrieval process employs the contriever model (Izacard et al., 2022). This model maps each document di \u2208D to an embedding E(di) by taking the mean pooling of the last hidden representation over the tokens in di. The cosine similarity is then used to determine the similarity between any two documents:\nThe retrieval model uses approximate nearest neighbour search with the faiss library (Johnson et al., 2019; Douze et al., 2024). We use product quantization (J\u00e9gou et al., 2011) to reduce the memory footprint and an IVF (inverted file) index structure to conduct efficient pairwise similarity search together with faiss big batch search. The OIVFBBS faiss framework is leveraged for this task, OIVFBBS refers to conducting offline search with queries of big batches with faiss inverted indexes. Further details can be found in Appendix A.2 and in the OIVFBBS demo in the faiss github repository github.com/facebookresearch/faiss/tree/main/demos/offline_ivf. During the retrieval process, when computing pairwise similarity among each document in the pretraining corpus, we found that the pretraining corpus contains many near duplicate documents. Hence, we further leverage the retrieval scores to eliminate near duplicate documents from the pretraining corpus. More details can be found in Appendix A.1. In \u00a74.2, we show that this deduplication step is crucial for achieving good performance of language models.\nThe retrieval model uses approximate nearest neighbour search with the faiss library (Johnson et al., 2019; Douze et al., 2024). We use product quantization (J\u00e9gou et al., 2011) to reduce the memory footprint and an IVF (inverted file) index structure to conduct efficient pairwise similarity search together with faiss big batch search. The OIVFBBS faiss framework is leveraged for this task, OIVFBBS refers to conducting offline search with queries of big batches with faiss inverted indexes. Further details can be found in Appendix A.2 and in the OIVFBBS demo in the faiss github repository github.com/facebookresearch/faiss/tree/main/demos/offline_ivf.\nDuring the retrieval process, when computing pairwise similarity among each document in the pretraining corpus, we found that the pretraining corpus contains many near duplicate documents. Hence, we further leverage the retrieval scores to eliminate near duplicate documents from the pretraining corpus. More details can be found in Appendix A.1. In \u00a74.2, we show that this deduplication step is crucial for achieving good performance of language models.\n# 2.2 CREATING INPUT CONTEXTS: DOCUMENT GRAPH TRAVERSAL\nGiven a set of documents D = {di} and nearest neighbours for each document N(di), our goal is to sort the documents to create input contexts such that each of them consists a list of related documents. Formally, we aim to form a set of input contexts C1 \u00b7 \u00b7 \u00b7 Cm where each context Ci = {d1, ...dk} \u2282D and m\ufffd Ci = D. Ideally, documents in Ci are nearest neighbors of each others.\n\ufffd A straightforward approach to form C1 \u00b7 \u00b7 \u00b7 Cm is to directly place each document and its retrieved topk documents together in the same input context (referred to as kNN), which has been used in some retrieval-augmented pretraining methods (Guu et al., 2020; Levine et al., 2022). This kNN approach maintains document similarity within each context but creates the data repeating problem: some documents frequently appear as nearest neighbors of other documents, causing that different input contexts contain overlapping documents, i.e., \u2203i \u0338= j, Ci \ufffdCj \u0338= \u2205. The data repeating problem exposes LMs to a less diverse set of documents given a fixed computational budget and could lead to overfitting of popular documents. Instead, we aim to build a set of contexts in a way that each document is included only once, which can be cast as a graph traversal problem.\n(1)\nAlgorithm 1 Maximum Traveling Salesman\nInput: Document graph G = (D, L)\nN(di) returns nearest neighbors for di\nmin_deg(D) returns a min-degree doc\nOutput: A path P\n1: P \u2190[]\n2: while |D| > 0 do\n3:\ndi \u2190min_deg(D)\n4:\nP.append(di)\n5:\nD.remove(di)\n6:\nwhile N(di) \u2229D \u0338= \u2205do\n7:\ndj \u2190arg mind\u2208N(di)\u2229D sim(di, d)\n8:\ndi \u2190dj\n9:\nP.append(di)\n10:\nD.remove(di)\n11:\nend while\n12: end while\n13: return P\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4729/4729182a-8011-47fb-a079-2b78c902a182.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 2: Illustration of IN-CONTEXT PRETRAINING. IN-CONTEXT PRETRAINING first finds related documents at scale to create a document graph (\u00a72.1) and then builds pretraining input contexts by traversing the document graph (\u00a72.2). Along the path, documents are concatenated into a sequence and subsequently divided to form fixed-sized input contexts (e.g., 8192 token length).\nDocument graph traversal. To achieve our goal of maximizing the chance that the related documents are concatenated together, an intuitive approach is to find a single path that visits each document once and maximize the chance that related documents are visited sequentially. Then we subsequently segment the path into multiple input contexts. We formulate it as the maximum traveling salesman problem (Flood, 1956) that aims to find the maximum weight path that traverses all nodes exactly once. We represent each document as a node in the graph and use document similarity as a edge weight. We design an undirected weighted graph representing the documents, symbolized as G = (D, L). Here, D represents the set of documents, while (d, d\u2217) \u2208L is a edge if d\u2217\u2208N(di) or di \u2208N(d\u2217). The weight of each edge corresponds to the document similarity (Equation 1). Solving large traveling salesman problems exactly is NP hard, but greedy algorithms are known to provide an efficient approximate solution. We adopt this approach, introducing modifications to better suit our context. Algorithm 1 shows the method to construct a maximum weight path. We show a path identified by our algorithm in Figure 2. Our algorithm starts by selecting a yet-to-be-visited document with the minimum degree as the starting node (Doc 0). The algorithm then progressively extends the current path by navigating to its unvisited neighboring document with highest weight (Doc 9), adding the document node to the path. This process continues until the path reaches a node where all neighboring documents have been visited, which happens because our graph is not complete, and only contains edges between documents where one is within the other\u2019s k nearest neighbors. In this case, we extend the graph with an edge of weight 0 to a random unvisited minimum degree document (Doc 1), and continue the above process. The motivation for starting at minimum degree documents is that they are most likely to have all their neighbors visited first, and therefore be connected to dissimilar documents in the final path. As a final step, we traverse the documents along the path and concatenate them to create fixed-sized input contexts suitable for pretraining. It is important to note that when forming the input training batches, we ensure the diversity among different input contexts within the same batch.\n# 3 EXPERIMENTS\nIn this section, we describe details of our pretraining setup (\u00a73.1), the baseline methods we use for comparison (\u00a73.2), and experimental results (\u00a73.3).\n# 3.1 PRETRAINING SETUP\nSince IN-CONTEXT PRETRAINING leaves other details of model training unchanged, and only changes the document ordering so that each context contains related documents, we can directly integrate it into pretraining pipelines as a preprocessing step during batching. For our experiment, we adopt the model architecture and pretraining objective of LLaMA (Touvron et al., 2023a;b) and pretrain LMs from scratch.\nPretraining Datasets. We use the English Commoncrawl dataset (Wenzek et al., 2020), the widelyused data source for pretraining LMs. Due to resource constraints, we randomly sample 235 million documents from this dataset, amounting to 306 billion tokens in total. We use the same pretraining data for all models.\nModel Details. We take the model architecture from LLaMA (Touvron et al., 2023a) and train models across various sizes: 0.3, 0.7, 1.5, and 7.0 billion parameters, all with an 8192-length context window. Following LLaMA, we employ the AdamW optimizer (Loshchilov & Hutter, 2018) with parameters \u03b21 = 0.9 and \u03b22 = 0.95, and a cosine learning rate schedule. The 7B model is pretrained using 128 A100 GPUs across 16 nodes with a batch size of 4 million tokens. It takes 9 days to train the 7B model on our pretraining dataset. Due to the long context window of our models, we use flash attention (Dao et al., 2022) to reduce memory consumption during pretraining. To perform the retrieval over our pretraining datasets, we employ the contriever model (Izacard et al., 2022) and encode the first 512 token of each document into an embedding. We then construct FAISS big batch search that is designed for conducting efficient similarity search with big batches of vectors (typically 50M\u2013100M vectors per batch). Given each query document, we retrieve top 10 documents (k=10). We split the data in batches of 50M embeddings, the search step is conducted in each batch before merging the results using 8 GPUs per batch. The total search time is 6 hours over 32 GPUs with average search time per batch is 4,738s. The document graph traversal phase requires 12 hours on a setup of 20 CPUs. More details are provided in the Appendix A.2.\n# More details are provided in the Appendix A.2.\n# 3.2 BASELINES\nWe compare IN-CONTEXT PRETRAINING with the following baselines: (1) Standard is the prior standard in pretraining that places randomly shuffled documents in the input contexts. This method is commonly adopted by existing models (Zhang et al., 2022; Scao et al., 2022; Touvron et al., 2023a). (2) kNN (also referred to as retrieval-augmented language model pretraining (Guu et al., 2020; Levine et al., 2022)) directly places each document and its retrieved top-k documents together in the same input context. Given the same number of training steps, kNN exposes LMs to a less diverse set of documents, since documents can repeat. For fair comparison, both standard and kNN methods are trained using the same pretraining data as IN-CONTEXT PRETRAINING and undergo an identical number of training steps, ensuring the same computation cost.\n# 3.3 RESULTS\nWe perform evaluations on tasks that require understanding of contexts including language modeling (\u00a7 3.3.1), in-context learning (\u00a7 3.3.2), reading comprehension (\u00a7 3.3.3) and open-book question answering (\u00a7 3.3.4), factuality (\u00a7 3.3.5) and long context reasoning (\u00a7 3.3.6).\n# 3.3.1 LANGUAGE MODELING\nDatasets & Metrics. We evaluate the language modeling perplexity of IN-CONTEXT PRETRAINING and baselines on the Wikipedia, Arxiv, and Books corpora. We follow the standard language modeling evaluation in concatenating randomly-ordered documents when computing perplexity.\nResults. Figure 3 shows average perplexity across different model sizes. First, kNN does not improve over the standard LM, likely due to the overfitting problem as discussed in \u00a72.2. ICLM, in contrast, outperforms both the standard LM and kNN on all three datasets, even when the evaluation documents are not sorted. The gains are consistent or larger as the size of the model scales. These improvements suggest that IN-CONTEXT PRETRAINING provides better pretraining signals, enabling LMs to better hone their language modeling abilities.\n# 3.3.2 IN-CONTEXT LEARNING FOR TEXT CLASSIFICATION\nDatasets & Metrics. In-context learning requires to perform a task without fine-tuning by conditioning on a few demonstration examples about the task. We evaluate the in-context learnig ability of ICLM using 32 demonstration examples. We use seven text classification datasets, including\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d278/d2784d97-45df-4d5c-bec7-99e355acf2bb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Figure 3: Language modeling perplexity (the lower the better) on Wikipedia, Arxiv, and Books (\u00a73.3.1). ICLM outperforms the baselines consistently across all model sizes.</div>\n<div style=\"text-align: center;\">Table 1: In-context learning performance on seven classification datasets (\u00a73.3.2). We use 32 incontext examples for all datasets. ICLM outperforms baselines on all datasets.</div>\nMethod\nSentiment\nHate Speech\nTopic Classification\nAverage\nAmazon\nSST2\nYelp\nHate\nOffensive\nAgnews\nDbpedia\nStandard\n94.6\n83.7\n74.3\n52.7\n55.7\n68.3\n61.5\n66.0\nkNN\n88.0\n80.2\n65.1\n50.1\n53.1\n65.7\n56.4\n61.8\nICLM\n96.5\n93.2\n77.4\n60.6\n57.3\n76.0\n63.2\n71.3\nsentiment analysis (SST-2 (Socher et al., 2013), Amazon and Yelp (Zhang et al., 2015a)), topic classificaiton (AGN (Zhang et al., 2015b) and Dbepdia (Lehmann et al., 2015)) and hate speech detection (Barbieri et al., 2020). We use label words from Min et al. (2022) and report accuracy as the metric.\nResults. As shown in Table 1, ICLM consistently demonstrates better performance across all text classification datasets, leading to 8% gain on average. This result suggests that ICLM is better at learning from demonstration examples. We later analyze the relationship between the number of demonstration examples and the performance of the in-context learning in \u00a74.3.\nDatasets & Metrics. Reading comprehension requires to answer the question based on the given paragraph. We consider the RACE reading comprehension benchmark (RACE-High and RACEMiddle) (Lai et al., 2017), SQuAD (Rajpurkar et al., 2016), BoolQ (Clark et al., 2019), DROP (Dua et al., 2019), and HotpotQA (Yang et al., 2018). We use 2-shot in-context learning for evaluation; we did not use more because some documents in reading comprehension tasks are very long. We report the exact match score for HotpotQA and SQuAD, and accuracy for other datasets that are multi-choice tasks (RACE, BoolQ, DROP), following the standard in prior work. Results. Table 2 highlights that ICLM consistently surpasses both the standard and kNN baselines across all datasets with an average improvement of 14%. In particular, we observe significant gains on HotpotQA, which requires multi-hop understanding of multiple related documents. The performance gain on reading comprehension tasks demonstrates that IN-CONTEXT PRETRAINING improves LMs\u2019 ability of undestanding and reasoning over the given context.\nDatasets & Metrics. Reading comprehension requires to answer the question based on the given paragraph. We consider the RACE reading comprehension benchmark (RACE-High and RACEMiddle) (Lai et al., 2017), SQuAD (Rajpurkar et al., 2016), BoolQ (Clark et al., 2019), DROP (Dua et al., 2019), and HotpotQA (Yang et al., 2018). We use 2-shot in-context learning for evaluation; we did not use more because some documents in reading comprehension tasks are very long. We report the exact match score for HotpotQA and SQuAD, and accuracy for other datasets that are multi-choice tasks (RACE, BoolQ, DROP), following the standard in prior work.\nResults. Table 2 highlights that ICLM consistently surpasses both the standard and kNN baselines across all datasets with an average improvement of 14%. In particular, we observe significant gains on HotpotQA, which requires multi-hop understanding of multiple related documents. The performance gain on reading comprehension tasks demonstrates that IN-CONTEXT PRETRAINING improves LMs\u2019 ability of undestanding and reasoning over the given context.\n3.3.4 RETRIEVAL-AUGMENTATION\n# 3.3.4 RETRIEVAL-AUGMENTATION\nDatasets & Metrics. Retrieval-augmentation is a method to retrieve a set of passages from the external text corpus (e.g., Wikipedia) and prepend it to the input query in order to better handle input queries that require factual knowledge (Lin et al., 2023; Xu et al., 2023; Su et al., 2023; Feng et al., 2024). We conduct evaluation on two well-studied open-domain QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). For both datasets, we report exact match scores (EM) and evaluate the model performance in both closed-book and open-book settings.\nTable 2: Reading comprehension results, using 2-shot in-context learning (\u00a73.3.3). ICLM outperforms baselines on all six datasets.\nMethod\nRACE-High\nRACE-Middle\nBoolQ\nSQuAD\nHotpotQA\nDROP\nAverage\nStandard\n39.5\n53.3\n68.9\n26.3\n10.5\n27.2\n37.6\nkNN\n36.2\n51.4\n65.3\n23.5\n14.4\n25.1\n36.0\nICLM\n41.5\n56.9\n73.0\n30.3\n21.9\n35.7\n43.2\nTable 3: Results on NQ and TQA (\u00a73.3.4) without retrieval (closed) and with retrieval (open).\n<div style=\"text-align: center;\">Table 3: Results on NQ and TQA (\u00a73.3.4) without retrieval (closed) and with retrieval (open).</div>\nMethod\nNQ\nTQA\nClosed\nOpen\nClosed\nOpen\nStandard\n17.0\n28.5\n49.3\n48.1\nkNN\n13.5\n20.1\n40.2\n43.2\nICLM\n17.0\n32.2\n48.0\n51.6\nIn the closed-book setting, we only provide the question to the model and the model has to answer the question based on its parametric knowledge. In the open-book setting, we follow Shi et al. (2023c) in providing the model with the top-10 retrieved documents from Wikipedia as additional context to the question.\nResults. Results are reported in Table 3. In the closed-book setting, ICLM performs comparably or slightly worse than the standard baseline, likely because our model memorizes less. Nonetheless, in the open-book setting, ICLM significantly outperforms the standard baseline in the open-book setting (+9%), obtaining much better performance than the closed book setting. It is also worth noting that the training objective of kNN is exactly the same as the retrieval-augmentation, but ICLM still achieves better performance, likely due to the overfitting problem of kNN as discussed in \u00a72.2.\n3.3.5 FACTUALITY\n# 3.3.5 FACTUALITY\nDatasets & Metrics. Prior work has found that language models generate text that is not factual nor faithful to the given context, especially when the context contradicts to knowledge the model has acquired during pretraining (often called parametric knowledge (Longpre et al., 2021; Zhou et al., 2023; Shi et al., 2023b; Wang et al., 2023a)). We evaluate LMs\u2019 ablities to follow instructions and contexts on two knowledge conflict datasets: NQ-Swap (Longpre et al., 2021) and MemoTrap (Liu & Liu, 2023). Both datasets contain instruction and contexts that are in conflict with the models\u2019 parametric knowledge. We report exact match score as the metric.\nResults. Table 4 shows that ICLM is better than the standard and kNN baselines on both datasets, implying that IN-CONTEXT PRETRAINING improves LMs\u2019 ability to generate outputs that are faithful to prior contexts. Gains are larger than those in other datasets, likely because NQ-Swap and MemoTrap highlight the challenge in reasoning about the given context, which the previous LMs struggle with.\n3.3.6 LONG CONTEXT REASONING\n# 3.3.6 LONG CONTEXT REASONING\nDatasets & Metrics. To evaluate the ability of long context reasoning, we compare ICLM with the standard and kNN baselines on the SCROLL benchmark (Shaham et al., 2022) that evaluates LMs\u2019 ability to synthesize information over long texts. Following the original paper setting, we finetune the pretrained LMs (standard, kNN, IN-CONTEXT PRETRAINING) on the training datasets of the scroll and evaluate them on the test datasets. We report F1 score for Narrative QA, Qasper and ContractNLI datasets and report ROUGE-1 score for QMSum and GovReport datasets in the SCROLL benchmark.\nResults. Results in Table 5 show that ICLM outperforms the baselines by around 5%, suggesting that ICLM is better at long context reasoning. We hypothesize that the gains from ICLM may fade\nTable 4: Results on two datasets with knowledge conflicts, requiring better reasoning of the given context (\u00a73.3.5).\nMethod\nNQ-Swap\nMemoTrap\nStandard\n39.6\n48.4\nkNN\n42.1\n54.3\nICLM\n45.8\n56.2\nable 5: Performance on long context reasoning benchmarks from SCROLL (Shaham et al., 2022) \u00a73.3.6). ICLM outperforms baselines on all five datasets.\n<div style=\"text-align: center;\">Table 5: Performance on long context reasoning benchmarks from SCROLL (Shaham et al., 2022) (\u00a73.3.6). ICLM outperforms baselines on all five datasets.</div>\nMethod\nNarrativeQA\nQasper\nContractNLI\nQMSum\nGovReport\nAverage\nF1\nROUGE-1\nStandard\n16.5\n34.2\n78.6\n25.1\n8.2\n32.5\nkNN\n16.8\n34.1\n79.5\n24.3\n6.6\n32.3\nICLM\n17.1\n36.7\n80.7\n26.8\n9.1\n34.1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6547/6547acaa-4f39-4aff-b9e2-f67c30f80f31.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Training loss and performance evolution on reading comprehension during pretraining. After training on around 150 billion tokens, ICLM is consistently better than the standard LM on reading comprehension and retrieval augmentation tasks.</div>\nMethod Design\nChoice\nPPL\nDocument\nRelevance\nRandom\n8.2\nClustering\n7.9\nLinks (final)\n7.3\nSemantic\nNo dedup\n8.3\nDedup\nDedup (final)\n7.3\nFigure 5: Ablation study of our method design.\n<div style=\"text-align: center;\">Figure 5: Ablation study of our method design.</div>\nout to some extent when the LMs are fine-tuned, which may explain the relatively small gains in this evaluation compared to our other experiments.\n# 4 ANALYSIS\n# 4.1 EVOLUTION OF PERFORMANCE DURING PRETRAINING\nThroughout the pretraining process, we closely monitor both the training loss and the downstream task performance for the ICLM as well as the standard LM. Figure 4 illustrates the trajectory of the training loss and the performance on the RACE reading comprehension tasks for the 7B models. The training loss for ICLM consistently remains lower than that of the standard LM. This suggests that, when predicting the next token, ICLM benefits from a richer set of relevant prior documents to refer to, while the standard LM has limited information to rely on, leading to higher loss. Figure 4 (b, c) shows that after training on around 150 billion tokens, ICLM is consistently better than the standard LM on reading comprehension tasks. This performance gap remains consistent throughout the remainder of the pretraining phase. This suggests the scale of improvements by IN-CONTEXT PRETRAINING does not diminish and remains consistent as training on more tokens.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0fc3/0fc31aad-67ed-4fed-b46a-9298187f7a39.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Performance with respect to the number of in-context examples (k).</div>\n# 4.2 ABLATION STUDY ON IN-CONTEXT PRETRAINING DESIGN\nWe perform analysis on two design choices of IN-CONTEXT PRETRAINING: a choice of methods for finding retrieved documents and deduplication. Ablations are done with 1.5B models and evaluated with perplexity on Wikipedia. The results are presented in Figure 5.\nWe perform analysis on two design choices of IN-CONTEXT PRETRAINING: a choice of methods for finding retrieved documents and deduplication. Ablations are done with 1.5B models and evaluated with perplexity on Wikipedia. The results are presented in Figure 5. Document relevance. A key design of IN-CONTEXT PRETRAINING is grouping documents by their relevance. We consider three levels of relevance: random (the standard baseline discussed in \u00a73.2), clustering, and our document linking method in IN-CONTEXT PRETRAINING. Clustering follows the method from Abbas et al. (2023) in clustering documents into 11k clusters based on their embeddings and sample documents from each cluster to form the training inputs. Documents grouped by clustering are sourced from the same clusters, indicating topical similarity but not necessarily close relation. In contrast, ICLM links documents as nearest neighbors, indicating a higher degree of similarity. The relevance between documents increases from random, clustering to linking. We observe that the perplexity of the language model decreases as the relevance increases. Deduplication. We compare perplexity of the models trained with and without the semantic deduplication step. Removing the semantic deduplication step leads to a significant decrease in perplexity. When near duplicate documents are present in the same context, language models might merely copy from the prior document, leading to training instability.\n# 4.3 DEMONSTRATION EXAMPLES SIZE FOR IN-CONTEXT LEARNING\nWe evaluate the 7B models trained with the standard method and IN-CONTEXT PRETRAINING, using a varying number of demonstration examples on text classification tasks described in \u00a73.3.2. As depicted in Figure 6, ICLM maintains consistent performance gains over the standard method, even as the number of demonstration examples grows. While the performance improves as the number of demonstration examples increases, it plateaus after 32 examples.\n# 5 RELATED WORK\nData batching based on similarity Previous work employs batching lexically similar segments in the same training batches to construct high-quality positive pairs for training retrieval-augmented language models. For instance, Zhong et al. (2022) use BM25 and same documents to ensure the segments in the same batch are similar to each other, while Min et al. (2023) group segments from the same documents in the same batch. Our method shares the same spirit with these methods except we maintain the relevance of documents in the same context window, yet context windows within batches are shuffled. Additionally, our focus is to apply the batching method to train the standard language models.\nPretraining with related documents. Several studies explore pretraining language models on a small-scale using related documents. For example, Yasunaga et al. (2022) incorporate Wikipedia documents with hyperlinks or citations into the input context and pretrain a masked LM. Yu et al. (2022); Wu et al. (2021) incorporate dictionary definitions of rare words or use contextual vectors from previously encountered contexts that mention these rare words during the pretraining phase. Caciularu et al. (2021) gather related documents using a human-curated multi-document news summarization dataset (11 million tokens) and continue to pretrain a masked LM. Lewis et al. (2020) place documents from the same date in the input context and pretrain LMs to summarize articles. However, hyperlinks are not always available across all domains and multi-document summarization datasets require human efforts to curate. Additionally, Lewis et al. (2020)\u2019s method restricts the scope of related documents to be from the same date. In contrast, we introduce a general method to collect web-scale related documents that does not require any metadata (e.g., hyperlinks, human curation or specific dates), which is necessary to scale the model to a pre-training setup.\nMultitask finetuning for in-context and instruction learning. Finetuning language models on a collection of downstream tasks to improve the instruction learning and in-context learning abilities of LMs has been investigated in several papers. As discussed by Min et al. (2022); Chen et al. (2022); Ivison et al. (2023); Wang et al. (2022; 2023b), a prevailing technique concatenates instructions,\ntraining samples from human-annotated downstream datasets into single text sequences, upon which the LM is subsequently finetuned. Following this line of work, Gu et al. (2023) create intrinsic downstream datasets by developing a task-specific retriever for each task. These retrievers are then used to retrieve demonstration examples from the pretraining corpus. The multitask finetuning method is complementary to IN-CONTEXT PRETRAINING as the former is tailored for the finetuning stage while the later focuses on the pretraining stage. Beyond improving LMs\u2019 in-context learning abilities, IN-CONTEXT PRETRAINING also improves their overall language modeling, reading comprehension, and fact-checking capabilities. We leave the combination of IN-CONTEXT PRETRAINING with multitask finetuning methods as future work. Training long-context language models. Recent studies have investigated the finetuning of LMs to extend their context length. Press et al. (2022); Chen et al. (2023); kaiokendev (2023) make modifications to position encoding and finetune LMs on randomly concatenated short documents and subsampled long documents from pretraining data. However, as highlighted by de Vries (2023), long sequence documents are notably rare in the pretraining data. For example, less than 5% of documents in CommonCrawl have longer than 2k tokens. In this work, we focus on constructing meaningful long-context data, making language models better leverage its context-window. Our sorted data can be used for both pretraining and finetuning stages to enhance LMs\u2019 ability to reason over contexts.\n# 6 CONCLUSION\nWe introduce IN-CONTEXT PRETRAINING, a new pretraining method that learns to generate text conditioned on a set of relevant documents, exposing LMs to relevant contexts and providing training signals beyond document boundaries. Our method is highly scalable and simple, and works with any pre-training pipeline by simply changing the document ordering during preprocessing. Our comprehensive evaluation demonstrates our method leads to significant improvements in a wide variety of settings that highlight the ability to understand and reason over the given context, including in-context learning, reading comprehension, retrieval augmentation, and more. Future research may delve into the inherent connections between documents within specific corpus domains or using multilingual retriever to group related multilingual documents in the same context. For example, the code scripts within the same repository are related. This insight paves the way for future exploration, where concatenating entire repositories into a unified whole could lead to the creation of meaningful long-context data sets.\n# REFERENCES\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540 2023.\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Dataefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1644\u20131650, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL https://aclanthology.org/2020.findings-emnlp.148. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew Peters, Arie Cattan, and Ido Dagan. CDLM: Cross-document language modeling. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2648\u20132662, Punta Cana, Dominican Republic, November 2021. Association\nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1644\u20131650, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL https://aclanthology.org/2020.findings-emnlp.148.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4849\u20134870, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023.acl-long.267. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929\u20133938. PMLR, 2020. Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning using cross-task nearest neighbors. In Findings of ACL, 2023. URL https://arxiv.org/abs/ 2212.00196. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id= jKN1pXi7b0. Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4849\u20134870, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023.acl-long.267. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929\u20133938. PMLR, 2020. Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning using cross-task nearest neighbors. In Findings of ACL, 2023. URL https://arxiv.org/abs/ 2212.00196. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id= jKN1pXi7b0. Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\u2013 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/ v1/P17-1147. URL https://aclanthology.org/P17-1147. Herv\u00e9 J\u00e9gou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33:117\u201328, 01 2011. doi: 10.1109/TPAMI.2010.57. kaiokendev. Things i\u2019m learning while training superhot, 2023. URL https://kaiokendev.github. io/til#. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785\u2013794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https: //aclanthology.org/D17-1082. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S\u00f6ren Auer, et al. Dbpedia\u2013a largescale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167\u2013195, 2015. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= lnEaqbTJIRz. Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing. Advances in Neural Information Processing Systems, 33:18470\u201318481, 2020. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrievalaugmented dual instruction tuning, 2023. Alisa Liu and Jiacheng Liu. The memotrap dataset. https://github.com/inverse-scaling/ prize/blob/main/data-release/README.md, 2023. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7052\u20137063, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.emnlp-main.565. URL https://aclanthology.org/2021.emnlp-main.565. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn\u2019t better, 2023.\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\u2013 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/ v1/P17-1147. URL https://aclanthology.org/P17-1147. Herv\u00e9 J\u00e9gou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33:117\u201328, 01 2011. doi: 10.1109/TPAMI.2010.57. kaiokendev. Things i\u2019m learning while training superhot, 2023. URL https://kaiokendev.github. io/til#. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785\u2013794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https: //aclanthology.org/D17-1082. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S\u00f6ren Auer, et al. Dbpedia\u2013a largescale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167\u2013195, 2015. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= lnEaqbTJIRz. Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing. Advances in Neural Information Processing Systems, 33:18470\u201318481, 2020. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrievalaugmented dual instruction tuning, 2023. Alisa Liu and Jiacheng Liu. The memotrap dataset. https://github.com/inverse-scaling/ prize/blob/main/data-release/README.md, 2023. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7052\u20137063, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.emnlp-main.565. URL https://aclanthology.org/2021.emnlp-main.565. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn\u2019t better, 2023.\nIan R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn\u2019t better, 2023.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2791\u20132809, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 201. URL https://aclanthology.org/2022.naacl-main.201. Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. Nonparametric masked language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 2097\u20132118, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https://aclanthology.org/2023.findings-acl.132.\nhttps://aclanthology.org/2023.findings-acl.132. OpenAI. Gpt-4 technical report, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383\u20132392, 2016. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 12007\u201312021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210\u201331227. PMLR, 2023a. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding, 2023b. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023c. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n# OpenAI. Gpt-4 technical report, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Resolving knowledge conflicts in large language models. arXiv preprint arXiv:2310.00935, 2023a.\nYike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, an Yulia Tsvetkov. Resolving knowledge conflicts in large language models. arXiv preprin arXiv:2310.00935, 2023a.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085\u20135109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL https://aclanthology.org/2022.emnlp-main.340. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and \u00c9douard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 4003\u20134012, 2020. Qiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. Taking notes on the fly helps language pre-training. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=lU5Rs_wCweN. Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369\u20132380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259. Michihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with document links. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8003\u20138016, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.551. URL https://aclanthology. org/2022.acl-long.551. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. 2023.\nWenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng, and Meng Jiang. Dict-BERT: Enhancing language model pre-training with dictionary. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 1907\u20131918, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.150. URL https://aclanthology.org/2022.findings-acl.150. Susan Zhang, Mona Diab, and Luke Zettlemoyer. Democratizing access to large-scale language models with opt-175b. Meta AI, 2022. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015a. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/ 250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015b. Zhuosheng Zhang, Hai Zhao, and Rui Wang. Machine reading comprehension: The role of contextualized language models and beyond. arXiv preprint arXiv:2005.06249, 2020. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021. Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5657\u20135673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.382. URL https://aclanthology.org/2022. emnlp-main.382. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large language models. ArXiv, abs/2303.11315, 2023.\n# A ADDITIONAL BACKGROUND\n# A.1 DEDUPLICATION\nCorpora often have semantic duplicates: pairs of documents that are semantically related, yet not entirely identical. Previous studies (Yasunaga et al., 2023) show that retraining highly similar documents in the input contexts during training hurts multimodal models\u2019 performance. We observed a similar behaviur: when near duplicate documents are present in the same context, language models might merely copy from the prior document, leading to training instability. Given that our retrieval approach inherently assesses pairwise document similarity, we can easily filter out near duplicate documents that have high cosine similarity with other documents. We find this deduplication step to be crucial for achieving performance of good language models (\u00a74.2).\n# A.2 FAISS INDEX\nWe used a product quantised inverted file (IVFPQ) FAISS index with a code size of 256 and the corresponding number of inverted lists 32768 , with total size of 62 gigabytes. The index contains 235266464 768-dimensional embeddings originally in float 32. The index was trained on a sample of 1572864 embeddings and the train time was 423 s. Successively, the data is split in batches of 50M embeddings and for each index shard the corresponding batch of embeddings is added to the trained index, the average adding embeddings time per index shard is 628.4 s. Finally, approximate nearest neighbor search is conducted per each shard before aggregating all results using faiss big batch search. The nprobe used for conducting approximate search is 64, this means that 0.2% of the inverted lists are probed during the nearest neighbors search.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of training language models (LMs) to predict tokens given document prefixes, highlighting the limitations of existing pretraining methods that concatenate random short documents without providing signals for predicting subsequent documents. The authors introduce a new method, IN-CONTEXT PRETRAINING, which encourages LMs to read and reason across related document sequences, aiming to enhance their performance on complex contextual reasoning tasks.",
        "problem": {
            "definition": "The problem is that existing pretraining methods for LMs do not effectively utilize the contextual relationships between documents, leading to poor performance in tasks requiring complex reasoning.",
            "key obstacle": "The primary challenge is the document sorting problem, where the goal is to maximize contextual similarity for each document in the input context without repeating any data."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that LMs can benefit from being exposed to sequences of related documents rather than random ones, as this can improve their reasoning capabilities.",
            "opinion": "IN-CONTEXT PRETRAINING involves reordering the pretraining data to create coherent contexts from semantically related documents, thus providing richer training signals.",
            "innovation": "This method differs from existing approaches by explicitly constructing input contexts from related documents, which enhances the model's ability to reason across document boundaries."
        },
        "method": {
            "method name": "IN-CONTEXT PRETRAINING",
            "method abbreviation": "ICLM",
            "method definition": "IN-CONTEXT PRETRAINING is a method that reorders documents during the pretraining of LMs to create input contexts consisting of related documents, enhancing their contextual understanding.",
            "method description": "The core of the method involves finding and sorting related documents to form coherent input contexts for LMs during pretraining.",
            "method steps": [
                "Identify related documents using a retrieval model.",
                "Sort the identified documents to maximize contextual similarity.",
                "Concatenate the sorted documents into input contexts for training."
            ],
            "principle": "The effectiveness of this method arises from its ability to expose LMs to longer, relevant contexts, thus providing them with better signals for understanding and generating text."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the CommonCrawl dataset, comparing ICLM with standard and kNN baselines across various tasks, including in-context learning and reading comprehension.",
            "evaluation method": "Performance was assessed through metrics such as accuracy, exact match scores, and perplexity across different datasets, with a focus on improvements in contextual reasoning abilities."
        },
        "conclusion": "IN-CONTEXT PRETRAINING significantly enhances LMs' performance across various tasks by improving their ability to understand and reason over longer contexts. The method is scalable and can be integrated into existing pretraining pipelines with ease.",
        "discussion": {
            "advantage": "The key advantage of ICLM is its ability to improve LMs' performance on tasks requiring complex contextual reasoning, leading to notable gains in various benchmarks.",
            "limitation": "A limitation of the method is that it may not fully address all aspects of document understanding, particularly in cases where the relationships between documents are less clear.",
            "future work": "Future research could explore methods to further improve the sorting algorithms and investigate the use of multilingual documents in the same context."
        },
        "other info": {
            "code repository": "The code for implementing IN-CONTEXT PRETRAINING is publicly available at github.com/swj0419/in-context-pretraining."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper introduces the concept of IN-CONTEXT PRETRAINING, which encourages language models to read and reason across related document sequences, enhancing their performance on complex contextual reasoning tasks."
        },
        {
            "section number": "1.3",
            "key information": "The method, IN-CONTEXT PRETRAINING, enhances large language models' ability to understand and reason over longer contexts by reordering documents during pretraining to create coherent input contexts."
        },
        {
            "section number": "2.1",
            "key information": "The paper discusses the limitations of existing pretraining methods that concatenate random short documents without providing signals for predicting subsequent documents, highlighting the need for improved contextual understanding."
        },
        {
            "section number": "3.1",
            "key information": "The method improves language models' adaptation to various contexts by maximizing contextual similarity for each document in the input context, addressing the document sorting problem."
        },
        {
            "section number": "4.1",
            "key information": "The design of IN-CONTEXT PRETRAINING significantly influences the outcomes of in-context learning by providing richer training signals through coherent contexts formed from semantically related documents."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of IN-CONTEXT PRETRAINING is that it may not fully address all aspects of document understanding, particularly in cases where the relationships between documents are less clear."
        },
        {
            "section number": "7",
            "key information": "The conclusion emphasizes that IN-CONTEXT PRETRAINING significantly enhances language models' performance across various tasks by improving their ability to understand and reason over longer contexts."
        }
    ],
    "similarity_score": 0.7084986517217532,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-context Pretraining_ Language Modeling Beyond Document Boundaries.json"
}