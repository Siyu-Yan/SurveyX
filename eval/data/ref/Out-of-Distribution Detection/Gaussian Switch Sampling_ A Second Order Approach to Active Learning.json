{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2302.12018",
    "title": "Gaussian Switch Sampling: A Second Order Approach to Active Learning",
    "abstract": "In active learning, acquisition functions define informativeness directly on the representation position within the model manifold. However, for most machine learning models (in particular neural networks) this representation is not fixed due to the training pool fluctuations in between active learning rounds. Therefore, several popular strategies are sensitive to experiment parameters (e.g. architecture) and do not consider model robustness to out-of-distribution settings. To alleviate this issue, we propose a grounded second-order definition of information content and sample importance within the context of active learning. Specifically, we define importance by how often a neural network \"forgets\" a sample during training - artifacts of second order representation shifts. We show that our definition produces highly accurate importance scores even when the model representations are constrained by the lack of training data. Motivated by our analysis, we develop Gaussian Switch Sampling (GauSS). We show that GauSS is setup agnostic and robust to anomalous distributions with exhaustive experiments on three in-distribution benchmarks, three out-of-distribution benchmarks, and three different architectures. We report an improvement of up to 5% when compared against four popular query strategies.",
    "bib_name": "benkert2023gaussianswitchsamplingsecond",
    "md_text": "@ARTICLE{benkert2022 TAI, author={R. Benkert, M. Prabhushankar, G. AlRegib, A. Parchami, and E. Corona} journal={IEEE Transactions on Artificial Intelligence}, title={Gaussian Switch Sampling: A Second Order Approach to Active Learning}, year={2023}\n\u00a92022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nrbenkert3@gatech.edu OR alregib@gatech.edu http://ghassanalregib.info/\nSUBITTED TO JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE\n# Gaussian Switch Sampling: A Second Order Approach to Active Learning\nan Benkert, Student Member, IEEE, Mohit Prabhushankar, Member, IEEE, Ghassan AlRegib, Fellow, IEEE Armin Pacharmi, Ford Motor Company, and Enrique Corona, Ford Motor Company\nAbstract\u2014In active learning, acquisition functions define informativeness directly on the representation position within the model manifold. However, for most machine learning models (in particular neural networks) this representation is not fixed due to the training pool fluctuations in between active learning rounds. Therefore, several popular strategies are sensitive to experiment parameters (e.g. architecture) and do not consider model robustness to out-of-distribution settings. To alleviate this issue, we propose a grounded second-order definition of information content and sample importance within the context of active learning. Specifically, we define importance by how often a neural network \u201dforgets\u201d a sample during training - artifacts of second order representation shifts. We show that our definition produces highly accurate importance scores even when the model representations are constrained by the lack of training data. Motivated by our analysis, we develop Gaussian Switch Sampling (GauSS). We show that GauSS is setup agnostic and robust to anomalous distributions with exhaustive experiments on three indistribution benchmarks, three out-of-distribution benchmarks, and three different architectures. We report an improvement of up to 5% when compared against four popular query strategies. Our code is available at https://github.com/olivesgatech/gauss. Impact Statement\u2014With the ever increasing demand for deep learning products in safety-critical applications, the acquisition of suitable training data has significantly increased in complexity. In several instances, labeling large quantities of data is associated with insurmountable costs (e.g. medical applications) while other instances require data diversity at scale with numerous edge cases (e.g. autonomous vehicles). Active learning offers a promising solution to both of these problems by selecting data to both improve annotation efficiency and data quality. For practical deployment, these algorithms must select robust datasets and further function in a wide variety of training setups in order to guarantee design requirements. However, existing algorithms base their acquisition function on the representation approaches which results in high performance fluctuations over training setups and robustness metrics. For instance, an algorithm might perform exceedingly well with one neural network architecture but underperform with another. Our work introduces a secondorder active learning approach for robustness and portability. We see our work as the first step of many in bringing theoretical active learning algorithms to real world deployment. Index Terms\u2014Active Learning, Learning Dynamics, Example Forgetting\n# I. INTRODUCTION\nA core factor in the success of machine learning algorithms is the selection of suitable data. While several samples contain\nThis paragraph of the first footnote will contain the date on which you submitted your paper for review. It will also contain support information, including sponsor and financial support acknowledgment. For example, \u201cThis work was supported in part by the U.S. Department of Commerce under Grant BS123456.\u201d This paragraph will include the Associate Editor who handled your paper.\nvaluable information for a given task, other samples may be redundant with little additional information or anomalous with contradicting features. Active Learning [14], [47] is a paradigm in machine learning that selects the most informative samples from a large unlabelled data pool for annotation and training. Due to its intuitive practicality, active learning has already impacted multiple industrial sectors including manufacturing [52], robotics [11], recommender systems [1], medical imaging [27], and autonomous vehicles [24]. At the core of every active learning approach stands its method to select the next set of annotation samples - its acquisition function. The function establishes a ranking based on information content and selects interactively from the most informative samples. Therefore, the success of an acquisition function heavily relies on 1) the definition of high information content and on 2) the information content approximation when the model representations are constrained due to the lack of annotated training data. In other words, a successful acquisition function relies on what it considers \u201cinformative\u201d and how well the model produces importance scores. Hence, an effective definition is both accurate in defining sample importance and simple to approximate by a constrained model. Intuitively, we would assume that an effective definition of informativeness would result in superior generalization performance. In our analysis, we find this assumption to be incorrect. In fact, definitions that produce accurate importance estimates can even hurt generalization performance for early active learning rounds when the model is heavily constrained. For a simple explanation, consider a toy classifier that distinguishes cats from dogs. The samples with the highest information content are frequently anomalous or subject to data noise. In our example, this could be an image where both a cat and a dog are present or the presence of a rare breed that significantly differs from the remaining data points. Within the context of active learning, training on highly informative (or anomalous) samples results in an inaccurate representation space and thus a lower generalization performance on the test set. We sketch such a scenario in Figure 1 and further show this behavior empirically in later sections. With acquisition functions that produce \u201ctoo accurate\u201d information content scores, the resulting model overfits to outlier data and therefore results in lower performance. Even though several existing approaches do not consider this essential design argument, they can still be effective in practical scenarios. In particular, existing algorithms (willingly or unwillingly) introduce noise within the acquisition function that results in inaccurate information content estimates and hence better performance.\nHowever, existing approaches define sample importance on the model representation directly which is noisy in early rounds due to the lack of data. As an example, entropy sampling [55] selects samples with the highest softmax entropy, a direct manifestation of the model representation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d60c/d60c50d1-99e9-4f12-9e4f-6a2d22927a28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Toy example of active learning with samples containing high/low information content. With a high information content outliers are selected for the training set Dtrain and can result in inaccurate decision boundaries.</div>\nEven though existing methods can be effective, relying on the representation directly has two hazardous implications: First, the acquisition function performance is highly dependent on experimental parameters (e.g. architecture) that directly influence the model representation. For this reason, several methods perform well for one parameter constellation but not in another. Second, the acquisition function does not consider model robustness to samples that do not originate from the training distribution (out-of-distribution). In particular, the model is incapable of reliably selecting informative samples that increase robustness due to the inherently unreliable model representations of outliers [26]. We show a toy example of two different protocols exposed to out-of-distribution settings in Figure 2. In active learning round N, we show two different protocols with similar test set performance. However, when exposed to out-of-distribution samples, the performance of both protocols significantly diverges and protocol 2 outperforms protocol 1 by a large margin. While papers on active learning generalization are ubiquitous, out-of-distribution active learning studies are significantly less common [34], [10]. For this purpose, popular active learning protocols lack robustness qualities and performance is unpredictable, especially in outof-distribution scenarios. In fact, our experiments in Section VI-A show significant performance variations of popular active learning approaches in out-of-distribution settings. In this paper, we propose a grounded approach to defining information content in active learning. Specifically, we consider a second-order approach where \u201cinformative\u201d is defined by representation shifts rather than the representation directly. We consider a sample most informative if it was \u201cforgotten\u201d the most during training. In practice, evaluating \u201cforgetting\u201d requires annotations which are not present within the unlabeled pool. For this purpose, we approximate \u201cforgetting\u201d as a switch within the sample prediction. This characteristic is especially important for selecting samples that make our model more robust to out-of-distribution samples. In a detailed analysis, we\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d69/2d6989ea-b581-4c1b-81df-74ac6ae35fe9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Toy example of out-of-distribution performance in active learning. Both protocols result in similar test performance but severely differ when deployed on out-of-distribution samples.</div>\nshow that our practical \u201cswitch\u201d definition is both exceedingly accurate in assessing sample importance as well as simple to approximate by a constrained model. To showcase the quality of our approach, we show samples with a high information content in Figure 3 according to different definitions of information content. Specifically, we show highly informative samples as by our definition and two popular definitions in active learning literature. We see that our informative samples are exceedingly harder to distinguish or are ambiguous which implies a higher information content. For instance, our \u201cinformative\u201d samples are ambiguous images with complex shapes or ambiguous colors (e.g. the frog image in the top row; third image from the right side). In contrast, the other definitions consider clear images informative that are highly similar among each other. For instance, entropy contains five similar car images that are clearly distinguishable. In order to provide favorable generalization performance, we further extend our method with an interactive sampling technique based on a gaussian mixture model. Our approach produces a targeted noise vector that successfully induces enough noise for model generalization while biasing the selection batch to highly informative samples that improve model robustness. We call our method Gaussian Switch Sampling or GauSS in short. We validate our algorithm with exhaustive experiments and compare against popular protocols used in practical active learning pipelines. Overall, we find that GauSS is robust to setup changes, performs favorably in outof-distribution settings, and achieves up to 5% improvement in terms of accuracy over popular existing strategies.\n# II. RELATED WORK\nA. First Order and Second Order Active Learning\nIn active learning, a strong research branch involves exploring different acquisition functions and their definitions of information content. In this context, the majority of existing approaches leverage the model representation directly to define\n<div style=\"text-align: center;\">RYAN BENKERT et al.: GAUSSIAN SWITCH SAMPLING: A SECOND ORDER APPROACH TO ACTIVE LEARNING</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c55/1c553ef0-f74f-4b87-8ff6-4895d8a2b856.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3. Examples of informative samples when using different definitions of information content. The examples are generated without limited data constrain From top to bottom: Our definition, entropy sampling, least confidence sampling.</div>\nthe next acquisition batch. Due to the direct dependency, we refer to these approaches as first-order acquisition functions. To the best of our knowledge, second-order approaches remain largely unexplored.\nOverall, active learning approaches differ in their definition of information content for samples within the unlabeled data pool. In this context, several approaches define information content with generalization difficulty. As an example, several approaches define sample importance using softmax probabilities [55], [44] where information content is related to the output logits of the network. For instance, entropy sampling queries samples that have the highest softmax entropy while least confidence sampling queries samples with the lowest prediction probability (or confidence) score. Closely related to this concept, the authors in [45], [53] query samples based on decision boundary proximity. For instance, [52] query samples with support vector machines. [19] define importance as model uncertainty and select the following sample batch based on Monte Carlo dropout. Finally [7] use an ensemble of classifiers to query the next set of samples. In contrast, other approaches define information content based on data representation within the dataset. Acquisition functions in this field are based on the assumption that representative samples best approximate the overall dataset structure and improve performance. In this context, a large group of approaches focus on constructing the core-set of the unlabeled data pool [46], [20]. Furthermore, [21] use a discriminative approach, where the authors reformulate the active learning problem as an adversarial training problem. Specifically, they formulate active learning as a binary classification problem and select samples that minimize the differences between the labeled and unlabeled pool. Finally, there are several approaches that consider the combination of both data representation and generalization\ndifficulty within their definition of information content. In several cases [5], [24], an ordering is established based on generalization difficulty and the representative component is introduced by sampling interactively from the ordering. [33] integrate both generalization difficulty and data representation by extending [28] to diverse batch acquisitions. Finally, [6], [29] deploy \u201dmix and match\u201d meta-active learning approaches, that switch strategies each active learning round. Even though several approaches are effective, they directly depend on the model representation of the data and are susceptible to setup shifts or out-of-distribution samples.\n# B. Out-of-Distribution Analysis\nWithin the field of deep learning, the importance of outof-distribution samples within the context of robustness is well established [26]. In this context, several approaches involve measurements of uncertainty [18], [31], [54], gradient representations [35], [36], [37], [39], [38], [42], [40], [3], or model calibration [22], [41]. For instance, [54] reject out-ofdistribution samples by ranking samples uncertainty scores from a deterministic neural network. [36], detect anomalies with by imposing a gradient constraint on the model that distinguishes inliers from outliers, [22] introduce temperature scaling as an effective method to calibrate neural networks. With a few exceptions, out-of-distibution scenarios remain largely unexplored within the context of active learning with the scarce examples [34] being first-order approaches.\n# C. Efficient Active Learning and Theory\nApart from developing new active learning acquisition functions, there has been extensive research regarding efficient active learning as well as its practical implementation. [15] improve computational efficiency by approximating the target\nmodel with a smaller proxy model. Further, [24] consider scalability issues of active learning for object detection. In particular they investigate the implications of varying diverse and uncertain active learning components on large-scale data. A substantial research direction is further dedicated to theoretical aspects of active learning. For instance, [56], [50], [23] consider subsampling in regression problems. Several works also focus on understanding the active learning process itself. In this context, [17] investigate the implicit bias inherent within the active learning process. In this work, we explore the capabilities of different protocols to assess information content. This represents a substantial contribution for designing efficient active learning paradigms as well as theoretic explanations for a acquisition function performance.\n# D. Continual Learning\nIn addition to active learning, our work is related to the field of continual learning and catastrophic forgetting [12], [32], [43], [51]. In this context, significant research efforts involve reducing forgetting through model constraints [2], [32], loss exploration [48], or augmentation [8], [9]. While the approaches effectively reduce forgetting, they do not consider (or estimate) information content in active learning settings Most significantly, our work shares a strong connection with the study of [51] on forgetting events. Within the article, the authors quantify neural networks forgetting in the training set and empirically show dependencies between forgetting and several important machine learning topics such as dataset compression and label noise. Even though the paper provides an interesting framework for quantifying neural network forgetting, the quantification requires labels and is unsuitable for information content estimation in practical active learning settings. In contrast, our work expands the concepts of [51] to function without annotation and develop a grounded definition of information content for active learning.\n# III. DEFINING SAMPLE INFORMATION WITH LEARNING DYNAMICS\nActive learning consists of iteratively selecting a set of unlabeled samples for annotation. We refer to a single iteration as active learning round. In this section we formulate information content as learning difficulty within each active learning round. Intuitively, we define informative as the frequency in which a network \u201cforgets\u201d unlabeled samples during training. We introduce active learning and neural network forgetting singularly and further analyze our definition in comparison to existing definitions of informativeness.\n# A. Active Learning\nIn active learning, the objective is to improve data efficiency iteratively by involving the model in the data annotation process. In this context, consider a dataset D where a small subset Dtrain represents the initial training data and Dpool represents the unlabelled data pool for the selection process. The goal within each active learning round is to select a batch\nof b samples X\u2217= {x\u2217 1, ..., x\u2217 b} that improves the model accuracy the most when added to the training data Dtrain. The selection of X\u2217can be defined as\n(1)\nwhere a represents the acquisition function and fw is the fully trained deep model conditioned on the parameters w. In this context, we define first-order algorithms and second-order algorithms by the manner in which they are conditioned on fw: Definition 1 (First- and Second-Order Active Learning). We define an active learning algorithm as first-order when the acquisition function is conditioned on the fully trained neural network fw exclusively. Further, an algorithm is of secondorder when the acquisition function is conditioned on dynamic fluctuations of fw caused by external interference (e.g. training).\nwhere a represents the acquisition function and fw is the fully trained deep model conditioned on the parameters w. In this context, we define first-order algorithms and second-order algorithms by the manner in which they are conditioned on fw:\n# B. Defining Importance with Forgetting Events\nIn this paper, we define importance through the learning difficulty of different unlabeled samples. Specifically, we build upon [51] and measure information content with the frequency in which samples are \u201dforgotten\u201d during training. Intuitively, a sample is \u201dforgotten\u201d if it was classified correctly (\u201dlearnt\u201d) at time t and subsequently misclassified (\u201dforgotten\u201d) at a later time t\u2032 > t.\nWithout loss of generalizability, we consider recognition tasks where the objective is to predict a label \u02dcyi of sample xi that corresponds to the ground truth annotation yi. The accuracy of the model for sample xi at an arbitrary time t can be defined as,\n(2)\nIn Equation 2, 1\u02dcyt i=yi describes a binary variable that indicates whether the classification was correct at time t. Specifically, it reduces to one if xi was correctly predicted or zero if the sample was misclassified. Based on our notation, we define a sample as \u201dforgotten\u201d if the accuracy decreases within two subsequent time steps:\n(3)\nHere, f t i is a binary result of accuracy variables and reduces to one if the accuracy within subsequent optimization steps decreases (a switch from correctly to incorrectly classified). Similar to [51], we define the binary event f t i as a forgetting event at time t. Based on these terminologies, we define the information content of each sample in a simple and intutive manner as follows:\nDefinition 2 (Information in Active Learning). Within the context of active learning, we quantify information by the amount of forgetting events that occur for a given sample each round. Specifically, we consider samples with the largest amount of forgetting events as the most informative while fewer forgetting events imply redundant information.\nDefinition 2 (Information in Active Learning). Within the context of active learning, we quantify information by the amount of forgetting events that occur for a given sample each round. Specifically, we consider samples with the largest amount of forgetting events as the most informative while fewer forgetting events imply redundant information.\nIn contrast to existing definitions, forgetting events are not based on the representation directly but are second-order artifacts of decision boundary shifts. We reason that this characteristic is favorable for distribution shifts and setup choices.\n# C. Prediction Switches\nEven though forgetting events are simple to formulate and provide an intuitive measure for importance, the computation is not tractable for practical active learning protocols. Specifically Equation 4 requires the label annotation yi which is unavailable for the unlabeled set Dpool by definition. For this purpose, we approximate forgetting events with prediction switches. A prediction switch occurs when the model output prediction \u02dcy changes within time intervals. Formally, we write\n(4)\nAnalogous to our previous definition, we call st i a switch event at time t.\nIV. INFORMATION DEFINITIONS IN ACTIVE LEARNING\nIn this section, we investigate the extent in which prediction switches differ to existing importance definitions. Specifically, we want to answer 1) how effectively do prediction switches estimate information content? and 2) how accurate are the importance scores when the representation is constrained? In practical settings, sample representations are constrained by the size of the training set Dtrain and can result in inaccurate importance quantification, especially when the definition is based on the representation directly. For this purpose, we conduct our analysis by artificially removing the constraint imposed by limited data and base our importance scores on fully trained model representations - i.e. the output of the acquisition function is based on representations derived from a model trained on both Dtrain and Dpool. Even though this analysis puts our method at a clear disadvantage we note that our importance rankings are significantly more accurate than existing protocols.\n# A. Accurate Importance Scores vs. Performance Trade-off\nIn active learning, acquisition functions define sample importance by the information content added to the dataset. However, informative samples in practice are frequently irregular and can result in insufficient data representations when trained on informative samples exclusively. Using our cat/dog example classifier from the introduction, a training set with informative samples could exclusively contain rare cat/dog breeds that are underrepresented within the underlying distribution. A classifier trained on this constrained dataset will result in low generalization performance on a test set containing mostly well represented breeds. Within the context of active learning, acquisition functions that render importance scores that are \u201ctoo accurate\u201d frequently result in significantly lower performance than the random baseline or other acquisition functions with \u201cless accurate\u201d importance scores.\nIn our analysis, we consider scenarios where the representation space is not constrained by limited data and therefore significantly improves the importance score accuracy of the compared strategies. For this reason, effective practical acquisition functions perform significantly lower than the random baseline and exhibit lower generalization performance with higher information content within the acquisition batch.\n# B. Importance in Optimal Settings\nWe investigate which acquisition functions provide the most informative samples when acquisition functions produce accurate measurements. For this purpose, we consider an optimal setting where we first train a model on the full CIFAR10 training data and derive the sample representations from the fully trained model. The following importance scores (e.g. softmax entropy) are subsequently calculated with the optimal sample representations. For our proposed definition, we count the switch events while training on the full training data and query the samples ordered from most to least switch events. In this section, we compare optimal settings only as the discussion on practical implementations with limited annotation access is thoroughly discussed in Section VI. In all of our active learning experiments, we start with an initial training set Dtrain of 128. In each round, we query 1024 additional samples with the repsective active learning strategy. For our active learning models we use a resnet18, as well as a resnet-34 architecture [25]. We optimize with the adam variant of SGD and a learning rate of 10\u22124. No augmentations are used for our active learning experiments. For our optimal representations, we train a network on the full CIFAR-10 dataset for 200 epochs and extract the representations for our active learning experiments. For the fully trained model, we optimize with SGD and use a multi step learning rate scheduler for training. Specifically, our initial learning rate is 0.1 and we divide the learning rate by 5 in epochs 100, 125, 150, and 175 respectively. We further use the augmentations random crop, random horizontal flip, and cutout. We choose this setup to derive competitive representations for the acquisition functions. We show the results of our analysis in Figure 4. Specifically, we compare switch events to entropy sampling [55], random sampling, least confidence sampling [55], and coreset [46]. We note that querying with optimal representations results in significantly lower generalization performance than the random baseline due to the high importance score accuracy. This is especially true for early rounds where only a limited amount of training data is available for the active learning model. Samples with a high information content are typically aberrant outliers that are disruptive to the representation space. Therefore, a more significant deviation from random sampling implies a higher information content. We note, that querying with switch events results in the most profound difference compared to the other strategies and implies the highest information content within each query. It is important to note that most existing active learning protocols outperform random sampling in practical settings - i.e. when the sample representations are inaccurate due to limited training data. In these cases, limited representation capabilities \u201chelp\u201d the acquisition function by introducing\na random noise factor within the query. Several strategies even introduce targeted sampling techniques to counteract accurate information scores [5], [24]. In GauSS we do this by sampling from a gaussian mixture model.\n# C. Approximation of Optimal Representations\nWe further investigate the approximation quality of different importance definitions with respect to their optimal counter part. As described within the previous section, most existing strategies outperform the random baseline while their optimal counter part significantly underperforms in terms of generalization accuracy. We follow that accuracy alone is not a sufficient metric to capture the optimal approximation capabilities of an active learning protocol. For this purpose, we measure protocol similarity through information content within the training set of each active learning round. In this context, we evaluate the importance score for each sample within the training set and compare the distributions of the importance scores for each round through statistical difference metrics. If the samples contain similar information, the resulting importance scores render similar distributions and vice versa. Specifically, we perform the following steps for each active learning round: 1) We evaluate the importance score for every training sample. 2) We estimate the distribution by creating histograms over the importance scores for every acquisition function. 3) We evaluate the statistical difference between different acquisition function pairs (in our case we use a smoothed KullbackLeibler divergence). We show our results in Figure 5. The different rows show the active learning protocols while the columns show the optimal counter parts. We compare switch events against entropy sampling and least confidence sampling as these are the most competitive definitions in our previous analysis. Further, we use forgetting events as the optimal counterpart of our switch event protocol. We choose this setup, as switch events should not only approximate their optimal switch event counterpart but should ideally approximate our original importance definition based on sample forgetting, not prediction switches. In our experiments, we use the same active learning setup as the previous section for experiments with optimal representations; The non-optimal protocol setup is identical with the exception of the data representation source - i.e. we follow the normal protocol workflow and gather sample representations from the model trained on Dtrain. We further generate importance scores for the histograms with forgetting events and compare histograms by calculating the Kullback-Leibler divergence smoothed by a gaussian filter. We average our result over the first 20 rounds. Overall, we note that switch events show the closest distribution similarity to all optimal strategies indicating the most accurate information content approximation within the round-wise training sets. We reason that switch events are not dependent on the model representation directly and are therefore more accurate in identifying the most informative batch regardless of the importance definition. This further manifests itself visually in the distribution histograms of individual rounds (Figure 6). We see that switch events exhibit the visually most similar distribution to optimal forgetting\nevents distribution than the other strategies. In particular, we note that the other strategies query more samples with less amounts of forgetting events than the switch event acquisition function which results in different distribution shapes. As a final note, we add that even though we compare distributions with forgetting events we observe a similar behavior when comparing against other importance definitions.\n# V. GAUSSIAN SWITCH SAMPLING\nBased on our initial analysis, we reason that switch events are effective for estimating information for unlabeled samples. Further, we reason that the second-order nature is potentially more robust to different training setups and out-of-distribution settings. However, we noted in our experiments that effective importance scores bias queries towards outliers that inhibit generalization capabilities. For this purpose, we force targeted random noise into our acquisition batch by probabilistic sampling with a bias towards higher switch events. Specifically, we model the switch event distribution with a two component gaussian mixture model and sample from the component with the higher switch event mean. Building on the classification of [51] into forgettable and unforgettable data, we model each sample within the unlabeled sampling pool to be either less-switching U or frequently switching F. The total amount of switch events Stotal can then be modeled as the sum of the switch events from both less switching SU and frequently switching samples SF :\n(5)\nWhen we query from the unlabeled pool, we would ideally target the most informative samples or, more specifically, the samples originating from class F. For this reason, we fit a gaussian mixture model with two components (one for U and one for F) to the switch event distribution and assign the lessswitching distribution and frequently switching distribution to the gaussian mixture with the lower and higher mean respectively. During selection, we distinguish frequently switching samples from less switching samples with the separate gaussian components and sample the next acquisition batch from the frequenly switching distribution. Mathematically, we model the switch event distribution as\n(6)\n  where \u03c9u/f, \u00b5u/f, and \u03c3u/f refer to the weight, mean, and standard deviation of the singular distribution components U and F respectively. We subsequently establish the acquisition batch by sampling data points with probabilities from the second switch event distribution component Nf:\n(7)\nIn Figure 7, we show a toy example of our approach. Specifically, we show a histogram over the switch events occurring for a given round and sketch the ideally fitted gaussian mixture components over the histogram distribution. In this case, we would sample from the right (green) gaussian component. We call our method Gaussian Switch Sampling or GauSS in short.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2fe8/2fe86f84-a9ae-40fd-8827-c8b52d6aee3f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ede/7ededba3-ce4f-405c-b593-7a5324abe113.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f69d/f69dcb3d-90dd-4357-b670-98f969733672.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFig. 5. Average KL-Divergence between the forgetting events distributions over 20 active learning rounds. We compare query strategies with their capability to approximate optimal settings with different importance definition. Columns represent optimal settings and rows show their practical approximations. Low scores indicate a high similarity in protocol behavior.\n# VI. EXPERIMENTS\nWith our experiments we want to answer two central questions: 1) How well does GauSS perform in conventional active learning where the only representation constraint is imposed by Dtrain?, and 2) How does the active learning performance change in out-of-distribution scenarios? We feel that both 1) and 2) are realistic scenarios in practical deployment and represent a comprehensive study of the generalization capabilites of GauSS.\n# A. Numerical Comparison\nIn our experiments, we compare GauSS against four popular active learning protocols. Specifically, we compare against entropy sampling [55], coreset [46], active learning by learning (ALBL) [29], least confidence sampling [55], and BatchBald [33]. We choose this constellation as it provides a large variety of importance definitions: Entropy sampling and least confidence sampling define sample importance with generalization\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f077/f077f5e4-ebc4-4700-b6ee-d6c97cd75a3c.png\" style=\"width: 50%;\"></div>\ndifficulty while coreset maximizes the diversity of the training pool; active learning by learning represents a bandit style approach that interactively switches between coreset and least confidence during each round. It represents a fusion protocol that combines both generalization difficulty and data diversity. Finally, Batchbald is a recent bayesian approach that extends the popular BALD [28] method. We consider three out-of-distribution settings as well as three in-distribution settings. For our out-of-distribution analysis, we sample our training set from the CIFAR10 training set and measure accuracy on CIFAR10-C [26], STL10 [13], and CINIC10 [16]. We choose these benchmarks, as they contain examples of data corruption and domain shifts both of which are common in practical machine learning pipelines: Both STL10 and CINIC10 represent examples of cross domain out-of-distribution. STL10 is a difficult dataset with 8000 test samples and nine out of ten overlapping classes with CIFAR10 (we only consider the overlapping classes for accuracy calculations). CINIC10 is significantly larger than CIFAR10 with a test set of 90000 images and fully overlapping classes with CIFAR10. Furthermore, CIFAR10-C is an artificially corrupted version of CIFAR10 with 20 different corruption types on five different severity levels. We consider all corruptions except \u201clabels\u201d, \u201cshot noise\u201d, and \u201cspeckle noise\u201d and test on level two as well as level five each round. We choose this setup as it includes realistic data corruptions on a wide variety of severity levels. Our in-distribution analysis considers the datasets CIFAR10, CINIC10, and CURE-TSR [49]. Both CIFAR10 and CINIC10 are popular in-distribution benchmarks with increasing difficulty and dataset size (60000 images for CIFAR10 and 270000 images for CINIC10). CURE-TSR represents the practically relevant application of traffic sign recognition with corrupted and uncorrupted training and test sets. The dataset is especially challenging as traffic signs occur at different frequencies resulting in a high class imbalance. Hence, an effective algorithm must perform on both well represented as well as underrepresented classes to improve generalization performance on the test set. In our experiments, ee consider the uncorrupted training and test set only. For all datasets except CURE-TSR, we start with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/448e/448eb270-c042-4bf1-9df1-36ebc9f52a48.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">g. 6. Histograms for different definitions of information content in different rounds. The first row, samples data points with the highest forgetting events thin an optimal setting - i.e. the importance scores are derived from a model trained on the fully labeled training set. The remaining rows are practica tive learning scenarios where the importance scores are derived from a model with a constrained training pool. This includes entropy sampling, coreset ast confidence sampling and prediction switches.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0545/054508ca-021e-4117-81ef-cb30889c41d3.png\" style=\"width: 50%;\"></div>\nFig. 7. Toy example of gaussian switch sampling. We sketch the switch event distribution within a single active learning round, as well as two gaussian mixture components. The next batch is generated by sampling from the most switching component in green.\n<div style=\"text-align: center;\">Fig. 7. Toy example of gaussian switch sampling. We sketch the switch event distribution within a single active learning round, as well as two gaussian mixture components. The next batch is generated by sampling from the most switching component in green.</div>\nan initial training pool of 128 randomly chosen samples and iteratively query 1024 samples each round according to the respective protocols. For BatchBald, we implement a bayesian model by applying monte-carlo droupout. Similar to the setup of [33], we apply a dropout layer with a dropout probability of 0.4 before the final fully connected layer and sample 70 monte-carlo instances. When experimenting with CURE-TSR, we start with an initial training pool of 32 samples and query an additional 32 samples each round. We choose this setup as it\nsignificantly increases the difficulty for traffic sign recognition. We further use three popular network architectures: Resnet18 [25], resnet-34 [25], and densenet-121 [30]. Similar to our previous experiments, we optimize with the Adam variant of SGD with a learning rate of 10\u22124 and no learning rate scheduler. No data augmentations are used and we use pretrained model weights for our out-of-distribution experiments as well as our experiments on the CIFAR10 dataset. We reason that this represents a practical scenario where data is abundant in one domain but scarce in another. Finally, we retrain our model from scratch each round to prevent warm starting [4]. For all constellations, our results are averaged over 5 random seeds. Overall, this amounts to a total of 450 separate active learning experiments (we count all CIFAR10-C datasets as one experiment). In several cases, the performance of active learning strategies differs across selection rounds. For instance, a strategy may show a strong accuracy increase for early rounds, and underperform in later rounds. Therefore, we quantify algorithm performance by summarizing the margin in which it outperforms (or underperforms) the random baseline. With random baseline, we refer to sampling random data points from the unlabeled pool each round. First, we calculate the difference to the random baseline for each round by subtracting the random baseline accuracy curve from the accuracy curve of the respective strategy. Second, we combine the difference values by integrating over the subtraction curve across several active learning rounds. In short, the metric amounts to the\ndifference in accuracy area compared to the random baseline. For reference, we provide the absolute area of the random baseline in the first row of each architecture. A negative difference (random accuracy is higher) contributes negatively to the integral and positive differences (comparison accuracy is higher) contributes positively to the integral. Hence, a negative integral indicates an overall higher random accuracy curve while a positive integral indicates an overall outperformance with respect to the random baseline. In addition, the higher the integral the more significant the positive difference and the larger the outperformance margin. By quantifying performance with differences to the same reference point, we remove biases from absolute values in each round. For instance, the difference in early rounds contributes to the integral with the same magnitude as later rounds even though the absolute accuracy is higher in later stages. For our results on CIFAR10C, we average the difference curve accross all corruptions and levels. We show the integration results in Table I. We note, that GauSS shows the overall best performance over both in-distribution, and out-of-distribution benchmarks in nearly every experiment constellation. In particular, we note that GauSS shows an overall better performance than the random baseline in nearly every constellation. In contrast, the other strategies can outperform or underperform the random baseline. For instance, entropy sampling outperforms the random baseline in several cases (e.g. CIFAR10-C with resnet-34), but underperforms in other cases (e.g. CIFAR10 in-distribution test set with densenet-121). Moreover, we note that GauSS is more consistent accross different dataset and architecture choices. In all instances except a few outliers, GauSS outperforms the random baseline. In contrast, we find high fluctuations with competitive protocols. For instance, least confidence sampling significantly outperforms the random baseline on CIFAR10-C and CIFAR10 when using resnet-18, but underperforms when switching the architecture to densenet-121. We reason that GauSS queries more consistently as sample queries and model representation are not directly coupled. If the model representation severely suffers within a specific experiment configuration (e.g. when densenet-121 is used instead of resnet-18), the acquisition batch severely degenerates. In contrast GauSS does not query based on the representation exclusively. Therefore switching experiment components does not affect the overall protocol performance. Finally, we discuss the behavior of GauSS in settings with high class imbalances on CURE-TSR. In our experiments, GauSS outperforms on two of the three architecture choices and consistently shows positive improvements over the random baseline (i.e. positive integral values). Similar to our previous observations, competitive strategies are strong in one setting but underperform the random baseline in another. For instance, entropy sampling underperforms the random baseline on resnet-18 but outperforms on resnet-34. We follow that GauSS proposes consistent and accurate data samples even when high imbalances are present. In addition to Table I, we provide absolute values of the different strategies (as well as the random baseline) in Table II. Specifically, we show accuracy values in rounds one, four, eight, and nine on CIFAR10, as well as its corrupted variant\nCIFAR10-C. We choose this constellation as it represents one early round, one intermediate round, and two late-stage rounds. Further, we conduct a t-test to determine the statistical significance of our results, where one refers to a statistically significant difference to GauSS, and zero to a statistically insignificant difference with GauSS. We note, that GauSS matches or outperforms the competing strategies across all rounds: GauSS either outperforms or is statistically insignificant with respect to the outperforming strategy. In contrast, existing strategies are either competitive in early rounds or later rounds but rarely show consistent performance across the entire active learning experiment.\n# B. Learning Curves\nWe further plot the learning curves of several experiment configurations in Figure 8. Specifically, we show the results of several configurations using a resnet-18 architecture on indistribution, as well as out-of-distribution experiments. As outof-distribution examples, we plot the CIFAR10-C corruptions brightness level two, JPEG compression level five, as well as the CINIC10 test set when training on CIFAR10. Further, we plot the in-distribution datasets CIFAR10, CININC10, and CURE-TSR. The learning curves, further confirm our observations from Table I. GauSS qualitatively outperforms the other strategies over a large variety of in-distribution, and out-of-distribution experiments. We further note, that the performance of GauSS is especially strong in early rounds where the training set contains few samples. We reason that the representation is especially inaccurate for early active learning rounds and that resulting importance metric is severly affected for the competitive strategies. For later rounds, the model develops a more accurate representation space and we observe GauSS outperforming by a significantly smaller margin. A further explanation for the superior performance is the targeted sampling approach in GauSS. As previously discussed, representation noise can be beneficial in scenarios where accurate importance definitions result in difficult outlier selections that can potentially damage the model representation (see our experiments with optimal representations in Section IV-B). This is especially true for the two protocols least confidence sampling, and entropy sampling that have a forcefully added random query noise due to the limited representation capabilities of the model. In contrast, GauSS adds targeted (or biased) noise that, in combination with accurate importance measurements, results in a more conclusive data representation when the representation is severely susceptible to outlier samples. While we only show a small subset of learning curves, we note that this behavior is consistent across the other experiment constellations as well but are omitted due to space limitations. With 16 different CIFAR10-C corruptions at two levels as well as the remaining benchmarks this amounts to 555 different active learning curves. In summary, we can report an improvement of up to 5% compared to the other strategies.\n# C. Incremental Analysis\nWithin this subsection, we investigate the performance of GauSS in settings with higher task complexity. For this pur-\nTABLE I AREA UNDER DIFFERENCE CURVE WITH REFERENCE TO RANDOM SAMPLING OVER SEVERAL DATASETS, QUERY STRATEGIES, AND ARCHITECTURES. POSITIVE, AND HIGHER NUMBERS ARE BETTER. NEGATIVE NUMBERS IMPLY THAT THE APPROACH HAS A LOWER ACCURACY CURVE THAN THE RANDOM BASELINE. THE SPREAD REPRESENTS THE STANDARD DEVIATION. RANDOM REFERENCE AREA SHOWS THE TOTAL AREA UNDER THE RANDOM BASELINE ACCURACY CURVE.\n# TABLE I\nOut-Of-Distr.\nIn-Distr.\nArchitectures\nAlgorithms\nCIFAR10-C\nCINIC10\nSTL10\nCIFAR10\nCINIC10\nCURE-TSR\nResNet-18\nRand. Reference\n649.53\n666.06\n399.29\n831.19\n636.35\n972.48\nALBL [29]\n10.19 \u00b1 8.24\n5.66 \u00b1 8.08\n-4.55 \u00b1 4.66\n7.25 \u00b1 6.86\n-2.54 \u00b1 5.12\n22.80 \u00b1 1.48\nCoreset [46]\n-41.87 \u00b1 2.02\n-35.07 \u00b1 2.13\n-20.58 \u00b1 3.14\n-41.04 \u00b1 0.71\n-48.54 \u00b1 3.11\n-38.60 \u00b1 0.83\nEntropy [55]\n7.83 \u00b1 1.03\n8.58 \u00b1 0.76\n2.16 \u00b1 3.79\n0.45 \u00b1 0.34\n-6.09 \u00b1 3.44\n-25.02 \u00b1 8.55\nL. Conf. [55]\n16.23 \u00b1 1.17\n13.47 \u00b1 0.12\n3.20 \u00b1 1.42\n9.93 \u00b1 0.94\n-2.54 \u00b1 1.22\n5.74 \u00b1 6.28\nBatchBald [33]\n-20.98 \u00b1 2.40\n-19.93 \u00b1 0.75\n-14.71 \u00b1 0.01\n-17.49 \u00b1 2.33\n-356.14 \u00b1 5.30\n-2.56 \u00b1 3.17\nGauSS\n17.76 \u00b1 1.61\n13.72 \u00b1 0.13\n7.17 \u00b1 4.48\n17.77 \u00b1 0.89\n5.23 \u00b1 0.47\n33.55 \u00b1 6.13\nDenseNet-121\nRand. Reference\n653.55\n674.07\n413.96\n860.81\n661.66\n1018.35\nALBL [29]\n7.69 \u00b1 1.41\n6.67 \u00b1 2.45\n-8.56 \u00b1 2.26\n3.28 \u00b1 3.08\n-4.58 \u00b1 2.61\n30.91 \u00b1 9.30\nCoreset [46]\n-23.40 \u00b1 2.32\n-14.36 \u00b1 0.70\n-24.98 \u00b1 1.94\n-21.74 \u00b1 2.89\n-12.72 \u00b1 0.77\n31.66 \u00b1 6.83\nEntropy [55]\n1.77 \u00b1 0.68\n2.52 \u00b1 2.75\n-10.25 \u00b1 5.34\n-7.76 \u00b1 4.20\n-23.17 \u00b1 3.92\n8.33 \u00b1 6.34\nL. Conf. [55]\n-0.20 \u00b1 1.59\n3.81 \u00b1 1.89\n-11.48 \u00b1 4.65\n-3.57 \u00b1 4.17\n-17.19 \u00b1 5.08\n43.25 \u00b1 8.85\nBatchBald [33]\n-8.85 \u00b1 0.47\n-11.98 \u00b1 0.01\n-1.15 \u00b1 1.79\n-10.51 \u00b1 0.64\n-383.24 \u00b1 19.51\n60.13 \u00b1 9.30\nGauSS\n13.64 \u00b1 1.96\n10.01 \u00b1 0.52\n\u22121.28 \u00b1 0.64\n11.46 \u00b1 0.18\n4.52 \u00b1 2.61\n53.25 \u00b1 5.47\nResNet-34\nRand. Reference\n705.04\n708.21\n427.31\n893.57\n687.10\n1018.21\nALBL [29]\n7.56 \u00b1 0.09\n9.67 \u00b1 2.42\n0.41 \u00b1 4.06\n13.96 \u00b1 2.40\n-4.55 \u00b1 3.78\n28.63 \u00b1 4.16\nCoreset [46]\n-8.25 \u00b1 0.17\n-7.18 \u00b1 0.24\n-9.23 \u00b1 4.28\n-5.21 \u00b1 0.27\n-14.16 \u00b1 3.70\n-9.40 \u00b1 6.88\nEntropy [55]\n14.80 \u00b1 0.16\n15.31 \u00b1 0.53\n5.93 \u00b1 1.34\n12.51 \u00b1 1.03\n-12.49 \u00b1 3.61\n10.01 \u00b1 6.47\nL. Conf. [55]\n17.52 \u00b1 1.21\n16.60 \u00b1 1.33\n4.91 \u00b1 5.17\n15.99 \u00b1 0.36\n-4.40 \u00b1 0.83\n47.01 \u00b1 3.83\nBatchBald [33]\n-25.90 \u00b1 0.39\n-25.84 \u00b1 0.93\n-11.85 \u00b1 0.76\n-26.10 \u00b1 1.49\n-391.38 \u00b1 11.78\n0.93 \u00b1 3.13\nGauSS\n18.14 \u00b1 0.96\n16.58 \u00b1 1.17\n2.12 \u00b1 2.66\n19.30 \u00b1 1.16\n9.34 \u00b1 1.04\n25.44 \u00b1 9.62\nTABLE II ACCURACY VALUES AND BINARY STATISTICAL SIGNIFICANCE OVER ACTIVE LEARNING ROUNDS ONE, FOUR, EIGHT, AND NINE ON CIFAR10 AND CIFAR10-C. A SIGNIFICANCE OF ONE INDICATES A STATISTICALLY SIGNIFICANT DIFFERENCE TO GAUSS WHILE ZERO INDICATES A STATISTICALLY INSIGNIFICANT DIFFERENCE WITH GA USS. WE SHOW OUR RESULTS ON RESNET-18 AND DENSENET-121.\n1152 Samples\n4224 Samples\n8320 Samples\n9344 Samples\nDataset\nAlgorithms\nAcc.\nStat. Sign.\nAcc.\nStat. Sign.\nAcc.\nStat. Sign.\nAcc.\nStat. Sign.\nResNet-18\nCIFAR10\nRandom\n42.48\n0\n57.05\n0\n63.89\n1\n65.02\n1\nALBL [29]\n39.14\n1\n57.16\n1\n64.39\n0\n65.89\n0\nCoreset [46]\n38.00\n1\n52.07\n1\n61.96\n1\n63.21\n1\nEntropy [55]\n37.35\n1\n54.90\n1\n64.02\n1\n65.00\n1\nL. Conf. [55]\n38.90\n1\n56.11\n1\n64.29\n0\n65.47\n0\nBatchBald [33]\n43.36\n0\n57.25\n0\n63.20\n1\n64.10\n1\nGauSS\n42.00\nBaseline\n57.06\nBaseline\n64.76\nBaseline\n66.11\nBaseline\nResNet-18\nCIFAR10-C\nRandom\n33.27\n0\n44.19\n0\n49.49\n1\n50.72\n1\nALBL [29]\n31.01\n1\n44.64\n0\n50.46\n0\n51.60\n0\nCoreset [46]\n29.83\n1\n39.49\n1\n47.44\n1\n48.44\n1\nEntropy [55]\n30.17\n1\n42.88\n1\n50.28\n0\n51.33\n0\nL. Conf. [55]\n31.15\n1\n43.68\n1\n50.45\n0\n51.59\n0\nBatchBald [33]\n33.97\n0\n43.69\n0\n48.83\n1\n49.92\n1\nGauSS\n33.09\nBaseline\n44.43\nBaseline\n50.43\nBaseline\n51.72\nBaseline\nDenseNet-121\nCIFAR10\nRandom\n44.17\n0\n59.27\n0\n66.73\n0\n67.63\n1\nALBL [29]\n38.53\n1\n59.34\n0\n67.22\n0\n68.81\n0\nCoreset [46]\n36.13\n1\n58.08\n1\n65.89\n1\n67.30\n1\nEntropy [55]\n37.53\n1\n57.54\n1\n66.94\n0\n68.17\n0\nL. Conf. [55]\n39.06\n1\n58.07\n1\n66.62\n0\n67.62\n1\nBatchBald [33]\n43.84\n0\n60.45\n0\n66.17\n0\n66.84\n1\nGauSS\n43.38\nBaseline\n59.53\nBaseline\n67.02\nBaseline\n69.01\nBaseline\nDenseNet-121\nCIFAR10-C\nRandom\n33.21\n0\n44.74\n0\n50.31\n1\n51.42\n1\nALBL [29]\n29.53\n1\n45.27\n0\n51.26\n0\n52.64\n0\nCoreset [46]\n26.94\n1\n43.25\n1\n49.52\n1\n50.55\n1\nEntropy [55]\n29.29\n1\n43.69\n1\n50.71\n1\n51.72\n1\nL. Conf. [55]\n30.07\n1\n44.18\n0\n50.19\n1\n51.63\n1\nBatchBald [33]\n32.87\n0\n45.43\n0\n50.12\n1\n50.82\n0\nGauSS\n33.06\nBaseline\n44.75\nBaseline\n51.65\nBaseline\n52.76\nBaseline\npose, we consider an incremental analysis where we perform active learning on a subset of the CIFAR100 dataset with 5, 10, 20, and 50 of the 100 classes respectively. In our experiments, we use a resnet-18 architecture and opt for a similar setup as our previous experiments on CIFAR10. In particular, we start with an initial training set size of 128 and query 1024 samples\n<div style=\"text-align: center;\">In-Distr.</div>\neach round until the entire unlabeled pool is fully annotated. We show the resulting learning curves of the four data subsets in Figure 9. We note that settings with less classes contain fewer samples in the data pool and are therefore fully queried in less rounds.\nOverall, GauSS shows favorable qualities with a higher\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f503/f503a31a-8e47-4916-8ab7-3269064b2799.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8. Learning curves of in-distribution, as well as out-of-distribution experiments with the resnet-18, resnet-34, and densenet-121. Top: Out-of-distribution experiments with resnet-18 on brightness level two (CIFAR10-C), JPEG compression level five (CIFAR10-C), and in-distribution experiment on CINIC10 with resnet-34. Bottom: In-distribution experiments with CIFAR10 with desnsenet-121, CINIC10 with densenet-121, and CURE-TSR with resnet-18.</div>\nnumber of tasks. While lower task quantities do not show distinguishable trends, GauSS performs well in complex settings with higher amounts of tasks. In particular, we note that GauSS performs well regardless of the active learning round. In contrast, the competitive strategies show a strong performance in early or later active learning rounds exclusively. As an example, entropy and least confidence sampling perform well in later rounds and underperform in early stages. On the other hand, Batchbald is particularly strong in early stages but is outperformed later stages where the data representations are more mature.\n# VII. CONCLUSION\nIn this paper, we introduced a grounded definition of information content for active learning protocols. In contrast to existing approaches, our definition is not based on the representation directly but defines importance with decision boundary shifts - or neural network \u201cforgetting\u201d. For practical usage, we approximate \u201cforgetting\u201d with prediction switches and find that prediction switches produce accurate importance scores when compared to other definitions. Finally, we develop a novel acquisition function that samples interactively with a gaussian mixture model. We validate our algorithm empirically with exhaustive experiments and compare against popular protocols used in practical active learning pipelines. Overall, GauSS is robust to setup changes, performs favorably in outof-distribution settings, and achieves up to 5% improvement in terms of accuracy.\nThe authors would like to thank the remaining members of OLIVES and the anonymous reviewers for their feedback. In particular, we would like to thank Dr. Gukyeong Kwon for fruitful discussions within the preliminary phase of the project. This work was funded by a Ford-Gerogia Tech Alliance Project.\n# REFERENCES\n[1] G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering, 17(6):734\u2013749, 2005. [2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), pages 139\u2013154, 2018. [3] Ghassan AlRegib and Mohit Prabhushankar. Explanatory paradigms in neural networks. arXiv preprint arXiv:2202.11838, 2022. [4] Jordan T. Ash and Ryan P. Adams. On the difficulty of warm-starting neural network training. CoRR, abs/1910.08475, 2019. [5] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671, 2019. [6] Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. Journal of Machine Learning Research, 5(Mar):255\u2013291, 2004. [7] William H. Beluch, Tim Genewein, Andreas N\u00a8urnberger, and Jan M. K\u00a8ohler. The power of ensembles for active learning in image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e18/6e185198-1b01-4708-bc7a-83d2a1fb4828.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9. Incremental analysis on CIFAR100. We perform active learning with a resnet-18 architecture on 5, 10, 20, and 50 of the 100 CIFAR100 cla respecitvely. We compare GauSS with coreset, entropy, least confidence, active-learning-by-learning, and batchbald.</div>\n[8] Ryan Benkert, Oluwaseun Joseph Aribido, and Ghassan AlRegib. Explaining deep models through forgettable learning dynamics. In 2021 IEEE International Conference on Image Processing (ICIP), pages 3692\u20133696. IEEE, 2021. [9] Ryan Benkert, Oluwaseun Joseph Aribido, and Ghassan AlRegib. Example forgetting: A novel approach to explain and interpret deep neural networks in seismic interpretation. IEEE Transactions on Geoscience and Remote Sensing, 2022. [10] Ryan Benkert, Mohit Prabhushankar, and Ghassan AlRegib. Forgetful active learning with switch events: Efficient sampling for out-ofdistribution data. In 2022 IEEE International Conference on Image Processing (ICIP), pages 2196\u20132200. IEEE, 2022. [11] Sylvain Calinon, Florent Guenter, and Aude Billard. On learning, representing, and generalizing a task in a humanoid robot. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 37(2):286\u2013298, 2007. [12] Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3):1\u2013207, 2018. [13] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011. [14] David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129\u2013145, 1996. [15] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. arXiv preprint arXiv:1906.11829, 2019.\n[16] Luke Nicholas Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. CINIC-10 is not imagenet or CIFAR-10. CoRR, abs/1810.03505, 2018. [17] Sebastian Farquhar, Yarin Gal, and Tom Rainforth. On statistical bias in active learning: How and when to fix it, 2021. [18] Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. arXiv preprint arXiv:1506.02158, 2015. [19] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International Conference on Machine Learning, pages 1183\u20131192. PMLR, 2017. [20] Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. CoRR, abs/1711.00941, 2017. [21] Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv preprint arXiv:1907.06347, 2019. [22] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321\u20131330. PMLR, 2017. [23] Lei Han, Kean Ming Tan, Ting Yang, and Tong Zhang. Local uncertainty sampling for large-scale multi-class logistic regression. arXiv preprint arXiv:1604.08098, 2016. [24] Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet, and Jose M Alvarez. Scalable active learning for object detection. In 2020 IEEE Intelligent Vehicles Symposium (IV), pages 1430\u20131435. IEEE, 2020. [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n[26] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [27] Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch mode active learning and its application to medical image classification. In Proceedings of the 23rd international conference on Machine learning, pages 417\u2013424, 2006. [28] Neil Houlsby, Ferenc Husz\u00b4ar, Zoubin Ghahramani, and M\u00b4at\u00b4e Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011. [29] Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015. [30] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016. [31] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in neural information processing systems, pages 5574\u20135584, 2017. [32] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs/1612.00796, 2016. [33] Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. CoRR, abs/1906.08158, 2019. [34] Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: Submodular information measures based active learning in realistic scenarios. Advances in Neural Information Processing Systems, 34, 2021. [35] Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, and Ghassan AlRegib. Distorted representation space characterization through backpropagated gradients. In 2019 IEEE International Conference on Image Processing (ICIP), pages 2651\u20132655. IEEE, 2019. [36] Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, and Ghassan AlRegib. Backpropagated gradient representations for anomaly detection. In European Conference on Computer Vision, pages 206\u2013226. Springer, 2020. [37] Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, and Ghassan AlRegib. Novelty detection through model-based characterization of neural networks. In 2020 IEEE International Conference on Image Processing (ICIP), pages 3179\u20133183. IEEE, 2020. [38] Jinsol Lee and Ghassan AlRegib. Gradients as a measure of uncertainty in neural networks. CoRR, abs/2008.08030, 2020. [39] Jinsol Lee and Ghassan AlRegib. Open-set recognition with gradientbased representations. In 2021 IEEE International Conference on Image Processing (ICIP), pages 469\u2013473. IEEE, 2021. [40] Mohit Prabhushankar and Ghassan AlRegib. Contrastive reasoning in neural networks. arXiv preprint arXiv:2103.12329, 2021. [41] Mohit Prabhushankar and Ghassan AlRegib. Introspective learning: A two-stage approach for inference in neural networks. 2021. [42] Mohit Prabhushankar, Gukyeong Kwon, Dogancan Temel, and Ghassan AlRegib. Contrastive explanations in neural networks. In 2020 IEEE International Conference on Image Processing (ICIP), pages 3289\u2013 3293. IEEE, 2020. [43] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, pages 3738\u20133748, 2018. [44] Dan Roth and Kevin Small. Margin-based active learning for structured output spaces. In European Conference on Machine Learning, pages 413\u2013424. Springer, 2006. [45] Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. In ICML, volume 2, page 6. Citeseer, 2000. [46] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489, 2017. [47] Burr Settles. Active learning literature survey. 2009. [48] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and XiaoMing Wu. Overcoming catastrophic forgetting in incremental fewshot learning by finding flat minima. Advances in Neural Information Processing Systems, 34, 2021. [49] D. Temel, G. Kwon, M. Prabhushankar, and G. AlRegib. CURE-TSR: Challenging unreal and real environments for traffic sign recognition. In Neural Information Processing Systems (NeurIPS) Workshop on Machine Learning for Intelligent Transportation Systems, 2017.\n[50] Daniel Ting and Eric Brochu. Optimal subsampling with influence functions. In Advances in neural information processing systems, pages 3650\u20133659, 2018. [51] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. [52] Simon Tong. Active learning: theory and applications, volume 1. Stanford University USA, 2001. [53] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. Journal of machine learning research, 2(Nov):45\u201366, 2001. [54] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep deterministic neural network. In International conference on machine learning, pages 9690\u20139700. PMLR, 2020. [55] Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112\u2013119. IEEE, 2014. [56] HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal of the American Statistical Association, 113(522):829\u2013844, 2018.\nRyan Benkert is a Ph.D. student in the Omni Lab for Intelligent Visual Engineering and Science (OLIVES) at the Georgia Institute of Technology. In his research, he addresses fundamental challenges in machine learning that bridge the gap between academic research and industrial deployment. His interests include active learning, uncertainty estimation, and neural network learning dynamics. Prior to Georgia Tech, he received his B.Sc and M.Sc from the RWTH Aachen University in Germany. In his free time, Ryan dances west coast swing and is an\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f18/0f180214-504e-480a-b0e7-4b1b42977100.png\" style=\"width: 50%;\"></div>\navid sports enthusiast.\nMohit Prabhushankar received his Ph.D. degree in electrical engineering from the Georgia Institute of Technology (Georgia Tech), Atlanta, Georgia, 30332, USA, in 2021. He is currently a Postdoctoral Fellow in the School of Electrical and Computer Engineering at the Georgia Institute of Technology in the Omni Lab for Intelligent Visual Engineering and Science (OLIVES) lab, working in the fields of image processing, machine learning, and explainable and robust AI. He is the recipient of the Best Paper award at ICIP 2019 and Top Viewed Special Session\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f5c/2f5c43ef-3359-49db-b86f-57414d98a43e.png\" style=\"width: 50%;\"></div>\nPaper Award at ICIP 2020. He is the winner of the Roger P Webb ECE Graduate Research Excellence award in 2022. He has served as a Teaching Fellow at Georgia Tech since 2020. He is an IEEE Member.\nGhassan AlRegib received his Ph.D. in electrical engineering from the Georgia Institute of Technology (Georgia Tech), Atlanta, Georgia, 30332, USA. He is currently the John and Marilu McCarty Chair Professor in the School of Electrical and Computer Engineering at the Georgia Institute of Technology. He was a recipient of the ECE Outstanding Graduate Teaching Award in 2001 and both the CSIP Research and the CSIP Service Awards in 2003, the ECE Outstanding Junior Faculty Member Award, in 2008, and the 2017 Denning Faculty Award for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f903/f903f094-1d85-4cf8-906f-e44ab0f97062.png\" style=\"width: 50%;\"></div>\nGlobal Engagement. His research group, the Omni Lab for Intelligent Visual Engineering and Science (OLIVES) works on research projects related to explainable machine learning, robustness in intelligent systems, interpretation of subsurface volumes, and expanding healthcare access and quality. He has participated in several service activities within the IEEE and served on the editorial boards of several journal publications. He served as the TP co-Chair for ICIP 2020 and GlobalSIP 2014. He served as expert witness on several patents infringement cases and advised several corporations on both technical and educational matters. He is IEEE Fellow.\nSUBITTED TO JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE\nSUBITTED TO JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE\nArmin Parchami recieved his B.E. in Software Engineering and M.Sc. in Artificial Intelligence from Bu-Ali Sina University and then he received his Ph.D. in Computer Science from UTA in 2017. His dissertation was on single shot face recognition using deep learning algorithms for security applications. Between 2017 and 2022, he was working at Ford on level 4 autonomous vehicles. He is currently managing the perception team at Ford ADAS developing perception algorithms for L2+ autonomy. His current research interests include monocular 3D earning, and wide baseline sensor fusion.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/178a/178ae271-b91e-46fc-8b21-217645d72f7f.png\" style=\"width: 50%;\"></div>\nobject detection, active learning, and wide baseline sensor fusion.\nEnrique Corona received his B.S.E.E. from Universidad de las Americas Puebla, Mexico with a joint M.Sc. in Electrical and Computer Engineering. He was awarded a Ph.D. in Electrical Engineering from Texas Tech University in 2012. His dissertation was about unsupervised learning methods for applications to segmentation of medical images using kernel mappings to improve clustering model selection. He then worked in the design and implementation of intelligent home appliances using computer vision until 2016. After joining Ford Motor Co. he started\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5be3/5be3e9e9-44d8-4fe7-a678-65833f50deb9.png\" style=\"width: 50%;\"></div>\nworking in the development of technology for smart city infrastructure and L4 autonomous vehicles. At present he is part of Ford ADAS collaborating in L2+ autonomy projects. He is currently interested in monocular 3D object detection and tracking, multiple view and geometric computer vision, and kernel methods.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of selecting suitable data in active learning, highlighting the limitations of existing methods that rely directly on model representation, which can lead to performance fluctuations based on experimental parameters and robustness to out-of-distribution samples.",
        "problem": {
            "definition": "The problem is the ineffective selection of informative samples in active learning due to reliance on model representation, which is noisy in early training rounds and does not account for out-of-distribution scenarios.",
            "key obstacle": "The core obstacle is that existing acquisition functions are sensitive to model architecture and do not consider robustness to samples that do not originate from the training distribution."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that sample importance can be defined by how often a neural network 'forgets' a sample during training, which reflects second-order representation shifts.",
            "opinion": "The proposed idea, Gaussian Switch Sampling (GauSS), involves defining sample importance through the frequency of prediction switches rather than direct model representation.",
            "innovation": "The key innovation is the introduction of a second-order approach that uses prediction switches to determine sample importance, making it more robust to varying training setups and out-of-distribution scenarios."
        },
        "method": {
            "method name": "Gaussian Switch Sampling",
            "method abbreviation": "GauSS",
            "method definition": "GauSS defines sample importance based on the frequency of prediction switches during training, allowing for a more accurate assessment of sample informativeness.",
            "method description": "The method involves sampling interactively from a Gaussian mixture model that captures the distribution of switch events.",
            "method steps": [
                "Train a neural network on the initial dataset.",
                "Monitor prediction switches for each sample during training.",
                "Fit a Gaussian mixture model to the switch event distribution.",
                "Sample from the component with the higher mean switch event count to form the acquisition batch."
            ],
            "principle": "This method is effective because it captures the dynamics of sample importance through forgetting events, which are less affected by noise in the model representation."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three in-distribution benchmarks (CIFAR10, CINIC10, CURE-TSR) and three out-of-distribution benchmarks (CIFAR10-C, STL10, CINIC10), using various architectures (ResNet-18, ResNet-34, DenseNet-121).",
            "evaluation method": "Performance was measured by comparing the accuracy of GauSS against four popular active learning protocols, calculating the area under the difference curve relative to a random sampling baseline."
        },
        "conclusion": "GauSS demonstrates robustness to setup changes, performs favorably in out-of-distribution settings, and achieves up to 5% improvement in accuracy compared to existing strategies.",
        "discussion": {
            "advantage": "The key advantages of GauSS include its setup-agnostic nature and its ability to maintain performance across different architectures and data distributions.",
            "limitation": "A limitation of the method is its reliance on accurate estimation of switch events, which may not always be feasible in practical scenarios with noisy data.",
            "future work": "Future research could explore further refinements to the Gaussian mixture model and investigate the application of GauSS in more complex active learning scenarios."
        },
        "other info": {
            "info1": "The authors acknowledge contributions from members of the OLIVES lab and funding from a Ford-Georgia Tech Alliance Project.",
            "info2": {
                "info2.1": "Ryan Benkert is a Ph.D. student focusing on machine learning and active learning.",
                "info2.2": "Mohit Prabhushankar is a Postdoctoral Fellow specializing in image processing and robust AI."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper highlights the limitations of existing methods in active learning, which rely on model representation, leading to performance fluctuations based on experimental parameters and robustness to out-of-distribution samples."
        },
        {
            "section number": "2.1",
            "key information": "The problem is defined as the ineffective selection of informative samples in active learning due to reliance on noisy model representation and failure to account for out-of-distribution scenarios."
        },
        {
            "section number": "3.5",
            "key information": "The proposed method, Gaussian Switch Sampling (GauSS), is an innovative approach that defines sample importance through the frequency of prediction switches during training, making it robust to varying training setups and out-of-distribution scenarios."
        },
        {
            "section number": "5.2",
            "key information": "The paper discusses how GauSS demonstrates robustness to setup changes and performs favorably in out-of-distribution settings, achieving up to 5% improvement in accuracy compared to existing strategies."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of GauSS is its reliance on accurate estimation of switch events, which may not always be feasible in practical scenarios with noisy data."
        }
    ],
    "similarity_score": 0.650664992438831,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Gaussian Switch Sampling_ A Second Order Approach to Active Learning.json"
}