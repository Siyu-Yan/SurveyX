{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.05218",
    "title": "Density estimation with LLMs: a geometric investigation of in-context learning trajectories",
    "abstract": " ABSTRACT\nABSTRACT\nLarge language models (LLMs) demonstrate remarkable emergent abilities to perform in-context learning across various tasks, including time series forecasting. This work investigates LLMs\u2019 ability to estimate probability density functions (PDFs) from data observed in-context; such density estimation (DE) is a fundamental task underlying many probabilistic modeling problems. We leverage the Intensive Principal Component Analysis (InPCA) to visualize and analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is that these LLMs all follow similar learning trajectories in a low-dimensional InPCA space, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). We interpret the LLaMA in-context DE process as a KDE with an adaptive kernel width and shape. This custom kernel model captures a significant portion of LLaMA\u2019s behavior despite having only two parameters. We further speculate on why LLaMA\u2019s kernel width and shape differs from classical algorithms, providing insights into the mechanism of in-context probabilistic reasoning in LLMs.\n# INTRODUCTION\nModern Large Language Models (LLMs) showcase surprising emergent abilities that they were not explicitly trained for (Brown et al., 2020; Dong et al., 2024), such as learning from demonstrations (Si et al., 2023) and analogies in natural language (Hu et al., 2023). Such capacity to extract patterns directly from input text strings, without relying on additional training data, is generally referred to as in-context learning (ICL). Recently, LLMs have been shown to achieve competitive performance on various mathematical problems, including time-series forecasting (Gruver et al., 2024), inferring physical rules from dynamical systems (Kantamneni et al., 2024; Liu et al., 2024), and learning random languages (Bigelow et al., 2024). To solve these tasks, an LLM must possess some capacity for probabilistic modelin",
    "bib_name": "liu2024densityestimationllmsgeometric",
    "md_text": "DENSITY ESTIMATION WITH LLMS: A GEOMETRIC INVESTIGATION OF IN-CONTEXT LEARNING TRAJECTORIES\n# ni J.B. Liu\u2217, Nicolas Boull\u00e9\u2020, Rapha\u00ebl Sarfati\u2217, Christopher J. Earl\n\u2217Cornell University, USA, \u2020Imperial College London, UK Correspondence: jl3499@cornell.edu\n\u2217Cornell University, USA, \u2020Imperial College London, UK Correspondence: jl3499@cornell.edu\n9 Oct 2024\n# ABSTRACT\nABSTRACT\nLarge language models (LLMs) demonstrate remarkable emergent abilities to perform in-context learning across various tasks, including time series forecasting. This work investigates LLMs\u2019 ability to estimate probability density functions (PDFs) from data observed in-context; such density estimation (DE) is a fundamental task underlying many probabilistic modeling problems. We leverage the Intensive Principal Component Analysis (InPCA) to visualize and analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is that these LLMs all follow similar learning trajectories in a low-dimensional InPCA space, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). We interpret the LLaMA in-context DE process as a KDE with an adaptive kernel width and shape. This custom kernel model captures a significant portion of LLaMA\u2019s behavior despite having only two parameters. We further speculate on why LLaMA\u2019s kernel width and shape differs from classical algorithms, providing insights into the mechanism of in-context probabilistic reasoning in LLMs.\n# INTRODUCTION\nModern Large Language Models (LLMs) showcase surprising emergent abilities that they were not explicitly trained for (Brown et al., 2020; Dong et al., 2024), such as learning from demonstrations (Si et al., 2023) and analogies in natural language (Hu et al., 2023). Such capacity to extract patterns directly from input text strings, without relying on additional training data, is generally referred to as in-context learning (ICL). Recently, LLMs have been shown to achieve competitive performance on various mathematical problems, including time-series forecasting (Gruver et al., 2024), inferring physical rules from dynamical systems (Kantamneni et al., 2024; Liu et al., 2024), and learning random languages (Bigelow et al., 2024). To solve these tasks, an LLM must possess some capacity for probabilistic modeling \u2014 the ability to infer conditional or unconditional probability distribution structures by collecting relevant statistics from in-context examples (Aky\u00fcrek et al., 2024). We investigate LLMs\u2019 ability to perform density estimation (DE), which involves estimating the probability density function (PDF) from data observed in-context. Our core experiment is remarkably straightforward. As illustrated in Figure 1, we prompt LLaMA-2 models (Touvron et al., 2023) with a series of data points {Xi}n i=1 sampled independently and identically from an underlying distribution p(x). We then observe that the LLaMA\u2019s predicted PDF, \u02c6pn(x), for the next data point gradually converges to the ground truth as the context length n (the number of in-context data points) increases..1 To interpret the internal mechanisms (Olsson et al., 2022; Bietti et al., 2023; Dai et al., 2023; von Oswald et al., 2023) underlying an LLM\u2019s in-context DE process, we leverage Intensive Principal Component Analysis (InPCA) (Teoh et al., 2020; Quinn et al., 2019; Mao et al., 2024) to embed the\n1Our prompts consist purely of comma-delimited multi-digit numbers, without any natural language instruc tions.\nestimated PDF at each context length { \u02c6Pn} in probability space (Figure 2). These visualizations reveal that the in-context DE process of these models all follow similar low-dimensional paths, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). By studying the geometric features of these in-context DE trajectories, we identify a strong bias towards Gaussianity, which we argue is a telltale feature of kernel-based density estimationRosenblat (1956); Wand & Jones (1994); Silverman (2018). This observation inspires us to model LLaMA in-context DE process as a kernel density estimator with adaptive kernel. Despite having only two parameters, kernel shape and width, this bespoke KDE model captures LLaMA\u2019s in-context learning trajectories with high precision. =     \nestimated PDF at each context length { \u02c6Pn} in probability space (Figure 2). These visualizations reveal that the in-context DE process of these models all follow similar low-dimensional paths, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). =     \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa91/aa91f1b8-de69-45a3-9d2a-e0d23e87589c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: In-context density estimation experiment. LLaMA-2 is prompted with 200 numbers sampled from p(x) (left in red) (Appendix A.7), and predicts the PDF \u02c6p(x) (right in blue) for the next number.</div>\n# Main contributions.\n1. We introduce a framework for probing an LLM\u2019s ability to estimate unconditioned PDFs from in-context data. 2. We apply InPCA to analyze the low-dimensional geometric features of LLaMA-2\u2019s incontext density estimation trajectories. 3. We interpret LLaMA-2\u2019s in-context DE algorithm as a form of adaptive kernel density estimation, providing evidence for a dispersive induction head mechanism (Olsson et al., 2022; Aky\u00fcrek et al., 2024).\n# 2 BACKGROUND\nIn-context learning of stochastic dynamical systems. Gruver et al. (2024) pioneered LLM-based time-series prediction by prompting LLMs with time-series data serialized as comma-delimited, multi-digit numbers. They observed competitive predictions for future states encoded as softmax probabilities. Their data serialization scheme proved simple yet effective, allowing for easy loglikelihood evaluation of predicted next numbers. It has since become a common prompting method (Jin et al., 2024; Requeima et al., 2024; Zhang et al., 2024; Liu et al., 2024) for evaluating LLMs\u2019 numerical abilities. Building on this prompting technique, Liu et al. (2024) tested LLMs\u2019 ability to in-context learn the transition rules of various stochastic and chaotic dynamical systems. They introduced a recursive algorithm, termed Hierarchy-PDF, to extract an LLM\u2019s predicted PDF for the next data point based on histories observed in-context: \u02c6p(xt|X0, . . . , Xt\u22121). This allowed them to rigorously compare the predicted PDF against the ground-truth transition probability p(xt|xt\u22121). Our work leverages the data serialization technique proposed by Gruver et al. (2024) and extracts LLMs\u2019 probabilistic predictions using the Hierarchy-PDF algorithm introduced by Liu et al. (2024). Density Estimation. DE (Izenman, 1991) is a long-standing statistical problem with both classical (Silverman, 2018; Rosenblat, 1956; Scott, 1979; Lugosi & Nobel, 1996; Parzen, 1962) and modern machine-learned based solutions Papamakarios et al. (2021); Sohl-Dickstein et al. (2015). It underlies the learning of more complex stochastic processes. For example, in the first-order Markov process studied in (Liu et al., 2024; Bigelow et al., 2024; Zekri et al., 2024), the transition probability p(xt|xt\u22121) has to be estimated in-context. This can be viewed as a conditional density estimation\nDensity Estimation. DE (Izenman, 1991) is a long-standing statistical problem with both classical (Silverman, 2018; Rosenblat, 1956; Scott, 1979; Lugosi & Nobel, 1996; Parzen, 1962) and modern machine-learned based solutions Papamakarios et al. (2021); Sohl-Dickstein et al. (2015). It underlies the learning of more complex stochastic processes. For example, in the first-order Markov process studied in (Liu et al., 2024; Bigelow et al., 2024; Zekri et al., 2024), the transition probability p(xt|xt\u22121) has to be estimated in-context. This can be viewed as a conditional density estimation\nproblem, where for each possible value of xt\u22121, one needs to estimate the density of xt. In other words, learning a Markov process involves performing multiple density estimations; one for each conditioning state. Our current focus on unconditional density estimation thus serves as a stepping stone towards understanding these more complex learning tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fa50/fa502f4e-663b-4316-9174-6e5bd7705e4d.png\" style=\"width: 50%;\"></div>\nFigure 2: Visualization pipeline of LLaMA\u2019s in-context density estimation process. (a) Data points are independently sampled from a ground truth distribution (Gaussian in this example), then serialized as comma-delimited, two-digit numbers to prompt LLaMA-2 models. (b) HierarchyPDF extracts LLaMA\u2019s estimated density function \u02c6pn at each context length n. (c) InPCA reveals low-dimensional structures in density estimation trajectories, capturing 91% of pairwise Hellinger distances in 2 dimensions. Visual guides: gray - uniform PDF representing maximal ignorance, deep blue - ground truth PDF, and pink - 1D submanifold of centered Gaussians with variances ranging from \u221eto 0. All three LLaMA-2 models exhibit similar DE trajectories, geometrically bounded between the geodesic and the Gaussian sub-manifold.\n# 3 METHODOLOGY\ndata, and how we extract and analyze its in-context learning trajectories with InPCA. Our methodology consists of 5 steps, the first 3 of which are illustrated in Fig. 2. 1. Sampling and prompt generation. Starting from a ground-truth probability density function p(x), we generate a series of independent samples from it X0, . . . , Xt\u22121. Consistent with Gruver et al. (2024), we then serialize and tokenize this series into a text string consisting of a sequence of comma-delimited, 2-digit numbers, forming a prompt that looks like \u201c6 1 , 4 2 , 5 9 , . . . , 8 1 , 3 2 , 5 8 , \u201d (Figure 2 (a)). 2. Extracting estimated densities with Hierarchy-PDF. Upon prompting with such a text string, we read out LLaMA\u2019s softmax prediction over the next token, yielding probabilities for 10 tokens (0-9)2, creating a coarse, 10-bin PDF spanning x \u2208(0, 100). We then read out the next token, which refines one of the tens bins by further dividing it into 10 ones bins. This process is repeated recursively 10 times, until all bins are refined, yielding a predicted PDF for the next state p(xt), which is a discrete PDF object consisting of 10N bins, where N is the number of digits used in representing each number.3 We interpret the extracted PDF \u02c6p(xt) as the LLM\u2019s estimation of the ground-truth p(x). 3. Visualizing DE trajectories with InPCA. When there are very few in-context data, LLaMA\u2019s estimated \u02c6p0(x) is close to a uniform distribution over the domain (0, 100), showing a state of neutral ignorance, which is a reasonable Bayesian prior (Xie et al., 2022). However, as the number of incontext data (n) increases, LLaMA gathers more information about the underlying distribution. As a result, the estimated \u02c6pn(x) gradually converges to the ground truth p(x) (Figure 2 (b)). The series of estimated PDFs \u02c6p0(x), \u02c6p1(x), ..., \u02c6pn(x) over context length n forms what we term the \"in-context DE trajectory\". These trajectories, we argue, offer important clues about how LLaMA-2 performs in-context learning. However, with our 2-digit representation, each \u02c6p(x) is a vector living in a 99-dimensional space4, making direct analysis of the trajectory challenging. We therefore use InPCA to embed these PDF objects into a low-dimensional Euclidean space (Figure 2 (c)), and then analyze their geometric features.5 4. Comparing with classical DE algorithms. To interpret the low-dimensional geometric features revealed by InPCA, we embed the DE trajectories of well-known algorithms \u2014 specifically, kernel density estimator (KDE) and Bayesian histograms \u2014 in the same two-dimensional (2D) space. Surprisingly, we find that the DE trajectories of LLaMA-2, KDE, and Bayesian histograms are simultaneously embeddable in this same low-dimensional subspace. Moreover, the DE trajectories of LLaMA-2 are geometrically bounded between those of KDE and Bayesian histograms. 5. Explaining LLaMA-2 with bespoke KDE. We observe that LLaMA\u2019s in-context DE trajectories consistently remain close to those of KDE, suggesting that LLaMA might be employing a kernel-like algorithm, internally. To investigate this hypothesis, we optimize a KDE algorithm with fitted kernel parameters (using only two parameters). We now provide a detailed explanation of how to apply InPCA to analyze in-context DE trajectories. We will also introduce two classical DE algorithms that serve as insightful comparisons.\n# 3.1 INPCA VISUALIZATION\nTo visualize the density estimation (DE) trajectories of various algorithms in a lower-dimensional space, we employ Intensive Principal Component Analysis (InPCA) (Quinn et al., 2019; Mao et al., 2024; Teoh et al., 2020). Given a set of PDFs {pi}m i=1, InPCA aims to embed them as points {Pi}m i=1 in a lower-dimensional space Rd (d \u226am), such that the Euclidean distances between embedded\n2The logits of all other tokens are ignored. 3We use the Hierarchy-PDF algorithm (Liu et al., 2024), which performs this recursive search efficiently for transformers. 4102 \u22121, where the \u22121 dimension is from the normalization constraint that \ufffd p(x)dx = 1. 5The idea of using InPCA to visualize the training dynamics of machine learning systems has been explored in Quinn et al. (2019); Mao et al. (2024), which is further discussed in Appendix A.2\npoints closely approximate the statistical distances between the corresponding PDFs:\nwhere D(\u00b7, \u00b7) is a chosen statistical distance measure between PDFs. In this work, we choose D(\u00b7, \u00b7) to be the Hellinger distance (Hellinger, 1909), defined as:\n\ufffd\ufffd\ufffd\ufffd The Hellinger distance is ideal for visualizing our PDF trajectories for the following reasons6: 1) it locally agrees with the KL-divergence (Liese & Vajda, 2006), a non-negative measure commonly used in training modern machine learning systems, including LLMs (Touvron et al., 2023), and 2) it is symmetric and satisfies the triangle inequality, making it a proper distance metric suitable for geometric analysis (Quinn et al., 2019; Teoh et al., 2020).\n\ufffd\ufffd\ufffd\ufffd The Hellinger distance is ideal for visualizing our PDF trajectories for the following reasons6: 1) it locally agrees with the KL-divergence (Liese & Vajda, 2006), a non-negative measure commonly used in training modern machine learning systems, including LLMs (Touvron et al., 2023), and 2) it is symmetric and satisfies the triangle inequality, making it a proper distance metric suitable for geometric analysis (Quinn et al., 2019; Teoh et al., 2020). Having chosen an appropriate distance measure, we proceed with InPCA involving three steps: 1. Calculate the Hellinger distance between each pair of PDFs, D2 Hel(pi, pj). This results in a pairwise distance matrix D \u2208Rm\u00d7m, whose Dij entry equals D2 Hel(pi, pj). 2. Obtain a \u201ccentered\u201d matrix W = \u22121 2LDL, where Lij = \u03b4ij \u22121/m. This step is closely related to multi-dimensional scaling (MDS) (Chen et al., 2007). 3. Perform the eigenvalue decomposition W = U\u03a3U T . The diagonal entries in \u03a3, sorted in descending order, represent the eigenvalues of W. The embedding coordinates for the PDFs are given by U\u03a31/2, where the columns of U are the corresponding eigenvectors.\n \u2208 2. Obtain a \u201ccentered\u201d matrix W = \u22121 2LDL, where Lij = \u03b4ij \u22121/m. This step is closely related to multi-dimensional scaling (MDS) (Chen et al., 2007).\n3. Perform the eigenvalue decomposition W = U\u03a3U T . The diagonal entries in \u03a3, sorted in descending order, represent the eigenvalues of W. The embedding coordinates for the PDFs are given by U\u03a31/2, where the columns of U are the corresponding eigenvectors.\nThe eigenvalues in \u03a3 represent the amount of variance explained by each dimension in the embedded probability space. The cumulative fraction of total variance captured with an increasing number of dimensions is shown in Fig. 2 (c). In our main experiments, approximately 90% of the Hellinger distance variance can be captured in just two dimensions, enabling faithful 2D visualization of the DE trajectories.\n# 3.2 CLASSICAL DE ALGORITHMS\nKernel density estimation. Kernel Density Estimation (KDE) is a non-parametric method for estimating the probability density function of a random variable based on a finite data sample. Given n samples X1, X2, . . . , Xn drawn from some distribution with an unknown density function f, the kernel density estimator \u02c6ph(x) is a function over the support of x defined as:\nwhere K is the kernel function and h > 0 is the bandwidth (smoothing parameter). The kernel function K is typically chosen to be a symmetric probability density function, such as the Gaussian kernel K(u) = 1 \u221a 2\u03c0e\u22121 2 u2. We discuss other kernel shapes in Appendix A.5.2.\nThe optimal bandwidth schedule is a central object of study in classical KDE literature (Appendix A.5). By analyzing the Asymptotic Mean Integrated Squared Error (AMISE) (Equation 12), researchers have derived a widely accepted scaling for the optimal bandwidth.\nwhere n is the sample size and C is a pre-coefficient (Equation 13). In the low-data regime, a larger bandwidth provides more smoothing bias, which compensates for data sparsity. While the n\u22121/5 scaling is widely accepted, determining the pre-coefficient C is more challenging (Wand & Jones, 1994; Silverman, 2018). Unless otherwise noted, we use C = 1 for classical KDE in this paper7.\n6See Appendix A.5.3 for an in-depth comparison of the Hellinger distance against other divergence measures 7In practice, various heuristics have been proposed to determine C, such as Silverman\u2019s rule of thumb (Silverman, 2018) (see Appendices A.5.1 and A.8).\n6See Appendix A.5.3 for an in-depth comparison of the Hellinger distance against other divergence measures. 7In practice, various heuristics have been proposed to determine C, such as Silverman\u2019s rule of thumb (Silverman, 2018) (see Appendices A.5.1 and A.8).\n(1)\n(2)\n(3)\nBayesian histogram. The Bayesian histogram (Lugosi & Nobel, 1996) is another non-parametric method for density estimation, and can be formulated as follows:\nwhere \u02c6pn(x) is the estimated probability density for bin i containing x, ni is the number of observed data points in bin i, n is the total number of observed data points, M is the total number of bins.8, and \u03b1 is the prior count for each bin. We set \u03b1 = 1, effectively populating each bin with one \"hallucinated\" data point prior to observing any data (Jeffreys, 1946). This choice ensures that the histogram algorithm starts from a state of maximal ignorance, consistent with LLaMA\u2019s in-context DE in the low data regime9.\n# 4 EXPERIMENTS AND ANALYSIS\nWe visualize and analyze the learning trajectories of LLaMA on two types of target (ground truth) distributions: uniform and Gaussian. To provide context and facilitate interpretation of the 2D space, we embed the following additional reference points and trajectories: \u2022 Ignorance: A point representing maximum entropy (uniform distribution over the entire support). \u2022 Truth: A point representing the ground truth distribution. \u2022 Geodesic: The shortest trajectory connecting Ignorance and Truth points (Mao et al., 2024). \u2022 Gaussian submanifold: A 1D manifold of centered Gaussians with variances ranging from \u221eto 0. Our experiments focus on LLaMA-2 13b, unless otherwise noted. While LLaMA-2 has a context window of 4096 tokens (equivalent to \u223c1365 comma-delimited, 2-digit data points), we limit our analysis to a context length of n = 200. This limitation is based on our observation that LLaMA\u2019s DE accuracy typically plateaus beyond this point.\n# 4.1 GAUSSIAN TARGET\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d21b/d21b9057-751c-407f-9515-435e60e6a1c7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: In-context density estimation trajectories for Gaussian targets. Top row: 2D InPCA embeddings of DE trajectories for Gaussian targets of decreasing width (left to right). Bottom row: Corresponding ground truth distributions. These 2D embeddings capture 92% of pairwise Hellinger distances between probability distributions.</div>\nWide Gaussian target. A wide Gaussian distribution serves as our simplest target, as it is close in Hellinger distance to the uniform distribution (total ignorance). In this scenario, both LLaMA and Gaussian KDE successfully approximate the target PDF within 200 data points.\n8M = 100 since our most refined estimation from Hierarchy-PDF consists of 100 one-digit bins 9A curious feature which we foreshadowed in Section 3 and further discuss throughout Section 4\n8M = 100 since our most refined estimation from Hierarchy-PDF consists of 100 one-digit bins 9A curious feature which we foreshadowed in Section 3 and further discuss throughout Section 4\n(5)\nNarrow Gaussian target. However, as the Gaussian target narrows, Gaussian KDE lags behind, while LLaMA maintains its ability to closely approximate the target distribution.\nwhile LLaMA maintains its ability to closely approximate the target distribution. In all three cases, the Bayesian histogram, by design, begins exactly at maximal ignorance, and then follows the geodesic trajectory. Despite following this geometrically shortest path, the Bayesian histogram is the slowest to converge to the target distribution; likely due to the strong influence of its uniform prior. Gaussian KDE, on the other hand, starts closer to the target, thanks to its unfair advantage of having a Gaussian-shape kernel. Interestingly, Gaussian KDE consistently lingers on the Gaussian submanifold throughout the DE process. This behavior of lingering on the Gaussian sub-manifold is not unique to Gaussian kernels; as demonstrated in Figure 13, KDEs with alternative kernel shapes (e.g., exponential and parabolic) also exhibit a strong propensity for the Gaussian submanifold. One potential explanation for this phenomenon lies in the nature of KDE itself. KDE can be expressed as a convolution of two distributions: the empirical data distribution and the kernel shape, as\nGaussian KDE, on the other hand, starts closer to the target, thanks to its unfair advantage of having a Gaussian-shape kernel. Interestingly, Gaussian KDE consistently lingers on the Gaussian submanifold throughout the DE process. This behavior of lingering on the Gaussian sub-manifold is not unique to Gaussian kernels; as demonstrated in Figure 13, KDEs with alternative kernel shapes (e.g., exponential and parabolic) also exhibit a strong propensity for the Gaussian submanifold. One potential explanation for this phenomenon lies in the nature of KDE itself. KDE can be expressed as a convolution of two distributions: the empirical data distribution and the kernel shape, as\nwhere Fn is the empirical distribution and Kh is the scaled kernel. This convolution operation tends to produce Gaussian-like distributions due to the central limit theorem (Fischer, 2011). Consequently, a DE trajectory near the Gaussian manifold may indicate a kernel-style density estimation algorithm.\n# 4.2 UNIFORM TARGET\nThe Gaussian distribution easily arises in data with additive noise (Fischer, 2011) and therefore likely dominates the training data (Touvron et al., 2023). What\u2019s more, the Gaussian distribution is everywhere smooth, which makes it very easy to estimate from a function approximation point of view (DeVore, 1993). Uniform distributions, on the other hand, feature non-differentiable boundaries; difficult to represent by both parametric (DeVore, 1993) and kernel-based (Wand & Jones, 1994, Chapter 2.9) methods. For these reasons, we now investigate the in-context DE trajectory with uniform targets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1be5/1be572b7-61d6-4941-a1e2-8353666f67e5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: In-context density estimation trajectories for uniform distribution targets. Top row: 2D InPCA embeddings of DE trajectories for uniform targets of decreasing width (left to right). Bottom row: Corresponding ground truth distributions. These 2D embeddings capture 89% of pairwise Hellinger distances between probability distributions.</div>\nWide uniform target. For a wide uniform target, both LLaMA and Gaussian KDE initially move rapidly towards the target distribution. However, they then linger at the point on the Gaussian submanifold nearest to the uniform target. As more data streams in, they slowly depart from the submanifold and converge to the target distribution. Narrow uniform target. As the uniform target narrows, Gaussian KDE\u2019s performance deteriorates, while LLaMA\u2019s in-context DE successfully reaches the target. Notably, LLaMA exhibits less bias towards the Gaussian sub-manifold, as compared with the narrow Gaussian target case.\n(6)\nIn the low data regime, Gaussian KDE and Bayesian histogram follow the same trajectories that they previously followed for the narrow Gaussian target. However, LLaMA already appears to differentiate this target by taking a path further from the Gaussian sub-manifold and closer to the geodesic. This behavior suggests that LLaMA\u2019s in-context DE algorithm is more flexible and adaptive than classical KDE with pre-determined width schedule and shape.\nBased on our observations in Sections 4.1 and 4.2, we propose a kernel-based interpretation of LLaMA\u2019s in-context density estimation algorithm. The bias towards Gaussian submanifolds in LLaMA\u2019s trajectories suggests a KDE-like approach. To test this hypothesis, we develop a bespoke KDE model with adaptive kernel shape and bandwidth, optimized to emulate LLaMA\u2019s learning trajectory.\n# 4.3.1 BESPOKE KDE MODEL\n4.3.1 BESPOKE KDE MODEL\nWe construct our bespoke KDE model in two steps: Step 1: Parameterize kernel shape We introduce a flexible kernel function Ks(x) parameterized by a shape parameter s:\nWe construct our bespoke KDE model in two steps: Step 1: Parameterize kernel shape\n# We construct our bespoke KDE model in two steps: Step 1: Parameterize kernel shape\nStep 1: Parameterize kernel shape We introduce a flexible kernel function Ks(x) parameterized by a shape parameter s:\nWe introduce a flexible kernel function Ks(x) parameterized by a shape parameter s:\nWe introduce a flexible kernel function Ks(x) parameterized by a shape parameter s:\nwhere Z(s) = 2\u0393( 1 s + 1) normalizes the kernel to integrate to 1, and b(s) = \ufffd \u0393( 3 s +1) 3\u0393( 1 s +1) scales it to maintain unit variance. This parameterization allows us to interpolate between common kernel shapes such as exponential (s = 1), Gaussian (s = 2), and tophat (s \u2192\u221e), as visualized in Figure 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e468/e468ba26-03d8-448a-8b1b-e2f0cd7d261f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Bespoke kernel interpolates various common kernel shapes</div>\nWe visualize the DE trajectories with these three common kernels (s = 1, 2, and \u221e) in Figure 13 of Appendix A.5.2. As the shape parameter (s) decreases, the DE trajectories gradually shift from outside the Gaussian submanifold to inside, moving closer to LLaMA\u2019s trajectory. This trend suggests that our parameterization (Equation 7) may be able to capture LLaMA\u2019s behavior by allowing s to take values below 1, extending beyond the range of common kernel shapes. We augment the standard KDE formula with kernel shape, resulting in two hyperparameters: h and s.\n# Step 2: Optimize kernel bandwidth (h) and shape (s)\nFor a given DE trajectory \u02c6p1(x), . . . , \u02c6pn(x), we optimize our bespoke KDE to minimize the Hellinge distance at each context length i:\nyielding the \"bespoke KDE\" bandwidth schedule {hi}n i=1 and shape schedule {si}n i=1, which together prescribe a sequence of fitted kernels of changing widths and shapes. We visualize such sequences of fitted kernels, and compare them against the Gaussian kernel with standard n\u22121/5 width schedule in Appendix A.6 and A.7. We implement this optimization numerically using SciPy, and estimate parameter uncertainties from the inverse Hessian of the loss function (Hellinger distance) at the optimum. This provides error bars for our fitted kernel shape and bandwidth parameters (Figure 6).\n(7)\n(8)\n(9)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f2b3/f2b375e7-d213-4331-9f38-8a17eb2376a0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3082/3082da2e-0011-41f6-9d26-f908623d1876.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Optimized kernel shape (s) and bandwidth (h) schedules for the bespoke KDE model, fitted to LLaMA-2 13b\u2019s DE trajectory on a narrow Gaussian target.</div>\nFigure 6 presents the optimized kernel shape and bandwidth schedules for our bespoke KDE model, fitted to LLaMA-2 13b\u2019s in-context DE trajectory for the narrow Gaussian target (Section 4.1). Two key observations emerge: 1. The fitted kernel width decays significantly faster than the standard KDE bandwidth schedule (n\u22121 5 ). This rapid decay explains the much longer trajectory of LLaMA\u2019s in-context DE within the inPCA embedding, as compared to standard Gaussian KDE (Figure 3). 2. Unlike fixed-shape KDE methods (e.g., Gaussian KDE with s = 2), LLaMA seems to implicitly employ an adaptive kernel. The shape parameter s evolves from \u223c0.1 to \u223c1, with increasing uncertainty for n \u227320. Figure 7 compares LLaMA-2 13b\u2019s in-context DE trajectory with its bespoke KDE counterpart, demonstrating the close fit achieved by our model. Significance of Bespoke KDE The existence of a bespoke KDE that closely imitates LLaMA\u2019s incontext DE processs is significant and non-trivial. Not all DE processes can be faithfully represented by KDE methods; for instance, the Bayesian histogram, despite its superficial similarity to a fixedwidth tophat kernel KDE, resists accurate modeling as a bespoke KDE process (Appendix A.3). While our initial experiments focused on regular distributions like Gaussian and uniform, for better generality we extended our experiments to randomly generated PDFs (Appendix A.7). Notably, LLaMA-13b\u2019s DE performance on these irregular distributions can still be effectively described using our bespoke KDE process (Appendix A.6 and A.7). Although the DE trajectories of these random PDFs lack low-dimensional embeddings, and thus provide less geometric insight, they nonetheless corroborate our findings from the cases with more regular distributions. This consistency across diverse target PDF types strengthens our core hypothesis about LLaMA\u2019s underlying DE mechanism. Specifically, we posit that LLaMA\u2019s in-context DE algorithm shares fundamental characteristics with kernel density estimation, albeit with adaptive kernel shape and bandwidth schedules that distinguish it from classical KDE approaches.\n# 5 CONCLUSIONS\nInspired by emergent, in-context abilities of LLMs to continue stochastic time series, the current work explores the efficacy of foundation models operating as kernel density estimators; a crucial element within such time series forecasting. Through an application of InPCA, we determine that such in-context kernel density estimation proceeds within a common, low dimensional probability space; along meaningful trajectories that allow for a comparison between histogram, Gaussian kernel density estimation, and LLM in-context kernel density estimation. Through the lens of InPCA, it becomes clear that a profitable characterization of this in-context learning can be made in terms of a\n<div style=\"text-align: center;\">Figure 7: Bespoke KDE trajectories (\u25e6) fitted to LLaMA-2 13b\u2019s DE trajectory (\u25b3) on a narrow Gaussian target.</div>\nFigure 7: Bespoke KDE trajectories (\u25e6) fitted to LLaMA-2 13b\u2019s DE trajectory (\u25b3) on a narrow Gaussian target.\ntwo-parameter, adaptive kernel density estimation framework; hinting at mechanistic basis (Appendix A.1) that points in the direction of future research. Future direction: towards dispersive induction heads. Recent research has identified induction heads as fundamental circuits underlying many in-context learning behaviors in stochastic systems with discrete states (Olsson et al., 2022; Bigelow et al., 2024). These emergent circuits increase the predicted probability of token combinations that are observed in-context. However, such discrete mechanisms are insufficient to explain the in-context learning of continuous stochastic systems, such as the density estimation task we\u2019ve studied. To address this gap, our Bespoke KDE analysis in Section 4.3.2 reveals that LLaMA might possess a kernel-like induction mechanism, which we term dispersive induction head. This is an extension to the induction head concept, and operates as follows: \u2022 Unlike standard induction heads (Aky\u00fcrek et al., 2024), a dispersive induction head increases the predicted probability of not just exact matches, but also of similar tokens or words. \u2022 The \"similarity\" is determined by an adaptive kernel, analogous to our bespoke KDE model. \u2022 The influence of each observation on dissimilar tokens decays over context length, mirroring the decreasing bandwidth in our KDE model. This concept of dispersive induction heads could potentially bridge the gap between discrete (Aky\u00fcrek\nThis concept of dispersive induction heads could potentially bridge the gap between discrete (Aky\u00fcrek et al., 2024) and continuous (Gruver et al., 2024; Liu et al., 2024) in-context learning mechanisms in transformers (Dong et al., 2024).\nACKNOWLEDGEMENTS\n# ACKNOWLEDGEMENTS\nThis work was supported by the SciAI Center, and funded by the Office of Naval Research (ONR), under Grant Numbers N00014-23-1-2729 and N00014-23-1-2716.\nREFERENCES\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, 2023. URL https:// arxiv.org/abs/2211.15661. Ekin Aky\u00fcrek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms, 2024. URL https://arxiv.org/abs/2401.12973.\nEric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, and Tomer D. Ullman. Incontext learning dynamics with random binary sequences, 2024. URL https://arxiv.org/ abs/2310.17639.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https: //arxiv.org/abs/2005.14165.\nChun-houh Chen, Wolfgang Karl H\u00e4rdle, and Antony Unwin. Handbook of data visualization Springer Science & Business Media, 2007.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers, 2023. URL https://arxiv.org/abs/2212.10559. RA DeVore. Constructive Approximation. Springer-Verlag, 1993. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. A survey on in-context learning, 2024. URL https://arxiv.org/abs/2301.00234. Hans Fischer. A history of the central limit theorem: from classical to modern probability theory, volume 4. Springer, 2011. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zeroshot time series forecasters, 2024. URL https://arxiv.org/abs/2310.07820. Ernst Hellinger. Neue begr\u00fcndung der theorie quadratischer formen von unendlichvielen ver\u00e4nderlichen. Journal f\u00fcr die reine und angewandte Mathematik, 1909(136):210\u2013271, 1909. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Xiaoyang Hu, Shane Storks, Richard L. Lewis, and Joyce Chai. In-context analogical reasoning with pre-trained language models, 2023. URL https://arxiv.org/abs/2305.17626. Alan Julian Izenman. Review papers: Recent developments in nonparametric density estimation. Journal of the american statistical association, 86(413):205\u2013224, 1991. Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453\u2013461, 1946. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models, 2024. URL https://arxiv.org/abs/2310. 01728. Subhash Kantamneni, Ziming Liu, and Max Tegmark. How do transformers \"do\" physics? investigating the simple harmonic oscillator, 2024. URL https://arxiv.org/abs/2405.17209. Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory. IEEE Transactions on Information Theory, 52(10):4394\u20134412, 2006. Toni J. B. Liu, Nicolas Boull\u00e9, Rapha\u00ebl Sarfati, and Christopher J. Earls. Llms learn governing principles of dynamical systems, revealing an in-context neural scaling law, 2024. URL https: //arxiv.org/abs/2402.00795. G\u00e1bor Lugosi and Andrew Nobel. Consistency of data-driven histogram methods for density estimation and classification. The Annals of Statistics, 24(2):687\u2013706, 1996. Jialin Mao, Itay Griniasty, Han Kheng Teoh, Rahul Ramesh, Rubing Yang, Mark K. Transtrum, James P. Sethna, and Pratik Chaudhari. The training process of many deep networks explores the same low-dimensional manifold. Proceedings of the National Academy of Sciences, 121(12), March 2024. ISSN 1091-6490. doi: 10.1073/pnas.2310002121. URL http://dx.doi.org/ 10.1073/pnas.2310002121. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. URL https://arxiv.org/ abs/2209.11895.\nJialin Mao, Itay Griniasty, Han Kheng Teoh, Rahul Ramesh, Rubing Yang, Mark K. Transtrum, James P. Sethna, and Pratik Chaudhari. The training process of many deep networks explores the same low-dimensional manifold. Proceedings of the National Academy of Sciences, 121(12), March 2024. ISSN 1091-6490. doi: 10.1073/pnas.2310002121. URL http://dx.doi.org/ 10.1073/pnas.2310002121.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. URL https://arxiv.org/ abs/2209.11895.\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference, 2021. URL https://arxiv.org/abs/1912.02762. Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathematical statistics, 33(3):1065\u20131076, 1962. Katherine N Quinn, Colin B Clement, Francesco De Bernardis, Michael D Niemack, and James P Sethna. Visualizing probabilistic models and data with intensive principal component analysis. Proceedings of the National Academy of Sciences, 116(28):13762\u201313767, 2019. Alfr\u00e9d R\u00e9nyi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pp. 547\u2013562. University of California Press, 1961. James Requeima, John Bronskill, Dami Choi, Richard E Turner, and David Duvenaud. Llm processes: Numerical predictive distributions conditioned on natural language. arXiv preprint arXiv:2405.12856, 2024. M Rosenblat. Remarks on some nonparametric estimates of a density function. Ann. Math. Stat, 27: 832\u2013837, 1956. David W Scott. On optimal and data-based histograms. Biometrika, 66(3):605\u2013610, 1979. Matthias Seeger. Gaussian processes for machine learning. International journal of neural systems, 14(02):69\u2013106, 2004. Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. Measuring inductive biases of in-context learning with underspecified demonstrations, 2023. URL https://arxiv. org/abs/2305.13299. Bernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/ 1503.03585. Han Kheng Teoh, Katherine N. Quinn, Jaron Kent-Dobias, Colin B. Clement, Qingyang Xu, and James P. Sethna. Visualizing probabilistic models in minkowski space with intensive symmetrized kullback-leibler embedding. Physical Review Research, 2(3), August 2020. ISSN 2643-1564. doi: 10.1103/physrevresearch.2.033221. URL http://dx.doi.org/10.1103/ PhysRevResearch.2.033221.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/ 1503.03585.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/ 1503.03585. Han Kheng Teoh, Katherine N. Quinn, Jaron Kent-Dobias, Colin B. Clement, Qingyang Xu, and James P. Sethna. Visualizing probabilistic models in minkowski space with intensive symmetrized kullback-leibler embedding. Physical Review Research, 2(3), August 2020. ISSN 2643-1564. doi: 10.1103/physrevresearch.2.033221. URL http://dx.doi.org/10.1103/ PhysRevResearch.2.033221.\nHan Kheng Teoh, Katherine N. Quinn, Jaron Kent-Dobias, Colin B. Clement, Qingyang Xu, and James P. Sethna. Visualizing probabilistic models in minkowski space with intensive symmetrized kullback-leibler embedding. Physical Review Research, 2(3), August 2020. ISSN 2643-1564. doi: 10.1103/physrevresearch.2.033221. URL http://dx.doi.org/10.1103/ PhysRevResearch.2.033221.\nMTCAJ Thomas and A Thomas Joy. Elements of information theory. Wiley-Interscience, 2006.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.\nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descen 2023. URL https://arxiv.org/abs/2212.07677.\nMatt P Wand and M Chris Jones. Kernel smoothing. CRC press, 1994. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit bayesian inference, 2022. URL https://arxiv.org/abs/ 2111.02080. Oussama Zekri, Abdelhakim Benechehab, and Ievgen Redko. Can llms predict the convergence of stochastic gradient descent?, 2024. URL https://arxiv.org/abs/2408.01736. Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. Large language models for time series: A survey, 2024. URL https://arxiv.org/abs/2402.01801.\n# A APPENDIX\nIn recent years a paradigm has emerged for LLMs\u2019 in-context learning abilities: the simulation and application of smaller, classical models, with well-understood algorithmic features, in response to prompt information. As an example, Olsson et al. (2022) identified \u201cinduction heads\" in pre-trained LLMs, which simulate 1-gram models for language learning. Then, Aky\u00fcrek et al. (2024) extended this analysis to \u201cn-gram heads\" for computing distributions of next tokens conditioned on n previous tokens. It has also been observed in the literature that transformers can perform in-context regression via gradient descent (von Oswald et al., 2023; Dai et al., 2023). These works proposed to understand induction heads as a specific case of in-context gradient descent. Aky\u00fcrek et al. (2023) showed that trained in-context learners closely match predictions of gradient-based optimization algorithms. More recently, Kantamneni et al. (2024) identified mechanisms in transformers that implicitly implement matrix exponential methods for solving linear ordinary differential equations (ODEs). Our approach differs from these aforementioned studies for the following reason: we do not train LLMs on synthetic data designed to induce specific in-context learning abilities. Instead, we investigate the emergent density estimation capabilities of pre-trained foundation models (the LLaMA2 suite) without any fine-tuning. This approach aligns more closely with recent works (Liu et al., 2024; Gruver et al., 2024) that focus on the inherent mathematical abilities of foundations models, rather than LLMs that are trained to induce certain behaviors.\n# A.2 LOW-DIMENSIONAL STRUCTURES IN LEARNING DYNAMICS.\nDespite the success of modern neural networks at learning patterns in high-dimensional data, the learning dynamics of these complex neural networks are often shown to be constrained to lowdimensional, potentially non-linear subspaces. Hu et al. (2021) demonstrated that the fine-tuning dynamics of LLMs such as GPT and RoBERTa can be well-captured within extremely low-rank, weighted spaces. More recently, Mao et al. (2024) showed that, during training, neural networks spanning a wide range of architectures and sizes trace out similar low-dimensional trajectories in the space of probability distributions. Their work focuses on learning trajectories pk=0, ..., pk=t, where k indexes the training epoch, and pk is a high-dimensional PDF describing the model\u2019s probabilistic classification of input data. Key to their observations is a technique termed Intensive Principal Component Analysis (InPCA), a visualization tool (Quinn et al., 2019) that embeds PDFs as points in low dimensional spaces, such that the geometric (Euclidean or Minkowski) distances between embedded points reflect the statistical distances - e.g. the Hellinger or Bhattacharyya distance between the corresponding PDFs. Inspired by Mao et al. (2024) and Quinn et al. (2019), we extend this line of inquiry to investigate whether the in-context learning dynamics of LLMs also follow low-dimensional trajectories in probability space. and \u02c6pk is a PDF of dimension 10N, where N is the number of digits used in the multi-digit representation (see 3). Each \u02c6pk describes the model\u2019s estimation of the underlying data distribution at a certain context length. Our findings show that in-context density estimation traces low-dimensional paths.\n# A.3 BESPOKE KDE CANNOT IMITATE ALL DE PROCESSES\nThis section examines the ability of Bespoke KDE to imitate various density estimation (DE processes, including LLaMA-2 models (7b, 13b, 70b), Bayesian histogram, and standard Gaussia KDE.\n# A.3.1 INPCA VISUALIZATION OF ORIGINAL DE TRAJECTORIES VS. BESPOKE KD IMITATIONS\nA.3.1 INPCA VISUALIZATION OF ORIGINAL DE TRAJECTORIES VS. BESPOKE KDE IMITATIONS\nFigure 8 illustrates the DE trajectories of various models alongside their bespoke KDE counterparts. We observe that: 1. Bespoke KDE successfully imitates the LLaMA suite with high precision. 2. It trivially replicates the Gaussian KDE trajectory, as expected. 3. However, it struggles to accurately capture the Bayesian histogram\u2019s trajectory. To further investigate these observations, we analyze the fitted kernel parameters for Gaussian KDE and Bayesian histogram:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59a6/59a6fce3-7095-46e7-8ae8-d5c6afc727ee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Comparison of original DE trajectories (\u25b3) and their bespoke KDE imitations (\u25e6) for LLaMA-2 models (7b, 13b, 70b), Gaussian KDE, and Bayesian histogram. The 2D embeddings capture 97% of pairwise Hellinger distances between probability distributions.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f38/9f388abe-40f5-4fab-9567-6639a1dd482d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Bespoke KDE parameters fitted to standard Gaussian-kernel KDE. The model accurately recovers the Gaussian kernel shape (s = 2) and bandwidth schedule (h = n\u22121 5 ).</div>\nFigure 9 demonstrates that bespoke KDE accurately recovers the parameters of standard Gaussian KDE. In contrast, Figure 10 reveals significant challenges in fitting Bespoke KDE to the Bayesian histogram: 1. The fitted kernel width remains constant, likely reflecting the fixed bin width of the histogram. 2. The near-zero shape parameter suggests a highly peaked distribution. 3. High uncertainties in both parameters indicate a fundamental mismatch between Histogram and KDE. These results highlight that while Bespoke KDE can effectively model certain DE processes (e.g., LLaMA models and Gaussian KDE), it cannot capture fundamentally different approaches like the Bayesian histogram.\nA.3.2 META-INPCA EMBEDDING OF TRAJECTORIES\n# A.3.2 META-INPCA EMBEDDING OF TRAJECTORIES\nTo quantify the similarity between the bespoke KDE model and various DE processes, we introduce a meta-distance measure for trajectories and apply InPCA at a higher level of abstraction.\n<div style=\"text-align: center;\">Figure 10: Bespoke KDE parameters fitted to Bayesian histogram. The constant fitted kernel width likely reflects the fixed bin width, while the near-zero shape parameter indicates a highly peaked distribution. High uncertainties suggest a fundamental mismatch between the models.</div>\nFigure 10: Bespoke KDE parameters fitted to Bayesian histogram. The constant fitted kernel width likely reflects the fixed bin width, while the near-zero shape parameter indicates a highly peaked distribution. High uncertainties suggest a fundamental mismatch between the models.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99ae/99ae1bb2-73da-4273-b9a3-ba56f4f0f29b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Meta-InPCA embedding of DE trajectories (\u25b3) and their bespoke KDE imitations (\u25e6). This 2D embedding captures 94% of pairwise meta-distances between trajectories.</div>\nTrajectory distance metric. We define a meta-distance between two trajectories {pi}n i=1 and {qi}n i=1 as the sum of Hellinger distances between corresponding points at each context length:\nwhere DHel is the Hellinger distance defined in Equation 2.\nMeta-InPCA procedure. Given a set of trajectories traj1, ..., trajl, we: 1. Compute the pairwise trajectory distance matrix D \u2208Rl\u00d7l using Equation 10. 2. Apply the InPCA procedure described in Section 3.1 to embed these trajectories in a lower-dimensional space. Unlike previous InPCA visualizations where each point represented a single PDF, in this metaembedding, each point represents an entire DE trajectory. Observations. Figure 11 reveals several key insights: 1. The Bayesian histogram and its bespoke KDE imitation are far apart, confirming the model\u2019s inability to capture this approach. 2. Gaussian KDE almost exactly overlaps with its bespoke KDE imitation, as expected. 3. LLaMA-2 models (7b, 13b, 70b) are relatively close to their bespoke KDE counterparts but do not overlap exactly. These observations suggest that while the 2-parameter bespoke KDE model captures much of LLaMA-2\u2019s in-context DE behavior, there are still certain nuances in LLaMA-2\u2019s algorithm that it doesn\u2019t fully encapsulate.\nObservations. Figure 11 reveals several key insights: 1. The Bayesian histogram and its bespoke KDE imitation are far apart, confirming the model\u2019s inability to capture this approach. 2. Gaussian KDE almost exactly overlaps with its bespoke KDE imitation, as expected. 3. LLaMA-2 models (7b, 13b, 70b) are relatively close to their bespoke KDE counterparts but do not overlap exactly. These observations suggest that while the 2-parameter bespoke KDE model captures much of LLaMA-2\u2019s in-context DE behavior, there are still certain nuances in LLaMA-2\u2019s algorithm that it doesn\u2019t fully encapsulate.\n(10)\n# A.4 COMPARISON OF INPCA AND STANDARD PCA  \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/41a4/41a4f047-f9dd-4328-b3db-748e45403571.png\" style=\"width: 50%;\"></div>\nFigure 12: Comparison of InPCA and PCA visualizations for density estimation trajectories of LLaMA-2 models. (a) 2D InPCA embedding preserves 91% of pairwise Hellinger distances, revealing LLaMA\u2019s DE trajectories geometrically bounded between the geodesic and Gaussian submanifold. (b) In contrast, 2D PCA preserves only 83% of pairwise L2 distances, displaying erratic oscillations in LLaMA\u2019s DE trajectories without clear geometric relationships.\nThis section provides more background on classical results regarding optimal bandwidth schedules {hi}n i=1 and kernel shapes. These results are derived by minimizing the Mean Integrated Squared Error (MISE) (Wand & Jones, 1994, Chapter 2.3):\nwhere \u02c6ph is the kernel density estimate with bandwidth h, and p is the true density.\nwhere \u02c6ph is the kernel density estimate with bandwidth h, and p is the tru\nA.5.1 OPTIMAL BANDWIDT\n# A.5.1 OPTIMAL BANDWIDTH\nWhile MISE (Equation 11) provides a reasonable measure of estimation error, it is often analytically intractable. To overcome this limitation, researchers developed the Asymptotic Mean Integrated Squared Error (AMISE) (Wand & Jones, 1994, Chapter 2.4), an approximation of MISE that becomes increasingly accurate as the sample size grows large:\nwhere R(f) = \ufffd f(x)2 dx, \u00b52(K) = \ufffd x2K(x) dx, and p\u2032\u2032 is the second derivative of the true density. AMISE provides a more manageable form for mathematical analysis. By minimizing AMISE with respect to h, we can derive the optimal bandwidth:\n\ufffd \ufffd AMISE provides a more manageable form for mathematical analysis. By minimizing AMISE with respect to h, we can derive the optimal bandwidth:\nIt\u2019s important to note that this expression involves many unknown quantities related to the true density p, such as the average curvature R(p\u2032\u2032), which is not known a priori. The key insight from this derivation is therefore the n\u22121/5 scaling of the optimal bandwidth. In practice, there are many methods for estimating the pre-coefficient C from data, such as Silverman\u2019s rule of thumb (Silverman, 2018):\nwhere \u02c6\u03c3 is the sample standard deviation, IQR is the interquartile range, and n is the sample size. This rule is derived heuristically based on minimizing the AMISE (Equation 12). The n\u22121/5 scaling reveals a fundamental trade-off in kernel density estimation: as more data becomes available, we can afford to use a narrower kernel, but the rate at which we can narrow the kernel is relatively slow.\nThe kernel function K can take various forms. Common choices include:\n# The kernel function K can take various forms. Common choices include:\n\u2022 Gaussian kernel: K(u) = 1 \u221a 2\u03c0e\u22121 2 u2 \u2022 Exponential kernel: K(u) = 1 2e\u2212|u| \u2022 Epanechnikov kernel: K(u) = 3 4(1 \u2212u2) for |u| \u22641, 0 otherwise \u2022 Tophat kernel: K(u) = \ufffd1 2 for |u| \u22641 0 otherwise\n\u2022 Epanechnikov kernel: K(u) = 3 4(1 \u2212u2) for |u| \u22641, 0 otherwise \u2022 Tophat kernel: K(u) = \ufffd1 2 for |u| \u22641 0 otherwise\nThe Epanechnikov kernel is a key result from optimal kernel theory as it minimizes the MISE. As shown in Figure 13, different kernel shapes can lead to varying density estimation trajectorie This visualization compares the performance of LLaMA-13b with various kernel density estimato in a 2D InPCA embedding, capturing 87% of the pairwise Hellinger distances between probabili distributions.\n(11)\n(12)\n(14)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/945f/945f9858-46c5-4652-82e4-e7a22cb1e478.png\" style=\"width: 50%;\"></div>\nFigure 13: 2D InPCA embedding of density estimation trajectories for LLaMA-13b and KDEs with various kernel shapes: exponential (s = 1), Gaussian (s = 2), and tophat (s = \u221e). As s decreases, the DE trajectories gradually shift from outside the Gaussian submanifold to inside, moving closer to LLaMA\u2019s trajectory. The ground truth is a narrow Gaussian distribution. This visualization captures 87% of the pairwise Hellinger distances between probability distributions.\n<div style=\"text-align: center;\">Figure 13: 2D InPCA embedding of density estimation trajectories for LLaMA-13b and KDEs with various kernel shapes: exponential (s = 1), Gaussian (s = 2), and tophat (s = \u221e). As s decreases, the DE trajectories gradually shift from outside the Gaussian submanifold to inside, moving closer to LLaMA\u2019s trajectory. The ground truth is a narrow Gaussian distribution. This visualization captures 87% of the pairwise Hellinger distances between probability distributions.</div>\nA.5.3 MISE VS. VALID STATISTICAL DISTANCES\nIn our InPCA analysis, we choose the the Hellinger distance because: 1. It locally agrees with the KLdivergence (Liese & Vajda, 2006); a measure motivated by information theory and commonly used in training modern machine learning systems, including LLMs (Touvron et al., 2023). Mathematically, this agreement means for distributions p and q that are close: D2 Hel(p, q) \u22481 2KL(p||q) 2. Unlike the closely-related Bhattacharyya distance (Bhattacharyya, 1943), KL-divergence, or cross-entropy (Thomas & Joy, 2006), Hellinger distance is both symmetric and satisfies the triangle inequality, making it a proper metric. This property makes it more suitable for visualization purposes.10 3. Although the L2 distance ||p \u2212q||2 2 = \ufffd |p(x) \u2212q(x)|2dx is also a proper distance metric, it does not locally agree with any information-theoretic divergence measure (Amari, 2016) and is therefore not suited to measure distance between PDFs.11. In contrast, classical KDE theory focuses on minimizing the Mean Integrated Squared Error (MISE), which is fundamentally an L2-type distance that disagrees (globally and locally) with informationtheoretic divergence measure. Nevertheless MISE is widely used in classical KDE literature for several reasons: 1. It can be easily analyzed mathematically using bias-variance decompositions (Wand & Jones, 1994, Chapter 2.3). 2. It leads to closed-form solutions for optimal kernels and bandwidths under certain assumptions. 3. It provides a tractable objective function for theoretical analysis and optimization. As a consequence of its popularity and mathematical tractability, MISE has been used to derive heuristic bandwidth schedules, such as Silverman\u2019s rule (Equation 14). We note that modern machine learning often prefers loss functions from the f-divergence family (R\u00e9nyi, 1961), such as KL-divergence, cross-entropy, and Hellinger distance (Equation 2). These measures have strong information-theoretic motivations and are often more appropriate for probabilistic models.\nWe note that modern machine learning often prefers loss functions from the f-divergence family (R\u00e9nyi, 1961), such as KL-divergence, cross-entropy, and Hellinger distance (Equation 2). These measures have strong information-theoretic motivations and are often more appropriate for probabilistic models.\n10InPCA embedding with Bhattacharyya distance or symmetrized KL-divergence typically results in negative distances in Minkowski space, which are harder to interpret (Teoh et al., 2020) 11Consequently, PCA embedding with such naive distance measure results in erratic trajectories with obscure geometrical features, as explored in Appendix 12\nDespite the prevalence of f-divergences in machine learning, to the best of our knowledge, there are no rigorous derivations of optimal kernel shapes or bandwidth schedules based on these measures. This gap presents an interesting avenue for future research, potentially bridging classical statistical theory with modern machine learning practices. Bridging this gap is beyond the scope of this paper. However, we speculate that this gap is the reason why the optimal kernel shape employed by LLMs differs significantly from those in classical optimal kernel theory (namely, Gaussian and Epanechnikov).\nThis section provides a comprehensive visual analysis of DE processes across various target distributions, including Gaussian, uniform, and randomly generated probability density functions. We present side-by-side comparisons of the DE trajectories for LLaMA-2, Gaussian KDE, and KDE with fitted kernel designed to emulate LLaMA-2\u2019s behavior. The fitted KDE process closely mirrors LLaMA-2\u2019s estimation patterns, while both diverge significantly from traditional Gaussian KDE approaches.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f94b/f94b6766-1648-4f5b-ab2f-df7e19a7dbfe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6312/6312bd46-54c5-428c-a046-6f776b28b53c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Fitted kernel width and shape schedule</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/657f/657f271a-7d9e-4b1e-96ff-3e83bd4d32c6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: Fitted kernel visualization with narrow gaussian target</div>\nWide Gaussian distribution target\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4f61/4f610a0e-3e80-441a-9c57-8ec2d0435741.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 19: Fitted kernel visualization with wide gaussian target</div>\nNarrow uniform distribution target\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a53/4a537793-aba0-4ad1-a75e-bd48a1498a39.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f83/1f834431-1f92-45e8-88c0-c4622d159501.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 22: Fitted kernel visualization with narrow uniform target</div>\nWide uniform distribution target\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3185/31858ef3-19d5-4ad7-bfb9-d3262cc78218.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/41ac/41ac882e-5b46-4194-8c96-d5a77d7e9c87.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 25: Fitted kernel visualization with wide uniform target</div>\nFor completion, we also test LLaMA-2\u2019s in-context DE ability on randomly generated PDFs. As noted in Appendix A.5 and Chapter 2.9 of Wand & Jones (1994), PDFs with high average curvature (defined as R(p\u2032\u2032) = \ufffd p\u2032\u2032(x)2dx) are more difficult to estimate from data. Therefore, we want to be able to control the average curvature R(p\u2032\u2032) of the generated random PDFs. We employ a technique for generating random PDF using Gaussian processes. This allows us to create a wide variety of smooth, continuous distributions with controllable average curvatures.\nGaussian Process Generation: We generate random PDFs using a Gaussian process with a predefined covariance matrix. The process is defined over the interval [0, 1], discretized into Nx = 10p points, where p is the precision parameter. The covariance matrix is constructed using a squared exponential kernel (Seeger, 2004):\nwhere x and y are points in the domain, and l is the correlation length, a crucial hyperparameter our generation process.\nCorrelation Length (l): The parameter l controls the smoothness and regularity of the generated PDFs. Specifically:\n\u2022 Large values of l produce more regular distributions with lower average curvature. These PDFs tend to have smoother features with lower curvatures. \u2022 Small values of l result in more irregular distributions with higher average curvature. These PDFs can have sharper features with higher curvatures.\n# Generation Process: The random PDF generation involves the following steps:\nation Process: The random PDF generation involves the following step\n1. Generate the covariance matrix using the squared exponential kernel. 2. Perform Cholesky decomposition on the covariance matrix. 3. Sample from the Gaussian process using the decomposed matrix. 4. Apply boundary conditions to ensure the PDF goes to zero at the domain edges. 5. Normalize the function to ensure it integrates to 1, making it a valid PDF.\nThis method allows us to generate a wide range of PDFs with varying degrees of complexity and smoothness, providing a robust test set for our density estimation algorithms. By adjusting the correlation length l, we can systematically explore how different estimation methods perform on targets of varying regularity and curvature. Given a randomly generated PDF, we can calculate its average curvature using two methods: numerical differentiation and analytical derivation. Numerically, we can approximate the second derivative using finite differences and then compute the average curvature as:\n\ufffd where N is the number of discretization points and \u2206x is the spacing between points. Analytically, we can leverage the fact that the derivative of a Gaussian process is itself a Gaussian process Seeger (2004). For a Gaussian process with squared exponential kernel k(x, x\u2032) = exp(\u2212(x\u2212x\u2032)2 2l2 ), the expected average curvature can be derived as: \ufffd\nIn our numerical experiments, we find that these two methods agree to high precision, typically within 10% relative error, validating our choice to use Gaussian processes to generate PDFs with\nIn our numerical experiments, we find that these two methods agree to high precision, typically within 10% relative error, validating our choice to use Gaussian processes to generate PDFs with controllable curvatures.\n(15)\n(16)\n(17)\nRandomly generated distribution at low curvature (l = 0.5)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1401/1401e98a-c42f-4b87-9afa-59bbfca81ab5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 28: Fitted kernel visualization with randomly generated target</div>\nRandomly generated distribution at medium curvature (l = 0.1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b673/b6735004-1218-42aa-a489-b4bf2bc53169.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9422/9422639c-3399-4aad-80a2-784fd99f2299.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 29: Fitted kernel width and shape schedule</div>\n<div style=\"text-align: center;\">Figure 31: Fitted kernel visualization with randomly generated target</div>\nRandomly generated distribution at high curvature (l = 0.02)\nRandomly generated distribution at high curvature (l = 0.02)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7b3f/7b3fc372-61b3-4c79-8fbb-086391c25a9a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/352f/352fe56d-1c1a-4dd7-bac4-af5d833453aa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 34: Fitted kernel visualization with randomly generated target</div>\n# A.8 KDE WITH SILVERMAN BANDWIDTH SCHEDULE\nFor the majority of our investigations, we have been comparing LLaMA\u2019s in-context DE process with Gaussian KDE with bandwidth schedule\nwith C = 1.However, as noted in Appendix A.5.1, in practice one often estimates the pre-coefficient from data, such as Silverman\u2019s rule of thumb (Equation 14). In this section, we replicate Figures 3 and 4 from Section 4, and use Silverman\u2019s rule-of-thumb bandwidth instead of C = 1. As shown in Figures 35 and 36, even with Silverman\u2019s pre-coefficient, Gaussian KDE still shows a clear bias towards the Gaussian submanifold.\n# Gaussian target\n<div style=\"text-align: center;\">Gaussian target</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6566/65664dd7-2b40-4bcc-ab98-db047fa17b2f.png\" style=\"width: 50%;\"></div>\nFigure 35: In-context density estimation trajectories for Gaussian targets. Top row: 2D InPCA embeddings of DE trajectories for Gaussian targets of decreasing width (left to right). Bottom row: Corresponding ground truth distributions. These 2D embeddings capture 94% of pairwise Hellinger distances between probability distributions.\n<div style=\"text-align: center;\">Figure 35: In-context density estimation trajectories for Gaussian targets. Top row: 2D InPCA embeddings of DE trajectories for Gaussian targets of decreasing width (left to right). Bottom row: Corresponding ground truth distributions. These 2D embeddings capture 94% of pairwise Hellinger distances between probability distributions.</div>\n# Uniform target\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4cd7/4cd7e82a-3050-41fb-ae26-a870c5d010fe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 36: In-context density estimation trajectories for Gaussian targets. Top row: 2D InPCA embeddings of DE trajectories for Gaussian targets of decreasing width (left to right). Bottom row: Corresponding ground truth distributions. These 2D embeddings capture 91% of pairwise Hellinger distances between probability distributions.</div>\nFigure 36: In-context density estimation trajectories for Gaussian targets. Top row: 2D InPCA embeddings of DE trajectories for Gaussian targets of decreasing width (left to right). Bottom row: Corresponding ground truth distributions. These 2D embeddings capture 91% of pairwise Hellinger distances between probability distributions.\nWith Silverman\u2019s heuristic bandwidth, the Gaussian KDE algorithm can now efficiently converge to the ground truth target distribution, performing on par with or even surpassing LLaMA-13b\u2019s incontext DE in terms of speed. This outcome is not surprising for two reasons: 1. LLaMA is a generalpurpose LLM that is not optimized for density estimation tasks. 2. We do not specifically prompt LLaMA to perform DE, so it must infer the task from the raw data sequence. This comparison is inherently unfair to LLaMA, as Gaussian KDE is specifically designed for DE tasks.\nWe would like to reiterate that the purpose of these visualizations is not to benchmark LLaMA against existing algorithms, but to distill geometric insights. The Gaussian KDE, with Silverman\u2019s bandwidth, features much shorter trajectory lengths, and therefore provides fewer geometric insights compared to our previous visualizations with fixed bandwidth.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of density estimation (DE) using large language models (LLMs), specifically LLaMA-2, which exhibit emergent abilities for in-context learning. Previous methods have struggled to effectively model probability density functions (PDFs) from in-context data, necessitating a new approach that leverages the unique characteristics of LLMs.",
        "problem": {
            "definition": "The problem is to estimate probability density functions (PDFs) from data observed in-context, a fundamental task in probabilistic modeling that existing methods like histograms and Gaussian kernel density estimation (KDE) do not address effectively.",
            "key obstacle": "The main challenge lies in the inability of traditional methods to adaptively learn from the in-context data, leading to suboptimal density estimates."
        },
        "idea": {
            "intuition": "The idea stems from the observation that LLMs can learn to estimate distributions from in-context examples, akin to kernel density estimation but with adaptive parameters.",
            "opinion": "The proposed method interprets LLaMA-2's in-context DE process as a form of adaptive kernel density estimation, where the kernel width and shape adjust based on the observed data.",
            "innovation": "The primary difference of this method is its ability to adaptively modify the kernel parameters during the density estimation process, unlike classical methods that use fixed parameters."
        },
        "method": {
            "method name": "Adaptive Kernel Density Estimation",
            "method abbreviation": "AKDE",
            "method definition": "A method that utilizes large language models to estimate probability density functions from in-context data by adaptively adjusting kernel parameters based on the observed data.",
            "method description": "The method leverages LLaMA-2's in-context learning capabilities to perform density estimation through an adaptive kernel approach.",
            "method steps": [
                "Sampling from a ground-truth probability density function to generate independent data points.",
                "Serializing the data into a text string for input into the LLaMA-2 model.",
                "Using the Hierarchy-PDF algorithm to extract the estimated densities from the model's predictions.",
                "Visualizing the DE trajectories using Intensive Principal Component Analysis (InPCA).",
                "Comparing the trajectories with classical DE algorithms to analyze the performance."
            ],
            "principle": "This method is effective because it captures the underlying distribution dynamics through adaptive adjustments to the kernel, allowing for more accurate density estimates as more data is observed."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using LLaMA-2 models with a focus on two types of target distributions: Gaussian and uniform. The evaluation involved comparing the in-context DE trajectories of LLaMA-2 with traditional methods such as Gaussian KDE and Bayesian histograms.",
            "evaluation method": "The performance was assessed by visualizing the estimated PDFs at various context lengths and measuring the pairwise Hellinger distances between the estimated and true distributions."
        },
        "conclusion": "The experiments demonstrate that LLaMA's in-context density estimation capabilities can effectively model probability distributions, revealing the potential for adaptive kernel methods in probabilistic modeling and suggesting directions for future research into the mechanisms of in-context learning.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its adaptability; it can modify its kernel parameters based on the observed data, leading to improved density estimation compared to traditional fixed-parameter methods.",
            "limitation": "One limitation is that LLaMA is a general-purpose model not specifically optimized for density estimation tasks, which may affect its performance relative to dedicated algorithms.",
            "future work": "Future research should explore optimizing LLaMA for density estimation tasks and further investigate the concept of 'dispersive induction heads' as a mechanism for continuous in-context learning."
        },
        "other info": {
            "acknowledgements": "This work was supported by the SciAI Center and funded by the Office of Naval Research (ONR).",
            "references": [
                "Gruver et al. (2024): LLM-based time-series prediction.",
                "Liu et al. (2024): In-context learning of stochastic systems.",
                "Teoh et al. (2020): Intensive Principal Component Analysis."
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning is addressed through the use of large language models (LLMs) like LLaMA-2, which exhibit emergent abilities for in-context learning."
        },
        {
            "section number": "1.3",
            "key information": "LLaMA-2's in-context density estimation capabilities allow for adaptive learning from in-context examples, enhancing the understanding of how large language models facilitate in-context learning."
        },
        {
            "section number": "3.2",
            "key information": "The proposed method interprets LLaMA-2's in-context density estimation process as a form of adaptive kernel density estimation, highlighting theoretical perspectives on in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "The adaptability of the proposed method allows it to modify kernel parameters based on observed data, significantly influencing the outcomes of in-context learning."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the approach is that LLaMA is a general-purpose model not specifically optimized for density estimation tasks, which may affect its performance relative to dedicated algorithms."
        },
        {
            "section number": "7",
            "key information": "The findings suggest directions for future research into the mechanisms of in-context learning, particularly in optimizing LLaMA for density estimation tasks."
        }
    ],
    "similarity_score": 0.7055845956407314,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Density estimation with LLMs_ a geometric investigation of in-context learning trajectories.json"
}