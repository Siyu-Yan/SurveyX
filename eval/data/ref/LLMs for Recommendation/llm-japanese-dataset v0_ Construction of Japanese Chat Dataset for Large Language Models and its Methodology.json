{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.12720",
    "title": "llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
    "abstract": "This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records. Recently, LLMs have been developed and gaining popularity. However, high-performing LLMs are usually mainly for English. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models. However, in both ways, datasets are necessary parts. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively. The results suggest that our dataset is possibly beneficial for LLMs. However, we also revealed some difficulties in constructing LLMs in languages other than English.",
    "bib_name": "hirano2023llmjapanesedatasetv0constructionjapanese",
    "md_text": "# llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology\nMasanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI\nAbstract This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records. Recently, LLMs have been developed and gaining popularity. However, high-performing LLMs are usually mainly for English. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models. However, in both ways, datasets are necessary parts. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively. The results suggest that our dataset is possibly beneficial for LLMs. However, we also revealed some difficulties in constructing LLMs in languages other than English.\n# 1 Introduction\nLarge language models (LLMs) have recently achieved remarkable progress in performance and generalization. Specifically, Transformer-based LLMs such as BERT [3] and the GPT series [17, 18, 1] have demonstrated high-performance thanks to their pre-training. Furthermore, models that have evolved from these, such as ChatGPT [14] and GPT4 [15], have gained popularity for their remarkable performance. Other models such as Bard [6], LLaMA [24], Dolly [2], Bloom [21], and Vicuna [26] have also emerged.\nSome of those models are already provided to consumers as a web service. Moreover, via API, those models and services are also now available for sub-parts of web services, and many spin-off services are emerging. However, despite the prosperity of language models, there are still challenges in handling diverse prompts, including prompts written in languages other than English. For example, Alpaca [23] dataset has been proposed due to the incompleteness of LLaMA\u2019s response. However, the dataset of Alpaca is only available in English, and the incompleteness pointed out by Alpaca has not been filled yet in the other languages. Moreover, LLaMA has difficultness to respond appropriately to some prompts in languages other than English. Considering these challenges, it is necessary to enhance models\u2019 performances in languages other than English. However, it is not a good idea to study a specific model in terms of performance improvements in the other language. Moreover, model development is still ongoing and very competitive, and the situation is changing dramatically recently. It is also easy to assume that newer models with better performance will emerge in a few months or even 1-2 months. Therefore, enhancing datasets that support model training may be more useful than focusing on specific models. This approach may also lower the barrier to adapting new models to languages other than English. Therefore, this study constructed a new chat dataset in Japanese for LLM training, which contains approximately 8.4 million data points, and demonstrated the performance of the dataset qualitatively. The dataset and trained models are opensourced and publicly available. The details are as follows:\n\u2022 Dataset: https://huggingface.co/datasets/izumi-lab/llm-japanese dataset \u2022 Trained Models (LLaMA 1 epoch): https://huggingface.co/izumi-lab/llama-13b-japanese-lorav0-1ep\nThe more details are explained in the following. Moreover, data expansion and additional model training are planned as future tasks.\n# 2 Dataset Construction: izumi-lab/llm-japanese-dataset v0\nIn this study, we created a Japanese chat dataset. The dataset1 contains 8,393,726 data points. In the following, we describe the details of datasets and their creation\nIn this study, we created a Japanese chat dataset. The dataset1 contains 8,393,726 data points. In the following, we describe the details of datasets and their creation process. The dataset is publicly available at the following URLs:\n\u2022 Hugging Face: https://huggingface.co/datasets/izumi-lab/llmjapanese-dataset \u2022 GitHub: https://github.com/masanorihirano/llm-japanese-dataset 1 In this paper, we mentioned v0, specifically v0.1.0. There is a possibility of future updates.\n\u2022 Hugging Face: https://huggingface.co/datasets/izumi-lab/llmjapanese-dataset \u2022 GitHub: https://github.com/masanorihirano/llm-japanese-dataset\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/07c3/07c3c7ee-a5f8-46eb-a53a-075073cc427f.png\" style=\"width: 50%;\"></div>\n# 2.1 Translation Task\nIn many LLM models, while the performance in English is good, the support for languages other than English, such as Japanese, is often immature. In such cases, if it were possible to link knowledge in English with other languages, such as Japanese, it would be possible to transfer the knowledge learned in English to multilingual settings. Therefore, we set up translation tasks from Japanese to English and from English to Japanese.\nTranslation Task Example 1\n### Instruction:\nPlease translate the following Japanese into English.1\n### Input:\nHe doesn\u2019t see his family in his busy life.1\n### Response:\nHe doesn\u2019t see his family in his busy life.\nTranslation Task Example 2\n### Instruction:\nPlease translate the title of the following English paper into Japanese. 1\n### Input:\nOdd viscosity in chiral active fluids\n### Response:\nOdd viscosity in chiral active fluids1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8fb5/8fb51d83-08c3-4cb8-b81d-593019cdda18.png\" style=\"width: 50%;\"></div>\n# 2.2 Knowledge-based Tasks\nNext, to appropriately learn knowledge in Japanese, knowledge-based tasks were incorporated into the training dataset. In the following, we explain the datasets used for creating our dataset and the created dataset examples.\n\u2022 Wikipedia8: By using the titles and first paragraphs of Wikipedia, the task of explaining words was created. The license is CC BY-SA 3.0. The dataset size created is 1,347,381. \u2022 Japanese WordNet [8]9: By using the word and definition parts, a chat was created that requests word explanations. The license is a BSD-like license unique to it. The dataset size created is 173,085. \u2022 AIO (AI King) Official Distribution Dataset Version 2.010: Since it is a quiz format dataset, it was incorporated directly as a chat dataset. The license is CC BY-SA 4.0. The dataset size created is 23,335. \u2022 Japanese Movie Recommendation Dialogue (JMRD) [10] 11: Only the metadata related to movies was extracted, and a chat dataset was created that shows movie descriptions and summaries. The license is CC BY-SA 4.0. The dataset size created is 260. \u2022 JCommonsenseQA [11]12: A Q&A dataset that questions common sense, which is part of the Japanese General Language Understanding Evaluation (JGLUE), was incorporated as its original form. The license is CC BY-SA 4.0. The dataset size created is 10,058. \u2022 Aozorra Paperback13: Only works of Japanese literature whose copyright has expired were selected from Aozora Paperback14, and questions were created that ask for the beginning of the sentence and the author and title from the beginning of the sentence. The dataset size created is 33,758.\n<div style=\"text-align: center;\">Combining them, the total number of datasets is 1,587,877. Knowledge-based Tasks: Wikipedia</div>\nKnowledge-based Tasks: Wikipedia\n### Instruction:\nPlease explain the input word.1\n### Input:\nThe University of Tokyo1\n### Response:\nThe University of Tokyo (Tokyo Daigaku) is a Japanese national university headquartered in\nBunkyo-ku, Tokyo. Its abbreviated name is Todai.1\nKnowledge-based Tasks: Japanese WordNet\n### Instruction:\nWhat does \u201dpossible\u201d mean?1\n### Response:\nHave the methods, skills or know-how or rights necessary to do something1\nKnowledge-based Tasks: AIO\n### Instruction:\nWhat bird is used to describe the personalities of three warlords, Nobunaga Oda, Hideyoshi\nToyotomi, and Ieyasu Tokugawa?1\n### Response:\nCuculus poliocephalus1\nKnowledge-based Tasks: JMRD (partially omitted)\n### Instruction:\nTell us about My Neighbor Totoro.1\n### Response:\nMy Neighbor Totoro is a movie produced in 1988 and directed by Hayao Miyazaki. Noriko\nHidaka and Chinatsu Sakamoto appear in the movie. The summary is as follows:\n[Summary]\nAn elementary school student Satsuki and her younger sister Mei move with their father to a\nfarming village in early summer for their mother\u2019s medical treatment. ...1\nKnowledge-based Tasks: JCommonsenseQA\n### Instruction:\nWhich of the following are primarily for children and contain illustrated stories?1\n### Input:\nWorld, photo book, illustrated book, research paper, picture book1\n### Response:\nIllustrated book1\nKnowledge-based Tasks: Aozorra Paperback (1)\n### Instruction:\nWhat is the beginning of Ivan Turgenev\u2019s work \u201cFirst Love\u201d?1\n### Response:\nIt begins with \u201cThe guests had long since departed. The clock struck half-past twelve. There\nremained in the room only the host, Sergy\u00b4ei Nikol\u00b4aevitch, and Vlad\u00b4\u0131mir Petr\u00b4ovitch. The host\nrang and ordered the remains of the supper to be removed.\u201d1\nKnowledge-based Tasks: Aozorra Paperback (2)\n### Instruction:\nPlease tell us the author and title of the literary work that begins, \u201cOnce there were two\nyoung gentlemen climbing up deep into the mountains. They dressed like British soldiers\nand walking on the dry leaves carrying shining guns on their shoulders.\u201d1\n### Response:\n\u201cThe Restaurant of Many Orders\u201d written by Kenji Miyazawa.1\n# 2.3 Other Tasks\n# 2.3.1 Creating a Paper Title and Imagining an Abstract\nWe utilized the aforementioned ParaNatCom [25] to create tasks related to our research paper. The license for the dataset is CC BY 4.0, and the size of the created dataset is 1,732.\nWe utilized the aforementioned ParaNatCom [25] to create tasks related to our re-\nsearch paper. The license for the dataset is CC BY 4.0, and the size of the created\ndataset is 1,732.\nTask Example 1 (partially omitted)\n### Instruction:\nPlease make a title from the abstract of the paper. 1\n### Input:\nSuperthin nanostructures, particularly with atomic-level thicknesses, typically display\nunique optical properties because of their exceptional light-matter interactions. Here, we\nreport a facile strategy for the synthesis of sulfur-doped molybdenum oxide nanorings ...1\n### Response:\nAtomic-level\nmolybdenum\noxide\nnanorings\nwith\nfull-spectrum\nabsorption\nand\nphotoresponsive properties1\nTask Example 2 (partially omitted)\n### Instruction:\nImagine and write an abstract from the paper\u2019s title.1\n### Input:\nSurface plasmon induced direct detection of long wavelength photons1\n### Response:\nMillimeter and terahertz wave photodetectors have long been of great interest due to a wide\nrange of applications, but they still face challenges in detection performance. Here, we\npropose a new strategy for the direct detection of millimeter and terahertz wave photons\nbased on localized surface-plasmon-polariton (SPP)-induced non-equilibrium ...1\nTask Example 1 (partially omitted)\n### Instruction:\nPlease make a title from the abstract of the paper. 1\n### Input:\nSuperthin nanostructures, particularly with atomic-level thicknesses, typically display\nunique optical properties because of their exceptional light-matter interactions. Here, we\nreport a facile strategy for the synthesis of sulfur-doped molybdenum oxide nanorings ...1\n### Response:\nAtomic-level\nmolybdenum\noxide\nnanorings\nwith\nfull-spectrum\nabsorption\nand\nphotoresponsive properties1\nphotoresponsive properties\nTask Example 2 (partially omitted)\n### Instruction:\nImagine and write an abstract from the paper\u2019s title.1\n### Input:\nSurface plasmon induced direct detection of long wavelength photons1\n### Response:\nMillimeter and terahertz wave photodetectors have long been of great interest due to a wide\nrange of applications, but they still face challenges in detection performance. Here, we\npropose a new strategy for the direct detection of millimeter and terahertz wave photons\nbased on localized surface-plasmon-polariton (SPP)-induced non-equilibrium ...1\n# 2.3.2 News Title Generation\nWe retrieved articles from the Japanese version of Wikinews15, and set the task of generating a title from the article body. The license is CC BY 2.5. The size of the created dataset is 4,265.\nTask Example (partially omitted)\n### Instruction:\nPlease write a title for the following news article.1\n### Input:\nOn January 11, Iwanami Shoten released the 6th edition of Kojien, the first major revision of\nthe Kojien dictionary in 10 years. The sixth edition will be the first revision in 10 years ...1\n### Response:\nKojien is revised for the first time in 10 years - Sixth edition released. 1\n# 2.3.3 Answer Extraction Task\nIn addition to knowledge and translation tasks, we also adopted a task of extracting answers from the given context. As the source data, we used JSQuAD [11]16, which is part of JGLUE tasks. The license is CC BY-SA 4.0, and the dataset size created is 67,491.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6a13/6a13ebd5-240d-476b-853e-0bf8823ea63c.png\" style=\"width: 50%;\"></div>\n# 2.3.4 Easy Japanese Task\nWe incorporated the task of rewriting normal Japanese into simpler Japanese. We processed SNOW T15: Japanese Simplified Corpus with Core Vocabulary [13] and SNOW T23: Crowdsourced Corpus of Sentence Simplification with Core Vocabulary [9] as the original data. The license is CC BY 4.0. The size of the created dataset is 84,300.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7bb7/7bb75488-e7b8-49f3-92d9-676c014a23f3.png\" style=\"width: 50%;\"></div>\n# 2.3.5 Japanese Translation of Existing Chat Dataset\nIn addition, we incorporated Japanese-translated versions of existing publicly available chat datasets. The following datasets were included:\nIn addition, we incorporated Japanese-translated versions of existing publicly available chat datasets. The following datasets were included: \u2022 Japanese-Alpaca-LoRA17: A translation of the Alpaca[23] dataset into Japanese. The license is Apache License 2.0. The dataset size is 52,002. \u2022 databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2]. The license is CC BY-SA 3.0. The dataset size is 15,015.\n\u2022 Japanese-Alpaca-LoRA17: A translation of the Alpaca[23] dataset into Japanese. The license is Apache License 2.0. The dataset size is 52,002. \u2022 databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2]. The license is CC BY-SA 3.0. The dataset size is 15,015.\n# 3 Model Construction and Evaluation Using Our Dataset\nThis study used LoRA [7] as a method to fine-tune LLMs without significant performance degradations. It is because building LLMs from scratch requires a massive amount of computational resources. Furthermore, LLMs with a large number of parameters require GPU resources not only for pre-training but also for fine-tuning. On the other hand, LoRA updates only small parts of LLM parameters. Therefore, LoRA is a feasible option for us to evaluate the benefits of our dataset. The main parameters used in the experiment are shown below.\n\u2022 Base model: LLaMA 13B [24] \u2022 Learning rate: 3e-4 \u2022 Input length: 256 tokens \u2022 Batch size: 130 \u2022 Epochs: 1 \u2022 r of LoRA: 4 \u2022 \u03b1 of LoRA: 16 \u2022 Dropout rate of LoRA: 0.05 \u2022 Tuning parameters in LoRA: q proj, v proj\nWe used PEFT[12] and DeepSpeed ZeRO 2[19] for the implementation. This tuned model is publicly available at https://huggingface.co/izum lab/llama-13b-japanese-lora-v0-1ep. In order to increase the reproducibility of the evaluation experiment, the temperature parameter for prompt generation was set to 0.0. Below are some qualitative comparisons we conducted to assess performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14dc/14dcab65-7c08-448b-b299-b16f4457524e.png\" style=\"width: 50%;\"></div>\n10\nMasanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI\nThe University of Tokyo is located at 1-1-1 Bunkyo, Bunkyo-ku, Tokyo.1\n(Authors\u2019 note: The correct address is 7-3-1 Hongo, Bunkyo-ku, Tokyo.)\nResponse Example (4)\n### Input:\nA phone rang. What should I do?1\n### Output(LLaMA+LoRA):\nTo hang up the phone.1\n### Output(LLaMA):\nThe phone rings. When the call is received, the person receiving the call should receive the\ncall.1\nResponse Example (5)\n### Input:\nWhat are the three major festivals in Kyoto?1\n### Output(LLaMA+LoRA):\nThe three major festivals inKyoto are the spring festival, the summer festival, and the autumn\nfestival.1\n(Authors\u2019 note: Correct answer is Aoi, Gion, Jidai festivals.)\n### Output(LLaMA):\nWhat are the three major festivals in Kyoto?\nWhat are the three major festivals in Kyoto?\nWhat are the three major festivals in Kyoto?1\n<div style=\"text-align: center;\">The University of Tokyo is located at 1-1-1 Bunkyo, Bunkyo-ku, Tokyo.1 (Authors\u2019 note: The correct address is 7-3-1 Hongo, Bunkyo-ku, Tokyo.)</div>\n# 4 Discussion\nIn this study, we constructed a dataset, izumi-lab/llm-japanese-dataset v0. The dataset consists of approximately 8.4 million data points, mainly focusing on Japanese-English translation tasks. The purpose of this dataset was to improve the performance of Japanese language processing capability and connect English language processing capabilities with Japanese input and output. Therefore, we tested additional LoRA tuning to the LLM using our datasets to evaluate its benefits. Reviewing the actual prompt responses after LoRA tuning, there appears to be a slight improvement in performance. However, since we only trained LLaMA with one epoch of LoRA tuning, the difference may be small. Therefore, increasing the number of epochs could be a future work. Nevertheless, the results suggest that our new dataset is effective. For example, the responses of our model (LLaMA + LoRA) in examples (4) and (5) were incorrect, but LoRA tuning seems to achieve generating more appropriate Japanese outputs. Additionally, although the responses of our model (LLaMA + LoRA) in examples (2) and (3) were still somewhat unfluent, they have achieved generated correct responses in terms of facts. However, this qualitative comparison is not fully appropriate, and establishing a comparison method is also a possible future work. To evaluate more accurately, it is necessary to construct evaluation tasks and datasets specialized for Japanese,\nand there may be a need for a Japanese version of the Language Model Evaluation Harness [5] evaluation scheme.\n# 5 Conclusion\nIn this study, we constructed a Japanese chat dataset for training LLMs on chat responses. The dataset can be used for tuning chat tasks to LLMs created mainly in English, and we qualitatively confirmed the performance of improvement in the LLM tuned by our dataset and LoRA tuning. Our dataset is now publicly available. Through the construction of the model to check the performance of the dataset, we have also clarified some issues in the construction of LLMs for languages other than English. Such a trial to adapt LLMs built mainly for English into other languages is now essential to make LLMs available fluently in wider languages.\nAcknowledgements This work was supported in part by JSPS KAKENHI Grant Number JP21K12010 and JST PRESTO Grant Number JPMJPR2267.\n# References\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The development of large language models (LLMs) has primarily focused on English, leaving a gap in performance for languages like Japanese. This study addresses the necessity of enhancing model performance in languages other than English by constructing a dedicated dataset.",
            "purpose of benchmark": "The benchmark aims to provide a dataset for tuning and training LLMs specifically for Japanese chat interactions, facilitating comparisons and improvements in model performance."
        },
        "problem": {
            "definition": "The benchmark is designed to address the challenges of training LLMs in Japanese, specifically focusing on tasks such as translation and knowledge-based interactions.",
            "key obstacle": "Existing benchmarks often do not support languages other than English effectively, leading to limitations in performance and adaptability for models trained primarily on English datasets."
        },
        "idea": {
            "intuition": "The inspiration for this benchmark stems from the observed gaps in LLM performance for Japanese, particularly in translation and knowledge tasks, highlighting the need for a dedicated dataset.",
            "opinion": "The authors believe that this benchmark is crucial for advancing the field of Japanese language processing and improving the adaptability of LLMs for non-English languages.",
            "innovation": "This benchmark introduces a comprehensive chat dataset specifically for Japanese, which includes diverse tasks that were previously underrepresented in existing benchmarks.",
            "benchmark abbreviation": "llm-japanese-dataset v0"
        },
        "dataset": {
            "source": "The dataset was constructed from various sources, including Wikipedia, Japanese WordNet, and other publicly available datasets, focusing on Japanese language tasks.",
            "desc": "The dataset contains approximately 8,393,726 data points, designed to support a variety of tasks including translation and knowledge-based queries.",
            "content": "The dataset includes text data related to translation tasks, knowledge-based tasks, and other chat interactions.",
            "size": "8,393,726",
            "domain": "Translation",
            "task format": "Chat-based interactions"
        },
        "metrics": {
            "metric name": "Qualitative performance evaluation",
            "aspect": "Model output fluency and correctness",
            "principle": "The metrics were chosen to assess the practical utility of the dataset in improving LLM responses in Japanese, focusing on qualitative improvements.",
            "procedure": "The evaluation involved qualitative comparisons of model outputs before and after fine-tuning with the dataset."
        },
        "experiments": {
            "model": "LLaMA 13B",
            "procedure": "The model was fine-tuned using the LoRA method with specific parameters such as a learning rate of 3e-4 and a batch size of 130 over one epoch.",
            "result": "The results indicated slight improvements in the model's ability to generate appropriate Japanese responses after tuning.",
            "variability": "Variability was accounted for by conducting qualitative evaluations of model outputs across different prompt scenarios."
        },
        "conclusion": "The study concludes that the constructed Japanese chat dataset is effective for tuning LLMs, demonstrating improvements in performance and highlighting the importance of adapting models for languages other than English.",
        "discussion": {
            "advantage": "The benchmark provides a significant resource for advancing Japanese language processing capabilities in LLMs, enabling better performance in non-English contexts.",
            "limitation": "The current dataset is limited to specific tasks and may not cover all aspects of Japanese language processing, which could affect comprehensive evaluations.",
            "future work": "Future research may focus on expanding the dataset, increasing the number of training epochs, and developing specialized evaluation tasks for Japanese."
        },
        "other info": {
            "acknowledgements": "This work was supported in part by JSPS KAKENHI Grant Number JP21K12010 and JST PRESTO Grant Number JPMJPR2267."
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The development of large language models (LLMs) has primarily focused on English, leaving a gap in performance for languages like Japanese."
        },
        {
            "section number": "2.3",
            "key information": "The benchmark aims to provide a dataset for tuning and training LLMs specifically for Japanese chat interactions, facilitating comparisons and improvements in model performance."
        },
        {
            "section number": "4.1",
            "key information": "This benchmark introduces a comprehensive chat dataset specifically for Japanese, which includes diverse tasks that were previously underrepresented in existing benchmarks."
        },
        {
            "section number": "4.2",
            "key information": "The dataset contains approximately 8,393,726 data points, designed to support a variety of tasks including translation and knowledge-based queries."
        },
        {
            "section number": "10.2",
            "key information": "Future research may focus on expanding the dataset, increasing the number of training epochs, and developing specialized evaluation tasks for Japanese."
        },
        {
            "section number": "11",
            "key information": "The study concludes that the constructed Japanese chat dataset is effective for tuning LLMs, demonstrating improvements in performance and highlighting the importance of adapting models for languages other than English."
        }
    ],
    "similarity_score": 0.7283165631537051,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/llm-japanese-dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology.json"
}