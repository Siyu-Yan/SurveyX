{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2308.00278",
    "title": "Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction",
    "abstract": "A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures -- Label-Trustworthiness and Label-Continuity (Label-T&C) -- advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.",
    "bib_name": "jeon2023classesclustersimprovinglabelbased",
    "md_text": "# Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction\non, Yun-Hsin Kuo, Micha\u00a8el Aupetit, Kwan-Liu Ma, and Jinwook Seo\nAbstract\u2014 A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures\u2014Label-Trustworthiness and Label-Continuity (Label-T&C)\u2014advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters. Index Terms\u2014Dimensionality Reduction, Reliability, Clustering, Clustering Validation Measures, Dimensionality Reduction Evaluation\n# 1 INTRODUCTION\nDimensionality reduction (DR) is one of the most widely used tools in conducting the visual cluster analysis of high-dimensional data [27,34, 52\u201354,70]. Using DR for cluster analysis relies on the assumption that the cluster structure of the original data is accurately represented in the low-dimensional DR embeddings. However, DR inherently generates distortions, i.e., the original cluster structure is imprecisely represented in the embeddings [2, 7, 26, 40, 41]. As distortions can make visual cluster analysis performed with DR unreliable [27,29], it is important to evaluate how well the original cluster structure is preserved in the DR embeddings [29,32,44,70], prior to the analysis. There exist ways to evaluate the reliability of cluster structures in DR embeddings, in either a perceptual [20,57,70] or computational [29,37,47,62] manner. A general process to evaluate the preservation of cluster structure in DR embeddings is to utilize class labels. This is done by assessing cluster-label matching (CLM), that is, the extent to which class labels form clusters in the embedded space [8, 22, 32, 69, 71, 73]. CLM is mostly evaluated by using clustering validation measures (CVMs) [42, 68], such as the Silhouette Coefficient [55]. CVMs inform how well the groups in the given label-based data partition form clear position-based clusters. The partitions that contain mutually separated and individually condensed groups are preferred. For the label-based evaluation of DR, data embeddings and class labels are used as an input dataset and partition, respectively. Embeddings with good CLM are considered to have good quality, assuming that the original data also have good CLM. However, such an assumption can hardly be guaranteed [3,23,28,70]. There is no constraint on labels\u2019 sources. Labels can come from an external source (e.g., human annotation), possibly unrelated to the features of the data space. Labels can also result from clustering techniques, which may not align with the actual clusters. Therefore, we do not know how well labels make up the clusters in the original data; a single class can consist of multiple separated clusters, and\nmultiple classes can be in close proximity or even overlapped [3] in a single cluster. These possibilities cast doubt on the conclusions derived from the general process of label-based DR evaluation. For instance, an embedding that accurately represents overlapping classes in the original space might be considered to have low quality as it has bad CLM. In this work, we revisit the process of evaluating DR using class labels. We introduce two measures\u2014Label-Trustworthiness (Label-T) and Label-Continuity (Label-C)\u2014which examine CLM in an alternative way to assess the reliability of cluster structures in DR embeddings. In contrast to the general label-based evaluation process, Label-T&C use CVM to quantify CLM distortions as the difference between CLM estimated in both original and embedded spaces. Label-T quantifies the distortion due to the degradation of CLM: the score is lower when the points of two different classes get closer in the embedding than in the original space. Conversely, Label-C evaluates the distortion regarding the exaggeration of CLM: the score is lower when the points of two different classes get farther apart in the embedding than in the original space. The rationale behind our measures is that in visual cluster analysis, it is important to investigate how class labels span the original cluster structure as seen through the embedding [3\u20135,12,67] (e.g., examine the individual density of a class or the pairwise proximity between classes). Since CLM distortions reduce the reliability of cluster structures represented by the embeddings, Label-T&C scores can be interpreted as proxies for the credibility of DR-based cluster analysis. We conduct a series of quantitative experiments to validate the effectiveness of Label-T&C. The results show that Label-T&C can better capture the distortions of cluster structures than the existing measures (e.g., Steadiness & Cohesiveness [29] and Trustworthiness & Continuity [62]) and the general process of label-based DR evaluation (i.e., naive application of CVMs). From the scalability analysis, we validate that the runtime of using Label-T&C is competitive with that of the existing methods. Finally, we demonstrate two case studies showing that Label-T&C can be used to reveal how different DR techniques or hyperparameter settings affect embedding results.\n# 2 BACKGROUND AND RELATED WORKS\nWe discuss the state-of-the-art in interpreting and measuring the reliability of DR embeddings. We then describe works about the common assumption that high-dimensional labeled data have good CLM.\n# 2.1 Reliability of Dimensionality Reduction 2.1.1 Dimensionality Reduction\nDimensionality reduction (DR), e.g., t-SNE [61], UMAP [45], aims to produce the low-dimensional embedding preserving the structure of\nthe input high-dimensional data. DR plays an important role in many visual analytics tasks, including cluster identification [27,69] or neighborhood search [20,21,40]. This research provides reliable measures for evaluating DR embeddings regarding the matching between clusters and classes in both input and embedding spaces, establishing a basis for more trustworthy DR-based visual analysis.\n# 2.1.2 Distortions in Dimensionality Reduction\nWhile transferring the data from broad high-dimensional space to narrow low-dimensional space, DR unavoidably introduces distortions [2,50]. As distortions make embeddings less reliable in representing the original data, informing distortions is important in utilizing DR for data analysis [29,50]. Several distortion types were defined to formally explain DR distortions. Aupetit [2] initially defined stretching and compression. Stretching describes the situation in which the pairwise distances in the embedded space are increased compared to the ones of the original space; conversely, compression indicates the case that the pairwise distances decreased. Missing Neighbors and False Neighbors [37, 40, 41, 63] were introduced as an interpretation of stretching and compression in terms of the neighborhood structure. Given a high-dimensional point x and its corresponding low-dimensional point z, Missing Neighbors are defined as the k-nearest neighbors of x that are not among the ones of z. Conversely, False Neighbors are defined as the k-nearest neighbors of z that are not among the ones of x. However, Missing and False Neighbors are insufficient to explain the distortions of complex, intertwined cluster structures. For example, the relative increase of cluster density in the embedding does not incur Missing and False Neighbors distortions, because it does not alter the k-nearest neighbor structure for small k values. As alternatives, cluster-level distortions, named Missing Groups and False Groups, were proposed by Jeon et al. [29]. Missing Groups occur when a cluster in the original space splits into multiple separated clusters in the embedding, and False Groups occur when a cluster in the embedding consists of multiple separated clusters in the original space. In the seminal work [29], Missing and False Groups distortions are examined based on the groups obtained by clustering techniques. In this work, we focus on evaluating the reliability of the cluster structure of DR embeddings by quantifying both Missing and False Groups. However, instead of extracting groups using clustering techniques, we focus on the groups given by the classes of labeled data.\n# 2.1.3 Distortion Measurement without Labels\nWe discuss distortion measures that do not rely on class labels. These measures take the original and embedded data as input and quantify their structural difference. Aligned with the aforementioned distortion types, they focus on three different levels of structural granularity: global, local, and cluster. Global measures, such as Kullback-Liebler divergence (KL Divergence) and Distance to Measure (DTM) [15,16], quantify how well the embeddings preserve the global structure of the original data against stretching and compression. Meanwhile, local measures focus on neighborhood preservation. Trustworthiness and Continuity (T&C) [62] measure how Missing and False Neighbors affected the distance-based ranking of the nearest neighbor for every data point in both spaces. Mean Relative Rank Errors (MRREs) [37] extends T&C by additionally regarding the ranking of True Neighbors: the points that are neighbors in both the original and embedded spaces. Still, despite local and global measures\u2019 wide usage in practice [29,30, 35,46,50,69], they do not properly capture cluster-level distortions [29]. This leads to the necessity of measures that capture distortions on cluster structures (i.e., cluster-level measures). Steadiness and Cohesiveness (S&C) [29] assess how much Missing and False Groups distortions have occurred by (1) extracting clusters from one space and (2) evaluating their dispersion in the other space. However, S&C require users to specify the way of extracting and investigating clusters in both spaces, e.g., using clustering techniques, making the results of the cluster-level distortion measures sensitive to the clustering technique and hyperparameters used. S&C also suffers from a scalability issue as it requires the iterative execution of a clustering technique [25,29].\nLabel-T&C is a pair of cluster-level measures that aim to tackle these limitations. At first, the measures require a CVM as the sole hyperparameter, which is used to evaluate CLM in the original and embedded spaces. Thanks to the low complexity of CVM [28,42], our measures are very scalable (Sect. 5.2). Furthermore, Label-T&C are more sensitive in distinguishing Missing and False Groups distortions compared to previous measures, including S&C (Sect. 5.1).\n# 2.1.4 Distortion Measurement with Labels\nExploiting labels is a common scheme in evaluating DR embeddings [8,17,22,32,69,71,73]. A general process to do so is to utilize CVM to measure the CLM of embeddings [8, 22, 32, 73]. However, the approach is prone to producing errors while examining the quality of DR embedding. For example, if the CLM of the original data is bad (e.g., some classes overlap), embeddings that have good CLM for bad reasons (e.g., DR artificially separates each class into a distinct cluster) will be considered high-quality embeddings. As non-expert users typically assume that DR techniques generate reliable embeddings of the original data, they may incorrectly conclude that CLM is also good in the high-dimensional space, while it is not actually true [3,28]. A sole pair of measures that relies on class labels but is independent of CVM is Class-Aware Trustworthiness and Continuity (CA-T&C) [17]. CA-T&C is a variant of T&C that assess the degradation of CLM (i.e., False Groups distortions) by estimating the extent to which Missing and False Neighbors occurred within and between classes, respectively. However, CA-T&C hardly captures the Missing Groups distortions as they do not consider the increase of CLM as distortions. The measures also mainly focus on local structures and thus cannot comprehensively examine CLM distortions. In this work, we propose Label-T&C as novel measures utilizing class labels to evaluate DR embeddings. As with the general process of label-based DR evaluation (i.e., the process of naively applying CVM in the embedded space), our measures utilize CVMs to evaluate CLM; however, by applying CVM to both the original and embedded spaces and assessing their difference, our measures precisely capture cluster-level distortions.\n# 2.2 The Cluster-Label Matching assumption\nThe assumption that the CLM is good in the high-dimensional data is used as a basis not only for the label-based evaluation of DR embeddings but also for other applications. For example, the labels are often utilized as the ground truth partition in clustering validations, where clustering techniques that generate a similar partition to that of labels obtain higher scores (i.e., external clustering validation; refer to Sect. 3.1 for details). Another application is the perception-based evaluation of DR techniques [20,21,57,69], where techniques that produce embeddings in which the visual clustering results of human subjects better match class labels are preferred. However, the assumption can be easily broken [3,23], which casts doubt on the applications\u2019 reliability. Despite such a threat, only a few solutions have emerged. A trivial solution is to modify datasets to make them better satisfy the assumption. Aupetit [3,6] proposed to check the linear or nonlinear separability of classes and then merge overlapped classes or preserve one of them while removing the others [3]. However, classes can be separable but adjacent, not forming proper clusters (no low density or wide empty space between them). Such a strategy also does not take into account whether each class forms a single, compact cluster. Another solution is to use synthetic datasets [29,30,46], where good CLM is guaranteed by design. Still, this makes the evaluation hardly generalizable to real data. Alternatively, Jeon et al. [28] suggested a systematic way to evaluate CLM; their purpose was to verify the validity of labeled datasets for use as clustering validation benchmarks. Still, they suggested only utilizing datasets with good CLM, which reduces the number of available datasets for evaluating DR embeddings. In this work, we neither verify the CLM of datasets in advance nor attempt to modify datasets to enhance CLM. Instead, we acknowledge that datasets may not satisfy the CLM, and rather assess whether the degree of CLM, either high or low, in the original dataset is well preserved in the embedding.\n# 3 GENERAL LABEL-BASED DR EVALUATION PROCESS\nThe general process of label-based DR evaluation mostly relies on CVMs. We describe what CVMs are and the process of using them to evaluate CLM. We then discuss the pitfalls of the process. Notations We define a high-dimensional data X = {xi \u2208RD,i = 1,2,\u00b7\u00b7\u00b7 ,N}. We denote the low-dimensional embedding of X as Z = {zi \u2208Rd | i = 1,2,\u00b7\u00b7\u00b7 ,N}, where D > d. For any set S \u2208{X,Z}, the distance function \u03b4 satisfies \u03b4(x,y) \u22650, \u03b4(x,y) = \u03b4(y,x) and \u03b4(x,y) = 0 if x = y \u2200x,y \u2208S. A partition of S is defined as P = {P1,P2,\u00b7\u00b7\u00b7 ,Pk} satisfying Pi \u2286S, Pi \u2229Pj = /0 and \u222ak i=1Pi = S. If a partition is defined by class labels, we denote the partition as PL. A clustering technique C takes S and \u03b4 as input and returns a partition PC of S.\n# 3.1 Clustering Validation Measures\nClustering validation measures (CVMs) evaluate how well-clustered the given partition (i.e., clustering) is in the given data. We use CVMs to find the optimal clustering technique or hyperparameter setting that produces the partition of the data that best matches its cluster structure. CVMs are largely divided into two types: internal CVM (IVM) [42, 43] and external CVM (EVM) [68]. IVMs evaluate a partition based on the internal structure of data. Formally, the IVM score mI(P,X,\u03b4) quantifies how well the groups within the partition P of X are individually condensed and mutually separated in X based on distance \u03b4. For example, the Silhouette Coefficient [55] examines how the within-group and between-group distances differ on average while using Euclidean distance as \u03b4. Alternatively, EVMs, such as the adjusted rand index [64], rely on a ground truth partition PGT . Here, the EVM score mE(P,PGT ) simply quantifies the degree of matching between the given partition P and PGT , regardless of the internal cluster structure of S. A higher score is assigned if P better matches with PGT . Data class labels PL are typically used as ground truth PGT [23,28].\n# 3.2 Using CVM to Evaluate CLM\nWe use CVMs to quantify the CLM of a DR embedding as a proxy for its reliability [8,32,69,73]. The process depends on the type of CVM: IVM-based evaluation For a given embedding Z, distance function \u03b4, and class labels PL, mI(PL,Z,\u03b4) represents the CLM between PL and Z. The Silhouette Coefficient is widely adopted in the visualization community [20, 22, 32, 65, 69]. The Davies-Bouldin index [18] is preferable in the context of star coordinates and Radviz [1,14]. Notably, while Distance Consistency (DSC) [59] was designed for DR visual quality evaluation [19,56,58], it can also be viewed as a CVM since it considers only the separation of class labels in the embeddings. EVM-based evaluation Given Z, \u03b4, PL, and a clustering technique C providing a partition PC = C(Z,\u03b4) of the embedded data, mE(PC,PL) represents CLM between PL and Z. K-Means and the adjusted rand index are commonly used for C and mE, respectively [31,71,74]. Notice that CVMs cannot account for the internal compactness of each class in isolation, but the CVM of a class partition will get worse if some of these classes lack compactness or split across several clusters.\n# 3.3 Pitfalls\nThe general process of label-based DR evaluation promotes embeddings with good CLM regardless of the CLM of the original data (Sect. 1). In other words, the process examines the extent to which CLM is harmed in embeddings while assuming that the original data has good CLM. Thus, if the assumption is broken, the process will frame embeddings that correctly represent overlapped classes to have False Groups distortions. As the process considers good CLM embeddings as high-quality ones, it is also incapable of detecting Missing Groups distortions that may arise from CLM amplification. These pitfalls were identified for the first time by Aupetit [3]. Our preliminary experiment confirms such a threat (Appendix D). The general process of label-based evaluation erroneously prefers DR techniques that maximize the separation among classes, instead of the ones that aim to preserve the original structure of data if the datasets have bad CLM. Here, we aim to introduce a new way of using class labels for DR evaluation that mitigates such a bias.\n# 4 LABEL-TRUSTWORTHINESS & LABEL-CONTINUITY\nWe introduce two distortion measures\u2014Label-Trustworthiness and Label-Continuity (Label-T&C)\u2014as an alternative way of using class labels for DR evaluation. Our measures examine how CLM differs in both the original and embedded spaces where CVM is used to quantify CLM. Label-T and Label-C capture the False and Missing Groups distortions, respectively. The measures are named after Trustworthiness and Continuity, two local distortion measures that focus on capturing False and Missing Neighbors [62].\nInputs, output, and hyperparameters Label-T&C take (1) the highdimensional data X, (2) its DR embedding Z, and (3) class labels PL = {PL,1,PL,2,\u00b7\u00b7\u00b7PL,k} as inputs. Both Label-T and Label-C output a number between 0 and 1; a higher value indicates lower distortions and a better embedding. For hyperparameters, a CVM m with distance function \u03b4 is given. If m is an EVM, we need to additionally select the clustering technique C as a hyperparameter (Sect. 3.2). The m should assign higher scores to better clusterings and range from 0 to 1 (refer to Sect. 4.2.1 for a detailed explanation about this requirement). Step 1. Measuring CLM in the original and embedded spaces We apply CVM to both the original and embedded spaces to examine CLM. Here, unlike the general process of label-based DR evaluation that applies CVM to all classes at once, we apply CVM to every pair of classes, so that we can take account of the relationships of classes in more detail. Formally, we construct the class-pairwise CLM matrices M(X) and M(Z), where the (i, j)-th cell of the matrices M(S)i,j (S \u2208 {X,Z}) is defined as:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f31b/f31bd5e4-a825-40cc-81db-272de7288f3c.png\" style=\"width: 50%;\"></div>\nLABEL-TRUSTWORTHINESS: 1\u2212avgi, jMFG i> j LABEL-CONTINUITY: 1\u2212avgi,jMMG i> j .\nNote that we subtract the average from 1 to make embeddings with fewer distortions receive higher quality scores.\n# 4.2 Selecting CVM for Label-T&C\nWe establish the requirements for CVM to get proper Label-T&C scores and present suitable CVM options. In this section, we denote m(P,S,\u03b4) as a CVM score with respect to P, S, and \u03b4. If m is an IVM mI, we set m(P,S,\u03b4) = mI(P,S,\u03b4). If m is an EVM mE, we set m(P,S,\u03b4) = mE(C(S,\u03b4),P) with C, the chosen clustering technique.\nWe set the first three requirements based on the following proposition: to be used for Label-T&C, a proper CVM should be comparable across X and Z. In other words, m shall consider only how well the given partition is clustered in the given data and be invariant to the characteristics that differ between X and Z but are not related to the cluster structure. For example, the scaling of the pairwise distances should not alter the score. Otherwise, the evaluation will be unreliable; for example, we can simply manipulate Label-T&C scores by scaling the original or embedded data while there is no change in the cluster structure. Previous works [10, 28] set axioms defining how a CVM can be independent of such features. They require CVMs to be invariant to the change of scale, dimensionality, and the number of points and classes\nand to have a fixed range. As X and Z already share the number of points and classes, we require CVMs to ensure the other three axioms. The first axiom requires CVMs to be invariant against the scaling of distances between points, which can be inherently different in X and Z: Scale Invariance [10] A CVM m is scale invariant if \u2200partition P, data S, and distance function \u03b4, m(P,S,\u03b4) = m(P,S,\u03b1\u03b4) \u2200\u03b1 > 0 (where \u03b1\u03b4 is a distance function satisfying \u03b1\u03b4(x,y) = \u03b1 \u00b7\u03b4(x,y), \u2200x,y \u2208S.).\n# (R1) A CVM should satisfy scale invarianc\nThe second axiom focuses on the effect of the data dimension on the distance \u03b4, due to the so-called curse of dimensionality [9]. The growing dimensions increase the average of pairwise distances while the variances remain constant [11,24,38], thus the differences between distances become negligible. To be used for Label-T&C, CVM should be shift invariant [38,39] to cancel the shift of the average distances due to the different dimensions of X and Z. Shift Invariance [28] A CVM m is shift invariant if \u2200P,S,\u03b4, m(P,S,\u03b4) = m(P,S,\u03b4 +\u03b2) \u2200\u03b2 > 0 (where \u03b4 +\u03b2 is a distance function satisfying (\u03b4 +\u03b2)(x,y) = \u03b4(x,y)+\u03b2, \u2200x,y \u2208S).\n(R2) A CVM should satisfy shift invariance.\nThe final axiom is about requiring CVMs to produce scores that conform to a fixed range of values, which is designed to capture the remaining subtle factors that are not influenced by the cluster structure. Range Invariance [28] A CVM m is range invariant if \u2200S,\u03b4, minP m(P,S,\u03b4) = \u03b1 and maxP m(P,X,\u03b4) = \u03b2, where \u03b1,\u03b2 are constants satisfying \u03b1 < \u03b2 (arbitrarily set to 0 and 1, respectively).\n(R3) A CVM should satisfy range invariance.\nAdditionally, we want CVMs to be stable against the change of hyperparameters. This is because the alteration of CVM scores due to the hyperparameter change can induce uncertainty in utilizing LabelT&C. This leads to the last axiom:\n(R4) A CVM should have no hyperparameter or should produce sim scores on the same input regardless of the hyperparameter settings\nWe examine CVMs commonly used for DR evaluation (Sect. 3.2) as potential candidates to be used for Label-T&C. For EVMs, we find that the combination of K-Means and adjusted rand index cannot be used. This is because the parameter K (i.e., number of clusters) in K-Means leads to the violation of R4. Indeed, as clustering techniques commonly require hyperparameters, EVMs hardly satisfy the aforementioned requirements. Studying how EVMs and clustering techniques can satisfy R4 is beyond the scope of this work. For IVMs, neither the Silhouette Coefficient [55] nor the DaviesBouldin index [18] satisfies shift invariance (R2; refer to Appendix A for the proof). However, we found that DSC satisfies all requirements, setting it as a proper CVM for Label-T&C (Appendix A). We additionally found that the between-dataset Calinski-Harabasz index (CHbtwn) [28], a variant of Calinski-Harabasz index [60], satisfies the four requirements: satisfaction of R1, R2, and R3 has been demonstrated earlier [28]; it also satisfies R4 as its unique hyperparameter is the number of Monte-Carlo simulations for normalizing the measure, which barely affects the output if the number is sufficiently high. We give a brief description of these two CVMs usable for Label-T&C: Distance Consistency (DSC) [59] DSC is defined as the number of data points closer to the centroid of another class than their own in the data, normalized by the total number of data points. As DSC ranges from 0.5 to 1 if the number of classes is two and assigns a lower score for a better CLM, we use the value 2(1\u2212DSC) to make it satisfy R3 (Sect. 4.1 (Step 1)). Between-dataset Calinski-Harabasz index (CHbtwn) [28] CHbtwn is defined as the ratio of compactness to separability. Compactness is defined as the distance between data points and the class centroids to which each point belongs, and separability is determined by the distances between class centroids and the centroid of the entire data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/18dd/18dd7b06-7012-4db0-a565-b8a8e54759f4.png\" style=\"width: 50%;\"></div>\nFig. 1. Guidelines to infer the CLM of the high-dimensional data based on the CLM of the embedded data (left column) and the scores given by Label-T (L-T) and Label-C (L-C) (first row) (see Sect. 4.3 for details).\n# 4.3 Guidelines to Interpret Label-T&C\nWe present the guidelines to interpret embeddings based on Label-T&C. If Label-T and Label-C are both high, the CLM of the embedding accurately depicts the CLM in the original space (Fig. 1A). High LabelT and low Label-C (Fig. 1B) mean that Missing Groups distortions have occurred, i.e., the CLM of the original data is worse than it appears in the embedding (first row); some pairs of classes appear more separated than they actually are in the data space. When the CLM of the embedding is already low (e.g. due to overlapping classes), Missing Groups distortions are more unlikely to happen as the CLM in the data would have to be even worse (second row). In contrast, low Label-T and high Label-C (Fig. 1C) inform that False Groups distortions have occurred; the CLM in the original data is better than in the embedding (second row). As False Groups distortions deteriorate the CLM of the embedding, the situation is unlikely to occur if the embedding has a good CLM, and thus can hardly become better (first row). Due to such a tradeoff between False and Missing Groups (i.e., more Missing Groups lead to fewer False Groups, and vice versa), it is unlikely to get low Label-T and Label-C at the same time (Fig. 1D). Our sensitivity analysis (Sect. 5.1; Fig. 4) confirms the existence of the tradeoff.\n# 4.4 Time Complexity\nThe complexity of Label-T&C depends on the CVM. As DSC is O(|S||PL|\u2206S), where \u2206S denotes the dimensionality of S, applying it to a pair of classes PL,i,PL, j requires O(|PL,i \u222aPL, j|\u2206S). As each class is considered |PL| times, Label-T&C with DSC is O(|S||PL|\u2206S). Similarly, as CHbtwn is O(|S||PL|2\u2206S), applying it to a pair of classes PL,i,PL, j requires O(|PL,i \u222aPL, j|\u2206S). Therefore, Label-T&C with CHbtwn is O(|S||PL|\u2206S). In both cases, the time complexity is linear in all variables. We evaluate the scalability of Label-T&C in Sect. 5.2.\n# 4.5 Implementation & Deployment\nWe deploy Label-T&C as a Python library. We provided an interface that allows users to implement and test custom CVM as a hyperparameter. The source code is available at github.com/hj-n/ltnc.\n# 5 QUANTITATIVE EVALUATIONS AND DISCUSSIONS\nWe conduct quantitative experiments to evaluate Label-T&C with DSC and CHbtwn, i.e., Label-T&C [DSC] and Label-T&C [CHbtwn], respectively. In the sensitivity analysis (Sect. 5.1), we check the accuracy of Label-T&C and competitors in quantifying distortions. We also evaluate the runtime of the measures (Sect. 5.2). Competitors. We first consider all distortion measures without labels (Sect. 2.1.3) as competitors. For global measures, we use KL divergence and DTM. T&C and MRRE are used as representative local measures. MRRE [Missing] and MRRE [False] target Missing and False Neighbors, respectively. We select S&C as the sole pair of measures targeting cluster-level distortions. For the measures using labels (Sect. 2.1.4), we first add CA-T&C. We then select Silhouette and DSC as representative CVMs used in the general label-based evaluation. For T&C, MRRE, and CA-T&C, we average their score across k-nearest neighbor values: k = [5,10,15,20,25], following Jeon et al. [29]. For KL divergence and DTM, we average the scores across different standard deviation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/541d/541deff1-0cd5-419c-8319-20d500624b60.png\" style=\"width: 50%;\"></div>\nFig. 2. The high-dimensional (HD) datasets and low-dimensional (LD) embeddings used in experiments A, B, and C of sensitivity analysis (Sect. 5.1). The experiments aim to check the distortion measures\u2019 ability to capture False Groups distortions. Class labels are mapped to colors. (A) The Coil-20 [49] dataset and the embeddings generated by randomizing the positions of the embedded points with a certain probability. (B) A HD dataset consists of six well-separated hyperballs (left) and its synthetic embeddings (right) made by initializing the embedding with six well-separated discs and gradually overlapping the discs in two different manners (B-1, 2). (C) The Fashion-MNIST [72] dataset and the PCA embeddings with different numbers of principal components (PC); here we depict the UMAP projection of PCA embeddings if it has more than two PCs (i.e., dimensionality is higher than two). We depict the relation between explained variance ratio and the number of PC in the line chart next to the embeddings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e6fb/e6fbbafc-f492-4129-9105-3a6ae96aebd9.png\" style=\"width: 50%;\"></div>\nFig. 3. The low-dimensional (LD) embeddings and corresponding high-dimensional (HD) datasets represented as UMAP embeddings, used in experiments D, E, and F of sensitivity analysis (Sect. 5.1) to examine distortion measures\u2019 ability to capture Missing Groups distortions. (D) An UMAP embedding of the Coil-20 [49] dataset (right), and the variants of the Coil-20 dataset made by randomizing the coordinates of data points in HD space with a certain probability. (E) A 2D embedding with six well-separated discs and synthetic HD datasets. We create the datasets by generating six 100D hyperballs and gradually overlapping them. (F) A 2D PCA embedding of the Fashion-MNIST dataset and corresponding HD datasets variants, created by slicing 20 principal components (PC) with different rankings. The line chart shows their corresponding explained variance ratio.\nvalues of Gaussian kernels \u03c3: [0.01,0.1,1], following Moor et al. [46]. For S&C, we use the default hyperparameter setting [29].\n# 5.1 Sensitivity Analysis\nWe conduct six experiments (A-E) to examine Label-T&C\u2019s sensitivity in quantifying False Groups (Fixed data and variable embeddings in experiments A, B, and C) or Missing Groups (Variable data and fixed embeddings in experiments D, E, and F) distortions. The labeled data and embeddings used in the experiments can be found in Fig. 2 (A, B, and C) and Fig. 3 (D, E, and F). In all of them, we run Label-T&C and competitors to evaluate the embeddings.\nExperiment A: Randomizing embeddings We examine whether Label-T&C and competitors can accurately quantify False Groups distortions. We generate a 2D UMAP embedding of the Coil-20 [49] dataset. We then create variants of the embedding with different levels of False Groups distortions by randomizing the location of the points. We create 21 variants, ranging the replacement probability from 0% (same as the original embedding) to 100% (totally randomized) with an interval of 5%. The original class assignments of Coil-20 are used as labels. We hypothesize that Label-T will decrease as the replacement probability grows, properly capturing False Groups distortions, while\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/32ae/32ae33b1-69fe-4676-b0e6-4aea7d3e60f2.png\" style=\"width: 50%;\"></div>\nFig. 4. The results of the sensitivity analysis (Sect. 5.1; experiments A-F). Solid lines and dashed lines represent the measure that focuses on compression (e.g., False Groups, False Neighbors) and stretching (e.g., Missing Groups, Missing Neighbors), respectively. Dotted lines represent global measures and CVMs. A pair of compression and stretching measures is represented with the same line color. Measure names in red, blue, and purple correspond to our approach, the measures without labels (Sect. 2.1.3), and the measures with labels (Sect. 2.1.4), respectively. In summary, Label-T (blue and orange bold line) and Label-C (blue and orange dotted line) accurately detect Missing and False Groups distortions, respectively. Meanwhile, all other measures, including general label-based DR evaluation (i.e. DSC and Silhouette), fail to capture these distortions.\nExperiment B: Overlapping discs We aim to check distortion measures\u2019 ability to precisely capture False Groups distortions, as with experiment A. We create a high-dimensional dataset consisting of six hyperballs with a radius of 5 lying in 100 dimensions. We set the hyperballs to be equidistant (= 10) from the origin. We then create an artificial 2D embedding consisting of six discs (radius of 1.5) evenly and equidistantly (= 4) distributed around the origin O. Data points and labels within each disc correspond to those of each hyperball. The positions of each point within the disc and hyperball are determined randomly. The label is also set based on the disc each point belongs to. We gradually overlap the discs to artificially generate distortions. Here, we use two overlapping schemes to evaluate the sensitivity of Label-T&C in detail, resulting in two separate subexperiments (B-1, B-2). In B-1, three independent pairs of adjacent discs are overlapped; for each pair of discs (A,B) with centers CA, CB, we adjusted \u2220CAOCB from 60\u25e6to 0\u25e6with an interval of 2.4\u25e6(25 embedding variants in total). In B-2, we overlap all discs at once by moving them toward the origin; for each disc A, we gradually decrease CAO from 4 to 0 with an interval of 0.16 (25 embedding variants). We hypothesize that the Label-T score will go down as False Groups distortions increase due to the overlap of the discs, while Label-C will stay still. We also hypothesize that Label-T will decrease more in B-2 than in B-1, as the overlap is larger. Experiment C: Decreasing the dimension of the embedded space We generate False Groups distortions by decreasing the dimensionality of embedded space and check whether the measures can detect the distortions. We prepare the Fashion-MNIST [72] as a high-dimensional dataset. We generate PCA embeddings with a decreasing number of top principal components (10 to 1 with an interval of 1; 10 embeddings in total). We expect the embeddings with a smaller number of principal components (i.e., embeddings lying in the space with fewer dimensions) to have more False Groups distortions as they have a smaller explained variance ratio (line chart in Fig. 2). We use the class assignments of the Fashion-MNIST dataset as labels. Our hypothesis is that Label-T will decrease as the dimensionality decreases, while Label-C will stay still. Experiment D: Randomizing the original data We want to evaluate Label-T&C and competitors\u2019 capability in accurately quantifying Miss-\ning Groups distortions. We first generate a fixed 2D UMAP embedding of the Coil-20 [49] dataset. We then generate the variants of the original data by mixing the points in the high-dimensional space with a fixed probability, producing Missing Groups distortions. We control the replacement probability from 0% to 100% with an interval of 5%, resulting in 21 variants. The class assignments of the original data are used as labels. We hypothesize that Label-C will decrease as Missing Groups distortions increase (i.e., replacement probability increase), and that Label-T will ignore the distortions. Experiment E: Overlapping hyperballs We want to evaluate whether Label-T&C and competitors can precisely capture Missing Groups distortions. We prepare variants of high-dimensional data and fixed low-dimensional embedding consisting of six 100D hyperballs and corresponding 2D discs, respectively. The points within the same disc have the same label. All discs are well separated from each other. We artificially overlap hyperballs to generate Missing Groups distortions. For each hyperball AH, we gradually decrease CAH O from 4 to 0 with an interval of 0.16 (25 variants in total). We hypothesize that Label-C will decrease as hyperballs overlap, while Label-T will stay still. Experiment F: Decreasing the dimension of the original data space We examine whether the distortion measures can detect the Missing Groups distortions made by the decrease in the dimensionality of the original data. We prepare a 2D PCA embedding of the Fashion-MNIST dataset. We then select ten 20D PCA embeddings with different sets of principal components as high-dimensional datasets; the i-th dataset variant consists of the (i)-th to (i+19)-th principal components, where 1 \u2264i \u226410. We expect the dataset with a higher order to have more Missing Groups distortions over the embedding as they have a smaller explained variance ratio (line chart in Fig. 3). We used the class assignments of the Fashion-MNIST dataset as labels. We hypothesize that Label-C will decrease as the starting index of principal components increases, while Label-T will stay still.\n# 5.1.2 Results\nFig. 4 shows the results of our experiments that we comment on below. Experiment A As the randomization probability grows, both Label-T [DSC] and Label-T [CHbtwn] similarly decrease linearly while Label-C\n[DSC] and Label-C [CHbtwn] slightly increase, confirming our hypothesis. Meanwhile, S&C and local measures decrease regardless of the distortion type, while global measures slightly increase. In the case of label-based measures, both CA-T&C and the general CVM-based process (DSC and Silhouette) show mainly decreasing scores. Experiment B In B-1, as the overlap between the discs grows, both Label-T [DSC] and Label-T [CHbtwn] decrease in a similar manner, while Label-Cs stay still. Such results validate our hypothesis, confirming Label-T&C\u2019s capability in properly detecting False Groups distortions. Meanwhile, S&C, T&C, and MRREs all decrease, while Steadiness, Trustworthiness, and MRRE [False] decrease more than Cohesiveness, CA-Continuity, and MRRE [Missing], respectively. Global measures stay still. CA-T&C partially succeed in properly detecting False Groups distortions; both CA-Continuity and CA-Trustworthiness decrease, but CA-Continuity\u2019s decrement was subtle compared to the one of CA-Trustworthiness. CVMs show a decreasing trend. In B-2, the amount of decrement becomes bigger than in B-1 for Label-T [DSC] and Label-T [CHbtwn] while Label-Cs again stay still, confirming our second hypothesis. The amount of decrement also becomes bigger than in B-1 for T&C, MRREs, and Cohesiveness, while Steadiness showed a similar drop as in B-1. In the case of KL divergence, DTM, and Silhouette, the patterns are almost identical to B-1 except that the scores rebound when the discs are nearly overlapped. The decrement becomes bigger also for CA-T&C and DSC. Experiment C As the number of PCs decreases, Label-Ts decrease while Label-Cs stay still, validating our hypothesis. Global measures (KL divergence, DTM) stay still while all other measures decrease. Experiment D As we increase the randomization probability, both Label-C [DSC] and Label-C [CHbtwn] decrease, while Label-Ts stay still, verifying our hypothesis. However, while Label-C [DSC] decreases right before the data are perfectly mixed, Label-C [CHbtwn] decreases from the start. For local measures, both T&C and MRREs decrease. Steadiness decreases, while Cohesiveness suddenly goes up after decreasing for a while. Global (KL divergence, DTM) measures increase in general. CA-Trustworthiness goes down while CA-Continuity stays still, and CVMs (DSC and Silhouette) stay still. Experiment E. When the overlap between hyperballs increases, both Label-C [DSC] and Label-C [CHbtwn] decrease, while Label-Ts stay still, verifying our hypothesis. However, as in experiment D, LabelC [DSC] and Label-C [CHbtwn] decrease differently; while Label-C [DSC] decreases right before the hyperballs perfectly overlap, Label-C [CHbtwn] decreases before Label-C [DSC] does. Meanwhile, local measures (T&C, MRRE) decrease, while global measures (KL divergence, DTM) stay still. Steadiness decreases while Cohesiveness temporarily pops up when Steadiness starts to decrease. CA-Trustworthiness maintains a maximum score while the CA-Continuity score increases before the perfect overlap of the hyperballs. CVMs stay still. Experiment F. The results confirm our hypothesis; as the starting index of the PCs that we slice increases, both Label-C [DSC] and Label-C [CHbtwn] decrease while Label-Ts stay still. Local measures (T&C, MRRE) decrease, and global measures (KL divergence, DTM) stay still. S&C decrease, while Steadiness decreases more than Cohesiveness. CA-T&C show a similar trend; CA-Trustworthiness decreases, while CA-Continuity decreases to a smaller extent. CVMs stay still.\n# 5.1.3 Discussions\nLabel-T&C and competitors\u2019 capability in detecting cluster-level distortions. The results from experiments A-C confirm that Label-T is sensitive to False Groups distortions, while Label-C is not, as we intended. Moreover, the difference between the B-1 and B-2 results validates Label-T\u2019s accuracy at measuring the amount of False Groups distortions. The results from experiments D-F, on the other hand, confirm that Label-C accurately captures Missing Groups distortions, while Label-T ignores them. The results also validate that previous measures fail to accurately detect the distortions or to distinguish specific distortion types. Global measures (KL Divergence, DTM) hardly discover distortions for all six experiments. Local measures (T&C, MRRE) fail to pinpoint specific distortion types; all measures decrease regardless of the type of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f2c2/f2c24306-84f4-4bbf-a287-cd742c298b9b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Time (s)</div>\nFig. 5. Results of the scalability analysis. Name and line colors match with Fig. 4. Label-T&C [DSC] (dark blue) is on par with CVMs (Silhouette, DSC), while Label-T&C [CHbtwn ] is similar to most of the other measures. S&C is the slowest.\ndistortion they aim to measure. Cluster-level measures (S&C) fail to distinguish False Groups distortions in experiments A-C. For experiments D-F, the situation is even worse; Steadiness reacts more sensitively to Missing Groups distortions although it was originally designed to aim at False Groups distortions. CA-T&C succeed in pinpointing False Groups distortions for B-1, but fails to do so for the remaining experiments. The general process of label-based DR evaluation based on CVMs (DSC and Silhouette) succeeds in detecting the False Groups distortions in experiments A-C. However, in experiments D-F, the process fails to detect Missing Groups distortions. Moreover, the process does not have a specific focus on distortion type and thus cannot explain whether the False or Missing Groups distortions occurred. Such results confirm the threat of using the general label-based evaluation of DR in practice, providing clear evidence for adopting Label-T&C instead. Effect of CVM choice on Label-T&C. Label-T&Cs with two different CVMs (DSC or CHbtwn) show a consistent pattern in experiments AC. However, they behave differently in experiments D and E; LabelC [CHbtwn] starts decreasing for the lower level of generated CLM distortions than Label-C [DSC]. This observation may be CVM-specific as DSC and CHbtwn use different schemes in examining how the classes are clustered. In Label-C [DSC], the score only drops when classes overlap. Therefore, Label-C [DSC] is sensitive to Missing Groups distortions only if the overlapped classes in the original space are more separated in the embedding. In contrast, CHbtwn decreases as the proximity between classes increases, whether the classes overlap or not. Thus, when proximity increases, Label-C [CHbtwn] is more sensitive to Missing Groups distortions than Label-C [DSC]. The results indicate that CHbtwn has a larger range of variation, being more sensitive to CLM than DSC, but it is less sensitive to class overlap. Creating a CVM both sensitive to CLM and class overlap while fulfilling our requirements (Sect. 4.2.1) constitutes an interesting future work. Discussions on the competitors. We discuss the patterns shown by competitors with more detail in Appendix B.\n# 5.1.4 Sensitivity Analysis with the Class Labels Generated by Clustering Techniques\nWe want to validate whether the results of our study are replicable with the labels that come from other sources. We thus conduct experiments A-F while generating class labels with clustering techniques (Appendix F). We find that Label-T&C show consistent results regardless of the sources of labels, while the general label-based DR evaluation process (i.e., CVMs) fails to do so. Such results confirm the robustness of the Label-T&C in evaluating the quality of DR embeddings.\n# 5.2 Scalability Analysis 5.2.1 Objectives and Design\nWe evaluate the scalability of Label-T&C against the competitors. We gather 96 labeled datasets [28] that vary in dimensionality, the number of data points, and the number of classes. We exclude two datasets as the implementation of S&C provided by the authors1 fails to process them, resulting in 94 datasets (Appendix C). We generate embeddings using t-SNE, UMAP, PCA, and random projection for all 94 datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/422a/422ac985-087b-4ff7-ae08-be32891f3e27.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. t-SNE embeddings of Fashion-MNIST [72] data with diverse perplexity (\u03c3) values. Combine dataset (Fig. 8), the patterns in the embeddings qualitatively support the findings about the effect of \u03c3</div>\n<div style=\"text-align: center;\">Fig. 6. t-SNE embeddings of Fashion-MNIST [72] data with diverse perplexity (\u03c3) values. Combined with the class-pairwise CLM of the original dataset (Fig. 8), the patterns in the embeddings qualitatively support the findings about the effect of \u03c3 revealed by Label-T&C (Fig. 7; Sect. 6.1).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/23ba/23ba9fd1-381a-4ab4-9f93-32d7615c1c75.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7. Overall reliability of t-SNE embeddings according to the \u03c3 value quantified by Label-T&C [DSC] and Label-T&C [CHbtwn]. For each \u03c3 value, we average the score of the embeddings generated from 94 labeled datasets (95% confidence interval shaded).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/423f/423f2eed-4e68-4a01-a1b4-1c17edbfec38.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8. Heatmaps detailing the CLM matrix of the Fashion-MNIST dataset (M(X) in Sect. 4.1). The color of each cell depicts the CVM (DSC, CHbtwn) score measured for each pair of classes corresponding to rows and columns.</div>\nWe check the overall execution time applying all measures to the embeddings, adding up the running times of the measures run in pairs (Label-T&C, T&C, MRRE, S&C, and CA-T&C). We use the provided implementation for S&C and scikit-learn [51] for the Silhouette. We implement the remaining measures in Python with Numba parallel computing [36] to maximize the scalability. We run the experiments on a Linux server with 40-core Intel Xeon Silver 4210 CPUs.\n# 5.2.2 Results and Discussion\nFig. 5 show that the running time of Label-T&C highly depends on the CVM. Among all measures, DSC is the fastest, followed by LabelT&C [DSC]. If CHbtwn is used as the CVM, Label-T&C becomes less scalable. Still, Label-T&C [CHbtwn] has scalability similar to local (T&C, MRRE) and global (KL Divergence, DTM) measures and to CA-T&C, all being more than twice faster than S&C.\nWe report two case studies demonstrating the usefulness of Label-T&C to characterize DR techniques and their hyperparameters.\n# 6.1 Examining the Effect of t-SNE Perplexity 6.1.1 Objectives and Design\nWe want to use Label-T&C to evaluate the reliability of the cluster structures from t-SNE embeddings (Sect. 5.1) depending on its perplexity hyperparameter \u03c3. \u03c3 adjusts the balance between local and global cluster structures [13,66]. We generate the t-SNE embeddings of the 94 labeled datasets used for the scalability analysis (Sect. 5.2) using different \u03c3 values (\u03c3 \u2208{2i | i = 0,\u00b7\u00b7\u00b7 ,10}) and evaluate them using Label-T&C [CHbtwn] and Label-T&C [DSC]. We also inspect the t-SNE embeddings of the Fashion-MNIST [72] dataset with various perplexity values (\u03c3 \u2208{4,16,64,256,1024}; Fig. 6) to gain more qualitative insights. Moreover, we compute the \u201cground-truth\u201d CLM matrix of the Fashion-MNIST dataset (Fig. 8), where the (i, j)-th cell represents the CVM score (CHbtwn or DSC) of the i-th and j-th classes. Note that this CLM matrix is identical to M(X) in Sect. 4.1.\n# 6.1.2 Results and Discussions\nIn the case of Label-T&C [CHbtwn], we found a clear tradeoff between Label-T and Label-C (Fig. 7A). When \u03c3 is low or high, Label-T [CHbtwn] gives low scores to t-SNE embeddings, indicating more False Groups distortions, while Label-C [CHbtwn] gives high scores, meaning fewer Missing Groups distortions. This means that t-SNE underrepresents the extent to which classes are clustered. In contrast, when \u03c3 has an intermediate value, Label-T&C [CHbtwn] indicate more Missing Groups and fewer False Groups distortions; hence, t-SNE exaggerates the degree to which classes are clustered. These results align well with the intent of \u03c3. With low \u03c3, t-SNE focuses more on a small number of neighbors, likely fewer than the clusters\u2019 sizes, interpreting each cluster as made of loosely-connected components in the data space. Thus, the embedding is more likely to split classes into several clusters in the embedding. This phenomenon occurs in the Fashion-MNIST embedding (Fig. 6); the Sneaker class is less dense if \u03c3 is low (region \u03b11) and relatively condensed when \u03c3 has intermediate values (\u03b12 and \u03b13). For the latter, the number of neighbors that t-SNE focuses on will likely match the size of natural clusters within the original data. Therefore, t-SNE embeddings will tend to dismiss the inter-cluster connections, exaggerating the betweencluster distances. The number of neighbors that t-SNE focuses on with high \u03c3 values will likely be bigger than the clusters\u2019 sizes. Thus, t-SNE will detect all data clusters as one densely-packed component and generate embeddings with smaller inter-cluster distances. The relation between the Trouser and Dress classes of the FashionMNIST embeddings (Fig. 6) qualitatively verifies these hypotheses. Their DSC scores are almost maximum (the black circle in Fig. 8), meaning they slightly overlap in the data space. However, their distance in the embedding is exaggerated with intermediate \u03c3 (\u03b21 and \u03b22) compared to high \u03c3 (\u03b23 and \u03b24). The same effect was observed qualitatively by Jeon et al. [29] while Label-T&C does so quantitatively. Meanwhile, Label-C [DSC] decreases slightly for intermediate values of \u03c3 (Fig. 7B dotted line). As Label-T&C [DSC] focuses more on class overlaps and less on between-class distances compared to Label-T&C [CHbtwn] (see D and E in Sect. 5.1), it indicates that t-SNE preserves well the extent to which classes overlap regardless of \u03c3. To\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a55e/a55e0983-5468-4824-9081-e2e381dc0bb9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9. CLM distortion evaluation of a linear (PCA) and five nonlinear (t-SNE, UMAP, Isomap, LLE, and Densmap) unsupervised DR techniques. A-D) Evaluation results with Label-T&C [CHbtwn/DSC] where class labels are obtained from the hierarchical clustering of the original data at multiple granularity levels (x-axis). Label-T&C evaluates more coarse-grained (global) clusterings for higher levels. See details in Sect. 6.2. (E-F) Evaluation esults of the techniques with T&C (E) and KL Divergence (F). Note that for all figures, higher scores indicate better embeddings.</div>\nquantitatively validate these findings, we searched for the overlapped classes within the CLM matrices, assuming that t-SNE accurately depicts class overlap for all \u03c3 values. We observed that the Pullover, Coat, and Shirt classes overlap in the high-dimensional space (red circles in Fig. 8; both their DSC and CHbtwn class-pairwise scores are low). We found that these classes overlap in all embeddings in Fig. 6 (\u03b31 to \u03b35), confirming our assumption. In summary, we can conclude that for non-overlapping classes in t-SNE embeddings, the amount of proximity between them depends essentially on \u03c3 and is not indicative of the proximity of these classes in the data space: t-SNE is not trustworthy regarding the original distance between visually separated classes. However, classes with strong overlaps in the data are depicted as overlapping in the embedding too: t-SNE is more trustworthy for overlapping classes. Such results align with the qualitative findings of Wattenberg et al. [66]. Overall, these findings demonstrate the effectiveness of Label-T&C to enhance our understanding of the effect of \u03c3 on t-SNE results. We conduct the same analysis utilizing the competitor measures we used in our evaluation (Sect. 5.1); refer to Appendix E for the results.\n# 6.2 Analyzing DR Techniques\u2019 Performance in Detail\n6.2.1 Objectives and Design\nWe use Label-T&C to analyze the quality of unsupervised DR techniques across fine-grained to coarse-grained cluster structures. We embed each of the previous 94 datasets using six DR techniques: tSNE, PCA, UMAP, Isomap, LLE, and Densmap [48]. We also apply hierarchical clustering, getting 20 clustering partitions with different granularity levels for each of these datasets. The levels of granularity are obtained by thresholding the pairwise distances computed by Ward linkage [33] into 20 equal ranges. We use Label-T&C [CHbtwn] and Label-T&C [DSC] to evaluate the embeddings using each of the 20 clusterings as class labels. We also want to check whether the results obtained by Label-T&C align with the ones made by previous measures. We thus evaluate the embeddings using T&C and KL divergence as representative local and global measures, respectively. We use the same hyperparameter setting with the sensitivity analysis (Sect. 5.1).\n# 6.2.2 Results and Discussions\nFig. 9 depicts the results. LLE generates few Missing Groups distortions (highest Label-C score; Fig. 9B, D) at any level, but more False Groups distortions as the granularity level increases (Label-T decreases; Fig. 9A, C). This finding aligns with the fact that LLE obtains the worst KL divergence score among all techniques (Fig. 9F). Such results are coherent with how LLE works, trying to reconstruct the \u201clocal patches\u201d consisting of each point and its nearest neighbors while neglecting the overlap between the patches. There is a Label-C downward trend across all other techniques as the level increases, while Label-C [DSC] shows higher scores than Label-C [CHbtwn] (Fig. 9B, D). This implies that Missing Groups distortions generally occur more for coarse-grained structures than for fine-grained ones; DR techniques exaggerate the separation between clusters at a global level. t-SNE and UMAP especially give the worst Label-C\nscores because they focus on the preservation of local neighborhoods, casting doubts on their reliability in identifying global clusters. T&C and KL divergence score provide strong evidence to the reliability of that claim. t-SNE and UMAP are in the top-2 highest ranks for T&C but fail to do so for KL divergence. For Label-T&C except Label-C [CHbtwn], PCA gets the best score at higher granularity, suggesting that PCA is more reliable to conduct global tasks such as the density and similarity identification of clusters. These results align with the fact that PCA earns the best score for KL divergence. The phenomenon confirms the experimental observation made by Xia et al. [69]. This is also coherent with the fact that PCA embeds the data along the top two principal axes that preserve most of their variance, better representing coarse-grained structures than fine-grained ones. We also find that Densmap, which is a variant of UMAP better preserving cluster density [48], gets worse Label-T [CHbtwn] scores than UMAP (Fig. 9A) but better Label-C [CHbtwn] scores (Fig. 9B), at all levels. This means that Densmap generates fewer Missing Groups but more False Groups distortions than UMAP. As Densmap approximately maintains the cluster locations of UMAP [48], such difference indicates that the clusters generally become bigger in Densmap compared to UMAP, hence the cluster density is relatively lower. Meanwhile, Densmap gets better Label-T&C [DSC] scores than UMAP for high granularity levels, confirming Densmap\u2019s advantage in investigating the overlap of clusters. The result is consistent with the KL divergence scores, indicating Densmap\u2019s advantage in preserving global structures when compared to UMAP (Fig. 9F). These findings confirm the ability of Label-T&C to reveal the characteristics of DR methods over a wide range of clustering granularities. Although typical evaluation approaches of DR quality using both local and global measures (Fig. 9E, F) [19,30,46] show consistent results, they cannot reveal how the quality changes across granularity levels, as different measures are incomparable.\n# 7 CONCLUSIONS\nThe general process of label-based DR evaluation relies on the assumption that the original data has good CLM, which can lead to erroneous conclusions when this assumption is violated. We introduce two new distortion measures\u2014Label-Trustworthiness and Label-Continuity (Label-T&C)\u2014that use class labels for DR evaluation while eliminating the need to check the validity of the CLM assumption. Our quantitative experiments show that Label-T&C outperforms previous DR measures in terms of precision and sensitivity in detecting Missing and False Groups distortions. Use cases show that Label-T&C can be used to characterize DR techniques and their hyperparameters. As future work, we will study new CVM to make Label-T&C more sensitive to the CLM distortions than using DSC or CHbtwn. Enriching the embedding with CLM distortions [40] could also better inform analysts about the credibility of visual patterns. Yet another direction would be to evaluate supervised DR techniques with Label-T&C. We also believe that supervised DR techniques using class labels in their optimization process could benefit from incorporating Label-T&C in their loss function. Overall, our proposal aims toward getting more trustworthy DR-based visual analysis.\nThis work was supported by NAVER Corporation (Cloud Data Box) and by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C200520911). The authors thank Seokhyeon Park for his valuable feedback in improving the figures. The authors also appreciate Sungbok Shin and SNU HCIL members for their feedback in improving the language.\n# REFERENCES\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Dimensionality reduction (DR) is a common technique used for visual cluster analysis of high-dimensional data. However, the process often relies on the assumption that the cluster structure of the original data is accurately represented in low-dimensional embeddings, which can lead to unreliable evaluations when distortions occur.",
            "purpose of benchmark": "The benchmark aims to evaluate the reliability of DR embeddings by introducing two new quality measures, Label-Trustworthiness and Label-Continuity (Label-T&C), which assess how well class labels correspond to clusters in both the original and embedded spaces."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of evaluating how well the original cluster structure is preserved in DR embeddings, particularly when class labels may not accurately represent the underlying data distribution.",
            "key obstacle": "Existing benchmarks often assume good cluster-label matching (CLM) in the original data, which can lead to erroneous conclusions when this assumption is violated. This creates a need for measures that can account for distortions in cluster structures."
        },
        "idea": {
            "intuition": "The development of Label-T&C was inspired by the observation that traditional measures fail to capture the complexities of cluster structures in high-dimensional data, particularly when classes overlap or are fragmented.",
            "opinion": "The authors believe that Label-T&C provides a more reliable framework for evaluating DR techniques, as it does not rely on the assumption of good CLM in the original data.",
            "innovation": "Label-T&C differ from previous measures by quantifying CLM distortions as the difference between CLM in the original and embedded spaces, thus capturing both Missing and False Groups distortions.",
            "benchmark abbreviation": "Label-T&C"
        },
        "dataset": {
            "source": "The dataset used for evaluation consists of various labeled high-dimensional datasets, which were embedded using different DR techniques.",
            "desc": "The dataset includes multiple classes and is designed to test the effectiveness of the proposed measures across different clustering scenarios.",
            "content": "The dataset contains numerical data representing high-dimensional points, with class labels used for evaluation.",
            "size": "94",
            "domain": "Dimensionality Reduction",
            "task format": "Clustering Validation"
        },
        "metrics": {
            "metric name": "Label-Trustworthiness, Label-Continuity",
            "aspect": "Model performance is measured in terms of the accuracy of preserving cluster structures and the reliability of DR embeddings.",
            "principle": "The metrics were chosen based on their ability to capture distortions in cluster structures, with a focus on both local and global relationships between classes.",
            "procedure": "The evaluation involves applying clustering validation measures (CVMs) to both original and embedded data to quantify CLM and assess distortions."
        },
        "experiments": {
            "model": "The benchmark evaluates various dimensionality reduction models, including t-SNE, UMAP, and PCA.",
            "procedure": "Models were trained using the provided datasets, and embeddings were generated for evaluation using the proposed metrics.",
            "result": "The experiments demonstrated that Label-T&C outperformed existing measures by accurately detecting distortions in cluster structures.",
            "variability": "The variability in results was accounted for through multiple trials and the evaluation of different datasets and embedding techniques."
        },
        "conclusion": "The study concludes that Label-T&C provides a more reliable framework for evaluating the quality of DR embeddings, effectively capturing distortions that traditional measures fail to detect.",
        "discussion": {
            "advantage": "Label-T&C offers a significant improvement in the evaluation of DR techniques by addressing the limitations of existing benchmarks and providing more accurate assessments of cluster structures.",
            "limitation": "The benchmark may still be sensitive to the choice of clustering validation measures, and further work is needed to enhance its robustness.",
            "future work": "Future research could focus on developing new clustering validation measures to improve sensitivity to CLM distortions and exploring the application of Label-T&C in supervised DR techniques."
        },
        "other info": {
            "funding": "This work was supported by NAVER Corporation (Cloud Data Box) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C200520911).",
            "additional notes": "The authors acknowledge contributions from various individuals and organizations that aided in improving the study."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "Dimensionality reduction (DR) is a common technique used for visual cluster analysis of high-dimensional data."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark aims to evaluate the reliability of DR embeddings by introducing two new quality measures, Label-Trustworthiness and Label-Continuity (Label-T&C)."
        },
        {
            "section number": "4.1",
            "key information": "The benchmark addresses the challenge of evaluating how well the original cluster structure is preserved in DR embeddings, particularly when class labels may not accurately represent the underlying data distribution."
        },
        {
            "section number": "4.2",
            "key information": "Existing benchmarks often assume good cluster-label matching (CLM) in the original data, which can lead to erroneous conclusions when this assumption is violated."
        },
        {
            "section number": "7.1",
            "key information": "Label-T&C provides a more reliable framework for evaluating DR techniques, as it does not rely on the assumption of good CLM in the original data."
        },
        {
            "section number": "7.4",
            "key information": "Future research could focus on developing new clustering validation measures to improve sensitivity to CLM distortions and exploring the application of Label-T&C in supervised DR techniques."
        }
    ],
    "similarity_score": 0.6681803397292531,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Classes are not Clusters_ Improving Label-based Evaluation of Dimensionality Reduction.json"
}