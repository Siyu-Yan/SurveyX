{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2304.11384",
    "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
    "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers\u2019 program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers\u2019 diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers\u2019 intents. We",
    "bib_name": "geng2023largelanguagemodelsfewshot",
    "md_text": "# Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning\nXiaoguang Mao xgmao@nudt.edu.cn College of Computer Science, National University of Defense Technology Changsha, China Xiangke Liao xkliao@nudt.edu.cn College of Computer Science, National University of Defense Technology Changsha, China\nABSTRACT\n# ABSTRACT\nCode comment generation aims at generating natural language descriptions for a code snippet to facilitate developers\u2019 program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers\u2019 diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers\u2019 intents. We\narXiv:2304.\n\u2020 Shangwen Wang and Dezun Dong are the corresponding authors. Shangwen Wang and Xiaoguang Mao are with the Key Laboratory of Software Engineering for Complex Systems. This work is supported by the National Key Research and Development Program Project \u201cHeterogeneous Computing Fusion of Cross-Domain Resources\u201d No.2022YFB4501702.\nthus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM\u2019s performances, which shed light on future research directions for using LLMs to achieve comment generation.\n# CCS CONCEPTS\n# \u2022 Software and its engineering \u2192Software maintenance tools; Maintaining software; Software evolution.\nKEYWORDS\nCode Summarization, Large Language Model, In-Context Learning\nACM Reference Format: Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via InContext Learning . In Proceedings of 46th International Conference on Software Engineering (ICSE 2024). ACM, New York, NY, USA, 13 pages. https: //doi.org/10.1145/xxxxxxx.xxxxxxx\n# 1 INTRODUCTION\nCode comment generation (a.k.a. code summarization) targets the ambition of automatically generating a concise and fluent natural language description of source code. It is considered as a critical\nway to facilitate program comprehension since developers usually forget or have no time to write such comments, and thus holds the potential of boosting software development and maintenance activities. During the years, a number of studies have been devoted into advancing the state of the art in this domain [4, 28, 32]. For instance, information retrieval techniques, which focus on extracting some important tokens from the code, are used in the early stage [25, 58], followed by some recent works applying advanced deep learning techniques on this task, such as the neural machine translation (NMT) model [5, 28]. Despite the achieved tremendous progress in this domain, one critical problem that downgrades the practicality of existing code comment generation approaches is that they can only generate comments describing one aspect of a given code snippet (and thus a one-to-one mapping). In practice, however, developers often write comments with diverse intents to summarize the code from different perspectives (e.g., what is the main functionality of the code and how can we use it). For instance, Zhai et al. [75] manually checked comments from real-world projects and identified six categories of intents hidden in the comments (as shown in Table 1). Mu et al. [47] did the statistics of top-starred Java projects on GitHub and found that around 67% of the methods contain more than one intent in their comments. The above observations indicate that what developers really need is a one-to-many mapping (i.e., generating multiple comments that summarize the given code from different perspectives), which is referred to as the multi-intent comment generation task in this paper. To tackle the aforementioned task, Mu et al. [47] proposed an approach named DOME, where an attention mechanism is used to focus on different parts of code for different intents. However, DOME is based on supervised learning, which limits its effectiveness due to the amount of data available for training. To address the data shortage problem, we propose to borrow the weapon of large language models (LLMs) [8], which are pre-trained on a data corpus of a very large scale in the self-supervised manner and have captured a lot of domain knowledge during such a process. The application of LLMs to the multi-intent comment generation task is motivated by two factors. Firstly, LLMs designed for the code domain are typically pre-trained using code and its associated pairwise comments to establish semantic connections between programming language and natural language [19, 67]. For example, the commonly used pre-training task, masked language modeling [15, 19, 24], is specifically intended to align programming language and natural language representations. Secondly, existing research has shown that code comments from real-world projects, which form the training corpus for LLMs, often contain multiple intents [47]. As a result, during pre-training, LLMs are trained to understand code from various perspectives, potentially allowing them to capture different code semantics. Thus, by fully exploiting the capabilities of pre-trained LLMs, we can achieve good performances on the multi-intent comment generation task. Recently, in-context learning has been shown to be an effective way to exploit the domain knowledge hidden in the LLMs [8, 11, 48, 60], since the format of the inputs to the model can be consistent to that during the pre-training process. Inspired by these studies, we aim to investigate the feasibility of addressing\nthe multi-intent comment generation task with in-context learning. Generally, in-context learning requires to provide a prompt to the model which is composed of a natural language instruction describing the detailed information of the task, (optionally) a handful of examples demonstrating how the task could be well done, and a query that is required to be addressed. Therefore, a followup question is that, with in-context learning, how can we obtain better results from the LLMs (e.g., if it is possible by designing prompts that can guide the LLMs towards the desired output). To provide empirical evidence on the aforementioned questions, we investigate the following aspects in this study: (a) Can the LLMs support to accomplish the multi-intent comment generation task using the in-context learning paradigm? (b) Can we improve the performance of the LLMs by designing customized demonstration selection strategies? and (c) Can we improve the performance of the LLMs by designing customized strategies to post-process the obtained results? To that end, we perform extensive experiments on two largescale Java language datasets, which are Funcom [36] and TLC [30]. We use the OpenAI Codex model as the representative LLM because of its superior performances on several code intelligence tasks [48, 54]. Our study makes the following important findings:\nF1: When the LLM is not adequately prompted (i.e., the number of demonstration examples is less than 10), the potential of the LLMs may not be fully exploited and the effectiveness is sub-optimal compared with that of the state-of-the-art supervised learning approach, DOME; in contrast, when the number of demonstration examples reaches ten, the LLM is more adequately prompted and its performance exceeds that of the DOME approach. F2: Demonstration selection strategies can help LLMs better understand the on-going task and thus enhance their effectiveness to a large extent: when the number of examples is ten and the code snippets which are most similar to the target one are used as the demonstration examples, the BLEU values of Codex can be increased by 97% and 131% on the two datasets, respectively, compared with random selection. F3: The outputs of LLMs can be reranked based on simple heuristics to achieve further performance enhancement: compared with the experiment setting mentioned above, the BLEU values of Codex can be improved by 9.9% and 9.6%, respectively, on the two datasets if the comment of the corpus code which is similar to the target one can be used for guiding the output reranking.\nOur study demonstrates that LLMs can potentially be applied to multi-intent comment generation since it builds strong performance baselines on this task, which should be considered by tool designers in future evaluation. Further implications include that devising better demonstration selection strategies as well as reranking strategies are both promising research directions.\n# 2 BACKGROUND AND RELATED WORKS\n# 2 BACKGROUND AND RELATED WORKS 2.1 Comment Generation\nAutomatic code comment generation, which aims at summarizing code with concise natural language descriptions, is a critical task to\n<div style=\"text-align: center;\">Table 1: The intent taxonomy of code comments [12, 75].</div>\nCategory\nDefinition\nExample\nWhat\nDescribes the functionality of a method\n\u201cChecks if the tile units at the given coordinates\nare displayed on the screen\u201d\nWhy\nExplains the reason why a method is provided\nor the design rationale of the method\n\u201cPrepare to start making calls to the currently\nregistered callbacks\u201d\nHow-to-use\nDescribes the usage or the expected set-up of\nusing a method\n\u201cCode executed before the intercepted method\u201d\nHow-it-is-done\nDescribes the implementation details of a method\n\u201cEnds the current table, discards it and pops the\ntop of the stack to be the new current table\u201d\nProperty\nAsserts properties of a method including\npre-conditions or post-conditions of a method\n\u201cReturns true if the value is a string that matches\na regex\u201d\nOthers\nUnspecified or ambiguous comments\n\u201cI am done with the model, free the resources \u201d\nm comprehension. Many approaches have been pro-\ncode generation [8, 16, 56]. The reason for their s\nfacilitate program comprehension. Many approaches have been proposed to construct a set of manually-defined complex rules, based on which comments can be generated following specific templates [25, 27]. With the recent advancement of the deep learning, a hot line of researches has suggested applying deep neural networks (DNNs) to this task. By modeling code as the input and comment as the output, such neural comment generation (NCG) approaches automatically learn a function, which is usually a DNN model such as the neural machine translation model, that can produce the output given the input. Such a DNN model is learned using existing largescale code-comment pairwise data. CodeNN [32] is an early attempt in this direction that uses only code token sequences, followed by various approaches that utilize the AST structure [4, 28, 29], API knowledge [30], type information [9], global context [7, 26, 66], reinforcement learning [22, 62, 65], multi-task learning [72], dual learning [68, 73], pre-trained language models [19, 21, 67], and hybrid approaches [69, 77]. In addition, a number of works also focus on generating latest and informative comments based on outdated comments (a.k.a comment updating) [39, 40]. The aforementioned approaches, however, can only generate comments describing one aspect of a given code snippet, which limits their practicality since developers usually express multiple intents when commenting the code [12, 47, 75]. That is to say, merely generating comments describing a specific aspect of a code snippet (e.g., the functionality of the code) may not meet the developers\u2019 requirements about comprehensively summarizing the code (e.g., how to use the code). Specifically, according to the previous studies [12, 47, 75], developers usually have six categories of intents when commenting the code, i.e., what, why, how-to-use, how-it-is-done, property, and others. In Table 1, we list the detailed definition and example for each category. The fact that developers usually express multiple intents in the comments cast threats to the practicality of existing single-intent comment generation techniques. To address this challenge, Mu et al. [47] propose a developer-intent driven code comment generation approach DOME, which aims to produce a comment coherent with a given intent. It works by leveraging the attention mechanism guided by the given intent to focus on the most relevant information from the code. To our best knowledge, DOME is so far the only existing technique that can generate diverse comments given different categories of intents.\n# 2.2 Large Language Models\nLarge language models (LLMs) trained on massive corpora of unlabelled data have been shown to perform well on a wide range of tasks, including natural language generation, semantic parsing, and\ncode generation [8, 16, 56]. The reason for their strong power can be concluded as they do not need task-specific training data and can be pre-trained on tremendous in-the-wild data in a self-supervised manner (a.k.a. pre-training), so that sufficient domain knowledge can be captured. The pioneer of this direction, the GPT model [55], was firstly proposed in 2018. After that, a number of follow-up studies continuously enhance the state-of-the-art performances by adjusting the model architecture (e.g., BERT [16]) or increasing the total amount of parameters (e.g., GPT-3 [8]). Codex, released by OpenAI, is an LLM based on the GPT-3 architecture (i.e., contains a Transformer-based decoder) [2]. It powers GitHub Copilot, an AI pair programmer that generates the whole code function given a natural language description. Codex is trained on a massive code corpus containing code-comment pairwise examples from many programming languages including Python, JavaScript, C/C++, Go, Perl, PHP, Ruby, Swift, TypeScript, SQL and Shell. Similar to GPT-3, Codex adopts the auto-regressive manner during the pre-training, in which given a sequence of code/comment tokens, it is trained to predict the next token and the predicted token is recursively used as the input for the next prediction until the end of the sequence. In our study, we use Codex as the representative LLM since it is a popular LLM in the software engineering domain and has been widely studied in the literature [10, 14, 18, 34, 49, 52, 54, 78].\n# 2.3 In-Context Learning\nPreviously, to apply a pre-trained model on downstream tasks, users need to further train it on the labelled data of downstream tasks in a supervised manner (a.k.a. fine-tuning) [16, 43]. Compared with training a model from scratch, this paradigm can exploit the knowledge learned by the pre-trained model and thus achieve better performance [38, 44]. Such a paradigm, however, mainly has two limitations. First, the data used for pre-training and fine-tuning are in different formats, which makes the learned knowledge of the model cannot be fully leveraged during the fine-tuning process [63]. Second, the fine-tuning process can be extremely time-consuming and resource-intensive, especially when it comes to large language models which usually contain billions of parameters [8]. To address the aforementioned limitations, in-context learning is recently proposed and quickly becomes a research hotspot after that [8]. Such a paradigm denotes that a few training examples and/or task descriptions together with a developer query that needs to be answered are sent into a large language model to produce a response of the query, without any parameter update. Basically, in the in-context learning paradigm, a prompt needs to be provided\nfor a code intelligence task, e.g., code summarization. By employing prompts, large language models are shown to be effective in different tasks that the model is not explicitly trained on, without the need of task-specific data [63]. Generally, the rationale of the in-context learning is that since large language models have been trained on corpora of a very large scale, they must have absorbed much domain knowledge and are thus expected to generalize well to unseen tasks without fine-tuning [8]. Our study shares a similar motivation. Specifically, considering that (1) large language models, e.g., Codex, are trained on a large-scale corpus containing tremendous amount of codecomment pairwise data from real-world, and (2) the real-world comments usually contain different categories of developers\u2019 intents, we postulate that the large language models are capable of understanding the code from different perspectives and thus hold the potential to generate comments with diverse intents given a code snippet. By using the in-context learning, such potentials of LLMs can be exploited.\n# 3 STUDY DESIGN\n# 3.1 Research Questions\nThe goal of our study is to investigate the effectiveness of large language models on multi-intent comment generation using the in-context learning paradigm. To this end, we propose to answer the following research questions.\n\u2022 RQ1: What is the effectiveness of Codex on multi-intent comment generation using zero-shot, one-shot, and fewshot learning? As the very first RQ, we aim at investigating the feasibility of addressing the multi-intent comment generation problem with in-context learning. Specifically, we do not use any customized design and only select code demonstrations randomly. Our target is to investigate how effective is the vanilla in-context learning compared with the state-of-the-art DOME approach. The results can also reflect to what extent the number of demonstrations (i.e., zero-shot, one-shot, and few-shot) affect the effectiveness. \u2022 RQ2: Can the effectiveness be improved by retrieval-based demonstration selections? Some recent works have demonstrated that the quality of the demonstrations in the prompt can significantly impact the effectiveness of in-context learning [45, 48, 60]. Inspired by these studies, we propose to investigate whether customized demonstration selection approaches can help improve the model\u2019s performance. Specifically, to answer this question, we design two retrieval-based approaches that select code examples similar to the code specified in the developer query, and evaluate their effectiveness. \u2022 RQ3: Can the effectiveness be improved by reranking strategies? A large language model experiences a sampling process to obtain the outputs [11, 49, 61, 78]. That is to say, a developer can obtain different results from the model for the identical input. In this RQ, we further investigate the feasibility of boosting the model\u2019s performance in a post-processing manner: by first obtaining a number of results and then reranking them through a pre-defined heuristic. Answering such a question can provide guidance for applying the approach in practice: it can make us\nclear about to what extent we can obtain more qualified resul by sampling multiple outputs.\n# 3.2 The Prompt Template for Multi-Intent Comment Generation\nFormally, a prompt is defined as \ud835\udc43= {\ud835\udc65test + CD + NL}, where NL is a natural language template, CD = {(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56)}\ud835\udc5b \ud835\udc56=1 is a set of code demonstrations composed by input code sequence (\ud835\udc65\ud835\udc56) and desired output sequence (\ud835\udc66\ud835\udc56), and \ud835\udc65test is a developer query to be inferred. Specifically, if \ud835\udc56= 0 which means there is no code demonstration, the setting is known as zero-shot learning; if \ud835\udc56= 1 which means there is only one code demonstration, the setting is known as one-shot learning; and few-shot learning means there is a number of code demonstrations. Also, there is a constraint that size(P) \u2264context-window, which means the prompt should fit within the context window limit of the language model. 1 Figure 1 illustrates a prompt template for the multi-intent comment generation task. The input prompt contains two sections: the code demonstrations CD and the query \ud835\udc65test . The natural language instructions are denoted by the lines starting with the special token \u201c#\u201d. In the first line of the prompt, we first tell the model the specific programming language it is working on (e.g., Java) and then the desired intent of the comment, as highlighted in the red, is specified by following the definitions shown in Table 1. In concrete, for the \u201cwhat\u201d intent, we add the prompt \u201cDescribe the functionality of the method\u201d; for the \u201cwhy\u201d intent, we add the prompt \u201cExplain the reason why the method is provided or the design rationale of the method\u201d; for the \u201chow-to-use\u201d intent, we add the prompt \u201cDescribe the usage or the expected set-up of using the method\u201d; for the \u201chow-it-is-done\u201d intent, we add the prompt \u201cDescribe the implementation details of the method\u201d; for the \u201cproperty\u201d intent, we add the prompt \u201cAssert properties of the method including preconditions pr post-conditions of the method\u201d. In this example, the illustrated prompt aims at generating a comment that fulfills the \u201cwhat\u201d intent. The first line is then followed by a number of code demonstrations that can help the LLM to understand the expected behavior and each demonstration contains one code snippet and one corresponding comment within the desired intent category. Each code demonstration is separated with a delimiter \u201c###\u201d. Finally, the model is asked to output the desired comment of the query code, which is shown at the bottom of the figure.\nNote that the code demonstrations used in RQ1 are randomly selected from a corpus. While in RQ2, we aim at investigating whether customized demonstration selection can enhance the effectiveness. Therefore, we design two strategies to retrieve similar code demonstration examples from the corpus whose comments\u2019 intents belong to the desired category. The rationale is that a few demonstrations that are similar to the target one may help the model better understand the desired behaviour [45, 48, 60]. The whole process of such a paradigm is shown in Figure 2: given a code snippet and the required intent category, we select code examples that are similar to the target one and use the retrieved code together with their\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d62/9d62433d-622e-4063-a47d-59716baec7cb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Multi-intent code summarization prompt template.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/83e2/83e29a45-2d4a-4aee-8dc7-c442dbb88798.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Overview of our in-context learning-based code summarization.</div>\n<div style=\"text-align: center;\">comments to construct a prompt whose template is shown in Figure 1. The prompt is used to query the model and obtain the results. We next introduce the two retrieval strategies in detail.</div>\ncomments to construct a prompt whose template is shown in Figure 1. The prompt is used to query the model and obtain the results. We next introduce the two retrieval strategies in detail.\n Token-based: The most commonly-used strategy to identify similar code is focusing on the overlap with respect to the code tokens [23, 33, 76]. Inspired by these studies, our first retrieval strategy is also based on the token level information, i.e., to rank the code snippets from the code corpus based on their token similarities with the target code. In concrete, we first pre-process the target code snippet and the code snippets in the retrieved code corpus by removing the keywords defined in the programming language (i.e., Java in our study). The behind intuition is that such frequently-used tokens may bring side effects to the similarity calculation because a large number of code snippets would contain them, inspired by the recent study [17]. Then, we further split identifiers into sub-tokens to adequately leverage the\nsemantic information hidden in the identifier names [53]. Specifically, such a process is achieved by utilizing the camel cases and the underscore naming convention of Java language. Finally, we convert all the sub-tokens to lower case. As for the token-based similarity between a candidate code snippet and the target code (\ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b), we exploit the Jaccard Coefficient [50] for the calculation, which is defined as follows: \ud835\udc60token = | tokens target \u2229tokens candidate | | tokens target \u222atokens candidate | where \ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61denotes the sub-token list of the target code and \ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52denotes the sub-token list of the candidate code. The value of \ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5branges from 0 to 1. A larger value of \ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5bindicates a higher similarity between the target code and the candidate code from the retrieved set. \u2022 Semantic-based: Recent studies in the clone detection domain have also revealed that beyond the lexical level code token similarity, understanding the code semantics is also important for finding similar code [64, 74]. Therefore, our second strategy relies\non the code semantics to retrieve similar code snippets. Specifically, we exploit the pre-trained sentence transformer model [57], which has been demonstrated to be capable of accurately capturing the semantics of code snippets by a recent study [48], to encode the code snippets as vectors which contain the corresponding semantic information. 2 The cosine similarity is exploited to retrieve the similar candidate code snippets whose vectors are close to that of the target code snippet in the vector space.\n# 3.4 Reranking Strategy\nTo rerank the generated comments, our intuition is that similar code snippets usually share similar comments, which is a common sense in the literature [37, 69\u201371]. Therefore, our strategy is to rerank the generated comments based on their similarities to the comment of the code snippet in the retrieval corpus that is similar to the target code. Specifically, we use the comment of the code snippet that is the most similar to the target code as the reference and also calculate comment similarities from two perspectives, i.e., the tokenbased and the semantic-based. For the token-based strategy, we focus on the token level information, since tokens in the comments are usually natural language words that have clear semantics. For the semantic-based, we exploit again the pre-trained sentence transformer model [57], embed the whole comment into a semantic vector, and calculate the cosine similarities.\n# 3.5 Datasets\nIn this study, we use the multi-intent comment generation datasets released by the previous study [47] as our evaluation datasets. In concrete, we use two datasets of Java programming language, i.e., the Funcom [36] and TLC [30] datasets, both of which are the most widely-used datasets for the code comment generation task [3, 13, 20, 35, 77]. Funcom contains 2.1M code-comment pairs from 29K Java projects, which were collected by Lopes et al. [1] and further cleaned by LeClair et al. [36]. TLC contains 87,136 codecomment pairs collected from more than 9K Java projects created from 2015 to 2016 with at least 20 stars. The intent categories of each comment in these two datasets are labelled by Mu et al. [47]: they first invited five domain experts to manually label the intents of 8K code snippets and then fine-tuned the CodeBERT model [19] on the labelled data, which was served as a classifier. Results show that the fine-tuned model can achieve an F1-score of around 90%, which is a relatively high value. Finally, the authors applied the fine-tuned model to predict the intent category of each comment in the datasets and used the prediction results as the ground-truth labels. Since manual labelling of such large-scale datasets would be infeasible, we reuse their provided results in our study. Also, the training/validation/test partition of the datasets is fixed and the statistics of these two datasets are shown in Table 2. Note that in the table, we do not show the statistics of the validation sets of the two datasets. This is because our approach does not need to train a model. In contrast, we only retrieve code examples from the training sets (by following Mu et al. [47]) with or without customized\n<div style=\"text-align: center;\">Table 2: The statistics of our evaluation datasets.</div>\nTable 2: The statistics of our evaluation datasets.\nDataset\nFuncom\nTLC\nTrain\nTest\nTrain\nTest\nWhat\n685,992\n44,330\n28,991\n2,724\nWhy\n152,026\n8,402\n5,935\n381\nHow-to-use\n24,648\n1,233\n838\n48\nHow-it-is-done\n146,571\n6,466\n11,478\n687\nProperty\n166,459\n8,326\n5,016\n396\nTotal\n1,175,696\n68,757\n52,258\n4,236\nategies and evaluate the effectiveness on the test sets. Theref\nstrategies and evaluate the effectiveness on the test sets. Therefore, the validation sets are not used in this study. Following existing studies [12, 47], we also exclude comments from the others intent category in our evaluation because these comments are considered as unspecified or ambiguous.\n# 3.6 Evaluation Metrics\nTo evaluate the performance of the Codex model on code summarization, we exploit the common metrics including BLEU [51], ROUGE-L [42] and METEOR [6]. BLEU (Bilingual Evaluation Understudy) [51] is a commonly-used evaluation metric in the code comment generation studies [28, 32, 47, 62], which measures the similarity between one sentence to a set of reference sentences using constituent n-grams precision scores. ROUGE denotes the Recall-oriented Understudy for Gisting Evaluation [42]. It computes the count of several overlapping units such as n-grams, word pairs, and sequences. ROUGE has several different variants from which we consider the most popular one ROUGE-L [7, 41, 47], which is calculated based on the longest common subsequence (LCS). METEOR [6], which denotes the Metric for Evaluation of Translation with Explicit ORdering, is another widely used metric to evaluate the quality of generated code summaries [29, 47, 65]. METEOR evaluates the generated summary by aligning it to the reference summary and calculating the similarity scores based on the unigram matching.\n# 3.7 Experiment Settings\nIn our experiments, beyond the zero-shot and one-shot settings, we choose to use five and ten code demonstrations for the few-shot setting. We cannot use too many code demonstrations since the input length is restricted by the context window limit. Therefore, we decide to provide the model with ten examples at most. The baseline for comparison is DOME [47] since it is so far the only approach that can address the multi-intent comment generation task. For running our experiments, we use the latest Codex model code-davinci-002. 3 We set the temperature as the default value, 0.5, to get a well-defined answer from Codex. We run all the experiments on an Hygon C86 7385 32-core CPU 2.50GHz machine with 2TB RAM. The running OS platform is Ubuntu 18.04. It is important to note that both the results of RQ1 and RQ2 are subject to randomness. RQ2 is affected by the sampling process, while RQ1 is further influenced by the selection of demonstrations. To address this issue, we repeated each setting one hundred times and reported the average values in the paper. Therefore, the results of RQ1 and RQ2 can be regarded as the expected average effectiveness of Codex under specific settings. In contrast, RQ3 investigates\n<div style=\"text-align: center;\">Table 3: The results of Codex on multi-intent comment generation using zero-shot, one-shot, and few-shot learning (in %).</div>\nusing zero-shot, one-shot, and few-shot learning (in %).\nIntent\nMethod\nFuncom\nTLC\nBLEU\nROUGE-L\nMETEOR\nBLEU\nROUGE-L\nMETEOR\nWhat\nDOME\n33.3\n41.7\n20.5\n25.4\n39.6\n18.2\nCodex-0-shot\n19.3\n23.5\n10.8\n17.8\n16.4\n15.5\nCodex-1-shot\n23.8\n27.6\n21.5\n22.5\n20.6\n17.4\nCodex-5-shot\n27.3\n41.8\n24.9\n25.7\n37.4\n19.9\nCodex-10-shot\n34.5\n58.6\n26.8\n32.4\n45.6\n23.1\nWhy\nDOME\n33.0\n42.3\n20.5\n21.9\n35.3\n15.7\nCodex-0-shot\n21.7\n20.3\n11.4\n19.6\n17.8\n9.6\nCodex-1-shot\n22.9\n28.8\n12.9\n20.8\n23.2\n11.9\nCodex-5-shot\n27.5\n45.8\n16.9\n24.1\n40.6\n13.5\nCodex-10-shot\n34.8\n76.1\n22.6\n26.2\n64.6\n15.8\nHow-to-use\nDOME\n31.6\n39.3\n19.3\n17.1\n26.1\n12.3\nCodex-0-shot\n22.3\n11.1\n16.8\n21.2\n10.9\n12.2\nCodex-1-shot\n23.1\n18.9\n17.5\n21.8\n16.6\n14.4\nCodex-5-shot\n27.9\n48.6\n19.8\n24.4\n40.5\n15.7\nCodex-10-shot\n33.3\n84.6\n22.3\n26.9\n76.4\n17.3\nHow-it-is-done\nDOME\n26.9\n39.5\n17.6\n20.4\n36.6\n14.7\nCodex-0-shot\n18.9\n37.9\n9.8\n16.8\n32.1\n9.6\nCodex-1-shot\n21.0\n39.6\n13.5\n19.1\n36.4\n12.1\nCodex-5-shot\n24.8\n49.2\n16.2\n21.1\n52.7\n12.8\nCodex-10-shot\n28.4\n79.3\n19.5\n21.9\n66.7\n14.9\nProperty\nDOME\n34.1\n49.4\n24.3\n26.0\n45.7\n21.2\nCodex-0-shot\n23.7\n33.3\n13.2\n18.8\n28.8\n9.5\nCodex-1-shot\n24.7\n38.4\n15.8\n21.3\n33.6\n12.4\nCodex-5-shot\n29.7\n79.2\n25.2\n26.5\n78.4\n22.3\nCodex-10-shot\n36.2\n81.9\n29.4\n28.7\n80.3\n24.7\nAverage\nDOME\n31.8\n42.5\n20.5\n22.2\n36.7\n16.5\nCodex-0-shot\n21.2\n25.2\n12.4\n18.8\n21.2\n11.3\nCodex-1-shot\n23.1\n30.7\n16.2\n21.1\n26.1\n13.6\nCodex-5-shot\n27.4\n52.9\n20.6\n24.4\n49.9\n16.8\nCodex-10-shot\n33.4\n76.1\n24.1\n27.2\n66.7\n19.2\nwhether better results can be achieved by leveraging the diversity of\nwhether better results can be achieved by leveraging the diversity of sampling results. To accomplish this, we repeated the experiments one hundred times and applied our reranking strategy based on the obtained results. The results of this RQ can thus be considered as the optimal achievable effectiveness of Codex.\n# 4 STUDY RESULTS\n# 4.1 RQ1: the Effectiveness of Vanilla In-Context Learning\nTable 3 lists the results of DOME and Codex on the multi-intent comment generation task. For Codex, the results of using 0, 1, 5, and 10 demonstration examples are respectively illustrated. Generally, we observe that the effectiveness of in-context learning will be better with the number of code demonstrations increases. For instance, for the \u201cwhat\u201d intent, the BLEU value of Codex is 19.3% when no code demonstration is used while this values increases to 34.5% when using ten examples, on the Funcom dataset. This is within our expectation because more examples will provide more guidance for the model about the on-going task. When compared with the state-of-the-art DOME, we note that the effectiveness of zero-shot and one-shot learning is lower than that of DOME. For instance, the average BLEU values of zero-shot learning on the two datasets are 21.2% and 18.8%, respectively, while the corresponding values of DOME are 31.8% and 22.2%. This indicates that without enough code demonstrations, the potential of LLMs on the multi-intent comment generation task may not be fully leveraged.\nFinding-1. Zero-shot and one-shot learning may not fully exploit the potential of the LLMs and their effectiveness is sub-optimal compared with that of the DOME approach.\nWhen the number of code demonstrations comes up to five, we observe the effectiveness of Codex is competitive to DOME: the values with respect to the ROUGE-L and METEOR metrics are higher\nthan those of DOME while the BLEU values are sightly lower. A potential reason is that the BLEU metric excessively focuses on measuring n-gram overlapping. In concrete, it requires strict consistency (i.e., the n-grams must be identical), which is difficult for models that have not been fine-tuned to achieve perfect alignment with the references. In contrast, the ROUGE-L and METEOR metrics release this requirement by focusing on the longest common subsequence and considering other features such as the word order in addition to n-grams, respectively. Nonetheless, when the number of code demonstrations reaches ten, Codex outperforms DOME consistently with respect to all the three metrics and two datasets. Specifically, the average values of Codex with respect to the three metrics are 33.4%/76.1%/24.1% and 27.2%/66.7%/19.2% on the Funcom and TLC datasets, respectively. Such performances outperform the state-of-the-art DOME by 5.0%/79.1%/17.6% and 22.5%/81.8%/16.4%, respectively, on the two datasets. We also find that the performance of different approaches varies across the intent categories: generally, all the approaches have relatively low performances on the \u201chow-it-is-done\u201d category. Such a finding is consistent with the results from the existing study [12].\nFinding-2. When the LLM is adequately prompted, its performance will exceed that of the state-of-the-art supervised learning approach. For instance, when the number of demonstrations is ten, the average ROUGE-L values of Codex on the two datasets are 76.1%/66.7%, respectively, outperforming DOME by 79.1%/81.8%.\n# 4.2 RQ2: the Effectiveness of Demonstration Selection\n# 4.2 RQ2: the Effectiveness of Demonstration\nThe results of different retrieval-based demonstration selection strategies are shown in Table 4. The zero-shot setting is excluded from this table since it does not use any code demonstration. We observe that the demonstration selections based on both token and semantic similarities significantly improve the performances compared with the vanilla random selection. For instance, when the number of selected examples is ten, the BLEU values of Codex on the Funcom and TLC datasets are 33.4% and 27.2%, respectively; while such values increase to 64.5% (65.9%) and 60.7% (62.8%) when the examples are selected based on token (semantic) similarities, with the relative improvements being 93% (97%) and 123% (131%). We also note that such performance improvements are universal (i.e., can be observed on each dataset no matter how many code examples are used). Moreover, we note that if similar examples are provided, the performance of 1-shot learning is even better than that of the vanilla 10-shot learning (e.g., the BLEU values on the Funcom dataset are 39.2% and 33.4%, respectively). Such results indicate the importance of the demonstration quality in the incontext learning: the model\u2019s performance could be improved if the given prompt is similar to the on-going task. Case analysis. For qualitative analysis, we present one case to show how the similar code helps to rectify the generated comment of Codex, which is shown in Figure 3. Given the test code whose oracle comment is \u201cPlays previous video in playlist\u201d, Codex with random selection generates a semantically-irrelevant comment \u201cPlays the next song or video\u201d. This comment is inappropriate since the attributive \u201cnext\u201d is wrong (the oracle is \u201cprevious\u201d) and\n<div style=\"text-align: center;\">Table 4: The results of different retrieval-based demonstration selection strategies (in %).</div>\nTable 4: The results of different retrieval-based demonstration selection strategies (in %\nIntent\nMethod\nFuncom\nTLC\nBLEU\nROUGE-L\nMETEOR\nBLEU\nROUGE-L\nMETEOR\nWhat\nCodex-1-shot\n23.8\n27.6\n21.5\n22.5\n20.6\n17.4\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n39.5\n84.6\n35.0\n35.6\n79.9\n31.4\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n36.7\n74.5\n36.1\n33.9\n71.6\n32.8\nCodex-5-shot\n27.3\n41.8\n24.9\n25.7\n37.4\n19.9\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n41.0\n82.3\n41.3\n38.6\n76.8\n37.7\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n41.1\n82.9\n39.3\n39.1\n78.9\n38.3\nCodex-10-shot\n34.5\n58.6\n26.8\n32.4\n45.6\n23.1\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n50.5\n90.0\n48.4\n44.8\n82.6\n43.9\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n40.4\n84.1\n38.7\n40.2\n79.5\n38.2\nWhy\nCodex-1-shot\n22.9\n28.8\n12.9\n20.8\n23.2\n11.9\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n32.8\n72.8\n27.7\n30.7\n68.4\n25.5\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n33.2\n70.9\n28.0\n31.6\n66.8\n26.2\nCodex-5-shot\n24.2\n45.5\n14.7\n24.1\n40.6\n13.5\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n37.8\n85.0\n32.9\n34.5\n78.7\n29.8\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n37.7\n82.1\n32.5\n35.1\n79.3\n30.2\nCodex-10-shot\n34.8\n76.1\n22.6\n26.2\n64.6\n15.8\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n74.9\n90.0\n75.1\n72.1\n81.4\n68.9\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n75.0\n89.4\n74.7\n72.4\n81.9\n73.0\nHow-to-use\nCodex-1-shot\n23.1\n18.9\n17.5\n21.8\n16.6\n14.4\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n56.3\n88.3\n53.7\n52.2\n81.6\n42.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n52.4\n74.4\n47.1\n46.8\n71.5\n42.3\nCodex-5-shot\n24.2\n48.1\n18.9\n24.4\n40.5\n15.7\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n48.0\n86.4\n45.9\n43.6\n80.3\n37.2\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n68.7\n86.2\n63.6\n66.4\n84.5\n58.4\nCodex-10-shot\n33.3\n84.6\n22.3\n26.9\n76.4\n17.3\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n69.6\n91.2\n70.7\n66.4\n84.3\n68.2\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n76.3\n91.2\n77.4\n71.6\n85.4\n73.6\nHow-it-is-done\nCodex-1-shot\n21.0\n39.6\n13.5\n19.1\n36.4\n12.1\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n31.9\n72.9\n25.8\n28.6\n69.4\n24.7\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n30.5\n69.6\n27.6\n28.2\n68.7\n25.9\nCodex-5-shot\n22.5\n48.9\n13.7\n21.1\n52.7\n12.8\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n33.7\n85.7\n30.8\n29.7\n78.4\n26.8\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n32.9\n80.0\n27.5\n28.3\n73.9\n25.1\nCodex-10-shot\n28.4\n79.3\n19.5\n21.9\n66.7\n14.9\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n47.9\n84.6\n49.6\n45.2\n80.8\n47.7\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n51.6\n86.4\n50.8\n48.9\n82.9\n47.9\nProperty\nCodex-1-shot\n24.7\n38.4\n15.8\n21.3\n33.6\n12.4\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n35.7\n67.7\n33.1\n33.2\n64.9\n30.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n36.4\n80.0\n35.9\n34.9\n62.8\n32.4\nCodex-5-shot\n29.7\n79.2\n25.2\n26.5\n78.4\n22.3\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n45.1\n89.2\n43.2\n41.5\n85.4\n40.6\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n43.9\n82.7\n40.3\n39.6\n82.1\n38.1\nCodex-10-shot\n36.2\n81.9\n29.4\n28.7\n80.3\n24.7\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n79.6\n84.2\n75.7\n74.8\n83.9\n68.9\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n86.3\n95.8\n87.4\n81.0\n86.4\n80.8\nAverage\nCodex-1-shot\n23.1\n30.7\n16.2\n21.1\n26.1\n13.6\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n39.2\n77.3\n35.1\n36.1\n72.8\n31.0\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n37.8\n73.9\n35.0\n35.1\n68.3\n31.9\nCodex-5-shot\n27.4\n52.9\n20.6\n24.4\n49.9\n16.8\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n41.1\n85.7\n38.8\n37.6\n79.9\n34.4\nCodex-5-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n44.9\n82.8\n40.6\n41.7\n79.7\n38.0\nCodex-10-shot\n33.4\n76.1\n24.1\n27.2\n66.7\n19.2\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n64.5\n88.0\n63.9\n60.7\n82.6\n59.5\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n65.9\n89.4\n65.8\n62.8\n83.2\n62.7\nntial maintainer of the code. Fortunately,\nWhen it comes to the comparison b\nwill thus mislead the potential maintainer of the code. Fortunately, after using the semantic-based demonstration selection strategy, Codex generates a comment that is semantically-identical to the oracle, i.e., \u201cPlays the previous video in your playlist\u201d. The achieved BLEU score reaches 73.1%, which is a relatively high performance. By investigating the most semantically-similar code in the corpus (listed in the bottom of the figure), we find that one potential reason for the success of Codex is that the example code shows it the attributive could come from the method name. Specifically, the comment for the semantically-similar code is \u201cPlay the first item\u201d and \u201cfirst\u201d is a token from the method name. With this example in mind, Codex generates the correct attributive \u201cprevious\u201d, which can also be extracted from the method name.\nWhen it comes to the comparison between the two selection strategies, we find that no strategy can consistently outperform the other under all the settings. For instance, when using one-shot learning, the performance of the token-based selection is better than that of the semantic-based selection on average; and vice versa when using few-shot learning (i.e., the number of examples are five or ten). Moreover, even if the semantic-based selection generally has a better performance when the number of examples is ten, it can also be outperformed by the token-based one under certain settings. For instance, on the what intent, the BLEU values of the token-based selection are 50.5% and 44.8%, respectively, on the two datasets, exceeding those of the semantic-based selection, which are 40.4% and 40.2%.\n<div style=\"text-align: center;\">Table 5: The results of different reranking strategies (in %).</div>\nTable 5: The results of different reranking strategies (in %).\nIntent\nMethod\nFuncom\nTLC\nBLEU\nROUGE-L\nMETEOR\nBLEU\nROUGE-L\nMETEOR\nwhat\nCodex-1-shot\n23.8\n27.6\n21.5\n22.5\n20.6\n17.4\nCodex-1-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n32.2\n76.1\n33.3\n28.9\n72.7\n29.3\nCodex-1-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n29.7\n76.5\n26.7\n27.1\n71.9\n24.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n39.5\n84.6\n35.0\n35.6\n79.9\n31.4\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n44.4\n84.9\n43.4\n41.8\n77.6\n38.5\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n45.8\n85.2\n44.9\n42.6\n75.8\n40.8\nCodex-10-shot\n34.5\n58.6\n26.8\n32.4\n45.6\n23.1\nCodex-10-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n36.9\n84.5\n29.3\n34.8\n76.9\n26.6\nCodex-10-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n39.7\n85.6\n36.5\n37.1\n81.0\n31.8\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n40.4\n84.1\n38.7\n40.2\n79.5\n38.2\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n58.6\n87.2\n61.3\n56.3\n82.9\n58.4\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n60.2\n89.4\n64.1\n58.3\n85.2\n60.9\nwhy\nCodex002-1-shot\n22.9\n28.8\n12.9\n20.8\n23.2\n11.9\nCodex-1-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n23.5\n67.6\n17.7\n22.6\n62.7\n19.4\nCodex-1-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n29.2\n68.0\n25.7\n26.7\n63.3\n20.1\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n32.8\n72.8\n27.7\n30.7\n68.4\n25.5\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n36.4\n81.0\n31.6\n34.4\n77.1\n28.9\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n38.6\n83.4\n35.9\n36.9\n80.2\n30.3\nCodex-10-shot\n34.8\n76.1\n22.6\n26.2\n64.6\n15.8\nCodex-10-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n36.8\n91.0\n24.8\n31.2\n86.1\n20.9\nCodex-10-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n35.3\n90.9\n23.2\n30.4\n85.2\n20.1\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n75.0\n89.4\n74.7\n72.4\n81.9\n73.0\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n78.3\n92.4\n76.6\n74.8\n88.7\n74.1\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n76.2\n90.6\n75.3\n73.5\n86.2\n73.6\nHow-to-use\nCodex-1-shot\n23.1\n18.9\n17.5\n21.8\n16.6\n14.4\nCodex-1-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n25.1\n62.0\n19.7\n24.2\n58.8\n17.6\nCodex-1-shot (\ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n28.5\n63.6\n22.9\n26.1\n61.3\n18.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n56.3\n88.3\n53.7\n52.2\n81.6\n42.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n63.8\n90.7\n66.3\n60.6\n85.3\n59.7\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n61.1\n85.7\n60.6\n58.4\n83.6\n57.2\nCodex-10-shot\n33.3\n84.6\n22.3\n26.9\n76.4\n17.3\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n32.7\n86.6\n27.0\n30.9\n82.4\n23.2\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n35.2\n85.6\n24.2\n32.8\n81.5\n21.6\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n76.3\n91.2\n77.4\n71.6\n85.4\n73.6\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n78.8\n93.5\n74.2\n71.9\n85.1\n73.9\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n79.1\n93.9\n75.2\n72.3\n85.7\n74.5\nHow-it-is-done\nCodex-1-shot\n21.0\n39.6\n13.5\n19.1\n36.4\n12.1\nCodex-1-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n29.8\n79.3\n22.2\n27.5\n74.8\n20.9\nCodex-1-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n29.4\n77.3\n21.7\n26.8\n73.1\n19.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n31.9\n72.9\n25.8\n28.6\n69.4\n24.7\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n33.8\n79.1\n28.9\n32.2\n77.4\n26.3\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n33.0\n77.6\n27.1\n31.4\n75.2\n25.8\nCodex-10-shot\n28.4\n79.3\n19.5\n21.9\n66.7\n14.9\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n30.6\n95.3\n24.7\n28.1\n90.8\n23.2\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n30.0\n95.2\n22.3\n27.6\n90.1\n20.1\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n51.6\n86.4\n50.8\n48.9\n82.9\n47.9\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n59.2\n89.3\n57.4\n56.1\n85.6\n53.5\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n57.6\n88.1\n55.2\n55.3\n83.1\n53.2\nProperty\nCodex-1-shot\n24.7\n38.4\n15.8\n21.3\n33.6\n12.4\nCodex-1-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n34.8\n59.8\n33.2\n30.6\n51.2\n28.7\nCodex-1-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n34.2\n59.5\n32.1\n29.5\n49.8\n27.6\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n35.7\n67.7\n33.1\n33.2\n64.9\n30.8\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n41.6\n74.2\n39.2\n38.4\n69.6\n35.9\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n40.1\n72.1\n36.0\n37.7\n67.2\n34.3\nCodex-10-shot\n36.2\n81.9\n29.4\n28.7\n80.3\n24.7\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n38.4\n84.2\n31.2\n35.2\n82.7\n28.6\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n36.2\n78.4\n30.9\n34.1\n81.2\n27.4\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n86.3\n95.8\n87.4\n81.0\n86.4\n80.8\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n87.2\n96.4\n88.7\n84.9\n89.1\n83.2\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n86.8\n96.1\n87.9\n82.5\n88.2\n82.1\nAverage\nCodex-1-shot\n23.1\n30.7\n16.2\n21.1\n26.1\n13.6\nCodex-1-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n29.1\n68.9\n25.2\n26.8\n64.0\n23.2\nCodex-1-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n30.2\n69.0\n25.8\n27.2\n63.9\n22.2\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n39.2\n77.3\n35.1\n36.1\n72.8\n31.0\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n44.0\n82.0\n41.9\n41.5\n77.4\n37.9\nCodex-1-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n43.7\n80.8\n40.9\n41.4\n76.4\n37.7\nCodex-10-shot\n33.4\n76.1\n24.1\n27.2\n66.7\n19.2\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n35.1\n88.3\n27.4\n32.0\n83.8\n24.5\nCodex-10-shot (\ud835\udc5f\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n35.3\n87.1\n27.4\n32.4\n83.8\n24.2\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n65.9\n89.4\n65.8\n62.8\n83.2\n62.7\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n72.4\n91.8\n71.6\n68.8\n86.3\n68.6\nCodex-10-shot (\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50+ \ud835\udc45\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n72.0\n91.6\n71.5\n68.4\n85.7\n68.9\nEffectiveness of Reranking\nf different reranking strategies (in %).\nFuncom\nTLC\nBLEU\nROUGE-L\nMETEOR\nBLEU\nROUGE-L\nMETEOR\n23.8\n27.6\n21.5\n22.5\n20.6\n17.4\n32.2\n76.1\n33.3\n28.9\n72.7\n29.3\n29.7\n76.5\n26.7\n27.1\n71.9\n24.8\n39.5\n84.6\n35.0\n35.6\n79.9\n31.4\n\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n44.4\n84.9\n43.4\n41.8\n77.6\n38.5\n\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n45.8\n85.2\n44.9\n42.6\n75.8\n40.8\n34.5\n58.6\n26.8\n32.4\n45.6\n23.1\n36.9\n84.5\n29.3\n34.8\n76.9\n26.6\n39.7\n85.6\n36.5\n37.1\n81.0\n31.8\n40.4\n84.1\n38.7\n40.2\n79.5\n38.2\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n58.6\n87.2\n61.3\n56.3\n82.9\n58.4\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n60.2\n89.4\n64.1\n58.3\n85.2\n60.9\n22.9\n28.8\n12.9\n20.8\n23.2\n11.9\n23.5\n67.6\n17.7\n22.6\n62.7\n19.4\n29.2\n68.0\n25.7\n26.7\n63.3\n20.1\n32.8\n72.8\n27.7\n30.7\n68.4\n25.5\n\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n36.4\n81.0\n31.6\n34.4\n77.1\n28.9\n\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n38.6\n83.4\n35.9\n36.9\n80.2\n30.3\n34.8\n76.1\n22.6\n26.2\n64.6\n15.8\n36.8\n91.0\n24.8\n31.2\n86.1\n20.9\n35.3\n90.9\n23.2\n30.4\n85.2\n20.1\n75.0\n89.4\n74.7\n72.4\n81.9\n73.0\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n78.3\n92.4\n76.6\n74.8\n88.7\n74.1\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n76.2\n90.6\n75.3\n73.5\n86.2\n73.6\n23.1\n18.9\n17.5\n21.8\n16.6\n14.4\n25.1\n62.0\n19.7\n24.2\n58.8\n17.6\n28.5\n63.6\n22.9\n26.1\n61.3\n18.8\n56.3\n88.3\n53.7\n52.2\n81.6\n42.8\n\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n63.8\n90.7\n66.3\n60.6\n85.3\n59.7\n\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n61.1\n85.7\n60.6\n58.4\n83.6\n57.2\n33.3\n84.6\n22.3\n26.9\n76.4\n17.3\n32.7\n86.6\n27.0\n30.9\n82.4\n23.2\n35.2\n85.6\n24.2\n32.8\n81.5\n21.6\n76.3\n91.2\n77.4\n71.6\n85.4\n73.6\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n78.8\n93.5\n74.2\n71.9\n85.1\n73.9\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n79.1\n93.9\n75.2\n72.3\n85.7\n74.5\n21.0\n39.6\n13.5\n19.1\n36.4\n12.1\n29.8\n79.3\n22.2\n27.5\n74.8\n20.9\n29.4\n77.3\n21.7\n26.8\n73.1\n19.8\n31.9\n72.9\n25.8\n28.6\n69.4\n24.7\n\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n33.8\n79.1\n28.9\n32.2\n77.4\n26.3\n\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50)\n33.0\n77.6\n27.1\n31.4\n75.2\n25.8\n28.4\n79.3\n19.5\n21.9\n66.7\n14.9\n30.6\n95.3\n24.7\n28.1\n90.8\n23.2\n30.0\n95.2\n22.3\n27.6\n90.1\n20.1\n51.6\n86.4\n50.8\n48.9\n82.9\n47.9\n\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b)\n59.2\n89.3\n57.4\n56.1\n85.6\n53.5\n\ud835\udc4e\ud835\udc5b\ufffd",
    "paper_type": "method",
    "attri": {
        "background": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers\u2019 program comprehension activities. Existing approaches can only generate one comment per code snippet, whereas developers need multiple perspectives on the code's functionality and usage. This study investigates the feasibility of using large language models (LLMs) to generate comments fulfilling diverse developer intents.",
        "problem": {
            "definition": "The problem addressed is the limitation of existing code comment generation methods that only produce single-intent comments, failing to meet the multi-intent needs of developers.",
            "key obstacle": "The main challenge is the reliance on supervised learning methods, like DOME, which restricts effectiveness due to insufficient training data."
        },
        "idea": {
            "intuition": "The intuition is that LLMs can understand code from different perspectives because they are pre-trained on code-comment pairs that capture various intents.",
            "opinion": "The proposed idea is to utilize LLMs in an in-context learning paradigm to generate multi-intent comments for code snippets.",
            "innovation": "The key innovation lies in leveraging in-context learning with LLMs, allowing them to produce multiple relevant comments based on diverse intents, which contrasts with existing single-intent generation methods."
        },
        "method": {
            "method name": "Multi-Intent Comment Generation using In-Context Learning",
            "method abbreviation": "MICG-ICL",
            "method definition": "This method involves prompting an LLM with code snippets and examples of desired outputs to generate multiple comments that address various developer intents.",
            "method description": "The core of the method is to provide a prompt to the LLM that includes code examples and a specific developer query to produce relevant comments.",
            "method steps": [
                "Select a code snippet for which comments are to be generated.",
                "Provide a prompt to the LLM that includes examples of similar code with corresponding comments.",
                "Specify the intent category for the comment (e.g., functionality, usage).",
                "Obtain the generated comments from the LLM."
            ],
            "principle": "This method is effective because it utilizes the extensive pre-training of LLMs, allowing them to generate contextually relevant comments based on a rich understanding of code semantics."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on two large-scale Java datasets, Funcom and TLC, which contain millions of code-comment pairs categorized by developer intents.",
            "evaluation method": "Performance was assessed using metrics such as BLEU, ROUGE-L, and METEOR, comparing the outputs of the LLM under different prompting conditions against the state-of-the-art DOME method."
        },
        "conclusion": "The study concludes that LLMs can effectively generate multi-intent comments, outperforming existing methods when adequately prompted. The findings highlight the potential of in-context learning strategies to enhance comment generation in software development.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to generate multiple relevant comments from a single code snippet, addressing diverse developer intents.",
            "limitation": "A limitation is that the effectiveness of the method heavily relies on the quality and relevance of the examples provided in the prompts.",
            "future work": "Future research should focus on improving demonstration selection strategies and exploring more sophisticated reranking methods to enhance the quality of generated comments."
        },
        "other info": {
            "info1": "The study utilized the OpenAI Codex model as the representative LLM due to its superior performance on code-related tasks.",
            "info2": {
                "info2.1": "The datasets used in the experiments were Funcom and TLC, which are widely recognized in the field of code comment generation.",
                "info2.2": "The experiments included zero-shot, one-shot, and few-shot learning settings to evaluate the effectiveness of the LLM under different prompting conditions."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning is utilized to generate multi-intent comments for code snippets, allowing large language models to understand code from different perspectives."
        },
        {
            "section number": "1.3",
            "key information": "The study investigates the feasibility of using large language models (LLMs) to generate comments fulfilling diverse developer intents."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Multi-Intent Comment Generation using In-Context Learning (MICG-ICL), allows LLMs to adapt to various developer intents by generating multiple relevant comments."
        },
        {
            "section number": "3.4",
            "key information": "The method involves prompting an LLM with code snippets and examples to produce comments, utilizing context to enhance in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts significantly influences the outcomes of in-context learning, as demonstrated by the method's reliance on providing relevant examples to generate diverse comments."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the proposed method is that its effectiveness heavily relies on the quality and relevance of the examples provided in the prompts."
        },
        {
            "section number": "6.4",
            "key information": "Future research should focus on improving demonstration selection strategies to enhance the scalability and applicability of the in-context learning approach for comment generation."
        }
    ],
    "similarity_score": 0.6925616278786377,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Large Language Models are Few-Shot Summarizers_ Multi-Intent Comment Generation via In-Context Learning.json"
}