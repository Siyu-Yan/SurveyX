{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.18262",
    "title": "Beyond Confidence: Reliable Models Should Also Consider Atypicality",
    "abstract": "While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction\u2019s reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical (rare) a sample or a class is and the reliability of a model\u2019s predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups without having access to the group attributes. Overall, we propose that models should use not only confidence but also atypicality to improve uncertainty quantification and performance. Our results demonstrate that simple post-hoc atypicality estimators can provide significant value.1",
    "bib_name": "yuksekgonul2023confidencereliablemodelsconsider",
    "md_text": "# Beyond Confidence: Reliable Models Should Also Consider Atypicality\n# Abstract\nWhile most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction\u2019s reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical (rare) a sample or a class is and the reliability of a model\u2019s predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups without having access to the group attributes. Overall, we propose that models should use not only confidence but also atypicality to improve uncertainty quantification and performance. Our results demonstrate that simple post-hoc atypicality estimators can provide significant value.1\narXiv:2305.18262v2\n# 1 Introduction\nTypicality is an item\u2019s resemblance to other category members [RM75]. For example, while a dove and a sparrow are typical birds, a penguin is an atypical bird. Many works from cognitive science (e.g., [Rip89, RSS73, MP80]) suggest that typicality plays a crucial role in category understanding. For instance, humans have been shown to learn, remember, and refer to typical items faster [Mur04]. Similarly, the representativeness heuristic is the tendency of humans to use the typicality of an event as a basis for decisions [TK74]. This cognitive bias is effective for making swift decisions, but it can lead to poor judgments of uncertainty. For instance, the likelihood of typical events can be overestimated [TK74] or uncertainty judgments can be inferior for atypical events [TK92]. While it is hard to quantify the uncertainty of human judgments, machine learning models provide confidence in their predictions. However, confidence alone can be insufficient to understand the reliability of a prediction. For instance, a low-confidence prediction could arise from an ambiguity that is easily communicated, or due to the sample being underrepresented in the training distribution.\n1Our code is available at https://github.com/mertyg/beyond-confidence-atypicality \u2021Joint Advisors.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e755/e755eb10-5609-4a5d-b88c-e814483ca43f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Atypicality in Uncertainty. Left: We show examples from the ImageNet-R dataset with our atypicality framework. Right: We provide a conceptualization of the quadrants. Using atypicality, we can understand prediction quality (\u00a73), improve predictions (\u00a74), and prediction sets (\u00a75).</div>\nSimilarly, a high-confidence prediction could be reliable or miscalibrated. Our main proposal is that models should quantify not only the confidence but also the atypicality to understand the reliability of predictions or the coverage of the training distribution. However, many machine learning applications rely on pretrained models that solely provide confidence levels, devoid of any measure of atypicality. Contributions: To support our position, we use a simple formalization of atypicality estimation. With the following studies, we show that by using simple atypicality estimators, we can: 1. Understand Prediction Quality: Calibration is a measure that assesses the alignment between predicted probabilities of a model and the true likelihoods of outcomes [GR07]. Neural networks [GPSW17] or even logistic regression [BMWX21a] can be miscalibrated out-of-the-box. Here, we argue that using atypicality can give insights into when a model\u2019s confidence is reliable. Through theoretical analysis and extensive experimentation, we demonstrate that atypicality results in lower-quality predictions. Specifically, we show that predictions for atypical inputs and samples from atypical classes are more overconfident and have lower accuracy. 2. Improve Calibration and Accuracy: Recalibration methods offer some mitigation to miscalibration [GPSW17] by adjusting a probabilistic model. We show that models need different adjustments according to the atypicality of inputs and classes, and atypicality is a key factor in recalibration. In light of these findings, we propose a simple method: Atypicality-Aware Recalibration. Our recalibration algorithm takes into account the atypicality of the inputs and classes and is simple to implement. We show that complementing recalibration methods with atypicality improves uncertainty quantification and the accuracy of predictors. Further, in a case study for skin lesion classification, we show that atypicality awareness can improve performance across different skin-tone subgroups without access to group annotations. 3. Improve Prediction sets: An alternative approach to quantify uncertainty is to provide prediction sets that contain the label with high probability [ABMJ20]. Here, we investigate existing methods with atypicality and show that prediction sets could underperform for atypical or low-confidence samples. By using atypicality, we demonstrate the potential for improving prediction sets. Overall, we propose that models should also consider atypicality, and we show simple- and easy-to-implement atypicality estimators can provide significant value.\n# 2 Interpreting Uncertainty with Atypicality\nMotivation: In many machine learning applications, we have access to a model\u2019s confidence, which aims to quantify the likelihood that a prediction will be accurate. In classification, model output is a probability distribution over classes and confidence is the predicted probability of the top class, i.e. maxy \u02c6P(Y = y|X = x). In practical scenarios, confidence is the primary tool used to evaluate the reliability of a prediction where higher confidence is associated with better predictions. However, the uncertainty in confidence can stem from different sources that require different treatment [MKvA+21].\nHere, we call a prediction reliable if it is high-confidence and well-calibrated. High confidence could be reliable or miscalibrated, and low confidence could be due to ambiguity or rare inputs. We propose that atypicality provides a natural way to understand reliability when combined with confidence. A sample is called typical if it is well-represented in the previously observed samples, e.g., an image of a dog that is similar to other dogs in the training data. However, if the image is unlike any other seen during training, it is atypical. We argue that atypicality can help us interpret a prediction\u2019s reliability. Below we categorize samples and predictions according to atypicality and confidence in four quadrants (Figure 1). High-confidence and representative: Reliable predictions often fall within the Reliable Quadrant, which includes typical, high-confidence samples. These samples are well-represented in the training dataset (typical), thus we expect the high-confidence prediction to be reliable. For instance, the first image on the top left (Figure 1) is a typical golden retriever and the model makes a reliable prediction. High-confidence yet far from the support: Having high-confidence does not always indicate reliability. If the sample does not have support in the training distribution, the confidence could be miscalibrated. Such samples lie in the Extrapolation Quadrant which contains atypical, highconfidence samples. For instance, the second image in the top right of Figure 1 is a toy hog and the model has not seen similar ones during training. Low confidence due to ambiguity: In contrast, low confidence could also be reliable when it correctly reflects an ambiguity. Such samples are in the Ambiguous Quadrant that contains typical, low-confidence samples. These are typical since they may represent multiple classes; yet, due to ambiguity, the model\u2019s confidence is low. For instance, the second image in the bottom left of Figure 1 can both be a hog and a comic book. Low confidence and rare: For samples that are not well-represented in training data, we expect to have low-quality predictions. Untrustworthy Quadrant comprises atypical, low-confidence samples that can include extremely rare subgroups, for which we expect miscalibration and lower accuracy. For example, the image in Figure 1 bottom right is an origami hog that was not seen in training. These examples suggest that relying solely on confidence does not provide a complete understanding of the reliability of the predictions, and we can use atypicality to interpret and improve reliability. Formalizing Atypicality: Atypicality here is defined with respect to the training distribution. Informally, an input or a class is atypical if it is not well-represented in the training distribution. For instance, if there are no or limited similar examples to an input, it can be called atypical. Note that this notion is not restricted to being \u2018out-of-distribution\u2019 [HG16a], since in-distribution groups could also be atypical or rare, and our goal is to perform reliably for the entire spectrum. Formally, let X \u2208Rd be the random variable denoting features and Y \u2208Y = {1, 2, ..., C} denote the class, where we focus on classification. Definition 2.1 (Input Atypicality). We define the atypicality of the input x as2 aX(x) = \u2212max log P(X = x|Y = y).\nWe use the logarithm of the class-conditional densities due to high dimensionality and density values being close to zero. Intuitively, for a dog image x, if P(X = x|Y = dog) has a low value, we call x an atypical dog image. Overall, if a(x) is high, then we call x an atypical input. Specifically, if an input is not typical for any class, then it is atypical with respect to the training distribution. Similarly, we can also use marginal density, P(X = x), or distance3 to quantify atypicality. Similarly, the notion of atypical (rare) classes is prevalent in imbalanced classification [CWG+19, ZCLJ21]. Ensuring reliable performance for atypical classes can be safety-critical, e.g., for a rare presence of dangerous melanoma [DVN+22]. We define class atypicality in the following: Definition 2.2 (Class Atypicality). For a class y, atypicality of a class is defined as\n2Here atypicality differs from \u2019typical sets\u2019 in information theory that refers to a sequence of variables [TJ06]. 3For an input x, if the nearest neighbor (NN) distance is large, then we call x atypical as all inputs in the training set are far from x. Density and distance are connected through non-parametric density estimation and [JKGG18] shows that NN distance can recover high-density regions. 4When the meaning is unambiguous, we omit the subscript to denote a(X) or a(Y ) for notational brevity.\nEstimating Atypicality for Discriminative Models: Quantifying input atypicality requires access to the class-conditional / marginal distributions. In practice, for neural networks trained for classification, these distributions are unavailable and we need to perform the estimation. This estimation can be challenging if the dimensionality is large, or the data is unstructured, requiring assumptions about the distributions. Prior works [MKvA+21, LLLS18] showed that Gaussian Mixture Models (GMMs) in the embedding space of neural networks can be used to model these distributions. In experiments, we use Gaussians with shared covariance, i.e. \u02c6P(X = x|Y = c) \u223cN(\u02c6\u00b5c, \u02c6\u03a3), to estimate input atypicality. We perform the estimation in the penultimate layer of neural networks used to make predictions, using maximum-likelihood estimation with samples from the training data. We explore other metrics, such as k-Nearest Neighbors distance. We give implementation details and results with different metrics in Appendix A.1. With these estimators, atypicality estimation is cheap and can run on a CPU. Our goal is to show that simple estimators can already reap large benefits. Our framework is flexible and exploring more sophisticated estimators is a topic for future work. Atypicality for LLMs: LLMs are increasingly used for classification [BMR+20]. Modern LLMs are autoregressive models that compute a marginal distribution, \u02c6PLLM(X). We compute the negative loglikelihood of a prompt or a label and use this as an atypicality metric, i.e. aX(x) = \u2212log \u02c6PLLM(x), aY (y) = \u2212log \u02c6PLLM(y). Similar to the discriminative setting, atypicality here aims to quantify whether a prompt is well-represented in the training data. A larger value for a(X) would imply that a prompt is more atypical. Below, we present typical and atypical prompts for the AGNews dataset:\nClassify the news articles into the categories of World, Sports, Business, and Technology. Article: Safin tallest obstacle to host #39;s patriotic games hope AS tennis fans go, Houston #39;s Jim #39;Mattress Mack #39; McIngvale is very rich, extremely forthright, exceedingly patriotic and unflinchingly Republican. Answer: Atypicality:, Percentile:\nClassify the news articles into the categories of World, Sports, Business, and Technology. Article: Safin tallest obstacle to host #39;s patriotic games hope AS tennis fans go, Houston #39;s Jim #39;Mattress Mack #39; McIngvale is very rich, extremely forthright, exceedingly patriotic and unflinchingly Republican. Answer: Atypicality: 353.45, Percentile: %94.5 Classify the news articles into the categories of World, Sports, Business, and Technology. Article: Delta Air Lines Prepares Chapter 11 Filing Delta Air Lines Inc. could file for Chapter 11 bankruptcy protection as soon as next week, a source familiar with the matter said yesterday. Answer: Atypicality: 171.50. Percentile: %0.9\n# 3 Understanding the Prediction Quality with Atypicality\nIn this section, we show how our framework can be applied to understand the quality of predictions. Experimental Setup: We investigate three classification settings across a range of datasets:\nExperimental Setup: We investigate three classification settings across a range of datasets: 1. Balanced Supervised Classification: We use ResNet18-50-152 [HZRS16], WideResNet28 [ZK16], RoBERTa [LOG+19] trained on ImageNet [DDS+09], CIFAR10,100 [Kri09], MNLI [WNB18] respectively. 2. Imbalanced Supervised Classification: We use ResNet18, ResNext50, ResNet152 trained on CIFAR-LT, ImageNet-LT and Places365-LT where models and data are mostly from [ZCLJ21, MKS+20]. Note that all of the test and validation sets have balanced class distributions. 3. Classification with LLMs: We use open-source Alpaca7B [TGZ+23] on IMDB [MDP+11], TREC [LR02], and AG News [ZZL15] datasets with the prompts from [ZWF+21].\nDetails on datasets, models, and prompts are in Appendix B. Our experiments were run on a single NVIDIA A100-80GB GPU. We report error bars over 10 random calibration/test splits.\n# 3.1 Atypicality is Correlated with Miscalibration\nWe first explore the importance of atypicality to understand model calibration. Calibration quantifies the quality of a probabilistic model [GR07]. Informally, a model is considered perfectly calibrated if all events that are predicted to occur P% of the time occur P% of the time for any P \u2208[0, 100]. For the sake of simplicity, consider a binary classification problem where the predictor is \u02c6P : X \u2192 [0, 1]. We quantify miscalibration with Calibration Error (CE):\nWe first explore the importance of atypicality to understand model calibration. Calibration quantifies the quality of a probabilistic model [GR07]. Informally, a model is considered perfectly calibrated if all events that are predicted to occur P% of the time occur P% of the time for any P \u2208[0, 100].\nCE[\u02c6P] = E[|P(Y |\u02c6P(X) = p) \u2212p|].\n| | \u2212| It is computationally infeasible to calculate the above expectation with the conditional probability P(Y |\u02c6P(X) = p). In practice, we use a binned version of this quantity, Expected Calibration Error (ECE) [NCH15, GPSW17], to estimate CE. See Appendix C.1 for a formal definition.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4980/4980875d-1d37-455f-96f9-09878dcc4294.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\nFigure 2: Atypical Samples Have Low-Quality Predictions. (a) Here, samples are grouped according to the Input Atypicality (x-axis) and Confidence (y-axis), to the right meaning more atypical. Values show the difference between confidence and accuracy, lighter color indicates more overconfidence. Within the same confidence range, atypical groups have more miscalibration and are more overconfident. (b,c,d) Predictions for atypical samples are less accurate and more miscalibrated in balanced and imbalanced supervised classification and classification with LLMs.\nHere, we aim to examine the relationship between model calibration and atypicality. Given any K > 1, we consider the quantiles of a(X), a1, a2, . . . , aK+1 such that P(a(X) \u2208(ak, ak+1]) = 1/K for k \u2208[K]. For imbalanced classification problems, we compute the quantiles using the class atypicality. Specifically, we investigate the atypicality-conditional calibration error ECE[\u02c6P | a(X) \u2208(ak, ak+1]], i.e., the expected calibration error of an input that falls within the atypicality quantile k. Atypical Examples are Poorly Calibrated: In Figure 2a, we show the distribution of miscalibration where each bin within the grid contains the intersection of the corresponding confidence and atypicality quantiles. We observe that within the same confidence range, predictions for atypical points have lower accuracies and are more overconfident. In other words, predictions in the Extrapolation or Untrustworthy regions are more miscalibrated than the ones in the typical regions. In Figure 2b, we split inputs into quantiles according to atypicality and compute the ECE and Accuracy for each group. Results show a monotonic relationship between atypicality and ECE or Accuracy across the three settings. Specifically, we see that predictions for atypical inputs or samples from rare classes are more miscalibrated and have lower accuracy. For samples from rare classes, the model overpredicts the probabilities of the typical class, hence we have overconfidence and low accuracy. Appendix C.3, and \u00a74 present figures and tables for all model and dataset pairs.\nIn Figure 2b, we split inputs into quantiles according to atypicality and compute the ECE and Accuracy for each group. Results show a monotonic relationship between atypicality and ECE or Accuracy across the three settings. Specifically, we see that predictions for atypical inputs or samples from rare classes are more miscalibrated and have lower accuracy. For samples from rare classes, the model overpredicts the probabilities of the typical class, hence we have overconfidence and low accuracy. Appendix C.3, and \u00a74 present figures and tables for all model and dataset pairs.\n# 3.2 Theoretical Analysis: Characterizing Calibration Error with Atypicality\nWe characterize how calibration error varies with atypicality in a tractable model that is commonly used in machine learning theory [BMWX21a, BMWX21b, ZDKZ22, CLKZ22]. Our theoretical analysis further supports our empirical findings. Data Generative Model: We consider the well-specified logistic model for binary classification with Gaussian data, where Y \u2208{\u22121, 1} and the P(Y = 1|X) is defined by the sigmoid function:\nP(Y = 1 | X) = \u03c3(\u27e8\u03b2\u2217, X\u27e9), X \u223cN(0, Id).\nWhere Id denotes the d-dimensional identity matrix, \u03b2\u2217is the ground truth coefficient vector, \u03c3(x) = 1/(1 + e\u2212x), and we have i.i.d. observations {(xi, yi)}n i=1 sampled from the above distribution. The Estimator: We focus on studying the solution produced by minimizing the logistic loss\nFor k \u2208{\u22121, 1}, \u02c6Pk(x) is an estimator of P(y = k|x), with the form \u02c6Pk(x) = 1 e\u2212k\u00b7 \u02c6 \u03b2\u22a4x+1. Calibration: We consider all x where P1(x) > 1/2, as P1(x) \u22641/2 can be analyzed similarly by symmetry (see Appendix G). For u \u2208(1/2, 1), the signed calibration error at a confidence level u is u \u2212P(Y = 1 | \u02c6P1(X) = u).\n<div style=\"text-align: center;\">(d)</div>\n<div style=\"text-align: center;\">(c)</div>\nWe want to show that when X is atypical, i.e., when a(X) := \u2225X\u22252/2 is larger5, the accuracy P(Y = 1 | \u02c6P1(X) = u) would be generally smaller than the confidence u (over-confidence). Theorem 3.1. Consider the data generative model and the learning setting above. For any K > 1, suppose we consider the quantiles of a(X), a1, a2, ..., aK, aK+1 such that P(a(X) \u2208(ak, ak+1]) = 1/K for k \u2208[K]. We assume \u2225\u03b2\u2217\u2225\u2264c0, and d/n = \u03ba, for some sufficiently small c0. Then, for sufficiently large n, for k = 2, . . . , K, we have\nEu\u223c\u02c6P1(X)[u \u2212P(Y = 1 | \u02c6P1(X) = u) | a(X) \u2208(ak, ak+1]] > Eu\u223c\u02c6P1(X)[u \u2212P(Y = 1 | \u02c6P1(X) = u) | a(X) \u2208(ak\u22121, ak]] \u22650.\nThat is, the resulting classifier is over-confident, and the level of over-confidence becomes larger when the data is more atypical (with larger a(X)). Further, the gap becomes larger for smaller sample sizes n. The proof of the theorem is in Appendix G.2 and builds on the results from [BMWX21a, SC19].\n# 4 Using Atypicality to Improve Recalibration\nHere, we show how atypicality can complement and improve post-hoc calibration. In \u00a72, we observed that predictions for atypical inputs and samples from atypical classes are more overconfident with lower accuracy. We next show that taking input and class atypicality into account improves calibration.\n# 4.1 Parametric Recalibration: Different Groups Need Different Temperatures\nTemperature scaling (TS), a single parameter variant of Platt Scaling [P+99], is a simple recalibration method that calibrates the model using a single parameter. The predictor is of the form\nwhere \u02c6P(Y |X) is the model that takes an input and outputs scores/logits, and \u03c4 is the temperature parameter. In practice, \u03c4 is optimized using a calibration set to minimize a proper scoring rule [GR07, BW19] such as the cross-entropy loss. To understand the behavior of TS with respect to atypicality, we separately perform TS on points grouped according to the atypicality quantiles. Let us denote the temperature fitted to the quantile covering a(X) \u2208(ak\u22121, ak] by \u03c4ak. In Appendix Figure 10 we observe an increasing relationship between ak and \u03c4ak. Different atypicality groups need different adjustments, and more atypical groups need larger temperatures. This suggests that being atypicality-aware can improve calibration. While a single temperature value improves average calibration, it may hurt certain groups.\nwhere \u02c6P(Y |X) is the model that takes an input and outputs scores/logits, and \u03c4 is the temperature parameter. In practice, \u03c4 is optimized using a calibration set to minimize a proper scoring rule [GR07, BW19] such as the cross-entropy loss.\nTo understand the behavior of TS with respect to atypicality, we separately perform TS on points grouped according to the atypicality quantiles. Let us denote the temperature fitted to the quantile covering a(X) \u2208(ak\u22121, ak] by \u03c4ak. In Appendix Figure 10 we observe an increasing relationship between ak and \u03c4ak. Different atypicality groups need different adjustments, and more atypical groups need larger temperatures. This suggests that being atypicality-aware can improve calibration. While a single temperature value improves average calibration, it may hurt certain groups.\n# 4.2 Atypicality-Aware Recalibration\nWe showed that predictions are more reliable when the input is typical. However, predictions are less reliable for atypical inputs, and we may need further revision. An analogy can be drawn to decisionmaking literature where opinions of individuals are combined with geometric averaging weighted by their expertise [FP98, AR89]. Analogously, we propose Atypicality-Aware Recalibration (AAR) a method designed to address the reliability issues identified in dealing with atypical inputs:\nwhere \u03c8(a(X)) is a function of input atypicality, SY is a tunable score for class Y , Z(X) is the normalization term. Intuitively, when the input is typical, we trust the model confidence; otherwise, we use a score for the given class estimated from the calibration set. Note that this form simplifies to\nlog \u02c6PAAR(Y |X) \u221d\u03d5(a(X)) log \u02c6P(Y |X) + SY ,\n(1)\n(2)\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd78/fd7888eb-4f03-453f-ab17-0e942e552274.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Uncalibrated TS AdaTS AAR(Ours)</div>\nFigure 3: Post-hoc Recalibration for Classification. (a) Balanced Supervised Classification: Atypicality-Aware Recalibration improves the calibration of models trained with balanced datasets, across atypicality groups. (b) Imbalanced Supervised Classification: Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of models trained with imbalanced datasets. (c) Classification with LLMs: Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of LLMs performing classification.\n<div style=\"text-align: center;\">Figure 3: Post-hoc Recalibration for Classification. (a) Balanced Supervised Classification: Atypicality-Aware Recalibration improves the calibration of models trained with balanced datasets, across atypicality groups. (b) Imbalanced Supervised Classification: Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of models trained with imbalanced datasets. (c) Classification with LLMs: Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of LLMs performing classification.</div>\nwhere we subsume (1 \u2212\u03c8(a(X)) into \u03d5(a(X)). We give a simple interpretation of this form: the multiplicative term is an atypicality-dependent temperature, and the additive term is a class-dependent correction where exp (SY ) can be considered to induce a correction distribution over classes estimated from the calibration set. Intuitively, when \u03c8(a(X)) = 0, the output reduces to a fixed distribution over classes that was estimated using the calibration set. This distribution can be seen to induce a prior probability over classes, and \u03c8 controls the tradeoff between this prior and the model\u2019s predictive distribution. As the point becomes more typical, this distribution is closer to the model\u2019s predictive distribution. In Appendix Figure 11, we show how these values behave with class atypicality. We find that rare classes require larger positive corrections with larger SY . Implementation Details: Following TS, we minimize the cross-entropy loss on a calibration set. With the temperature-atypicality relationship observed in Figure 10 we choose to instantiate the multiplicative factor as a quadratic function, where \u03d5(a(X)) = c2a(X)2 + c1a(X) + c0 and in total we have |{S1, .., S|Y|, c0, c1, c2}| = |Y| + 3 interpretable parameters. Once the embeddings and logits are computed, AAR runs on a CPU in under 1 minute for all experimented settings. Similar to our adaptive interpretation, a concurrent work, Adaptive Temperature Scaling (AdaTS) [JPL+23], uses temperature scaling where the temperature is parameterized by a Variational Autoencoder(VAE) [KW13] and a multi-layer perceptron on top of the VAE embeddings. In the below experiments, we give results with AdaTS as a baseline when applicable.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/82c2/82c2d38c-74c6-4910-b074-d9cef4dc716c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Improving Group Performance through Atypicality-Awareness. Here we show that AAR improves the calibration and accuracy of models across different skin tone groups. With AAR, we can improve both the worst group performance and overall performance significantly without using group attributes. TS curve is less visible since it significantly overlaps with Skin Tone Conditional.</div>\nFor Balanced Supervised Classification, in Figure 3a, we observe that being atypicality aware improves recalibration across all groups. We perform comparably to AdaTS, where the temperature function has in the order of millions of parameters, whereas AAR has only |Y| + 3 parameters. In Imbalanced Supervised Classification (Figure 3b), our algorithm not only provides better calibration rates across all classes but also improves overall accuracy. Note that only our method can change accuracy (due to the additive term), and it performs better than other baselines in terms of ECE across all classes. Further, the second column shows using Progressive Balancing [ZWF+21] in training, showing that our post-hoc method can complement methods that modify training procedures. For Classification with LLMs, we add an LLM calibration baseline Content-Free Calibration (CF) [ZWF+21]. We cannot use AdaTS as the embeddings are not fixed in size. In Figure 3c, we see AAR has better calibration and accuracy across the three datasets. Namely, by adjusting the LLM output using the LLM atypicality, we can adjust the probabilities to increase the prediction quality.\n# 4.3 Case Study: Fairness through Atypicality-Awareness\nMachine learning models reportedly have performance disparity across subgroups [BHN17] due to factors such as varying sample size or noise levels [CJS18]. For instance, skin lesion classifiers can exhibit performance disparity across different skin tones [DVN+22]. Fitzpatrick17k [GHS+21] is a dataset of clinical images with Fitzpatrick skin tone annotations between 1-to-6, where a larger number means darker skin tones, and when annotators do not agree, it is labeled as \u2018Unknown\u2019. We explore the classification problem with 9 classes indicating the malignancy and the type of skin condition, using a ResNet18/34 pretrained on ImageNet and finetuned on this task (See Appendix F). When the goal is to improve performance across groups, one can use group annotations and optimize performance within each group [HJKRR18, KGZ19]. Here, we investigate how complementing recalibration with atypicality can improve prediction quality across all groups without group annotations. For comparison, we perform 3 recalibration methods: TS, AAR, and Skin-Tone Conditional TS which calibrates the model individually for each skin-tone group with TS. Since the skin-tone conditional calibration uses group attributes, ideally it should act as an oracle. In Figure 4, we give the Accuracy and ECE analyses where AAR improves performance across all groups. For instance, the worst-group Accuracy (0.69) or ECE (0.072) with AAR is close to the best-group Accuracy (0.63) or ECE (0.062) with the other two methods. Overall, our findings suggest that Atypicality-Awareness can complement fairness-enforcing methods, and improve performance even when the group annotations are unavailable. We hypothesize that with AAR, we can perform better than using supervised group attributes since groups may not have sufficient sample size in the calibration set (131, 1950, 1509, 555 samples for Unknown, 1&2, 3&4, and 5&6 respectively), and we can leverage atypicality to offer some mitigation. Further investigating how to leverage atypicality to improve fairness and factors affecting performance disparities is a promising direction for future work [CJS18].\n# 5 Improving Prediction Sets with Atypicality\nConformal Prediction [SV08, AB21] is a framework that assigns a calibrated prediction set to each instance. The goal is to find a function C : X \u21922Y that returns a subset of the label space such that Y \u2208C(X) with high probability. The framework aims to guarantee marginal coverage, i.e., P(Y \u2208C(X)) \u22651 \u2212\u03b1, for a choice of \u03b1. We investigate two conformal calibration methods,\nAdaptive Prediction Sets (APS) [RSC20] and Regularized APS (RAPS) [ABMJ20]. Let \u03c0(X) be the permutation of the label set that sorts \u02c6P(Y = c|X), i.e. the predicted probabilities for each class c after TS. The prediction sets are produced by the function C(x) = {y : s(x, y) \u2264\u02c6q}, and these methods fit the threshold \u02c6q for a choice of the scoring function. APS uses the cumulative sum of the predicted probabilities s(x, y) = \ufffdc j=1 \u02c6P(Y = j|X), where y = \u03c0c(X). Intuitively, if the model was perfectly calibrated, we would have expected to have \u02c6q = 1 \u2212\u03b1. Similarly, RAPS builds on the idea that tail probabilities are noisy and regularizes the number of samples in the prediction set. Building on our ideas in the previous sections we implement Atypicality-Aware prediction sets, namely AA-APS and AA-RAPS in the following way: We first group points according to their confidence and atypicality quantiles. A group G here is defined using 4 thresholds, namely G = x : (l(G) a < qa(x) \u2264h(G) a ) \u2227(l(G) c < qc(x) \u2264h(G) c ) where qa(x) and qc(x) denote the atypicality and confidence quantiles for the sample x, l(G) a and h(G) a denote the atypicality lower and upper bounds for group G, and l(G) c and h(G) c denote the confidence lower and upper bounds for group G. Using a calibration set, these bounds are simply determined by the quantiles of confidence and atypicality statistics. Then, we fit separate thresholds \u02c6qG for each group\u2019s prediction sets with APS or RAPS as subroutines. This allows us to have an adaptive threshold depending only on the atypicality and confidence of predictions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/738c/738cfdba-49e9-4e66-970a-5a553f2fe806.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Improving Conformal Calibration with Atypicality for ResNet50 on ImageNet. Here we show that atypicality awareness improves conformal calibration performance across different groups. Methods are fitted to satisfy 95% coverage. We observe that APS and RAPS do not satisfy conditional coverage for high atypicality regions or low confidence regions.</div>\nIn Figure 5, we provide the coverage plots for APS and RAPS in the first and third columns. Even though marginal coverage is satisfied, models do not satisfy conditional coverage for atypical inputs or low-confidence predictions. We observe that being Atypicality-Aware improves coverage across otherwise underperforming groups. Further, AA-APS has lower set sizes on average than APS (15.6 vs 21.3). While RAPS has a lower average set size than AA-RAPS (4.2 vs 9.1) AA-RAPS has smaller set sizes for high-confidence samples, whereas a larger set size for low-confidence samples where the coverage is not met for RAPS. In Appendix D.3, we provide the same analysis for ResNet18,50,152 at different coverage levels along with analyzing the performance in the Confidence and Atypicality dimensions individually. For instance in Figure 8, we observe that RAPS and APS do not satisfy coverage for high atypicality regions, even when averaged across different confidence levels.\n# 6 Additional Related Work\nUncertainty and Atypicality: [MKvA+21, PBC+20] use density estimation to disentangle epistemic and aleatoric uncertainty. Following this, they show improvements in active learning and OOD detection [LLLS18]. We note that our goal is not this disentanglement (e.g. Untrustworthy quadrant can have both aleatoric or epistemic uncertainty), or Ambiguity could be due to a lack of features or noise. [LLP+20] propose the related notion of distance awareness, and that it leads to better uncertainty quantification. They offer architecture and training modifications whereas we analyze existing models using our framework including imbalanced and LLM settings, and propose simple and post-hoc approaches. \u2018OOD\u2019 [HG16b] or \u2018anomaly\u2019 [HMD18] notions are tied to atypicality, yet our goal is not to make a binary distinction between \u2018in\u2019 or \u2018out\u2019. We argue that in-distribution samples could also be atypical (e.g. rare groups), and the goal is to perform reliably in the entire spectrum. Other works with an atypicality notion include bounding calibration of groups by the\nexcess risk [LSH19], miscalibration under distribution shifts [OFR+19], uncertainty in Gaussian Processes [Ras04], forgetting time for rare examples [MGLK22], the poor performance of groups with lower sample sizes [CJS18], energy-based models improving calibration [GWJ+20], relating perplexity to zero-shot classification performance for LLMs [GIB+22], grouping loss and local definitions of miscalibration [PLMV23], the relationship between active learning and atypicality [HDW22], sample size as a factor for subgroup performance disparity [CJS18]. [PBZ21] provide insightful discussion around the nature of softmax confidence, and here we show that its reliability depends on the atypicality of the input. Our new findings include showing that predictions for atypical samples are more miscalibrated and overconfident, and atypicality awareness improves prediction quality. Overall, while there are other relevant notions in the literature, our distinct goal is to show that post-hoc atypicality estimation and recalibration is a simple yet useful framework to understand and improve uncertainty quantification that complements existing methods. Recalibration and Conformal Prediction: There is a rich literature on recalibration methods and prediction sets: TS [GPSW17], Platt Scaling [P+99], conformal calibration [SV08, ABMJ20] among many. [LLC+22, RBSC19, BYR+21, BGJ+22] make a relevant observation, showing that the coverage of conformal prediction is not equal across all groups. They propose group conformal calibration, which requires group labels whereas our proposal is unsupervised and does not depend on any attribute information. Concurrent work [JPL+23] explores AdaTS, where they train a separate VAE and MLP to produce an adaptive temperature. However, our parameterization of temperature has 3 parameters and is interpretable.\n# 7 Conclusion\nAtypicality offers a simple yet flexible framework to better understand and improve model reliability and uncertainty. We propose that pretrained models should be released not only with confidence but also with an atypicality estimator. While there are other relevant notions in the literature, our main goal is to show that atypicality can provide a unifying perspective to discuss uncertainty, understand individual data points, and improve fairness. Here we focus on classification problems; it would be interesting to extend atypicality to regression and generation settings. Furthermore, we would like to extend the theoretical analysis to more general settings, as our empirical results demonstrate that the observed phenomena hold more broadly.\n# Acknowledgments\nWe would like to thank Adarsh Jeewajee, Bryan He, Edward Chen, Federico Bianchi, Kyle Swanson, Natalie Dullerud, Ransalu Senanayake, Sabri Eyuboglu, Shirley Wu, Weixin Liang, Xuechen Li, Yongchan Kwon, Yu Sun, and Zach Izzo for their comments and suggestions on the manuscript. Linjun Zhang\u2019s research is partially supported by NSF DMS-2015378. Carlos Ernesto Guestrin is a Chan Zuckerberg Biohub \u2013 San Francisco Investigator.\n# References\nReferences\n[AB21] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021. 8, 23 [ABMJ20] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. Uncertainty sets for image classifiers using conformal prediction. arXiv preprint arXiv:2009.14193, 2020. 2, 9, 10, 23, 24 [AR89] J\u00b4anos Acz\u00b4el and Fred S Roberts. On the possible merging functions. Mathematical Social Sciences, 17(3):205\u2013243, 1989. 6 [BGJ+22] Osbert Bastani, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Practical adversarial multivalid conformal prediction. arXiv preprint arXiv:2206.01067, 2022. 10 [BHN17] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial, 1:2, 2017. 8\n[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 4 BMWX21a] Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. Don\u2019t just blame overparametrization for over-confidence: Theoretical analysis of calibration in binary classification. In International Conference on Machine Learning, pages 566\u2013576. PMLR, 2021. 2, 5, 6, 27, 33 BMWX21b] Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. Understanding the undercoverage bias in uncertainty estimation. Advances in Neural Information Processing Systems, 34:18307\u201318319, 2021. 5 [BW19] David Bolin and Jonas Wallin. Local scale invariance and robustness of proper scoring rules. arXiv preprint arXiv:1912.05642, 2019. 6 [BYR+21] Noam Barda, Gal Yona, Guy N Rothblum, Philip Greenland, Morton Leibowitz, Ran Balicer, Eitan Bachmat, and Noa Dagan. Addressing bias in prediction models by improving subpopulation calibration. Journal of the American Medical Informatics Association, 28(3):549\u2013558, 2021. 10 [CJS18] Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? Advances in neural information processing systems, 31, 2018. 8, 10 [CLKZ22] Lucas Clart\u00b4e, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00b4a. Theoretical characterization of uncertainty in high-dimensional linear classification. arXiv preprint arXiv:2202.03295, 2022. 5 [CWG+19] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. Advances in neural information processing systems, 32, 2019. 3 [DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019. 17 [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. 4, 17 [DVN+22] Roxana Daneshjou, Kailas Vodrahalli, Roberto A Novoa, Melissa Jenkins, Weixin Liang, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, et al. Disparities in dermatology ai performance on a diverse, curated clinical image set. Science advances, 8(31):eabq6147, 2022. 3, 8 [FP98] Ernest Forman and Kirti Peniwati. Aggregating individual judgments and priorities with the analytic hierarchy process. European journal of operational research, 108(1):165\u2013169, 1998. 6 [GHS+21] Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri. Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1820\u20131828, 2021. 8, 26 [GIB+22] Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022. 10 [GPSW17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017. 2, 4, 10, 23\n[GR07] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378, 2007. 2, 4, 6 [GWJ+20] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In International Conference on Learning Representations, 2020. 10 [HDW22] Guy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on a budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794, 2022. 10 [HG16a] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and outof-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. 3 [HG16b] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. ArXiv, abs/1610.02136, 2016. 9 HJKRR18] Ursula H\u00b4ebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In International Conference on Machine Learning, pages 1939\u20131948. PMLR, 2018. 8, 33 [HMD18] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. 9, 19 [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 4 [JKGG18] Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classifier. Advances in neural information processing systems, 31, 2018. 3 [JPL+23] Tom Joy, Francesco Pinto, Ser-Nam Lim, Philip HS Torr, and Puneet K Dokania. Sample-dependent adaptive temperature scaling for improved calibration. AAAI, 2023. 7, 10, 23 [KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box postprocessing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247\u2013254, 2019. 8, 33 [Kri09] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. 4, 17 [Kum22] Sawan Kumar. Answer-level calibration for free-form multiple choice question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 665\u2013679, 2022. 19 [KW13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 7 [LLC+22] Charles Lu, Andr\u00b4eanne Lemay, Ken Chang, Katharina H\u00a8obel, and Jayashree KalpathyCramer. Fair conformal predictors for applications in medical imaging. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12008\u201312016, 2022. 10 [LLLS18] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018. 4, 9 [LLP+20] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Information Processing Systems, 33:7498\u20137512, 2020. 9\n[LN89] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1):503\u2013528, 1989. 23 [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 4 [LR02] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. 4 [LSH19] Lydia T Liu, Max Simchowitz, and Moritz Hardt. The implicit fairness criterion of unconstrained learning. In International Conference on Machine Learning, pages 4051\u20134060. PMLR, 2019. 10 [LVdMJ+21] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario \u02c7Sa\u02c7sko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl\u00b4ement Delangue, Th\u00b4eo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franc\u00b8ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, 2021. 17 [MDP+11] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. 4 [MGLK22] Pratyush Maini, Saurabh Garg, Zachary C Lipton, and J Zico Kolter. Characterizing datapoints via second-split forgetting. arXiv preprint arXiv:2210.15031, 2022. 10 [MKS+20] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip HS Torr, and Puneet K Dokania. Calibrating deep neural networks using focal loss. 2020. 4, 17 [MKvA+21] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncertainty: A simple baseline. arXiv e-prints, pages arXiv\u20132102, 2021. 2, 4, 9, 16 [MP80] Carolyn B Mervis and John R Pani. Acquisition of basic object categories. Cognitive Psychology, 12(4):496\u2013522, 1980. 1 [MR10] S\u00b4ebastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM international conference on Multimedia, pages 1485\u20131488, 2010. 17 [Mur04] Gregory Murphy. The big book of concepts. MIT press, 2004. 1 [NCH15] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015. 4 [OFR+19] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019. 10 [P+99] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999. 6, 10\nLOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 4\n[LR02] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. 4\n[LSH19] Lydia T Liu, Max Simchowitz, and Moritz Hardt. The implicit fairness criterion of unconstrained learning. In International Conference on Machine Learning, pages 4051\u20134060. PMLR, 2019. 10\n[Mur04] Gregory Murphy. The big book of concepts. MIT press, 2004. 1\n[NCH15] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015. 4\n[OFR+19] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019. 10\n[P+99] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999. 6, 10\n[P+99] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999. 6, 10\n[PBC+20] Janis Postels, Hermann Blum, Cesar Cadena, Roland Siegwart, Luc Van Gool, and Federico Tombari. Quantifying aleatoric and epistemic uncertainty using density estimation in latent space. arXiv preprint arXiv:2012.03082, 2020. 9 [PBZ21] Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding softmax confidence and uncertainty. arXiv preprint arXiv:2106.04972, 2021. 10 PLMV23] Alexandre Perez-Lebel, Marine Le Morvan, and Gael Varoquaux. Beyond calibration: estimating the grouping loss of modern neural networks. In The Eleventh International Conference on Learning Representations, 2023. 10 [Ras04] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine learning, pages 63\u201371. Springer, 2004. 10 [RBSC19] Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel J Cand`es. With malice towards none: Assessing uncertainty via equalized coverage. arXiv preprint arXiv:1908.05428, 2019. 10 [Rip89] Lance J . Rips. Similarity, typicality, and categorization, page 21\u201359. Cambridge University Press, 1989. 1 [RM75] Eleanor Rosch and Carolyn B Mervis. Family resemblances: Studies in the internal structure of categories. Cognitive psychology, 7(4):573\u2013605, 1975. 1 [RSC20] Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Classification with valid and adaptive coverage. Advances in Neural Information Processing Systems, 33:3581\u2013 3591, 2020. 9 [RSS73] Lance J Rips, Edward J Shoben, and Edward E Smith. Semantic distance and the verification of semantic relations. Journal of verbal learning and verbal behavior, 12(1):1\u201320, 1973. 1 [SC19] Pragya Sur and Emmanuel J Cand`es. A modern maximum-likelihood theory for highdimensional logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516\u201314525, 2019. 6, 27, 33 [SV08] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(3), 2008. 8, 10 TGZ+23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instructionfollowing llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 4, 18 [TJ06] MTCAJ Thomas and A Thomas Joy. Elements of information theory. WileyInterscience, 2006. 3 [TK74] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. science, 185(4157):1124\u20131131, 1974. 1 [TK92] Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5(4):297\u2013323, 1992. 1 [VGS05] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Springer Science & Business Media, 2005. 23 WDS+20] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, 2020. 17\n[VGS05] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Springer Science & Business Media, 2005. 23\n[WDS+20] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, 2020. 17\n[WNB18] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, 2018. 4, 17 [ZCLJ21] Zhisheng Zhong, Jiequan Cui, Shu Liu, and Jiaya Jia. Improving calibration for long-tailed recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16489\u201316498, 2021. 3, 4, 18 [ZDKZ22] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In International Conference on Machine Learning, pages 26135\u2013 26160. PMLR, 2022. 5 [ZK16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 4 [ZKL+16] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database for deep scene understanding. arXiv preprint arXiv:1610.02055, 2016. 18 [ZWF+21] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR, 2021. 4, 8, 18, 19 [ZZL15] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015. 4\n# A Atypicality Estimation\nA.1 Input Atypicality Estimation\n# A.1 Input Atypicality Estimation\nTo estimate input atypicality, we use two ways to estimate the likelihood of a point under the trainin distribution. First, we give methods for the discriminative models.\nFitting individual Gaussians to class conditionals Here, we follow a similar approach to [MKvA+21]. Namely, we model the class conditionals with a Gaussian distribution, where the covariance matrix is tied across classes:\nWe fit the parameters \u00b5y and \u03a3 with maximum likelihood estimation. The reason to tie the covariance matrix is due to the number of samples required to fit the density. Namely, for a d-dimensional problem, the total number of parameters to fit individual matrices becomes O(yd2), which results in low-quality estimates. Then, the atypicality becomes\nComputing distance with k-Nearest Neighbors k-Nearest Neighbors: Similarly, we can use the nearest neighbor distance. Concretely, we use the nearest neighbor distance, aX(x) = dmin(x, Dtrain) = minx\u2032\u2208Dtrain |x\u2032 \u2212x| , as the atypicality metric. Alternatively, we can use different notions such as the average of k-nearest neighbors, or the distance to the kth neighbor. Below, we report the results by using the average distance to 5-nearest neighbors.\nFitting the estimators For all of the atypicality estimators, we fit the estimators using samples from the training sets and make inferences for the calibration and test sets. For instance, we use the training split of ImageNet to fit the corresponding density estimator and compute the atypicality for the samples from the validation/test split of ImageNet. All of our results using atypicality are reported for the test splits of the below datasets.\nAtypicality Estimation with LLMs: For language models we simply compute the negative loglikelihood of each prompt as the atypicality metric: aX(x) = \u2212log \u02c6PLLM(x). To define confidence, we use the logits of the language model, conditioned on the prompt. We use the logit of the first token of each class label and compute the predicted probabilities by applying softmax to the logits of each class.\n# A.2 Class Atypicality\nTo estimate class atypicality, we simply count the fraction of examples from a particular label in the training dataset. Let us have a training dataset Dtrain = {(x1, y1), (x2, y2), . . . , (xN, yN)}. Then, we estimate class atypicality with\n# A.3 Atypicality and Confidence\nAre atypicality and confidence equally informative? Beyond the data perspective given in Figure 1, here we provide quantitative results to demonstrate the difference. In Figure 6, we give a grid plot where the x-axis indicates the typicality quantile of a point, and the y-axis indicates the confidence of a point. The coloring on the left indicates the accuracy within a bin split according to accuracy, and the right has the difference between average confidence and accuracy. Observe that for a specific confidence interval, larger values of typicality mean better quality probabilistic estimates and larger atypicality means more miscalibration.\n(4)\n(5)\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f391/f391841b-86d0-44d1-a9b1-530385ccb1c7.png\" style=\"width: 50%;\"></div>\nFigure 6: Input Atypicality and Confidence. Here, the x-axis reflects the input atypicality quantile, and the y-axis indicates confidence. The coloring for the figure on the left indicates the accuracy within a bin, and the figure on the right has the difference between confidence and accuracy within a bin. We observe that even within the same confidence range, atypical examples tend to be more miscalibrated and overconfident compared to typical examples.\n# B Experimental Details\n# B.1 Balanced Supervised Classification\n# B.1.1 Datasets\nBelow is a full list of datasets for balanced classification:\nBelow is a full list of datasets for balanced classification: 1. ImageNet [DDS+09] from Torchvision [MR10] is an object recognition dataset with 1000 classes. We use the ImageNet-1k version. 2. CIFAR10/100 [Kri09] from Torchvision [MR10] are object recognition datasets with 10/100 classes. 3. MNLI [WNB18] from Huggingface Datasets [LVdMJ+21] is a natural language inference dataset with 3 classes, indicating entailment, neutral, and contradiction outcomes.\n1. ImageNet [DDS+09] from Torchvision [MR10] is an object recognition dataset with 1000 classes. We use the ImageNet-1k version. 2. CIFAR10/100 [Kri09] from Torchvision [MR10] are object recognition datasets with 10/100 classes.\n# B.1.2 Models\nMost of the models are public models, e.g., obtained from the Transformers Library [WDS+20] or Torchvision [MR10]. Below we give the full model details and how one can access them:\n1. RoBERTa(HuggingFace roberta-large-mnli) trained on the MNLI dataset. One can use the id given here on HuggingFace to download the model.\n2. ResNet18, ResNet50, ResNet152 from (Torchvision [MR10]) trained on ImageNet.\n3. WideResNet28 trained on CIFAR10,100 obtained from [MKS+20].\nFor all BERT [DCLT19] style models we use the [CLS] token embeddings in the final layer to perform classification. For all vision models, we use the penultimate layer embeddings to fit the density estimators and perform the analyses. In the experiments, we randomly split the test sets into two equal halves to have a calibration split and a test split, and repeat the experiments over 10 random seeds.\n# B.2.1 Datasets\nAll of our imbalanced classification datasets are previous benchmarks obtained from the GitHu repository6 of [ZCLJ21] with corresponding training, validation, and test splits. All of these datase have an exponential class imbalance. 1. ImageNet-LT is the long-tailed variant of ImageNet with 1000 classes. 2. CIFAR10/100-LT is the long-tailed variant of CIFAR10/100 with 10/100 classes. 3. Places365-LT is the long-tailed variant of Places365 [ZKL+16] with 365 classes.\n# B.2.2 Models\n1. ResNeXt50 trained on ImageNet-LT with and without Progressive Balancing, which is a strategy to address class imbalance during training. 2. ResNet152 trained on Places365-LT 3. ResNet18 trained on CIFAR100-LT trained by us. This model is pretrained on ImageNet and finetuned on CIFAR100-LT. We use the validation splits of these datasets as the calibration set, and report the results on the test set.\n# B.3 Classification with LLMs\n# B.3.1 Model\nWe use Alpaca-7B [TGZ+23] in a zero-shot setting, where we simply prompt the model with the classification question. We use the prompting strategy from Content-Free Calibration [ZWF+21]. Below, we show examples of each dataset and prompt.\n# B.3.2 Datasets\nIMDB is a binary classification dataset of movie reviews, where the goal is to classify the sentiment in a review. The example prompt has the form \u2018The following review was written for a movie: [Review].\\n What is the sentiment, Positive or Negative? Answer: \u2019. Below is an example:\nThe following review was written for a movie: I and a friend rented this movie. We both found the movie soundtrack and production techniques to be lagging. The movie\u2019s plot appeared to drag on throughout with little surprise in the ending. We both agreed that the movie could have been compressed into roughly an hour giving it more suspense and moving plot. What is the sentiment, Positive or Negative? Answer:\nwhere the correct answer should be \u2018Negative\u2019. We filter out the examples that exceed the context length limit of Alpaca7B (512). We noticed that the \u2018validation\u2019 split of IMDB leads to significantly worse calibration compared to splitting the test set. Thus, for all experiments, we use the test split of IMDB and split it into 2 sets (instead of using the validation split as a calibration set as in the other two datasets). TREC is a 6-class question classification dataset where the goal is to predict whether a question will have an answer that is an \u2018Abbreviation\u2019, \u2018Entity\u2019, \u2018Description\u2019, \u2018Human\u2019, \u2018Location\u2019, or a \u2018Number\u2019. We format the prompts with \u2018Classify the questions based on their Answer Type. Potential Answer Types are: Number, Location, Person, Description, Entity, or Abbreviation.\\n\\nQuestion: [question]\\n\\nAnswer Type: \u2019. Below is an example prompt:\n6https://github.com/dvlab-research/MiSLAS\nClassify the questions based on their Answer Type. Potential Answer Types are: Number, Location, Person, Description, Entity, or Abbreviation.\n# Classify the questions based on their Answer Type. Potential Answer Types are: Number, Location, Person, Description, Entity, or Abbreviation.\nQuestion: What county is Modesto , California in ? Answer Type:\nwhere the correct answer should be \u2018Location\u2019. AG News is a news classification dataset. The goal is to classify a given news into 4 potential classes: \u2018World\u2019, \u2018Sports\u2019, \u2018Business\u2019, or \u2018Science and Technology\u2019. We format the prompts with \u2018Classify the news articles into the categories of World, Sports, Business, and Technology.\\n\\n Article: [article]\\n\\nAnswer: \u2019. Below is an example prompt:\nArticle: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street\u2019s dwindling band of ultra-cynics, are seeing green again.\nArticle: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street\u2019s dwindling band of ultra-cynics, are seeing green again. Answer:\nwhere the correct answer should be \u2018Business\u2019. Furthermore, we also use [ZWF+21] as another calibration baseline. Fo\nwhere the correct answer should be \u2018Business\u2019.\nFurthermore, we also use [ZWF+21] as another calibration baseline. Following their paper, we use N/A, [MASK], and the empty string as content-free. Concretely, we follow their paper to first obtain the average predicted probabilities for each label token for the content-free input, denoted by pcf We then let\nWhen making test-time predictions, we compute Softmax(W T p) as the new predicted probabilities In our experiments, we observe that it does not perform as well in this setting, as was previously suggested by [Kum22].\n# C Calibration\nWe run all our experiments with 10 different random seeds, where the seeds are {0, 1, 2, . . . , 9}. Randomness is over fitting the atypicality estimators, and calibration-test splits (we use the same splits with the recalibration experiments for the sake of consistency).\n# C.1 Expected Calibration Error\nTo compute ECE, we generate B = {B1, B2, ..., BM}, M equally-spaced bins where samples are sorted and grouped according to their confidence. Bm here denotes the set of the data indices where the confidence of the prediction for the sample falls into the interval ( m\u22121 M , m M ]. We compute ECEby\nwhere acc(Bm) = 1 |Bm| \ufffd|Bm| i=1 1[\u02c6yi = yi] is the accuracy for the bin m, and conf(Bm) = 1 |Bm| \ufffd|Bm| i=1 \u02c6P(Y = \u02c6yi|X = xi) gives the average confidence within the bin. |Bm| is the size of the bin m, N is the total number of samples, and 1[\u00b7] is the indicator function. Throughout our experiments, we let the number of bins |B| = 10 by default when computing ECE. Similarly, below we report results with RMSCE (Root Mean Squared Error) [HMD18] as another calibration metric, which is formulated as the following:\n(8)\nWe further experiment with different atypicality metrics, such as the average distance to the 5-nearest neighbors (Figure 7). We broadly observe that while there are slight differences in the quantitative results between different atypicality metrics, the qualitative phenomena remain intact. In Tables 1, 2, and 3 we give all the results in the tabular form.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca1f/ca1fd010-960e-45a1-84c1-aefe53ebe0ec.png\" style=\"width: 50%;\"></div>\nFigure 7: Atypicality with 5-nearest neighbors and Uncertainty. Here, we report the results of the same experiments as Figure 3 with the average of the distance to the 10-nearest neighbors as the atypicality metric. See Tables 1 for the results in tabular format.\nTable 1: Recalibration Results for Balanced Classification. For each dataset and atypicality quantile, the best results are marked in bold. We provide the standard errors next to the means over 10 random seeds.\nTable 1: Recalibration Results for Balanced Classification. For each dataset and atypicality quantile, the best results are marked in bold. We provide the standard errors next to the means over\n10 random seeds.\nUncalibrated\nTS\nAdaTS\nAtypicality-Aware\nAtypicality\nECE\nRMSE\nECE\nRMSE\nECE\nRMSE\nECE\nRMSE\nGMM\nResNet152\nImageNet\n0.2\n0.029 \u00b1 0.001\n0.077 \u00b1 0.001\n0.014 \u00b1 0.001\n0.066 \u00b1 0.002\n0.014 \u00b1 0.001\n0.064 \u00b1 0.002\n0.016 \u00b1 0.001\n0.065 \u00b1 0.001\n0.4\n0.039 \u00b1 0.001\n0.086 \u00b1 0.001\n0.020 \u00b1 0.001\n0.074 \u00b1 0.001\n0.020 \u00b1 0.002\n0.075 \u00b1 0.002\n0.019 \u00b1 0.001\n0.070 \u00b1 0.001\n0.6\n0.050 \u00b1 0.001\n0.097 \u00b1 0.001\n0.026 \u00b1 0.001\n0.079 \u00b1 0.002\n0.025 \u00b1 0.002\n0.081 \u00b1 0.002\n0.026 \u00b1 0.000\n0.078 \u00b1 0.001\n0.8\n0.062 \u00b1 0.001\n0.106 \u00b1 0.001\n0.024 \u00b1 0.001\n0.076 \u00b1 0.001\n0.023 \u00b1 0.002\n0.078 \u00b1 0.002\n0.024 \u00b1 0.001\n0.077 \u00b1 0.001\n1.0\n0.084 \u00b1 0.001\n0.123 \u00b1 0.001\n0.037 \u00b1 0.001\n0.088 \u00b1 0.001\n0.033 \u00b1 0.004\n0.085 \u00b1 0.003\n0.026 \u00b1 0.001\n0.080 \u00b1 0.001\nResNet18\nImageNet\n0.2\n0.017 \u00b1 0.001\n0.074 \u00b1 0.001\n0.031 \u00b1 0.001\n0.084 \u00b1 0.001\n0.028 \u00b1 0.004\n0.081 \u00b1 0.004\n0.016 \u00b1 0.001\n0.069 \u00b1 0.001\n0.4\n0.016 \u00b1 0.001\n0.074 \u00b1 0.001\n0.020 \u00b1 0.001\n0.079 \u00b1 0.001\n0.023 \u00b1 0.003\n0.079 \u00b1 0.003\n0.015 \u00b1 0.001\n0.072 \u00b1 0.002\n0.6\n0.025 \u00b1 0.001\n0.080 \u00b1 0.001\n0.023 \u00b1 0.001\n0.077 \u00b1 0.001\n0.024 \u00b1 0.003\n0.080 \u00b1 0.002\n0.019 \u00b1 0.001\n0.074 \u00b1 0.001\n0.8\n0.040 \u00b1 0.001\n0.092 \u00b1 0.001\n0.024 \u00b1 0.001\n0.078 \u00b1 0.001\n0.029 \u00b1 0.002\n0.082 \u00b1 0.003\n0.018 \u00b1 0.001\n0.074 \u00b1 0.001\n1.0\n0.054 \u00b1 0.001\n0.100 \u00b1 0.001\n0.033 \u00b1 0.001\n0.085 \u00b1 0.001\n0.032 \u00b1 0.003\n0.083 \u00b1 0.003\n0.017 \u00b1 0.002\n0.073 \u00b1 0.002\nResNet50\nImageNet\n0.2\n0.021 \u00b1 0.001\n0.069 \u00b1 0.002\n0.018 \u00b1 0.001\n0.071 \u00b1 0.001\n0.017 \u00b1 0.001\n0.068 \u00b1 0.002\n0.018 \u00b1 0.001\n0.067 \u00b1 0.002\n0.4\n0.029 \u00b1 0.001\n0.079 \u00b1 0.001\n0.023 \u00b1 0.001\n0.077 \u00b1 0.001\n0.020 \u00b1 0.002\n0.074 \u00b1 0.001\n0.022 \u00b1 0.001\n0.074 \u00b1 0.001\n0.6\n0.041 \u00b1 0.001\n0.091 \u00b1 0.001\n0.023 \u00b1 0.001\n0.078 \u00b1 0.001\n0.026 \u00b1 0.002\n0.079 \u00b1 0.002\n0.026 \u00b1 0.001\n0.078 \u00b1 0.001\n0.8\n0.042 \u00b1 0.001\n0.092 \u00b1 0.001\n0.024 \u00b1 0.001\n0.078 \u00b1 0.001\n0.024 \u00b1 0.001\n0.080 \u00b1 0.002\n0.020 \u00b1 0.001\n0.075 \u00b1 0.001\n1.0\n0.078 \u00b1 0.000\n0.118 \u00b1 0.000\n0.049 \u00b1 0.001\n0.095 \u00b1 0.001\n0.047 \u00b1 0.004\n0.096 \u00b1 0.003\n0.031 \u00b1 0.001\n0.082 \u00b1 0.001\nRoBERTa\nMNLI\n0.2\n0.005 \u00b1 0.001\n0.062 \u00b1 0.004\n0.013 \u00b1 0.001\n0.077 \u00b1 0.002\n0.006 \u00b1 0.002\n0.063 \u00b1 0.005\n0.006 \u00b1 0.001\n0.063 \u00b1 0.002\n0.4\n0.016 \u00b1 0.001\n0.091 \u00b1 0.004\n0.013 \u00b1 0.001\n0.083 \u00b1 0.002\n0.010 \u00b1 0.002\n0.075 \u00b1 0.005\n0.008 \u00b1 0.001\n0.072 \u00b1 0.002\n0.6\n0.034 \u00b1 0.002\n0.116 \u00b1 0.002\n0.015 \u00b1 0.001\n0.092 \u00b1 0.004\n0.016 \u00b1 0.001\n0.093 \u00b1 0.005\n0.017 \u00b1 0.002\n0.090 \u00b1 0.004\n0.8\n0.061 \u00b1 0.002\n0.153 \u00b1 0.003\n0.031 \u00b1 0.002\n0.113 \u00b1 0.004\n0.024 \u00b1 0.002\n0.104 \u00b1 0.004\n0.028 \u00b1 0.002\n0.114 \u00b1 0.004\n1.0\n0.065 \u00b1 0.002\n0.156 \u00b1 0.002\n0.028 \u00b1 0.002\n0.112 \u00b1 0.003\n0.021 \u00b1 0.002\n0.106 \u00b1 0.004\n0.023 \u00b1 0.003\n0.107 \u00b1 0.004\nWideResNet28\nCIFAR10\n0.2\n0.003 \u00b1 0.000\n0.041 \u00b1 0.001\n0.009 \u00b1 0.000\n0.057 \u00b1 0.000\n0.003 \u00b1 0.000\n0.051 \u00b1 0.002\n0.001 \u00b1 0.000\n0.041 \u00b1 0.001\n0.4\n0.005 \u00b1 0.001\n0.051 \u00b1 0.002\n0.007 \u00b1 0.001\n0.055 \u00b1 0.001\n0.002 \u00b1 0.001\n0.047 \u00b1 0.002\n0.002 \u00b1 0.000\n0.048 \u00b1 0.002\n0.6\n0.004 \u00b1 0.001\n0.050 \u00b1 0.003\n0.008 \u00b1 0.001\n0.058 \u00b1 0.001\n0.004 \u00b1 0.001\n0.049 \u00b1 0.004\n0.002 \u00b1 0.000\n0.044 \u00b1 0.002\n0.8\n0.009 \u00b1 0.001\n0.066 \u00b1 0.003\n0.004 \u00b1 0.001\n0.056 \u00b1 0.002\n0.003 \u00b1 0.001\n0.058 \u00b1 0.003\n0.002 \u00b1 0.001\n0.055 \u00b1 0.002\n1.0\n0.142 \u00b1 0.002\n0.240 \u00b1 0.002\n0.073 \u00b1 0.002\n0.168 \u00b1 0.003\n0.046 \u00b1 0.002\n0.129 \u00b1 0.004\n0.041 \u00b1 0.002\n0.123 \u00b1 0.002\nWideResNet28\nCIFAR100\n0.2\n0.007 \u00b1 0.001\n0.059 \u00b1 0.003\n0.018 \u00b1 0.001\n0.084 \u00b1 0.001\n0.021 \u00b1 0.003\n0.089 \u00b1 0.005\n0.021 \u00b1 0.001\n0.094 \u00b1 0.002\n0.4\n0.029 \u00b1 0.001\n0.110 \u00b1 0.002\n0.005 \u00b1 0.001\n0.070 \u00b1 0.003\n0.008 \u00b1 0.002\n0.073 \u00b1 0.003\n0.005 \u00b1 0.001\n0.078 \u00b1 0.003\n0.6\n0.101 \u00b1 0.002\n0.201 \u00b1 0.002\n0.052 \u00b1 0.001\n0.136 \u00b1 0.002\n0.044 \u00b1 0.004\n0.124 \u00b1 0.007\n0.049 \u00b1 0.001\n0.132 \u00b1 0.003\n0.8\n0.257 \u00b1 0.004\n0.297 \u00b1 0.002\n0.106 \u00b1 0.003\n0.191 \u00b1 0.003\n0.101 \u00b1 0.003\n0.187 \u00b1 0.002\n0.105 \u00b1 0.003\n0.190 \u00b1 0.003\n1.0\n0.371 \u00b1 0.004\n0.351 \u00b1 0.002\n0.105 \u00b1 0.004\n0.200 \u00b1 0.003\n0.101 \u00b1 0.004\n0.199 \u00b1 0.003\n0.109 \u00b1 0.005\n0.207 \u00b1 0.005\nKNN\nResNet152\nImageNet\n0.2\n0.035 \u00b1 0.001\n0.085 \u00b1 0.001\n0.014 \u00b1 0.001\n0.062 \u00b1 0.002\n0.016 \u00b1 0.002\n0.068 \u00b1 0.001\n0.011 \u00b1 0.001\n0.061 \u00b1 0.001\n0.4\n0.039 \u00b1 0.001\n0.085 \u00b1 0.001\n0.015 \u00b1 0.001\n0.067 \u00b1 0.001\n0.018 \u00b1 0.001\n0.069 \u00b1 0.002\n0.017 \u00b1 0.001\n0.071 \u00b1 0.001\n0.6\n0.035 \u00b1 0.001\n0.083 \u00b1 0.001\n0.021 \u00b1 0.001\n0.076 \u00b1 0.002\n0.024 \u00b1 0.003\n0.079 \u00b1 0.004\n0.019 \u00b1 0.001\n0.072 \u00b1 0.001\n0.8\n0.057 \u00b1 0.002\n0.102 \u00b1 0.001\n0.031 \u00b1 0.001\n0.084 \u00b1 0.001\n0.033 \u00b1 0.003\n0.085 \u00b1 0.003\n0.032 \u00b1 0.001\n0.082 \u00b1 0.001\n1.0\n0.099 \u00b1 0.001\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of understanding the reliability of machine learning model predictions by introducing the concept of atypicality, which relates to how well-represented a sample is in the training dataset. Previous methods primarily focused on confidence levels in predictions, which can be misleading. The authors argue that incorporating atypicality is crucial for better uncertainty quantification and model performance.",
        "problem": {
            "definition": "The problem is that traditional machine learning models often rely solely on confidence scores to assess the reliability of predictions, which can be inadequate, especially for atypical or rare inputs.",
            "key obstacle": "The main challenge is that existing methods do not account for atypicality, leading to overconfident predictions for atypical inputs that are often miscalibrated and less accurate."
        },
        "idea": {
            "intuition": "The inspiration for this idea comes from the observation that atypical samples tend to be miscalibrated and that understanding atypicality can provide insights into the reliability of predictions.",
            "opinion": "The proposed idea is that machine learning models should quantify both confidence and atypicality to improve the reliability of their predictions.",
            "innovation": "The key innovation is the introduction of atypicality-aware recalibration methods, which adjust model predictions based on the atypicality of inputs and classes."
        },
        "method": {
            "method name": "Atypicality-Aware Recalibration",
            "method abbreviation": "AAR",
            "method definition": "AAR is a method that recalibrates model predictions by considering the atypicality of inputs and classes, allowing for more reliable confidence scores.",
            "method description": "The core of AAR is to adjust model predictions based on the atypicality of the inputs, enhancing the reliability of predictions.",
            "method steps": [
                "Estimate the atypicality of the input and class.",
                "Group inputs based on their atypicality and confidence levels.",
                "Apply different recalibration strategies depending on the atypicality of the groups."
            ],
            "principle": "This method is effective because it allows the model to adjust its predictions based on the likelihood that the input is atypical, thereby reducing miscalibration and improving accuracy."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted across various datasets and models, including balanced and imbalanced classification tasks, using neural networks and large language models.",
            "evaluation method": "Performance was assessed using metrics such as Expected Calibration Error (ECE) and accuracy, comparing the proposed method against traditional calibration techniques."
        },
        "conclusion": "The results demonstrate that incorporating atypicality into model predictions significantly enhances reliability and performance, particularly for atypical inputs. The authors advocate for the inclusion of atypicality estimators in machine learning models to improve their predictive capabilities.",
        "discussion": {
            "advantage": "The AAR method improves the calibration and accuracy of predictions, particularly for atypical inputs, leading to better overall model reliability.",
            "limitation": "One limitation is that the method may not perform optimally in all scenarios, especially in cases where the training data is highly imbalanced or lacks diversity.",
            "future work": "Future research could explore more sophisticated methods for atypicality estimation and its applications in other areas of machine learning."
        },
        "other info": {
            "code repository": "https://github.com/mertyg/beyond-confidence-atypicality",
            "acknowledgments": "The authors thank various individuals for their contributions to the manuscript and acknowledge funding sources."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Atypicality is crucial for better uncertainty quantification and model performance."
        },
        {
            "section number": "2.1",
            "key information": "The problem is that traditional machine learning models often rely solely on confidence scores to assess the reliability of predictions, which can be inadequate, especially for atypical or rare inputs."
        },
        {
            "section number": "2.5",
            "key information": "The proposed idea is that machine learning models should quantify both confidence and atypicality to improve the reliability of their predictions."
        },
        {
            "section number": "3.4",
            "key information": "The key innovation is the introduction of atypicality-aware recalibration methods, which adjust model predictions based on the atypicality of inputs and classes."
        },
        {
            "section number": "6.1",
            "key information": "Atypicality-aware recalibration (AAR) is a method that recalibrates model predictions by considering the atypicality of inputs and classes, allowing for more reliable confidence scores."
        },
        {
            "section number": "7.1",
            "key information": "One limitation is that the method may not perform optimally in all scenarios, especially in cases where the training data is highly imbalanced or lacks diversity."
        },
        {
            "section number": "8",
            "key information": "Incorporating atypicality into model predictions significantly enhances reliability and performance, particularly for atypical inputs."
        }
    ],
    "similarity_score": 0.6995277181544393,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Beyond Confidence_ Reliable Models Should Also Consider Atypicality.json"
}