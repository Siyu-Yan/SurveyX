{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.04903",
    "title": "Concept Drift Detection using Ensemble of Integrally Private Models",
    "abstract": "Deep neural networks (DNNs) are one of the most widely used machine learning algorithm. DNNs requires the training data to be available beforehand with true labels. This is not feasible for many real-world problems where data arrives in the streaming form and acquisition of true labels are scarce and expensive. In the literature, not much focus has been given to the privacy prospect of the streaming data, where data may change its distribution frequently. These concept drifts must be detected privately in order to avoid any disclosure risk from DNNs. Existing privacy models use concept drift detection schemes such ADWIN, KSWIN to detect the drifts. In this paper, we focus on the notion of integrally private DNNs to detect concept drifts. Integrally private DNNs are the models which recur frequently from different datasets. Based on this, we introduce an ensemble methodology which we call 'Integrally Private Drift Detection' (IPDD) method to detect concept drift from private models. Our IPDD method does not require labels to detect drift but assumes true labels are available once the drift has been detected. We have experimented with binary and multi-class synthetic and real-world data. Our experimental results show that our methodology can privately detect concept drift, has comparable utility (even better in some cases) with ADWIN and outperforms utility from different levels of differentially private models. The source code for the paper is available \\hyperlink{https://github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models}{here}.",
    "bib_name": "varshney2024conceptdriftdetectionusing",
    "md_text": "# Concept Drift Detection using Ensemble of Integrally Private Models\nyush K. Varshney1[0000\u22120002\u22128073\u22126784] and Vicen\u00b8c Torra1[0000\u22120002\u2212\nAyush K. Varshney1[0000\u22120002\u22128073\u22126784] and Vicen\u00b8c Torra1[0000\u22120002\u22120368\u22128037]\n\u2212\u2212\u2212] Department of Computing Sciences Ume\u02daa University 90740, Sweden {ayushkv,vtorra}@cs.umu.se\n[\u2212\u2212\u2212][\u2212 Department of Computing Sciences Ume\u02daa University 90740, Sweden {ayushkv,vtorra}@cs.umu.se\nAbstract. Deep neural networks (DNNs) are one of the most widely used machine learning algorithm. DNNs requires the training data to be available beforehand with true labels. This is not feasible for many realworld problems where data arrives in the streaming form and acquisition of true labels are scarce and expensive. In the literature, not much focus has been given to the privacy prospect of the streaming data, where data may change its distribution frequently. These concept drifts must be detected privately in order to avoid any disclosure risk from DNNs. Existing privacy models use concept drift detection schemes such ADWIN, KSWIN to detect the drifts. In this paper, we focus on the notion of integrally private DNNs to detect concept drifts. Integrally private DNNs are the models which recur frequently from different datasets. Based on this, we introduce an ensemble methodology which we call \u2019Integrally Private Drift Detection\u2019 (IPDD) method to detect concept drift from private models. Our IPDD method does not require labels to detect drift but assumes true labels are available once the drift has been detected. We have experimented with binary and multi-class synthetic and real-world data. Our experimental results show that our methodology can privately detect concept drift, has comparable utility (even better in some cases) with ADWIN and outperforms utility from different levels of differentially private models. The source code for the paper is available here. 1\nKeywords: Data privacy \u00b7 Integral privacy \u00b7 Concept Drift \u00b7 Private drift \u00b7 Deep neural networks \u00b7 Streaming data.\n# 1 Introduction\nIn recent years, the interest in deep learning models has witnessed a steady increase, despite encountering various challenges such as explainability, privacy, and data dependency. To address these issues, significant advancements have been made, including approaches to enhance explainability [1], privacy-preserving techniques [2], adopting a data-centric perspective to facilitate model training with high-quality data, and more. However, limited attention has been given in the context of streaming data, which refers to the continuous arrival of data in\n1 Accepted for publication in MLCS co-located with ECML-PKDD 2023\nreal-world scenarios, often accompanied by the problem of concept drift. Concept drift implies that the statistical properties of the data may change over time, necessitating the model to adapt to these changes to ensure reliable predictions. Noisy data at one point of time may become useful data over time. These changes in data distributions can be due to various hidden factors. Handling of such drifts is a must and has been employed in many applications such as spam detection [3], demand prediction [4]. Learned models must have the ability to detect concept drifts and incorporate them by retraining on the new data. Three types of concept drifts have been shown in Fig. 1. Abrupt drifts are sudden changes in the data distribution. E.g. complete lockdown in many countries due to COVID-19 pandemic. Gradual drifts are the drifts which changes the distribution over time. E.g. in fraud detection system, fraudsters adapt according to the improving security policies in place. Incremental concept drift are the drifts where old concepts vanishes completely with time. E.g. after lifting COVID-19 lockdown, people may be hesitant to return to their normal behaviour.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d00/9d00d7a9-1a09-4fb4-9c9f-81be9cb3b97b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: Types of drifts in the data</div>\nIn the literature of concept drift detection, there has been several algorithms which can detect concept drifts such as Adaptive windowing (ADWIN) [5] and its variant, and Kolmogorov-Smirnov Windowing (KSWIN). These are the two prominent drift detection methods used in the streaming settings. To detect drifts, these techniques were originally proposed assuming the availability of true labels which is unrealistic in most real-world assumptions. ADWIN employs two windows, one fixed size and one variable size, which slide over the incoming data stream. The fixed size window keeps the most recent points and the variable size window keeps the earlier points. If the statistics of the two windows differs significantly then ADWIN indicates that the drift has been detected. In case of DNNs, training requires huge amount of data and acquisition of ground truth to detect drift can be very costly. A recently proposed uncertainty drift detection scheme [6] detects drift during inference without the availability of true labels. It computes values for prediction uncertainty using dropout in the DNNs and uses entropy of these uncertainty values to detect drifts. Another approach to get prediction uncertainty is through the ensemble of DNN models. Different DNNs produce different probabilities during predictions and the overall uncertainty in their predictions can be used to detect the drift. In the literature, almost none of the approaches focus on the privacy perspective of drift detection.\nPrivacy is a crucial factor to take into account in concept drift detection as data is often sensitive. There exists many privacy models such as k-anonymity [7], differential privacy (DP) [8], integral privacy [9] and others for static environment but their counter-parts for online learning are rather limited. For online learning, k-anonymity [10] tries to protect against identity disclosure by guaranteeing k-anonymity for addition, deletion, and updating the records but may fail to protect attribute disclosure; differential privacy (DP) perturbs the data or the model in order to generate privacy-preserving outputs against the disclosure of sensitive information. Even though DP provides theoretically sound privacypreserving models, it has a number of practical drawbacks. For instance, when aiming for high privacy (small \u03f5), the amount of noise added can become very high. Moreover, there is a finite privacy budget for multiple searches, and high sensitivity queries demand a bigger amount of noise. DP may struggle with the privacy budget when the data distribution changes frequently. You may end up loosing utility or privacy or both in the long run. Also, the addition of a lot of noise to the output can make machine learning models less useful. Most of the privacy approaches in the online learning literature focuses on either storing the data or predicting the output privately. None or very few approaches in literature focuses on detecting drifts privately. In our approach, we have considered Integral Privacy as an alternative to DP to generate high utility, privacy-preserving machine learning models. Integrallyprivate models provide sound defence against membership inference attacks and model comparison attacks. A membership inference attack is about getting access to the records used in the training process. On similar lines, a model comparison attack gives intruder access to the complete training set or to a huge portion of the training set through intersectional analysis. A machine learning model is integrally private [9] if it can be generated by multiple disjoint datasets. For an intruder whose aim is to do membership inference attacks or model comparison attacks, integrally-private models create ambiguity as the models are generated by multiple disjoint datasets. It has been proven in [11] that under some conditions it is possible to obtain, with probability close to one, the same parameter updates for a model with multiple minimatchs. They also find that a small fraction of a dataset can also lead to good results. One of the first works which shows the framework for model comparison attack and the defence by integral privacy for decision trees was given in [12]. The authors generate the complete model space and return the integrally private decision tree models which have approximately same model parameters. Generating complete (or approximately complete) model space can be a very computationally intensive task for a dataset with only few thousand instances. For DNNs, generating model space and comparing models to find integrally private models can be tricky. This is due to the fact that for a given layer of two different models, equivalent neurons can be placed in different positions. Also, due to huge number of learning parameters in DNNs, there can be very few recurring models. In order to overcome these challenges for DNNs, a relaxed variant of integral privacy, \u2206-Integral privacy, was proposed in [13]. \u2206-Integral\nprivacy (\u2206-IP) considers models which are at most \u2206distance apart, and then recommends the mean of these models (in the \u2206range) as the integrally private model. The \u2206-IP algorithm can recommend up to X number of integrally private models which can be used as an ensemble of private models to detect concept drifts in streaming data. In this paper, we propose a methodology for drift detection through an ensemble of \u2206-integrally private models. We compute an ensemble of \u2206-IP models and use them to compute a measure of prediction uncertainty. This prediction uncertainty of \u2206-IP models on the incoming datastream is used to detect concept drift. Our methodology only requires true labels to recompute the \u2206-IP models once a drift has been detected. We also present the probabilistic analysis for the recurring models. Our theoretical analysis is inspired from the work in [11] which focuses on forging a minibatch. In our case, the analysis focus on learning similar parameters after complete training. Our experimental setup shows results for ANN (one hidden layer with 10 neurons) and DNN of 3 hidden layers (10-20-10 hidden layer architecture). We evaluate our proposed methodology for 3 real-world dataset and 4 synthetic dataset. We have also compared our results with different levels of privacy in DP models. We show that our approach outperforms the DP alternatives. We find that ensemble of integrally private models can successfully detect concept drifts while maintaining the utility of non-private models. The rest of the paper is organized as follows. Section 2 describes the background for the proposed drift detection methodology. Section 3 describes our proposed work. Section 4 gives the experimental analysis. The paper finishes with some conclusions and future work.\n# 2 Background\nIn this section we describe the major concepts that are needed in this work\n# In this section we describe the major concepts that are needed in this work.\n# 2.1 Uncertainty in Neural Networks\nUnderstanding the uncertainty of a model is essential to understand the model\u2019s confidence. In DNNs, class probabilities can not be the proxy for model\u2019s confidence. For unseen data, DNNs may give high probability even when the predictions are wrong. This can be the case in concept drifts i.e, the prediction may be uncertain but the system can give high class probability. Ensemble methods find the uncertainty using predictions from the family of DNNs. Here you train multiple DNNs with different initializations. In this way you generate a set of confidence parameters from multiple DNNs, and the variance of the output can be interpreted as the model uncertainty. In our work, we estimate the model uncertainty using an ensemble of private models. With drift in estimated uncertainty as an indicator for concept drift, we can employ drift detection schemes such as ADWIN, KSWIN to detect concept drift.\nIntegral privacy [9] is a privacy model which provides defense against model comparison attacks and membership inference attacks. In a model Comparison\nAttack [12], [13], an intruder aims to get access to the sensitive information or do membership inference analysis by comparing the model parameters. A model comparison attack assumes that the intruder has access to the global model M trained using algorithm A on the training set X a subset of the population D, and some background information S\u2217(\u2286D). The intruder wants to get the maximum (or total) number of records used in the training process. I.e. the intruder wants to maximize access to X. The intruder draws a number of samples S1,S2,...,Sn from D and compares the model generated by each Si with the global model M. Then, the intruder selects the Si corresponding to the most similar model to M and hence guesses the records used in the training. In case that there are multiple models similar to M, the intruder can do intersectional analysis for membership inference. That is, find common data records which lead to the model M. In case of DNNs, model comparison can be tricky as highlighted before in [13]. The comparison between models is done by comparing each layer and neurons in respective layers. In order to defend against such attacks, integral privacy requires you to chose a model which recurs from different disjoint datasets. Disjoint datasets are needed to avoid intersectional analysis. In this way an intruder cannot identify the training set because multiple training sets lead to the same model. As explained in Section 1, due to the huge number of parameters in DNNs there are very few recurring models. \u2206-IP relaxes the equivalence relation between neurons. It allows the two models to be considered as equal if neurons in each layer of the respective model are at most \u2206distance apart. Formally, \u2206-IP can be defined as follows. \u2206-Integral Privacy Let D be the population, S\u2217\u2282D be the background knowledge, and M \u2282M be the model generated by an algorithm A on an unknown dataset X \u2282D. Then, let Gen\u2217(M,S,\u2206) represent the set of all generators consistent with background knowledge S\u2217and model M or models at most \u2206different. Then, k-anonymity \u2206-IP holds when Gen\u2217(M,S,\u2206) has atleast k-elements and \u22c2  = \u2205\n# 3 Proposed Methodology\nIn this section, we provide the details of our proposed methodology which we call Integrally private drift detection (IPDD) scheme. Our proposed IPDD methodology detects drifts with unlabeled data but assumes that true labels are available on request. Our approach is based on the detection of concept drifts from the measure of uncertainty in prediction by ensemble of private models. Previous works [14] [6] show that prediction uncertainty from DNN is correlated with prediction error. We argue on similar lines and use drift in prediction uncertainty as a proxy for detecting concept drift. We use Shanon entropy to evaluate the uncertainty over different c class labels. Then any change detection algorithm such as ADWIN can be employed to detect drifts using this uncertainty measure. We chose ADWIN as it works well\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe06/fe0653da-e8d6-45ac-9e8b-e8f6e9eb94cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7edc/7edc3324-f909-445e-b3ad-3f3f4aadcf75.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d89/7d8916f9-225e-42ae-87a9-72de36e7c696.png\" style=\"width: 50%;\"></div>\nwith real-valued inputs. The flowchart of the methodology is shown in Fig. 2. First k integrally private models are computed from the initial available training data. With incoming data instances, we predict the output and input the prediction uncertainty from each of the k private models to ADWIN. If drift has been detected, true labels are requested and the training data must be updated\nAlgorithm 2 Drift detection using \u2206-Integrally private models\nInputs: D - Dataset\nAlgorithm:\ntraining data = Initial data(D)\n\u25b7Initial Data to train private DNNs\nPrivate Models = Algorithm 1(training data)\nwhile D has elements do\n\u25b7While stream has incoming data\nReceive incoming data xt\npred, uncertainty \u2190Private Models.predict(xt)\nAdd uncertainty to ADWIN\nif ADWIN detects drift then\nRequest true labels yt for xt\nUpdate training data with xt,yt\nPrivate Models = Algorithm 1(training data)\nend if\nend while\nwith new instances. Here, our methodology does not require true labels to detect concept drift. New private models are computed from the updated training data. If the training data exceeds the threshold, records are removed on a first-come, first-served basis. In case there is no drift, then the prediction for new instances continues. Algorithm 1 describes the computation for k private models. First, samples (i.e., sets of records) are generated from the training data in such a way that pairs of samples have empty intersection (i.e., they do not share any record). Models for these samples are computed using the same initialization. Models within \u2206 distance apart are kept in the same bucket. Buckets are sorted in descending order according to the number of models in each bucket. The algorithm then returns the mean of the top k recurring set of models as an ensemble of k integrally private models. Algorithm 2 describes how to detect drifts privately. First, it uses initial available data as training data and computes private models using Algorithm 1 on the training data. Predictions on new incoming instances are used to compute the uncertainty measure and to see if the drift is detected using ADWIN. If the drift is detected, true labels must be requested, then training data must be updated and the private models are recomputed on a new training data. This process continues as long as the new data is available for prediction.\nIn this section, we present the probabilistic analysis for the recurrence of DNNs. DNNs are trained using mean samplers such as SGD, Adam etc. This analysis is inspired by the forgeability analysis done in [11]. The analysis in [11] computes the probability of forging a single minibatch while we focus on probabilistic analysis of learning the same model parameters after learning from different training\ndata. Let us consider a set of disjoint datasamples, D1,D2,...,Dm, i.i.d. (independent and identically distributed) sampled from a given N-dimensional dataset D with some distribution. Here, each of the Di is composed of b minibatches \u02c6x = x1,x2,...,xb. M1,M2,...,Mm be the DNN models we want to train which have the same initialization. The update rule looks like g(w, \u02c6x) = 1 b \u2211b i=1 g(w,xi). The update rule g(w,x) can be seen as a random variable with mean \u00b5 and \u03c32 (= \u2211N i=1 \u03c32 i , where \u03c32 i is the covariance of the ith component of a random variable x sampled with distribution D) as the trace of the covariance matrix. The mean sampler for the batch \u02c6x, g(w, \u02c6x) is still \u00b5 ( 1 b \u2211b i=1 g(w,xi) = 1 b \u2217b\u00b5) but individual variance will get the 1 b i.e. now the trace of the covariance matrix is 1 b\u03c32. Since the data samples are i.i.d sampled from D and each xi is i.i.d sampled from data samples, then xi follows the same distribution of D. Then by Markov\u2019s inequality we can say that,\n# P(\u2223g(w, \u02c6x) \u2212\u00b5\u22232 \u2265\u2206) = P(\u2223g(w, \u02c6x) \u2212\u00b5\u22232 2 \u2265\u22062) \u2264E(\u2223g(w, \u02c6x) \u2212\u00b5\u2223 2) \u22062\n(\u2223() \u2212\u2223 \u2265) =(\u2223() \u2212\u2223  \u2265) \u2264 Here the first equality is true by the property of monotonicity of squares and the second is Markov\u2019s inequality. Note that E(\u2223g(w, \u02c6x) \u2212\u00b5\u22232 2) is just the trace of the covariance matrix ( 1 b\u03c32). Then, we can write:\nFig. 3: Two models Mj,Mk at most \u22062 distance apar from \u00b5 with probability de fined in Eq. (5) \u21d2(\u2223() \u2212\u2223  \u2264) \u2265 \u2212 Let us consider two models Mj and Mk, training on data samples Dj and Dk with \u02c6xj, \u02c6xk. From Eq. (5), we can say P(\u2223g(w, \u02c6xj)\u2212\u00b5\u22232 2 \u2264\u22062) \u2265 (1 \u2212 \u03c32 b\u22062 ) and P(g(w, \u02c6xk) \u2212\u00b5\u22232 2 \u2264\u22062) \u2265(1 \u2212 \u03c32 b\u22062 ) at ith epoch As demonstrated in Fig. 3, if g(w, \u02c6xj),g(w, \u02c6xk) are in the \u22062 ball of \u00b5 with probability defined in Eq. (5) then with probability \u2265(1 \u2212 \u03c32 b\u22062 )2 we can say both models are utmost 2\u22062 distant. I.e. P(\u2223g(w, \u02c6xj)\u2212g(w, \u02c6xk)\u2223\u22642\u22062) \u2265(1\u2212\u03c32 b\u22062 )2. Then, the probability that out of m models there exists two models which are in the \u22062 ball of \u00b5 would be:\n(\u2223() \u2212()\u2223  \u2264) \u2265 \u2211 = (  )( \u2212  ) (  ) This is equivalent to having at least 2 models out of m in the 2\u22062 ball of \u00b5. The probability that m models are in the 2\u22062 ball of \u00b5 would be:\n(2)\n(4)\n(\u2223() \u2212()\u2223  \u2264) \u2265( \u2212  ) Equation (6) and (7) represents the probability of weights updating in the 2\u22062 ball of \u00b5 for a single epoch. For T iterations, the probability that there exists a model having weight updates in the 2\u22062 ball of \u00b5 is at least (\u2211m r=2 (m r )( \u03c32 b\u22062 )m\u2212r(1 \u03c32 b\u22062 )r)T . After T epochs, the probability of m models to be in the 2\u22062 ball of \u00b5 will be at least ((1 \u2212 \u03c32 b\u22062 )m)T . So, for samples sampled i.i.d. from some dataset, we can conclude that the lower bound for the probability that there exists recurrent models within 2\u22062 ball is at least (\u2211m r=2 (m r )( \u03c32 b\u22062 )m\u2212r(1\u2212\u03c32 b\u22062 )r)T . In addition, the probability that all the m models are in the 2\u22062 ball of \u00b5 is at least ((1 \u2212 \u03c32 b\u22062 )m)T . From this discussion, we have the following theorems. Theorem 1. If D1,D2,...,Dm are i.i.d samples from the dataset D with some distribution and b is the number of minibatches used for training in each of T epochs. Then under similar training environment i.e. same initialization, learning rate, etc. with probability greater than (\u2211m r=2 (m r )( \u03c32 b\u22062 )m\u2212r(1 \u2212 \u03c32 b\u22062 )r)T , the model will recur.\n(\u2223() \u2212()\u2223  \u2264) \u2265( \u2212  ) Equation (6) and (7) represents the probability of weights updating in the 2\u22062 ball of \u00b5 for a single epoch. For T iterations, the probability that there exists a model having weight updates in the 2\u22062 ball of \u00b5 is at least (\u2211m r=2 (m r )( \u03c32 b\u22062 )m\u2212r(1\u2212 \u03c32 b\u22062 )r)T . After T epochs, the probability of m models to be in the 2\u22062 ball of \u00b5 will be at least ((1 \u2212 \u03c32 b\u22062 )m)T . So, for samples sampled i.i.d. from some dataset, we can conclude that the lower bound for the probability that there exists recurrent models within 2\u22062 ball is at least (\u2211m r=2 (m r )( \u03c32 b\u22062 )m\u2212r(1\u2212\u03c32 b\u22062 )r)T . In addition, the probability that all the m models are in the 2\u22062 ball of \u00b5 is at least ((1 \u2212 \u03c32 b\u22062 )m)T . From this discussion, we have the following theorems.\n (( \u2212  )) Theorem 1. If D1,D2,...,Dm are i.i.d samples from the dataset D with some distribution and b is the number of minibatches used for training in each of T epochs. Then under similar training environment i.e. same initialization, learning rate, etc. with probability greater than (\u2211m r=2 (m r )( \u03c32 b\u22062 )m\u2212r(1 \u2212 \u03c32 b\u22062 )r)T , the model will recur.\n  Theorem 2. With the above mentioned properties, a model satisfies k-anonymo integral privacy with probability atleast (\u2211m r=k (m r )( \u03c32 b\u22062 )m\u2212r(1 \u2212 \u03c32 b\u22062 )r)T\n (\u2211 = (  )(  )\u2212( \u2212  )) Proof: See Eq. (6) for proof. k-Anonymity integral privacy is equivalent to having at least k models out of m in the \u2206ball of \u00b5. Remark on the choice of \u2206,m: In order to generate higher k-Anonymity integrally private models, from theorem 2 we can say that increasing the number of i.i.d samples (m), b (Number of batches used in each epochs) and \u2206(the distance value) increases the probability of getting recurrent models. Role of initialization: The probabilistic analysis presented here gives you the lower bound that the model will recur from the samples having similar distribution. The probability can further improve when models are initialized with the same weight as the learning from similar dataset would result in the similar learning for the models.\n# 4 Experiments\nIn this section, we present the experimental results for our proposed methodology. We will show that our methodology performs well with Categorical, Real, and Integer data with arbitrary number of classes. We perform our experiments on 3 real-world datasets namely Cover type (CovType), Electricity, and Susy dataset [15]. We also run our experiments on artificially generated Sine data and Insects data with abrupt, gradual and incremental drifts [16]. Table 1 shows the number of instances and other details of these datasets.\n(5)\nDataset\n# instances # attribute Data type # classes\nCovType\n581012\n54\nCategorical\nInteger\n7\nElectricity\n45312\n8\nReal\nInteger\n2\nSusy\n5000000\n18\nReal\n2\nSine\n200000\n4\nReal\n2\nInsects ab\n52848\n33\nReal\n6\nInsects grad\n24150\n33\nReal\n6\nInsects incre\n57018\n33\nReal\n6\nTable 1: Details of the used Datasets\nFor our experiments, we have randomly considered a NN with a single hidden layer (10 neurons) architecture (We will call this architecture ANN) and a three hidden layer NN architecture with 10-20-10 number of neurons (we will call it DNN). For our experimental purpose we have chosen \u2206= 0.01 and ADWIN parameter, \u03b4 = 0.001. For all the datasets, we have initially trained ANN and DNN over 10% of the dataset, and then stream is evaluated with 2% of the dataset at each time instance. We compare our results (Integrally private drift detection, IPDD) with No retraining (No retrain), ADWIN with unlimited label availability (ADWIN unlim), and ADWIN with limited labels (ADWIN lim). We have used three levels of differentially private models: high privacy (\u03f5 = 0.1) under limited label availability (DP 01), moderate privacy (\u03f5 = 0.5) under limited label availability (DP 05) and low privacy (\u03f5 = 1.0) under limited label availability (DP 10). All the results have been computed for ANN as well as DNN. The No retraining model approach does not check for drifts, it trains the model with initial data once and only does the prediction for the rest of the data stream. For ADWIN unlim we assume it has access to all the true labels of the incoming data stream and it detects drifts using the true labels only. The ADWIN lim can have true labels upon request but detect the drifts using uncertainty through the ADWIN model. Similar settings were assumed for DP 01, DP 05, DP 10 and IPDD. We can observe that our methodology IPDD has better or comparable accuracy score for both ANN and DNN. Table 2 provides the accuracy of the learned models. IPDD performs better than its counterparts for CovType and Electricity datasets, it has comparable accuracy score for Insects ab and comparable results with ADWIN unlim method. Table 3 provides the results for Mathews correlation coefficient (mcc) in the range [\u22121,1] (higher the better). MCC is a reliable statistical rate which assigns high value to a classifier if it performs good in all four confusion matrix categories. In comparison with differentially private models, IPDD performs much better than all three levels of differential privacy for all datasets except Insects grad and Insects incre. For ANN, IPDD performs performs better for CovType dataset and has comparable mcc rate with the rest. In case of DNNs, IPDD performs better than its counterparts for\nElectricity, Susy and Insects ab datasets; and performs comparable results for the rest of the datasets. Table 4 shows the score for the area under the curve (auc score). Auc score is the probability that a model ranks a random positive instance higher than a random negative instance. Table 4 highlights that auc score for IPDD\u2019s ANN and DNN performs better than its counterparts in case of all the datasets except Insect grad and Insect incre dataset.\nWe observed that with the addition of noise DP models may struggle to detect drifts. Table 5 shows the number of drifts detected by each method. It highlights that DP models at times may detect very few drifts because of the noise. On the other hand, IPDD detects comparable drifts to ADWIN unlim and ADWIN lim for both ANN and DNN. Table 5 also highlights that proposed IPDD does not detect unnecessary drifts i.e. IPDD does not necessary detect any drift when there is none (counterparts of IPDD does not detect any drift). As expected when the noise for DP models decreases, more drifts were detected by DP models as shown in Fig. 4. In most of the cases, differentially private models does not perform as good as IPDD mod-\nels even when the \u03f5 is very high (very low privacy). This is shown in Fig. 5. It compares the accuracy score between DNN model of DP and IPDD for all the datasets. Fig. 5a, 5b, 5c, 5d, 5e, and 5f highlights that even though the accuracy score improves for DP models, IPDD still performs better than DP. Only in case of Fig. 5g the DP model has slightly better accuracy score than IPDD even in case of high privacy. Section 3.1 shows the probabilistic analysis on the lower bound of the recurrence of Integrally private models. As discussed in the remarks of Section 3.1, the higher the value of \u2206the higher the number k-anonymity in integrally private models. For models with same initialization trained on 100 i.i.d samples (D1,D2,...,D100) from Sine dataset, the k in k-anonymity Integral privacy has been plotted against increasing \u2206in Fig. 6a. As can be seen increasing the \u2206 value leads to the higher value of k in k-anonymity Integral privacy. Similarly, we can see in Fig. 6b that for a fixed \u2206= 0.01, increasing the number of i.i.d samples leads to a higher value of k in k-Anonymity integral privacy. It is important to highlight the distinction between k-anonymity and ensemble of k models chosen with k-anonymity Integral privacy. Simply, in k-Anonymity integral privacy, there exists a bucket which has at least k models while we require an ensemble of k such buckets. We observe in Fig. 7a that for a fixed \u2206= 0.01, increasing the number of i.i.d samples also leads to the higher k in k-Anonymity integral privacy. In cases where all the models are clustered to only one IP model, generating an ensemble of such models can be tricky. An easier way to avoid this problem is to generate\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4803/4803bcfe-907d-4e9a-a2c3-3bcb89aa4dac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4: Drift detected by different \u03f5-differentially private models</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd39/cd39c0f6-ac74-4c39-ab2a-3379cb598092.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Comparison of the accuracy score between differential privacy and integral privacy: (a) CovType (b) Electricity (c) Susy (d) Sine (e) Insect ab (f) Insect grad (g) Insect incr.</div>\nFig. 5: Comparison of the accuracy score between differential privacy and integral privacy: (a) CovType (b) Electricity (c) Susy (d) Sine (e) Insect ab (f) Insect grad (g) Insect incr.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/76a8/76a85d38-beeb-4a28-8201-1536ea83f995.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\n<div style=\"text-align: center;\">Fig. 6: K-anonymity integral privacy against (a) increasing \u2206(b) increasing the number of i.i.d samples</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a1e2/a1e21ea6-7222-41f4-a6e1-dda6e2ffe320.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7: Number of integral private models in an ensemble against (a) increasing the number of i.i.d samples (b) increasing the number of different initialization. (c) k-anonymity against different initialization</div>\nSine\n0.9505 0.9536 0.93425 0.9677 0.9421 0.9577 0.6204 0.6134 0.7600 0.6447 0.8129 0.9005 0.9365 0.9369\nInsects ab\n0.4416 0.4434 0.5252 0.5237 0.2903 0.5348 0.1739 0.1666 0.2437 0.1667 0.2126 0.1667 0.5241 0.5331\nInsects grad 0.2810 0.2841 0.2293 0.2412 0.2221\n0.232 0.2104 0.2206 0.2238 0.2192 0.2213 0.2224 0.2227 0.2368\nInsects incr 0.2181 0.2475 0.2282 0.2475 0.2335 0.2437 0.2140 0.2089 0.1914 0.2255 0.2143 0.2143 0.2022 0.2101\nTable 3: Mathews Correlation Coefficient.\nDataset\nNo retrain\nAdwin unlim\nAdwin lim\nDP 01\nDP 05\nDP 10\nIPDD\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nCovType\n0.0816 0.1940 0.1618 0.2941 0.1421 0.2140 0.1012 0.0599 0.0183 0.0261 -0.0158 -0.0014 0.3575 0.2871\nElectricity\n0.2652\n0.0\n0.1459 0.2101 0.1559 0.1898 0.0906 0.1120 0.1615 0.0033 0.1576\n0.1817 0.2593 0.2837\nSusy\n0.5936 0.5936 0.5936 0.5936 0.5936 0.5936 0.3608 0.4782 0.4627 0.5411 0.5050\n0.5623\n0.5591 0.5902\nSine\n0.8995 0.9059 0.8667 0.9345 0.8824 0.9142 0.1879 0.1974 0.5105 0.3335 0.6670\n0.8024 0.8726 0.8722\nInsects ab\n0.3841 0.3849 0.4412 0.4398 0.1274 0.4528 0.0129\n0.0\n0.1164 0.0014 0.0588\n0.0\n0.4397 0.4505\nInsects grad 0.1356 0.1181 0.0967 0.1586 0.0858 0.1291 0.0675 0.1176 0.1100 0.1333 0.1248\n0.1258\n0.1112 0.1181\nInsects incr 0.0628 0.0978 0.0746 0.0978 0.0809 0.0962 0.0834 0.0755 0.0356 0.0924 0.0902\n0.0954\n0.0448 0.0553\nTable 2: Accuracy Score.\nDataset\nNo retrain\nAdwin unlim\nAdwin lim\nDP 01\nDP 05\nDP 10\nIPDD\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nCovType\n0.4182 0.3552 0.4437 0.4582 0.4246 0.4012 0.4789 0.3996 0.4434 0.3787 0.4563 3688 0.5836 0.4621\nElectricity\n0.5754 0.5790 0.5928 0.6154 0.5953 0.6096 0.5875 0.5911 0.5994 0.5729 0.5978 0.6083 0.6281 0.6343\nSusy\n0.7985 0.7985 0.7985 0.7985 0.7985 0.7985 0.6844 0.7391 0.7327 0.7727 0.7539 0.7822 0.7729 0.7934\nSine\n0.9505 0.9536 0.93425 0.9677 0.9421 0.9577 0.6204 0.6134 0.7600 0.6447 0.8129 0.9005 0.9365 0.9369\nInsects ab\n0.4416 0.4434 0.5252 0.5237 0.2903 0.5348 0.1739 0.1666 0.2437 0.1667 0.2126 0.1667 0.5241 0.5331\nInsects grad 0.2810 0.2841 0.2293 0.2412 0.2221\n0.232 0.2104 0.2206 0.2238 0.2192 0.2213 0.2224 0.2227 0.2368\nInsects incr 0.2181 0.2475 0.2282 0.2475 0.2335 0.2437 0.2140 0.2089 0.1914 0.2255 0.2143 0.2143 0.2022 0.2101\nTable 3: Mathews Correlation Coefficient.\nDataset\nNo retrain\nAdwin unlim\nAdwin lim\nDP 01\nDP 05\nDP 10\nIPDD\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nCovType\n0.0816 0.1940 0.1618 0.2941 0.1421 0.2140 0.1012 0.0599 0.0183 0.0261 -0.0158 -0.0014 0.3575 0.2871\nElectricity\n0.2652\n0.0\n0.1459 0.2101 0.1559 0.1898 0.0906 0.1120 0.1615 0.0033 0.1576\n0.1817 0.2593 0.2837\nSusy\n0.5936 0.5936 0.5936 0.5936 0.5936 0.5936 0.3608 0.4782 0.4627 0.5411 0.5050\n0.5623\n0.5591 0.5902\nSine\n0.8995 0.9059 0.8667 0.9345 0.8824 0.9142 0.1879 0.1974 0.5105 0.3335 0.6670\n0.8024 0.8726 0.8722\nInsects ab\n0.3841 0.3849 0.4412 0.4398 0.1274 0.4528 0.0129\n0.0\n0.1164 0.0014 0.0588\n0.0\n0.4397 0.4505\nInsects grad 0.1356 0.1181 0.0967 0.1586 0.0858 0.1291 0.0675 0.1176 0.1100 0.1333 0.1248\n0.1258\n0.1112 0.1181\nInsects incr 0.0628 0.0978 0.0746 0.0978 0.0809 0.0962 0.0834 0.0755 0.0356 0.0924 0.0902\n0.0954\n0.0448 0.0553\nTable 4: Auc Score.\nDataset\nNo retrain\nAdwin unlim\nAdwin lim\nDP 01\nDP 05\nDP 10\nIPDD\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nANN\nDNN\nCovType\n0.7349 0.8801 0.7985 0.9058 0.7982 0.8596 0.5188 0.5197 0.5679 0.5422 0.5910 0.5293 0.8881 0.9087\nElectricity\n0.6195\n0.5\n0.5209 0.5508 0.5239 0.5433 0.5261 0.5430 0.5299 0.5004 0.5277 0.5424 0.6311 0.6431\nSusy\n0.7923 0.7923 0.7923 0.7923 0.7923 0.7923 0.6751 0.7275 0.7217 0.7674 0.7451 0.7741 0.7582 0.7828\nSine\n0.9492 0.9531 0.9311 0.9677 0.9403 0.9565 0.5769 0.5776 0.7247 0.5979 0.8156 0.8912 0.9381 0.9339\nInsects ab\n0.7989\n0.80\n0.8469 0.8587 0.6310 0.8469 0.5829 0.5137 0.5966 0.5749 0.5528 0.5516 0.8474 0.8581\nInsects grad 0.6283 0.6206 0.6190 0.6068 0.6024 0.6118 0.5377 0.6220 0.5800 0.6067 0.5779 0.6219 0.5508 0.5956\nInsects incr 0.5439 0.5795 0.5470 0.5799 0.5577 0.5730 0.6026 0.5841 0.6035 0.6245 0.6044 0.6161 0.5249 0.5362\nTable 5: Number of Drifts detected by each algorithm.\nDataset\nNo retrain Adwin unlim Adwin lim\nDP 01\nDP 05\nDP 10\nIPDD\nANN DNN ANN DNN ANN DNN ANN DNN ANN DNN ANN DNN ANN DNN\nCovType\n0\n0\n35\n35\n31\n32\n24\n29\n24\n31\n18\n31\n35\n33\nElectricity\n0\n0\n37\n37\n42\n39\n13\n14\n8\n6\n15\n16\n23\n24\nSusy\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nSine\n0\n0\n28\n28\n33\n28\n27\n31\n27\n21\n28\n28\n17\n14\nInsects ab\n0\n0\n24\n24\n23\n17\n0\n0\n5\n0\n12\n0\n16\n14\nInsects grad\n0\n0\n18\n18\n16\n17\n5\n5\n5\n5\n5\n7\n14\n15\nInsects incr\n0\n0\n0\n0\n16\n14\n7\n5\n8\n6\n6\n7\n15\n19\nan ensemble of k-anonymity models using different initializations. The reason for this could be attributed to the comparable learning process when using similar training data. For 100 i.i.d samples of Sine data, in Fig. 7b, x-axis shows the number of different initialization and y-axis shows the number of different IP models in an ensemble. It is important to note here that for 100 samples if the number of IP models increases in an ensemble, k-anonymity of each IP model will decrease as depicted in Fig. 7c.\n# 4.1 Limitations of our approach:\nThe analysis of our method as well as our experiment permits us to state the following.\n1. Generating k-anonymous integrally private models requires training on large number of samples which is a time consuming process. The proposed IPDD methodology has running time as the cost of privacy. 2. As shown in Section 3.1, the generation of integrally private models is a probabilistic approach and depends on the samples selected. That is, different runs can provide different results.\n# 5 Conclusion and Future work\nIn this paper we have presented a private drift detection methodology called \u2019Integral Privacy Drift Detection\u2019 (IPDD). Our methodology detects drifts using an ensemble of k-anonymity integrally private models. Simply, we generate an ensemble of k models which are recurring from multiple disjoint datasets. Our methodology does not require the ground truth to detect concept drift but assumes they are available for retraining. We find that our methodology can successfully detect concept drifts while maintaining the utility of non-private models. It is useful in generating models which have comparable (better in some cases) accuracy score, mcc score and auc score against ADWIN with unlimited label availability and limited label availability. In comparison with its differentially private counterpart, IPDD perfoms significantly better in most of the cases. As shown above different parameters can lead to different levels of privacy. It can also affect the number of drifts detected and the utilty of the model. Fine-tuning of these parameters for each application is an interesting direction for future work. Furthermore, extension of our work for non-i.i.d. samples would be an interesting future direction. Acknowledgement: This work was partially supported by the Wallenberg Al, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by the supercomputing resource Berzelius provided by National Supercomputer Centre at Link\u00a8oping University and the Knut and Alice Wallenberg foundation.\n# References\n[1] P. Schwab and W. Karlen, \u201cCxplain: Causal explanations for model interpretation under uncertainty,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019. [2] R. Shokri and V. Shmatikov, \u201cPrivacy-preserving deep learning,\u201d in Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, 2015, pp. 1310\u20131321. [3] J. Gama, P. Medas, G. Castillo, and P. Rodrigues, \u201cLearning with drift detection,\u201d in Advances in Artificial Intelligence\u2013SBIA 2004: 17th Brazilian Symposium on Artificial Intelligence, Sao Luis, Maranhao, Brazil, September 29Ocotber 1, 2004. Proceedings 17, Springer, 2004, pp. 286\u2013295. [4] I. \u02c7Zliobait\u02d9e, M. Pechenizkiy, and J. Gama, \u201cAn overview of concept drift applications,\u201d Big data analysis: new algorithms for a new society, pp. 91\u2013114, 2016. [5] A. Bifet and R. Gavalda, \u201cLearning from time-changing data with adaptive windowing,\u201d in Proceedings of the 2007 SIAM international conference on data mining, SIAM, 2007, pp. 443\u2013448. [6] L. Baier, T. Schl\u00a8or, J. Sch\u00a8offer, and N. K\u00a8uhl, \u201cDetecting concept drift with neural network model uncertainty,\u201d arXiv preprint arXiv:2107.01873, 2021. [7] P. Samarati, \u201cProtecting respondents identities in microdata release,\u201d IEEE transactions on Knowledge and Data Engineering, vol. 13, no. 6, pp. 1010\u20131027, 2001. [8] C. Dwork, \u201cDifferential privacy,\u201d in Automata, Languages and Programming, M. Bugliesi, B. Preneel, V. Sassone, and I. Wegener, Eds., Berlin, Heidelberg: Springer Berlin Heidelberg, 2006, pp. 1\u201312. [9] V. Torra, G. Navarro-Arribas, and E. Galv\u00b4an, \u201cExplaining recurrent machine learning models: Integral privacy revisited,\u201d in International Conference on Privacy in Statistical Databases, Springer, 2020, pp. 62\u201373. 10] J. Salas and V. Torra, \u201cA general algorithm for k-anonymity on dynamic databases in Data privacy management, cryptocurrencies and blockchain technology, Springer 2018, pp. 407\u2013414. 11] A. Thudi, H. Jia, I. Shumailov, and N. Papernot, \u201cOn the necessity of auditable algorithmic definitions for machine unlearning,\u201d in 31st USENIX Security Symposium (USENIX Security 22), 2022, pp. 4007\u20134022. 12] N. Senavirathne and V. Torra, \u201cIntegrally private model selection for decision trees,\u201d computers & security, vol. 83, pp. 167\u2013181, 2019. 13] A. K. Varshney and V. Torra, \u201cIntegrally private model selection for deep neural networks,\u201d DEXA 2023, DOI: https://doi.org/10.21203/rs.3.rs-2944008/v1, 2023. 14] A. Kendall and Y. Gal, \u201cWhat uncertainties do we need in bayesian deep learning for computer vision?\u201d Advances in neural information processing systems, vol. 30, 2017. 15] D. Dua and C. Graff, UCI machine learning repository, 2017. [Online]. Available: http://archive.ics.uci.edu/ml. 16] V. M. Souza, D. M. dos Reis, A. G. Maletzke, and G. E. Batista, \u201cChallenges in benchmarking stream learning algorithms with real-world data,\u201d Data Mining and Knowledge Discovery, vol. 34, pp. 1805\u20131858, 2020.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of concept drift detection in streaming data while ensuring data privacy. Previous methods, such as ADWIN and KSWIN, assume the availability of true labels, which is often impractical. Therefore, a new approach is necessary to detect concept drifts without relying on labeled data.",
        "problem": {
            "definition": "The problem is the detection of concept drifts in streaming data where the statistical properties of the data change over time, making it difficult for models to maintain reliable predictions.",
            "key obstacle": "Existing methods for detecting concept drifts typically require true labels for effective drift detection, which is often not feasible in real-world applications."
        },
        "idea": {
            "intuition": "The idea stems from the observation that integrally private deep neural networks (DNNs) can be used to detect concept drifts without needing labeled data initially, only requiring true labels after a drift has been identified.",
            "opinion": "The proposed method, called Integrally Private Drift Detection (IPDD), utilizes an ensemble of integrally private models to detect concept drifts based on prediction uncertainty.",
            "innovation": "The primary innovation of the IPDD method lies in its ability to detect concept drifts without requiring labeled data, contrasting with existing methods that depend heavily on true labels."
        },
        "method": {
            "method name": "Integrally Private Drift Detection",
            "method abbreviation": "IPDD",
            "method definition": "IPDD is a methodology for detecting concept drifts using an ensemble of integrally private models that do not require labeled data for initial drift detection.",
            "method description": "The method leverages prediction uncertainty from multiple integrally private models to identify changes in data distribution over time.",
            "method steps": [
                "Train initial integrally private models using available data.",
                "As new data arrives, predict outputs and calculate uncertainty.",
                "Use uncertainty values to detect drifts through ADWIN.",
                "Upon detecting a drift, request true labels and update the training data.",
                "Recompute the integrally private models with the updated data."
            ],
            "principle": "The effectiveness of the method is based on the fact that prediction uncertainty correlates with concept drift, allowing for the detection of changes in data distribution without requiring true labels initially."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three real-world datasets (CovType, Electricity, and Susy) and synthetic datasets (Sine and Insects) with various types of concept drifts (abrupt, gradual, incremental).",
            "evaluation method": "The performance of the IPDD method was assessed by comparing its accuracy, Matthews correlation coefficient (MCC), and area under the curve (AUC) against other methods, including ADWIN and differentially private models."
        },
        "conclusion": "The results demonstrate that the IPDD methodology can effectively detect concept drifts while maintaining comparable or better utility than existing methods, particularly in scenarios where privacy is a concern.",
        "discussion": {
            "advantage": "The key advantages of the IPDD approach include its ability to detect concept drifts without requiring true labels initially and its strong performance compared to existing methods in terms of utility.",
            "limitation": "A limitation of the method is that generating integrally private models can be computationally intensive and may require a large number of samples.",
            "future work": "Future research could focus on optimizing the training process for integrally private models and extending the methodology to non-i.i.d. samples."
        },
        "other info": {
            "acknowledgement": "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem is the detection of concept drifts in streaming data where the statistical properties of the data change over time, making it difficult for models to maintain reliable predictions."
        },
        {
            "section number": "2.1",
            "key information": "Existing methods for detecting concept drifts typically require true labels for effective drift detection, which is often not feasible in real-world applications."
        },
        {
            "section number": "3.5",
            "key information": "The proposed method, called Integrally Private Drift Detection (IPDD), utilizes an ensemble of integrally private models to detect concept drifts based on prediction uncertainty."
        },
        {
            "section number": "3.5",
            "key information": "The primary innovation of the IPDD method lies in its ability to detect concept drifts without requiring labeled data, contrasting with existing methods that depend heavily on true labels."
        },
        {
            "section number": "6.1",
            "key information": "The effectiveness of the method is based on the fact that prediction uncertainty correlates with concept drift, allowing for the detection of changes in data distribution without requiring true labels initially."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the method is that generating integrally private models can be computationally intensive and may require a large number of samples."
        },
        {
            "section number": "7.2",
            "key information": "Future research could focus on optimizing the training process for integrally private models and extending the methodology to non-i.i.d. samples."
        }
    ],
    "similarity_score": 0.6450026529265085,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Concept Drift Detection using Ensemble of Integrally Private Models.json"
}