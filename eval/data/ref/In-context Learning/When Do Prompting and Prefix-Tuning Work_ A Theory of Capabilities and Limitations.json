{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.19698",
    "title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations",
    "abstract": "Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, softprompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefixtuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.",
    "bib_name": "petrov2024promptingprefixtuningworktheory",
    "md_text": "# WHEN DO PROMPTING AND PREFIX-TUNING WORK? A THEORY OF CAPABILITIES AND LIMITATIONS\nAleksandar Petrov, Philip H.S. Torr & Adel Bibi Department of Engineering Science University of Oxford Oxford, United Kingdom {aleksandar.petrov,philip.torr,adel.bibi}@eng.ox.ac.uk\nAleksandar Petrov, Philip H.S. Torr & Adel Bibi Department of Engineering Science University of Oxford Oxford, United Kingdom {aleksandar.petrov,philip.torr,adel.bibi}@eng.ox.ac.uk\n# ABSTRACT\nContext-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, softprompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefixtuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.\narXiv:2310.19698v2\nLanguage model advances are largely driven by larger models and more training data (Kaplan et al., 2020; Rae et al., 2021). Training cutting-edge models is out of reach for most academic researchers, small enterprises, and individuals, and it has become common to instead fine-tune open-source pretrained models (Devlin et al., 2019; Min et al., 2021). Yet, due to escalating computational demands, even fine-tuning of the larger models has become prohibitively expensive (Lialin et al., 2023). As a result, there is an acute need for more efficient fine-tuning methods, either by sparsely modifying the parameters of the model or modifying its input context. Examples of the first type include adapter modules which introduce a few trainable layers to modify the behaviour of the frozen pretrained network (Rebuffi et al., 2017; Houlsby et al., 2019; Hu et al., 2023). One can also use low-rank updates, which also results in a reduced number of trainable parameters (Hu et al., 2021). Context-based fine-tuning has been motivated by the success of few-shot and zero-shot learning (Wei et al., 2021; Kojima et al., 2022). The most popular context-based approach is prompting, where generation is conditioned on either human-crafted or automatically optimized tokens (Shin et al., 2020; Liu et al., 2023). In-context learning \u2014prompting via providing input-label examples\u2014 is another widely used technique (Brown et al., 2020). Given the challenges of discrete optimization over tokens, there is a growing interest in methods that optimize real-valued embeddings (Lester et al., 2021). It is widely believed that these soft prompts offer greater expressiveness due to the expansive nature of continuous space. Furthermore, beyond only optimizing input embeddings, one can optimize the inputs of every attention layer (Li and Liang, 2021). This technique, prefix-tuning, has proven to be very successful and competitive to full fine-tuning (Liu et al., 2022). While context-based fine-tuning approaches have witnessed impressive empirical successes and widespread adoption, we have little theoretical understanding of how they work. In this work, we analyse the influence of prompts and prefixes on the internal computations of a pretrained model and delineate their limitations. Specifically, we address the following questions:\n1. Soft prompting and prefix-tuning are motivated by the embedding space being larger than the token space. However, can a transformer utilize the additional capacity? We show that with a careful choice of transformer weights, controlling a single embedding can generate any of the V N completions of N tokens, while controlling a token can produce only V completions, with V being the vocabulary size. Thus, a transformer can indeed exploit the embedding space. 2. Since prefix-tuning is more expressive than prompting, is it as expressive as full fine-tuning? Despite the expressiveness of continuous space, prefix-tuning has structural limitations. A prefix cannot change the relative attention over the content tokens and can only bias the output of the attention block in a constant direction. In contrast, full fine-tuning can learn new attention patterns and arbitrarily modify attention block outputs, making it strictly more powerful. 3. If context-based fine-tuning methods suffer from such structural limitations, how come they have high empirical performance? We show that the prefix-induced bias can steer the model towards a pretraining task. Prefix-tuning can also combine skills picked up during pretraining to solve some new tasks similar to pretraining tasks. However, it may not learn a completely new task. This is not simply because of the small number of learnable parameters: fine-tuning the same number of parameters can be sufficient to learn the novel task. Hence, context-based finetuning can elicit or combine pretrained model skills but cannot learn completely new behaviors.\n# 2 BACKGROUND\n# 2.1 THE TRANSFORMER ARCHITECTURE\nWe outline a simplified decoder-only transformer architecture (Vaswani et al., 2017). Assume that the model has vocabulary size V (also referred to as number of tokens). The input is a sequence (x1, . . . , xp), xi\u2208{1, . . . , V }, \u2200i. Each token is mapped to a de-dimensional vector that is the xith column of an embedding matrix E\u2208Rde\u00d7V . The attention mechanism is position-invariant, so typically position encodings are added. For a model with maximum input length N (context size), we use a one-hot position encoding eN(i) concatenated with the embedding. Therefore, the embedding for the i-th position provided to the first attention block would be xi = [E\u22a4 :,xi, e\u22a4 N(i)]\u22a4. A transformer consists of alternating attention blocks which operate across the whole sequence and Multi-Layer Perceptrons (MLPs) that operate on each individual element. Each attention block consists of H heads. Each head h is parameterized by query, key, and value matrices W h Q, W h K\u2208Rk\u00d7din, W h V \u2208Rdout\u00d7din.1 The attention matrix Ah\u2208Rp\u00d7p for head h then has elements\n\ufffd \ufffd \ufffd where p \u2264N is the current length of the input and T > 0 is an inverse temperature parameter.2 Equation (1) is the softmax function, hence with high enough T, it will result in approximately one-hot encoding of the maximum j. The output of the attention block A with H heads is then (t1, . . . , tp), where each position i is the sum of the attention-weighted values across all heads:\n \ufffd \ufffd A transformer then applies an MLP to each output of an attention block before passing them to next attention block. We will consider linear layers L[M, b](x)=Mx+b and ReLU-activated linear layers \u02c6L[M, b](x)= ReLU(Mx+b). When we compose attention blocks and linear or softmax layers, we will implicitly assume that the linear layer is applied to all positions of the sequence. Furthermore, we will use the then operator \ufffdfor left-to-right function composition. Therefore, a transformer model predicting confidences over the vocabulary can, for example, be represented as: (y1, . . . , yp) = \ufffd A1 \ufffd\u02c6L1,1 \ufffdL1,2 \ufffdA2 \ufffd\u02c6L2,1 \ufffdL2,2 \ufffdsoftmax \ufffd\ufffd\ufffd E:,x1 eN(1) \ufffd , . . . , \ufffd E:,xp eN(p) \ufffd\ufffd , (3)\n\ufffd  \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd 1For the first block, din must be de + N but may be different for the deeper blocks. 2A causal model has Aij = 0 for j > i. This does not affect our results so we will skip the masking step\n(1)\n(2)\n (3)\nwhere the output dimension of the last layer has to be V . The next token for a deterministic transformer is selected to be the last element\u2019s largest logit: xp+1 = arg maxu\u22081,...,V yp,u. Given an input (x1, . . . , xp), the model then autoregressively extends this sequence one token at a time, following Equation (3) either until the sequence reaches a length N or until a special termination token. A transformer has no separation between the system prompt S, user provided input X and the autoregressively response Y. Thus, a sequence conditional on user input is denoted as (S1, ..., SnS, X1, ..., XnX, Y1, ..., YnY ) and one without user input as (S1, ..., SnS, Y1, ..., YnY ).\nWe now define prompting, soft prompting and prefix-tuning with the previously introduced notat\nPrompting. The most frequently used content-based fine-tuning approach is prompting: prefixing the input (X1, ..., XnX) with a token sequence S \u2208{1, ..., V }nS to guide the model response: (S1, ..., SnS, X1, ..., XnX). This is how most people interact with language models such as ChatGPT. Soft prompting. Soft prompting replaces the embeddings of the system input E:,Si with learned vectors si \u2208Rde called virtual tokens (Hambardzumyan et al., 2021; Lester et al., 2021; Qin and Eisner, 2021). Hence, the input in Equation (3) is modified to be:\nwith si chosen to maximize the likelihood of a target response Y =(Y1, ..., YnY ), i.e., arg maxs1,...,snS \u2208Rde \ufffdnY j=1 log ynS+nX+j,Yj, where ynS+nX+j are autoregressively generated.\n\ufffd Prefix-tuning. Prefix-tuning applies soft prompting across the depth of the model (Li and Liang, 2021; Liu et al., 2022). The first nS positions for all attention blocks are learnable parameters, replacing the input (xl 1, . . . , xl nX) for layer l with (sl 1, . . . , sl nS, xl 1, . . . , xl nX), where all sl i constitute the prefix. Hence, prefix-tuning can be formulated as arg max{s1 i ,...,sL i } nS i=1 \ufffdnY j=1 log ynS+nX+j,Yj. Prefix-tuning has been successful at fine-tuning models (Vu et al., 2022; Wu and Shi, 2022; Choi and Lee, 2023; Ouyang et al., 2023; Bai et al., 2023), leading to calls for language models provided as a service (La Malfa et al., 2023) to allow providing prefixes instead of prompts (Sun et al., 2022). Any token-based prompt (S1, ..., SnS) has a corresponding soft prompt (si=E:,Si) but the reverse does not hold. Similarly, every soft prompt (s1, ..., snS) can be represented as a prefix by setting the deeper prefixes to be the values that the model would compute at these positions (sl i=(A1 \ufffd ... \ufffdLl\u22121,\u22121)([s\u22a4 1 , e\u22a4 N(1)]\u22a4, ..., [s\u22a4 l , e\u22a4 N(l)]\u22a4)). The reverse also does not hold: there are prefixes that cannot be represented as a soft prompt. A hierarchy emerges: prompting < soft prompting < prefix-tuning, with prefix-tuning the most powerful of the three. Hence, we focus on examining its performance relative to full fine-tuning but our findings also apply to prompting and soft prompting.\n# 3 SOFT PROMPTING HAS MORE CAPACITY THAN PROMPTING\nThe success of soft prompting (and prefix-tuning) is commonly attributed to the larger capacity of the continuous embeddings compared to the finite tokens. Yet, increased capacity is beneficial only if the model can utilize it. We show this is indeed the case by constructing a transformer generating exponentially more completions by varying a single virtual token than by varying a hard token. Consider unconditional generation (representing a function with no inputs) with a single system token: (Y1, ..., YN)=f(S1)=fS1. For a deterministic autoregressive function, there are a total of V functions in this family, hence the upper bound on the number of outputs of length N that one can generate by varying the first token S1 is V : the first token fully determines the rest of the sequence. Generally, if one varies the first NS tokens, there are at most V NS unique outputs. What if instead of the token S1 we vary a single virtual token s1: (Y1, ..., YN)=f(s1)=fs1? This family of functions is indexed by a real vector and hence is infinite: in principle, one could generate all V N possible\noutput sequences by only controlling s1.3 Still, a transformer may not be able to represent a function that achieves that in practice, i.e., it is not obvious if there is a surjective map from {fs1 : s1 \u2208Rde} to {1, ..., V }N. We show that, in fact, there is a transformer f for which such a surjective map exists: Theorem 1 (Exponential unconditional generation capacity of a single virtual token). For any V, N>0, there exists a transformer with vocabulary size V , context size N, embedding size de=N, one attention layer with two heads and a three-layer MLP such that it generates any token sequence (Y1, ..., YN)\u2208{1, ..., V }N when conditioned on the single virtual token s1= ((Y1\u22121)/V , ..., (YN\u22121)/V ). However, conditional generation is more interesting: given a user input (X1, ..., XnX), we want to generate a target response (Y1, ..., YnY ). Even in the simple case of one system token, the user provides one token and the model generates one token in response (Y1=f(S1, X1)=fS1(X1)), we cannot control response of the model to any user input with the system token. There are V V maps from X1 to Y1, but S1 can take on only V values: |{fS1 : S1 \u22081, ..., V }| = V < V V . Hence, tokens cannot be used to specify an arbitrary map from user input to model output. However, a single virtual token can specify any of the V V maps, i.e., there exists a transformer fs1(X2) for which there is a surjective map from {fs1 : s1 \u2208Rde} to {1, ..., V }{1,...,V }. Theorem 2 (Conditional generation capacity for a single virtual token (nX=nY =1)). For any V >0, there exists a transformer with vocabulary size V , context size N=2, embedding size de=V , one attention layer with two heads and a three-layer MLP that reproduces any map m:[1, ..., V ]\u2192[1, ..., V ] from a user input token to a model response token when conditioned on a single virtual token s1=(m(1)/V , ..., m(V )/V ). That is, by selecting s1 we control the model response to any user input. Theorem 2 builds on Theorem 1 by showing that soft prompting is also more expressive for governing the conditional behavior of a transformer model. This also holds for longer responses nY > 1 by increasing the length of the soft prompt, or longer user inputs nX > 1, by increasing the depth of the model. We provide proofs in Appendix A, as well as working Python implementations. This section showed that soft prompting, and by implication, prefix-tuning, possess greater expressiveness than prompting. As we can fully determine the map from user input to model response using virtual tokens, our findings may appear to suggest that soft prompting is as powerful as full fine-tuning. However, this is not at all the case. There are structural constraints on the capabilities of soft prompting and prefix-tuning; they cannot facilitate the learning of an entirely new task. The following section elucidates this discrepancy and reconciles these seemingly contradictory results.\nWe just saw that soft prompting and prefix-tuning can fully control the conditional behavior of a transformer. However, that assumed a specific design for the network weights. Given a fixed pretrained model, as opposed to a manually crafted one, can prefix-tuning be considered equally powerful to full fine-tuning? In this section, we show that, for an arbitrary pretrained model, a prefix S cannot change the relative attention over the content X, Y and can only bias the attention block outputs in a subspace of rank nS, the prefix length, making it less powerful than full fine-tuning. While full fine-tuning can alter the attention pattern of an attention head, prefix-tuning cannot. Recall the attention Aij position i gives to position j for a trained transformer (Equation (1)):\nWe just saw that soft prompting and prefix-tuning can fully control the conditional behavior of a transformer. However, that assumed a specific design for the network weights. Given a fixed pretrained model, as opposed to a manually crafted one, can prefix-tuning be considered equally powerful to full fine-tuning? In this section, we show that, for an arbitrary pretrained model, a prefix S cannot change the relative attention over the content X, Y and can only bias the attention block outputs in a subspace of rank nS, the prefix length, making it less powerful than full fine-tuning.\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd here W \u22a4 Q WK=H. Full fine-tuning can enact arbitrary changes to WQ and WK and hence, assumng the input does not change (e.g., at the first attention layer), we get the following attention:\nAft ij = exp \ufffd T/ \u221a k x\u22a4 i Hxj + T/ \u221a k x\u22a4 i \u2206Hxj \ufffd \ufffdp r=1 exp \ufffd T/ \u221a k x\u22a4 i Hxr + T/ \u221a k x\u22a4 i \u2206Hxr \ufffd,\n\ufffd \ufffd \ufffd 3For example, LLaMA-7B (Touvron et al., 2023) has 24 426 unique completions when prompted with each of its 32 000 tokens and we found a non-exhaustive set of 46 812 unique 10-token-long sequences by controlling the first virtual token. Hence, in practice, one can generate more outputs by soft prompting than by prompting.\n(5)\nwhere the changes to WQ and WK are folded into \u2206H. It is clear that by varying \u2206H full finetuning can change the attention patterns arbitrarily. However, let us see how is attention affected by the presence of a prefix. For now, assume we have a prefix of length one (s1) at position 0.\n\ufffd \ufffd The numerator of Apt ij is the same as in Equation (5), i.e., the prefix does not affect it. It only adds the term exp(T/ \u221a k x\u22a4 i Hs1) to the denominator. Therefore, the attention position i gives to the content positions j\u22651 is simply scaled down by the attention it now gives to the prefix. If tomato attends the most to salad in a particular context, no prefix can change that. This becomes evident by rewriting Apt ij as the attention of the pretrained model scaled by the attention \u201cstolen\u201d by the prefix:\n\ufffd Hence, prefix-tuning cannot affect the relative attention patterns across the content, it will only scale them down. In other words, one cannot modify what an attention head attends to via prefix-tuning.4\n\ufffd Hence, prefix-tuning cannot affect the relative attention patterns across the content, it will only scale them down. In other words, one cannot modify what an attention head attends to via prefix-tuning.4 Prefix-tuning only adds a bias to the attention block output. Let us see how this attention scaling down affects the output of the attention block. Following Equation (2), the output at position i for the pretrained (ti), the fully fine-tuned (tft i ) and the prefix-tuned (tpt i ) models are as follows:5\nPrefix-tuning only adds a bias to the attention block output. Let us see how this attention scaling down affects the output of the attention block. Following Equation (2), the output at position i for the pretrained (ti), the fully fine-tuned (tft i ) and the prefix-tuned (tpt i ) models are as follows:5\nHence, prefix-tuning only biases the attention block value at each position i towards the constant vector WV s1, which is independent of the content (x1, ..., xp). I.e., the prefix-tuned activation is a linear combination of the pretrained activation and the constant vector WV s1. The content only affects the scale Apt i0 of the bias via the amount of attention on the prefix. In contrast, in full finetuning \u2206WQ, \u2206WK and \u2206WV allow for a content-dependent change of the attention and value computation. These results hold for suffix-tuning (placing the prefix after the input) but not for suffix soft-prompting. We validate that this indeed is the case when prefix-tuning real-world transformers. In Figures 5 and 6, we show that a prefix applied to LLaMA\u2019s first layer does not change the relative attention distribution over the content positions X and results in a bias with a constant direction. Longer prefixes define larger subspaces for the bias but are not fully utilized in practice. In the case of a longer prefix (s1, . . . , snS), the bias vector is in a subspace of dimensionality nS: tpt i = \ufffdnS j=1 Apt i,SjWV sj + (1 \u2212\ufffdnS j=1 Apt i,Sj)ti, where i goes over the content and j over the prefix positions. Larger prefixes thus have a larger subspace to modify the attention block output. The specific direction is determined by the relative distribution of attention across the prefix positions. However, when we examine the distribution of attention across the prefix positions for various inputs as in Appendix B, it appears that the prefixes do not span this subspace. Regardless of the input, the attention Apt i,Sj over the prefix positions remains nearly constant. Thus, prefix-tuning does not seem to make full use of the space that the vectors WV sj span. We hypothesise that this is due to the two competing optimization goals for the vectors sj: at the same time they need to \u201cgrab attention\u201d when interacting with WK and determine the bias direction when multiplied with WV . So, is prefix-tuning equivalent to full fine-tuning or is it less powerful than full fine-tuning? In Section 3, we showed that prefix-tuning, in principle, has a large capacity to influence the behavior of the model. But then, in this section, we showed that it has some severe limitations, including not being able to affect the attention pattern and only biasing the attention layer activations. These two results seem to be contradicting one another, so how do we reconcile them? The constructions for the results in Section 3 (described in Appendix A) are simply an algorithm that extracts the completion from a lookup table encoded in the virtual tokens. The attention patterns\nHence, prefix-tuning only biases the attention block value at each position i towards the constant vector WV s1, which is independent of the content (x1, ..., xp). I.e., the prefix-tuned activation is a linear combination of the pretrained activation and the constant vector WV s1. The content only affects the scale Apt i0 of the bias via the amount of attention on the prefix. In contrast, in full finetuning \u2206WQ, \u2206WK and \u2206WV allow for a content-dependent change of the attention and value computation. These results hold for suffix-tuning (placing the prefix after the input) but not for suffix soft-prompting. We validate that this indeed is the case when prefix-tuning real-world transformers. In Figures 5 and 6, we show that a prefix applied to LLaMA\u2019s first layer does not change the relative attention distribution over the content positions X and results in a bias with a constant direction.\nthe case of a longer prefix (s1, . . . , snS), the bias vector is in a subspace of dimensionality nS: tpt i = \ufffdnS j=1 Apt i,SjWV sj + (1 \u2212\ufffdnS j=1 Apt i,Sj)ti, where i goes over the content and j over the prefix positions. Larger prefixes thus have a larger subspace to modify the attention block output. The specific direction is determined by the relative distribution of attention across the prefix positions. However, when we examine the distribution of attention across the prefix positions for various inputs as in Appendix B, it appears that the prefixes do not span this subspace. Regardless of the input, the attention Apt i,Sj over the prefix positions remains nearly constant. Thus, prefix-tuning does not seem to make full use of the space that the vectors WV sj span. We hypothesise that this is due to the two competing optimization goals for the vectors sj: at the same time they need to \u201cgrab attention\u201d when interacting with WK and determine the bias direction when multiplied with WV . So, is prefix-tuning equivalent to full fine-tuning or is it less powerful than full fine-tuning? In Section 3, we showed that prefix-tuning, in principle, has a large capacity to influence the behavior of the model. But then, in this section, we showed that it has some severe limitations, including not being able to affect the attention pattern and only biasing the attention layer activations. These two results seem to be contradicting one another, so how do we reconcile them? The constructions for the results in Section 3 (described in Appendix A) are simply an algorithm that extracts the completion from a lookup table encoded in the virtual tokens. The attention patterns 4Likhosherstov et al. (2021) show that a fixed attention head can approximate any sparse attention pattern.\n4Likhosherstov et al. (2021) show that a fixed attention head can approximate any sparse attention pattern However, they require control over all the input embeddings while we can only control the prefix ones. 5He et al. (2021a) show a similar analysis but do not study the expressiveness of prefix-tuning.\nfor j\u22651.\n(6)\n(7)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2350/23503ae5-96a2-403b-8403-e85e02c6ee38.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Attention patterns of a small transformer pretrained on sorting in ascending order. The model is given the prefix S and user input X and generates Y autoregressively. We have highlighted the attention when the first response Y1 is being generated. Full fine-tuning sorts in descending order but prefix-tuning cannot as it cannot update the learned attention. Note how the relative attention of X to X in the left and right plots is exactly the same: the prefix cannot change the attention pattern for the same inputs. The relative attention of X to X in the center plot is very different because full fine-tuning can arbitrarily change WQ and WK.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c85c/c85cf53f-2270-49d0-abc0-f11b0eed55ad.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Model pretrained on the four tasks. The four attention heads specialize in the skills necessary to solve these tasks: look at the elements in order, look first at the smallest elements or first at the largest elements.</div>\nare simply extracting the current position embedding and the virtual token and hence the attention does not depend on the actual content in the tokens. There is no need to learn a new attention pattern to learn a different map from input to output.6 Furthermore, the virtual token designates the map precisely by acting as a bias. Therefore, the observations in these two sections do not contradict one another. Soft prompting and prefix-tuning can be on par with full fine-tuning but only in very limited circumstances: when all the knowledge is represented in the virtual token as a lookup table and the model simply extracts the relevant entry. Transformers do not behave like this in practice. Models are typically trained with token inputs rather than virtual tokens. Moreover, if we had a lookup table of the responses to each input we would not need a learning algorithm in the first place. Therefore, the limitations from this section hold for real-world pretrained transformers. Then how come prefix-tuning has been reported to achieve high accuracy and often to be competitive to full fine-tuning? The next section aims to explain when and why prefix-tuning can work in practice.\n# THE BIAS CAN ELICIT SKILLS FROM THE PRETRAINED M\nPretraining potentially exposes a model to different types of completions for the same token sequence. For a string like I didn\u2019t enjoy the movie, the model may have seen completions such as I found the acting to be sub par, This is negative sentiment or Je n\u2019ai pas aim\u00b4e le film. Hence, a pretrained model could do text completion, sentiment analysis, or translation. Still, the input does not fully determine the desired completion type and the model can generate any one of them. Hence, following our results from Section 4, we hypothesise that prefix-tuning cannot gain new knowledge\n6In a follow-up work (Petrov et al., 2024), we utilize this observation to show that, in fact, there exist pretrained weights for which a transformer can be a universal approximator for sequence-to-sequence functions when prefixed. This is not in contradiction with the present results as these transformers can approximate any function without having to modify their attention mechanism.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6eed/6eeded2a-c480-42e2-961e-52e6e261cf50.png\" style=\"width: 50%;\"></div>\nFigure 3: Attention block activations for ten sequences at the last input position (10) when pretrained on the four tasks. The left plot shows the pretrained activations t10 are not predictive of the completion. The right plot shows prefixes cluster the activations tpt 10. Connecting the pretrained and prefixed activations highlights the bias. No dimensionality reduction is used; the clustering is solely due to the prefixes.\nbut can bring to the surface latent knowledge present in the pretrained model.7 We test this hypothesis by constructing small transformers trained on one or few tasks. We use a minimal transformer model (Karpathy, 2020) to show that prefix-tuning struggles to learn a new task that full fine-tuning can. Then, that prefix-tuning can easily elicit a latent skill from pretraining. Finally, we show how it can even learn some new tasks, provided they can be solved by combining pretraining skills.\nPrefix-tuning may not learn a new task requiring a different attention pattern. To check if prefix-tuning can learn a new task, we train a 1-layer, 1-head transformer to sort numbers into ascending order and then fine-tune it to sort in descending order. During training, the model sees random sequences of 10 digits from 0 to 7 followed by their ascending sorted order. The pretrained accuracy (fully matching the sorted sequence) is 91%. Full fine-tuning on the descending task leads to\n85% test accuracy, hence full fine-tuning successfully learns the new task. However, prefix-tuning with a prefix size nS=1 results in 0% accuracy, hence prefix-tuning fails to learn the new task at all. The attention patterns in Figure 1 show why this is the case: the pretrained model learns to attend first to the smallest numbers and then to the larger ones. When fully fine-tuned, the attention patterns are reversed: they now first attend to the largest values. However, following Section 4, prefix-tuning cannot change the attention pattern over the input sequence and will still attend to the smallest values. Hence, prefix-tuning may indeed struggle to learn a new task requiring new attention patterns.\nPrefix-tuning can elicit a skill from the pretrained model. The second part of our hypothesis was that prefix-tuning can elicit latent skills in the pretrained model. To test that, we pretrain a 1-layer, 4-head model with solutions sorted in ascending (\u2197) or descending (\u2198) order, or adding one (+1) or two (+2) to each element of the input sequence. Each solution is shown with 25% probability. The model has no indication of what the task is, hence, it as-\n<div style=\"text-align: center;\">Table 2: A transformer pretrained on several tasks can be prefix-tuned for one of them. 10 random seeds.</div>\nAccuracy on:\n\u2197\n\u2198\n+1\n+2\nPretrained\n25\u00b113% 25\u00b112% 24\u00b111% 22\u00b17%\nPrefix-tune on \u219795\u00b1 2%\n0\u00b1 0%\n0\u00b1 0%\n0\u00b10%\nPrefix-tune on \u2198\n0\u00b1 0% 90\u00b1 3%\n1\u00b1 1%\n1\u00b11%\nPrefix-tune on +1\n0\u00b1 0%\n1\u00b1 3% 95\u00b1 6%\n0\u00b11%\nPrefix-tune on +2\n0\u00b1 0%\n0\u00b1 0%\n1\u00b1 2% 98\u00b15%\nsigns equal probability to all tasks, as shown in the first row in Table 2. Full fine-tuning for each task naturally results in high accuracy. However, prefix-tuning (nS=1) can also reach accuracy above 90% for all tasks. Compared to the previous case, prefix-tuning is more successful here because the pretrained model contains the attention mechanisms for solving the four tasks, as shown in Figure 2. If all a prefix does is bias the attention layer activations, how can it steer the model to collapse its distribution onto one task? This is likely due to the attention block solving all tasks in parallel and placing their solutions in different subspaces of the residual stream (intermediate representation, Elhage et al., 2021). As the MLP needs to select one solution to generate, a further indicator on the selected task (or lack of selection thereof) should also be represented. The bias induced by the prefix then acts on this \u201cselection subspace\u201d to nudge the MLP to select the desired solution.\nimilar hypothesis has also been proposed by Reynolds and McDonell (2021) for \nTable 1: A transformer pretrained on sorting in ascending order cannot be prefix-tuned to sort in descending order. 10 random seeds.\nAscending Descending\nPretrain on asc.\n91\u00b15%\n0\u00b10%\nFull fine-tune on desc.\n0\u00b10%\n85\u00b15%\nPrefix-tune on desc.\n0\u00b10%\n0\u00b10%\nThis can be clearly seen from the activations of the attention layer at the last input position (XnX): the position where the task selection happens as the first output element fully describes the task. Figure 3 shows plots of randomly selected dimensions of the residual stream with and without a prefix. The attention block activations of the pretrained model (without prefix) show no correlation with the output it is about to generate, demonstrating that the choice of completion is indeed not determined by the attention block. However, the prefix-tuned activations for the same inputs are clustered as a result of the prefix-induced bias. This indicates that the bias induced by the prefix may act as a \u201ctask selector\u201d of the subspace of the residual stream specializing in the desired task. Prefix-tuning can combine knowledge from pretraining tasks to solve new tasks. Prefix-tuning eliciting one type of completion learned in pretraining starts to explain its practical utility. Still, prefix-tuning seems to be successful also at tasks that the pretrained model has not seen. As we showed above, a model trained to sort in one order cannot be prefix-tuned to sort in the other. Then how is it possible for prefix-tuning to learn a new task? We posit that this can happen, as long as the \u201cskill\u201d required to solve the new task is a combination of \u201cskills\u201d the pretrained model has seen.\nWe test this by pretraining a 40-layer 4-head model with the same four tasks. We prefixtune (nS=12) for two new tasks: incrementing the ascending sorted sequence (\u2197+1) and double histogram (mapping each element to the number of elements with the same value, e.g., 3,0,0,1\ufffd\u21921,2,2,1, H). The pretrained model has not seen either task. Prefix-tuning results in 93% accuracy for \u2197+1 which is a combination of the \u2197and +1 pretraining tasks and just 1% for the H task which requires different skills: finding other instances\n<div style=\"text-align: center;\">Table 3: Prefix tuning can learn a new task requiring only pretraining skills (\u2197+1) but cannot learn a completely new task (H). Average accuracy over 3 seeds.</div>\nAccuracy on:\n\u2197\n\u2198\n+1\n+2 \u2197+1\nH\nPretrained\n17%\n23%\n34%\n25%\n0% 0%\nPrefix-tune on \u2197\n100%\n0%\n0%\n0%\n0% 0%\nPrefix-tune on \u2198\n0% 100%\n0%\n0%\n0% 0%\nPrefix-tune on +1\n0%\n0% 100%\n0%\n0% 0%\nPrefix-tune on +2\n0%\n0%\n0% 100%\n0% 0%\nPrefix-tune on \u2197+1\n0%\n0%\n0%\n0% 93% 0%\nPrefix-tune on H\n0%\n0%\n0%\n0%\n0% 1%\nof the same token and counting. H is not a hard task: it requires 2 layers and 2 heads to be solved exactly (Weiss et al., 2021). Therefore, prefix-tuning is can indeed combine skills that the model has learned in order to solve a novel task but may not learn a completely new task requiring new skills.\n6 EFFECTS OF PREFIX-TUNING BEYOND THE SINGLE ATTENTION LAYER\nSection 4 focused exclusively on a single attention layer. Still, even if a prefix only induces a bias on its output, this bias can exhibit complex behaviors via the subsequent MLPs and attention layers. This section shows how a prefix can change the attention pattern of the following attention layer but only in a linear fashion while full fine-tuning also has bilinear effects. Appendix C further argues that the representational capacity of prefix-tuning may be limited. Therefore, prefix-tuning appears to be less expressive than full fine-tuning, even with the same number of learnable parameters. Prefix-tuning can change the attention, albeit the one of the next layer Let us examine how the prefix of one attention layer affects the following one. Assume no MLPs, residual connections or layer norms: the output t(1) i of the first is the input x(2) i of the second. The pretrained outputs are t(1) i = \ufffdp j=1 A(1) ij W (1) V x(1) j , resulting in the second layer attention \u02dc A(2) ij =T/ \u221a k t(1)\u22a4 i H(2)t(1) j . Here \u02dc Aij is the pre-softmax attention, i.e., Aij=exp \u02dc Aij/\ufffdp r=1 exp \u02dc Air. For prefix-tuning we then have:\nSection 4 focused exclusively on a single attention layer. Still, even if a prefix only induces a bias on its output, this bias can exhibit complex behaviors via the subsequent MLPs and attention layers. This section shows how a prefix can change the attention pattern of the following attention layer but only in a linear fashion while full fine-tuning also has bilinear effects. Appendix C further argues that the representational capacity of prefix-tuning may be limited. Therefore, prefix-tuning appears to be less expressive than full fine-tuning, even with the same number of learnable parameters.\nPrefix-tuning can change the attention, albeit the one of the next layer Let us examine how the prefix of one attention layer affects the following one. Assume no MLPs, residual connections or layer norms: the output t(1) i of the first is the input x(2) i of the second. The pretrained outputs are t(1) i = \ufffdp j=1 A(1) ij W (1) V x(1) j , resulting in the second layer attention \u02dc A(2) ij =T/ \u221a k t(1)\u22a4 i H(2)t(1) j . Here \u02dc Aij is the pre-softmax attention, i.e., Aij=exp \u02dc Aij/\ufffdp r=1 exp \u02dc Air. For prefix-tuning we then have:\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd The presence of \u00b5 shows that the prefix of layer 1 can change the attention pattern of the following layer. This change is content-specific: the second and the third terms depend on the inputs, hence a simple bias can affect the attention when passed through MLPs and further attention blocks. Compare with Equation (6), which showed a prefix cannot change the attention of the same layer. Still,\neven considering this cross-layer effect, prefix-tuning is more limited in its expressiveness than full fine-tuning. While the second and the third terms are input-dependent, each depends on one input position only. The prefix does not change the bilinear dependency on both the query and key. This is something that the full fine-tuning can achieve: \u02dc Aft(2) ij = T/ \u221a k tft(1)\u22a4 i (H(2) + \u2206H(2))tft(1) j . Even if prefix-tuning could be a universal approximator, it would not be parameter-efficient. Prefix-tuning appears to be less parameter-efficient than other comparable approaches. We designed an experiment to this end. Our pretrained model in Section 5 failed to learn the double histogram task (H). A rank-1 Low Rank Adaptation (LoRA, Hu et al., 2021) applied only to the MLPs in a 4-layer 4-head model pretrained in the exact same way results in 92% accuracy on the H task. The number of parameters for the LoRA fine-tuning is exactly the same as for a prefix of size 12. However, as can be expected from the results in Section 5, training this prefix results in 0% accuracy. Hence, prefix-tuning fails at a task that LoRA with the same number of parameters can learn. In conclusion, while prefix-tuning, prompting, soft prompting and in-context learning have complex effects in deeper models, the interaction of the learnable parameters and the model inputs likely still results in very limited expressiveness. In particular, we demonstrated that LoRA can be used to learn a completely new task while prefix-tuning with the exact same number of parameters fails to.\n# 7 DISCUSSION AND RELATED WORKS\nUnderstanding fine-tuning and prefix-tuning. Prior works show that prefixes have low intrinsic dimension allowing transfer to similar tasks and initialization of prefixes for new tasks (Qin et al., 2021; Su et al., 2022; Zhong et al., 2022; Wang et al., 2022b; Zheng et al., 2023). In this work, we offered theoretical insights into their results: this subspace is the span of the prefix-induced bias. Another line of work shows that skills can be localized in the parameter space of pretrained models (Wang et al., 2022a; Panigrahi et al., 2023). Here, we showed that it is also possible to identify subspaces of the residual stream corresponding to individual tasks and select them via prefix-tuning. Prompting and in-context learning. Prompting and in-context learning are a special case of prefix-tuning. Therefore, the limitations and mechanisms discussed in this work apply to prompting as well: prompts cannot change the distribution of attention of the first attention layer over the content following it and can only induce a bias on the output of this layer (Section 4). Even considering the cross-layer effects, a prompt is strictly less expressive than full fine-tuning (Section 6) and prompting is unlikely to enable the model to solve a completely new task. Our theory thus explains why Kossen et al. (2023) observed that in-context examples cannot overcome pre-training skills. While context-based fine-tuning approaches may not learn arbitrary new tasks, as shown in Section 5, they can leverage pre-trained skills. Wies et al. (2023) have PAC-learnability results that also show that when pretraining is on a mixture tasks, they can be efficiently learned via in-context learning, Moreover, transformers can learn linear models in-context by mimicking gradient descent (Von Oswald et al., 2023) or approximating matrix inversion (Aky\u00a8urek et al., 2022). This is consistent with our theory: the prediction updates are enacted as biases in the attention block activations. Hence, despite the limitations discussed in this work, context-based methods can result in powerful fine-tuning if the pretrained model has \u201ctransferable skills\u201d such as algorithmic fundamentals. Still, in-context learning will likely fail for non-algorithmic tasks, e.g., translating to a language that the model has never seen before, even if large number of translation pairs are provided in-context. Implications for catastrophic forgetting and model alignment. The lack of expressiveness of context-based fine-tuning can be a feature: desirable properties will be maintained. Full fine-tuning can result in catastrophic forgetting (He et al., 2021b; Luo et al., 2023; Mukhoti et al., 2023). Our theory shows that context-based methods won\u2019t lose pretrained skills. Model alignment poses the reverse problem: ensuring that the model cannot pick up undesirable skills during fine-tuning Our results show that prompting and prefix-tuning might be unable to steer the model towards new adversarial behaviors. Hence, the recent successes in adversarial prompting (Zou et al., 2023) indicate that current model alignment methods just mask the undesirable skills rather than removing them. Implications for model interpretability. An open question for language model interpretability is whether attention is sufficient for explainability (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Section 5 points toward the negative: by interfering in the output of the attention layer with the bias induced by a prefix, we can change the behavior of the model, without changing its attention.\nOn the flip side, prefix-tuning can be used to understand what \u201cskills\u201d a model has: if prefix-tuning for a task fails, then the model likely lacks one of the key \u201cskills\u201d for that task. Limitations. The present analysis is largely limited to prefixing with prompts, soft prompts and for prefix-tuning. While our theoretical results hold for suffix-tuning, they do not necessarily apply to suffixing with prompts or soft prompts. That is because the deeper representations for prompt and soft prompt suffixes would depend on the previous positions. This does not apply to suffix-tuning as it fixes all intermediate representations. Therefore, whether suffixing is more expressive than prefixing remains an open question. Separately, while we provided evidence towards context-based fine-tuning methods being parameter inefficient learners, the formal analysis of the conditions under which they may be universal approximators remain an open question. Finally, we mostly considered simple toy problems. In practice, however, language models are pretrained with very large datasets and can pick up very complex behaviors. Hence, the extent to which the limitations we demonstrated apply to large-scale pretrained transformers also remains for future work.\nOn the flip side, prefix-tuning can be used to understand what \u201cskills\u201d a model has: if prefix-tuning for a task fails, then the model likely lacks one of the key \u201cskills\u201d for that task.\n# 8 CONCLUSION\nThis paper formally showed that fine-tuning techniques working in embedding space, such as soft prompting and prefix-tuning, are strictly more expressive than prompting which operates in the discrete token space. However, we then demonstrated that despite this larger expressivity, prefixtuning suffers from structural limitations that prevent it from learning new attention patterns. As a result, it can only bias the output of the attention layer in a direction from a subspace of rank equal to the size of the prefix. We showed that this results in practical limitations by constructing minimal transformers where prefix tuning fails to solve a simple task. This result seems to be at odds with the empirical success of prefix-tuning. We provided explanations towards that. First, we showed that prefix-tuning can easily elicit a skill the pretrained model already has and can even learn a new task, if it has picked up the skills to solve it during pretraining. Second, we showed that the effect of the prefix-induced bias is more complicated and powerful when combined with downstream non-linear operations. However, it appears to be still less expressive than full fine-tuning.\n# REPRODUCIBILITY STATEMENT\nIn order to facilitate the reproduction of our empirical results, validating our theoretical results, and further studying the properties of context-based fine-tuning, we release all our code and resources used in this work. Furthermore, in Appendix A we offer explicit constructions of transformers with the properties discussed in Section 3. We also provide Python implementations of these constructions that validate their correctness.\n# ACKNOWLEDGEMENTS\nThis work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1) and the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (EP/S024050/1). AB has received funding from the Amazon Research Awards. We also thank the Royal Academy of Engineering and FiveAI.\n# REFERENCES\nEkin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. In International Conference on Learning Representations. Jiaqi Bai, Zhao Yan, Jian Yang, Xinnian Liang, Hongcheng Guo, and Zhoujun Li. 2023. Knowprefix-tuning: A two-stage prefix-tuning framework for knowledge-grounded dialogue generation. arXiv preprint arXiv:2306.15430. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems.\nEkin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. In International Conference on Learning Representations. Jiaqi Bai, Zhao Yan, Jian Yang, Xinnian Liang, Hongcheng Guo, and Zhoujun Li. 2023. Knowprefix-tuning: A two-stage prefix-tuning framework for knowledge-grounded dialogue generation. arXiv preprint arXiv:2306.15430. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems.\nYunSeok Choi and Jee-Hyong Lee. 2023. CodePrompt: Task-agnostic prefix tuning for program and language generation. In Findings of the Association for Computational Linguistics: ACL 2023. Ana Santos Costa, Montserrat Comesa\u02dcna, and Ana Paula Soares. 2022. PHOR-in-One: A multilingual lexical database with PHonological, ORthographic and PHonographic word similarity estimates in four languages. Behavior Research Methods. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Mohamad H Hassoun. 1995. Fundamentals of artificial neural networks. MIT press. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021a. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations. Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and Fuchun Peng. 2021b. Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933. Sarthak Jain and Byron C. Wallace. 2019. Attention is not explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Andrej Karpathy. 2020. minGPT GitHub Repository. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems. Jannik Kossen, Tom Rainforth, and Yarin Gal. 2023. In-context learning in large language models learns label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375.\nEmanuele La Malfa, Aleksandar Petrov, Christoph Weinhuber, Simon Frieder, Ryan Burnell, Anthony G. Cohn, Nigel Shadbolt, and Michael Wooldridge. 2023. The ARRT of Language-Modelsas-a-Service: Overview of a new paradigm and its challenges. arXiv preprint arXiv:2309.16573. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. 2021. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. PTuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys. Jishnu Mukhoti, Yarin Gal, Philip H. S. Torr, and Puneet K. Dokania. 2023. Fine-tuning can cripple your foundation model; preserving features may be the solution. arXiv preprint arXiv:2308.13320. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang, and Xinyu Dai. 2023. On prefix-tuning for lightweight out-of-distribution detection. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. 2023. Task-specific skill localization in fine-tuned language models. In International Conference on Machine Learning. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2024. Prompting a pretrained transformer can be a universal approximator. arXiv preprint arXiv:2402.14753. Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\nEmanuele La Malfa, Aleksandar Petrov, Christoph Weinhuber, Simon Frieder, Ryan Burnell, Anthony G. Cohn, Nigel Shadbolt, and Michael Wooldridge. 2023. The ARRT of Language-Modelsas-a-Service: Overview of a new paradigm and its challenges. arXiv preprint arXiv:2309.16573. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. 2021. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. PTuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys. Jishnu Mukhoti, Yarin Gal, Philip H. S. Torr, and Puneet K. Dokania. 2023. Fine-tuning can cripple your foundation model; preserving features may be the solution. arXiv preprint arXiv:2308.13320. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang, and Xinyu Dai. 2023. On prefix-tuning for lightweight out-of-distribution detection. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. 2023. Task-specific skill localization in fine-tuned language models. In International Conference on Machine Learning. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2024. Prompting a pretrained transformer can be a universal approximator. arXiv preprint arXiv:2402.14753. Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\nYujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, et al. 2021. Exploring universal intrinsic task subspace via prompt tuning. arXiv preprint arXiv:2110.07867. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, et al. 2021. Scaling language models: Methods, analysis & insights from training Gopher. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Mirko Solazzi and Aurelio Uncini. 2004. Regularising neural networks using flexible multivariate activation function. Neural Networks, 17(2):247\u2013260. Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, and Jie Zhou. 2022. On transferability of prompt tuning for natural language processing. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In International Conference on Machine Learning. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou\u2019, and Daniel Cer. 2022. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. 2022a. Finding skill neurons in pre-trained transformer-based language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. 2022b. Multitask prompt tuning enables parameter-efficient transfer learning. In International Conference on Learning Representations.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like transformers. In International Conference on Machine Learning. Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Noam Wies, Yoav Levine, and Amnon Shashua. 2023. The learnability of in-context learning. arXiv preprint arXiv:2303.07895. Hui Wu and Xiaodong Shi. 2022. Adversarial soft prompt tuning for cross-domain sentiment analysis. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Yuanhang Zheng, Zhixing Tan, Peng Li, and Yang Liu. 2023. Black-box prompt tuning with subspace learning. arXiv preprint arXiv:2305.03518. Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2022. PANDA: Prompt transfer meets knowledge distillation for efficient model adaptation. arXiv preprint arXiv:2208.10160. Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.\nA CONSTRUCTING TRANSFORMERS THAT UTILIZE THE CAPACITY OF THE EMBEDDING SPACE\nUNCONDITIONAL GENERATION FOR A SINGLE VIRTUAL TOKEN\nThis section provides an explicit construction of a transformer with the properties described in Theorem 1. The goal is to construct a transformer that, by varying the choice of the virtual token, can generate any sequence of N tokens. First, we need to specify how we encode the target sequence (Y1, . . . , YN) into the virtual token s1. We chose the size of the embedding (and hence of s1) to be N. This way, each element of s1 can represent one position of the target sequence. We then represent the token value by discretizing each element of s1 into V levels:\nNote that this means that each element of s1 is in [0, 1).\nWhen predicting the token for the i + 1 position, the transformer needs to pick the i-th element of s1, and then decode the corresponding value as a one-hot encoding representing the Yi-th token. We extract the i-th element of s1 using one attention block of two heads. The fst head always looks at the first position which is our virtual token s1. For that purpose we create an attention head that always has Afst ij = 1 if j = 1 and Afst ij = 0 otherwise together with a value matrix W fst V that extracts the embedding. This is achieved with\nand a sufficiently high inverse temperature parameter T. The pos head instead extracts the one-hot encoding of the current position. This can be done with an attention head that always attends only to the current position and a value matrix W pos V that extract the position embedding as a one-hot vector:\nThe pos head instead extracts the one-hot encoding of the current position. This can be done with an attention head that always attends only to the current position and a value matrix W pos V that extracts the position embedding as a one-hot vector:\nWhen the outputs of these two attention heads are summed, then only the element of s1 that corresponds to the current position will be larger than 1. From Equation (2) the output at the i-th position of the attention block is:\n\ufffd \ufffd where x1 = s1 and xj = E:,Yj\u22121 for j > 1. We can extract the value of s1 corresponding to the current position by substracting 1 from the hidden state and apply ReLU: \u02c6Lex = \u02c6L[IN, \u22121N]. Now, we are left with only one non-zero entry and that\u2019s the one corresponding to the next token. We can retain only the non-zero entry if we just sum all the entries of the hidden state with \u02c6Lsum = \u02c6L[1\u22a4 N, 0]. The final step is to map this scalar to a V -dimensional vector which has its maximum value at index Yi. This task is equivalent to designing V linear functions, each attaining its maximum at one of 0, 1/V , . . . , (V \u22121)/V . To construct this, we use the property of convex functions that their tangent is always under the plot of the function. Therefore, given a convex function \u03b3(x), we construct the i-th linear function to be simply the tangent of \u03b3 at i\u22121/V . If we take \u03b3(x) = (x \u22121/2)2, this results in the following linear layer: \ufffd \ufffd\nFigure 4 shows the predictors for each individual token id. With just two attention heads and three linear layers, the transformer A[(W fst Q , W pos Q ), (W fst K , W pos K ), (W fst V , W pos V )] \ufffd\u02c6Lex \ufffd\u02c6Lsum \ufffdLproj \ufffdsoftmax achieves the upper bound of V N unique outputs by controlling a single virtual token at its input. Note that for this\n(8)\n(9)\n (10)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a368/a36867ae-27df-4baf-a480-413f678e8e88.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Illustration of the predictors for each token in the Lproj linear layer for V = 10. The layer is constructed in such a way that the i-th token has the highest confidence when the input is i\u22121/V .</div>\nconstruction, the choice of embedding matrix E \u2208RN\u00d7V does not matter. The same transformer architecture can generate only V unique outputs if we only control the first token instead. Therefore, it is indeed the case that the embedding space has exponentially more capacity for control than the token space. You can see this transformer implemented and running in practice in Section 2 of this notebook.\nA.2 CONDITIONAL GENERATION FOR A SINGLE VIRTUAL TOKEN (nX = nY = 1)\nThis section provides an explicit construction of a transformer with the properties described in Theorem 2. The goal is to construct a transformer that, by varying the choice of the virtual token, can cause the model to act as any map m : [1, . . . , V ] \u2192[1, . . . , V ]. In other words, by selecting the virtual token, we can fully control how the model will respond to any token the user may provide. First, we need to specify how the map m will be encoded in the virtual token s1. We choose the embedding size de to be V . Now, we can use the same encoding scheme as before, but now each element in s1 corresponds to a different user token, rather than to a position in the generated sequence:\nTherefore, the first element of s1 designates the response if the user provides token 1, the second element is the response to the token 2, and so on. Extracting the Yi-th value from s1 and decoding it can be done in a very similar way as for the unconditional case. The only difference is that instead of looking at the user input position, we look at its value. Take E = IV and N = 2. Hence we have the following val head (only differing in the WV matrix from Equation (9)):\nWe also need embedding of the first token, so we have a modified version of Equation (8): W fst Q = [0V , 1, 1], W fst K = [0V , 1, 0], W fst V = [IV , 0V \u00d72].\nWe also need embedding of the first token, so we have a modified version of Equation (8):\nhence the output of this attention block at the second position would be:\nSimilarly to the unconditional case, only the entry of t2 corresponding to the user token will have a value above 1 and that value would be 1 + m(x1)/V .\nWe can now extract the one-hot representation of the target token using the same approach as before, just adjusting for the different hidden state size: \u02c6Lex = \u02c6L[IV , \u22121V ], \u02c6Lsum = \u02c6L[1\u22a4 V , 0], and the same projection had as before (Equation (10)). The final transformer is then: A[(W fst Q , W val Q ), (W fst K , W val K ), (W fst V , W val V )] \ufffd\u02c6Lex \ufffd\u02c6Lsum \ufffdLproj \ufffdsoftmax . You can see this transformer implemented and running in practice in Section 3 here.\nA.3 CONDITIONAL GENERATION FOR LONGER RESPONSES (nX = 1, nY > 1)\nA.3 CONDITIONAL GENERATION FOR LONGER RESPONSES (nX = 1, nY > 1) We can obtain longer responses via a simple extension. If the response length is N0, then we can encode the map m : [1, . . . , V ] \u2192[1, . . . , V ]N0 in N0 virtual tokens, each corresponding to one of the target positions: si = (m(1)i/V , . . . , m(V )i/V ) for i = 1, . . . , N0. For this model we would then have N = 2N0 and de = V . First, we need a head that always looks at the token provided by the user, which will be at position No + 1: W user Q = [0V , 1N], W user K = [0(V +No), 1, 0(No\u22121)], W user V = [IV , 0V \u00d7N]. In order to consume the map at the right location, we need to also look at the embedding of the token No positions before the one we are trying to generate: W back Q = \ufffd 0N0\u00d7(N0+V ) IN0 0N0\u00d7(N0+V ) 0N0\u00d7N0 \ufffd , W back K = [0N\u00d7V , IN], W back V = [IV , 0V \u00d7N]. From here on, the decoding is exactly the same as in the nX = nY = 1 case. The final transformer is then: A[(W user Q , W back Q ), (W user K , W back K ), (W user V , W back V )] \ufffd\u02c6Lex \ufffd\u02c6Lsum \ufffdLproj \ufffdsoftmax . You can see this transformer implemented and running in practice in Section 4 here.\nWe can obtain longer responses via a simple extension. If the response length is N0, then we can encode the map m : [1, . . . , V ] \u2192[1, . . . , V ]N0 in N0 virtual tokens, each corresponding to one of the target positions:\nFor this model we would then have N = 2N0 and de = V . First, we need a head that always looks at the token provided by the user, which will be at position No + 1:\nIn order to consume the map at the right location, we need to also look at the embedding of the toke No positions before the one we are trying to generate:\nFrom here on, the decoding is exactly the same as in the nX = nY = 1 case. The final transformer is then: A[(W user Q , W back Q ), (W user K , W back K ), (W user V , W back V )] \ufffd\u02c6Lex \ufffd\u02c6Lsum \ufffdLproj \ufffdsoftmax . You can see this transformer implemented and running in practice in Section 4 here.\n    A.4 CONDITIONAL GENERATION FOR LONGER USER INPUTS (nX > 1, nY = 1)\nFinally, we consider the case when the user input X is longer. This is a bit more complicated because we need to search through a domain of size V V . We will only consider the case with nX = 2 where we would need two attention layers. A similar approach can be used to construct deeper models for nX > 2. Finally, combining the strategy in the previous section for longer responses with the strategy in this section for longer user inputs allows us to construct transformers that map from arbitrary length user strings to arbitrary length responses. In order to encode a map m : [1, . . . , V ]2 \u2192[1, . . . , V ] into a single virtual token we would need a more involved construction than before. Similarly to how we discretized each element of the virtual token s1 in V levels before, we are going to now discretize it into V V levels. Each one of these levels would be one of the V V possible maps from the second user token to the response. The first user token would be used to select the corresponding element of s1. Then this scalar will be \u201cunpacked\u201d into a new vector of V elements using the first attention block. Then, the second user token will select an element from this unpacked vector, which will correspond to the target token. We construct the virtual token as follows:\nAn additional change from the previous constructions is that we are going to divide the residual stream into two sections. This is in line with the theory that different parts of the residual stream specialize for different communications needs by different attention heads (Elhage et al., 2021). We will use the first half of the residual stream to extract and \u201cunpack\u201d the correct mapping from second token to target token, while the second half of the residual stream will be used to copy the second token value so that the second attention layer can use it to extract the target. As usual, the embedding\nmatrix will be the identity matrix: E = IV . Finally, for convenience, we will also use a dummy zero virtual token that we will attend to when we want to not attend to anything. This results in context size N = 4 with the input being\nWe want the output at the last position to be the target m(X1, X2), that is\nWe want the output at the last position to be the target m(X1, X2), that is: arg max u\u22081,...,V y4,u = m(X1, X2) for any m, X1, X2. The first attention block will have three attention heads. As before, we want to extract the value of s1 that corresponds to the first token the user provided (X1) and place it in the first half of the residual stream. We want only the third position to do that, while the rest of the positions keep the first half of their residual stream with zeros. Hence we have the following fst head: W fst Q = \ufffd 02\u00d7V 1 1 0 1 0 0 1 0 \ufffd , W fst K = \ufffd 02\u00d7V 1 0 0 0 0 1 0 0 \ufffd , W fst V = \ufffd IV 0V \u00d7N 0V \u00d7V 0V \u00d7N \ufffd . The user1 head extracts the value of the first user-provided token (X1) and also places it in the first half of the residual stream: W user1 Q = \ufffd 02\u00d7V 1 1 0 1 0 0 1 0 \ufffd , W user1 K = \ufffd 02\u00d7V 1 0 0 0 0 0 1 0 \ufffd , W user1 V = \ufffd IV 0V \u00d7N 0V \u00d7V 0V \u00d7N \ufffd . And the user2 head does the same for the value of the second user-provided token (X2), placing it in the second half of the residual stream: W user2 Q = \ufffd 02\u00d7V 1 1 1 0 0 0 0 1 \ufffd , W user2 K = \ufffd 02\u00d7V 1 0 0 0 0 0 0 1 \ufffd , W user2 V = \ufffd 0V \u00d7V 0V \u00d7N 2IV 0V \u00d7N \ufffd , where the factor 2 is there because, as usual, the first linear layer will subtract 1 from everything in order to extract the value selected by the first token. This linear layer looks as usual: \u02c6Lex2 = \u02c6L[I2V , \u221212V ]. The result is that the first V elements will be 0 except one which designates which map from second user token to output we should use, and the second V elements have a one hot-encoding of the second user token. Constructing an MLP that unpacks the mapping can become quite involved so we do not provide an explicit form for it. But from the universal approximation theorems and the finiteness of the domain and range, we know that such an MLP should exist. We thus designate by unpack the MLP that decodes the first half of the residual stream to: \ufffd \ufffd\nThe first attention block will have three attention heads. As before, we want to extract the value of s1 that corresponds to the first token the user provided (X1) and place it in the first half of the residual stream. We want only the third position to do that, while the rest of the positions keep the first half of their residual stream with zeros. Hence we have the following fst head:\n\ufffd and keeps the second half unchanged\nand keeps the second half unchanged. And now, by using two attention heads, the second attention block extracts the value of the abov vector at the position designated by the second token, in a fashion not dissimilar to all the previou cases:\nAnd now, by using two attention heads, the second attention block extracts the value of the above vector at the position designated by the second token, in a fashion not dissimilar to all the previous\nAnd finally, with \u02c6Lex = \u02c6L[IV , \u22121V ], \u02c6Lsum = \u02c6L[1\u22a4 V , 0], and the same projection had as before (Equation (10)), we get the target token. The final transformer is then: A[(W fst Q , W user1 Q , W user2 Q ), (W fst K , W user1 K , W user2 K ), (W fst V , W user1 V , W user2 V )] \ufffd\u02c6Lex2 \ufffdunpack \ufffd A[(W emb Q , W user2\u2019 Q ), (W emb K , W user2\u2019 K ), (W emb V , W user2\u2019 V )] \ufffd\u02c6Lex \ufffd\u02c6Lsum \ufffdLproj \ufffdsoftmax . You can see this transformer implemented and running in practice in Section 5 here.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/86e7/86e711fb-c4bd-4a45-94a0-9e19661e75e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f35/2f35edf2-1d57-43a8-bddf-7a99bd49825e.png\" style=\"width: 50%;\"></div>\nFigure 5: The attention of the twelfth head of the first layer of LLaMA (Touvron et al., 2023). The left plot shows the attention with a prefix of length one. The second plot shows the same attention but normalized such that the attenion over the non-prefix positions sums to 1. The right plot shows the attention of the pre-trained model (without prefix). The center and the right plots are the same, illustrating that the presence of the prefix indeed only scales down the attention over the content (non-prefix positions) but does not change its relative distribution, providing empirical validation of Equation (6). The test sequence is TABLE: Fourth Round Qualifying : NEW ENTRIES THIS ROUND : 24 TEXT: Fourth round qualifying had 24 new entries. from the DART table-to-test dataset (Nan et al., 2021).\nActual pre\ufffdxed activations\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c38/9c387c0d-8eb9-4e19-9600-9a04ad8ff4c8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 4 5 5 0 5 5 6 0 6 5 7 0 7 5 8 0 8 5 9 0 9 5 1 0 0 1 0 5 1 1 0 1 1 5 1 2 0 1 2 5 Residual stream</div>\nFigure 6: The activations of the twelfth head of the first layer of LLaMA (Touvron et al., 2023). The left plot shows the activations in the presence of the prefix. The right plot shows the activations ti of the pretrained model, scaled by one minus the attention that the prefix would take and then biased in the direction WV s1. The two plots are the same, illustrating that our theory, Equation (7) in particular, also holds for real-world large transformer models. The test sequence is the same as in Figure 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fcab/fcaba470-1b1c-4292-86f4-a390a3ab28ee.png\" style=\"width: 50%;\"></div>\nPredicted activations\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7cc5/7cc5793a-561c-48ff-b8a4-d4a06a464c42.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 4 5 5 0 5 5 6 0 6 5 7 0 7 5 8 0 8 5 9 0 9 5 1 0 0 1 0 5 1 1 0 1 1 5 1 2 0 1 2 5 Residual stream</div>\nPre\ufffdx 1\nPre\ufffdx 2\nPre\ufffdx 3\nPre\ufffdx",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of understanding the theoretical limitations and capabilities of context-based fine-tuning methods such as prompting, soft prompting, and prefix-tuning in transformer models. These methods have gained popularity due to their ability to match the performance of full fine-tuning while using fewer parameters. However, the authors argue that there is a lack of theoretical understanding regarding how these techniques influence the internal computations of the model and their expressiveness limitations.",
        "problem": {
            "definition": "The main problem is the lack of theoretical understanding of how context-based fine-tuning methods influence the internal computation of transformer models and their limitations in expressiveness compared to full fine-tuning.",
            "key obstacle": "The key obstacle is that while context-based methods can effectively elicit skills present in pretrained models, they are unable to learn new tasks that require different attention patterns."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that the continuous embedding space is more expressive than the discrete token space, leading to techniques like soft prompting and prefix-tuning.",
            "opinion": "The authors believe that while soft prompting and prefix-tuning are more expressive than prompting, they have inherent structural limitations that prevent them from learning new attention patterns.",
            "innovation": "The main innovation of this work is the formal demonstration that prefix-tuning can bias the output of attention layers but cannot change the relative attention patterns over the input tokens, making it less expressive than full fine-tuning."
        },
        "Theory": {
            "perspective": "The theoretical perspective analyzes the expressiveness of different fine-tuning methods in transformer architectures, particularly focusing on the differences between prompting, soft prompting, and prefix-tuning.",
            "opinion": "The authors assume that while context-based methods can leverage pretrained skills, they cannot enable models to solve completely new tasks that require different attention mechanisms.",
            "proof": "The authors provide theoretical proofs demonstrating that prefix-tuning can only bias outputs in a fixed direction and cannot modify the underlying attention patterns."
        },
        "experiments": {
            "evaluation setting": "The authors conducted experiments using minimal transformer models trained on simple tasks to evaluate the effectiveness of prefix-tuning compared to full fine-tuning.",
            "evaluation method": "The evaluation involved testing the ability of prefix-tuning to learn new tasks and elicit latent skills from pretrained models, measuring performance through accuracy metrics."
        },
        "conclusion": "The paper concludes that while context-based fine-tuning methods like soft prompting and prefix-tuning are more expressive than prompting, they suffer from structural limitations that prevent them from learning new attention patterns. Prefix-tuning can elicit existing skills from pretrained models but struggles to learn completely new tasks.",
        "discussion": {
            "advantage": "The advantage of this paper is its theoretical insights into the limitations of context-based fine-tuning methods, providing a clearer understanding of when and why these methods succeed or fail.",
            "limitation": "A limitation of the study is that the theoretical results are primarily based on simplified models, and the applicability of these findings to larger, more complex transformers remains to be fully explored.",
            "future work": "Future work could explore the conditions under which context-based fine-tuning methods may become universal approximators and investigate their performance on more complex tasks and larger models."
        },
        "other info": [
            {
                "info1": "The authors released all code and resources used in this work to facilitate reproducibility and further studies."
            },
            {
                "info2": {
                    "info2.1": "The study highlights the importance of understanding the internal mechanisms of transformer models.",
                    "info2.2": "The findings have implications for model interpretability and alignment, suggesting that context-based methods may preserve pretrained skills."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of understanding the theoretical limitations and capabilities of context-based fine-tuning methods such as prompting, soft prompting, and prefix-tuning in transformer models."
        },
        {
            "section number": "1.2",
            "key information": "The authors argue that there is a lack of theoretical understanding regarding how context-based methods influence the internal computations of the model and their expressiveness limitations."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective analyzes the expressiveness of different fine-tuning methods in transformer architectures, particularly focusing on the differences between prompting, soft prompting, and prefix-tuning."
        },
        {
            "section number": "3.3",
            "key information": "The main innovation of this work is the formal demonstration that prefix-tuning can bias the output of attention layers but cannot change the relative attention patterns over the input tokens."
        },
        {
            "section number": "6.1",
            "key information": "The key obstacle is that while context-based methods can effectively elicit skills present in pretrained models, they are unable to learn new tasks that require different attention patterns."
        },
        {
            "section number": "6.4",
            "key information": "A limitation of the study is that the theoretical results are primarily based on simplified models, and the applicability of these findings to larger, more complex transformers remains to be fully explored."
        }
    ],
    "similarity_score": 0.7030228758680606,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/When Do Prompting and Prefix-Tuning Work_ A Theory of Capabilities and Limitations.json"
}