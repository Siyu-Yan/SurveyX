{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.00909",
    "title": "LLM-TAKE: Theme Aware Keyword Extraction Using Large Language Models",
    "abstract": "Keyword extraction is one of the core tasks in natural language processing. Classic extraction models are notorious for having a short attention span which make it hard for them to conclude relational connections among the words and sentences that are far from each other. This, in turn, makes their usage prohibitive for generating keywords that are inferred from the context of the whole text. In this paper, we explore using Large Language Models (LLMs) in generating keywords for items that are inferred from the items textual metadata. Our modeling framework includes several stages to fine grain the results by avoiding outputting keywords that are non informative or sensitive and reduce hallucinations common in LLM. We call our LLM-based framework Theme-Aware Keyword Extraction (LLM TAKE). We propose two variations of framework for generating extractive and abstractive themes for products in an E commerce setting. We perform an extensive set of experiments on three real data sets and show that our modeling framework can enhance accuracy based and diversity based metrics when compared with benchmark models.",
    "bib_name": "maragheh2023llmtakethemeawarekeyword",
    "md_text": "# LLM-TAKE: Theme-Aware Keyword Extraction Using Large Language Models\nReza Yousefi Maragheh*, Chenhao Fang*, Charan Chand Irugu*, Parth Parikh, Jason Cho, Jianpeng Xu, Saranyan Sukumar, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan Walmart Global Tech 680 W California Ave Sunnyvale, CA 94086\n# Reza Yousefi Maragheh*, Chenhao Fang*, Charan Chand Irugu*, Parth Parikh, Jason Cho, Jianpeng Xu, Saranyan Sukumar, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan Walmart Global Tech 680 W California Ave Sunnyvale, CA 94086\n{Reza.Yousefimaragheh,Chenhao.Fang,Charanchand.Irugu, Parth.Parikh, Jason.Cho, Jianpeng.Xu, Saranyan.Sukumar, MPatel, EKorpeoglu, Sushant.Kumar, Kannan.Achan}@walmart.com\nAbstract\u2014Keyword extraction is one of the core tasks in natural language processing. Classic extraction models are notorious for having a short attention span which make it hard for them to conclude relational connections among the words and sentences that are far from each other. This, in turn, makes their usage prohibitive for generating keywords that are inferred from the context of the whole text. In this paper, we explore using Large Language Models (LLMs) in generating keywords for items that are inferred from the items\u2019 textual metadata. Our modeling framework includes several stages to fine grain the results by avoiding outputting keywords that are non-informative or sensitive and reduce hallucinations common in LLM\u2019s. We call our LLM-based framework Theme-Aware Keyword Extraction (LLM-TAKE). We propose two variations of framework for generating extractive and abstractive themes for products in an E-commerce setting. We perform an extensive set of experiments on three real data sets and show that our modeling framework can enhance accuracy-based and diversity-based metrics when compared with benchmark models. Index Terms\u2014Large Language Models, keyword extraction, context-aware\narXiv:2312.00909v1\n# I. INTRODUCTION\nI. INTRODUCTION\nKeyword extraction is defined as the task generating a set of relevant keywords to summarize and characterize an input text [1]. The generated keywords are either extractive or abstractive. Extractive keywords focus on summarizing an input text using the words which exist in the text, while abstractive keywords can be inferred from the text but not exactly contained in the text [2]. The purpose of keyword extraction is to improve the efficiency of understanding texture information for a human by providing a limited number of keywords. In E-commerce settings, this task may help the customers to quickly learn the characteristics of the product from the extracted keywords, and potentially improve the customers\u2019 shopping efficiency and experience. Classic algorithms for keyword extraction tasks differ in the paradigm of strategies used to extract keywords. Some models extracting the keywords in a supervised manner while others\n*All three authors contributed equally to this research. 979-8-3503-24457/23/$31.00 \u00a92023 IEEE\ndo this in an unsupervised manner [3]. Also, generally these models either do the task in one stage or two stages. One-stage models performs the task of keywords recall set generation and keyword ranking simultaneously, while two-stage models first generate the recall set and then selects the top keywords in the importance extraction stage [4]. However, most of these models suffer from being a \u201dnarrow expert as they are very domain specific and normally work well on texts similar to their training data due to lack of enough background knowledge [5], [6]. Pre-trained Language Models (PLM) have proposed to alleviate this issue by being trained on a larger corpora of text and mapping the text tokens and segments into embedding spaces. However, due to limitations in training data, it is difficult for some PLMs to reason from input text and output results that understand the theme of the entire input text. In addition to this, the traditional language models, as well as some PLMs, are notorious for having short attention spans, which makes it hard for them to draw relational conclusions for words which are apart from each other in a text and thus, make their usage prohibitive in extracting keywords that are obtain from the entire text [7]. In other words, their usage can be limited when the language task is context dependent [8]. Large language models have recently shown the potential for language tasks. They outperform the previous PLMs by having the ability of reasoning and wider knowledge background obtained from very large corpora of training data. Their larger architectures allow them to have a larger attention span and can understand the context of the input text better [9]. In particular, they demonstrated beating state-of-the-art models in other language tasks such as summarization and sentiment analysis [10]\u2013[12]. In this paper, we propose a multi-stage framework which utilizes the power of the large language models to derive theme-aware keywords for items in E-commerce settings to further help the customers in their shopping journey. We call this LLM-based proposed framework Theme-Aware Keyword Extraction model (LLM-TAKE). Our experiments on both\nproprietary and public data sets show the efficiency of the proposed approach in improving relevant metrics when compared with the state-of-the art models. We discuss how each stage of the framework helps improve the quality of the output keywords and reduce hallucinations. The rest of the paper is organized as following. In Section II, we review the related work from the literature. In section III, we introduce and discuss the LLM-TAKE framework. In section IV, we present and analyze the experimental results. Finally, we conclude the paper in section V\nII. LITERATURE REVIEW\n# II. LITERATURE REVIEW\nTraditional keyword extraction models are based on statistical or graph-based approaches to the problem. Statistical models rely on various statistical features, such as word frequency, N-grams, location, and document grammar [3]. However, these features may not adequately capture the complex intricate relationships between words in a document. The fundamental principle of statistical approaches is to determine a given term\u2019s score using diverse statistics either within a single document or across several documents. Upon calculating the scores, the method ranks terms according to their scores and highlights the top n terms as essential keywords, with different approaches utilizing distinct methods for calculating N-gram scores (see [13], [14], [15]). In parallel to statistical-based approaches, graph-based keyword extraction has emerged as one of the most effective and widely adopted unsupervised keyword extraction models. Graph-based models represent human language as intricate networks and leverage graphs to encapsulate the multifaceted relationships that exist between words or phrases within a document [16]. For instance, [17], drawing inspiration from PageRank [18], creates TextRank, which models a document as a graph where nodes symbolize words or phrases and edges represent their connections. [19] propose PositionRank, a model that integrates the positional information of a word\u2019s occurrences into a biased TextRank, significantly enhancing its performance for longer documents. Following this, several strategies are proposed to enrich the information contained within document graphs. For example, [20] introduces TopicRank, a model aimed at allocating importance scores to topics through candidate keyword clustering. This model applies the TextRank ranking algorithm to evaluate topics and extract keywords by selecting the most indicative candidate from the highest-ranked topics. Utilizing the background information obtained from external textual corpora in traditional keyword extraction models has been a long-standing challenge [21] as incorporating external knowledge or additional features lead to further enhancement in keyword extraction task according to [22]. Pre-trained embedding models possess a large amount of information, enabling them to accurately represent the relationships between words or phrases. Consequently, pre-trained language model (PLM)-based keyword extraction has witnessed significant progress in recent years according to [3].\n[23] introduce Key2Vec, a method for training embeddings, which proved effective in creating thematic representations of scientific articles and assigning thematic weights to potential keywords. Additionally, they incorporate theme-weighted PageRank [18] to rank these keywords within their framework. However, Key2Vec is only applicable to the domain of scientific articles. [24] presente EmbedRank, which utilizes the cosine similarity between candidate keywords embeddings and the document\u2019s sentence embeddings. In light of this development, [25] proposes an embedding-based model called SIFRank which fuses the sentence embedding model SIF by [26] with the autoregressive pre-trained language model ELMo [27], thereby achieving remarkable performance in keyword extraction, particularly for concise documents. [25] further enhance SIFRank by employing document segmentation and contextual word embeddings alignment, ensuring both speed and accuracy are maintained. In addition, they introduce SIFRank+ for long documents, incorporating position-biased weight to significantly improve its performance on extended texts. To make the keywords extraction model more contextaware, AttentionRank by [28] employs a pretrained language model to determine the self-attention of a candidate within a sentence\u2019s context and cross-attention between a candidate and sentences in a document, thereby assessing the local and global significance of candidates. KeyBERT by [29], a toolkit for keyword extraction using BERT, first extracts document embedding to obtain a document-level representation. Subsequently, it extracts word embeddings for N-gram words or phrases. KeyBERT then applies cosine similarity to identify the words or phrases most similar to the document, which are considered the best keywords of the entire document. As an upgrade to KeyBERT, [30] introduce AdaptKeyBERT, a pipeline for training keyword extractors with LLM bases, by integrating regularized attention into a pre-training phase for downstream domain adaptation.\n# III. MODELING FRAMEWORK\nIn this section, we discuss our LLM-TAKE\u2019s framework for generating theme-aware keywords. We go through different stages of the framework we utilized for reducing the hallucinations which is typical to LLM-based methods.\n# A. Theme Recall Set Generation\nWe start with generating a recall set of candidates of keywords. We use a sophisticated LLM like ChatGPT from [31] for generating the recall set. As discussed in the introduction we aim at generating two sets of keywords: (i) Abstractive keywords, and (ii) Extractive keywords. We use different prompt strategies for generating each set of keywords. The following figure 1 illustrates the prompts we used for each method. Note that generating abstractive keywords may involve more reasoning as the LLM may infer the theme of the product which is not obtained from the inputted text. However, this in turn, may cause more hallucinations, and the generated\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1d3/e1d37896-f834-4771-96b6-a6dd5a0c9c6c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Prompts for abstractive method and extractive method.</div>\nkeywords may not be inferred from text. In this case, extractive keywords may be safer for some of the use cases in industrial applications. In our prompting strategies, we use the same base prompts for generating extractive and abstractive keywords, however, we explicitly add the constraint of being present in text when prompting for generation of extractive keywords. Interestingly, we observe that even when explicitly instruct the LLM to generate the output from the text, it sometimes generates the keywords which do not appear in the inputted text. Because of this one may check and drop all the keywords which do not exist in the inputted text even when prompting to obtain the extractive keywords.\nB. Hallucination Reduction and Theme Quality Improvement Steps\n# B. Hallucination Reduction and Theme Quality Improvement Steps\nLLM-TAKE also includes a series of steps to reduce the hallucinations and improve the quality of final generated keywords as discussed in the following subsections. 1) Constructing the Reference Set of Themes: To alleviate the hallucinations, we first generate a set of keywords with a computationally cheaper model for larger set of products. This computationally cheaper model can be another LLM with lower number of parameters. For reference, we call this computationally cheaper model \u201cLLM2\u201d. In our experiments of propitiatory data set, we do this for about 10 million items obtained from similar product categories. This set of itemtheme pairs act as reference for final set of items of interest. When generating set of theme-aware keywords using main LLM for any given item in the target set of items, we cross check the frequency of the generated themes in the item-theme pair dictionary. If the number of times that the a given theme appears in the reference dictionary is less than a threshold, we eliminate that generated theme. In this way, we avoid themes which are too unique to a given item of interest and hence avoiding too innovative outcomes which have a higher chance being the result of hallucinations. We speculate that some of the unique themes which are generated as the result of hallucinations has a lower chance of reappearing many times in the reference item-theme set. Thus, if a theme has appeared for many items it may be due to prevalence of theme and hence not generated as a result of hallucinations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c76/8c763293-3d12-428c-b75c-ffc4368d9b40.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Prompt for theme importance extraction.</div>\n2) Eliminating Non-Informative and General Themes: The objective of the framework is to generate themes that are informative enough and can help the users in their decision journey. Thus, a very general theme may not have the differentiating power to help in user\u2019s decision making process. Because of this we eliminate the very general words like \u201cPerfect\u201d or \u201cGreat\u201d which do not add any information value to compare and contrast the product. The initial set of general themes is obtained from Top 500 adjectives used in Oxford [32]. More keywords are added to this set of general words through manual investigation of sample keyword sets. This constitutes the first block-list we use to eliminate some of the generated keywords by LLM. 3) Eliminating Sensitive Words: Although many precautions were taken to avoid sensitive responses when training the LLM, we still observed the output of some words that might be interpreted as sensitive in some contexts. To eliminate most of these words, we generate a set of sensitive words using googleprofanity-word Github repository from [33], which contains a full list of bad words and top swear words banned by Google Inc. . This gives us a second set of block-list of words that are used to eliminate some of the generated themes/keywords by LLM.\n# C. Theme Importance Extraction\nTo revalidate the relevancy of the generated set of themes, and after cross checking the them with the reference itemtheme pairs and eliminating the non-informative and sensitive keywords, we obtain the relevancy of generated themes to the item of interest. For doing this, we perform another round of prompts asking the LLM to output a confidence score that measures how descriptive the generated keywords are for the input product. The following figure 2 illustrates this step of our framework: Interestingly, when doing this step, we observe a set of generated scores with a very low confidence scores for the relevant products. After doing this step in our proprietary data set, about 10% of item-theme pairs dropped for a score threshold of 0.2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b49d/b49ddf84-e84b-4314-b2d7-f1c89278e04a.png\" style=\"width: 50%;\"></div>\nFig. 3. Theme Ranking in our framework is based on retrieved score and frequency metric for themes.\n# D. Theme Ranking\nWe use the generated scores from the previous step, as the primary criteria of ranking. In this case, we may encounter a tie in scores for two themes. The tie is broken by ranking the theme with higher frequency in our reference item-theme set (see subsection III-B1) in higher position. In this case, we make sure to de-prioritize more unique items as they may have higher chances of being result of hallucination. If the frequency count is also equal, the keywords are ranked randomly (this is a very rare occurrence according to our experiments as the reference set of item-theme pairs are constructed on a very large number of items). Note that regardless of rank of the theme, in our framework, we eliminate all of the item-theme pairs that have a score lower than a pre-specified threshold.\n# E. Keyword Diversification\nFinally, in order to further improve the final set of keyword themes generated for the items, we perform a synonymity check to avoid extracting keywords that are semantically similar. For instance, words \u2019fun\u2019 and \u2019funny\u2019 may appear as a theme for a given product. By doing this step, we eliminate the lower-rank theme which is semantically similar to a higher rank. For obtaining the similarity scores for pairs of words, we use en core web md embedding model from SpaCy Python library to generate words embeddings [34] . Figure 5 summarizes the workflow of our proposed framework.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/953d/953db413-7551-4d16-8302-310035a63211.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4. By having a reference item-theme pair set we can measure the generality versus uniqueness for a given theme.</div>\nF. Approximating Theme Generality vs Uniqueness Normally, in E-commerce settings item spaces are divided into broad product categories like Electronics, Toys and Games, Home and Garden etc. These broad product categories are also in turn divided into subcategories. For instance, electronics category can be divided into laptops, headsets etc. In this subsection, we want to emphasise on one other capability that the reference item-theme pair set provides us. For all the items in a given product sub categories we can count the frequency of each generated theme for the items of that subcategory. This will give us a measure of the generality versus uniqueness of that generated theme among the items of that product sub category. For instance, consider the items of the \u201cboard games\u201d sub category. We observe that the theme \u201ccollaborative\u201d arises more frequently than the theme \u201cnostalgic\u201d. Meaning that more number of board games are collaborative and less number of board games are nostalgic (probably have theme of nostalgia in their design or they remind the old childhood times for their players). In this example, we may conclude that \u201ccollaborative\u201d is a more general theme than \u201cnostalgic\u201d. This will enable us to chose different themes when needing to emphasize on more unique or more general themes of the items. See Figure 4.\nIV. EMPIRICAL RESULTS\n# IV. EMPIRICAL RESULTS\nIn this section, we evaluate the performance of the proposed LLM-TAKE method on real-world data on two proprietary data sets obtained from an E-commerce platform, and DUC2001 public data. One of the proprietary includes set of items from Electronics Category (denoted by P-Electronics) and the other includes items from Toys and Games category (denoted by P-Toys). The performance of LLM-TAKE is compared against various state-of-the-art models keyword extraction models. The number of documents of the these three datasets and the number of unique keywords generated by LLM-Take and all the benchmark models are shown in table I.\n# A. Datasets\nThe proprietary dataset is comprised of 188 high-traffic items from an E-commerece platform, with their respective\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8642/8642504d-e175-4ac9-b5b9-aabe2acfbd69.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. An overview of LLM-TAKE Framework</div>\ntextual meta-data serving as documents for keyword extraction. This dataset includes 94 items from the Toys category of items and another 94 items from the Electronics category. DUC2001 [35] public dataset is also used to evaluate the performance of different models. This dataset is manually annotated in the ExpandRank paper [35]. Originally designed for document summarization, the DUC2001 dataset consists of 308 news articles covers 30 distinct news topics. On average, each article contains 740 words.\n<div style=\"text-align: center;\">NUMBER OF DOCUMENTS, NUMBER OF UNIQUE KEYWORDS GENERATED FOR EACH DATA SET, AND THE AVERAGE NUMBER OF GENERATED KEYWORDS FOR EACH DATASET</div>\nDataset\n# Documents\n# Unique Keywords\n# of keywords\nper document\nP-Electronics\n94\n179\n3.05\nP-Toys\n94\n184\n3.24\nDUC2001\n308\n2488\n8.08\n# B. Benchmark Models\nWe compare our model with both traditional keyword extraction models and embedding based keyword extraction models. We evaluate YAKE [14], SIFRank, SIFRank+ [25], KeyBERT [29] and AdaptKeyBERT [30]. For YAKE, we set the window size to be 1, deduplication threshold equal to 0.9 and n-gram length equal to 1. For SIFRank and SIFRank+, we use the codebase and the same parameters suggested by the authors. We only change the number of returned Top-n words to 3. For KeyBERT, we use the toolkit developed by\n[29]. And we implement the AdaptKeyBERT using an opensource python library AdaptKeyBERT by [30], which was built upon KeyBERT toolkit [29]. We implement the LLM-TAKE method with GPT-3.5 API by OpenAI [31]. We also generate the reference set for item-theme pairs using a computationally cheaper LLM. We test both abstractive and extractive LLMTAKE methods. Since the quantity of keywords annotated varies across documents, the number of keywords extracted (N) is set to 3. In this study, we utilize macro Precision (P), Recall (R), and F1 score (F1) at 3 to evaluate the models [36].\n# C. Performance Comparison\nA team of product experts manually evaluate and select the top keywords for each item for the proprietary dataset. 322 unique keywords are selected by this team across all 188 items, averaging 3.14 keywords per document. For the 94 Toy items, 186 distinct keywords are selected by product experts, resulting in an average of 3.24 keywords per document. In addition, the 94 electronics items have 179 unique keywords selected, with an average of 3.05 keywords per document. Table II presents the top 3 Precision, Recall, and F1 scores for all extraction methods discussed in the paper. The table is divided into two sections: the first displays benchmark results, while the second demonstrates LLM-TAKE\u2019s performance in both extractive and abstractive forms. Among the benchmark models, we observe that the YAKE model, despite being easy to implement obtains the comparable scores to other benchmark models. The SIFRank and SIFRank+ display identical outcomes, as we only truncate the top 3 keywords, which are ranked highly in both models.\n<div style=\"text-align: center;\">TABLE II COMPARISON OF LLM-TAKE WITH OTHER BENCHMARK MODELS</div>\nP-Electronics\nP-Toys\nDUC2001\nMethod\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nYAKE\n0.142\n0.139\n0.07\n0.158\n0.147\n0.076\n0.264\n0.122\n0.083\nKeyBERT\n0.129\n0.13\n0.065\n0.128\n0.117\n0.061\n0.257\n0.121\n0.082\nAdaptKeyBERT\n0.078\n0.083\n0.04\n0.11\n0.098\n0.052\n0.262\n0.124\n0.084\nSIFRank\n0.119\n0.105\n0.056\n0.188\n0.166\n0.088\n0.131\n0.06\n0.041\nSIFRank+\n0.119\n0.105\n0.056\n0.188\n0.166\n0.088\n0.131\n0.06\n0.041\nLLM-TAKE extractive\n0.371\n0.274\n0.157\n0.344\n0.239\n0.141\n0.305\n0.15\n0.1\nLLM-TAKE abstractive\n0.504\n0.464\n0.241\n0.496\n0.42\n0.227\n0.079\n0.037\n0.025\nIn the DUC2001 dataset, the AdaptKeyBERT maintains a slight edge over other benchmark models. The abstractive version of our LLM-TAKE model has achieved state-of-the-art results in P-Electronics and P-Toys. Meanwhile, the extractive version of LLM-TAKE secures the best performance in the DUC2001 dataset, which consists of long news documents. The distinct behavior of abstractive and extractive LLM-TAKE methods can be attributed to the differences in the dataset annotation process. In the DUC2001 dataset, two graduate students [35] employed extractive methods (not abstractive) for annotating the data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9828/98287b88-9ddc-41b0-86ba-07f9035a8e21.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. Screen shots of generated keywords for a TV product from the Ecommerce platform.</div>\nOn the other hand, for P-Electronics and P-Toys, the labeling process involved combining outputs from all baseline models, as well as LLM-TAKE abstractive and extractive outputs. This combined output list was then shuffled and forwarded to a team of product experts without informing them about the source method for each keyword. A team of product experts subsequently selected and ranked the top three results to serve as labels.\n# D. Online Experiment\nAfter the offline experiments, which show the state-of-theart performance, we decide to advance to an online experiment.\nIn these online tests, we display the top three keywords derived from product descriptions as highlights for specific items, see Figure 6. We introduce this feature across over 200 product categories on the e-commerce platform. Table III shows the outcomes of the online experiment, which reveals a statistically significant enhancement in various aspects, including different business metrics. We mask the name of business metrics for proprietary reasons. These findings serve as a strong indication that the LLM-TAKE methodology is indeed highly effective. This further emphasizes the importance and potential impact of the LLM-TAKE in the realm of e-commerce applications.\n<div style=\"text-align: center;\">TABLE III ONLINE EXPERIMENT RESULTS</div>\nMetrics\nPercentage Lift\nP-Value\nBusiness Metric 1\n9.76%\n0.011\nBusiness Metric 2\n6.99%\n0.025\nBusiness Metric 3\n5.17%\n0.047\n# V. CONCLUSION\nClassic keyword extraction model suffer both from limited training data as well as short attention span. Because of this, their usage becomes limited when it comes to generating context-aware and theme-aware keywords from any text document. In this paper, we propose a LLM-based framework for generating context-based and theme-aware keywords of products in E-commerce settings. We call our modeling framework Theme-Aware Keyword Extraction (LLM-TAKE) method. We propose two variations for generating extractive and abstractive keywords from the textual meta-data of products. We discuss different stages of the framework implemented to reduce the chance of generating keywords that are sensitive or not informative. We also discuss our methodology to reduce hallucinations by cross-checking any given LLM-generated themes a reference set of item-theme pairs which are generated by computationally cheaper model. Furthermore, we illustrate how this reference item-theme pair can be used as a measure of generality versus uniqueness for any generated theme. Our experiments on three annotated real world data sets show that our framework lead to higher accuracy based metrics\n# and thus, proves the capability of large language models in generating theme-aware keywords for some data sets.\nand thus, proves the capability of large language models in generating theme-aware keywords for some data sets.\nVI. LIMITATIONS\nCommon risk associated with LLMs is a possibility of hallucination where generated text can be factually inaccurate or erroneous in nature. Though we have incorporated multiple steps in our methodology to counter the hallucination which includes maintaining a reference set, eliminating noninformative words, and de-prioritizing more unique words as explained in detail in the earlier sections, probability of hallucination would still be non-zero as LLM might be challenged by limited contextual understanding due to the noise in the input.\n# REFERENCES\n[1] N. Firoozeh, A. Nazarenko, F. Alizon, and B. Daille, \u201cKeyword extraction: Issues and methods,\u201d Natural Language Engineering, vol. 26, no. 3, pp. 259\u2013291, 2020. [2] P. Over and M. Hurst, \u201cIntroduction to duc-2001: an intrinsic evaluation of generic news text summarization systems,\u201d in In Proceedings of DUC2001, 2001. [3] E. Papagiannopoulou and G. Tsoumakas, \u201cA review of keyphrase extraction,\u201d Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 10, no. 2, p. e1339, 2020. [4] M. Song, Y. Feng, and L. Jing, \u201cA survey on recent advances in keyphrase extraction from pre-trained language models,\u201d Findings of the Association for Computational Linguistics: EACL 2023, pp. 2108\u20132119, 2023. [5] T. Tomokiyo and M. Hurst, \u201cA language model approach to keyphrase extraction,\u201d in Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment, 2003, pp. 33\u201340. [6] Q. Guo, F. Zhuang, C. Qin, H. Zhu, X. Xie, H. Xiong, and Q. He, \u201cA survey on knowledge graph-based recommender systems,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 8, pp. 3549\u20133568, 2020. [7] M. Daniluk, T. Rockt\u00a8aschel, J. Welbl, and S. Riedel, \u201cFrustratingly short attention spans in neural language modeling,\u201d arXiv preprint arXiv:1702.04521, 2017. [8] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \u201cTransformer-xl: Attentive language models beyond a fixed-length context,\u201d arXiv preprint arXiv:1901.02860, 2019. [9] A. K. Lampinen, I. Dasgupta, S. C. Chan, K. Matthewson, M. H. Tessler, A. Creswell, J. L. McClelland, J. X. Wang, and F. Hill, \u201cCan language models learn from explanations in context?\u201d arXiv preprint arXiv:2204.02329, 2022. [10] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, \u201cLlama: Open and efficient foundation language models,\u201d 2023. [11] R. Y. Maragheh, L. Morishetti, R. Giahi, K. Nag, J. Xu, J. Cho, E. Korpeoglu, S. Kumar, and K. Achan, \u201cLlm-based aspect augmentations for recommendation systems,\u201d 2023. [12] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto, \u201cBenchmarking large language models for news summarization,\u201d arXiv preprint arXiv:2301.13848, 2023. [13] J. Ramos et al., \u201cUsing tf-idf to determine word relevance in document queries,\u201d in Proceedings of the first instructional conference on machine learning, vol. 242, no. 1. Citeseer, 2003, pp. 29\u201348. [14] R. Campos, V. Mangaravite, A. Pasquali, A. Jorge, C. Nunes, and A. Jatowt, \u201cYake! keyword extraction from single documents using multiple local features,\u201d Information Sciences, vol. 509, pp. 257\u2013289, 2020. [15] B. Li, X. Yang, R. Zhou, B. Wang, C. Liu, and Y. Zhang, \u201cAn efficient method for high quality and cohesive topical phrase mining,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 1, pp. 120\u2013137, 2018. [16] R. F. I. Cancho and R. V. Sol\u00b4e, \u201cThe small world of human language,\u201d Proceedings of the Royal Society of London. Series B: Biological Sciences, vol. 268, no. 1482, pp. 2261\u20132265, 2001.\n[17] R. Mihalcea and P. Tarau, \u201cTextrank: Bringing order into text,\u201d in Proceedings of the 2004 conference on empirical methods in natural language processing, 2004, pp. 404\u2013411. [18] S. Brin and L. Page, \u201cThe anatomy of a large-scale hypertextual web search engine,\u201d Computer Networks, vol. 30, pp. 107\u2013117, 1998. [Online]. Available: http://www-db.stanford.edu/\u223cbackrub/google.html [19] C. Florescu and C. Caragea, \u201cPositionrank: An unsupervised approach to keyphrase extraction from scholarly documents,\u201d in Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: long papers), 2017, pp. 1105\u20131115. [20] A. Bougouin, F. Boudin, and B. Daille, \u201cTopicrank: Graph-based topic ranking for keyphrase extraction,\u201d in International joint conference on natural language processing (IJCNLP), 2013, pp. 543\u2013551. [21] A. Aizawa, \u201cAn information-theoretic perspective of tf\u2013idf measures,\u201d Information Processing & Management, vol. 39, no. 1, pp. 45\u201365, 2003. [22] M. E. Peters, S. Ruder, and N. A. Smith, \u201cTo tune or not to tune? adapting pretrained representations to diverse tasks,\u201d arXiv preprint arXiv:1903.05987, 2019. [23] D. Mahata, J. Kuriakose, R. Shah, and R. Zimmermann, \u201cKey2vec: Automatic ranked keyphrase extraction from scientific articles using phrase embeddings,\u201d in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018, pp. 634\u2013639. [24] K. Bennani-Smires, C. Musat, A. Hossmann, M. Baeriswyl, and M. Jaggi, \u201cSimple unsupervised keyphrase extraction using sentence embeddings,\u201d arXiv preprint arXiv:1801.04470, 2018. [25] Y. Sun, H. Qiu, Y. Zheng, Z. Wang, and C. Zhang, \u201cSifrank: a new baseline for unsupervised keyphrase extraction based on pre-trained language model,\u201d IEEE Access, vol. 8, pp. 10 896\u201310 906, 2020. [26] S. Arora, Y. Liang, and T. Ma, \u201cA simple but tough-to-beat baseline for sentence embeddings,\u201d in International conference on learning representations, 2017. [27] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, \u201cDeep contextualized word representations,\u201d 2018. [28] H. Ding and X. Luo, \u201cAttentionrank: Unsupervised keyphrase extraction using self and cross attentions,\u201d in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 1919\u2013 1928. [29] M. Grootendorst, \u201cKeybert: Minimal keyword extraction with bert,\u201d Zenodo, 2020. [30] A. Priyanshu and S. Vijay, \u201cAdaptkeybert: An attention-based approach towards few-shot\\& zero-shot domain adaptation of keybert,\u201d arXiv preprint arXiv:2211.07499, 2022. [31] OpenAI. (2023) Chatgpt. [Online]. Available: https://openai.com/blog/ chatgpt [32] O. E. Dictionary, \u201cOxford english dictionary,\u201d Simpson, Ja & Weiner, Esc, vol. 3, 1989. [33] R. J. Gabriel. (2023) google-profanity-word. [Online]. Available: https://github.com/coffee-and-fun/google-profanity-words [34] M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd, \u201cspaCy: Industrial-strength Natural Language Processing in Python,\u201d 2020. [35] X. Wan and J. Xiao, \u201cSingle document keyphrase extraction using neighborhood knowledge.\u201d in AAAI, vol. 8, 2008, pp. 855\u2013860. [36] L. Derczynski, \u201cComplementarity, f-score, and nlp evaluation,\u201d in Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), 2016, pp. 261\u2013266.\n",
    "paper_type": "method",
    "attri": {
        "background": "Keyword extraction is a core task in natural language processing, but classic models struggle with short attention spans, making it difficult to generate contextually relevant keywords. This paper introduces a new framework utilizing Large Language Models (LLMs) to generate theme-aware keywords for e-commerce products, addressing limitations of previous methods.",
        "problem": {
            "definition": "The problem is the difficulty in generating keywords that accurately summarize and characterize input text, particularly in e-commerce settings where context is crucial for understanding product features.",
            "key obstacle": "Existing keyword extraction methods are often domain-specific and fail to generalize across different contexts due to their limited attention spans and background knowledge."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of LLMs to understand context better than traditional models, enabling them to generate more relevant and diverse keywords.",
            "opinion": "The proposed method, LLM-TAKE, aims to utilize LLMs to derive theme-aware keywords that enhance the shopping experience by providing customers with more informative product descriptions.",
            "innovation": "LLM-TAKE differs from existing approaches by incorporating a multi-stage framework that reduces hallucinations and improves keyword relevance through a reference set of item-theme pairs."
        },
        "method": {
            "method name": "Theme-Aware Keyword Extraction",
            "method abbreviation": "LLM-TAKE",
            "method definition": "LLM-TAKE is a multi-stage framework that generates theme-aware keywords for e-commerce products using Large Language Models.",
            "method description": "The method involves generating both extractive and abstractive keywords through a series of steps aimed at enhancing accuracy and reducing irrelevant outputs.",
            "method steps": [
                "Generate a recall set of candidate keywords using LLMs.",
                "Reduce hallucinations by cross-referencing generated themes with a reference item-theme set.",
                "Eliminate non-informative and sensitive themes.",
                "Rank themes based on confidence scores and frequency.",
                "Perform synonymity checks to avoid semantic redundancy."
            ],
            "principle": "The effectiveness of LLM-TAKE is based on its ability to leverage LLMs' larger attention spans and reasoning capabilities to produce contextually relevant keywords."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three datasets: two proprietary datasets from e-commerce (P-Electronics and P-Toys) and the DUC2001 public dataset, comparing LLM-TAKE against various benchmark models.",
            "evaluation method": "Performance was assessed using macro Precision, Recall, and F1 scores at a keyword count of 3, with manual evaluation by product experts for the proprietary datasets."
        },
        "conclusion": "The LLM-TAKE framework demonstrates significant improvements in generating context-aware keywords compared to traditional methods, effectively addressing the limitations of short attention spans and domain specificity in keyword extraction.",
        "discussion": {
            "advantage": "LLM-TAKE provides enhanced accuracy and diversity in keyword extraction, making it particularly suitable for e-commerce applications where context is critical.",
            "limitation": "Despite efforts to reduce hallucinations, the risk of generating factually inaccurate or irrelevant keywords remains due to the inherent challenges of LLMs in understanding context.",
            "future work": "Future research could focus on refining the hallucination reduction techniques and exploring the application of LLM-TAKE in other domains beyond e-commerce."
        },
        "other info": {
            "authors": [
                "Reza Yousefi Maragheh",
                "Chenhao Fang",
                "Charan Chand Irugu",
                "Parth Parikh",
                "Jason Cho",
                "Jianpeng Xu",
                "Saranyan Sukumar",
                "Malay Patel",
                "Evren Korpeoglu",
                "Sushant Kumar",
                "Kannan Achan"
            ],
            "affiliation": "Walmart Global Tech",
            "email": "Reza.Yousefimaragheh,Chenhao.Fang,Charanchand.Irugu, Parth.Parikh, Jason.Cho, Jianpeng.Xu, Saranyan.Sukumar, MPatel, EKorpeoglu, Sushant.Kumar, Kannan.Achan}@walmart.com",
            "arxiv": "arXiv:2312.00909v1"
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The proposed method, LLM-TAKE, aims to utilize LLMs to derive theme-aware keywords that enhance the shopping experience by providing customers with more informative product descriptions."
        },
        {
            "section number": "2.1",
            "key information": "Keyword extraction is a core task in natural language processing, crucial for summarizing and characterizing input text, particularly in e-commerce settings."
        },
        {
            "section number": "2.3",
            "key information": "LLM-TAKE is a multi-stage framework that generates theme-aware keywords for e-commerce products using Large Language Models."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of LLM-TAKE is based on its ability to leverage LLMs' larger attention spans and reasoning capabilities to produce contextually relevant keywords."
        },
        {
            "section number": "5.1",
            "key information": "LLM-TAKE provides enhanced accuracy and diversity in keyword extraction, making it particularly suitable for e-commerce applications where context is critical."
        },
        {
            "section number": "10.1",
            "key information": "Despite efforts to reduce hallucinations, the risk of generating factually inaccurate or irrelevant keywords remains due to the inherent challenges of LLMs in understanding context."
        }
    ],
    "similarity_score": 0.7605479262100364,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLM-TAKE_ Theme Aware Keyword Extraction Using Large Language Models.json"
}