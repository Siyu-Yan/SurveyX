{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.08329",
    "title": "Understanding User Experience in Large Language Model Interactions",
    "abstract": "In the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence. This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration. This gap in research becomes increasingly critical as LLMs become more integrated into people's everyday and professional interactions. This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations. Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification. Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns. This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs. Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments. This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications.",
    "bib_name": "wang2024understandinguserexperiencelarge",
    "md_text": "# Understanding User Experience in Large Language Mode Interactions\n# Understanding User Experience in Large Language Model Interactions\nJIAYIN WANG, Tsinghua University, China WEIZHI MA, Tsinghua University, China PEIJIE SUN, Tsinghua University, China MIN ZHANG, Tsinghua University, China JIAN-YUN NIE, University of Montreal, Canada\nIn the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence. This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration. This gap in research becomes increasingly critical as LLMs become more integrated into people\u2019s everyday and professional interactions. This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations. Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification. Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns. This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs. Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments. This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications. CCS Concepts: \u2022 Human-centered computing \u2192Empirical studies in collaborative and social computing; Empirical studies in HCI; \u2022 Computing methodologies \u2192Natural language processing. Additional Key Words and Phrases: large language model, user intent, user satisfaction ACM Reference Format: Jiayin Wang, Weizhi Ma, Peijie Sun, Min Zhang, and Jian-yun Nie. 2024. Understanding User Experience in Large Language Model Interactions. In . ACM, New York, NY, USA, 20 pages. https://doi.org/10.1145/nnnnnnn. nnnnnnn\n# CS Concepts: \u2022 Human-centered computing \u2192Empirical studies in collaborative and social comuting; Empirical studies in HCI; \u2022 Computing methodologies \u2192Natural language processing.\nAdditional Key Words and Phrases: large language model, user intent, user satisfaction\n# 1 INTRODUCTION\nThe rapid evolution of Large Language Models (LLMs) has marked a significant milestone in the field of artificial intelligence and computer technology. With their expanding capabilities, LLMs have given rise to new user interfaces, conversational-based service systems, and swiftly amassed a considerable user base. Services like ChatGPT, characterized by their fluent comprehension and expression capabilities, rich and profound world knowledge, and reasoning abilities, are equipped to assist users in accomplishing various everyday and professional tasks [9].\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym \u2019XX, June 03\u201305, 2024, Woodstock, NY \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn\nDespite their increasing involvement in interactions with humans, most research on LLMs has predominantly focused on assessing and improving their abilities as independent intelligence. For example, there are a lot of benchmarks for evaluating the ability of large language models on specific tasks, such as world knowledge and problem-solving (MMLU [26]), common sense reasoning (HellaSwag [71], WinoGrande [49]), grade school exams (AI2 Reasoning Challenge (ARC) [14], GSM-8K [15]), coding (HumanEval [11]), etc. These benchmarks give overall scores to create objective ability leaderboards that show how close different models are to general intelligence. However, this evaluation approach might not align with the actual needs of users who employ LLMs as collaborative tools with both factual and creative intentions. Furthermore, relying solely on overall performance scores can be misleading; a model with a lower general score might outperform a higher-scoring model in certain situations, depending on specific scenarios and user expectations. This discrepancy highlights the need for user-centric, fine-grained evaluation methods that reflect the practical utility of LLMs in diverse scenarios. This paper aims to bridge this research gap as few studies examine LLMs as user-centric interfaces, precisely their effectiveness and capability in meeting user needs from the human-AI collaboration standpoint. To investigate the practical performances of LLMs in complex real-world scenarios, recognizing the diverse intents of users behind the simple inputs is essential for customizing these models to align more closely with user expectations, enabling better user experience and higher efficiency and utilities. In this paper, we aim to answer the following research questions: (RQ1) What are the primary user intents for engaging with conversational interfaces powered by large language models (LLMs)? (RQ2) How do users perceive their experience when interacting with current LLM services in real-world settings? (RQ3) What key concerns do users have for using large language models? (RQ4) What are future directions in building user-centered large language models for better human-AI collaboration?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8d8/c8d8c731-b2ed-466a-af25-a6c8b9f82e0c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig. 1. In this work, we (1) propose the taxonomy of user intents when engaging with large language model nterfaces, (2) design and conduct a survey to understand user satisfaction with current LLMs, (3) conclude 1 insightful findings on usage frequency, user experience, and concerns with LLMs, (4) discuss 6 research directions for future human-AI collaboration studies.</div>\nOur study begins by defining a taxonomy of user interaction intents with LLMs based on related analysis and real-world user logs. This classification paves the way for further research into humanLLM interaction dynamics. Following the intent categorizations, we conducted an in-depth user survey to assess usage frequency, user experience, and concerns when engaging with large language\nmodels. With contributions from 411 participants, we obtained valuable firsthand perspectives on user satisfaction with existing LLM interfaces. Notably, the dataset compiled from this survey will be publicly available alongside the publication of this paper. We then conduct a thorough empirical analysis of this data, leading to 11 key findings. The insights gleaned from the user study inform our discussion on 6 future research directions in large language model development, highlighting areas in human-AI collaboration that are not fully explored. To summarize, our main contributions are as follows: We (1) develop a comprehensive taxonomy of user intents for interacting with general large language model interfaces, which is grounded by real-world logs and human verification, (2) design and implement a survey to gauge user satisfaction with large language models and collect 411 high-quality anonymous user feedback, which will be released along with the publication of this paper, (3) analyze and present 11 insightful findings on the frequency of usage, user experiences across different scenarios, and prevalent concerns regarding large language models, (4) identify and articulate gaps in existing LLM research with real-world applications, pointing out 6 directions for future studies to enhance user interactions with generative AI interfaces. In conclusion, our study advocates a user-centered approach, emphasizing the need to develop LLMs that are not only technically sophisticated but also genuinely beneficial in human-AI collaborations. This perspective is essential for advancing AI in a way that truly resonates with human needs and real-world utilities.\nmodels. With contributions from 411 participants, we obtained valuable firsthand perspectives on user satisfaction with existing LLM interfaces. Notably, the dataset compiled from this survey will be publicly available alongside the publication of this paper. We then conduct a thorough empirical analysis of this data, leading to 11 key findings. The insights gleaned from the user study inform our discussion on 6 future research directions in large language model development, highlighting areas in human-AI collaboration that are not fully explored. To summarize, our main contributions are as follows: We (1) develop a comprehensive taxonomy of user intents for interacting with general large language model interfaces, which is grounded by real-world logs and human verification, (2) design and implement a survey to gauge user satisfaction with large language models and collect 411 high-quality anonymous user feedback, which will be released along with the publication of this paper, (3) analyze and present 11 insightful findings on the frequency of usage, user experiences across different scenarios, and prevalent concerns regarding large language models, (4) identify and articulate gaps in existing LLM research with real-world applications, pointing out 6 directions for future studies to enhance user interactions with generative AI interfaces. In conclusion, our study advocates a user-centered approach, emphasizing the need to develop\n# 2 RELATED WORK 2.1 User Intent Analysis\n# 2 RELATED WORK\n# 2.1 User Intent Analysis\nThe exploration of user intent in interacting with AI has been a focal point of research. Extensive work has been conducted on user intents in various information-seeking processes, including web search [29, 70], product search [56], multimedia search [33, 59], question-answering and conversational search [10, 46, 47, 65]. For more recent areas like generative AI, there have been only a few works in text-to-image generation systems [68] and the large language model powered Bing Chat [52], which is also a search-guided product [32]. These studies have collectively underscored the significance of understanding user intent and have effectively categorized and used user intents within certain interfaces. Classfication methods include query log analysis [3, 28] and automated taxonomy generation [52]. The proposed wellestablished intent taxonomy in the search domain categorizes queries as navigational, informational, and transactional [6]. Works [61] further discuss personalized search algorithms after understanding user intents. While user intent is extensively studied in traditional information-seeking services, the advent of generative AI, especially LLMs, introduces new interfaces that are not yet fully explored. For instance, conversational search studies categorize intents into original, repeat, clarifying, or followup questions [47], focusing mainly on information-seeking behavior, which may not align well with the broader range of interactions in general LLM-powered services, although they share the same multi-round and natural language based characteristics with current LLM services. The research that most closely aligns with LLM interactions has primarily used closed-source logs from Bing Chat, a search-guided product [32], and employed GPT-4 alongside human verification to generate intent taxonomies [52]. The taxonomy driven from this work lays the groundwork for our study, as detailed in Section 3.1.2. Our research takes this foundation a step further to interactions within the general LLM-powered environment.\n# 2.2 Evaluation of Large Language Models\nBoth LLM developers release technical reports and research field conduct benchmarks and surveys, most of which are ability-driven evaluations. Technical reports detail the capabilities of models like GPT-4 [1], which showcases proficiency across professional exams, academic benchmarks, and performance in various languages and modalities. Similarly, Mistral [30] report highlights performance across tasks like commonsense reasoning, world knowledge, reading comprehension, Math, code, and popular aggregated results, like MMLU [26], BBH [57], and AGI Eval [73]. Additionally, some reports incorporate human-in-the-loop evaluations. The Gemini technical report [60] includes a small section on human preference evaluations, assessing aspects like creativity, instructionfollowing, and safety through side-by-side blind evaluations by human raters. Llama [62] also conducts human evaluations focusing on helpfulness and safety. However, these reports often lack details for comparative evaluations between models, and most provide only overall scores without delving into fine-grained performance across different scenarios. In the broader research landscape, surveys on LLM evaluations offer a comprehensive overview. One such survey[9] categorizes evaluation criteria into areas of natural language processing [2, 38], robustness/ethics/biases/trustworthiness[64], social science [22], natural science and engineering [7], medical applications [21], agent applications [27] and other applications like education [17], search and recommendation [16], personality testing [4] and specific tasks [36]. This classification underscores the multi-dimensional nature of LLM evaluations, but these tasks are often pre-defined without fully understanding the coverage and difficulty degree of real-world user usage.\n# 2.3 Empirical Studies on human-AI Collaborati\nThe field of human-AI collaboration has seen extensive research across various domains, including medical AI [58], legal analysis [12], tourism [50], music [13], and even areas as unique as romantic love [55]. Additionally, general guidelines on trust in AI have also been explored [63]. These studies delve into critical aspects such as human attitudes towards AI, the development and impact of trust in AI systems, and how AI influences human decision-making [34, 43]. These diverse research areas collectively contribute to a deeper understanding of the dynamics in human-AI interactions across different fields and contexts. As for the new generative AI interfaces, user experience, such as intents and satisfaction, with general large language models, has been unexplored. Our research hopes to provide a initial step and call for more follow-up work on developing powerful generative AI from a user perspective.\n# 3 REAL-WORLD USER INTENTS FOR ENGAGING WITH LARGE LANGUAGE MODELS (RQ 1)\nUnderstanding the diverse user intents is essential for tailoring services at a more granular level to suit user demands. In this context, our work begins by understanding users\u2019 primary intents and establishing a taxonomy of user interaction with general-purpose LLMs grounded in related analysis, real-world logs, and further user studies.\n# 3.1 Taxonomy Development\n3.1.1 Step 1: Generation Based on Related Literature. As presented in the previous section, related studies have extensively explored user intents in information systems, with a limited focus on new large language model interfaces. Notably, a line of research has proposed user intent taxonomies specifically for LLMs [52], utilizing closed-source conversation logs from Bing Chat, which is a mainly search-oriented product [32]. We adopt this taxonomy as our starting point, initiating a validation process to assess its applicability to general LLM interfaces.\n3.1.2 Step 2: Validation through Real-World Logs. The validation process in related literature [52] involved two human coders discussing and deliberating on 30 closed-source Bing Chat conversation segments. We proceed the taxonomy further on open-source ChatGPT conversation logs (ShareGPT 1) to further assess its applicability in general LLM interfaces. We randomly selected 50 English conversation logs and engaged 3 human annotators for validation. Initially, annotators independently assessed 10 logs, followed by a phase of discussion and alignment. Subsequently, the annotators collaboratively reviewed and annotated 20 logs in unison. In the final stage, they independently annotated another 20 conversations, culminating in a consensus on the taxonomy. This iterative process leads to an enhanced taxonomy (see Figure 2), including adding three new intents and consolidating one original category. \u201cSeek Creativity\u201d and \u201cAsk for Advice\u201d are added, broadening the taxonomy\u2019s scope to encompass the general LLM interface usage rather than limiting it to search-related interfaces. The \u201cLearning\u201d category was merged into \u201cInformation Retrieval\u201d and \u201cSolve Problem\u201d depending on whether users acquire direct or inferential information, as \u201cLearning\u201d aligns greatly with their information-seeking traits. Additionally, \u201cAPI Usage\u201d was added to cover interactions through programming interfaces instead of graphical user interfaces. Consequently, we establish a refined user intent taxonomy for general large language model conversational interfaces.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/338d/338d92b8-d53a-4f41-8d1b-619bd97e5262.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. User Intent Taxonomy.</div>\n3.1.3 Step 3: Testing via User Survey. This taxonomy, especially the newly proposed intents, is further examined through the following user study that involves 411 participants. For details about the users, please see Section 4.2.\n# 3.2 Classification result\nThis section presents our user intent taxonomy when engaging with general large language model They are introduced in alphabet order (excluding others): (1) Ask for Advice Career development, personal counseling, gift recommendation, Creating personal schedules, travel plans, shopping lists, etc. (2) Information Retrieval Fast and direct access to factual information (3) Leisure Movie, music, or trip recommendations, gaming, and other entertaining activities (4) Seek Creativity Brainstorming for inspiration, innovative ideas, etc. (5) Solve Problems in Specialized Areas Seek answers, explanations or learn in the fields of engineering (especially coding), natura sciences, humanities, social sciences, etc.\nThis section presents our user intent taxonomy when engaging with general large language models. They are introduced in alphabet order (excluding others):\n(6) Text Assistant Summarize, translate, revise, generate text, etc. (7) Use through API Use through Application Programming Interface instead of graphic user interfaces Utilize, test, and explore LLM capabilities, such as evaluating it on various tasks, simulating agents, environments, or datasets, etc. (8) Others Uses that cannot be covered in the above categories\n# 4 DESIGN OF USER STUDY\nWith the established understanding of user intents, we conducted a detailed survey on user satis faction with current large language models, including usage patterns, experience, and concerns.\n# 4.1 Questionnaires\nThe survey contains 12 questions, with 10 required and 2 optional (see Appendix A for detailed questions), and takes 5-10 minutes to complete. It is composed of the following five parts: Q 1-2 Usage Patterns 1 Services Used 2 Frequency of Usage Q 3-9 User Experience across Intents 3-4 Intents Distribution Choose the intents that they have used before. Opinions about the above intent taxonomy (optional). 5 User Satisfaction across Intents 6-8 User Expectation for Different Answer Types across Intents Choose between 3 pairs of answer types: detailed or concise, factual or creative, professional knowledge or common sense. 9 User Expectation for Tool Utilization across Intents Tools include web browsing, input analysis, personalization, programming, mathematical operations, documentation generation, and multimedia creation. Q 10 Anchor Question 10 If the user does not follow the instructions (select B for this question), this questionnaire would be an invalid response. This helps to control the feedback quality. Q 11 Major Concerns 11 Identify aspects of the system that need optimization, such as hallucinations, long context processing, multi-modal understanding, personalization, privacy, and safety, etc. Q 12 Other Comments 12 Comments about the questionnaire or large language model interfaces (optional).\nThe survey contains 12 questions, with 10 required and 2 optional (see Appendix A for detailed questions), and takes 5-10 minutes to complete. It is composed of the following five parts: Q 1-2 Usage Patterns 1 Services Used 2 Frequency of Usage Q 3-9 User Experience across Intents 3-4 Intents Distribution Choose the intents that they have used before. Opinions about the above intent taxonomy (optional). 5 User Satisfaction across Intents 6-8 User Expectation for Different Answer Types across Intents Choose between 3 pairs of answer types: detailed or concise, factual or creative, professional knowledge or common sense. 9 User Expectation for Tool Utilization across Intents Tools include web browsing, input analysis, personalization, programming, mathematical operations, documentation generation, and multimedia creation. Q 10 Anchor Question 10 If the user does not follow the instructions (select B for this question), this questionnaire would be an invalid response. This helps to control the feedback quality. Q 11 Major Concerns 11 Identify aspects of the system that need optimization, such as hallucinations, long context processing, multi-modal understanding, personalization, privacy, and safety, etc. Q 12 Other Comments 12 Comments about the questionnaire or large language model interfaces (optional).\n# 4.2 Participants\nThe survey was distributed through WeChat Moments for the Chinese version and via WeChat Moments, a graduate program for international students, and X for the English version. The data collection phase lasted one month, with participants limited to a single submission. As we do not record any personal information in the survey, such as age, gender, or profession, the only demographic analysis is the IP distribution automatically recorded by the questionnaire, as detailed in Appendix B. As the survey is spread mainly through social media of graduate students and professors, there might be demographic bias. This is further discussed in Section 7.\n# 5 RESULTS ON USER ENGAGEMENT WITH LLMS (RQ 2 AND 3) 5.1 Feedback Statistic\n# 5 RESULTS ON USER ENGAGEMENT WITH LLMS (RQ 2 AND 3)\n# 5.1 Feedback Statistic\nWe collected 411 feedback, including 297 for the Chinese version and 114 for the English version. All the feedback passes the anchor question and is treated as valid responses. In the following subsection, we will report the usage frequency, intent analysis, user satisfaction, expected response type, tool utilization, and major concerns about LLM interfaces. Note that each section is labeled with the corresponding questionnaire number (Qx) and has takeaway findings.\n# 5.2 Usage Frequency (Q2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/00e3/00e3848f-c7e4-42eb-97e7-c73c2624b6b0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e697/e697f2da-92e4-4969-bf52-83c68e5e3960.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Chinese questionnaire (b) English questionnaire</div>\nFig. 3. Usage Frequency of the LLM-powered interfaces. Results show that a great number of users intera with large language models on a daily basis.\nThe usage frequency data, as illustrated in Figure 3, reveals insights into the popularity of large language models. Notably, half of the English and 42.09% of Chinese respondents report daily use of LLMs, emphasizing their growing importance in everyday activities. Additionally, approximately 80% of participants from both language groups engage with LLMs at least weekly. While acknowledging the potential demographic bias as introduced in Section 4.2, it remains an incontrovertible observation that within this sampled group, large language models are extensively utilized. Compared to a small percentage (2.36%) of Chinese users who have never used LLMs, the complete adoption among English respondents suggests a potentially higher market penetration in English-speaking demographics. This trend may be influenced by various factors, including perhaps differences in product functionality across languages and the general willingness of populations to embrace new technologies\nFinding 1: Large language model interfaces are used at least weekly by around 80% of participants.\n# 5.3 Intent Analysis (Q3,4)\n5.3.1 Usage Distribution. Analyzing user intent distribution in the language interface, as shown in Figure 4, reveals patterns in real-world applications of large language models. Predominantly, LLMs are used under intents Text Assistant, Information Retrieval, and Solve Problem, indicating a high demand for tasks involving information seeking and processing. This trend likely stems from user familiarity with existing tools such as search engines and high demands for paperwork and professional assistants. This result suggests a lower barrier to adopting\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40b8/40b83c53-b78d-412e-ad82-e466c833b9db.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Fig. 4. User Intent Distribution: the percentage of users who reported using LLMs under each intent. T intents are ranked from top to bottom according to their frequency of usage in the Chinese questionnair</div>\nthese new interfaces as productivity-enhancing tools. The newly proposed intents, Seek Creativity and Ask for Advice, resonate with about 40% of users, validating the inclusion of these categories in our intent taxonomy. However, engagement with LLMs for Leisure purposes is considerably lower, possibly due to the models\u2019 focus on improving objective metrics like fact-based accuracy, which might overlook elements desired in leisurely interactions, such as serendipity and humor. Notably, English users exhibit a higher propensity for Use through API, suggesting a greater inclination towards embracing new AI tools for scale-up developments. This discrepancy in using LLMs for subjective versus objective tasks underscores the importance of conducting tailored analyses across different intents. Such fine-grained assessments are crucial for developing LLMs that align closely with human needs and behaviors, fostering better human-AI collaboration. Additionally, the intent distribution points to opportunities for growth in personal and recreational uses of LLMs, which might be overlooked by current research. Observations also indicate these interfaces need to adapt to diverse cultural and linguistic needs. Note that in the Others option, No one reports valid intents besides our proposed categories. This sideways validates our taxonomy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bf09/bf09677b-d57b-4873-9b40-b4c21f3dc235.png\" style=\"width: 50%;\"></div>\nFig. 5. Pairwise Relationships between Intents: we execute a chi-square test to scrutinize the interdependence of user engagement with each intent. Pairs exhibiting a p-value below 0.05 were identified, signifying a statistically significant correlation. This analytical approach reveals the seven intents distributed across three clusters: Objective Usage through GUIs, Subjective Usage through GUIs, and Usage through APIs.\n5.3.2 Intents Analysis. In our study, we explore the interrelationships among user intents based on data indicating whether users engage with each specific intent. To this end, we employ a chi-square test to assess the independence of every pair of intents, analyzing users\u2019 engagement patterns across these intent combinations. The outcomes of this examination are depicted in Figure 5. We draw connections, represented by lines, between intent pairs where the chi-square test yields a p-value less than 0.05. This threshold indicates a statistically significant association, suggesting that the connected intents are not independent. Then, based on this statistical relevance, we cluster the seven intents into three groups. They are characterized as objective and subjective usage through graphical user interfaces (GUIs) and usage through application programming interfaces (APIs).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a9ff/a9ffa810-a02d-4f1d-889c-aebc75c2c093.png\" style=\"width: 50%;\"></div>\n# 5.4 User Satisfaction (Q5)\nBased on the above analysis of user intents, we further investigate the user self-reported satisfaction in different scenarios and the relationship between satisfaction and usage percentage. 5.4.1 Rating Analysis. We present the dissatisfaction, neutral, and satisfaction ratios between intents, as shown in Figure 6. Note that for each scenario, we only focus on users who have reported using LLMs under this intent, ignoring the N/A feedback.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d1c/9d1c8d3f-59d0-4595-a231-b0df877abdaa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. User Satisfaction for engaging with LLMs. We show the ratio of each rating level among users wh have used that intent. Vertical coordinates are sorted according to frequency of use in Chinese feedback.</div>\nHere are a few interesting insights from the user satisfaction statistics: Text Assistant elicited the highest satisfaction rates across both English and Chinese users, with over 80% reporting being satisfied or very satisfied. This suggests that this conversational service are well-suited to language-based assistance tasks. Seeking Creativity use cases had the highest negative feedback. Around 9% of Chinese users and 18% of English users reported not being satisfied. This indicates that current LLMs have room for improvement when generating novel or imaginative outputs. While Solving Problem ranked second top for English users, satisfaction was much lower for Chinese at around 58%. This cross-cultural gap highlights the need for models to be tailored to different contexts to be equally effective globally. Satisfaction was consistently higher and dissatisfaction lower for most Chinese use cases than\nin English. This hints at sociocultural norms influencing evaluations to some extent. Besides, the varying degrees of satisfaction between English and Chinese users across different scenarios highlight the impact of cultural and linguistic differences on user experience with LLMs. This suggests a need for more localized and culturally aware services to enhance user satisfaction across the globe. In summary, textual conversations are effectively supported, but creative and problem-solving capabilities need more tailored development to satisfy diverse global users. The responses also underscore the importance of considering cultural and linguistic nuances in LLM development and application. Addressing such disparities could further expand the benefit of human-LLM interactions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/97d3/97d38680-f125-49cc-8fe9-8e3c63d431f8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7. User Usage Percentage and Rating across Intents. Usage percentage represents the ratio of users who used LLMs under the intent. User ratings are calculated by assigning 1-5 to \"very dissatisfied\" - \"very satisfied\" and then averaging the scores of the users who participated under these intentions. For Chinese feedback, when \"use through API\" is ignored and only use through the GUI is considered, the frequency of use and user satisfaction for each intent is approximately a U-shape.</div>\n5.4.2 Satisfaction across Usage Frequency. For Chinese feedback, there\u2019s a U-shaped correlation between usage frequency and user satisfaction for each intent when ignoring API use and focusing only on GUI interactions. For English feedback, a similar trend of \"high on both sides, low in the middle\" exists. This implies that users are either satisfied with frequently used features or find less frequently used features surprisingly delightful. Both users and developers need to enhance the visibility and appeal of these hidden gems within LLM platforms, capitalizing on the strengths of lesser-used features and improving medium-frequency collaboration.\nFinding 5: User studies verify that LLMs are highly effective in text manipulation tasks. Finding 6: Subjective areas, such as Seeking Creativity, require further advances to boost user satisfaction. Finding 7: When both frequencies of use and satisfaction are considered, they approximate a U-shape: both highly and infrequently used scenarios yield higher satisfaction levels.\n# 5.5 Expected Response Types (Q6,7,8)\nIn this section, we analyze user expectations under diverse scenarios. The results are shown in Figure 8. Overall, our analysis reveals significant variation in expectations across different intents. Concerning response length, under Leisure scenarios, less than 40% of both Chinese and English respondents indicate a preference for detailed responses. This trend appears to contradict current reward methodologies that encourage more extended responses [37]. This discrepancy highlights a\nFigure 8. Overall, our analysis reveals significant variation in expectations across different intents. Con cerning response length, under Leisure scenarios, less than 40% of both Chinese and English respondents indicate a preference for detailed responses. This trend appears to contradict current reward methodologies that encourage more extended responses [37]. This discrepancy highlights a\npotential misalignment between user preferences and evaluation criteria in certain scenarios. In the context of factual versus creative responses, less than 40% of feedback in Ask for Advice, Text Assistant, Leisure, and Seek Creativity favors factual answers. This observation matches the focus on discovering creative capabilities of LLMs [53, 54], as they are predominantly trained to adhere to existing contexts. When considering professional knowledge versus common sense, only the Solve Problems and Use through API categories exceed a 50% preference for professional knowledge in both Chinese and English feedback. This trend is partially reflected in current LLM evaluations, which often include professional exams and common sense reasoning benchmarks. However, the former tends to be more emphasized compared to the latter [1], suggesting a possible imbalance with our observed user expectations. These findings indicate that while LLMs are evaluated on various dimensions, there may be a misalignment with actual user preferences in certain scenarios. Understanding these variances is essential for refining LLMs to better suit real-world user needs in diverse interaction contexts.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a7c/0a7ca118-3c64-4f75-9523-a80a98741e81.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8. Expected Response Type under Different Intents. Intents are ordered clockwise according to user expectations of the profession. The grid spacing is 25%, with 0% at the center and 100% at the periphery.</div>\n<div style=\"text-align: center;\">5.6 Tool Utilization (Q9)</div>\nFig. 9. Tool Utilization: users vote for the tools they think are helpful in each scenario. We present the\n<div style=\"text-align: center;\">Fig. 9. Tool Utilization: users vote for the tools they think are helpful in each scenario. We present the percentage of votes and plot a heat map from red to green based on the magnitude of the values. Since both Chinese and English feedback versions have the same pattern, we show the joint results. Both the horizontal and vertical coordinates are sorted according to overall rating percentages.</div>\nAnalyzing the results horizontally, from the intent perspective, reveals a distinct alignment of specific tools with each intent. For instance, Text Assistant commonly utilizes Doc/PPT Generation,\nInformation Retrieval leans on Web Browsing, and Problem Solving frequently employs Programming and Math Operations. This relationship underscores the importance of fine-grained intent categorization, which allows for rapid adaptation of appropriate external tools once the user intent is detected. Such adaptability enhances the relevance of responses and improves the general base model\u2019s capability in various scenarios. Vertically examining the data highlights the significant expectation of Personalization across all subjective scenarios, including Seek Creativity, Ask for Advice, and Leisure. The popularity of recommender systems shows that users expect systems to provide different services even with the same intent as their characteristics and preferences are different. As user-centric services, LLMs need to learn and respond to this dynamic need. These observations collectively suggest that users expect LLMs to be multifaceted, capable of accurately solving complex professional tasks, and rich in providing personalized or novel responses.\nFinding 9: Users anticipate specific tool utilization based on intent, underscoring the necessity of fine-grained scenario segmentation based on user intent. Finding 10: Personalization ability is valued across all subjective usage of LLMs (Seek Creativity, Ask for Advice, and Leisure).\n# 5.7 Major Concerns (Q11,12)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40de/40dedc79-8fd4-4975-a4a9-1aa0e25b4b67.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">mprovements about current LLM interfaces. Vertical coordinate</div>\n<div style=\"text-align: center;\">Fig. 10. User Concerns and Expected Improvements about current LLM interfaces. Vertical coordina sorted according to voting percentages.</div>\nUser concerns and desired improvements for large language models (LLMs) can be broadly classified into two categories: Capability and Trustworthiness. The predominant three concerns, including Hallucination, Long Context Processing, and Multimodal Ability, each surpassing 50% in user votes, highlight the demand for LLMs to handle diverse and streaming inputs proficiently. In addition to performance, trustworthiness emerges as another aspect of user concerns. This encompasses ensuring private and safe interactions, which is critical for user confidence and acceptance of this emerging technology. Effectively addressing both these dimensions \u2013 capability and trustworthiness \u2013 is essential for the successful integration and broader acceptance of LLMs in human-AI collaborations.\n<div style=\"text-align: center;\">elf-reported expected improvements in large language models. This is besides the concerns of Long Context Processing, Multimodal Ability, Personalization, Privacy, and Safety.  Concerns in the option Type</div>\n improvements in large language models. This is besides the concerns of sing, Multimodal Ability, Personalization, Privacy, and Safety.  option Type\n 1. User self-reported expected improvements in large language models. This is besides the concer\ncination, Long Context Processing, Multimodal Ability, Personalization, Privacy, and Safety.\nUser Concerns in the Others option\nType\nProfessionalism, although it can answer my question, but it\ntalks nonsense if I ask for more professional details. So this\ngeneric big model is too generic\nProfessionalism\nBasically unhelpful for specialized fields\nProfessionalism\nThe dialog is inaccurate, the answers are too generalized,\nand search engines can find them as well\nInaccurate,\nProfessionalism\nInaccurate\nInaccurate\nLogic ability\nAbility (logic)\nDoes not seem to recognize typos very well in Chinese\nAbility (linguistic)\nLow degree of freedom and many restrictions\nFreedom\nBesides the give options illustrated in Figure 10, there are 8 textual responses under the \"Others\" option in the Chinese feedback. After confirming that they were valid and free of sensitive information, the remaining 7 responses are translated into English, shown in table 1. In addition to original user-filled content, we summarize and attach their type.\n# 5.8 Summary\nIn this section, we present the results of the user survey and conclude 11 insightful findings around the current stage of human-LLMs interactions. In the following section, we proceed to discuss future directions based on this empirical analysis.\n# 6 DISCUSSION ABOUT FUTURE DIRECTION (RQ 4)\n# 6.1 User-centric Evaluation\nStarting with the evaluation methodologies sets the stage for what follows. Presently, benchmarks for large language models primarily consist of structured assessments, such as multiple-choice questions or responses evaluated by other reward models [9]. However, it is posited that these test cases and evaluation criteria may not always be congruent with the practical applications and user expectations of generative AI services in real-world usage. For instance, LLMs are often employed as assistants in both everyday and professional tasks, spanning from objective tasks like information retrieval to more subjective scenarios, including creative or advisory roles, as illustrated in Figure 5. While existing benchmarks primarily emphasize subjective usage, focusing predominantly on gauging general intelligence degrees in world knowledge, logical reasoning, and common sense [1], they tend to overlook personal, creative, and leisure-oriented applications. Concerning evaluation criteria, prevailing evaluators such as GPT-4 [37] have been noted to exhibit a bias towards longer contexts [20], a tendency that may not consistently align with real-world user preferences, as discussed in Section 5.5. Therefore, when evaluating generative AI systems, we cannot rely solely on their performance on standardized tests or rankings derived from general evaluators. Our research posits the necessity for a user-centric evaluation framework. This should encompass test scenarios that are more pertinent to real-world applications and utilize evaluators calibrated to human preferences. Such an approach is imperative for comprehensively appraising of the systems\u2019 applicability and practical utility in diverse real-life contexts.\n# 6.2 User Intent Modeling\nAs daily used interfaces, LLMs encounter a variety of usage scenarios. Despite the generalist nature of LLMs, the feedback users anticipate can vary markedly, influenced by their specific intent and context. Users may seek different types of responses and tool utilization as delineated in Section 5.5 and 5.6. This expectation of delivering relevant and tailored services raises the research challenge of deciphering different user intents underlying seemingly straightforward inputs to provide further user-centered services. There are scholarly endeavors focused on intent analysis, which elevate the understanding of user interactions [51, 52]. Moreover, methodologies like Reinforcement Learning with Human Feedback (RLHF) align generative AI more closely with human preferences [31, 35]. An opportunity arises to refine the adaptability of LLMs further, enabling them to learn from condensed experience and extensive data to recognize and accommodate the diversity inherent in different scenarios.\n# 6.3 Personalization\nThe widespread adoption of recommendation systems has elucidated that even under identical intents, variances in user preferences can precipitate differences in desired feedback [41]. Thus, the subsequent goal of user-centric LLMs is to enable personalization, empowering them to understand and respond better to user\u2019s personal characteristics. The first process of continuous user modeling necessitates the effective handling of streaming inputs [19, 39, 67] and the ability to preserve both immediate and enduring memories or states. Additionally, it calls for advanced methodologies to interpret user preferences and behavioral patterns accurately. Once user characteristics are understood, the next step is to respond to these insights adaptively. To reach this degree of adaptability, LLMs need to be structured to dynamically respond to evolving user patterns. Implementing pluggable modules [66, 72] for regular updates and employing efficient tuning mechanisms [23, 44] can facilitate this responsiveness. These adjustments are vital for evolving LLMs into dynamic interfaces that comprehend and proactively adapt to user needs, thereby offering a more personalized interaction experience and higher efficiency in human-AI collaborations.\n# 6.4 Tool Utilization\nAnother central research direction is exploring how external tools can be integrated with LLMs to enhance their capabilities and user experience. The initial phase entails instructing LLMs with predefined experiences, as illustrated in Section 5.6, demonstrating the association of specific intents with corresponding tools. Subsequently, the process evolves to acquiring available tools through self-exploration [18, 45, 69]. Moreover, LLMs could progress from the users of external tools to autonomous toolmakers [8], crafting custom utilities to interact freely with the digital and physical world, enabling future embodied intelligence.\n# 6.5 Trustworthiness\nEstablishing trustworthiness in large language models (LLMs) is paramount, with users expressing concerns about input data privacy and output information reliability, detailed in Section 5.7. To ensure data privacy, strategies like excluding sensitive user data from training [40], processing messages locally on users\u2019 devices [48], or enabling LLMs to unlearn information selectively [5] are critical. Equally important is data safety, where preventing toxic responses and integrating ethical considerations [24] are key measures. Upholding ethical standards in LLM development, focusing on fairness, inclusivity, and bias mitigation, is crucial for gaining user trust and aligning with a user-centric approach.\n# .6 Cross Linguistic and Cultural Development\nThe empirical analysis reveals disparities in the user behaviors of English and Chinese-spoken participants, such as usage distributions in information retrieval and APIs, as detailed in Section 5.3.1, and the levels of user satisfaction in asking for advice and leisure, discussed in Section 5.4.1. In light of these observations, it becomes incumbent for generative AI to foster both general competence and cross-linguistic and cultural understandings, as discussed in literature [25, 42]. Ultimately, advancing human-LLM collaborations requires a holistic approach beyond mere technical optimizations, including intent understanding and user modeling, self-directed learning of external tools and internal capabilities, enhancing trustworthiness, and user-centered evaluations for real-world utilities, etc. Tackling these multifaceted challenges is essential for building effective and responsible LLMs to propel human-AI collaborations.\n# 7 LIMITATIONS\nThe participant demographics of our user survey, primarily disseminated through social media of graduate students and university professors, suggest a likely skew towards highly educated individuals. This user group, potentially more experienced in human-AI collaboration, may have a predisposition to engage with and deeply utilize new technologies. Additionally, the voluntary nature of the survey participation introduces a selection bias, as it tends to attract users with a pre-existing interest in LLMs or those inclined to contribute to anonymous user studies. Given these factors, our study is not representative of a random, general user group but rather a more specific segment that potentially has deep and sophisticated engagements with LLMs. While this offers valuable insights, it\u2019s important to acknowledge this limitation regarding the broader applicability and generalization of the findings.\n# 8 CONCLUSION\nIn this study, we embrace a user-centric approach for evaluating large language models (LLMs). This perspective steers us toward an in-depth understanding of user intents. We broaden the scope of existing classifications from traditional information-seeking systems to general LLM-driven interfaces, formulating a taxonomy of 7 intents. This refined classification, grounded by open-source ChatGPT user logs and supplemented with human verification, sets the stage for fine-grained explorations of user satisfaction with LLMs. Building on this understanding, we conducted a user study, collecting anonymous feedback from 411 individuals. This feedback spans aspects of usage frequency, experience corresponding to each intent, and concerns regarding LLMs. This insightful dataset will be released upon the publication of our paper. Analysis of this survey yielded 11 compelling findings, starting with the confirmation of the widespread daily use of the LLM. The proposed intents are categorized into three statistically relevant clusters: subjective uses through graphical user interfaces (GUIs), objective uses through GUIs, and usage through application programming interfaces (APIs). Notably, subjective usage through GUIs are potentially overlooked by existing research. The observations derived from empirical analysis further guide us in discussions of future directions for advancing LLMs, focusing not just on technical optimizations but on augmenting human-AI collaboration. This shift towards a more user-centric development of LLMs is crucial for ensuring that these models meet the diverse and evolving needs in real-world interactions.\n# ACKNOWLEDGEMENT\nThanks to all the participants for taking the user study and giving valuable feedback about th first-hand experience with the large language model interfaces.\n# REFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 Technical Report. arXiv preprint (2023). [2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023). [3] Michael Bendersky and W Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proceedings of the 2009 workshop on Web Search Click Data. 8\u201314. [4] Bojana Bodroza, Bojana M Dinic, and Ljubisa Bojic. 2023. Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3\u2019s personality instruments results. arXiv preprint arXiv:2306.04308 (2023). [5] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 141\u2013159. [6] Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, Vol. 36. ACM New York, NY, USA, 3\u201310. [7] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023). [8] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large language models as tool makers. arXiv preprint arXiv:2305.17126 (2023). [9] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109 (2023). [10] Long Chen, Dell Zhang, and Levene Mark. 2012. Understanding user intent in community question answering. In Proceedings of the 21st international conference on world wide web. 823\u2013828. [11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [12] Jonathan H Choi and Daniel Schwarcz. 2023. AI assistance in legal analysis: An empirical study. (2023). [13] Hyeshin Chu, Joohee Kim, Seongouk Kim, Hongkyu Lim, Hyunwook Lee, Seungmin Jin, Jongeun Lee, Taehwan Kim, and Sungahn Ko. 2022. An Empirical Study on How People Perceive AI-generated Music. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. [14] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint (2018). [15] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021). [16] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182 (2023). [17] Wei Dai, Jionghao Lin, Hua Jin, Tongguang Li, Yi-Shan Tsai, Dragan Ga\u0161evi\u0107, and Guanliang Chen. 2023. Can large language models provide feedback to students? A case study on ChatGPT. In 2023 IEEE International Conference on Advanced Learning Technologies (ICALT). IEEE, 323\u2013325. [18] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. 2022. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. Advances in Neural Information Processing Systems 35 (2022), 5982\u20135994. [19] Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, and Ron Hoory. 2023. Speak While You Think: Streaming Speech Synthesis During Text Generation. arXiv preprint arXiv:2309.11210 (2023). [20] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387 (2023). [21] Dat Duong and Benjamin D Solomon. 2023. Analysis of large-language model versus human performance for genetics questions. European Journal of Human Genetics (2023), 1\u20133. [22] Michael C Frank. 2023. Baby steps in evaluating the capacities of large language models. Nature Reviews Psychology 2, 8 (2023), 451\u2013452. [23] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2023. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG) (2023). [24] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 (2020). [25] Paula Helm, G\u00e1bor Bella, Gertraud Koch, and Fausto Giunchiglia. 2023. Diversity and language technology: how\ntechno-linguistic bias can cause epistemic injustice. arXiv preprint arXiv:2307.13714 (2023). [26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). [27] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045 (2023). [28] Bernard J Jansen. 2006. Search log analysis: What it is, what\u2019s been done, how to do it. Library & information science research 28, 3 (2006), 407\u2013432. [29] Bernard J Jansen, Danielle L Booth, and Amanda Spink. 2007. Determining the user intent of web search engine queries. In Proceedings of the 16th international conference on World Wide Web. 1149\u20131150. [30] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [31] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H\u00fcllermeier. 2023. A Survey of Reinforcement Learning from Human Feedback. arXiv preprint arXiv:2312.14925 (2023). [32] Dominique Kelly, Yimin Chen, Sarah E Cornwell, Nicole S Delellis, Alex Mayhew, Sodiq Onaolapo, and Victoria L Rubin. 2023. Bing Chat: The Future of Search Engines? Proceedings of the Association for Information Science and Technology (2023). [33] Christoph Kofler, Martha Larson, and Alan Hanjalic. 2016. User intent in multimedia search: a survey of the state of the art and future challenges. ACM Computing Surveys (CSUR) 49, 2 (2016), 1\u201337. [34] Vivian Lai, Chacha Chen, Q Vera Liao, Alison Smith-Renner, and Chenhao Tan. 2021. Towards a science of human-ai decision making: a survey of empirical studies. arXiv preprint arXiv:2112.11471 (2021). [35] Nathan Lambert and Roberto Calandra. 2023. The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. arXiv preprint arXiv:2311.00168 (2023). [36] Pier Luca Lanzi and Daniele Loiacono. 2023. Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. arXiv preprint arXiv:2303.02155 (2023). [37] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. GitHub repository (2023). [38] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint (2022). [39] Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R Gormley, and Jason Eisner. 2020. Limitations of autoregressive models and their alternatives. arXiv preprint arXiv:2010.11939 (2020). [40] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models\u2019 Alignment. arXiv preprint arXiv:2308.05374 (2023). [41] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015. Recommender system application developments: a survey. Decision support systems 74 (2015), 12\u201332. [42] Avinash Madasu and Shashank Srivastava. 2022. What do Large Language Models Learn beyond Language? arXiv preprint arXiv:2210.12302 (2022). [43] Patrick Mikalef and Manjul Gupta. 2021. Artificial intelligence capability: Conceptualization, measurement calibration, and empirical study on its impact on organizational creativity and firm performance. Information & Management (2021). [44] Matthias Paulik, Matt Seigel, Henry Mason, Dominic Telaar, Joris Kluivers, Rogier van Dalen, Chi Wai Lau, Luke Carlson, Filip Granqvist, Chris Vandevelde, et al. 2021. Federated evaluation and tuning for on-device personalization: System design & applications. arXiv preprint arXiv:2102.08503 (2021). [45] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023. Tool learning with foundation models. arXiv preprint arXiv:2304.08354 (2023). [46] Chen Qu, Liu Yang, W Bruce Croft, Johanne R Trippas, Yongfeng Zhang, and Minghui Qiu. 2018. Analyzing and characterizing user intent in information-seeking conversations. In The 41st international acm sigir conference on research & development in information retrieval. 989\u2013992. [47] Chen Qu, Liu Yang, W Bruce Croft, Yongfeng Zhang, Johanne R Trippas, and Minghui Qiu. 2019. User intent prediction in information-seeking conversations. In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval. 25\u201333. [48] Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, and Soheil Salehi. 2023. Quantized Transformer Language Model Implementations on Edge Devices. arXiv preprint arXiv:2310.03971 (2023). [49] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd\nschema challenge at scale. Commun. ACM 64, 9 (2021), 99\u2013106. [50] Mehmet Bahri Saydam, Hasan Evrim Arici, and Mehmet Ali Koseoglu. 2022. How does the tourism and hospitality industry use artificial intelligence? A review of empirical studies and future research agenda. Journal of Hospitality Marketing & Management (2022). [51] Chirag Shah and Emily M Bender. 2022. Situating search. In Proceedings of the 2022 Conference on Human Information Interaction and Retrieval. 221\u2013232. [52] Chirag Shah, Ryen W White, Reid Andersen, Georg Buscher, Scott Counts, Sarkar Snigdha Sarathi Das, Ali Montazer, Sathish Manivannan, Jennifer Neville, Xiaochuan Ni, et al. 2023. Using large language models to generate, validate, and apply user intent taxonomies. arXiv preprint arXiv:2309.13063 (2023). [53] Murray Shanahan and Catherine Clarke. 2023. Evaluating Large Language Model Creativity from a Literary Perspective. arXiv preprint arXiv:2312.03746 (2023). [54] Ritwik Sinha, Zhao Song, and Tianyi Zhou. 2023. A mathematical abstraction for balancing the trade-off between creativity and reality in large language models. arXiv preprint arXiv:2306.02295 (2023). [55] Xia Song, Bo Xu, and Zhenzhen Zhao. 2022. Can people experience romantic love for artificial intelligence? An empirical study of intelligent assistants. Information & Management (2022). [56] Ning Su, Jiyin He, Yiqun Liu, Min Zhang, and Shaoping Ma. 2018. User intent, behaviour, and perceived satisfaction in product search. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. 547\u2013555. [57] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 (2022). [58] Lu Tang, Jinxu Li, and Sophia Fantus. 2023. Medical artificial intelligence ethics: A systematic review of empirical studies. Digital Health (2023). [59] Xiaoou Tang, Ke Liu, Jingyu Cui, Fang Wen, and Xiaogang Wang. 2011. Intentsearch: Capturing user intention for one-click internet image search. IEEE transactions on pattern analysis and machine intelligence 34, 7 (2011), 1342\u20131353. [60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [61] Jaime Teevan, Susan T Dumais, and Daniel J Liebling. 2008. To personalize or not to personalize: modeling queries with variation in user intent. In Proceedings of the 31st annual international ACM SIGIR conference. [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv. org/abs/2307.09288 (2023). [63] Oleksandra Vereschak, Gilles Bailly, and Baptiste Caramiaux. 2021. How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies. Proceedings of the ACM on Human-Computer Interaction (2021). [64] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. 2023. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095 (2023). [65] Congying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang, and Philip S Yu. 2018. Zero-shot user intent detection via capsule neural networks. arXiv preprint arXiv:1809.00385 (2018). [66] Chaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min Chan, Yankai Lin, Zhiyuan Liu, Xiangyang Li, Zhonghua Li, Zhao Cao, and Maosong Sun. 2023. Plug-and-play document modules for pre-trained models. (2023). [67] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453 (2023). [68] Yutong Xie, Zhaoying Pan, Jinge Ma, Luo Jie, and Qiaozhu Mei. 2023. A prompt log analysis of text-to-image generation systems. In Proceedings of the ACM Web Conference 2023. 3892\u20133902. [69] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. 2023. Holodeck: Language Guided Generation of 3D Embodied AI Environments. arXiv preprint arXiv:2312.09067 (2023). [70] Xing Yi, Hema Raghavan, and Chris Leggetter. 2009. Discovering users\u2019 specific geo intention in web search. In Proceedings of the 18th international conference on World wide web. 481\u2013490. [71] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830 (2019). [72] Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan Liu, Peng Li, Maosong Sun, et al. 2023. Plug-and-play knowledge injection for pre-trained language models. arXiv preprint (2023). [73] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint (2023).\n# APPENDIX\n# A QUESTIONNAIRE\n(3) What are the following do you use the system for? [Multi Choices] Detailed explanations about these categories are included, which is the same as Section 3.2. Ask for Advice, Information Retrieval, Leisure, Seek Creativity, Solve Problems in Specialized Areas, Text Assistant, Use through API, Others [fill in]. (4) Do you have anything to add or comment to the categories above? [Blank]\n(3) What are the following do you use the system for? [Multi Choices] Detailed explanations about these categories are included, which is the same as Section 3.2. Ask for Advice, Information Retrieval, Leisure, Seek Creativity, Solve Problems in Specialized Areas, Text Assistant, Use through API, Others [fill in]. (4) Do you have anything to add or comment to the categories above? [Blank]\n(5) For the following usage scenarios, your rating on the system performance is? [Single Choice] If you have not used the system for certain requirements, please select N/A. For each intent, the options are: Very dissatisfied, dissatisfied, neutral, satisfied, very satisfied, N/A.\n(5) For the following usage scenarios, your rating on the system performance is? [Single Choice] If you have not used the system for certain requirements, please select N/A. For each intent, the options are: Very dissatisfied, dissatisfied, neutral, satisfied, very satisfied, N/A. (6) For the following usage scenarios, do you think the system\u2019s response needs to be [concise] or [detailed]? [Single Choice * 7 intents] (7) For the following usage scenarios, do you think the system\u2019s answers need to be [based on facts] or [based on creations]? [Single Choice * 7 intents] (8) For the following usage scenarios, do you think the system\u2019s answers need to be [based on professional knowledge] or [based on common sense]? [Single Choice * 7 intents] (9) In each scenario, which of the following tools helps the system generate higher-quality responses? [Multi Choices * 7 intents] Web Browsing: use search engines, browse and analyze web content, etc. Input Analysis: understand uploaded documents, pictures, etc. Personalization: recommend content based on user interests\n(5) For the following usage scenarios, your rating on the system performance is? [Single Choice] If you have not used the system for certain requirements, please select N/A. For each intent, the options are: Very dissatisfied, dissatisfied, neutral, satisfied, very satisfied, N/A. (6) For the following usage scenarios, do you think the system\u2019s response needs to be [concise] or [detailed]? [Single Choice * 7 intents] (7) For the following usage scenarios, do you think the system\u2019s answers need to be [based on facts] or [based on creations]? [Single Choice * 7 intents] (8) For the following usage scenarios, do you think the system\u2019s answers need to be [based on professional knowledge] or [based on common sense]? [Single Choice * 7 intents] (9) In each scenario, which of the following tools helps the system generate higher-quality responses? [Multi Choices * 7 intents] Web Browsing: use search engines, browse and analyze web content, etc. Input Analysis: understand uploaded documents, pictures, etc. Personalization: recommend content based on user interests Programming: write and execute code Mathematical Operations: Assist with mathematical calculations, logical reasoning Documentation Generation: create diagrams, documents and slides Multimedia Creation: generate pictures and videos Other tools: please input\nN/A. (6) For the following usage scenarios, do you think the system\u2019s response needs to be [concise] or [detailed]? [Single Choice * 7 intents] (7) For the following usage scenarios, do you think the system\u2019s answers need to be [based on facts] or [based on creations]? [Single Choice * 7 intents] (8) For the following usage scenarios, do you think the system\u2019s answers need to be [based on professional knowledge] or [based on common sense]? [Single Choice * 7 intents]\n(6) For the following usage scenarios, do you think the system\u2019s response needs to be [concise] or [detailed]? [Single Choice * 7 intents] (7) For the following usage scenarios, do you think the system\u2019s answers need to be [based on facts] or [based on creations]? [Single Choice * 7 intents] (8) For the following usage scenarios, do you think the system\u2019s answers need to be [based on professional knowledge] or [based on common sense]? [Single Choice * 7 intents]\n(9) In each scenario, which of the following tools helps the system generate higher-quality responses? [Multi Choices * 7 intents] Web Browsing: use search engines, browse and analyze web content, etc. Input Analysis: understand uploaded documents, pictures, etc. Personalization: recommend content based on user interests Programming: write and execute code Mathematical Operations: Assist with mathematical calculations, logical reasoning Documentation Generation: create diagrams, documents and slides Multimedia Creation: generate pictures and videos Other tools: please input\n# (10) Please select B for this question, thanks! [Single Choice]\n(11) Which aspects of the existing system do you think are lacking and most in need of optimization? [Multi Choice] Hallucination, Long context processing, multimodal understanding and generation, personalization, privacy, safety, others [fill in]. (12) Do you have other comments and suggestions about this questionnaire or large language model services? Thanks for your participation and support! [Blank]\n(11) Which aspects of the existing system do you think are lacking and most in need of optimization? [Multi Choice] Hallucination, Long context processing, multimodal understanding and generation, personalization, privacy, safety, others [fill in]. (12) Do you have other comments and suggestions about this questionnaire or large language model services? Thanks for your participation and support! [Blank]\n# B DEMOGRAPHIC INFORMATION\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3057/3057ae71-93f7-4b27-9b49-90cc7521a29d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 11. IP distribution excluding China.</div>\nIn our survey, personal questions were not included. The sole demographic information obtained was the automatically recorded IP addresses captured by the questionnaire system. For the Chinese version of survey, we recorded 291 IPs originating from China and 6 from other countries. In the English version of the survey, there were 66 Chinese IPs and 48 IPs from various other countries. It is pertinent to note that the English survey was disseminated amongst an international graduate program based in China, resulting in approximately 30 responses from English-speaking students located within China. The distribution of IP addresses from outside China is depicted in Figure 11.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to address the gap in research that overlooks the user-centric perspective of large language models (LLMs) by understanding user satisfaction and experiences with LLMs in real-world interactions.",
            "scope": "The survey encompasses user intents, experiences, concerns, and future research directions related to LLMs, while excluding technical performance evaluations and purely theoretical discussions."
        },
        "problem": {
            "definition": "The core issue explored is the need to understand user satisfaction and intents when interacting with LLMs, shifting the focus from model capabilities to user experiences.",
            "key obstacle": "Researchers face challenges in aligning LLM functionalities with actual user needs, as existing evaluations often prioritize model performance over user-centric considerations."
        },
        "architecture": {
            "perspective": "The survey introduces a taxonomy of user intents for LLM interactions, categorizing them into seven distinct intents that reflect user needs beyond mere information retrieval.",
            "fields/stages": "The survey organizes current research into three clusters: subjective uses through GUIs, objective uses through GUIs, and usage through APIs, providing a structured view of user interactions."
        },
        "conclusion": {
            "comparisions": "The analysis reveals significant differences in user satisfaction across intents, with Text Assistant tasks receiving the highest satisfaction, while Seeking Creativity and Solving Problems showed lower satisfaction rates.",
            "results": "Key findings indicate that LLMs are widely used, but improvements are needed in subjective tasks like creativity and problem-solving, highlighting the importance of tailoring models to diverse user needs."
        },
        "discussion": {
            "advantage": "Current research has established a foundational understanding of user intents and satisfaction, paving the way for more user-centric LLM developments.",
            "limitation": "The survey sample may not represent the general population, as it skews towards highly educated individuals familiar with LLMs, potentially limiting the generalizability of findings.",
            "gaps": "Unanswered questions remain regarding the integration of user feedback into LLM development and the need for cross-cultural considerations in model design.",
            "future work": "Future research should focus on enhancing personalization, integrating external tools, and ensuring trustworthiness in LLMs to better meet user expectations and foster effective human-AI collaboration."
        },
        "other info": {
            "dataset availability": "The dataset from the user survey, comprising 411 responses, will be publicly available alongside the publication of this paper.",
            "research questions": {
                "RQ1": "What are the primary user intents for engaging with conversational interfaces powered by LLMs?",
                "RQ2": "How do users perceive their experience when interacting with current LLM services?",
                "RQ3": "What key concerns do users have for using LLMs?",
                "RQ4": "What are future directions in building user-centered LLMs for better human-AI collaboration?"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This survey aims to address the gap in research that overlooks the user-centric perspective of large language models (LLMs) by understanding user satisfaction and experiences with LLMs in real-world interactions."
        },
        {
            "section number": "1.2",
            "key information": "The core issue explored is the need to understand user satisfaction and intents when interacting with LLMs, shifting the focus from model capabilities to user experiences."
        },
        {
            "section number": "1.3",
            "key information": "Key findings indicate that LLMs are widely used, but improvements are needed in subjective tasks like creativity and problem-solving, highlighting the importance of tailoring models to diverse user needs."
        },
        {
            "section number": "2.1",
            "key information": "The survey introduces a taxonomy of user intents for LLM interactions, categorizing them into seven distinct intents that reflect user needs beyond mere information retrieval."
        },
        {
            "section number": "2.2",
            "key information": "The survey organizes current research into three clusters: subjective uses through GUIs, objective uses through GUIs, and usage through APIs, providing a structured view of user interactions."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on enhancing personalization, integrating external tools, and ensuring trustworthiness in LLMs to better meet user expectations and foster effective human-AI collaboration."
        },
        {
            "section number": "10.3",
            "key information": "Unanswered questions remain regarding the integration of user feedback into LLM development and the need for cross-cultural considerations in model design."
        }
    ],
    "similarity_score": 0.7759631679822117,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Understanding User Experience in Large Language Model Interactions.json"
}