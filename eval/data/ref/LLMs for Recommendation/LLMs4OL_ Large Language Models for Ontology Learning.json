{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.16648",
    "title": "LLMs4OL: Large Language Models for Ontology Learning",
    "abstract": "We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \\textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.",
    "bib_name": "giglou2023llms4ollargelanguagemodels",
    "md_text": "# LLMs4OL: Large Language Models\u200c for Ontology Learning\nHamed Babaei Giglou[0000\u22120003\u22123758\u22121454], Jennifer D\u2019Souza[0000\u22120002\u22126616\u22129509], and S\u00a8oren Auer[0000\u22120002\u22120698\u22122864]\nTIB Leibniz Information Centre for Science and Technology, Hannover, Germany {hamed.babaei,jennifer.dsouza,auer}@tib.eu\nAbstract. We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text? To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS. The obtained empirical results show that foundational LLMs are not sufficiently suitable for ontology construction that entails a high degree of reasoning skills and domain expertise. Nevertheless, when effectively fine-tuned they just might work as suitable assistants, alleviating the knowledge acquisition bottleneck, for ontology construction.\nKeywords: Large Language Models \u00b7 LLMs \u00b7 Ontologies \u00b7 Ontology Learning \u00b7 Prompting \u00b7 Prompt-based Learning.\n# 1 Introduction\nOntology Learning (OL) is an important field of research in artificial intelligence (AI) and knowledge engineering, as it addresses the challenge of knowledge acquisition and representation in a variety of domains. OL involves automatically identifying terms, types, relations, and potentially axioms from textual information to construct an ontology [29]. Numerous examples of human-expert created ontologies exist, ranging from general-purpose ontologies to domain-specific ones, e.g., Unified Medical Language System (UMLS) [8], WordNet [40], GeoNames [52], Dublin Core Metadata Initiative (DCMI) [64], schema.org [19], etc. Traditional ontology creation relies on manual specification by domain experts, which can be time-consuming, costly, error-prone, and impractical when knowledge constantly evolves or domain experts are unavailable. Consequently, OL\ntechniques have emerged to automatically acquire knowledge from unstructured or semi-structured sources, such as text documents and the web, and transform it into a structured ontology. A quick review of the field shows that traditional approaches to OL are based on lexico-syntactic pattern mining and clustering [65,41,36,25,4,20,59,53,27,2,23,22]. In contrast, recent advances in natural language processing (NLP) through Large Language Models (LLMs) [45] offer a promising alternative to traditional OL methods. The ultimate goal of OL is to provide a cost-effective and scalable solution for knowledge acquisition and representation, enabling more efficient and effective decision-making in a range of domains. To this end, we introduce the LLMs4OL paradigm and empirically ground it as a foundational first step. Currently, there is no research explicitly training LLMs for OL. Thus to test LLMs for OL for the first time, we made some experimental considerations. The first being: Do the characteristics of LLMs justify ontology learning? First, LLMs are trained on extensive and diverse text, similar to domain-specific knowledge bases [50]. This aligns with the need for ontology developers to have extensive domain knowledge. Second, LLMs are built on the core technology of transformers that have enabled their higher language modeling complexity by facilitating the rapid scaling of their parameters. These parameters represent connections between words, enabling LLMs to comprehend the meaning of unstructured text like sentences or paragraphs. Further, by extrapolating complex linguistic patterns from word connections, LLMs exhibit human-like response capabilities across various tasks, as observed in the field of \u201cemergent\u201d AI. This behavior entails performing tasks beyond their explicit training, such as generating executable code, diverse genre text, and accurate text summaries [57,62]. Such ability of LLMs to extrapolate patterns from simple word connections, encoding language semantics, is crucial for OL. Ontologies often rely on analyzing and extrapolating structured information connections, such as term-type taxonomies and relations, from unstructured text [17]. Thus LLMs4OL hypothesis of LLMs\u2019 fruitful application for OL appeared conceptually justified. LLMs are being developed at a rapid pace. At the time of writing of this work, at least 60 different LLMs are reported [5]. This led to our second main experimental consideration. Which LLMs to test for the LLMs4OL task hypothesis? Empirical validation of various LLMs is crucial for NLP advancements and selecting suitable models for research tasks. Despite impressive performances in diverse NLP tasks, LLM effectiveness varies. For the foundational groundwork of LLMs4OL, we comprehensively selected eight diverse model families based on architecture and reported state-of-the-art performances at the time of this writing. The three main LLM architectures are encoder, decoder, and encoder-decoder. The selected LLMs for validation are: BERT [15] (encoder-only); BLOOM [55], MetaAI\u2019s LLaMA [58], OpenAI\u2019s GPT-3 [9], GPT-3.5 [45], GPT-4 [46] (all decoder-only); and BART [32] and Google\u2019s Flan-T5 [10] (encoder-decoder). Recent studies show that BERT excels in text classification and named entity recognition [15], BART is effective in text generation and summarization [32], and LLaMA demonstrates high accuracy in various NLP tasks, including reason-\ning, question answering, and code generation [58]. Flan-T5 emphasizes instruction tuning and exhibits strong multi-task performance [10]. BLOOM\u2019s unique multilingual approach achieves robust performance in tasks like text classification and sequence tagging [55]. Lastly, the GPT series stands out for its humanlike text generation abilities [9,45,46]. In this work, we aim to comprehensively unify these LLMs for their effectiveness under the LLMs4OL paradigm for the first time. With the two experimental considerations in place, we now introduce the LLMs4OL paradigm and highlight our contributions. LLMs4OL is centered around the development of ontologies that comprise the following primitives [38]: 1. a set of strings that describe terminological lexical entries L for conceptual types; 2. a set of conceptual types T; 3. a taxonomy of types in a hierarchy HT ; 4. a set of non-taxonomic relations R described by their domain and range restrictions arranged in a heterarchy of relations HR; and 5. a set of axioms A that describe additional constraints on the ontology and make implicit facts explicit. The LLMs4OL paradigm, introduced in this work, addresses three core aspects of OL as tasks, outlined as the following research questions (RQs). \u2013 RQ1: Term Typing Task \u2013 How effective are LLMs for automated type discovery to construct an ontology? \u2013 RQ2: Type Taxonomy Discovery Task \u2013 How effective are LLMs to recognize a type taxonomy i.e. the \u201cis-a\u201d hierarchy between types? \u2013 RQ3: Type Non-Taxonomic Relation Extraction Task \u2013 How effective are LLMs to discover non-taxonomic relations between types? The diversity of the empirical tests of this work are not only w.r.t. LLMs considered, but also the ontological knowledge domains tested for. Specifically, we test LLMs for lexico-semantic knowledge in WordNet [40], geographical knowledge in GeoNames [1], biomedical knowledge in UMLS [7], and web content type representations in schema.org [47]. For our empirical validation of LLMs4OL, we seize the opportunity to include PubMedBERT [18], a domain-specific LLM designed solely for the biomedical domain and thus applicable only to UMLS. This addition complements the eight domain-independent model families introduced earlier as a ninth model type. Summarily, our main contributions are: \u2013 The LLMs4OL task paradigm as a conceptual framework for leveraging LLMs for OL. \u2013 An implementation of the LLMs4OL concept leveraging tailored prompt templates for zero-shot OL in the context of three specific tasks, viz. term typing, type taxonomic relation discovery, and type non-taxonomic relation discovery. These tasks are evaluated across unique ontological sources wellknown in the community. Our code source with templates and datasets per task are released here https://github.com/HamedBabaei/LLMs4OL. \u2013 A thorough out-of-the-box empirical evaluation of eight state-of-the-art domain independent LLM types (10 models) and a ninth biomedical domain-specific LLM type (11th model) for their suitability to the various OL tasks considered in this work. Furthermore, the most effective overall LLM is finetuned and subsequently finetuned LLM results are reported for our three OL tasks.\n# 2 Related Work\nThere are three avenues of related research: ontology learning from text, prompting LLMs for knowledge, and LLM prompting methods or prompt engineering. Ontology Learning from Text. One of the earliest approaches [22] used lexicosyntactic patterns to extract new lexicosemantic concepts and relations from large collections of unstructured text, enhancing WordNet [40]. WordNet is a lexical database comprising a lexical ontology of concepts (nouns, verbs, etc.) and lexico-semantic relations (synonymy, hyponymy, etc.). Hwang [23] proposed an alternative approach for constructing a dynamic ontology specific to an application domain. The method involved iteratively discovering types and taxonomy from unstructured text using a seed set of terms representing high-level domain types. In each iteration, newly discovered specialized types were incorporated, and the algorithm detected relations between linguistic features. The approach utilized a simple ontology algebra based on inheritance hierarchy and set operations. Agirre et al.[2] enhanced WordNet by extracting topically related words from web documents. This unique approach added topical signatures to enrich WordNet. Kietz et al.[27] introduced the On-To-Knowledge system, which utilized a generic core ontology like GermaNet [21] or WordNet as the foundational structure. It aimed to discover a domain-specific ontology from corporate intranet text resources. For concept extraction and pruning, it employed statistical term frequency count heuristics, while association rules were applied for relation identification in corporate texts. Roux et al.[53] proposed a method to expand a genetics ontology by reusing existing domain ontologies and enhancing concepts through verb patterns extracted from unstructured text. Their system utilized linguistic tools like part-of-speech taggers and syntactic parsers. Wagner [59] employed statistical analysis of corpora to enrich WordNet in non-English languages by discovering relations, adding new terms to concepts, and acquiring concepts through the automatic acquisition of verb preferences. Moldovan and Girju [42] introduced the Knowledge Acquisition from Text (KAT) system to enrich WordNet\u2019s finance domain coverage. Their method involved four stages: (1) discovering new concepts from a seed set of terms, expanding the concept list using dictionaries; (2) identifying lexical patterns from new concepts; (3) discovering relations from lexical patterns; and (4) integrating extracted information into WordNet using a knowledge classification algorithm. In [4], an unsupervised method is presented to enhance ontologies with domain-specific information using NLP techniques such as NER and WSD. The method utilizes a general NER system to uncover a taxonomic hierarchy and employs WSD to enrich existing synsets by querying the internet for new terms and disambiguating them through cooccurrence frequency. Khan and Luo [25] employed clustering techniques to find new terms, utilizing WordNet for typing. They used the self-organizing tree algorithm [16], inspired by molecular evolution, to establish an ontology hierarchy. Additionally, Xu et al. [65] focused on automatically acquiring domainspecific terms and relations through a TFIDF-based single-word term classifier, a lexico-syntactic pattern finder based on known relations and collocations, and a relation extractor utilizing discovered lexico-syntactic patterns.\nPredominantly, the approaches for OL [60] that stand out so far are based on lexico-syntactic patterns for term and relation extraction as well as clustering for type discovery. Otherwise, they build on seed-term-based bootstrapping methods. The reader is referred to further detailed reviews [6,37] on this theme for a comprehensive overall methodological picture for OL. Traditional NLP was defined by modular pipelines by which machines were equipped step-wise with annotations at the linguistic, syntactic, and semantic levels to process text. LLMs have ushered in a new era of possibilities for AI systems that obviate the need for modular NLP systems to understand natural language which we tap into for the first time for the OL task in this work. Prompting LLMs for Knowledge. LLMs can process and retrieve facts based on their knowledge which makes them good zero-shot learners for various NLP tasks. Prompting LLMs means feeding an input x using a template function fprompt(x), a textual string prompt input that has some unfilled slots, and then the LLMs are used to probabilistically fill the unfilled information to obtain a final string x\u2032, from which the final output y can be derived [34]. The LAMA: LAnguage Model Analysis [51] benchmark has been introduced as a probing technique for analyzing the factual and commonsense knowledge contained in unidirectional LMs (i.e. Transformer-XL [12]) and bidirectional LMs (i.e. BERT and ELMo [48]) with cloze prompt templates from knowledge triples. They demonstrated the potential of pre-trained language models (PLMs) in probing facts \u2013 where facts are taken into account as subject-relation-object triples or questionanswer pairs \u2013 with querying LLMs by converting facts into a cloze template which is used as an input for the LM to fill the missing token. Further studies extended LAMA by the automated discovery of prompts [24], finetuning LLMs for better probing [3,31,66], or a purely unsupervised way of probing knowledge from LMs [49]. These studies analyzed LLMs for their ability to encode various linguistic and non-linguistic facts. This analysis was limited to predefined facts that reinforce the traditional linguistic knowledge of the LLMs, and as a result do not reflect how concepts are learned by the LLMs. In response to this limitation, Dalvi et al. [13] put forward a proposal to explore and examine the latent concepts learned by LLMs, offering a fresh perspective on BERT. They defined concepts as \u201ca group of words that are meaningful,\u201d i.e. that can be clustered based on relations such as lexical, morphological, etc. In another study [54], they propose the framework ConceptX by extending their studies on seven LLMs in latent space analysis with the alignment of the grouped concepts to human-defined concepts. These works show that using LLMs and accessing the concept\u2019s latent spaces, allows us to group concepts and align them to predefined types and type relations discovery. Prompt Engineering. As a novel discipline, prompt engineering focuses on designing optimal instructions for LLMs to enable successful task performance. Standard prompting [61] represents a fundamental approach for instructing LLMs It allows users to craft their own customized \u201cself-designed prompts\u201d to effectively interact with LLMs [9] and prompt them to respond to the given prompt instruction straightaway with an answer. Consider the manually crafted FLAN\ncollection [35] addressing diverse NLP tasks other than OL as an exemplar. Notably, the nature of some problems naturally encompass a step-by-step thought process for arriving at the answer. In other words, the problem to be solved can be decomposed as a series of preceding intermediate steps before arriving at the final solution. E.g., arithmetic or reasoning problems. Toward explainability and providing language models in a sense \u201ctime to think\u201d helping it respond more accurately, there are advanced prompt engineering methods as well. As a first, as per the Chain-of-Thought (CoT) [63] prompting method, the prompt instruction is so crafted that the LLM is instructed to break down complex tasks as a series of incremental steps leading to the solution. This helps the LLM to reason stepby-step and arrive at a more accurate and logical conclusion. On the other hand Tree-of-Thoughts (ToT) [67] has been introduced for tasks that require exploration or strategic lookahead. ToT generalizes over CoT prompting by exploring thoughts that serve as intermediate steps for general problem-solving with LLMs. Both CoT and ToT unlock complex reasoning capabilities through intermediate reasoning steps in combination with few-shot or zero-shot [28] prompting. Another approach for solving more complex tasks is using decomposed prompting [26], where we can further decompose tasks that are hard for LLMs into simpler solvable sub-tasks and delegate these to sub-task-specific LLMs. Given the LLMs4OL task paradigm introduced in this work, complex prompting is not a primary concern, as our current focus is on the initial exploration of the task to identify the areas where we need further improvement. We want to understand how much we have accomplished so far before delving into more complex techniques like CoT, ToT, and decomposed prompting. Once we have a clearer picture of the model\u2019s capabilities and limitations in a standard prompting setting, we can then consider other than standard prompt engineering approaches by formulating OL as a stepwise reasoning task.\n# 3 The LLMs4OL Task Paradigm\nThe Large Language Models for Ontology Learning (LLMs4OL) task paradigm offers a conceptual framework to accelerate the time-consuming and expensive construction of ontologies exclusively by domain experts to a level playing field involving powerful AI methods such as LLMs for high-quality OL results; consequently and ideally involving domains experts only in validation cycles. In theory, with the right formulations, all tasks pertinent to OL fit within the LLMs4OL task paradigm. OL tasks are based on ontology primitives [38], including lexical entries L, conceptual types T, a hierarchical taxonomy of types HT , non-taxonomic relations R in a heterarchy HR, and a set of axioms A to describe the ontology\u2019s constraints and inference rules. To address these primitives, OL tasks [44] include: 1) Corpus preparation - selecting and collecting source texts for ontology building. 2) Terminology extraction - identifying and extracting relevant terms. 3) Term typing - grouping similar terms into conceptual types. 4) Taxonomy construction - establishing \u201dis-a\u201d hierarchies between types. 5) Relationship extraction - identifying semantic relationships beyond \u201dis-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8b1b/8b1b14a3-00f3-4625-8a6f-06a43fd7fc03.png\" style=\"width: 50%;\"></div>\nFig. 1. The LLMs4OL task paradigm is an end-to-end framework for ontology learning in various knowledge domains, i.e. lexicosemantics (WordNet), geography (GeoNames), biomedicine (NCI, MEDICIN, SNOMEDCT), and web content types (schema.org). The three OL tasks empirically validated in this work are depicted within the blue arrow, aligned with the greater LLMs4OL paradigm.\na.\u201d 6) Axiom discovery - finding constraints and inference rules for the ontology. This set of six tasks forms the LLMs4OL task paradigm. See Figure 1 for the proposed LLMs4OL conceptual framework. In this work, we empirically ground three core OL tasks using LLMs as a foundational basis for future research. However, traditional AI paradigms rely on testing models only on explicitly trained tasks, which is not the case for LLMs. Instead, we test LLMs for OL as an \u201demergent\u201d behavior [57,62], where they demonstrate the capacity to generate responses on a wide range of tasks despite lacking explicit training. The key to unraveling the emergent abilities of LLMs is to prompt them for their knowledge, as popularized by GPT-3 [9], via carefully designed prompts. As discussed earlier (see section 2), prompt engineering for LLMs is a new AI sub-discipline. In this process, a pre-trained language model receives a prompt, such as a natural language statement, to generate responses without further training or gradient updates to its parameters [34]. Prompts can be designed in two main types based on the underlying LLM pretraining objective: cloze prompts [50,11], which involve filling in blanks in an incomplete sentence or passage and suit masked language modeling pre-training; and prefix prompts [33,30], which generate text following a given starting phrase and offer more design adaptability to the underlying model. The earlier introduced LLMs4OL paradigm is empirically validated for three select OL tasks using respective prompt functions fprompt(.) suited to each task and model. Task A - Term Typing. A generalized type is discovered for a lexical term. The generic cloze prompt template is f A c\u2212prompt(L) := [S?]. [L] [Pdomain] is a [MASK]. where S is an optional context sentence, L is the lexical term prompted for, Pdomain is a domain specification, and the special MASK token is the type output expected from the model. Since prompt design is an important factor that determines how the LLM responds, eight different prompt template instantiations of the generic template were leveraged with final results reported\nfor the best template. E.g., if WordNet is the base ontology, the part-of-speech type for the lexical term is prompted. In this case, template 1 is \u201c[S]. [L] POS is a [MASK].\u201d Note here \u201c[Pdomain]\u201d is POS. Template 2 is \u201c[S]. [L] part of speech is a [MASK].\u201d Note here \u201c[Pdomain]\u201d is \u201cpart of speech.\u201d In a similar manner, eight different prompt variants from the generic template were created. However, the specification of \u201c[Pdomain]\u201d depended on the ontology\u2019s knowledge domain. The prefix prompt template reuses the cloze prompt template but appends an additional \u201cinstruction\u201d sentence and replaces the special [MASK] token with a blank or a \u201c?\u201d symbol. Generically, it is f A p\u2212prompt(T) = [instruction] + f A c\u2212prompt(T), where the instruction is \u201cPerform a sentence completion on the following sentence:\u201d Based on the eight variations created from the generic cloze template prompt, subsequently eight template variations were created for the prefix prompting of the LLMs as well with best template results reported. Task B - Taxonomy Discovery. Here a taxonomic hierarchy between pairs of types is discovered. The generic cloze prompt template is f B c\u2212prompt(a, b) := [a|b] is [Phierarchy] of [b|a]. This statement is [MASK]. Where (a, b) or (b, a) are type pairs, Phierarchy indicates superclass relations if the template is initialized for top-down taxonomy discovery, otherwise indicates subclass relations if the template is initialized for bottom-up taxonomy discovery. In Task B, the expected model output for the special [MASK] token for a given type pair was true or false. Similar to term typing, eight template variations of the generic template were created. Four of which were predicated on the top-down taxonomy discovery. E.g., \u201c[a] is the superclass of [b]. This statement is [MASK].\u201d Note here, [Phierarchy] is \u201csuperclass\u201d. Other three templates were based on [Phierarchy] \u2208 parent class, supertype, ancestor class. And four more template instantiations predicated on the bottom-up taxonomy discovery were based on [Phierarchy] \u2208 subclass, child class, subtype, descendant class. Thus eight experiments per template instantiation for the applicable LLM were run and the results from the best template were reported. The prefix prompt template, similarly, reuses the cloze prompt template with the [MASK] token replaced with a blank or \u201c?\u201d symbol. It is f B p\u2212prompt(a, b) = [instruction] + f B c\u2212prompt(a, b), with instruction \u201cIdentify whether the following statement is true or false:\u201d Task C - Non-Taxonomic Relation Extraction. This task discovers non-taxonomi semantic heterarchical relations between types. The cloze prompt template is f C c\u2212prompt(h, r, t) := [h] is [r] [t]. This statement is [MASK]. Where h is a head type, t is a tail type, and r is a non-taxonomic relationship between h and r. To support the discovery of a heterarchy that can consist of a 1-M relational cardinality, for a given relation, all possible type pairs of the ontology were created. The expected output for the [MASK] token was again true or false. Note, unlike in Task A and B, the given template was used as is and no variations of it were created.\nThe cloze prompt template is f C c\u2212prompt(h, r, t) := [h] is [r] [t]. This statement is [MASK]. Where h is a head type, t is a tail type, and r is a non-taxonomic relationship between h and r. To support the discovery of a heterarchy that can consist of a 1-M relational cardinality, for a given relation, all possible type pairs of the ontology were created. The expected output for the [MASK] token was again true or false. Note, unlike in Task A and B, the given template was used as is and no variations of it were created.\nAgain, the prefix prompt template reuses the cloze prompt template as the other tasks, with instructions similar to task B. It is f C p\u2212prompt(h, r, t) = [instruction] + f C c\u2212prompt(h, r, t)\n# 4 LLMs4OL - Three Ontology Learning Tasks Evaluations\n# 4.1 Evaluation Datasets - Ontological Knowledge Sources\n4.1 Evaluation Datasets - Ontological Knowledge Sources\nTo comprehensively assess LLMs for the three OL tasks presented in the previous section, we cover a variety of ontological knowledge domain sources. Generally, across the tasks, four knowledge domains are represented, i.e. lexicosemantic \u2013 WordNet [40], geographical \u2013 GeoNames [1], biomedicine \u2013 Unified Medical Language System (UMLS) [7] teased out as the National Cancer Institute (NCI) [43], MEDCIN [39], and Systematized Nomenclature of Medicine \u2013 Clinical Terms United States (SNOMEDCT US) [56] subontologies, and content representations in the web \u2013 schema.org [47]. Tasks A, B, and C applied only to UMLS. In other words, the ontology has a supporting knowledge base with terms that can be leveraged in the test prompts for term typing as Task A, taxonomic hierarchical relational prompts as Task B, and non-taxonomic heterarchical relational prompts as Task C. The GeoNames source came with a knowledge base of terms instantiated for types and taxonomic relations, therefore, was leveraged in the Task A and B as OL tests with LLMs of this work. The WordNet source could be leveraged only in Task A since it came with an instantiated collection of lexical terms for syntactic types. It was not applicable in the Tasks B and C for OL defined in this work since the semantic relations in WordNet are lexicosemantic, in other words, between terms directly and not their types. Finally, since the schema.org source offered only typed taxonomies as standardized downloads, it was leveraged only in the OL Task B of this work. In this case, we refrained from scraping the web for instantiations of the schema.org taxonomy. For all other ontological knowledge sources considered in this work that were relevant to Task A, the term instantiations were obtained directly from the source. This facilitates replicating our Task A dataset easily. Detailed information on the ontological knowledge sources per task with relevant dataset statistics are presented next. Task A Datasets. Table 1 shows statistical insights for the Task A dataset where we used terms from WordNet, GeoNames, and UMLS. For WordNet we used the WN18RR data dump [14] that is derived from the original WordNet but released as a benchmark dataset with precreated train and test splits. Overall, it consists of 40,943 terms with 18 different relation types between the terms and four term types (noun, verb, adverb, adjective). We combined the original validation and test sets as a single test dataset. GeoNames comprises 680 categories of geographical locations, which are classified into 9 higher-level categories, e.g. H for stream, lake, and sea, and R for road and railroad. UMLS contains almost three million concepts from various sources which are linked together by semantic relationships. UMLS is unique in that it is a greater semantic ontological network that subsumes other biomedical problem-domain restricted subontolo-\nTable 1. Task A term typing dataset counts across three core ontological knowledge sources, i.e. WordNet, GeoNames, and UMLS, where for Task A UMLS is represented only by the NCI, MEDCIN, and SNOMEDCT US subontological sources. The unique term types per source that defined Task A Ontology Learning is also provided.\nParameter\nWordNet GeoNames\nNCI MEDCIN SNOMEDCT US\nTrain Set Size\n40,559\n8,078,865 96,177\n277,028\n278,374\nTest Set Size\n9,470\n702,510 24,045\n69,258\n69,594\nTypes\n4\n680\n125\n87\n125\ngies. We grounded the term typing task to the semantic spaces of three select subontological sources,i.e. NCI, MEDCIN, and SNOMEDCT US. The train datasets were reserved for LLM fine-tuning. Among the 11 models, we selected the most promising one based on its zero-shot performance. The test datasets were used for evaluations in both zero-shot and fine-tuned settings. Task B Datasets. From GeoNames, UMLS, and schema.org we obtained 689, 127, and 797 term types forming type taxonomies. Our test dataset was constructed as type pairs, where half represented the taxonomic hierarchy while the other half were not in a taxonomy. This is based on the following formulations.\n# \u2200(a \u2208Tn, b \u2208Tn+1) \ufffd\u2212\u2192(aRb \u2227b\u00acRa)\n\u2200(a \u2208Tn, b \u2208Tn+1, c \u2208Tn+2); (aRb \u2227bRc) \ufffd\u2212\u2192aRc \u2200(a \u2208Tn, b \u2208Tn+1, c \u2208Tn+2); (c\u00acRb \u2227b\u00acRa) \ufffd\u2212\u2192c\u00acRa\nWhere a, b, and c are types at different levels in the hierarchy. T is a collection of types at a particular level in the taxonomy, where n + 2 > n + 1 > n and n is the root. The symbol R represents \u201ca is a super class of type b\u201d as a true taxonomic relation. Conversely, the \u00acR represents \u201cb is a super class of type a\u201d as a false taxonomic relation. Furthermore, transitive taxonomic relations, (aRb \u2227bRc) \ufffd\u2212\u2192aRc, were also extracted as true relations, while their converse, i.e. (c\u00acRb \u2227b\u00acRa) \ufffd\u2212\u2192c\u00acRa were false relations. Task C Datasets. As alluded to earlier, Task C evaluations, i.e. non-taxonomic relations discovery, were relegated to the only available ontological knowledge source among those we considered i.e. UMLS. It reports 53 non-taxonomic relations across its 127 term types. The testing dataset comprised all pairs of types for each relation, where for any given relation some pairs are true while the rest are false candidates. Task B and Task C datasets\u2019 statistics are in Table 2.\n# 4.2 Evaluation Models - Large Language Models (LLMs)\nAs already introduced earlier, in this work, we comprehensively evaluate eight main types of domain-independent LLMs reported as state-of-the-art for different tasks in the community. They are: BERT [15] as an encoder-only architecture, BLOOM [55], LLaMA [58], GPT-3 [9], GPT-3.5 [45], and GPT-4 [46] as decoderonly models, and finally BART [32] and Flan-T5 [10] as encoder-decoder models.\nTable 2. Dataset statistics as counts per reported parameter for Task B type taxonomic hierarchy discovery and Task C type non-taxonomic heterarchy discovery across the pertinent ontological knowledge sources respectively per task.\nTask\nParameter\nGeoNames\nUMLS\nschema.org\nTask B\nTypes\n689\n127\n797\nLevels\n2\n3\n6\nPositive/Negative Samples\n680/680\n254/254\n2,670/2,670\nTrain/Test split\n272/1,088\n101/407\n1,086/4,727\nTask C\nNon-Taxonomic Relations\n-\n53\n-\nPositive/Negative Samples\n-\n5,641/1,896\n-\nTrain/Test Split\n-\n1,507/6,030\n-\nNote these LLMs are released at varying parameter sizes. Thus qualified by the size in terms of parameters written in parenthesis, in all, we evaluate seven LLMs: 1. BERT-Large (340M), 2. BART-Large (400M), 3. Flan-T5-Large (780M), 4. Flan-T5-XL (3B), 5. BLOOM-1b7 (1.7B), 6. BLOOM-3b (3B), 7. GPT-3 (175B), 8. GPT-3.5 (174B), 9. LLaMA (7B), and GPT-4 (>1T). Additionally, we also test an eleventh biomedical domain-specific model PubMedBERT [18]. In this work, since we propose the LLMs4OL paradigm for the first time, in a sense postulating OL as an emergent ability of LLMs, it is important for us to test different LLMs on the new task. Evaluating different LLMs supports: 1) Performance comparison - this allows us to identify which models are effective for OL, 2) Model improvement - toward OL one can identify areas where the models need improvement, and 3) Research advancement - with our results from testing and comparing different models, researchers interested in OL could potentially identify new areas of research and develop new techniques for improving LLMs.\n# 4.3 Evaluations\nMetrics. Evaluations for Task A are reported as the mean average precision at k (MAP@K), where k = 1, since this metric was noted as being best suited to the task. Specifically, in our case, for term typing, MAP@1 measures the average precision of the top-1 ranked term types returned by an LLM for prompts initialized with terms from the evaluation set. And evaluations for Tasks B and C are reported in terms of the standard F1-score based on precision and recall.\nMetrics. Evaluations for Task A are reported as the mean average precision at k (MAP@K), where k = 1, since this metric was noted as being best suited to the task. Specifically, in our case, for term typing, MAP@1 measures the average precision of the top-1 ranked term types returned by an LLM for prompts initialized with terms from the evaluation set. And evaluations for Tasks B and C are reported in terms of the standard F1-score based on precision and recall. Results - Three Ontology Learning Tasks Zero-shot Evaluations. The per task overall evaluations are reported in Table 3. The three main rows of the table marked by alphabets A, B, and C correspond to term typing, type taxonomy discovery, and type non-taxonomic relational hetrarchy discovery results, respectively. The five subrows against Task A shows term typing results for WordNet, GeoNames, and the three UMLS subontologies, viz. NCI, SNOMEDCT US, and MEDCIN. The three subrows against Task B shows type taxonomy discovery results for GeoNames, UMLS, and schema.org, respectively. Task C evaluation\nResults - Three Ontology Learning Tasks Zero-shot Evaluations. The per task overall evaluations are reported in Table 3. The three main rows of the table marked by alphabets A, B, and C correspond to term typing, type taxonomy discovery, and type non-taxonomic relational hetrarchy discovery results, respectively. The five subrows against Task A shows term typing results for WordNet, GeoNames, and the three UMLS subontologies, viz. NCI, SNOMEDCT US, and MEDCIN. The three subrows against Task B shows type taxonomy discovery results for GeoNames, UMLS, and schema.org, respectively. Task C evaluation\nTable 3. Zero-shot results across 11 LLMs and finetuned Flan-T5-Large and Flan-T5XL LLMs results reported for ontology learning Task A i.e. term typing in MAP@1, and as F1-score for Task B i.e. type taxonomy discovery, and Task C i.e. type nontaxonomic relation extraction. The results are in percentages.\nZero-Shot Testing\nFinetuned\nTask\nDataset\nBERT-Large\nPubMedBERT\nBART-Large\nFlan-T5-Large\nFlan-T5-XL\nBLOOM-1b7\nBLOOM-3b\nGPT-3\nGPT-3.5\nLLaMA-7B\nGPT-4\nFlan-T5-Large\u2217\nFlan-T5-XL\u2217\nA\nWordNet\n27.9\n-\n2.2 31.3 52.2 79.2 79.1 37.9 91.7 81.4 90.1 76.9 86.3\nGeoNames\n38.3\n- 23.2 13.2 33.8 28.5 28.8 22.4 35.0 29.5 43.3 16.9\n18.4\nNCI\n11.1\n5.9\n9.9\n9.0\n9.8 12.4 15.6 12.7 14.7\n7.7 16.1 31.9 32.8\nSNOMEDCT US 21.1 28.5 19.8 24.3 31.6 37.0 37.7 24.4 25.0 13.8 27.8 33.4 43.4\nMEDCIN\n8.7 15.6 12.7 13.0 18.5 28.8 29.8 25.7 23.9\n4.9 23.7 38.4 51.8\nB\nGeoNames\n54.5\n- 55.4 59.6 52.4 36.7 48.3 53.2 67.8 33.5 55.4 62.5\n59.1\nUMLS\n48.2 33.7 49.9 55.3 64.3 38.3 37.5 51.6 70.4 32.3 78.1 53.4 79.3\nschema.org\n44.1\n- 52.9 54.8 42.7 48.6 51.3 51.0 74.4 33.8 74.3 91.7 91.7\nC\nUMLS\n40.1 42.7 42.4 46.0 49.5 43.1 42.7 38.8 37.5 20.3 41.3 49.1 53.1\nresults are provided only for UMLS. We first examine the results in the zero-shot setting, i.e. for LLMs evaluated out-of-the-box, w.r.t. three RQs. RQ1: How effective are LLMs for Task A, i.e. automated type discovery? We examine this question given the results in 5 subrows against the row A, i.e. corresponding to the various ontological datasets evaluated for Task A. Of the five ontological sources, the highest term typing results were achieved on the 4-typed WordNet at 91.7% MAP@1 by GPT-3.5. This high performance can be attributed in part to the simple type space of WordNet with only 4 types. However, looking across the other LLMs evaluated on WordNet, in particular even GPT-3, scores in the range of 30% MAP@1 seem to be the norm with a low of 2.2% by BART-Large. Thus LLMs that report high scores on WordNet should be seen as more amenable to syntactic typing regardless of the WordNet simple type space. Considering all the ontological sources, Geonames presents the most fine-grained types taxonomy of 680 types. Despite this, the best result obtained on this source is 39.4% from GPT-4 with BERT-Large second at a close 38.3%. This is better than the typing evaluations on the three biomedical datasets. Even the domain-specific PubMedBERT underperforms. In this regard, domain-independent models with large-scale parameters such a BLOOM (3B) are more amenable to this complex task. Since biomedicine entails deeper domain-specific semantics, we hypothesize better performance not just from domain-specific finetuning but also strategically for task-specific reasoning. The results overview is: 91.7% WordNet by GPT-3.5 > 39.4% GeoNames by GPT-4 > 37.7% SNOMEDCT US by BLOOM-3b > 29.8% MEDCIN by BLOOM-3b > 16.1% NCI by GPT-4.\nRQ2: How effective are LLMs to recognize a type taxonomy i.e. the \u201cis-a\u201d hierarchy between types? We examine this question given the results in the 3 subrows against the main row B, i.e. corresponding to the three ontological sources evaluated for Task B. The highest result was achieved for UMLS by GPT-4 at 78.1%. Of the open-source models, Flan-T5-XL achieved the best result at 64.3%. Thus for the term taxonomy discovery LLMs on average have proven most effective in the zero-shot setting on the biomedical domain. The results overview is: 78.1% UMLS by GPT-4 > 74.4% schema.org by GPT-3.5 > 67.8% GeoNames by GPT-3.5. Note the three GPT models were not open-sourced and thus we tested them with a paid subscription. For the opensource models, the results overview is: 64.3% UMLS by Flan-T5-XL > 59.6% GeoNames by Flan-T5-XL > 54.8% schema.org by Flan-T5-Large. RQ3: How effective are LLMs to discover non-taxonomic relations between types? We examine this question given the results in Table 3 row for Task C, i.e. for UMLS. The best result achieved is 49.5% by Flan-T5-XL. We consider this a fairly good result over a sizeable set of 7,537 type pairs that are in true non-taxonomic relations or are false pairs. Finally, over all the three tasks considered under the LLMs4OL paradigm, term typing proved the hardest obtaining the lowest overall results for most of its ontological sources tested including the biomedical domain in particular. Additionally in our analysis, GPT, Flan-T5, and BLOOM variants showed improved scores with increase in parameters, respectively. This held true for the closed-sourced GPT models, i.e. GPT-3 (175B) and GPT-3.5 (175B) to GPT-4 (>1T) and the open-sourced models, i.e. Flan-T5-Large (780M) to Flan-T5-XL (3B) and BLOOM from 1.7B to 3B. Thus it seems apparent that with an increased number of LLM parameters, we can expect an improvement in ontology learning.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ed6/2ed63f14-3c73-4901-b3e5-1f1cb059851f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. An illustration of the LLM finetuning workflow on tasks for ontology learning.</div>\nResults - Three Ontology Learning Tasks Finetuned LLM Evaluations. Our zero-shot test results indicate that while LLMs seem promising for OL they would need task-specific finetuning to be a practically viable solution. To this\nend, we adopt the method of \u201cinstruction tuning\u201d proposed as the FLAN collection which is the only known systematically deconstructed, effective way to finetune LLMs [35]. For finetuning, we choose the Flan-T5 LMM for two reasons: 1) it is open-source: we intend to foster future research directions for models unhidden behind paywalls to aid in democratizing LLM research, and 2) it showed consistently good performance across all tasks. The finetuning instructions were instantiated from a small selection of eight samples of each knowledge source\u2019 reserved training set and fed in a finetuning workflow shown in Figure 2. The finetuned Flan models\u2019 results (see last two columns in Table 3) are significantly boosted across almost all tasks. For task A, we observed an average improvement of 25% from zero-shot to the finetuned model for both Flan-T5 variants. Notably, SNOMEDCT US showed least improvement of 9%, while the WordNet showed the most improvement of 45%. For task B we marked an average improvement of 18%, and for task C 3%. Given an illustration of the results in ?? shows that on average finetuned models, even with fewer parameters outperforms models with 1000x or more parameters across the three OL tasks. These insights appear crucial to expedite developmental research progress for practical tools for OL using LLMs which we plan to leverage in our future work.\n# 5 Conclusions and Future Directions\nVarious initiatives benchmark LLM performance, revealing new task abilities [57,62]. These benchmarks advance computer science\u2019s understanding of LLMs. We explore LLMs\u2019 potential for Ontology Learning [17,38] through our introduced conceptual framework, LLMs4OL. Extensive experiments on 11 LLMs across three OL tasks demonstrate the paradigm\u2019s proof of concept. Our codebase facilitates replication and extension of methods for testing new LLMs. Our empirical results are promising to pave future work for OL. Future research directions in the field of OL with LLMs can focus on several key areas. First, there is a need to enhance LLMs specifically for ontology learning tasks, exploring novel architectures and fine-tuning to capture ontological structures better. Second, expanding the evaluation to cover diverse knowledge domains beyond the ones examined in the current work would provide a broader understanding of LLMs\u2019 generalizability. Third, hybrid approaches that combine LLMs with traditional ontology learning techniques, such as lexico-syntactic pattern mining and clustering, could lead to more accurate and comprehensive ontologies. Fourth, further research can delve into the extraction of specific semantic relations, like part-whole relationships or causality, to enhance the expressiveness of learned ontologies. Standardizing evaluation metrics, creating benchmark datasets, exploring dynamic ontology evolution, and domain-specific learning are important directions. Additionally, integrating human-in-the-loop approaches with expert involvement would enhance ontology relevance and accuracy. Exploring these research directions will advance LLM-based ontology learning, enhancing knowledge acquisition and representation across domains.\nSupplemental Material Statement: Our LLM templates, detailed results, and codebase are publicly released as supplemental material on Github https:// github.com/HamedBabaei/LLMs4OL.\n# Author Contributions\nHamed Babaei Giglou: Conceptualization, Methodology, Software, Validation, Investigation, Resources, Data Curation, Writing - Original Draft, Visualization. Jennifer D\u2019Souza: Conceptualization, Methodology, Investigation, Resources, Writing - Original Draft, Writing - Review & Editing, Supervision, Project administration, Funding acquisition. S\u00a8oren Auer: Conceptualization, Methodology, Investigation, Resources, Review & Editing, Supervision, Project administration, Funding acquisition.\n# Acknowledgements\nA 16-page final version of this paper has been accepted for publication in the research track of the 22nd International Semantic Web Conference (ISWC 2023). We thank the anonymous reviewers for their detailed and insightful comments on an earlier draft of the paper. This work was jointly supported by the German BMBF project SCINEXT (ID 01lS22070), DFG NFDI4DataScience (ID 460234259), and ERC ScienceGraph (ID 819536).\n# References\n1. Geonames geographical database (2023), http://www.geonames.org/ 2. Agirre, E., Ansa, O., Hovy, E., Mart\u00b4\u0131nez, D.: Enriching very large ontologies using the www. In: Proceedings of the First International Conference on Ontology Learning-Volume 31. pp. 25\u201330 (2000) 3. Akkalyoncu Yilmaz, Z., Wang, S., Yang, W., Zhang, H., Lin, J.: Applying BERT to document retrieval with birch. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. pp. 19\u201324. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-3004, https://aclanthology.org/D19-3004 4. Alfonseca, E., Manandhar, S.: An unsupervised method for general named entity recognition and automated concept discovery. In: Proceedings of the 1st international conference on general WordNet, Mysore, India. pp. 34\u201343 (2002) 5. Amatriain, X.: Transformer models: an introduction and catalog. arXiv preprint arXiv:2302.07730 (2023) 6. Asim, M.N., Wasim, M., Khan, M.U.G., Mahmood, W., Abbasi, H.M.: A survey of ontology learning techniques and applications. Database 2018, bay101 (2018) 7. Bodenreider, O.: The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Research 32(suppl 1), D267\u2013D270 (01 2004). https://doi.org/10.1093/nar/gkh061, https://doi.org/10.1093/nar/gkh061\n# A Apendix\nA.1 Task A\nPrompt Templates for WordNet Dataset. Templates for WordNet in zero shot testing for Task A are presented in Table 4.\nPrompt Templates for GeoNames Dataset. Templates for GeoNames in zero-shot testing for Task A are presented in Table 5. As a sentence S, we used [L] is a place in [COUNTRY ]. template.\nPrompt Templates for UMLS Dataset. Templates for UMLS sources (NCI, MEDCIN, and SNOMEDCT US) in zero-shot testing for Task A are presented in Table 6.\n# A.2 Task B\nTemplates for GeoNames, UMLS, and Schema.Org in zero-shot testing for Task B is for LLMs are presented in Table 7.\nTable 4. The WordNet zero-shot testing prompt templates for task A. L represen lexical entries, S represents sentence containing L. In the BERT/BART\u200c LLMs, f BART the [MASK] is being replaced by < mask > \u201d.\nLLMs\nPrompt Templates\nBERT/BART\n[S]. [L] POS is a [MASK] .\n[S]. [L] part of speech is a [MASK] .\n[S]. \u2019[L]\u2019 POS is a [MASK] .\n[S]. \u2019[L]\u2019 part of speech is a [MASK] .\n[L] POS is a [MASK] .\n[L] part of speech is a [MASK] .\n\u2019[L]\u2019 POS is a [MASK] .\n\u2019[L]\u2019 part of speech is a [MASK] .\nFlan-T5\n[S]. [L] POS is a ?\n[S]. [L] part of speech is a ?\n[S]. \u2019[L]\u2019 POS is a ?\n[S]. \u2019[L]\u2019 part of speech is a ?\n[L] POS is a ?\n[L] part of speech is a ?\n\u2019[L]\u2019 POS is a ?\n\u2019[L]\u2019 part of speech is a ?\nBLOOM/GPT-3\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. [L] POS is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. [L] part of speech is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. \u2019[L]\u2019 POS is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. \u2019[L]\u2019 part of speech is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [L] POS is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [L] part of speech is a\nPerform a sentence completion on the following sentence: \\n\nSentence: \u2019[L]\u2019 POS is a\nPerform a sentence completion on the following sentence: \\n\nSentence: \u2019[L]\u2019 part of speech is a\nLLaMA\nPerform a sentence completion on the following sentence: [S]. [L]\nPOS is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S]. [L]\npart of speech is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S].\n\u2019[L]\u2019 POS is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S].\n\u2019[L]\u2019 part of speech is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [L]\nPOS is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [L]\npart of speech is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \u2019[L]\u2019\nPOS is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \u2019[L]\u2019\npart of speech is a\n. \\n The answer is\nTable 5. The GeoNames zero-shot testing prompt templates for task A. L represents lexical entries, S represents sentence containing L. In the BERT/BART\u200c LLMs, for BART the [MASK] is being replaced by < mask > \u201d.\nLLMs\nPrompt Templates\nBERT/BART\n[S]. [L] is a [MASK] .\n[S]. [L] geographically is a [MASK] .\n[S]. \u2019[L]\u2019 is a [MASK] .\n[S]. \u2019[L]\u2019 geographically is a [MASK] .\n[L] is a [MASK] .\n[L] geographically is a [MASK] .\n\u2019[L]\u2019 is a [MASK] .\n\u2019[L]\u2019 geographically is a [MASK] .\nFlan-T5\n[S]. [L] is a ?\n[S]. [L] geographically is a ?\n[S]. \u2019[L]\u2019 is a ?\n[S]. \u2019[L]\u2019 geographically is a ?\n[L] is a ?\n[L] geographically is a ?\n\u2019[L]\u2019 is a ?\n\u2019[L]\u2019 geographically is a ?\nBLOOM/GPT-3\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. [L] is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. [L] geographically is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. \u2019[L]\u2019 is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. \u2019[L]\u2019 geographically is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [L] is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [L] geographically is a\nPerform a sentence completion on the following sentence: \\n\nSentence: \u2019[L]\u2019 is a\nPerform a sentence completion on the following sentence: \\n\nSentence: \u2019[L]\u2019 geographically is a\nLLaMA\nPerform a sentence completion on the following sentence: [S]. [L]\nis a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S]. [L]\ngeographically is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S].\n\u2019[L]\u2019 is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \\n [S].\n\u2019[L]\u2019 geographically is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \\n [L]\nis a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \\n [L]\ngeographically is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \\n \u2019[L]\u2019\nis a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \\n \u2019[L]\u2019\ngeographically is a\n. \\n The answer is\nTable 6. The UMLS zero-shot testing prompt templates for task A. L represents lexical entries, S represents sentence containing L. In the BERT/BART\u200c LLMs, for BART the [MASK] is being replaced by < mask > \u201d.\nLLMs\nPrompt Templates\nBERT/BART\n[S]. [L] in medicine is a [MASK] .\n[S]. [L] in biomedicine is a [MASK] .\n[S]. \u2019[L]\u2019 in medicine is a [MASK] .\n[S]. \u2019[L]\u2019 in biomedicine is a [MASK] .\n[L] in medicine is a [MASK] .\n[L] in biomedicine is a [MASK] .\n\u2019[L]\u2019 is a [MASK] .\n\u2019[L]\u2019 in biomedicine is a [MASK] .\nFlan-T5\n[S]. [L] in medicine is a ?\n[S]. [L] in biomedicine is a ?\n[S]. \u2019[L]\u2019 in medicine is a ?\n[S]. \u2019[L]\u2019 in biomedicine is a ?\n[L] in medicine is a ?\n[L] in biomedicine is a ?\n\u2019[L]\u2019 in medicine is a ?\n\u2019[L]\u2019 in biomedicine is a ?\nBLOOM/GPT-3\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. [L] in medicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. [L] in biomedicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. \u2019[L]\u2019 in medicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [S]. \u2019[L]\u2019 in biomedicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [L] in medicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: [L] in biomedicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: \u2019[L]\u2019 in medicine is a\nPerform a sentence completion on the following sentence: \\n\nSentence: \u2019[L]\u2019 in biomedicine is a\nLLaMA\nPerform a sentence completion on the following sentence: Sen-\ntence: [S]. [L] in medicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S]. [L]\nin biomedicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S].\n\u2019[L]\u2019 in medicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [S].\n\u2019[L]\u2019 in biomedicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [L] in\nmedicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: [L] in\nbiomedicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \u2019[L]\u2019\nin medicine is a\n. \\n The answer is\nPerform a sentence completion on the following sentence: \u2019[L]\u2019\nin biomedicine is a\n. \\n The answer is\n<div style=\"text-align: center;\">Table 7. The GeoNames, UMLS, and Schema.Org zero-shot testing prompt templates for task B. In the type pairs (a, b) or (b, a), where a is parent and b is child.</div>\nLLMs\nPrompt Templates\nBERT\n[a] is the superclass of [b]. This statement is a [MASK] .\n[b] is a subclass of [a]. This statement is a [MASK] .\n[a] is the parent class of [b]. This statement is a [MASK] .\n[b] is a child class of [a]. This statement is a [MASK].\n[a] is a supertype of [b]. This statement is a [MASK] .\n[b] is a subtype of [a]. This statement is a [MASK] .\n[a] is an ancestor class of [b]. This statement is a [MASK] .\n[b] is a descendant class of [a]. This statement is a [MASK] .\nBART\n[a] is the superclass of [b]. This statement is a < mask > .\n[b] is a subclass of [a]. This statement is a < mask > .\n[a] is the parent class of [b]. This statement is a < mask > .\n[b] is a child class of [a]. This statement is a < mask >.\n[a] is a supertype of [b]. This statement is a < mask > .\n[b] is a subtype of [a]. This statement is a < mask > .\n[a] is an ancestor class of [b]. This statement is a < mask > .\n[b] is a descendant class of [a]. This statement is a < mask > .\nFlan-T5\n[a] is the superclass of [b]. This statement is a\n[b] is a subclass of [a]. This statement is a\n[a] is the parent class of [b]. This statement is a\n[b] is a child class of [a]. This statement is a\n[a] is a supertype of [b]. This statement is a\n[b] is a subtype of [a]. This statement is a\n[a] is an ancestor class of [b]. This statement is a\n[b] is a descendant class of [a]. This statement is a\nBLOOM/GPT-3\nIdentify whether the following statement is true or false: \\n\nStatement: [a] is the superclass of [b]. \\n This statement is a\nIdentify whether the following statement is true or false: \\n\nStatement: [b] is a subclass of [a]. \\n This statement is a\nIdentify whether the following statement is true or false: \\n\nStatement: [a] is the parent class of [b]. \\n This statement is\na\nIdentify whether the following statement is true or false: \\n\nStatement: [b] is a child class of [a]. \\n This statement is a\nIdentify whether the following statement is true or false: \\n\nStatement: [a] is a supertype of [b]. \\n This statement is a\nIdentify whether the following statement is true or false: \\n\nStatement: [b] is a subtype of [a]. \\nThis statement is a\nIdentify whether the following statement is true or false: \\n\nStatement: [a] is an ancestor class of [b]. \\n This statement is a\nIdentify whether the following statement is true or false: \\n\nStatement: [b] is a descendant class of [a]. \\n This statement\nis a\n# B Flan-T5 Training Setups and Hyperparameter\nWe finetune Flan-T5 LM on three tasks and evaluate them on all three tasks using zero-shot testing. It involved employing different sources, i.e. WordNet (task A), GeoNames (task A and B), UMLS (the NCI source representing medical sources in task A, B, and C), and schema.org (task B). Considering task A as an 8-shot instance for training we combined samples task B and C training with the condition that only samples that are in the task A 8-shot instances are considered for inclusion. Next, using task-specific prompt templates, Flan-T5 inputs are generated for finetuning. Following, using designed prompt templates Flan-T5 is fine-tuned.\nWe utilized a consistent training strategy for all datasets and models, except for a few hyperparameters: batch size and finetuning steps. All the models were trained using AdamW optimizer with a learning rate of 1e-5. For the Flan-T5Large model, a batch size of 8 is used during training, while for the Flan-T5XL model, a batch size of 4 is employed on\u200c all datasets. The WordNet and Schema.Org datasets were finetuned for 5 training epochs on both models, similarly, UMLS was finetuned using 10 epochs, while GeoNames was finetuned on Flan-T5-Large for 10 epochs and on Flan-T5-XL for 6 epochs.\n# C Detailed Results\nThe Table 8 and Table 9 represent prompt template results across all the templates and LLMs for term typing. While the Table 10 represents prompt template results across all the templates and LLMs for taxonomy discovery.\nTable 8. The detailed results of zero-shot testing and finetuning across seven LLMs reported for ontology learning Task A, term typing in MAP@1. The results are in percentages. The \u2217denotes finetuning model results. Results for WordNet and GeoName datasets.\nDataset\nLLMs\nPrompt Templates\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nWordNet\nBERT-Large\n2.19\n9.36\n9.18\n19.41\n4.72\n19.34\n9.93\n27.85\nBART-Large\n0.01\n0.28\n0.22\n2.16\n0.01\n0.03\n0.0\n0.19\nFlan-T5-Large\n0.17\n19.70\n5.54\n31.26\n0.0\n3.03\n5.70\n26.80\nBLOOM-1b7\n66.83 71.53 79.20 76.84 40.08 61.96 68.39 70.03\nFlan-T5-XL\n2.81\n40.26 17.83 52.21\n0.01\n7.75\n18.47 18.85\nBLOOM-3b\n63.33 75.29 79.08 77.06 37.40 65.32 68.99 71.62\nLLaMA-7B\n37.26 74.61 70.16 75.97 24.28 76.61 67.41 81.38\nGPT-3\n15.32 26.55 37.86 27.57\n8.47\n27.13 27.51 24.65\nGPT-3.5\n24.27 80.81 89.46 91.72\n0.81\n60.76 49.38 82.41\nGPT-4\n-\n-\n-\n90.11\n-\n-\n-\n-\nFlan-T5-Large \u221773.32 76.74 54.57 76.90 10.83 61.36 54.29 69.32\nFlan-T5-XL \u2217\n84.51 84.77 77.27 86.28 50.23 76.46 72.38 80.51\nGeoNames\nBERT-Large\n38.34 29.79 30.86 35.32 23.61 25.66 11.32 30.44\nBART-Large\n8.47\n0.57\n2.23\n0.98\n21.48 20.51\n7.83\n23.21\nFlan-T5-Large\n11.55\n3.57\n13.16\n4.68\n9.45\n6.05\n8.17\n7.38\nBLOOM-1b7\n2.71\n2.54\n2.89\n3.20\n28.51 18.38 25.86 19.80\nFlan-T5-XL\n33.81 15.71 19.77 20.78 15.36 12.41 18.43 15.82\nBLOOM-3b\n3.76\n4.70\n2.64\n3.43\n28.84 18.08 25.64 20.71\nLLaMA-7B\n29.49 14.16 25.54 15.95 13.91\n9.44\n17.79 16.79\nGPT-3\n22.42\n8.72\n-\n7.50\n-\n-\n-\n-\nGPT-3.5\n35.00\n-\n-\n-\n-\n-\n-\n-\nGPT-4\n43.28\n-\n-\n-\n-\n-\n-\n-\nFlan-T5-Large \u221715.08 15.17 14.93 15.12 15.77 16.28 15.93 16.91\nFlan-T5-XL \u2217\n18.35 18.12 18.12 17.91 17.26 17.32 17.45 17.64\nTable 9. The detailed results of zero-shot testing and finetuning across seven LLMs reported for ontology learning Task A, term typing in MAP@1. The results are in percentages. The \u2217denotes finetuning model results. Results for NCI, SNOMEDCT US, and MEDCIN datasets.\nDataset\nLLMs\nPrompt Templates\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nNCI\nBERT-Large\n9.94\n9.76\n2.61\n2.90\n11.09 10.96\n1.12\n1.36\nPubMedBERT\n5.87\n5.36\n4.52\n2.79\n3.36\n1.61\n1.33\n0.65\nBART-Large\n7.09\n7.87\n5.14\n6.32\n9.10\n9.94\n7.24\n8.26\nFlan-T5-Large\n4.59\n5.06\n7.53\n8.96\n3.06\n4.25\n5.48\n5.84\nBLOOM-1b7\n12.03 12.10 11.22 12.43 10.95 10.45 11.13 11.49\nFlan-T5-XL\n4.44\n5.65\n7.41\n9.83\n2.12\n3.29\n3.87\n6.28\nBLOOM-3b\n13.77 14.35 12.94 14.41 14.26 14.06 14.92 15.56\nLLaMA-7B\n3.78\n4.05\n3.24\n4.77\n3.67\n3.92\n5.25\n7.71\nGPT-3\n9.30\n9.17\n11.03 12.74\n9.37\n8.75\n9.14\n9.11\nGPT-3.5\n11.04\n9.52\n14.70 14.22\n8.56\n8.13\n12.68 11.24\nGPT-4\n-\n-\n16.05\n-\n-\n-\n-\n-\nFlan-T5-Large \u221730.60 31.59 31.32 31.92 29.11 29.28 31.29 30.79\nFlan-T5-XL \u2217\n31.51 30.99 32.78 32.05 30.01 29.70 31.76 31.35\nSNOMEDCT\nBERT-Large\n19.83\n8.02\n1.06\n0.12\n21.10 12.76\n0.45\n0.04\nPubMedBERT\n28.48 22.47 13.91\n5.70\n7.96\n3.58\n2.29\n1.51\nBART-Large\n19.16 19.81\n4.16\n4.04\n17.54 17.89 10.06\n9.43\nFlan-T5-Large\n19.26 19.89 21.04 24.32\n8.07\n8.90\n11.54 12.92\nBLOOM-1b7\n32.43 37.02 13.78 19.97 29.48 30.40 31.24 33.86\nFlan-T5-XL\n25.21 26.23 30.09 31.65\n7.21\n8.22\n15.58 17.22\nBLOOM-3b\n34.26 37.69 27.18 27.87 31.06 32.21 33.29 35.47\nLLaMA-7B\n7.56\n6.75\n7.89\n8.06\n10.74 10.80 13.15 13.81\nGPT-3\n21.06 20.33 22.73 24.36 19.20 18.99 20.20 20.09\nGPT-3.5\n21.81 17.99 25.02 24.50 18.24 15.71 22.71 19.87\nGPT-4\n-\n-\n22.36 27.83\n-\n-\n-\n-\nFlan-T5-Large \u221732.27 31.99 31.56 31.36 32.00 31.50 33.39 33.05\nFlan-T5-XL \u2217\n43.39 42.03 42.76 41.75 40.89 40.31 42.60 42.48\nMEDCIN\nBERT-Large\n7.33\n1.25\n0.14\n0.05\n8.71\n1.19\n0.08\n0.01\nPubMedBERT\n15.62\n9.71\n5.20\n1.58\n5.68\n2.32\n1.27\n0.61\nBART-Large\n11.67 12.65\n2.27\n2.31\n9.40\n9.22\n5.47\n4.82\nFlan-T5-Large\n9.30\n8.08\n10.97 12.96\n2.89\n3.59\n6.71\n6.78\nBLOOM-1b7\n27.58 28.67\n2.70\n4.97\n26.38 28.76 26.89 26.69\nFlan-T5-XL\n15.24 15.89 18.04 18.51\n4.47\n5.44\n11.14 11.09\nBLOOM-3b\n23.05 28.31 14.39 10.82 22.58 24.23 27.30 29.81\nLLaMA-7B\n3.40\n2.80\n3.37\n3.73\n4.90\n4.47\n3.17\n3.80\nGPT-3\n22.40 22.56 25.72 24.91 19.75 17.80 19.92 18.57\nGPT-3.5\n22.51 22.06 23.92 23.58 20.46 19.84 22.37 20.23\nGPT-4\n-\n-\n21.25 23.61\n-\n-\n-\n-\nFlan-T5-Large \u221738.37 36.37 37.43 35.86 31.26 30.00 33.11 31.91\nFlan-T5-XL \u2217\n51.80 50.90 51.80 51.16 47.88 45.38 49.86 49.09\n<div style=\"text-align: center;\">Table 10. The detailed results of zero-shot testing and finetuning across seven LLMs reported for ontology learning Task B, type taxonomy discovery in F1-score. The results are in percentages. The \u2217denotes finetuning model results.</div>\nTable 10. The detailed results of zero-shot testing and finetuning across seven LLMs reported for ontology learning Task B, type taxonomy discovery in F1-score. The results are in percentages. The \u2217denotes finetuning model results.\nDataset\nLLMs\nPrompt Templates\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nGeoNames\nBERT-Large\n41.00 51.69 40.55 48.70 37.16 41.07 41.70 54.54\nBART-Large\n38.11 41.03 40.55 52.50 39.09 45.80 36.67 55.40\nFlan-T5-Large\n59.63 48.24 54.08 48.24 44.40 51.30 36.40 38.44\nBLOOM-1b7\n33.16 31.04 33.16 32.83 33.77 33.53 36.67 32.92\nFlan-T5-XL\n49.37 44.05 45.09 52.41 43.92 46.34 49.98 44.29\nBLOOM-3b\n35.85 39.12 53.92 30.22 35.62 33.60 48.26 37.73\nLLaMA-7B\n33.49 33.49 33.49 33.49 33.49 33.49 33.49 33.49\nGPT-3\n43.43 51.74 42.70 53.20 46.04 52.56 45.49 52.62\nGPT-3.5\n59.40 47.79 67.78 41.95 48.02 51.72 45.25 43.86\nGPT-4\n38.56 52.46 34.00 38.89 44.06 55.43 33.78 36.23\nFlan-T5-Large\u221742.53 59.40 40.29 62.46 46.03 57.41 42.49 62.04\nFlan-T5-XL\u2217\n48.41 34.80 55.23 46.96 57.48 36.29 59.05 49.26\nUMLS\nBERT-Large\n48.21 38.84 41.46 40.41 45.88 40.91 41.04 42.92\nPubMedBERT 33.71 33.71 33.71 33.71 33.71 33.71 33.71 33.71\nBART-Large\n36.02 48.21 41.42 49.90 39.37 47.47 42.39 45.46\nFlan-T5-Large\n47.55 51.22 55.32 40.94 49.45 50.87 44.23 42.90\nBLOOM-1b7\n33.71 36.18 33.71 38.26 33.71 35.89 33.27 33.60\nFlan-T5-XL\n64.25 46.53 51.00 41.54 60.07 42.83 51.25 41.18\nBLOOM-3b\n33.16 37.23 34.82 35.77 33.16 35.89 33.05 37.48\nLLaMA-7B\n32.94 32.94 32.94 32.94 32.94 32.94 32.94 32.94\nGPT-3\n51.58 49.41 49.86 42.90 50.57 46.07 45.36 46.72\nGPT-3.5\n61.38 70.38 63.91 66.82 63.14 67.27 56.64 64.41\nGPT-4\n41.19 76.99 42.55 63.88 50.28 78.11 36.59 60.72\nFlan-T5-Large\u221737.17 48.66 36.07 42.12 48.39 46.65 53.42 35.97\nFlan-T5-XL\u2217\n63.69 50.04 36.91 41.34 78.12 50.12 79.25 39.27\nschema.org\nBERT-Large\n43.85 41.17 44.06 43.20 43.70 40.05 42.15 43.72\nBART-Large\n34.62 38.69 39.28 52.90 38.20 41.17 43.26 42.74\nFlan-T5-Large\n46.98 49.92 46.11 54.78 40.27 54.47 42.06 47.93\nBLOOM-1b7\n33.39 47.83 33.39 39.77 38.92 48.56 44.35 39.57\nFlan-T5-XL\n42.70 33.45 33.59 42.76 36.69 34.04 33.75 36.45\nBLOOM-3b\n41.64 47.16 47.98 45.25 39.73 40.75 51.28 48.73\nLLaMA-7B\n33.37 33.37 33.37 33.37 33.37 33.37 33.37 33.37\nGPT-3\n49.64 49.28 50.97 48.03 47.19 48.63 48.87 49.48\nGPT-3.5\n56.84 74.38 58.52 70.16 53.35 72.35 54.16 71.03\nGPT-4\n58.47 72.82 65.83 63.30 50.56 74.24 57.45 63.69\nFlan-T5-Large\u221735.35 85.43 29.82 89.24 41.30 91.68 42.46 56.39\nFlan-T5-XL\u2217\n91.06 57.46 74.68 65.32 91.54 50.63 91.70 33.33\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Ontology Learning (OL) addresses the challenge of knowledge acquisition and representation in various domains, traditionally relying on manual specification by domain experts, which is time-consuming, costly, and error-prone. The emergence of Large Language Models (LLMs) offers a promising alternative to traditional OL methods by automating the extraction and structuring of knowledge from unstructured text.",
            "purpose of benchmark": "The benchmark aims to evaluate the effectiveness of LLMs in performing OL tasks, specifically term typing, taxonomy discovery, and extraction of non-taxonomic relations, thereby facilitating a cost-effective and scalable solution for knowledge acquisition."
        },
        "problem": {
            "definition": "The benchmark is designed to measure LLMs' ability to automate the identification and structuring of knowledge from textual information into ontologies.",
            "key obstacle": "Existing benchmarks often lack the capacity to evaluate the reasoning skills and domain expertise required for effective ontology construction, which the new benchmark aims to address."
        },
        "idea": {
            "intuition": "The inspiration behind the benchmark stems from the observation that LLMs, trained on extensive and diverse text, could potentially capture complex language patterns necessary for OL tasks.",
            "opinion": "The authors believe that the benchmark is crucial for validating the applicability of LLMs in OL and could significantly impact the efficiency of ontology construction.",
            "innovation": "This benchmark is innovative as it is the first to evaluate LLMs for OL tasks, incorporating diverse ontological knowledge domains and providing a structured evaluation framework.",
            "benchmark abbreviation": "LLMs4OL"
        },
        "dataset": {
            "source": "The dataset is sourced from established ontological knowledge bases including WordNet, GeoNames, and UMLS, combining real-world data with structured ontological information.",
            "desc": "The dataset includes a variety of terms and relations across different ontological sources, facilitating comprehensive evaluation of LLMs.",
            "content": "The dataset contains terms, types, and relations relevant to ontology learning, specifically focusing on lexicosemantic, geographical, and biomedical knowledge.",
            "size": "2,500,000",
            "domain": "Ontology Learning",
            "task format": "Term Typing"
        },
        "metrics": {
            "metric name": "MAP@1, F1-score",
            "aspect": "Model performance in terms of accuracy and relevance of the identified terms and relations.",
            "principle": "The metrics were chosen to effectively measure the precision of LLMs in identifying correct term types and relationships, reflecting their utility in OL tasks.",
            "procedure": "Model performance is evaluated using MAP@1 for term typing and F1-score for taxonomy discovery and non-taxonomic relation extraction, based on the predictions made by the LLMs."
        },
        "experiments": {
            "model": "The benchmark evaluates a mix of state-of-the-art LLMs including BERT, GPT-3, and Flan-T5, among others.",
            "procedure": "Models were fine-tuned on specific tasks and evaluated on their ability to perform OL tasks using tailored prompt templates.",
            "result": "The experiments revealed varying degrees of effectiveness across models, with some achieving high precision in term typing and taxonomy discovery.",
            "variability": "Variability was accounted for through multiple trials and testing across different subsets of the dataset to ensure robust evaluation."
        },
        "conclusion": "The benchmark demonstrates the potential of LLMs for OL tasks, highlighting the need for further research into model improvement and task-specific fine-tuning.",
        "discussion": {
            "advantage": "The benchmark provides a structured framework for evaluating LLMs in OL, contributing to the advancement of automated knowledge acquisition.",
            "limitation": "Some limitations include the potential for models to underperform on complex tasks requiring deep domain knowledge, which may affect the overall evaluation.",
            "future work": "Future research should explore enhancing LLMs for OL, expanding evaluation domains, and integrating traditional OL techniques with LLM capabilities."
        },
        "other info": {
            "code repository": "https://github.com/HamedBabaei/LLMs4OL",
            "supported projects": {
                "project1": "SCINEXT",
                "project2": "NFDI4DataScience",
                "project3": "ERC ScienceGraph"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The emergence of Large Language Models (LLMs) offers a promising alternative to traditional Ontology Learning (OL) methods by automating the extraction and structuring of knowledge from unstructured text."
        },
        {
            "section number": "2.3",
            "key information": "The benchmark aims to evaluate the effectiveness of LLMs in performing OL tasks, specifically term typing, taxonomy discovery, and extraction of non-taxonomic relations."
        },
        {
            "section number": "4.1",
            "key information": "The benchmark is innovative as it is the first to evaluate LLMs for OL tasks, incorporating diverse ontological knowledge domains and providing a structured evaluation framework."
        },
        {
            "section number": "5.1",
            "key information": "The benchmark demonstrates the potential of LLMs for OL tasks, highlighting the need for further research into model improvement and task-specific fine-tuning."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore enhancing LLMs for OL, expanding evaluation domains, and integrating traditional OL techniques with LLM capabilities."
        }
    ],
    "similarity_score": 0.7256084513328932,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLMs4OL_ Large Language Models for Ontology Learning.json"
}