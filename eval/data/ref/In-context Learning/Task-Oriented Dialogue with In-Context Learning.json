{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.12234",
    "title": "Task-Oriented Dialogue with In-Context Learning",
    "abstract": "We describe a system for building taskoriented dialogue systems combining the incontext learning abilities of large language models (LLMs) with the deterministic execution of business logic. LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling taskoriented dialogue systems to a large number of tasks. We make our implementation available for use and further study1.",
    "bib_name": "bocklisch2024taskorienteddialogueincontextlearning",
    "md_text": "# Task-Oriented Dialogue with In-Context Learning\nTom Bocklisch Thomas Werkmeister Daksh Varshneya Alan Nichol\u2217 Rasa\n# Abstract\nWe describe a system for building taskoriented dialogue systems combining the incontext learning abilities of large language models (LLMs) with the deterministic execution of business logic. LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling taskoriented dialogue systems to a large number of tasks. We make our implementation available for use and further study1.\narXiv:2402.12234v1\n# 1 Introduction\nThe workhorse of industrial task-oriented dialogue systems and assistants is a modular architecture comprising three components: natural language understanding (NLU), dialogue management (DM) 2 , and natural language generation (NLG) (Young et al., 2013; Young, 2007). Utterances spoken or written by end users are translated into dialogue acts, where a dialogue act comprises an intent and a set of entities. For example, an utterance such as \u201cI need a taxi to the station\u201d might be assigned to the intent book taxi\n\u2217alan@rasa.com 1The approach described in this paper is implemented in Rasa under the name CALM: https://rasa.com/docs/rasapro/calm/ 2While in the research literature, the dialogue manager is often separated into dialogue state tracking (DST) and dialogue policy components, this distinction is rarely made in industrial applications.\nand the entity destination with value \u201cstation\u201d. This dialogue act representation acts as the interface between the NLU and DM components of the system. The dialogue manager contains the logic to react to a book taxi intent by initiating a taxi booking task, prompting the end user for the time, pick-up location, etc. These fields are typically called slots. As the dialogue progresses, subsequent user messages are also represented as dialogue acts, such as inform(time=3pm). The dialogue manager reacts to this sequence of inputs by executing actions and responding to the end user, either via a rule-based or a model-based dialogue policy. We refer to this as the intent-based NLU approach, and it is used by the major industry platforms for building chat- and voice-based dialogue systems like Rasa (Bocklisch et al., 2017), Dialogflow3, Microsoft Luis4, and IBM Watson5.\n# 1.1 Limitations of an intent-based NLU approach\nA key feature of the intent-based NLU approach is that it poses natural language understanding as a classification task: messages are \u201cunderstood\u201d by assigning them to a predefined intent. This is a powerful simplifying assumption. In theory, intents provide an interface that fully abstracts the language understanding component from the dialogue manager. However, working with a fixed list of intents has limitations which become more pronounced as an application matures and scales:\n\u2022 The taxonomy of intents becomes difficult to remember and reason about when the number of intents reaches several hundred, complicat-\n3https://cloud.google.com/dialogflow 4https://www.luis.ai/ 5https://www.ibm.com/docs/en/cloudprivate/3.1.0?topic=services-watson-assistant\ning annotation & feedback loops as well as application debugging.\n# ing annotation & feedback loops as well as application debugging.\n Because the dialogue manager is coded to expect specific sequences of intents, making changes to intent definitions and introducing new intents becomes increasingly errorprone, as shifts in classifier outputs introduce regressions.\n Intents are typically defined to map closely to the tasks the assistant can perform, but user utterances often do not correspond directly to a specific task. A developer may create intents like replace card and block card, but end users often describe situations in their own terms (e.g. \u201cI lost my wallet\u201d), which could map to a number of different tasks.\n\u2022 Messages are assigned to the same intent irrespective of context.6\nFurthermore, salient information is often discarded when translating a surface form into a dialogue act, and it falls on the dialogue manager to reinterpret the output of the NLU module to account for context. For example, the dialogue manager may prompt the user with a yes/no question in order to fill a boolean slot (e.g. \u201cWould you like to proceed?\u201d). When the end user\u2019s response is mapped to an affirm or deny intent, the dialogue manager reinterprets this output and sets the boolean slot to true or false respectively. Similarly, the NLU module might detect a generic entity like person or location, which is then mapped to a task-specific slot like transfer recipient or taxi destination.\n# 1.2 Desiderata\nGiven the impressive capabilities of recent LLMs on language understanding benchmarks (Huang et al., 2022) and in-context learning (Brown et al., 2020), we investigate whether a superior approach to task-oriented dialogue can be developed by reconsidering the split of responsibilities between the NLU, DM, and NLG components.\n6It is possible to overcome this limitation either by adding heuristics to the NLU module, or by training a supervised model which includes conversation history in the input. In practice neither of these approaches has been widely adopted.\n6It is possible to overcome this limitation either by adding heuristics to the NLU module, or by training a supervised model which includes conversation history in the input. In practice neither of these approaches has been widely adopted.\nIn this work we aim to develop a system for building industrial task-oriented dialogue systems with the following attributes: Fast iteration: The system should allow for rapid prototyping and testing. The delay between making a change (e.g. modifying task logic) and testing it should ideally be measured in seconds. Short development time: The system should provide general conversational capabilities out of the box, so that developers can focus on implementing their unique business logic. Concise representation of business logic: Both developers and subject-matter experts with less technical knowledge should be able to create and modify task logic easily. Reliable execution of business logic: Business logic of arbitrary complexity should be executed reliably, i.e. we should not rely on a language model to remember and follow a set of steps and branching conditions. Explainable and debuggable: It should be possible to explain why the system responded in a certain way at any given time. Scalable to a large number of tasks: It is common for AI assistants in industry to support hundreds of tasks. The system needs to be able to identify the correct task out of hundreds of possibilities. Maintaining the system and adding new tasks should not become more complex as the system increases in size. Model agnostic: Progress in LLMs is rapid and the approach should allow developers to adopt the latest models without having to re-implement their business logic.\n# 2 Related Work\nHere we briefly describe some recent streams of research which have developed alternatives to the intent-based NLU paradigm for building taskoriented dialogue.\n# 2.1 End-to-End Learning\nThe advent of seq2seq models (Sutskever et al., 2014) gave rise to a number of end-to-end approaches, notably (Bordes et al., 2016) who used a synthetic task to study whether an end-to-end model could learn the business logic for a taskoriented dialogue system purely from conversation transcripts grounded in knowledge base queries.\nIn this stream of work, models are learned end-toend, but assistant responses selected from a candidate list rather than being generated. This framing has also been called next-utterance classification. Continuing this line of work are Hybrid Code Networks (Williams et al., 2017) which combine endto-end learned models with domain-specific software to run parts of the business logic. In a similar vein, dialogue transformers (Vlasov et al., 2019) have been proposed, which use self-attention over the dialogue history to make learned dialogue policies more robust to digressions. (Mosharrof et al., 2023) also learn a dialogue policy by training a seq2seq model which outputs the dialogue state, knowledge base queries and system actions at every turn to fulfill the goal of the user. The learned policy is shown to generalize well across unseen domains.\nThe PolyResponse model (Henderson et al., 2019) is a next-utterance classification approach that leverages transfer learning. The model is trained on a large corpus of unlabeled dialogue data and fine-tuned on domain-specific data to build a task-oriented system for a given domain.\nHowever, it has been previously noted that collecting hand-annotated examples for training task-oriented dialogue systems is challenging (Budzianowski et al., 2018) and hence a lot of recent work aims at making the learning more data-efficient. (Peng et al., 2021) leverages transfer learning to make learning end-to-end dialogue models more data efficient, but where assistant responses are generated (as in a seq2seq model) rather than retrieved.\n(Jang et al., 2022) leverage offline reinforcement learning (RL) to clone the behaviour exhibited by human agents in human-human conversations and use it to critique the actions taken by fine-tuned LLM-based agent at training time. On the other hand, (Hong et al., 2023) leverage task descriptions to generate relevant and diverse synthetic dialogues and use them to learn a dialogue agent via offline RL. This approach matches the performance of an LLM-based assistant using incontext learning, (Ouyang et al., 2022) but with more concise and informative responses. However, this approach still requires fine-tuning an LLM which can be prohibitively time consuming for rapidly iterating on an implementation.\n# 2.2 Alternative Representations of Dialogue\nOther approaches have developed new representations and data structures to change how the task of building a dialogue system is formulated. (Cheng et al., 2020) introduced a hierarchical graph structure which represents the ontology of a task. Dialogue state tracking is then framed as a semantic parsing task over this structure. (Andreas et al., 2020) represents the dialogue state as a dataflow graph, where the effect of each turn in a dialogue is to modify this graph. They show that using this representation, a generic seq2seq model can match the performance of neural architectures specifically designed for dialogue state tracking. One similarity between our system and the dataflow approach is that both use the output of a model to generate instructions, and then deterministically execute some logic. However, there are significant differences in the two approaches. Our work also uses a graph representation of computational steps, but only to represent the business logic for a specific task, as designed explicitly be a developer. Additionally, the dataflow approach produces computational steps such as the refer operation, which handles anaphora and entity resolution. In our system, the dialogue manager does not participate in language understanding, and coreference resolution is always handled implicitly, by including the conversation transcript in the LLM prompt and generating commands with the arguments already fully resolved. More generally, our approach uses the conversation transcript as a general-purpose representation of conversation state, while we use an explicit state representation only to track progress within the logic of a given task.\n# 2.3 Language Model \u201cAgents\u201d\nRecently, researchers have explored using the incontext learning (Brown et al., 2020) abilities of LLMs to have them act fully independently as dialogue systems (Yao et al., 2023), an approach sometimes referred to as LLM \u201cAgents\u201d. This line of work assumes that the business logic required to complete a task is not known a priori and must be inferred on-the-fly as a conversation progresses. This approach explores the possibility of creating fully open-ended assistants which can help with an infinite number of tasks, with the caveat that the developer of the assistant does not control the task logic. Industrial dialogue systems, on the other\nhand, typically support a known set of tasks whose logic needs to be followed faithfully. Another approach is to frame the task of dialogue state tracking as a text-to-SQL problem (Hu et al., 2022). By leveraging in-context learning, this work demonstrates a dialogue agent that outperforms previously built fine-tuned agents (Shin et al., 2022; Lee et al., 2021). This approach leverages example conversations, which are retrieved at run time and inserted into an LLM prompt. Finally, (Heck et al., 2023) leverage incontext learning without any insertion of conversation examples in the prompt to explore the capability of ChatGPT as a task oriented dialogue agent. They find the system to be competitive to previously built zero-shot (Hu et al., 2022) and few-shot (Lee et al., 2021; Shin et al., 2022) dialogue agents.\n# 3 Architecture\n4. The dialogue manager executes the relevant business logic deterministically, including any repair patterns, and continues executing actions until additional user input is required.\n# 3.1 Business Logic\nBusiness Logic describes the steps required to complete a specific task, such as transferring money. Tasks are defined in a declarative format called flows. A minimal flow comprises a description and a list of steps. The steps describe (i) what information is needed from the user (e.g. the amount of money and the recipient), (ii) what information is needed from APIs (e.g. the user\u2019s account balance) and (iii) any branching logic\nbased on the information that was collected. The following is an example definition of a minimal transfer money flow, which collects the recipient and amount from the user before initiating the transfer:\nbased on the information that was collected. The following is an example definition of a minimal transfer money flow, which collects the recipient and amount from the user before initiating the transfer: transfer_money: description: send money to another account steps: - collect: recipient - collect: amount - action: initiate_transfer This task specification is created by the developer of the assistant. In addition to this core logic, they have to define the data types of the recipient and amount slots, and provide the templated utterances for the steps in the flow. slots: recipient: type: text amount: type: float responses: utter_ask_recipient: - text: Who are you sending money to? utter_ask_amount: - text: How much do you want to send?\nThese two code snippets are all that is required to implement a task; there is no training data required for language understanding. The money transfer flow given here is a minimal example, but flows can include branching logic, function calls, calls to other flows, and more. A more complex example can be found in appendix A. Note that the flow definition does not make any reference to the user side of the conversation. Neither dialogue acts nor commands are represented. Business logic only describes the steps required to complete a task. It does not specify how the end user provides that information. While the flow only specifies the \u201chappy path\u201d, an assistant with this flow can already handle a large number of conversations, including repair cases like corrections, digressions, interruptions, and cancellations. This is described in section 3.3.\n# 3.2 Dialogue Understanding\nIn lieu of an NLU module, our system has a Dialogue Understanding module that leverages the in-context learning abilities of LLMs. Dialogue understanding, framed as a command generation problem, improves upon intent-based NLU in key ways: \u2022 While NLU interprets one message in isolation, DU considers the greater context: the\nwhole running transcript of the conversation as well as the assistant\u2019s business logic. Flow definitions and conversation state provide additional, valuable context for understanding users. This is especially useful for extracting slot values, which often requires coreference resolution.\n\u2022 While NLU systems output intents and entities representing the semantics of a message, DU outputs a sequence of commands representing the pragmatics of how the user wants to progress the conversation 7.\n\u2022 DU requires no additional annotated data beyond the specification of flows.\n While NLU systems assign a user message to one of a fixed list of intents, DU instead is generative, and produces a sequence of commands according to a domain-specific language and available business logic. This representation can express what users are asking with more nuance than a simple classification.\nThe following section illustrates these improvements with examples.\n# 3.2.1 Commands as a Domain-Specific Language\nThe output of the Dialogue Understanding component is a short sequence of commands (typically 13) describing how the end user wants to progress the conversation with the assistant. The allowed commands are shown in table 1. The following are some example user messages along with the corresponding command output. In the simplest case, the user expresses a wish that directly corresponds to one of the defined flows:\nAlternatively, the user may directly provide some of the required information.\n7This is a similar idea to (Andreas et al., 2020), where they also use a model to predict the perlocutionary force of an utterance\nStartFlow(flow name) CancelFlow SetSlot(slot name, slot value) ChitChat KnowledgeAnswer HumanHandoff Clarify(flow name 1, flow name\nTable 1: A list of the commands which the dialogue understanding component can produce. Commands exist for starting and cancelling flows, for setting slots, for handing non-task dialogue (e.g. chitchat or content from a knowledge base), for handing over the conversation to a human agent, and for triggering an additional clarification step to handle disambiguation.\nStartFlow(transfer money), SetSlot(recipient, John), SetSlot(amount, 55)\nNote that in the NLU-based approach, the values \u201cJohn\u201d and 45 would typically be extracted as generic person and number entities, but in our system they are directly mapped to task-specific slots by the dialogue understanding component using the information from the flow definitions. Because the context of the conversation is taken into account, slots are filled correctly even in cases of pragmatic implicature, something that is very difficult to achieve with intent-based NLU:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9661/966194ea-1112-4528-b384-04ede9706f16.png\" style=\"width: 50%;\"></div>\nComplex utterances can be represented faithfully as commands, when this would be difficult to achieve with an intent-based NLU approach. For example, a user correcting a previous input and starting another task:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e12e/e12e74c2-f001-403a-b878-4a134930dd99.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Section 3.3 describes how corrections, interrup-</div>\n# tions, and digressions are subsequently handled by our system.\ntions, and digressions are subsequently handled by our system.\n# 3.3 Conversation Repair\nAs described in section 3.1, a flow only specifies the steps required to complete a task. It does not represent a graph of possible conversation paths. Conversation repair defines a set of patterns, which are meta flows that describe how the assistant behaves in conversations that deviate from the \u201chappy path\u201d of a flow. We define the \u201chappy path\u201d as any conversation in which the end user, every time they are prompted for information via a collect step, successfully provides that information, progressing to the next step in the business logic. In production systems, end users frequently stray from the happy path, for example when they:\n# \u2022 cancel the current interaction\n# \u2022 interrupt the current process to achieve something else before continuing\n\u2022 correct something they said earlier\n\u2022 say something that requires further clarification\nFor these common situations, conversation repair provides patterns that are triggered through either specialized commands (e.g. CancelFlow) or when specific dialogue states are reached. For example, a specific pattern is triggered when a previously interrupted flow is resumed, because the interrupting flow finished or was cancelled. All patterns have a default implementation that can be overwritten by the developer of an assistant. The following example requires a clarification step because the developer has created flows for multiple card-related tasks, and the user\u2019s opening message does not provide enough information to infer which one they want:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b273/b273966c-78e9-4c4c-8403-918cbe26e33c.png\" style=\"width: 50%;\"></div>\nImplementing this behaviour using intent-based NLU is extremely challenging. Words like \u2018card\u2019 and \u2018cancel\u2019 are not, on their own, indicative of a specific intent. Similarly, end users often start a conversation with very long messages that also require clarification. These types of utterances tend to fit poorly into an intent classifier\u2019s taxonomy. Furthermore, the dialogue manager would have to be programmed to handle many such sequences for various task combinations. Our system handles disambiguation out of the box, without any additional effort from the developer.\n# 3.4 The Dialogue Stack\nOur system leverages a dialogue stack to process commands and execute business logic. The dialogue stack is a high-level representation of the conversation state. It maintains a Last-infirst-out (LIFO) stack of active flows, as well as the state of each individual flow. The dialogue stack is also used to provide additional context to the dialogue understanding module, for example which slots the active flow needs to fill, along with their data types and allowed values. This helps the LLM generate the correct commands. In turn, the commands generated by the LLM are used to manipulate the dialogue stack and conversation state with a specific set of operations. Commands can set slots and push new flows on to the dialogue stack. Beyond that, the commands the LLM generates do not directly manipulate the dialogue stack. They cannot directly remove or modify existing flows on the stack, eliminating the possibility of a malicious user overriding the business logic via prompt injection (Liu et al., 2023). After all commands are processed, the dialogue manager takes the dialogue stack and executes the topmost flow deterministically. This execution pauses when it reaches a listen step. This way, executing business logic and complex operations on the dialogue stack alike are the domain of deterministic, developer-defined logic and not left to an LLM.\nWith this approach, our system leverages the powerful language understanding capabilities of LLMs while limiting their access to directly intervene in the business logic.\n# 3.5 Optional Components\nThis section describes optional components which extend the core functionality of our system.\n# 3.6 Contextual Rephrasing\nThe Contextual Response Rephraser is an optional component which can be used to improve the end user experience. It uses an LLM to rephrase the templated response to better account for the conversation\u2019s context, improving fluency. This is especially helpful for generic messages. For example, responding to a request that is out of scope:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0bd6/0bd6d538-303e-4696-83ff-d2d25a7adaf1.png\" style=\"width: 50%;\"></div>\nWhile this can enhance the fluency and naturalness of conversations, there is a possibility that the rephrased response does not preserve the exact meaning of the templated utterance, depending on the choice of LLM, the prompt, and the sampling parameters. It is left to the developer of the assistant to decide if this is an acceptable trade-off.\n# 3.7 Flow Pre-Selection\nAs the number of tasks in an assistant increases, eventually the information required by the LLM exceeds the length of its context window. For these cases, the Dialogue Understanding component can be configured to pre-select a list of candidate flows to be included for in-context learning. This is achieved by using the latest user utterance to retrieve the k most similar flows, as measured by embedding both the user utterance and the descriptions of the flows. This can potentially introduce a performance drop as the correct flow is not guaranteed to be among the top k. We find that for sufficiently large k (e.g. k = 20), this error is negligibly small; in our experiments we were able to achieve a Hit@20 score of 100%.\n# 3.8 Information Retrieval\nIndustrial dialogue systems often combine taskoriented dialogue, built on business logic, with information retrieval. While flows are ideal for multi-step tasks that rely on real-time data fetched from APIs, end users often have questions which can be answered based on static data. In our architecture, this is achieved by having the Dialogue Understanding component generate a KnowledgeAnswer command. The \u2018KnowledgeAnswer\u2018 command is handled as a pattern (a prebuilt meta flow, see section 3.3). This pattern invokes an information retrieval component, which uses the latest user message as a query to a knowledge base, returning a selection of potentially relevant information. The results are then either presented directly to the user, or used as part of an LLM prompt to formulate an answer to the user. The latter approach is frequently called Retrieval-Augmented Generation (Gao et al., 2024). When the pattern triggered by a KnowledgeAnswer command has completed, it is removed from the top of the stack and the conversation proceeds as before.\n# 4 Evaluation\nQuantitatively evaluating an approach and system for building conversational AI is difficult (Bohus and Rudnicky, 2009). Desirable qualities like ease of use and a quick learning curve can be studied by recruiting participants for a controlled study. Adoption is another signal of success and can attest to the scalability of the solution. In lieu of an absolute evaluation of the effectiveness of our system, we compare it in relative terms to an implementation in Rasa that follows an intentbased NLU approach. Our evaluation compares the effort required to achieve a similar level of functionality in both systems.\n# 4.1 Example Assistant\nAs an example system we implemented a virtual assistant in English for a travel rewards bank account. The implementations, tests, and instructions for reproducing these experiments are available online8. The assistant supports the following tasks:\n8https://github.com/RasaHQ/tod-in-context-learning\nTransferring money Adding, listing, and removing known contacts Showing recent transactions Ordering a replacement card Searching for restaurants and hotels Setting up recurring payments Verifying an account\n# 4.2 Metrics\nWe evaluate both our system and intent-based implementations through a suite of 71 test conversations designed to test a variety of conversational abilities. Both implementations were built using a these tests as a guide, adopting a test-driven development approach. The tests cover each of the tasks implemented in the assistant and a combination of happy paths and conversations involving repair. The test conversations vary in length with a minimum of 2 turns, a maximum of 19, and an average of 7.8 turns. Note that each test conversation represents a distinct conversation \u201cpath\u201d, meaning that our tests are not designed to evaluate understanding of variations in phrasing, but rather variations in user behaviour. The full set of test conversations is available together with the implementation in the github respository, and some example test conversations are shown in appendix B As a proxy for \u201ceffort\u201d, we measure the lines of code and data in each implementation. The implementation using the system described in this paper comprises 14 flows and 47 slots. The baseline implementation comprises 26 intents, 10 entity types, and 41 slots. Table 2 compares the pass rate of both implementations on our test conversations as well as the lines of code and data in each implementation. For these experiments, we used GPT-4 as the LLM to power the dialogue understanding component, since it performed best in our experiments. A comparison of the performance of different LLMs across languages and use cases is left to future work.\n# 5 Discussion\nWhile informative, evaluating our system by comparing two implementations has a number of limitations and should not be overly relied upon as a quantitative guide. For one, a simple metric like \u201clines of code and data\u201d does not capture the com-\nTable 2: Fraction of passing tests and number of lines of code and data for two implementations, one using the system described in this paper and a baseline system using intent-based NLU.\nCategory\n# tests\nours\nbaseline\nhappy path\n24\n24\n22\ncancellations\n6\n6\n2\ncorrections\n13\n13\n0\nrepetitions\n2\n2\n1\ndisambiguation\n4\n3\n0\ninput validation\n5\n3\n3\nnegations\n3\n3\n0\nchitchat\n2\n2\n2\ndigressions\n9\n9\n2\nknowledge\n3\n3\n2\nTable 3: Number of passing tests in different categories, for ours and NLU-based implementations of similar effort.\nplexity of the two implementations. In our own subjective experience, the cognitive load of working with our system is much lower than with the intent-based NLU approach, and we hypothesise that this would be reflected in a controlled study involving external participants, manifesting for example as a reduced time to complete a given task. Second, our evaluation does not quantify the ability of either system to handle the lexical variation of real-world user input. This would be better evaluated by deploying both implementations side-byside in an A/B test. Nonetheless, we see distinctly that our system allows developers to build dialogue systems which can handle a multitude of conversation patterns with a modest amount of effort. Note also that unlike the baseline, the implementation using our system only addresses the happy paths, while corrections, digressions, and more are handled by outof-the-box conversation repair. The disambiguation cases are worth noting as well, as these are handled automatically by our system, while presenting an extreme challenge for an intent-based NLU approach. It is worth commenting on the impressive performance of our system on conversations involving corrections, especially in light of previous evidence that LLMs show poor per-\nformance on conversation repair (Balaraman et al., 2023). We believe this is because the repair-QA dataset, on which previous studies were based, is a far more challenging task that requires an LLM to produce free-form answers from the the world knowledge implicit in its parameters. Handling corrections in our system only requires the LLM to reason over the conversation transcript and produce the appropriate command, with the correct slot value typically present verbatim within the prompt.\n# 6 Future Work\nImportant avenues for further work in this area include more comprehensive evaluation of the current system, including case studies of production systems. In addition it would be valuable to study of the performance of different LLMs for the dialogue understanding task in various languages, including code-switching and multilingual applications. Also, given that the dialogue understanding task is well defined and requires an LLM only to produce a short sequence of known commands, it would be valuable to investigate whether smaller models can deliver similar performance at smaller cost and latency. Finally, it is crucial for production systems that these can be improved on the basis of interactions with real end-users9. There is a distinct difference in this regard between working with a system based on supervised learning (like intent-based NLU) versus one based on incontext learning. Finding ways to (beyond prompt engineering) incorporate the signal from real user feedback is an active area of investigation.\n# 7 Conclusion\nWe have introduced and described a system for developing industrial task-oriented dialogue systems, combining the in-context learning abilities of LLMs with the deterministic execution of business logic. We discussed the handling of both \u201chappy path\u201d and conversation repair-type dialogues, and show how these can be handled effectively with relatively little effort, compared to the traditional approach of intent-based NLU. We have made our system, as well as the example implementations, available for use and further study, and hope that this will facilitate the development of many successful conversational assistants.\n9We use the term Conversation-Driven Development to describe this process(Nichol, 2022).\n# Acknowledgments\nWe would like to thank Oliver Lemon for providing feedback on a draft of this paper. We thank numerous colleagues at Rasa for support with implementing and testing the system.\nacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-Oriented Dialogue as Dataflow Synthesis. Transactions of the Association for Computational Linguistics, 8:556\u2013571.\nVevake Balaraman, Arash Eshghi, Ioannis Konstas, and Ioannis Papaioannou. 2023. No that\u2019s not what i meant: Handling third position repair in conversa Tom Bocklisch, Joey Faulkner, Nick Pawlowski, and Alan Nichol. 2017. Rasa: Open source language understanding and dialogue management. arXiv preprint arXiv:1712.05181. Dan Bohus and Alexander I Rudnicky. 2009. The ravenclaw dialog management framework: Architecture and systems. Computer Speech & Language, 23(3):332\u2013361. Antoine Bordes, Y-Lan Boureau, and Jason Weston. 2016. Learning end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, BoHsiang Tseng, I\u02dcnigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u02c7si\u00b4c. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016\u20135026, Brussels, Belgium. Association for Computational Linguistics. Jianpeng Cheng, Devang Agrawal, H\u00b4ector Mart\u00b4\u0131nez Alonso, Shruti Bhargava, Joris\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae0a/ae0a0c70-4aaa-4525-8c33-15cafee0d90d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d0ee/d0ee5ade-8d4b-4322-9754-52df8752c7c3.png\" style=\"width: 50%;\"></div>\n# A Complex flow Example\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8528/852808ea-1e25-4e8a-a25a-e6354300ab5f.png\" style=\"width: 50%;\"></div>\n# B Example Test Conversations\nHere we show some example test conversations.\n<div style=\"text-align: center;\">B.1 Disambiguation</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c187/c1874327-df40-4a16-a545-68b4d2a8d3b2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d100/d100f043-d3d9-42fe-8dd3-ebdf637b97a4.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of intent-based natural language understanding (NLU) approaches in task-oriented dialogue systems, highlighting the need for a new method that leverages the in-context learning capabilities of large language models (LLMs) for improved dialogue management and execution of business logic.",
        "problem": {
            "definition": "The problem is the inefficiency and rigidity of existing intent-based NLU systems, which struggle to handle complex dialogues and require extensive effort to develop and maintain.",
            "key obstacle": "The main difficulty lies in the fixed taxonomy of intents that complicates the handling of user utterances, which often do not map directly to predefined intents and can lead to errors and regressions when changes are made."
        },
        "idea": {
            "intuition": "The idea was inspired by the capabilities of LLMs in understanding language and their potential to improve dialogue systems by allowing for more flexible and context-aware interactions.",
            "opinion": "The proposed idea involves using LLMs to translate user utterances into a domain-specific language (DSL) that can directly drive business logic execution, thus streamlining the dialogue process.",
            "innovation": "The innovation lies in replacing the traditional intent-based NLU approach with a generative dialogue understanding module that produces commands based on the entire conversation context, rather than classifying user inputs into fixed intents."
        },
        "method": {
            "method name": "CALM",
            "method abbreviation": "CALM",
            "method definition": "CALM is a system designed to build task-oriented dialogue systems by combining LLMs' in-context learning abilities with deterministic execution of business logic.",
            "method description": "CALM enables rapid development of chatbots that can navigate complex dialogues more effectively than traditional NLU-based systems.",
            "method steps": [
                "Define task flows in a declarative format.",
                "Use LLMs for dialogue understanding to generate commands based on user input.",
                "Execute the relevant business logic deterministically using a dialogue manager."
            ],
            "principle": "The effectiveness of CALM stems from its ability to leverage the context of the entire conversation and the specific business logic defined by developers, allowing for more nuanced and accurate command generation."
        },
        "experiments": {
            "evaluation setting": "The evaluation involved comparing CALM with a baseline intent-based NLU implementation using a suite of 71 test conversations designed to assess various conversational abilities in a travel rewards banking assistant.",
            "evaluation method": "Performance was measured based on the number of passing tests across different conversational scenarios, as well as the lines of code and data required for implementation."
        },
        "conclusion": "The experiments demonstrate that CALM significantly reduces development effort while effectively handling complex dialogues, outperforming traditional intent-based NLU systems and facilitating the creation of scalable conversational assistants.",
        "discussion": {
            "advantage": "Key advantages of CALM include reduced cognitive load for developers, improved handling of conversation repair and disambiguation, and the ability to scale to numerous tasks without increasing complexity.",
            "limitation": "A limitation of CALM is that it may not yet fully capture the lexical variation of real-world user inputs, and its performance may vary based on the specific LLM used.",
            "future work": "Future research should focus on comprehensive evaluations of CALM in production systems, exploring the performance of different LLMs, and investigating ways to incorporate real user feedback into the dialogue understanding process."
        },
        "other info": {
            "acknowledgments": "The authors thank Oliver Lemon for feedback and numerous colleagues at Rasa for support with implementing and testing the system.",
            "repository": "The implementation and tests are available online at https://github.com/RasaHQ/tod-in-context-learning."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the limitations of intent-based natural language understanding (NLU) approaches in task-oriented dialogue systems, highlighting the need for a new method that leverages the in-context learning capabilities of large language models (LLMs) for improved dialogue management."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea involves using LLMs to translate user utterances into a domain-specific language (DSL) that can directly drive business logic execution, thus streamlining the dialogue process."
        },
        {
            "section number": "3.1",
            "key information": "CALM enables rapid development of chatbots that can navigate complex dialogues more effectively than traditional NLU-based systems."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of CALM stems from its ability to leverage the context of the entire conversation and the specific business logic defined by developers, allowing for more nuanced and accurate command generation."
        },
        {
            "section number": "5.4",
            "key information": "The experiments demonstrate that CALM significantly reduces development effort while effectively handling complex dialogues, outperforming traditional intent-based NLU systems and facilitating the creation of scalable conversational assistants."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of CALM is that it may not yet fully capture the lexical variation of real-world user inputs, and its performance may vary based on the specific LLM used."
        }
    ],
    "similarity_score": 0.6902237706926663,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Task-Oriented Dialogue with In-Context Learning.json"
}