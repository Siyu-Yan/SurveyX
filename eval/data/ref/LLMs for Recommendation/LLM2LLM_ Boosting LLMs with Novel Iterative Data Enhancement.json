{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.15042",
    "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
    "abstract": "Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the lowdata regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available at https: //github.com/SqueezeAILab/LLM2LLM.",
    "bib_name": "lee2024llm2llmboostingllmsnovel",
    "md_text": "# M2LLM: Boosting LLMs with Novel Iterative D\n# osting LLMs with Novel Iterative Data Enhanc\nNicholas Lee\u22171 Thanakul Wattanawong\u22171 Sehoon Kim1 Karttikeya Mangalam1 Sheng Shen1 Gopala Anumanchipalli1 Michael W. Mahoney1,2,3 Kurt Keutzer1 Amir Gholami1,2 1UC Berkeley 2ICSI 3LBNL\n{nicholas.lee, j.wat, sehoonkim, mangalam, s.sheng, gopala, mahoneymw, keutzer, amirgh}@berkeley.edu\n# Abstract\nPretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the lowdata regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available at https: //github.com/SqueezeAILab/LLM2LLM.\n13 Jul 2024\n# 1 Introduction\nPretrained large language models (LLMs) have achieved impressive performance on various benchmarks and datasets that have previously required\n*Equal contribution\nspecialized neural network architectures. For many of these general benchmarks (Hendrycks et al., 2020; Zhong et al., 2023), LLMs are prompted with custom instructions or in-context examples. However, in various real-world applications, these prompting strategies are not a one-size-fitsall solution. For instance, LLMs have a limit on the amount of input context they can process, thus limiting the number of in-context examples or instructions we can input to make the LLM follow a certain behavior. For simple tasks that are closely aligned with the data that the LLM was pretrained on, extensive prompting may not be necessary. However, applying LLMs to specialized domains (e.g., a specific medical field (Nori et al., 2023) or private data with niche protocols) can be more challenging, often requiring prohibitively long prompts to achieve adequate performance. Even if the prompt length does not exceed the limit, processing long prompts increases the latency and cost of each inference. Additionally, LLMs also tend to forget or ignore information in long contexts (Liu et al., 2023b), leading to potential accuracy drops even when the model can handle long input prompts. While Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has been developed to address some of these challenges, it may sometimes retrieve irrelevant passages or documents, which can potentially degrade the generation performance. Furthermore, RAG does not necessarily solve the latency and cost issue as processing a long input prompt may still be required. A promising method for addressing this is finetuning. With the emergence of Parameter Efficient Fine-tuning (PEFT) (Hu et al., 2021; Mangrulkar et al., 2022), the computational resources required to fine-tune a task-specific LLM have decreased significantly. However, herein lies a new problem: successful fine-tuning requires enough training data. This can be challenging for some applications, where we only have access to a small amount\nof task-specific data. Often, collecting, cleaning, and labeling additional data can be costly and timeconsuming. So the key question is: how should we increase the user\u2019s training data to be enough for fine-tuning? Data augmentation is a known method that could help effectively expand the training dataset. For natural language processing (NLP) tasks, one can use approaches such as synonym replacement, character replacement (e.g., by intentionally introducing spelling errors), random swapping, and back translation, just to name a few (Wei and Zou, 2019; Belinkov and Bisk, 2017; Coulombe, 2018; Zhang et al., 2018). However, these approaches fail to effectively expand the training data for fine-tuning LLMs in the case of new and specialized tasks, as we will show later in Section 4.3. To address this, several recent papers have explored using an LLM to expand the fine-tuning dataset (Dai et al., 2023; Kumar et al., 2020; Zhou et al., 2023; Chen et al., 2023; Cao et al., 2023; Wei et al., 2023; Zhu et al., 2023). This approach has proven to be more effective than traditional data augmentation methods. However, these approaches often apply LLM-based data augmentation on all of the available training dataset, without considering the LLM\u2019s prediction accuracy on individual training data points. We have observed that for various reasoning tasks such as arithmetic and reading comprehension, the LLM correctly solves simpler examples in the fine-tuning dataset, but may struggle with harder examples. It will be sub-optimal to keep augmenting data points for which the LLM is already achieving high accuracy on. To address these challenges, we introduce LLM2LLM, a new targeted and iterative data augmentation framework that uses a teacher LLM to expand the training dataset, with a targeted and iterative approach. In more detail, we make the following contributions:\n We propose LLM2LLM, a targeted and iterative LLM-based data augmentation technique that efficiently and effectively augments small task-specific datasets. LLM2LLM achieves this by (1) fine-tuning a student LLM on the initial dataset, (2) evaluating on the training data and extracting data points which the model got incorrect after training, and (3) using a Self-Instruct (Wang et al., 2023) style data augmentation to augment these data points, which are then added back into the training data (Section 3.1).\n\u2022 We benchmark LLM2LLM on randomly sampled subsets of GSM8K (Cobbe et al., 2021), CaseHOLD (Zheng et al., 2021), SNIPS (Coucke et al., 2018), TREC (Li and Roth, 2002) and SST-2 (Socher et al., 2013) in order to evaluate the effectiveness of our approach in the low-data regime (Section 4.2). Here, we get up to a 24.2% improvement on GSM8K, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC, and 39.8% on SST-2 (Table 1). \u2022 We conduct a series of ablations studies comparing LLM2LLM to several existing baselines as well as to variants of LLM2LLM to evaluate the effectiveness of our design decisions (Section 4.5). We observe that both the iterative and targeted nature of LLM2LLM are critical to improving model performance.\n# 2 Background and Related Work\n# 2 Background and Related Work 2.1 Instruction Following LLMs\n# 2.1 Instruction Following LLMs\nThe earliest works (Wei et al., 2021; Longpre et al., 2023; Chung et al., 2022; Aribandi et al., 2021; Sanh et al., 2021; Muennighoff et al., 2023; Wang et al., 2022b; Mishra et al., 2022; Wang et al., 2022a; Xu et al., 2022) in instruction fine-tuning involved gathering and processing different existing NLP datasets in order to improve the performance of LLMs on a wide range of tasks. SelfInstruct (Wang et al., 2023) removed the reliance on existing datasets by introducing a framework for bootstrapping instruction datasets with the outputs of the model itself. Follow-up work (Ouyang et al., 2022; Taori et al., 2023; Geng et al., 2023; Chiang et al., 2023; Xu et al., 2023; Mukherjee et al., 2023; Mitra et al., 2023; Kang et al., 2023; Nori et al., 2023) took advantage of stronger models (Achiam et al., 2023; Touvron et al., 2023a,b) in order to fine-tune stronger general-purpose instructionfollowing models.\n# 2.2 Self-Improving LLMs\nVarious early works (Zelikman et al., 2023; Haluptzok et al., 2023; Zelikman et al., 2022; Madaan et al., 2023; Gulcehre et al., 2023; Singh et al., 2023) explore using self-improvement for finetuning LLMs. These works generally filtered the outputs of the model before fine-tuning it on its own outputs. LLM2LLM differs from these methods, as we do not directly fine-tune on the outputs of our own model, and we employ a teacher model to provide feedback in the form of synthetic data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/43d2/43d28110-c8a4-462c-aa79-013292579afb.png\" style=\"width: 50%;\"></div>\n# LLM2LLM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8e2f/8e2f43e9-a75e-45c8-b1dd-a99011e0e626.png\" style=\"width: 50%;\"></div>\nQuestion: When the lengths of each side of a rectangle  are 12 and 5, what is the length of the diagonal? Answer: sqrt(12 ** 2 + 5 ** 2) = 13\nFigure 1: LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement. One iteration of LLM2LLM begins with training and evaluating the model on the training data. Incorrect answers from the training data are used as inputs to generate extra samples with similar styles to the teacher model. Then, a new student model is trained using a combination of the old training data and newly generated samples. After the model is fine-tuned, we evaluate and find questions that the model got incorrect. The teacher model is used to generate additional data points based on the wrong examples, which test for similar concepts and ideas. These synthetic data points are folded back into training dataset. This process then repeats, training the student model on increasingly targeted data points.\nConcurrent with our work, several papers have been recently published that use an iterative approach to improving LLMs (Chen et al., 2024; Anil et al., 2023; Burns et al., 2023; Li et al., 2023b; Yuan et al., 2024). These works combine ideas from Reinforcement Learning (RL) and SelfPlay (Samuel, 2000; Tesauro et al., 1995) in order to iteratively build stronger LLMs by fine-tuning on the outputs of the model itself. LLM2LLM is distinguished by how it focuses on the lowdata regime for task-specific fine-tuning of LLMs whereas others are attempting to create stronger general-purpose LLMs. In addition, our technique exclusively uses the data points that the model got incorrect during training, and it uses the teacher model to augment these data points. Instead of providing feedback in the form of a critique or rationale, the teacher model\u2019s feedback is only in the form of synthetic data points, which simplifies the training pipeline.\n# 2.3 Data Augmentation\nData augmentation has been long studied in NLP. Early work augmented at the character (Belinkov and Bisk, 2017; Coulombe, 2018) and word (Wei and Zou, 2019) level. Notably, Easy Data Augmentation (EDA) (Wei and Zou, 2019) was a popular early method that used word level augmentations:\nGenerate additional data with the wrong examples\nsynonym replacement, random insertion, swap, and deletion to augment data for text classification. We refer the reader to (Feng et al., 2021) for a more complete summary of data augmentation in NLP. A popular new approach is to use LLMs themselves to synthesize new training data (Deng et al., 2023a; Prasad et al., 2023; Fu et al., 2023b; Dai et al., 2023; Ubani et al., 2023; Fang et al., 2023; Liu et al., 2023a; Yu et al., 2023; Kumar et al., 2020; Yoo et al., 2021; Wang et al., 2021; Ding et al., 2023; Li et al., 2023a; Liang et al., 2023). A noteworthy example is AugGPT (Dai et al., 2023), which used ChatGPT to rephrase text to augment text classification tasks. Many of these techniques generate very large amounts of synthetic data. Recent work (Chen et al., 2023; Cao et al., 2023; Wei et al., 2023; Zhou et al., 2023) found that one could replicate the results of fine-tuning on these large datasets with significantly smaller subsets.\n# 3 Methodology\nWe assume that we are given an LLM model M (e.g., GPT-3.5 or Llama-2-7B) that is pre-trained on some source dataset (e.g., Common Crawl). The goal is to adapt M (hereon called the student model) to a new target domain by using a small\nAlgorithm 1 LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement. Given a seed dataset D0, we finetune the model Mi student, evaluate, and extract training set data points that the model gets wrong. These are used to generate new training data points using the teacher model Mteacher for the next step.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dbfe/dbfed8c7-c012-44f6-9294-1de2ce3462fa.png\" style=\"width: 50%;\"></div>\nM\n1: procedure LLM2LLM(M0\nstudent, Mteacher, D0)\n2:\ni \u21900\n3:\nwhile i < n do\n4:\nMi\nstudent \u2190Finetune(M0\nstudent, Di)\n5:\nEi \u2190Evaluate(Mi\nstudent, D0)\n\u25b7Evaluate on seed data\n6:\nW i \u2190Filter(Ei,D0)\n\u25b7Keep wrong answers\n7:\nAi \u2190Generate(Mteacher, W i)\n\u25b7Augment using teacher\n8:\nDi+1 \u2190Di + Ai\n\u25b7Append to data\n9:\ni \u2190i + 1\n10:\nend while\n11:\nEvaluate M \u2217\nstudent\n12: end procedure\nseed dataset D, where D potentially has unseen characteristics, compared to the pre-trained dataset (e.g., a medical dataset with specific terminology, or a private database with specific characteristics). In this case, the model\u2019s zero-shot or fine-tuned performance is likely to be unsatisfactory. While strategies to address this challenge have been explored, e.g., through enhanced few-shot learning methods as discussed in Section 2, here we strictly focus on enriching the provided target dataset D with an LLM. This method is orthogonal to the aforementioned techniques, offering a complementary solution that can be applied alongside them. To enrich D, AugGPT (Dai et al., 2023) has introduced a promising approach that generates additional augmented data by applying a prompted LLM to all available data points in the target training dataset. However, this method falls short by indiscriminately augmenting data without considering the student model\u2019s varying performance across different data points. For instance, the model may easily solve the majority of the dataset, but it may struggle with a small subset of more challenging examples. In this case, rather than indiscriminately expanding the dataset by replicating simpler cases, a better augmentation strategy would be to generate more data points that align conceptually with these challenging examples. This is because the former approach could lead to longer training time without noticeable performance improvement. Here, we propose a more general formulation of an LLM-based data augmentation pipeline that addresses the aforementioned limitation. To do so,\n# we consider the following iterative process:\nDn+1 = f(Mteacher, Mstudent, Dn, \u00b7 \u00b7 \u00b7 , D0).\nIn Equation (1), Mteacher is the teacher model, Mstudent is the student model (potentially being fine-tuned in many iterations), n refers to the nth step of data augmentation, Dn+1 is the new training dataset at the next iteration, and f is the datageneration algorithm. At each step, the teacher model has access to how the student model performs at the nth step (e.g., correct/incorrect labels, or possibly prediction distributions for white-box models), and based on that it can edit training data points for the next iteration. Note that LLM2LLM is different from knowledge distillation (Hinton et al., 2015). Knowledge distillation is generally applicable to cases where the teacher model has high accuracy on the target data. In contrast, in this case, it is possible that the teacher model also performs sub-optimally on the target data (e.g., in the private database case, where the teacher lacks domain-specific knowledge). However, if the teacher model has enough reasoning capability to produce conceptually similar but semantically different examples when it is given both the prompt and answer, then our framework can improve performance. In LLM2LLM, we consider a specific instantiation of Equation (1), as discussed next.\n# 3.1 LLM2LLM\nThe end-to-end algorithm of LLM2LLM is presented in Algorithm 1. Inspired by Self-Instruct (Wang et al., 2023), we use the teacher model Mteacher to generate synthetic data from the data points that the model got incorrect during training in order to target these deficiencies in the student model. In more detail, we first train the baseline student model Mstudent on the provided target data D0, and we evaluate its performance (lines 4-5 of Algorithm 1). We then filter the results and keep the incorrect training examples that the student model struggled to answer correctly (Ei in line 6). Then the teacher model is prompted to create additional training data points that are conceptually aligned but semantically different (line 7, see Section B.4 for specifics on the prompt). The teacher model does not necessarily need to be bigger, although that could potentially improve performance. The primary requirement for the teacher model is to have reasoning capability to be able to follow the\nDataset\n% Data\n# Seed Examples\n# Augmented\nTest Accuracy (%)\nBaseline\nLLM2LLM\nGSM8K\n0\n0\n0\n0.001\nN/A\n1\n74\n391\n0.99\n19.56\n2\n149\n802\n1.52\n25.70\n5\n373\n1641\n9.63\n27.07\n10\n747\n2573\n21.27\n30.93\n20\n1494\n4028\n25.70\n35.03\n50\n3737\n8252\n33.89\n38.67\n100\n7473\n14925\n36.01\n41.24\nCaseHOLD\n0\n0\n0\n12.28\nN/A\n0.5\n225\n490\n33.94\n66.50\n1\n450\n751\n46.25\n70.97\n2\n900\n580\n69.44\n74.97\n5\n2250\n423\n74.14\n76.83\n10\n4500\n505\n77.03\n78.21\n20\n9000\n1100\n78.00\n78.97\n50\n22500\n2709\n80.39\n82.92\n100\n45000\n5805\n87.94\n88.14\nSNIPS\n0\n0\n0\n11.86\nN/A\n0.5\n70\n38\n60.14\n92.14\n0.8\n105\n109\n69.71\n93.71\n1.0\n140\n91\n85.43\n93.86\nTREC\n0\n0\n0\n11.20\nN/A\n1.1\n60\n105\n26.20\n78.80\n1.6\n90\n22\n80.80\n90.20\n2.2\n120\n44\n81.20\n91.20\nSST-22\n0\n0\n0\n27.06\nN/A\n0.02\n20\n44\n52.87\n92.66\n0.04\n30\n46\n62.04\n93.00\n0.06\n40\n14\n82.80\n94.04\nTable 1: LLM2LLM on datasets under evaluation. The % Data and # Seed Examples columns indicate th percentage and number of data points respectively that were sampled from the original training data as seed dat The # Augmented column shows the number of data points created by LLM2LLM. The last column (Test Accurac %) shows the baseline accuracy from fine-tuning with the original seed examples (Baseline), as well as whe training with augmented data added to the dataset (LLM2LLM). Overall, test accuracy improves significantly wit LLM2LLM, especially in low data regimes.\ndata augmentation instruction, and the ability to create data points similar to the incorrect examples. This process is schematically illustrated in Figure 1. A subtle but important design decision in LLM2LLM is that we only use examples from the seed data when prompting the teacher model to generate additional data points. This is similar to Alpaca (Taori et al., 2023), but unlike EvolInstruct (Xu et al., 2023). There are two main reasons for this. First, our approach prevents data degradation from multiple augmentation iterations. Early experiments revealed that while the teacher model could generate high-quality augmentations, some examples contained logical errors. Therefore,\n1The 0% here is because our prompting does not include the formatting that we used to extract the answer with. Adding some instructions to make sure the output format is correct results in a 0.30% test accuracy, which is in line with the rest of the results in Table 1. 2SST-2 has no test set, therefore we evaluate on the dev set instead, see Section A.3.\n1The 0% here is because our prompting does not include the formatting that we used to extract the answer with. Adding some instructions to make sure the output format is correct results in a 0.30% test accuracy, which is in line with the rest of the results in Table 1. 2SST-2 has no test set, therefore we evaluate on the dev set instead, see Section A.3.\nfurther augmentation applied to these examples could potentially propagate the error, degrading the quality of the dataset over time. This is highlighted in our ablation studies in Table 4, where using both seed and synthetic data for data augmentation leads to an accuracy drop. Second, this approach limits the amount of new data being generated overall. Suppose that the original seed dataset is of size n, and at each iteration, the student model gets pi proportion of the training dataset Di wrong, where 0 < pi < 1. If we include the augmented data into the seed data for data generation, then the size of the dataset Dj at step j will be\nThis has a lower bound that grows exponentially with each step. Limiting the input wrong answers\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8cf5/8cf52645-c923-4800-a1be-c94bbdf3e2b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9453/9453617c-73ee-48a0-bffb-5603622a78f3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">500 1000 1500 2000 2500 3000 Total Data Size (Seed + LLM2LLM Data)</div>\nFigure 2: LLM2LLM on GSM8K (left) and CaseHOLD (right) with various seed data sizes. Each line shows the test accuracy of the finetuned Llama-2-7B model with each step of LLM2LLM with varying seed dataset size. The first (left-most) data point on each line represents finetuning only on the seed data. Each point afterward corresponds to the performance after one more iteration of LLM2LLM. The total data size (x-axis) represents the total amount of seed plus LLM2LLM data that was used to train the model at that step. By applying LLM2LLM with low amounts of seed data and iteratively improving the training dataset, we can attain significant performance improvements. In particular, we can see that running LLM2LLM can match or even exceed the performance of simply annotating more real data in some cases (detailed breakdown provided in Table 1).\nW i during dataset generation to only data from the original seed data allows us to bound the total number of training data points to\nwhich has an upper-bound that grows linearly with the number of steps. The empirical evaluations shown in Section 4.5.2 (Table 4) corroborates this.\n# 4 Results\n# 4.1 Experimental Setup\nTo evaluate the performance of LLM2LLM, we applied our framework to fine-tune Llama-2-7B on various target datasets, including GSM8K (Cobbe et al., 2021), CaseHOLD (Nori et al., 2023), SNIPS (Coucke et al., 2018), TREC (Li and Roth, 2002) and SST-2 (Socher et al., 2013). We subsampled these datasets with different sampling rates from 0.02% to 50% to evaluate performance across different low-data regimes. Our teacher model for these results is GPT-3.5 (1106 release) unless otherwise specified. We considered several other teacher models, including GPT-3.5, GPT-4-Turbo, Llama2-70B (Touvron et al., 2023b), and Airoboros-L270B (Durbin, 2023) in Section 4.4. We include a more detailed experimental setup in Section A. Additionally, we conducted additional experiments (Section B.6) to ensure that the augmented data from the teacher models differs from the test dataset\n<div style=\"text-align: center;\">Total Data Size (Seed + LLM2LLM Data)</div>\nused to evaluate final model accuracy. This addresses the issue of potential test data leakage that could have happened if the teacher model had been trained on similar data.\n# 4.2 Main Results\nHere, we discuss LLM2LLM\u2019s performance with varying amount of training data by presenting results for fine-tuning Llama-2-7B on GSM8K using GPT-3.5 as the teacher model. We then discuss how these trends extend to different datasets (Table 1). The final model accuracy after applying 10 iterations of LLM2LLM is given in Table 1. For a low-data regime with 74 available examples (i.e., 1% of the GSM8K training dataset), vanilla finetuning achieves only 0.99% test accuracy. However, LLM2LLM boosts the accuracy to 19.56% by iteratively generating 391 additional examples based on data points where the model makes mistakes. With slightly more available data of 149 seed examples (i.e., 2% of the training dataset) we can achieve 25.70% accuracy. As shown in the baseline accuracy with 20% data in Table 2, we would need over 10\u00d7 more training data points to match this accuracy if we only rely on vanilla fine-tuning. We also highlight that LLM2LLM can lead to noticeable gains with data-sufficient regimes (e.g., 100% data), albeit at a smaller improvement over the baseline compared to lower-data regimes. We observe a similar trend for CaseHOLD, SNIPS, TREC, and SST-2, where LLM2LLM helps improve performance in the low-data regime.\nDataset\nTechnique\n# Seed\nTotal Aug.\nAcc. (%)\nGSM8K\nFine-tuning\n100\n0\n1.59\nEDA\n500\n15.16\nAugGPT\n500\n18.12\nMore Data\n471\n19.86\nLLM2LLM\n471\n23.73\nCaseHOLD\nFine-tuning\n100\n0\n28.78\nEDA\n200\n62.19\nAugGPT\n200\n63.42\nMore Data\n198\n37.11\nLLM2LLM\n198\n64.50\nSNIPS\nFine-tuning\n70\n0\n60.14\nEDA\n70\n91.43\nAugGPT\n70\n89.86\nMore Data\n70\n89.00\nLLM2LLM\n38\n92.14\nTREC\nFine-tuning\n60\n0\n26.20\nEDA\n120\n72.40\nAugGPT\n120\n32.80\nMore Data\n138\n89.20\nLLM2LLM\n135\n78.80\nSST-2\nFine-tuning\n20\n0\n52.87\nEDA\n40\n63.19\nAugGPT\n40\n88.07\nMore Data\n40\n72.94\nLLM2LLM\n44\n92.66\nTable 2: Results of LLM2LLM compared to other baseline methods. Column Technique refers to the augmentation method used as described in Section A.3. Column # Seed indicates the size of the initial seed dataset. Column Total Aug. represents the total amount of LLM2LLM data generated. For GSM8K and CaseHOLD, we randomly sample 100 data points while for SNIPS, TREC, and SST-2, we sample 10 samples per class. Column Acc. indicates the final test accuracy. Clearly, LLM2LLM outperforms all of synthetic baselines; even sometimes when adding in more real data from the dataset.\nInterestingly, LLM2LLM generally generates proportionally more augmented data for GSM8K than other datasets. This is because the baseline accuracy is lower for GSM8K overall, suggesting that it is a more difficult dataset compared to the others. However, in all cases, we find that LLM2LLM helps recover a high-performing model. In Figure 2, we also illustrate how the baseline accuracy improves on GSM8K and CaseHOLD with each iteration of applying LLM2LLM. We can observe a rapid increase in test accuracy in the first few iterations of LLM2LLM, especially in lower-data regimes.\n# 4.3 Comparison with Other Augmentation Methods\nIn Table 2, we compare our method against other augmentation techniques, including EDA (Wei and\nZou, 2019) and AugGPT (Dai et al., 2023). We also compare against adding more data from the unseen training set. The details of all augmentation methods we used in our comparison are provided in Section A.3. On GSM8K, LLM2LLM outperforms naive fine-tuning by over 20%, EDA by over 8%, and AugGPT by over 5%. Similarly, on CaseHOLD, LLM2LLM outperforms the fine-tuning baseline by approximately 35%, EDA by 2.3%, and AugGPT by 1.1%. These improvements, particularly in comparison to AugGPT, can be attributed to LLM2LLM\u2019s capability to generate more targeted examples based on where the model struggles, as opposed to AugGPT which augments data indiscriminately. This allows for more effective and targeted use of the augmented data budget.\n# 4.4 Choice of Teacher Model\nThus far, we have illustrated LLM2LLM\u2019s performance with GPT-3.5 as the teacher model, but other LLMs can serve this role as well. A stronger teacher model is expected to yield higher-quality augmentation and, consequently, higher accuracy. Table A.1 demonstrates the LLM2LLM\u2019s accuracy with GPT-4-Turbo, Llama-2-70B, and AiroborosL2-70B as the teacher model on GSM8K. With 74 seed data examples, LLM2LLM only achieves 11.8% accuracy with Llama-2-70B, which can be contrasted with 15.0% with Airoboros and 19.8% with GPT-4-Turbo. This aligns with our expectation, as GPT-4-Turbo\u2019s mathematical reasoning is known to be better than the other models, being generally on par with that of GPT-4 (Fu et al., 2023a; Deng et al., 2023b). The qualitative analysis of augmented data using different models (Figure B.16) further supports this, showing that Llama and Airoboros models produce less varied data than GPT-3.5 or GPT-4-Turbo.\n# 4.5 Ablation Studies\nHere, we provide ablation studies to justify the design decisions we made in LLM2LLM.\n# 4.5.1 Iterative Augmentation vs One-Shot Augmentation\nWe first evaluate the efficacy of iterative augmentation versus adding all augmented data at once. To evaluate this, we compare the final accuracy achieved by augmenting data over 10 iterations against adding the equivalent amount of data in\nDataset\nSteps\nTotal Aug.\nAcc. (%)\nGSM8K\n1 (one-shot)\n490\n16.30\n10 (iterative)\n471\n23.73\nCaseHOLD\n1 (one-shot)\n276\n59.94\n10 (iterative)\n198\n64.50\nTable 3: Ablation on the iterative nature of LLM2LLM with 100 seed data points. Steps refers to the total number of augmentation steps in LLM2LLM. For the case of 1 iteration, we prompt the teacher model to generate more samples all at once, whereas in the 10 steps case the teacher model only generates 1 new data point per wrong example. The results clearly show that the latter iterative approach results in better performance.\nDataset\nOnly Aug.\nSeed Data\nTotal\nAug.\nAcc. (%)\nGSM8K\n\u2717\n4302\n18.32\n\u2713\n471\n23.75\nCaseHOLD\n\u2717\n351\n63.75\n\u2713\n198\n64.50\nTable 4: Ablation study on whether to augment previously generated LLM2LLM data. Only Aug. Seed Data refers to augmenting only the seed data vs. also re-augmenting the augmented data. Total Aug. refers to the total number of augmentations generated over 10 steps of LLM2LLM.\na single iteration, for both the GSM8K and CaseHOLD datasets. As shown in Table 3, using a single augmentation step with a larger amount of augmented data significantly underperforms the alternative of executing 10 iterative steps of LLM2LLM with a smaller number of augmentations per iteration. In particular, on GSM8K, augmenting one data point per example over 10 steps yields a 7.4% higher accuracy than augmenting five data points per example in a single step. Similarly, on CaseHOLD, iterative augmentation of one data points per example over 10 steps results in a 4.6% improvement over a one-shot augmentation with four data points per example. This justifies the LLM2LLM\u2019s iterative augmentation approach that generates one data point per each incorrectly answered example.\n# 4.5.2 Data Augmentation with Seed Data vs Augmented Data\nIn each iteration, LLM2LLM evaluates the student model\u2019s performance only on the original seed dataset and generates augmented data from incorrect seed examples. However, a possible alternative is performing evaluation and data augmentation using both seed and previously augmented data. The latter often leads to sub-optimal performance as\nDataset\nFrom-scratch\nFine-tuning\nTotal\nAug.\nAcc. (%)\nGSM8K\n\u2717\n230\n14.71\n\u2713\n471\n23.75\nCaseHOLD\n\u2717\n154\n60.50\n\u2713\n198\n64.50\nTable 5: Ablation study on whether to fine-tune from scratch or to do continuous fine-tuning. From-scratch Fine-tuning refers to whether we fine-tune the base model from scratch vs. fine-tune the previous step\u2019s model. Total Aug. refers to the total number of augmentated examples generated over 10 steps of LLM2LLM. well as excessive amounts of total augmented data points, as we demonstrate in Table 4. On GSM8K, generating augmented data from the previous iteration\u2019s augmented data yields 18.3% accuracy, while using the seed data for further augmentation improves the accuracy to 23.75%. We observe a similar trend for CaseHOLD. As discussed in Section 3.1, a potential reason for the performance drop, when using augmented data for further augmentation, has to do with a deviation from the original data distribution.\n# 4.5.3 From-scratch Fine-tuning vs Continuous Fine-tuning\nAnother key decision for LLM2LLM is whether to continue fine-tuning from the last iteration\u2019s checkpoint (i.e. continuous fine-tuning) or to restart finetuning from the pre-trained model at each iteration (i.e. from-scratch fine-tuning). Considering the non-convex nature of the optimization target and complex loss landscapes, this decision is not necessarily obvious. Nevertheless, as shown in Table 5, we observe that from-scratch fine-tuning consistently and significantly outperforms continuous fine-tuning, with up to 9% accuracy improvement. The inferior performance of continuous fine-tuning can be attributed to a potential overfitting to small seed data over multiple iterations of fine-tuning, especially in lower-data regimes where the seed data is small. This can be alleviated by restarting fine-tuning from scratch in each iteration with sufficient augmented data appended to the seed data to form the training dataset.\n# 5 Conclusion\nWe have introduced LLM2LLM, an adaptive and iterative LLM-based data augmentation framework that uses LLMs to scale up smaller fine-tuning datasets in lieu of manually generating more data. This framework substantially reduces the amount\nof real data needed, and it allows us to efficiently scale the dataset with synthetic data that can match or even exceed the effect of hand-collecting more data. The method is effective because of the iterative and targeted nature of the process, which allows us to boost the signal from data points that the LLM gets wrong. As a result, we were able to achieve a 24.2% improvement on GSM8K, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC, and 39.8% on the SST-2 dataset in the low-data regime using a Llama-2-7B student model. Future work can focus on tuning the hyperparameters of our framework as well as incorporating our approach with other LLM techniques such as prompt tuning and few-shot learning.\n# Limitations\nOur results primarily reflect improvements that occur in a low training data regime, from tens of examples to a couple thousand. However, practitioners may deal with larger datasets from time to time, in which our method may be out of scope. Furthermore, there could be other factors that help explain the disparity in performance between different teacher models. Also, we have analyzed the generated data for differences in quality, but there may be other ways to close the gap between open-source models and the GPT models as a teacher model. This warrants further investigation. Our focus primarily reflects a specific use case where there is low training data available due to difficulty in data collection such as labor or resource constraints. Exploring the effects of using synthetic data to further eke out performance when there is abundant data is a promising research direction.\n# Ethics Statement\nLLM2LLM relies on using LLMs to augment a training dataset in order to train another student LLM more efficiently. This can reduce the energy and monetary cost of experimentation and machine learning research, as it enables those with smaller datasets to achieve better performance on a domainspecific task. Of course, misuse of this method may lead to unethical data being generated, which can lead to societal harm. This is not a concern specific to this work, but to LLM research in general. Furthermore, there are still open questions about latent implicit biases and ethical issues surrounding the generated output of LLMs that the authors and prac-\n# titioners of this method are aware of and continue to consider throughout the whole process.\ntitioners of this method are aware of and continue to consider throughout the whole process.\n# Acknowledgements\nWe appreciate the valuable feedback from Andrew Aikawa. We acknowledge gracious support from Furiosa team. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer\u2019s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel One-API center of excellence, Apple, Samsung, Panasonic, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Amir Gholami was supported through funding from Samsung SAIT. Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.\n# References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. 2021. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations.\nYonatan Belinkov and Yonatan Bisk. 2017. Synthetic and natural noise both break neural machine translation. arXiv preprint arXiv:1711.02173.\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Alice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore Bluche, Alexandre Caulier, David Leroy, Cl\u00e9ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190. Claude Coulombe. 2018. Text data augmentation made simple by leveraging nlp cloud apis. arXiv preprint arXiv:1812.04718. Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, and Xiang Li. 2023. Auggpt: Leveraging chatgpt for text data augmentation. Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. 2023a. Rephrase and respond: Let large language models ask better questions for themselves. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2023b. Gpt-4 turbo v.s. gpt-4 comparison. https://github.com/da03/implicit_ chain_of_thought/tree/main/gpt4_baselines. Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. Is gpt-3 a good data annotator? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11173\u201311195. Jon Durbin. 2023. Jondurbin/airoboros-l2-70b-3.1.2 \u00b7 hugging face. Luyang Fang, Gyeong-Geon Lee, and Xiaoming Zhai. 2023. Using gpt-4 to augment unbalanced data for automatic scoring. Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968\u2013988, Online. Association for Computational Linguistics. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023a. Chain-of-thought hub: A continuous effort to measure large language models\u2019 reasoning performance.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023b. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced selftraining (rest) for language modeling. arXiv preprint arXiv:2308.08998. Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2023. Language models can teach themselves to program better. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, and Sung Ju Hwang. 2023. Knowledgeaugmented reasoning distillation for small language models in knowledge-intensive tasks. Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data augmentation using pre-trained transformer models. In Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18\u201326. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474. Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan Liu, and Diyi Yang. 2023a. Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1487\u20131505. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023b. Self-alignment with instruction back-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dbb1/dbb1cfe1-f751-43e0-98e1-bf9f695d160e.png\" style=\"width: 50%;\"></div>\nConference on Computational Linguistics. Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, and Ashwin Kalyan. 2023. Let gpt be a math tutor: Teaching math word problem solvers with customized exercise generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14384\u201314396. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381. Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. 2023a. Tinygsm achieving 80% on gsm8k with small language models. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: designing data and methods for effective instruction tuning. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651. Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. Peft: State-of-the-art parameterefficient fine-tuning methods. https://github. com/huggingface/peft. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487. Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev,\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev,\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Annual Meeting of the Association for Computational Linguistics.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.\nArchiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. 2023. Rephrase, augment, reason: Visual grounding of questions for vision-language models.\nArthur L Samuel. 2000. Some studies in machine learning using the game of checkers. IBM Journal of research and development, 44(1.2):206\u2013226.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. 2021. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.\nAvi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. 2023. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7.\nGerald Tesauro et al. 1995. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58\u201368. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Solomon Ubani, Suleyman Olcay Polat, and Rodney Nielsen. 2023. Zeroshotdataaug: Generating and augmenting training data with chatgpt. arXiv preprint arXiv:2304.14334. Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want to reduce labeling cost? gpt-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4195\u20134205. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484\u201313508, Toronto, Canada. Association for Computational Linguistics. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022a. Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109. Yufei Wang, Jiayi Zheng, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, and Daxin Jiang. 2022b. Knowda: All-in-one knowledge mixture model for data augmentation in few-shot nlp. arXiv preprint arXiv:2206.10265. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 6382\u20136388, Hong Kong, China. Association for Computational Linguistics.\nGerald Tesauro et al. 1995. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58\u201368. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Solomon Ubani, Suleyman Olcay Polat, and Rodney Nielsen. 2023. Zeroshotdataaug: Generating and augmenting training data with chatgpt. arXiv preprint arXiv:2304.14334. Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want to reduce labeling cost? gpt-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4195\u20134205. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484\u201313508, Toronto, Canada. Association for Computational Linguistics. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022a. Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109. Yufei Wang, Jiayi Zheng, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, and Daxin Jiang. 2022b. Knowda: All-in-one knowledge mixture model for data augmentation in few-shot nlp. arXiv preprint arXiv:2206.10265. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 6382\u20136388, Hong Kong, China. Association for Computational Linguistics.\nason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 6382\u20136388, Hong Kong, China. Association for Computational Linguistics.\nLai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. 2023. Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4.\nLai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. 2023. Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. 2022. Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4235\u20134252. Kang Min Yoo, Dongju Park, Jaewook Kang, SangWoo Lee, and Woomyeong Park. 2021. Gpt3mix: Leveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. arXiv preprint arXiv:2401.10020. Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. 2023. Self-taught optimizer (stop): Recursively self-improving code generation. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations.\n2023. Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. 2022. Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4235\u20134252. Kang Min Yoo, Dongju Park, Jaewook Kang, SangWoo Lee, and Woomyeong Park. 2021. Gpt3mix: Leveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. arXiv preprint arXiv:2401.10020. Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. 2023. Self-taught optimizer (stop): Recursively self-improving code generation. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations. Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. 2021. When does pretraining help? assessing self-supervised learning for law and the casehold dataset. In Proceedings of the 18th International Conference on Artificial Intelligence and Law. Association for Computing Machinery. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models.\n# A Experimental Setup\n# A.1 Datasets\nWe evaluate LLM2LLM on five different datasets that are either multiple-choice or classification tasks and were widely adopted in prior works including (Ubani et al., 2023). Our datasets are as follows:\n1. GSM8K: a grade school math work multiplechoice dataset that consists of 7.5K train problems and 1K test problems (Cobbe et al., 2021). Figure B.1 shows an example.\n2. CaseHOLD: a multiple-choice law dataset that requires one to choose the relevant holding (i.e. the court\u2019s determination) of a cited case which backs up the proceeding argument (Zheng et al., 2021). Figure B.2 shows an example.\n3. SNIPS: a 7-way classification dataset to determine the correct user intent for a voice assistant. (Coucke et al., 2018). Figure B.3 shows an example.\n4. TREC: a 6-way classification dataset where one must classify the type of text into a category e.g., abbreviation, location, or numeric value (Li and Roth, 2002). Figure B.4 shows an example.\n5. SST-2: a binary classification dataset to decide whether a sentence of positive or negative sentiment (Socher et al., 2013). Figure B.5 shows an example.\nFor each dataset, we sample between 0.02% to 50% of the total training data and use this as the seed data for each experiment. This allows us to measure how effectively LLM2LLM scales up small task-specific datasets. For consistency, we use identical samples of seed data across different experiments (e.g. 1% on GSM8K) to avoid introducing new randomness with different samples. In particular, for SNIPS, TREC, and SST-2, we always uniformly sample the same number of examples per class, similar to (Dai et al., 2023; Ubani et al., 2023). In Table 1, we sample 10, 15, and 20 examples per class to measure the efficacy of LLM2LLM in the extreme low-data regime. These three tasks are relatively simpler than GSM8K and CaseHOLD, and therefore using an extremely small amount of training data is sufficient to achieve exemplary performance.\n# A.2 Models\nFor all of our experiments, we use Llama-2-7B (Touvron et al., 2023b) as the student model. We perform minimal prompt tuning for each task, only formatting the data as necessary and not employing many few-shot examples, which can be seen in Figure B.1 and Figure B.2. Using excessive prompting would undermine the benefits of fine-tuning and would muddle the evaluation of the effectiveness of LLM2LLM. Our fine-tuning settings are described in Section A.4. For our main experiments, we use GPT-3.5 (1106 release) as the teacher model for data generation. In Section 4.4 and Table A.1, we show that our framework can be extended to different teacher models such as the more powerful GPT-4-Turbo (1106 release) model as well as open source LLMs such as Llama-2-70B-chat (Touvron et al., 2023b) and Airoboros-l2-70b-3.1.2 (Durbin, 2023).\n# A.3 Baselines and Evaluation\nTo measure the efficacy of LLM2LLM, we finetune the student model using samples of different sizes from each dataset and evaluate on the validation sets of each of these datasets. We then run 10 steps of LLM2LLM, and use the validation sets to select the best performing model. In Section 4.2, we compare these results against basic fine-tuning on just the seed data. For GSM8K and TREC, since there is no development set, we choose the best checkpoint\u2019s test set results to be representative for the overall improvement. Similarly, for SST-2, since the test set labels are not public, we use the development set results. For all other datasets, we record the test set performance of the checkpoint that performs best on the development set. For TREC, SST-2, and SNIPS, since these are simple classification tasks, we perform string matching between the generated output and the ground truth after some cleanup. For CaseHOLD, which is a multiple choice task, we extract the letter of the answer that the model generates. For GSM8K, we use a regular expression extraction based on the answer format that GSM8K provides. Specifically, we extract the number after the #### token. In Section 4.3, we only sample 100 examples from GSM8K and CaseHOLD. For SNIPS, TREC, and SST-2, we sample 10 examples per class. We\nrun an extensive ablation study against several different baselines:\n\u2022 Fine-tuning: Standard fine-tuning on the initial seed data.\n\u2022 EDA: EDA (Wei and Zou, 2019) which uses synonym replacement plus random swap, insert, and deletion to augment text with 10% probability. Note that we take care not to augment any special formatting or structural elements for each dataset.\n\u2022 AugGPT: Augment our seed data using the teacher prompts from Section 3.1 with no filtering, similar to (Dai et al., 2023; Ubani et al., 2023; Yoo et al., 2021).\n# \u2022 More Data: Randomly sampling new data from unseen train data and adding to the training data.\n\u2022 More Data: Randomly sampling new data from unseen train data and adding to the training data.\nA.4 Fine-tuning Settings\n# A.4 Fine-tuning Settings\nFollowing the example from Alpaca (Taori et al., 2023), all models are fine-tuned with a maximum sequence length of 1024 for 3 epochs with a batch size of 128 examples. We use a learning rate of 2 \u00d7 10\u22125 with 0 weight decay and a warmup ratio of 0.03 with a cosine learning rate scheduler. These models were trained using either 4 NVIDIA A100-80GB or 8 NVIDIA A6000s. We do full finetuning for simplicity and to reduce the complexity of our experiments, but in practice, one can also use some form of parameter-efficient fine-tuning method such as LoRA (Hu et al., 2021).\n# B LLM2LLM Details\n# B.1 Fine-tuning\nThe fine-tuning step trains a small student model using seed data and previously generated LLM2LLM data, if any. This LLM2LLM data is generated by a process further detailed in Section 3.1 that generates synthetic data targeted toward data points the model got wrong. We always fine-tune the original student model on the full dataset (seed data + LLM2LLM data) at each step. Our ablation study in Section 4.5.3 shows that finetuning the original baseline model from scratch on the full dataset always outperformed re-using the already-fine-tuned model. We hypothesize that this is because fine-tuned models have already seen most of the data, causing them to overfit and fail to converge to a better optimum.\n# B.2 Evaluation\nAfter fine-tuning, the model needs to be evaluated on the original (training) seed data to identify the examples that the model gets wrong. This allows the LLM2LLM framework to use those failed examples to generate targeted synthetic data for the model to train on. For example, if the model was unable to solve problems involving the Pythagorean theorem as in Figure 1, these examples will be used to generate more problems with this concept. For many datasets, this evaluation step can be extremely costly, as traditional NLP datasets can have more than thousands of data points. However, this evaluation step is cost-effective and relatively quick in the low-data regime where the seed dataset size is small, thereby not slowing down the LLM2LLM process.\n# B.3 Filtering Generated Dataset\nOnce the teacher model generates the synthetic data, we need to apply simple filtering of the output for quality insurance. Like in previous work (Wang et al., 2023; Taori et al., 2023), we use regex filters to ensure that the basic format of the output is aligned with our expectations. We also use a ROUGE (Lin, 2004) filter in order to enforce that the augmented data points are sufficiently different from previous samples. However, we use a weaker ROUGE filter of 0.95 to filter out similar instructions, rather than the score of 0.85 used in other works like Alpaca (Taori et al., 2023) and Selfinstruct (Wang et al., 2023). The reason we can do this is because unlike Alpaca and Self-Instruct, we do not require as much diversity in the generations, as we are not targeting general-purpose instructionfollowing. In fact, we would like to constrain the generated data points to the task and domain of the datasets as much as we can. Thus, we are able to use a weaker filter so that we can simply filter out exact matches during generation. In addition, since we are augmenting each sample individually, there is already enough inherent diversity in the generation process.\n# B.4 Prompting Details\nWe devised simple but thorough prompts for each task that the teacher model uses while augmenting the dataset. Previous work in open-domain dataset generation such as (Wang et al., 2023; Mukherjee et al., 2023; Xu et al., 2023) used generic system\nDataset\n# Seed\nTeacher\nTotal # Aug\nAccuracy (%)\nGSM8K\n74 (1%)\nLlama-2-70B\n333\n11.83\nAiroboros\n345\n15.01\nGPT-3.5\n391\n19.56\nGPT-4-Turbo\n388\n19.79\n149 (2%)\nLlama-2-70B\n661\n17.59\nAiroboros\n671\n19.33\nGPT-3.5\n802\n25.70\nGPT-4-Turbo\n805\n25.78\n343 (5%)\nLlama-2-70B\n1308\n19.33\nAiroboros\n1286\n21.76\nGPT-3.5\n1641\n27.07\nGPT-4-Turbo\n1739\n28.43\nTable A.1: Experiments on how the quality of teacher model affects the performance of LLM2LLM. For each of these experiments, we only change the teacher model to measure the effect of the teacher model on the final outcome.\ninstructions for generating new data points from stronger models such as GPT-3.5 and GPT-4. This was necessary as these approaches targeted improving the LLM over a wide range of different tasks. However, for LLM2LLM, we are trying to improve the LLM at domain-specific tasks. Thus, for each task, the system prompt that we give to the teacher-model differs on a per-task basis. This allows the user to inject and leverage domain-specific knowledge about the task into the dataset generation procedure, creating higher quality fine-tuning data. In practice, we also use in-context learning with few-shot prompting to bootstrap the teacher model\u2019s ability to generate relevant questions. The detailed system prompt and in-context examples for each dataset are provided below:\ninstructions for generating new data points from stronger models such as GPT-3.5 and GPT-4. This was necessary as these approaches targeted improving the LLM over a wide range of different tasks. However, for LLM2LLM, we are trying to improve the LLM at domain-specific tasks. Thus, for each task, the system prompt that we give to the teacher-model differs on a per-task basis. This allows the user to inject and leverage domain-specific knowledge about the task into the dataset generation procedure, creating higher quality fine-tuning data. In practice, we also use in-context learning with few-shot prompting to bootstrap the teacher model\u2019s ability to generate relevant questions. The detailed system prompt and in-context examples for each dataset are provided below: 1. GSM8K: System (Figure B.6) and In-Context Examples (Figure B.7) 2. CaseHOLD: System (Figure B.8) and InContext Examples (Figure B.9) 3. SNIPS: System (Figure B.10) and In-Context Examples (Figure B.11) 4. TREC: System (Figure B.12) and In-Context Examples (Figure B.13) 5. SST-2: System (Figure B.14) and In-Context Examples (Figure B.15)\n# 1. GSM8K: System (Figure B.6) and In-Context Examples (Figure B.7)\n# B.5 Training and Data Generation Costs\nIn Table B.2, we report the training and data generation costs to perform LLM2LLM. This includes the cost of generating new data from OpenAI as well as the amount of GPU hours required to train and evaluate the student models. We measured\nDataset\n% Data\nCost ($)\nTime (Hours)\nGSM8K\n1%\n0.35\n3.28\n5%\n1.48\n9.07\n10%\n3.64\n14.54\nCaseHOLD\n1%\n1.50\n6.68\n5%\n0.84\n16.87\n10%\n2.19\n31.95\nSNIPS\n0.5%\n0.02\n0.85\n0.8%\n0.05\n1.29\n1%\n0.05\n1.40\nTREC\n1.1%\n0.05\n0.67\n1.6%\n0.01\n0.44\n2.2%\n0.02\n0.61\nSST-2\n0.02%\n0.01\n0.54\n0.04%\n0.01\n0.80\n0.06%\n0.00\n0.64\nTable B.2: Training and data generation Costs of LLM2LLM. The first and second columns indicate the dataset and percentage of the training data used as initial seed data for that experiment. The third column indicates the total cost to generate the data from the GPT-3.5 teacher model. The fourth column shows the total time in hours to train and evaluate the student model. As we can see, data generation costs for LLM2LLM are relatively little compared to the cost of manually curating new data. Furthermore, fine-tuning and evaluation of the student model finishes in a reasonable time.\nthese numbers using 4xA100-80GB PCIe based NVIDIA GPUs. As we can see, generating data for LLM2LLM costs relatively little compared to the cost of collecting new data points manually. Furthermore, the process of fine-tuning the student model also finishes in a reasonable amount of time.\n# B.6 Decontamination Experiments\nWhen using an LLM to generate data, there are potential concerns of data contamination i.e., when\nDataset\nAvg.\n>66% (%)\nMax % (Count)\nGSM8K Train\n52.08\n11.46\n81.48 (1)\nGSM8K Test\n26.38\n0\n46.67 (1)\nCaseHOLD Train\n31.01\n0\n50.42 (2)\nCaseHOLD Test\n25.66\n0\n38.46 (1)\nSNIPS Train\n59.94\n34.21\n85.71 (1)\nSNIPS Test\n45.90\n7.89\n80.00 (1)\nTREC Train\n47.62\n17.04\n80.00 (4)\nTREC Test\n33.47\n6.67\n80.00 (1)\nSST-2 Train\n34.35\n0\n57.14 (2)\nSST-2 Dev\n25.46\n0\n42.86 (1)\nTable B.3: Word overlap by dataset of synthetic examples generated after 10 steps of LLM2LLM using 100 seed examples as in Section 4.3. Column Dataset indicates the dataset and train split being used to compare the synthetic data with. Column Avg. is the average overlap percentage. Column >66% is the percentage of examples with above 66% overlap, and column Max % (Count) indicates the maximum overlap percentage and the number of examples at that overlap percentage.\nthe teacher LLM has been trained on the test data and thus leaks it into the student model\u2019s training data which would artificially inflate the student model\u2019s scores. To test for this we measure the word overlap percentage between examples generated by the teacher model and examples from the train and test set for each dataset in the same manner as (Ubani et al., 2023). Word overlap is computed by first removing stop words and punctuation from each example. Then, for each example pair, we count the number of words that are common between the two, and divide by the number of words in longer example. This provides a metric for measuring how many words are similar between the two examples. For each dataset, we run LLM2LLM for 10 steps using the same amount of seed data as in Section 4.3. Then, we take all the synthetic data generated for the final step and calculate our word overlap metric per example. We then calculate word overlap summary statistics similarly to (Ubani et al., 2023) which are the percentage of examples above 66% similarity, the maximum overlap, and the number of examples at the maximum overlap. Looking at Table B.3, we can see varying levels of overlap between the synthetic data and the training data. This is to be expected, as LLM2LLM uses training data as seed data in order to generate new synthetic examples based on that training data.\nRegarding the test/dev set results, we see that for GSM8K, CaseHOLD, and SST-2, 0% of the generated examples have above 66% word overlap with the test set. This indicates that the model is not leaking test set data into the student model\u2019s training data. For SNIPS and TREC, we see that the percent of examples with above 66% overlap is still well below 10% thus indicating no large-scale overlap. For SNIPS and TREC specifically the maximum overlap percentage is 80%. Upon closer inspection this is because these two datasets have very short examples. Given the small number of such examples we don\u2019t believe this poses a high risk of data contamination. (Ubani et al., 2023) came to a similar conclusion regarding these two datasets.\n# GSM8K Example:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e87/3e879a9f-5da2-4ea1-9f7e-7f1c55cbd362.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure B.1: Formatted example from GSM8K.</div>\nCaseHOLD Example:\n# CaseHOLD Example:\nThe following context is from a judicial decision where the holding statement has been masked out as <HOLDING>.\n# The following context is from a judicial decision where the holding statement has been masked out as <HOLDING>.\nContext: from behind the bench in the robe is protected by the First Amendment, even if his use of the trappings of judicial office were notprotected by First Amendment); Halleck v. Berlinger, 427 F. Supp. 1225, 1241 (D.D.C. 1977) (applying the First Amendment in disciplinary proceeding to comments made from the bench",
    "paper_type": "method",
    "attri": {
        "background": "Pretrained large language models (LLMs) are state-of-the-art for natural language processing tasks, but fine-tuning them in low-data regimes is challenging. Traditional data augmentation methods are insufficient for specialized tasks, which necessitates a new approach to enhance small seed datasets.",
        "problem": {
            "definition": "The problem is the difficulty of fine-tuning LLMs effectively when only a small amount of task-specific data is available, particularly in specialized domains.",
            "key obstacle": "Existing methods fail to improve performance on challenging examples, as they do not consider the model's prediction accuracy on individual training data points."
        },
        "idea": {
            "intuition": "The idea stems from observing that LLMs perform well on simpler tasks but struggle with more complex examples, suggesting a need for targeted data augmentation.",
            "opinion": "LLM2LLM is proposed as a targeted iterative data augmentation strategy that utilizes a teacher LLM to generate synthetic data for fine-tuning based on incorrect predictions.",
            "innovation": "The key innovation of LLM2LLM lies in its iterative process that focuses on augmenting only the data points where the student model performed poorly, unlike traditional methods that augment indiscriminately."
        },
        "method": {
            "method name": "LLM2LLM",
            "method abbreviation": "LLM2LLM",
            "method definition": "LLM2LLM is a framework that iteratively augments training datasets by using a teacher LLM to generate synthetic data from incorrect predictions made by a student LLM.",
            "method description": "The method involves training a student model, evaluating its performance, generating additional data from a teacher model based on errors, and repeating this process.",
            "method steps": [
                "Fine-tune the student LLM on the initial dataset.",
                "Evaluate the model's performance on the training data.",
                "Extract incorrect predictions.",
                "Use the teacher LLM to generate synthetic data based on these incorrect predictions.",
                "Append the generated data to the training dataset.",
                "Repeat the process for a specified number of iterations."
            ],
            "principle": "The effectiveness of LLM2LLM is based on its ability to amplify learning from challenging examples, thereby improving the model's performance in low-data scenarios."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on datasets including GSM8K, CaseHOLD, SNIPS, TREC, and SST-2, with varying amounts of seed data sampled from 0.02% to 50%. The teacher model used was GPT-3.5.",
            "evaluation method": "The performance of LLM2LLM was measured by comparing test accuracies before and after applying the method, using metrics such as accuracy improvements across different datasets."
        },
        "conclusion": "LLM2LLM significantly enhances the performance of LLMs in low-data regimes, achieving improvements of up to 24.2% on GSM8K, 32.6% on CaseHOLD, and other datasets, demonstrating its effectiveness as a scalable solution for data-constrained tasks.",
        "discussion": {
            "advantage": "LLM2LLM allows for effective fine-tuning in low-data scenarios by generating targeted synthetic data, reducing reliance on extensive manual data collection.",
            "limitation": "The method primarily benefits low-data settings, and its effectiveness may diminish with larger datasets where traditional methods might suffice.",
            "future work": "Future research could explore hyperparameter tuning, integration with other LLM techniques, and assessment of performance in higher data regimes."
        },
        "other info": {
            "code repository": "https://github.com/SqueezeAILab/LLM2LLM",
            "acknowledgements": "The authors acknowledge support from various institutions and individuals for their contributions to the research."
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "Pretrained large language models (LLMs) are state-of-the-art for natural language processing tasks, but fine-tuning them in low-data regimes is challenging."
        },
        {
            "section number": "2.3",
            "key information": "The key innovation of LLM2LLM lies in its iterative process that focuses on augmenting only the data points where the student model performed poorly, unlike traditional methods that augment indiscriminately."
        },
        {
            "section number": "4.1",
            "key information": "LLM2LLM is a framework that iteratively augments training datasets by using a teacher LLM to generate synthetic data from incorrect predictions made by a student LLM."
        },
        {
            "section number": "5.1",
            "key information": "LLM2LLM allows for effective fine-tuning in low-data scenarios by generating targeted synthetic data, reducing reliance on extensive manual data collection."
        },
        {
            "section number": "10.1",
            "key information": "The problem is the difficulty of fine-tuning LLMs effectively when only a small amount of task-specific data is available, particularly in specialized domains."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore hyperparameter tuning, integration with other LLM techniques, and assessment of performance in higher data regimes."
        }
    ],
    "similarity_score": 0.7461863241863698,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLM2LLM_ Boosting LLMs with Novel Iterative Data Enhancement.json"
}