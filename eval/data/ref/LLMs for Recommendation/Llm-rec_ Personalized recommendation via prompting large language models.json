{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.15780",
    "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
    "abstract": "Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.",
    "bib_name": "lyu2024llmrecpersonalizedrecommendationprompting",
    "md_text": "# LLM-Rec: Personalized Recommendation via Prompting Large Language Models\nHanjia Lyu1, Song Jiang2, Hanqing Zeng3, Yinglong Xia3, Qifan Wang3, Si Zhang3, Ren Chen3, Christopher Leung3, Jiajie Tang3, Jiebo Luo1 1University of Rochester 2UCLA 3Meta AI hlyu5@ur.rochester.edu, jluo@cs.rochester.edu\nAbstract\nText-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-REC, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-REC lies in its prompting strategies, which effectively tap into the language model\u2019s comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.\narXiv:2307.15780v3\n# 1 Introduction\nText-based recommendation systems exhibit a broad spectrum of applications, spanning across diverse domains and industries. This versatility mainly stems from the capability of natural language to effectively describe nearly any type of items, encompassing not only products, movies, and books but also news articles and user-generated content, including short videos and social media posts (Pazzani and Billsus, 2007; Javed et al., 2021; Poirier et al., 2010; Bai et al., 2022; Wu et al., 2020; Oppermann et al., 2020; Chen et al., 2017; Gupta and Varma, 2017; Wang et al., 2018). Nonetheless, these text-based recommendation systems are\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/42bb/42bb1396-f7a6-4c08-ab72-ef61f10178bb.png\" style=\"width: 50%;\"></div>\nFigure 1: LLM-REC enhances original item descriptions by prompting LLMs to augment important keywords (e.g., adjectives). It applies to various domains and is not limited to datasets with rich textual content.\nfrequently challenged by the inherent limitation of incomplete or insufficient information within item descriptions, which hinders the task of accurately aligning item characteristics with user preferences (Perez et al., 2007; Dumitru et al., 2011). The incompleteness may arise from two sources: a limited comprehension of the items themselves and an insufficient understanding of the users for whom recommendations are generated. This challenge is not confined only to domains with well-defined and categorized items (e.g., movies), but also extends to domains characterized by novel, unclassified, or less categorically structured items, as observed in the case of usergenerated content. In the context of movie recommendations, a movie\u2019s description usually include the main actors, and a brief plot summary. However, this limited information may not capture crucial elements like genre, tone, cinematography style, or thematic depth, resulting in less effective recommendations. As for user-generated content, imagine a social platform where users regularly post recipes which are often accompanied with brief textual descriptions like the name of the dish and a few ingredients, but limited details regarding\npreparation time, dietary restrictions, or flavor profiles. Consider a user who follows a vegan diet and is interested in discovering new plant-based recipes. Since the user-generated content often lacks comprehensive dietary information and may not explicitly mention terms like \u201cvegan\u201d, \u201cplant-based\u201d, or \u201cvegetarian\u201d, in this scenario, the recommendation system, relying solely on the incomplete descriptions, may struggle to discern the veganfriendliness of the recipes. The recent advances in the development of large language models (LLMs) underscore their exceptional capacity to store comprehensive world knowledge (Peters et al., 2018; Goldberg, 2019; Tenney et al., 2019; Petroni et al., 2019), engage in complex reasoning (Wei et al., 2022; Zhou et al., 2022), and function as versatile task solvers (Zhao et al., 2023; Ouyang et al., 2022; Kaplan et al., 2020). In light of this advancement and recognizing the challenge posed by incomplete item descriptions, our study introduces the LLM-REC framework. This approach is designed to exploit various prompting strategies to enrich input text with the intrinsic capabilities of LLMs for personalized recommendations. By leveraging LLMs, which have been fine-tuned on extensive language datasets (Ouyang et al., 2022; Touvron et al., 2023a), our goal is to unlock their potential in generating input text that is not only contextually aware but also of high quality, thereby elevating the overall recommendation quality. Through comprehensive empirical experiments, we evaluate the effectiveness of the LLM-REC framework. We find that integrating the augmented text as the new input achieves comparable or even superior recommendation performance compared to more advanced content-based recommendation approaches that rely solely on the original item descriptions. Further in-depth analyses reveal that the devised prompting strategies prompt LLMs to generate words that represent both general and specific item characteristics. It is applicable in a diverse range of domains and is not limited to datasets with rich textual information (Figure 1). Our study provides insights into the impact of different prompting strategies on recommendation performance and sheds light on the potential of leveraging LLMs for personalized recommendation.\n# 2 Related Work\nLLM-REC closely aligns with two research directions: (1) augmentation in text-based recommenda-\ntion, and (2) LLM for recommendation. A comprehensive discussion is provided in Appendix C. Augmentation in Text-based Recommendation. Text-based recommendation systems leverage natural language processing and machine learning techniques to provide personalized recommendations to users based on textual information (Lops et al., 2019; Qiang et al., 2020). However, the performance of such systems can be compromised when dealing with incomplete or insufficient textual information. To address this limitation, several studies have suggested strategies for enhancing textual information. For instance, Li et al. (2010) proposed to extract contextual cues from online reviews, leveraging these narratives to uncover users\u2019 preferences and underlying factors influencing their choices (Sachdeva and McAuley, 2020). Other approaches infer linguistic attributes from diverse sources, including emotion, sentiment, and topic, to refine the modeling of both items and users (Sun et al., 2015; Sailunaz and Alhajj, 2019; Ramage et al., 2010; Chen et al., 2010). Furthermore, some works explore the integration of external knowledge bases to enrich the contextual understanding of items (Di Noia et al., 2012; Musto et al., 2018). In a more recent development, Bai et al. (2022) introduced an approach that employs pre-trained language models to generate additional product attributes, such as product names, to augment item contextual information. Diverging from these prior approaches, our contribution is the LLM-REC framework, which employs large language models to enhance input text, providing a versatile solution for recommendations. A more detailed discussion on the distinctions between LLM-REC and these related work can be found in Section 5. LLM for Recommendation. Due to LLMs\u2019 remarkable text generation ability, many studies have leveraged LLMs as a data augmentation tool (Dai et al., 2023a; Li et al., 2022). Liu et al. (2023a) used an LLM to produce multimodal language-image instruction-following datasets. Through a process of instruction tuning using this generated data, their proposed framework demonstrated an impressive aptitude in advancing vision and language comprehension. There have also been efforts to use LLMs to augment the input side of personalized recommendation. For instance, Chen (2023) incorporated user history behaviors, such as clicks, purchases, and ratings, into LLMs to generate user profiles. These profiles were then combined with the history interaction sequence and candidate items to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f47a/f47a4c19-3b5c-425f-9337-7d2b60bbfac6.png\" style=\"width: 50%;\"></div>\nFigure 2: LLM-REC employs four prompting strategies to augment the original item descriptions which often contain incomplete information for recommendation. The augmented text is then concatenated to form the new input for the following recommendation module. LLM-REC plays a crucial role in enabling large language models to provide relevant context and help better align with user preferences. Prompts and augmented texts are highlighted.\n<div style=\"text-align: center;\">Figure 2: LLM-REC employs four prompting strategies to augment the original item descriptions which often contain incomplete information for recommendation. The augmented text is then concatenated to form the new input for the following recommendation module. LLM-REC plays a crucial role in enabling large language models to provide relevant context and help better align with user preferences. Prompts and augmented texts are highlighted.</div>\nconstruct the final recommendation prompt. LLMs were subsequently employed to predict the likelihood of user-item interaction based on this prompt. Xi et al. (2023) introduced a method that leverages the reasoning knowledge of LLMs regarding user preferences and the factual knowledge of LLMs about items. However, our study focuses specifically on using LLMs\u2019 knowledge and reasoning ability to generate augmented input text that better captures the characteristics and nuances of items, leading to improved recommendation performance.\n# 3 LLM-Rec\nWhen composing a summary for recommendation purposes, it is customary to infuse it with specific emphases grounded in the author\u2019s comprehension of the movie. This might involve accentuating the movie\u2019s distinctive attributes that set it apart from other movies. For instance, one may opt to incorporate genre information as a crucial element for classifying the movie. However, the decision to leverage the concept of genre for enhancing the summary is predicated on the author\u2019s understanding that the genre is a meaningful construct, effectively aligning the summary with the preferences and expectations of the intended audience. This paper aims to explore the potential of large language models when prompted to generate informative item descriptions and subsequently how to\nleverage this augmented text for enhancing personalized recommendations. Figure 2 shows the diagram of LLM-REC. Specifically, our study focuses on investigating four distinct LLM prompting strategies for description enrichment, namely basic prompting, recommendation-driven prompting, engagement-guided prompting, and the combination of recommendation-driven and engagementguided prompting. The enriched text is then fed into the final recommendation module.\nleverage this augmented text for enhancing personalized recommendations. Figure 2 shows the diagram of LLM-REC. Specifically, our study focuses on investigating four distinct LLM prompting strategies for description enrichment, namely basic prompting, recommendation-driven prompting, engagement-guided prompting, and the combination of recommendation-driven and engagementguided prompting. The enriched text is then fed into the final recommendation module. Basic Prompting. The concept of basic prompting closely resembles the task of crafting a general movie summary. Within this scope, we consider three basic prompting variants and refer to them as ppara, ptag, and pinfer, respectively in the following experiments. ppara instructs LLMs to paraphrase the original item description, emphasizing the objective of maintaining the same information without introducing any additional details. Given the original content description, the prompt we use is \u201cThe description of an item is as follows \u2018{description}\u2019, paraphrase it.\u201d ptag aims to guide LLMs to summarize the content description by using tags, striving to generate a more concise overview that captures key information. The corresponding prompt is \u201cThe description of an item is as follows \u2018{description}\u2019, summarize it with tags.\u201d pinfer instructs LLMs to deduce the characteristics of the original content description and\nprovide a categorical response that operates at a broader, less detailed level of granularity. We use the following prompt in the experiments: \u201cThe description of an item is as follows \u2018{description}\u2019, what kind of emotions can it evoke?\u201d\nbroader, less detailed level of granularity. We use the following prompt in the experiments: \u201cThe description of an item is as follows \u2018{description}\u2019, what kind of emotions can it evoke?\u201d Recommendation-driven Prompting. This prompting strategy is to add a recommendationdriven instruction, into the basic prompting, resembling the task of creating a paragraph intended for making recommendations. We refer to the three recommendation-driven prompting as prec para, prec tag, and prec infer, respectively in the following experiments, aligning with their counterparts in the basic prompting strategy. prec para represents the prompt: \u201cThe description of an item is as follows \u2018{description}\u2019, what else should I say if I want to recommend it to others?\u201d The prompt for prec tag is \u201cThe description of an item is as follows \u2018{description}\u2019, what tags should I use if I want to recommend it to others?\u201d The prompt for prec infer is \u201cThe description of an item is as follows \u2018{description}\u2019, recommend it to others with a focus on the emotions it can evoke.\u201d Engagement-guided Prompting. As previously elucidated, the deficiency in item descriptions can also emanate from a limited comprehension of the user cohort for whom the recommendations are being generated. Typically, item descriptions are initially formulated for broad, general purposes, devoid of specific targeting toward particular user groups. As a result, they often fall short in capturing the intricate nuances of items required for a more fine-grained alignment with individual user preferences. The goal of the engagementguided prompting strategy is to leverage user behavior, specifically the interactions between users and items (i.e., user-item engagement) to devise prompts with the intention to steer LLMs towards a more precise comprehension of the attributes within the items, thus generating more insightful and contextually relevant descriptions that align more closely with the preferences of intended users. We refer to this variant as peng. To create the engagement-guided prompt, we combine the description of the target item, denoted as dtarget, with the descriptions of T important neighbor items, represented as d1, d2, \u00b7 \u00b7 \u00b7 , dT . The importance is measured based on user engagement. More details can be found in Appendix A.6. The exact prompt of this prompting strategy is \u201cSummarize the commonalities among the following descriptions: \u2018{description}\u2019; \u2018{descriptions\nThis\nRecommendation-driven + Engagement-guided Prompting. It intends to incorporate both the recommendation-driven and engagement-guided instructions, which we denote as prec+eng: \u201cThe description of an item is as follows: \u2018{description}\u2019. What else should I say if I want to recommend it to others? This content is considered to hold some similar attractive characteristics as the following descriptions: \u2018{descriptions of other important neighbors}\u2019.\u201d\nRecommendation-driven + Engagement-guided Prompting. It intends to incorporate both the recommendation-driven and engagement-guided instructions, which we denote as prec+eng: \u201cThe description of an item is as follows: \u2018{description}\u2019. What else should I say if I want to recommend it to others? This content is considered to hold some similar attractive characteristics as the following descriptions: \u2018{descriptions of other important neighbors}\u2019.\u201d How does LLM-REC affect personalized recommendation? In our experiments, we discover that first and foremost, LLM-REC stands out as a versatile yet simple framework, largely unrestricted by the type of items. Our experimental results on two datasets including the items that are categorically structured and extensively studied to items that are relatively novel and unclassified such as user-generated content, consistently demonstrate the substantial improvement in personalized recommendations. Simple models, such as MLP, can achieve performance on par with, or even better than, more advanced and complex models with the augmented text. This finding underscores the potential of simplified training to address challenges due to more complex models. More importantly, compared to other knowledge-based text augmentation methods, LLM-REC achieves superior recommendation performances and requires considerably less domain expertise compared to prior studies, making it much more accessible for implementation. Second, LLM-REC contributes to increased recommendation transparency and explainability. The ability to directly investigate the augmented text not only enhances our understanding of the recommendation models but also offers insights into the characteristics of the items. It is invaluable for both users and system designers seeking to comprehend the rationale behind recommendations.\n# 4 Experiments\n# 4.1 Experiment Setup\nDatasets and Baslines. Two widely adopted recommendation benchmarks are used, Movielens1M (Harper and Konstan, 2015) for movie recommendation, and Recipe (Majumder et al., 2019) for recipe recommendation. To assess LLM-REC\u2019s efficacy, we compare it against two distinct categories of baselines. The first category includes contentbased baselines that takes solely the original item\nMovielens-1M\nRecipe\nPrecision@10\nRecall@10\nNDCG@10\nPrecision@10\nRecall@10\nNDCG@10\nItem Popularity\n0.0426 \u00b10.0019\n0.0428 \u00b10.0028\n0.0530 \u00b10.0035\n0.0116 \u00b10.0025\n0.0274 \u00b10.0083\n0.0201 \u00b10.0053\nMLP\n0.2922 \u00b10.0019\n0.2455 \u00b10.0031\n0.3640 \u00b10.0039\n0.0325 \u00b10.0021\n0.0684 \u00b10.0066\n0.0580 \u00b10.0054\nAutoInt (Song et al., 2019)\n0.2149 \u00b10.0078\n0.1706 \u00b10.0075\n0.2698 \u00b10.0092\n0.0351 \u00b10.0032\n0.0772 \u00b10.0102\n0.0658 \u00b10.0089\nDCN-V2 (Wang et al., 2021)\n0.2961 \u00b10.0050\n0.2433 \u00b10.0057\n0.3689 \u00b10.0033\n0.0360 \u00b10.0036\n0.0786 \u00b10.0104\n0.0653 \u00b10.0085\nEDCN (Chen et al., 2021)\n0.2935 \u00b10.0036\n0.2392 \u00b10.0051\n0.3678 \u00b10.0053\n0.0354 \u00b10.0030\n0.0772 \u00b10.0091\n0.0652 \u00b10.0071\nTagGPT (Li et al., 2023a)\nLLAMA-2-7B\n0.2991 \u00b10.0017\n0.2556 \u00b10.0038\n0.3723 \u00b10.0023\n0.0353 \u00b10.0024\n0.0751 \u00b10.0067\n0.0641 \u00b10.0057\nGPT-3\n0.3001 \u00b10.0027\n0.2569 \u00b10.0028\n0.3747 \u00b10.0042\n0.0356 \u00b10.0032\n0.0752 \u00b10.0084\n0.0637 \u00b10.0068\nKAR (Xi et al., 2023)\n0.3056 \u00b10.0026\n0.2623 \u00b10.0034\n0.3824 \u00b10.0042\n0.0298 \u00b10.0018\n0.0611 \u00b10.0049\n0.0525 \u00b10.0043\n- augmented with ground truth\n0.3075 \u00b10.0015\n0.2636 \u00b10.0035\n0.3853 \u00b10.0027\n-\n-\n-\nLLM-REC\nLLAMA-2-7B\n0.3102 \u00b10.0014\n0.2712 \u00b10.0026\n0.3867 \u00b10.0027\n0.0359 \u00b10.0024\n0.0770 \u00b10.0076\n0.0632 \u00b10.0052\n(+6.16%)\n(+10.47%)\n(+6.24%)\n(+10.46%)\n(+12.57%)\n(+8.97%)\nGPT-3\n0.3150 \u00b10.0023\n0.2766 \u00b10.0030\n0.3951 \u00b10.0035\n0.0394 \u00b10.0033\n0.0842 \u00b10.0098\n0.0706 \u00b10.0084\n(+7.80%)\n(+12.67%)\n(+8.54%)\n(+21.23%)\n(+23.10%)\n(+21.72%)\nTable 1: Average recommendation performance between LLM-REC and baseline approaches across five different train/test splits. The best results are highlighted in bold, the second-best results are underlined, and relative gains compared to the MLP baseline are indicated in green.\ndescriptions as input. The second category includes different text augmentation methods. Details including dataset statistics, preprocessing specifics, baselines, model training, hyper-parameter settings and implementation are discussed in Appendix A. Language Models. Two large language models are selected for experiments. The first is GPT3 (Brown et al., 2020), particularly its variant text-davinci-003. This variant is an advancement over the InstructGPT models (Ouyang et al., 2022). We select this variant due to its ability to consistently generate high-quality writing, effectively handle complex instructions, and demonstrate enhanced proficiency in generating longer form content (Raf, 2023). The second is LLAMA2 (Touvron et al., 2023b), which is an open-sourced model that has shown superior performance across various external benchmarks in reasoning, coding, proficiency, and knowledge tests. Specifically, we use the LLAMA-2-CHAT variant of 7B parameters. Evaluation Protocols. We follow the same evaluation methodology of Wei et al. (2019). We randomly divide the dataset into training, validation, and test sets using an 8:1:1 ratio. Negative training samples are created by pairing users and items without any recorded interactions (note that these are pseudo-negative samples). For the validation and test sets, we pair each observed user-item interaction with n items that the user has not previously interacted with. Here we follow the methodology outlined in the previous study (Wei et al., 2019) and set n to 1, 000. It is important to note that there is no overlap between the negative samples in the training set and the unobserved user-item pairs in the validation and test sets. This ensures the inde-\npendence of the evaluation data. We use metrics such as Precision@K, Recall@K and NDCG@K to evaluate the performance of top-K recommendations, where K is set to 10. We report the average scores across five different splits of the testing sets. The recommendation module of LLM-REC is the combination of an MLP model and a dot product.\n# 4.2 Main Results\nIntegrating the text augmented by LLM-REC boosts recommendation performance. Table 1 shows the average recommendation performance between LLM-REC and baseline approaches across five different splits. In addition to the aforementioned baselines, we include another baseline relies only on item popularity and does not involve any learning process, referred to as Item Popularity. Note that LLM-REC uses the same recommendation module as the MLP baseline. It can be seen that LLM-REC exhibits significant relative gains over the MLP method. For instance, LLM-REC achieves improvements in NDCG@10 from 6.24% to 8.54% in Movielens-1M and from 8.97% to 21.72% in Recipe. These improvements highlight the value of the augmented text in the recommendation process. More importantly, when comparing LLM-REC with the other three contentbased methods, we observe that LLM-REC empowers simple MLP models to attain comparable or even superior performance, surpassing other more complex feature-based recommendation methods. What extra information does LLM-REC incorporate that contributes to its performance improvement? To better understand why LLM-REC is effective, particularly regarding the additional\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4b0c/4b0c82b1-3610-4140-94a6-c81fec332e1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Example responses generated by GPT-3. The additional information augmented via the recommendatio driven prompting is highlighted in blue. Examples generated by LLAMA-2 can be found in Table 13.</div>\ninformation contributed by its prompting approach, we conduct both qualitative and quantitative studies. We find that the augmented content contains more detailed and expressive descriptions, emphasizing item characteristics which helps in understanding items more comprehensively than with their original descriptions and contributing to the performance improvement. Figure 3 shows example responses generated by GPT-3 with ppara and prec para. The first example suggests that the response via prec para categorizes the movie as a psychological thriller and recommends it as a must-watch for fans of this genre. It also positions the movie as both exciting and thought-provoking, appealing to those looking for more than just entertainment. These distinctive words describe user preferences and item characteristics including genre description, descriptive elements, and viewer recommendation. While Figure 3 might suggest that the LLM-generated text for the Recipe dataset adds only modifiers, these phrases, like \u201ceasy to make,\u201d actually reflect key characteristics valued in the Recipe dataset, such as simplicity. Some authors may also add #easytomake to their recipe descriptions (Majumder et al., 2019). Consistent patterns are also observed when comparing the responses of ptag with prec tag (Tables 11 and 14), and pinfer with prec infer (Tables 12 and 15). A more thorough analysis shows that LLM-REC can be applied to diverse\nitem domains and it is not restricted to datasets with rich textual information. Please see Appendix B.2. We hypothesize that these generated words contribute to improving recommendation performance. To further validate this hypothesis, we design two variants of the response generated by GPT3, namely pmask para and pkeyword para . To construct pmask para , we mask the words that appear in the response of prec para but are absent in the response of ppara. To construct pkeyword para , we append the words that (1) appear in the response of prec para and (2) are predefined user-preference-related words such as genres to the end of the response of ppara (more details in Appendix A.9). These two variants are then fed into MLP models to form baselines. Comparing the performance of prec para and pmask para in Table 2, we observe a discernible decline in recommendation performance when words unique to the response of prec para are selectively masked. This outcome highlights the pivotal role played by the supplementary insights introduced through the augmented text. Further, our investigation reveals that the incorporation of vital keywords, as opposed to the inclusion of all response words, can yield even superior recommendation performance. This may be attributed to potential discrepancies or extraneous elements within the response of prec para.\nLLM-REC augmentation outperforms other text augmentation methods for recommenda-\nMovielens-1M\nRecipe\nppara\n0.3746 \u00b10.0028\n0.0611 \u00b10.0053\npkeyword\npara\n0.3822 \u00b10.0049\n(+2.03%)\n0.0615 \u00b10.0060\n(+0.65%)\nprec\npara\n0.3777 \u00b10.0028\n0.0646 \u00b10.0072\npmask\npara\n0.3769 \u00b10.0040\n(-0.21%)\n0.0611 \u00b10.0066\n(-0.52%)\nTable 2: Average NDCG@10 across five splits.\ntion. We compare LLM-REC with two recent advancements in the field of using LLMs to augment item information, specifically Knowledge Augmented Recommendation (KAR) as proposed by Xi et al. (2023), and TagGPT, as proposed by Li et al. (2023a). KAR introduces a fusion of domain knowledge and prompt engineering to generate factual knowledge pertaining to the items (for detailed implementation information, see Appendix A.7). Since the augmented information may not necessarily be correct, we further implement a variant with ground truth knowledge. It aligns with strategies akin to those introduced by Di Noia et al. (2012), who harnessed external databases to enhance item information. In a manner consistent with this approach, we incorporate genre information into the item descriptions. Note that genre constitutes one of the metadata components in Movielens-1M. Such categorical characteristics are absent in Recipe. Therefore, we only apply this variant to the Movielens-1M dataset. As shown in Table 1, the incorporation of knowledge-based text augmentation offers significant improvements in recommendation performance for well-classified items, such as movies. However, it becomes evident that this approach faces limitations when applied to items, like usergenerated content, that are inherently more novel and dynamic in nature. The example response in Figure 4 shows that one key reason that knowledge augmentation approaches do not yield optimal improvement regarding recommendation performance may lie in the potential mismatch between the generated knowledge and the target item. For instance, while the generated ingredient information may be correct for most meatloaf recipes, it could be entirely wrong for a specific recipe without additional context. In contrast to these knowledge augmentation methods, LLM-REC\u2019s recommendationdriven prompts provide augmented information that describes the target item at a broader, less granular\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ddf0/ddf03cac-4b58-4c78-999f-0ff4363ebe4b.png\" style=\"width: 50%;\"></div>\nFigure 4: An example response generated using knowledge augmentation prompts (Xi et al., 2023). The additional information is highlighted in red.\nlevel, especially when compared to KAR. More importantly, LLM-REC does not require domain knowledge throughout the entire process. In terms of the second text augmentation baseline, TagGPT (Li et al., 2023a), which extracts tags using LLMs, several key observations can be made. First, we note an improvement in recommendation performance using tag generation compared to the baseline methods. Second, the prompts specifically designed within our LLM-REC framework demonstrate superior effectiveness compared to those used in TagGPT.\n# 4.3 Ablation Study\nHow does each prompting strategy perform? We conduct an ablation study to examine the impact on recommendation performance when models use either original item descriptions alone or a combination of these descriptions with augmented text derived from one of four distinct prompting strategies. The results, presented in Table 3, reveal a noteworthy and consistent enhancement in recommendation performance across various prompting strategies within two benchmark datasets. We also note variations in the performance of these strategies across different domains, aligning with our expectations. In Movielens-1M, the strategy combining recommendation-driven and engagement-guided approaches yields the best results. Conversely, in Recipe, the recommendationdriven strategy alone proves most effective. This variability suggests that combining multiple objectives in a single prompting strategy does not\nMovielens-1M\nRecipe\nItem\nDescription\n0.3640 \u00b10.0039\n0.0580 \u00b10.0054\nBasic\n0.3747 \u00b10.0042\n(+2.94%)\n0.0644 \u00b10.0068\n(+11.03%)\nRec\n0.3786 \u00b10.0041\n(+4.01%)\n0.0649 \u00b10.0069\n(+11.90%)\nEng\n0.3801 \u00b10.0032\n(+4.42%)\n0.0628 \u00b10.0077\n(+8.28%)\nRec+Eng\n0.3802 \u00b10.0037\n(+4.45%)\n0.0625 \u00b10.0060\n(+7.76%)\nTable 3: Average NDCG@10 across five different splits among different prompting strategies.\nalways lead to superior performance. When LLMs are tasked with generating descriptions serving multiple purposes, the balance of information becomes crucial. If neighboring item descriptions vary widely, it can challenge the model\u2019s ability to generate useful content, potentially leading to less optimal improvements. To address this, LLM-REC integrates all enriched text and leverages the subsequent recommendation module to effectively model the extra information. Future work can investigate different prompt designs, aiming to effectively achieve multiple objectives simultaneously. How does concatenating the augmented responses affect recommendation? In Table 1, we show that the MLP model, which combines all augmented text with the original description, outperforms more advanced models that rely solely on the original description as input. Now we take a deeper look at the quality of the combined augmented text. We employ the same recommendation module (i.e., an MLP with a dot product) and evaluate the recommendation performance of various concatenation combinations. As shown in Figure 5, the model denoted as Basic uses the embeddings of text augmented through ppara. Concat-Basic represents the model that concatenates the embeddings of the input text augmented by all basic prompts. Additionally, Concat-Rec is the model that employs the concatenation of the embeddings of input text augmented by all recommendation-driven prompts. Lastly, Concat-All stands for the model that combines the embeddings of input text augmented by all four prompting strategies. Our findings reveal that concatenating more information consistently enhances recommendation performance. This emphasizes the added value of incorporating augmented text as opposed to relying solely on the original content description. Complete results of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9e77/9e770e8a-9d8f-4504-9b9e-7cd6a592822f.png\" style=\"width: 50%;\"></div>\nFigure 5: The ablation study shows that recommendation benefits from concatenating the embeddings of the input text augmented by LLM.\nMovielens-1M\nRecipe\nItem Description\n0.3640 \u00b10.0039\n0.0580 \u00b10.0054\nDuplication\n0.3567 \u00b10.0043\n0.0590 \u00b10.0068\nText Concatenation\n0.3853 \u00b10.0027\n0.0591 \u00b10.0065\nConcat-All (ours)\n0.3951 \u00b10.0035\n0.0706 \u00b10.0084\nTable 4: Average NDCG@10 across five splits among different methods of extra information integration.\nTable 4: Average NDCG@10 across five splits among different methods of extra information integration.\nFigure 5 can be found in Figure 8. How to effectively integrate the augmented responses to maximize improvement? Table 4 shows the recommendation performances of other concatenation variants: (1) Duplication: We duplicate the embeddings of the original item description to match the dimension size of the embeddings of Concat-All; (2) Text concatenation: Instead of concatenating the embeddings of all response (i.e., Concat-All), we concatenate the responses first, and then convert it to embeddings. Through a comparative analysis of the model\u2019s performance, contrasting the first variant with Concat-All, it becomes evident that the observed improvement in performance is not attributable to changes in embedding size. Further, by comparing the performance of the second variant against Concat-All, we discover that in scenarios where the text encoder remains unmodified, the most effective strategy to integrate all enriched information is by first converting the text into embeddings and then concatenating these embeddings. This approach surpasses the method of concatenating text prior to its conversion into embeddings. Future research can explore the potential of modifying the text encoder to further enhance model efficiency and effectiveness. Does modifying the word choice in the designed prompts significantly affect the augmented output? To investigate this, we construct one variant prompt for each of LLM-REC\u2019s prompts, ensuring\nPrompting Strategy\nVariant\nBasic\nppara\n0.8859 \u00b10.0898\nptag\n0.9112 \u00b10.1000\npinfer\n0.6819 \u00b10.1500\nRec\nprec\npara\n0.7011 \u00b10.1369\nprec\ntag\n0.8627 \u00b10.1248\nprec\ninfer\n0.8458 \u00b10.0652\nEng\npeng\n0.6218 \u00b10.1012\nRec+Eng\nprec+eng\n0.8542 \u00b10.0802\nTable 5: Average cosine similarity between the variant prompt responses and those generated from LLM-REC.\nthey convey the same meaning but with different word choices. As shown in Table 5, despite variations in wording of the prompts, the responses remain largely similar. The observed lower cosine similarity is primarily attributed to differences in the format of the responses which can be mitigated through various strategies, such as additional fine-tuning of the model or incorporating specific instructions within the prompts.\n# 5 Discussions and Conclusions\nIn this study, we have investigated the effectiveness of LLM-REC as a simple yet impactful mechanism for improving recommendation through LLMs. Our approach is among those early attempts (Lin et al., 2023; Chen et al., 2023) that leverage LLMs for text augmentation in recommendation. There are three key contributions that distinguish our work from the concurrent ones. First, while previous work, such as KAR (Xi et al., 2023), focuses on design augmentation algorithm for a specific recommendation model, our model focuses on input text augmentation with LLMs, which is suitable for any content-based backbone recommendation models, demonstrating the flexibility of our approach. Second, in addition to our recommendation-driven augmentation using LLMs, we also design engagement-guided prompts to augment the input, which contains more personalized item characteristics. Overall, we conduct comprehensive experiments, with different combinations of prompting strategies, to not only illustrate the superior performance of our approach but also uncover the underlying rationale of the improvements. We introduce LLM-REC, which enhances recommendation by augmenting the original item descriptions which often contains incomplete information for effective recommendations using large\nlanguage models. We observed from extensive experiments that combining augmented input text and original item descriptions yields notable improvements in recommendation quality. These findings show the potential of using LLMs and strategic prompting techniques to enhance the accuracy and relevance of recommendation with an easier training process. By incorporating additional context, we enable the recommendation algorithms to capture more nuanced information and generate recommendations that better align with user preferences.\n# Limitations\nIn this study, we have investigated the effectiveness of LLM-REC as a simple yet effective mechanism for improving recommendation through large language models. While effective, LLM-REC does have its limitations. First, there is extra computational cost associated with LLM-REC framework. The primary computational load comes from the augmentation phase including the augmented output text length. Our findings indicate that selecting important words for inclusion, rather than incorporating all response words, can lead to improved recommendation performance, as evidenced in Table 2. Our future work will explore the balance between the number of words generated and the resulting performance enhancements. Second, similar to many LLM-based studies, LLM-REC faces challenges in promptly incorporating the latest knowledge. We plan to investigate methods in future work for LLMs to autonomously gather and summarize current knowledge from external sources, thereby improving text augmentation effectiveness.\n# Acknowledgments\nLuo was supported in part by NSF Award #2238208.\n# References\nXiao Bai, Lei Duan, Richard Tang, Gaurav Batra, and Ritesh Agrawal. 2022. Improving text-based similar product recommendation for dynamic product advertising at yahoo. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 2883\u20132892.\nChristian Bizer, Jens Lehmann, Georgi Kobilarov, S\u00f6ren Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. 2009. Dbpedia-a crystallization point for the web of data. Journal of web semantics, 7(3):154\u2013165.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250. Sergey Brin. 1998. The pagerank citation ranking: bringing order to the web. Proceedings of ASIS, 1998, 98:161\u2013172. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Bo Chen, Yichao Wang, Zhirong Liu, Ruiming Tang, Wei Guo, Hongkun Zheng, Weiwei Yao, Muyu Zhang, and Xiuqiang He. 2021. Enhancing explicit and implicit feature interactions via information sharing for parallel deep ctr models. In Proceedings of the 30th ACM international conference on information & knowledge management, pages 3757\u20133766. Jilin Chen, Rowan Nairn, Les Nelson, Michael Bernstein, and Ed Chi. 2010. Short and tweet: experiments on recommending content from information streams. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 1185\u2013 1194. Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023. When large language models meet personalization: Perspectives of challenges and opportunities. arXiv preprint arXiv:2307.16376. Ting Chen, Liangjie Hong, Yue Shi, and Yizhou Sun. 2017. Joint text embedding for personalized content-based recommendation. arXiv preprint arXiv:1706.01084. Zheng Chen. 2023. Palr: Personalization aware llms for recommendation. arXiv preprint arXiv:2305.07622. Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, et al. 2023a. Auggpt: Leveraging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023b. Uncovering chatgpt\u2019s capabilities in recommender systems. arXiv preprint arXiv:2305.02182. Tommaso Di Noia, Roberto Mirizzi, Vito Claudio Ostuni, Davide Romito, and Markus Zanker. 2012. Linked open data to support content-based recommender systems. In Proceedings of the 8th international conference on semantic systems, pages 1\u20138.\n# Sergey Brin. 1998. The pagerank citation ranking: bringing order to the web. Proceedings of ASIS, 1998, 98:161\u2013172.\nSergey Brin. 1998. The pagerank citation ranking: bringing order to the web. Proceedings of ASIS, 1998, 98:161\u2013172.\nTommaso Di Noia, Roberto Mirizzi, Vito Claudio Ostuni, Davide Romito, and Markus Zanker. 2012. Linked open data to support content-based recommender systems. In Proceedings of the 8th international conference on semantic systems, pages 1\u20138.\nTommaso Di Noia, Roberto Mirizzi, Vito Claudio Ostuni, Davide Romito, and Markus Zanker. 2012. Linked open data to support content-based recommender systems. In Proceedings of the 8th international conference on semantic systems, pages 1\u20138.\nHoratiu Dumitru, Marek Gibiec, Negar Hariri, Jane Cleland-Huang, Bamshad Mobasher, Carlos CastroHerrera, and Mehdi Mirakhorli. 2011. On-demand feature recommendations derived from mining public product descriptions. In Proceedings of the 33rd international conference on software engineering, pages 181\u2013190. Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chatrec: Towards interactive and explainable llmsaugmented recommender system. arXiv preprint arXiv:2303.14524. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems, pages 299\u2013315. Yoav Goldberg. 2019. Assessing bert\u2019s syntactic abilities. arXiv preprint arXiv:1901.05287. Shashank Gupta and Vasudeva Varma. 2017. Scientific article recommendation by using distributed representations of text and graph. In Proceedings of the 26th international conference on world wide web companion, pages 1267\u20131268. F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1\u2013 19. Oktie Hassanzadeh and Mariano P Consens. 2009. Linked movie data base. In LDOW. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web, pages 173\u2013182. Umair Javed, Kamran Shaukat, Ibrahim A Hameed, Farhat Iqbal, Talha Mahboob Alam, and Suhuai Luo. 2021. A review of content-based and contextbased recommendation systems. International Journal of Emerging Technologies in Learning (iJET), 16(3):274\u2013306. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, and Ying Shan. 2023a. Taggpt: Large language models are zero-shot multimodal taggers. arXiv preprint arXiv:2304.03022. Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. 2022. Elevater: A benchmark and toolkit for evaluating languageaugmented visual models. Advances in Neural Information Processing Systems, 35:9287\u20139301.\nRuyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan. 2023b. Exploring the upper limits of text-based collaborative filtering using large language models: Discoveries and insights. arXiv preprint arXiv:2305.11700. Yize Li, Jiazhong Nie, Yi Zhang, Bingqing Wang, Baoshi Yan, and Fuliang Weng. 2010. Contextual recommendation based on text mining. In Coling 2010: Posters, pages 692\u2013700. Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023. How can recommender systems benefit from large language models: A survey. arXiv preprint arXiv:2306.05817. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023b. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149. Peter Lofgren. 2015. Efficient algorithms for personalized pagerank. Stanford University. Pasquale Lops, Dietmar Jannach, Cataldo Musto, Toine Bogers, and Marijn Koolen. 2019. Trends in contentbased recommendation: Preface to the special issue on recommender systems based on rich item descriptions. User Modeling and User-Adapted Interaction, 29:239\u2013249. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. 2019. Generating personalized recipes from historical user preferences. arXiv preprint arXiv:1909.00105. Cataldo Musto, Pierpaolo Basile, Pasquale Lops, Marco de Gemmis, and Giovanni Semeraro. 2017. Introducing linked open data in graph-based recommender systems. Information Processing & Management, 53(2):405\u2013435. Cataldo Musto, Pasquale Lops, Marco de Gemmis, and Giovanni Semeraro. 2018. Semantics-aware recommender systems exploiting linked open data and graph-based features. In Companion Proceedings of the The Web Conference 2018, pages 457\u2013460. Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLPIJCNLP), pages 188\u2013197. Michael Oppermann, Robert Kincaid, and Tamara Munzner. 2020. Vizcommender: Computing text-based similarity in visualization repositories for contentbased recommendations. IEEE Transactions on Visualization and Computer Graphics, 27(2):495\u2013505.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744. Michael J Pazzani and Daniel Billsus. 2007. Contentbased recommendation systems. In The adaptive web: methods and strategies of web personalization, pages 325\u2013341. Springer. Luis G Perez, Manuel Barranco, and Luis Martinez. 2007. Building user profiles for recommender systems from incomplete preference relations. In 2007 IEEE International Fuzzy Systems Conference, pages 1\u20136. IEEE. Matthew E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018. Dissecting contextual word embeddings: Architecture and representation. arXiv preprint arXiv:1808.08949. Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066. Damien Poirier, Isabelle Tellier, Fran\u00e7oise Fessant, and Julien Schluth. 2010. Towards text-based recommendations. In RIAO 2010: 9th international conference on Adaptivity, Personalization and Fusion of Heterogeneous Information, pages 0\u20130. Jipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan, and Xindong Wu. 2020. Short text topic modeling techniques, applications, and performance: a survey. IEEE Transactions on Knowledge and Data Engineering, 34(3):1427\u20131445. Raf. 2023. How do text-davinci-002 and text-davinci003 differ? OpenAI. Daniel Ramage, Susan Dumais, and Dan Liebling. 2010. Characterizing microblogs with topic models. In Proceedings of the International AAAI Conference on Web and Social Media, volume 4, pages 130\u2013137. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. Bpr: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618. Noveen Sachdeva and Julian McAuley. 2020. How useful are reviews for recommendation? a critical review and potential improvements. In proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, pages 1845\u20131848. Kashfia Sailunaz and Reda Alhajj. 2019. Emotion and sentiment analysis from twitter text. Journal of Computational Science, 36:101003.\nWeiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1161\u20131170. Jianshan Sun, Gang Wang, Xusen Cheng, and Yelin Fu. 2015. Mining affective text to improve social media item recommendation. Information Processing & Management, 51(4):444\u2013457. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153. Nan Wang, Hongning Wang, Yiling Jia, and Yue Yin. 2018. Explainable recommendation via multi-task learning in opinionated text data. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 165\u2013174. Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD\u201917, pages 1\u20137. Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021, pages 1785\u2013 1797. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837. Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia, pages 1437\u20131445.\nFangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. 2020. MIND: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3597\u20133606, Online. Association for Computational Linguistics. Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards open-world recommendation with knowledge augmentation from large language models. arXiv preprint arXiv:2306.10933. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for webscale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD \u201918, page 974\u2013983, New York, NY, USA. Association for Computing Machinery. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223. Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.\n# A Additional Details of Experiment Setup\nA Additional Details of Experiment Setup\n# A.1 Datasets\nDataset\n# Interaction\n# Item\n# User\nMovieLens-1M\n1,000,209\n3,706\n6,040\nRecipe\n132,246\n4,125\n2,589\n<div style=\"text-align: center;\">Table 6: Benchmark Statistics.</div>\nTable 6: Benchmark Statistics.\nMovieLens-1M (Harper and Konstan, 2015) is a highly recognized benchmark dataset commonly used for evaluating item recommendation systems.1 It contains a vast collection of 1,000,209 ratings provided by 6,040 MovieLens users, covering 3,900 movies. Each user has at least 20 ratings. Following He et al. (2017), we convert the rating data into implicit feedback. More specifically, each entry is marked as 0 or 1 indicating whether the user has rated the corresponding item. The original movie data only contain movie titles and genres. We employ GPT-3 (text-davinci-003) to generate the content description of each movie using the following prompt: \u201cSummarize the movie {title} with one sentence. The answer cannot include the movie title.\u201d The response from GPT-3 is used as the item description. Temperature is set at 0 to generate more focused and deterministic responses. Note that inclusion of the movie title is entirely optional. We opt not to include the title intentionally, as our design for LLM-REC emphasizes its role as a general prompting framework. This versatility is important, as it is intended to function across a wide array of item types, including those that may not possess pre-defined titles, such as short videos. Recipe (Majumder et al., 2019) is another benchmark dataset we use to assess the recommendation performance. This dataset consists of recipe details and reviews sourced from Food.com.2 The metadata includes ratings, reviews, recipe names, descriptions, ingredients, directions, and so on. For instance, an example recipe description is \u201cAll the delicious flavors of mac n\u2019 cheese transformed into a warm, comforting bowl of soup!\u201d. In our evaluation, we employ the recipe descriptions as item descriptions for the four prompting strategies. Similar to the MovieLens-1M dataset, we apply filtering criteria, excluding users with fewer than 20 ratings and items with fewer than 30 ratings. Note that\n1License: https://files.grouplens.org/datasets /movielens/ml-1m-README.txt 2License: https://www.kaggle.com/datasets/shuy angli94/food-com-recipes-and-user-interactions\n1License: https://files.grouplens.org/datasets /movielens/ml-1m-README.txt 2License: https://www.kaggle.com/datasets/shuy angli94/food-com-recipes-and-user-interactions\nall original descriptions presented as examples in this paper have been paraphrased to protect user privacy. The selection of these benchmarks is mainly motivated by two factors. First, we select movies and recipes as they represent two distinct types of content. Movies, being more categorically organized and widely researched, contrast sharply with recipes, which are diverse, user-generated content from social media platforms, often lacking a strict categorical structure and presenting more novelty. Second, the nature of their descriptions differs significantly: movie descriptions typically comprise narrative summaries, whereas recipe descriptions are instructional. Evaluating our model on these varied datasets allows for a comprehensive analysis of how different prompting strategies affect recommendation outcomes, providing valuable insights into their effectiveness across diverse content types.\n# A.2 Baselines\nTo assess LLM-REC\u2019s efficacy, we compare it against two distinct categories of baselines. The first category includes baselines that takes solely the original item descriptions as input. This includes models from MLP to more complex contentbased approaches. Specifically, we choose three more advanced, content-based recommendation models. AutoInt is a multi-head self-attentive neural network with residual connections designed to explicitly model feature interactions within a lowdimensional space (Song et al., 2019). DCN-V2 represents an enhanced version of DCN and incorporates feature crossing at each layer (Wang et al., 2021, 2017). Lastly, EDCN (Chen et al., 2021) introduces a bridge module and a regulation module to collaboratively capture layer-wise interactive signals and learn discriminative feature distributions for each hidden layer in parallel networks, such as DCN. The purpose of this comparison is to evaluate the added value of augmented text in improving recommendation outcomes. The second category includes different text augmentation methodologies. Here, LLM-REC is evaluated against two recent advancements in the field of using LLMs to augment item information. The first method is Knowledge Augmented Recommendation (KAR) as proposed by Xi et al. (2023). KAR introduces a fusion of domain knowledge and prompt engineering to generate factual knowledge pertaining to the items (for implementation details, see Appendix A.8) In contrast to KAR\u2019s approach,\nLLM-REC places a particular emphasis on the innate common-sense reasoning capabilities of large language models and notably does not mandate domain expertise. Since the augmented information may not necessarily be correct, we further implement a variant with ground truth knowledge. It aligns with strategies akin to those introduced by Di Noia et al. (2012), who harnessed external databases to enhance item information. The second method, TagGPT, proposed by Li et al. (2023a), extracts tags using LLMs, similar to one of our prompting strategies for item descriptions. Although collaborative Filtering (CF) is another widely used technique in recommendation systems, given our primary focus on addressing the issue of incomplete item descriptions, we do not conduct experiments under CF settings (Li et al., 2023b). Instead, we concentrate on comparing our method with other input augmentation approaches.\n# A.3 Item and User Modules\nWe use Sentence-BERT (Reimers and Gurevych, 2019) to derive the textual embeddings from the original item description and augmented text. The embedding model is all-MiniLM-L6-v2. We directly apply it to convert the natural language to embeddings without fine-tuning it, freezing only the text-encoder and not the user encoder to avoid the additional computational cost associated with training and fine-tuning the sentence transformer model. For users, we employ an embedding table to convert user ID into latent representations. For both MovieLens-1M and Recipe datasets, the output dimension is set at 128. We have considered using embeddings derived from the LLMs. However, as our goal is to propose a general framework that can leverage both open-source and proprietary models, we do not pursue further exploration of this aspect in the current study.\n# A.4 Model Training\nTo facilitate the model training process, we employ the binary cross-entropy loss, expressed as:\n(1)\nwhere (u, i) represents the user-item pair, and Y denotes the set that contains all positive and negative samples. The variable yu,i serves as a label, with a value of 1 indicating that user u has engaged\nwith item i, and 0 representing the absence of interaction. The prediction score \u02c6yu,i, ranging from 0 to 1, reflects the likelihood of user u interacting with item i. In our dataset, each instance of useritem interaction is considered a positive sample. Alongside these positive samples, we incorporate negative samples by randomly pairing users with items that lack any prior recorded interactions. To mitigate the risk of overfitting and enhance training efficiency, we implement an early stopping mechanism. The window size and evaluation frequency are both configured to be 5. It is noteworthy that we have also explored the viability of employing the Bayesian Personalized Ranking (BPR) Loss (Rendle et al., 2012) within our framework. However, subsequent experimentation reveals that the BPR Loss does not offer superior performance when compared to the binary cross-entropy loss. Consequently, we opt to use the binary cross-entropy loss as our loss function.\n# A.5 Hyper-parameter Settings\nLarge Language Models. We perform experiments with two large language models. For GPT-3 (text-davinci-003), temperature, max_token, top_p, frequency penalty, and presence penalty are set as 0, 512, 1, 0.0, and 0.6, respectively. For LLAMA-2 (7B LLAMA-2-CHAT), we set do_sample to be True, top_k 10, and the num_return_sequences 1. LLAMA-2\u2019s generation is conducted on 8 NVIDIA GeForce RTX 2080 Ti GPUs, each equipped with 11 GB of memory. Recommendation Modules We initialize the model parameters randomly, following a Gaussian distribution. To optimize the framework, we employ the AdamW algorithm (Loshchilov and Hutter, 2017) with a weight decay value of 0.0005. For the MLP model, the hyper-parameter grids for the learning rate and dropout rate are {0.0001, 0.0005, 0.001} and {0.1, 0.3, 0.5}, respectively. For AutoInt (Song et al., 2019), the hyper-parameter grids for the learning rate, dropout rate, hidden layer size, number of attention layers, and attention heads are {0.001, 0.005, 0.01}, {0.1, 0.3, 0.5}, {16, 32, 64, 128}, {1, 2}, and {1, 2}, respectively. For DCN-V2 (Wang et al., 2021), the learning rate, dropout rate, hidden layer size, and number of cross layers are searched in {0.001, 0.005, 0.01}, {0.1, 0.3, 0.5}, {16, 32, 64, 128}, {1, 2}, and {1, 2}, respectively. Since the network structure of EDCN (Chen et al., 2021) is similar with DCN-V2 (Wang et al., 2021),\nwe apply the hyper-parameter settings of DCN-V2 to EDCN. The performance is evaluated every five epochs, and the early stop mechanism is configured to have a patience of 5. We set the batch size to 4096 for all baselines except for AutoInt which is 1024 due to the memory limitation. Settings that achieve the highest Recall@K on the validation set are chosen for the evaluation on the testing set.\n# A.6 Importance Measurement for Engagement-guided Prompting\nIn our study, we show an example of using Personalized PageRank (PPR) (Brin, 1998) score as the metric to find the important neighbor items. PPR is a widely employed technique for finding significant neighbors in recommendation systems (Ying et al., 2018). In particular, we first construct the user-item bipartite graph G = (V, E). In this notation, G represents the bipartite graph, E denotes the set of nodes, and E represents the set of edges. There are two types of nodes including users Vuser \u2282V and items Vitem \u2282V (Vuser \u222aVitem = V, Vuser \u2229Vitem = \u2205). An edge e \u2208E between a user node v \u2208Vuser and an item node i \u2208Vitem is created if this user interacts with this item. Next, we proceed by calculating the Personalized PageRank (PPR) score for each item node. The PPR value \u03c0(s, t), where s is the source node and t is the target node, signifies the probability that a random walk initiated from node s concludes at node t. This value offers a quantified measure of their relative importance from the standpoint of an individual node (Lofgren, 2015). For every item node, we construct a set of significant neighboring items. By identifying the top T item nodes with the highest PPR scores, we pinpoint important neighbor items guided by user engagement. The rationale behind this approach lies in the observation that when users frequently engage with two items, there tends to be a greater similarity between these two items through the lens of user preferences. By incorporating this information, we aim to capture user preferences more effectively, leading to enhanced performance in content recommendation. For both datasets, we set T = 3. For the Movielens1M dataset, we find the important neighbor items that share the same genre as the target item.\n# A.7 Implementation Details\nOur methods are implemented and experiments are conducted using PyTorch. The computation of PPR scores is facilitated by the use of the torch-ppr\nlibrary. Each experiment is run on one NVIDIA A100 GPU with 80 GB of memory at a time. Further, we adapt the codes of the DeepCTR3 and DeepCTR-Torch4 repositories to implement AutoInt (Song et al., 2019), DCN-V2 (Wang et al., 2021), and EDCN (Chen et al., 2021). Table 7 summarizes all prompts of LLM-REC.\n# A.8 KAR Augmentation Details\nIn KAR, Xi et al. (2023) applied a specific prompt to elicit factual knowledge about movies of the Movielens-1M dataset (Harper and Konstan, 2015). The prompt instructed the model to: \u201cIntroduce movie {item description} and describe its attributes precisely (including but not limited to scenario-specific factors)\u201d. In their study, the item description was the movie titles. Human experts were enlisted to refine the answers generated by LLMs in response to the question: \u201cList the importance factors or features that determine whether a user will be interested in a movie.\u201d These refined factors were then considered as the {scenario-specific factors}, including genre, actors, directors, theme, mood, production quality, and critical acclaim. Because the responses generated using these prompts were not publicly released, we re-implement the same methodology, employing LLMs to generate the factual knowledge of items. In the case of the Recipe dataset (Majumder et al., 2019), we use recipe description as the item description. The same approach is then adopted to identify scenario-specific factors. Initially, the prompt is adapted to: \u201cList the importance factors or features that determine whether a user will be interested in a recipe.\u201d Subsequently, the answers generated by CHATGPT are validated (see Table 8). The resulting set of scenario-specific factors for Recipe comprises a diverse range of attributes, including dietary preferences, ingredients, cuisine type, cooking time, nutritional value, allergies, taste preferences, skill level, occasion, cost, health and wellness goals, food allure, reviews and ratings, cooking equipment, personal experience, season and weather, cultural or ethical considerations, creativity and variety, simplicity, popularity and trends. These prompts are then employed to enrich the factual knowledge of both movies and recipes using GPT-3 (text-davinci-003). For illustrative examples of the responses, please refer to Table 9. 3https://github.com/shenweichen/DeepCTR\n3https://github.com/shenweichen/DeepCTR 4https://github.com/shenweichen/DeepCTR-Torch\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c7cc/c7cc393d-e4ba-4e43-81e6-8efdda2afaf7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 7: LLM-REC prompts.</div>\nKAR is also composed of a preference reasoning prompt for user information augmentation. Since we only focus on the item side, only the item factual prompt is implemented to examine how different focuses on LLMs\u2019 ability between LLM-REC and KAR affect recommendation performance.\n# A.9 Keywords Construction\nThe keyword generation process differs between the Movielens-1M and Recipe datasets. For Movielens-1M, the keywords are derived from genre labels, which are intrinsic components of the dataset\u2019s metadata. In the case of Recipe, the process involves multiple steps. Initially, we compile a list of unique words found in the responses generated through the recommendation-driven prompting strategy. Subsequently, we filter out stopwords and proceed to construct unigrams and bigrams using the NLTK package. Following this, a manual review is conducted to identify phrases that appear at least five times in the corpus. These phrases are then scrutinized to determine whether they contain words relevant for categorizing recipes. The final list of keywords for Recipe contain \u201ceasy\u201d, \u201chomemade\u201d, \u201cbaking\u201d, \u201chealth\u201d, \u201chealthy\u201d, \u201cdessert\u201d, and \u201cdinner\u201d. These keywords collectively serve as indicative descriptors for recipes within the dataset.\nThe factors or features that determine whether a user will be interested in a recipe can vary from person to person, but some important factors commonly include: 1. Dietary Preferences: Whether the recipe aligns with the user\u2019s dietary restrictions, such as vegetarian, vegan, gluten-free, or keto. 2. Ingredients: The availability and appeal of the ingredients used in the recipe. 3. Cuisine Type: Whether the recipe falls within a cuisine the user enjoys, like Italian, Mexican, or Asian. 4. Cooking Time: The user\u2019s available time for cooking, as some may prefer quick and easy recipes, while others enjoy longer cooking processes. 5. Nutritional Value: Users may consider the nutritional content of the recipe, such as calorie count, protein, or fiber. 6. Allergies: Any known food allergies that need to be accommodated in the recipe. 7. Taste Preferences: Whether the flavors and taste profile of the recipe match the user\u2019s preferences. 8. Skill Level: Some users may look for recipes that match their cooking skills, such as beginner-friendly or gourmet. 9. Occasion: If the recipe is suitable for a particular occasion, like a holiday, party, or a regular weekday meal. 10. Cost: The affordability of the ingredients and whether it fits within the user\u2019s budget. 11. Health and Wellness Goals: Recipes that align with specific health or weight management goals, like low-carb or hearthealthy. 12. Food Allure: The visual appeal and presentation of the dish through images and descriptions. 13. Reviews and Ratings: User reviews and ratings on the recipe, which can indicate its popularity and quality. 14. Cooking Equipment: Whether the user has the necessary kitchen tools and appliances for the recipe. 15. Personal Experience: Previous experiences with similar recipes or dishes may influence interest. 16. Season and Weather: Seasonal ingredients or dishes that match the current season or weather. 17. Cultural or Ethical Considerations: Recipes that respect cultural, ethical, or environmental values. 18. Creativity and Variety: The desire for trying something new and innovative in cooking. 19. Simplicity: Some users may prefer straightforward, minimal-ingredient recipes. 20. Popularity and Trends: Trending recipes or those recommended by friends and family may pique a user\u2019s interest. These factors can significantly impact a user\u2019s decision to explore a recipe, and personalized recommendation systems often consider several of these factors to suggest recipes tailored to individual preferences.\nThe factors or features that determine whether a user will be interested in a recipe can vary from person to person, but some\nimportant factors commonly include:\n1. Dietary Preferences: Whether the recipe aligns with the user\u2019s dietary restrictions, such as vegetarian, vegan, gluten-free, or\nketo.\n2. Ingredients: The availability and appeal of the ingredients used in the recipe.\n3. Cuisine Type: Whether the recipe falls within a cuisine the user enjoys, like Italian, Mexican, or Asian.\n4. Cooking Time: The user\u2019s available time for cooking, as some may prefer quick and easy recipes, while others enjoy longer\ncooking processes.\n5. Nutritional Value: Users may consider the nutritional content of the recipe, such as calorie count, protein, or fiber.\n6. Allergies: Any known food allergies that need to be accommodated in the recipe.\n7. Taste Preferences: Whether the flavors and taste profile of the recipe match the user\u2019s preferences.\n8. Skill Level: Some users may look for recipes that match their cooking skills, such as beginner-friendly or gourmet.\n9. Occasion: If the recipe is suitable for a particular occasion, like a holiday, party, or a regular weekday meal.\n10. Cost: The affordability of the ingredients and whether it fits within the user\u2019s budget.\n11. Health and Wellness Goals: Recipes that align with specific health or weight management goals, like low-carb or heart-\nhealthy.\n12. Food Allure: The visual appeal and presentation of the dish through images and descriptions.\n13. Reviews and Ratings: User reviews and ratings on the recipe, which can indicate its popularity and quality.\n14. Cooking Equipment: Whether the user has the necessary kitchen tools and appliances for the recipe.\n15. Personal Experience: Previous experiences with similar recipes or dishes may influence interest.\n16. Season and Weather: Seasonal ingredients or dishes that match the current season or weather.\n17. Cultural or Ethical Considerations: Recipes that respect cultural, ethical, or environmental values.\n18. Creativity and Variety: The desire for trying something new and innovative in cooking.\n19. Simplicity: Some users may prefer straightforward, minimal-ingredient recipes.\n20. Popularity and Trends: Trending recipes or those recommended by friends and family may pique a user\u2019s interest.\nThese factors can significantly impact a user\u2019s decision to explore a recipe, and personalized recommendation systems often\nconsider several of these factors to suggest recipes tailored to individual preferences.\nTable 8: Response generated by CHATGPT about the question \u201cList the importance factors or features tha determine whether a user will be interested in a recipe.\u201d\nPrompt: Introduce movie Batman Forever and describe its attributes precisely (including but not limited to genre, actors, directors, theme, mood, production quality, and critical acclaim) Response: Batman Forever is a 1995 American superhero film directed by Joel Schumacher and produced by Tim Burton. It is the third installment in the Batman film series, based on the DC Comics character of the same name. The film stars Val Kilmer as Bruce Wayne/Batman, alongside Tommy Lee Jones, Jim Carrey, Nicole Kidman, Chris O\u2019Donnell, Michael Gough, and Pat Hingle. The film follows Batman as he battles Two-Face and The Riddler, both of whom are determined to wreak havoc in Gotham City. The film\u2019s theme is the struggle between good and evil, and the power of redemption. The mood of the film is dark and brooding, with a hint of humor. The production quality of Batman Forever is high, with impressive special effects and cinematography. The film was well-received by critics, who praised its action sequences, performances, and visual style. It was a box office success, grossing over $336 million worldwide.\nPrompt: Introduce recipe \u2018Numerous Amish bread recipes have been shared, but none seem to include the starter. Therefore, I thought it would be helpful to provide this. Happy baking! Here\u2019s the Amish Friendship Bread and its starter.\u2019 and describe its attributes precisely (including but not limited to dietary preferences, ingredients, cuisine type, cooking time, nutritional value, allergies, taste preferences, skill level, occasion, cost, health and wellness goals, food allure, reviews and ratings, cooking equipment, personal experience, season and weather, cultural or ethical considerations, creativity and variety, simplicity, popularity and trends) Response: Amish Friendship Bread is a sweet, yeasty bread that is made with a starter that is passed from person to person. It is a traditional Amish recipe that has been around for generations. Dietary Preferences: Vegetarian, Gluten-Free (if using gluten-free flour) Ingredients: All-purpose flour, sugar, active dry yeast, milk, butter, eggs, vanilla extract, cinnamon, and salt. Cuisine Type: American Cooking Time: 10 minutes to prepare the starter, 1 hour to prepare the dough, and 1 hour to bake the bread.Nutritional Value: High in carbohydrates, low in fat and protein. Allergies: Contains dairy, eggs, and wheat. Taste Preferences: Sweet, yeasty, and slightly spiced. Skill Level: BeginnerOccasion: Breakfast, brunch, or snack. Cost: Low Health and Wellness Goals: A healthier alternative to store-bought breads. Food Allure: Comfort food that is easy to make. Reviews and Ratings: Generally positive reviews. Cooking Equipment: Large bowl, measuring cups and spoons, wooden spoon, loaf pan, and oven. Personal Experience: Delicious and easy to make. Season and Weather: Can be made year-round. Cultural or Ethical Considerations: Traditional Amish recipe. Creativity and Variety: Can be adapted to include different flavors and ingredients. Simplicity: Easy to make with basic ingredients. Popularity and Trends: Popular among home bakers.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d1f/2d1f89fc-81cc-414f-9e5a-7ef310fe37f9.png\" style=\"width: 50%;\"></div>\nTable 9: Response generated via the item factual prompt proposed by Xi et al. (2023).\n# B Additional Analysis\n# B.1 Additional Discussions on Augmented Text\nOverall, we observe that LLM-REC effectively elicits LLMs to enrich item descriptions with additional information. This augmentation results in descriptions that are not only more detailed but also more expressive, enhancing the original item descriptions. Tables 11 and 12 show example responses generated by GPT-3 comparing the recommendation-driven and basic promptings in terms of tag and infer. Tables 13, 14, and 15 show example responses generated by LLAMA2 comparing the recommendation-driven and basic promptings. Responses from the recommendation prompting strategies provides several additional pieces of information and context compared to the responses from the basic prompting strategies. Take the responses in Table 11 as an example, the recommendation prompting strategy introduces new themes like \u201cSupernatural\u201d, \u201cParanormal\u201d, and \u201cPsychological Thriller\u201d, which are not present in the responses from the basic promptings. These themes suggest a broader and more specific context for the story, indicating not just communication with the dead, but also elements of horror and suspense. The term \u201cTroubled Child\u201d adds a new dimension to the \u201cYoung Boy\u201d mentioned in the first sentence, suggesting that the child\u2019s character may face internal conflicts or challenges. What extra information does engagementguided strategy prompt LLMs to augment? Consistent with our previous experiments, we curate exemplary responses obtained from peng for closer examination (Figure 6). Our analysis reveals a distinct pattern compared to what we have observed with recommendation-driven prompting. There are primarily two scenarios to consider. First, if the descriptions of the important neighbor items and the target items exhibit high similarity, the impact of peng resembles that of ppara, as exemplified in the second Recipe example in Figure 6. Second, peng guides LLMs to generate additional information, which may be derived from the descriptions of the important neighbor items. Consequently, how the engagement-guided strategy influences LLMs\u2019 text generation\u2014whether it aligns with one of the behaviors we have described, both of them, or even other unexplored patterns\u2014largely depends on the composition of the important neighbor items. This composition, in turn, is contingent on the neighbor\nsampling method which is out of the scope of our study. We leave a more in-depth exploration of this topic to future research endeavors. Interestingly, the recommendation-driven + engagement-guided prompting strategy is able to generate text that shares similar characteristics with both sub-strategies. How they quantitatively form the final generation remains an open question. Examples can be found in Table 17. Table 16 shows example responses of LLAMA2 to the engagement-guided prompting strategy. Table 17 shows example responses of GPT-3 to the recommendation-driven and engagement-guided prompting strategy. Overall, the 7B LLAMA-2CHAT performs poorly compared to GPT-3. In some cases, there is no generated content as we have also observed in Appendix D.\n# B.2 Additional Discussions on Applicable Item Domains and Available Textual Information\nThe applicability of LLM-REC beyond movies and recipes, particularly in domains with sparse textual information, remains a question. To address this, we conduct an analysis to determine LLM-REC\u2019s efficacy in enriching text across various domains and text lengths. We use item descriptions from ten distinct domains in the Amazon review dataset (Ni et al., 2019), which includes product metadata like descriptions, category, price, brand, and image features. The selected domains are all beauty, appliances, automotive, digital music, grocery and gourmet food, pet supplies, sports and outdoors, video games, magazine subscriptions, and industrial and scientific. For each domain, we sample 50 items and prompt GPT-3 using prec para (i.e., recommendation-driven prompting). Our previous discussions highlight that the most valuable information for improving recommendation performance typically aligns with expressive words pertinent to item characteristics. While no single metric directly quantifies this added information, we use the increase in the number of adjectives as a proxy. Additionally, the total word count serves as a straightforward metric to approximate the volume of augmented information. By comparing the augmented texts with the original item descriptions, we calculate the percentage increase in the number of adjectives. Note that the adjective increase is computed as a ratio of the difference in adjective count to the original word count. As Figure 1 demonstrates, LLM-REC effec-\ntively enriches item descriptions across multiple domains, including those lacking in rich textual content. For instance, the average word counts in movie and digital music descriptions are only 20.34 and 30.18 words, respectively. LLM-REC enhances expressiveness, with a notable increase in the use of adjectives.\n# B.3 Additional Experiments on Applying LLM-Rec to Other Baselines\nWe extend the application of LLM-REC to other text-based recommendation systems and replicate the experiments. The results, as presented in Table 10, indicate that LLM-REC can be easily adapted to various text-based recommendation systems and generally enhances recommendation performance compared to using the original text.\n# B.4 Additional Discussions on Integration Process\n# B.4 Additional Discussions on Integration\n# B.4 Additional Discussions on Integration Process\nIn our setup, the text encoder is frozen (not finetuned), with a fixed output dimension for all vectors. The fundamental difference between the Concat-All and Text Concatenation methods lies in their processing sequence. The Concat-All method initially transforms individual text segments into embeddings and subsequently concatenates these embeddings. In contrast, the Text Concatenation method first concatenates the text segments and then converts this combined text into a single embedding. The observed superiority of the Concat-All method can be attributed to how these processes handle information density. When lengthy text segments are concatenated before encoding, there is a higher likelihood of information loss, particularly given the constraints of a frozen text encoder. This encoder, not being fine-tuned for the specific nuances of our data, may struggle to effectively capture and retain crucial information from longer text inputs. Therefore, processing shorter text segments individually before concatenation (as in Concat-All) may help in preserving important features and nuances in the embeddings.\n# B.5 Additional Discussions on Prompt Design\nTo investigate whether modifying the word choice in the designed prompts significantly affects the augmented output, we construct one variant prompt for each of LLM-REC\u2019s prompts, ensuring they convey the same meaning but with different word choices. Take ppara as an exam-\nple, ppara is \u201cThe description of an item is as follows \u2018{description}\u2019, paraphrase it.\u201d. One variant is \u201cSummarize the given item description, \u2018{description}\u2019, using different words.\u201d Next, we randomly sample 50 items from Movielens-1M, and prompt GPT-3 with these variants. The cosine similarity between the responses generated from the variant prompt and LLM-REC\u2019s prompt is computed and shown in Table 5. Tables 18-25 shows the example responses.\n# B.6 Additional Discussions on Dynamic Prompts\nThe concept of dynamic prompts in recommendation systems is an intriguing area that holds the potential for enhancing personalization. By incorporating descriptions of a user\u2019s most recently interacted items into prompts, the system can generate item descriptions on-the-fly that are more closely aligned with the user\u2019s current interests and preferences. This approach could lead to more precise and tailored recommendations, as the generated descriptions would reflect the user\u2019s evolving tastes. One of the primary considerations is the computational cost associated with generating dynamic prompts. Each user interaction would require realtime processing to update the prompt, which could be resource-intensive, especially for large-scale systems with many users and items. To mitigate computational costs, several strategies can be employed. Developing efficient algorithms for prompt generation and item description generation can help mitigate computational costs. Implementing caching mechanisms for frequently accessed data can reduce the processing time required for updating prompts. Instead of completely regenerating prompts after each interaction, the system could employ incremental updates to modify prompts based on recent changes in user behavior. While the implementation of dynamic prompts presents several challenges, it also offers a promising avenue for enhancing personalization in recommendation systems. With careful consideration, this approach has the potential to cater more effectively to individual user needs.\n# C Extended Related Work\nAugmentation in Text-based Recommendation. Traditionally, researchers have advocated the augmentation of item descriptions through the incorporation of external knowledge sources (Di Noia et al.,\nMovielens-1M\nRecipe\nPrecision@10\nRecall@10\nNDCG@10\nPrecision@10\nRecall@10\nNDCG@10\nAutoInt (Song et al., 2019)\n0.2149 \u00b10.0078\n0.1706 \u00b10.0075\n0.2698 \u00b10.0092\n0.0351 \u00b10.0032\n0.0772 \u00b10.0102\n0.0658 \u00b10.0089\n- Basic\n0.2590 \u00b10.0038\n0.2193 \u00b10.0049\n0.3224 \u00b10.0052\n0.0361 \u00b10.0030\n0.0797 \u00b10.0097\n0.0664 \u00b10.0076\n- Rec\n0.2593 \u00b10.0035\n0.2197 \u00b10.0068\n0.3242 \u00b10.0059\n0.0357 \u00b10.0029\n0.0794 \u00b10.0096\n0.0660 \u00b10.0079\n- Eng\n0.2323 \u00b10.0011\n0.1857 \u00b10.",
    "paper_type": "method",
    "attri": {
        "background": "Text-based recommendation systems face challenges due to incomplete item descriptions that hinder effective alignment with user preferences. The advent of large language models (LLMs) provides an opportunity to enhance these descriptions, thus improving recommendation performance.",
        "problem": {
            "definition": "The problem lies in the incomplete or insufficient information within item descriptions, which limits the ability to accurately match item characteristics with user preferences, particularly in diverse domains.",
            "key obstacle": "The core obstacle is the inherent limitation of item descriptions that often fail to capture crucial attributes necessary for effective recommendations, especially in cases of user-generated content."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of LLMs to generate enriched item descriptions that better reflect both item characteristics and user preferences.",
            "opinion": "The proposed idea involves utilizing LLMs to augment original item descriptions through various prompting strategies, thereby enhancing the relevance of recommendations.",
            "innovation": "The primary innovation of LLM-REC lies in its ability to apply distinct prompting strategies for text enrichment, significantly improving recommendation performance compared to existing methods."
        },
        "method": {
            "method name": "LLM-REC",
            "method abbreviation": "LLM-REC",
            "method definition": "LLM-REC is a framework that leverages large language models to enhance item descriptions through prompting strategies, thus improving personalized recommendations.",
            "method description": "The method enriches item descriptions using four distinct prompting strategies to generate more informative and contextually relevant text.",
            "method steps": [
                "Identify the original item description.",
                "Apply one of the four prompting strategies to generate augmented text.",
                "Concatenate the augmented text with the original description.",
                "Use the combined text as input for the recommendation module."
            ],
            "principle": "The effectiveness of LLM-REC is based on the ability of LLMs to generate contextually rich descriptions that align better with user preferences, enhancing the overall recommendation quality."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using two datasets: Movielens-1M for movie recommendations and Recipe for recipe recommendations, comparing LLM-REC against various baseline methods.",
            "evaluation method": "Performance was assessed using metrics such as Precision@K, Recall@K, and NDCG@K across five different train/test splits, with a focus on the impact of text augmentation on recommendation performance."
        },
        "conclusion": "The experiments demonstrated that LLM-REC significantly improves recommendation quality by augmenting item descriptions, with simple models achieving performance comparable to complex methods. This highlights the potential of LLMs in enhancing recommendation systems.",
        "discussion": {
            "advantage": "LLM-REC provides a versatile framework that can be applied across various domains, enhancing recommendation performance with minimal domain expertise required.",
            "limitation": "The method incurs additional computational costs due to the text augmentation phase and may struggle to incorporate the latest knowledge promptly.",
            "future work": "Future research could explore methods to reduce computational costs and improve the integration of current knowledge into the LLM-REC framework."
        },
        "other info": {
            "acknowledgments": "Luo was supported in part by NSF Award #2238208.",
            "datasets": {
                "Movielens-1M": {
                    "interaction": 1000209,
                    "item": 3706,
                    "user": 6040
                },
                "Recipe": {
                    "interaction": 132246,
                    "item": 4125,
                    "user": 2589
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The problem lies in the incomplete or insufficient information within item descriptions, which limits the ability to accurately match item characteristics with user preferences."
        },
        {
            "section number": "1.2",
            "key information": "The advent of large language models (LLMs) provides an opportunity to enhance item descriptions, thus improving recommendation performance."
        },
        {
            "section number": "2.3",
            "key information": "The primary innovation of LLM-REC lies in its ability to apply distinct prompting strategies for text enrichment, significantly improving recommendation performance compared to existing methods."
        },
        {
            "section number": "4.1",
            "key information": "LLM-REC is a framework that leverages large language models to enhance item descriptions through prompting strategies, thus improving personalized recommendations."
        },
        {
            "section number": "4.2",
            "key information": "The effectiveness of LLM-REC is based on the ability of LLMs to generate contextually rich descriptions that align better with user preferences, enhancing the overall recommendation quality."
        },
        {
            "section number": "5.1",
            "key information": "The proposed idea involves utilizing LLMs to augment original item descriptions through various prompting strategies, thereby enhancing the relevance of recommendations."
        },
        {
            "section number": "7.1",
            "key information": "LLM-REC provides a versatile framework that can be applied across various domains, enhancing recommendation performance with minimal domain expertise required."
        },
        {
            "section number": "10.1",
            "key information": "The method incurs additional computational costs due to the text augmentation phase and may struggle to incorporate the latest knowledge promptly."
        }
    ],
    "similarity_score": 0.8360786274758675,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLM-Rec_ Personalized Recommendation via Prompting Large Language Models.json"
}