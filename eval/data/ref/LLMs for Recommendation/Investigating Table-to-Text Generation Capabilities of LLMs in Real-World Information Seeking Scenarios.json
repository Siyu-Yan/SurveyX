{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.14987",
    "title": "Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios",
    "abstract": "Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LOGICNLG and our newlyconstructed LOTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for querybased generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users\u2019 information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., T\u00dcLU and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https: //github.com/yale-nlp/LLM-T2T.",
    "bib_name": "zhao2023investigatingtabletotextgenerationcapabilities",
    "md_text": "# Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios\nYilun Zhao\u22171 Haowei Zhang\u22172 Shengyun Si\u22172 Linyong Nan1 Xiangru Tang1 Arman Cohan1,3\n1Yale University, 2Technical University of Munich, 3Allen Institute for AI yilun.zhao@yale.edu {haowei.zhang, shengyun.si}@tum.de\n# Abstract\nTabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LOGICNLG and our newlyconstructed LOTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for querybased generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users\u2019 information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., T\u00dcLU and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https: //github.com/yale-nlp/LLM-T2T.\narXiv:2305.14987v2\n# 1 Introduction\nIn an era where users interact with vast amounts of structured data every day for decision-making and information-seeking purposes, the need for intuitive, user-friendly interpretations has become paramount (Zhang et al., 2023; Zha et al., 2023; Li et al., 2023). Given this emerging necessity, table-to-text generation techniques, which transform complex tabular data into comprehensible narratives tailored to users\u2019 information needs, have \u2217Equal Contributions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b50/2b50ae46-1442-467c-ad53-6734007c60ec.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">RQ2: Can we use LLMs to assess factual consistency of table-to-text generation?</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb80/bb8091a9-0792-417f-9de4-eacda14cd667.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">RQ3: How can fine-tuned models benefit from LLMs' strong</div>\n<div style=\"text-align: center;\">RQ3: How can fine-tuned models benefit from LLMs' strong table-to-text abilities?</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/277d/277ddfaa-f9cf-4315-a89f-74f52d947967.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The real-world table information seeking scenarios and research questions investigated in this paper.</div>\ndrawn considerable attention (Parikh et al., 2020; Chen et al., 2020a; Nan et al., 2022b; Zhao et al., 2023c). These techniques can be incorporated into a broad range of applications, including but not limited to game strategy development, financial analysis, and human resources management. However, existing fine-tuned table-to-text generation models (Nan et al., 2022a; Liu et al., 2022b,a; Zhao et al., 2023b) are typically task-specific, limiting their adaptability to real-world applications. The emergence and remarkable achievements of LLMs (Brown et al., 2020; Scao et al., 2022; Wang\nDataset\n# Table\n# Examples\nControl Signal\nRich in Reasoning?\nData Insight Generation\nLOGICNLG (Chen et al., 2020a)\n862\n4,305\nNone\n\u2713\nLOTNLG (ours)\n862\n4,305\nReasoning type\n\u2713\nQuery-based Generation\nFeTaQA (Parikh et al., 2020)\n2,003\n2,003\nUser query\n\u2717\nF2WTQ (ours)\n4,344\n4,344\nUser query\n\u2713\nTable 1: Experimental dataset statistics for the test set. Examples of our newly-constructed LOTNLG and F2WT datasets are displayed in Figure 2 and 3, respectively.\net al., 2023; Scheurer et al., 2023; OpenAI, 2023; Touvron et al., 2023a; Taori et al., 2023; Touvron et al., 2023b) have sparked a significant transformation in the field of controllable text generation and data interpretations (Nan et al., 2021; Zhang et al., 2022; Goyal et al., 2022; K\u00f6ksal et al., 2023; Gao et al., 2023b; Madaan et al., 2023; Zhou et al., 2023). As for table-based tasks, recent work (Chen, 2023; Ye et al., 2023; Gemmell and Dalton, 2023) reveals that LLMs are capable of achieving competitive performance with state-of-the-art fine-tuned models on table question answering (Pasupat and Liang, 2015; Nan et al., 2022b) and table fact checking (Chen et al., 2020b; Gupta et al., 2020). However, the potential of LLMs in generating text from tabular data for users\u2019 information-seeking purposes remains largely underexplored. In this paper, we investigate the table-to-text generation capabilities of LLMs in two real-world table information seeking scenarios: 1) Data Insight Generation (Chen et al., 2020a), where users aim to promptly derive significant facts from the table, anticipating the systems to offer several data insights; and 2) Query-based Generation (Pasupat and Liang, 2015; Nan et al., 2022b), where users consult tables to answer specific questions. To facilitate a rigorous evaluation of LLM performance, we also construct two new benchmarks: LOTNLG for data insight generation conditioned with specific logical reasoning types; and F2WTQ for free-form question answering that requires models to perform human-like reasoning over Wikipedia tables. We provide an overview of table information seeking scenarios and our main research questions in Figure 1, and enumerate our findings as follows: RQ1: How do LLMs perform in table-to-text generation tasks? Finding: LLMs exhibit significant potential in generating coherent and faithful natural language\nRQ2: Can we use LLMs to assess factual consistency of table-to-text generation? Finding: LLMs using chain-of-thought prompting can serve as reference-free metrics for tableto-text generation evaluation. These metrics demonstrate better alignment with human evaluation in terms of both fluency and faithfulness.\nRQ3: How can fine-tuned models benefit from LLMs\u2019 strong table-to-text abilities? Finding: LLMs that utilize chain-of-thought prompting can provide high-quality natural language feedback in terms of factuality, which includes explanations, corrective instructions, and edited statements for the output of other models. The edited statements are more factually consistent with the table compared to the initial ones.\n# 2 Table Information Seeking Scenarios\nTable 1 illustrates the data statistics for the four datasets used in the experiments. We investigate the performance of the LLM in the following two real-world table information-seeking scenarios.\n# 2.1 Data Insight Generation\nData insight generation is an essential task that involves generating meaningful and relevant insights from tables. By interpreting and explaining tabular data in natural language, LLMs can play a crucial\nrole in assisting users with information seeking and decision making. This frees users from the need to manually comb through vast amounts of data. We use the following two datasets for evaluation.\n# 2.1.1 LOGICNLG Dataset\nThe task of LOGICNLG (Chen et al., 2020a) involves generating five logically consistent sentences from a given table. It aims to uncover intriguing facts from the table by applying various logical reasoning operations (e.g., count and comparison) across different table regions.\n# 2.1.2 LOTNLG Dataset\nOur preliminary experiments revealed that when applied to the LOGICNLG dataset, table-to-text generation systems tend to generate multiple sentences that employ the same logical reasoning operations. For instance, in a 0-shot setting, the GPT-3.5 model is more inclined to generate sentences involving numerical comparisons, while overlooking other compelling facts within tables. This lack of diversity in data insight generation poses a significant limitation because, in real-world information-seeking scenarios, users typically expect systems to offer a variety of perspectives on the tabular data. To address this issue, application developers could tailor the table-to-text generation systems to generate multiple insights that encompass different logical reasoning operations (Perlitz et al., 2022; Zhao et al., 2023b). In order to foster a more rigorous evaluation of LLMs\u2019 abilities to utilize a broader range of logical reasoning operations while generating insights from tables, we have developed a new dataset, LOTNLG, for logical reasoning typeconditioned table-to-text generation. In this setup, the model is tasked with generating a statement by performing the logical reasoning operations of the specified types on the tables.\n# LOTNLG Dataset Construction Following\nChen et al. (2020b), we have predefined nine types of common logical reasoning operations (e.g., count, comparative, and superlative), with detailed definitions provided in Appendix A.1. We use examples from the LOGICNLG test set to construct LOTNLG. Specifically, for each statement from LOGICNLG, we assign two annotators to independently label the set of logical reasoning types used in that statement, ensuring that no more than two types were identified per statement. If there are discrepancies in the labels, an expert annotator is\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/faf0/faf02cf6-b9fe-4b23-a3e6-7aec61ad80cc.png\" style=\"width: 50%;\"></div>\nFigure 2: An example of LOTNLG, where models are required to generate statements using the specified types of logical reasoning operations\nbrought in to make the final decision. The distribution of logical reasoning types in LOTNLG is illustrated in Figure 4 in Appendix A.1.\n# brought in to make the final decision. The distribution of logical reasoning types in LOTNLG is illustrated in Figure 4 in Appendix A.1.\n# 2.2 Query-based Generation\nQuery-based table-to-text generation pertains to producing detailed responses based on specific user queries in the context of a given table. The ability to answer users\u2019 queries accurately, coherently, and in a context-appropriate manner is crucial for LLMs in many real-world applications, such as customer data support and personal digital assistants. We utilize following two datasets to evaluate LLMs\u2019 efficiency in interacting with users and their proficiency in table understanding and reasoning.\n# 2.2.1 FeTaQA Dataset\nNan et al. (2022b) introduces a task of free-form table question answering. This task involves retrieving and aggregating information from Wikipedia tables, followed by generating coherent sentences based on the aggregated contents.\n# 2.2.2 F2WTQ Dataset\nQueries in the FeTaQA dataset typically focus on surface-level facts (e.g., \"Which country hosted the 2014 FIFA World Cup?\"). However, in real-world information-seeking scenarios, users are likely to consult tables for more complex questions, which require models to perform human-like reasoning over tabular data. Therefore, we have constructed a new benchmark, named F2WTQ, for more challenging, free-form table question answering tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48cb/48cb4652-b5ca-4c80-8832-96f340ecce2f.png\" style=\"width: 50%;\"></div>\nFigure 3: An example of F2WTQ, where models need to perform human-like reasoning to generate response.\n<div style=\"text-align: center;\">Figure 3: An example of F2WTQ, where models need to perform human-like reasoning to generate response.</div>\nF2WTQ Dataset Construction We adopt the WTQ dataset (Pasupat and Liang, 2015) as a basis to construct F2WTQ. The WTQ dataset is a short-form table question answering dataset, which includes human-annotated questions based on Wikipedia tables and requires complex reasoning. However, we do not directly use WTQ for LLM evaluation because, in real-world scenarios, users typically prefer a natural language response over a few words. In the development of F2WTQ, for each QA pair in the WTQ test set, we assign an annotator who assumes the role of an agent that analyzes the table and provides an expanded, sentencelong response. We found that the original questions in the WTQ dataset occasionally contained grammatical errors or lacked a natural linguistic flow. In these cases, the annotators are required to rewrite the question to ensure it was fluent and natural.\n# 3 Evaluation System\n# 3.1 Automated Evaluation\nWe adopt following popular evaluation metrics for automated evaluation:\n\u2022 BLEU (Papineni et al., 2002) uses a precisionbased approach, measuring the n-gram matches between the generated and reference statements. \u2022 ROUGE (Lin, 2004) uses a recall-based approach, and measures the percentage of overlapping words and phrases between the generated output and reference one.\n\u2022 BLEU (Papineni et al., 2002) uses a precisionbased approach, measuring the n-gram matches between the generated and reference statements.\n\u2022 ROUGE (Lin, 2004) uses a recall-based approach, and measures the percentage of overlapping words and phrases between the generated output and reference one.\n TAPEX-Acc (Liu et al., 2022a) employs TAPEX (Liu et al., 2022b) fine-tuned on the TabFact dataset as the backbone. Recent works (Liu et al., 2022a; Zhao et al., 2023b) have revealed that NLI-Acc and TAPAS-Acc is overly positive about the predictions, while TAPEX-Acc serves as a more reliable faithfulness-level metric.\n# \u2022 Exact Match & F-Score for Logical Reason-\ning Type For LOTNLG evaluation, the exact match measures the percentage of samples with all the labels classified correctly, while the FScore provides a balanced metric that considers both type I and type II errors.\n\u2022 Answer Accuracy refers to the proportion of correct predictions out of the total number of predictions in F2WTQ generation.\n# 3.2 Human Evaluation\nTo gain a more comprehensive understanding of the system\u2019s performance, we also conduct human evaluation. Specifically, the generated statements from different models are evaluated by humans based on two criteria: faithfulness and fluency. For faithfulness, each sentence is scored 0 (refuted) or 1 (entailed). For fluency, scores range from 1 (worst) to 5 (best). We average the scores across different human evaluators for each criterion. We do not apply more fine-grained scoring scales for faithfulness-level evaluation, as each statement in LOGICNLG consists of only a single sentence.\n# 4 Experiments\nIn the following subsections, we discuss the three key research questions about adopting LLMs into real-world table information seeking scenarios. Specifically, we explore LLMs\u2019 capabilities for table-to-text generation tasks, their ability to assess factual consistency, and whether they can benefit smaller fine-tuned models. The examined systems for each experiment are discussed in Appendix B.\nType\nModels\nSP-Acc\nNLI-Acc\nTAPAS-Acc\nTAPEX-Acc\nFine-tuned\nGPT2-C2F\n43.6\n71.4\n46.2\n43.8\nR2D2\n53.2\n86.2\n60.2\n61.0\nPLOG\n52.8\n84.2\n63.8\n69.6\nLOFT\n53.8\n86.6\n67.4\n61.4\n0-shot*\nGPT-3.5\n54.2\n87.6\n81.6\n79.4\nGPT-4\n43.2\n90.4\n91.8\n91.0\n1-shot Direct\nGPT-3.5\n60.2\n79.0\n80.4\n79.2\nGPT-4\n57.6\n82.0\n87.6\n88.0\n1-shot CoT\nGPT-3.5\n51.6\n70.0\n81.8\n78.2\nGPT-4\n59.8\n80.8\n89.4\n90.8\n2-shot Direct\nPythia-12b\n39.4\n53.2\n39.4\n40.4\nLLaMA-13b\n47.2\n58.4\n47.0\n43.2\nLLaMA-7b\n38.6\n63.4\n45.8\n43.6\nLLaMA2-70b-chat\n56.0\n52.4\n54.6\n52.4\nLLaMA-30b\n45.4\n55.8\n53.8\n53.0\nAlpaca-13b\n44.0\n70.6\n58.0\n54.6\nLLaMA-65b\n52.2\n57.2\n58.4\n56.8\nT\u00dcLU -13b\n44.4\n68.4\n63.4\n59.6\nVicuna-13b\n51.8\n71.4\n66.2\n65.2\nGPT-3.5\n64.0\n78.4\n78.8\n81.2\nGPT-4\n55.4\n85.8\n92.0\n89.6\n2-shot CoT\nPythia-12b\n41.8\n54.0\n41.2\n42.8\nLLaMA-7b\n38.0\n63.2\n48.0\n43.0\nLLaMA-13b\n44.2\n53.2\n49.2\n48.6\nLLaMA-30b\n45.0\n56.6\n60.8\n54.2\nLLaMA-65b\n48.0\n58.8\n57.4\n57.4\nT\u00dcLU -13b\n46.0\n69.8\n61.6\n58.8\nVicuna-13b\n44.6\n70.8\n63.0\n61.6\nAlpaca-13b\n45.4\n68.2\n64.0\n64.0\nLLaMA2-70b-chat\n52.6\n66.8\n69.4\n69.2\nGPT-3.5\n60.4\n70.2\n84.0\n83.4\nGPT-4\n62.2\n76.8\n88.8\n90.4\nTable 2: Faithfulness-level automated evaluation results on the LOGICNLG dataset. Within each experiment setting, we used TAPEX-Acc as the ranking indicator of model performance. \u2217: It is challenging for other LLMs  follow the instructions in 0-shot prompt to generate five statements for the input table.\n# 4.1 RQ1: How do LLMs perform in table-to-text generation tasks?\nWe experiment with two in-context learning methods, Direct Prediction (Figure 5 in Appendix) and Chain of Thoughts (CoT, Figure 6 in Appendix), to solve the table-to-text generation tasks.\n# Data Insight Generation Results\nData Insight Generation Results The results on the LOGICNLG dataset, as displayed in Table 2 and Table 3, indicate that GPT-* models generally surpass the current top-performing fine-tuned models (i.e., LOFT and PLOG) even in a 0-shot setting. Meanwhile, LLaMA-based models (e.g., LLaMA, Alpaca, Vicuna, T\u00dcLU) manage to achieve comparable performance to these top-performing finetuned models in a 2-shot setting. However, when it comes to the more challenging LOTNLG dataset, the automated evaluation result shows that only GPT-4 is capable of generating faithful statements\nthat adhere to the specified logical reasoning types (Table 6 in Appendix). Moreover, increasing the number of shots or applying chain-of-thought approach does not always yield a performance gain, motivating us to explore more advanced prompting methods for data insight generation in future work.\n# Query-based Generation Results\nin Appendix display the automated evaluation results for the FeTaQA and F2WTQ datasets, respectively. On FeTaQA, both LLaMA-based LLM and GPT-* models achieve comparable performance to the current top-performing fine-tuned models in a 2-shot setting, indicating the capability of LLMs to answer questions requiring surface-level facts from the table. However, a significant performance gap exists between other LLMs and GPT-* models on the more challenging F2WTQ dataset. Moreover, increasing the number of shots or applying\nModel\nFluency (1-5) Faithfulness (0-1)\nGPT2-C2F\n3.85\n0.54\nR2D2\n4.29\n0.72\nPLOG\n4.23\n0.77\nLOFT\n4.42\n0.81\nGPT-4 0-shot\n4.82\n0.90\nVicuna 2-shot Direct\n4.69\n0.71\nVicuna 2-shot CoT\n4.65\n0.73\nLLaMA2 2-shot Direct\n4.75\n0.79\nLLaMA2 2-shot CoT\n4.70\n0.83\nGPT-4 2-shot Direct\n4.71\n0.89\nGPT-4 2-shot CoT\n4.77\n0.92\n<div style=\"text-align: center;\">Fluency (1-5) Faithfulness (0-1)</div>\nTable 3: Human evaluation results on LOGICNLG.\nthe chain-of-thought approach can both yield performance gains for query-based generation.\n# 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation?\nIn RQ1, we demonstrate that LLMs can generate statements with comparative or even greater factual consistency than fine-tuned models. One natural follow-up question is whether we can employ LLMs to evaluate the faithfulness of table-to-text generation systems. This capability is crucial, as it ensures that tabular data is accurately interpreted for users, thereby preserving the credibility and reliability of real-world applications. As discussed in Section 3.1, existing faithfulnesslevel NLI-based metrics are trained on the TabFact dataset (Chen et al., 2020b). Recent work (Chen, 2023) has revealed that large language models using chain-of-thought prompting can achieve competitive results on TabFact. Motivated by this finding, we use the same 2-shot chain-of-thought prompt (Figure 7 in Appendix) as Chen (2023) to generate factual consistency scores (0 for refuted and 1 for entailed) for output sentences from LogicNLG. We use GPT-3.5 and GPT-4 as the backbones, as they outperforms other LLMs in RQ1 experiments. We refer to these new metrics as CoT3.5-Acc and CoT-4-Acc, respectively.\n# CoT-Acc Metrics Achieve Better Correlation\nwith Human Judgement We leverage the human evaluation results of models (excluding GPT4 models) in RQ1 as the human judgement. We then compare the system-level Pearson\u2019s correlation between each evaluation metric and this human judgement. As shown in Table 4, the proposed CoT-4-Acc and CoT-3.5-Acc metrics achieve the highest and third highest correlation with human judgement, respectively. This result demonstrates\nMetric\nAcc on Tabfact\nPearson\u2019s correlation\nSP-Acc\n63.5\n.458\nNLI-Acc\n65.1\n.526\nTAPAS-Acc\n81.0\n.705\nTAPEX-Acc\n84.2\n.804\nCoT-3.5-Acc\n78.0\n.787\nCoT-4-Acc\n80.9\n.816\nTable 4: System-level Pearson\u2019s correlation bettwen each automated evaluation metric and human judgement. We also report the accuracy of automated evaluation metrics on the TabFact dataset for reference.\nLLMs\u2019 capabilities in assessing the faithfulness of table-to-text generation. It\u2019s worth noting that although TAPAS-Acc and TAPEX-Acc perform better than CoT-4-Acc on the TabFact dataset, they exhibit lower correlation with human judgement on table-to-text evaluation. We suspect that this can be largely attributed to over-fitting on the TabFact dataset, where negative examples are created by rewriting from the positive examples. We believe that future work can explore the development of a more robust faithfulness-level metric with better alignment to human evaluation.\n# 4.3 RQ3: How can fine-tuned models benefit from LLMs\u2019 strong table-to-text abilities?\nIn RQ1 and RQ2, we demonstrate the strong capability of state-of-the-art LLMs in table-to-text generation and evaluation. We next explore how fine-tuned smaller models can benefit from these abilities. We believe such exploration can provide insights for future work regarding the distillation of text generation capabilities from LLMs to smaller models (Gao et al., 2023a; Scheurer et al., 2023; Madaan et al., 2023). This is essential as deploying smaller, yet performance-comparable models in real-world applications could save computational resources and inference time.\n# Generating Feedback for Improving Factual\nConsistency Utilizing human feedback to enhance neural models has emerged as a significant area of interest in contemporary research (Liu et al., 2022c; Gao et al., 2023a; Scheurer et al., 2023; Madaan et al., 2023). For example, Liu et al. (2022c) illustrates that human-written feedback can be leveraged to improve factual consistency of text summarization systems. Madaan et al. (2023) demonstrates that LLMs can improve their initial outputs through iterative feedback and refinement. This work investigates whether LLMs can provide\nModels\nTAPAS-Acc\nTAPEX-Acc\nGPT2-C2F\n46.2\n43.8\nEdit by LLaMA2-70b-chat\n58.0 (+11.8)\n50.0 (+6.2)\nEdit by GPT-3.5\n71.0 (+24.8)\n68.4 (+24.6)\nEdit by GPT-4\n81.0 (+34.8)\n82.0 (+38.2)\nR2D2\n60.2\n61.0\nEdit by LLaMA2-70b-chat\n65.0 (+4.8)\n60.0 (-1.0)\nEdit by GPT-3.5\n74.0 (+13.8)\n74.0 (+13.0)\nEdit by GPT-4\n87.0 (+26.8)\n89.0 (+28.0)\nPLOG\n63.8\n69.6\nEdit by LLaMA2-70b-chat\n75.0 (+11.2)\n66.0 (-3.6)\nEdit by GPT-3.5\n70.6 (+6.8)\n67.0 (-2.6)\nEdit by GPT-4\n91.0 (+27.2)\n86.0 (+16.4)\nLOFT\n67.4\n61.4\nEdit by LLaMA2-70b-chat\n72.0 (+4.6)\n64.0 (+2.6)\nEdit by GPT-3.5\n70.0 (+2.6)\n65.6 (+4.2)\nEdit by GPT-4\n81.0 (+13.6)\n86.0 (+24.6)\nTable 5: Automated evaluation results on LOGICNLG using statements pre-edited and post-edited by LLMs.\nhuman-like feedback for outputs from fine-tuned models. Following Liu et al. (2022c), we consider generating feedback with three components: 1) Explanation, which determine whether the initial statement is factually consistent with the given table; 2) Corrective Instruction, which provide instructions on how to correct the initial statement if it is detected as unfaithful; and 3) Edited Statement, which edits the initial statement following the corrective instruction. Figure 8 in Appendix shows an example of 2-shot chain-of-thought prompts we use for feedback generation.\nassess the quality of generated feedback through automated evaluations. Specifically, we examine the faithfulness scores of Edited Statements in the generated feedback, comparing these scores to those of the original statements. We report TAPAS-Acc and TAPEX-Acc for experimental results, as these two metrics exhibit better alignment with human evaluation (Section 4.2). As illustrated in Table 5, LLMs can effectively edit statements to improve their faithfulness, particularly for outputs from lowerperformance models, such as GPT2-C2F.\n# 5 Related Work\nTable-to-Text Generation Text generation from semi-structured knowledge sources, such as web tables, has been studied extensively in recent years (Parikh et al., 2020; Chen et al., 2020a; Cheng et al., 2022; Zhao et al., 2023a). The goal of the table-to-text generation task is to generate natural\nlanguage statements that faithfully describe information contained in the provided table region. The most popular approach for table-to-text generation tasks is to fine-tune a pre-trained language model on a task-specific dataset (Chen et al., 2020a; Liu et al., 2022a; Zhao et al., 2022; Nan et al., 2022a; Zhao et al., 2023b). To the best of our knowledge, we are the first to systematically evaluate the performance of LLMs on table-to-text generation tasks. Large Language Models LLMs have demonstrated remarkable in-context learning capabilities (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022; Chung et al., 2022; OpenAI, 2023), where the model receives a task demonstration in natural language accompanied by a limited number of examples. The Chain-of-Thought prompting methods (Wei et al., 2022; Wang et al., 2022) further empower LLMs to perform complex reasoning tasks (Han et al., 2022; Zhao et al., 2023c; Ye et al., 2023; Chen, 2023). More recent works (Chen, 2023; Nan et al., 2023) investigate in-context learning capabilities of LLMs on tablebased tasks, including table question answering (Pasupat and Liang, 2015; Iyyer et al., 2017; Zhong et al., 2018) and table fact checking (Chen et al., 2020b; Gupta et al., 2020). However, the potential of LLMs in generating text from tabular data remains underexplored.\n# 6 Conclusion\nThis paper investigates the potential of applying LLMs in real-world table information seeking scenarios. We demonstrate their superiority in faithfulness, and their potential as evaluation systems. Further, we provide valuable insights into leveraging LLMs to generate high-fidelity natural language feedback. We believe that the findings of this study could benefit real-world applications, aimed at improving user efficiency in data analysis.\n# Ethical Consideration\nLOTNLG and F2WTQ were constructed upon the test set of LOGICNLG (Chen et al., 2020a) and WTQ (Pasupat and Liang, 2015) datasets, which are publicly available under the licenses of MIT1 and CC BY-SA 4.02, respectively. These licenses permit us to modify, publish, and distribute additional annotations upon the original dataset.\n1https://opensource.org/licenses/MIT 2https://creativecommons.org/licenses/ by-sa/4.0/\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc. Wenhu Chen. 2023. Large language models are few(1)shot table reasoners. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1120\u20131130, Dubrovnik, Croatia. Association for Computational Linguistics. Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a. Logical natural language generation from open-domain tables. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7929\u2013 7942, Online. Association for Computational Linguistics. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020b. Tabfact: A large-scale dataset for table-based fact verification. In International Conference on Learning Representations. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. 2022. HiTab: A hierarchical table dataset for question answering and natural language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1094\u20131110, Dublin, Ireland. Association for Computational Linguistics. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311. Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, et al. 2022. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling.\nGe Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi. 2023a. Continually improving extractive qa via human feedback. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023b. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554. Carlos Gemmell and Jeffrey Stephen Dalton. 2023. Generate, transform, answer: Question specific tool synthesis for tabular data. ArXiv, abs/2303.10138. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356. Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020. INFOTABS: Inference on tables as semi-structured data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2309\u20132324, Online. Association for Computational Linguistics.\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir R. Radev. 2022. Folio: Natural language reasoning with first-order logic. ArXiv, abs/2209.00840. Jonathan Herzig, Pawel Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4320\u20134333, Online. Association for Computational Linguistics. Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1821\u2013 1831, Vancouver, Canada. Association for Computational Linguistics. Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and Weizhu Chen. 2022. OmniTab: Pretraining with natural and synthetic data for few-shot tablebased question answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 932\u2013942, Seattle, United States. Association for Computational Linguistics. Abdullatif K\u00f6ksal, Timo Schick, Anna Korhonen, and Hinrich Sch\u00fctze. 2023. Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\nHongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. 2023. Sheetcopilot: Bringing software productivity to the next level through large language models. ArXiv, abs/2305.19308.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.\nLinyong Nan, Lorenzo Jaime Flores, Yilun Zhao, Yixin Liu, Luke Benson, Weijin Zou, and Dragomir Radev. 2022a. R2D2: Robust data-to-text with replacement detection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6903\u20136917, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kry\u00b4sci\u00b4nski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. 2022b. FeTaQA: Free-form table question answering. Transactions of the Association for Computational Linguistics, 10:35\u201349.\ninyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Opendomain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 432\u2013447, Online. Association for Computational Linguistics.\nLinyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, and Dragomir Radev. 2023. Enhancing few-shot text-tosql capabilities of large language models: A study on prompt design strategies.\nOpenAI. 2023. Gpt-4 technical report. ArXiv abs/2303.08774.\nPanupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1470\u2013 1480, Beijing, China. Association for Computational Linguistics.\n# Yotam Perlitz, Liat Ein-Dor, Dafna Sheinwald, Noam Slonim, and Michal Shmueli-Scheuer. 2022. Diversity enhanced table-to-text generation via type control. ArXiv, abs/2205.10938.\nYotam Perlitz, Liat Ein-Dor, Dafna Sheinwald, Noam Slonim, and Michal Shmueli-Scheuer. 2022. Diversity enhanced table-to-text generation via type control. ArXiv, abs/2205.10938.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/111e/111e0776-abec-4ef1-be6d-2acacc1efbb3.png\" style=\"width: 50%;\"></div>\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2022. Selfconsistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. How far can camels go? exploring the state of instruction tuning on open resources.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. ArXiv, abs/2301.13808. Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao. 2023. Tablegpt: Towards unifying tables, nature language and commands into one gpt. Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yue Ting Zhuang. 2023. Data-copilot: Bridging billions of data and humans with autonomous workflow. ArXiv, abs/2306.07209. Yusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, and Rui Zhang. 2022. Macsum: Controllable summarization with mixed attributes. arXiv preprint arXiv:2211.05041. Yilun Zhao, Boyu Mi, Zhenting Qi, Linyong Nan, Minghao Guo, Arman Cohan, and Dragomir Radev. 2023a. OpenRT: An open-source framework for reasoning over tabular data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 336\u2013347, Toronto, Canada. Association for Computational Linguistics. Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, and Dragomir Radev. 2022. ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9006\u20139018, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yilun Zhao, Zhenting Qi, Linyong Nan, Lorenzo Jaime Flores, and Dragomir Radev. 2023b. LoFT: Enhancing faithfulness and diversity for table-to-text generation via logic form control. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 554\u2013 561, Dubrovnik, Croatia. Association for Computational Linguistics. Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, and Dragomir Radev. 2023c. Qtsumm: A new benchmark for query-focused table summarization. Victor Zhong, Caiming Xiong, and Richard Socher. 2018. Seq2SQL: Generating structured queries from natural language using reinforcement learning. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. arXiv preprint arXiv:2303.11315.\n# A Table-to-Text Generation Benchmarks A.1 LOTNLG Dataset Logical Reasoning Type Definition\n# A.1 LOTNLG Dataset Logical Reasoning Type Definition\n\u2022 Aggregation: operations involving sum or average operation to summarize the overall statistics. Sentence: The total number of scores of xxx is xxx. The average value of xxx is xxx. \u2022 Negation: operations to negate. Sentence: xxx did not get the first prize. \u2022 Superlative: superlative operations to get the highest or lowest value. Sentence: xxx achieved the most scores. \u2022 Count: operations to count the amount of entities that fulfil certain conditions. Sentence: There are 4 people born in xxx. \u2022 Comparative: operations to compare a specific aspect of two or more entities. Sentence: xxx is taller than xxx. \u2022 Ordinal: operations to identify the ranking of entities in a specific aspect. Sentence: xxx is the third youngest player in the game. \u2022 Unique: operations to identify different entities. Sentence: The players come from 7 different cities. \u2022 All: operations to summarize what all entities do/have in common. Sentence: All of the xxx are more expensive than $25. \u2022 Surface-Level: no logical reasoning type above. Sentence: xxx is moving to xxx.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae2a/ae2a2e4f-b6fc-414b-b154-49a6d822ac0b.png\" style=\"width: 50%;\"></div>\nFigure 4: Distribution of logical reasoning types for the LOTNLG dataset.\n# B Examined Systems B.1 Fine-tuned Models\n# B Examined Systems\n# B.1 Fine-tuned Models\n\u2022 BART (Lewis et al., 2020) is a pre-trained denoising autoencoder with transformer-based architecture and shows effectiveness in NLG tasks.\n\u2022 Flan-T5 (Chung et al., 2022) enhances T5 (Raffel et al., 2020) by scaling instruction fine-tuning and demonstrates better human-like reasoning abilities than the T5. \u2022 GPT2-C2F (Chen et al., 2020a) first generates a template which determines the global logical structure, and then produces the statement using the template as control. \u2022 R2D2 (Nan et al., 2022a) trains a generative language model both as a generator and a faithfulness discriminator with additional replacement detection and unlikelihood learning tasks, to enhance the faithfulness of table-to-text generation. \u2022 TAPEX (Liu et al., 2022b) continues pre-training the BART model by using a large-scale corpus of synthetic SQL query execution data, showing better table understanding and reasoning abilities. \u2022 OmniTab (Jiang et al., 2022) uses the same backbone as TAPEX, and is further pre-trained on collected natural and synthetic Table QA examples. \u2022 ReasTAP (Zhao et al., 2022) enhances the table understanding and reasoning abilities of BART by pre-training on a synthetic Table QA corpus. \u2022 PLOG (Liu et al., 2022a) continues pre-training text generation models on a table-to-logic-form generation task (i.e., T5 model), improving the faithfulness of table-to-text generation. \u2022 LOFT (Zhao et al., 2023b) utilizes logic forms as fact verifiers and content planners to control table-to-text generation, exhibiting improved faithfulness and text diversity.\n# B.2 Large Language Models\n Pythia (Biderman et al., 2023) is a suite of 16 open-sourced LLMs all trained on public data in the exact same order and ranging in size from 70M to 12B parameters. This helps researchers to gain a better understanding of LLMs and their training dynamics.\n\u2022 LLaMA (Touvron et al., 2023a,b) is an opensource LLM trained on large-scale and publicly available datasets. We evaluate both LLaMA and LLaMA2 in this paper.\n\u2022 Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) are fine-tuned from LLaMA with instruction-following data, exhibiting better instruction-following capabilities.\n\u2022 T\u00dcLU (Wang et al., 2023) further trains LLaMA on 12 open-source instruction datasets, achieving better performance than LLaMA.\n GPT (Brown et al., 2020; Wei et al., 2022) is a powerful large language model which is capable of generating human-like text and performing a wide range of NLP tasks in a few-shot setting. We use the OpenAI engines of gpt-3.5-0301 and gpt-4-0314 for GPT-3.5 and GPT-4 models, respectively.\nTo formulate the prompt, we linearize the table as done in previous work on table reasoning (Chen, 2023) and concatenate it with its corresponding reference statements as demonstrations. We use the table truncation strategy as proposed by Liu et al. (2022b) to truncate large table and ensure that the prompts are within the maximum token limitation for each type of LLMs. For LLM parameter settings, we used a temperature of 0.7, maximum output length of 512, without any frequency or presence penalty.\n# C Experiments\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e99/6e997990-d4ef-4b5e-a43d-8eb04a5b8653.png\" style=\"width: 50%;\"></div>\nExample 1:\nTitle: 1941 vfl season\nTable:\nhome team | home team score | away team | away team score | venue | crowd | date\nrichmond | 10.13 (73) | st kilda | 6.11 (47) | punt road oval | 6000 | 21 june 1941\nhawthorn | 6.8 (44) | melbourne | 12.12 (84) | glenferrie oval | 2000 | 21 june 1941\ncollingwood | 8.12 (60) | essendon | 7.10 (52) | victoria park | 6000 | 21 june 1941\ncarlton | 10.17 (77) | fitzroy | 12.13 (85) | princes park | 4000 | 21 june 1941\nsouth melbourne | 8.16 (64) | north melbourne | 6.6 (42) | lake oval | 5000 | 21 june 1941\ngeelong | 10.18 (78) | footscray | 13.15 (93) | kardinia park | 5000 | 21 june 1941\nFive generated statements:\n1.\nfootscray scored the most point of any team that played on 21 june, 1941.\n2.\ngeelong was the home team with the highest score.\n3.\nkardinia park was the one of the six venues that were put to use.\n4.\nnorth melbourne away team recorded an away score of 6.6 (42) while melbourne \nrecorded an away score of 12.12 (84).\n5.\nall six matches took place on 21 june 1941.\nExample 2:\nTitle: {title}\nTable: \n{table}\nFigure 5: An example of 1-shot direct-prediction prompting for the LOGICNLG task.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a950/a950be92-02ed-44cd-a11f-2bd3930f75fe.png\" style=\"width: 50%;\"></div>\nTitle: {title} Table:  {table}\nFigure 6: An example of 1-shot chain-of-thought prompting for the LOGICNLG task.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f7b3/f7b3b9fc-affb-442c-90ca-8d2de0ed358e.png\" style=\"width: 50%;\"></div>\nFigure 7: An example of 2-shot chain-of-thought prompting adopted from Chen (2023) for faithfulnesslevel automated evaluation.\nType\nModels\nSP-Acc\nNLI-Acc\nTAPAS-Acc\nTAPEX-Acc\nType EM\nType F1\n0-shot*\nGPT-3.5\n51.2\n77.2\n70.8\n66.8\n59.2\n43.8\nGPT-4\n69.2\n79.4\n85.6\n84.2\n75.2\n60.0\n1-shot Direct\nGPT-3.5\n53.8\n75.6\n71.6\n71.0\n51.2\n38.1\nGPT-4\n60.2\n72.8\n83.8\n84.2\n76.6\n63.0\n1-shot CoT\nGPT-3.5\n50.8\n78.8\n79.2\n79.4\n46.2\n30.2\nGPT-4\n59.2\n74.8\n84.4\n85.8\n70.0\n51.6\n2-shot Direct\nPythia-12b\n44.2\n60.6\n41.8\n43.0\n19.0\n12.2\nLLaMA-7b\n41.0\n62.2\n46.2\n46.2\n18.2\n13.4\nVicuna-13b\n48.6\n71.2\n57.4\n54.4\n22.0\n15.2\nLLaMA-13b\n44.6\n62.4\n50.8\n48.8\n22.6\n15.8\nAlpaca-13b\n46.2\n73.8\n50.8\n54.0\n21.8\n15.8\nLLaMA2-70b-chat\n44.2\n60.0\n56.0\n58.0\n24.2\n15.8\nLLaMA-30b\n40.0\n62.6\n53.0\n52.6\n24.2\n16.4\nLLaMA-65b\n46.2\n57.8\n54.0\n51.8\n21.0\n17.2\nT\u00dcLU -13b\n44.2\n72.8\n60.8\n56.8\n26.6\n17.4\nGPT-3.5\n55.2\n76.2\n70.8\n67.6\n52.2\n35.0\nGPT-4\n61.4\n72.2\n84.6\n83.2\n73.4\n54.8\n2-shot CoT\nPythia-12b\n42.0\n53.8\n41.2\n41.0\n15.2\n11.6\nLLaMA-30b\n41.0\n60.4\n52.6\n59.2\n20.4\n13.2\nLLaMA-7b\n37.6\n61.2\n43.8\n45.0\n17.2\n13.4\nLLaMA2-70b-chat\n48.2\n64.6\n56.0\n67.8\n20.2\n13.4\nLLaMA-13b\n45.0\n56.6\n51.2\n51.2\n18.8\n14.0\nLLaMA-65b\n45.2\n62.4\n59.4\n58.8\n21.2\n15.2\nVicuna-13b\n43.4\n72.0\n62.2\n61.0\n18.4\n16.0\nAlpaca-13b\n40.4\n71.6\n58.4\n57.8\n23.0\n16.2\nT\u00dcLU -13b\n45.8\n65.8\n60.8\n61.0\n23.2\n16.2\nGPT-3.5\n49.2\n74.4\n77.2\n75.4\n49.4\n35.0\nGPT-4\n59.2\n72.0\n85.6\n83.2\n67.6\n55.6\nTable 6: Faithfulness-level automated evaluation results on LOTNLG. We do not evaluate fine-tuned models a LOTNLG does not contain a training set. \u2217: It is challenging for other LLMs to follow the instructions in 0-sho prompt to generate a statement using the specified types of logical reasoning operations.\nType\nModels\nBLEU-1/2/3\nROUGE-1/2/L\nTAPAS-Acc\nTAPEX-Acc\nFine-tuned\nBART\n63.2/50.8/42.0\n67.6/46.0/57.2\n94.8\n68.8\nFlan-T5\n62.2/49.6/41.0\n66.8/45.0/56.2\n94.2\n69.2\nOmniTab\n63.4/50.8/41.8\n67.4/45.2/56.2\n94.6\n71.6\nReasTAP\n63.6/51.0/42.2\n67.6/45.8/57.2\n94.6\n71.4\nTAPEX\n63.6/50.8/42.0\n66.4/45.0/56.2\n96.2\n73.0\n0-shot\nGPT-3.5\n56.4/42.6/33.4\n60.6/38.0/49.4\n92.4\n72.8\nGPT-4\n52.4/40.2/31.8\n63.8/40.4/51.6\n94.0\n74.4\n1-shot Direct\nGPT-3.5\n56.8/43.2/34.2\n63.0/39.8/51.4\n91.8\n74.6\nGPT-4\n56.4/43.6/34.8\n66.2/43.0/54.4\n94.0\n73.8\n1-shot CoT\nGPT-3.5\n43.2/32.4/25.2\n57.4/35.8/46.8\n94.2\n67.0\nGPT-4\n59.6/45.8/36.4\n64.0/41.0/52.4\n91.0\n76.4\n2-shot Direct\nPythia-12b\n38.8/26.6/19.4\n43.2/22.6/35.2\n76.6\n35.0\nLLaMA-7b\n40.6/28.6/21.4\n48.2/26.6/39.0\n86.2\n47.8\nLLaMA-13b\n48.4/35.2/26.8\n51.0/29.4/42.2\n85.4\n57.4\nAlpaca-13b\n52.2/38.4/29.6\n56.4/33.6/46.2\n88.4\n57.4\nT\u00dcLU -13b\n50.6/37.4/29.0\n54.2/31.8/44.6\n86.4\n60.0\nLLaMA-30b\n50.4/37.0/28.2\n56.2/33.2/45.4\n87.0\n60.2\nVicuna-13b\n56.0/42.2/32.8\n59.0/36.2/48.0\n87.6\n62.4\nLLaMA-65b\n53.6/39.8/30.8\n57.0/34.0/46.6\n88.4\n63.0\nLLaMA2-70b-chat\n54.6/41.0/31.8\n58.4/35.8/47.8\n89.4\n66.2\nGPT-4\n55.0/42.8/34.6\n66.0/42.8/54.0\n95.2\n75.8\nGPT-3.5\n55.8/42.8/34.0\n63.2/40.0/51.6\n92.2\n76.0\n2-shot CoT\nPythia-12b\n38.8/25.4/17.8\n39.2/18.8/32.2\n69.0\n36.2\nLLaMA-7b\n33.0/22.2/16.0\n41.0/21.2/33.2\n77.6\n42.0\nLLaMA-13b\n43.2/30.4/22.6\n45.4/25.2/37.6\n82.0\n50.8\nAlpaca-13b\n47.4/34.4/26.2\n51.4/30.0/42.0\n82.8\n54.4\nT\u00dcLU -13b\n37.0/25.8/18.8\n43.6/24.0/35.2\n86.2\n55.8\nLLaMA-30b\n45.4/33.2/25.6\n52.4/30.8/42.2\n86.2\n63.6\nVicuna-13b\n50.4/37.6/29.4\n53.8/32.4/44.6\n85.6\n65.8\nLLaMA-65b\n50.2/37.0/28.4\n54.8/32.8/44.6\n87.8\n66.0\nLLaMA2-70b-chat\n53.8/40.2/31.4\n57.4/34.8/47.0\n89.2\n66.2\nGPT-3.5\n50.8/38.8/30.8\n60.6/38.2/49.0\n92.8\n70.8\nGPT-4\n62.2/48.6/39.2\n65.8/42.8/54.4\n91.2\n79.2\nTable 7: Automated evaluation results on the FeTaQA dataset.\nType\nModels\nBLEU-1/2/3\nROUGE-1/2/L\nTAPAS-Acc\nTAPEX-Acc\nAccuracy\n0-shot\nGPT-3.5\n63.2/49.2/39.4\n64.4/40.0/56.4\n73.0\n74.6\n54.0\nGPT-4\n60.6/46.8/37.4\n64.6/40.4/54.8\n78.6\n80.6\n62.4\n1-shot Direct\nGPT-3.5\n62.0/48.4/39.0\n64.0/40.0/56.8\n75.0\n73.2\n51.8\nGPT-4\n63.2/49.8/40.4\n66.2/42.6/58.0\n78.4\n79.0\n66.0\n1-shot CoT\nGPT-3.5\n55.0/42.4/33.8\n62.8/39.0/54.8\n72.4\n72.2\n55.2\nGPT-4\n62.2/49.0/39.6\n66.2/42.2/58.4\n78.2\n78.6\n69.8\n2-shot Direct\nPythia-12b\n12.4/7.6/5.2\n19.6/9.2/17.4\n74.6\n62.4\n7.8\nLLaMA-7b\n14.4/9.6/6.8\n26.2/13.4/23.0\n71.8\n53.0\n19.0\nLLaMA-13b\n7.6/4.8/3.4\n20.2/10.4/18.2\n78.4\n56.0\n21.4\nVicuna-13b\n43.0/31.6/24.4\n46.0/27.2/40.6\n74.6\n64.2\n30.2\nAlpaca-13b\n40.8/29.2/21.6\n46.6/26.2/40.4\n71.8\n57.6\n31.2\nLLaMA-30b\n34.0/24.4/18.2\n44.6/25.0/39.8\n74.0\n61.0\n31.8\nT\u00dcLU -13b\n49.6/36.4/28.0\n51.4/29.4/45.8\n78.8\n60.4\n33.8\nLLaMA-65b\n45.8/33.8/26.0\n48.8/28.2/43.6\n73.6\n64.4\n36.2\nLLaMA2-70b-chat\n51.2/38.4/30.0\n50.4/29.6/45.4\n72.4\n68.4\n37.6\nGPT-3.5\n63.4/49.8/40.2\n64.8/40.8/57.2\n74.8\n73.6\n51.8\nGPT-4\n62.8/49.2/39.6\n65.8/41.8/57.6\n78.6\n81.4\n63.6\n2-shot CoT\nPythia-12b\n27.2/18.0/12.8\n35.6/17.4/31.4\n66.0\n48.8\n15.8\nLLaMA-7b\n13.2/8.4/5.8\n28.0/13.2/24.0\n73.4\n47.8\n24.2\nLLaMA-13b\n22.2/14.8/10.4\n35.2/18.0/31.4\n74.0\n56.2\n26.2\nAlpaca-13b\n33.2/23.6/17.8\n47.6/26.4/41.2\n75.0\n55.4\n32.2\nLLaMA-30b\n37.4/26.2/19.6\n46.2/24.8/40.6\n72.6\n60.0\n35.6\nT\u00dcLU -13b\n25.8/17.0/12.0\n35.4/17.4/31.0\n79.0\n65.6\n35.8\nVicuna-13b\n45.2/33.2/25.4\n53.6/31.2/47.6\n75.6\n62.2\n38.6\nLLaMA-65b\n51.2/37.8/29.0\n51.6/29.4/45.6\n75.6\n67.6\n41.6\nLLaMA2-70b-chat\n46.2/34.2/26.6\n49.6/28.8/44.2\n75.8\n66.6\n43.2\nGPT-3.5\n57.4/44.4/35.4\n64.0/40.0/55.4\n73.6\n72.8\n58.6\nGPT-4\n63.0/49.6/40.0\n66.2/42.4/58.8\n76.4\n79.6\n68.4\nTable 8: Automated evaluation results on the F2WTQ dataset. We do not evaluate fine-tuned models as F2WTQ does not contain a training set.\n[INSTRUCTION] Your task is to provide feedback on statements derived from tables. Your feedback should  consist of  1) Explanation, which determine whether the initial statement is factually consistent with the given  table; 2) Corrective Instruction, which provide instructions on how to correct the initial statement if it is detected  as unfaithful; and 3) Edited Statement, which edits the initial statement following the corrective instruction.  There are two types of errors: intrinsic and extrinsic. Intrinsic errors refer to mistakes that arise from within the  statement itself, while extrinsic errors are caused by factors external to the statement. To help you provide  accurate feedback, we have provided instruction templates for your use. These templates include \"remove,\"  \"add,\" \"replace,\" \"modify,\" \"rewrite,\" and \"do nothing\".  It is important to note that you should be capable of identifying logical operations when reviewing statements.  Examples of such operations include superlatives, exclusives (such as \"only\"), temporal relationships (such as  \"before/after\"), quantitative terms (such as \"count\" or \"comparison\"), inclusive/exclusive terms (such as  \"both/neither\"), and arithmetic operations (such as \"sum/difference\" or \"average\"). To guide your responses, we have provided two examples with three statements each. Use these templates to  structure your answer, provide reasoning for your feedback, and suggest improved statements. We encourage  you to think through each step of the process carefully. Remember, your final output should always include a \u201cEdited Statement\u201d no matter if there is error or not. Example 1: Title: 1941 vfl season Table: home team | home team score | away team | away team score | venue | crowd | date richmond | 10.13 (73) | st kilda | 6.11 (47) | punt road oval | 6000 | 21 june 1941 hawthorn | 6.8 (44) | melbourne | 12.12 (84) | glenferrie oval | 2000 | 21 june 1941 collingwood | 8.12 (60) | essendon | 7.10 (52) | victoria park | 6000 | 21 june 1941 carlton | 10.17 (77) | fitzroy | 12.13 (85) | princes park | 4000 | 21 june 1941 south melbourne | 8.16 (64) | north melbourne | 6.6 (42) | lake oval | 5000 | 21 june 1941 geelong | 10.18 (78) | footscray | 13.15 (93) | kardinia park | 5000 | 21 june 1941 Statement: st kilda scored the most point of any team that played on 21 june, 1941 Explanation: footscray scored the most point of any team that played on 21 june, not st kilda. So the statement  has instrinsic error.  Corrective Instruction: replace st kilda with footscray. Edited Statement: footscray scored the most point of any team that played on 21 june, 1941. Example 2: (...abbreviate\u2026) Now please give feedback to the statement of the new table. Let's think step by step and follow the given  example. Remember to include \u201cExplanation\u201d, \u201cCorrective Instruction\u201d, and \u201cEdited Statement\u201d parts in the  output. Title: {title} Table:  {table} Statement: {sent}\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0946/0946a7c2-a31c-4944-bc33-506eed9a94d5.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The need for intuitive, user-friendly interpretations of structured data has become paramount as users interact with vast amounts of tabular information for decision-making and information-seeking purposes. Current table-to-text generation techniques are often task-specific, limiting their adaptability to real-world applications, necessitating a more versatile approach.",
            "purpose of benchmark": "The benchmark aims to evaluate the table-to-text generation capabilities of large language models (LLMs) in real-world information-seeking scenarios, facilitating comparisons between various models and enhancing the understanding of their performance in generating coherent and faithful narratives from tabular data."
        },
        "problem": {
            "definition": "The benchmark is designed to address the challenges of generating meaningful insights from tables and accurately answering user queries based on tabular data, simulating real-world information-seeking tasks.",
            "key obstacle": "Existing benchmarks are often limited in their scope and do not adequately capture the diversity of logical reasoning operations required for effective data insight generation, leading to a significant performance gap between high-performing models like GPT-4 and other open-source LLMs."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the observation that current models struggle to generate diverse insights from tabular data, often relying on similar logical reasoning operations, which does not meet the expectations of users seeking varied perspectives.",
            "opinion": "The authors believe that the benchmark is crucial for advancing the field of table-to-text generation, as it highlights the capabilities of LLMs and their potential to improve user efficiency in data analysis.",
            "innovation": "The benchmark introduces new datasets (LOTNLG and F2WTQ) that condition generation on specific logical reasoning types and require human-like reasoning for complex queries, thereby offering improvements over previous benchmarks that lacked this level of detail.",
            "benchmark abbreviation": "LOTNLG"
        },
        "dataset": {
            "source": "The datasets were constructed from existing datasets (LOGICNLG and WTQ) with additional annotations to enhance their applicability in real-world scenarios.",
            "desc": "The LOTNLG dataset contains 862 instances, while the F2WTQ dataset has 4,344 instances, both designed to evaluate LLMs' performance in generating insights and answering queries based on tabular data.",
            "content": "The datasets include text generated from tables, focusing on data insight generation and query-based generation tasks.",
            "size": "5,206",
            "domain": "Data Insight Generation",
            "task format": "Query-based Generation"
        },
        "metrics": {
            "metric name": "TAPEX-Acc, BLEU",
            "aspect": "Accuracy and fluency of generated narratives.",
            "principle": "The metrics were selected based on their ability to evaluate the coherence and factual consistency of the generated text against reference outputs.",
            "procedure": "Model performance is evaluated using automated metrics like TAPEX-Acc and BLEU, as well as human evaluations based on faithfulness and fluency criteria."
        },
        "experiments": {
            "model": "The experiments involved testing state-of-the-art models including GPT-3.5 and GPT-4, as well as various open-source LLMs like LLaMA and T\u00dcLU.",
            "procedure": "Models were evaluated in both zero-shot and few-shot settings, utilizing different prompting strategies to assess their table-to-text generation capabilities.",
            "result": "GPT-4 consistently outperformed other models in generating coherent and faithful text from tables, while LLaMA-based models showed comparable performance in certain settings.",
            "variability": "Variability in results was accounted for through multiple trials and evaluation across different datasets, ensuring comprehensive performance assessment."
        },
        "conclusion": "The findings demonstrate the potential of LLMs in enhancing table-to-text generation for real-world information-seeking scenarios, with implications for improving user efficiency in data analysis.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by providing a structured evaluation framework for assessing LLMs' capabilities in generating insights from tabular data, facilitating comparisons and advancements in model performance.",
            "limitation": "One limitation is that the benchmark may not cover all possible real-world scenarios, and the performance of models may vary significantly based on specific datasets and tasks.",
            "future work": "Future research could explore the development of more robust evaluation metrics and expand the benchmark to include additional datasets and tasks reflecting diverse real-world applications."
        },
        "other info": {
            "data availability": "The datasets and code are publicly available at https://github.com/yale-nlp/LLM-T2T."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The need for intuitive, user-friendly interpretations of structured data has become paramount as users interact with vast amounts of tabular information for decision-making and information-seeking purposes."
        },
        {
            "section number": "2.3",
            "key information": "The benchmark aims to evaluate the table-to-text generation capabilities of large language models (LLMs) in real-world information-seeking scenarios, facilitating comparisons between various models and enhancing the understanding of their performance in generating coherent and faithful narratives from tabular data."
        },
        {
            "section number": "4.1",
            "key information": "The findings demonstrate the potential of LLMs in enhancing table-to-text generation for real-world information-seeking scenarios, with implications for improving user efficiency in data analysis."
        },
        {
            "section number": "8.1",
            "key information": "The benchmark introduces new datasets (LOTNLG and F2WTQ) that condition generation on specific logical reasoning types and require human-like reasoning for complex queries, thereby offering improvements over previous benchmarks that lacked this level of detail."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore the development of more robust evaluation metrics and expand the benchmark to include additional datasets and tasks reflecting diverse real-world applications."
        }
    ],
    "similarity_score": 0.7419990749631837,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios.json"
}