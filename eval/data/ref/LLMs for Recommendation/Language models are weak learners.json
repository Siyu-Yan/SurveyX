{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2306.14101",
    "title": "Language models are weak learners",
    "abstract": "A central notion in practical and theoretical machine learning is that of a weak learner, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting. The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for tasks involving small numbers of data points. The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning pipelines.",
    "bib_name": "manikandan2023languagemodelsweaklearners",
    "md_text": "# Language models are weak learners\nHariharan Manikandan1 Yiding Jiang1 J Zico Kolter1,2 1Carnegie Mellon University 2Bosch Center for AI {hmanikan, yidingji, zkolter}@cs.cmu.edu\n# Abstract\nA central notion in practical and theoretical machine learning is that of a weak learner, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting. The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for tasks involving small numbers of data points. The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning pipelines.\n# 1 Introduction\nWeak learners refer to classifiers that are able to attain better performance than random chance, by some given margin, on any specified distribution over training data. One of the early breakthroughs in machine learning established that this weak learning was sufficient for arbitrarily strong classification, via an ensembling procedure [Schapire, 1990]. This approach in turn led to the development of boosting algorithms [Freund and Schapire, 1997], a class of approaches that continue to perform extremely well, particularly on tabular datasets that lack the input space regularity of vision or language tasks. In a seemingly separate thread of research, large language models (LLMs) based on transformers [Vaswani et al., 2017] in recent years have come to dominate many natural language domains. These models are often finetuned on the data of new downstream tasks [Devlin et al., 2018, Liu et al., 2019], but in recent years have also been shown to exhibit strong performance as zero-shot or few-shot learning solely via prompting the model [Brown et al., 2020] with a piece of context string. In this paper, we align these two threads of research and ask a simple question: can LLMs also serve as weak learners in a boosting framework, specifically on tabular data (where boosting methods are most commonly applied)? We answer this question largely in the affirmative. Specifically, we show that by appropriately converting tabular data to text form, and asking LLMs to summarize a carefully chosen set of examples from the data, we produce a summary of the examples that can serve as a template (i.e., a prompt) for a tabular data classifier, and one which typically achieves this weak learning aim. This enables us to correspondingly integrate this collection of LLM-generated weak learners into a boosting framework.\nWe show that the resulting approach performs well in many settings, easily outperforming zero-shot and few-shot classification, as well as \u201csingle-shot\u201d summaries generated by the LLM. This is all done without any retraining or finetuning of the LLM itself, but rather only via prompting. Furthermore, on certain domains (particularly those with very few examples, where leveraging the prior knowledge built into LLMs would be of particular importance), we show that the approach can even outperform traditional tree-based boosting and LLM-based finetuning methods and its performance would likely improve as LLMs capabilities improve. Overall, we believe this work highlights the potential of incorporating LLMs as sub-routines of a larger machine learning system.\n# 2 Related Works\nDeep Learning for Tabular Data. Tabular data refers to a generic data format that represents data as a collection of discrete or continuous attributes [Borisov et al., 2021]. Due to their flexibility, tabular data are ubiquitous in many ML settings. However, such flexibility comes with a cost \u2013 they lack the inherent structure found in images or text, which makes applying deep learning to them challenging. Furthermore, they are often domain-specific and may have a relatively small number of data points. As a result, traditional deep learning methods, which thrive on large datasets and high-dimensional data, have seen limited success when applied to tabular data [Gorishniy et al., 2021, Shwartz-Ziv and Armon, 2022]. Recently, however, there has been increasing interest in applying deep learning to tasks related to tables such as data integration, imputation [Narayan et al., 2022], semantic parsing, and even running SQL queries [Herzig et al., 2020, Yin et al., 2020]. Deep learning models have also been successful at learning tabular data classification by optimizing loss functions [Hollmann et al., 2022, Sch\u00e4fl et al., 2022, Dinh et al., 2022]. Unlike these approaches, we study how we can use LLM for classifying tabular data without finetuning or building a new language model. Since many tabular data can be grounded in natural language, texts are in fact a natural representation for tabular data. Motivated by the observation that LLMs can convert tables to text through prompting alone [Saha et al., 2022], we utilize LLMs to do this conversion. After the conversion, our classification algorithm also interacts with existing LLMs strictly through prompts. This creates an abstraction between the underlying language model and the learning procedure which may be desirable for various applications since access to the gradients or parameter updates are not required. Prompting Prompting [Liu et al., 2023] refers to providing initial text or instructions to guide the response of a language model. The advancements in Language Model-based Learning (LLM) have unveiled new capabilities, such as chain of thought reasoning [Wei et al., 2022], zero-shot reasoning [Kojima et al., 2022], compositional problem solving [Zhou et al., 2022a], and selfimprovement [Huang et al., 2022, Ho et al., 2022, Haluptzok et al., 2022]. As a result, prompting has gained widespread application across various Natural Language Processing (NLP) tasks, including arithmetic, common reasoning [Wang et al., 2022b], among others [Brown et al., 2020]. While prompts offer flexibility, it is crucial to note that LLMs interpret them differently from humans. Therefore, the process of prompt tuning, which involves carefully engineering prompts, becomes essential for obtaining accurate and relevant outputs [Reynolds and McDonell, 2021]. At its core, prompt tuning is an optimization process that aims to find the best prompt for a certain downstream task. Though a long line of works propose gradient-guided search to optimize \u201ccontinuous prompt\u201d instead of the language tokens [Liu et al., 2021, Qin and Eisner, 2021, Lester et al., 2021, Shin et al., 2020, Rakotonirina et al., 2023, Wang et al., 2022c, Diao et al., 2023], gradient-based updates can be limiting, as LLMs become bigger and the access to these models become increasingly API-based. Our approach aligns more with discrete search methods based on the fact that LLMs can automatically generate prompts for themselves [Zhou et al., 2022b, Zhang et al., 2022, Yu et al., 2022]. Specifically, we prompt the LLM to summarize the tabular dataset. The summary in turn acts as a prompt that the LLM uses to make predictions as it encodes knowledge of the dataset. A sequence of such prompts summarizing different subsets of the data can be seen as weak learners for a boosting procedure. Boosting Boosting [Schapire, 1990, Freund and Schapire, 1997] is a widely used technique to improve the accuracy of a model by combining weak learners (models that perform slightly better than random guessing) to make a strong learner (model with high accuracy). Common boosting algorithms include AdaBoost [Freund and Schapire, 1997], Gradient Boosting [Friedman, 2001], and Stochastic Gradient Boosting [Friedman, 2002]; the XGBoost library [Chen and Guestrin, 2016] in particular is a commonly used implementation of gradient boosting. In concurrent work most relevant\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2792/27925f3a-9db5-460a-98ec-9ade8b59c271.png\" style=\"width: 50%;\"></div>\nto ours, Hou et al. [2022] integrate LLM into AdaBoost for natural language inference (NLI) tasks, by training an MLP projection of the final hidden state of a special token. Our proposed method diverges from theirs in two key aspects. Firstly, our method avoids the overhead of learning additional parameters, is gradient-free, and does not require access to the model\u2019s internal states. Secondly, instead of storing knowledge in parameters, our approach concentrates on condensing knowledge into an intermediary representation referred to as \"summary.\" This alternative strategy enhances interpretability and strictly learns through prompts, rendering it particularly suitable for small tabular data, where the prior knowledge in LLM can significantly benefit the learning process.\n# 3 Summary Boosting with Language Models\nWe now describe the main methodology of our paper, which uses LLMs to generate weak learners, and in turn, uses these weak learners within a boosting framework. We refer to the full method as Summary Boosting, as the core learning process is one that uses a language model to create a summary of (specifically chosen) samples from the dataset; these summaries themselves function as prompts by which we can make predictions on new examples. Finally, we use boosting to construct an ensemble of these summaries that gives the overall predictions on new data points.\n# 3.1 Data conversion\nTo utilize large language models (LLMs) with tabular data, it is necessary to first convert the records into natural language descriptions. We will refer to these as data descriptions. Template matching, commonly used in previous approaches [Dinh et al., 2022], inserts attribute values into predefined templates. However, this approach often produces unnatural descriptions that differ from how humans might describe the data. Depending on the dataset, designing the template by hand can also be challenging. To overcome this, we propose using LLMs as a more suitable solution. As illustrated in Figure 1, we can get these data descriptions with little effort by zero-shot prompting the LLM with information about the dataset (which is generally available as metadata for tabular datasets) and a textual representation of the tabular record (e.g., parsed JSON). Specifically, to ensure examples can serve as both training data and query inputs, we extract the descriptions of the features and concatenate them with the target label using a separator token (refer to Appendix A.1). Interestingly, we find that descriptions generated by LLM this way often perform better than those from a template. This ablation study can be found in Section 5. One key challenge in this process is how to encode numerical attributes effectively; naively including numerical values in the descriptions can lead to poor performance in subsequent learning tasks. To address this, we adopt a straightforward approach: we bin all numerical features into percentiles and encode them descriptively as \u201clow,\u201d \u201cmedium,\u201d and \u201chigh,\u201d. In Section 5, we compare the performance of several such approaches and discuss more examples in Appendix A.6. Overall, the data descriptions can be generated automatically with minimal manual engineering.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c9c9/c9c92f1b-73d1-48b5-b102-2f709a3be5f0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The process of generating summaries and using them to make predictions on new data The top half describes how the weak learning hypothesis (summary) is generated. The bottom half illustrates how the summary is used to perform inference.</div>\n# 3.2 Weak learning via summarization\nA typical method for performing few-shot learning with large language models (LLMs) involves providing a small number of demonstrations of the intended task as a prompt and then asking the model to generate an answer. One could, for instance in the few-shot setting, simply present the natural language descriptions above and generate predictions on new examples. However, for tabular data, there may be a larger number of data points that do not fit within the LLM context. Furthermore, we observed that increasing the number of examples in the context naively does not always improve performance (Figure 4, right bottom), and there was no obvious way to manage weighted distributions over examples as is required in boosting methods. These observations necessitate alternative approaches to weak learning via LLMs. We propose instead that producing summaries of a collection of examples can serve as a powerful proxy for learning models based upon some number of examples, as summarization naturally encourages extraction of representative information in the data. Concretely, given a set of data descriptions, we first perform summarization on the data by calling the LLM (e.g., by concatenating a list of examples in natural language form and appending the prompt \u201ctldr\u201d). This resulting summary can be seen as a hypothesis as it provides an explanation for the data. By using the summary as a prompt, the LLM in turn uses the hypothesis to perform inference instead of the raw data description (shown in Figure 2). Since the sampled summary can sometimes be noisy, we generate a fixed number of summaries and pick the one with the smallest validation error rate. In case of a tie, we choose the one with a higher training error, i.e., lower generalization gap (see Appendix A.2). Several methods of building such summaries are possible, but simple approaches such as the \u201ctldr\u201d approach mentioned above tend to work as well as more sophisticated alternatives, as we show in Section 5.\n(Weighted) Cluster Sampling. Since the context size of existing LLMs is still limited, we cannot in general fit the entire dataset into the context for summarization. Furthermore, boosting algorithms require that we provide weak learners on weighted samples of the training set, effectively guiding the boosting process to focus on \u201charder\u201d examples as the boosting process continued. Thus, instead of attempting to summarize the entire dataset, we propose to use only a representative subset of the dataset. The size of this subset is governed by the maximum context size and size of the data descriptions. To select this representative subset, we use weighted stratified sampling using subpopulations defined by clusters of language embeddings of each data description. The language embeddings are sentence representations generated by GPT-3. In particular, we use hierarchical agglomerative clustering [Nielsen, 2016] to identify clusters in the embedding. This process is shown in Algorithm 1. As we will show in Section 5, this process is able to consistently produce weak learners, and able to improve upon random guessing under the distribution of interest (denoted by the input p to the algorithm). We share more details in Appendix A.7.\nAlgorithm 1 Cluster Sampling\n1: Input: X, all training data; y, all training label; r, ratio of classes; p, AdaBoost weights of the\ncurrent round; s, target number of samples.\n\u25b7r[k] is the proportion of examples in class k.\n2: S \u2190new empty set\n3: w \u2190new array with same length as X filled with -1. \u25b7w[i] is probability of sampling example i.\n4: for k = 1 to number of target classes in y do\n5:\nE \u2190GPTEmbedding(X[y == k])\n\u25b7E refers to the embeddings of the data descriptions\n6:\nC \u2190AgglomerativeClustering(E).\n\u25b7Cj is set of data indices present in the jth cluster.\n7:\nc \u2190new empty array same size as C.\n\u25b7c[j] will store sampling probability of cluster j.\n8:\nfor j = 1 to len(C) do\n9:\nc[j] \u2190\nlen(X)\nlen(Cj)\n10:\nend for\n11:\nfor i = 1 to len(X) do\n12:\nw[i] \u2190c[j], such that, i \u2208Cj\n13:\nend for\n14:\nw \u2190Normalize(Normalize(w) \u00d7 p) \u25b7Normalize turns weights to a probability distribution.\n15:\nSample s \u00d7 r[c] examples from X using categorical distribution w and append to S.\n16: end for\n17: Return S\nAlgorithm 2 Summary Boosting [Compact]\n1: Input: X, all training data; y, all training label; T: maximum number of rounds; s: size of the\nsampling subset; r: ratio of classes.\n\u25b7r[k] denotes the proportion of examples in class k.\n2: h, \u03f5, \u03b1 \u2190new empty arrays of length T.\n3: N \u2190len(X); K \u2190number of target classes in y.\n4: w \u2190new array of length N filled with 1\nN.\n\u25b7w is the data distribution\n5: for r = 1 to T do\n6:\n(Xs, ys) \u2190ClusterSampling(X, y, r, w, s) \u25b7sample s training examples from distribution w.\n7:\nh[r] \u2190Summary(Xs, ys)\n\u25b7h[r] is the weak learner in the current round r.\n8:\n\u03f5[r] \u2190\n\ufffdN\ni=1 w[i]\u00d71{h[r](X[i])\u0338=y[i]}\n\ufffdN\ni=1 w[i]\n\u25b7\u03f5[r] is the weighted error at round r.\n9:\nif \u03f5[r] \u22651 \u22121\nK then\n10:\nGoto Step 6.\n11:\nend if\n12:\n\u03b1[r] \u2190log\n\ufffd1\u2212\u03f5[r]\n\u03f5[r]\n\ufffd\n+ log(K \u22121) \u25b7\u03b1[r] refers to coefficient of the hypothesis at round r.\n13:\nfor i = 1 to N do\n14:\nw[i] = w[i] \u00d7 exp(\u03b1[r]1{h[r](X[i]) \u0338= y[i]})\n15:\nend for\n16:\nw \u2190Normalize(w)\n17: end for\n18: Return h, \u03b1\n# 3.3 Boosting\nFinally, we use the AdaBoost [Freund and Schapire, 1997] algorithm to produce an ensemble with these collections of summary-based weak learners. The central idea of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The algorithm is carried out over T rounds, where the weights of the training data points are adjusted based on the training error. Given a new data point, the predictions from classifiers from all rounds are then combined through a weighted majority vote to produce the final prediction. We use the error on a holdout validation set to determine the number of rounds T. A compact version of this process is presented in Algorithm 2. In the algorithm, the Summary method summarizes the examples in the prompt via the process discussed in Section 3.2. Each summary can be treated as a hypothesis that can classify new data. However, unlike the summary process in Section 3.2, where we resample multiple times to find the best learner, the boosting process returns immediately when a summary with an error rate better than random guessing is found (refer Appendix A.2). We use ClusterSampling to subsample a mini-batch\nof examples that fit within the LLM\u2019s max context length. Appendix A.10 dissects time complexity analysis of this method and A.8 covers full version of our boosting procedure that works in practice.\n# 4 Experiments\nWe conduct all of our experiments with OpenAI\u2019s GPT-3 API [Brown et al., 2020] and choose a collection of 18 tabular datasets from the UCI dataset [Dua and Graff, 2017] and OpenML [Vanschoren et al., 2014]. All main experiments are done with the Curie variant of GPT-3 unless otherwise specified, which has 13B parameters1. We compare the following methods:\nFurthermore, we compared Summary Boosting against popular baselines for tabular data that do not use prompting:\n\u2022 KNN: first embed the data descriptions with the GPT-3 embedding API 2 and then use Knearest neighbor to classify a new data description. This simple baseline demonstrates how much information can the naive representation produced by LLMs provide about the tasks. \u2022 LIFT [Dinh et al., 2022]: Language-Interfaced Fine-Tuning (LIFT) finetunes the LM with data descriptions (without binning) and their corresponding labels in a zero-shot. \u2022 TabPFN [Hollmann et al., 2022]: TabPFN is a transformer-based architecture that performs Bayesian inference on the entire training and test data points at the same time. \u2022 XGBoost [Chen and Guestrin, 2016]: XGBoost (eXtreme Gradient Boosting) is a regularized gradient boosting algorithm that is widely used for tabular data.\nFor each method and dataset, we use a 50/10/40 split for train, validation, and test sets and repeat each experiment for 3 random seeds The results are shown in Table 1 and 2.\n# 4.1 Analysis of prompting-based methods\nAs a general trend from Table 1, test performance improves in the order of Zero-shot < Few-shot < Summary < Summary Boosting. Firstly, unlike most works on zero-shot reasoning with LLMs, the LLMs do not have enough prior knowledge to make the correct prediction without additional information. As a result, we observe that Zero-shot performs poorly on all of the datasets. This observation highlights the necessity of learning from the data, and unlike other tasks, the LLMs themselves do not have enough built-in knowledge to succeed at tabular data zero-shot. Since zero-shot does not have enough prior knowledge to classify tabular data, we use few-shot in-context learning (Few-shot) to see if the added information helps make better predictions. As expected, on all the datasets other than visualizing-hamster, and wholesale-customers, Few-shot consistently improves the test performance compared to Zero-shot, suggesting that this added information is crucial for LLMs to work on tabular datasets. Unlike naively stacking examples inside the prompt in Few-shot, Summary condenses knowledge from these examples and is the first important algorithmic component of our framework for creating weak learners using LLMs. We see that Summary consistently improves upon Few-shot on all the datasets other than haberman-survival and wholesale-customers. This observation suggests\nTable 1: Test errors for prompting-based methods on all datasets (\u2193). Data Type indicates the number and types of attributes the dataset has (c is continuous and d is discrete). Size denotes the number of data points. In square bracket (if present) next to every dataset name, we provide its acronym referred to in our main text. In the bracket next to each dataset name is either the OpenML ID of the dataset or a reference to the dataset\u2019s associated publication. Error represents one standard deviation.\nDataset\nData Type\nSize\nZero-shot\nFew-shot\nSummary\nSummary Boosting\ncaesarian [cae] (42901)\n1c4d\n80\n0.425\u00b1 0.04\n0.388\u00b1 0.02\n0.350\u00b1 0.04\n0.300\u00b1 0.04\niris (61)\n4c0d\n150\n0.680\u00b1 0.02\n0.460\u00b1 0.01\n0.275\u00b1 0.07\n0.193\u00b1 0.03\ntae (48)\n1c4d\n151\n0.556\u00b1 0.07\n0.494\u00b1 0.01\n0.474\u00b1 0.02\n0.454\u00b1 0.03\nglass (41)\n9c0d\n214\n0.486\u00b1 0.01\n0.473\u00b1 0.01\n0.466\u00b1 0.02\n0.370\u00b1 0.02\nbreast-cancer [bc] (13)\n7c5d\n277\n0.754\u00b1 0.02\n0.516\u00b1 0.02\n0.337\u00b1 0.02\n0.288\u00b1 0.02\nvisualizing-environmental [ve] (678)\n3c0d\n111\n0.522\u00b1 0.01\n0.366\u00b1 0.01\n0.304\u00b1 0.02\n0.268\u00b1 0.03\nanalcatdata-chlamydia [ac] (535)\n2c2d\n100\n0.200\u00b1 0.00\n0.200\u00b1 0.00\n0.170\u00b1 0.01\n0.170\u00b1 0.01\nwine (43571)\n13c0d\n178\n0.820\u00b1 0.03\n0.674\u00b1 0.02\n0.475\u00b1 0.01\n0.320\u00b1 0.01\nblood-transfusion-center [btc] (1464)\n4c0d\n748\n0.544\u00b1 0.01\n0.430\u00b1 0.00\n0.258\u00b1 0.04\n0.240\u00b1 0.04\nsomerville-happiness-survey [shs] [Koczkodaj, 2018]\n0c7d\n143\n0.416\u00b1 0.03\n0.385\u00b1 0.03\n0.422\u00b1 0.02\n0.350\u00b1 0.02\nvehicle (54)\n18c0d\n846\n0.765\u00b1 0.00\n0.560\u00b1 0.01\n0.510\u00b1 0.02\n0.410\u00b1 0.04\nstatlog-heart [stath] [Dua and Graff, 2017]\n6c7d\n270\n0.551\u00b1 0.01\n0.528\u00b1 0.01\n0.444\u00b1 0.05\n0.430\u00b1 0.01\nverterbra-column [vc] (1524)\n6c0d\n310\n0.714\u00b1 0.03\n0.435\u00b1 0.06\n0.327\u00b1 0.01\n0.262\u00b1 0.01\necoli (1011)\n7c0d\n336\n0.581\u00b1 0.02\n0.562\u00b1 0.01\n0.480\u00b1 0.01\n0.270\u00b1 0.03\nhaberman-survival [hs] (43)\n3c0d\n306\n0.308\u00b1 0.02\n0.262\u00b1 0.01\n0.277\u00b1 0.01\n0.250\u00b1 0.01\ndiabetes [dia] (37)\n8c0d\n768\n0.446\u00b1 0.04\n0.400\u00b1 0.00\n0.360\u00b1 0.01\n0.344\u00b1 0.01\nvisualizing-hamster [hams] (708)\n5c0d\n73\n0.464\u00b1 0.03\n0.481\u00b1 0.05\n0.360\u00b1 0.02\n0.207\u00b1 0.00\nwholesale-customers [wc] (1511)\n6c1d\n440\n0.364\u00b1 0.01\n0.347\u00b1 0.01\n0.349\u00b1 0.02\n0.330\u00b1 0.00\n<div style=\"text-align: center;\">Table 2: Test errors for chosen methods on all datasets (\u2193). (acronym and notation in Table 1)</div>\n\u2193\nDataset\nData Type\nSize\nSummary Boosting\nLIFT\nKNN\nTabPFN\nXgboost\ncae (42901)\n1c4d\n80\n0.300\u00b1 0.04\n0.312\u00b1 0.02\n0.300\u00b1 0.00\n0.425\u00b1 0.07\n0.412\u00b1 0.05\niris (61)\n4c0d\n150\n0.193\u00b1 0.03\n0.100\u00b1 0.01\n0.106\u00b1 0.02\n0.027\u00b1 0.00\n0.054\u00b1 0.04\ntae (48)\n1c4d\n151\n0.454\u00b1 0.03\n0.480\u00b1 0.04\n0.532\u00b1 0.01\n0.450\u00b1 0.13\n0.464\u00b1 0.01\nglass (41)\n9c0d\n214\n0.370\u00b1 0.02\n0.218\u00b1 0.02\n0.294\u00b1 0.03\n0.158\u00b1 0.05\n0.254\u00b1 0.05\nbc (13)\n7c5d\n277\n0.288\u00b1 0.02\n0.318\u00b1 0.01\n0.277\u00b1 0.02\n0.264\u00b1 0.01\n0.270\u00b1 0.01\nve (678)\n3c0d\n111\n0.268\u00b1 0.03\n0.430\u00b1 0.04\n0.308\u00b1 0.01\n0.370\u00b1 0.04\n0.279\u00b1 0.02\nac (535)\n2c2d\n100\n0.170\u00b1 0.01\n0.180\u00b1 0.06\n0.170\u00b1 0.01\n0.090\u00b1 0.01\n0.110\u00b1 0.04\nwine (43571)\n13c0d\n178\n0.320\u00b1 0.01\n0.065\u00b1 0.01\n0.214\u00b1 0.05\n0.040\u00b1 0.01\n0.040\u00b1 0.01\nbtc (1464)\n4c0d\n748\n0.240\u00b1 0.04\n0.270\u00b1 0.01\n0.238\u00b1 0.00\n0.209\u00b1 0.01\n0.219\u00b1 0.01\nshs [Koczkodaj, 2018]\n0c7d\n143\n0.350\u00b1 0.02\n0.419\u00b1 0.02\n0.326\u00b1 0.03\n0.392\u00b1 0.00\n0.406\u00b1 0.00\nvehicle (54)\n18c0d\n846\n0.410\u00b1 0.04\n0.111\u00b1 0.16\n0.636\u00b1 0.01\n0.178\u00b1 0.01\n0.260\u00b1 0.00\nstath [Dua and Graff, 2017]\n6c7d\n270\n0.430\u00b1 0.01\n0.122\u00b1 0.17\n0.244\u00b1 0.03\n0.148\u00b1 0.03\n0.215\u00b1 0.00\nvc (1524)\n6c0d\n310\n0.262\u00b1 0.01\n0.192\u00b1 0.03\n0.318\u00b1 0.02\n0.135\u00b1 0.00\n0.187\u00b1 0.04\necoli (1011)\n7c0d\n336\n0.270\u00b1 0.03\n0.126\u00b1 0.03\n0.211\u00b1 0.03\n0.036\u00b1 0.02\n0.066\u00b1 0.01\nhs (43)\n3c0d\n306\n0.250\u00b1 0.01\n0.314\u00b1 0.03\n0.278\u00b1 0.00\n0.262\u00b1 0.02\n0.281\u00b1 0.02\ndia (37)\n8c0d\n768\n0.344\u00b1 0.01\n0.324\u00b1 0.04\n0.353\u00b1 0.02\n0.238\u00b1 0.03\n0.234\u00b1 0.00\nhams (708)\n5c0d\n73\n0.207\u00b1 0.00\n0.334\u00b1 0.08\n0.528\u00b1 0.02\n0.328\u00b1 0.01\n0.411\u00b1 0.01\nwc (1511)\n6c1d\n440\n0.330\u00b1 0.00\n0.125\u00b1 0.04\n0.043\u00b1 0.00\n0.088\u00b1 0.00\n0.098\u00b1 0.02\nthat summarization is a powerful way to improve few-shot performance and has potential for even other tasks using LLMs. Finally, for every dataset we tested, boosting with summarization consistently outperforms all other prompting-based approaches. This observation corroborates our hypothesis that LLMs with summarization are a good candidate for creating weak learners in boosting.\n# 4.2 Comparison to other tabular methods\nIn Table 2, we also observe that LLMs have a hard time reasoning about continuous attributes without finetuning, especially on the glass, wine, iris and vehicle datasets. In particular, when the datasets have many continuous features, the performance of Summary Boosting can be considerably worse than other methods such as LIFT or Xgboost. This may be because LLMs are fairly bad at quantitive reasoning without finetuning [Lewkowycz et al., 2022], which may be overcome in future LLMs. While KNN is a relatively simple baseline, its performance is surprisingly good at a few tasks such as wholesale-customers, statlog-heart, ecoli and wine. This highlights that LLMs have a remarkable amount of general prior knowledge about the worlds compared to methods like XGboost that sometimes this knowledge alone can produce good performances. Finally, we observe that Summary Boosting performs very well when the size of the dataset is very small. This makes sense since the strength of using LLMs as weak learners is that they have a large amount of generic prior about the world from pre-training. When the dataset is large, this prior knowledge might become less relevant and methods like finetuning become more competitive.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f09/2f09881e-f998-4411-917c-ea51cca536ee.png\" style=\"width: 50%;\"></div>\nFigure 3: Ablation experiments compared for the Summary method. The dataset acronymns are referred to 1. Left: Performance of curie vs davinci. Second from left: Comparison with and without task-specific attribute names. Middle: Effect of Cluster vs. Random sampling on the number of rounds till convergence and Second from Right: their final test errors. Right: Performance of templatized vs LLM-generated data descriptions.\n<div style=\"text-align: center;\">Figure 3: Ablation experiments compared for the Summary method. The dataset acronymns are referred to 1. Left: Performance of curie vs davinci. Second from left: Comparison with and without task-specific attribute names. Middle: Effect of Cluster vs. Random sampling on the number of rounds till convergence and Second from Right: their final test errors. Right: Performance of templatized vs LLM-generated data descriptions.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/92f9/92f97bcb-2723-431c-b000-cd63a8d5fe97.png\" style=\"width: 50%;\"></div>\nFigure 4: Additional ablations for the Summary method. (Left) Prompt design choices. First plot shows the effect of shuffling examples vs presenting them by class. Center plot compares tldr vs a more explicit prompt for inducing summary. Last plot compares prompts for doing inference. (Right Top) Performance of methods for discretizing continuous attributes on the Wine dataset. (Right Bottom) Performance of Few-shot and Summary as a function of number of examples in the context.\n# 5 Ablations\nSummarization forms the core of our methodology for generating weak learner. Consequently, it becomes important to identify ideal setting that can induce high-quality summaries. We perform ablation studies over the Summary method, to decide hyperparameters for getting a good weak learner. Preprocessing of continuous attributes. We tried several encoding techniques for continuous features, including binning, percentiles, and standard deviations. We chose the approach of describing them in technical language terms as well as assigning quantifiers for each level, as illustrated in Figure 4 right top. We observed that binning with quantifiers such as \u201clow\u201d, \u201cmedium\u201d, and \u201chigh\u201d was most effective for comparing examples and generating high-quality summaries. After hyperparameter tuning, we identified that using 5 bins provides sufficient granularity to distinguish variations in the continuous values. More details can be found in the Appendix A.6. Does the LLM explore prior knowledge to infer? To demonstrate the LLM\u2019s utilization of prior knowledge, we conduct an ablation study by masking the attribute names and using a template \u201cThis example has features f1 = {}, f2 = {} and so on.\u201d Figure 3 (second from left) shows the result. Using true variable names in the data descriptions leads to superior few-shot learning performance compared to using dummy names. This confirms that the model indeed leverages its prior knowledge of variables for predictions.\nHow does model size affect the performance? A natural question to ask is how the model size affects the downstream performance. We compare the Summary performances of GPT-3-davinci (175B parameters) and GPT-3-curie (13B parameters) on 5 datasets in Figure 3 (left). Surprisingly, we find that the larger model (davinci) does not consistently improve upon the smaller model. We also compare ChatGPT in A.12 and discuss the effects of RLHF [Ouyang et al., 2022]. How does the performance scale with more examples? In Figure 4 (right bottom), we study how the behavior of Few-shot and Summary change with different support set sizes (i.e., the number of data descriptions that are summarized). Few-shot performance reaches an optimal size around the medium context length and degrades with more examples. In contrast, Summary improves with more data, which is the more desirable behavior. Ordering of examples. Unlike conventional machine learning models, the ordering of examples in summarization affects the generation of hypotheses. There are two approaches: 1. presenting descriptions randomly (shuffled), and 2. grouping descriptions by classes (grouped). In Figure 4 (left), we compare these approaches on 4 datasets with Summary and find no significant difference. We use shuffled for all other experiments. Different summary and inference prompts. The LLM easily generates concise summaries on standard datasets like iris using a simple \u201ctl;dr\u201d prompt, but requires a more explicit prompt on complex datasets like \u201cvertebra-column\u201d. Comparing their performance in Figure 4 (left), both prompting modes are equally effective, so detailed prompts were used in all other experiments. See Appendix table 3 for the complete list of summary prompts used. In the left of Figure 4, we also compare two strategies for inference - prefix prompt (e.g. \u201cThis flower will be classified as\u201d), and two-stage chain-of-thought prompting (e.g. \u201cLet\u2019s think step by step\u201d) [Kojima et al., 2022]. We observe no statistically significant difference between them3. Since the prefix prompt more often completes the query well under lesser compute, we use prefix prompt for all the other experiments. Texts generated from template vs. GPT. In Figure 3 (right), we see that using GPT-generated data descriptions consistently achieves better results. Refer Appendix A.9 for examples of these templates. This may be due to the fact that the data description generated by LLMs conforms to natural text distribution more closely, which is desirable for performing inference using LLMs. Effect of cluster sampling. Cluster sampling improves the performance of the LLM by selecting a representative set of texts that generalize better, reducing validation error faster compared to random sampling during boosting. Although it may require more resampling to achieve a weak learner, cluster sampling converges much faster than random sampling as we see in Figure 3 - middle and second from right. However, with sufficient boosting rounds, the performances of the two sampling methods are not statistically different.\nEffect of cluster sampling. Cluster sampling improves the performance of the LLM by selecting a representative set of texts that generalize better, reducing validation error faster compared to random sampling during boosting. Although it may require more resampling to achieve a weak learner, cluster sampling converges much faster than random sampling as we see in Figure 3 - middle and second from right. However, with sufficient boosting rounds, the performances of the two sampling methods are not statistically different.\n# 6 Limitations\nAlthough the weak learning approach we develop here shows promise, there are currently several drawbacks. Although summarization and boosting alleviate manual prompt tuning to large extent, we still had to minimally tune some parts of the pipeline to get ideal performance (see Appendix table 3). Additionally, when the dataset contains many continuous attributes, there is a non-trivial gap between Summary Boosting and the other methods such as XGBoost or finetuning. Finally, the max input length of GPT-3 makes it harder to generate good summaries with just subset sampling. Eventually on larger datasets, after a certain number of boosting rounds, the summaries derived from a subset of examples may not further decrease the weighted error across all training examples. Rigorous techniques such as structured prompting handle this issue by rescaling attention weights [Hao et al., 2022]. We believe this issue could be solved with more powerful LLMs such as GPT-4.\n# 7 Conclusion\nLLMs have been widely used in recent years, not just for their generative abilities but for their ability to serve as zero- or few-shot learners with proper prompting. This work aims to situate them\nwithin another context of \u201csimple\u201d learning algorithms \u2013 the weak learner paradigm that forms the foundation of boosting approaches. We show that leveraging the summarization capabilities of LLMs indeed leads to models that function as weak learners, and which can thus be integrated into boosting frameworks. This result leads to new potential paradigms for treating the results of LLM prompting not just as individual predictors, but as part of a larger set of meta-models as well.\n# Acknowledgements\nYiding is supported by funding from Bosch Center of Artificial Intelligence.\n# References\nThe design of the prompt plays a pivotal role in our entire process. Specifying instructions precisely can create a significant difference, whether it comes to effectively describing tabular data, generating reliable summaries or inferring accurate predictions (shown in Figures 1 and 2). While soft prompting has been successful at instructing LLMs [Lester et al., 2021], it cannot be applied to our setting because our classification algorithm learns a human-level summary as a prompt for classifying data, rather than a soft prompt. Instead we chose to write prompts that ask the LLM to perform these tasks. In this way, our prompting method is entirely gradient-free. Hand-written prompts also offer flexibility, aligning well with our core methodology for creating weak learner. While carefully handcrafting prompts this way might seem intensive, we will show that once we identify the ideal hyperparameter settings they can be framed with little effort.\n# A.1 Data Conversion Challenges\nLanguage models (LLMs) have demonstrated impressive performance on standard tasks with minimal supervision [Wang et al., 2022b, Brown et al., 2020]. However, for converting tabular data to text, there were several considerations to meet the requirements of our task. As highlighted in Section 3.1, we will refer to these texts as data descriptions.\nEnsuring Uniform Length Firstly, the data descriptions should not be too long or short, also be of comparable length. Excessively long descriptions limit the number of examples that can be fit inside the prompt and summarized. We observed in Figure 4 (right bottom) that the summary performance also scales with more examples, so it makes sense to accomodate as many of them as possible by having descriptions of approximately uniform length. A straightforward way of achieving this uniformity would be by specifying a max word length as part of the conversion prompt itself, as in \u201cDescribe in not more than 80 words\u201d. However, we found this approach can falter, sometimes leading to overly simplistic descriptions like \"These are annual spendings of a customer.\" (in the wholesale-customers dataset). Consequently, we adopt more nuanced strategy by first modifying the prompt with the terms \u201cconcisely\u201d and \u201caccurately\u201d to emphasize the brevity and preciseness of the generated descriptions (shown in Figure 1). Then, we implement a resampling strategy, that generates descriptions until finding the one with a desired length ranging between 20 to 80 words. This process achieves consistent and uniformly long descriptions.\nIncluding Metadata Prepending metadata to the prompt enhances the contextual awareness of the task, resulting in higher-quality descriptions (shown in Figure 1).\nSeparating Features from Labels In our method, the data descriptions function dual role, both as training examples and as query for inferring class labels. This suggests that, when converting data to text, the features need to be described separately from the target label as illustrated in Figure 1. The resulting strings are then concatenated to form the data description. Instead, if the whole tabular record were passed to the LLM, it often produces texts that assimilate the classification label information in the meat of the description itself, rendering it difficult to extract a query for doing inference. While one might cleverly come up with prompts that can allow the LLM to describe the features and target label in separate sentences, we found it to be more sensible to supply just the features for describing and not reveal any information about the target task. Sometimes that can liberate the LLM to hallucinate some facts about the task and form biased data to begin with.\nNatural-Sounding Descriptions While the LLM generates a different-styled response every time, to explicitly ensure that the generated descriptions are not template-like by chance, add a directive at the end of the prompt: \u201cUse your creativity\u201d. This encourages the LLM to produce more natural narratives of the record. Alternatively, setting a higher temperature during decoding achieves a similar effect.\nThere are several aspects worth considering that can contribute to high-quality summaries.\nSampling candidate summaries A well-crafted summary is a one that captures salient information of the dataset, in a way that facilitates inferring predictions off it. However, the process of generating summary using a LLM is inherently stochastic due to temperature sampling, as a result, the generated summary can be noisy. From our experiments with tuning this temperature, we found 0.80 to be ideal through Bayesian optimization. Even at this value, on average only 1 out of 3 summaries were meaningful. A noisy summary can be distinguished quite easily. For instance, on the vehicle dataset, the tl;dr prompt elicits summaries as naive as \u201cThe given data describes a bus, van, or saab silhouette.\u201d or \u201cThe data in this table identifies a vehicle as a bus, saab, opel, or van. The compactness, circularity, distance circularity, radius ratio, hollows ratio, and symmetry are all predictive of the vehicle\u2019s type.\u201d which does not offer actionable insight. This observation indicates that summaries need to be sampled quite a few times and the best one can be determined based on the validation error. As a result, for the Summary learning procedure in Section 3.2, we resample approximately 25 times to find a good summary. Also, given that our datasets are small, it is not unusual for the summaries to have the same validation error. When tied, we pick one having a higher training error rate, i.e. lower generalization gap. Differently, in our Summary boosting procedure explained in Section 3.3, we resample only until finding a summary whose training error is better than random guessing and return immediately. Ordering examples inside the summarization prompt Unlike gradient descent, prompting is not robust to the presentation of the examples to the learning algorithm. While we show via ablation studies in Section 5 that there is no statistically significant difference in performance between either shuffling the examples or listing them by class, we can generally expect that depending on the dataset and the number of target classes, one might be preferred over the other. For instance, in a multi-class setting, listing examples by class might be more helpful in reaching a weak learner quickly. However, in a two-class setting, the summary might actually benefit from randomness in the shuffled examples.\nSampling candidate summaries A well-crafted summary is a one that captures salient information of the dataset, in a way that facilitates inferring predictions off it. However, the process of generating summary using a LLM is inherently stochastic due to temperature sampling, as a result, the generated summary can be noisy. From our experiments with tuning this temperature, we found 0.80 to be ideal through Bayesian optimization. Even at this value, on average only 1 out of 3 summaries were meaningful. A noisy summary can be distinguished quite easily. For instance, on the vehicle dataset, the tl;dr prompt elicits summaries as naive as \u201cThe given data describes a bus, van, or saab silhouette.\u201d or \u201cThe data in this table identifies a vehicle as a bus, saab, opel, or van. The compactness, circularity, distance circularity, radius ratio, hollows ratio, and symmetry are all predictive of the vehicle\u2019s type.\u201d which does not offer actionable insight. This observation indicates that summaries need to be sampled quite a few times and the best one can be determined based on the validation error. As a result, for the Summary learning procedure in Section 3.2, we resample approximately 25 times to find a good summary. Also, given that our datasets are small, it is not unusual for the summaries to have the same validation error. When tied, we pick one having a higher training error rate, i.e. lower generalization gap. Differently, in our Summary boosting procedure explained in Section 3.3, we resample only until finding a summary whose training error is better than random guessing and return immediately.\nOrdering examples inside the summarization prompt Unlike gradient descent, prompting is not robust to the presentation of the examples to the learning algorithm. While we show via ablation studies in Section 5 that there is no statistically significant difference in performance between either shuffling the examples or listing them by class, we can generally expect that depending on the dataset and the number of target classes, one might be preferred over the other. For instance, in a multi-class setting, listing examples by class might be more helpful in reaching a weak learner quickly. However, in a two-class setting, the summary might actually benefit from randomness in the shuffled examples.\nCustomizing the summarization prompt The way of asking the LLM to summarize examples can also give rise to good/bad summaries. For instance, one can prompt the LLM with a simple tl;dr or specify the task more elaborately. We will refer to the latter option as explicit. As we demonstrated in Figure 4 (left), both are means to the goal and do not statistically differ in terms of performance induced. However, in our experiments on certain datasets, we would rather be incentivized choosing the explicit over the tl;dr to attain a weak learner more quickly. This choice becomes important purely for compute reasons as it will take relatively lesser resampling, while the tl;dr still works. For instance, this scenario can happen when the LLM cannot decipher what the summary is supposed to say, by just observing the examples. As examples, the tl;dr prompt suffices on datasets such as iris, diabetes, and wine that are commonly encountered in prediction context, whereas the LLM might not be very familar with the goals of vertebra-column or somerville-happiness-survey data, necessitating the use of the explicit prompt. For these other datasets, the fact that it is a classification problem based on some features and target classes may not be very apparent from just the examples and metadata. So, providing a directive such as \u201cSummarize in detail how we can tell apart people with normal and abnormal vertebra-column\u201d reduces ambiguity in the task setup and reduces probability of a noisy summary. While manual intervention is necessary, framing this prompt can be done with little effort. We provide a comprehensive list of these parameters for all datasets in Table 3.\nIncluding Metadata Similar to data conversion, including meta-data information in the prompt offers better penetration into the world of the dataset, as a result improves boosting performance.\n<div style=\"text-align: center;\">Table 3: Prompt design: Prompt parameter settings for every dataset.</div>\nDataset\nPrompting hyperparameters\ncaesarian\nmetadata: This dataset contains information about caesarian section results of 80 pregnant women with the most\nimportant characteristics of delivery problems in the medical field.The goal is to predict whether a woman will undergo\nnormal or caesarian delivery.\nclasses: [normal, caesarian]\nsummary directive: Tl;dr\ninference directive: Hence this woman\u2019s delivery mode is likely to be (normal or caesarian):\niris\nmetadata: This is the iris dataset, perhaps the best known database to be found in the pattern recognition literature.\nFisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data\nset contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable\nfrom the other 2; the latter are NOT linearly separable from each other. Predicted attribute- class of iris plant- setosa,\nversicolor, virginica.\nclasses: [setosa, versicolor, virginica]\nsummary directive: Tl;dr\ninference directive: Based on the above information, predict if this flower will be classified as setosa, versicolor, virginica\ntae\nmetadata: The data consist of evaluations of teaching performance over three regular semesters and two summer semesters\nof 151 teaching assistant (TA) assignments at the Statistics Department of the University of Wisconsin-Madison. The\nscores were divided into 3 roughly equal-sized categories (\"low\", \"medium\", and \"high\") to form the class variable.\nclasses: [low, medium, high]\nsummary directive: Tl;dr\ninference directive: Predict whether this class will score low or medium or high:\nglass\nmetadata: This is the glass dataset from USA Forensic Science Service; 6 types of glass; defined in terms of their oxide\ncontent (i.e. Na, Fe, K, etc). The study of classification of types of glass was motivated by criminological investigation.\nAt the scene of the crime, the glass left can be used as evidence...if it is correctly identified!\nclasses: [building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed,\ncontainers, tableware, headlamps]\nsummary directive: Tl;dr\ninference directive: There are 6 possible type of glass: building_windows_float_processed,\nbuilding_windows_non_float_processed, vehicle_windows_float_processed, containers, tableware, headlamps. Predict\nwhich one will this sample be:\nbreast-cancer\nmetadata: This is one of three domains provided by the Oncology Institute that has repeatedly appeared in the machine\nlearning literature. This data set includes 201 instances of one class and 85 instances of another class. The instances are\ndescribed by 9 attributes, some of which are linear and some are nominal. It contains information about women that had\na recurrence or non-relapse of breast cancer after their first time.\nclasses: [recurrence, non-relapse]\nsummary directive: Based on the above examples, figure out under what conditions will a woman have recurrence or\nnon-relapse of breast cancer?\ninference directive: Predict whether this woman will have a recurrence or non-relapse:\nvisualizing-environmental\nmetadata: This is the visualizing-environmental dataset, one of the 22 data sets from the book Visualizing Data published\nby Hobart Press (books@hobart.com). This data describes indicators for a positive/negative environment based on ozone,\nradiation and temperature. classes: [positive, negative]\nsummary directive: Tl;dr\ninference directive: There are clear signs of this environment being (positive or negative):\nanalcatdata-chlamydia\nmetadata: This chlamydia dataset is one of the data sets used in the book \"Analyzing Categorical Data\" by Jeffrey S.\nSimonoff, Springer-Verlag, New York, 2003. It contains results of individuals that tested for chlamydia.\nclasses: [positive, negative]\nsummary directive: Tl;dr\ninference directive: Predict if this person will test positive or negative for chlamydia:\nwine\nmetadata: This is the Wine recognition data. Updated Sept 21, 1998 by C.Blake. It contains results of a chemical analysis\nof wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities\nof 13 constituents found in each of the three types of wines.\nclasses: [1, 2, 3]\nsummary directive: Using these examples and based on the contents of constituents, summarize what distinguishes wines\nof type 1 or 2 or 3?\ninference directive: Hence this wine will be classified as ->type\nblood-transfusion-center\nmetadata: Data taken from the Blood Transfusion Service Center in Hsin-Chu City in Taiwan - this is a classification\nproblem. The goal is to predict whether a given individual will consent or avoid donating blood.\nclasses: [consent, avoid]\nsummary directive: Tl;dr\ninference directive: Therefore, this individual is likely to (avoid/consent):\nsomerville-happiness-survey\nmetadata: This is the Somerville Happiness Survey Data Set. It has ratings collected from a survey of Somerville\nresidents. From the responses of a resident, the goal is to predict whether they feel happy or unhappy about the place.\nclasses: [unhappy, happy]\nsummary directive: Based on the Somerville happiness survey, how can we predict whether a resident is happy or\nunhappy with their place?\ninference directive: So this resident is (happy or unhappy):\nvehicle\nmetadata: This is the Statlog (Vehicle Silhouettes) Data Set. The purpose is to classify a given silhouette as one of four\ntypes of vehicle - bus, saab, opel or a van, using a set of features extracted from the silhouette. The vehicle may be\nviewed from one of many different angles.\nclasses: [bus, saab, opel, van]\nsummary directive: Using these examples, summarize how can we differentiate if a silhouette is that of a bus, saab, opel\nor a van.\ninference directive: Out of saab, bus, van and opel, this vehicle is likely to be a\nstatlog-heart\nmetadata: This dataset is a heart disease database similar to a database already present in the repository (Heart Disease\ndatabases) but in a slightly different form. It has data on individuals having and not having heart disease.\nclasses: [present, absent]\nsummary directive: Differentiate people with heart disease present from ones absent.\ninference directive: In this case, heart disease is likely to be (present/absent):\n# A.3 Inference\nMapping labels from LLM responses Answer mapping refers to the process of assigning the model\u2019s answer to a target output class. This step might be trivial when the answer is the class itself, for example when the LLM responds with \u201cnon-relapse\u201d or \u201crecurrence\u201d to a query on the breast-cancer dataset. However, in other instances, it can become tricky when the LLM\u2019s responses are \u201cwill not recur\u201d or \u201chas higher chance of non-relapse than recurrence\u201d, requiring a more complex decoding logic to identify the target class. Previous works have handled this problem by disguising the task as a Cloze prompt and learning a verbalizer, i.e. MLP projection of hidden state of the [MASK] token, that maps the predicted token to the target classes [Cui et al., 2022, Hu et al., 2021]. By training a verbalizer, one can determinsically go from token vocabulary to the label space. There also exist unsupervised statistical techniques for achieving label mapping [Wang et al., 2022a]. In our method however, we strictly interact with the LLM through prompts and do not access the hidden state nor gradients. As a result, our inference process shown in Figure 2 focusses on inferring the class label solely through prefix prompts, without relying on learning an explicit mapping. Specifically, by conditioning on a suitable prefix, we constrain the LLM to return exactly the class label string. For example, the prefix \u201cTherefore this iris flower is likely to be (setosa, versicolor, virginica):\u201d works for the iris dataset. A key observation guiding the design of such a prefix prompt is the fact that specifying the output classes entices the LLM to predict from among these classes. With a rather plain prefix like \u201cPredict what will be the type of this flower.\u201d, the LLM\u2019s answer space is unconstrained and it might liberally go on to explain a chain of reasoning such as \u201cThe flower has short petals and long sepals, hence it is versicolor, and not setosa.\u201d preventing a simple keyword search for the class label. For a full list of these inference prompts, refer Table 3.\nTwo-step prompting, Davinci vs. Curie It is worth mentioning that a two-step prompting trick, by first calling \u201cLets think step by step\u201d then concatenating the response with the prefix prompt also results in accurate answers, as we have shown in Figure 4 (left). However, it could only be implemented on the larger model Davinci but not Curie which is primarily used in our experiments.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0539/05396996-55ba-4f67-87a0-b5e947d8b624.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Steps of Zeroshot prompting</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2345/2345c65c-7ad9-4fad-b0c2-9aa91145e9dc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Workflow in fewshot prompting</div>\nInterestingly Davinci\u2019s chain of thought reasoning even outperforms its prefix prompt counterpart. In all our experiments with Curie however, the prefix technique works reasonably well. The Davinci API also offers a suffix argument which can be invoked for predicting in a more natural way. For instance, for the breast-cancer dataset, the prompt can be posed with prefix \u201cAll in all, this woman is more likely to have a\u201d and a suffix \u201c of breast cancer.\u201d directly expecting the LLM to fill in with \u201crecurrence\u201d or \u201cnon-relapse.\u201d\n# A.4 Zeroshot Setting\nWe extend the analysis of prompting-based methods in Section 4.1 by further delving into the Zeroshot experiment. We illustrate this experimental setting in Figure 5. It only consists of the inference prompt, wherein the LLM is presented with the metadata and a query. To facilitate answer mapping, the output classes are also indicated in the prompt. Unlike Summary, the zeroshot process is not stochastic as there is no learning involved. For inference, the predicted tokens are sampled greedily at temperature = 0.\n# A.5 Few-shot Setting\nExtending the analysis from Section 4, we explain the Fewshot setting. As illustrated in Figure 6, it is an instance of in-context learning, wherein the support set examples are enlisted in the prompt along with a query in the end. The support set was chosen from the training set through the stratified cluster sampling outlined in Algorithm 1. This results in a semantically diverse collection of examples evenly spread across the classes. Since we observe in Figure 4 (right bottom) that the Fewshot performance drops with more examples, we choose approximately 15 examples to fit in the prompt. Similar to the Summary method, this inference prompt carries meta-information about the dataset, and also indicates the output classes. The prompt also injects support set examples that are stringed together as we show in Figure 6. The predictions are made greedily at a temperature of 0. Again, the Fewshot method is a stochastic process whose outcome depends on the selection of the support set. So finding the ideal prompt, requires sampling different support sets from the training\nset quite a few times. We perform this resampling approximately 25 times and pick the best promp based on the validation error.\n# A.6 Preprocessing continuous attributes\nExtending from the ablation studies in Section 5, we demonstrate in Table 4, concrete examples of these encoding techniques applied to continuous features. Every numerical column in the dataset was subject to the transformation independently of the rest.\nWe applied several encoding techniques for continuous features, including binning, percentiles, and standard deviations. Our approach involved using technical language terms to describe these ranges, such as \u201cfalls in the nth bin/nth percentile or n deviations above/below mean\". We also characterize them in a more naturalistic way by assigning quantifiers such as low, medium, and high to each level in the binning technique. To create effective textual descriptions, we examined three high-level approaches: 1. presenting only numerical values, 2. using solely textual encodings, and 3. concatenating both. We observed that utilizing textual encoding alone outperformed the other methods. As a result, we focused on mainly comparing textual encoding methods as shown in Figure 4 (right top). Through Bayesian optimization, we found that binning with \u201c5\u201d quantifiers was ideal for generating high-quality summaries. We describe each encoding technique as follows:\n\u2022 Binning: It involves creating a histogram with the given number of bins. As outputs, the values are directly described as \u201cfalling in the n-th bin\u201d as illustrated in the 10 bins experiment. However, in the presence of degree quantifiers which are categorical names assigned to the bins, these tags are used instead. We found that as opposed to calling out the bin number, describing in terms of these quantifiers further aids the LLM in comparing the relative extent to which features match and improving the estimation of similarities. This led us to tune the number of bins against these degree quantifiers, selecting values in the range of 4, 5, 7, and 9 bins. The first four rows in Table 4 show how these tags get translated into the record. \u2022 Percentile: It is given by computing the percentile rank of a value relative to that series of values. Then, the value is described as falling in that percentile rank in words. This is closer to representation of the true numerical values per se, but helps the LLM draw comparisons on a scale of 1-100. \u2022 Standard deviations: In this procedure, the values are segmented into six ranges based on distance from the mean, given by one/two/three standard deviations above/below the mean. \u2022 Quartiles: Here, we consider the first and third quartiles, and the median as landmarks to bucketize the values into four partitions.\nAmong these methods, the \u201c5 bins with quantifiers\u201d strikes a balance in granularity scale. It is not excessively fine-grained as \u201cpercentile\u201d, nor overly abstract, as the \u201c4-bin\u201d approach. This balance ultimately leads to optimal performance.\n# A.7 Cluster Sampling components\nWe discuss more of the functions in Algorithm 1. GPT-Embedding is OpenAI\u2019s text similarity model text-embedding-ada-002 that takes a maximum input size of 8191 tokens. It returns a 1536-dimensional embedding for text. OpenAI recommends cosine distance for comparing ada embeddings in downstream tasks. As a result, the AgglomerativeClustering algorithm applies hierarchical clustering over these features using cosine distance, average linkage and a heuristically selected distance threshold of 0.05. It yields a set of clusters C and each Cj contains a list of indices of data points that belong to that cluster j.\n# A.8 Adaboost Optimizations\nWe additionally apply several run-time optimizations to the boosting algorithm described in 3.3. Thus we present its full version in Algorithm 3.\nAlgorithm 3 Summary Boosting\n1: Input: X, all training data; y, all training label; T: maximum number of rounds; s: size of the\nsampled subset.\n2: h, P, \u03f5, \u03b1 \u2190empty array of size T. \u25b7h holds the round-wise hypotheses, P are the corresponding\nlabel mappings, \u03f5 gathers the weighted train errors, and \u03b1 are coefficients of the hypotheses.\n3: N \u2190len(X)\n4: c \u2190set of target classes\n5: K \u2190len(c)\n6: w \u2190new array of size N filled with 1\nN.\n\u25b7w is the weighted data distribution\n7: for r = 1 to T do\n8:\n(Xs, ys) \u2190Cluster-sample s examples from training distribution w.\n9:\nh[r] \u2190Summary (Xs, ys)\n\u25b7h[r] is the weak learner in the current round\n10:\n\u02c6y[i] \u2190h[r](X[i])\n\u25b7\u02c6y refers to predictions on training set\n11:\n\u03be \u2190empty hashmap\n\u25b7\u03be[p] will have error rate of the corresponding label mapping p\n12:\nfor p in PermutedLabelMappings(c) do\n13:\n\u03be[p] \u2190\n\ufffdN\ni=1 w[i]\u00d71{p[\u02c6y[i]]\u0338=y[i]}\n\ufffdN\ni=1 w[i]\n14:\nend for\n15:\np\u2217\u2190arg minp \u03be[p]\n16:\nif \u03be[p\u2217] > 1 \u22121\nK \u2212\u00b5 OR AllSame(\u02c6y) then Goto Step 8.\n17:\nelse\n18:\nP[r] \u2190p\u2217; \u03f5[r] \u2190\u03be[p\u2217]\n19:\nend if\n20:\nif \u03f5[r] == 0 then\n21:\nBreak\n22:\nend if\n23:\n\u03b1[r] \u2190log\n\ufffd\n1\u2212\u03f5[r]\n\u03f5[r]\n\ufffd\n+ log(K \u22121)\n24:\nfor i = 1 to N do\n25:\nw[i] = w[i] \u00d7 exp(\u03b1[r]1{P[r][h[r](X[i])] \u0338= y[i]})\n26:\nend for\n27:\nw \u2190Normalize(w)\n28: end for\n29: Return h, \u03b1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f08/2f0825c2-42c9-46fa-ba74-788b3cda05ab.png\" style=\"width: 50%;\"></div>\n\u2022 Raising the bar for a weak learner: Our goal was to create high-quality summaries that dramatically reduce the validation error rate and significantly accelerate the convergence of the boosting procedure. Thus we raise the performance threshold to a notch slightly higher than random guessing probability (see Step 15 in Algorithm 3), provoking insightful summaries. We resample until finding a weak learner that satisfies this threshold. The positive quantity \u00b5 is a hyperparameter that typically takes values 0.08 for 2-class problem and 0.16 for 3-class problem, and so on. Although this step increases compute, it yields better weak learners and improves convergence overall.\n# \u2022 Permuting the predicted class label assignments:\nWe harness the potential of permuting the class assignments by exploring K! different mappings of predictions to classes using the PermutedLabelMappings function in steps 11-14. This process helps us identify the mapping that minimizes the training error to the greatest extent. By considering multiple permutations of predictions across the label space, as outlined in Steps 11-14 of Algorithm 3, we obtain a hashmap p from the PermutedLabelMappings function. This hashmap maps the predictions \u02c6y to the permuted label space. Selecting the mapping, p\u2217that results in the lowest training error effectively diminishes the cumulative training error during boosting iterations and proves to be an effective strategy for generating strong weak learners. This technique is particularly advantageous in scenarios involving more than two classes.\nWe harness the potential of permuting the class assignments by exploring K! different mappings of predictions to classes using the PermutedLabelMappings function in steps 11-14. This process helps us identify the mapping that minimizes the training error to the greatest extent.\nBy considering multiple permutations of predictions across the label space, as outlined in Steps 11-14 of Algorithm 3, we obtain a hashmap p from the PermutedLabelMappings function. This hashmap maps the predictions \u02c6y to the permuted label space. Selecting the mapping, p\u2217that results in the lowest training error effectively diminishes the cumulative training error during boosting iterations and proves to be an effective strategy for generating strong weak learners. This technique is particularly advantageous in scenarios involving more than two classes.\n\u2022 Sanity checks: Finally, to ensure robustness of the weak learner when faced with skewed datasets, we have implemented a policy that disallows a naive all-ones classifier. The condition calling AllSame in Step 15 of Algorithm 3 performs this check.\n# A.9 Text Templates\nIn the ablation study which involves comparing the descriptions created manually vs. by a LLM, as illustrated in Figure 3 (right), we transform the descriptive attributes into the textual format by applying a pre-defined template. In Table 5 we provide examples of these templates for selected datasets.\n# A.10 Complexity Analysis\nWe provide the time complexity analysis comparing our boosting procedure to finetuning the LLM. For finetuning, the complexity is O(TNf), where f is runtime of the LLM, T is number of epochs, N is the number of data points. For summary boosting, the complexity is O(TRf), where f is runtime of the LLM, T is number of boosting rounds and R is the number of resampling per round. Concretely, for a dataset with 175 examples, finetuning takes 20 epochs \u00d7 175 examples \u00d7 2 = 7000 passes through the LLM. 2 stands for both forward and backward passes through the model. For the same dataset boosting requires 50 rounds \u00d7 25 resampling on average = 1250 passes through the LLM. Thus, we believe the complexity of our algorithm is at least comparable to, if not better than, that of finetuning (without considering the cost of the actual API calls).\n# A.11 Estimating the cost of API calls\nWhile our method is applicable to any large language model, we primarily conducted experiments using GPT-3. Each API call to GPT-3 incurs a specific dollar cost. After analyzing the running time complexity of summary boosting, which is O(TRf), we can provide a rough estimation of the cost associated with training a classifier on any given dataset. To begin, when making a call to summarize examples, the prompt is filled up to the maximum context length, which is 2048 tokens for the query prompt and completion. We\u2019ll refer to these summary tokens as St = 2048. Additionally, if N represents the size of the dataset and we allocate (50 + 10) Now, to obtain a weak learner at boosting round r, we may need to resample up to R candidate summaries. Furthermore, we calculate the training error for each candidate summary to determine if it performs better than random guessing. Once the desired weak learner is found, we compute the validation error for that round only once. Therefore, each round requires querying R \u00d7 (St + 0.5N \u00d7 Pt) + 0.1N \u00d7 Pt tokens. Considering that the maximum number of rounds is denoted as T, the total number of tokens exchanged would be T \u00d7 [R \u00d7 (St + 0.5N \u00d7 Pt) + 0.1N \u00d7 Pt]. For instance, let\u2019s consider a dataset with 175 examples. In this case, the cost would be 30 rounds \u00d7 [20 resampling \u00d7 (2048 summary tokens + (0.5 \u00d7 175 training examples) \u00d7 210 prediction tokens) + (0.1 \u00d7 175 validation examples) \u00d7 210 prediction tokens] = 12364050 tokens, which approximately costs $25 for Curie at a rate of $0.002/1K tokens.\n# A.12 Can ChatGPT function as a weak learner?\nOne would expect that it is more advantageous to try newer LLMs such as ChatGPT that produce increasingly more human-like text and are far more sample-efficient, i.e. can summarize more examples since they come with a larger context length. To investigate this, we conduct experiments by feeding ChatGPT with the same tabular data descriptions and using identical prompts to create weak learners. The results are presented in Table 6.\nSurprisingly ChatGPT outperforms Curie in classifying datasets with more numerical features, such as wine, wholesale-customers, and iris. This observation suggests that LLMs are becoming more adept at quantitative reasoning from finetuning with more data. However, the reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022] poses a limitation as it still ensures the generated text does not deviate too much from its prior. The generated text distribution adheres closely to the behavior programmed into the LLM induced by optimizing with such a reward model. Consequently it becomes challenging to bias the LLM with adversarial examples that might occasionally emerge in the training set. For example, ChatGPT does not mostly generalize well on datasets with medical information such as verterbra-column, breast-cancer, caesarian and blood-transfusion-center where there can be examples contrary to common medical beliefs. In these cases, the RLHF is more restrictive due to its conformity to human preferences and does not neutrally summarize examples at hand from a classification standpoint. However, boosting imposes a significantly higher penalty on examples that the model fails to classify correctly, causing ChatGPT to not decrease training error after a few epochs. While these models exhibit promise in terms of higher-order problem-solving skills, their capabilities can also be limited by their alignment with human preferences.\nTable 4: Continuous variable transformations applied to an example from the wholesale-customers dataset. The raw tabular record is as follows: spending on fresh products: 6353.0, spending on milk products: 8808.0, spending on grocery products: 7684.0, spending on frozen products: 2405.0, spending on detergents and paper products: 3516.0, spending on delicatessen products: 7844.0 and customer\u2019s region: Outside Lisbon and Porto.\nMethod\nData Representation\nExample as text\n4 bins + quantifiers\n{very low, low, high,\nvery high}\n- spending on fresh products : low\n- spending on milk products : very high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : high\n- spending on delicatessen products : very high\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends low amounts on fresh products, very high\namounts on milk products, high amounts on grocery products,\nfrozen products, detergents and paper products, and very high\namounts on delicatessen products. They are located outside of\nLisbon and Porto.\n5 bins + quantifiers\n{very low, low, medium,\nhigh, very high}\n- spending on fresh products : medium\n- spending on milk products : very high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : high\n- spending on delicatessen products : very high\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer from outside Lisbon and Porto spends medium on\nfresh products, very high on milk products, high on grocery\nproducts, high on frozen products, high on detergents and paper\nproducts, and very high on delicatessen products.\n7 bins + quantifiers\n{extremely low, very low,\nlow, medium, high,\nvery high, extremely high}\n- spending on fresh products : low\n- spending on milk products : very high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : very high\n- spending on delicatessen products : extremely high\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer situated outside Lisbon and Porto spends low on\nfresh products, very high on milk products, high on grocery\nproducts, high on frozen products, very high on detergents and\npaper products, and extremely high on delicatessen products.\n9 bins + quantifiers\n{lowest, extremely low,\nvery low, low, medium,\nhigh, very high,\nextremely high, highest}\n- spending on fresh products : low\n- spending on milk products : extremely high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : very high\n- spending on delicatessen products : highest\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends low amounts on fresh products, extremely\nhigh amounts on milk products, high amounts on grocery\nproducts, frozen products, detergents and paper products, and\nhighest amounts on delicatessen products. They are located\noutside Lisbon and Porto.\n10 bins\n- spending on fresh products : falls in the first out of ten\nbins of values.\n- spending on milk products : falls in the second out of\nten bins of values\n- spending on grocery products : falls in the first out of\nten bins of values\n- spending on frozen products : falls in the first out of\nten bins of values\n- spending on detergents and paper products : falls in\nthe first out of ten bins of values\n- spending on delicatessen products : falls in the second\nout of ten bins of values\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends relatively little on fresh, grocery, frozen\nand detergents/paper products, and more on milk and\ndelicatessen products. They are based outside Lisbon and Porto.\nPercentile\n- spending on fresh products : falls in the forty-first\npercentile\n- spending on milk products : falls in the eighty-second\npercentile\n- spending on grocery products : falls in the sixty-fifth\npercentile\n- spending on frozen products : falls in the sixty-third\npercentile\n- spending on detergents and paper products : falls in\nthe seventy-second percentile\n- spending on delicatessen products : falls in the ninety\n-eighth percentile\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer has an annual spending of 41st percentile on fresh\nproducts, 82nd percentile on milk products, 65th percentile on\ngrocery products, 63rd percentile on frozen products, 72nd\npercentile on detergents and paper products, and 98th percentile\non delicatessen products, and is located outside of Lisbon and\nPorto.\nStandard deviation\n- spending on fresh products : is within one std-dev\nbelow the mean value\n- spending on milk products : is within one std-dev\nabove the mean value\n- spending on grocery products : is within one std-dev\nbelow the mean value\n- spending on frozen products : is within one std-dev\nbelow the mean value\n- spending on detergents and paper products : is\nwithin one std-dev above the mean value\n- spending on delicatessen products : is two std-dev\nabove the mean value\n- customer\u2019s region : Outside Lisbon and Porto\nThe customer has annual spending on fresh products, milk\nproducts, grocery products, frozen products, detergents and paper\nproducts, and delicatessen products within one standard deviation\nof the mean, except for delicatessen products which is two\nstandard deviations above the mean. The customer is located\noutside Lisbon and Porto.\nQuartiles\n- spending on fresh products : is between the first\nquartile and median values\n- spending on milk products : is more than the third\nquartile value\n- spending on grocery products : is between median\nand third quartile values\n- spending on frozen products : is between median\nand third quartile values\n- spending on detergents and paper products : is\nbetween median and third quartile values\n- spending on delicatessen products : is more than\nthe third quartile value\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends more than the third quartile value on milk,\ndelicatessen and detergents and paper products. The customer\u2019s\nspending on fresh, grocery, and frozen products falls between the\nmedian and third quartile values, while the customer is located\noutside of Lisbon and Porto.\n22\n<div style=\"text-align: center;\">Table 5: Templatized descriptions: Templates used to format examples for the ablation study between LLM-created data descriptions vs. template descriptions</div>\nDataset\nDescriptive attribute values\nTemplate\ncaesarian\nage: [very young, young, middle-aged, old, very old]\ndelivery_number: [first, second, third, fourth, fifth]\ndelivery_time: [timely, premature, latecomer]\nblood_pressure: [low, normal, high]\nheart_problem: [has, doesn\u2019t have]\ndelivery_mode: [normal, caesarian]\nThis {age} woman is in her {delivery_number} delivery and it is\n{delivery_time}. She has a {blood_pressure} blood pressure and\n{heart_problem} heart problems. ### Based on these attributes, this\nwoman is likely to deliver by {delivery_mode}\niris\nsepal_length, petal_length: [very short, short,\nmedium length, long, very long]\nsepal_width, petal_width: [very narrow, narrow,\nmedium width, wide, very wide]\nflower_type: [setosa, versicolor, virginica]\nThis iris flower has {sepal_length} and {sepal_width} sepals. It also\nhas {petal_length} and {petal_width} petals. ### Hence this flower\nis a {flower_type}\nvertebral-column\npelvic_incidence, pelvic_tilt, lumbar_lordosis_angle,\nsacral_slope, pelvic_radius, grade_of_spondylolisthesis:\n[very low, low, medium, high, very high]\nresult: [normal, abnormal]\nThis patient has a {pelvic_incidence} pelvic incidence, {pelvic_tilt}\npelvic tilt, and {lumbar_lordosis_angle} lumbar lordosis angle,\n{sacral_slope} sacral slope, {pelvic_radius} pelvic radius and\n{grade_of_spondylolisthesis} grade of spondylolisthesis. ### As a\nresult, the patient\u2019s vertebral-column is likely to be {result}\nstatlog-heart\nage: [very young, young, middle-aged, old, very old]\nsex: [male, female]\nchest_pain_type: [asymptomatic, nonanginal pain,\natypical angina, typical angina]\nbp, cholesterol, st_depression, heart_rate,\nnum_major_vessels: [very low, low, medium, high,\nvery high]\nfasting_blood_sugar: [high, low]\nelectrocardiographic_results: [having left ventricular\nhypertrophy, normal, having ST-T wave abnormality]\nslope_st_segment: [flat, upsloping, downsloping]\nexercise_induced_angina: [has, do not have]\ndefect_type: normal, reversible, fixed\npresence_of_heart_disease: [present, absent]\nThis individual is a/an {age} {sex} with {chest_pain_type} chest pain,\n{bp} resting blood pressure, and {cholesterol} serum cholesterol.\nTheir fasting blood sugar {fasting_blood_sugar} >120 mg/dl, they are\n{electrocardiographic_results} and a {heart_rate} maximum heart rate.\nThey {exercise_induced_angina} exercise-induced angina, and have a\n{st_depression} ST depression induced by exercise relative to rest.\nTheir peak exercise ST segment has a {slope_st_segment} slope, and\nthey have a {num_major_vessels} number of major vessels. The defect\ntype is {defect_type}. ### Hence heart disease is likely to be\n{presence_of_heart_disease}.\nhaberman-survival\nage_at_time_of_op: [very young, young,\nmiddle-aged, old, very old]\nyear_of_op: [1964, 1962, 1965, 1959, 1958, 1960,\n1966, 1961, 1967, 1963, 1969,1968]\nnum_pos_axillary_nodes: [very low, low, medium,\nhigh, very high]\nsurvival_status: [survived, died]\nThis patient was {age_at_time_of_op} at the time of operation in\n{year_of_op}. They had a {num_pos_axillary_nodes} number of\npositive axillary nodes detected. ### Therefore 5 years down the line,\nthe patient {survival_status}\nTable 6: Comparing test error rate of Summary Boosting backended by Curie and ChatGPT on all datasets (\u2193). Refer to caption of Table 1 for the notations used.\n<div style=\"text-align: center;\">Table 6: Comparing test error rate of Summary Boosting backended by Curie and ChatGPT on all datasets (\u2193). Refer to caption of Table 1 for the notations used.</div>\n\u2193\nDataset\nData Type\nSize\nCurie\nChatGPT\ncaesarian [cae] (42901)\n1c4d\n80\n0.300\u00b1 0.04\n0.406\u00b1 0.03\niris (61)\n4c0d\n150\n0.193\u00b1 0.03\n0.083\u00b1 0.01\ntae (48)\n1c4d\n151\n0.454\u00b1 0.03\n0.443\u00b1 0.04\nglass (41)\n9c0d\n214\n0.370\u00b1 0.02\n0.492\u00b1 0.02\nbreast-cancer [bc] (13)\n7c5d\n277\n0.288\u00b1 0.02\n0.360\u00b1 0.01\nvisualizing-environmental [ve] (678)\n3c0d\n111\n0.268\u00b1 0.03\n0.333\u00b1 0.04\nanalcatdata-chlamydia [ac] (535)\n2c2d\n100\n0.170\u00b1 0.01\n0.300\u00b1 0.06\nwine (43571)\n13c0d\n178\n0.320\u00b1 0.01\n0.250\u00b1 0.01\nblood-transfusion-center [btc] (1464)\n4c0d\n748\n0.240\u00b1 0.04\n0.433\u00b1 0.01\nsomerville-happiness-survey [shs] [Koczkodaj, 2018]\n0c7d\n143\n0.350\u00b1 0.02\n0.430\u00b1 0.02\nvehicle (54)\n18c0d\n846\n0.410\u00b1 0.04\n0.350\u00b1 0.16\nstatlog-heart [stath] [Dua and Graff, 2017]\n6c7d\n270\n0.430\u00b1 0.01\n0.370\u00b1 0.17\nverterbra-column [vc] (1524)\n6c0d\n310\n0.262\u00b1 0.01\n0.669\u00b1 0.03\necoli (1011)\n7c0d\n336\n0.270\u00b1 0.03\n0.193\u00b1 0.03\nhaberman-survival [hs] (43)\n3c0d\n306\n0.250\u00b1 0.01\n0.415\u00b1 0.03\ndiabetes [dia] (37)\n8c0d\n768\n0.344\u00b1 0.01\n0.297\u00b1 0.04\nvisualizing-hamster [hams] (708)\n5c0d\n73\n0.207\u00b1 0.00\n0.400\u00b1 0.08\nwholesale-customers [wc] (1511)\n6c1d\n440\n0.330\u00b1 0.00\n0.199\u00b1 0.04\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of effectively utilizing large language models (LLMs) as weak learners in boosting algorithms for tabular data classification, highlighting the limitations of previous methods and the necessity for a new approach that leverages LLMs' capabilities.",
        "problem": {
            "definition": "The problem focuses on the challenge of classifying tabular data using weak learners, specifically how to effectively integrate LLMs into boosting frameworks without the need for retraining or fine-tuning.",
            "key obstacle": "The main difficulty lies in the inherent structure of tabular data, which lacks the regularity found in other data types, making traditional deep learning methods less effective."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that LLMs can generate natural language summaries of tabular data, which can serve as effective prompts for classification tasks.",
            "opinion": "The proposed idea entails using LLMs to create summaries of carefully selected examples from tabular datasets, which can then be used in a boosting framework to improve classification performance.",
            "innovation": "The primary innovation is the introduction of a method called Summary Boosting, which utilizes LLM-generated summaries as weak learners in a boosting algorithm, differentiating it from traditional boosting methods that rely on retraining or fine-tuning."
        },
        "method": {
            "method name": "Summary Boosting",
            "method abbreviation": "SB",
            "method definition": "Summary Boosting is defined as a technique that employs large language models to generate summaries of tabular data examples, which are then utilized as weak learners within a boosting framework.",
            "method description": "The core of the method involves converting tabular data into natural language descriptions, generating summaries, and using these summaries to classify new data points.",
            "method steps": [
                "Convert tabular records into natural language descriptions using LLMs.",
                "Generate summaries from a selected subset of these descriptions.",
                "Use the summaries as prompts in a boosting algorithm to classify new examples."
            ],
            "principle": "This method is effective because it leverages the LLM's ability to summarize and extract relevant information from data, allowing it to function as a weak learner that can improve classification performance in a boosting context."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using 18 tabular datasets from the UCI repository and OpenML, comparing Summary Boosting against traditional methods like KNN, LIFT, TabPFN, and XGBoost, with a 50/10/40 train/validation/test split.",
            "evaluation method": "The performance of the method was assessed by measuring test errors across datasets and comparing results against baseline methods, using repeated experiments with different random seeds."
        },
        "conclusion": "The results demonstrate that Summary Boosting outperforms traditional methods, highlighting the potential of using LLMs as weak learners in boosting frameworks and suggesting new avenues for integrating LLMs into machine learning pipelines.",
        "discussion": {
            "advantage": "Key advantages include improved classification performance on tabular data, especially in scenarios with limited data, and the elimination of the need for finetuning LLMs.",
            "limitation": "Limitations include challenges with datasets having many continuous attributes, where Summary Boosting may underperform compared to methods like XGBoost.",
            "future work": "Future research should focus on enhancing the method's performance with larger datasets and exploring the use of more advanced LLMs, such as GPT-4, to further improve classification accuracy."
        },
        "other info": {
            "acknowledgements": "Yiding is supported by funding from Bosch Center of Artificial Intelligence."
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The method called Summary Boosting employs large language models to generate summaries of tabular data examples, which are then utilized as weak learners within a boosting framework."
        },
        {
            "section number": "4.2",
            "key information": "Summary Boosting leverages the LLM's ability to summarize and extract relevant information from data, allowing it to function as a weak learner that can improve classification performance in a boosting context."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on enhancing the performance of Summary Boosting with larger datasets and exploring the use of more advanced LLMs, such as GPT-4, to further improve classification accuracy."
        },
        {
            "section number": "1.2",
            "key information": "The paper highlights the necessity for a new approach that leverages LLMs' capabilities in boosting algorithms for tabular data classification."
        },
        {
            "section number": "3.2",
            "key information": "The proposed idea entails using LLMs to create summaries of carefully selected examples from tabular datasets, which can then be used in a boosting framework to improve classification performance."
        }
    ],
    "similarity_score": 0.7325931484782892,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Language models are weak learners.json"
}