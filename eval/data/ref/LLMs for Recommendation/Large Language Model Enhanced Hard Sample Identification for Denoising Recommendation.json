{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.10343",
    "title": "Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation",
    "abstract": "Implicit feedback, often used to build recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to alleviate this by identifying noisy samples based on their diverged patterns, such as higher loss values, and mitigating the noise through sample dropping or reweighting. Despite the progress, we observe existing approaches struggle to distinguish hard samples and noise samples, as they often exhibit similar patterns, thereby limiting their effectiveness in denoising recommendations. To address this challenge, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically, we construct an LLM-based scorer to evaluate the semantic consistency of items with the user preference, which is quantified based on summarized historical user interactions. The resulting scores are used to assess the hardness of samples for the pointwise or pairwise training objectives. To ensure efficiency, we introduce a variance-based sample pruning strategy to filter potential hard samples before scoring. Besides, we propose an iterative preference update module designed to continuously refine summarized user preference, which may be biased due to false-positive user-item interactions. Extensive experiments on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our approach.",
    "bib_name": "song2024largelanguagemodelenhanced",
    "md_text": "# Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation\nTianrui Song1, Wenshuo Chao1, Hao Liu1*\n1The Hong Kong University of Science and Technology (Guangzhou) tsong847@connect.hkust-gz.edu.cn, wchao@connect.ust.hk, liuh@ust.hk\nAbstract\nImplicit feedback, often used to build recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to alleviate this by identifying noisy samples based on their diverged patterns, such as higher loss values, and mitigating the noise through sample dropping or reweighting. Despite the progress, we observe existing approaches struggle to distinguish hard samples and noise samples, as they often exhibit similar patterns, thereby limiting their effectiveness in denoising recommendations. To address this challenge, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically, we construct an LLM-based scorer to evaluate the semantic consistency of items with the user preference, which is quantified based on summarized historical user interactions. The resulting scores are used to assess the hardness of samples for the pointwise or pairwise training objectives. To ensure efficiency, we introduce a variance-based sample pruning strategy to filter potential hard samples before scoring. Besides, we propose an iterative preference update module designed to continuously refine summarized user preference, which may be biased due to false-positive user-item interactions. Extensive experiments on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our approach.\narXiv:2409.10343v1\n# Introduction\nRecommender systems are designed to learn user preferences and suggest items across various online platforms, such as e-commerce, news portals, and social networks (2020; 2020; 2023). To train these systems, implicit feedback derived from user actions (e.g., clicks and purchases) is commonly employed due to its wide availability. Typically, each observed interaction is assumed to reflect a user\u2019s genuine interest in an item and is therefore assigned a positive label, while non-interacted items are considered negative (2020; 2021a). However, such a routine has recently been questioned that interacted items may be plagued by falsepositive noise (e.g., due to misclicks or popularity bias), while non-interacted items may suffer from false-negative noise (e.g., due to position bias) (2021b). These noisy interactions lead to inaccurate estimation of user preferences, hindering the performance of recommendation systems.\n*Corresponding author.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3f0/b3f007c9-828d-4702-9aa6-8e662e3ab201.png\" style=\"width: 50%;\"></div>\nFigure 1: Loss values and prediction scores during training LightGCN on Yelp dataset. We observe that hard and noisy samples exhibit similar values in prediction score and loss, making it difficult to differentiate them.\nDenoising recommendation has been proposed to mitigate the negative impact of noisy interactions through two primary strategies: 1) sample dropping and 2) sample reweighting. Dropping methods aim to improve model performance by selecting clean samples and discarding noisy ones during training (2021a; 2021). In contrast, re-weighting approaches assign lower weights to interactions identified as noisy, thereby reducing their influence on the model\u2019s learning process (2023; 2022). The success of these denoising techniques heavily depends on the accuracy of distinguishing between clean and noisy samples. Consequently, various data patterns have been explored as noisy signals (2021a; 2022; 2023). To name a few, loss value is one of the most commonly used signals, as noisy interactions typically exhibit higher loss values compared to clean ones (2021a; 2024). In addition, other indicators such as prediction scores (Wang et al. 2022) and gradients (Wang et al. 2023) have also been investigated to identify noisy samples. Despite significant advancements, existing methods often face the challenge of misidentifying hard samples as noisy ones. As illustrated in Figure 1, while noisy samples exhibit distinct patterns compared to easy samples, we observed that hard samples and noisy samples tend to present similar patterns in both prediction scores and loss values. Con-\nsequently, previous denoising approaches that rely solely on data patterns struggle to accurately distinguish between hard and noisy samples. This misclassification is problematic because hard samples have been shown to be beneficial, both empirically (2012) and theoretically (2023). Mistakenly treating hard samples as noise during the recommender training ultimately leads to suboptimal results. Recently, Large Language Models (LLMs) have demonstrated a promising ability to understand user preferences (Wu et al. 2024) and enhance item semantics (Wei et al. 2024), presenting a valuable opportunity to tackle the challenge of hard sample identification. Our key insight is that LLMs can be harnessed to summarize user preference and act as a scorer to analyze the consistency between user preferences and items, thereby identifying hard samples with the resulting scores. For example, when optimizing a model using a Bayesian Personalized Ranking (BPR) objective, the LLM scorer can effectively evaluate user preference scores of positive and negative items. As a result, samples with similar positive and negative scores are pinpointed as hard samples because they are inherently incompatible with the BPR training objective, which aims to maximize the divergence in scores. This allows us to mitigate the hard samples\u2019 misclassification issue in denoising recommender training. However, leveraging LLMs for this task is nontrivial due to two primary challenges. First, given the vast number of users and items, assessing the preferences of all users across all items is computationally intensive, especially considering the high inference cost of LLMs. Second, while LLMs can derive user preference by concluding interacted items, the presence of false-positive items in historical interactions can lead to biased user preference summarization. To address the challenges mentioned above, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework for recommendation, which comprises three key modules: Variance-based Sample Pruning, LLM-based Sample Scoring, and Iterative Preference Updating. To ensure efficiency, we first introduce a variancebased pruning strategy that progressively selects a small subset of hard sample candidates. Following this, we construct the LLM-based Sample Scoring module, where hard samples are identified by evaluating how well they satisfy the training objective. Specifically, the LLM scores the user preference for a given item by summarizing user preference, assesses the sample\u2019s hardness based on the pointwise or pairwise training objective, and determines whether it qualifies as a hard sample. Additionally, to enhance the accuracy of the summarized user preferences, we propose an Iterative Preference Updating module. It refines user preferences by adjusting for items that are mistakenly identified or overlooked during the summarization process, thereby improving the overall reliability of the LLMHD framework. Our main contributions are summarized as follows:\n\u2022 We propose LLMHD , a novel denoising recommendation approach that differentiates between hard and noisy samples leveraging LLMs. To the best of our knowledge, this is the first attempt to integrate LLMs into denoising recommendations.\n\u2022 The LLMHD addresses efficiency concerns through a variance-based sample pruning process. Furthermore, we enhance the effectiveness of the model by employing an iterative preference updating strategy, improving the LLMs\u2019 understanding of genuine user preferences. \u2022 Extensive experiments conducted on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our method. The results show that LLMHD delivers impressive performance and robust noise resilience.\n# Related Work\n# Related Work Denoise Recommendation\n# Denoise Recommendation\nRecommenders are pointed out to be affected by users\u2019 unconscious behaviors (2021b), leading to noisy data. As a result, many efforts are dedicated to alleviating the problem. These approaches can be categorized into two paradigms: sample dropping (2012; 2023) and sample re-weighting (2023; 2022). Sample dropping methods aim to keep clean samples and discard noisy ones. For instance, T-CE (2021a) observes that noisy samples exhibit high loss values and remove them during training. IR (2021c) iteratively generates pseudo-labels to discover noisy examples. Sample reweighting methods try to mitigate the impact of noisy samples by assigning lower weights to them. Typically, R-CE (2021a) assigns lower weights to noisy samples according to the prediction score. BOD (2023) considers the weight assignment as a bi-level optimization problem. Although these methods achieve promising results, they rely on data patterns to recognize noisy samples (e.g., loss values, and prediction scores) leading to difficulties in identifying hard samples from noise samples as they exhibit similar patterns.\n# LLMs for Recommendation\nLarge Language Models (LLMs) are effective tools for the Natural Language Processing field and have gained significant attention in the domain of Recommendation Systems (RS). For the adaption of LLMs in recommendations, existing works can be divided into three categories (2024): LLM as RS, LLM Embedding for RS, and LLM token for RS. The LLM as RS aims to transform LLMs into effective recommendation systems (Chao et al. 2024), such as LC-Rec (2024a) and LLM-TRSR (2024b). In contrast, the LLM embedding and LLM token for RS views the language model as an enhancer, where embeddings and tokens generated by LLMs are utilized for promoting recommender systems. The former typically adopts embeddings related to users and items, incorporating semantic information in the recommender (Ren et al. 2024). While the latter generates text tokens to capture potential preferences through user and item semantics (Wei et al. 2024; Xi et al. 2023). Despite the progress, these methods overlook the potential of LLMs in enhancing data denoising for recommendation.\n# Preliminary\nThe objective of training a recommender system is to learn a scoring function \u02c6yu,i = f\u03b8(u, i) from interactions between users u \u2208U and items i \u2208I. We assume that user-interacted\nitems y\u2217 ui = 1 are preferred by the user, while those not interacted y\u2217 ui = 0 are not. To optimize the scoring function f\u03b8(u, i), We employ Bayesian Personalized Ranking (BPR) loss and Binary Cross-Entropy (BCE) loss as loss function Lrec. These are formulated as:\n(1)\nwhere j denotes sampled negative items according to the pairwise sampling distribution PD\u2217, and D\u2217= {(u, i, y\u2217 ui) | u \u2208U, i \u2208I} represents the interaction dataset. The optimal parameter set \u03b8\u2217is obtained by minimizing the loss function:\n(3)\nBut this assumption is unreliable for two reasons: (1) False positive issue, user-interacted items might not reflect real user preference due to factors such as accidental clicks and position bias. (2) False negative issue, non-interacted items are not necessarily user dislikes, they may have been overlooked due to factors such as suboptimal display positions. These issues introduce noisy interactions, formally defined as \u02dcD = {(u, i, \u02dcy) | \u02dcy \u0338= y\u2217}. To address this, we formulate the denoising recommender training task as:\n(4)\naiming to learn high-quality recommender with parameters \u03b8\u2217by eliminating the effect of noisy samples. In this work, we focus on the challenge of hard samples, which are often mistakenly identified as noisy samples in existing denoising approaches, leading to suboptimal performance.\n# Proposed Method\nTo differentiate hard and noisy samples when denoising, we proposed the LLMHD framework, as illustrated in Figure 2. Before diving into the details of each module, we assume that each item i is accompanied by a text profile Pi. Additionally, we summarize the user\u2019s preference Pu = LLMs(Tsum({Pi | yu,i = 1})) by the profiles of interacted items with a prompt template Tsum designed for LLMs. Our LLMHD identifies hard samples through three key modules: (1) Variance-based Sample Pruning, (2) LLMbased Sample Scoring, and (3) Iterative Preference Updating. Variance-based Sample Pruning reduces the computation of calling LLMs by selecting a subset of hard sample candidates. LLM-based Sample Scoring evaluates the hardness of samples based on user preferences. Iterative Preference Updating refines the understanding of user preference, ensuring accurate identification of hard samples.\n# Loss-based Denoising\nWe first introduce the denoising module implemented based on the widely accepted assumption (2021a) that samples\nwith higher loss values are more likely to be noisy. Specifically, for each data sample b in the mini-batch B, we calculate the corresponding loss value l(b) and sort all samples in the ascending order,\n(5)\nwhere |B| denotes the batch size. This operation assists the noisy sample identification, which we formulate as BN,\n(6)\n\ufffd \ufffd where T denotes the current training iteration. The \u03b5l represents a dynamic threshold, calculated as,\n(7)\nwhere \u03b5max l is a hyper-parameter representing the maximum noise ratio, and \u03b1 is a factor that modulates the growth rate of the noise threshold. The \u03b5l increases as the stability of prediction scores incrementally improves during training, following previous works (Wang et al. 2021a). It is worth mentioning that the BN inadvertently contain hard samples, given that both hard and noisy samples manifest similar patterns in loss values. This requires further refinement to distinguish genuine noisy data and hard samples.\n# Variance-based Sample Pruning\nAlthough it is possible to present all identified noisy samples BN to the LLMs for scoring, this approach would be prohibitively time-consuming due to the massive interactions in the recommender system. Specifically, hard sample candidates are selected based on the observation of previous work (2020), which demonstrated that hard samples exhibit relatively higher prediction score variance compared to noisy samples. Therefore, for samples b \u2208BN, we calculate the prediction scores variance of positive vp,b and negative vn,b items across multiple epochs (see Equation 17). Then sort them in descending order based on vp and vn respectively,\n(9)\nwhere |Bp N| and |Bn N| denotes the number of positive and negative items in the BN respectively. Hard sample candidates BHC are collected by,\n (10)\nwhere \u03b5v \u2208[0, 1] denotes the proportion of hard samples. With the increasing |BN| more candidates will be selected in latter training iterations and provided to LLM-based Sample Scoring to identify hard samples further.\n# LLM-based Sample Scoring\nOwing to the resemblance in data patterns between hard and noisy samples, distinguishing them solely through numerical disparities is ineffective. To eliminate this issue, we introduce the LLM-based Sample Scoring method. LLMs act as scorer to provide auxiliary information that evaluates the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b28d/b28d7784-b55a-4755-8e0b-3d3f4a36d4f6.png\" style=\"width: 50%;\"></div>\nFigure 2: The overview of the LLMHD framework. LLMHD leverages LLMs to differentiate hard and noisy samples, thereby enhancing the denoising recommender training task. The framework identifies hard samples through three main modules: (1) Variance-based Sample Pruning, (2) LLM-based Sample Scoring, and (3) Iterative Preference Updating.\nsample\u2019s hardness. Formally, we prompt LLMs to score the user preference for item su,i with a template Tscore(Pu, Pi) that wraps the user preference text Pu and item profile Pi,\n(11)\nThe resulting score su,i is adopted to specify the sample\u2019s hardness by analyzing its compatibility with the training objective. Lower compatibility samples are considered harder as they are more challenging to satisfy the objective. Given that most recommenders are trained to minimize pointwise (e.g., BCE) or pairwise (e.g., BPR) losses, we devise two paradigms for hard sample identification: (1) Pointwise Preference Scoring, and (2) Pairwise Preference Scoring. Pointwise Sample Scoring The pointwise BCE loss, as shown in Equation 2, aims at reducing the classification uncertainty of a (user, item) pair. For a data sample (u, ipos) or (u, ineg), if the user\u2019s preference for the item is ambiguous, the sample is of low compatibility with the training objective. Therefore, positive pair with lower su,ipos and negative pair with higher su,ineg are harder samples, thereby hard samples are identified by,\n(12)\n\uf8f4 \uf8f3 where the \u03b5pos and \u03b5neg are thresholds that control the hardness. In addition, since previous works (2021a) discussed that fitting harder samples at the early training stage might hurt the generalization ability, we smoothly change \u03b5pos and \u03b5neg during each training iteration T as follows,\n(14)\nwhere \u03b5max pos , \u03b5min pos , \u03b5max neg , \u03b5min neg are hyper-parameters. In this way, harder positive (u, ipos) and negative (u, ineg) samples will be identified in the latter iterations, benefiting the recommender by gradually increasing the hardness. Pairwise Sample Scoring Similar to the pointwise sample scoring, we identify hard samples under the pairwise training schema. Specifically, according to Equation 1, the pairwise BPR loss aims to maximize the divergence of prediction scores between positive and negative items. For a sample (u, ip, in), if the user\u2019s preference for the positive item does not significantly surpass that for the negative, the sample is less compatible with the objective. Therefore, hard samples are identified through the indicator function,\n(15)\n  where the threshold \u03b5pair also gradually decreases to increase the hardness by the number of iteration T,\n(16)\nBased on the above technique, we differentiate hard samples in both pointwise and pairwise training schema.\n# Iterative Preference Updating\nAccurate user preference Pu is critical for effective LLM sample scoring. However, the Pu summarized based on interacted items do not fully capture user interests due to the inclusion of disliked items, i.e., false-positives, and the exclusion of liked items, i.e., false-negatives. To mitigate this problem, we refine user preferences iteratively by excluding dislikes and incorporating likes. For every epoch t, we calculate the variance score vd of user-item pairs d = (u, i),\n(17)\nwhere \u02c6yj d is the prediction score of user-item pair d in the j-th training epoch, and the variance vd is calculated over m time intervals prior to the t-th training iteration. We divided variance scores into two groups, positive and negative samples, and ordered from lowest to highest,\n(18)\n(19)\nwhere dp k, dn k are the k-th positive and negative sample respectively. To identify whether a sample is a false positive or false negative in the j-th epoch, we use the indicators Ij fp(dp k \u2264dp \u03b5l) and Ij fn(dn k \u2265dn \u03b5l) respectively. The threshold \u03b5l employed here follows the same definition as introduced in Equation 7. We design a robust mechanism to select confident items for preference updates. Formalized as follows,\n(20)\nthe \u03b5\u03b3 is a confidence threshold. We then leverage LLMs to refine preference Pu based on identified false-positives (u, ifp) and false negatives (u, ifp) with the template TFP(Pu, Pifp) and TFN(Pu, Pifn),\n(21) (22)\nP P P where P\u2217 u is the updated user preference text description. The template TFP intend to add descriptioins about ifp in the user preference Pu, while the TFN reduce the feature of ifn.\n# Denoising Training with Hard Samples\nThe denoising training is done by keeping hard samples and dropping noisy samples. We first define the set of identified hard samples BH as,\n(23)\n   where the ILLM is either Ipoint or Ipair based on the format of data samples. The recommendation loss Lrec is then calculated in the following format,\n(24)\nIn this way, hard samples have remained and the noisy samples are dropped while training the recommender.\n# Experiments\nWe compare LLMHD with state-of-the-art denoise approaches on four backbones and three real-world datasets to demonstrate the effectiveness of our method. Experiments are directed by the following research questions (RQs): \u2022 RQ1: How does LLMHD performs compared with other state-of-the-art denoise baselines across the datasets? \u2022 RQ2: Does the LLMHD demonstrate robustness when tackling different levels of noisy data? \u2022 RQ3: What is the effect of different components and hyper-parameters within the LLMHD on performance?\nWe compare LLMHD with state-of-the-art denoise approaches on four backbones and three real-world datasets to demonstrate the effectiveness of our method. Experiments are directed by the following research questions (RQs):\n\u2022 RQ1: How does LLMHD performs compared with other state-of-the-art denoise baselines across the datasets? \u2022 RQ2: Does the LLMHD demonstrate robustness when tackling different levels of noisy data? \u2022 RQ3: What is the effect of different components and hyper-parameters within the LLMHD on performance?\n# Experiment Settings\nExperiment Settings Datasets. We conduct evaluations of our LLMHD on three public datasets: (1) Amazon-Books collected from the Amazon platform. We conduct experiments on the book subcategories. (2) Yelp is a large-scale dataset that provides check-in history. (3) Steam consists of users and electronic games on the Steam platform. Since we adopt the item profile provided in (Ren et al. 2024), we process datasets following their settings. Details are in the Appendix. Evaluation Metrics. Following existing works on denoising recommendation (2021c; 2024), we report the results w.r.t. two widely used metrics: NDCG@K and Recall@K, where higher scores indicate better performance. For a comprehensive comparison, we set the K as 5 and 10 for both metrics on all three datasets. Baselines. To evaluate the performance of LLMHD, we apply it to the following backbone recommenders: \u2022 NGCF (2019) models the user-item interaction graph with GNN for collaborative filtering. \u2022 LightGCN (2020) removes the feature transformation and non-linear activation in NGCF. \u2022 SGL (2021) generates positive views of with model-level node and edge dropout for self-supervised learning. \u2022 NCL (2022) exploit the neighborhood structure to conduct self-supervised learning in graph recommenders. To compare the denoising effect, each recommender is trained with the following approaches: \u2022 BCE represents the model is trained with the base pointwise binary cross-entropy loss. \u2022 BPR (2009) represents the model is trained with the pairwise Bayesian Personalized Ranking loss. \u2022 T-CE (2021a) removes the samples with higher loss through dynamic threshold. \u2022 R-CE (2021a) is guided by a similar assumption as TCE, while allocating lower weights to noise samples. \u2022 RGCF (2022) discard noisy edges according to the structure representation cosine similarity and enhance the diversity with graph self-supervised learning. \u2022 DCF (2024) gradually relabel noisy samples to address the scarcity issue when denoising.\nImplementation Details. All methods are trained with a batch size of 1024 and a learning rate of 0.005 with Adam optimizer for up to 200 epochs. We adopt the early stopping during training. We adopt the RecBole implementation for all backbone models. Hyper-parameters are selected based on the origin setting. We did a grid search on the following hyper-parameters to find the optimal result for LLMHD, including \u03b1 and \u03b5max l , which are explored within {3k, 5k, 10k, 30k, 50k} and {0.01, 0.03, 0.05, 0.1, 0.2} respectively. The hard sample proportion \u03b5v is selected from {0.1, 0.3, \u00b7 \u00b7 \u00b7 , 0.9}. All thresholds are fixed as follows, \u03b5max pos =8, \u03b5min pos = 6, \u03b5max neg = 4 and \u03b5min neg = 2 for pointwise hard sample identification. The pairwise \u03b5max pair and \u03b5min pair are fixed as 7 and 3 respectively. The confidence threshold \u03b5\u03b3 to update user preference is explored within {3,5,7,9}.\nDataset\nAmazon-book\nYelp\nSteam\nBackbone\nMethod\nR@5\nR@10\nN@5\nN@10\nR@5\nR@10\nN@5\nN@10\nR@5\nR@10\nN@5\nN@10\nNGCF\nBCE\n0.0353\n0.0570\n0.0365\n0.0438\n0.0236\n0.0431\n0.0283\n0.0350\n0.0223\n0.0405\n0.0236\n0.0305\nBPR\n0.0389\n0.0651\n0.0406\n0.0494\n0.0280\n0.0495\n0.0338\n0.0405\n0.0381\n0.0629\n0.0453\n0.0525\nT-CE\n0.0393\n0.0650\n0.0402\n0.0489\n0.0259\n0.0450\n0.0313\n0.0373\n0.0257\n0.0448\n0.0288\n0.0354\nR-CE\n0.0366\n0.0587\n0.0369\n0.0444\n0.0254\n0.0438\n0.0302\n0.0360\n0.0236\n0.0435\n0.0254\n0.0328\nRGCF\n0.0415\n0.0658\n0.0422\n0.0502\n0.0287\n0.0485\n0.0344\n0.0406\n0.0401\n0.0644\n0.0472\n0.0543\nDCF\n0.0398\n0.0617\n0.0399\n0.0472\n0.0281\n0.0488\n0.0353\n0.0414\n0.0264\n0.0446\n0.0308\n0.0365\nLLMHDBCE\n0.0406\n0.0668\n0.0413\n0.0503\n0.0276\n0.0477\n0.0329\n0.0386\n0.0267\n0.0459\n0.0297\n0.0364\nLLMHDBPR\n0.0455\n0.0743\n0.0455\n0.0552\n0.0338\n0.0579\n0.0398\n0.0474\n0.0418\n0.0696\n0.0496\n0.0579\nLightGCN\nBCE\n0.0558\n0.0849\n0.0565\n0.0665\n0.0390\n0.0660\n0.0481\n0.0557\n0.0448\n0.0732\n0.0529\n0.0612\nBPR\n0.0587\n0.0904\n0.0598\n0.0704\n0.0359\n0.0609\n0.0446\n0.0516\n0.0510\n0.0828\n0.0597\n0.0693\nT-CE\n0.0590\n0.0895\n0.0592\n0.0697\n0.0401\n0.0677\n0.0504\n0.0580\n0.0463\n0.0758\n0.0555\n0.0640\nR-CE\n0.0557\n0.0834\n0.0566\n0.0658\n0.0389\n0.0650\n0.0474\n0.0550\n0.0461\n0.0757\n0.0543\n0.0630\nRGCF\n0.0619\n0.0956\n0.0644\n0.0753\n0.0420\n0.0693\n0.0501\n0.0579\n0.0519\n0.0849\n0.0599\n0.0702\nDCF\n0.0590\n0.0898\n0.0596\n0.0701\n0.0403\n0.0680\n0.0503\n0.0579\n0.0477\n0.0778\n0.0562\n0.0650\nLLMHDBCE\n0.0607\n0.0921\n0.0607\n0.0711\n0.0408\n0.0689\n0.0514\n0.0589\n0.0469\n0.0767\n0.0563\n0.0647\nLLMHDBPR\n0.0652\n0.0999\n0.0655\n0.0767\n0.0427\n0.0731\n0.0518\n0.0611\n0.0536\n0.0867\n0.0624\n0.0722\nSGL\nBCE\n0.0589\n0.0902\n0.0604\n0;.0707\n0.0377\n0.0655\n0.0470\n0.0548\n0.0433\n0.0682\n0.0505\n0.0676\nBPR\n0.0608\n0.0956\n0.0621\n0.0736\n0.0373\n0.0629\n0.0465\n0.0538\n0.0529\n0.0838\n0.0613\n0.0704\nT-CE\n0.0602\n0.0909\n0.0622\n0.0720\n0.0408\n0.0697\n0.0502\n0.0587\n0.0449\n0.0720\n0.0532\n0.0609\nR-CE\n0.0591\n0.0901\n0.0601\n0.0702\n0.0386\n0.0645\n0.0476\n0.0550\n0.0456\n0.0732\n0.0538\n0.0618\nRGCF\n0.0675\n0.1049\n0.0681\n0.0808\n0.0416\n0.0715\n0.0512\n0.0606\n0.0552\n0.0881\n0.0639\n0.0736\nDCF\n0.0626\n0.0933\n0.0641\n0.0740\n0.0413\n0.0683\n0.0506\n0.0583\n0.0455\n0.0727\n0.0536\n0.0615\nLLMHDBCE\n0.0615\n0.0931\n0.0640\n0.0739\n0.0414\n0.0708\n0.0509\n0.0596\n0.0462\n0.0742\n0.0543\n0.0619\nLLMHDBPR\n0.0693\n0.1051\n0.0717\n0.0837\n0.0426\n0.0718\n0.0523\n0.0619\n0.0546\n0.0887\n0.0641\n0.0739\nNCL\nBCE\n0.0574\n0.0871\n0.0598\n0.0694\n0.0391\n0.0647\n0.0477\n0.0548\n0.0450\n0.0731\n0.0529\n0.0612\nBPR\n0.0605\n0.0942\n0.0628\n0.0740\n0.0369\n0.0609\n0.0451\n0.0515\n0.0511\n0.0835\n0.0602\n0.0698\nT-CE\n0.0599\n0.0898\n0.0619\n0.0719\n0.0411\n0.0679\n0.0507\n0.0582\n0.0461\n0.0751\n0.0543\n0.0627\nR-CE\n0.0585\n0.0874\n0.0604\n0.0696\n0.0399\n0.0655\n0.0487\n0.0558\n0.0459\n0.0750\n0.0540\n0.0625\nRGCF\n0.0694\n0.1045\n0.0706\n0.0819\n0.0396\n0.0660\n0.0480\n0.0560\n0.0534\n0.0863\n0.0621\n0.0718\nDCF\n0.0619\n0.0929\n0.0624\n0.0727\n0.0424\n0.0696\n0.0513\n0.0589\n0.0465\n0.0759\n0.0550\n0.0635\nLLMHDBCE\n0.0609\n0.0915\n0.0629\n0.0730\n0.0417\n0.0690\n0.0517\n0.0591\n0.0468\n0.0762\n0.0551\n0.0633\nLLMHDBPR\n0.0719\n0.1053\n0.0741\n0.0846\n0.0432\n0.0716\n0.0542\n0.0620\n0.0540\n0.0861\n0.0624\n0.0717\nTable 1: Performance comparison of backbone recommenders trained with different denoising approaches. R and N refer to Recall and NDCG, respectively. The highest scores are in bold, and the runner-ups are with underline. All results are statistically significant according to the t-tests with a significance level of p < 0.01.\nTable 1: Performance comparison of backbone recommenders trained with different denoising approaches. R and N refer to Recall and NDCG, respectively. The highest scores are in bold, and the runner-ups are with underline. All results are statistically significant according to the t-tests with a significance level of p < 0.01.\n# Performance Comparison (RQ1)\nWe evaluated the effectiveness of our proposed method in both pointwise LLMHDBCE and pairwise LLMHDBPR setting. The performance against other denoising recommendation strategies is shown in Table 1. Notably, LLMHD significantly enhances the performance of all backbone models trained with normal BCE or BPR on all datasets, demonstrating its superior denoising capability. Moreover, LLMHD consistently outperforms other advanced denoising methods across the majority of datasets and backbones. We attribute this improvement to the extended hard sample identification, where baselines like T-CE and RGCF lack the capability, rendering them less effective in comparison. We also observed that for most datasets and backbones, RGCF and DCF are inferior to us alone. In contrast, other denoise baselines like T-CE and R-CE perform worse than them. This can be explained by the fact that both RGCF and DCF are designed to insert or preserve high-confidence interactions, a feature not inherent to T-CE and R-CE. Since\nLLMHD also focuses on retaining more interactions (i.e., hard samples), we posit that maintaining a more extensive set of samples is beneficial in enhancing performance.\n# Noise Robustness (RQ2)\nWe conduct random noise training to evaluate the robustness of the noise resistance capability of LLMHD, comparing it with the most competitive RGCF and the classical approach T-CE. Following previous work (2021), the proportion of noise injected into the training set spanned from 5% to 20%, while keeping the samples in the testing set unchanged. We report the result in Figure 3. The result shows that: (1) As the noise ratio increases, we observe a consistent downward trend in the performance across all backbone models and denoising strategies. This decline can be caused by the rising noise level leading to the increasing data corruption, which complicates discerning genuine user preferences. (2) LLMHD outperforms the backbone model and other denoise approaches in all noise ratios. This empha-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d397/d397777e-fae0-4abd-b5b8-f7fb241df1e1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Performance comparison of denoise training with random noises in Amazon-books.</div>\nMethods\nAmazon-books\nR@5\nR@10\nN@5\nN@10\nLightGCN\n0.0587\n0.0904\n0.0598\n0.0704\n+ LLMHDLD\n0.0614\n0.0965\n0.0628\n0.0743\n+ LLMHDLD+RS+LMS\n0.0623\n0.0970\n0.0635\n0.0749\n+ LLMHDLD+VS+LMS\n0.0638\n0.0986\n0.0655\n0.0767\n+ LLMHDLD+VS+LMS+PU\n0.0652\n0.0999\n0.0665\n0.0771\nTable 2: The effect of each components in LLMHDBPR with the LightGCN on Amazon-books dataset.\nsizes LLMHD\u2019s promising noise resistance, attributed to the correctly identified hard and noisy samples. (3) we also observed that RGCF shows sub-optimal results and slower performance degradation with increasing noise. This is probably because it employs random edge augmentation, which makes the model adapt to Gaussian noise. However, when confronted with real-world data noise, i.e., when additional noise is smaller, its performance falls short of LLMHD.\n# In-depth Model Analysis (RQ3)\nAblation Study. We conduct experiments to assess each module in LLMHD, including Variance-based Sample Pruning, LLM-based Sample Scoring, and Iterative Preference Updating, the result is shown in Table 2. (1) We investigate whether including Variance-based Sample Pruning (VS) enables an effective hard sample candidate selection. Specifically, we compare it with Random Selection (RS) and select the same amount of candidates as the VS. According to the result, converting the VS to RS leads to a performance drop in all metrics. This reveals the superiority of the Variancebased Sample Pruning in selecting hard sample candidates. (2) We discover whether LLM-identified hard samples enhance the recommendation performance. The comparison is made between the backbone that only adopts a Loss-based Denoise Module (LD) and the one that includes LLM-based Sample Scoring (LMS). Significant improvement is demonstrated after using LLMs to detect hard samples, indicating the advancement of LLM in hard sample identification. (3) We explore the influence of adopting Iterative Preference Updating (PU). Compared with discarding the preference updating, the performance of adopting it increases, demon-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a101/a101db6b-6b0d-4b43-9437-7094365742f6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Hyper-parameter analysis in LLMHDBPR with LightGCN backbone on the Amazon-books.</div>\nstrating the effectiveness of Preference Updating in understanding genuine user preference.\nEffect of Hyper-parameters. For a more elaborate analysis, we adjust the hyper-parameters within the range described in the Implementation Details section. The results are shown in Figure 4. From our observations: (1) Growth Rate \u03b1: A moderate increase in \u03b1 enhances performance, as it retains more samples per iteration, mitigating data scarcity during training. However, excessively high values degrade performance. (2) Max Noise Scale \u03b5max l : Elevating \u03b5max l initially improves LLMHD by filtering out more noise, but an overly high setting results in excessive sample loss, hampering the learning of user preferences. (3) Hard Sample Candidate Proportion \u03b5v: Increasing \u03b5v presents more hard sample candidates, boosting performance. But setting it too high may confuse noisy samples for hard ones, lowering overall effectiveness. (4) Confidence Threshold \u03b5\u03b3: Gradually raising \u03b5\u03b3 initially benefits the model by promoting item selection for preference update. However, a high confidence restricts item discovery and a low confidence finds incorrect items, both diminishing user preference understanding.\n# Conclusion\nIn this work, we introduced the Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework to address the challenge of distinguishing hard samples from noise samples for recommender systems. By utilizing an LLM-based scorer to evaluate semantic consistency between users and items and assessing sample hardness according to its compatibility with training objectives, we can differentiate hard samples from noise samples. We further introduce a variance-based sample pruning strategy to effectively select candidates. In addition, the iterative preference update refines user preference and mitigates biases introduced by false-positive interactions. Extensive experiments on realworld datasets and recommenders demonstrated the effectiveness of LLMHD in improving recommendation quality.\n# References\nChao, W.; Zheng, Z.; Zhu, H.; and Liu, H. 2024. Make Large Language Model a Better Ranker. arXiv:2403.19181. Chen, C.; Zheng, S.; Chen, X.; Dong, E.; Liu, X.; Liu, H.; and Dou, D. 2021. Generalized Data Weighting via Classlevel Gradient Manipulation. In Neural Information Processing Systems. Ding, J.; Quan, Y.; Yao, Q.; Li, Y.; and Jin, D. 2020. Simplify and robustify negative sampling for implicit collaborative filtering. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920. Red Hook, NY, USA: Curran Associates Inc. ISBN 9781713829546. Gantner, Z.; Drumond, L.; Freudenthaler, C.; and SchmidtThieme, L. 2012. Personalized ranking for non-uniformly sampled items. In Proceedings of KDD Cup 2011, 231\u2013247. PMLR. Gao, Y.; Du, Y.; Hu, Y.; Chen, L.; Zhu, X.; Fang, Z.; and Zheng, B. 2022. Self-guided learning to denoise for robust recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1412\u20131422. He, X.; Deng, K.; Wang, X.; Li, Y.; Zhang, Y.; and Wang, M. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201920, 639\u2013648. New York, NY, USA: Association for Computing Machinery. ISBN 9781450380164. He, Z.; Wang, Y.; Yang, Y.; Sun, P.; Wu, L.; Bai, H.; Gong, J.; Hong, R.; and Zhang, M. 2024. Double Correction Framework for Denoising Recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Lin, W.; Zhao, X.; Wang, Y.; Zhu, Y.; and Wang, W. 2023. Autodenoise: Automatic data instance denoising for recommendations. In Proceedings of the ACM Web Conference 2023, 1003\u20131011. Lin, Z.; Tian, C.; Hou, Y.; and Zhao, W. X. 2022. Improving graph collaborative filtering with neighborhoodenriched contrastive learning. In Proceedings of the ACM web conference 2022, 2320\u20132329. Luo, H.; Zhou, J.; Bao, Z.; Li, S.; Culpepper, J. S.; Ying, H.; Liu, H.; and Xiong, H. 2020. Spatial Object Recommendation with Hints: When Spatial Granularity Matters. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. Ren, X.; Wei, W.; Xia, L.; Su, L.; Cheng, S.; Wang, J.; Yin, D.; and Huang, C. 2024. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024, 3464\u20133475. Rendle, S.; Freudenthaler, C.; Gantner, Z.; and SchmidtThieme, L. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, 452\u2013461. Arlington, Virginia, USA: AUAI Press. ISBN 9780974903958.\nShi, W.; Chen, J.; Feng, F.; Zhang, J.; Wu, J.; Gao, C.; and He, X. 2023. On the theories behind hard negative sampling for recommendation. In Proceedings of the ACM Web Conference 2023, 812\u2013822. Tian, C.; Xie, Y.; Li, Y.; Yang, N.; and Zhao, W. X. 2022. Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201922, 122\u2013132. New York, NY, USA: Association for Computing Machinery. ISBN 9781450387323. Wang, W.; Feng, F.; He, X.; Nie, L.; and Chua, T.-S. 2021a. Denoising implicit feedback for recommendation. In Proceedings of the 14th ACM international conference on web search and data mining, 373\u2013381. Wang, W.; Feng, F.; He, X.; Zhang, H.; and Chua, T.-S. 2021b. Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1288\u20131297. Wang, X.; He, X.; Wang, M.; Feng, F.; and Chua, T.-S. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval, 165\u2013174. Wang, Y.; Xin, X.; Meng, Z.; Jose, J. M.; Feng, F.; and He, X. 2022. Learning robust recommenders through crossmodel agreement. In Proceedings of the ACM Web Conference 2022, 2015\u20132025. Wang, Z.; Gao, M.; Li, W.; Yu, J.; Guo, L.; and Yin, H. 2023. Efficient bi-level optimization for recommendation denoising. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2502\u20132511. Wang, Z.; Xu, Q.; Yang, Z.; Cao, X.; and Huang, Q. 2021c. Implicit feedbacks are not always favorable: Iterative relabeled one-class collaborative filtering against noisy interactions. In Proceedings of the 29th ACM International Conference on Multimedia, 3070\u20133078. Wei, W.; Ren, X.; Tang, J.; Wang, Q.; Su, L.; Cheng, S.; Wang, J.; Yin, D.; and Huang, C. 2024. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 806\u2013815. Wu, J.; Wang, X.; Feng, F.; He, X.; Chen, L.; Lian, J.; and Xie, X. 2021. Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, 726\u2013735. Wu, L.; Zheng, Z.; Qiu, Z.; Wang, H.; Gu, H.; Shen, T.; Qin, C.; Zhu, C.; Zhu, H.; Liu, Q.; et al. 2024. A Survey on Large Language Models for Recommendation. arXiv:2305.19860. Xi, Y.; Liu, W.; Lin, J.; Cai, X.; Zhu, H.; Zhu, J.; Chen, B.; Tang, R.; Zhang, W.; Zhang, R.; and Yu, Y. 2023. Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models. arXiv:2306.10933. Zhang, W.; Liu, H.; Xiong, H.; Xu, T.; Wang, F.; Xin, H.; and Wu, H. 2023. RLCharge: Imitative Multi-Agent Spatiotemporal Reinforcement Learning for Electric Vehicle Charging\nStation Recommendation. IEEE Transactions on Knowledge and Data Engineering, 35: 6290\u20136304. Zheng, B.; Hou, Y.; Lu, H.; Chen, Y.; Zhao, W. X.; Chen, M.; and Wen, J.-R. 2024a. Adapting large language models by integrating collaborative semantics for recommendation. In 2024 IEEE 40th International Conference on Data Engineering (ICDE), 1435\u20131448. IEEE. Zheng, Z.; Chao, W.; Qiu, Z.; Zhu, H.; and Xiong, H. 2024b. Harnessing large language models for text-rich sequential recommendation. In Proceedings of the ACM on Web Conference 2024, 3207\u20133216.\n# Supplementary Materials\nDetails of Dataset\nIn this section, we offer details of the preprocessed dataset adopted in the experiment. We take the datasets in RLMRec (Ren et al. 2024), in which each item contains a corresponding item text profile. Therefore, we follow the preprocessing setting in the RLMRec. Specifically, interactions with ratings below 3 for both the Amazon-books and Yelp data are filtered out. No rating-based filtering is adopted in Steam. K-core filtering is also performed and split into training, validation, and test sets using a 3:1:1 ratio. The statistics of datasets preprocessed following RLMRec are shown in Table 3.\nDatasets\n# Users # Items # Interactions # Sparsity\nAmazon-books\n11,000\n9,332\n120,464\n99.88%\nYelp\n11,091\n11,010\n166,620\n99.86%\nSteam\n23,310\n5,237\n316,190\n99.74%\n<div style=\"text-align: center;\">Table 3: Statistics of preprocessed datasets.</div>\nHowever, previous works adopted the rating score to label noise and clean data. For example, T-CE regards a rating score below 3 as a false-positive interaction. As a result, the dataset filtered with ratings in RLMRec is regarded to contain less noisy interactions. To compare the denoising ability of different methods, we add 5% noisy interactions to the training set. These noisy interactions are selected from the interactions that are rated below 3. Experiments are then conducted on these noise-inserted datasets.\n# Details of API Token Cost\nSince we adopt the GPT-3.5-turbo as the LLM in the LLMHD, here we provide the token number for training with LLMHD in Table 4. The total token number of Tscore in LLMHD is highly dependent on two aspects: the number of interactions in the dataset, the value of the maximum noise scale \u03b5max l , and the hard sample proportion \u03b5v. Whears, the Tsum is not correlated to the hyper-parameters. We also report the token number for TF N, and TF P during training when the confidence threshold is set to \u03b5\u03b3 = 3.\nWe provide the details of plotting the Figure 1. For the noise samples, we follow the settings of (Wang et al. 2021a), tak-\nDatasets\nTemplate\n\u03b5max\nl\n\u03b5v\n# Token\nAmazon-books\nTsum\n-\n-\n16m\nTF N\n-\n-\n6m\nTF P\n-\n-\n3m\nTscore\n0.05\n0.5\n11m\n0.10\n0.5\n21m\n0.05\n0.3\n7m\nYelp\nTsum\n-\n-\n19m\nTF N\n-\n-\n8m\nTF P\n-\n-\n3m\nTscore\n0.05\n0.5\n16m\n0.10\n0.5\n30m\n0.05\n0.3\n10m\nSteam\nTsum\n-\n-\n40m\nTF N\n-\n-\n10m\nTF P\n-\n-\n4m\nTscore\n0.05\n0.5\n35m\n0.10\n0.5\n62m\n0.05\n0.3\n20m\n<div style=\"text-align: center;\">Table 4: OpenAI API token number.</div>\ning the interactions that rate below 3 as the false-positive noise. By flipping labels of ground truth records in the test set, we can obtain a set of false-negative interactions that are positively labeled but unobserved during the negative sampling process. Samples in which the positive item is falsepositive and the negative item is false-negative are considered noisy samples. We then identify hard and easy samples according to the setting in (Ding et al. 2020). For each positive interaction, D negative items are sampled. Then the negative item with the highest prediction score of \u02c6yu,i is adopted as the negative instance during training. Thus, when the D gets higher, the sample becomes harder. According to this setting, we collect the prediction score and loss value results when D = 1 and that of hard samples when D = 3.\n# Details of Prompt Template\nIn this section, we offer comprehensive information on the templates utilized in the LLMHD. Real examples from the Amazon-book dataset are used as a showcase. The templates used in the Yelp and Steam datasets are with minor differences in the instructions provided to represent different data.\nIn this section, we offer comprehensive information on the templates utilized in the LLMHD. Real examples from the Amazon-book dataset are used as a showcase. The templates used in the Yelp and Steam datasets are with minor differences in the instructions provided to represent different data. Example of User Preference Summarization. Figure 5 cases an example of user preference profile generation, specifically for the Amazon-book dataset. The instruction provided to the language model for all users remains consistent, directing the LLMs to summarize the characteristics of books that would appeal to the user. The input item profiles consist of the book title and a corresponding item description from the dataset. To facilitate parsing the output, we enforce the output format to the XML. The generated result demonstrates that the LLMs, in this case ChatGPT, capture the common features from the interacted items. Example of LLM Sample Scoring. Figure 6 illustrates the process of scoring user preference for an item with LLMs. In order to achieve the user preference for a specific\nExample of LLM Sample Scoring. Figure 6 illustrates the process of scoring user preference for an item with LLMs. In order to achieve the user preference for a specific\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/29a1/29a17e7e-302c-49d5-ae51-2a46f8918b54.png\" style=\"width: 50%;\"></div>\n# User Preference Summarization  (\ud835\udc7b\ud835\udc94\ud835\udc96\ud835\udc8e)\nYou are a professional book editor. Below is the information about books that a reader has read: 1. <ITEM PROFILE 1> \u2026 N. <ITEM PROFILE N> \nYou are a professional book editor. Below is the information about books that a reader has read: 1. <ITEM PROFILE 1>\n# Based on the books the reader has read, please summarize characteristics of books this reader may  enjoy.\nYou MUST provide the summarization with the following format: <summarization>A summarization of  what kinds of books this reader is likely to enjoy.</summarization>. Please ensure that the \" summarization \" is no longer than 100 words. You should not provide any explanation except the summarization.\n# User Preference Scoring (\ud835\udc7b\ud835\udc94\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea05/ea05e82f-547f-43d6-920f-9d3c1d098966.png\" style=\"width: 50%;\"></div>\nYou are a professional book editor. Below is the reading preference of a reader: <USER PROFILE>\nNow here is the descriptions of a book: <ITEM PROFILE>\nYou MUST provide\n<s>8</s>\nFigure 6: Example of user preference scoring process on Amazon-books dataset.\n# User Preference Update with False Positives (\ud835\udc7b\ud835\udc6d\ud835\udc77)\nUser Preference Update with False Positives (\ud835\udc7b\ud835\udc6d\ud835\udc77)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3e6/b3e63d38-3265-4da7-ad49-ec9ec5f97f83.png\" style=\"width: 50%;\"></div>\nYou are a professional book editor.\nYou will help me to determine a reader\u2019s reading preference.\nI will provide you with: \nREADER PROFILE: a description of the reader\u2019s potential reading preferences.\nNOT INTERESTED BOOKS: descriptions of books that the reader might not interested in.\nREADER PROFILE: <USER PROFILE>\nNOT INTERESTED BOOKS:\n1. <ITEM PROFILE 1>\n\u2026\nBased on the characteristics of \"NOT INTERESTED BOOKS\", please adjust the provided \"READER \nPROFILE\" to make it more compatible with the reader\u2019s actual reading preferences\nYou MUST answer in following format: <profile>....</profile> (e.g. <profile>The reader is likely to \nenjoy...</profile>)\nPlease ensure that the \"profile\" is no longer than 100 words.\nDo not provide any other text except the \"profile\" string.\n# system prompt\n# item profile\n# task description\n# output format\n# user profile\n<profile>The reader is likely to enjoy books that explore magical worlds and supernatural beings, with a \nfocus on strong female leads \u2026 </profile>\n# LLM Response\nUpdated User Profile\nInput Prompt\nYou are a professional book editor \u2026\u2026\nREADER PROFILE:\nThe reader enjoys a variety of genres including supernatural romance, young adult fantasy, \ncontemporary romance, and paranormal romance \u2026..\nNOT INTERESTED BOOKS:\n1. Fans of paranormal romance and fiction who enjoy reading series with prequels and sequels \nwould enjoy New Beginnings: Prequel to Others of Edenton. \n\u2026..\n# item profile\n# user profile\nFigure 7: Example of update user preference with False-positive item.\nUser Preference Update with False Negatives (\ud835\udc7b\ud835\udc6d\ud835\udc75)\nYou are a professional book editor.\nYou will help me to determine a reader\u2019s reading preference.\nI will provide you with: \nREADER PROFILE: a description of the reader\u2019s potential reading preferences.\nINTERESTED BOOKS: descriptions of books that the reader might be interested in.\nREADER PROFILE: <USER PROFILE>\nINTERESTED BOOKS:\n1. <ITEM PROFILE 1>\n\u2026\nBased on the characteristics of \u201cINTERESTED BOOKS\u201d, please adjust the provided \u201cREADER PROFILE\u201d to \nmake it more compatible with the reader\u2019s actual reading preferences\nYou MUST answer in following format: <profile>....</profile> \u2026\n# system prompt\n# item profile\n# task description\n# output format\n# user profile\nFigure 8: Example of update user preference with false-negative item.\nYou are a professional book editor. You will help me to determine a reader\u2019s reading preference.\nNOT INTERESTED BOOKS: 1. <ITEM PROFILE 1>\n\u2026\nBased on the characteristics of \"NOT INTERESTED BOOKS\", please adjust the provided \"READER PROFILE\" to make it more compatible with the reader\u2019s actual reading preferences You MUST answer in following format: <profile>....</profile> (e.g. <profile>The reader is likely to  enjoy...</profile>) Please ensure that the \"profile\" is no longer than 100 words. Do not provide any other text except the \"profile\" string.\nBased on the characteristics of \"NOT INTERESTED BOOKS\", please adjust the provided \"READE PROFILE\" to make it more compatible with the reader\u2019s actual reading preferences\nYou MUST answer in following format: <profile>....</profile> (e.g. <profile>The reader is likely to  enjoy...</profile>) Please ensure that the \"profile\" is no longer than 100 words. Do not provide any other text except the \"profile\" string.\nYou are a professional book editor \u2026\u2026\nREADER PROFILE:\n# User Preference Update with False Negatives (\ud835\udc7b\ud835\udc6d\ud835\udc75)\nYou are a professional book editor. You will help me to determine a reader\u2019s reading preference.\nI will provide you with:  READER PROFILE: a description of the reader\u2019s potential reading preferences. INTERESTED BOOKS: descriptions of books that the reader might be interested in.\nINTERESTED BOOKS: 1. <ITEM PROFILE 1>\nBased on the characteristics of \u201cINTERESTED BOOKS\u201d, please adjust the provided \u201cREADER PROFILE\u201d to  make it more compatible with the reader\u2019s actual reading preferences\nYou MUST answer in following format: <profile>....</profile> \u2026\nitem, the prompt template incorporates the user preference profile generated in the User preference Profile generation and the item profile in the dataset. By utilizing both information, LLMs are empowered to infer the user\u2019s preference. In the presented example, leveraging the user preference profile\nand item profile, the large language model assesses the preference for the book accurately. In addition, to maintain consistency, we ask the model to generate scores from 1 to 10, and correlated descriptions about the meaning of different scores are also provided. This enables the model to generate\nscores in a specific range, making subsequent judgments of hard samples easier.\nExample of User Preference Update. Figure 7 shows the overall process of updating user preference with Falsepositive items. With the provided item profile and user profile, the LLM successfully refined the provided user preference profile by removing correlated user-dislike item characteristics. This demonstrates the promising ability of LLMs to understand actual user preferences by providing user-like or disliked items. A similar effect is also achieved by updating user preference with False-negative items, where the prompt template is shown in Figure 8.\n",
    "paper_type": "method",
    "attri": {
        "background": "Implicit feedback in recommender systems often suffers from noise due to misclicks and position bias. Previous methods have focused on identifying noisy samples through diverged patterns, but they struggle to distinguish hard samples from noise, limiting their effectiveness in denoising recommendations.",
        "problem": {
            "definition": "The paper addresses the challenge of accurately differentiating hard samples from noise samples in the context of denoising recommendations.",
            "key obstacle": "Existing methods often misidentify hard samples as noise due to their similar patterns in prediction scores and loss values, leading to suboptimal performance in recommender systems."
        },
        "idea": {
            "intuition": "The authors propose that Large Language Models (LLMs) can effectively summarize user preferences and evaluate item semantics to identify hard samples.",
            "opinion": "The proposed idea involves an LLM-based scorer that assesses the semantic consistency of items with user preferences, enabling the identification of hard samples.",
            "innovation": "The key innovation is the integration of LLMs into the denoising process, allowing for a more accurate distinction between hard and noisy samples compared to traditional methods."
        },
        "method": {
            "method name": "Large Language Model Enhanced Hard Sample Denoising",
            "method abbreviation": "LLMHD",
            "method definition": "LLMHD is a framework designed to differentiate hard samples from noise in recommender systems using LLMs to evaluate semantic consistency and sample hardness.",
            "method description": "The method leverages LLMs to enhance the identification of hard samples through a scoring mechanism based on user preferences.",
            "method steps": [
                "Variance-based Sample Pruning to filter potential hard samples.",
                "LLM-based Sample Scoring to evaluate the hardness of samples.",
                "Iterative Preference Updating to refine user preferences."
            ],
            "principle": "The method is effective due to its ability to leverage LLMs for understanding user preferences and accurately assessing the compatibility of samples with training objectives."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three real-world datasets (Amazon-Books, Yelp, Steam) using four backbone recommenders to evaluate LLMHD's performance.",
            "evaluation method": "The performance was assessed using metrics such as NDCG@K and Recall@K, comparing LLMHD with state-of-the-art denoising methods."
        },
        "conclusion": "The LLMHD framework successfully distinguishes hard samples from noise, significantly improving recommendation quality across various datasets and recommenders.",
        "discussion": {
            "advantage": "LLMHD demonstrates superior performance in identifying hard samples, leading to enhanced denoising capabilities compared to existing methods.",
            "limitation": "The method may still face challenges in computational efficiency due to the high inference cost associated with LLMs.",
            "future work": "Future research could explore optimizing the efficiency of LLM usage and further refining the identification process for hard samples."
        },
        "other info": {
            "datasets": [
                {
                    "name": "Amazon-Books",
                    "users": 11000,
                    "items": 9332,
                    "interactions": 120464,
                    "sparsity": "99.88%"
                },
                {
                    "name": "Yelp",
                    "users": 11091,
                    "items": 11010,
                    "interactions": 166620,
                    "sparsity": "99.86%"
                },
                {
                    "name": "Steam",
                    "users": 23310,
                    "items": 5237,
                    "interactions": 316190,
                    "sparsity": "99.74%"
                }
            ],
            "hyper_parameters": {
                "learning_rate": 0.005,
                "batch_size": 1024,
                "epochs": 200
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommendation algorithms are crucial for enhancing user experience, particularly in distinguishing hard samples from noise in implicit feedback."
        },
        {
            "section number": "3.2",
            "key information": "The LLMHD method integrates Large Language Models to enhance semantic understanding, allowing for a more accurate distinction between hard and noisy samples."
        },
        {
            "section number": "4.1",
            "key information": "LLMHD utilizes Large Language Models to evaluate semantic consistency and sample hardness, showcasing the capabilities of LLMs in understanding user preferences."
        },
        {
            "section number": "8",
            "key information": "The integration of LLMs into the denoising process significantly improves the identification of hard samples, addressing limitations of traditional methods."
        },
        {
            "section number": "10.1",
            "key information": "Existing methods misidentify hard samples as noise, which is a challenge in conventional recommendation systems that LLMHD aims to overcome."
        },
        {
            "section number": "11",
            "key information": "The LLMHD framework enhances recommendation quality by effectively distinguishing hard samples from noise, reflecting the importance of LLM integration in recommendation systems."
        }
    ],
    "similarity_score": 0.7502275529487121,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation.json"
}