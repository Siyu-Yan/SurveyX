{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.12406",
    "title": "Enhancing In-context Learning via Linear Probe Calibration",
    "abstract": " Abstract\n 22 Jan 2024\nIn-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model\u2019s output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improvement of up to 21%, and up to a 50% improvement in some cases, and significantly boosts the performance of PEFT methods, especially in the low resource regime. Moreover, LinC achieves lower expected calibration error, and is highly robust to varying label proportions, prompt templates, and demonstration permutations. Our code is available at https://github.com/mominabbass/LinC.\n[cs.CL]\n# 1 Introduction\nLarge language models (LLMs), have remarkably showcased their capabilities across a broad range of natural\nProceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain. PMLR: Volume TBD. Copyright 2024 by the author(s).\nlanguage processing tasks [11, 13, 3, 29, 59, 39]. The cost of training these large models can be prohibitively expensive. Therefore, the commonly adopted approach is to first pre-train with large amounts of unlabled data and then fine-tune the model to downstream tasks. Although fine-tunin",
    "bib_name": "abbas2024enhancingincontextlearninglinear",
    "md_text": "# -context Learning via Linear Probe Calib\n# Enhancing In-context Learning via Linear Probe Calibration\nMomin Abbas\u22c6 Yi Zhou\u2020 Parikshit Ram\u2020 Nathalie Baracaldo\u2020 Horst Samulowitz\u2020 Theodoros Salonidis\u2020 Tianyi Chen\u22c6 \u22c6Rensselaer Polytechnic Institute \u2020IBM Research\n# Abstract\n 22 Jan 2024\nIn-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model\u2019s output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improvement of up to 21%, and up to a 50% improvement in some cases, and significantly boosts the performance of PEFT methods, especially in the low resource regime. Moreover, LinC achieves lower expected calibration error, and is highly robust to varying label proportions, prompt templates, and demonstration permutations. Our code is available at https://github.com/mominabbass/LinC.\n[cs.CL]\n# 1 Introduction\nLarge language models (LLMs), have remarkably showcased their capabilities across a broad range of natural\nProceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain. PMLR: Volume TBD. Copyright 2024 by the author(s).\nlanguage processing tasks [11, 13, 3, 29, 59, 39]. The cost of training these large models can be prohibitively expensive. Therefore, the commonly adopted approach is to first pre-train with large amounts of unlabled data and then fine-tune the model to downstream tasks. Although fine-tuning LLMs can be effective, it is prone to instability [34] due to numerous hyperparameter configurations resulting in failed runs, unstable results, and over-fitting [41, 11, 25]. Moreover, fine-tuning models of such large size may also be expensive and also requires explicit access to the architecture and weights of LLMs, which may not be publicly available [60].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/00a1/00a12ae9-e720-484c-bd93-2fafcc3d89be.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Example of ICL with a LLM \u03b8\u2217.</div>\nTo avoid large fine-tuning times and avoid requiring access to the weights of the model, recently, LLMs, exemplified by GPT-3 [3], have demonstrated the ability to perform in-context learning (ICL), a capability whereby a model can generate an appropriate output for a given query input based on a prompt that includes input-output example pairs specific to the task at hand. ICL can work with an API without explicit access to the LLM. Figure 1 provides a visual representation of ICL. The prompts utilized in ICL entail task-specific in conjunction with a series of input-label pairs, referred to as demonstrations. Such a capability of LLM to learn \u201cin-context\u201d presents an intriguing aspect whereby the model is capable of acquiring knowledge and performs well on a wide range of downstream tasks without any task-specific parameter fine-tuning [3, 48, 2, 40]. More specifically, the aim of ICL is to make a prediction on some query test sample x by conditioning on a prompt sequence (fx(x1), fy(y1), . . . , fx(xk), fy(yk), fx(x)) containing k-shot samples (xi, yi)k i=1 (i.e. demonstrations) and the query test sample, where the functions fx(.), fy(.) denote template functions that attach pre-defined\nTo avoid large fine-tuning times and avoid requiring access to the weights of the model, recently, LLMs, exemplified by GPT-3 [3], have demonstrated the ability to perform in-context learning (ICL), a capability whereby a model can generate an appropriate output for a given query input based on a prompt that includes input-output example pairs specific to the task at hand. ICL can work with an API without explicit access to the LLM. Figure 1 provides a visual representation of ICL. The prompts utilized in ICL entail task-specific in conjunction with a series of input-label pairs, referred to as demonstrations. Such a capability of LLM to learn \u201cin-context\u201d presents an intriguing aspect whereby the model is capable of acquiring knowledge and performs well on a wide range of downstream tasks without any task-specific parameter fine-tuning [3, 48, 2, 40].\nMore specifically, the aim of ICL is to make a prediction on some query test sample x by conditioning on a prompt sequence (fx(x1), fy(y1), . . . , fx(xk), fy(yk), fx(x)) containing k-shot samples (xi, yi)k i=1 (i.e. demonstrations) and the query test sample, where the functions fx(.), fy(.) denote template functions that attach pre-defined\ndescriptions to the input and output, respectively (c.f. text highlighted in yellow in Figure 1). The output template function fy(.), in addition, may transform labels yi into a natural language format instead into numeric/one-hot labels (e.g. for binary classification transforming labels (0, 1) to (Positive, Negative) (c.f. labels in Figure 1)). Mathematically, for an input x, a prompt P is defined as:\nP(x, (xi, yi)k i=1) \u225cd1 \u2295d2 \u2295\u00b7 \u00b7 \u00b7 \u2295dk \u2295fx(x) (1\n(1)\nwhere each demonstration di is given by fx(xi)\u2295fy(yi) and \u2295denotes the concatenation operation.\nRecent studies suggest that ICL exhibits high variability in performance across different prompt templates, demonstrations, and their arrangement within the prompt arising from biases that favor outputting certain answers [63, 32], resulting in performance variation from random guess to state-of-the-art. Further, when the prompt P includes several more demonstrations, ICL\u2019s performance is thwarted by the inherent maximum sequence length limitation of the underlying language model. Moreover, our initial analysis reveals that while GPT-like models\u2019 ICL ability delivers acceptable results, their predictions cannot be considered reliable when assessed using Shannon entropy. We will discuss these limitations and challenges of ICL in detail in Section 2. In this paper, we argue that by training very few parameters and subsequently utilizing an affine transformation on the output probabilities to calibrate the model, the performance, as well as the reliability of these predictions, can be significantly enhanced. We propose linear probe calibration (LinC), which optimizes the calibration parameters (i.e. a low-dimensional matrix and vector) using only a few extra samples and minimal computation. Experimental results reveal that LinC significantly outperforms the baselines. Moreover, LinC produces reliable predictions that exhibit consistency across a range of prompt templates and various permutations of demonstrations.\nRecent studies suggest that ICL exhibits high variability in performance across different prompt templates, demonstrations, and their arrangement within the prompt arising from biases that favor outputting certain answers [63, 32], resulting in performance variation from random guess to state-of-the-art. Further, when the prompt P includes several more demonstrations, ICL\u2019s performance is thwarted by the inherent maximum sequence length limitation of the underlying language model. Moreover, our initial analysis reveals that while GPT-like models\u2019 ICL ability delivers acceptable results, their predictions cannot be considered reliable when assessed using Shannon entropy. We will discuss these limitations and challenges of ICL in detail in Section 2.\n# We summarize our contributions below:\nWe summarize our contributions below:\nC1) We present a novel insight that, while GPT-like models often exhibit acceptable ICL performance, their predictions appear to have very low confidence when measured using the Shannon entropy, highlighting a potential cause for their highly variable performance. C2) We propose linear probe calibration (LinC), a simple and black-box method that enhances model\u2019s reliability and performance by linearly calibrating output probabilities without requiring any access to model weights or architecture.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb6b/bb6b2ed6-4005-44f8-b938-a744874554fe.png\" style=\"width: 50%;\"></div>\nFigure 2: The efficacy of ICL is restricted by GPT tokenizer\u2019s maximum sequence length limit. Black-dashed lines demarcate the point beyond which additional shots cannot be utilized.\nC3) We empirically show that LinC consistently outperforms baselines on various benchmark datasets, with a performance boost of up to 21%, on average, and up to a 50% improvement in some cases, compared to the vanilla ICL baseline.\n# 2 ICL: Challenges and Opportunities\nIn this section, we will first highlight two key limitations of ICL with current LLMs, which then serve as the motivation of our subsequent algorithm design.\n# 2.1 Maximum Sequence length Limitation\nWe demonstrate the maximum sequence length limitation of ICL [58] on different datasets in Figure 2 using GPT-2 tokenizer. We observe that the test performance of ICL improves consistently, which aligns with the power-law relations between the generalization error and data size [24, 42]. However, beyond a certain point, no additional demonstrations can be added within the prompt since the model reaches its maximum sequence length limit1. Alas, the potential for performance enhancement through the acquisition of more examples is often constrained by this limitation of ICL, even in few-shot settings.\n# Motivated by this, we ask:\nGiven a few additional samples, beyond the LLM sequence length limit, can we further improve ICL test performance without utilizing LLM fine-tuning?\n1It is noteworthy that although some recent models do have expanded context windows (e.g. GPT-4 with 32k tokens), the majority of them still maintain smaller context windows. Smaller models are often preferred in various applications due to their flexibility in adapting to specific tasks and their ability to achieve comparable performance to larger models while using much fewer compute.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/540e/540ebb81-0e95-4648-b4c3-52d6e4276511.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Shannon entropy histograms of using vanilla ICL on GPT-2-XL (1.5B) vs our method on SST-2 (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n# 2.2 Entropy of Few-Shot Learning with GPT\nIn practical decision-making systems, it is crucial for prediction networks to not only exhibit high accuracy but also have the ability to identify the likelihood of incorrect predictions. For example, automated healthcare systems should be designed such that when the confidence level of a disease diagnosis network is low, the control is transferred to human doctors for making the diagnoses [21]. Well-calibrated confidence estimates play a crucial role in making machine learning models more interpretable. Since humans possess an inherent cognitive understanding of probabilities [8], having reliable confidence estimates can enhance the user\u2019s confidence in the model\u2019s predictions. This is especially relevant for neural networks, where the rationale behind their classification decisions can be complex and challenging to decipher. One way to assess the reliability of a model\u2019s predictions is to measure the Shannon entropy. Shannon entropy [44] quantifies the amount of expected uncertainty in a probability distribution. Mathematically, Shannon entropy is given by:\n(2)\nwhere C is the number of classes and pc represents the output probability of class c. Entropy value is used to gauge prediction confidence, with a low value indicating high confidence and vice versa. We compare the performance of ICL on GPT before (vanilla ICL) and after our calibration (Section 3) using different numbers of shots on various datasets. Figure 3 encapsulates the results on SST-2 (for other datasets, see Appendix B). We observe that, in contrast to the performance of our model, using vanilla ICL on GPT leads to high values of entropy, implying that most test predictions were made with very low confidence, i.e., close to random guessing. This observation indicates that although vanilla ICL (i.e., uncalibrated) on GPT yields satisfactory results in terms of test accuracy (refer to Table 4), the confidence associated with\nwhere C is the number of classes and pc represents the output probability of class c. Entropy value is used to gauge prediction confidence, with a low value indicating high confidence and vice versa.\nthese predictions is not entirely reliable. These findings align with previous studies, including [18], that have demonstrated the inadequacy of the conventional GPT decision boundary in effectively distinguishing between predictions by using the output with the highest probability as the predicted label. While low entropy (or high confidence) does not always imply a high accuracy, high entropy typically implies a high degree of uncertainty in predictions. This uncertainty may contribute to the increased variability of test performance, such as variations due to varying label proportions, prompt templates, and demonstration permutations (see Section 5.2). Motivated by this, we ask:\nCan we make the predictions made by GPT-like models more reliable and accurate?\nAs we will see in the upcoming sections, our proposal to linearly calibrate model\u2019s output probabilities via meticulously learned parameters not only improves its test performance but also enhances its reliability.\n# 3 Linear Probe Calibration\nIn order to be considered reliable, a model must furnish a calibrated confidence measure along with its predictions, where the probability assigned to the predicted class corresponds to its actual likelihood [16].\nSince vanilla ICL predictions may be unreliable due to the presence of high entropy prediction probabilities, we aim to investigate whether probability calibration can lead to improvements. In this section, we present linear probe calibration (LinC) that calibrates the model\u2019s output probabilities, making it more reliable.\n# One method for modifying output probabilities applies an affine transformation [38, 16]:\nOne method for modifying output probabilities applies an affine transformation [38, 16]:\n(3)\n# \u02dcp = softmax(Ap + b)\nwhere A and b are parameters to be applied to the original probabilities p to get new probabilities \u02dcp. For example, for classification tasks p is the set of proba-\n(4)\nfor a model M\u03b8\u2217parameterized by \u03b8\u2217.\nGiven a few additional samples, we then optimize A and b using a validation set, starting with a zero initialization; we find that our method exhibits remarkable insensitivity to initialization and works quite well for zero/random initialization (see Figure 10). More specifically, given a validation set (xv i , yv i )Nv i=1 of size Nv, we create Nv validation prompts via:\n(5)\nOur objective is to solve the following problem\n(6)\nwhere L represents the loss function of the calibration parameters A and b. We use a gradient-based optimizer to optimize A and b using prompts P v i . Algorithm 1 encapsulates our proposed LinC method. While we employ the stochastic gradient descent algorithm, our method can be utilized with any available optimization algorithm. LinC incurs negligible computational overhead and can be implemented in just a few lines of code to compute and store A and b. Moreover, LinC utilizes only k +Nv extra samples to learn A and b. Finally, test predictions are obtained by calculating Ap + b and taking the argument of the maxima after the softmax operator.\nComparison with other methods. Unlike calibration methods that rely on the raw data (xv i , yv i )Nv i=1, our approach draws inspiration from ICL and employs prompts {P v i }Nv i=1 to learn the calibration parameters. Moreover, ICL performance is limited by the maximum input sequence length constraint of the underlying language model, causing poor scalability with an increase in the number of available training samples as shown in Figure 2. In contrast, our approach is scalable with respect to the number of available data samples, since we use the available samples to optimize the calibration parameters. In fact, our method requires only a handful of additional samples (typically in the range 10-100) to effectively optimize the calibration parameters, maintaining the few-shot regime and making it much more sample efficient than fine-tuning LLMs which require orders of magnitude larger number of samples. LinC is also much more computationally efficient as compared to other methods used to enhance LLM performance, such as fine-tuning, since it optimizes extremely low 2For detailed information about where the transforma-\n2For detailed information about where the transformation is applied, please refer to Appendix B.\nAlgorithm 1 Linear Probe Calibration (LinC)\n1: Input: LLM \u03b8\u2217, validation set (xv\ni , yv\ni )Nv\ni=1, k-shots\n(xj, yj)k\nj=1, loss function L\n2: Initialize: parameters A0\n0, b0\n0 via zero initialization,\nstep-size \u03b1, number of epochs T\n3: For each P v\ni in (5), obtain logits (4): pv\ni = p(P v\ni )\n4: for t = 1, \u00b7 \u00b7 \u00b7 , T do\n5:\nfor i = 1, \u00b7 \u00b7 \u00b7 , Nv do\n6:\nCompute \u02c6pv\ni = Ai\u22121\nt\u22121pv\ni + bi\u22121\nt\u22121\n7:\nAi\nt\u22121 = Ai\u22121\nt\u22121 \u2212\u03b1\u2207AL(\u02c6pv\ni , yv\ni )\n8:\nbi\nt\u22121 = bi\u22121\nt\u22121 \u2212\u03b1\u2207bL(\u02c6pv\ni , yv\ni )\n9:\nend for\n10:\nA0\nt = ANv\nt\u22121 and b0\nt = bNv\nt\u22121\n11: end for\n12: return A0\nT , b0\nT\ndimensional calibration parameters. Lastly, unlike incontext fine-tuning and meta-training methods [33, 55], LinC operates using API access alone and doesn\u2019t need access to the LLM architecture and weights.\nShot\nMethod\nSST-2\nTREC\nSubj\n4-shot\nNoC\n66.313.1\n24.06.3\n52.84.9\nConC\n78.98.4\n41.34.7\n67.88.9\nNoC*\n68.98.6\n35.90.7\n69.310.5\nConC*\n75.93.9\n43.52.5\n61.910.2\nLinC\n86.22.9\n45.93.3\n72.911.1\n8-shot\nNoC\n57.28.0\n31.88.1\n56.610.7\nConC\n73.710.5\n45.41.7\n68.19.0\nNoC*\n62.77.7\n40.57.1\n70.48.8\nConC*\n75.88.1\n47.22.3\n63.07.7\nLinC\n79.110.0\n48.05.4\n76.83.5\nTable 1: Comparison under same number of samples.\nTable 1: Comparison under same number of samples.\n# 4 LinC: A Viable Solution\nBefore presenting our results, we first highlight the significance of linear calibration and why it is important when we are handicapped by limited resources. Therefore we ask: assuming that the maximum sequence length limit is not exceeded, what is the best way to use the additional samples in ICL? Is calibration the optimal way to use these additional samples? To answer this question, we introduce two additional baselines, NoC* and ConC*3, which utilize the same 10 validation samples used for training the calibration parameters in our method LinC. For these baselines, these 10 samples are treated as additional in-context test demonstrations within the test prompts. Consequently, NoC* and ConC* should be considered 3for details about NoC and ConC, see Section 5.1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/002b/002bf30c-28a1-476d-9960-c5360f6e61e1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 4: LinC outperforms ICL on all few-shot experiments, and substantially enhances PEFT, especially in the low resource regime, while maintaining almost identical data and compute requirements.\nto be in the 14-shot and 18-shot regimes instead of the 4-shot and 8-shot regimes, respectively. The choice of using 10 samples is to ensure that the maximum length limit does not exceed for any of the datasets used in this experiment. Table 1 shows the results on three different datasets in the 4-shot and 8-shot settings on GPT-2-XL. It is noteworthy that for all three datasets, using the same samples to learn the calibration parameters is better than using them within the test prompts as additional test demonstrations. Moreover, we encapsulate our finding in Figure 4 by assessing the trade-off between performance and resource consumption, quantified as the product of trainable parameters, samples, and epochs. The raw ICL baselines (0/8-shot) are denoted by horizontal lines. Notably, LinC outperforms ICL baselines by a substantial margin while maintaining resource efficiency. In addition, it enhances the performance of Parameter-Efficient Fine-tuning (PEFT) methods, such as Soft Prompt Tuning (SPT) [27] and Low-Rank Adaptation (LoRA) [19], particularly in scenarios with limited data and computational resources. These observations render LinC particularly effective in situations where compute resources are limited and training data is scarce.\n# 5 Experiments\nThis section will demonstrate the effectiveness of LinC on several benchmarks in the few-shot and PEFT settings. We run our experiments on GPT-2-XL [39] with 1.5B parameters, GPT-J [53] with 6B parameters, and Llama-2 [48] with 13B4 parameters on 2\n4Note that this is the largest model that can fit into our available GPU memory.\nDataset\nGPT-J (6B)\nICL\nConC\nLinC\nSST-2\n0.0592\n0.1974\n0.0458\nSST-5\n0.2254\n0.0933\n0.0970\nAGNews\n0.2858\n0.0686\n0.0577\nTREC\n0.2934\n0.1108\n0.1613\nDBPedia\n0.2713\n0.2281\n0.0502\nRTE\n0.0471\n0.0825\n0.0421\nTable 2: Expected Calibration Error (ECE) comparison between baselines and LinC (a model with perfect calibration would exhibit an ECE of 0).\nDataset (D/C)\nGPT-J (6B)\nICL\nConC\nLinC\nHamster (5/2)\n55.6\u00b18.3\n53.3\u00b19.4\n60.0\u00b114.4\nCustomers (8/2)\n67.4\u00b10.5\n54.9\u00b12.3\n68.6\u00b10.5\nBreast (7/2)\n66.7\u00b13.3\n55.8\u00b19.2\n71.3\u00b13.3\nSpambase (57/2)\n40.0\u00b10.0\n51.2\u00b19.5\n61.1\u00b11.7\nTAE (5/3)\n45.2\u00b14.6\n48.4\u00b19.5\n50.5\u00b111.0\nVehicle (18/4)\n26.9\u00b10.7\n29.0\u00b12.8\n29.0\u00b11.5\nLED (7/10)\n16.3\u00b110.4\n27.3\u00b16.9\n28.0\u00b17.1\n<div style=\"text-align: center;\">GPT-J (6B)</div>\nTable 3: Performance comparison between baselines and LinC on OpenML datasets; D represents the number of features and C represents the number of classes.\nNVIDIA GeForce RTX 3090 GPUs. We thoroughly study different label proportions, prompt templates, and demonstration permutations.\n# 5.1 Experimental Setup\nWe evaluate the effectiveness of our LinC method using seven widely used text-classification datasets: sentiment analysis using SST-2 [46] and SST-5 [46], topic classification using the 4-way AGNews [61] and 14way DBPedia [61], 6-way question classification using TREC [51], textual entailment using binary RTE [9] from SuperGLUE [52], and subjectivity classification using Subj [36]. We also tested LinC on seven diverse non-text classification tasks using OpenML [49] datasets with varying number of classes and features. A fixed prompt format was utilized for each dataset, unless stated otherwise, which is demonstrated alongside examples in Appendix A, Table 6 and Table 7. Experiments were conducted under 0-shot, 1-shot, 4shot and 8-shot learning settings. Five different sets of test demonstrations were chosen at random, and arranged in an arbitrary order in the prompt, and the mean and standard deviation were computed across all\nExperiments were conducted under 0-shot, 1-shot, 4shot and 8-shot learning settings. Five different sets of test demonstrations were chosen at random, and arranged in an arbitrary order in the prompt, and the mean and standard deviation were computed across all\nModel\nShots Method SST-2\nSST-5\nAGNews TREC\nDBpedia RTE\nSubj\nAvg\nGPT-2-XL 1.5B\n0-shot\nNoC\n64.50.0\n33.70.0\n44.30.0\n28.70.0\n58.70.0\n48.00.0\n56.70.0\n47.8\nConC\n70.90.0\n20.30.0\n65.30.0\n41.70.0\n50.00.0\n50.50.0\n73.00.0\n53.1\nLinC\n71.60.0\n41.30.0 65.70.0\n42.00.0 73.00.0\n54.50.0 73.30.0\n60.2\n1-shot\nNoC\n59.713.2\n28.39.7\n39.610.3\n27.15.9\n40.515.9\n53.41.0\n54.68.8\n43.3\nConC\n76.71.8\n31.14.8\n63.93.3\n40.53.1\n62.37.5\n52.51.7\n61.37.4\n55.5\nLinC\n83.216.9 40.23.7 64.86.3\n42.95.1 63.17.4\n54.41.9 73.75.5\n60.3\n4-shot\nNoC\n66.313.1\n34.14.8\n40.414.1\n24.06.3\n66.710.2\n52.23.2\n52.84.9\n48.1\nConC\n78.98.4\n34.46.8\n60.46.8\n41.34.7\n72.14.8\n53.00.9\n67.88.9\n58.3\nLinC\n87.11.9\n39.15.6 69.06.5\n46.03.2 73.14.8\n53.60.6 73.610.8 63.1\n8-shot\nNoC\n57.28.0\n31.89.3\n41.45.8\n31.88.1\n59.016.4\n52.50.9\n56.610.7\n47.2\nConC\n73.710.5\n28.25.4\n57.912.2\n45.41.7\n71.85.7\n53.41.1 68.19.0\n56.9\nLinC\n79.110.0 38.19.6 63.37.2\n48.16.1 71.95.5\n53.20.9\n76.93.9\n61.5\nGPT-J 6B\n0-shot\nNoC\n66.30.0\n33.70.0\n36.00.0\n24.70.0\n19.70.0\n55.60.0\n65.70.0\n43.1\nConC\n58.00.0\n40.70.0\n56.00.0\n40.00.0\n48.00.0\n52.40.0\n58.70.0\n50.5\nLinC\n74.30.0\n46.00.0 64.30.0\n70.70.0 69.30.0\n56.70.0 70.70.0\n64.6\n1-shot\nNoC\n67.36.7\n35.33.5\n65.314.3\n40.38.8\n64.916.6\n50.73.7\n65.19.9\n55.6\nConC\n88.31.7\n46.93.1\n74.95.8\n62.64.7\n80.23.3\n53.41.2\n59.12.2\n66.5\nLinC\n88.51.7\n50.41.3 81.74.6\n63.04.4 82.63.7\n56.01.8 69.97.5\n70.3\n4-shot\nNoC\n88.93.3\n46.33.2\n72.36.1\n37.74.2\n82.114.4\n55.07.1\n57.46.9\n62.8\nConC\n92.83.1\n49.44.8\n75.24.1\n46.05.1\n88.94.9\n56.01.9\n65.311.8\n67.7\nLinC\n94.90.9\n51.13.7 77.95.5\n65.31.5 89.45.4\n58.43.7 68.411.2 72.2\n8-shot\nNoC\n91.86.0\n44.54.3\n76.89.9\n43.56.3\n88.63.2\n60.32.8\n82.25.7\n69.7\nConC\n93.81.8\n44.44.4\n78.55.7\n52.38.3\n90.82.5\n58.84.7\n81.36.2\n71.4\nLinC\n94.81.2\n51.30.8 83.71.8\n67.33.8 90.13.9\n63.24.2 84.74.9\n76.4\nLlama-2 13B\n0-shot\nNoC\n56.30.0\n34.00.0\n73.30.0\n48.70.0\n54.70.0\n66.10.0\n47.30.0\n54.3\nConC\n69.30.0\n33.30.0\n72.30.0\n71.30.0\n75.00.0\n67.50.0\n47.00.0\n62.2\nLinC\n75.30.0\n47.30.0 85.70.0\n75.00.0 90.70.0\n69.00.0 48.30.0\n70.2\n1-shot\nNoC\n73.912.8\n43.93.5\n81.52.7\n67.07.2\n92.31.7\n70.33.5\n50.53.9\n68.5\nConC\n94.11.7\n42.43.9\n80.61.9\n76.22.5\n92.31.2\n60.97.7\n53.212.2\n71.4\nLinC\n94.11.7\n51.01.6 84.11.7\n77.32.6 93.61.6\n75.92.6 55.610.2 75.9\n4-shot\nNoC\n92.92.6\n48.74.1\n82.73.7\n62.514.7 94.21.0\n68.88.7\n72.011.8\n74.5\nConC\n97.40.4\n44.95.2\n80.52.3\n75.37.2\n94.81.2\n75.12.0\n72.59.8\n77.2\nLinC\n97.50.5\n53.00.9 86.01.2\n75.72.1 95.30.9\n77.00.9 78.79.6\n80.5\n8-shot\nNoC\n92.23.1\n49.17.3\n87.00.5\n77.34.7\n94.91.7\n72.95.6\n82.77.4\n79.4\nConC\n96.70.5\n47.14.4\n83.81.6\n79.13.7\n94.61.9\n75.33.0\n82.55.5\n79.9\nLinC\n97.00.2\n51.46.1 87.10.6\n79.73.9 95.01.4\n76.40.6 82.13.1\n81.2\nTable 4: Comparisons among the conventional approach (NoC; [3]), ConC [63] and LinC (Ours) on GPT-2, GPT-J and Llama-2. We report the mean and the standard deviation of test accuracy across different choices of the test demonstrations (the prompt is fixed). We also report the average performance across seven datasets.\nfive test prompts. We used the cross-entropy loss as\n(7)\nwhere yv i,c is the binary variable indicating if class c is the correct label for validation input xv i , and \u02dcpc denotes the output from the softmax defined in (3) with p = p(P v i ) = p(P(xv i , (xj, yj)k j=1)) defined in (4). In (6), we keep the loss general as we can use different losses depending on the task.\nwhere yv i,c is the binary variable indicating if class c is the correct label for validation input xv i , and \u02dcpc denotes the output from the softmax defined in (3) with p = p(P v i ) = p(P(xv i , (xj, yj)k j=1)) defined in (4). In (6), we keep the loss general as we can use different losses depending on the task. For each experiment, we fine-tuned the step size \u03b1. The number of epochs T was chosen from {1, 5, 15, 50, 100} and the number of validation prompts Nv was chosen\nFor each experiment, we fine-tuned the step size \u03b1. The number of epochs T was chosen from {1, 5, 15, 50, 100} and the number of validation prompts Nv was chosen\nfrom {1, 5, 10, 30, 100, 300} (for details, see Appendix B). If the validation set is provided for a dataset, we utilize it as is. However, if the validation set is not available, we create one by randomly selecting a subset from the training set. As baselines, we used the vanilla ICL method [3] that does not use any calibration (NoC) and contextual calibration (ConC) [63]. The results of ConC were replicated using the released code5. Both ConC and our approach, LinC, utilize an affine transformation. However, the crucial distinction lies in the fact that LinC acquires the transformation parameters through the learning process with a few additional samples (following the same format\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4680/46802aa8-3975-49c4-8ec5-af6670908c7e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Comparison across six different templates.</div>\nas instruction-tuning). In contrast, ConC does not engage in learning and opts for a pre-defined initialization instead (for details, see Appendix C). The demonstrations\u2019 labels were not artificially balanced. Moreover, we observed that our method performs well with any set of k-shot demonstrations used in the validation prompts, but utilizing a different set can lead to improved performance, thus we conducted experiments with different random seeds to obtain a better set of the validation demonstrations.\n# 5.2 Simulation Results\nLinC enhances both the average and minimum accuracy. Table 4 shows the results on GPT-2-XL, GPT-J and Llama-2, respectively. LinC consistently outperforms baselines in almost all experiments, demonstrating its strong generalization ability across different model sizes and few-shot settings. We observe that, on average, LinC achieves up to 21% improvement as compared to the vanilla ICL baseline and 14% improvement as compared to contextual calibration for 0-shot learning on GPT-J. Moreover, in certain cases, LinC can deliver a significant boost in performance, up to 50% absolute improvement, as observed in the GPTJ 0-shot experiment on the DBPedia dataset. LinC demonstrates significant performance gain on some datasets, such as TREC, while moderate improvement is observed for other datasets, such as SST-2. Additionally, the performance improvement of LinC is more prominent on GPT-J than on GPT-2-XL and Llama-2. We also compare our results with prototypical calibration [18], despite the fact that they use orders of magnitude larger number of samples and thus fall outside the few-shot learning regime. Table 8 in Appendix B shows that LinC outperforms prototypical calibration in 16 out of 28 cases while using fewer number of samples (see Table 9). When using the same number of samples, LinC outperforms prototypical calibration in 20 of 28 cases. LinC\u2019s exceptional ability to generalize effectively could be supported by a\nrecent discovery indicating that ICL in a conventional Transformer block is equivalent to adjusting the output layer using linear modeling of meta-learned deep data representations in few-shot settings, and LinC can be seen as explicitly adjusting this output layer with the additional samples [50]. We also performed an experiment to show the impact of the model sizes (e.g., 7B vs 13B) on performance within the same model family (e.g., Llama-2); see Table 11. The results demonstrate that LinC consistently improves performance regardless of the model size. LinC reduces variance across demonstrations. Figure 15 shows the difference in standard deviation between the calibration methods and the NoC baseline for all GPT-J experiments in Table 4. In most cases, LinC significantly reduces variance while only slightly increasing it in the remaining cases. Moreover, on average, LinC achieves a greater reduction in standard deviation than ConC, indicating that the predictions made by LinC are more consistent and reliable. LinC improves the Expected Calibration Error (ECE). To further evaluate LinC\u2019s model calibration, we use the ECE metric [35] that is widely-used to quantify the alignment between predicted and actual probabilities. Table 2 shows the results on GPT-J, 0-shot setting (see Appendix B for other settings). In most instances, LinC demonstrates the lowest ECE, and in others, it ranks as the second-best method, with only a minimal difference from the best method. LinC improves accuracy and reduces variance across varying prompt templates. Next we keep the set of demonstrations fixed and vary the prompt format. We use six different prompt formats and label spaces for SST-2 dataset (for details, refer to Appendix A, Table 5) on GPT-2-XL under 4-shot setting. From Figure 5, we observe that while ConC generally enhances the average accuracy, it results in high variance. In contrast, LinC exhibits a considerable enhancement in accuracy with much lower variance, demonstrating its effectiveness in improving the model\u2019s performance across various prompt templates. LinC is robust to class imbalance and permutations of demonstrations. Prior works [63, 32] have shown that the order in which the demonstrations are set in the prompt can significantly affect the performance. We evaluated the performance of our method on five 8-shot prompts for SST-2 on GPT-2XL, each with varying class proportions, and measured the accuracy across eight random permutations for each proportion. The demonstrations in the test prompt are kept the same for each proportion for both baselines and our method. Figure 6 illustrates that vanilla ICL (NoC) can achieve satisfactory performance with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/abed/abeda8a3-8c29-4ba8-a155-8f64ee0381d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 6: Comparison under varying label proportions and permutations of demonstrations; each box represents the test accuracy obtained from eight random permutations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4262/42623fd9-9b74-4422-932b-b8a1bb1ee70b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 7: Performance across varying validation sizes; \u2021 denotes NoC, and \u2217denotes ConC. Black dotted line marks the maximum accuracy.\ncertain proportions and permutations, but performs much worse in most cases. This result is in line with the observations made in previous works [32]. Moreover, ConC achieves superior test accuracy on average when compared to NoC but the performance is volatile across different permutations. LinC stands out as the most effective model and displays low variance across different permutations, suggesting that it is robust to varying class proportions and permutations. LinC needs very few additional samples for improving ICL. Figure 7 investigates the effect of additionally available validation samples on the performance of three different datasets on GPT-2-XL under the 4-shot setting. The number of extra samples employed in the validation prompts (i.e. Nv+k) is plotted along the x-axis. We observe that performance can be greatly improved by increasing the number of validation samples within a certain small range. However, further increasing the number of samples does not yield any additional improvement. LinC demonstrates high sample efficiency, achieving maximum accuracy with less than 30 additional samples on most datasets. Re-\nmarkably, some datasets require only five additional samples (e.g., Subj) to achieve maximum accuracy.\nLinC also improves performance on nonlanguage tasks. We also evaluated the performance of our method on non-language classification tasks using seven real tabular datasets in OpenML [49] following the settings in [12]. Table 3 demonstrates the performance improvement of LinC over raw ICL and ConC across all tabular datasets, regardless of the number of classes (C) and data features (D). In contrast, ConC sometimes falls short in improving performance, especially in datasets with fewer classes and features, and has worse performance than raw ICL in datasets such as Hamster, Customers, and Breast.\nLinC improves PEFT methods in the low-data low-compute regime. We evaluated the performance of LinC on two popular PEFT methods: 1) SPT [27] and 2) LoRA [19] (details in Appendix B.1). Tables 12 and 13 demonstrate that LinC consistently boosts both methods during fine-tuning across different sample sizes. However, performance improvement is most prominent when data and compute resources are limited. For example, for a sample size of 120 (out of 120k available training samples), LinC yields an accuracy improvement of +38.7% for SPT and +29.3% for LoRA. This makes LinC especially viable in scenarios with constrained compute and a scarcity of data.\n# 6 Related Work\nUnderstanding ICL. Some recent works focus on explaining the working of ICL. For example, [57] suggested that ICL is an implicit Bayesian inference and proved it through a synthetic dataset with a mixture of hidden Markov models in pretraining. [15] showed that Transformers can learn effective learning algorithms for unseen linear functions based on demonstration samples and achieve comparable error to least squares estimator in ICL models. [10] explain that ICL can be considered as implicit finetuning, where LLM generates meta-gradients from in-context demonstrations to adjust the model\u2019s behavior. [30] framed ICL as an algorithm learning problem and demonstrated that Transformers can effectively implement a function class through implicit empirical risk minimization based on demonstrations. [50] showed self-attention-only Transformers trained on simple regression tasks exhibit significant similarity to models learned by gradient descent, revealing how trained Transformers execute gradient descent during their forward pass. [5] showed that ICL\u2019s performance is influenced by the distributional properties of training data, with improved performance observed when training data consists of clustered examples and sufficient rare classes. ICL is also inher-\nently connected to multi-task learning [4, 62] and metalearning [14, 1, 6, 23, 20]. But a key difference between ICL and those methods is that, in ICL, adaptation to a new task is done implicitly through input prompt not running explicit gradient-based optimization.\nEnhancing ICL. To enhance the performance of ICL, meta-learning has been introduced by [7, 33] to improve the adaptation of LLMs to ICL. [56] suggest augmenting the demonstrations by incorporating human-aided reasoning steps, leading to an improvement in performance on a range of arithmetic and reasoning tasks. [55, 43] suggest instruction tuning as a method to further pre-train the language model with a variety of downstream tasks in a shared prompting format. Several works focus on finding good in-context demonstrations to improve ICL performance [31, 28]. [54] investigates ICL using a Bayesian approach, and proposes an algorithm for selecting optimal demonstrations, showing empirical improvement compared to a random selection baseline. [45] studies model reliability using four different facets. [18] proposes estimating prototypical clusters for all classes, mapping each cluster to the corresponding label, and calibrating the test predictions by their most likely cluster. It has been found in [22] that the sensitivity of the language model stems from the label shift of the model in the data distribution, where the model exhibits a shift in the label marginal while maintaining a strong label conditional. Their solution involves a generative calibration approach, adjusting the label marginal through MonteCarlo sampling over the in-context model to calibrate the predictive distribution. [64] introduce Batch Calibration (BC), a zero-shot calibration method aimed at reducing bias from the batch. Closely related to our work is [63] which observed that the few-shot performance of language models is not consistent across different in-context settings. Additionally, [63] notes that language models tend to predict certain labels due to bias or demonstration permutations. However, the proposed calibration method of selecting content-free test inputs in [63] cannot accurately reflect the bias of models, which can result in sub-optimal performance. In contrast, our LinC approach achieves calibration of bias by accurately optimizing calibration parameters at the expense of minimal compute and data.\n# 7 Conclusions\nThis paper investigates in-context learning (ICL), which relies on GPT-like models to generate outputs based on in-context demonstrations. Our findings reveal that ICL predictions may be unreliable when evaluated using Shannon entropy. To overcome these limitations, we propose the linear probe calibration (LinC) method, which significantly boosts the test performance\nof GPT models on various benchmark datasets with only a minimal number of additional samples. LinC\u2019s ability to reduce ECE and variance across different sets of demonstrations, and maintain robustness towards varying label proportions, prompt templates, and demonstration permutations implies that the predictions made by the LLM were more reliable, supporting our original conclusion from the Shannon entropy metric analysis. We believe that these findings carry important implications for future research and the development of more reliable and effective natural language processing models.\n# Limitations and Future Work\nWhile our focus remains on calibrating the model for better accuracy and reliability, it is worth discussing how to combine our framework with approaches for selecting better examples and prompt templates, such as those proposed in [31, 47]. Our work focuses on querying LLMs, which can generate content with potential ethical risks such as fairness and bias; thus, combining our framework with methods [17] that mitigate such risks is worth further discussion. One limitation of our work is the choice of k-shot validation demonstrations used to optimize the calibration parameters can impact the quality of the learned parameters, potentially leading to suboptimal results. Besides overcoming this limitation, we aim to expand our method to other NLP tasks such as summarization, text generation, and generative question answering.\n# Acknowledgment\nThe work of T. Chen and M. Abbas was supported by the Rensselaer-IBM AI Research Collaboration (http://airc.rpi.edu), part of the IBM AI Horizons Network (http://ibm.biz/AIHorizons).\n# References\n[1] Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. Sharp-maml: Sharpnessaware model-agnostic meta learning. In Proc. of International Conference on Machine Learning, Baltimore, MD, 2022. [2] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901, 2020.\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901, 2020. [4] Rich Caruana. Multitask learning. Machine learning, 28:41\u201375, 1997. [5] Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent incontext learning in transformers. In Advances in Neural Information Processing Systems, volume 35, pages 18878\u201318891. Curran Associates, Inc., 2022. [6] Lisha Chen, Songtao Lu, and Tianyi Chen. Understanding benign overfitting in gradient-based meta learning. In Advances in Neural Information Processing Systems, pages 19887\u201319899, 2022. [7] Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, Dublin, Ireland, May 2022. [8] Leda Cosmides and John Tooby. Are humans good intuitive statisticians after all? rethinking some conclusions from the literature on judgment under uncertainty. Cognition, 58(1):1\u201373, 1996. [9] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In First PASCAL Machine Learning Challenges Workshop, pages 177\u2013190, 2005. 10] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics, pages 4005\u20134019, Toronto, Canada, July 2023. 11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 12] Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn,\nDimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. In Advances in Neural Information Processing Systems, pages 11763\u201311784, 2022. [13] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pretraining for natural language understanding and generation. In Advances in Neural Information Processing Systems, volume 32, 2019. [14] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proc. of International Conference on Machine Learning, 2017. [15] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn incontext? a case study of simple function classes. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 30583\u201330598. Curran Associates, Inc., 2022. [16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017. [17] Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, and Aram Galstyan. Mitigating gender bias in distilled language models via counterfactual role reversal. arXiv preprint arXiv:2203.12574, 2022. [18] Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. Prototypical calibration for few-shot learning of language models. In Proc. of International Conference on Learning Representations, 2023. [19] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [20] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Theoretical convergence of multi-step model-agnostic meta-learning. J. Mach. Learn. Res., 23(1), jan 2022. [21] Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association : JAMIA, 19:263\u201374, 03 2012.\n[22] Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, and Kang Liu. Generative calibration for incontext learning. arXiv preprint arXiv:2310.10266, 2023. [23] Suhyun Kang, Duhun Hwang, Moonjung Eo, Taesup Kim, and Wonjong Rhee. Meta-learning with a geometry-adaptive preconditioner. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16080\u201316090, June 2023. [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [25] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. [26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. [27] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [28] Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. arXiv preprint arXiv:2212.06800, 2022. [29] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [30] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on\nMachine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19565\u201319594. PMLR, 23\u201329 Jul 2023.\nMachine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19565\u201319594. PMLR, 23\u201329 Jul 2023. [31] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland, May 2022. [32] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 8086\u20138098, Dublin, Ireland, May 2022. [33] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States, July 2022. Association for Computational Linguistics. [34] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines. In International Conference on Learning Representations, 2021. [35] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Proc. of AAAI Conference on Artificial Intelligence, pages 2901\u2013 2907, April 2015. [36] Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278, Barcelona, Spain, July 2004. [37] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021. [38] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Adv. Large Margin Classif., 10, 06 2000.\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [40] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. [42] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In International Conference on Learning Representations, 2020. [43] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. [44] Claude E Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948. [45] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable. In The Eleventh International Conference on Learning Representations, 2023. [46] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing, pages 1631\u20131642, 2013.\n[47] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 819\u2013862, Dublin, Ireland, May 2022.\n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[49] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49\u201360, 2014.\n[50] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023.\n[51] Ellen M. Voorhees and Dawn M. Tice. Building a question answering test collection. page 200\u2013207, New York, NY, USA, 2000. Association for Computing Machinery.\n2] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n[53] Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021.\n[54] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023.\n[55] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. [56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Curran Associates, Inc., 2022. [57] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. [58] Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. $k$NN prompting: Beyond-context learning with calibration-free nearest neighbor inference. In The Eleventh International Conference on Learning Representations, 2023. [59] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [61] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. [62] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 34(12):5586\u20135609, 2022. [63] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on\nMachine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR, 18\u201324 Jul 2021. [64] Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. arXiv preprint arXiv:2309.17249, 2023.\nSupplementary Material for\n\u201cEnhancing In-context Learning via Linear Probe Calibration\"\nFormat # Prompt Template\nLabel Space\n1\nReview: Perhaps the best sports movie I have ever seen.\nSentiment: Positive\nReview: This pathetic junk is barely an hour long.\nSentiment:\nPositive, Negative\n2\nInput: Perhaps the best sports movie I have ever seen.\nPrediction: Positive\nInput: This pathetic junk is barely an hour long.\nPrediction:\nPositive, Negative\n3\nReview: Perhaps the best sports movie I have ever seen.\nSentiment: good\nReview: This pathetic junk is barely an hour long.\nSentiment:\ngood, bad\n4\nInput: Perhaps the best sports movie I have ever seen.\nPrediction: good\nInput: This pathetic junk is barely an hour long.\nPrediction:\ngood, bad\n5\nPerhaps the best sports movie I have ever seen. My overall\nfeeling was that the movie was good\nThis pathetic junk is barely an hour long.\nMy over-\nall feeling was that the movie was\ngood, bad\n6\nReview: Perhaps the best sports movie I have ever seen.\nQuestion: Is the sentiment of the above review Positive or\nNegative?\nAnswer: Positive\nReview: This pathetic junk is barely an hour long.\nQuestion: Is the sentiment of the above review Positive or\nNegative?\nAnswer:\nPositive, Negative\nTable 5: A list of different prompt templates that were used to investigate the impact of templates on SST-2. For bre here we show only one demonstration.\nDataset\nPrompt Template\nLabel Space\nAll OpenML datasets When we have x1=r.x1, x2=r.x2, . . . , xK=r.xD, what should\nbe y? ### y=r.y @@@\n0,. . . ,C\nble 6: Prompt template used for non-language classification tasks using the real tabular datasets in OpenML [49]; for\nDataset\nPrompt Template\nLabel Space\nAll OpenML datasets When we have x1=r.x1, x2=r.x2, . . . , xK=r.xD, what should\nbe y? ### y=r.y @@@\n0,. . . ,C\nTable 6: Prompt template used for non-language classification tasks using the real tabular datasets in OpenML [49]; for a sample r, D denotes the number of features; Following [12], we follow OpenAI\u2019s recommendation by using \"###\" for separating questions and answers, and \"@@@\" to indicate the end of answer.\nble 6: Prompt template used for non-language classification tasks using the real tabular datasets in OpenML [49]; for a mple r, D denotes the number of features; Following [12], we follow OpenAI\u2019s recommendation by using \"###\" for parating questions and answers, and \"@@@\" to indicate the end of answer.\nDataset\nPrompt Template\nLabel Space\nSST-2\nReview: Perhaps the best sports movie I have ever seen.\nSentiment: Positive\nReview: This pathetic junk is barely an hour long.\nSentiment:\nPositive, Negative\nAGNews\nClassify the news articles into the categories of World, Sports,\nBusiness, and Technology.\nArticle: UK lender Barclays says it is in talks with South\nAfrica\u2019s Absa about buying a majority stake in the bank.\nAnswer: Business\nArticle: New music sharing network allows users to amass\npoints by referring buyers.\nAnswer:\nWorld, Sports, Business, Tech-\nnology\nTREC\nClassify the questions based on whether their answer type is\na Number, Location, Person, Description, Entity, or Abbre-\nviation.\nQuestion: What does the abbreviation AIDS stand for?\nAnswer Type: Abbreviation\nQuestion: What country do the Galapagos Islands belong\nto?\nAnswer Type:\nNumber, Location, Person, De-\nscription, Entity, Abbreviation\nDBPedia\nClassify the documents based on whether they are about a\nCompany, School, Artist, Athlete, Politician, Transportation,\nBuilding, Nature, Village, Animal, Plant, Album, Film, or\nBook.\nArticle: Hoodlum & Son is a 2003 comedy-crime film.\nAnswer: Film\nArticle: Nachan Main Audhay Naal is the seventh album\nof Pakistani pop and bhangra singer Abrar-ul-Haq. It was\nreleased on March 2007.\nAnswer:\nCompany, School, Artist, Ath-\nlete, Politician, Transporta-\ntion, Building, Nature, Village,\nAnimal, Plant, Album, Film,\nBook\nSST-5\nReview: The film is bright and flashy in all the right ways.\nSentiment: great\nReview: The film never finds its tone and several scenes run\ntoo long.\nSentiment:\nterrible, bad, okay, good, great\nSubj\nInput: All social structures break down and a new world\norder emerges from the heart of the desert.\nType: objective\nInput: A zombie movie in every sense of the word \u2013 mindless,\nlifeless, meandering, loud , painful, obnoxious.\nType:\nobjective, subjective\nRTE\nExperts say that Mr. Abbas will need that big win to show\nthat he has the support of most Palestinian people in order\nto push through his aims of peace talks with Israel.\nquestion: Analysts had said that Mr. Abbas needed a large\nmargin of victory in order to push his agenda of peace talks\nwith Israel. True or False?\nanswer: True\nThe city is twinned with Glasgow, Dortmund, Pleven, and\nLe Mans.\nquestion: Dortmund is twinned with Glasgow. True or False?\nanswer:\nTrue, False\nTable 7: The prompts templates used for different datasets. For brevity, here we show only one demonstration per datase\n# B Additional Experiments\nIn this section, we provide details of the experimental set-up and present additional results.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd38/cd380b62-86f4-44ad-9d68-42c0ebe8c980.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Shannon entropy histograms of using vanilla ICL on GPT-2-XL (1.5B) vs our method on Subj (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e34/3e3492cb-f6a1-48ca-8829-700e060b1393.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Shannon entropy histograms of using vanilla ICL on GPT-2-XL (1.5B) vs our method on RTE (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0126/012699ab-e981-4779-9ba7-0038df404db5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Shannon entropy histograms of using vanilla ICL on GPT-2-XL (1.5B) vs our method on AGNews (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\nHyperparameter search. After finding Nv and T via random search using the sets defined in Section 5.1, we narrowly fine-tune the learning rate \u03b1 for each experiment from the range [1e \u22125, 2e1]. Therefore, in each experiment, the value of \u03b1 might vary, and to ensure transparency and reproducibility, we have compiled some hyperparameter sets in the examples_ssh folder within the provided code. Furthermore, given that the calibration network constitutes a basic linear convex problem, determining an appropriate learning rate presents no issue, as the fixed rate can readily be substituted with a schedule that gradually reduces the learning rate as training advances, based on specified criteria (e.g., a linear scheduler that gradually diminishes the rate for each parameter group by applying a small multiplicative factor until a predetermined epoch count is attained). Where is the affine transformation applied? The classification tasks we considered apply the affine transformation to the set of probabilities that are associated with each class in the label space i.e. task-specific\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5bc1/5bc179b4-06cf-4524-b112-f7c1733ea94f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Shannon entropy histograms of using vanilla ICL on Llama-2 (13B) vs our method on SST-2 (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/934c/934c970c-9545-47a0-862a-1713a79b96da.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Shannon entropy histograms of using vanilla ICL on Llama-2 (13B) vs our method on Subj (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f876/f876ae17-e4d7-471d-847b-e49e48957a86.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e 13: Shannon entropy histograms of using vanilla ICL on Llama-2 (13B) vs our method on RTE (higher entropy es higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebfb/ebfb4ecf-0a35-4e06-8c5d-f5bf14764233.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Shannon entropy histograms of using vanilla ICL on Llama-2 (13B) vs our method on AGNews (higher entropy implies higher uncertainty); we use logarithmic base two. Refer to Section 2 for a detailed explanation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e89b/e89b2c15-7bdf-440a-9846-2772dc1b0b12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">C diminishes the standard deviation of accuracy across different demonstrations. The difference in standard en the calibration methods and the unadjusted baseline (NoC) from Table 4 is plotted.</div>\ntokens. In other words, after we get the logits which are in the dimension (batch_size, seq_length, vocab_size), we sample the tokens that correspond to the classes in the label space to a get a probability vector of dimension (batch_size, num_classes). We then apply the affine transformation on this probability vector. Model Shots Method SST-5 AGNews NoC 32.3 57.3 Expected Calibration Error (ECE) [35]. The ECE computes the Expected Calibration Error across the bins in the following manner:\ntokens. In other words, after we get the logits which are in the dimension (batch_size, seq_length, vocab_size), we sample the tokens that correspond to the classes in the label space to a get a probability vector of dimension (batch_size, num_classes). We then apply the affine transformation on this probability vector.\nExpected Calibration Error (ECE) [35]. The ECE computes the Expected Calibration Error across the bins in the following manner:\nwhere om represents the accuracy i.e. the true fraction of positive instances in bin m, em represents the model confidence i.e. is the mean of the post-calibrated probabilities for the instances in bin m, and the fraction Bm n denotes the empirical probability (fraction) of all instances that fall into bin. Therefore, The ECE can be seen as evaluating the extent to which a model\u2019s estimated \"probabilities\" align with the actual (observed) probabilities by computing a weighted average of the absolute difference between accuracy and confidence. A model\u2019s calibration is considered better when ECE values are lower and an ECE of zero is indicative of a perfectly calibrated model.\n# B.1 Experiment Settings\nFor LoRA, we used a step size of 1.5e-6, a batch size of 16, a scaling factor of 32, and incorporated a 0.1 dropout probability within the LoRA layers. For SPT, we used a step size of 3.5e-4, a batch size of 16, specified 8 virtual tokens (i.e. the num_vir_tokens argument), and employed an initial text for prompt tuning (i.e. the prompt_tun\nan initial text for prompt tuning (i.e. the prompt_tuning_init_text argument) that consisted of a list of classes separated by commas (for example, \"World, Sports, Business, Technology\" for AGNews). Across all our experiments, we performed fine-tuning over a total of 20 epochs. For the non-language tasks that used tabular datasets from OpenML [49], following [12], we selected the maximum number of instances that could be accommodated within the contextual window for each dataset. Additionally, our results were reported as an average across three random seeds.\nModel\nShots Method SST-5\nAGNews\nLlama-2 7B\n0-shot\nNoC\n32.30.0\n57.30.0\nConC\n35.00.0\n63.30.0\nLinC\n45.00.0 82.00.0\n1-shot\nNoC\n39.39.8\n83.92.1\nConC\n43.01.1\n82.03.3\nLinC\n50.13.5 84.32.6\n4-shot\nNoC\n49.71.9\n87.21.1\nConC\n44.32.5\n87.21.2\nLinC\n53.52.8 87.61.1\n8-shot\nNoC\n48.95.5\n86.70.9\nConC\n48.72.2\n87.50.5\nLinC\n52.03.3 87.60.6\nLlama-2 13B\n0-shot\nNoC\n34.00.0\n73.30.0\nConC\n33.00.0\n72.30.0\nLinC\n47.30.0 85.70.0\n1-shot\nNoC\n43.93.5\n81.52.7\nConC\n42.43.9\n80.61.9\nLinC\n51.01.6 84.11.7\n4-shot\nNoC\n48.74.1\n82.73.7\nConC\n44.95.2\n80.52.3\nLinC\n53.00.9 86.01.2\n8-shot\nNoC\n49.17.3\n87.00.5\nConC\n47.14.4\n83.81.6\nLinC\n51.46.1 87.10.6\nTable 11:\nTable 11: Performance under same model family (i.e. Llama-2) but different sizes (7B/13B).\nTable 11: Performance under same model family (i.e. Llama-2) but different sizes (7B/13B).\nShot\nMethod\nSST-2\nSST-5\nAGNews TREC\nDBpedia RTE\nSubj\nAvg\nGPT-J 6B\n0-shot\nNo Calibration\n66.30.0\n33.70.0\n36.00.0\n24.70.0\n19.70.0\n55.60.0\n65.70.0\n43.1\nContextual Calibration\n58.00.0\n40.70.0\n56.00.0\n40.00.0\n48.00.0\n52.40.0\n58.70.0\n50.5\nPrototypical Calibration\u2021 74.20.2\n42.10.8\n55.10.4\n53.46.1\n66.11.5\n57.01.0 69.50.2\n59.6\nPrototypical Calibration\u221774.11.1\n32.23.8\n49.84.3\n44.76.7\n20.74.7\n56.51.5\n66.81.8\n49.3\nLinear Calibration\n74.30.0 46.00.0 64.30.0\n70.70.0 69.30.0\n56.70.0\n70.70.0 64.6\n1-shot\nNo Calibration\n67.36.7\n35.33.5\n65.314.3\n40.38.8\n64.916.6\n50.73.7\n65.19.9\n55.6\nContextual Calibration\n88.31.7\n46.93.1\n74.95.8\n62.64.7\n80.23.3\n53.41.2\n59.12.2\n66.5\nPrototypical Calibration\u2021 90.81.7 47.62.5\n79.85.4\n55.36.4\n90.02.2\n56.73.1\n77.94.8 71.2\nPrototypical Calibration\u221789.31.9\n35.28.9\n78.17.5\n45.99.6\n62.24.9\n57.71.8 74.95.9\n63.2\nLinear Calibration\n88.51.7\n50.41.3 81.74.6\n63.04.4 82.63.7\n56.01.8\n69.97.5\n70.3\n4-shot\nNo Calibration\n88.93.3\n46.33.2\n72.36.1\n37.74.2\n82.114.4\n55.07.1\n57.46.9\n62.8\nContextual Calibration\n92.83.1\n49.44.8\n75.24.1\n46.05.1\n88.94.9\n56.01.9\n65.311.8 67.7\nPrototypical Calibration\u2021 95.00.4 46.24.6\n79.96.6\n57.15.3\n91.92.6\n61.22.7 79.45.8 73.0\nPrototypical Calibration\u221794.60.6\n47.95.5\n81.52.0\n49.010.2 63.84.3\n60.83.1\n78.98.0\n68.1\nLinear Calibration\n94.90.9\n51.13.7 77.95.5\n65.31.5 89.45.4\n58.43.7\n68.411.2 72.2\n8-shot\nNo Calibration\n91.86.0\n44.54.3\n76.89.9\n43.56.3\n88.63.2\n60.32.8\n82.25.7\n69.7\nContextual Calibration\n93.81.8\n44.44.",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. This paper shows that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. To solve this problem, the authors propose a new technique called Linear Probe Calibration (LinC), which calibrates the model\u2019s output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples).",
        "problem": {
            "definition": "The problem addressed is the unreliability of predictions made by GPT-like models using in-context learning, which can vary significantly across different prompt templates and demonstration arrangements.",
            "key obstacle": "The main difficulty is the inherent variability in performance due to biases in the models that lead to unreliable predictions, particularly when assessed using Shannon entropy."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that while ICL can produce acceptable results, the confidence in these predictions is often low, indicating potential unreliability.",
            "opinion": "The proposed idea, Linear Probe Calibration (LinC), aims to enhance the reliability and performance of predictions made by GPT-like models by linearly calibrating output probabilities without requiring access to model weights or architecture.",
            "innovation": "The primary difference between LinC and existing approaches is that LinC optimizes the calibration parameters using only a few additional samples, making it more sample-efficient and computationally efficient compared to traditional fine-tuning methods."
        },
        "method": {
            "method name": "Linear Probe Calibration",
            "method abbreviation": "LinC",
            "method definition": "LinC calibrates the output probabilities of GPT-like models to improve the reliability of predictions made through in-context learning.",
            "method description": "LinC enhances the reliability of predictions made by GPT-like models through a linear calibration of output probabilities.",
            "method steps": "1. Obtain logits from the model for a set of validation prompts. 2. Optimize the calibration parameters using a gradient-based optimizer. 3. Apply the learned calibration parameters to the model's output probabilities.",
            "principle": "The effectiveness of LinC lies in its ability to calibrate the model's output probabilities, aligning predicted probabilities with actual likelihoods, thereby improving reliability."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved testing LinC on various benchmark datasets in both few-shot and parameter-efficient fine-tuning (PEFT) settings, using models such as GPT-2-XL, GPT-J, and Llama-2.",
            "evaluation method": "The evaluation method included measuring the performance improvements across different datasets and comparing the results of LinC against baseline methods such as vanilla ICL and contextual calibration."
        },
        "conclusion": "The paper concludes that Linear Probe Calibration (LinC) significantly improves the test performance of GPT models on various benchmark datasets with minimal additional samples, reducing expected calibration error and variance across different sets of demonstrations, thus making predictions more reliable.",
        "discussion": {
            "advantage": "LinC stands out by providing a substantial performance boost while maintaining resource efficiency, especially in low-data and low-compute scenarios.",
            "limitation": "One limitation is that the choice of k-shot validation demonstrations can impact the quality of the learned calibration parameters, potentially leading to suboptimal results.",
            "future work": "Future research directions include combining LinC with methods for selecting better examples and prompt templates, and extending the method to other NLP tasks such as summarization and text generation."
        },
        "other info": [
            {
                "info1": "The code for LinC is available at https://github.com/mominabbass/LinC."
            },
            {
                "info2": {
                    "info2.1": "LinC requires only a few additional samples (typically 10-100) to effectively optimize the calibration parameters.",
                    "info2.2": "LinC is computationally efficient, as it can be implemented with minimal code."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models."
        },
        {
            "section number": "1.3",
            "key information": "GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy."
        },
        {
            "section number": "3.1",
            "key information": "The problem addressed is the unreliability of predictions made by GPT-like models using in-context learning, which can vary significantly across different prompt templates and demonstration arrangements."
        },
        {
            "section number": "3.3",
            "key information": "The proposed technique called Linear Probe Calibration (LinC) calibrates the model\u2019s output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples."
        },
        {
            "section number": "4.1",
            "key information": "LinC enhances the reliability of predictions made by GPT-like models through a linear calibration of output probabilities."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that the choice of k-shot validation demonstrations can impact the quality of the learned calibration parameters, potentially leading to suboptimal results."
        },
        {
            "section number": "7",
            "key information": "The paper concludes that Linear Probe Calibration (LinC) significantly improves the test performance of GPT models on various benchmark datasets with minimal additional samples."
        }
    ],
    "similarity_score": 0.7181947025203765,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Enhancing In-context Learning via Linear Probe Calibration.json"
}