{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2209.01975",
    "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
    "abstract": "Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.",
    "bib_name": "su2022selectiveannotationmakeslanguage",
    "md_text": "# SELECTIVE ANNOTATION MAKES LANGUAGE MODELS BETTER FEW-SHOT LEARNERS\nHongjin Su\u2660 Jungo Kasai\u2663\u2666 Chen Henry Wu\u2665 Weijia Shi\u2663 Tianlu Wang\u2666Jiayi Xin\u2660 Rui Zhang\u22c6Mari Ostendorf\u2663 Luke Zettlemoyer\u2663\u2666Noah A. Smith\u2663\u2666Tao Yu\u2660\u2663 \u2660The University of Hong Kong \u2663University of Washington \u2666Allen Institute for AI \u2665Carnegie Mellon University \u22c6Penn State University \u2666Meta AI\n{hjsu,tyu}@cs.hku.hk, henrychenwu@cmu.edu, ostendor@uw.edu {jkasai,swj0419,lsz,nasmith}@cs.washington.edu\nABSTRACT\nMany recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, vote-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100\u00d7 less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks.1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3896/3896155d-5421-436e-81eb-bf1ae9ffa4e0.png\" style=\"width: 50%;\"></div>\nFigure 1: Left: Our two-step framework for in-context learning. Instead of assuming access to large labeled data, we first select a small number of (diverse and representative) unlabeled examples to annotate before test time. At test time, we retrieve in-context examples from the small annotated pool. Right: In-context learning performance over varying annotation budgets averaged over three representative tasks (HellaSwag commonsense reasoning, MRPC paraphrase detection, and MWOZ dialogue state tracking). Here we experiment with GPT-J and Codex-davinci-002. Two selective annotation methods are presented: random selection and our vote-k method. We observe that an appropriate selective annotation method largely improves the in-context learning performance with smaller variance over random selection under varying annotation budgets.\nMuch recent work builds approaches to natural language tasks on the impressive abilities of large language models (e.g., GPT-3; Brown et al., 2020). Large language models can perform downstream tasks by conditioning generation on a few task demonstrations, thereby avoiding the need for any parameter updates. This new, few-shot learning paradigm is called in-context learning and has become an attractive alternative to supervised finetuning (Liu et al., 2021). In this work, we study the implications of this remarkable capability of large language models for dataset creation and annotation. We extensively examine how to reduce the manual annotation cost while retaining high in-context learning performance. Although in-context learning was originally proposed for few-shot learning, recent works show that retrieving prompts from a large set of annotated examples is necessary to achieve good performances (Liu et al., 2022; Rubin et al., 2022). In particular, they show that the performance substantially improves when similar examples (under some embedding function) are retrieved as in-context examples specifically for each test input (Liu et al., 2022). Each test sample only requires a few in-context examples in its prompt. Different test instances, however, require different in-context examples with their associated annotations, necessitating a large set of annotated examples. Distinct from these recent efforts, we establish a two-step framework to better understand and improve the annotation efficiency (Fig. 1): the first step is selective annotation that picks a small number of instances to get annotated before test time, followed by prompt retrieval that retrieves in-context examples for each test instance from the annotated data. The total annotation budget is the number of examples selected and annotated in the first step. The second step is bounded by the number of examples that can fit as input to a language model. Based on this framework, we propose an unsupervised, graph-based selective annotation method, named vote-k, that selects diverse and representative instances to be annotated. Our extensive experiments over 10 datasets across diverse tasks (covering classification, commonsense reasoning, dialogue, and text/code generation; see Tab. 2) demonstrate that our graph-based selective annotation method, vote-k (\u00a72.1), substantially improves the in-context learning performance by balancing the diversity and representativeness of annotated samples. For instance, vote-k, combined with similarity-based prompt retrieval (Liu et al., 2022; Rubin et al., 2022), achieves a 11.4% relative gain under a budget of 100 annotated examples and a 12.9% relative gain when only 18 examples are annotated; 18 samples can fit into language models\u2019 input, meaning the prompt retrieval step is not needed. Moreover, the improvement is consistent across language models with varying sizes (2B-175B parameters) (\u00a74.2). This finding is in contrast with finetuning, where we cannot see the effectiveness of selective annotation over random baseline, due to outliers (Karamcheti et al., 2021) or training instability (D\u2019Arcy & Downey, 2022). We hypothesize that in-context learning with similarity-based prompt retrieval is more robust to small annotation sizes and outliers because only the most similar examples are retrieved for each test instance. Indeed, we observe that random prompt retrieval fails to benefit from selective annotation (\u00a74.4), providing support for our hypothesis. Besides performance comparisons within a fixed annotation budget, we show that selective annotation provides better few-shot performance with 5-100\u00d7 less annotation cost for new natural language tasks. In-context learning with 18 examples selected by vote-k achieves higher performance than 100 randomly selected examples on 6 out of the 10 tasks. It also outperforms strong finetuning methods by a large margin (Fig. 2) and requires 10-100\u00d7 less annotations for similar performance (\u00a74.1). We observe that in-context learning quickly (100 or 300 samples are annotated) converges to decent performance when vote-k selective annotation is applied. These results suggest that large language models do not require large annotated datasets (e.g., 10K) due to their ability to adapt to new tasks through simple prompting. Selective annotation also makes in-context learning much more stable. In real-world scenarios, even collecting unlabeled data is non-trivial and introduces randomness. We simulate such randomness in our experimental setting by subsampling the original unlabeled data multiple times. Our results suggest that vote-k selective annotation largely reduces the variance of in-context learning even in this setting (Tab. 2). Further analysis shows larger improvements when there is a domain shift between training and test data (e.g., text from different Amazon users; Koh et al., 2021; \u00a74.3). Finally, when compared with previous selective annotation methods designed for supervised training/finetuning, we demonstrate that vote-k selective annotation consistently improves the performance (\u00a74.5). As\nin-context learning has been applied to increasingly more natural language processing applications, we hope that our annotation-efficient framework will provide useful guidance for both researchers and practitioners.\n# 2 SELECTIVE ANNOTATION FOR IN-CONTEXT LEARNING\nIn-context learning only requires a few annotated examples per test instance (few-shot learning), while avoiding expensive finetuning on the whole training data. It is, however, often assumed that all annotated training data are available for prompt retrieval (e.g., Liu et al., 2022; Rubin et al., 2022). Yet the implied total annotation costs are hardly discussed in previous work. We develop a better practice for few-shot learning with large language models by carefully studying the total annotation cost required for in-context learning. We also study how examples should be selected to annotate, in order to make in-context learning perform better for new tasks. We formulate a general framework (Fig. 1 left) that consists of two steps: selective annotation (\u00a72.1) and prompt retrieval (\u00a72.2).\n# 2.1 SELECTIVE ANNOTATION\nThe first step chooses examples to annotate before test time. This process thus determines the total annotation budget. This selective annotation process is largely ignored in the recent literature for in-context learning. We will demonstrate, however, that the annotation cost can be substantially reduced by choosing a small set of diverse, representative examples, while retaining the downstream performance (\u00a73). Formally, given a set of unlabeled samples X = {xi}N i=1, selective annotation aims at selecting a subset L \u2282X to be annotated, where |L| = M is the annotation budget. We discuss our vote-k selective annotation method and other selective annotation baselines below.\nVote-k The goal of selective annotation for in-context learning is to select diverse and representative examples; representativeness will help many test instances to find similar demonstrations, while diversity increases the total coverage. We develop vote-k, a graph-based method that promotes both diversity and representativeness. A detailed algorithm can be found in Appendix G. We first compute a vector representation for each unlabeled training instance using Sentence-BERT (Reimers & Gurevych, 2019) by averaging the resulting vectors over the text input words.2 We then use the embedding vectors to create a directed graph G = (V, E) where the vertices V are the unlabeled instances X as defined above. For each vertex v \u2208V , we create an edge to its k nearest vertices in terms of the cosine similarity between the embeddings. Now let L and U denote the sets of already chosen (i.e., labeled) samples and remaining samples, respectively. Initially, L = \u2205. Every vertex u \u2208U is scored by a modified degree: \ufffd\n\ufffd \u2208{|\u2208\u2208U} where s discounts v that is close to the already selected instances, thereby encouraging diversity. In every iteration, we take arg maxu\u2208U score(u) and move it from U to L. We run M/10 of these iterations; after this process, the current labeled L has M/10 samples (up to Line 7 in Algorithm 1). Subsequently, we use L as the in-context learning examples for large language model, e.g.,GPT-J (Wang & Komatsuzaki, 2021), and generate a prediction for every instance in U. We then compute the average log probability over the generation output as the model\u2019s confidence score (Line 8 to Line 10 in Algorithm 1). We then partition U into M equal-sized buckets, based on their confidence scores (e.g., if M = 100, we group the unlabeled instances by percentile). We add to L the example with the maximum score from each of the first 9M/10 buckets (discarding the M/10 buckets with the most confident examples), resulting in |L| = M (Line 11 to Line 18 in Algorithm 1). This further encourages diversity by selecting instances with varying confidence scores from in-context learning. We tuned k and \u03c1 in our preliminary experiments, and found that k=150 and \u03c1=10 perform well across many datasets. We will explore other selective annotation methods from prior work on active learning or coreset selection (\u00a74.5) and see that vote-k outperforms these alternative methods.\nexperiments three times and report the average score. We will show that these baselines substantially underperform the vote-k method (\u00a73.3), demonstrating the importance of the selective annotation step to reduce the total annotation cost.\n# 2.2 PROMPT RETRIEVAL\nOnce we have a set of annotated examples L from selective annotation, we retrieve a few examples from the annotated set as in-context examples for each test instance. Following recent work (Liu et al., 2022), we will compute embeddings for all annotated samples using Sentence-BERT and find the most similar examples to each test instance in terms of cosine similarity.\n# 3 EXPERIMENTS\nWe conduct extensive experiments over 10 diverse datasets, spanning 9 distinct tasks, and show a better approach to few-shot learning than previously considered. In general, we find that the first step of selective annotation is particularly crucial to reduce the amount of required annotation.\n# 3.1 DATASETS AND TASKS\nWe use 10 diverse NLP datasets across 9 tasks that are listed in Table 1. These datasets involve different task formulations, thereby allowing for extensive evaluations in varying scenarios. Some of those are included in the widely-used GLUE benchmark (Wang et al., 2019). Appendix A illustrates details of the 10 datasets with examples. For each dataset, we use the standard train/dev./test split available from the Transformers library (Wolf et al., 2020). In the selective annotation step, we remove all labels in the training data. For the datasets that have test data available publicly, we use the the test data for evaluation (SST-5, XSUM, MWoZ, and DBpedia). For the others, we follow prior work (e.g., Jiang et al., 2020; Lan et al., 2020; Gao et al., 2021) and use the dev. data for evaluation.3 We evaluate the methods by accuracy for all classification and multiple-choice selection datasets, joint accuracy (Budzianowski et al., 2018) for MWoZ, test suite accuracy (Zhong et al., 2020) for GeoQuery, exact matching (Rajpurkar et al., 2016) for NQ, and ROUGE-L (Lin, 2004) for XSum.\nDataset\nTask\nIn-Context Learning Models\nClassification\nMRPC (Dolan et al., 2004)\nParaphrase Detection\nGPT-Neo, GPT-J, GPT-3\nSST-5 (Socher et al., 2013)\nSentiment Analysis\nGPT-J\nDBpedia (Lehmann et al., 2015)\nTopic Classification\nGPT-J\nMNLI (Williams et al., 2018)\nNatural Language Inference\nGPT-J\nRTE (Bentivogli et al., 2009)\nNatural Language Inference\nGPT-J\nMultiple-Choice HellaSwag (Zellers et al., 2019)\nCommonsense Reasoning\nOPT, GPT-Neo, GPT-J, GPT-3\nDialogue\nMWoZ 2.4 (Budzianowski et al., 2018)\nDialogue State Tracking\nCodex-{cushman, davinci-002}\nGeneration\nGeoQuery (Zelle & Mooney, 1996)\nSemantic Parsing\nCodex-davinci-002\nNQ (Kwiatkowski et al., 2019)\nOpen-Domain QA\nCodex-davinci-002\nXSUM (Narayan et al., 2018)\nSummarization\nGPT-J\nTable 1: All the 10 datasets and the in-context learning models used in our experiments. GPT-J and Codex-davinci-002 are used by default. Other in-context learning models are explored in analysis.\nTable 1: All the 10 datasets and the in-context learning models used in our experiments. GPT-J and Codex-davinci-002 are used by default. Other in-context learning models are explored in analysis.\nMeasuring Stability Given a set of unlabeled data, our vote-k selective annotation algorithm is deterministic, without any randomness. However, we note that in real scenarios, even getting unlabeled samples is not trivial, and getting unlabeled samples can be a process with large variance. To simulate this real setting, we perform selective annotation from 3K instances that are randomly subsampled from the original training data for each task. For each experiment, we repeat this subsampling three times, and results are averaged over the three trials. We will find that vote-k still substantially improves stability over alternative selective annotation methods.\nWe mainly perform experiments using GPT-J with 6B parameters (Wang & Komatsuzaki, 2021) due to our computational budget. The exceptions are the MWoZ, GeoQuery, and NQ datasets, where we use Codex-davinci-002 (Chen et al., 2021),4 a variant of GPT-3 finetuned on code data from the web. Codex is particularly effective for structured prediction such as semantic parsing, and we found it is indeed effective on three datasets (MWoZ, GeoQuery, and NQ) in our preliminary experiments. We will explore the effectiveness of selective annotation on the largest publically available language models, OPT-175B (Zhang et al., 2022) for HellaSwag (Fig. 4) and Codex-davinci-002 for MWoZ, over varying annotation budgets. We will also explore other language models with different sizes for three representative tasks (HellaSwag, MWoZ, and SST-5) in \u00a74.2: GPT-3 with 175B (Brown et al., 2020) and GPT-Neo with 2.7B parameters (Black et al., 2021). Our later experiments will show the same patterns among selective annotation methods over these different language models. For the classification and multiple-choice tasks, we compute the average log score for each choice and choose the maximum one. For generation tasks, we simply perform beam-search decoding. See Appendix B for our in-context learning prompt templates for all 10 datasets. For every test instance, we feed as much retrieved samples as possible into the language model until the maximum token length is reached. On average, the number of samples N fed into the language model is 13.4 across different experiments. The in-context examples are concatenated in the ascending order of the similarity so that more similar examples benefit from the recency bias (Lu et al., 2022).\n# 3.3 MAIN RESULTS\nMethod\nClassification\nMulti-Choice Dialogue\nGeneration\n|L|\nSelection\nMRPC SST-5 MNLI DBpedia RTE\nHSwag\nMWoZ\nGeoQ NQ XSum\n100\nRandom\n63.5\n44.2\n37.4\n89.8\n51.5\n65.2\n47.2\n78.6 30.8 15.3\n100\nVote-k\n70.7\n53.0\n47.3\n93.4\n55.5\n70.7\n51.4\n82.8 33.6 17.2\n100 \u2206Absolute gain\n+7.2\n+8.8\n+9.9\n+3.6\n+4.0\n+5.5\n+4.2\n+4.2 +2.8 +1.9\n18\nRandom\n59.6\n39.8\n36.7\n77.6\n50.4\n62.5\n33.6\n62.4 29.8 13.6\n18\nVote-k\n64.2\n47.6\n41.0\n87.1\n54.3\n67.4\n42.8\n72.5 32.3 15.2\n18 \u2206Absolute gain\n+4.8\n+7.8\n+4.3\n+9.5\n+3.9\n+4.9\n+8.8\n+9.9 +2.5 +1.6\nTable 2: In-context learning results with randomly-selected and vote-k selective annotation methods on all 10 datasets, with an annotation budget of 100 or 18. There is no prompt retrieval step when only 18 samples are annotated since all annotated samples can fit into the in-context learning model\u2019s input. Across the board, selective annotation with vote-k substantially outperforms the randomly-selected annotation baseline for in-context learning. Further, vote-k largely reduces the variance over three trials (see the min and max results in Appendix C), making in-context learning more stable.\nSeen in Table 2 are our results from all 10 diverse datasets with the annotation budgets of |L| \u2208 {18, 100}. 18 is chosen so that all annotated examples can be fit to the prompt for the language models without prompt retrieval. Over all datasets, vote-k selective annotation outperforms the random baseline by a large margin (5.2% absolute gain and 11.4% relative gain on average) when the annotation budget is 100. Even when only 18 examples are annotated and fixed as the in-context examples for all testing instances (no prompt retrieval step), in-context learning with vote-k still improves the randomly-selected annotation baseline (5.8% absolute gain and 12.9% relative gain on average). Particularly noteworthy is that in-context learning with 18 examples selected by vote-k achieves higher performance than the one with 100 randomly selected examples on 6 out of 10 tasks. Moreover, vote-k is a deterministic selective annotation method, conditioned on a set of unlabeled samples. Therefore, the variance of vote-k comes solely from how the unlabeled samples are collected, largely improving the robustness of in-context learning. We therefore recommend that researchers and practitioners use selective annotation (e.g., our vote-k method) to better benefit from the few-shot learning capability of large language models with stability. Our later experiments will also illustrate that vote-k consistently outperforms alternative selective annotation methods (\u00a74.5).\nOur extensive experiments demonstrated that selective annotation is important for the success of in-context learning. Here we conduct detailed analysis to provide further guidance for researchers and practitioners of few-shot in-context learning. We analyze selective annotation for in-context learning from a variety of perspectives: comparisons to finetuning methods (\u00a74.1), varying language model sizes (\u00a74.2), test data domain shifts (\u00a74.3), prompt retrieval methods (\u00a74.4), and alternative selective annotation methods (\u00a74.5).\n4.1 IN-CONTEXT LEARNING VS. FINETUNING\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f83a/f83afeaf-e1ff-4ad9-96ad-8a2d7cc31903.png\" style=\"width: 50%;\"></div>\nFigure 2: Comparisons between the in-context learning and finetuning paradigms over varying annotation budgets on three representative tasks: HellaSwag commonsene reasoning, MRPC paraphrase detection, and MWoZ dialogue state tracking. Four configurations are presented: finetuning with examples that are randomly selected to annotate (FT-random) or selected by our vote-k selective annotation method (\u00a72.1; FT-vote-k) and in-context learning with randomly-selected annotation (ICLrandom) or vote-k selection (ICL-vote-k). See \u00a74.1 for experimental details. Selective annotation largely improves the in-context learning performance compared to randomly-selected annotation even when the annotation budget is 18. In-context learning with wisely-selected labeled samples is a much better few-shot practice than a strong finetuning method.\nAs discussed earlier, in-context learning is an alternative learning paradigm to conventional finetuning. Through the lens of our two-step framework, we observed that selective annotation and prompt retrieval are key to the success of in-context learning. A new question now arises: how does incontext learning compare with finetuning under limited annotation budgets? We empirically compare the two paradigms in this section. We experiment with three representative tasks: MRPC (classification), HellaSwag (multiple-choice), and MWoZ (dialogue). Strong, state-of-the-art pretrained models are used for finetuning: large-sized RoBERTa (Liu et al., 2019) for MRPC and HellaSwag and DS2-T5 (Shin et al., 2022) for MWoZ. In-context learning uses GPT-J for MRPC, GPT-J and OPT 175B (Fig 4) for HellaSwag, and Codexdavinci-002 for MWoZ. Note that we do not aim to conduct head-to-head comparisons with exactly the same pretrained model; finetuning a large left-to-right language model (e.g., GPT-J and GPT-3) is computationally (and thus financially) infeasible in many cases. Here we examine the two paradigms from the practical perspective and benefit from the advantage of in-context learning, which requires no parameter updates of massive language models. Fig. 2 compares the two paradigms across varying annotation sizes ({18, 100, 300, 800}). Over all three tasks, we observe that in-context learning with vote-k selection outperforms the finetuning performance of state-of-the-art pretrained language models. Specifically, we find that to achieve similar performance to vote-k with |L| = 18 or 100, finetuning requires 1000 annotated examples for HellaSwag and 800 for MWoZ (10-100\u00d7 annotation cost). Note that the in-context learning performance usually converges when 100 or 300 examples are carefully selected and annotated, suggesting that a large annotated dataset is unnecessary for in-context learning to achieve strong\nperformance. Interestingly, selective annotation helps in-context learning, but not finetuning. This result is consistent with recent work showing that many active learning algorithms perform similarly to random baseline, when pretrained language models are finetuned (Karamcheti et al., 2021; D\u2019Arcy & Downey, 2022). They proposed that it might be due to outliers and the instability of finetuning on a limited number of annotated samples. We hypothesize that in-context learning with similarity-based prompt retrieval is more robust to outliers and small annotation sizes because only the most similar examples are retrieved for each test instance. We find two pieces of evidence for this hypothesis. First, \u00a7 4.4 shows that random (as opposed to similarity-based) prompt retrieval does not benefit from vote-k selective annotation. Second, in Appendix E, we show that explicitly removing outliers also helps finetuning to benefit from vote-k.\n4.2 LANGUAGE MODELS WITH VARIOUS SIZES\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1315/13158372-2710-4332-b397-c6a19aa81ddc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparisons of various models with 100 annotated examples. Vote-k selective annotation consistently improves in-context learning with pretrained language models of varying sizes.</div>\nFig. 3 shows performance with varying sizes of language models (GPT-Neo 2B, Black et al., 2021; GPT-J 6B, Wang & Komatsuzaki, 2021; GPT-3, Brown et al., 2020) on HellaSwag commonsense reasoning, SST-5 sentiment analysis, and MWoZ dialogue state tracking. In general, when a smaller model is used, the performance gap between random and vote-k selection is larger. In the HellaSwag task, vote-k outperforms randomly-selected annotation by 7.5% using GPT-Neo, but only 2.6% using GPT-3. Nonetheless, we see consistent performance gains from vote-k selection over varying sizes.\n# 4.3 EFFECTS OF DOMAIN SHIFT\nRecent work observed that when a large, pretrained language model is finetuned, the performance gain from active learning is limited (D\u2019Arcy & Downey, 2022), but it can be larger if there is a domain shift between training and evaluation (Tamkin et al., 2022). We have demonstrated that selective annotation consistently improves in-context learning, but here we explore cases of domain shifts.\nMethod\nCivilComments\nAmazon\n|L|\nSelection\nRandom\nDomain\nRandom\nDomain\n100\nRandom\n73.8\n66.8\n50.3\n30.7\n100\nVote-k\n79.3\n76.7\n56.3\n39.0\n100\n\u2206Absolute gain\n+5.5\n+9.9\n+6.0\n+8.3\nFollowing Tamkin et al. (2022), we use two natural language datasets from the WILDS benchmark (Koh et al., 2021): CivilComments (toxicity classification; Borkan et al., 2019) and Amazon (review classification; Ni et al., 2019). Each comes with both a random split and a domain split: the former splits data randomly and the latter is based on the domains (demographic identities for CivilComments and users for Amazon), simulating cases where a model is deployed in a new scenario unseen during annotations. Similar to \u00a73.3, we conduct experiments with GPT-J under two settings: random/vote-k\nselective annotation, followed by similarity-based prompt retrieval. Both selective annotation and prompt retrieval are conducted on the source domain. Tab. 3 shows our results. We see that the gain from vote-k is more pronounced under the domain splits: e.g., 9.9 vs. 5.5 accuracy point improvements on CivilComments. This suggests that selective annotation and prompt retrieval are particularly crucial when there is a domain shift in the evaluation data, as in many realistic scenarios (Koh et al., 2021; Longpre et al., 2022).\nTab. 3 shows our results. We see that the gain from vote-k is more pronounced under the domain splits: e.g., 9.9 vs. 5.5 accuracy point improvements on CivilComments. This suggests that selective annotation and prompt retrieval are particularly crucial when there is a domain shift in the evaluation data, as in many realistic scenarios (Koh et al., 2021; Longpre et al., 2022).\nMethod\nDataset\n|L|\nSelection\nRetrieval\nHellaSwag\nSST-5\nMWoZ\n100\nVote-k\nSimilar\n70.7\n53.0\n51.4\n100\nRandom\nSimilar\n65.2\n44.2\n47.2\n100\nVote-k\nRandom\n62.5\n41.6\n35.6\n100\nRandom\nRandom\n63.2\n40.6\n43.8\nTable 4: Comparison of random and similar prompt retrieval. Random retrieval fails to benefit from diverse and representative annotated examples from vote-k.\nWe have performed similarity-based prompt retrieval so far. Here we experiment with a random baseline for the prompt retrieval step to quantify the effect of prompt retrieval (Tab. 4). Interestingly, when random prompt retrieval is performed, vote-k does not necessarily improve upon the randomlyselected annotation baseline: e.g., 62.5 vs. 63.2 on HellaSwag and 35.7 vs. 43.8 on MWoZ. This suggests that random prompt retrieval fails to benefit from diverse, representative 100 samples, selected by vote-k selective annotation. Combining selective annotation and prompt retrieval is thus crucial for the success of in-context learning.\n<div style=\"text-align: center;\">4.5 ALTERNATIVE SELECTIVE ANNOTATION METHODS</div>\nRandom\nMFL\nDiversity\nLeast-confidence\nFast vote-k\nVote-k\nHellaSwag\n65.2\n66.5\n68.2\n68.4\n69.5\n70.7\nSST-5\n44.2\n45.6\n48.5\n46.2\n51.9\n53.0\nMWoZ\n47.2\n48.3\n49.2\n49.4\n50.2\n51.4\nTable 5: Comparisons of various selective annotation methods with 100 annotated examples. Performance is averaged over three random trials. Vote-k outperforms all the other selective annotation methods. Fast vote-k, a faster version of voke-k without the need for confidence score computations, can achieve similar performance to vote-k while being more computationally efficient.\n# Here we explore four additional methods for selective annotation:\n\u2022 Maximizing facility location (MFL; Lin & Bilmes, 2009) aims at optimizing the representativeness of the selected samples. Since this objective satisfies the submodular objective, maximization can be approximated via a greedy algorithm (see Appendix G.2). \u2022 Diversity focuses on maximizing the diversity of the embeddings for selected examples in the first step (Appendix G.3). \u2022 Least-confidence (Lewis & Gale, 1994) iteratively adds least-confident examples to the annotated set. \u2022 Fast vote-k is a fast, efficient alternative to our vote-k method (\u00a72.1) that does not use confidence scores. It picks M samples with the largest vote-k scores. It avoids using the pretrained language model to compute a confidence score for every instance, resulting in a 10+ times speedup. Notice that MFL, diversity, and least-confidence do not have hyperparameters other than the anotation budget. As shown in Tab. 5, vote-k outperforms all the other methods. It is noteworthy,\nNotice that MFL, diversity, and least-confidence do not have hyperparameters other than the an notation budget. As shown in Tab. 5, vote-k outperforms all the other methods. It is noteworthy\nhowever, that fast vote-k can achieve similar performance to vote-k. Fast vote-k is thus an attractive method for researchers and practitioners with a limited computational budget. Like vote-k, MFL also optimizes representativeness and Diversity also optimizes diversity. In particular, MFL defines representativeness as a sum over distances from the selected examples to all other examples, and Diversity defines diversity as the distances between selected examples. Since they do not significantly outperform randomly-selected annotation, we conjecture that jointly optimize diversity and representativeness is needed for selective annotation. Moreover, the way vote-k defines and diversity are also different from the baselines: vote-k defines representativeness as the number of neighbors during similarity-based prompt retrieval, which is effectively tailored to in-context learning; vote-k directly optimizes for the diversity of selected samples using the in-context learning model\u2019s prediction confidence.\n# 5 RELATED WORK\nIn-Context Learning In-context learning with large language models has recently received an increasing amount of interest, partly due to its flexibility and sample efficiency (Liu et al., 2021). Several recent works proposed methods to improve in-context learning in many aspects: e.g., metatraining (Chen et al., 2022; Min et al., 2022b), task instructions (Efrat & Levy, 2020; Mishra et al., 2022; Wei et al., 2021; Sanh et al., 2022), or task formulation (Holtzman et al., 2021; Zhao et al., 2021; Min et al., 2022a). In this paradigm, the choice of in-context (i.e., demonstration) examples has been shown crucial (Liu et al., 2022; Rubin et al., 2022; Lu et al., 2022), while recent work raised questions as to the degree to which correct labels are necessary (Min et al., 2022c). This work proposes an annotation-efficient in-context learning framework by focusing on the choice of examples and its implications on the annotation cost.\nActive Learning Active learning aims to enable machine learning models to achieve similar or greater performance with fewer labeled training instances (Cohn et al., 1994; Settles, 2009). Our selective annotation step for in-context learning shares the same goal of reducing the annotation cost. Most active learning methods involve iterative parameter updates (e.g., Wang et al., 2017; Kasai et al., 2019), which are computationally expensive for large language models used in in-context learning. Similar to our vote-k algorithm, Lin & Bilmes (2009) used the facility location objective to optimize representativeness. We observed that this objective largely underperforms vote-k for in-context learning, probably due to the fact the vote-k (1) is effectively tailored to the prompt retrieval step of in-context learning and (2) directly optimizes the diversity of selected samples (see \u00a74.5). More recently, the effectiveness of active learning has been questioned when large-scale pretrained models are finetuned for various tasks (Karamcheti et al., 2021; D\u2019Arcy & Downey, 2022). Our experiments (\u00a73) showed that selective annotation helps reduce the annotation cost of in-context learning, departing from the recent observations on finetuning with active learning. We hypothesize that it is because in-context learning with similarity-based prompt retrieval is more robust to outliers since each test instance only retrieves its most similar examples. This is supported by \u00a7 4.4, where random prompt retrieval does not benefit from selective annotation.\n# 6 CONCLUSION\nMuch recent work illustrated the ability of large language models to adapt to new tasks simply from a few demonstration examples. We presented in-depth studies on the implications of this ability for dataset annotation through the lens of selective annotation and introduced an annotation-efficient practice. The best selective annotation method explored in this paper, our vote-k method, selects diverse, representative examples to annotate. In terms of the task performance, vote-k improves the performance on 10 diverse tasks by a large margin. Moreover, vote-k selective annotation yields similar performance to state-of-the-art supervised finetuning with 10-100\u00d7 less annotation cost. We further show that the effectiveness of vote-k is consistent with different language model sizes and domain shifts between training and test data. We hope that our findings will help researchers and practitioners efficiently design new natural language tasks and beyond.\nWe thank Sewon Min, Pradeep Dasigi, Yanda Chen, Yushi Hu, Alisa Liu, and the ARK group at UW for their helpful feedback on this work.\nREFERENCES\nREFERENCES Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. In Proc. of TAC, 2009. URL https://tac.nist.gov/publications/2009/additional.papers/ RTE5_overview.proceedings.pdf. Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021. URL https://zenodo. org/record/5551208. Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion Proceedings of WWW, 2019. URL https://doi.org/10.1145/3308560.3317593. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. of NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u00b4c. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proc. of EMNLP, 2018. URL https://arxiv.org/ abs/1810.00278.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. In Proc. of TAC, 2009. URL https://tac.nist.gov/publications/2009/additional.papers/ RTE5_overview.proceedings.pdf.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021. URL https://zenodo. org/record/5551208.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. In Proc. of ACL, May 2022. URL https://arxiv.org/abs/ 2110.07814. David A. Cohn, Les E. Atlas, and Richard E. Ladner. Improving generalization with active learning. Machine Learning, 1994. URL https://doi.org/10.1007/BF00993277. Mike D\u2019Arcy and Doug Downey. Limitations of active learning with deep transformer language models, 2022. URL https://openreview.net/forum?id=Q8OjAGkxwP5. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019. URL https://arxiv.org/abs/810.04805.\nBill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proc. of COLING, 2004. URL https:// aclanthology.org/C04-1051. Avia Efrat and Omer Levy. The Turking test: Can language models understand instructions?, 2020. URL https://arxiv.org/abs/2010.11982. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proc. of ACL, 2021. URL https://aclanthology.org/2021.acl-long. 295. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn\u2019t always right. In Proc. of EMNLP, 2021. URL https://arxiv.org/abs/2104.08315. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Proc. of ACL, 2020. URL https://aclanthology.org/2020. acl-main.197. Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher Manning. Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering. In Proc. of ACL, 2021. URL https://arxiv.org/abs/2107.02331. Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, and Lucian Popa. Low-resource deep entity resolution with transfer and active learning. In Proc. of ACL, 2019. URL https://arxiv. org/abs/1906.08042. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In Proc. of ICML, 2021. URL https://proceedings.mlr.press/v139/koh21a.html. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. TACL, 2019. URL https://research. google/pubs/pub47761/. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In Proc. of ICLR, 2020. URL https://arxiv.org/abs/1909.11942. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, S\u00f6ren Auer, and Christian Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 2015. URL http://svn.aksw.org/papers/2013/SWJ_DBpedia/public.pdf. David D Lewis and William A Gale. A sequential algorithm for training text classifiers. In SIGIR\u201994, pp. 3\u201312. Springer, 1994. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Proc. of Text Summarization Branches Out, 2004. URL https://www.aclweb.org/anthology/ W04-1013/. Hui-Ching Lin and Jeff A. Bilmes. How to select a good training-data subset for transcription: submodular active selection for sequences. In Proc. of INTERSPEECH, 2009. URL https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1. 1.149.6843&rep=rep1&type=pdf. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proc. of DeeLIO 2022, 2022. URL https: //arxiv.org/abs/2101.06804.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In Proc. of ICLR, 2022. URL https://arxiv.org/ abs/2110.08207.\nBurr Settles. Active learning literature survey. Computer sciences technical report, University of Wisconsin\u2013Madison, 2009. URL https://burrsettles.com/pub/settles. activelearning.pdf. Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea Madotto, and Juneyoung Park. Dialogue summaries as dialogue states (ds2), template-guided summarization for few-shot dialogue state tracking. In ACL FINDINGS, 2022. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP, 2013. URL https://aclanthology.org/D13-1170. Alex Tamkin, Dat Nguyen, Salil Deshpande, Jesse Mu, and Noah D. Goodman. Active learning helps pretrained models learn the intended task, 2022. URL https://doi.org/10.48550/ arXiv.2204.08491. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR., 2019. URL https://arxiv.org/abs/1804.07461. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, 2021. Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-effective active learning for deep image classification. TCSVT, 2017. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Proc. of ICLR, 2021. URL https://arxiv.org/abs/2109.01652. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL, 2018. URL https://arxiv.org/ abs/1704.05426. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. HuggingFace\u2019s transformers: State-of-theart natural language processing. In Proc. of EMNLP: System Demonstrations, October 2020. URL https://aclanthology.org/2020.emnlp-demos.6. John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic programming. In Proc. of AAAI, 1996. URL http://www.aaai.org/Library/AAAI/ 1996/aaai96-156.php. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proc. of ACL, 2019. URL https://arxiv.org/abs/1905. 07830. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang (eds.), Proc. of ICML, 2021. URL https://arxiv.org/abs/2102.09690. Ruiqi Zhong, Tao Yu, and Dan Klein. Semantic evaluation for text-to-SQL with distilled test suites. In Proc. of EMNLP, 2020. URL https://arxiv.org/abs/2010.02840.\nDataset\nTask\nExamples\nHellaSwag\nCommonsense\nReasoning\nA woman is outside with a bucket and a dog. The dog is running\naround trying to avoid a bath. She. . .\nA) rinses the bucket off with soap and blow dry the dog\u2019s head.\nB) uses a hose to keep it from getting soapy.\n\u0013C) gets the dog wet, then it runs away again.\nD) gets into a bath tub with the dog.\nMRPC\nParaphrase\nDetection\nSales rose 37 per cent year-on-year to 1.76bn, beating expectations.\nSales for the quarter beat expectations, rising 37 percent\nyear-on-year to 1.76 billion euros.\n\u2192\u0013 Paraphrase\nSST\nSentiment Analysis\nA warm, funny, engaging film.\u2192Positive\nSuffers from the lack of a compelling narrative.\u2192Negative\nMWoZ 2.4\nDialogue State\nTracking\nI am looking for ALexender b&b\nDialogue state: alexander bed and breakfast\nGeoQuery\nSemantic Parsing\nWhat is the area of California?\nSELECT state.area FROM state WHERE state.\nstate_name=\u2019california\u2019\nDBpedia\nTopic Classification\nThe keeled box turtle (Cuora mouhotii syn. Pyxidea mouhotii) is a\nspecies of turtle in the family Geoemydidae. It is native to Asia\nwhere it occurs in China India Laos Burma Vietnam Thailand and\nBhutan. Other common names include keel-backed terrapin and\njagged-shelled turtle.\nTopic: animal\nMNLI\nNatural Language\nInference\nThe F/A-18-E/F program eliminated over 40 percent of the parts\nused to build predecessor aircraft to make the design more robust for\nmanufacturing and identified critical manufacturing processes,\nbringing them under control before the start of production. The new\ndesign with robustness also increased the safety of machines.\n\u2192\u0013 neutral\nRTE\nNatural Language\nInference\nJudie Vivian, chief executive at ProMedica, a medical service\ncompany that helps sustain the 2-year-old Vietnam Heart Institute in\nHo Chi Minh City (formerly Saigon), said that so far about 1,500\nchildren have received treatment. The previous name of Ho Chi\nMinh City was Saigon.\n\u2192\u0013 entailment\nNatural\nQuestions\nOpen-Domain QA\nwhen was music first played on the radio\n\u2192\u0013 1917\nXSUM\nSummarization\nBliss said there was a shortage of neonatal nurses and doctors, and\nsafety standards were not being met. ...... Dr Jenny Calvert, of the\nWales Neonatal Network, said they are working to further develop\nmedical training in neonatology to help recruit more trainee doctors.\nSummary: Neonatal services across Wales are overstretched and\nunder pressure with the safety of vulnerable babies at risk,\naccording to a charity.\nTable 6: All of the 10 datasets with examples used in our experiments. The 10 datasets span\nTable 6: All of the 10 datasets with examples used in our experiments. The 10 datasets span various formations, including classification (SST-5, Socher et al., 2013; MRPC, Dolan et al., 2004), multiple-choice selection (HellaSwag, Zellers et al., 2019), and code/text generation (MWoZ 2.4, Budzianowski et al., 2018; GeoQuery, Zelle & Mooney, 1996; NQ, Kwiatkowski et al., 2019).\n# Appendices\nA DATASETS AND TASKS\n# B PROMPT TEMPLATES\n# B.1 HELLASWAG\n# Input:\nThe topic is Grooming dog. Two women attempt to wash two dogs. they get in the tub with the dogs and do shampoo, soap, and then rinse the dogs. ...... The topic is Bathing dog. A couple is outside with a bucket and a dog. The dog is running around trying to avoid a bath. they Output:\nThe topic is Grooming dog. Two women attempt to wash two dogs. they get in the tub with the dogs and do shampoo, soap, and then rinse the dogs. ...... The topic is Bathing dog. A couple is outside with a bucket and a dog. The dog is running around trying to avoid a bath. they\n# Output:\nget the dog wet, then it runs away again.\n# B.2 MRPC\n# Input:\n# Output:\nequivalent\n# B.3 SST5\n# Input:\n# Output:\nnegative\n# B.4 MULTIWOZ\n# Input:\n# Output:\n# B.5 GEOQUERY\n# Input:\n# Output:\nRIVERalias0.RIVER_NAME FROM HIGHLOW AS HIGHLOWalias0 , RIVER AS RIVERalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) AND RIVERalias0.TRAVERSE = HIGHLOWalias0.STATE_NAME ORDER BY RIVERalias0. LENGTH DESC LIMIT 1 ;\n# B.6 DBPEDIA\n# Input:\ntitle: Cupressus funebris; content: Cupressus funebris (Chinese Weeping Cypress) is a species of cypress native to southwestern and central China. It may also occur naturally in Vietnam. plant ...... title: Keeled box turtle; content: The keeled box turtle (Cuora mouhotii syn. Pyxidea mouhotii) is a species of turtle in the family Geoemydidae. It is native to Asia where it occurs in China India Laos Burma Vietnam Thailand and Bhutan. Other common names include keel-backed terrapin and jagged-shelled turtle.\n# B.7 MNLI\n# Input:\nIdeally, the design fixes for the failures should be corrected prior to manufacturing production units.. Based on that information, is the claim The fixes should be addressed before they reach the assembly line if this was a smart plan. \"True\", \"False\", or \"Inconclusive\"? answer:Inconclusive\n# Output:\n# B.8 RTE\n# Input:\nAfter giving nearly 5,000 people a second chance at life, doctors are celebrating the 25th anniversary of Britian\u2019s first heart transplant which was performed at Cambridgeshire\u2019s Papworth Hospital in 1979..\\par question: The first heart transplant in Britian was performed in 1979.. True or False? answer:True\n# B.9 NATURAL QUESTION\n# Input:\nWrite an answer: who invented the radio during the industrial revolution other Guglielmo Marconi, 1st Marquis of Marconi ...... Write an answer: when was music first played on the radio\nother 1917\nB.10 XSUM\n# Input:\n# Output:\nNeonatal services across Wales are overstretched and under pressure wit the safety of vulnerable babies at risk, according to a charity.\n# C DETAILED MAIN RESULTS\nThis section provides a detailed version of our main results in Table 2, where the maximum performance and minimum performances among the three trials are reported. Results are shown in Table 7\nThis section provides a detailed version of our main results in Table 2, where the maximum performance and minimum performances among the three trials are reported. Results are shown in Table 7 and Table 8.\nMethod\nClassification\n|L| Selection\nMRPC\nSST-5\nMNLI\nDBpedia\nRTE\n100 Random 63.5/66.0/60.5 44.2/47.3/41.8 37.4/41.0/33.2 89.8/91.0/88.3 51.5/53.9/48.4\n100 Vote-k\n70.7/72.3/69.1 53.0/54.7/51.2 47.3/50.0/44.5 93.4/94.1/92.6 55.5/57.0/53.9\n18 Random 59.6/64.8/52.7 39.8/46.1/37.1 36.7/40.6/30.9 77.6/82.0/71.9 50.4/53.5/45.7\n18\nVote-k\n64.2/67.6/59.0 47.6/50.0/44.5 41.0/44.5/37.1 87.1/90.6/85.2 54.3/56.2/51.6\nMethod\nMulti-Choice\nDialogue\nGeneration\n|L| Selection\nHSwag\nMWoZ\nGeoQ\nNQ\nXSum\n100 Random 65.2/66.4/63.3 47.2/49.2/44.5 78.6/80.5/77.3 30.8/32.8/28.1 15.3/16.4/14.8\n100 Vote-k\n70.7/71.5/69.5 51.4/53.1/49.6 82.8/83.6/82.0 33.6/35.2/31.6 17.2/17.6/16.4\n18 Random 62.5/66.4/57.4 33.6/39.5/25.0 62.4/65.2/57.8 29.8/31.6/26.6 13.6/14.5/12.5\n18\nVote-k\n67.4/71.1/64.8 42.8/47.7/40.2 72.5/74.2/69.5 32.3/33.6/30.1 15.2/16.0/14.5\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5b54/5b54c9c8-e93c-420e-9f7c-15fd3076fdf3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: OPT-175B performance of ICL-random and ICL-vote-k on HellaSwag</div>\nE REMOVING OUTLIERS FOR FINETUNING\nHere we show that explicitly removing outliers also helps finetuning to benefit from vote-k.\nF DIVERSITY AND REPRESENTATIVENESS OF SELECTED SAMPLES\nWe hypothesized that both representativeness and diversity are crucial for selective annotation (\u00a72.1). Here we evaluate the diversity and representativeness of samples that are selected by different methods,\nOutliers not removed 10% outliers removed\nFT-random FT-vote-k FT-random FT-vote-k\nHellaSwag\n55.6\n53.5\n56.8\n59.6\nMRPC\n56.3\n55.6\n57.9\n60.4\nTable 9: Effects of vote-k in finetuning(FT) with annotation budget of 100. 10% outliers removed means that we removed 10% of examples farthest to the training data, measured by average cosine similarity. The selection is conducted after example removal. After removing outliers, vote-k selection improves the model few-shot performance.\nusing the methods from prior work on active learning (Margatina et al., 2021); their measures of diversity and representativeness use token overlap or embedding cosine similarities. As shown in Table 10, vote-k improves both the diversity and the representativeness as compared to random selection.\nMethod\nDIV-I\nDIV-F\nREPR.\nSelection\nHellaSwag\nSST-5\nMWoZ\nHellaSwag\nSST-5\nMWoZ\nHellaSwag\nSST-5\nMWoZ\nRandom\n0.1820.007 0.0990.003 0.3680.008\n0.4150.008 0.3170.004 0.6750.006\n0.5580.007 0.4240.003 0.6960.004\nVote-k\n0.191\n0.108\n0.379\n0.425\n0.321\n0.683\n0.565\n0.426\n0.702\nTable 10: DIV-I refers to diversity in input space, which measures the diversity of selected data in the input feature space, i.e., raw text; DIV-F refers to diversity in feature space, which measures the diversity in the dense feature space, i.e., sentence embeddings; REPR. refers to representativeness, which measures the representativeness of selected data. Subscripts stand for standard deviation.\n# G DETAILS OF SELECTIVE ANNOTATION METHODS\nLin & Bilmes (2009) proposed to maximize the facility location objective to optimize representativeness of the selected samples. Since this objective satisfies the submodular property, they applied a greedy algorithm as an approximation. Algorithm 2 describes the selective annotation method adapted from this greedy algorithm.\n# G.3 EMBEDDING DIVERSITY\nThis method aims to find diverse samples to annotate using embedding vectors. We first compute a vector representation for each unlabeled training instance by Sentence-BERT (Reimers & Gurevych, 2019), which is a variant of BERT (Devlin et al., 2019), finetuned to detect paraphrases.5 For instance, consider an example from SST-5 sentiment analysis in Table 6:A very well-made, funny and entertaining picture. We simply run Sentence-BERT on this text input and average the resulting vectors over the words to obtain a vector representation. Once embeddings are computed for all training data, we use them to find a diverse set of training instances. The intuition here is that a diverse set of annotated examples facilitates the subsequent prompt retrieval step since similar in-context examples can be found for many test instances. To find\nThis method aims to find diverse samples to annotate using embedding vectors. We first compute a vector representation for each unlabeled training instance by Sentence-BERT (Reimers & Gurevych, 2019), which is a variant of BERT (Devlin et al., 2019), finetuned to detect paraphrases.5 For instance, consider an example from SST-5 sentiment analysis in Table 6:A very well-made, funny and entertaining picture. We simply run Sentence-BERT on this text input and average the resulting vectors over the words to obtain a vector representation.\nAlgorithm 1 Voke-k Selective Annotation\nAlgorithm 1 Voke-k Selective Annotation\n1: Input: X = {xi}N\ni=1: a set of unlabeled samples; M: the number of samples to be selected; LM: inference\nlanguage model.\n2: Initialization: L = \u2205, U = X. G = (V, E), where V = X and (u, v) \u2208E if v is one of u\u2019s k nearest\nvertices in terms of the cosine similarity between the embeddings.\n3: while |L| < M/10 do\n4:\nu\u2217= arg maxu\u2208U\n\ufffd\nv\u2208{v|(v,u)\u2208E,v\u2208U} s(v),\nwhere s(v) = \u03c1\u2212|{\u2113\u2208L|(v,\u2113)\u2208E}|,\n\u03c1 > 1\n5:\nL = L \u222a{u\u2217}\n6:\nU = U \\ {u\u2217}\n7: end while\n8: for u in U do\n9:\nscore(u) = 1\nq\n\ufffd\nt logp(qt|q<t, z; \u0398), where p is LM prediction function and \u0398 is LM parameters\n10: end for\n11: for j = 1, . . . , 9 do\n12:\nUj = indices[(j \u22121)|U|/10 : j|U|/10]\n13:\nfor i = 1, . . . , |Uj| do\n14:\nu\u2217= arg maxu\u2208Uj\n\ufffd\nv\u2208{v|(v,u)\u2208E,v\u2208Uj} s(v),\nwhere s(v) = \u03c1\u2212|{\u2113\u2208L|(v,\u2113)\u2208E}|,\n\u03c1 > 1\n15:\nL = L \u222a{u\u2217}\n16:\nUj = Uj \\ {u\u2217}\n17:\nend for\n18: end for\n19: Return: L: selected samples.\nAlgorithm 2 Greedy Algorithm for Facility Location Objective\n1: Input: U = {xi}N\ni=1: a set of unlabeled samples; M: the number of samples to be selected.\n2: Initialization: L = \u2205, U = V . \u2200i, \u03c1i = \u22121: maximum similarity of xi to selected samples.\n3: while |L| < M do\n4:\nu\u2217= arg maxu\u2208U\n\ufffdN\ni=1 (max {0, cos(xi, xu) \u2212\u03c1i})\n5:\nL = L \u222a{u\u2217}\n6:\nU = U \\ {u\u2217}\n7:\n\u2200i, \u03c1i = max {\u03c1i, cos(xi, xu\u2217)}\n// update maximum similarity of each xi to selected samples\n8: end while\n9: Return: L: selected samples.\nAlgorithm 1 Voke-k Selective Annotation\n1: Input: X = {xi}N\ni=1: a set of unlabeled samples; M: the number of samples to be selected; LM: inferenc\nlanguage model.\n2: Initialization: L = \u2205, U = X. G = (V, E), where V = X and (u, v) \u2208E if v is one of u\u2019s k neares\nvertices in terms of the cosine similarity between the embeddings.\n3: while |L| < M/10 do\n4:\nu\u2217= arg maxu\u2208U\n\ufffd\nv\u2208{v|(v,u)\u2208E,v\u2208U} s(v),\nwhere s(v) = \u03c1\u2212|{\u2113\u2208L|(v,\u2113)\u2208E}|,\n\u03c1 > 1\n5:\nL = L \u222a{u\u2217}\n6:\nU = U \\ {u\u2217}\n7: end while\n8: for u in U do\n9:\nscore(u) = 1\nq\n\ufffd\nt logp(qt|q<t, z; \u0398), where p is LM prediction function and \u0398 is LM parameters\n10: end for\n11: for j = 1, . . . , 9 do\n12:\nUj = indices[(j \u22121)|U|/10 : j|U|/10]\n13:\nfor i = 1, . . . , |Uj| do\n14:\nu\u2217= arg maxu\u2208Uj\n\ufffd\nv\u2208{v|(v,u)\u2208E,v\u2208Uj} s(v),\nwhere s(v) = \u03c1\u2212|{\u2113\u2208L|(v,\u2113)\u2208E}|,\n\u03c1 > 1\n15:\nL = L \u222a{u\u2217}\n16:\nUj = Uj \\ {u\u2217}\n17:\nend for\n18: end for\n19: Return: L: selected samples.\nAlgorithm 2 Greedy Algorithm for Facility Location Objective\n1: Input: U = {xi}N\ni=1: a set of unlabeled samples; M: the number of samples to be selected.\n2: Initialization: L = \u2205, U = V . \u2200i, \u03c1i = \u22121: maximum similarity of xi to selected samples.\n3: while |L| < M do\n4:\nu\u2217= arg maxu\u2208U\n\ufffdN\ni=1 (max {0, cos(xi, xu) \u2212\u03c1i})\n5:\nL = L \u222a{u\u2217}\n6:\nU = U \\ {u\u2217}\n7:\n\u2200i, \u03c1i = max {\u03c1i, cos(xi, xu\u2217)}\n// update maximum similarity of each xi to selected samples\n8: end while\n9: Return: L: selected samples.\na set of diverse embeddings, we take a simple, iterative approach: in every iteration, we choose an instance furthest from the already chosen ones. Specifically, let L and U denote the sets of already chosen (i.e., labeled) samples and unlabeled samples, respectively. Suppose also that M is the target number of labeled examples (i.e., the annotation budget). Then, in every iteration, we choose the unlabeled sample that has the largest total cosine distance from L: arg minu\u2208U \ufffd \u2113\u2208L cos(u, \u2113). Here we abuse u and \u2113to mean both the instances and their embedding vectors from Sentence-BERT. The first labeled sample is randomly selected from the 3K unlabeled examples (\u00a73.1), and the iterative process continues until |L|=M.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving the efficiency of dataset annotation for natural language tasks by leveraging the capabilities of large language models in few-shot learning.",
        "problem": {
            "definition": "The problem is the high cost and inefficiency of manual annotation required for training language models on new tasks, which limits their applicability.",
            "key obstacle": "Existing methods often require large annotated datasets for effective in-context learning, leading to significant annotation costs and time."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that selective annotation can significantly improve the performance of language models while reducing the number of required annotated examples.",
            "opinion": "The proposed idea involves a two-step framework: first, selectively annotating a small set of diverse examples from unlabeled data, and second, retrieving these examples during testing to enhance model performance.",
            "innovation": "The innovation lies in the vote-k method, which selects diverse and representative instances for annotation, contrasting with previous methods that randomly selected examples."
        },
        "method": {
            "method name": "vote-k",
            "method abbreviation": "VK",
            "method definition": "Vote-k is an unsupervised, graph-based selective annotation method that identifies a subset of unlabeled data to annotate based on their diversity and representativeness.",
            "method description": "The method involves creating a directed graph of unlabeled instances and selecting examples that maximize representativeness while ensuring diversity.",
            "method steps": [
                "Compute vector representations for unlabeled instances using Sentence-BERT.",
                "Create a directed graph based on cosine similarity between embeddings.",
                "Select instances iteratively to ensure diversity and representativeness.",
                "Retrieve in-context examples from the annotated pool during testing."
            ],
            "principle": "The effectiveness of this method is based on the principle that diverse and representative examples improve the retrieval of relevant in-context examples for each test instance, thereby enhancing performance."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on 10 diverse NLP datasets covering classification, commonsense reasoning, dialogue, and text/code generation.",
            "evaluation method": "Performance was assessed by comparing task accuracy and other relevant metrics against random selection and state-of-the-art supervised finetuning methods."
        },
        "conclusion": "The experiments demonstrate that the vote-k selective annotation method significantly enhances in-context learning performance, achieving similar results to supervised finetuning with substantially reduced annotation costs.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to achieve high performance with fewer annotated examples, making it cost-effective and efficient for new tasks.",
            "limitation": "A limitation of the method is that its effectiveness may vary depending on the specific task and the nature of the unlabeled data.",
            "future work": "Future research could explore further refinements to the selective annotation process and investigate its applicability across more diverse tasks and datasets."
        },
        "other info": {
            "additional notes": "The research highlights the potential of large language models to adapt to new tasks with minimal annotations, emphasizing the importance of selective annotation in this context."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of improving the efficiency of dataset annotation for natural language tasks by leveraging the capabilities of large language models in few-shot learning."
        },
        {
            "section number": "1.2",
            "key information": "The high cost and inefficiency of manual annotation required for training language models on new tasks limits their applicability."
        },
        {
            "section number": "3.1",
            "key information": "The vote-k method selects diverse and representative instances for annotation, contrasting with previous methods that randomly selected examples."
        },
        {
            "section number": "3.4",
            "key information": "The effectiveness of the vote-k method is based on the principle that diverse and representative examples improve the retrieval of relevant in-context examples for each test instance."
        },
        {
            "section number": "4.1",
            "key information": "The proposed approach achieves high performance with fewer annotated examples, making it cost-effective and efficient for new tasks."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the method is that its effectiveness may vary depending on the specific task and the nature of the unlabeled data."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrate that the vote-k selective annotation method significantly enhances in-context learning performance, achieving similar results to supervised finetuning with substantially reduced annotation costs."
        }
    ],
    "similarity_score": 0.718231695516537,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Selective Annotation Makes Language Models Better Few-Shot Learners.json"
}