{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.03028",
    "title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models",
    "abstract": "Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today\u2019s largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.1.",
    "bib_name": "liu2024incompleteloopinstructioninference",
    "md_text": "# An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models\nEmmy Liu & Graham Neubig Language Technologies Institute Carnegie Mellon University emmy@cmu.edu, gneubig@cs.cmu.edu Jacob Andreas CSAIL Massachusetts Institute of Technology jda@mit.edu\nEmmy Liu & Graham Neubig Language Technologies Institute Carnegie Mellon University emmy@cmu.edu, gneubig@cs.cmu.ed\n# Abstract\nModern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today\u2019s largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.1.\narXiv:2404.03028v3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8629/86294031-7df7-4479-948a-6db5675d5922.png\" style=\"width: 50%;\"></div>\n# 1 Introduction\nSuppose a friend is teaching you to cook. You watch them place a pan on the stove and heat olive oil at low heat, adding minced garlic and chili flakes to the olive oil once it gets hot. Later, you decide to make the recipe yourself, but you are out of olive oil. You hypothesize that the olive oil served to cook the garlic without burning it, and so substitute butter for olive oil, as it should fulfill the same function. Here, you learned a generalizable cooking procedure by reasoning abductively\u2014finding the rule that best explains your experience (Frankfurt, 1958; Peirce, 1965). This is only one way of learning: the friend could have instead provided a recipe, from which you could have reasoned deductively about how to apply it in your kitchen. If you had remembered other times when you put but-\nFigure 1: Diagram of abductive reasoning for an LM. Red arrows show data flow in inductive reasoning (few-shot prompting), while blue arrows show data flow in deductive reasoning (instruction following). Black arrows indicate data flow unique to abductive reasoning (instruction induction). Instruction inference generally improves on few-shot prompting and zero-shot chain of thought. However, success at inductive reasoning and success at instruction inference are not related.\n1Code and data, including generated hypotheses may be found at https://github.com nightingal3/rule induction\nReasoning type\nAnalogue in LMs\nCitations\nDeductive\nInstruction following\nWei et al. (2022a); Sanh et al. (2022)\nInductive\nIn-context learning\nBrown et al. (2020)\nAbductive\nChain-of-thought (one hypothesis)\nWei et al. (2022b); Kojima et al. (2022)\nAbductive\nExternally aided instruction inference (many hypotheses)\nQiu et al. (2024); Wang et al. (2024);\nZhu et al. (2023); Yang et al. (2024),\nAbductive\nLM-guided instruction inference (many hypotheses)\nThis paper\nTable 1: Summary of reasoning types and analogues in language models. Citations to instruction following, in-context learning, and chain-of-thought are limited to the original paper due to the high number of papers on these topics, while we have tried to list all currently available papers on instruction inference (for abductive reasoning) with LMs.\nter on a hot pan before cooking, without explicitly reasoning about why, you could have cooked the same meal inductively.\nEach form of reasoning has a close analogue in current procedures for steering language models (LMs). In order to induce LMs to perform new tasks, we may condition them on explicit commands (instruction following; Wei et al., 2022a; Sanh et al., 2022), or a collection of examples from the task of interest (few-shot prompting; Brown et al., 2020); recently, several methods have been proposed that prompt LMs to generate textual commands from examples before conditioning on commands during prediction (instruction inference; Andreas et al., 2018; Honovich et al., 2023). But it is often unclear when to prefer one of these procedures over the other, and more generally how these different capabilities relate in current LMs. Does the ability to learn effectively from few-shot prompts imply the ability to perform instruction inference, or vice-versa? Are there tasks that can be learned from few-shot prompts, but not instructions? In this paper, we examine these questions through two tasks: learning numerical functions (\u00a74.1) and translation models (\u00a74.2, \u00a74.3). We ask two questions:\nFinding: Instruction inference improves over few-shot prompting in simple cases (linear function learning and simple artificial language learning), but suffers from incorrect hypotheses in a more complex case (low-resource machine translation).\nRQ2 How does the ability to learn from instructions (deductively) relate to the ability to learn from in-context examples (inductively or abductively)?\nFinding: The ability to learn through abduction (proposing hypotheses) is generally not related to learning through induction (few-shot learning). Deductive reasoning is generally strong, with large performance gains over using inductive reasoning alone when the provided hypothesis is correct.\nOur results highlight the diverse capabilities (and diversity of different reasoning mechanisms) triggered by different prompts and examples in current LMs; future work may investigate how these reasoning types can be combined or made consistent to enhance problem solving in LMs.\n# 2 Three Types of Reasoning in Language Models\nWe first give a formal definition of instruction following, in-context learning, and instruction inference, relating these processes to deductive reasoning, inductive reasoning, and abductive reasoning respectively. (Other ways of interacting with LMs may also evoke one or more of these forms of reasoning, but we focus on important and widely used prompting strategies.) Figure 1 gives a schematic of the reasoning loop, while Table 1 gives examples of\nwork falling in each category.2 Throughout this section, we assume that we have an input x, and we want to produce an output y with an autoregressive LM conditioned on some additional piece of data D that specifies the target task: pLM(y | x, D). We use in-context learning of linear functions as a running example.\n# 2.1 Instruction Following\nIn instruction following, we want to map each input x to a y according to a general instruction or prediction rule D that is specified in the input. This may be viewed as a kind of deductive reasoning, which in begins with one or more premises, and applies logically valid rules to reach a conclusion (Shapiro & Kouri Kissel, 2024). For instance, the instruction D might name a general function, such as Apply this function to the input: y = 5x + 3, followed by a specific query: Input: -3. This is illustrated in Figure 1.\n# 2.2 Few-Shot Prompting\nFew-shot prompting, by contrast, specifies the target task implicitly through examples. For each input x, the task specification D consists of a set of k training examples D = {(x1, y1), ..., (xk, yk)}. We then sample from pLM(y | x, D). Few-shot learning requires LMs to perform inductive reasoning (Hawthorne, 2021). Unlike deductive reasoning, there is no explicit premise stated, but the model must complete the task in a similar way to the examples. In Figure 1, this is pairs of inputs and outputs, i.e. Input: 5, Output: 28. These examples help specify the task as a numeric prediction task, as well as the identity of the specific target function.\n# 2.3 Instruction Inference\nInstruction inference connects instruction following and in-context learning: given examples D = {(xi, yi)}k i=1 and input x, we can instruct the model to generate a hypothesis h about the identity of the task, i.e. an instruction describing the task associated with D. We may sample one hypothesis and immediately condition on it (a form of chain-of-thought prompting), or sample several and select the most promising one. Compared to chain-of-thought, fewer studies have explored multi-hypothesis instruction induction; those that do typically rely on an external validation model rather than using the LM itself to evaluate. Our approach (in \u00a73) has the following high-level form: after sampling n hypotheses h1, ..., hn from pLM(h | t, x), we evaluate each hypothesis by assigning a score score(hi, t) based on the hypothesis and in-context examples. We choose the best one h\u2217= arg max score(hi, D), and feed it back into the context as an instruction.3 Abductive reasoning is often called \u201cinference to the best explanation\u201d (Lipton, 2001; Douven, 2021). Suppose that the model generates the hypotheses shown in Figure 1. We may parse hypotheses and apply them back to the in-context examples, then ranking them by prediction error. Then the model simply has to follow the instruction as in Section 2.1.\n# 3 Methods\nWe describe the implementation of instruction inference with multiple hypotheses in this section. As the other settings (few-shot, zero-shot chain-of-thought, etc) are already well understood, we do not explain them further, but include the exact prompts used to implement methods in Appendix B, F, and J.\n2We note that there is some disagreement in the philosophy literature about the exact distinction about inductive and abductive reasoning. Here we use commonly-cited definitions, which happen to distinguish in-context learning from the more explicit process of verbalizing and testing hypotheses. 3Several recent papers have recently proposed similar processes, but called this inductive reasoning We believe that abductive reasoning may be a more apt term in any process that includes hypothesis evaluation and selection. See Section 6 for more discussion.\nDomain\nIn-context examples (Di)\nQuery\nex-\nample (xi)\nExpected answer (yi)\nHypothesis\nexample\n(hi)\nFunctions\n(-10, -213), (9, 167), (4, 67), ...\n15\n287\nf (x) = 20x \u221213\nColours\n(lug dax, blue green), (lug zup, blue\nyellow), (lug bluf, blue blue), ...\nlug\nwalm\ndax bluf\nblue blue blue green\ngreen\nlug \u2192blue\nKalamang\nvocab (ek)\n(That is their place., Tompat ma me\nmuin), (I\u2019m getting pandandus, I\nwant to make a mat.), ...\nSakina\nis\npouching\nguavas.\nSakina sarimara lawat.\nguava \u2192sarim\nKalamang\ngrammar\n(ek)\n\u201c\u201d\n\u2013\n\u2013\nOrder of subject and\nverb: SV\n<div style=\"text-align: center;\">Table 2: Examples of in-context examples, queries, and hypotheses in each domain.</div>\nInstruction inference [Abduction with many hypotheses] After generating n hypotheses H = {h1, ..., hn}, we explore methods for reranking them. For all experiments, n = 5. Generally, these reranking methods capture \u201cfit\u201d to training data. External validator reranking was used only in the functions domain (see Section 4.2). The other reranking methods used a language model. Given scores of each hypothesis, we choose:\nSettings with instruction inference are referred to collectively as instruction inference. We detail the score functions below: Verbalized confidence We directly prompt the model to estimate the probability of the hypothesis given Di. scoreVerbal(hi, Di) is set to the model\u2019s probability estimation. P(data) We use a separate LM, 4 to generate log probabilities for in-context examples given the hypothesis. scoreP(data)(hi, Di) is the sum of log-probabilities of tokens of Di. P(answer) This is similar to P(data) and uses the same template, but scoreP(answer)(hi, Di) only sums log-probabilities of tokens in answers yi in the in-context examples. External validator For structured hypotheses, it is possible to parse them and apply them back to in-context examples, with the score as the negative error. It may not be possible to parse all hypotheses, if they are in natural language or inconsistent formats.\n# 4 Domains and Evaluation\nWe investigate LM behavior in three domains: linear function inference, an artificial language learning task, and vocabulary + typological feature learning in the Kalamang language. We refer to the underlying task we would like to improve (output prediction, translation) as the base task, and the task of inferring an explicit natural language hypothesis from few-shot examples as the abductive task. Examples of each task are in Table 2. We also evaluate hypotheses themselves in each domain. For the exact prompts used in each domain, refer to Appendix B, F and J. Models used across all domains consisted of gpt family models (gpt-3.5-turbo; Brown et al., 2020, gpt-4-turbo, OpenAI et al., 2024) and llama family models (llama-2-7b-chat, llama-2-70b-chat, Touvron et al., 2023).\n# 4.1 Linear Functions\nFollowing investigations of language models\u2019 in-context learning abilities (Garg et al., 2022; Aky\u00a8urek et al., 2023), we construct a dataset of 40 linear functions f (x) = ax + b, where a and b are uniformly sampled from the integers [\u221220, 20], along with 5 test examples for each function, yielding 200 test examples. For each test example, we randomly generate 5\next-davinci-002 was used as the logprobs-generating LM for all models\nin-context examples of the function (xi, yi)5 i=1 and one test example. The test example and inputs for in-context examples are also uniformly sampled from the integers [\u221220, 20]. We refer to this as the functions domain. Prompts used are given in Appendix B. Hypotheses In this domain, hypotheses are proposed using all in-context examples for each test example. The model was presented with functions (and instructed to write them) in the form y = ax + b. For external validator reranking, we parse the generated hypothesis, and score it by its negative mean-squared error (MSE) when applied to the in-context examples.5 If hi(xik) represent executing the parsed hypothesis represented by hi on example xik:\nHypotheses In this domain, hypotheses are proposed using all in-context examples for each test example. The model was presented with functions (and instructed to write them) in the form y = ax + b. For external validator reranking, we parse the generated hypothesis, and score it by its negative mean-squared error (MSE) when applied to the in-context examples.5 If hi(xik) represent executing the parsed hypothesis represented by hi on example xik:\nEvaluation To evaluate the base task, we use 0-1 accuracy, as well as median squared errors. To evaluate hypotheses, we examine the accuracy of model-proposed a and b coefficients, as well as the Spearman correlation between proposed coefficients and the ground truth.\n# 4.2 Simple Artificial Languages\nInspired by compositional instruction-following datasets, we generate a simple dataset where the inputs are nonce words such as lug, and the outputs are colour terms such as blue (Lake & Baroni, 2018; Lake et al., 2019). We call the expanded dataset the colours domain. The ruleset for this domain can be found in Appendix E. We generate 200 test examples and 800 training examples.6 Prompts used are in Appendix F. Hypotheses Following the original miniscan, we create a fixed minimal set of 5 in-context examples that contains each nonce word at least once, and from which the meaning of each nonce word can be reasonably inferred. During instruction inference, we isolate one nonce word at a time from the sentence, and have the model try to induce the vocabulary mapping for that word from 5 retrieved in-context examples containing that word. The ground truth grammar was written by the author, and consisted of production rules for each nonce term. Prompts used in the abductive and base tasks can be found in Appendix F. Evaluation We also use 0-1 accuracy to evaluate the base task of nonce word translation, as well as corpus-level chrF (Popovi\u00b4c, 2015). To evaluate the quality of the hypotheses themselves, we extract the production rules proposed by models and compare 0-1 accuracy against the ground truth. As a lenient evaluation on the \u201crepeat\u201d terms (see Appendix E for nonce terms and meanings), we marked a hypothesis for \u201crepeat\u201d terms as correct if it contained the term repeat or the numerals 2 and 3 respectively.\nInspired by compositional instruction-following datasets, we generate a simple dataset where the inputs are nonce words such as lug, and the outputs are colour terms such as blue (Lake & Baroni, 2018; Lake et al., 2019). We call the expanded dataset the colours domain. The ruleset for this domain can be found in Appendix E. We generate 200 test examples and 800 training examples.6 Prompts used are in Appendix F.\n# 4.3 Kalamang Translation\nFor low-resource translation, we use the Machine Translation with One Book (MTOB) dataset, an English\u2013Kalamang dataset with a grammar book (Tanzer et al., 2024). Kalamang is an extremely low-resource language with fewer than 200 speakers, and virtually no text on the web. The base task is to perform sentence-level translation in both directions, while the abductive task is to infer correct grammar features and vocabulary mappings. The dataset consisted of 100 test sentences (50 in each direction) and 400 train sentences. A ground-truth bilingual dictionary was provided (Visser, 2020). A grammar book was included as well, but to check correctness of high-level grammar inferences, we compiled a high level grammar sketch instead from WALS and GramBank features (Dryer & Haspelmath, 2013; Skirg\u02daard et al., 2023). More details about the grammar sketch can be found in Appendix K. Otherwise, we use the same experimental settings as the baseline, summarized in Appendix I.\n5If a model produced an unparsable hypothesis (for instance, I don\u2019t know, or a generic answer like y = ax + b), that hypothesis was assigned a score of \u2212\u221e. 6We generate this dataset instead of using the original miniscan to mitigate memorization of the original nonce words, as well as to gain more test examples, as the original dataset has only 10.\nDomain\nDeductive\nreasoning works\non average\nAbductive\nreasoning works\non average\nHypothesis\nproposal works\non average\nAbductive\nreasoning related\nto inductive\nreasoning\nFunctions\n\u2713\n\u2713\n\u2717\n\u2717\nColours\n\u2713\n\u2713\n\u2717\nKalamang\n\u2717\n\u2717\n\u2717\nTable 3: Summary of results. A checkmark indicates that the property held for all or almost all language models, a half-checkmark indicates a partial success for all or almost all language models, while an X-mark represents lack of success for most language models.\nHypotheses We split hypotheses into vocabulary and grammar. The ground truth instruction included retrieved examples from the wordlist and the grammar sketch. To induce the grammar sketch, we posed each grammar feature as a question, for instance: What is the order of subject and verb in Kalamang?, and sampled 5 sentence pairs at a time with the requisite parts of speech until the model proposed an answer to the question. If the model responded that it was unclear, this would repeat up to a maximum of 10 iterations. We only performed this process once, for GPT-3.5-turbo and GPT-4-turbo respectively. Each model used its self-induced grammar sketch at test time.7 For vocabulary induction, we followed a similar process as in the colours domain.8 Evaluation We use corpus-level chrF. To evaluate the grammar sketch and vocabulary induction, we respectively compare to the ground truth wordlist and grammar sketch.\nEvaluation We use corpus-level chrF. To evaluate the grammar sketch and vocabulary induction, we respectively compare to the ground truth wordlist and grammar sketch.\n# 5 Results\nIn this section, we first evaluate models\u2019 concrete performance across different domains (RQ1). We highlight the significant improvement instruction inference offers in some cases in synthetic tasks, yet despite this, improvements are not uniform across tasks, and not attested in challenging domains like Kalamang. Our analysis also highlights an inconsistent correlation between the quality of generated hypotheses and few-shot learning success (RQ2), meaning that the ability to generate or follow instructions doesn\u2019t reliably predict task mastery or vice versa. These results (Table 3) suggest that while structured instructions can boost performance in simpler scenarios, their impact is less predictable in complex settings. Furthermore, the relationship between instruction induction, instruction following, and in-context learning is complex, and each capability may rely on separate unknown aspects of model architecture, training procedures, or data.\n# 1 When Does Instruction Inference Improve Over In-Context L\nHow effective is using the true instruction? In the domains we study, the ground-truth instruction tends to yield accurate results. Figure 3 displays mean accuracy in the linear functions and colours domains over six trials, sampled at different temperatures (see Appendix A) for details. Notably, in linear functions (3A), GPT-4-turbo\u2019s accuracy increases to 96% from a baseline of 30%, with GPT-3.5 also notably improving. In the colours domain, (3B), we see the true instruction also helps all models except for Llama-2-7b. However, this trend does not extend to the Kalamang task, where most models struggled to leverage the provided wordlist and grammar sketch effectively, indicated by chrF scores in Figure 4. How effective are models\u2019 induced instructions? Self-generated instructions also improve on the baseline in many cases, with variations by domain. In linear functions, models\u2019 hypothesis induction markedly surpasses the few-shot baseline, with both the verbalized\n7Llama-2 models were found to be unable to propose grammar features with our prompts, so we used the GPT-3.5-turbo induced grammar sketch for these models. 8For Kalamang, due to computational costs, we cache the first parseable hypothesis proposed for each word and reuse it on subsequent sentences containing that word.\nconfidence and log-probability based reranking methods yielding comparable improvements (see Figure 3A again). For the colours domain (3B), instruction inference benefits performance, though not as strongly as in linear functions. chrF scores follow a similar trend, and are depicted in Appendix G. Interestingly, for gpt-4-turbo and Llama-2-7b, using the models\u2019 self-proposed hypotheses benefits performance more than using the ground-truth grammar for the colours language, despite the fact that self-proposed are not always correct. Unlike the two synthetic domains, inducing grammar and vocabulary items for Kalamang does not improve translation metrics in most cases.\n# 5.2 How Does the Ability to Induce Instructions Relate to In-Context Learning\n5.2 How Does the Ability to Induce Instructions Relate to In-Context Learning?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7dc3/7dc38ece-f2c8-4e77-b020-ca5a3b45da2e.png\" style=\"width: 50%;\"></div>\nHow accurate are model-generated hypotheses? In assessing accuracy of models\u2019 self-generated hypotheses across different domains, our findings reveal significant variations in accuracy. In the linear functions domain, we plot hypotheses generated by two models in Figure 2, and list the Spearman \u03c1 (Spearman, 1904) as well as p-values of model-predicted coefficients in Table 4. GPT-3.5-turbo and GPT-4-turbo\u2019s proposed coefficients are positively correlated with the real coefficients, and this is statistically significant. However, this level of accuracy is not observed with Llama-2 models, indicating a disparity in model capabilities. In the colours domain, most models, except GPT-4-turbo, tend to generate inaccurate hypotheses. The exact accuracy of proposed hypotheses for colours is shown in Appendix H. Mappings of simple vocabulary items such as lug tend to more accurate, with GPT-4-turbo achieving an 87% mean accuracy for hypotheses about this word. On the other hand, the difficulty increases with the terms which involved repeats, with GPT-3.5-turbo only achieving a 15% mean accuracy on bluf, the \u201crepeat twice\u201d term.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/77f0/77f02d72-62a8-4caa-9ddc-32fe3a83d63c.png\" style=\"width: 50%;\"></div>\nFigure 2: Real coefficients of linear functions and relationship to hypothesized coefficients for GPT3.5-turbo and GPT-4-turbo. Remaining models can be found in Appendix D. The x-axis has been truncated for visualization purposes (as there are some large outlier hypotheses). GPT-4-turbo is able to induce a reasonable function in-context, but other models struggle.\nHowever, when extending these analyses to Kalamang, all models\u2019 performance in predicting grammar features is relatively poor, with GPT-3.5-turbo predicting 5/18 and 4/18 features correct respectively. Appendix L shows the correctness of each grammar feature. Vocabulary induction accuracy is also generally low, falling between 10-20% for most models in both the En-\nModel\nb corr.\nb p-val\na corr\na p-val\nGPT-3.5-turbo\n0.27\n5.0 \u00d7 10\u221215\n0.56\n2.7 \u00d7 10\u221267\nGPT-4-turbo\n0.85\n2.2 \u00d7 10\u2212292\n0.91\n0.0\nLlama-2-7b\n-0.0069\n0.90\n0.14\n0.0081\nLlama-2-70b\n0.14\n0.060\n-0.018\n0.82\nTable 4: Spearman correlation and p-values of true vs predicted coefficients for each model in the functions domain.\nglish to Kalamang as well as Kalamang to English directions. See Appendix M for details, as well as averaged segment-level chrF for the vocabulary hypotheses. Is abductive reasoning related to in-context learning ability? In our final analysis, we examine whether the ability to induce correct instructions correlates with success in incontext learning. Specifically, we focus on the final hypotheses selected by the gpt model family, given that many hypotheses generated by llama models were incoherent or not\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/296d/296d93f4-f0f9-462c-83df-61bfdf3b8362.png\" style=\"width: 50%;\"></div>\nFigure 3: Accuracy of models in synthetic domains with and without hypothesis generation. Error bars indicate standard error. The top row shows results for the functions domain, while the bottom row shows results for the colours domain. Results are aggregated across 6 runs, and zero values are marked with \u20180\u2019.\nformatted correctly. We use the point-biserial correlation (Pearson, 1895) to assess the relationship between the accuracy of a model\u2019s final hypothesis and its success in in-context learning. In linear functions, we examine GPT-3.5-turbo because of GPT-4-turbo\u2019s consistent accuracy in selecting correct hypotheses.9 The analysis reveals a chance-level agreement (0.0013), suggesting GPT-3.5-turbo may be able to predict outputs without identifying the underlying function well, or vice versa. This reveals a dissociation between prediction accuracy and instruction induction. In the colours domain, we examine both gpt models and conduct a similar analysis for each nonce word. We divide the test examples into examples containing each word, and examine the point-biserial correlation between accuracy of induced word meanings and correct translations in the few-shot context. This correlation is generally low, and few p-values are significant after correcting for multiple comparisons with false discovery rate (FDR) (Benjamini & Hochberg, 1995). See Appendix H for details. In Kalamang, we repeat the process of computing point-biserial correlation between vocabulary induction correctness with segment-level chrF in the few-shot translations. Unlike the other domains, there is a small positive correlation between correct vocabulary hypotheses and chrF in gpt models. See Appendix M for details. We note that chrF is a more finegrained measure than accuracy, and the initial scores were low enough that copying some correct vocabulary items may have had a slight impact on otherwise completely incorrect translations.\nformatted correctly. We use the point-biserial correlation (Pearson, 1895) to assess the relationship between the accuracy of a model\u2019s final hypothesis and its success in in-context learning.\n# 6 Related Work\nHypothesis Proposal with Language Models Recent work explores hypothesis proposalto improve language model performance in synthetic tasks, namely ACRE, the original MiniSCAN, ListOps, and versions of the ARC dataset (Qiu et al., 2024; Wang et al., 2024). These methods often rely on domain-specific interpreters or code generation by LMs, akin to our functions domain\u2019s ground-truth reranker. We additionally explores probabilitybased reranking for rule selection across different domains and assesses the accuracy of model-induced rules prior to reranking.\ns means agreement cannot be computed with the point-biserial correlatio\nLanguage models have also been used to automate hypothesis discovery as an end in itself, to discover distributional differences in text (Zhong et al., 2022; 2023). In this case, the hypothesis proposer LM is paired with a validator trained to filter out irrelevant hypotheses. We similarly find that hypotheses generated by language models themselves may not be very accurate inherently.\nAbductive Reasoning in Language Models Datasets for abductive commonsense reasoning (Wang et al., 2019; Bhagavatula et al., 2020; Zhang et al., 2020) and logical reasoning (Sinha et al., 2019; Yang et al., 2024) have been proposed. Bhagavatula et al. (2020) focuses on the ability for a model to select the more plausible hypothesis from pairs of hypotheses, while Yang et al. (2024) focuses on proposing natural language rules and evaluating them against ground-truth human rules. Zhao et al. (2023) use contrastive explanations to tune a model to recognize fluent ones. Comparatively, we do not focus on commonsense reasoning, and examine relationships between reasoning types on the same task. Program Synthesis and Library Learning The general approach we outline for abductive reason-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f06e/f06e3336-ae1d-4422-8bc3-57818d1ddf12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: chrF scores for Kalamang under different methods, in English to Kalamang direction (top row) and Kalamang to English direction (bottom row)</div>\ning can be considered a soft form of program synthesis with language models. Library learning, in which a set of reusable tools is learned from examples, is related (Ellis et al., 2023). A similar approach to ours using LLMs is proposed by Zhu et al. (2023), who use a similar two-step process to learn a library of rules for an arithmetic domain as well as a previous synthetic dataset (Sinha et al., 2019). We find similar gains on synthetic domains, and further examine challenges in applying abductive reasoning in complex domains. LM-generated Instructions Instruction backtranslation is a related concept (Honovich et al., 2023; Li et al., 2024), in which an LM generates instructions for textual data. However, it differs in that the proposed instructions are not directly used at the same time that they are proposed, and it does not focus on generating rules.\n# 7 Conclusion\nWe have examined the interplay between deductive, inductive, and abductive reasoning in LMs through the tasks of hypothesis proposal, in-context learning, and self-generated instruction following. Across three domains (linear function learning, artificial language translation, and Kalamang translation), we show that instruction inference is able to improve over few-shot prompting in simple synthetic domains, but that the relationship between these types of reasoning is complex, and they may not work together as expected presently when models are solving complex tasks. As abductive reasoning seems to be a relatively weaker capability in current language models as compared to instruction following, future work could develop more advanced mechanisms for natural-language hypothesis verification and correction. The use of hypothesis proposal during training remains underexplored, and joint training of models on question answering and hypothesis proposal with enforced consistency may help models display more consistent behaviour. Enhancements in these areas could accelerate progress towards models capable of autonomous learning and self-improvement.\n# Acknowledgements\nThank you to Kiril Gashteovski, Ekin Aky\u00a8urek, Shuyan Zhou, and Linlu Qiu for helpful discussions on this project. Additional thanks to David Mortensen for suggestions on grammar features and linguistic consultation. Thank you to NEC Labs Europe for supporting this project through the Student Research Fellowship program. We also acknowledge the support of the National Sciences and Engineering Research Council of Canada (NSERC) through a postgraduate fellowship given to EL (PGSD). We also appreciate support from Intel and the National Science Foundation through grants CCF-2217064 and IIS-2238240.\n# References\nEkin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In Proceedings of the Eleventh International Conference on Learning Representations (ICLR), 2023. Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2166\u20132179, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-1197. URL https://aclanthology.org/N18-1197. Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289\u2013300, 1995. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=Byg1v1HKDB. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Igor Douven. Abduction. In Edward N. Zalta (ed.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2021 edition, 2021. Matthew S. Dryer and Martin Haspelmath (eds.). WALS Online (v2020.3). Zenodo, 2013. doi: 10.5281/zenodo.7385533. URL https://doi.org/10.5281/zenodo.7385533. Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sabl\u00b4e-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake\u2013sleep bayesian program learning. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 381(20220050), 2023. doi: 10.1098/rsta.2022.0050. URL https://doi.org/10. 1098/rsta.2022.0050. H. Frankfurt. Peirce\u2019s notion of abduction. Journal of Philosophy, 55:593\u2013596, 1958. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022.\nKevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sabl\u00b4e-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake\u2013sleep bayesian program learning. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 381(20220050), 2023. doi: 10.1098/rsta.2022.0050. URL https://doi.org/10. 1098/rsta.2022.0050.\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022.\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022.\nJames Hawthorne. Inductive Logic. In Edward N. Zalta (ed.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2021 edition, 2021. Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1935\u20131952, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022. Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00a8assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 2879\u20132888. PMLR, 2018. URL http: //proceedings.mlr.press/v80/lake18a.html. Brenden M. Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. In Annual Meeting of the Cognitive Science Society, 2019. URL https://api. semanticscholar.org/CorpusID:58006558. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR), 2024.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00b4on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00b4ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila\nBelbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00b4on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. Karl Pearson. Notes on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58:240\u2013242, 1895. Charles Sanders Peirce. Collected Papers of Charles Sanders Peirce, Volume 5, volume 5. Harvard University Press, 1965. URL http://www.hup.harvard.edu/catalog.php?isbn= 9780674138001. Maja Popovi\u00b4c. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392\u2013395, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR), 2024. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In Proceedings of the Tenth International Conference on Learning Representations (ICLR), 2022. URL https://arxiv.org/abs/2110. 08207. Stewart Shapiro and Teresa Kouri Kissel. Classical Logic. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2024 edition, 2024. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4506\u20134515, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1458. URL https://aclanthology.org/D19-1458. Hedvig Skirg\u02daard, Hannah J. Haynie, Dami\u00b4an E. Blasi, Harald Hammarstr\u00a8om, Jeremy Collins, Jay J. Latarche, Jakob Lesage, Tobias Weber, Alena Witzlack-Makarevich, Sam Passmore, Angela Chira, Luke Maurits, Russell Dinnage, Michael Dunn, Ger Reesink, Ruth Singer, Claire Bowern, Patience Epps, Jane Hill, Outi Vesakoski, Martine Robbeets, Noor Karolin Abbas, Daniel Auer, Nancy A. Bakker, Giulia Barbos, Robert D. Borges, Swintha Danielsen, Luise Dorenbusch, Ella Dorn, John Elliott, Giada Falcone, Jana Fischer, Yustinus Ghanggo Ate, Hannah Gibson, Hans-Philipp G\u00a8obel, Jemima A. Goodall, Victoria\nGruner, Andrew Harvey, Rebekah Hayes, Leonard Heer, Roberto E. Herrera Miranda, Nataliia H\u00a8ubler, Biu Huntington-Rainey, Jessica K. Ivani, Marilen Johns, Erika Just, Eri Kashima, Carolina Kipf, Janina V. Klingenberg, Nikita K\u00a8onig, Aikaterina Koti, Richard G. A. Kowalik, Olga Krasnoukhova, Nora L.M. Lindvall, Mandy Lorenzen, Hannah Lutzenberger, T\u02c6onia R.A. Martins, Celia Mata German, Suzanne van der Meer, Jaime Montoya Samam\u00b4e, Michael M\u00a8uller, Saliha Murado\u02d8glu, Kelsey Neely, Johanna Nickel, Miina Norvik, Cheryl Akinyi Oluoch, Jesse Peacock, India O.C. Pearey, Naomi Peck, Stephanie Petit, S\u00a8oren Pieper, Mariana Poblete, Daniel Prestipino, Linda Raabe, Amna Raja, Janis Reimringer, Sydney C. Rey, Julia Rizaew, Eloisa Ruppert, Kim K. Salmon, Jill Sammet, Rhiannon Schembri, Lars Schlabbach, Frederick W.P. Schmidt, Amalia Skilton, Wikaliler Daniel Smith, Hil\u00b4ario de Sousa, Kristin Sverredal, Daniel Valle, Javier Vera, Judith Vo\u00df, Tim Witte, Henry Wu, Stephanie Yam, Jingting Ye, Maisie Yong, Tessa Yuditha, Roberto Zariquiey, Robert Forkel, Nicholas Evans, Stephen C. Levinson, Martin Haspelmath, Simon J. Greenhill, Quentin D. Atkinson, and Russell D. Gray. Grambank reveals global patterns in the structural diversity of the world\u2019s languages. Science Advances, 9, 2023. doi: 10.1126/sciadv.adg6175.\nCharles Spearman. The proof and measurement of association between two things. American Journal of Psychology, 15:72\u2013101, 1904.\nGarrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi.  benchmark for learning to translate a new language from one grammar book, 2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\nEline Visser. Kalamang dictionary. Dictionaria, (13):1\u20132737, 2020. URL https://dictionaria. clld.org/contributions/kalamang.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4020\u20134026, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1393. URL https://aclanthology.org/P19-1393. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman. Hypothesis search: Inductive reasoning with language models. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR), 2024. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Proceedings of the Tenth International Conference on Learning Representations (ICLR), 2022a. URL https://arxiv.org/abs/2109.01652. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS), 2022b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS), 2022b.\nZonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 209\u2013225, St. Julian\u2019s, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. eacl-long.13. Hongming Zhang, Xinran Zhao, and Yangqiu Song. WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5736\u20135745, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.508. URL https://aclanthology.org/2020.acl-main.508. Wenting Zhao, Justin Chiu, Claire Cardie, and Alexander Rush. Abductive commonsense reasoning exploiting mutually exclusive explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14883\u2013 14896, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.acl-long.831. URL https://aclanthology.org/2023.acl-long.831. Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing differences between text distributions with natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00b4ari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 27099\u201327116. PMLR, 2022. URL https://proceedings.mlr.press/v162/zhong22a.html. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules, 2023.\nZonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 209\u2013225, St. Julian\u2019s, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. eacl-long.13. Hongming Zhang, Xinran Zhao, and Yangqiu Song. WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5736\u20135745, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.508. URL https://aclanthology.org/2020.acl-main.508. Wenting Zhao, Justin Chiu, Claire Cardie, and Alexander Rush. Abductive commonsense reasoning exploiting mutually exclusive explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14883\u2013 14896, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.acl-long.831. URL https://aclanthology.org/2023.acl-long.831. Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing differences between text distributions with natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00b4ari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 27099\u201327116. PMLR, 2022. URL https://proceedings.mlr.press/v162/zhong22a.html. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules, 2023.\n# Appendix\n# A Model Inference Settings\nGeneration settings, as well as other details, used in the three domains are detailed here. Each model was tested 6 times for each setting (few-shot, zs-cot...) at different temperatures, and the aggregate results are shown in figures and tables throughout the paper. The exact model versions used for gpt models were gpt-3.5-turbo-0613 and gpt-4-1106-preview. Functions When answering questions in the base task, gpt models were tested 3 times each at temperature T = {0, 1} (note that T = 0 is nondeterministic in gpt models because of hardware). No max number of tokens was set for the generation. For llama models, T = {0.1, 1} was used, also with no max number of tokens. When generating hypotheses, we always used the same model as performed the base task. That is to say, gpt-3.5-turbo would generate hypotheses used by gpt-3.5-turbo, and so on. Hypotheses were always generated with a temperature above 0 to encourage generation of diverse hypotheses. For gpt models, hypotheses were generated with T = 1 and for llama models, with T = 1.0625. The specific value for llama models was because llama would usually generate the same hypothesis 5 times at T = 1, but higher values greatly increased the number of nonsensical and badly formatted hypotheses. When self-evaluating hypotheses, the verbalized confidence score was generated at T = 0. When using the log-probabilities from text-davinci-002 to rerank hypotheses, the model was also set to T = 0. Colours As in the functions domain, gpt models and llama models were respectively tested 3 times each at temperature T = {0, 1} and T = {0.1, 1} with no max number of tokens. When generating hypotheses, T = 1 was used for all models. When self-evaluating hypotheses, settings were the same as in the functions domain. Kalamang For the base translation task, the same settings were used for generation as in Tanzer et al. (2024). Due to cost constraints, we ran only once in each translation direction on each setting with T = 0.05. Vocabulary hypotheses for all models were generated with T = 1. Grammar feature hypotheses were generated independently of translations, and the first non-null hypothesis was chosen due to cost constraints. T = 0.7 was used for gpt grammar hypotheses, and T = 1 was used for llama grammar hypotheses. When self-evaluating hypotheses, T = 0 was once again used for all models.\n# B Prompts for Linear Functions Domain\nTable 5: Prompts for the functions domain. Newlines are depicted visually for ease of reading. Variables that are substituted depending on the question are marked like {this}. The wording for the \u201dprompt with self-induced hypothesis\u201d and \u201dzero-shot chain-of-thought prompt\u201d were slightly changed from the few-shot examples prompt because models were sometimes confused by long-winded hypotheses, and responded with a long-winded answer in return, causing failures to parse their answers. In comparison, models mostly returned outputs in correct formats in other settings.\nPrompt Type\nUsage\nPrompt Text\nBase system\nprompt\nFor reasoning\nwith in-context\nexamples\nYou are a problem solving system. Your job is to use the\ninput-output pairs to solve the problem as well as you can.\nHypothesis\nproposal system\nprompt\nFor proposing\nhypotheses based\non in-context\nexamples\nYou are a pattern recognition system. Your job is to come up with\na function that describes the data as well as you can.\nInstruction\nfollowing system\nprompt\nFor applying a\nproposed\nhypothesis or\nground-truth\nhypothesis to the\ninput\nYou are a problem solving system. Your job is to apply the\nfunction to the data in order to produce an answer.\nFew-shot\nexamples prompt\nFor reasoning\nwith in-context\nexamples only\nReturn the output preceded by \u2019Output:\u2019\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nInput: {query input}\nPrompt with\nground-truth\nhypothesis\nUsed when\nprompting the\nmodel to directly\napply the correct\nhypothesis to the\ninput. In-context\nFunction:\n{The real function}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nReturn the output preceded by \u2019Output:\u2019\nPrompt Type\nUsage\nPrompt Text\nBase system\nprompt\nFor reasoning\nwith in-context\nexamples\nYou are a problem solving system. Your job is to use the\ninput-output pairs to solve the problem as well as you can.\nHypothesis\nproposal system\nprompt\nFor proposing\nhypotheses based\non in-context\nexamples\nYou are a pattern recognition system. Your job is to come up with\na function that describes the data as well as you can.\nInstruction\nfollowing system\nprompt\nFor applying a\nproposed\nhypothesis or\nground-truth\nhypothesis to the\ninput\nYou are a problem solving system. Your job is to apply the\nfunction to the data in order to produce an answer.\nPrompt Type\nUsage\nPrompt Text\nBase system\nprompt\nFor reasoning\nwith in-context\nexamples\nYou are a problem solving system. Your job is to use the\ninput-output pairs to solve the problem as well as you can.\nHypothesis\nproposal system\nprompt\nFor proposing\nhypotheses based\non in-context\nexamples\nYou are a pattern recognition system. Your job is to come up with\na function that describes the data as well as you can.\nInstruction\nfollowing system\nprompt\nFor applying a\nproposed\nhypothesis or\nground-truth\nhypothesis to the\ninput\nYou are a problem solving system. Your job is to apply the\nfunction to the data in order to produce an answer.\nFew-shot\nexamples prompt\nFor reasoning\nwith in-context\nexamples only\nReturn the output preceded by \u2019Output:\u2019\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nInput: {query input}\nPrompt with\nground-truth\nhypothesis\nUsed when\nprompting the\nmodel to directly\napply the correct\nhypothesis to the\ninput. In-context\nexamples are also\nincluded.\nFunction:\n{The real function}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nReturn the output preceded by \u2019Output:\u2019\nInput: {query input}\nPrompt for\nhypothesis\ninduction\nUsed to have the\nmodel propose a\nsingle hypothesis\nfor the function\nbased on\nin-context\nexamples\nWrite the function that captures the relationship\nbetween inputs and outputs.\nYou should write it in the form y = ax\u02c60 + bx\u02c61.\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nFunction (please write explicitly in the exact form\"\n\u2018Output: y = ax\u02c60 + bx\u02c61)\u2019:\nFew-shot\nexamples prompt\nFor reasoning\nwith in-context\nexamples only\nReturn the output preceded by \u2019Output:\u2019\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nInput: {query input}\nPrompt with\nground-truth\nhypothesis\nUsed when\nprompting the\nmodel to directly\napply the correct\nhypothesis to the\ninput. In-context\nexamples are also\nincluded.\nFunction:\n{The real function}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nReturn the output preceded by \u2019Output:\u2019\nInput: {query input}\nPrompt for\nhypothesis\ninduction\nUsed to have the\nmodel propose a\nsingle hypothesis\nfor the function\nbased on\nin-context\nexamples\nWrite the function that captures the relationship\nbetween inputs and outputs.\nYou should write it in the form y = ax\u02c60 + bx\u02c61.\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nFunction (please write explicitly in the exact form\"\n\u2018Output: y = ax\u02c60 + bx\u02c61)\u2019:\nTable 5: Prompts for the functions domain. Newlines are depicted visually for ease of reading. Variables that are substituted depending on the question are marked like {this}. The wording for the \u201dprompt with self-induced hypothesis\u201d and \u201dzero-shot chain-of-thought prompt\u201d were slightly changed from the few-shot examples prompt because models were sometimes confused by long-winded hypotheses, and responded with a long-winded answer in return, causing failures to parse their answers. In comparison, models mostly returned outputs in correct formats in other settings.\nUse this function to apply to the input example to get the correct output.\nPrompt with a\nself-induced\nhypothesis\nUsed similarly to\nthe \u201dprompt with\nground-truth\nhypothesis\u201d,\nexcept with a\nself-generated\nhypothesis. The\nwording is slightly\nchanged.\n{ model\u2019s hypothesis }\nHowever, just write the output like\nwhat\u2019s shown in these examples.\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nReturn the output preceded by \u2019Output:\u2019\nInput: {query input}\nPrompt for\nzero-shot\nchain-of-thought\nUsed to encourage\nthe model to\ngenerate a chain\nof thought.\nReturn the output preceded by \u2019Final Output:\u2019\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nLet\u2019s think step by step about what the function\ncould be. Remember to write down \u2019Final Output:\u2019\nbefore your final answer.\nInput: {query input}\nPrompt for\nhypothesis\nprobability\nestimate\nUsed to prompt a\nlanguage model\ndirectly for\nreranking\nhypotheses\nHow likely is this hypothesis\nabout the function to be true given the data?\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nFunction explanation: {model\u2019s hypothesis}\nPlease give a probability between 0 and 1 inclusive,\nand only answer with a number.\nProbability:\nPrompt for data\nlogprobs given\nhyp estimate\nUsed to rerank\nhypotheses based\non logprobs.\nThese are examples of applying this function:\n{model\u2019s hypothesis}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\n17\n<div style=\"text-align: center;\">Please give a probability between 0 and 1 inclusive, and only answer with a number.</div>\nPrompt for data\nlogprobs given\nhyp estimate\nUsed to rerank\nhypotheses based\non logprobs.\nThese are examples of applying this function:\n{model\u2019s hypothesis}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\n# C Prediction Fit of Other Models on Linear Functions\nFigure 5 shows predictions made by each model when using a few-shot prompt, true instruction, or self-induced instruction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8486/848634de-4e6f-4a4b-9f10-abb9e0ad0449.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Model predictions plotted against true function output for all models. Range is restricted to the [-400, 400] range for visualization purposes, although there are large outlier values for all models.</div>\n<div style=\"text-align: center;\">Figure 5: Model predictions plotted against true function output for all models. Range is restricted to the [-400, 400] range for visualization purposes, although there are large outlier values for all models.</div>\n# D Predicted Coefficients Compared to Real Coefficients in Linear Functions\n# D Predicted Coefficients Compared to Real Coefficients in Linear\nFigure 6 plots model hypotheses about the coefficients of x0 and x1 in a linear function against the actual coefficients\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a317/a317dc3b-52be-44d8-93bf-583121f4bf33.png\" style=\"width: 50%;\"></div>\nFigure 6: Model predictions plotted against true function output for all models. Range is restricted to the [-400, 400] range for visualization purposes, although there are large outlier values for all models.\n# E Grammar and Details of Colours Domain\nThe rules of the colours domain are expressed through production rules below. Models were found to respond better to verbal instructions than formal ones in initial testing (e.g. a verbal statement \u201drepeat twice\u201d rather than [[x]]bluf \u2192[[x]][[x]])\nListing 1: Grammar of the colours language, as presented to LMs. lug -> blue dax -> green wif -> red zup -> yellow bluf -> repeat the last action twice walm -> repeat the last action three times\nTraining and test data was automatically generated by generating sentences of up to 5 nonce words on the source side, with shorter sentences being more likely (the respective probabilities for sentence lengths from 1 to 5 are [0.4, 0.3, 0.15, 0.1, 0.05]). Each colour could also be repeated with the repeat actions, and whether repeat nonce words were inserted was also random, though skewed toward no repetition (the probabilities were respectively [0.8,\nInput: wif walm Output: red red red\nInput: lug walm dax bluf Output: blue blue blue green green\n# F Prompts for Colours Domain\nTable 6: Prompts for the colours domain. Newlines are depicted visually for ease of reading. Variables that are substituted depending on the question are marked like {this}. The \u201dprompt with a self-induced hypothesis\u201d was slightly modified from the base prompt in order to encourage models to follow the correct formatting, while the \u201dprompt for zero-shot chain-of-thought\u201d was slightly modified to encourage models to generate a concrete chain of thought, and also to follow the correct formatting.\nPrompt Type\nUsage\nPrompt Text\nBase system\nprompt\nFor reasoning\nwith in-context\nexamples\nYou are a problem solving system. Your job is to use the\ninput-output pairs to solve the problem as well as you can.\u2019\u2019\nHypothesis\nproposal system\nprompt\nFor proposing\nhypotheses based\non in-context\nexamples\nYou are a rule induction system. Your job is to figure out the\nrules underlying a problem and report on them. Use the examples\nto guide your thinking.\nInstruction\nfollowing system\nprompt\nFor applying a\nproposed\nhypothesis or\nground-truth\nhypothesis to the\ninput\nYou are a parser. Carefully use the grammar to parse inputs to\ndetermine the correct output.\nFew-shot\nexamples prompt\nFor reasoning\nwith in-context\nexamples only\nReturn the output preceded by \u2019Output:\u2019\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nInput: {query input}\nRemember to start your answer with \u2019Output:\u2019\nTable 6: Prompts for the colours domain. Newlines are depicted visually for ease of reading. Variables that are substituted depending on the question are marked like {this}. The \u201dprompt with a self-induced hypothesis\u201d was slightly modified from the base prompt in order to encourage models to follow the correct formatting, while the \u201dprompt for zero-shot chain-of-thought\u201d was slightly modified to encourage models to generate a concrete chain of thought, and also to follow the correct formatting.\nPrompt with\nground-truth\nhypothesis\nUsed when\nprompting the\nmodel to directly\napply the correct\nhypothesis to the\ninput. In-context\nexamples are also\nincluded.\nUse this grammar to parse the input example\nto get the correct output.\nGrammar:\n{colours domain grammar}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nReturn the output preceded by \u2019Output:\u2019\nInput: {query input}\nPrompt for\nhypothesis\ninduction\nUsed to have the\nmodel propose a\nsingle hypothesis\nfor the translation\nof a word.\nThe below examples contain the nonce word {word}.\nUsing the examples, deduce what {word} means.\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nWrite your answer like this: word -> meaning.\nMeaning can be a word or a general rule dependent on the context.\nRule:\nPrompt with a\nself-induced\nhypothesis\nUsed similarly to\nthe \u201dprompt with\nground-truth\nhypothesis\u201d,\nexcept with a\nself-generated\nhypothesis. The\nwording is slightly\nchanged.\nUse this grammar to parse the input example\nto get the correct output.\n{ model\u2019s hypothesis grammar }\nHowever, just write the output like\nwhat\u2019s shown in these examples.\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nReturn the output preceded by \u2019Output:\u2019\nInput: {query input}\n<div style=\"text-align: center;\">Return the output preceded by \u2019Output:\u2019 Input: {query input}</div>\nPublished as a conference paper at COLM 2024\nTable 6: Prompts for the colours domain. Newlines are depicted visually for ease of reading.\nVariables that are substituted depending on the question are marked like {this}. The \u201dprompt\nwith a self-induced hypothesis\u201d was slightly modified from the base prompt in order to encour-\nage models to follow the correct formatting, while the \u201dprompt for zero-shot chain-of-thought\u201d\nwas slightly modified to encourage models to generate a concrete chain of thought, and also to\nfollow the correct formatting.\nPrompt for\nzero-shot\nchain-of-thought\nUsed to encourage\nthe model to\ngenerate a chain\nof thought.\nReturn the output preceded by \u2019Final Output:\u2019\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nLet\u2019s think step by step about what the translation could be.\nWork through your answer step by step and show your work.\nRemember to write down \u2019Final Output:\u2019\nbefore your final answer.\nInput: {query input}\nPrompt for\nhypothesis\nprobability\nestimate\nUsed to prompt a\nlanguage model\ndirectly for\nreranking\nhypotheses\nThese are examples of the\ntranslation of the word {word}.\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nGiven these examples, how likely is\nthis hypothesis about the meaning of {word}?\n{model\u2019s word hypothesis}\nPlease give a probability between 0 and 1 inclusive,\nand only answer with a number.\nProbability:\nPrompt for data\nlogprobs given\nhyp estimate\nUsed to rerank\nhypotheses based\non logprobs.\nThese are examples of applying this function:\n{model\u2019s word hypothesis}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\nTable 6: Prompts for the colours domain. Newlines are depicted visually for ease of reading. Variables that are substituted depending on the question are marked like {this}. The \u201dprompt with a self-induced hypothesis\u201d was slightly modified from the base prompt in order to encourage models to follow the correct formatting, while the \u201dprompt for zero-shot chain-of-thought\u201d was slightly modified to encourage models to generate a concrete chain of thought, and also to follow the correct formatting.\n<div style=\"text-align: center;\">Please give a probability between 0 and 1 inclusive, and only answer with a number.</div>\nProbability:\nPrompt for data\nlogprobs given\nhyp estimate\nUsed to rerank\nhypotheses based\non logprobs.\nThese are examples of applying this function:\n{model\u2019s word hypothesis}\nExamples:\nInput: {input1}\nOutput: {output1}\nInput: {input2}\nOutput: {output2}\n...\n<div style=\"text-align: center;\">These are examples of the translation of the word {word}.</div>\n# G chrF for Colours Domain\nFigure 7 depicts the mean chrF score over 6 trials for each model in each setting. Reranking methods are averaged for instruction-induction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a5f7/a5f70b8a-17d7-4dd7-838d-146ff814a2bb.png\" style=\"width: 50%;\"></div>\nFigure 7: chrF scores for the colours domain under different methods. All reranking method are averaged.\n<div style=\"text-align: center;\">Figure 7: chrF scores for the colours domain under different methods. All reranking methods are averaged.</div>\n# H Vocabulary Induction Accuracy for Colours Domain\nWe show in Table 7 the mean accuracy for each colour term for gpt models, computed over all selected (highest-ranked) word hypotheses. Point-biserial correlation between hypothesis correctness (in the instruction-induction case) and few-shot correctness is also shown. P-values are corrected with FDR in the trials for each model. We performed the correction per model rather than across all models because we were separately interested in the behaviour of each model. We parsed the production rule generated by the model in order to determine hypothesis correctness.\n<div style=\"text-align: center;\">Table 7: Vocabulary induction correctness and relation to few-shot translation correctness for gpt models. Significant correlations after correction for multiple comparisons are bolded.</div>\n models. Significant correlations after correction for multiple comparisons are bold\nModel\nWord\nmethod\nMean Correctness\nCorrelation\np-value\np-value corrected\ngpt-3.5-turbo\nlug\nverbal conf\n0.60\n-0.037\n0.30\n0.53\ngpt-3.5-turbo\ndax\nverbal conf\n0.23\n0.067\n0.064\n0.30\ngpt-3.5-turbo\nwif\nverbal conf\n0.33\n0.036\n0.65\n0.82\ngpt-3.5-turbo\nzup\nverbal conf\n0.61\n0.15\n0.029\n0.20\ngpt-3.5-turbo\nbluf\nverbal conf\n0\n\u2013\n\u2013\n\u2013\ngpt-3.5-turbo\nwalm\nverbal conf\n0\n\u2013\n\u2013\n\u2013\ngpt-4-turbo\nlug\nverbal conf\n0.88\n0.044\n0.17\n3.58e-1\ngpt-4-turbo\ndax\nverbal conf\n0.92\n-0.042\n0.19\n3.58e-1\ngpt-4-turbo\nwif\nverbal conf\n0.86\n0.0445\n0.57\n7.32e-1\ngpt-4-turbo\nzup\nverbal conf\n0.87\n0.058\n0.42\n5.75e-1\ngpt-4-turbo\nbluf\nverbal conf\n0.15\n0.13\n0.26\n3.83e-1\ngpt-4-turbo\nwalm\nverbal conf\n0.17\n0.049\n0.69\n7.55e-1\ngpt-3.5-turbo\nlug\np data\n0.29\n0.059\n0.098\n0.30\ngpt-3.5-turbo\ndax\np data\n0.10\n0.010\n0.78\n0.84\ngpt-3.5-turbo\nwif\np data\n0.20\n0.24\n0.0016\n0.023\ngpt-3.5-turbo\nzup\np data\n0.4\n-0.11\n0.10\n0.30\ngpt-3.5-turbo\nbluf\np data\n0.077\n-0.17\n0.14\n0.33\ngpt-3.5-turbo\nwalm\np data\n0\n\u2013\n\u2013\n\u2013\ngpt-4-turbo\nlug\np data\n0.73\n-0.080\n0.012\n5.31e-2\ngpt-4-turbo\ndax\np data\n0.73\n-0.041\n0.20\n3.58e-1\ngpt-4-turbo\nwif\np data\n0.77\n-0.026\n0.74\n7.55e-1\ngpt-4-turbo\nzup\np data\n0.76\n0.022\n0.76\n7.55e-1\ngpt-4-turbo\nbluf\np data\n0.10\n-0.14\n0.22\n3.58e-1\ngpt-4-turbo\nwalm\np data\n0.14\n-0.26\n0.027\n9.89e-2\ngpt-3.5-turbo\nlug\np answer\n0.44\n-0.013\n0.72\n0.84\ngpt-3.5-turbo\ndax\np answer\n0.19\n0.0040\n0.91\n0.91\ngpt-3.5-turbo\nwif\np answer\n0.35\n-0.074\n0.34\n0.53\ngpt-3.5-turbo\nzup\np answer\n0.59\n-0.088\n0.21\n0.43\ngpt-3.5-turbo\nbluf\np answer\n0.051\n-0.055\n0.63\n0.82\ngpt-3.5-turbo\nwalm\np answer\n0\n\u2013\n\u2013\n\u2013\ngpt-4-turbo\nlug\np answer\n0.70\n-0.066\n0.038\n1.15e-1\ngpt-4-turbo\ndax\np answer\n0.71\n0.19\n3.67e-9\n6.61e-8\ngpt-4-turbo\nwif\np answer\n0.82\n-0.25\n0.0013\n1.13e-2\ngpt-4-turbo\nzup\np answer\n0.68\n-0.14\n0.045\n1.15e-1\ngpt-4-turbo\nbluf\np answer\n0.23\n-0.29\n0.0089\n5.31e-2\ngpt-4-turbo\nwalm\np answer\n0.057\n-0.058\n0.63\n7.55e-1\n# I MTOB Experimental Settings\nWe generally used the same experimental setup as in mtob. For the few-shot condition, we used two reference sentences for each word on the source side, selected via longest subsequence. For the true-instruction setting, we used reference sentences, in addition to the wordlist and true grammar sketch. For each word in the source sentence, the most similar word from the wordlist was also retrieved based on the longest common substring. In the instruction-inference settings, the model\u2019s self-induced grammar sketch was substituted for the true grammar sketch, and the model was also prompted to first create hypotheses about the translation of each word in the source sentence. At the time that we conducted experiments, the Kalamang to English training set was not available, so we created this training set by reversing the source and target in the English to Kalamang training set. This may have slightly harmed translations in this direction, as this caused the training set to be different from the one reported in the benchmark results.\nAt the time that we conducted experiments, the Kalamang to English training set was not available, so we created this training set by reversing the source and target in the English to Kalamang training set. This may have slightly harmed translations in this direction, as this caused the training set to be different from the one reported in the benchmark results.\n# J MTOB Prompts\nTable 8: Prompts for Kalamang translation with MTOB. We follow the same prompt formatting as in Tanzer et al. (2024), with the exception of replacing retrieved grammar book passages with the grammar sketch in Appendix K. Newlines are depicted visually for ease of reading. Variables that are substituted depending on the question are marked like {this}. The English to Kalamang direction is depicted in the examples. \u2018\u2018\u2019\u2019 indicates that the prompt is the same as in Appendix F.\nPrompt Type\nUsage\nPrompt Text\nBase system\nprompt\nFor reasoning\nwith in-context\nexamples\n\u2018\u2018\u2019\u2019\nHypothesis\nproposal system\nprompt\n(Grammar)\nFor proposing\ngrammar feature\nhypotheses based\non in-context\nexamples\n\u2018\u2018\u2019\u2019\nHypothesis\nproposal system\nprompt (Vocab)\nFor proposing\nvocabulary\nmappings based\non in-context\nexamples\n\u2018\u2018\u2019\u2019\nInstruction\nfollowing system\nprompt\nFor applying a\nproposed\nhypothesis or\nground-truth\nhypothesis to the\ninput\n\u2018\u2018\u2019\u2019\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e60/7e608261-38c0-4b2a-9c2b-a0f98cc172e7.png\" style=\"width: 50%;\"></div>\nTable 8: Prompts for Kalamang translation with MTOB. We follow the same prompt formatting\nas in Tanzer et al. (2024), with the exception of replacing retrieved grammar book passages\nwith the grammar sketch in Appendix K. Newlines are depicted visually for ease of reading.\nVariables that are substituted depending on the question are marked like {this}. The English to\nKalamang direction is depicted in the examples. \u2018\u2018\u2019\u2019 indicates that the prompt is the same as\nin Appendix F.\nFew-shot\nexamples prompt\nFor reasoning\nwith in-context\nexamples only\nHuman: Kalamang is a language spoken on the Karas Islands\nin West Papua. Translate the following sentence\nfrom English to Kalamang:\n{query input}\nTo help with the translation, here is a translated sentence with words\nsimilar to \u2018{word}\u2019 in a list of translated\nKalamang-English reference sentences:\nEnglish sentence: {eng sentence}\nK",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of how different reasoning types in language models (LMs) relate to instruction following, few-shot prompting, and instruction inference. It highlights the complexities in these mechanisms and their implications for learning tasks.",
        "problem": {
            "definition": "The primary problem is understanding how various prompting procedures\u2014instruction following, few-shot prompting, and instruction inference\u2014affect the reasoning capabilities of LMs.",
            "key obstacle": "A significant challenge is the non-systematic nature of reasoning in LMs, where the effectiveness of different reasoning types does not correlate as expected."
        },
        "idea": {
            "intuition": "The idea was inspired by the different ways humans learn tasks, such as through explicit instructions or by deriving rules from examples.",
            "opinion": "The authors propose that LMs exhibit distinct reasoning types, each with its own strengths and weaknesses, which complicates the learning process.",
            "innovation": "The main innovation lies in the exploration of instruction inference as a distinct reasoning type that can improve performance over traditional few-shot prompting in certain contexts."
        },
        "Theory": {
            "perspective": "The perspective involves a framework categorizing reasoning types in LMs\u2014deductive, inductive, and abductive\u2014each corresponding to different prompting strategies.",
            "opinion": "The authors assume that these reasoning types can interact in complex ways that are not yet fully understood.",
            "proof": "The paper provides empirical evidence through experiments showing the performance of LMs across different reasoning tasks, highlighting the variable success rates."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using four LMs from the GPT and Llama families across three domains: linear function learning, artificial language tasks, and low-resource machine translation.",
            "evaluation method": "The evaluation involved comparing model outputs against ground truth across various tasks, measuring accuracy and error rates."
        },
        "conclusion": "The findings indicate that while instruction inference can enhance performance in simpler tasks, its effectiveness varies significantly, particularly in complex domains. The relationship between different reasoning types remains intricate.",
        "discussion": {
            "advantage": "One advantage of this paper is its comprehensive analysis of how LMs utilize different reasoning types, contributing to a deeper understanding of their capabilities.",
            "limitation": "A limitation is the inconsistent performance of LMs across tasks, particularly in more complex scenarios like Kalamang translation.",
            "future work": "Future research could focus on developing better mechanisms for hypothesis verification and exploring joint training methods to enhance reasoning consistency."
        },
        "other info": [
            {
                "info1": "The paper discusses the implications of reasoning types for the development of more advanced LMs.",
                "info2": {
                    "info2.1": "The authors suggest potential applications in autonomous learning systems.",
                    "info2.2": "The paper includes detailed appendices with experimental prompts and settings."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of how different reasoning types in language models relate to instruction following, few-shot prompting, and instruction inference."
        },
        {
            "section number": "1.2",
            "key information": "The primary problem is understanding how various prompting procedures affect the reasoning capabilities of language models."
        },
        {
            "section number": "3.1",
            "key information": "The paper provides empirical evidence through experiments showing the performance of LMs across different reasoning tasks, highlighting the variable success rates."
        },
        {
            "section number": "3.2",
            "key information": "The perspective involves a framework categorizing reasoning types in LMs\u2014deductive, inductive, and abductive\u2014each corresponding to different prompting strategies."
        },
        {
            "section number": "4.1",
            "key information": "The main innovation lies in the exploration of instruction inference as a distinct reasoning type that can improve performance over traditional few-shot prompting in certain contexts."
        },
        {
            "section number": "6.1",
            "key information": "A limitation is the inconsistent performance of LMs across tasks, particularly in more complex scenarios."
        },
        {
            "section number": "6.4",
            "key information": "Future research could focus on developing better mechanisms for hypothesis verification and exploring joint training methods to enhance reasoning consistency."
        }
    ],
    "similarity_score": 0.7272938551751963,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/An Incomplete Loop_ Instruction Inference, Instruction Following, and In-context Learning in Language Models.json"
}