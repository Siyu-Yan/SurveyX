{
    "from": "google",
    "scholar_id": "7J4z55bh0l8J",
    "detail_id": null,
    "title": "I N-C ONTEXT L EARNING D EMONSTR",
    "abstract": " A BSTRACT\n\nLarge Language Models (LLMs) have showcased their In-Context Learning (ICL) capabilities, enabling few-shot learning without the need for gradient updates. Despite its advantages, the effectiveness of ICL heavily depends on the choice of demonstrations. Selecting the most effective demonstrations for ICL remains a significant research challenge. To tackle this issue, we propose a demonstration selection method named InfICL, which utilizes influence functions to analyze impacts of training samples. By identifying the most influential training samples as demonstrations, InfICL aims to enhance the ICL generalization performance. To keep InfICL cost-effective, we only use the LLM to generate sample input embeddings, avoiding expensive fine-tuning. Through empirical studies on various real-world datasets, we demonstrate advantages of InfICL compared to state-ofthe-art baselines.\n\nKeywords large language models \u00b7 in-context learning \u00b7 demonstration selection \u00b7 influence functions\n\n# 1 Introduction\n\nLarge Language Models (LLMs) have demonstrated their ability to perform few-shot inference through In-Context Learning (ICL) [Brown et al., 2020]. Specifically, by providing a few demonstrations for the given task, the LLM is able to perform test case inference without performing any model gradient update.\nICL has several benefits such as few-shot learning, avoiding model fine-tuning, and versatility to different learning tasks. Despite these benefits, the ICL performance is sensitive to the selected demonstrations. To address this limitation, many different approaches have been proposed for demonstration selection, e.g., selecting demonstrations which are similar to the test case in the embedding space [Gao et al., 2021, Liu et al., 2022, Wu et al., 2023, Qin et al., 2023, Yang et al., 2022], learning a deep learning-based demonstration retriever [Rubin et al., 2022, Luo et al., 2023, Chen et al., 2020, Karpukhin et al., 2020, Scarlatos and Lan, 2023, Zhang et al.,",
    "bib_name": "IN-CONTEXT1",
    "md_text": "# I N-C ONTEXT L EARNING D EMONSTRATION S ELECTION VIA I NFLUENCE A NALYSIS\n\nA PREPRINT\nVinay M.S.\u2217\nUniversity of Arkansas\nFayetteville, AR 72701, USA\nvmadanbh@uark.edu\nMinh-Hao Van\u2217\nUniversity of Arkansas\nFayetteville, AR 72701, USA\nhaovan@uark.edu\nXintao Wu\nUniversity of Arkansas\nFayetteville, AR 72701, USA\nxintaowu@uark.edu\n# A BSTRACT\n\nLarge Language Models (LLMs) have showcased their In-Context Learning (ICL) capabilities, enabling few-shot learning without the need for gradient updates. Despite its advantages, the effectiveness of ICL heavily depends on the choice of demonstrations. Selecting the most effective demonstrations for ICL remains a significant research challenge. To tackle this issue, we propose a demonstration selection method named InfICL, which utilizes influence functions to analyze impacts of training samples. By identifying the most influential training samples as demonstrations, InfICL aims to enhance the ICL generalization performance. To keep InfICL cost-effective, we only use the LLM to generate sample input embeddings, avoiding expensive fine-tuning. Through empirical studies on various real-world datasets, we demonstrate advantages of InfICL compared to state-ofthe-art baselines.\n\nKeywords large language models \u00b7 in-context learning \u00b7 demonstration selection \u00b7 influence functions\n\n# 1 Introduction\n\nLarge Language Models (LLMs) have demonstrated their ability to perform few-shot inference through In-Context Learning (ICL) [Brown et al., 2020]. Specifically, by providing a few demonstrations for the given task, the LLM is able to perform test case inference without performing any model gradient update.\nICL has several benefits such as few-shot learning, avoiding model fine-tuning, and versatility to different learning tasks. Despite these benefits, the ICL performance is sensitive to the selected demonstrations. To address this limitation, many different approaches have been proposed for demonstration selection, e.g., selecting demonstrations which are similar to the test case in the embedding space [Gao et al., 2021, Liu et al., 2022, Wu et al., 2023, Qin et al., 2023, Yang et al., 2022], learning a deep learning-based demonstration retriever [Rubin et al., 2022, Luo et al., 2023, Chen et al., 2020, Karpukhin et al., 2020, Scarlatos and Lan, 2023, Zhang et al., 2022, Li et al., 2023], selecting demonstrations based on LLM feedback [Li and Qiu, 2023, Chen et al., 2023b, Wang et al., 2023], etc. However, there is a lack of consensus regarding the most effective demonstration selection approach [Nguyen and Wong, 2023]. The current research challenge is to identify those demonstrations which are the most effective or influential for improving the ICL generalization performance. We address this challenge by employing influence functions [Koh and Liang, 2017]. Specifically, influence functions provide mechanisms to analyze effects or influences of training samples on the model without retraining the model. For example, influence functions can be used to analyze the model effects after up-weighting or removing a training sample. The training samples which have higher influences naturally provide more contributions to the model learning process. Intuitively, identifying these influential training samples can aid in improving the ICL generalization performance.\nIn this work, we focus on the text classification problem, and propose an influence function analysis-based demonstration selection method called InfICL. Since we need to perform influence function analysis on the training samples, an obvious approach is to calculate these influence scores by using the LLM itself [Grosse et al., 2023]. However, for\n\n* These authors contributed equally to this work.\n\nlarge and complex deep learning models, the influence function analysis becomes erroneous [Basu et al., 2021]. An other approach is to fine tune the final layers of the LLM and perform influence function analysis by using these fina layers. However, fine tuning LLM is a highly resource intensive task. To address these practical challenges, we only employ the LLM to generate sample embeddings. By employing these LLM generated training sample embeddings we train a simple classifier. We analyze the influence of each training sample by using the classifier and a validation set. Finally, we select the most influential training samples from each class as the demonstration set. We summarize our main contributions below.\n\n\u2022 We propose a ICL demonstration selection method called InfICL which is based on influence function analysis.\n\u2022 We present a running cost analysis study and compare our InfICL to other advanced influence analysisbased demonstration selection methods [Nguyen and Wong, 2023, Chang and Jia, 2023]. In particular, we demonstrate that these contemporary methods require an exceedingly high number of LLM access calls in comparison to our InfICL.\n\u2022 We present an empirical study conducted on multiple real-world datasets and four LLMs of varying sizes. In this empirical study, we show that our InfICL can outperform the contemporary demonstration selection methods.\n\n# 2 Related Work\n\nOur work mainly focuses on designing an demonstration selection method for ICL through influence analysis.\nDemonstration Selection.  Recently, the problem of demonstration selection for ICL has received a significant attention in the literature. We direct the interested readers to [Liu et al., 2021, Dong et al., 2023] for detailed surveys regrading different demonstration selection methods. One of the popular approaches for demonstration selection is to select those training samples as demonstrations which are similar to the test sample in the embedding space [Gao et al., 2021, Liu et al., 2022, Wu et al., 2023, Qin et al., 2023, Yang et al., 2022].\nAnother popular approach is to employ a demonstration retriever to perform demonstration selection. Specifically, the demonstration retriever is a deep learning based model. Rubin et al. [2022] and Luo et al. [2023] train their demonstration retriever by employing contrastive loss [Chen et al., 2020]. Li et al. [2023] employ in-batch negative loss [Karpukhin et al., 2020]. Scarlatos and Lan [2023] and Zhang et al. [2022] employ reinforcement learning to train their demonstration retriever. In our work, we do not utilize any complex demonstration retriever, and design a simple method which operates on LLM embeddings.\nRecently, LLM feedback based demonstration selection methods have been proposed. Specifically, the LLM is queried for its prediction confidence on each training sample. Li and Qiu [2023] identify training samples which are more informative. Chen et al. [2023b] select training points which are less sensitive to predictions. Wang et al. [2023] fine tune the LLM by using only the final emdedding layer and model the demonstration selection as a topic model. These methods can also be considered as influence based methods because they analyze the influence of training samples by using direct LLM feedback.\nInfluence Functions. For machine learning applications, influence functions have been used for different tasks, e.g., filtering or relabeling mislabeled training data [Kong et al., 2022], designing data poisoning attacks [Fang et al., 2020, Jagielski et al., 2021], designing data augmentation strategies [Lee et al., 2020, Oh et al., 2021], and analyzing label memorization effects [Feldman and Zhang, 2020]. For LLMs, influence functions have been used to identify data artifacts [Han et al., 2020], identify biases in word embeddings [Brunet et al., 2019], and explaining the LLM performance [Grosse et al., 2023, Han and Tsvetkov, 2021].\nInfluence analysis can be broadly divided into two categories: retraining based [Ilyas et al., 2022] and gradient based methods also called as influence functions [Koh and Liang, 2017]. The retraining based methods collect random subsets of the training set. Then, the influence of each training sample in the collected subset is calculated by either model retraining or by learning a linear surrogate. However, the retraining based methods have high running costs, and are not scalable to large datasets because to effectively cover all the training samples, a large number of subsets have to be constructed and evaluated [Grosse et al., 2023]. Nguyen and Wong [2023] and Chang and Jia [2023] employ retraining based influence analysis to construct the demonstration sets and as a result, their proposed demonstration selection methods incur high running costs. We provide a detailed design description about these demonstration selection methods and compare their running costs against our InfICL in Section 3.2. Specifically, we show that by using the gradient based influence analysis for constructing demonstration sets, we can overcome the high running cost challenge associated with the retraining based influence analysis methods.\n\n# 3 Proposed Method\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a54/7a54a1e8-eda0-496e-9d97-459dd326bc2b.png\" style=\"width: 50%;\"></div>\nFigure 1: Illustration of ICL for the text classification task through our InfICL. Initially, by employing the local LLM, embeddings for all the training and validation set inputs are generated. A local classifier is then trained by employing training input embeddings and labels. InfICL determines K demonstration examples based on influence scores. Finally, the demonstration set and each test case are sent to an external LLM for inference.\nWe consider the text classification task having a training set T with n training points denoted as z i = {(x i, y i)} n i =1. Here, x i and y i denote the embedding vector for the i th training sample input s i  and its corresponding label, respectively. Let C denote the class set for the target variable y i and y i \u2208C. We employ a validation set denoted as V.\n\n# 3.1 Algorithm\n\nFigure 1 shows our influence analysis based demonstration selection method. We employ separate LLMs for demonstration selection and test case inference called local LLM P and external LLM Q, respectively. For local LLM P, to reduce training costs, we employ a light-weight LLM and use it to generate embeddings for the input texts. Let E P denote the embedding layer of P which generates the sample embeddings. Here, x i = E P (s i, \u03d5), where parameter \u03d5 \u2208 \u03a6, and \u03a6 denotes the local LLM (P) parameter space. We denote L nt (s i, \u03d5) as the next token prediction loss for P. For external LLM Q, we opt a powerful and heavier LLM. We include a local classifier denoted as F (x i, \u03b8) with the input of embeddings and parameterized with \u03b8 \u2208 \u0398, and \u0398 denotes the classifier (F) parameter space. We denote L f (z i, \u03b8) as the classifier training loss.\nOur goal is to select K suitable demonstrations for the given text classification task. Note that K is analogous to the number of shots in few-shot learning and is constrained by the employed external LLM. We employ a balanced selection approach wherein we select equal number of demonstrations from each class c \u2208C. Specifically, we select R (R = \u230a K/ |C|\u230b) suitable training set points from each class as demonstrations.\nAlgorithm 1 shows the pseudo code of our InfICL. The inputs include training set T, validation set V, classifier F, loss L f, the number of demonstration examples per class R, and local LLM P. Initially, by employing the local LLM P, we generate embeddings for all training and validation inputs. In lines 2-4, we train the local classifer F using the embeddings and labels. Next, we calculate influence score of each training point (lines 5-7). For each class c \u2208C, we select the topR training points {z c i} R i =1 as demonstrations from T based on influence scores (lines 8-10). Finally, we return the constructed demonstration set \u222a c \u2208C {z c i} R i =1.\n\nAlgorithm 1 InfICL demonstration selection.\nInputs: T , V, F, Lf, R, and P.\nOutput: demonstration set \u222ac\u2208C{zc\ni }R\ni=1.\n1: generate embeddings for all training and validation inputs through P;\n2: for each training epoch do\n3:\ntrain classifier F on T by using Lf;\n4: for each zi = (xi, yi) \u2208T do\n5:\ncalculate its influence score by using Eq 1;\n6: for each class c \u2208C do\n7:\nselect top-R training points {zc\ni }R\ni=1 from T based on influence scores;\n8: return \u222ac\u2208C{zc\ni }R\ni=1\nInfluence Functions. The main goal of the influence functions is to study the effect of training points on model prediction [Koh and Liang, 2017]. Influence functions provide a practical solution wherein, the model parameter change can be studied without retraining the model. Let 1\nn \ufffd n i =1 L f (z i, \u03b8). It is assumed that the empirical risk is twice differentiable and strictly convex. However, this assumption can be practically relaxed. The influence of up-weighting training point z on the classifier parameter \u03b8 can be calculated by using the influence function as\nn \ufffd n i =1 L f (z i, \u03b8) be the empirical risk and its minimizer is given by \ufffd \u03b8 = arg min \u03b8 \u2208 \u0398 1\n\n\uf8fb\n\uf8f0\n\ufffd\n\ufffd \ufffd\nSpecifically, the highly influential training points are those with most positive (\u2212I up,loss (z, V)) scores [Koh and Liang, 2017]. We employ Inf (z, V) = \u2212I up,loss (z, V) as the influence score to analyze the influence of up-weighting each training point z on the loss L f at V. This is because, the training points which have high influences on the validation loss provide richer information for model learning, and can become better demonstrations for the ICL task.\nPersonalized Demonstration Selection. We can easily extend InfICL to construct a personalized demonstration set for each test case x test. Specifically, we can extend InfICL to this setting by scoring each training point z i \u2208T as\nwhere\n\nscore (z i) = \u03bbInf (z i, V) + (1 \u2212 \u03bb) sim (x i, x test) (2) wh\nsim (\u00b7, \u00b7) denotes the cosine similarity between the input embeddings, and \u03bb is the weight which can be set analyzing the accuracy performance on the validation set. The topR training points from each class based score (z i) are included in the demonstration set.\n\n# 3.2 Running Cost Analysis\n\nIn this section, we study the running costs of our InfICL along with other influence analysis based demonstration selection methods, Influence [Nguyen and Wong, 2023] and Curation [Chang and Jia, 2023]. Note that both methods employ retraining based influence analysis approach and our InfICL employs gradient based influence analysis approach. We show running cost benefits of our InfICL over Influence and Curation.\nWe quantify the running costs of demonstration selection methods by analyzing the total number of LLM access (API) calls for both local and external LLMs. Specifically, the unit cost of local LLM (P) access call for generating an embedding for a single training point is denoted as C P. Similarly, the unit cost of external LLM (Q) access call for performing inference on a single test or validation case is denoted as C Q. Note that C Q is usually much higher than C P. This is because C Q involves ICL cost w.r.t external LLM and C P only generates the final layer embeddings which\n\nwhere\n\n(2)\n\nforward pass cost. We show the running costs of different influence analysis based demonstration in Table 1 and provide a detailed description below.\n\nMethods\nInfluence [Nguyen and Wong, 2023]\nCuration [Chang and Jia, 2023]\nInfICL\nCondAcc\nData Models\nRunning Cost\nO (CQ|V|M)\nO (CQ|V|MK!)\nO (CQ|V|M)\nO (CP|T | + CQ)\nInfICL. We generate embeddings for all training points and generating embedding for each training point requires a single local LLM access call. Thus, the total cost of local LLM access calls for embedding generation is C P |T |. For the test case inference, we require a single external LLM access call and the cost is C Q. Thus, the the running cost of our demonstration selection method is given by O (C P |T | + C Q). For influence estimation, we use a fully connected neural network as the backbone architecture for the classifier F. Let d be the number of parameters in F. Calculating the loss of |T | training samples takes O (d |T |). In the implementation, we use LiSSA Agarwal et al.\n[2017] method to approximate the inverse Hessian-Vector product (iHVP) of \ufffd 1\n|V| \ufffd\nz j \u2208V \u2207 \u03b8 L f \ufffd z j, \ufffd \u03b8 \ufffd \ufffd \u22a4 H \u2212 1 \ufffd \u03b8,\nwhich costs O (|V| d + rjd) where r is the recursion depth and j is the number of repeats. As both validation set and \u03b8 are fixed, there is only one computation of iHVP. The sorting time needed for ranking potential demonstrations by influence analysis is O (|T | log(|T |)) on average. Consequently, the influence estimation process takes O (d |T | + |V| d + rjd + |T | log(|T |)). In a practical setting, |V| is sufficiently small compared to |T | (|V| \u226a|T |) and rj \u2248|T |. Therefore, the running time for calculating influence scores is O (d |T | + |T | log(|T |)).\nInfluence [Nguyen and Wong, 2023]. Initially, M random demonstrations are constructed from T  . For each constructed demonstration set S i where | S i | = K, its ICL generalization performance on the entire validation set V is calculated by using the external LLM. Then, the influence of each training point z j \u2208 S i is calculated as the difference between the average performance of demonstration sets including z j and the average performance of demonstration sets omitting z j. Through this design analysis, we can infer the running cost of Influence as O (C Q |V| M).\nCuration [Chang and Jia, 2023]. There are two variants: CondAcc and Data Models. Specifically, the CondAcc variant is almost similar to Influence. However, for each constructed random demonstration set, the ICL generalization performance of its each permutation on V is separately evaluated. Thereby, the running cost of CondAcc is given by O (C Q |V| MK!). In the Data Models variant, a surrogate linear model is trained to mimic the prediction performance of the external LLM. Similar to Influence, M random demonstration sets are constructed. Each random demonstration set is used to train a separate linear model. For a given random demonstration set, the employed linear model training loss calculates the difference between generalization performances of linear model and external LLM (based on ICL) on the validation set. After this training, the influence of each training point belonging to a random demonstration set is calculated by analyzing the linear model parameters. Through this design analysis, we can infer the running cost of Data Models as O (C Q |V| M).\nThe running costs of both Influence and Curation are dominated by the term C Q |V| M. Here, M which denotes the number of constructed random demonstration sets, needs to be large in-order to effectively cover the entire training set, and to obtain good estimates of influence scores [Nguyen and Wong, 2023]. As a consequence, both Influence and Curation incur an extremely large amount of external LLM access calls. For InfICL, we approximately require |T | local LLM access calls, which makes InfICL much more cost-effective than both Influence and Curation.\n\n# 3.3 Design Intuitions\n\nIn this section, we describe our intuitions behind the design of our InfICL. Specifically, we describe about the plausibility that the influential training points identified for the classifier F can also become influential for both local LLM P and external LLM Q. For our analysis, to differentiate influence functions for classifier F and local LLM P, we denote I up,params (z i, \u03b8) and I up,params (s i, \u03d5) as the up-weighted influence functions for F and P, respectively. Here, the up-weighted influence for local LLM P w.r.t next token prediction loss L nt is given by:\n\n\ufffd\n\ufffd\ufffd\ufffd \ufffd \ufffd\nConsider the scenario when the embedding space is clustered and training points in the same cluster share the same label. This scenario is not unrealistic because P tends to generate closer embeddings for those training inputs which are similar to each other and share the same label. Consider two training points z i = (x i, y i) and z j = (x j, y j) belonging to dense and sparse clusters, respectively. Influence functions typically assign higher influence scores to\n\n<div style=\"text-align: center;\">Table 2: Dataset Details.\n</div>\nDataset\nSize\nPositive Class\n|T |\n|V|\nTest Set Size\nCoLA\n9594\n70%\n8466\n85\n1043\nRTE\n2717\n50%\n2466\n24\n277\nSST2\n20872\n50%\n19800\n200\n872\ntraining points from sparse clusters compared to those from dense clusters. This is because, in dense clusters, the removal of a single training point is compensated for by the many similar points within the cluster that can effectively fill its absence. Hence, for the classifier F, we can hypothesize that I up,params (z i, \u03b8) \u2264I up,params (z j, \u03b8).\nSince the embedding space is generated by the local LLM P, we can apply the same argument used for F, and can further hypothesize that for P we have that I up,params (s i, \u03d5) \u2264I up,params (s j, \u03d5). Therefore, the influential training points for F can also become influential for P.\nMost LLMs are pre-trained using the next token prediction strategy and memorize their underlying training data. Consequently, the external LLM Q tends to generate a dense cluster containing s i and numerous other similar training inputs in its own embedding space. As a result, s i tends to have lower influence than s j for Q. Thus, it is plausible that the influential training points for the local LLM P can also be influential for the external LLM Q. This hypothesis was also empirically validated in [Grosse et al., 2023].\n\n# 4 Experiments\n\n# 4.1 Experimental Setup\n\nDatasets. We use three real-world datasets for our empirical evaluation study, Corpus of Linguistic Acceptability (CoLA) [Warstadt et al., 2018], Recognizing Textual Entailment (RTE) [Dagan et al., 2005], and Stanford Sentiment Tree-bank version2 (SST2) [Socher et al., 2013]. The CoLA dataset contains sentences from different linguistics publications, which are expertly annotated for grammatical acceptability by their original authors. Each sentence is either labeled as acceptable or unacceptable. The RTE dataset sample contains two text fragments denoted as premise and hypothesis, and the corresponding label indicates whether the meaning of the hypothesis can be inferred from the text (yes or no). The SST2 is a sentiment analysis dataset wherein, each sentence is labeled as either positive or negative. Table 2 shows dataset details including training, validation, and test splits.\nBaselines. We employ two different groups of baselines called the non-influence analysis based baselines which select demonstrations without analyzing influences of training points and influence analysis based baselines which employ influence score calculation to select demonstrations. We select three non-influence analysis based baselines: Zero-shot which directly performs test case inference without any demonstrations, Random where demonstrations are selected based on random sampling, and RICES [Yang et al., 2022] where the training points are scored based on their cosine similarity to the test sample in the embedding space and then the topR training points from each class are selected as demonstrations. We select three influence analysis based baselines: Influence [Nguyen and Wong, 2023], CondACC and Data Models [Chang and Jia, 2023]. We have described these three baselines in Section 3.2. We also compare InfICL against another simple baseline called Classifier where we directly employ a three layer neural network for test case inference.\nTraining Details. We employ Llama-2-7B [Touvron et al., 2023] as the local LLM. For the external LLM, we separately evaluate on OPT-6.7B, Llama-2-7B, Llama-2-13B, and Llama-2-70B. All Llama-family models are chat versions. The embedding size is 4096. For the classifier, we employ a fully connected neural network with three layers. All experiments are executed on V100-32GB GPU with Intel Xeon 6258R for small models and A100-40GB with AMD EPYC 7543 for large models. We train the classifier using Adam optimizer in 20 epochs with learning rates of 0.001 for CoLA and 0.01 for RTE and SST2.\n\n# 4.2 Experimental Results\n\nComparison to non-influence analysis based baselines. We show performances of our InfICL and non-influence analysis based baselines on external LLMs in Table 3. Clearly, our InfICL shows an overall better performance than Zero-shot, Random, and RICES across all three datasets and four external LLMs. Zero-shot does not involve any demonstrations. Therefore, the external LLM does not get any opportunity to better understand the given task and as a result, Zero-shot performance is not noticeable. Random under-performs compared to InfICL, indicating that randomly selecting demonstrations does not offer a high-quality learning opportunity to the LLM. Although RICES\n\n<div style=\"text-align: center;\">rmances of our InfICL and non-influence analysis based baselines (mean\u00b1std) for external LLMs. Scores fter 5 runs. For each external LLM, the best values for each shot are bold highlighted. \u2018N/A\u2019 denotes e and \u2018\u2013\u2019 denotes non-feasible results due to the limitation of LLM\u2019s context length.\n</div>\nExternal LLM (Q)\nShots (K)\nMethod\nCoLA\nRTE\nSST2\nAccuracy (%)\u2191\nF1 (%)\u2191\nAccuracy (%)\u2191\nF1 (%)\u2191\nAccuracy (%)\u2191\nF1 (%)\u2191\nN/A\nN/A\nClassifier\n82.83 \u00b10.00\n88.18 \u00b10.00\n57.76 \u00b10.00\n58.95 \u00b10.00\n94.50 \u00b10.00\n94.48 \u00b10.00\nLlama-2-7B\n0\nZero-shot\n63.39 \u00b10.00\n68.81 \u00b10.00\n69.19 \u00b10.00\n68.83 \u00b10.00\n88.76 \u00b10.00\n88.11 \u00b10.00\n8\nRandom\n70.35 \u00b13.68\n75.70 \u00b14.53\n74.97 \u00b10.21\n77.31 \u00b11.19\n93.58 \u00b11.82\n93.88 \u00b11.54\nRICES\n70.74 \u00b10.41\n78.50 \u00b10.28\n77.38 \u00b11.16\n80.34 \u00b10.61\n93.88 \u00b10.07\n94.12 \u00b10.06\nInfICL\n74.19 \u00b12.39\n81.10 \u00b12.49\n77.26 \u00b11.25\n80.16 \u00b11.36\n94.92 \u00b10.70\n95.04 \u00b10.66\n16\nRandom\n70.20 \u00b12.30\n75.54 \u00b12.6\n77.02 \u00b10.83\n79.24 \u00b11.20\n93.16 \u00b12.84\n93.58 \u00b12.39\nRICES\n73.71 \u00b10.52\n80.97 \u00b10.42\n76.77 \u00b11.37\n80.44 \u00b11.29\n93.88 \u00b10.96\n94.10 \u00b10.92\nInfICL\n74.75 \u00b11.32\n81.39 \u00b10.92\n78.58 \u00b10.55\n80.98 \u00b10.41\n95.26 \u00b10.07\n95.39 \u00b10.13\n32\nRandom\n73.00 \u00b11.68\n78.74 \u00b12.00\n77.38 \u00b11.10\n79.87 \u00b11.02\n91.78 \u00b14.22\n92.43 \u00b13.43\nRICES\n74.02 \u00b10.51\n80.96 \u00b10.86\n73.89 \u00b10.55\n75.86 \u00b10.48\n91.82 \u00b10.18\n92.02 \u00b10.12\nInfICL\n73.48 \u00b10.74\n79.50 \u00b11.19\n77.74 \u00b10.55\n79.92 \u00b11.14\n95.15 \u00b10.13\n95.30 \u00b10.09\nLlama-2-13B\n0\nZero-shot\n50.07 \u00b10.00\n45.29 \u00b10.00\n77.25 \u00b10.00\n78.82 \u00b10.00\n84.40 \u00b10.00\n86.07 \u00b10.00\n8\nRandom\n73.17 \u00b13.76\n78.53 \u00b15.15\n80.39 \u00b10.21\n82.51 \u00b10.80\n95.49 \u00b10.13\n95.61 \u00b10.11\nRICES\n73.42 \u00b10.92\n81.37 \u00b10.75\n77.86 \u00b11.16\n81.89 \u00b10.84\n94.30 \u00b10.57\n94.59 \u00b10.49\nInfICL\n76.66 \u00b11.71\n82.31 \u00b11.47\n82.43 \u00b12.21\n84.25 \u00b11.74\n95.64 \u00b10.80\n95.67 \u00b10.85\n16\nRandom\n75.40 \u00b11.48\n81.48 \u00b11.98\n82.31 \u00b11.57\n84.08 \u00b11.29\n95.60 \u00b10.40\n95.70 \u00b10.36\nRICES\n73.94 \u00b10.88\n82.11 \u00b10.48\n79.66 \u00b11.37\n82.80 \u00b11.27\n93.04 \u00b11.47\n93.50 \u00b11.26\nInfICL\n77.47 \u00b10.32\n84.58 \u00b10.47\n83.63 \u00b10.21\n85.08 \u00b10.39\n95.87 \u00b10.11\n95.94 \u00b10.16\n32\nRandom\n75.95 \u00b11.74\n83.06 \u00b11.27\n81.76 \u00b11.26\n82.49 \u00b11.54\n94.72 \u00b10.60\n94.96 \u00b10.51\nRICES\n73.23 \u00b10.70\n82.12 \u00b10.69\n77.08 \u00b10.91\n77.70 \u00b10.99\n92.51 \u00b11.92\n93.05 \u00b11.65\nInfICL\n76.05 \u00b10.81\n84.20 \u00b10.41\n82.67 \u00b11.08\n83.67 \u00b11.14\n95.95 \u00b10.13\n96.04 \u00b10.15\nOPT-6.7B\n0\nZero-shot\n66.92 \u00b10.00\n80.07 \u00b10.00\n54.15 \u00b10.00\n60.44 \u00b10.00\n54.82 \u00b10.00\n54.29 \u00b10.00\n8\nRandom\n63.37 \u00b10.17\n75.43 \u00b12.64\n56.92 \u00b12.73\n67.98 \u00b12.81\n60.78 \u00b10.30\n71.74 \u00b10.24\nRICES\n64.30 \u00b10.11\n76.85 \u00b10.20\n55.60 \u00b11.30\n69.02 \u00b10.06\n69.72 \u00b10.70\n58.66 \u00b11.30\nInfICL\n63.50 \u00b10.78\n76.76 \u00b10.33\n57.76 \u00b10.63\n70.43 \u00b10.80\n91.40 \u00b11.39\n91.95 \u00b11.12\n16\nRandom\n62.03 \u00b10.50\n77.07 \u00b12.87\n54.51 \u00b10.63\n63.84 \u00b10.86\n59.44 \u00b12.02\n71.31 \u00b10.91\nRICES\n63.69 \u00b10.06\n76.03 \u00b10.01\n52.11 \u00b11.10\n66.31 \u00b10.68\n75.84 \u00b10.13\n70.08 \u00b10.18\nInfICL\n63.79 \u00b10.55\n76.48 \u00b10.70\n57.28 \u00b10.91\n70.14 \u00b10.50\n90.71 \u00b11.58\n91.34 \u00b11.21\n32\nRandom\n59.66 \u00b10.70\n72.24 \u00b11.62\n\u2013\n\u2013\n61.28 \u00b10.52\n72.09 \u00b10.36\nRICES\n61.39 \u00b10.22\n74.38 \u00b10.32\n\u2013\n\u2013\n79.05 \u00b10.33\n75.38 \u00b10.45\nInfICL\n61.77 \u00b10.77\n73.48 \u00b11.60\n\u2013\n\u2013\n93.58 \u00b10.40\n93.69 \u00b10.37\nLlama-2-70B\n0\nZero-shot\n74.02 \u00b10.00\n78.61 \u00b10.00\n80.14 \u00b10.00\n79.25 \u00b10.00\n93.12 \u00b10.00\n93.45 \u00b10.00\n8\nRandom\n74.78 \u00b14.51\n78.79 \u00b15.00\n86.28 \u00b10.36\n87.53 \u00b10.35\n89.18 \u00b14.60\n90.35 \u00b13.76\nRICES\n78.91 \u00b10.47\n85.29 \u00b10.40\n84.72 \u00b10.21\n86.45 \u00b10.24\n91.40 \u00b10.11\n91.14 \u00b10.13\nInfICL\n79.71 \u00b13.02\n84.84 \u00b13.31\n87.61 \u00b10.91\n88.46 \u00b10.87\n94.80 \u00b10.75\n95.02 \u00b10.65\n16\nRandom\n77.28 \u00b11.42\n81.73 \u00b11.49\n86.04 \u00b11.10\n87.64 \u00b10.64\n90.79 \u00b13.85\n91.60 \u00b13.15\nRICES\n77.82 \u00b10.31\n84.62 \u00b10.33\n83.39 \u00b10.63\n85.72 \u00b10.46\n91.36 \u00b10.07\n91.11 \u00b10.10\nInfICL\n80.92 \u00b11.60\n86.32 \u00b11.10\n87.97 \u00b10.21\n89.03 \u00b10.22\n94.61 \u00b11.09\n94.76 \u00b10.96\n32\nRandom\n78.65 \u00b10.87\n83.56 \u00b11.56\n87.00 \u00b10.36\n88.56 \u00b10.41\n92.32 \u00b12.60\n92.85 \u00b12.22\nRICES\n76.93 \u00b10.24\n84.24 \u00b10.17\n80.14 \u00b10.21\n82.54 \u00b10.13\n91.44 \u00b10.07\n91.53 \u00b10.05\nInfICL\n78.94 \u00b11.30\n85.36 \u00b10.93\n88.09 \u00b10.36\n89.11 \u00b10.27\n95.53 \u00b10.34\n95.67 \u00b10.32\noffers personalized demonstrations, it fails to select highly influential demonstrations. This selection is crucial for enhancing the ICL performance. Hence, RICES also under-performs relative to InfICL.\nFor Llama-2-7B and SST2 dataset, our InfICL shows superior performance against baselines. However, RICES outperforms InfICL with 8 and 32 shots for RTE and CoLA datasets, respectively. This is because, in a few cases, choosing personalized demonstrations that are similar to the test sample can enhance performance compared to influence analysis. For Llama-2-13B and across all three datasets, our InfICL clearly outperforms baselines. Counter-intuitively, InfICL performs better with 16 shots compared to 32 shots. This outlier phenomenon can sometimes occur due to the information interference effect between demonstrations [Chen et al., 2023a]. For OPT-6.7B and both RTE and SST2 datasets, InfICL maintains its superior performance over baselines. However, for CoLA dataset, Zero-shot outperforms other methods. OPT-6.7B is a small sized LLM compared to other external LLMs. Consequently, in some datasets like CoLA, it does not effectively utilize demonstrations. For the Llama-2-70B and across all datasets, our InfICL outperforms baselines.\nWe further perform student\u2019s t-test between InfICL and non-influence analysis based baselines on all three datasets and four external LLMs. We perform this analysis on accuracy scores and the results are shown in Table 4. Out of 24 t-test cases, the p-values show statistical significance in 20 cases (based on the threshold of 0.05), which demonstrating the superiority of our InflCL.\nCorrelation between influence scores and InfICL performance. We conduct an empirical study to analyze the correlation between influence scores and InfICL performance. As previously mentioned in Section 3.1, training points\n\nTable 4: Student\u2019s t-test analysis results between our InfICL and non-influence analysis based baselin is calculated by using the accuracy scores for all shots and runs. Statistically significant p-values are b (p-value <0. 05).\n\nTable 4: Student\u2019s t-test analysis results between our InfICL and is calculated by using the accuracy scores for all shots and runs. S (p-value <0. 05).\n\n4: Student\u2019s t-test analysis results between our InfICL and non-influence analysis based baselines. The p-value ulated by using the accuracy scores for all shots and runs. Statistically significant p-values are bold highlighted\n\nExternal LLM\nDataset\nMethod 1\nMethod 2\np-value\nLlama-2-7B\nCoLA\nInfICL\nRandom\n0.0449\nRICES\n0.7363\nRTE\nInfICL\nRandom\n0.0207\nRICES\n0.0286\nSST2\nInfICL\nRandom\n0.0296\nRICES\n0.0002\nLlama-2-13B\nCoLA\nInfICL\nRandom\n0.0229\nRICES\n0.0007\nRTE\nInfICL\nRandom\n0.0384\nRICES\n0.0005\nSST2\nInfICL\nRandom\n0.0324\nRICES\n0.0001\nOPT-6.7B\nCoLA\nInfICL\nRandom\n0.0686\nRICES\n0.8550\nRTE\nInfICL\nRandom\n0.1190\nRICES\n0.0075\nSST2\nInfICL\nRandom\n0.0001\nRICES\n0.0001\nLlama-2-70B\nCoLA\nInfICL\nRandom\n0.0247\nRICES\n0.0125\nRTE\nInfICL\nRandom\n0.0002\nRICES\n0.0001\nSST2\nInfICL\nRandom\n0.0030\nRICES\n0.0001\n<div style=\"text-align: center;\">ble 5: Effect of choosing training points from different range of influence scores on the InfICL performance. Score reported after 5 runs. External model: Llama-2-7B. Dataset: CoLA.\n</div>\nTable 5: Effect of choosing training points from different range of influence scores on the InfICL p are reported after 5 runs. External model: Llama-2-7B. Dataset: CoLA.\n\nShots\nInfl. Scores\nAccuracy (%)\u2191\nF1 (%)\u2191\n8\nTop Positive\n74.19 \u00b12.39\n81.10 \u00b12.49\nMiddle\n72.94 \u00b11.88\n80.22 \u00b12.56\nTop Negative\n71.54 \u00b11.43\n81.67 \u00b10.67\n16\nTop positive\n74.75 \u00b11.32\n81.39 \u00b10.92\nMiddle\n73.46 \u00b12.12\n81.01 \u00b12.46\nTop negative\n69.78 \u00b13.54\n78.58 \u00b13.69\n32\nTop positive\n73.02 \u00b11.73\n82.05 \u00b11.16\nMiddle\n71.81 \u00b14.13\n79.09 \u00b14.63\nTop negative\n64.29 \u00b13.40\n71.24 \u00b14.45\nexhibiting higher positive influence scores have the potential to enhance the InfICL predictive performance. In this empirical study, we assess how selecting demonstrations from varying influence ranges impacts the InfICL performance. We initially rank training points based on their influence scores in descending order, then form different demonstration sets using three strategies: selecting training points with the highest positive influence, those within the mid-range of influence, and those with the highest negative influence. We report the InfICL performance for different ranges of influence scores in Table 5. Notably, opting for training points with the highest positive influence scores as demonstrations yields the most favorable performance.\nComparison to influence analysis based baselines. Since the employed influence analysis based baselines Influence, CondACC, and Data Models have an extremely high running costs, they can only run on a small size training and validation sets. To conduct a fair comparison, we run our InfICL and baselines in the same dataset setting as mentioned in Nguyen and Wong [2023], which has train/validation/test size as 400/200/500, respectively. In-order to reduce the high cost of experimentation, we conduct our empirical study using two external LLMs OPT-6.7B and Llama-2-7B and on two datasets CoLA and RTE.\nWe show the empirical results comparing our InfICL with other influence analysis based baselines in Table 6. For the CoLA dataset and for both Llama-2-7B and OPT-6.7B, InfICL shows an overall better performance than other baselines. For the RTE dataset and Llama-2-7B, InfICL again outperforms Influence. However, for OPT-6.7B, InfICL has a lower accuracy than Influence for 12 shots. This indicates that the chosen 12 demonstrations based on InfICL do not convey sufficient information that can be exploited by OPT-6.7B. For the setting of Llama-2-7B and 4 shots on CoLA dataset, our InfICL incurs 10 minutes of execution latency, Influence takes 3.5 hours, and both CondAcc and Data Models take more than 80 hours.\n\nTable 6: Test accuracy of our InfICL and influence analysis based baselines on different external denotes the results extracted from Nguyen and Wong [2023]. Cells marked \u2019\u2013\u2019 denotes non-feasib extremely high training latency.\n\n6: Test accuracy of our InfICL and influence analysis based baselines on different external LLMs. Asterik s the results extracted from Nguyen and Wong [2023]. Cells marked \u2019\u2013\u2019 denotes non-feasible results due to\n\nDataset\nExt. LLM\nShots\nInfICL\nInfluence\nCondAcc\nData Models\nCoLA\nOPT-6.7B\n4\n68.20\n31.80\n48.40\n45.50\n8\n64.80\n46.00\n35.60\n36.50\n16\n69.00\n46.80\n\u2013\n\u2013\n32\n69.20\n58.60\n\u2013\n\u2013\nLlama-2-7B\n4\n77.80\n74.40\n72.90\n71.2\n8\n77.80\n78.20\n\u2013\n\u2013\n16\n77.20\n73.20\n\u2013\n\u2013\n32\n76.80\n74.40\n\u2013\n\u2013\nRTE\nOPT-6.7B\n12\n51.20\n62.70\u2217\n\u2013\n\u2013\nLlama-2-7B\n12\n77.60\n75.60\n\u2013\n\u2013\nIn this work, we introduced a demonstration selection method for ICL by analyzing influences of training samples using influence functions. Our approach utilizes a local LLM to generate sample embeddings thereby, avoiding the expensive fine-tuning of the LLM. Empirical studies on various real-world datasets demonstrated advantages of our method over state-of-the-art baselines. For future work, we aim to expand our demonstration selection method to Large Vision-language Models (LVMs), and extend our method to address more complex problems such as massive multitask language understanding. We release our source code at https://tinyurl.com/edry6nn4.\n\n# 6 Limitations\n\nAlthough we have demonstrated that influence function analysis can be effective for selecting ICL demonstrations, we have not conducted an in-depth interpretability study on why influence functions improve ICL performance. We based our use of influence functions on the intuition that highly influential training samples benefit model learning. However, since ICL does not involve any model gradient updates and differs significantly from gradient update-based learning, a theoretical study is needed to connect mechanisms of ICL with gradient update-based models [Xie et al., 2022], and show that highly influential training samples can also enhance ICL performance.\n\n# Acknowledgement\n\nThis work was supported in part by NSF grants 1920920 and 1946391.\n\n# References\n\nNaman Agarwal, Brian Bullins, and Elad Hazan. 2017. Second-order stochastic optimization for machine learning in linear time. The Journal of Machine Learning Research, 18(1):4148\u20134187.\nSamyadeep Basu, Phil Pope, and Soheil Feizi. 2021. Influence functions in deep learning are fragile. In International Conference on Learning Representations.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems.\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the origins of bias in word embeddings. In Proceedings of the 36th International Conference on Machine Learning.\nTing-Yun Chang and Robin Jia. 2023. Data curation alone can stabilize in-context learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.\nJiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023a. How many demonstrations do you need for in-context learning? In Findings of the Association for Computational Linguistics: EMNLP.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML.\n\nYanda Chen, Chen Zhao, Zhou Yu, Kathleen R. McKeown, and He He. 2023b. On the relation between sensitivity and accuracy in in-context learning. In Findings of the Association for Computational Linguistics: EMNLP.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234.\nMinghong Fang, Neil Zhenqiang Gong, and Jia Liu. 2020. Influence function based data poisoning attacks to top-n recommender systems. In Proceedings of The Web Conference.\nVitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long tail via influence estimation. In Annual Conference on Neural Information Processing Systems.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.\nRoger B. Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukosiute, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. 2023. Studying large language model generalization with influence functions. CoRR, abs/2308.03296.\nXiaochuang Han and Yulia Tsvetkov. 2021. Influence tuning: Demoting spurious correlations via instance attribution and instance-driven updates. In Findings of the Association for Computational Linguistics: EMNLP.\nXiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov. 2020. Explaining black box predictions and unveiling data artifacts through influence functions. ArXiv.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 2022. Datamodels: Understanding predictions with data and data with predictions. In International Conference on Machine Learning, ICML.\nMatthew Jagielski, Giorgio Severi, Niklas Pousette Harger, and Alina Oprea. 2021. Subpopulation data poisoning attacks. In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\nPang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning.\nShuming Kong, Yanyan Shen, and Linpeng Huang. 2022. Resolving training biases via influence-based data relabeling. In International Conference on Learning Representations.\nDonghoon Lee, Hyunsin Park, Trung Pham, and Chang D. Yoo. 2020. Learning augmentation network via influence functions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for in-context learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.\nXiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586.\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Seyed Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y. Zhao. 2023. Dr.icl: Demonstration-retrieved in-context learning. CoRR, abs/2305.14128.\nTai Nguyen and Eric Wong. 2023. In-context example selection with influences. CoRR, abs/2302.11042. Sejoon Oh, Sungchul Kim, Ryan A. Rossi, and Srijan Kumar. 2021. Influence-guided data augmentation for neural tensor completion. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management.\n\nChengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. 2023. In-context learning with iterative demonstration selection. CoRR, abs/2310.09881.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL.\nAlexander Scarlatos and Andrew S. Lan. 2023. Reticl: Sequential retrieval of in-context examples with reinforcement learning. CoRR, abs/2305.14502.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00b4elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.\nXinyi Wang, Wanrong Zhu, and William Yang Wang. 2023. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv:2301.11916.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An empirical study of GPT-3 for few-shot knowledge-based VQA. In Thirty-Sixth Conference on Artificial Intelligence, AAAI.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of selecting effective demonstrations for In-Context Learning (ICL) in Large Language Models (LLMs), highlighting the sensitivity of ICL performance to demonstration selection and the limitations of existing methods.",
        "problem": {
            "definition": "The problem involves identifying the most effective demonstrations for ICL, which is critical for enhancing the generalization performance of LLMs during few-shot learning.",
            "key obstacle": "The main challenge is the lack of a consensus on the most effective demonstration selection approach, compounded by the high resource costs associated with existing methods."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that certain training samples have a greater influence on model learning, and identifying these samples can improve ICL performance.",
            "opinion": "The proposed method, InfICL, utilizes influence functions to analyze the impact of training samples and selects the most influential ones as demonstrations to enhance ICL.",
            "innovation": "InfICL differentiates itself by employing a cost-effective approach that only requires generating sample embeddings through an LLM, avoiding the need for expensive fine-tuning."
        },
        "method": {
            "method name": "InfICL",
            "method abbreviation": "InfICL",
            "method definition": "InfICL is a demonstration selection method that leverages influence function analysis to identify and select the most influential training samples for ICL.",
            "method description": "InfICL generates embeddings for training samples using a local LLM and selects demonstrations based on their influence scores.",
            "method steps": [
                "Generate embeddings for all training and validation inputs using a local LLM.",
                "Train a classifier on the generated embeddings.",
                "Calculate influence scores for each training point.",
                "Select the top R training points from each class based on influence scores."
            ],
            "principle": "The effectiveness of InfICL lies in its ability to identify training samples that significantly contribute to model learning, which can enhance the generalization performance in ICL tasks."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three real-world datasets: CoLA, RTE, and SST2, comparing InfICL against various baseline methods.",
            "evaluation method": "The performance of InfICL was assessed by measuring accuracy and F1 scores across different external LLMs and shot configurations."
        },
        "conclusion": "The empirical results demonstrate that InfICL outperforms state-of-the-art demonstration selection methods in terms of accuracy and efficiency, providing a significant contribution to the field of ICL.",
        "discussion": {
            "advantage": "InfICL is more cost-effective than existing methods, requiring fewer LLM access calls while achieving superior performance in demonstration selection.",
            "limitation": "The method has not been thoroughly interpreted in terms of why influence functions improve ICL performance, necessitating further theoretical exploration.",
            "future work": "Future research will focus on extending InfICL to more complex tasks, including its application to Large Vision-language Models (LVMs)."
        },
        "other info": {
            "source code": "The source code is available at https://tinyurl.com/edry6nn4.",
            "acknowledgement": "This work was supported in part by NSF grants 1920920 and 1946391."
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The paper proposes a method called InfICL, which utilizes influence functions to analyze the impact of training samples and selects the most influential ones as demonstrations to enhance in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "InfICL identifies training samples that significantly contribute to model learning, enhancing the generalization performance in in-context learning tasks."
        },
        {
            "section number": "4.1",
            "key information": "The design of demonstrations in InfICL can significantly influence the outcomes of in-context learning by selecting the most effective training samples."
        },
        {
            "section number": "6.2",
            "key information": "InfICL is more cost-effective than existing methods, requiring fewer LLM access calls while achieving superior performance in demonstration selection."
        },
        {
            "section number": "6.4",
            "key information": "The main challenge in demonstration selection involves the lack of a consensus on the most effective approach, compounded by high resource costs associated with existing methods."
        },
        {
            "section number": "7",
            "key information": "The empirical results demonstrate that InfICL outperforms state-of-the-art demonstration selection methods in terms of accuracy and efficiency, providing a significant contribution to the field of in-context learning."
        }
    ],
    "similarity_score": 0.7226028168016005,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a54/7a54a1e8-eda0-496e-9d97-459dd326bc2b.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/I N-C ONTEXT L EARNING D EMONSTR.json"
}