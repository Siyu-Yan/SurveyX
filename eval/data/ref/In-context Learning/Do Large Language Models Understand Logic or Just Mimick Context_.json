{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.12091",
    "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
    "abstract": "Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they \u201cguess\u201d the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses. This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs.",
    "bib_name": "yan2024largelanguagemodelsunderstand",
    "md_text": "unbing Yan1, Chengyu Wang2, Jun Huang2, Wei Zhan 1 Alibaba Group, Hangzhou, China 2 East China Normal University, Shanghai, China {junbingyan531,zhangwei.thu2011}@gmail.com {chengyu.wcy,huangjun.hj}@alibaba-inc.com,\n1 Alibaba Group, Hangzhou, China 2 East China Normal University, Shanghai, China {junbingyan531,zhangwei.thu2011}@gmail.com {chengyu.wcy,huangjun.hj}@alibaba-inc.com,\n# Abstract\nOver the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they \u201cguess\u201d the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses. This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs.\narXiv:2402.12091v1\n# 1 Introduction\nLogical reasoning is a core component of human cognition that is essential for comprehending, interacting with, and influencing our environment. In contrast to artificial intelligence systems that typically depend on vast datasets and substantial training to build skills, humans excel at employing logical reasoning to deduce, troubleshoot, and assimilate new knowledge from limited data or abstract principles. Moreover, humans demonstrate an exceptional capacity to derive novel insights \u2217Correspondence to Wei Zhang.\n\u2217Correspondence to Wei Zhang.\nfrom a minimal number of instances or from theoretical frameworks, a capability that stands in sharp contrast to the extensive, supervised datasets necessitated by deep learning algorithms. Over the past two years, advancements in large language models (LLMs) have led to extraordinary achievements (Brown et al., 2020a; Ouyang et al., 2022a; Bommasani et al., 2021; Lu et al., 2021). These models have not only excelled in open-ended tasks such as generating creative dialogues, but have also performed exceptionally well in complex problems that necessitate logical reasoning, common sense, and mathematical skills (Ouyang et al., 2022a; Wei et al., 2022a; Wang et al., 2022), thanks in part to innovations such as in-context learning (Brown et al., 2020a; Min et al., 2022; Mishra et al., 2022a; Chen et al., 2022; Mishra et al., 2022b) and Chainof-Thought (COT) prompting (Wei et al., 2022b). In the literature, COT (Wei et al., 2022b) is designed to improve the performance in mathematical problem solving by using intermediate steps as prompts, thereby incrementally guiding LLMs through the necessary reasoning process. LogicalCOT (Liu et al., 2023) extends this strategy of intermediate prompting to logical reasoning tasks. While these prompting-based methods have enhanced the performance of LLMs on tasks that require logical reasoning, there is still a gap in our understanding of whether these models have genuinely grasped the underlying logical rules, or whether they simply become more effective at converging to the correct answers. Therefore, the question remains: do the observed proficiencies of LLMs stem from true understanding, or do they merely remember the results based on large-scale parameters, extensive pre-training on large corpora, and a plethora of contextual examples that allow for a broader retention of knowledge? To delve into the topic, we establish a comprehensive evaluation framework based on in-context learning. We first define the texts, the\nlogical reasoning chain, and reasoning keywords in in-context examples. We test whether larger models exhibit different behaviors on texts that have undergone modifications or deletions of these components. Furthermore, we add concepts related to logical definitions and test whether the models understand the relationships between these logical terms by replacing the logical concepts. Through extensive analysis, the main important findings are summarized as follows:\n\u2022 The Chain of Thought (COT) in-context examples markedly improve the performance of large-scale models on logical reasoning tasks. Across a range of models with 7 to 200 billion parameters, these examples significantly enhance the clarity, normativity, and accuracy of the generated responses.\n Large models demonstrate resilience to distracting elements within in-context examples, such as extraneous text, reasoning chains, and patterns. When various segments of the in-context example content are replaced with text from within or outside the domain, large models (70B and 200B parameters) maintain their output accuracy. In contrast, smaller models (7B and 13B parameters) suffer notable declines in performance when standard in-context examples are not used.\n Large models do not genuinely comprehend logical principles; rather, they rely on probabilistic associations between input examples and outputs. Efforts to alter the definitions of logical symbols and direct the models to revise their outputs accordingly were met with a minimal rate of successful adaptation across all model sizes. Attempts to enhance the rate of successful adjustments using either prompt or in-context guidance yielded limited improvement.\n# 2 Related Work\n# 2.1 Large Language Models\nPrior to the emergence of the Large Language Model (LLM) trend, Pre-trained Language Models (PLMs) were already in the spotlight for their proficiency in acquiring contextual representations (Qiu et al., 2020; Min et al., 2021). With the escalating size of PLM parameters, there has been a notable enhancement in their performance across a range of\nNLP tasks, with decoder-only models showing particularly impressive gains. Among these, the 175Bparameter ChatGPT stands out, exhibiting the capacity to craft responses that closely mimic human conversation, leveraging GPT-3\u2019s foundational architecture (Brown et al., 2020b). Subsequent to the introduction of ChatGPT, the designation \"Large Language Model (LLM)\" has become commonplace when describing PLMs of considerable scale and exceptional generative capabilities. Following ChatGPT\u2019s launch, the field has seen the advent of numerous LLMs. A selection of prominent open-source LLMs comprises LLaMA (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), BLOOM (Scao et al., 2022), BLOOMZ (Muennighoff et al., 2023), Galactica (Taylor et al., 2022), GLM (Zeng et al., 2023), Pythia (Biderman et al., 2023), among others. In terms of training methodology, the tripartite framework of \"pre-training, supervised fine-tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF)\" as proposed by (Ouyang et al., 2022b) has gained wide recognition and adoption within the community.\n# 2.2 Counterfactual Prompt\nA number of recent works have investigated generating counterfactual text in specific language domains (e.g., court view (Wu et al., 2020), dialogue generation (Zhu et al., 2020), Natural Language Inference (Kaushik et al., 2019; Gokhale et al., 2021), named entity recognition (Zeng et al., 2020)). Counterfactual explanations offer a pathway to gain deeper insight into the workings of models. This approach may provide more advantageous interpretations for state-of-the-art Large Language Models (LLMs).\n# 2.3 Logical Reasoning\nLogical reasoning constitutes a fundamental facet of human cognition and is an essential feature for artificial intelligence systems. To endow AI with this capability, researchers have investigated a multitude of strategies, such as rule-based and symbolic systems (MacCartney and Manning, 2007), the refinement of expansive language models (Wang et al., 2018), and the integration of neural and symbolic methodologies (Li and Srikumar, 2019). Since the introduction of Large Language Models (LLMs) and the development of chain-ofthought prompting (Wei et al., 2022b), there has been a marked enhancement in the logical reasoning capabilities of these models, as evidenced by\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/36fb/36fb2c09-9bf1-40c1-aa1b-bcbf9ac901a3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Text</div>\n<div style=\"text-align: center;\">Reasoning Chain</div>\nFigure 1: Tasks and datasets used in our experiment: Text: in blue color; Reasoning Chain: in ora Pattern: in purple color.\nimproved performance metrics across a range of logic tasks. To our knowledge, we are the first to employ counterfactual methods to examine the extent to which these expansive models comprehend logical rules and definitions.\n# 3 Method\nThis study aims to investigate which parts of the in-context examples make a major contribution to the reasoning process of Language Models and whether LLMs understand the reasoning process demonstrated within the examples. To achieve this, we have systematically divided the text within examples into three components: text, reasoning chain, and pattern. Additionally, we have included definitions of logical symbols as supplementary text. Text: A sequence of tokens that describe the question to be answered (e.g.,) and the text that contains the given information. Reasoning Chain: The thought process regarding the answer to the question, which includes the reasoning pathway pertinent to the current question. Pattern: Key symbols, answers, and other special texts within the in-context examples. Definition: Natural language text providing definitions of logical symbols. The operations on the aforementioned parts mainly involve two actions: replacement and modification. Replacement: Replacement for the Text, Reason-\n<div style=\"text-align: center;\">Pattern</div>\ning Chain and Pattern. This operation involves replacing the current content with content from another example within the same domain (in-domain) or with unrelated text (out-of-domain). Through replace operation, we can observe which parts of the data are more important for establishing the logical reasoning of the large model. Furthermore, we can explore the model\u2019s robustness to disturbances and its ability to understand patterns. Modification: To test the large model\u2019s understanding of logical rules, modifications are made to the definitions of logical concepts. For example, we modify the definitions of AND and OR. We follow the input examples with a statement that reassigns the original meaning of AND to OR, and vice versa. Given that the input examples utilize the standard interpretations of AND and OR, altering their definitions should result in an inversion of the corresponding relational statements in the output. If the model predominantly learns through probabilistic associations between tokens, the probability of correctly interchanging AND and OR in its output is expected to be low. However, if the model genuinely comprehends the logical symbols and their governing rules, it should accurately replace AND with OR, and OR with AND in the output, reflecting this new understanding.1\n1For specific examples, please refer to Table 1.\nOrigin\nAfter Operation\nText\nBased on the statements that: [A set of conditions]\nWhich of the following conclusions can be inferred?\n[A set of conditions]\nBased on the statements that: [A set of conditions\nfrom other samples] / [A paragragh from Wikipedia]\nWhich of the following conclusions can be inferred?\n[A set of conditions from other samples] / [A set of\nsentences from Wikipedia]\nChain\n[BECAUSE] [statement1] [AND] [statement2] [IN-\nFER] [Inference1]\n[BECAUSE] [Statement1 from other samples] / [A\nsentence from Wikipedia] [AND] [statement2 from\nother samples] / [A sentence from Wikipedia] [IN-\nFER] [Inference1 from other samples] / [A sentence\nfrom Wikipedia]\nPattern\n[BECAUSE] [statement1] [AND] [statement2] [IN-\nFER] [Inference1]\n[A word from BECAUSE, AND, OR, INFER] / [A\nrandom word] [statement1] [A word from BECAUSE,\nAND, OR, INFER] / [A random word] [statement2]\n[A word from BECAUSE, AND, OR, INFER] / [A\nrandom word] [Inference1]\nDefinition\nThe definition of logical AND is as follows: [The\ndefinition of AND from Wikipedia]. The definition\nof logical OR is as follows: [The definition of OR\nfrom Wikipedia]. Based on the definitions, answer the\nfollowing question.\nThe concepts of logical AND and logical OR have\nnow been swapped. The definition of logical AND is\nas follows: [The definition of OR from Wikipedia].\nThe definition of logical OR is as follows: [The defi-\nnition of AND from Wikipedia]. Based on the revised\ndefinitions, answer the following question.\nTable 1: The comparison between raw data and data after replacement or modification operation from Entailm Bank. In-domain replace are printed in blue, and out-of-domain replace are printed in red.\n# 4 Experiment\n# In this section, we conduct extensive experiments to explore LLMs\u2019 ability for logic understanding.\nIn this section, we conduct extensive experiments to explore LLMs\u2019 ability for logic understanding.\n# 4.1 Models\nIn exploring LLMs\u2019 ability to understand rules, we have employed two model series from the Open LLM Leaderboard2, each with varying scales of parameter sizes, to conduct our experiments. LLaMA2 (Touvron et al., 2023c), open-sourced and developed by Meta, represents a suite of pretrained and fine-tuned LLMs. These models vary in complexity, featuring sizes from 7B to 70B parameters. Additionally, we employed models from the Qwen series 3, which range in size from 7B to 200B parameters. These models have undergone stable pre-training on up to 3 trillion tokens of multilingual data, encompassing a broad spectrum of domains and languages with an emphasis on Chinese and English. Among these, the 200Bparameter model is essentially the largest in terms of the number of parameters available to us.4\n2https://huggingface.co/spaces/HuggingFaceH4/ open_llm_leaderboard 3Qwen models from 7B to 72B are downloaded from https://github.com/QwenLM/Qwen. The outputs of the 200B model are obtained via API calls. 4We do not train the models; instead, we test these models on their in-context learning capabilities and abilities to understand logical rules through specific inputs.\n# 4.2 Datasets\nAs our experiments require intermediate reasoning steps, we utilized the dataset released by Liu et al., 2023, known as LogicalCOT.5 The specific tasks include the following three types: Folio (Language to Logic): This process involves translating natural language into a more formal logical notation, a fundamental task that requires comprehending and interpreting logical statements articulated in natural language and transforming them into a formalized logical framework. Entailment Bank (Inference Chains): This instructional approach advances logical reasoning by requiring the model to ascertain the probability of a potential inference from a given set of premises. Subsequently, the model must delineate the sequence of logical deductions leading to the conclusion. Such an approach fosters deeper logical analysis and the capability to formulate cogent arguments. The examples provided for practice are formulated either in a symbolic language or articulated in natural language for greater accessibility and comprehension. MRC: Machine Reading Comprehension (MRC) serves as the primary task for evaluating the reasoning capabilities of LLMs, wherein a model is provided with a passage and a corresponding question\n5https://huggingface.co/datasets/csitfun/ LogiCoT\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/454a/454ad304-73c1-4cea-87e7-533c4203801f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The impact of different replacement parts on Entailment Bank for Qwen series models\u2019 performance.</div>\nand is tasked with identifying the correct answer. This domain encompasses tasks that necessitate a deep comprehension of the provided text, often requiring the model to recognize, extract, or deduce information from the text. Models may be tasked with resolving scenarios depicted in the text, identifying fallacies within an argument, or determining information that could bolster or undermine a presented argument. Data Source for Replacement: We utilize other samples as the in-domain data. For out-of-domain data, we use the English Wikipedia (2020/03/01) 6 as the out-of-domain data source. We randomly selected a paragraph from one of the 2.6 billion documents to replace the content of the text and reasoning chain.\n# 4.3 Influence of In-Context Examples\nIn Table 2, we observe a positive correlation between the number of in-context examples and the accuracy of the model\u2019s predictions. The improvement brought about by using in-context examples is quite evident, which is consistent with Mishra et al., 2022a; Chen et al., 2022; Mishra et al., 2022b. However, in our results, using 8 examples does not yield a significant enhancement over using 4 examples. Furthermore, this relationship is amplified as model size scales (from 7B to 200B parameters), suggesting that larger models benefit disproportionately from an increased number of examples. Additionally, in-context examples contribute to the standardization of the output format, thereby facilitating the generation of outputs that are consistent with the expected structure.\n6https://dumps.wikimedia.org/enwiki/\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0fc8/0fc87d57-4ef1-45de-b66f-ce2bb909409e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The impact of different replacement parts on Entailment Bank for LLaMA series models\u2019 performance.</div>\nFigure 3: The impact of different replacement parts on Entailment Bank for LLaMA series models\u2019 performance.\n# 4.4 Influence of Texts\nIn-Domain: We observe that smaller-scale models (7B/13B) exhibit a pronounced decline in accuracy when the context provided in examples is modified, as delineated in Table 2. Conversely, as we can see from Figure 2 and Figure 3, larger models (70B/200B) demonstrate resilience to such contextual manipulations, with negligible impacts on accuracy. We hypothesize that the augmented capacity of larger models equips them with enhanced resistance to perturbations of textual input, enabling them to extract and retain salient information from a prescribed format while remaining focused on the central question. In contrast, smaller models appear to be more susceptible to textual interference, predominantly assimilating linguistic details from the context, which consequently precipitates inaccuracies in addressing the question. Out-of-Domain: When utilizing out-of-domain data, the observations bear a resemblance to those gleaned from in-domain data. However, a clear disparity emerges in the robustness of smaller models compared to their larger counterparts when confronted with out-of-domain text. Smaller models exhibit a marked decrease in performance. In contrast, the performance of larger models remains largely stable, showing a negligible impact from such perturbations. Paradoxically, when examining performance on in-domain text, we find that models trained with out-of-domain data not only match but occasionally surpass the outcomes attained with in-domain data. This finding runs counter to conventional expectations. The question arises as to why models yield superior results when trained on seemingly irrelevant data and why this enhancement is more\nModels\nw/o\n4 In-context Examples\n8 In-context Examples\nRaw\nText\nChain\nPattern\nRaw\nText\nChain\nPattern\nIn\nOut\nIn\nOut\nRandom\nIn\nOut\nIn\nOut\nRandom\nEntailment Bank\nLLaMA2-7B-Chat\n46.2\n56.4\n42.2\n49.0\n45.5\n49.9\n53.8\n57.1\n41.8\n48.5\n46.7\n48.2\n53.3\nLLaMA2-13B-Chat\n72.2\n76.7\n42.4\n47.8\n44.5\n65.2\n71.9\n75.8\n45.4\n45.4\n42.9\n60.2\n73.0\nLLaMA2-70B-Chat\n74.8\n83.9\n82.3\n80.8\n83.2\n81.3\n83.8\n84.1\n83.6\n83.7\n82.5\n81.8\n84.2\nQwen-7B-Chat\n53.5\n62.7\n45.6\n52.1\n48.8\n50.8\n59.3\n64.4\n43.3\n44.1\n47.4\n49.7\n60.5\nQwen-14B-Chat\n72.1\n78.7\n50.9\n52.6\n45.1\n63.2\n73.5\n76.6\n46.1\n45.8\n48.7\n62.3\n75.4\nQwen-72B-Chat\n76.4\n85.9\n84.3\n84.8\n85.6\n85.0\n86.2\n87.7\n86.1\n86.5\n87.0\n85.4\n86.6\nQwen-200B-Chat\n80.9\n92.8\n90.2\n91.8\n92.2\n90.0\n93.4\n92.6\n90.4\n91.5\n92.3\n88.8\n93.3\nFolio\nLLaMA2-7B-Chat\n45.4\n57.9\n40.2\n41.8\n/\n/\n55.0\n60.2\n38.7\n39.9\n/\n/\n55.6\nLLaMA2-13B-Chat\n68.2\n72.4\n45.8\n45.1\n/\n/\n63.7\n72.5\n44.1\n43.4\n/\n/\n64.5\nLLaMA2-70B-Chat\n73.8\n82.6\n80.4\n80.5\n/\n/\n82.7\n83.0\n79.4\n80.9\n/\n/\n82.6\nQwen-7B-Chat\n60.2\n68.6\n46.8\n46.2\n/\n/\n68.9\n69.0\n48.9\n49.2\n/\n/\n68.6\nQwen-14B-Chat\n72.8\n84.6\n63.2\n65.8\n/\n/\n83.4\n85.1\n62.4\n63.8\n/\n/\n83.9\nQwen-72B-Chat\n84.6\n93.7\n90.2\n92.2\n/\n/\n94.6\n92.9\n90.4\n91.5\n/\n/\n91.0\nQwen-200B-Chat\n85.8\n94.2\n92.5\n94.0\n/\n/\n95.1\n93.9\n91.3\n93.8\n/\n/\n93.5\nMRC\nLLaMA2-7B-Chat\n30.8\n32.1\n27.6\n28.7\n28.1\n27.6\n/\n33.2\n27.7\n28.5\n27.6\n28.0\n/\nLLaMA2-13B-Chat\n40.2\n42.0\n36.2\n38.7\n35.1\n36.6\n/\n45.2\n38.1\n40.3\n40.4\n40.7\n/\nLLaMA2-70B-Chat\n59.2\n65.5\n62.0\n62.6\n63.1\n62.9\n/\n67.8\n64.1\n64.7\n46.7\n48.2\n/\nQwen-7B-Chat\n43.4\n56.6\n53.3\n53.7\n54.0\n54.9\n/\n60.4\n58.2\n58.0\n65.2\n65.8\n/\nQwen-14B-Chat\n60.5\n68.9\n61.3\n62.8\n63.4\n63.2\n/\n69.2\n62.4\n63.1\n64.0\n64.5\n/\nQwen-72B-Chat\n74.6\n79.5\n78.1\n78.8\n80.1\n78.2\n/\n80.0\n78.4\n79.3\n80.4\n78.5\n/\nQwen-200B-Chat\n78.9\n80.6\n80.2\n80.1\n79.3\n79.1\n/\n81.9\n80.5\n79.7\n79.0\n80.1\n/\nTable 2: Results for the LLaMA and Qwen model series on the logical datasets. (Acc. %) Here, w/o stand without in-context example, while Raw denotes results enhanced with regular in-context examples.\npronounced in smaller models. We hypothesize that the enhanced performance can be attributed to the greater divergence of out-of-domain data from the original data distribution. Such divergence may enable the model to distinguish irrelevant text with heightened clarity, thereby sharpening its focus on content pertinent to the task at hand.\n# 4.5 Influence of Reasoning Chain\nIn-Domain: Upon replacing the reasoning chains in our experiment, we observed phenomena analogous to those documented during text substitution. Notably, smaller models demonstrated a disproportionately substantial decline in accuracy, with a large reduction for 7B and 13B models as opposed to a slight decrease for 70B and 200B LLM.7 Regardless of the model size, the decrease in accuracy of reasoning outcomes, engendered by the substitution of reasoning chains, proved less pronounced than that occasioned by text replacement. This disparity can be attributed to the text\u2019s integral role in defining the problem and potential solutions, which facilitates the model\u2019s ability to forge connections\n7For details, see Table 2.\nbetween the input and the expected output, thereby mitigating the influence of alterations in the reasoning chain.\nOut-of-Domain: Upon substituting out-of-domain data for the reasoning chain, we observed an unexpected phenomenon. It can be seen from Figure 2 and Figure 3 that 7B and 13B models exhibited only a modest reduction in reasoning performance when utilizing out-of-domain data in Entailment Bank, as opposed to a more substantial decline with in-domain data. Conversely, 70B and 200B models demonstrated a more pronounced decrease in performance with out-of-domain data compared to in-domain data. This divergence in behavior between smaller and larger models warrants further investigation. We hypothesize that the stark contrast in data distributions between out-of-domain and original datasets prompts smaller models to disregard the textual content within the reasoning chains. Consequently, these models form a direct association between the input text and the corresponding output answer, largely ignoring intermediate reasoning steps. In contrast, larger models, equipped with more robust comprehension capa-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afbf/afbfb5af-7e94-45af-8537-bbacc2bfbbd0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Results of different scales of LLaMA and Qwen models over Entailment Bank when using diffe settings. Each target example has 4 in-context samples as the demonstration.</div>\nbilities, are significantly affected by the content of the reasoning chains. This heightened sensitivity to the reasoning process results in more substantial disruptions in their output when confronted with out-of-domain data.\n# 4.6 Influence of Pattern\nOur investigation extends to the model\u2019s sensitivity to substitutions of specific patterns within the text. We conducted experiments where lexical items such as [AND], [OR], and [BECAUSE] were interchanged (e.g., [AND] \u2194[OR], [OR] \u2194[BECAUSE]). Notably, substituting [AND] for [OR] resulted in the model producing outputs where the corresponding terms were interchanged. However, it can be seen from Table 2 that despite maintaining the logical relationships among these conditions, such alterations did not significantly impact the model\u2019s output accuracy. Additionally, introducing non-sequitur substitutions (e.g., [AND] \u2194[APPLE/BANANA]) did not meaningfully reduce the accuracy of the model\u2019s outputs. These findings suggest that the model primarily recognizes the necessity for a syntactic linkage between adjoining sentences, as signified by the presence of markers enclosed within brackets [], rather than comprehending the nuanced semantic\ninfluence exerted by logical connectives such as [AND] or [OR]. The implication is that the model may be relying on surface-level cues to maintain coherence rather than deeply processing the logical relationships underpinning the text\u2019s structure.\n# 4.7 Test for Logical Understanding Ability\nTo evaluate the model\u2019s grasp of logical reasoning, we implemented a methodology that introduces prompts subsequent to the examples. This approach serves to ascertain the model\u2019s comprehension of logical constructs. Modify Symbols and Logical Predicates: It has been observed that altering symbols and logical predicates within a given context does not compromise the performance of large language models in terms of generating output. However, these outputs are logically inconsistent at a relational level. For instance, conclusions predicated on the use of an [AND] logical connector do not retain their validity when the [OR] connector is substituted. Modification of Logical Predicates: Our approach utilizes the definitions of logical predicates and symbols as delineated by Wikipedia. We introduce a prompt subsequent to an in-context example.8. This is done to evaluate the model\u2019s com8For deatails, see Table 1\n8For deatails, see Table 1\nprehension of logical terminology\u2014[AND], [OR], and others. The expectation is that the model will generate text where conjunctions previously denoted by [AND] ([OR]) are now conveyed through [OR] ([AND]), with a higher rate of modification indicating a better result. Examination of the data reveals that smaller models (7B/13B) demonstrate a negligible modification rate below 1%, while the modification rate for larger models is below 5%. This suggests that, although the smaller models seem to address logic-related queries adequately, their grasp of logical semantics in particular scenarios is limited. Similarly, the performance of the larger models (70B/200B) is suboptimal. They exhibit a rudimentary understanding of these logical predicates\u2014presumably acquired during their pre-training phase\u2014but fall short of achieving satisfactory performance. It is worth noting that in such scenarios, larger models may produce outputs that reveal underlying confusions or rationales. Here is an example output by Qwen-200B-Chat: I apologize, but there seems to be a misunderstanding. The provided examples don\u2019t adhere to the new definitions of logical AND and OR. However, based on the modified meanings of logical OR (where both conditions must be true for the conclusion to hold), we can infer that ...\n# 4.8 Enhancing Logical Comprehension Ability for LLM\nThe question arises whether it is possible to augment the logical reasoning capabilities of largescale models without resorting to further training. To address this, we have explored two distinct approaches: Prompt-based Guidance: Expanding upon the modified definitions, this study incorporated a supplementary instructional prompt directing the model to interchange the logical operators [OR] with [AND], and [AND] with [OR], while ensuring grammatical correctness and logical consistency. Subsequent to the application of this prompt, a discernible enhancement in the model\u2019s performance in executing operator swaps was observed; however, the improvements did not fulfill our expectations. Example-based Guidance: The capacity for comprehension enhancement through mere promptbased instruction in models is constrained. To address this, we endeavored to enrich the instructional framework by supplementing guiding\nprompts with illustrative modifications. For example, we provided a practice scenario as follows: \"Original Statement: \u2019[BECAUSE] [Statement1] [AND] [Statement2] [INFER] [Inference1].\u2019 Your Modification: \u2019[BECAUSE] [Statement1] [OR] [Statement2] [INFER] [Inference1].\u2019 Now, it is your turn to modify.\" Subsequent to the implementation of both guiding prompts and contextualized example-based instruction, there was an observable augmentation in the modification rate by the large-scale model to over 40 50%. This increment indicates a substantial dependency of the model on contextually provided examples. The recurrence of certain logical operator predicate patterns in precedent examples suggests that mere reliance on definitions or prompts is inadequate for mitigating these patterns. Instead, incorporating examples that mirror the anticipated format of modifications is imperative for realizing a significant improvement. Thus, the exploration of methods to enhance the model\u2019s logical reasoning capabilities independent of context-based examples constitutes an avenue for future research.\n# 5 Conclusion\nIn this study, we investigate the capacity of LLMs, with parameters varying from 7B to 200B, to comprehend logical rules. The observed performance disparity between smaller and larger models indicates that size alone does not guarantee a profound understanding of logical constructs. While larger models may show traces of semantic learning, their outputs often lack logical validity when faced with swapped logical predicates. Our findings suggest that while LLMs may improve their logical reasoning performance through in-context learning and methodologies such as COT, these enhancements do not equate to a genuine understanding of logical operations and definitions, nor do they necessarily confer the capability for logical reasoning.\n# Limitations\nDespite employing prompts and in-context examples that ostensibly improve the model\u2019s capacity for logical reasoning, the enhancement remains marginal. To date, a method that markedly augments the model\u2019s comprehension through incontext learning has not been identified. The prevailing pre-training mechanism focuses on nexttoken prediction by estimating the subsequent word based on a probability distribution and may not be\nideally suited for logical tasks. These tasks often necessitate the processing of longer-span dependencies and the integration of global information for effective reasoning. Consequently, we believe that devising an alternative pre-training strategy tailored to these requirements presents a promising avenue for future research.\n# References\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 2397\u20132430. PMLR.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li FeiFei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. 2021. On the opportunities and risks of foundation models. CoRR, abs/2108.07258.\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020a. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners. In NeurIPS. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 719\u2013730. Association for Computational Linguistics. Tejas Gokhale, Abhishek Chaudhary, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. 2021. Semantically distributed robust optimization for vision-and-language inference. arXiv preprint arXiv:2110.07165. Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2019. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations. Tao Li and Vivek Srikumar. 2019. Augmenting neural networks with first-order logic. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 292\u2013302, Florence, Italy. Association for Computational Linguistics. Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. 2023. Logicot: Logical chain-of-thought instruction tuning. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 2908\u20132921. Association for Computational Linguistics. Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2021. Pretrained transformers as universal computation engines. CoRR, abs/2103.05247. Bill MacCartney and Christopher D. Manning. 2007. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 193\u2013200, Prague. Association for Computational Linguistics. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. CoRR, abs/2111.01243. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048\u201311064. Association for Computational Linguistics.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. CoRR, abs/2003.08271.\neven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100.\nugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.\nugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\ntuned chat models. CoRR, abs/2307.09288. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. CoRR, abs/2212.10560. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS. Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao, Yueting Zhuang, Luo Si, and Fei Wu. 2020. De-biased court\u2019s view generation with causality. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763\u2013780. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130B: an open bilingual pre-trained model. In ICLR. OpenReview.net. Xiangji Zeng, Yunliang Li, Yuchen Zhai, and Yin Zhang. 2020. Counterfactual generator: A weaklysupervised method for named entity recognition. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7270\u20137280. Qingfu Zhu, Weinan Zhang, Ting Liu, and William Yang Wang. 2020. Counterfactual off-policy training for neural dialogue generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3438\u20133448.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of understanding logical reasoning in large language models (LLMs), which have shown exceptional performance in complex tasks but lack clarity on whether they genuinely comprehend logical rules or rely on probabilistic mappings.",
        "problem": {
            "definition": "The core problem is to determine the extent to which LLMs understand logical rules versus merely leveraging probabilistic associations to generate answers.",
            "key obstacle": "A significant challenge is assessing the reasoning capabilities of LLMs, particularly in distinguishing genuine understanding from probabilistic guessing."
        },
        "idea": {
            "intuition": "The idea was inspired by the observed success of LLMs in logical reasoning tasks and the need to explore the underlying mechanisms behind this success.",
            "opinion": "The paper posits that LLMs do not truly understand logical principles but rather enhance their likelihood of correct responses through in-context learning.",
            "innovation": "The main innovation lies in employing counterfactual methods to evaluate LLMs' reasoning capabilities, specifically by modifying context and logical concepts."
        },
        "Theory": {
            "perspective": "The theoretical perspective focuses on the limitations of LLMs in understanding logical constructs, positing that their outputs are based on learned probabilistic associations rather than true comprehension.",
            "opinion": "The authors argue that while LLMs can perform well in logical reasoning tasks, this does not equate to genuine understanding of logical operations.",
            "proof": "The paper provides empirical evidence showing that altering logical definitions and context significantly disrupts LLM outputs, indicating a lack of true understanding."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized two model series (LLaMA and Qwen) with varying parameter sizes (7B to 200B) on datasets including LogicalCOT, Folio, and Entailment Bank.",
            "evaluation method": "The evaluation involved manipulating in-context examples by replacing and modifying text, reasoning chains, and logical definitions to assess the models' reasoning capabilities."
        },
        "conclusion": "The study concludes that while larger LLMs exhibit some semantic learning, they do not demonstrate a genuine understanding of logical rules, as evidenced by their inability to adapt outputs to modified logical predicates.",
        "discussion": {
            "advantage": "The paper provides critical insights into the limitations of LLMs, highlighting the need for improved mechanisms to enhance logical reasoning.",
            "limitation": "The enhancement of logical reasoning capabilities through prompts and in-context examples remains marginal, indicating that current pre-training methods may not be suited for logical tasks.",
            "future work": "Future research should focus on developing alternative pre-training strategies that better accommodate the requirements of logical reasoning tasks."
        },
        "other info": [
            {
                "info1": "The paper emphasizes the importance of understanding the underlying mechanisms of LLMs to improve their reliability in logical reasoning.",
                "info2": {
                    "info2.1": "Counterfactual methods are highlighted as a novel approach for evaluating model understanding.",
                    "info2.2": "The findings suggest that size alone does not guarantee a profound understanding of logical constructs."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The core problem is to determine the extent to which LLMs understand logical rules versus merely leveraging probabilistic associations to generate answers."
        },
        {
            "section number": "1.2",
            "key information": "The paper posits that LLMs do not truly understand logical principles but rather enhance their likelihood of correct responses through in-context learning."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective focuses on the limitations of LLMs in understanding logical constructs, positing that their outputs are based on learned probabilistic associations rather than true comprehension."
        },
        {
            "section number": "3.3",
            "key information": "The evaluation involved manipulating in-context examples by replacing and modifying text, reasoning chains, and logical definitions to assess the models' reasoning capabilities."
        },
        {
            "section number": "6.1",
            "key information": "The enhancement of logical reasoning capabilities through prompts and in-context examples remains marginal, indicating that current pre-training methods may not be suited for logical tasks."
        },
        {
            "section number": "6.4",
            "key information": "The findings suggest that size alone does not guarantee a profound understanding of logical constructs."
        }
    ],
    "similarity_score": 0.7470351981856835,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Do Large Language Models Understand Logic or Just Mimick Context_.json"
}