{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.02443",
    "title": "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation",
    "abstract": "The recent advancements in Large Language Models (LLMs) have sparked interest in harnessing their potential within recommender systems. Since LLMs are designed for natural language tasks, existing recommendation approaches have predominantly transformed recommendation tasks into open-domain natural language generation tasks. However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. Furthermore, practical ID-based recommendation strategies, reliant on a huge number of unique identities (IDs) to represent users and items, have gained prominence in real-world recommender systems due to their effectiveness and efficiency. Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations. In this paper, we introduce an Elegant Effective Efficient Extensible solution for large language models for Sequential Recommendation (E4SRec), which seamlessly integrates LLMs with traditional recommender systems that exclusively utilize IDs to represent items. Specifically, E4SRec takes ID sequences as inputs, ensuring that the generated outputs fall within the candidate lists. Furthermore, E4SRec possesses the capability to generate the entire ranking list in a single forward process, and demands only a minimal set of pluggable parameters, which are trained for each dataset while keeping the entire LLM frozen. We substantiate the effectiveness, efficiency, and extensibility of our proposed E4SRec through comprehensive experiments conducted on four widely-used real-world datasets. The implementation code is accessible at https://github.com/HestiaSky/E4SRec/.",
    "bib_name": "li2023e4sreceleganteffectiveefficient",
    "md_text": "# E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation\nXinhang Li Tsinghua Univerisity Beijing, China xh-li20@mails.tsinghua.edu.cn Chong Chen Huawei Cloud BU Beijing, China chenchong55@huawei.com\nChong Chen Huawei Cloud BU Beijing, China chenchong55@huawei.com Xiangyu Zhao City Univerisity of Hong K Hong Kong xianzhao@cityu.edu.hk\nChong Chen Huawei Cloud BU Beijing, China chenchong55@huawei.com\nXinhang Li Tsinghua Univerisity Beijing, China xh-li20@mails.tsinghua.edu.cn\nYong Zhang Tsinghua Univerisity Beijing, China hangyong05@tsinghua.edu.cn Chunxiao Xing Tsinghua Univerisity Beijing, China xingcx@tsinghua.edu.cn\n 5 Dec 2023\n# ABSTRACT\nThe recent advancements in Large Language Models (LLMs) have sparked interest in harnessing their potential within recommender systems. Since LLMs are designed for natural language tasks, existing recommendation approaches have predominantly transformed recommendation tasks into open-domain natural language generation tasks. However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. Furthermore, practical ID-based recommendation strategies, reliant on a huge number of unique identities (IDs) to represent users and items, have gained prominence in real-world recommender systems due to their effectiveness and efficiency. Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations. In this paper, we introduce an Elegant Effective Efficient Extensible solution for large language models for Sequential Recommendation (E4SRec), which seamlessly integrates LLMs with traditional recommender systems that exclusively utilize IDs to represent items. Specifically, E4SRec takes ID sequences as inputs, ensuring that the generated outputs fall within the candidate lists. Furthermore, E4SRec possesses the capability to generate the entire ranking list in a single forward process, and demands only a minimal set of pluggable parameters, which are trained for each dataset while keeping the entire LLM frozen. We substantiate the effectiveness, efficiency, and extensibility of our proposed E4SRec through comprehensive experiments conducted on four widely-used real-world datasets. The implementation code is accessible at https://github.com/HestiaSky/E4SRec/.\narXiv:2312.02443v1\n# CCS CONCEPTS\n\u2022 Information systems \u2192Recommender systems; Language models; Personalization.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW \u201924, May 13\u201317, 2024, Singapore, Singapore \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/10.1145/1122445.1122456\nXiangyu Zhao City Univerisity of Hong Kong Hong Kong xianzhao@cityu.edu.hk\nChunxiao Xing Tsinghua Univerisity Beijing, China xingcx@tsinghua.edu.cn\nKEYWORDS Large Language Model, Sequential Recommendation, Item ID and Indexing\nLarge Language Model, Sequential Recommendation, Item ID an Indexing\nACM Reference Format: Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023. E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation. In Proceedings of the ACM Web Conference 2024 (WWW \u201924), May 13\u201317, 2024, Singapore, Singapore. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/1122445.1122456\n# 1 INTRODUCTION\nRecommender systems, in existence for decades, are instrumental in mitigating information overload and enhancing user experience on the Web [1, 21, 35]. These systems discern user preferences to offer tailored recommendations on content or items [6, 24, 36]. Nowadays, the rise of Large Language Models (LLMs) [20] is revolutionizing our familiar landscapes [43], which excel in assimilating real-world knowledge from the Web and achieving proficient natural language generation. Recently, there has been a significant upsurge in research endeavors focused on leveraging LLMs for recommendation tasks and this trend is progressively becoming more inevitable [47]. While many non-tuning approaches [10, 14, 25], including prompting and in-context learning [3], strive to leverage the zero/few-shot learning ability, tuning approaches usually outperform them as they are fine-tuned for specific tasks using dedicated data. However, bridging the substantial gap between natural language generation tasks and recommendation tasks remains a formidable challenge. To tackle above challenge, existing approaches [2, 11, 17, 50] predominantly convert the recommendation task into a natural language generation task to align it with the inherent capabilities of LLMs. This involves the direct generation of item names or ratings based on appropriate prompts. However, such a solution has several limitations as depicted in Figure 1. First, these methods aim to harness the inherent knowledge of LLMs for recommendation through fine-tuning, essentially crafting an external knowledgeaugmented content-based recommendation [47]. Hence, they demand rich semantic information through the prompt. When the semantic information is insufficient or vague due to a huge number of homogeneous items, which is very common in recommendation, these methods are not able to yield satisfying performance.\nLLM-based Sequential Recommendation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4050/4050e422-96ff-45f0-a1b1-3713ee2e6de6.png\" style=\"width: 50%;\"></div>\nE4SRec\n<div style=\"text-align: center;\">Figure 1: Illustration of LLM-based sequential recommendation. The upper part denotes the existing methods that fail to model the IDs and undesirably generate out-of-range results while the lower part denotes E4SRec that can effectively and efficiently handle the IDs.</div>\nDifferent from content-based filtering methods, collaborative filtering methods utilize a huge number of unique identities (IDs) to represent users and items and have dominated the recommender system area for years. Such IDs merely function as indices for users or items without encapsulating any semantic information. Consequently, existing LLM-based recommendation methods struggle to manage the IDs to conduct ID-based recommendation and more critically, fail to leverage collaborative data vital for recommendations. Second, the problem definition of the existing approaches that is generate the results from the whole vocabulary will often lead to out-of-range results [17]. Such erratic generation not only diverges from the expectations of recommender systems but also negatively impacts user experience. Last but not least, the existing methods are only able to generate one recommendation result each time for the characteristics of LLMs. They mainly focus on the ranking task by attaching all the candidates in the prompt [50], and cannot tackle the matching task that requires matching scores of all the candidates. Nevertheless, the efficiency is still unacceptable for the recommender systems that require low latency and high concurrency. Hence, the trajectory of current LLM-based recommendation techniques fails to meet the needs of contemporary recommender systems and lacks practicality. To overcome the aforementioned limitations, we propose an Elegant Effective Efficient Extensible solution of large language models for Sequential Recommendation (E4SRec) by incorporating LLMs and traditional recommendation models with only IDs to represent items. Specifically, our proposed E4SRec accepts only ID sequences as inputs and ensures controllable generation with high efficiency by making predictions on and only on all the candidates in each forward process. Our proposed E4SRec solution encompasses four key phases: sequential recommendation model pretraining, LLM instruction tuning, E4SRec model training and E4SRec model deployment. For each given sequential recommendation dataset,\nwe first pretrain a traditional sequential recommendation model and then extract the item ID embeddings to prepare for the ID injection of LLM. An instruction tuning process of the LLM is also carried out to stimulate its capability to follow instructions and this tuned LLM is shared for all the task-specific models. Then, in the training stage of E4SRec, we wrap the sequences of item IDs into prompts by a linear projection of the item ID embeddings for ID injection. We freeze all the parameters of the LLM and only train an additional minimal set of parameters for adaption on the specific dataset. The recommendation results are made by computing the joint probability distribution between the output of LLM and all the candidate items via an item linear projection. Finally, once being trained, E4SRec can be deployed for practical application in a lightweight manner necessitating merely four pluggable components: the item ID embeddings, the input linear projection, the adapter and the item linear projection. The contribution of this paper can be summarized as follows: \u2022 We pioneer an innovative and effective strategy to address the unique challenges of integrating IDs in applying LLMs for recommendation tasks. \u2022 We address the prevailing issues of out-of-range outputs and generation efficiency, achieving controllable and efficient generative recommendation. \u2022 We propose an Elegant Effective Efficient Extensible solution of large language models for Sequential Recommendation (E4SRec), which is able to build an industrial-level recommender system from scratch. \u2022 Comprehensive experiments across four prominent real-world sequential recommendation datasets demonstrate the superiority and effectiveness of our proposed E4SRec model with in-depth analyses underscoring its efficiency and extensibility in realworld applications.\n# 2 METHODOLOGY\nIn this section, we will introduce the complete solution of our proposed E4SRec in detail, including our pathway of ID injection, the tuning strategy of the backbone foundation models, the model structure of E4SRec and the deployment of E4SRec.\n# 2.1 Overview\nIn order to deliver a clearer and more concise schema for better understanding, we provide the whole architecture of our proposed E4SRec solution in Figure 2. As illustrated in the lower right part, our E4SRec solution consists of four stages, which are sequential recommendation model pretraining, LLM instruction tuning, E4SRec model training and E4SRec model deployment. Specifically, the pretraining of sequential recommendation models and the instruction tuning of LLMs are the preliminaries and these two stages are essentially decoupled from the following stages of E4SRec by only providing sets of parameters as pluggable components. In the training stage of E4SRec, there are also a small number of parameters of several pluggable components being trained while the entire LLM is frozen and the personalization on the specific dataset is provided by an adapter. Once being trained, the E4SRec model could be easily deployed to conduct the sequential recommendation task on the given dataset by simply replacing the parameters of the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2235/22354114-e588-45ee-8134-9c0db6d13a80.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ce0/0ce0338c-f3f8-458f-99bc-e85b0eda171e.png\" style=\"width: 50%;\"></div>\nTraining\n<div style=\"text-align: center;\">igure 2: Architecture of E4SRec solution. The left part illustrates the structure of E4SRec, including the input layer, the large anguage model layer and the prediction layer. The upper right part describes the efficient inference process. The lower right art shows the complete solution of E4SRec.</div>\npluggable components. Through the above stages, we provide the whole solution of E4SRec that can be used to build an industriallevel LLM-based recommender system from scratch. Please find the detailed description of the whole pipeline of E4SRec solution in Appendix A.\n# 2.2 ID Injection\nAlthough LLMs are powerful in modeling natural language and are able to produce rational responses, they are unaware of the meanings of IDs without textual features and thus are not able to handle pure ID information. However, the collaborative information contained in the IDs has been proven to be very effective and crucial in personalized recommender systems for a long time. Therefore, the disability of utilizing ID information strongly limits the practical value of LLMs in recommender systems so far. Considering the huge number of IDs and the extreme sparsity of collaborative signals, it is especially challenging to incorporate ID information into LLMs. The existing works [16, 49] have explored various methods to introduce IDs into LLMs for recommendation, including vocabulary expansion, character decomposition, sequential grouping and collaborative clustering. Nevertheless, these methods are all insufficient to effectively capture the collaborative information. Meanwhile, the projection of item names or semantic descriptions falls into content-based recommendation without personalization and is only effective when the number of items is quite small with succinct informative textual features. To address the disability of LLMs to handle ID information, we propose a novel approach by injecting the ID embeddings into the LLMs rather than learning them through the training or tuning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3652/36529f17-d8cf-4072-a0bf-3ef2850f6639.png\" style=\"width: 50%;\"></div>\nprocess of LLMs. Specifically, the ID embeddings are obtained by directly extracting from a pretrained sequential recommendation model. In this paper, we select the SASRec [18] model to provide ID embeddings considering its effectiveness and generalization. It is worth noting that it could be replaced by any other sequential recommendation models for improvement. The selected sequential recommendation model is pretrained on the given dataset for iterations to get its best performance. Then the ID embeddings of the items are directly extracted without any modification to be ready for the input of E4SRec, which can be represented as E \u2208R\ud835\udc41\u00d7\ud835\udc51\ud835\udc60. \ud835\udc41and \ud835\udc51\ud835\udc60are the number of items and the dimension of ID embeddings, respectively. Since this sequential recommendation model is pretrained with only sequences of IDs, there is no information on any other features being exposed to the ID embeddings.\n# 2.3 Backbone Tuning\nThe open-sourced LLMs are mainly created for the general purpose of natural language generation to answer any questions from users so that the output formats are diverse and typically wordy. However, in specific scenarios like recommendation, we expect the generated outputs of LLMs to strictly adhere to the given format usually as simple as a number or a word. At the same time, it is not desirable for us to get a LLM that is only able to achieve the given task by directly tuning on it. Hence, we aim to modify the LLMs through instruction tuning [45] on various tasks to enable the LLMs to complete any instructions according to the given template. Specifically, we choose the LLaMA2-13B [42] model as the backbone LLM due to its strong power and generalization. For the instruction tuning process, we follow the settings in Platypus [23]\nTable 1: Example of Alpaca template for instruction tuning.\nInstruction Input\n### Instruction:\nPlease give the maximum common\nsubsequence of the following two strings.\n### Input:\nLarge Language Model, Language Models\nInstruction Output\n### Response:\nLanguage Model\nto apply a Parameter-Efficient Fine-Tuning (PEFT) [26] method, LoRA [15], on the gate_proj, down_proj and up_proj modules of LLaMA2-13B. Using the PEFT method, only the parameters of the specified modules are trained (about 0.07% of the total parameters) so that the training efficiency is highly improved. The initial datasets and more detailed settings can be found at the project page of Platypus1. The instruction tuning [44] is conducted for one epoch using the Alpaca [40] template. An example of the Alpaca style prompt is illustrated in Table 1, which consists of instruction, input and response parts.\n# 2.4 E4SRec\n2.4.1 Input Layer. In this input layer, we aim to inject the IDs into LLMs along with the standard textual prompts. As illustrated in the left part of Figure 2, E4SRec also follows the Alpaca template and the item ID sequences are taken as the input part of the prompt. From another perspective, each item ID can be seen as a \u2018word\u2019 in the LLM but is projected using our proposed ID injection component rather than the lookup table of LLM. Thus, we obtain the corresponding ID embeddings of the items through the aforementioned ID injection component while the rest of the prompt is projected to word embeddings as the typical LLM inputs. The final inputs are the concatenation of the word embeddings and the ID embeddings in their original positions. Since the pretrained ID embeddings usually have a much lower dimension (e.g., 64 in our pretrained SASRec) than the word embeddings in LLM (e.g., 5120 in LLaMA213B), we employ a linear projection to convert the ID embeddings into the same dimension with the word embeddings. As mentioned above, we do not want the LLMs to encounter the \u2018catastrophic forgetting\u2019 phenomenon during the training on the recommendation task. Certainly, we also do not want the learned collaborative information to be destroyed either. Therefore, only the linear projection component is trainable, which is as small as a weight matrix W\ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\u2208R\ud835\udc51\ud835\udc60\u00d7\ud835\udc51\ud835\udc58. \ud835\udc51\ud835\udc60and \ud835\udc51\ud835\udc58represent the dimensions of the ID embeddings in pretrained sequential recommendation model and the word embeddings in LLM, respectively. 2.4.2 Large Language Model Layer. The large language model layer employs the instruction-tuned LLaMA2-13B, which is the merge of the original LLaMA2-13B and the LoRA adapter parameters for instruction tuning. Similar to the LLM instruction tuning stage, we also introduce an additional LoRA adapter on the gate_proj, down_proj and up_proj modules to model the personalization of the given recommendation task.\n2.4.2 Large Language Model Layer. The large language model layer employs the instruction-tuned LLaMA2-13B, which is the merge of the original LLaMA2-13B and the LoRA adapter parameters for instruction tuning. Similar to the LLM instruction tuning stage, we also introduce an additional LoRA adapter on the gate_proj, down_proj and up_proj modules to model the personalization of the given recommendation task.\n1https://platypus-llm.github.io/\n2.4.3 Prediction Layer. The prediction layer receives the output of the LLM and makes the predictions for recommendation. Existing LLM-based recommendation approaches mainly define the recommendation task as an open-domain natural language generation task, which is in line with the purpose of LLMs themselves. However, due to the characteristics of the task definition, they often generate out-of-range results and are only able to generate one recommendation result each time. Such a pathway impairs the reliability of the recommendation results and limits the recommendation efficiency. Therefore, these approaches can only work in the ranking stages (usually less than one hundred items) but are impractical in the matching stages (usually more than thousands of items). Unlike these existing approaches that directly adopt the task definition of LLMs without modification, we stick to their underlying definition of modeling joint probability distribution for generation. That is to say, we can compute the prediction results for a given sequence over all the candidates in each forward process. In order to achieve this goal, we dive deep into the structure of LLMs and propose to employ an item linear projection to replace the original prediction layer in LLMs via a weight matrix W\ud835\udc42\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61\u2208R\ud835\udc51\ud835\udc58\u00d7\ud835\udc41where \ud835\udc41denotes the total number of candidate items. Then, the predictions of a given sequence could be represented as a \ud835\udc41-dimensional vector \u02c6y \u2208R\ud835\udc41. In the training stage, we adopt the cross entropy loss between predictions and ground-truth next items as the learning objective of E4SRec as follows:\n# 2.5 Inference & Deployment\nThe inference time is also an important and bothering problem for the application of LLMs. Although we cannot improve the performance of LLMs themselves, our proposed E4SRec solution can ensure that the overall inference time of the recommendation task is as close as that of a vanilla LLM. Review the structure of our proposed E4SRec, the additional components are all so small compared with the backbone LLM that they will only cost a little more time. More than that, the time-consuming softmax operation in the training stage is also unnecessary. Therefore, the inference process can be further simplified to the nearest neighbor search between the output of LLM and the vectors in the item linear projection component as shown in the upper right part of Figure 2. After the training stage is completed, our proposed E4SRec can be deployed in a very lightweight manner. The backbone LLM is onesize-fits-all for all the tasks including sequential recommendation so that the instruction tuning only needs to be done once and is shared across all the downstream tasks. For each coming sequential recommendation dataset, only the ID embeddings E, the linear projection in the input layer W\ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61, LoRA weights \u0398 and the item linear projection W\ud835\udc42\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61are required to be trained and stored for deployment. Compared with the billions of parameters in the LLMs, these parameters are as tiny as about 1%. Meanwhile, all these components are completely pluggable so that the recommender system can quickly adapt to a specific dataset by simply replacing these pluggable components.\n<div style=\"text-align: center;\">Table 2: Statistics of the datasets.</div>\nDataset\n# Users\n# Items\n# Actions\nSparsity\nBeauty\n22,363\n12,101\n198,502\n99.93%\nSports\n25,598\n18,357\n296,337\n99.95%\nToys\n19,412\n11,924\n167,597\n99.93%\nYelp\n30,431\n20,033\n316,354\n99.95%\n# 3 EXPERIMENTS\nIn this section, we will evaluate our proposed E4SRec on several real-world datasets with a selected set of widely-used baseline methods in sequential recommendation. Meanwhile, we will also present the ablation study, robustness analysis, efficiency analyses and discussions in order to answer the following questions: \u2022 RQ1: How does E4SRec compare with traditional sequential recommendation models on performance? \u2022 RQ2: How do the injected IDs and the large language model affect the performance of E4SRec? \u2022 RQ3: How effective is E4SRec in leveraging collaborative information to alleviate the data sparsity problem? \u2022 RQ4: How efficient is E4SRec on both inference time and storage space in deployment? \u2022 RQ5: How extensible is E4SRec for new items in industrial applications?\n# 3.1 Experimental Settings\n3.1.1 Datasets. To evaluate the effectiveness of E4SRec, we conduct the experiments on four widely-used real-world datasets. Specifically, Beauty, Sports and Toys are the datasets of sub-categories \u2018Beauty\u2019, \u2018Sports and Outdoors\u2019 and \u2018Toys and Games\u2019 in the Amazon review data [27]. Yelp is a popular platform and the dataset is widely-used in various recommendation tasks. Here, we only utilize the data after January 1st, 2019. The statistics of the datasets are shown in Table 2. For sequential recommendation, the interaction sequences of users are sorted by timestamps in ascending order. Following the previous works [34, 37], we apply the 5-core settings to filter the unpopular items with fewer than 5 interactions to ensure robust evaluation.\n3.1.2 Evaluation Metrics. To avoid selection bias and provide more reliable results, we evaluate the performance of predictions on the whole item set, which is effective in evaluating the matching ability. Following previous works [18, 31], we also apply the leave-one-out strategy. For each interaction sequence of users, the last item is taken as test data, the second last one is taken as validation data and the remaining sequence is used for training. As for the evaluation metrics, we choose two types of widely-used metrics, which are topk Hit Ratio (HR@k) and top-k normalized Discounted Cumulative Gain (nDCG@k) with k = {5, 10, 20}. These metrics are averaged over all the users for report. Besides, we also perform an additional evaluation with negative sampling in the Appendix C according to [51, 52]. Specifically, 99 negative items are randomly sampled for each positive item. In this setting, we are able to evaluate the ranking ability of E4SRec.\n3.1.3 Baseline Methods. The baseline methods chosen for comparison can be split into three categories: non-sequential methods, traditional sequential methods and self-supervised sequential methods. For non-sequential methods, we have: \u2022 POP is a heuristic method that directly ranks the items using their popularity defined as the interaction numbers. \u2022 BPR [33] utilizes the Bayesian Personalized Ranking (BPR) loss to optimize the matrix factorization (MF) [22] model for characterizing the pair-wise interactions. For traditional sequential methods, we have: \u2022 GRU4Rec [13] implements the GRU recurrent neural network for sequential modeling and then makes predictions for recommendation. \u2022 Caser [39] integrates both horizontal and vertical convolutional operations to better capture the high-order interactions within item sequences for recommendation. \u2022 SASRec [18] is a self-attentive sequential recommendation model with multi-head self-attention to model the complex sequential information. For self-supervised sequential methods, we have: \u2022 BERT4Rec [37] employs the Cloze [41] objective rather than the next-item prediction for sequential recommendation in a pretraining-tuning manner. \u2022 S3-Rec [51] applies contrastive learning to capture the correlations among items, sub-sequences and attributes. Specifically, we take the variant with only Mask Item Prediction (MIP) objective. \u2022 CL4SRec [48] incorporates the contrastive learning with the transformer-based sequential recommendation model to obtain more robust results. \u2022 ICLRec [4] leverages a latent intent variable to learn the users\u2019 intent distribution from unlabeled item sequences to improve the transformer-based sequential recommendation model. Note that our proposed E4SRec only utilizes the ID information and no other features are exposed to E4SRec during the training and inference. Therefore, those methods that either implement data augmentation techniques [52, 53] or incorporate other features [8, 28] are orthogonal to E4SRec and are thus excluded for fair comparison. 3.1.4 Implementation Details. For GRU4Rec, Caser, BERT4Rec, S3Rec and ICLRec, we implement them using the public resources released by their authors. For other models, we implement them using PyTorch 2.0.1. The embedding dimension is set to 64 and the maximum sequence length is set to 50 for all the models on all the datasets. The model parameters are initialized with Xavier initialization and are optimized using Adam [19]. For our proposed E4SRec, we obtain the LLaMA2-13B using HuggingFace2 and conduct instruction tuning for one epoch. The ID embeddings are directly extracted from the pretrained SASRec model without any modification. To obtain better performance, we perform the grid search of the training configurations using the validation set. Specifically, we aim to find a better combination of learning rate, training epochs and LoRA modules. The best combinations for all the datasets are listed in Appendix B due to limited space. All the experiments are implemented using 8 NVIDIA Tesla\n3.1.4 Implementation Details. For GRU4Rec, Caser, BERT4Rec, S3Rec and ICLRec, we implement them using the public resources released by their authors. For other models, we implement them using PyTorch 2.0.1. The embedding dimension is set to 64 and the maximum sequence length is set to 50 for all the models on all the datasets. The model parameters are initialized with Xavier initialization and are optimized using Adam [19]. For our proposed E4SRec, we obtain the LLaMA2-13B using HuggingFace2 and conduct instruction tuning for one epoch. The ID embeddings are directly extracted from the pretrained SASRec model without any modification. To obtain better performance, we perform the grid search of the training configurations using the validation set. Specifically, we aim to find a better combination of learning rate, training epochs and LoRA modules. The best combinations for all the datasets are listed in Appendix B due to limited space. All the experiments are implemented using 8 NVIDIA Tesla 2https://huggingface.co/meta-llama/Llama-2-13b\n2https://huggingface.co/meta-llama/Llama-2-13b\n<div style=\"text-align: center;\">Table 3: Performance comparison of different methods. The best performance is highlighted in bold while the second best performance is underlined. The last column indicates the improvements over the best baseline models and all the results of E4SRec are statistically significant with p < 0.01 compared to the best baseline models.</div>\nDataset\nMetric\nPOP\nBPR\nGRU4Rec\nCaser\nSASRec\nBERT4Rec\nS3-Rec\nCL4SRec\nICLRec\nE4SRec\nImprov.\nBeauty\nHR@5\n0.0072\n0.0120\n0.0164\n0.0251\n0.0333\n0.0193\n0.0327\n0.0407\n0.0436\n0.0525\n20.41%\nHR@10\n0.0114\n0.0361\n0.0289\n0.0418\n0.0581\n0.0401\n0.0591\n0.0626\n0.0653\n0.0758\n16.08%\nHR@20\n0.0195\n0.0589\n0.0478\n0.0643\n0.0915\n0.0596\n0.0898\n0.0957\n0.0974\n0.1071\n9.96%\nnDCG@5\n0.0040\n0.0065\n0.0086\n0.0127\n0.0179\n0.0187\n0.0175\n0.0223\n0.0240\n0.0360\n50.00%\nnDCG@10\n0.0053\n0.0122\n0.0142\n0.0193\n0.0258\n0.0254\n0.0268\n0.0317\n0.0338\n0.0435\n28.70%\nnDCG@20\n0.0073\n0.0179\n0.0169\n0.0258\n0.0342\n0.0361\n0.0370\n0.0396\n0.0416\n0.0514\n23.56%\nSports\nHR@5\n0.0055\n0.0092\n0.0137\n0.0139\n0.0170\n0.0176\n0.0157\n0.0217\n0.0238\n0.0281\n18.07%\nHR@10\n0.0090\n0.0188\n0.0274\n0.0231\n0.0289\n0.0326\n0.0265\n0.0374\n0.0393\n0.0410\n4.33%\nHR@20\n0.0149\n0.0258\n0.0438\n0.0389\n0.0477\n0.0493\n0.0460\n0.0582\n0.0553\n0.0626\n7.56%\nnDCG@5\n0.0040\n0.0053\n0.0096\n0.0085\n0.0091\n0.0105\n0.0098\n0.0129\n0.0152\n0.0196\n28.95%\nnDCG@10\n0.0051\n0.0083\n0.0137\n0.0126\n0.0129\n0.0153\n0.0135\n0.0184\n0.0212\n0.0237\n11.79%\nnDCG@20\n0.0066\n0.0121\n0.0171\n0.0166\n0.0177\n0.0195\n0.0182\n0.0239\n0.0250\n0.0291\n16.40%\nToys\nHR@5\n0.0064\n0.0120\n0.0097\n0.0166\n0.0445\n0.0274\n0.0492\n0.0484\n0.0509\n0.0566\n11.20%\nHR@10\n0.0079\n0.0211\n0.0196\n0.0281\n0.0698\n0.0460\n0.0698\n0.0706\n0.0725\n0.0798\n10.07%\nHR@20\n0.0108\n0.0312\n0.0301\n0.0420\n0.0999\n0.0688\n0.0962\n0.0984\n0.1018\n0.1107\n8.74%\nnDCG@5\n0.0037\n0.0082\n0.0059\n0.0107\n0.0236\n0.0174\n0.0342\n0.0327\n0.0350\n0.0405\n15.71%\nnDCG@10\n0.0057\n0.0120\n0.0098\n0.0151\n0.0318\n0.0230\n0.0375\n0.0404\n0.0423\n0.0479\n13.24%\nnDCG@20\n0.0062\n0.0136\n0.0116\n0.0179\n0.0394\n0.0291\n0.0431\n0.0466\n0.0493\n0.0557\n12.98%\nYelp\nHR@5\n0.0056\n0.0127\n0.0152\n0.0142\n0.0161\n0.0186\n0.0173\n0.0216\n0.0240\n0.0266\n10.83%\nHR@10\n0.0083\n0.0245\n0.0263\n0.0252\n0.0265\n0.0291\n0.0282\n0.0352\n0.0381\n0.0418\n9.71%\nHR@20\n0.0120\n0.0346\n0.0371\n0.0406\n0.0443\n0.0564\n0.0538\n0.0585\n0.0630\n0.0675\n7.14%\nnDCG@5\n0.0036\n0.0076\n0.0104\n0.0096\n0.0102\n0.0121\n0.0114\n0.0130\n0.0150\n0.0189\n26.00%\nnDCG@10\n0.0043\n0.0119\n0.0137\n0.0129\n0.0134\n0.0171\n0.0163\n0.0185\n0.0203\n0.0238\n17.24%\nnDCG@20\n0.0056\n0.0143\n0.0145\n0.0156\n0.0179\n0.0223\n0.0201\n0.0235\n0.0256\n0.0297\n16.02%\n# A800 GPUs. The implementation code is available online3 and will be accessible to the public for ease of reproducibility.\nA800 GPUs. The implementation code is available online3 and will be accessible to the public for ease of reproducibility.\n# 3.2 Main Results (RQ1)\nThe performance comparison of our proposed E4SRec with other traditional sequential recommendation models is shown in Table 3. Here we have the following observations:\n\u2022 Our proposed E4SRec can significantly outperform all the baseline methods on all four datasets thanks to the powerful ability of LLM. The relative improvements on performance over the best baseline methods are about 12% on HR@k and 21% on nDCG@k, which fully demonstrate the effectiveness of E4SRec. \u2022 Generally, the performance gains are greater on nDCG@k metrics than on HR@k metrics and are greater on smaller k values. This indicates that our proposed E4SRec can capture the users\u2019 preference more accurately and thus generate more reliable recommendation results. \u2022 The non-sequential methods are much worse than sequential methods due to the disadvantage of modeling sequential information. The dramatically bad performance of POP also indicates the important and crucial role of personalization in these datasets. \u2022 The self-supervised sequential methods are usually stronger than the traditional sequential methods. This phenomenon shows the\n3https://github.com/HestiaSky/E4SRec/\neffectiveness of introducing self-supervised learning to provide additional training signals in improving the sequential recommendation performance. \u2022 CL4SRec and ICLRec achieve much better performance with the help of contrastive learning in improving the robustness of item representations. However, contrastive learning applies data augmentation in a certain extent by introducing augmented sequences, which may be a little bit unfair to the other methods. Therefore, SASRec is still a very strong method compared with them and is more suitable for reference. In such a situation, our proposed E4SRec will have an even more significant performance gain of up to 115%.\n# 3.3 Ablation Study (RQ2)\nIn order to explore the impacts of the injected IDs and the LLMs on the overall performance of E4SRec, we perform an ablation study by changing the injected IDs and the LLMs to design and then compare the following four variants:\nIn order to explore the impacts of the injected IDs and the LLMs on the overall performance of E4SRec, we perform an ablation study by changing the injected IDs and the LLMs to design and then compare the following four variants: \u2022 BPR + LLaMA: This variant employs the BPR model to provide item ID embeddings and the LLaMA2-13B model. \u2022 SASRec w/o LLM: This is a LLM-free variant that utilizes the SASRec as the sequential model for ID injection. Specifically, it has no LLM and only uses linear projections for prediction. \u2022 SASRec + BERT: A BERT [7] model is utilized in the LLM layer. Here we use BERT-base-uncased with 110M parameters.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9e6/b9e65033-6360-405d-bb0f-b0c56f7dcb8a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Ablation study of ID injection and LLM.</div>\n\u2022 SASRec + LLaMA: It is the basic version of our proposed E4SRec for reference with SASRec for ID injection and LLaMA2-13B as the LLM. From the performance comparison of HR@k metrics in Figure 3, we have the following observations: \u2022 The qualities of item ID embeddings do have effects on the performance. The performance of BPR + LLaMA is slightly worse than SASRec + LLaMA. Although BPR is unable to capture sequential information and has poor performance, the performance of BPR + LLaMA is still satisfying. This phenomenon may indicate that the main effect of ID injection is to provide collaborative information while the LLM is already sufficient to well capture sequential information during the training stage. \u2022 The extremely poor performance of SASRec w/o LLM demonstrates that the LLMs are necessary to conduct recommendation. \u2022 The inherent ability of the LLMs also has a significant impact on overall performance. Although BERT is already a very powerful language model, the performance gap is still significant compared to LLaMA. This also implies the effectiveness of our proposed E4SRec solution in leveraging the capabilities and tapping into the potentials of LLMs.\n# 3.4 Robustness Analysis (RQ3)\nData sparsity problem is a common issue of recommender systems that defects the performance in applications. For example, since most of the users only have limited interactions, the user cold-start problem is typically severe and thus harms the user experience. Traditional recommendation methods alleviate the data sparsity problems by jointly leveraging the data from other users and items via collaborative filtering. To verify the effectiveness of our proposed E4SRec in incorporating collaborative information, we compared it with SASRec on the robustness with different sparsity levels\u2019 data as shown in Figure 4. Specifically, we split the users into three groups based on their number of interactions. Based on the results, we have observations as follows:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b031/b0313f09-9018-4d31-acf2-b36b6324f668.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance comparison on different user groups with respect to the length of interaction sequences.</div>\n\u2022 The performance gains are more significant on sparser user groups than denser ones. Such a phenomenon may be originated from the few-/zero-shot learning ability of LLMs to enable E4SRec to achieve satisfying performance with insufficient data that traditional recommendation methods can hardly learn from.\n# 3.5 Efficiency Analysis (RQ4)\nIn addition to being effective in sequential recommendation, our proposed E4SRec solution is also very efficient in both time and space. To support this statement, we provide a comparison of inference time and storage space between the backbone LLM, LLaMA213B and E4SRec on 1 Nvidia Tesla A800 GPU as illustrated in Table 4. As mentioned above, E4SRec introduces only a tiny pluggable set of parameters to the backbone LLMs and doesn\u2019t change the overall data flow too much. Therefore, the inference time of E4SRec is very close to the backbone LLaMA2-13B. Considering the ability to generate results without the need for multiple times generation, our E4SRec is obviously more efficient by orders of magnitude on the actual inference time than the existing solutions which directly apply the task formulation of natural language generation. Meanwhile, the additional parameters in E4SRec are as tiny as 1% of the total parameters of the backbone LLM. Since this set of additional parameters is pluggable to have no effect on the backbone\nTable 4: Comparisons of inference time and storage space between LLaMA2-13B and E4SRec.\nModel\nInference Time\nStorage Space\nLLaMA2-13B\n0.21s/Instance\n12.12GB\nE4SRec\n0.23s/Instance\n12.23GB\nLLM, E4SRec can be stored and deployed with only one shared backbone LLM and multiple independent pluggable components for specific datasets. In this manner, there is only 2 times of space needed than the backbone LLM to store 100 E4SRec models, which is very economical in industrial applications.\n# 3.6 Discussions (RQ5)\nIn industrial applications, there are many new items emerging into the recommender systems every day. Apparently, retraining the recommendation model for each coming item is not practical so the ability to extend to new items is very crucial. Our E4SRec solution guarantees that the pluggable components are independent of IDs, which means adding a new item in the dataset only requires adding a new row in the linear projections without the need to retrain the entire model. Therefore, our proposed E4SRec solution is considerably extensible.\n# 4 RELATED WORKS 4.1 Sequential Recommendation\n# 4 RELATED WORKS\n# 4.1 Sequential Recommendation\nSequential recommendation is an important task in personalized recommender system which aims to capture the users\u2019 preference using their historical behavior sequences. Early works of sequential recommendation mainly lie in the pattern of Markov Chains [12, 32] that uses an item-item transition pattern to directly predict the next item with the previous items. For example, FPMC [34] uses Matrix Factorization (MF) to model the users\u2019 preference and utilizes Markov Chains to capture the sequential patterns for making prediction. With the booming development of deep learning, many sequential recommendation models with deep neural networks emerged. Caser [39] employs Convolutional Neural Network (CNN) and GRU4Rec [13] utilizes Recurrent Neural Network (RNN) to capture the high-order interactions within the item sequences for sequential modeling. In recent years, more powerful models are proposed with more advanced architectures to better leverage sequential information. SASRec [18] takes advantage of the multihead self-attention to attentively model sequential information in an unidirectional manner. BERT4Rec [37] improves such a manner by employing the Cloze [41] objective to predict the masked item to leverage the bidirectional information. S3-Rec [51] introduces contrastive learning to fuse the information of distinct items, subsequences and attributes. Similarly, CL4SRec [48] and ICLRec [4] both apply contrastive learning to better capture the users\u2019 preference with sequential information. There are also many other approaches that aim to incorporate other features, e.g. DuoRec [28] and EMKD [8], or employ data augmentation, e.g. FMLP-Rec [52] and ECL-SR [53] for further improvements.\nNevertheless, traditional sequential recommendation models are usually limited on performance for the limitations on the model scale and may lead to sub-optimal prediction. Unlike them, our proposed E4SRec takes advantages of LLMs to achieve more advanced performance and empower the generative recommendation, which can better comprehend human intentions and generate more human-like language responses.\n# 4.2 LLMs for Recommendation\nLarge Language Models (LLMs) have been proven to be very powerful in natural language processing and their strong power has encouraged researchers to make efforts to apply LLM for recommendation [9]. Early approaches view LLMs as feature extractors to generate knowledge-aware embeddings for recommendation. U-BERT [29] proposes a pretraining-tuning framework to learn users\u2019 representations and conduct user modeling. UserBERT [46] employs two self-supervision tasks on unlabeled behavior data to empower user modeling. With the emergence of generative LLMs like GPT, LLM-based recommendation has also shifted towards generative recommendation. These methods translate recommendation tasks as natural language tasks to directly generate the recommendation results [47]. At first, most approaches focus on using prompting [10, 38] or in-context learning [5, 25] to adapt LLMs for recommendation. However, these approaches fail to surpass the traditional recommendation models trained specifically for a given task on specific data. Therefore, many efforts are made to align the LLMs to recommendation by further fine-tuning recently. P5 [11] first proposes a unified framework to integrate five recommendation tasks via fine-tuning on FLAN-T5 [30]. Following it, InstructRec [50] adapts FLAN-T5 model to several downstream recommendation tasks by instruction tuning with more diverse texts. TALLRec [2] aligns the LLaMA model to the binary recommendation task by two stages of instruction tuning in few-shot scenario. GenRec [17] directly conducts instruction tuning on the LLaMA model with plain texts to achieve generative recommendation. However, all the above methods are essentially content-based recommendation and require rich semantic features to achieve satisfying performance. Therefore, they fail to handle the IDs and are unable to leverage collaborative information. Compared with them, our proposed E4SRec solution is more effective in handling IDs, more efficient on both time and space perspective and more extensible to fulfill real-world needs in application.\n# 5 CONCLUSION\nExisting approaches of LLM for recommendation face challenges in handling IDs, efficiency, extensiblility and thus are not able to fulfill the requirements of real-world applications. In this paper, we propose a novel E4SRec solution, which is elegant, effective, efficient and extensible to apply LLMs for sequential recommendation. Specifically, we introduce an elegant way to address the issue of handling IDs by injecting item ID embeddings into the LLM. Meanwhile, with the help of a modified prediction layer, we effectively solve the challenging out-of-range problem of generated results to ensure their legality and efficiently generate the predictions over all the candidates at once. The design of pluggable\ncomponents in E4SRec enables the model to be trained and deployed in a lightweight manner. The extensive experiments on four popular read-world datasets fully demonstrate the effectiveness and superiority of our proposed E4SRec solution. We hope our E4SRec solution will contribute to the research in applying LLM for recommender systems. Looking forward, we will pivot towards crafting elegant solutions for other recommendation tasks, such as CTR prediction, and continually pushing the frontiers of generative recommendations.\n[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions. IEEE Trans. Knowl. Data Eng. 17, 6 (2005), 734\u2013749. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023. ACM, 1007\u20131014. [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. [4] Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. 2022. Intent Contrastive Learning for Sequential Recommendation. In WWW \u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. ACM, 2172\u20132182. [5] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. In Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023. ACM, 1126\u20131132. [6] Mukund Deshpande and George Karypis. 2004. Item-based top-N recommendation algorithms. ACM Trans. Inf. Syst. 22, 1 (2004), 143\u2013177. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 4171\u20134186. [8] Hanwen Du, Huanhuan Yuan, Pengpeng Zhao, Fuzhen Zhuang, Guanfeng Liu, Lei Zhao, Yanchi Liu, and Victor S. Sheng. 2023. Ensemble Modeling with Contrastive Knowledge Distillation for Sequential Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023. ACM, 58\u201367. [9] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender Systems in the Era of Large Language Models (LLMs). CoRR abs/2307.02046 (2023). 10] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. CoRR abs/2303.14524 (2023). 11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In RecSys \u201922: Sixteenth ACM Conference on Recommender Systems, Seattle, WA, USA, September 18 - 23, 2022. ACM, 299\u2013315. 12] Ruining He and Julian J. McAuley. 2016. Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. In IEEE 16th International Conference on Data Mining, ICDM 2016, December 12-15, 2016, Barcelona, Spain. IEEE Computer Society, 191\u2013200. 13] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. 14] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, and Wayne Xin Zhao. 2023. Large Language Models are Zero-Shot Rankers for Recommender Systems. CoRR abs/2305.08845 (2023). 15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. [16] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How to Index Item IDs for Recommendation Foundation Models. CoRR abs/2305.06569 (2023). [17] Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, and Yongfeng Zhang. 2023. GenRec: Large Language Model for Generative Recommendation. CoRR abs/2307.00457 (2023). [18] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Recommendation. In IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018. IEEE Computer Society, 197\u2013206. [19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. [20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In NeurIPS. [21] Joseph A. Konstan, Bradley N. Miller, David A. Maltz, Jonathan L. Herlocker, Lee R. Gordon, and John Riedl. 1997. GroupLens: Applying Collaborative Filtering to Usenet News. Commun. ACM 40, 3 (1997), 77\u201387. [22] Yehuda Koren, Robert M. Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. Computer 42, 8 (2009), 30\u201337. [23] Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023. Platypus: Quick, Cheap, and Powerful Refinement of LLMs. CoRR abs/2308.07317 (2023). [24] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon.com Recommendations: Item-to-Item Collaborative Filtering. IEEE Internet Comput. 7, 1 (2003), 76\u201380. [25] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. CoRR abs/2304.10149 (2023). [26] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. https://github.com/huggingface/peft. [27] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015. ACM, 43\u201352. [28] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. In WSDM \u201922: The Fifteenth ACM International Conference on Web Search and Data Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022. ACM, 813\u2013823. [29] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. 2021. U-BERT: Pre-training User Representations for Improved Recommendation. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 4320\u20134327. [30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2020), 140:1\u2013140:67. [31] Ruiyang Ren, Zhaoyang Liu, Yaliang Li, Wayne Xin Zhao, Hui Wang, Bolin Ding, and Ji-Rong Wen. 2020. Sequential Recommendation with Self-Attentive Multi-Adversarial Network. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. ACM, 89\u201398. [32] Steffen Rendle. 2010. Factorization Machines. 2010 IEEE International Conference on Data Mining (2010), 995\u20131000. [33] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI 2009, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, Montreal, QC, Canada, June 18-21, 2009. AUAI Press, 452\u2013461. [34] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized Markov chains for next-basket recommendation. In Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010. ACM, 811\u2013820. [35] Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. 1994. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In CSCW \u201994, Proceedings of the Conference on Computer Supported Cooperative Work, Chapel Hill, NC, USA, October 22-26, 1994. ACM, 175\u2013186. [36] Badrul Munir Sarwar, George Karypis, Joseph A. Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the Tenth International World Wide Web Conference, WWW 10, Hong Kong, China, May 1-5, 2001. ACM, 285\u2013295. [37] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019. ACM, 1441\u20131450.\n[38] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. CoRR abs/2304.09542 (2023). [39] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018. ACM, 565\u2013573. [40] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_ alpaca. [41] Wilson L. Taylor. 1953. \u201cCloze Procedure\u201d: A New Tool for Measuring Readability. Journalism & Mass Communication Quarterly 30 (1953), 415 \u2013 433. [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023). [43] Eva Anna Maria van Dis, Johan Bollen, Willem Zuidema, Robert van Rooij, and Claudi L H Bockting. 2023. ChatGPT: five priorities for research. Nature 614 (2023), 224\u2013226. [44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 13484\u201313508. [45] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language Models are Zero-Shot Learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. [46] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2022. UserBERT: Pre-training User Model with Contrastive Self-supervision. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 2087\u20132092. [47] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023. A Survey on Large Language Models for Recommendation. CoRR abs/2305.19860 (2023). [48] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022. IEEE, 1259\u20131273. [49] Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2023. OpenP5: Benchmarking Foundation Models for Recommendation. CoRR abs/2306.11134 (2023). [50] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. CoRR abs/2305.07001 (2023). [51] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020. ACM, 1893\u20131902. [52] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-enhanced MLP is All You Need for Sequential Recommendation. In WWW \u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. ACM, 2388\u20132399. [53] Peilin Zhou, Jingqi Gao, Yueqi Xie, Qichen Ye, Yining Hua, Jaeboum Kim, Shoujin Wang, and Sunghun Kim. 2023. Equivariant Contrastive Learning for Sequential Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023. ACM, 129\u2013140.\n<div style=\"text-align: center;\">Algorithm 1: Pipeline of E4SRec Solution.</div>\nAlgorithm 1: Pipeline of E4SRec Solution.\nInput\n:recommendation dataset D, instruction dataset\nD\ud835\udc3c\ud835\udc5b\ud835\udc60and pretrained LLM M\ud835\udc3f\ud835\udc3f\ud835\udc40\nOutput:E4SRec model M\n// Preliminary\n1 Train a sequential recommendation model M\ud835\udc46\ud835\udc52\ud835\udc5eon D.\n2 Extract item ID embeddings E.\n3 Instruction tune the LLM model M\ud835\udc3f\ud835\udc3f\ud835\udc40on D\ud835\udc3c\ud835\udc5b\ud835\udc60.\n// Model Training\n4 Initialize pluggable components \u0398, including input linear\nprojection, LoRA weights and item linear projection.\n5 for \ud835\udc47\u21900 to \ud835\udc47\ud835\udc5a\ud835\udc4e\ud835\udc65iterations do\n6\nSample an instance for training.\n// ID Injection\n7\nObtain the corresponding ID embeddings from E;\nProject ID embeddings to the same dimension with\nM\ud835\udc3f\ud835\udc3f\ud835\udc40using input linear projection.\n// Prediction\n8\nFeed ID embeddings and prompt to M\ud835\udc3f\ud835\udc3f\ud835\udc40for output;\nPredict candidate items using item linear projection.\n// Update Parameters\n9\nCompute the cross-entropy loss L via Equation 1;\nUpdate \u0398 using L.\n10 end\n// Model Deployment\n11 Deploy backbone model M\ud835\udc3f\ud835\udc3f\ud835\udc40.\n12 Deploy E4SRec model for dataset D with\nM = M\ud835\udc3f\ud835\udc3f\ud835\udc40\u2190E, \u0398.\n<div style=\"text-align: center;\">Table 5: Training configuration on the four datasets.</div>\nParameter\nBeauty\nSports\nToys\nYelp\nTraining Epochs\n3\n3\n2\n5\nLearning Rate\n3e-4\n2e-4\n2e-4\n3e-4\nBatch Size\n16\nLoRA Rank\n16\nLoRA Alpha\n16\nLoRA Dropout\n0.05\nLoRA Modules\n[gate_proj, down_proj, up_proj]\nLearning Rate Scheduler\nCosine Scheduler\nWeight Decay\n0.1\nWarmup Steps\n100\n200\n100\n300\n# A PIPELINE OF E4SREC SOLUTION\nThe whole pipeline of E4SRec solution is described in Algorithm 1. Following such a process, one can easily train and deploy a E4SRec model in a lightweight manner.\n# B TRAINING CONFIGURATION\nLLMs are sensitive to the training configuration. In order to get desirable results, we implement different configurations on different datasets as shown in Table 5. Typically, different datasets require\n<div style=\"text-align: center;\">Table 6: Performance comparison of different methods with sampled negative items. The best performance is highlighted old while the second best performance is underlined. The last column indicates the improvements over the best basel models and all the results of E4SRec are statistically significant with p < 0.01 compared to the best baseline models.</div>\nDataset\nMetric\nPOP\nBPR\nGRU4Rec\nCaser\nSASRec\nBERT4Rec\nE4SRec\nImprov.\nBeauty\nHR@1\n0.0678\n0.0405\n0.1337\n0.1337\n0.1870\n0.1531\n0.2274\n21.60%\nHR@5\n0.2105\n0.1461\n0.3125\n0.3032\n0.3741\n0.3640\n0.4088\n9.28%\nnDCG@5\n0.1391\n0.0934\n0.2268\n0.2219\n0.2848\n0.2622\n0.3221\n13.10%\nHR@10\n0.3386\n0.2311\n0.4106\n0.3942\n0.4696\n0.4739\n0.5068\n6.94%\nnDCG@10\n0.1803\n0.1207\n0.2584\n0.2512\n0.3156\n0.2975\n0.3503\n10.99%\nMRR\n0.1558\n0.1096\n0.2308\n0.2263\n0.2852\n0.2614\n0.3182\n11.57%\nSports\nHR@1\n0.0763\n0.0489\n0.1160\n0.1135\n0.1455\n0.1255\n0.1732\n19.04%\nHR@5\n0.2293\n0.1603\n0.3055\n0.2866\n0.3466\n0.3375\n0.3721\n7.36%\nnDCG@5\n0.1538\n0.1048\n0.2126\n0.2020\n0.2497\n0.2341\n0.2701\n8.17%\nHR@10\n0.3423\n0.2491\n0.4299\n0.4014\n0.4622\n0.4722\n0.4821\n2.10%\nnDCG@10\n0.1902\n0.1334\n0.2527\n0.2390\n0.2869\n0.2775\n0.2991\n4.25%\nMRR\n0.1660\n0.1202\n0.2191\n0.2100\n0.2520\n0.2378\n0.2675\n6.15%\nToys\nHR@1\n0.0585\n0.0257\n0.0997\n0.1114\n0.1878\n0.1262\n0.2075\n10.49%\nHR@5\n0.1977\n0.0978\n0.2795\n0.2614\n0.3682\n0.3344\n0.3908\n6.14%\nnDCG@5\n0.1286\n0.0614\n0.1919\n0.1885\n0.2820\n0.2327\n0.3115\n10.46%\nHR@10\n0.3008\n0.1715\n0.3896\n0.3540\n0.4663\n0.4493\n0.4850\n4.01%\nnDCG@10\n0.1618\n0.0850\n0.2274\n0.2183\n0.3136\n0.2698\n0.3353\n6.92%\nMRR\n0.1430\n0.0819\n0.1973\n0.1967\n0.2842\n0.2338\n0.3040\n6.97%\nYelp\nHR@1\n0.0801\n0.0624\n0.2053\n0.2188\n0.2375\n0.2405\n0.2725\n13.31%\nHR@5\n0.2415\n0.2036\n0.5437\n0.5111\n0.5745\n0.5976\n0.6202\n3.78%\nnDCG@5\n0.1622\n0.1333\n0.3784\n0.3696\n0.4113\n0.4252\n0.4532\n6.58%\nHR@10\n0.3609\n0.3153\n0.7265\n0.6661\n0.7373\n0.7597\n0.7755\n2.08%\nnDCG@10\n0.2007\n0.1692\n0.4375\n0.4198\n0.4642\n0.4778\n0.5037\n5.42%\nMRR\n0.1740\n0.1470\n0.3630\n0.3595\n0.3927\n0.4026\n0.4306\n6.95%\ndifferent training epochs and learning rates to achieve better performance (usually 2-5 epochs is enough) while the number of warmup steps is also important. The batch size is better to be 16 while larger or smaller batch size may lead to failure in convergence. E4SRec is not so sensitive to the rank and alpha of LoRA but changing the dropout or modules of LoRA will lead to significant performance fluctuation. We choose to use the cosine scheduler with a weight decay of 0.1 to control the learning rate in training process.\n# C ADDITIONAL RESULTS\nIn addition to the main results, we also perform an evaluation with negative sampled items following the strategy of previous works [18, 51, 52]. Specifically, we adopt HR@1 (nDCG@1=HR@1), HR@5, HR@10, nDCG@5, nDCG@10 and MRR as evaluation metrics considering the much shorter candidate lists. As illustrated in Table 6, our proposed E4SRec can still exceed all the baseline models with a significant margin. Such results prove the superiority of E4SRec on the ranking ability.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of integrating Large Language Models (LLMs) with traditional recommender systems that rely on unique IDs for user and item representation. Previous methods have struggled with low efficiency, out-of-range results, and the inability to effectively model IDs, necessitating a new solution to enhance the practicality of LLMs in recommendation tasks.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of effectively utilizing LLMs for sequential recommendations, particularly in modeling item IDs without losing collaborative information.",
            "key obstacle": "The main difficulty lies in the incapacity of existing LLM-based methods to handle IDs, leading to out-of-range results and inefficiencies in generating recommendations."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to combine the rich modeling capabilities of LLMs with the traditional ID-based recommendation approaches, allowing for better handling of sequential data.",
            "opinion": "The proposed E4SRec method integrates LLMs with ID-based recommendation systems, ensuring that ID sequences are used as inputs to generate valid recommendations.",
            "innovation": "E4SRec innovates by allowing LLMs to process ID sequences directly and generate an entire ranking list in one forward pass, overcoming the limitations of previous methods that generated results one at a time."
        },
        "method": {
            "method name": "E4SRec",
            "method abbreviation": "E4SRec",
            "method definition": "E4SRec is a solution that combines LLMs with traditional sequential recommendation models by injecting ID embeddings and generating recommendations efficiently.",
            "method description": "E4SRec utilizes ID sequences as inputs to generate recommendations over all candidates in a single forward process.",
            "method steps": [
                "Pretrain a traditional sequential recommendation model to obtain ID embeddings.",
                "Tune the LLM to follow specific instructions.",
                "Train the E4SRec model using the ID embeddings and tuned LLM.",
                "Deploy the E4SRec model for practical use."
            ],
            "principle": "The effectiveness of E4SRec stems from its ability to model collaborative information through ID embeddings while leveraging the generative capabilities of LLMs for accurate recommendations."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on four real-world datasets: Beauty, Sports, Toys, and Yelp, comparing E4SRec against various baseline methods.",
            "evaluation method": "Performance was assessed using metrics such as Hit Ratio (HR@k) and normalized Discounted Cumulative Gain (nDCG@k) across different k values, ensuring robust evaluation."
        },
        "conclusion": "The experiments demonstrate that E4SRec significantly outperforms existing methods, achieving better recommendation accuracy and efficiency, while effectively integrating LLMs with ID-based recommendation strategies.",
        "discussion": {
            "advantage": "E4SRec stands out due to its ability to handle IDs, generate recommendations efficiently in one pass, and maintain high performance across various datasets.",
            "limitation": "The method may still face challenges in scenarios with extremely sparse data or where additional contextual information is necessary for better recommendations.",
            "future work": "Future research will explore extending E4SRec to other recommendation tasks and refining its capabilities to further enhance generative recommendations."
        },
        "other info": {
            "implementation code": "Available at https://github.com/HestiaSky/E4SRec/",
            "datasets": {
                "Beauty": {
                    "users": 22363,
                    "items": 12101,
                    "actions": 198502,
                    "sparsity": "99.93%"
                },
                "Sports": {
                    "users": 25598,
                    "items": 18357,
                    "actions": 296337,
                    "sparsity": "99.95%"
                },
                "Toys": {
                    "users": 19412,
                    "items": 11924,
                    "actions": 167597,
                    "sparsity": "99.93%"
                },
                "Yelp": {
                    "users": 30431,
                    "items": 20033,
                    "actions": 316354,
                    "sparsity": "99.95%"
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.2",
            "key information": "E4SRec integrates LLMs with ID-based recommendation systems, ensuring that ID sequences are used as inputs to generate valid recommendations."
        },
        {
            "section number": "3.3",
            "key information": "E4SRec allows LLMs to process ID sequences directly and generate an entire ranking list in one forward pass, overcoming the limitations of previous methods that generated results one at a time."
        },
        {
            "section number": "2.3",
            "key information": "The paper addresses the issue of integrating LLMs with traditional recommender systems that rely on unique IDs for user and item representation."
        },
        {
            "section number": "10.1",
            "key information": "The main difficulty lies in the incapacity of existing LLM-based methods to handle IDs, leading to out-of-range results and inefficiencies in generating recommendations."
        },
        {
            "section number": "7.1",
            "key information": "E4SRec is a solution that combines LLMs with traditional sequential recommendation models by injecting ID embeddings and generating recommendations efficiently."
        }
    ],
    "similarity_score": 0.7503329128386761,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/E4SRec_ An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation.json"
}