{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2302.10888",
    "title": "Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy",
    "abstract": "Learning meaningful protein representation is important for a variety of biological downstream tasks such as structure-based drug design. Having witnessed the success of protein sequence pretraining, pretraining for structural data which is more informative has become a promising research topic. However, there are three major challenges facing protein structure pretraining: insufficient sample diversity, physically unrealistic modeling, and the lack of protein-specific pretext tasks. To try to address these challenges, we present the 3D Geometric Pretraining. In this paper, we propose a unified framework for protein pretraining and a 3D geometric-based, data-efficient, and protein-specific pretext task: RefineDiff (Refine the Diffused Protein Structure Decoy). After pretraining our geometric-aware model with this task on limited data(less than 1% of SOTA models), we obtained informative protein representations that can achieve comparable performance for various downstream tasks.",
    "bib_name": "huang2023dataefficientprotein3dgeometric",
    "md_text": "# Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy\nYufei Huang * 1 2 Lirong Wu * 1 2 Haitao Lin 1 2 Jiangbin Zheng 1 2 Ge Wang 1 2 Stan Z. Li 2\n# Abstract\nLearning meaningful protein representation is important for a variety of biological downstream tasks such as structure-based drug design. Having witnessed the success of protein sequence pretraining, pretraining for structural data which is more informative has become a promising research topic. However, there are three major challenges facing protein structure pretraining: insufficient sample diversity, physically unrealistic modeling, and the lack of protein-specific pretext tasks. To try to address these challenges, we present the 3D Geometric Pretraining. In this paper, we propose a unified framework for protein pretraining and a 3D geometric-based, data-efficient, and protein-specific pretext task: RefineDiff (Refine the Diffused Protein Structure Decoy). After pretraining our geometric-aware model with this task on limited data(less than 1% of SOTA models), we obtained informative protein representations that can achieve comparable performance for various downstream tasks.\narXiv:2302.10888v1\n# 1. Introduction\nProteins are involved in various important life processes, such as immune response and DNA replication, so understanding proteins is important for deciphering the mystery of life and treating various diseases.(Jumper et al., 2021) With the advancement of deep learning technology and the accumulation of protein data, the use of deep learning to explore the unknown protein space has gained increasing attention, with one of the keys being the acquisition of informationrich protein representations (Jumper et al., 2021; Meier et al., 2021; Rao et al., 2021). Protein sequence pretraining has been a huge success in the last few years. Researchers have migrated the powerful techniques of sequence pretraining in\n*Equal contribution 1Department of Computer Science, Zhejiang University, Hangzhou, China 2AI Research and Innovation Lab, Westlake University, Hangzhou, China. Correspondence to: Stan Z. Li <Stan.ZQ.Li@westlake.edu.cn>.\nnatural language processing to protein sequence modeling, pretraining amazing protein sequence representation models with increasingly large data (up to hundreds of millions) and increasingly large models (as large as 15B parameters)(Rao et al., 2020; 2021; Lin et al., 2022b). Whereas protein structure determines function, Protein structure data contains more information than simple sequence data, and inspired by the success of sequence pretraining, structure pretraining became a natural idea. The emergence of high-precision structure prediction tools in recent years has enabled researchers to obtain large amounts of new structural data with reasonable confidence, which has led the idea of structure pretraining to reality.(Zhang et al., 2022; Chen et al., 2022; Cheng et al., 2021) Pretraining usually consists of three elements: data, model, and pretext task. Protein features or properties can be broadly classified as scalars (e.g., amino acid type, angle, the distance between amino acids) and vectors (e.g., coordinates of protein atoms, the orientation of amino acids, etc.). Rethinking the development of protein pretraining in conjunction with the above two aspects, we find that: (1) Phase I: Big Data, model and pretext task are in 1D: in the beginning, protein sequence pretraining models were trained using huge amounts of data (hundreds of millions of data), based on a non-geometry-aware Transformer, using scalar properties prediction on protein sequences as pretext task (i.e., tasks such as amino acid type prediction)(Rives et al., 2019; Rao et al., 2020; 2021; Lin et al., 2022b). (2) Phase II: Much less data, model and pretext task are in 2D: Subsequent protein structure pretraining models are trained using much less data (millions or so), based on a 2D Graph Neural Network(GNN), using scalar attribute prediction on protein structures as pretext task (i.e., tasks such as distance prediction, dihedral angle prediction, etc.)(Zhang et al., 2022; Hermosilla & Ropinski, 2022; Cheng et al., 2021). Currently, 2D protein structure pretraining has made considerable progress but still faces three major problems. (1) Data Homogeneity: The gains from expanding the pretraining dataset can rapidly decline while the costs increase significantly. This is partly due to the severe lack of data diversity in large structure datasets when modeling\nin 2D.(Zhang et al., 2022; Hsu et al., 2022) (2) Physically unrealistic modeling: For example, in Distance Map prediction, the distance given by the model may violate the triangle inequality(Jumper et al., 2021; Zhou et al., 2022; Lin et al., 2022a) which indicates a lack of inductive bias. (3) limited pretext tasks: The 2D geometric pretraining constrains the pretext task to the framework of 2D GNN pretraining while not being able to explore protein-specific tasks(typically in 3D) with structure biology insights(Wu et al., 2022b). (3) Phase III: Towards pretraining in 3D, costly to train yet data-efficient: To solve the problems above, pretraining in 3D is a promising direction although there have been few attempts in 3D geometric pretraining. Meanwhile, 3D Geometric representation models(Jing et al., 2020; Hsu et al., 2022; Jumper et al., 2021) of proteins and 3D Geometric pretraining in molecules(Zhou et al., 2022) are developing rapidly recently. Considering the current dilemmas, the trends in the field, and the models available, we propose 3D Geometric Pretraining on proteins. In this paper, we propose a data-efficient, 3D geometricbased, and protein-specific pretext task named RefineDiff (Refine the Diffused protein Decoy). Due to the limitation of computational resources, we pretrain an orientation-aware roto-translation invariant transformer(Jumper et al., 2021) on limited data (1% of SOTA structure pretrain models) using this pretraining task, but it achieved comparable results on various downstream tasks. Our main contributions can be summarized as follows: \u2022 We present a unified framework for protein pretraining and representation learning that provides a clear overview of previous work and reveals promising future directions. \u2022 We propose a novel, data-efficient, and protein-specific pretext task for 3D Geometric pretraining. \u2022 We pretrained an orientation-aware roto-translation invariant transformer on limited data, and systematically tested and compared its performance on various downstream tasks. The results initially reveal the power of 3D Geometric pretraining.\n# 2. Related Work\nProtein Structure Refinement. Protein Structure Refinement(PSR) is a long-standing problem in computational structural biology. This task inspired us to propose RefineDiff as a protein 3D geometric pretraining task. The PSR Challenge is to increase the accuracy of the predicted protein structure or known as Decoy(Hiranuma et al., 2021). In the past, PSR methods were mainly based on molecular dynamics simulations as well as conformational sampling, which tried to approximate the true folding energy landscape in terms of empirical force fields and statistics(Adiyaman\n& McGuffin, 2019). Subsequent deep learning methods have attempted to directly approximate the energy landscape(Roney & Ovchinnikov, 2022). Before Alphafold2, deep learning was typically combined with traditional methods to model at the pseudo-3D level. Using neural networks to guide or directly optimize the protein scalar feature Distance Map, on top of which tools such as Rosetta were used to sample lower energy conformations(Hiranuma et al., 2021; Jing et al., 2020). Following Alphafold2, the field shifted to direct modelling in 3D, using equivariant neural networks to directly obtain atomic-level optimization results(Wu & Cheng, 2022). DDPM for Protein Structure. Denoising Diffusion Probability Model (DDPM)(Sohl-Dickstein et al., 2015; Ho et al., 2020), a recently developed generative model, has been applied to protein 3D structure generation. Its forward diffusion process inspired RefineDiff. Unusually, the first pieces of work are the direct generation of 3D structures(not pseudo-3D features)(Anand & Achim, 2022), and until now, it\u2019s been the 3D generation that dominated. Subsequent work has seen further developments in application areas, forward diffusion processes(Luo et al., 2022), sampling(Trippe et al., 2022), conditional generation(Ingraham et al., 2022), network structures(Watson et al., 2022), etc. Efficient DDPM based on pseudo-3D features (like distance maps(Lee & Kim, 2022), and torsion angles(Wu et al., 2022a)) have also emerged during this period. Recently, RFDiffusion(Watson et al., 2022) reveals the connection between denoising models and structure prediction models, and Chroma(Ingraham et al., 2022) significantly developed the protein conditional generation model. Protein Pretraining. With the development of protein representation models, pretraining was naturally introduced into the field. The first great successes were protein sequence pretraining. The Evolutionary Scale Modeling (ESM) family of models has achieved remarkable success in difficult downstream tasks such as mutation prediction(Meier et al., 2021) and protein structure prediction models(Hsu et al., 2022). With the development of structure prediction tools as well as GNN pretraining, structure pretraining has taken off, and several recent works have advanced the development of pseudo-3D structure pretraining. They typically downscale 3D structures into scalar features on 2D GNNs and migrate the pretext task of 2D GNN Pretraining(Zhang et al., 2022; Hermosilla & Ropinski, 2022; Chen et al., 2022). Thanks to the development of geometric deep learning, end-to-end 3D pretraining started a limited exploration(Mansoor et al., 2021), facing the difficulty of insufficient 3D pretraining tasks. The 3D molecular pretraining model Uni-Mol(Zhou et al., 2022) has shown powerful representation capabilities, and we believe that protein 3D pretraining is an equally promising direction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e10a/e10a62e6-1051-487b-bdff-8f7e156ae757.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. A unified framework for protein pretraining. The part framed by the blue line is the direction worth noting. 1D means protein sequence, 2D means 2D protein graph which is based on KNN, 3D means modeling in 3D space or 3D geometric aware</div>\n# 3. A Unified Framework for Protein Pretraining\n3.1. Notations and Problem Statement\nProtein data can be modeled at multiple levels: sequence, amino acid level, full atom level, etc. Here we model proteins uniformly as an Attributed Relational Graph: G = (V, E, N, R), where V represents the ordered set of graph nodes (can be amino acids or atoms) and E \u2208V \u00d7 V represents the corresponding set of edges connecting the nodes (some relationship between nodes, e.g., distance less than 4 \u02daA). every vertex v \u2208V in G can have both scalar and vector attributes nv = (Sv, Vv) \u2208N, where Sv \u2208RS and Vv \u2208R3\u00d7V . Similarly, each edge e \u2208E have attributes rv = (Se, Ve) \u2208R, where Se \u2208RN and Ve \u2208R3\u00d7T . G can contain empty sets. When the sets E and R are empty sets, G degenerates to a single sequence representation. Furthermore, if N contains only amino acid composition, G degenerates to the amino acid sequence.\nProtein Pretraining usually consists of three elements: data D, representation model f\u03b8(\u00b7), and pretext tasks as losses\nD\u00b7 {LI pre(\u03b8, D), LII pre(\u03b8, D), \u00b7 \u00b7 \u00b7 , LT pre(\u03b8, D)}. Each task corresponds to a specific projection head {gT (\u00b7)}T T =1. Protein pretraining is generally performed in two steps: (1) Pretraining the representation model f\u03b8(\u00b7) with (a) pretext task(s) in pretraining dataset Dpre; and (2) Fine-tuning the pretrained representation model f\u03b8pre(\u00b7) with a prediction head gdownstream(\u00b7) under the supervision of a specific downstream task Ltask(\u03b8, Dtask). The whole process can be formulated as\n(1)\n\ufffd where {\u03bbk}K k=1 are task weight hyperparameters. In partic-\nular, if we set K = 1 and L(1) pre(\u03b8, D) = Ltask(\u03b8, Dtask), it is equivalent to train from scratch on a downstream task like many previous works on protein representation, which can be considered a special case of our framework.\n# 3.2. Equivariant and Invariant\nThe scalar and vector attributes we mentioned before should be strictly defined as invariant or equivariant attribute under rotation or translation of the protein. Formally, f : R3 \u2192 RS is an invariant function w.r.t SE(3) group, if for any rotation and translation transformation in the group, which is represented by R as orthogonal matrices and t \u2208R3 respectively, f(Rx + t) = f(x). Attributes generated by invariant function are invariant and scalar. If f : R3 \u2192R3 is an equivariant w.r.t. SE(3) group, then f(Rx + t) = Rf(x)+t. Attributes generated by equivariant function are equivariant and vector. f(\u00b7) can be neural networks, and definitions still apply.\n# 3.3. A Unified Framework for Protein Pretraining\nAlthough protein pretraining is growing fast, it is growing wildly, unlike NLP and graph representation learning, where there are relatively clear frameworks and directions for development. Therefore, we hope to integrate previous works into a clear framework, rethink their essential contributions, and hopefully provide valuable suggestions for the further development of the field. Here, We present a unified framework for protein pretraining, considering data, model architecture, and pretext tasks. Data. After the boom of sequence pretraining, protein pretraining is shifting from sequence-based to structure-based. However, there is still a gap in the quantity and quality of structural data compared to sequence data, so there are also explorations to combine sequence and structural in-\nAlthough protein pretraining is growing fast, it is growing wildly, unlike NLP and graph representation learning, where there are relatively clear frameworks and directions for development. Therefore, we hope to integrate previous works into a clear framework, rethink their essential contributions, and hopefully provide valuable suggestions for the further development of the field. Here, We present a unified framework for protein pretraining, considering data, model architecture, and pretext tasks.\nformation. Meanwhile, pretraining is gradually evolving from generic to specialized, such as antibody sequence pretraining(Ruffolo et al., 2021) and protein pocket structure pretraining(Zhou et al., 2022). Model Architecture. The model architecture has changed with the data and has evolved from sequence transformer, GNN, to ENN, and then to co-modeling. Co-modeling currently focuses on a) initializing sequence pretraining model embedding as structural model features; b) modeling sequence and structure with different networks (basically, sequence representation as structure node feature or pair representation as sequence attention bias). Pretext Task. The development of pretext task has also witnessed several stages: a) 1D: Mask Language Modeling and its variants, supervised task based on evolutionary information such as multiple sequence matching and protein families b) 2D: Migrated 2D GNN pretraining task (node attribute prediction, edge attribute prediction), pretraining task based on contrast learning c) 1D+2D: Mask Inverse Folding d) 3D: Coordinate Denoising. Future Direction. Sequence-based pretraining is maturing, but structure-based pretraining is just beginning. Promising future directions include a) domain-specific pretraining; b) methods to better exploit large-scale predictive structure data; c) efficient neural networks; d) novel architecture on 3D graph data; e) 3D pretext task design; f) co-modeling pretraining (via network architecture, especially via pretext tasks). These advances may be related, and many have substantial implications for general deep learning. Based on this framework, we discovered that while data and models are available for 3D Geometric Pretraining, an appropriate pretext task is absent. Therefore, we propose RefineDiff.\nFuture Direction. Sequence-based pretraining is maturing, but structure-based pretraining is just beginning. Promising future directions include a) domain-specific pretraining; b) methods to better exploit large-scale predictive structure data; c) efficient neural networks; d) novel architecture on 3D graph data; e) 3D pretext task design; f) co-modeling pretraining (via network architecture, especially via pretext tasks). These advances may be related, and many have substantial implications for general deep learning. Based on this framework, we discovered that while data and models are available for 3D Geometric Pretraining, an appropriate pretext task is absent. Therefore, we propose RefineDiff.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d154/d154fbb7-8d8b-4d35-b879-85d46d82b599.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. A illustration of protein folding energy landscape(Dill & MacCallum, 2012) 4. Methodology</div>\n# Figure 2. A illustration of protein folding energy landscape(Dill & MacCallum, 2012) 4. Methodology\nOne of the primary benefits of 3D Geometric Pretraining is physical-level realistic modeling that can learn from the original dimension of the data without losing information due to dimensionality reduction. Furthermore, it enables us to think about physically meaningful pretext tasks and learn generic representations while solving real-world physical challenges. So we started to consider the physics-based\ntasks of protein structural biology, and we found Protein Structure Refinement (PSR). The challenge of PSR is to estimate the folding energy landscape and the direction of energy minimization (expressed via coordinates shift), which is also the source and direction of the folding dynamics.\nis also the source and direction of the folding dynamics. PSR naturally becomes a potential choice for the pretext task. We hope that the model will learn information about the protein folding energy landscape through this task and thus obtain a more descriptive representation of the protein folding state for a variety of downstream tasks. However, the direct use of PSR as the pretext task tends to make the representation model quickly fall into the local optimum. We found in the pretraining process that when the accuracy of the predicted structure reached a certain level, it was difficult for the model to optimize it further; continuing pretraining at this point also resulted in little or no improvement in performance on downstream tasks 5.4. The main reason for this is that the protein decoys with reasonable accuracy are positioned at a local minimum of the protein folding landscape, and the neural network struggles to overcome the energy barrier to find conformations with lower energies. As a result, the representation model fails to comprehend a complete energy landscape, and the representation of the protein folding state is unable to become more accurate, failing to produce a more meaningful representation of the protein structure. Taking a step deeper, we can further understand this optimal representation problem from the perspective of information bottleneck theory following (Dubois et al., 2020). Information Bottleneck (IB) is an information-theoretic method that explains deep representation learning under supervision. IB is based on the idea that, in order to prevent potential over-fitting, a representation Z should be as highly informative about Y as possible (sufficient) while including no additional information about X (minimum). The sets of sufficient representations S and minimal sufficient representations M are defined as follows:\n(2)\n\u2208S To obtain optimal representation, the IB criterion which representation models learn to minimize can be formulated as the Lagrangian relaxation of Eq.2:\n(3)\nIn the setting of PSR, mutual information I[Y ; Z] corresponds to the similarity S[Y ; D] where Z is the decoded protein structure decoy from representation Z using the best possible structure decoder. So we can rewrite the Eq.3 as:\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2617/2617d353-d090-4ccb-9aa3-e45755982562.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3. The motivation and general process for RefineDiff.</div>\nWhen the input X is structurally similar to the label Y (i.e., structure decoys have acceptable accuracy), S[Y ; D] and S[X; D] will be more likely to have the same monotonicity with respect to Z, leading LIB into a dilemma of difficult trade-offs which block the learning of structure representation model. To solve the above problem of PSR, we perform a forward diffusion process on the protein structure decoys to help them jump out of the local optimum before the structure refinement step, and finally propose a novel 3D Geometric protein pretext task named RefineDiff: Refine Diffused Structure Decoy. The forward diffusion process of protein structure decoys can help them escape from the local minima of the folding energy landscape in which they were initially located and assist the model in finding the following local minima with lower energy, i.e., possibly closer to the conformation of the real structure. In this process, the model is able to see a complete energy landscape and obtain a more accurate representation of the folding state, resulting in a more generalized and informative representation. Additionally, from the viewpoint of information bottleneck theory, the forward diffusion process on the protein structure decoy reduces the similarity between inputs and labels, and when the Structure Refinement step is carried out afterward, the model is unable to avoid learning by memorization of the inputs. This can help alleviate the previously mentioned dilemma of difficult trade-offs, help reduce the mutual information between the learned representations and inputs, increase the mutual information between representations and labels, and finally produce more general and powerful representations.\nWhen the input X is structurally similar to the label Y (i.e., structure decoys have acceptable accuracy), S[Y ; D] and S[X; D] will be more likely to have the same monotonicity with respect to Z, leading LIB into a dilemma of difficult trade-offs which block the learning of structure representation model.\nTo solve the above problem of PSR, we perform a forward diffusion process on the protein structure decoys to help them jump out of the local optimum before the structure refinement step, and finally propose a novel 3D Geometric protein pretext task named RefineDiff: Refine Diffused Structure Decoy.\n# 4.2. RefineDiff: Refine the Diffused Protein Structure Decoy\nWe propose a novel, data-efficient, and protein-specific pretext task for 3D Geometric Pretraining named RefineDiff. It consists of two steps, a forward diffusion process for structure decoys, followed by a protein refinement step. The combination of these two steps allows structure decoys to jump out of the local minima of the folding energy landscape and enables the representation model to gain deeper understanding of the protein folding space without prematurely falling into local optima, resulting in generalized representations that are applicable to various downstream tasks.\nInstead of reducing the amino acid molecule to a single C\u03b1 atom, we pursued a finer-grained modeling, so we considered all the amino acid backbone atoms (C, N, O, C\u03b1). Considering physical plausibility (the bond lengths and bond angles between backbone atoms are relatively fixed, i.e. the relative positions between backbone atoms can be considered fixed), we modeled the backbone atoms as a frame, such that an amino acid backbone structure is determined by two vector properties: translation t (coordinates of the C\u03b1 atom) and orientation O (which determines the final coordinates):\nwhere P is the protein backbone structure, N is the length of the protein sequence, xa i is the a-type backbone atom of residue i (a \u2208C, N, O, C\u03b1). Oxa lit is the coordinate of the a-type atom when the C\u03b1 atom is at the origin and under the unit orthogonal group, which is the normalized coordinate of the amino acid backbone atom(Jumper et al., 2021).\nwhere P is the protein backbone structure, N is the length of the protein sequence, xa i is the a-type backbone atom of residue i (a \u2208C, N, O, C\u03b1). Oxa lit is the coordinate of the a-type atom when the C\u03b1 atom is at the origin and under the unit orthogonal group, which is the normalized coordinate of the amino acid backbone atom(Jumper et al., 2021). At this point, to perturb the conformation of the structure decoys, both t \u2208R3 and O \u2208SO(3) vectors need to be\nAt this point, to perturb the conformation of the structure decoys, both t \u2208R3 and O \u2208SO(3) vectors need to be\nprocessed. t is a regular continuous variable that can be perturbed by adding Gaussian noise:\n\ufffd where T represents the diffusion time step which is sampled randomly from [1, T] in practice, \u03b2T pos means the variance schedule which controls the rate of diffusion and its value increases from 0 to 1 as timp step goes from 0 to t. \u00af\u03b1T pos = \ufffdt T =1(1 \u2212\u03b2T pos). However, O belongs to the SO(3) group, and we cannot add simple Gaussian noise to it. Therefore we use the method in SE(3) diffusion(Leach et al., 2022; Luo et al., 2022) to directly perform the forward Gaussian process for the variable O \u2208SO(3).\n\ufffd However, O belongs to the SO(3) group, and we cannot add simple Gaussian noise to it. Therefore we use the method in SE(3) diffusion(Leach et al., 2022; Luo et al., 2022) to directly perform the forward Gaussian process for the variable O \u2208SO(3).\n (7)\n\ufffd IGSO(3) denotes the isotropic Gaussian distribution on SO(3) parameterized by a mean rotation and a scalar variance(Leach et al., 2022). \u03bb(\u03b3, x) = exp(\u03b3 log(x)) is the geodesic flow from I to x by the amount of \u03b3.\nTogether, these two processes constitute the forward diffusion process of RefineDiff.\n4.2.2. THE STRUCTURE REFINEMENT STEP\nThe goal of the Protein Refinement step is to teach the representation model to comprehend the protein folding space and the folding energy landscape. For this, the model must be able to forecast the direction of conformational optimization, or the direction in which the system will minimize its energy, based on the model\u2019s learned understanding of the folding energy landscape and the current conformations of structure decoys that have already undergone the forward diffusion process.\nGiven a protein structure decoy P that has undergone a forward diffusion process, the model needs to predict the direction of conformational change for further optimization, including translation change \u2206t \u2208R3 and orientation change \u2206O \u2208SO(3) for every residue. We can formulate this process as:\n(8)\nwhere E is the equivariant neural network and \u25e6corresponds to the composition of elements in SO(3) group. We repeat the above process several times for further optimization. In 3D space, even the same protein structure may differ significantly in coordinates due to global translation or rotation. Therefore structure decoy and ground truth structure\nwhere E is the equivariant neural network and \u25e6corresponds to the composition of elements in SO(3) group. We repeat the above process several times for further optimization.\nneed to be aligned with each other first. To avoid the timeconsuming process of structural alignment, we use the MSE loss under the local frame(Jumper et al., 2021) as the training objective:\n(9)\nwhere i \u2208{1, \u00b7 \u00b7 \u00b7 , Nres}, j \u2208{C, N, O, C\u03b1}, Ti and T true i corresponds to the predicted and ground truth frame (O, t) of residue i. And T \u22121 i \u25e6xj = O\u22121(xj \u2212ti) converts the coordinates of backbone atoms from global to local frame. Since we predict the frame of each residue separately, the directly connected residues may be too far apart, which is not physically realistic. Therefore, we set up an additional loss function to help the model learn the inter-residue bonds(peptide bonds):\nwhere li pred is the bond length in a predicted structure and li lit is the idealized peptide bond length. Nbonds is the number of bonds in the structure, and r is the tolerance threshold.\n4.2.3. THE LINK BETWEEN REFINEDIFF AND SCORE MATCHING\nRFDifussion(Watson et al., 2022) illustrates the relationship between protein structure refinement and protein structure generation. A good protein structure refinement model can be naturally used as a denoiser in the protein structure DDPM framework, where the generative process can be formulated as follows:\n(11)\nwhere \u00b5\u03b8(\u00b7) is a protein refinement model that removes the standard Gaussian noise \u03f5j \u223cN(0, I) added to real structures. The same holds for denoising orientation O. And at this time their training objectives can be both written in the form of denoising score matching:\nHere E is an equivariant neural network that predicts the standard Gaussian noise added to the real structure. As seen above, RefineDiff can be naturally associated with denoising score matching when we perform the forward diffusion process and the PSR step on the real structure. An important difference between the RefineDiff task and the above scenario is that alignment is required between the\nstructure decoys and the ground truth structure, which leads to the difficulty of calculating score matching. However, we can make a natural generalization of RefineDiff so that it can be performed on the real structure where we replace structure decoys with real structures in the original RefineDiff task. This eliminates the need for structure decoys construction and allows us to obtain both the representation model and the generation model at the same time, like works in 3D point clouds(Luo & Hu, 2021). We leave this generalization for future work.\n# 4.3. Model Architecture\nIn this subsection, we will briefly describe the neural network architecture used in the pretraining process and in the downstream tasks. As shown in section 3.3, the model architecture includes a 3D geometry-aware representation network as well as 3D prediction heads for 3D geometric pretraining and task-specific projection heads for downstream tasks. First, we employ Multiple Layer Perceptrons(MLPs) to generate sequential and pairwise embeddings for protein structure decoys. The sequential embedding MLPs map residue features containing information of amino acid type, torsion angle, and 3D coordinates of amino acid backbone atoms to embedding vectors {mi}Nres i=1 . And the pairwise embedding MLPs convert the geometric distance map and dihedral angle map(Dauparas et al., 2022) into feature vectors {zij}Nres i,j=1. Next, we interactively update the sequential embedding and pairwise embedding then fuse the two into a single sequence representation {si}Nres i=1 using shallow layers of sequential attention and triangle attention(Lin et al., 2022b; Jumper et al., 2021). Furthermore, we adopt an orientation-aware roto-translation invariant transformer(Luo et al., 2022; Jumper et al., 2021) to encode {si} and the 3D geometric information of current protein decoy structure P into hidden representations, because the nature of the proteins will not be changed by translation or rotation and protein structure representations should be invariant under the rotation or translation of protein structures. Finally, the representations are fed into corresponding MLPs as 3D prediction heads to predict SO(3) vector and translation vector tlocal in local frame. We obtain \u2206O by converting SO(3) vector to rotation matrix and calculate \u2206t in the global frame by \u2206t = Ocurtlocal. The frame-based update method ensures that the model is equivariant to global translation and rotation.\nIn this subsection, we will briefly describe the neural network architecture used in the pretraining process and in the downstream tasks. As shown in section 3.3, the model architecture includes a 3D geometry-aware representation network as well as 3D prediction heads for 3D geometric pretraining and task-specific projection heads for downstream tasks.\n# 5. Experiments\nIn this section, we first introduce the experimental setup for both pretraining and downstream tasks. We then show the performance of the pretrained model on protein structure refinement and standard downstream tasks, including Enzyme\nCommission number prediction, and Gene Ontology term prediction following (Gligorijevi\u00b4c et al., 2021).\n# 5.1. Experimental Setup\nPretraining Dataset. We use the DeepAccNet protein structure decoy dataset(Hiranuma et al., 2021) for pretraining. This dataset contains 7992 protein targets (retrieved from the PISCES server and deposited to PDB by May 2018) with limited sequence length (50-300), maximum sequence redundancy of 40% and minimum resolution of 2.5 \u02daA. Since our pretraining task has an actual physical scenario, we use the dataset of the CASP14 Protein Refinement track(Simpkin et al., 2021) as a test set to examine the performance of the pretrained model on the protein refinement task.\nDownstream Tasks. We adopt four tasks proposed in (Gligorijevi\u00b4c et al., 2021) as downstream tasks for evaluation. Enzyme Commission (EC) number prediction aims to forecast the EC numbers of various proteins, which describe their catalysis of biological activities in a tree structure. It\u2019s a multi-label classification task with 538 categories. Gene Ontology(GO) term prediction aims to annotate a protein with GO terms which describe the Molecular Function (MF) of gene products, the Biological Processes (BP) in which those actions occur and the Cellular Component (CC) where they are present. Thus, GO term prediction is actually composed of three different sub-tasks: GO-MF, GO-BP, GO-CC.\nBaselines and Training. We evaluate our model on the benchmark proposed by (Zhang et al., 2022; Gligorijevi\u00b4c et al., 2021) and compare our encoder with various baselines including sequence based encoder( ResNet, LSTM, Transformer(Rao et al., 2019)), structure-based encoders( GVP(Jing et al., 2020), GraphQA(Baldassarre et al., 2021), New IEconv(Hermosilla & Ropinski, 2022)), large scale pretrained model( ESM-1b(Rives et al., 2019) and GearNet(Zhang et al., 2022)) and co-modeling encoder( DeepFRI(Gligorijevi\u00b4c et al., 2021) and LM-GVP(Wang et al., 2021)). Following (Gligorijevi\u00b4c et al., 2021), we use the multi-cutoff split methods for EC and GO tasks to guarantee the test set contains only PDB chains with sequence identity less than 95% to the training set. We pretrain our encoder using RefineDiff for 150 epochs and then fine-tune it on downstream tasks for 100 epochs. And We train our model on downstream tasks for 100 epochs from scratch. All training is performed on 2 Tesla V100 GPUs. Evaluation. For downstream tasks, we use the proteincentric maximum F-score Fmax as metric following (Zhang et al., 2022). Besides, we evaluate performance on PSR with GDT(Zemla, 2003) and lDDT(Mariani et al., 2013),\n<div style=\"text-align: center;\">Table 1. Performance on the AlphaFold2 refinement models in CASP14. The performance of our method is highlighted boldly</div>\nTable 2. Fmax on EC and GO following benchmark propose by (Zhang et al., 2022), where bold and underline denote the best and second metrics. We use our pretext task name to represent pretrained model.\nMethod\nGDT-HA\u2191\nGDT-TS \u2191\nlDDT\u2191\nStarting\n70.13\n85.56\n80.84\nGNNRefine\n-3.84\n-2.14\n-2.76\nGNNRefine-plus\n-2.69\n-1.49\n-2.24\nDeepAccNet\n-5.61\n-4.69\n-5.26\nRefineDiff\n+0.09\n+0.14\n+0.12\nCategory\nMethod\nPretraining\nDataset(Size)\nEC\nGO\nBP\nMF\nCC\nTrain from Scratch\n1D-based\nResNet\n-\n0.605\n0.280\n0.405\n0.304\nLSTM\n-\n0.425\n0.225\n0.321\n0.283\nTransformer\n-\n0.238\n0.264\n0.211\n0.405\n2D-based\nGraphQA\n-\n0.509\n0.308\n0.329\n0.413\nNew IEConv\n-\n0.735\n0.374\n0.544\n0.444\nGearNet\n-\n0.810\n0.400\n0.581\n0.430\n3D-based\nGVP\n-\n0.489\n0.326\n0.426\n0.420\nIPAFormer(ours)\n-\n0.783\n0.410\n0.583\n0.455\nPretrain and Finetune\n1D + 2D\nDeepFRI\nPfam (10M)\n0.631\n0.399\n0.465\n0.460\nLM-GVP\nURf100 (216M)\n0.664\n0.417\n0.545\n0.527\n1D to 2D\nESM-1b\nUniRef50 (24M)\n0.864\n0.452\n0.657\n0.477\nGearNet\nAFDB (805K)\n0.874\n0.490\n0.654\n0.488\n3D\nRefineDiff(ours)\nDADB (8K)\n0.865\n0.455\n0.648\n0.528\n# 5.2. Performance on Protein Refinement Task\nSince AlphaFold2 is currently able to achieve relatively high accuracy predictions, the challenge lies in optimizing the results of AlphaFold2. Therefore we evaluate our pretrained model on AlphaFold2 refinement models on CASP14 which is the gold standard in the field of protein structure prediction. From the above table, it can be seen that the structure decoy predicted by AlphaFold2 is deep in the local minima, so after the optimization of other models, the accuracy has decreased instead. However, the model pretrained by RefineDiff can continue to optimize on this basis, although it is not optimized much due to the limitation of model capability. This shows that Protein Structure Refinement is learning the protein folding landscape, and when faced with a conformation that is at a local minimum, it is difficult for the model to find further optimization in the critical domain, but instead it pushes the conformation to a worse direction. The forward diffusion process used by RefineDiff helps the conformation escape from the local optimum, and thus allows the model to optimize further.\n<div style=\"text-align: center;\">Table 3. Ablation studies on pretraining and downstream task EC. \u2206GDT = GDTrefine \u2212GDTprev and \u2206GDT \u2208[0, 1].</div>\n# 5.3. Performance on Downstream Tasks\nWe compared with multiple types of baselines, including sequence-based models, structure-based models, comodeling models and pretrained models on protein sequences as well as on protein structure. Some baselines and tasks are missing due to the computational burden and the lack of codes. We do not include MSA-based baselines since these evolution-based methods require a lot of resources to search and store protein homology sequences following (Zhang et al., 2022). Here, GearNet is the short for GearNet-Edge-IEConv pretrained with Multi-view Contrastive which is the model with generally best performance in their original paper. Our pretrained model has a large improvement on multiple downstream tasks compared to models trained from scratch on downstream tasks. In addition, We achieved SOTA on the GO-CC task and second best on the GO-BP and EC tasks, but it is worth noting that we used less than 10,000 protein targets, which is 1% of GearNet which are based on protein 2D geometric pretraining. The above results illustrate that our proposed pretext task and 3D geometric pretraining is data-efficient, powerful and promising.\n# 5.4. Ablation Studies\n \u2212 \u2208\nMethod\n\u2206GDT\nFmax\nIPAFormer\n0.12\n0.783\n- w/ single layer of triangle attention\n0.02\n0.791\nIPAFormer (RefineDiff)\n0.12\n0.865\n- pretrain with PSR directly\n0.00\n0.824\n- use early checkpoints\n0.01\n0.847\n- w/ single layer of triangle attention\n0.02\n0.853\nIn this section, we compare the model that directly uses the protein structure refinement as the pretext task with the model that uses RefineDiff for pretraining. From the table above, we can see that it is difficult for the model to further optimize the protein structure decoy when trained directly using protein structure refinement, and the improvement in performance is limited when migrating to downstream tasks compared to pretraining with RefineDiff. Interestingly, despite the fact that early in the training of RefineDiff the capability of pretrained model for the protein structure decoy refinement is not that strong , the model using RefineDiff as the pretraining task still achieves better performance on the downstream task than the model trained directly using PSR as the pretraining task. This suggests that even if the model does not learn RefineDiff well due to insufficient training or ability, it can still benefit from seeing a more complete energy landscape compared to using PSR directly as a pretraining task.\nInterestingly, despite the fact that early in the training of RefineDiff the capability of pretrained model for the protein structure decoy refinement is not that strong , the model using RefineDiff as the pretraining task still achieves better performance on the downstream task than the model trained directly using PSR as the pretraining task. This suggests that even if the model does not learn RefineDiff well due to insufficient training or ability, it can still benefit from seeing a more complete energy landscape compared to using PSR directly as a pretraining task.\n# 6. Conclusion\nIn this paper, we present the development of protein pretraining and propose a unified framework; then we analyze the\ncurrent problems of 2D protein structure pretraining (data homogeneity, physically unrealistic modeling and limited pretext task), and point out the need for 3D geometric pretraining on proteins. We take the pretext task as a starting point and propose a data-efficient and protein-specific pretext task for 3D geometric pretraining on proteins. After pretraining our geometric-aware model with this task on limited data(less than 1% of SOTA models), we obtained informative protein representations that can achieve comparable performance for various downstream tasks.\n# References\nAdiyaman, R. and McGuffin, L. J. Methods for the refinement of protein structure 3d models. International journal of molecular sciences, 20(9):2301, 2019.\nProtein Model Quality Assessment. Some work uses neural networks to predict the accuracy of Decoy (which is a twin problem of PSR called Model Quality Assessment, MQA) as a way to guide conformational sampling toward the lowest point of folding energy(Hiranuma et al., 2021). Other works model at the pseudo-3D level, starting from the scalar feature Distance Map of the protein, optimizing the prediction of the Distance Map by a neural network such as GNN, and then using tools such as Rosetta to perform conformational sampling based on the optimized Distance Map(Jing & Xu, 2021). Protein Representation Model. Protein representation models are one of the core challenges in applying deep learning to protein understanding, and they evolve as deep learning does(Wu et al., 2022b). Initially, researchers attempted to represent proteins using convolutional neural networks, which were typically based on handcrafted feature maps(Yang et al., 2022; Derevyanko et al., 2018; Amidi et al., 2018); however, with the advancement of Natural Language Processing(NLP) and the similarities between amino acid sequences and natural language, powerful NLP models such as Transformer were introduced to protein representation(Rao et al., 2020; 2021; Lin et al., 2022b). Simultaneously, graph neural networks were applied to protein structure representation(Baldassarre et al., 2021; Gligorijevi\u00b4c et al., 2021), with promising results in fields such as protein structure design(Ingraham et al., 2019; Dauparas et al., 2022). With the rise of geometric deep learning, Equivariant Neural Networks(ENN) began to be applied to protein representation, allowing us to directly learn protein structures end-to-end and achieve more powerful results(Hsu et al., 2022; Jing et al., 2020; Wu & Cheng, 2022). AlphaFold2 is likely the pinnacle of these techniques, revolutionizing structural biology and becoming one of the best instances of AI applications. Inspired by this, many works have tried to combine sequence and structure representation, expecting that co-modeling can combine the advantages of both modalities.(Lin et al., 2022b; Mansoor et al., 2021; You & Shen, 2022)\nProtein Representation Model. Protein representation models are one of the core challenges in applying deep learning to protein understanding, and they evolve as deep learning does(Wu et al., 2022b). Initially, researchers attempted to represent proteins using convolutional neural networks, which were typically based on handcrafted feature maps(Yang et al., 2022; Derevyanko et al., 2018; Amidi et al., 2018); however, with the advancement of Natural Language Processing(NLP) and the similarities between amino acid sequences and natural language, powerful NLP models such as Transformer were introduced to protein representation(Rao et al., 2020; 2021; Lin et al., 2022b). Simultaneously, graph neural networks were applied to protein structure representation(Baldassarre et al., 2021; Gligorijevi\u00b4c et al., 2021), with promising results in fields such as protein structure design(Ingraham et al., 2019; Dauparas et al., 2022). With the rise of geometric deep learning, Equivariant Neural Networks(ENN) began to be applied to protein representation, allowing us to directly learn protein structures end-to-end and achieve more powerful results(Hsu et al., 2022; Jing et al., 2020; Wu & Cheng, 2022). AlphaFold2 is likely the pinnacle of these techniques, revolutionizing structural biology and becoming one of the best instances of AI applications. Inspired by this, many works have tried to combine sequence and structure representation, expecting that co-modeling can combine the advantages of both modalities.(Lin et al., 2022b; Mansoor et al., 2021; You & Shen, 2022)\n# A.2. More Analysis\nSince the current structure pretraining does not directly model 3D structural data, the structural features involved are scala so 2D Graph-based GNN can be used, which is why we call it pseudo-3D geometric pretraining. There have been few attempts in 3D geometric pretraining.(Mansoor et al., 2021)\nCompared with the sequence pretraining model, the 2D geometry pretraining model handles more complex data, and it training efficiency is reduced by orders of magnitude(Zhang et al., 2022). However, by constructing structural scalar feature and using structure-related scalar prediction tasks, it successfully achieves geometry awareness, so it can achieve comparabl results with sequence pretraining with much less data (1% of sequence pretraining), i.e., data-efficient(Mansoor et al., 2021\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenges of protein structure pretraining, including insufficient sample diversity, physically unrealistic modeling, and a lack of protein-specific pretext tasks, highlighting the necessity for a new method to improve performance in structural data representation.",
        "problem": {
            "definition": "The main issue is the inadequacy of existing protein structure pretraining methods to effectively utilize structural data for learning informative protein representations.",
            "key obstacle": "The core challenge is the limited diversity in existing datasets, leading to a plateau in performance and the inability to explore protein-specific tasks effectively."
        },
        "idea": {
            "intuition": "The idea stems from recognizing the limitations of current 2D protein structure models and the potential benefits of 3D geometric representations.",
            "opinion": "The proposed method, RefineDiff, aims to refine protein structure decoys through a forward diffusion process, enhancing the learning of protein folding landscapes.",
            "innovation": "The key innovation lies in the introduction of a 3D geometric-based, data-efficient pretext task that allows the model to escape local minima in the protein folding landscape."
        },
        "method": {
            "method name": "RefineDiff",
            "method abbreviation": "RD",
            "method definition": "RefineDiff is a two-step pretext task that involves a forward diffusion process followed by a refinement step for protein structure decoys.",
            "method description": "The method aims to enhance protein representation learning by enabling structures to escape local minima in the folding energy landscape.",
            "method steps": [
                "1. Apply a forward diffusion process to protein structure decoys.",
                "2. Perform a protein refinement step to optimize the conformational predictions."
            ],
            "principle": "The method is effective due to its ability to model physical properties of proteins and to provide a more comprehensive view of the protein folding landscape."
        },
        "experiments": {
            "evaluation setting": "The experiments used the DeepAccNet protein structure decoy dataset for pretraining and evaluated performance on the CASP14 Protein Refinement track.",
            "evaluation method": "Performance was assessed using metrics such as GDT and lDDT for structure refinement, and Fmax for downstream tasks including EC number and GO term predictions."
        },
        "conclusion": "The proposed RefineDiff method shows promise in enhancing protein representation learning and achieving competitive results on downstream tasks with significantly less data compared to state-of-the-art models.",
        "discussion": {
            "advantage": "The main advantage of RefineDiff is its ability to effectively utilize limited data while achieving comparable performance to larger models.",
            "limitation": "A limitation is the potential for the model to converge prematurely to local minima, which may hinder further optimization.",
            "future work": "Future research could explore improving the pretext task design and expanding the dataset diversity to enhance model performance."
        },
        "other info": {
            "info1": "The method was pretrained on less than 1% of the data used by state-of-the-art models.",
            "info2": {
                "info2.1": "The model architecture includes a 3D geometry-aware representation network.",
                "info2.2": "Evaluation metrics included protein-centric maximum F-score for downstream tasks."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "5",
            "key information": "The proposed RefineDiff method shows promise in enhancing protein representation learning and achieving competitive results on downstream tasks with significantly less data compared to state-of-the-art models."
        },
        {
            "section number": "5.1",
            "key information": "RefineDiff is a two-step pretext task that involves a forward diffusion process followed by a refinement step for protein structure decoys."
        },
        {
            "section number": "5.2",
            "key information": "The method aims to enhance protein representation learning by enabling structures to escape local minima in the folding energy landscape."
        },
        {
            "section number": "5.4",
            "key information": "Future research could explore improving the pretext task design and expanding the dataset diversity to enhance model performance."
        },
        {
            "section number": "2.1",
            "key information": "The main issue is the inadequacy of existing protein structure pretraining methods to effectively utilize structural data for learning informative protein representations."
        },
        {
            "section number": "2.2",
            "key information": "The core challenge is the limited diversity in existing datasets, leading to a plateau in performance and the inability to explore protein-specific tasks effectively."
        }
    ],
    "similarity_score": 0.5519494222382141,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy.json"
}