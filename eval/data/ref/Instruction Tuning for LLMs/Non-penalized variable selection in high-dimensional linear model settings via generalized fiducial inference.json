{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1702.07283",
    "title": "Non-penalized variable selection in high-dimensional linear model settings via generalized fiducial inference",
    "abstract": "Standard penalized methods of variable selection and parameter estimation rely on the magnitude of coefficient estimates to decide which variables to include in the final model. However, coefficient estimates are unreliable when the design matrix is collinear. To overcome this challenge an entirely new perspective on variable selection is presented within a generalized fiducial inference framework. This new procedure is able to effectively account for linear dependencies among subsets of covariates in a high-dimensional setting where $p$ can grow almost exponentially in $n$, as well as in the classical setting where $p \\le n$. It is shown that the procedure very naturally assigns small probabilities to subsets of covariates which include redundancies by way of explicit $L_{0}$ minimization. Furthermore, with a typical sparsity assumption, it is shown that the proposed method is consistent in the sense that the probability of the true sparse subset of covariates converges in probability to 1 as $n \\to \\infty$, or as $n \\to \\infty$ and $p \\to \\infty$. Very reasonable conditions are needed, and little restriction is placed on the class of possible subsets of covariates to achieve this consistency result.",
    "bib_name": "williams2018nonpenalizedvariableselectionhighdimensional",
    "md_text": "# NON-PENALIZED VARIABLE SELECTION IN HIGH-DIMENSIONAL LINEAR MODEL SETTINGS VIA GENERALIZED FIDUCIAL INFERENCE\nBy Jonathan P Williams and Jan Hannig\nUniversity of North Carolina at Chapel Hill\nStandard penalized methods of variable selection and parameter estimation rely on the magnitude of coefficient estimates to decide which variables to include in the final model. However, coefficient estimates are unreliable when the design matrix is collinear. To overcome this challenge an entirely new perspective on variable selection is presented within a generalized fiducial inference framework. This new procedure is able to effectively account for linear dependencies among subsets of covariates in a high-dimensional setting where p can grow almost exponentially in n, as well as in the classical setting where p \u2264n. It is shown that the procedure very naturally assigns small probabilities to subsets of covariates which include redundancies by way of explicit L0 minimization. Furthermore, with a typical sparsity assumption, it is shown that the proposed method is consistent in the sense that the probability of the true sparse subset of covariates converges in probability to 1 as n \u2192\u221e, or as n \u2192\u221eand p \u2192\u221e. Very reasonable conditions are needed, and little restriction is placed on the class of possible subsets of covariates to achieve this consistency result.\n1. Introduction. A strategy for developing variable selection procedures with desirable consistency properties entails exploiting some distinguishing property of the theoretical true data generating model. For example, standard penalized methods of variable selection within a linear model framework such as LASSO of Tibshirani (1996), SCAD of Fan and Li (2001), and the Dantzig Selector of Candes and Tao (2007) rely on the magnitude of the coefficients in the true data generating model being relatively larger than those of the other coefficients. Johnson and Rossell (2012) use this property to construct nonlocal prior densities over all subsets of covariates. The defining property of their nonlocal density is that it takes the value of zero for subsets containing a covariate with a zero-valued coefficient. We propose a more desirable way for eliminating redundancies from the sample space of candidate subsets which does not explicitly rely on coefficient\nMSC 2010 subject classifications: 62J05, 62F12, 62A01 Keywords and phrases: variable selection, best subset selection, high dimensional regression, L0 minimization, generalized fiducial inference\nmagnitudes. That is, any candidate true model should be non-redundant in the sense that it contains the minimal amount of information necessary for explaining and/or predicting the observed data. One such criterion to exploit this non-redundancy property is that the only subsets with nonzero posterior probability should be those which cannot be predicted to some chosen precision by a subset of fewer covariates. Such a criterion requires constructing a probability distribution on the space of candidate models, which is consistent with a Bayesian or fiducial variable selection paradigm. The literature on high-dimensional linear models is vast, but we hope to contribute to it by using this setting to build a foundation for a fresh perspective on variable selection. Recent work in the Bayesian high-dimensional linear model setting includes Ro\u02c7ckov\u00b4a and George (2016) who develop methods for separable and non-separable spike-and-slab penalized estimation, the credible set approach of Bondell and Reich (2012), and Narisetty and He (2014) who propose a method based on shrinking and diffusing coefficient priors in which the variance of the priors are sample size dependent. Lai, Hannig and Lee (2015) layout framework for penalized estimation within a GFI approach. Ghosh and Ghattas (2015) provide insights into complications in Bayesian variable selection. Namely, the size of the sample space (2p) is often too large to compute all model probabilities, and even typically larger than can reasonably be sampled by Markov chain Monte Carol (MCMC) methods. Thus, the nonlocal prior approach of Johnson and Rossell (2012) can achieve asymptotic consistency (where other approaches can only achieve pairwise consistency) because it is able to effectively eliminate a large enough portion of the 2p subsets from the sample space. To illustrate this point, consider the following simple example. Let \ufffd \ufffd\n(1)\n\ufffd \ufffd where \u03b2j \u2208R and x(j) \u2208Rn for j \u2208{1, . . . , p}, and \u03c3 > 0. Further, suppose that the true but unknown values of (\u03b21, \u03b22, \u03b23, . . . , \u03b2p)\u2032 are (b1, b2, 0, . . . , 0)\u2032. Within the nonlocal prior framework, the only subsets with non-negligible posterior probability are contained in the set \ufffd {x(1)}, {x(2)}, {x(1), x(2)} \ufffd . When viewed as a prior density on the coefficients, nonlocal priors assign zero prior density to the true parameter value when the true parameter value is zero. From a Bayesian perspective this is philosophically problematic, but very insightful for consistency of model selection. The insight lends itself to the question: What other properties might the true model have which can be exploited to develop a statistical procedure with the ability to effectively eliminate subsets from the sample space?\nIn addressing this question, we build our proposed methods from the idea that any candidate true model, as determined by the actual non-zero parameter values, should be non-redundant in the sense that it contains the minimal amount of information necessary for explaining and/or predicting the observed data. We denote such subsets of the parameter space as \u03b5admissible, and define them precisely in Definition 2.1. Then, using the above nonlocal prior example, the entire model space {x(1), x(2), x(3)} for instance, is not \u03b5-admissible because it can be perfectly predicted by the smaller subset {x(1), x(2)}. To further illustrate the intuition behind our proposal, consider an example where x(2) is highly collinear with all of x(3), . . . , x(p) but is not correlated with x(1), and that the true values of (\u03b21, \u03b22, \u03b23, . . . , \u03b2p)\u2032 are (b1, b2, b3, . . . , bp)\u2032 with bj \u0338= 0 for all j \u2208{1, . . . , p}. In this case, assuming strong enough collinearity, \u2203c \u2208R with c \u00b7 x(2) \u2248b2 \u00b7 x(2) + \u00b7 \u00b7 \u00b7 + bp \u00b7 x(p),\nIn addressing this question, we build our proposed methods from the idea that any candidate true model, as determined by the actual non-zero parameter values, should be non-redundant in the sense that it contains the minimal amount of information necessary for explaining and/or predicting the observed data. We denote such subsets of the parameter space as \u03b5admissible, and define them precisely in Definition 2.1. Then, using the above nonlocal prior example, the entire model space {x(1), x(2), x(3)} for instance, is not \u03b5-admissible because it can be perfectly predicted by the smaller subset {x(1), x(2)}. To further illustrate the intuition behind our proposal, consider an example where x(2) is highly collinear with all of x(3), . . . , x(p) but is not correlated with x(1), and that the true values of (\u03b21, \u03b22, \u03b23, . . . , \u03b2p)\u2032 are (b1, b2, b3, . . . , bp)\u2032 with bj \u0338= 0 for all j \u2208{1, . . . , p}. In this case, assuming strong enough collinearity, \u2203c \u2208R with c \u00b7 x(2) \u2248b2 \u00b7 x(2) + \u00b7 \u00b7 \u00b7 + bp \u00b7 x(p), i.e., \ufffd\ufffd\ufffd b1 \u00b7 x(1) + \u00b7 \u00b7 \u00b7 + bp \u00b7 x(p)\ufffd \u2212 \ufffd b1 \u00b7 x(1) + c \u00b7 x(2)\ufffd\ufffd\ufffd< \u03b5 where \u03b5 > 0 is some desired precision. Thus, for much of the parameter space the subset {x(1), . . . , x(p)} is not \u03b5-admissible, but would be assigned nonzero posterior probability in the nonlocal prior framework. We construct a posterior-like probability distribution over all subsets, which assigns negligible probability to elements that are not \u03b5-admissible. In constructing the posterior-like probability distribution we adopt a generalized fiducial inference (GFI) approach because it has similar to an objective Bayes interpretation with data driven priors, gives a systematic method of constructing a distribution function given a data generating equation such as a linear model, and it does not suffer from the issue of arbitrary normalizing constants which arise in many objective Bayesian priors (Berger et al., 2001). In this manuscript we will provide a gentle introduction to GFI. A fuller account of GFI is given in the recent review paper Hannig et al. (2016). An advantage of both our approach and the nonlocal prior approach of Johnson and Rossell (2012) is that in addition to providing theoretical guar-\n\ufffd\ufffd\ufffd  \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\ufffd \u2212 \ufffd  \u00b7 \u00b7\ufffd\ufffd\ufffd where \u03b5 > 0 is some desired precision. Thus, for much of the parameter space the subset {x(1), . . . , x(p)} is not \u03b5-admissible, but would be assigned nonzero posterior probability in the nonlocal prior framework. We construct a posterior-like probability distribution over all subsets, which assigns negligible probability to elements that are not \u03b5-admissible. In constructing the posterior-like probability distribution we adopt a generalized fiducial inference (GFI) approach because it has similar to an objective Bayes interpretation with data driven priors, gives a systematic method of constructing a distribution function given a data generating equation such as a linear model, and it does not suffer from the issue of arbitrary normalizing constants which arise in many objective Bayesian priors (Berger et al., 2001). In this manuscript we will provide a gentle introduction to GFI. A fuller account of GFI is given in the recent review paper Hannig et al. (2016). An advantage of both our approach and the nonlocal prior approach of Johnson and Rossell (2012) is that in addition to providing theoretical guarantees, our statistical procedures yield estimates of the posterior distribution over subsets of covariates. This is in contrast to frequentist penalization based methods or Bayesian procedures fully dedicated to maximum a posteriori probability (MAP) estimation. Such methods do not yield the posterior probability of a chosen model (i.e., the relative probability, given the observed data, of a given model against competing models). Furthermore, Ghosh and Ghattas (2015) argue that joint summaries of subsets of covariates are more robust to collinearity. With our approach to constructing a posterior-like distribution whose probability mass function value is negligible for subsets of the parameter\nspace which are not \u03b5-admissible, we are able to show that the probability of the true data generating model converges to 1 asymptotically in n and p. This consistency is shown to be true even with p growing almost exponentially in n. The reason being that the true model yields a stronger signal since it no longer has to compete within an overly redundant sample space. The paper is organized as follows. Section 2 serves to introduce the general methodology and computational algorithm for carrying out our variable selection procedure based on a recent algorithm for explicit L0 minimization (Bertsimas, King and Mazumder, 2016), which is fast enough to be used on real data. The conditions needed for the main results, and the main results are presented and discussed in Section 3. Proofs are organized in the appendix. We demonstrate the empirical performance of our procedure and compare it to other Bayesian and frequentist methods in simulation setups on synthetic data in Section 4. Computer code implementing our procedure is provided at https://jonathanpw.github.io/software.html.\n2. Methodology. As described in the previous section, our idea behind exploiting a non-redundancy property of the true data generating model relies on constructing a probability distribution concentrated on what we denote as \u03b5-admissible subsets. This object is defined precisely in Definition 2.1, but first an aside on the notation used throughout the paper. Let Y be an n-dimensional random vector, X an n\u00d7p matrix with columns scaled to have unit norm, and \u03b20 a fixed p-dimensional vector with nonzero (or active) components indexed by the subset Mo \u2282{1, . . . , p}, with \ufffd \ufffd\n(2)\n\ufffd \ufffd The design matrix denoted by XMo is defined as the matrix composed of only those columns of X corresponding to the index set Mo. The subscript \u2018o\u2019 refers to the interpretation of Mo as corresponding to the \u2018oracle\u2019 subset of covariates. Moreover, \u03b20 Mo denotes the true values of the oracle coefficients, while \u03b2Mo is understood as a random vector whose uncertainty resides in not knowing the true coefficients \u03b20 Mo. For any subset M the vector \u03b20 M refers to the projection of the column space of XM on the true coefficients \u03b20 Mo, that is, \u03b20 M = (X\u2032 MXM)\u22121X\u2032 MXMo\u03b20 Mo = Ey(\ufffd\u03b2M). Lastly, \u03c30 Mo > 0 denotes the true unknown error standard deviation, while \u03c3Mo is a random variable whose distribution expresses the uncertainty from not knowing \u03c30 Mo, under the oracle model. The objective is to construct a statistical procedure which can be shown, asymptotically and demonstrated empirically, to be able to identify Mo as the index set of the oracle model within the sample space of all 2p candidate\nsubsets M \u2282{1, . . . , p}. For each index set, M, in the sample space the conditional sampling distribution of the data is assumed as \ufffd \ufffd\n(3)\n\ufffd \ufffd The centerpiece of our methodology is then the following definition. The function | \u00b7 | denotes the absolute value function if its argument is scalarvalued, and denotes the cardinality function if its argument is set-valued. The norms \u2225\u00b7 \u22252 and \u2225\u00b7 \u22250 refer, respectfully, to the usual L2 and L0 norms defined on finite-dimensional Euclidean spaces.\nDefinition 2.1. Assume fixed \u03b5 > 0. A given \u03b2M coupled within som index subset M \u2282{1, . . . , p} is said to be \u03b5-admissible if and only if h(\u03b2M) = 1, where \ufffd \ufffd\n(4)\n\ufffd \ufffd and bmin solves min b\u2208Rp 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|M| \u22121.\n\ufffd \ufffd and bmin solves min b\u2208Rp 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|M| \u22121.\nObserve that this definition is consistent with the heuristic description of \u03b5-admissible subsets given in the previous section. In particular, if the subset of covariates indexed by M is linearly dependent or if one of the components of \u03b2M is zero, then h(\u03b2M) = 0. The subtlety in this definition is assuming an appropriately chosen \u03b5 which is able to strike an optimal balance for distinguishing signal from noise. Intuitively, \u03b5 = \u03b5(n, p, M), i.e., is a function of the amount of information available given by n, the difficulty of the problem represented by p, and information about a given M being considered such as |M|. For instance, if |M| > n then h(\u03b2M) = 0 because XM cannot have full rank. In this case any \u03b5 > 0 will work, but the choice of \u03b5 matters a lot if |M| \u2264n. The choice of \u03b5 is a major focus of Section 3 where the main results of the paper are presented, and from where we suggest the following default choice:\n(5)\n \ufffd \ufffd \ufffd where \u039bM := tr \ufffd (HMX)\u2032HMX \ufffd and \ufffd\u03c32 M := RSSM/(n\u2212|M|) with RSSM := y\u2032(In \u2212HM)y and HM := XM(X\u2032 MXM)\u22121X\u2032 M, and the vector y an observation from the true model (2). The parameter po represents prior belief about |Mo|, the number of covariates in the true model Mo. In practice, a value of po can be directly specified or selected by cross-validation. A built-in cross-validation procedure is included in the accompanying software to this paper. Details are provided with the simulation study in Section 4.\nWithin the h function in Definition 2.1 the quantity 1 2\u2225X\u2032(XM\u03b2M \u2212 Xbmin)\u22252 2 represents the difference in prediction for a subset M against all subsets with fewer covariates. This measure of distance has been adapted from Candes and Tao (2007), but they deal with the error \u2225X\u2032(y \u2212Xb)\u2225\u221e over b \u2208Rp. This is very different from using XM\u03b2M in place of y because the former results in a noiseless measure of distance. To illustrate, observe that Ey \ufffd \u2225X\u2032(Y \u2212Xb)\u22252 2 \ufffd = \u2225X\u2032(XMo\u03b2Mo \u2212Xb)\u22252 2 + \u03c32 Mo \u00b7 p, where Ey(\u00b7) is used to denote the expectation taken with respect to the sampling distribution of the data Y . There are various reasons for using the quantity X\u2032(XM\u03b2M \u2212Xb) from the Dantzig selector (Candes and Tao, 2007) versus simply the difference in predictions (XM\u03b2M \u2212Xb) as in the LASSO (Tibshirani, 1996). One reason is that X\u2032(XM\u03b2M \u2212Xb) accounts for difference in predictions as well as correlations with the explanatory data, as discussed in Berk (2008). If the difference in predictions is small but is highly correlated with the design matrix, then it is likely that the smaller subset of covariates is unable to account for the effect of one or more of the covariates in M. Thus, using X\u2032(XM\u03b2M \u2212Xb) instead of just the difference in predictions is a method of controlling for potential omitted variable effects which could incorrectly find a close fitting subset to M. Another advantage of X\u2032(XM\u03b2M \u2212Xb) is that it is invariant under orthogonal transformations of the design matrix, as pointed out in Candes and Tao (2007). Now that the foundation for \u03b5-admissible subsets of the parameter space has been laid out, it remains to show how Definition 2.1 can be coupled with a likelihood based approach for constructing a probability distribution over index subsets M \u2282{1, . . . , p}. This is a common strategy for Bayesian approaches, i.e., construct a prior density with desirable properties for variable selection and then couple the prior with a likelihood function of the data to study the resulting posterior distribution. However, it is not clear what sort of a prior to use within our \u03b5-admissible subsets approach, and recent developments in generalized fiducial inference (GFI) offer a systematic method of deriving objective Bayes-like posterior distributions. To illustrate as in Hannig et al. (2016), suppose that some data Y = G(U, \u03b8) for some deterministic data generating equation G(\u00b7, \u00b7), some parameters \u03b8, and some random component U whose distribution is independent of \u03b8 and is completely known. The generalized fiducial distribution of \u03b8 is then given by f(y, \u03b8)J(y, \u03b8)\n\ufffd \u0398  where f is the likelihood function and\n# GFI VARIABLE SELECTION\n\ufffd\ufffd\ufffd with D(A) = (det A\u2032A) 1 2 . The component J(y, \u03b8) is termed the Jacobian because it results from inverting the data generating equation on the data. We are committing a slight abuse of notation as r(\u03b8|y) is not a conditional density in the usual sense. Instead, we are using this notation to stress that the generalized fiducial distribution is a function of the observed data y. To make matters concrete in the linear model setting of (3), the parameters are \u03b8 = (\u03b2M, \u03c3M), the data generating equation is specified as G \ufffd U, (\u03b2M, \u03c3M) \ufffd = XM\u03b2M + \u03c3MU where U \u223cNn(0, In), and the Jacobian term reduces to J \ufffd y, (\u03b2M, \u03c3M) \ufffd = \u03c3\u22121 M | det(X\u2032 MXM)| 1 2 RSS 1 2 M. Thus,\n\ufffd | \ufffd \u221d  | |  \u00b7 where the factor of h(\u03b2M) appears in the likelihood from only considering \u03b5-admissible subsets of the parameter space. Accordingly, as is done with a Bayesian posterior density and in Section 3 of Hannig et al. (2016), define the GFI probability of a given subset M to be proportional to the normalizing constant of r \ufffd (\u03b2M, \u03c3M)|y \ufffd . That is, \ufffd \ufffd \ufffd \ufffd \ufffd\nwhich simplifies to\n(6)\n\ufffd \ufffd where the expectation is taken with respect to the location-scale multivariate T distribution, \ufffd \ufffd\n(7)\n\ufffd \ufffd  \u2212|| \ufffd with \ufffd\u03b2M := (X\u2032 MXM)\u22121X\u2032 My. Notice that the quantity E(h(\u03b2M)) is a function of the observed data y. Observe that (6) expresses the relative likelihood of the subset M over all 2p possible subsets. The expression can be described as a product of two terms, the first being comprised of information from the sampling distribution of the data and largely driven by the residual sum of squares, RSSM, and the second having to do with the \u03b5-admissibility of \u03b2M, in the\nform of E(h(\u03b2M)). Thus, the support of r(M|y) in (6) is dominated by the \u03b5-admissible subsets, as desired. Section 3 provides the conditions and supporting lemmas and theorems needed to show that r(Mo|Y ) \u21921 in probability as n, p \u2192\u221e. First however, a few remarks are provided about computing r(M|y) on actual data.\n2.1. Remarks on computation. With a probability distribution now defined over \u03b5-admissible subsets, it must be demonstrated that r(M|y) in (6) can be efficiently computed. There are two main computational issues to deal with. The first is to evaluate h(\u03b2M) for a given \u03b2M, and the second is to sample subsets M via pseudo-marginal based MCMC. The computational complexity and the need for pseudo-marginal based MCMC arises because neither h(\u03b2M) nor E(h(\u03b2M)) have a closed form solution. To evaluate h(\u03b2M) for a given \u03b2M we adapt an explicit L0 minimization algorithm introduced in Bertsimas, King and Mazumder (2016). The authors state that their algorithm borrows ideas from projected gradient descent and methods in first-order convex optimization, and solves problems of the form minb\u2208Rp g(b) subject to \u2225b\u22250 \u2264\u03ba, where g(b) \u22650 is convex and has Lipschitz continuous gradient: \u2225\u2207g(b) \u2212\u2207g(\ufffdb)\u22252 \u2264l\u2225b \u2212\ufffdb\u22252. The algorithm is not guaranteed to find a global optimum (unless formal optimality tests are run, which can take a long time), but Bertsimas, King and Mazumder (2016) provide provable guarantees that the algorithm will converge to a first-order stationary point, which is defined as a vector \u02dcb \u2208Rp with \u2225\u02dcb\u22250 \u2264\u03ba which satisfies \u02dcb = \u02dcb\u22121 l \u2207g(\u02dcb). Paraphrasing from Bertsimas, King and Mazumder (2016), their algorithm detects the active set after a few iterations, and then takes additional time to estimate the coefficient values to a high accuracy level. In our application of their algorithm we are not first-most interested in finding a global optimum. To evaluate h(\u03b2M), we need only determine if minb\u2208Rp 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2 is smaller than \u03b5 (as in (5)). For \u03b2M which are not \u03b5-admissible, the objective function, 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2, can be made small very quickly via our implementation of the L0 minimization algorithm. To illustrate how consider the following specifics of our implementation. The precise details regarding the algorithm can be found accompanying our software documentation at https://jonathanpw.github.io/software.html. First, to estimate E(h(\u03b2M)) we use a sample mean of sample vectors drawn from the location-scale multivariate T distribution in (7). This multivariate T distribution is centered at the least squares estimator, \ufffd\u03b2M, and multivariate theory suggests that \ufffd\u03b2M will on average be close to the coefficients \u03b20 M. By warm starting the L0 minimization algorithm at \ufffd\u03b2M with the smallest coefficient removed, subsets corresponding to \u03b20 M with at least one\nzero coefficient typically yield h(\u03b2M) = 0 within a few steps of the algorithm. Second, as per the definition of h(\u00b7) in (4) the objective function is minimized over all b \u2208Rp with \u2225b\u22250 \u2264|M| \u22121. Hence, the \u03ba required for the L0 minimization algorithm from Bertsimas, King and Mazumder (2016) is naturally chosen for us as \u03ba = |M|\u22121. Knowing how to choose \u03ba greatly reduces the L0 optimization problem. Moreover, our implementation is further simplified by the fact that the closest prediction to XM\u03b2M for a given M is guaranteed to have |M| \u22121 covariates. Accordingly, the objective function in h(\u03b2M) need not be minimized over all b \u2208Rp with \u2225b\u22250 \u2264|M| \u22121, but can be minimized over all b \u2208Rp with \u2225b\u22250 = |M| \u22121. The second computational issue is to sample subsets M via pseudomarginal based MCMC. We do this by using the Grouped Independence Metropolis Hastings (GIMH) algorithm from Andrieu and Roberts (2009), but originally introduced in Beaumont (2003). The reason standard MCMC techniques do not apply is that there is no obvious closed form expression for the probability mass function (6) because of the expectation, E(h(\u03b2M)), in the expression. As described in Andrieu and Roberts (2009) such situations warrant introducing a latent variable to yield analytical expressions or easier implementation. In the case of r(M|y) in (6), we introduce the latent location-scale multivariate T vector in (7) from within the expectation E(h(\u03b2M)). Our pseudomarginal based MCMC is carried out by sampling an index subset M along with sampling some pre-specified number, N, of multivariate T vectors (corresponding to M) from (7). The sample of multivariate T vectors, say B, is then used to compute the sample mean estimate of E(h(\u03b2M)). Accordingly, we define a joint Markov chain on (M, B), but discard B to obtain samples from the marginal distribution of M. As argued in Andrieu and Roberts (2009), this is a valid MCMC sampling strategy, but is known to suffer from slower mixing than if we were able to integrate the \u03b2M out of the mass function r(M|y) in (6), i.e., analytically evaluate E(h(\u03b2M)). However, this is not possible given the h function in (4). Additionally, the mixing associated with pseudo-marginal approaches is known to be poor when the number of importance samples (N, the sample size of B) is small. These practical bottlenecks outline avenues for future research. Nonetheless, we demonstrate in Section 4 that our computational strategies are efficient enough to be implemented on actual data, in comparison to other common penalized likelihood and Bayesian approaches.\n3. Theoretical results. The main objective of this section is to show under what conditions, asymptotically, r(Mo|Y ) in (6) will converge to 1,\nparticularly if p >> n. The \u03b5-admissible subsets approach is able to achieve such a strong consistency result because the resulting sample space is effectively reduced to only those subsets with no redundancies. The essence of the mathematical result is that the space of \u03b5-admissible sets is small enough that the true model can be detected. This addresses the issue raised in Ghosh and Ghattas (2015) that high-dimensional settings often lead to arbitrarily small probabilities for all models (including the true model) simply because there are too many models to consider.\nparticularly if p >> n. The \u03b5-admissible subsets approach is able to achieve such a strong consistency result because the resulting sample space is effectively reduced to only those subsets with no redundancies. The essence of the mathematical result is that the space of \u03b5-admissible sets is small enough that the true model can be detected. This addresses the issue raised in Ghosh and Ghattas (2015) that high-dimensional settings often lead to arbitrarily small probabilities for all models (including the true model) simply because there are too many models to consider. 3.1. Discussion of the conditions. The first two conditions, Condition 3.1 and Condition 3.2, are to ensure that the true model, Mo, is identifiable. Observe from (4) that \u03b5 is used to control the sensitivity and specificity for identifying \u03b5-admissible subsets. In particular, if \u03b5 is too large, then h(\u03b2Mo) will incorrectly be set to zero implying that \u03b2Mo is not \u03b5-admissible. Condition 3.1 specifies how large \u03b5 can be whilst the true model remains identifiable. This condition turns out to be critically important in actual data applications because computing h(\u03b2Mo) is closely related to the comparison in equation (8).\nbecause there are too many models to consider. 3.1. Discussion of the conditions. The first two conditions, Condition 3.1 and Condition 3.2, are to ensure that the true model, Mo, is identifiable. Observe from (4) that \u03b5 is used to control the sensitivity and specificity for identifying \u03b5-admissible subsets. In particular, if \u03b5 is too large, then h(\u03b2Mo) will incorrectly be set to zero implying that \u03b2Mo is not \u03b5-admissible. Condition 3.1 specifies how large \u03b5 can be whilst the true model remains identifiable. This condition turns out to be critically important in actual data applications because computing h(\u03b2Mo) is closely related to the comparison in equation (8).\n(8)\nwhere bmin solves min b\u2208Rp 1 2\u2225X\u2032(XMo\u03b20 Mo \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|Mo| \u22121.\nwhere bmin solves min b\u2208Rp 1 2\u2225X\u2032(XMo\u03b20 Mo \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|Mo| \u22121.\nCondition 3.2 is born from Lemma A.1 which is an important necessary result for the main result of this paper, Theorem 3.9. The term log(n)\u03b3 represents the sparsity assumption for the true model, i.e., the number of covariates in the true model must not exceed log(n)\u03b3 for some fixed scalar \u03b3 > 0. The \u03b3 parameter indicates that the asymptotic results remain true if the true model grows faster than log(n), but not faster than some power of log(n). In finite-sample applications, \u03b3 has no consequence and can be ignored. The constant \u03b1 \u2208(0, 1) reflects the only explicit restriction needed on the sample space of 2p subsets to show that r(Mo|Y ) \u21921 in probability for large n and p, Theorem 3.9. The residual sum of squares term in r(M|Y ) in (6) cannot be controlled (as a ratio to r(Mo|Y )) for arbitrary subsets M with |M| = O(n) because the column span of XM includes y \u2208Rn when rank(XM) = n. To eliminate such subsets from the sample space, Condition 3.2 requires that only subsets of size |M| \u2264n\u03b1 can be given nonzero probability. However, recall from Definition 2.1 that h(\u03b2M) = 0 if\n|M| > n because in this case the columns of XM must be linearly dependent. Accordingly, all subsets M with |M| > n are given zero probability, by definition. Evidenced by this fact, the only explicit restriction placed on the sample space is that subsets M with |M| \u2208(n\u03b1, n) are excluded. In sparse settings it is assumed that |Mo| << n anyway, so neglecting such subsets is reasonable. Convergence to the true model Mo will be quicker for smaller \u03b1 because there are less models to consider, but too small of an \u03b1 will exclude Mo from the sample space. In Condition 3.2, and for the remainder of this section assume that \u03b3 > 0, say \u03b3 = 1, and \u03b1 \u2208(0, 1), say \u03b1 = .5, have been chosen and fixed at appropriate values.\n\u2192\u221e for large n and p, where \u2206M := \u2225XMo\u03b20 Mo\u2212HMXMo\u03b20 Mo\u22252 2 as in Lai, Hannig\n\u2192\u221e for large n and p, where \u2206M := \u2225XMo\u03b20 Mo\u2212HMXMo\u03b20 Mo\u22252 2 as in Lai, Hann and Lee (2015).\nThis is a slightly weaker version of condition (11) in Lai, Hannig and Lee (2015). They relate Condition 3.2 to the sparse Riesz condition (Zhang and Huang, 2008) which requires that the eigenvalues of X\u2032 MXM/n are uniformly bounded away from 0 and \u221e. Essentially, \u2206M is a measure of how distinct the true model predictions XMo\u03b20 Mo are from their projection onto the column space of XM for M \u0338= Mo and |M| \u2264|Mo|. Recall that HM := XM(X\u2032 MXM)\u22121X\u2032 M. In particular, if XM is orthogonal to XMo, then \u2206M = \u2225XMo\u03b20 Mo\u22252 2 which will be much larger than the denominator, |Mo| log(p). The requirements of this condition are reasonable because \u2206M grows very fast for M such that Mo \u0338\u2286M. Condition 3.2 is important for being able to identify the true model amongst other models M with |M| \u2264|Mo|. The next two conditions address the requirements for M with |M| > |Mo|, which primarily rely on the fact that such subsets are not \u03b5-admissible. Conditions 3.3 and 3.4 demonstrate how large \u03b5 needs to be to achieve the consistency result of the main theorem. Condition 3.3 states that for subsets of covariates with redundancies, \u03b5 needs to be larger than the difference in projections of the true model prediction, XMo\u03b20 Mo, onto M and onto a strict subset of M. This condition facilitates the intuition that the variable selection procedure will not concentrate on subsets M with redundant covariates.\n12\nIf a given subset M is not \u03b5-admissible, then the difference in projections will be small so that the condition is easily achieved.\n  where HM(\u22121) is the projection matrix for M after omitting the covariate which minimizes \u2225X\u2032(HM \u2212HM(\u22121))XMo\u03b20 Mo\u22252 2.\nIn fact, if Mo \u2282M with |Mo| < |M|, then HMXMo\u03b20 Mo = HM(\u22121)XMo\u03b20 Mo in which case Condition 3.3 holds trivially. Lastly, Condition 3.4 describes the rate at which \u03b5 needs to grow to achieve the consistency of the main result. The distinction between Condition 3.3 and Condition 3.4 is that the former provides a necessary lower bound for arguing that E(h(\u03b2M)) vanishes for M such that |M| > |Mo|, while the latter provides the rate at which \u03b5 must grow to achieve the consistency result of Theorem 3.9. The terms which compete with \u03b5 arise in the proofs of Lemma A.1 and Theorem 3.9. Recall that \ufffd\u03c32 M := RSSM/(n\u2212|M|), where RSSM is the classical residual sum of squares for model M, and that \u039bM := tr \ufffd (HMX)\u2032HMX \ufffd . Condition 3.4.\nwhere\n\u03d5(M, n, p) := 4e2n\u03b1 + (D1 + (1 + 4e2) log(p))|M| + log \ufffd 1 \n\ufffd \ufffd and D1 = 1 2 log \ufffd 6\u03c0 1\u2212n\u03b1+2 n \ufffd . Additionally, \u03b5 9\u039bM \ufffd\u03c32 M < n\u2212|M| 2 for all M wit |M| \u2264n\u03b1.\nand D1 = 1 2 log \ufffd 6\u03c0 1\u2212n\u03b1+2 n \ufffd . Additionally, \u03b5 9\u039bM \ufffd\u03c32 M < n\u2212|M| 2 for all M with \u03b1\nThe \u039bM term arises from Lemmas 3.5 and 3.6. It is intimately related to the presence of collinearity amongst the covariates, and Condition 3.4 implies that \u03b5 must account for collinearity by controlling for \u039bM. Observe that if X is orthogonal, then \u039bM = |M|.\n3.2. Main result. The first two results are lemmas which are needed in the proofs of Theorems 3.7 and 3.8. Lemma 3.5 illustrates the rate at which \u03b2M concentrates around its mean, \ufffd\u03b2M, the least squares estimator, and Lemma 3.6 illustrates the rate at which \ufffd\u03b2M concentrates around its mean,\nEy(\ufffd\u03b2M). Theorem 3.7 uses these two lemmas to bound the rate at which \u03b2M concentrates around Ey(\ufffd\u03b2M) for subsets M with |M| > |Mo|. This yields an upper bound on E(h(\u03b2M)) with a probabilistic guarantee, and implies that E(h(\u03b2M)) vanishes for large n and p, for large non-\u03b5-admissible subsets. The proofs are relegated to Appendix A.\nLemma 3.5. For any fixed c1 \u2208(0, 1) assume |M| \u2264c1n, and choose n and p such that \u03b5 9\u039bM \ufffd\u03c32 M < n\u2212|M| 2 . If\n\ufffd \ufffd where \ufffd\u03b2M = (X\u2032 MXM)\u22121X\u2032 My, then\n\ufffd \ufffd In the next lemma, Py is used to denote the probability measure associated with the sampling distribution of the data Y .\nLemma 3.6. Assume |M| < n, and Y |\u03b2M, \u03c32 M \u223cNn \ufffd XM\u03b2M, \u03c32 MIn \ufffd . Then the classical least squares estimator \ufffd\u03b2M \u223cN(Ey(\ufffd\u03b2M), \u03c32 M(X\u2032 MXM)\u22121), and \u221a\n\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd Combining these two lemmas gives the following non-asymptotic concentration result for models that are larger than the true model. Recall that the expectation E(h(\u03b2M)) depends on the observed data y. The following two theorems study the frequentist behavior of this quantity with respect to the sampling distribution of Y . Theorem 3.7. For any fixed c1 \u2208(0, 1) suppose |Mo| < |M| \u2264c1n, choose n and p such that \u03b5 9\u039bM \ufffd\u03c32 M < n\u2212|M| 2 , and assume that \u03b5 satisfies Condition 3.3. Then\n\ufffd The next result is a probabilistic guarantee the true model is \u03b5-admissible given \u03b5 satisfies Condition 3.1. This result is a statement that Mo is identi fiable.\n\ufffd The next result is a probabilistic guarantee the true model is \u03b5-admissible given \u03b5 satisfies Condition 3.1. This result is a statement that Mo is identi-\nTheorem 3.8. For any fixed c1 \u2208(0, 1) suppose |Mo| \u2264c1n, choose n and p such that \u03b5 9\u039bMo \ufffd\u03c32 Mo < n\u2212|Mo| 2 , and assume that \u03b5 satisfies Condition\n  The following result is the main result of the paper. It shows that the ratio of the generalized fiducial probability of the true model to the sum over that of all other subsets of covariates M satisfying |M| \u2264n\u03b1 will converge to 1 in probability for large n and p. Note that the restriction to subsets M with |M| \u2264n\u03b1 is a stronger restriction than |M| \u2264c1n, which is sufficient for Theorems 3.7 and 3.8. The reason being that the main result is stronger than the results of these two theorems. In fact, Theorems 3.7 and 3.8 are non-asymptotic results that hold for each fixed model M separately, while Theorem 3.9 is an asymptotic result which applies uniformly over all |M| \u2264n\u03b1. Just like with a conditional distribution, r(M|Y ) is obtained by replacing the observed data y with the random variable Y (random with respect to the sampling distribution), in (6).\nTheorem 3.9. Given Conditions 3.1-3.4, the true model Mo satisfies r(Mo|Y ) \ufffd \ufffd Py \u2212\u21921\n\ufffd as n \u2192\u221eor n, p \u2192\u221e.\nAlthough this is an asymptotic result, many of the ingredients that are used in its proof are non-asymptotic concentration results which are valid if Conditions 3.1-3.4 are satisfied. Therefore, it can be expected that when these conditions are satisfied, in finite-sample situations the generalized fiducial distribution will concentrate on the true model Mo. This expectation is indeed validated by the empirical performance of the procedure, which is demonstrated in the following section.\n4. Simulation results. This section serves to demonstrate the empirical performance of our algorithm on synthetic data. It is comprised of essentially two simulation setups. The first setup, similar to that presented in Johnson and Rossell (2012), compares our procedure to the nonlocal prior approach, the spike and slab LASSO of Ro\u02c7ckov\u00b4a and George (2016), the elastic net as implementated in the Python module scikit-learn (Pedregosa et al., 2011), and to the SCAD as implementated in the R package ncvreg (Breheny and Huang, 2011). The authors of the nonlocal prior and the spike\nand slab LASSO, respectively, have made available the R packages mombf and SSL for implementing their methods. The second setup illustrates a critical difference between our \u03b5-admissible subsets approach and the nonlocal prior approach. Namely, for highly collinear finite-sample settings in which the true model is not uniquely expressed, given the level of noise in the data (i.e., \u03c30 Mo), we demonstrate that our approach concentrates (in the sense of the MAP estimator) on subsets with fewer covariates without sacrificing prediction error. The intuition for why this should be the case was discussed in Section 1.\n4.1. Simulation setup 1. Here we generate 2000 data vectors y according to model (2) with Mo consisting of 8 covariates corresponding to \u03b20 Mo = (\u22121.5, \u22121, \u2212.8, \u2212.6, .6, .8, 1, 1.5)\u2032, and \u03c30 Mo = 1. The n \u00d7 p design matrix X is generated with rows from the Np(0, \u03a3) distribution, where the diagonal components \u03a3ii = 1 and the off-diagonal components \u03a3ij = \u03c1 for i \u0338= j. The first 1000 y correspond to an independent design with \u03c1 = 0, while the last 1000 y correspond to \u03c1 = .25, as in the simulation setup of Johnson and Rossell (2012). Note that 2000 design matrices X are generated, and one y is generated from each design. The sample size n is set at n = 100, and p = 100, 200, 300, 400, 500 are all considered. We implement our algorithm on each of the 2000 synthetic data sets for 15000 MCMC steps with the first 5000 discarded. Squared coefficient estimates from elastic net (using scikit-learn) added by n\u22122 serve as MCMC covariate proposal weights. The default \u03b5 in (5) is used, and we implemented a 10-fold cross-validation scheme for choosing our tuning parameter po (prior to starting the algorithm). The cross-validation consists of breaking the data into 10 folds (with a different set of 10 observations held out at each fold since n = 100), and implementing our MCMC algorithm separately for each po in the grid {1, 2, . . . , 10}, on each of the 10 training sets. Each of the 10 implementations of the MCMC on each of the 10 training sets is run for 200 steps with the first 100 steps discarded (N = 30 is set during the crossvalidation procedure). Squared nonzero coefficient estimates from elastic net (using scikit-learn) serve as MCMC covariate proposal weights within the cross-validation procedure. The MAP estimated subset for each MCMC chain is used to compute the Bayesian information criterion (BIC) on the held-out test set, and the computed BIC values are averaged over the 10 test sets, for each po \u2208{1, 2, . . . , 10}. The po corresponding to the minimum average test set BIC is then selected. Finally, for our implementation of the algorithm post-selection of po, the number of importance samples for estimating E(h(\u03b2M)) within each step of\nthe algorithm is set at N = 100 which, through empirical experimentation, seems to be enough. All competing variable selection procedures are implemented using existing software at default specifications. The one exception is that the nonlocal prior procedure is set to run for 5000 steps, as is the case in the simulation setup of Johnson and Rossell (2012). The nonlocal prior procedure/software did not scale well for increased p, and required over a weeks worth of parallel computations on a computing cluster to obtain the results for the first simulation setup. The tuning parameters for all methods are chosen with the default cross-validation procedures provided with the software. Lastly, as in the simulation section for Ro\u02c7ckov\u00b4a and George (2016) their \u03bb1 is set at 1 (with a grid of 20 \u03bb0 values ending at 50), and their adaptive (best performing) procedure is used with \u03b8 \u223cBeta(1, p). Figure 1 shows results of the first simulation setup. The first row of plots displays the average generalized fiducial probability of the true model (i.e., average r(Mo|y)), or the average posterior probability of the true model for the Bayesian nonlocal procedure (i.e., average P(Mo|y)), over the 1000 synthetic data sets (for \u03c1 = 0 and \u03c1 = .25, respectively). Conditional on the data, these plots address the consistency of the procedures with respect to the uncertainty from not knowing Mo. This generalized fiducial or Bayesianlike consistency is that which is dealt with in Theorem 3.9. Note that frequentist and MAP estimators do not yield posterior probability estimates, and thus cannot be compared to in the first row of plots. The second row of Figure 1 shows the average proportion of correct model selections over the 1000 synthetic data sets (for \u03c1 = 0 and \u03c1 = .25, respectively). For the GFI and the Bayesian procedures the MAP subset is taken to be the estimator of the true model, and in the frequentist procedures the estimated model is considered to be the subset of covariates with nonzero coefficient estimates. These plots address the consistency of the procedures with respect to repeated sampling (i.e., frequentist) uncertainty. Finally, the third row of Figure 1 presents the average root mean squared error (RMSE) over the 1000 synthetic data sets (for \u03c1 = 0 and \u03c1 = .25, respectively). The MAP estimated model is used to compute the RMSE for the GFI and Bayesian procedures. For all procedures, the RMSE is computed on an outof-sample test set of 100 observations. Note that the criterions used in the first two rows of Figure 1 are very strict. They only reflect instances when the procedures are exactly correct, and count the procedure as incorrect if it is missing even one covariate from the true model or includes even one spurious covariate. Often the elastic net and SCAD are able to identify all of the true covariates but estimate extra coefficients to be nonzero. This results in poor identification of the true\nsubset, and worse out-of-sample prediction error. The remaining procedures (including our \u03b5-admissible subsets method) struggle to identify the two covariates with smallest coefficient magnitudes, but typically do not introduce more than one or two false positives.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79e8/79e85894-3151-4181-9163-44b673ba0802.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig 1. The average r(Mo|y), or average P(Mo|y) is displayed in the first row, the average proportion of correct model selections in the second row, and the average RMSE in the third. Averages are over 1000 synthetic data sets (for \u03c1 = 0 and \u03c1 = .25, respectively). For the GFI and Bayesian procedures the MAP subset is used as the estimator of the true model, and in the frequentist procedures the estimated model is considered to be the subset of covariates with nonzero coefficient estimates. The RMSE is computed on an out-of-sample test set of 100 observations.</div>\nOur \u03b5-admissible subsets procedure evidently performs the best at assigning highest posterior probability to the true model. And even in comparison to the frequentist-oriented metrics, proportion of correct model selections and out-of-sample RMSE, our \u03b5-admissible subsets procedure performs more or less on par with the best performing methods considered. Note that in collinear design settings (i.e., \u03c1 > 0) it may be the case that a strict subset of the true data generating model is identified as the \u2018true\u2019 model within the \u03b5-admissible framework. This is meaningful because it manifests the fact that the true model may not be minimal (i.e., contains redundant predictors) in collinear, finite-sample settings. Furthermore, it explains the larger difference in proportion of true model selections between the \u03b5-admissible and nonlocal prior performances in the \u03c1 = .25 case (versus the \u03c1 = 0 case), which is accompanied by only a very small change in the difference between RMSE performance. This phenomenon is illustrated in a more extreme case of collinearity in the next simulation setup.\nOur \u03b5-admissible subsets procedure evidently performs the best at assigning highest posterior probability to the true model. And even in comparison to the frequentist-oriented metrics, proportion of correct model selections and out-of-sample RMSE, our \u03b5-admissible subsets procedure performs more or less on par with the best performing methods considered. Note that in collinear design settings (i.e., \u03c1 > 0) it may be the case that a strict subset of the true data generating model is identified as the \u2018true\u2019 model within the \u03b5-admissible framework. This is meaningful because it manifests the fact that the true model may not be minimal (i.e., contains redundant predictors) in collinear, finite-sample settings. Furthermore, it explains the larger difference in proportion of true model selections between the \u03b5-admissible and nonlocal prior performances in the \u03c1 = .25 case (versus the \u03c1 = 0 case), which is accompanied by only a very small change in the difference between RMSE performance. This phenomenon is illustrated in a more extreme case of collinearity in the next simulation setup. 4.2. Simulation setup 2. The \u03b5-admissible subsets approach has been developed in this paper as a method of obtaining a posterior-like distribution which effectively eliminates (i.e., assigns negligible probability to) all subsets with redundancies. To illustrate that our constructed methods do just that, consider the following setup in which the true data generating model lacks uniqueness for the small sample size n = 30: \ufffd \ufffd\n4.2. Simulation setup 2. The \u03b5-admissible subsets approach has been developed in this paper as a method of obtaining a posterior-like distribution which effectively eliminates (i.e., assigns negligible probability to) all subsets with redundancies. To illustrate that our constructed methods do just that, consider the following setup in which the true data generating model lacks uniqueness for the small sample size n = 30: \ufffd \ufffd\n(9)\n\ufffd where x(1), x(2), x(3) iid \u223cNn(0, In), and\n\ufffd \ufffd With standard deviations of .1 and a model error standard deviation of 1, covariates x(4), . . . , x(9) can all be approximately expressed as a linear combination of x(1), x(2), x(3). Accordingly, with a small increase in error variance, model (9) can be re-expressed using various combinations of the 9 predictors. However, observe that a subset with 4 or more predictors predominantly contains redundant information.\nRecall from Section 1 that the nonlocal prior approach of Johnson and Rossell (2012) is designed to assign negligible probabilities to subsets containing predictor(s) with coefficients of zero. So, in theory, the full subset {x(1), . . . , x(9)} will remain the best candidate for the true model within the nonlocal prior framework. In fact, this is demonstrated to be the case in Table 1 which shows the performance of both the nonlocal prior and the \u03b5-admissible subsets approach on 1000 data vectors y generated according to (9), with each covariate having a \u2018true\u2019 coefficient of 1. Note that as in the first simulation setup 1000 design matrices X are generated to generate the 1000 y vectors.\nThe average number of covariates in the MAP estimator, MMAP, (|MMAP|) is presented in the first column, the average RMSE in the second, and the average r(MMAP|y) or P(MMAP|y) in the third. Averages are over 1000 synthetic data sets from model (9). The RMSE is computed on an out-of-sample test set of 30 observations. Table 1 shows that the MAP estimate for the \u03b5-admissible subsets approach contains 3-4 covariates, on average, and that in fact the average RMSE is smaller than that of the nonlocal prior approach. Indeed, the MAP estimates for the nonlocal prior procedure typically includes all 9 covariates even though the y vectors can be mostly explained by only 3 of the predictors. This simple simulation illustrates a pivotal difference between the nonlocal prior and \u03b5-admissible subsets approaches. With p = 9 the implication of not discriminating against redundant subsets may seem trivial. However, the 2p size of the sample space grows rapidly in p and thus, puts exponentially more burden on procedures which do not discriminate based on redundancies. This is reflected by comparing the differences in strength of asymptotic consistency achieved for the two procedures. Though, the consistency result of the nonlocal prior method from Johnson and Rossell (2012) is argued to be stronger in an as-of-yet unpublished manuscript by Shin, Bhattacharya and Johnson (2015). 5. Concluding remarks. In this paper we have developed a new perspective for variable selection to exploit a non-redundancy property of a true data generating model. The basic idea calls for defining a true model as one which contains minimal amount of information necessary for explaining and/or predicting the observed data. The difference between our definition of a true model and the usual definition arises only in finite-sample applications, and was illustrated in Section 4.2. Within our variable selection framework, this definition allows us to show that the posterior-like proba-\n5. Concluding remarks. In this paper we have developed a new perspective for variable selection to exploit a non-redundancy property of a true data generating model. The basic idea calls for defining a true model as one which contains minimal amount of information necessary for explaining and/or predicting the observed data. The difference between our definition of a true model and the usual definition arises only in finite-sample applications, and was illustrated in Section 4.2. Within our variable selection framework, this definition allows us to show that the posterior-like proba-\nbility of the true model converges to 1 asymptotically, even with p growing almost exponentially in n, with the intuition that redundancies in the sample space are very effectively eliminated. Moreover, our empirical simulation results are consistent with this strong consistency result, and as desired, it is demonstrated in a situation of high collinearity that the \u03b5-admissible subsets approach yields a posterior-like distribution which is concentrated over subsets with fewer covariates, without sacrificing prediction error. A non-redundancy property of a true data generating model is seemingly general enough to extend to variable or feature selection problems beyond the linear model setting, but would seem infeasible if it could not be developed for the linear model setting. Thus, the goal of this paper has been to establish the potential feasibility of exploiting such a property. In future work we hope to extend our methods to more complicated settings.\n# References.\nAndrieu, C. and Roberts, G. (2009). The psuedo-marginal approach for efficient monte carlo computations. The Annals of Statistics 37 697-725. Beaumont, M. A. (2003). Estimation of population growth or decline in genetically monitored populations. Genetics 164 1139-1160. Berger, J. O., Pericchi, L. R., Ghosh, J., Samanta, T., De Santis, F., Berger, J. and Pericchi, L. (2001). Objective Bayesian methods for model selection: introduction and comparison. Lecture Notes-Monograph Series 135\u2013207. Berk, R. A. (2008). Statistical Learning from a Regression Perspective. Springer, New York, NY. Bertsimas, D., King, A. and Mazumder, R. (2016). Best Subset Selection via a Modern Optimization Lens. The Annals of Statistics 44 813-852. Bondell, H. D. and Reich, B. J. (2012). Consistent high-dimensional Bayesian variable selection via penalized credible regions. Journal of the American Statistical Association 107 1610-1624. Breheny, P. and Huang, J. (2011). Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Annals of Applied Statistics 5 232\u2013253. Candes, E. and Tao, T. (2007). The Dantzig Selector: Statistical estimation when p is much greater than n. The Annals of Statistics 35 2313-2351. Fan, J. and Li, R. (2001). Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properities. Journal of the American Statistical Association 96 1348-1360. Ghosh, J. and Ghattas, A. E. (2015). Bayesian Variable Selection Under Collinearity. The American Statistician 69 649-660. Hannig, J., Iyer, H., Lai, R. C. S. and Lee, T. C. M. (2016). Generalized Fiducial Inference: A Reviev and New Results. Journal of American Statistical Association 111 1346-1361. Jameson, G. J. O. (2013). Inequalities for gamma function ratios. American Math. Monthly 120 936-940. Johnson, V. E. and Rossell, D. (2012). Bayesian Model Selection in High-Dimensional Settings. Journal of the American Statistical Association 107.\nLai, R. C. S., Hannig, J. and Lee, T. C. M. (2015). Generalized Fiducial Inference for Ultrahigh-Dimensional Regression. Journal of the American Statistical Association 110 760-772. Lan, H., Chen, M., Flowers, J. B., Yandell, B. S., Stapleton, D. S., Mata, C. M., Mui, E. T., Flowers, M. T., Schueler, K. L., Manly, K. F., Williams, R. W., Kendziorski, K. and Attie, A. D. (2006). Combined expresssion trait correlations and expression quantitative trait locus mapping. PLoS Genetics 6e. Luo, S. and Chen, Z. (2013). Extended BIC for Linear Regression Models With Diverging Number of Relevant Features and High or Ultra-High Feature Spaces. Journal of Statistical Planning and Inference 143 494-504. Narisetty, N. N. and He, X. (2014). Bayesian variable selection with shrinking and diffusing priors. The Annals of Statistics 24 789-817. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 2825\u20132830. Rossell, D. and Telesca, D. (2017). Nonlocal Priors for High-Dimensional Estimation. Journal of the American Statistical Association 112 254\u2013265. Ro\u02c7ckov\u00b4a, V. and George, E. I. (2016). The Spike-and-Slab LASSO. Journal of the American Statistical Association. Shin, M., Bhattacharya, A. and Johnson, V. E. (2015). Scalable Bayesian variable selection using nonlocal prior densities in ultrahigh-dimensional settings. arXiv preprint arXiv:1507.07106. Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society 58 267-288. Yuan, M. and Lin, Y. (2005). Efficient Empirical Bayes Variable Selection and Estimation in Linear Models. Journal of the American Statistical Association 100 1215-1225. Zhang, C. and Huang, J. (2008). The Sparsity and Bias of the Lasso Selection in HighDimensional Linear Regression. The Annals of Statistics 36 1567-1594.\n# APPENDIX A: PROOFS\nProof of Lemma 3.5. From the distributional assumption on \u03b2M it follow that \ufffd\nProof of Lemma 3.5. From the distributional assumption on \u03b2M it follows \ufffd\nThus,\n\ufffd  \u2212||  \u2212|| where A = X\u2032XM(X\u2032 MXM)\u22121 2 is a p \u00d7 |M| matrix which has the singular value decomposition A = QDW \u2032 for Q and W each orthogonal matrices. By the definition of the multivariate T distribution, \ufffd \ufffd \ufffd\nT = \ufffd n \u2212|M| V Z =\u21d2 W \u2032T = \ufffd n \u2212|M| V W \u2032Z =: \ufffd n \u2212|M| V \ufffdZ =: \ufffdT where V \u223c\u03c72 n\u2212|M| and Z, \ufffdZ \u223cMVN(0, I|M|). Then\n \ufffd  \ufffd \ufffd \ufffd \ufffd  \ufffd where \u039bM = \ufffd|M| 1 \u03bbi, and \u03bbi is the ith eigenvalue of A\u2032A. Observe that \u039bM also has the more intuitive expression \u039bM = tr(A\u2032A) = tr \ufffd (HMX)\u2032HMX \ufffd which illustrates that it is intimately related to the presence of collinearity amongst the covariates. Recall that \ufffd\u03c32  := RSS/(n \u2212|M|). Then\n(10)\n \ufffd where the last inequality follows because for the standard normal CDF, \u03a6\n \ufffd where the last inequality follows because for the standard normal CDF, \u03a6 for x > 0, \u2212x2\n \ufffd  inequality follows because for the standard normal CDF, \u03a6\n\u2212 \u2264 x o simplify the bound, observe first that Jameson (2013) gives\n(11)\nSecond, observe that for 0 \u2264x \u2264n, \ufffd \ufffd\n\ufffd \ufffd By assumption |M| \u2264c1n, and \u03b5 9\u039bM \ufffd\u03c32 M < n\u2212|M| 2 which implies that\n(12)\n \ufffd Therefore, applying (11) and (12) to (10) gives\nProof of Lemma 3.6. Similar to the proof of Lemma 3.5. Proof of Theorem 3.7. Recall that \ufffd \ufffd\n\ufffd   \ufffd where bmin solves min b\u2208Rp 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|M| \u22121 Observe that \ufffd \ufffd\n\ufffd \ufffd where bmin solves min b\u2208Rp 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|M| \u22121.\n\ufffd \ufffd here bmin solves min b\u2208Rp 1 2\u2225X\u2032(XM\u03b2M \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|M| \u22121.\n(13)\n\ufffd \ufffd \ufffd where \ufffd\u03b2M(\u22121) is the least squares estimate corresponding to the subset of covariates M with one covariate removed so that \u2225Ey(\ufffd\u03b2M(\u22121))\u22250 \u2264|M| \u22121. The covariate removed is chosen to correspond to the smallest (in magnitude) component of \ufffd\u03b2M. To refine the bound on E(h(\u03b2M)), decompose the last expression in (13) using the triangle inequality as follows.\n(14)\n\ufffd \ufffd where HM := XM(X\u2032 MXM)\u22121X\u2032 M. Observe that by Lemma 3.5 the first term on the right hand side,\n\ufffd \ufffd Note that the last two terms are written as an indicator function because the uncertainty here comes from \u03b2M. However, the second term results from the uncertainty in observing Y . Accordingly, by Lemma 3.6,\n\ufffd\ufffd\ufffd \ufffd Recall that Py is used to denote the probability measure associated with the sampling distribution of Y . The third term in (14) must be zero by Condition 3.3. \u25a0 Proof of Theorem 3.8. Recall that \ufffd \ufffd\n\ufffd \ufffd where b\u2032 min solves min b\u2208Rp 1 2\u2225X\u2032(XMo\u03b2Mo \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|Mo|\u22121. To show the desired result, let bmin be the solution to min b\u2208Rp \u2225X\u2032(XMo\u03b20 Mo \u2212Xb)\u22252 2 subject to \u2225b\u22250 \u2264|Mo| \u22121. Then observe that\n \u2225\u2225 \u2264|| \u2212 \u2225X\u2032(XMo\u03b20 Mo \u2212Xbmin)\u22252 \u2264\u2225X\u2032(XMo\u03b20 Mo \u2212Xb\u2032 min)\u22252 \u2264\u2225X\u2032XMo(\u03b20 Mo \u2212\u03b2Mo)\u22252 + \u2225X\u2032(XMo\u03b2Mo \u2212Xb\u2032 min)\n \u2225\u2225 \u2264|| \u2212 \u2225X\u2032(XMo\u03b20 Mo \u2212Xbmin)\u22252 \u2264\u2225X\u2032(XMo\u03b20 Mo \u2212Xb\u2032 min)\u22252\nNote the difference between bmin and b\u2032 min here. The rightmost term is the quantity of interest because it will become E(h(\u03b2Mo)) in the next few steps. The term on the left of the inequality corresponds to the quantity in Condition 3.1. Adding and subtracting \ufffd\u03b2Mo inside the first term on the right side of the second inequality, and applying the triangle inequality gives,\n\ufffd \ufffd \ufffd + I \ufffd1 2\u2225X\u2032XMo(\ufffd\u03b2Mo \u2212Ey(\ufffd\u03b2Mo)\u22252 2 \u2265\u03b5 \ufffd + E(h(\u03b2Mo)),\n\ufffd and by applying Lemma 3.5, \ufffd\n\ufffd \ufffd\ufffd \ufffd where the middle term is written as an indicator function because the uncertainty here comes from \u03b2Mo. This indicator is 0 by Lemma 3.6 with probability exceeding (15). Therefore, since Condition 3.1 implies that the indicator on the left side of the inequality is 1,\n\ufffd  \u2212 with probability exceeding\n(15)\n\ufffd due to the uncertainty in observing Y . \u25a0 The following lemma is needed for the proof of the main result, Theorem\n# 3.9.\nLemma A.1. Assume all conditions and notations of Theorem 3.9, and without loss of generality suppose \u03c30 Mo = 1, where \u03c30 Mo is the true but unknown error standard deviation. Then the following holds.\nand bn := n\u2212n\u03b1\u2212|Mo| n\u2212|Mo|\u22122 \u21921.\nNote that V1 and V2 both vanish for large n and p by Condition 3.2. This condition also ensures that \u03ben,j \u2208(0, 1) which is needed in the proof of this lemma. Proof of Lemma A.1. There are two cases to consider.\nCase 1 For M \u2208Mj with j \u2264|Mo| let \u03ben,j = 1 \u22122 log(n)\u03b3 log(p) (n\u2212j\u22121)/2 \u2208(0, 1) by Condition 3.2 for large n and p. Then \ufffd \ufffd\n(16)\n\ufffd \ufffd \ufffd since by assumption Y = XMo\u03b20 Mo + U with U \u223cNn(0, In), and so RSSMo = U \u2032(In \u2212HMo)U and RSSM = \u2206M + 2\u03b20\u2032 MoX\u2032 Mo(In \u2212HM)U + U \u2032(In \u2212HM)U = Z\u2206M + U \u2032(In \u2212HM)U\nwhere Z\u2206M := \u2206M + 2\u221a\u2206MZ, and Z \u223cN(0, 1). Recall that \u2206M := \u03b20\u2032 MoX\u2032 Mo(In\u2212HM)XMo\u03b20 Mo. Continuing in (16) by subtracting \u03c72 n\u2212|Mo| from both sides of the inequality gives, \ufffd \ufffd\n(17)\n\ufffd \ufffd The last inequality follows because the \u03c72 |Mo| random variable is nonnegative, and removing it simplifies the remaining argument. Then Py \ufffdRSSMo > \u03ben,j \ufffd\n\ufffd \ufffd The last inequality follows because the \u03c72 |Mo| random variable is nonnegative, and removing it simplifies the remaining argument. Then \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd  \u2212 \ufffd For the second and third term, apply the Chernoff bound, and evaluate the moment generating function for the \u03c72 j and \u03c72 n\u2212|Mo| distributions at 1/4. Accordingly,\n\ufffd \ufffd \ufffd Finally the remaining probability can be controlled by the bound for the CDF of a standard normal random variable for x > 0,\nHence,\nwhere the last inequality follows by observing that j \u2264|Mo|, and recalling the expression for \u03ben,j. Therefore, the probability that \ufffd RSSMo RSSM \ufffdn\u2212j\u22121 2 > \u03be n\u2212j\u22121 2 n,j is satisfied for some M with |M| \u2264|Mo| is \uf8eb \uf8f6\nNote that\n\ufffd \ufffd In fact, Luo and Chen (2013) show that if log(j)/ log(p) \u2192\u03b4 as p \u2192\u221e, for some \u03b4 > 0, then log \ufffdp j \ufffd = j log(p)(1 \u2212\u03b4)(1 + o(1)). Thus,\n+ e\u2212 \u03ben,|Mo| 48 (n\u2212|Mo|\u22121)\u2206M log(n)\u03b3 log(p) + n\u2212|Mo| 2 +j log(p) \ufffd\nSince \u03ben,|Mo| \u21921, this bound vanishes by Condition 3.2. Therefore, with probability exceeding one minus the above bound,\nuniformly over all M such that |M| \u2264|Mo|. Case 2 Consider any subset M with |Mo| < |M| \u2264n\u03b1 for some positive constant \u03b1 < 1, and let {an} be an arbitrarily sequence of numbers. To\nbegin, repeating the steps in (17), but subtracting \u03c72 n\u2212j/\u03ben,j on both sides instead of \u03c72 n\u2212|Mo|, and replacing the label \u03ben,j with an, yields\n\ufffd \ufffd \ufffd \ufffd where Z\u2206M = \u2206M + 2\u221a\u2206MZ, Z \u223cN(0, 1), and \u2206M = \u03b20\u2032 MoX\u2032 Mo(In \u2212 HM)XMo\u03b20 Mo. Since Mo \u2282M implies \u2206M = 0, the above bound can be simplified by including in the subset M any covariates in Mo not already included in M. Accordingly, let M\u2032 := M \u222aMo which includes j + l covariates, where l \u2208{0, . . . , |Mo|} is the number of covariates not shared by M and Mo. Then because RSSM\u2032 \u2264RSSM, \ufffd \ufffd \ufffd \ufffd\nand for any nonnegative s \u2208R,\n(19)\n\ufffd \ufffd Consider each of these last two terms in turn. For the first term apply the Chernoff bound, and evaluate the moment generating function for the \u03c72 j+l distribution at 1/4. That gives\n\ufffd   \ufffd For the second term in (19) write out the expression to evaluate the probability explicitly, and then apply the simple bound e\u2212x \u22641 for all x \u22650. Noting that s > 0,\n\ufffd \ufffd where the last inequality follows from the well known Sterling lower bound on the gamma function\n \u2265 It is clear from the last expression that for the probability to vanish, e \u00b7 s must grow no faster than n\u2212j\u2212l 2 . Accordingly, choosing s = n\u2212j\u2212l e2\nIt is clear from the last expression that for the probability to vanish, e \u00b7 s must grow no faster than n\u2212j\u2212l 2 . Accordingly, choosing s = n\u2212j\u2212l e2 gives\n\ufffd \ufffd \ufffd Combining the two bounds for (19) now yield\n\ufffd \ufffd \ufffd  \u2212 \u2212 It only remains to choose the smallest an such that the first term in the bound vanishes exponentially fast so that the cumulative probability will vanish in probability over all subsets M with |M| \u2264n\u03b1. Accordingly, it should become apparent shortly that a good choice is\n(20)\n \u2212 \u2212 The probability that \ufffd RSSMo RSSM \ufffdn\u2212j\u22121 2 > a n\u2212j\u22121 2 n is satisfied for some M with |Mo| < |M| \u2264n\u03b1 is \uf8eb \uf8f6\nThus, bounding the binomial coefficient as in (18), and substitutin (20) for an yields \uf8eb \uf8f6\n\ufffd where bn := n\u2212n\u03b1\u2212|Mo| n\u2212|Mo|\u22122 \u21921. Note that this bound vanishes by Condition 3.2. Therefore, with probability exceeding one minus the above bound,\nuniformly over all M such that |Mo| < |M| \u2264n\u03b1.\nProof of Theorem 3.9. Without loss of generality suppose \u03c30 Mo = 1, where \u03c30 Mo is the true but unknown error standard deviation. For j \u2208{1, . . . , p} define the following classes of subsets Mj := {M \u0338= Mo : |M| = j}. Recall that \ufffd\u03c32 M := RSSM/(n \u2212|M|). It will first be shown that for any subset of covariates M \u0338= Mo the ratio r(M|Y ) r(Mo|Y ) vanishes in probability for large n and p. Accordingly, for any M \u2208Mj, \ufffd \ufffd\n\ufffd \ufffd Before proceeding with the rest of the proof, a the following notation is needed. V1 and V2 are as stated in Lemma A.1. As in Theorem 3.8, define \ufffd\nand corresponding to Theorem 3.7, define\nby bounding the binomial coefficient as in (18). Note that V4 then vanishes by Condition 3.4. Further, recall that RSSMo \u223c\u03c72 n\u2212|Mo|, so by the Chernoff bound (evaluating the moment generating function at 1/4),\n\ufffd \ufffd\ufffd \ufffd With this notation it is now possible to account for all of the uncertaint due to Y . Accordingly, by Theorem 3.8, with probability exceeding 1 \u2212V3 \u2212V5, \ufffd \ufffd\n(21)\n(21) where\nwhere\n\ufffd  \u2212 \u2212   Further, fix A1 \u2208(0, 1), and by Condition 3.4 choose n and p sufficiently large so that g1(Mo, n, p) < A1.\nSuppose j \u2264|Mo|. As in Jameson (2013), the ratio of gamma function can be bounded by \ufffd \ufffd \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd Applying Lemma A.1 to bound the ratio of residual sums of squares, and bounding the expectation by 1, with probability exceeding 1 \u2212V1 \u2212V3 \u2212V5,\n\ufffd \ufffd \ufffd \ufffd Applying Lemma A.1 to bound the ratio of residual sums of squares, and bounding the expectation by 1, with probability exceeding 1 \u2212V1 \u2212V3 \u2212V5 (21) implies\n(22)\n\uf8ed \ufffd \ufffd \uf8f8 \ufffd \ufffd \ufffd \ufffd Applying Theorem 3.7 to bound the expectation, and applying Lemma A.1 to bound the ratio of residual sums of squares, with probability exceeding 1 \u2212V2 \u2212V3 \u2212V4 \u2212V5, (21) implies\n(23)\n\ufffd Notice that Theorem 3.7 is being applied here with n\u03b1 in place of c1n. This can be done without loss of generality because n\u03b1 grows slower than c1n, for any choices of \u03b1, c1 \u2208(0, 1). It can now be shown that \ufffdn\u03b1 j=1 \ufffd M\u2208Mj r(M|Y ) r(Mo|Y ) vanishes in probability for large n and p. Apply the bounds in (22) and (23) in the following argument. n\u03b1 \ufffd j=1 \ufffd M\u2208Mj r(M|Y ) r(Mo|Y ) \u2264 |Mo| \ufffd j=1 \ufffdp j \ufffd max M\u2208Mj r(M|Y ) r(Mo|Y ) \ufffd \ufffd\ufffd \ufffd =: S1 + n\u03b1 \ufffd j=|Mo|+1 \ufffdp j \ufffd max |M|=j r(M|Y ) r(Mo|Y ) \ufffd \ufffd\ufffd \ufffd =: S2\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe17/fe17310f-fc56-40d4-857b-69a7db956853.png\" style=\"width: 50%;\"></div>\n\ufffd by bounding the binomial coefficient as in (18), and with probability exceeding 1 \u2212V2 \u2212V3 \u2212V4 \u2212V5,\n\ufffd for some positive constant A2. The last inequality follows by Condition 3.4. Thus, for sufficiently large n and p, with probability exceeding 1 \u2212V1 \u2212 V2 \u2212V3 \u2212V4 \u2212V5, n\u03b1 \ufffd j=1 \ufffd M\u2208Mj r(M|Y ) r(Mo|Y ) \u2264|Mo|e\u2212(2 log(n)\u03b3\u2212|Mo|) log(p) 1 \u2212A1 + A2 n1\u2212\u03b1 max |Mo|<|M|\u2264n\u03b1 \ufffd\ufffd\u03c32 M\u039bM \u03b5 \ufffd1 2 which by Conditions 3.1-3.4 vanishes for large n and p. The proof is now complete by noticing that r(Mo|Y ) = r(Mo|Y ) \ufffdn\u03b1 j=1 \ufffd M:|M|=j r(M|Y ) = 1 1 + \ufffdn\u03b1 j=1 \ufffd Mj r(M|Y ) r(Mo|Y ) .\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of variable selection in high-dimensional linear models, highlighting the limitations of standard penalized methods due to coefficient estimate unreliability in collinear design matrices. The authors propose a new approach based on generalized fiducial inference to effectively manage linear dependencies among covariates.",
        "problem": {
            "definition": "The problem involves selecting the true model from a potentially exponential number of candidate models in high-dimensional settings, where the number of predictors p can grow much larger than the number of observations n.",
            "key obstacle": "Existing methods struggle with the high dimensionality and collinearity of predictors, leading to unreliable coefficient estimates that hinder effective variable selection."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to eliminate redundancy in model selection, ensuring that candidate models contain only the essential covariates needed for accurate prediction.",
            "opinion": "The proposed method defines true models as those that are non-redundant and contain the minimal information necessary to explain the data, utilizing a probabilistic approach to model selection.",
            "innovation": "This method differs from previous approaches by focusing on \u03b5-admissible subsets that do not rely on coefficient magnitudes, thus effectively reducing the sample space of candidate models."
        },
        "method": {
            "method name": "Generalized Fiducial Inference for Variable Selection",
            "method abbreviation": "GFI",
            "method definition": "The method constructs a probability distribution over subsets of covariates that are \u03b5-admissible, allowing for effective variable selection in high-dimensional settings.",
            "method description": "The core of the method involves defining \u03b5-admissible subsets and constructing a posterior-like distribution that assigns negligible probability to redundant subsets.",
            "method steps": [
                "Define the \u03b5-admissibility condition for candidate subsets.",
                "Construct a likelihood function based on the sampling distribution of the data.",
                "Use a pseudo-marginal MCMC approach to sample from the distribution over subsets.",
                "Estimate the posterior probabilities for the selected models."
            ],
            "principle": "The effectiveness of the method is based on the ability to eliminate redundant subsets from consideration, thereby concentrating on the true model as n and p grow large."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using synthetic datasets generated under controlled conditions, comparing the proposed method against existing Bayesian and frequentist variable selection methods.",
            "evaluation method": "Performance was assessed using metrics such as model selection accuracy, root mean squared error (RMSE), and the average posterior probability of the true model across multiple simulations."
        },
        "conclusion": "The proposed method demonstrates strong consistency in identifying the true model in high-dimensional settings, effectively managing collinearity and redundancy among predictors, as supported by empirical results.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to focus on non-redundant subsets, leading to improved model selection accuracy and robustness against collinearity compared to traditional methods.",
            "limitation": "A limitation of the method is its computational complexity, particularly in evaluating the \u03b5-admissibility condition and sampling from the posterior distribution, which may pose challenges in very high-dimensional settings.",
            "future work": "Future research will explore extending the methods to more complex modeling scenarios beyond linear regression, as well as improving computational efficiency."
        },
        "other info": {
            "software": "Computer code implementing the proposed method is available at https://jonathanpw.github.io/software.html.",
            "MSC 2010 subject classifications": "62J05, 62F12, 62A01",
            "keywords": [
                "variable selection",
                "best subset selection",
                "high dimensional regression",
                "L0 minimization",
                "generalized fiducial inference"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of variable selection in high-dimensional linear models, highlighting the limitations of standard penalized methods due to coefficient estimate unreliability in collinear design matrices."
        },
        {
            "section number": "1.2",
            "key information": "The problem involves selecting the true model from a potentially exponential number of candidate models in high-dimensional settings, where the number of predictors p can grow much larger than the number of observations n."
        },
        {
            "section number": "2.2",
            "key information": "The proposed method constructs a probability distribution over subsets of covariates that are \u03b5-admissible, allowing for effective variable selection in high-dimensional settings."
        },
        {
            "section number": "3.1",
            "key information": "The core of the method involves defining \u03b5-admissible subsets and constructing a posterior-like distribution that assigns negligible probability to redundant subsets."
        },
        {
            "section number": "3.3",
            "key information": "A limitation of the method is its computational complexity, particularly in evaluating the \u03b5-admissibility condition and sampling from the posterior distribution, which may pose challenges in very high-dimensional settings."
        },
        {
            "section number": "8.4",
            "key information": "Future research will explore extending the methods to more complex modeling scenarios beyond linear regression, as well as improving computational efficiency."
        }
    ],
    "similarity_score": 0.548521770231726,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0858_fine-/papers/Non-penalized variable selection in high-dimensional linear model settings via generalized fiducial inference.json"
}