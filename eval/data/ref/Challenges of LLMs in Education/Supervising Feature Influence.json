{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1803.10815",
    "title": "Supervising Feature Influence",
    "abstract": "Causal influence measures for machine learnt classifiers shed light on the reasons behind classification, and aid in identifying influential input features and revealing their biases. However, such analyses involve evaluating the classifier using datapoints that may be atypical of its training distribution. Standard methods for training classifiers that minimize empirical risk do not constrain the behavior of the classifier on such datapoints. As a result, training to minimize empirical risk does not distinguish among classifiers that agree on predictions in the training distribution but have wildly different causal influences. We term this problem covariate shift in causal testing and formally characterize conditions under which it arises. As a solution to this problem, we propose a novel active learning algorithm that constrains the influence measures of the trained model. We prove that any two predictors whose errors are close on both the original training distribution and the distribution of atypical points are guaranteed to have causal influences that are also close. Further, we empirically demonstrate with synthetic labelers that our algorithm trains models that (i) have similar causal influences as the labeler\u2019s model, and (ii) generalize better to out-of-distribution points while (iii) retaining their accuracy on in-distribution points.",
    "bib_name": "sen2018supervisingfeatureinfluence",
    "md_text": "# Supervising Feature Influence\n# Shayak Sen, Piotr Mardziel, Anupam Datta, Matthew Fredrikson Carnegie Mellon University\n# Abstract\nCausal influence measures for machine learnt classifiers shed light on the reasons behind classification, and aid in identifying influential input features and revealing their biases. However, such analyses involve evaluating the classifier using datapoints that may be atypical of its training distribution. Standard methods for training classifiers that minimize empirical risk do not constrain the behavior of the classifier on such datapoints. As a result, training to minimize empirical risk does not distinguish among classifiers that agree on predictions in the training distribution but have wildly different causal influences. We term this problem covariate shift in causal testing and formally characterize conditions under which it arises. As a solution to this problem, we propose a novel active learning algorithm that constrains the influence measures of the trained model. We prove that any two predictors whose errors are close on both the original training distribution and the distribution of atypical points are guaranteed to have causal influences that are also close. Further, we empirically demonstrate with synthetic labelers that our algorithm trains models that (i) have similar causal influences as the labeler\u2019s model, and (ii) generalize better to out-of-distribution points while (iii) retaining their accuracy on in-distribution points.\narXiv:1803.10815v2\n# Introduction\nData processors employing machine learning algorithms are increasingly being required to provide and account for reasons behind their predictions due to regulations such as the EU GDPR [European Commission, 2016]. This call for reasoning tools has intensified with the increasing use of machine learning systems in domains like criminal justice [Angwin et al., 2016], credit [Byrnes, 2017], and hiring [Ideal Inc., 2017]. Understanding the reasons behind prediction by measuring the importance or influence of attributes for predictors has been an important area of study in machine learning. Traditionally, influence measures were used to inform feature selection [Breiman, 2001]. Recently, influence measures have received renewed interest as part\nof a toolbox to explain operations and reveal biases of inscrutable machine learning systems [Ribeiro et al., 2016; Datta et al., 2016; Adler et al., 2018; Kilbertus et al., 2017; Datta et al., 2017]. Causal influence measures are a particularly important constituent of this toolbox [Datta et al., 2016; Kilbertus et al., 2017]. By identifying attributes that directly affect decisions, they provide insight about the operation of complex machine learning systems. In particular, they enable identification of principal reasons for decisions (e.g., credit denials) by evaluating counterfactual queries that ask whether changing input attributes would produce a change in the decision. This determination is used to explain and guard against unjust biases. For example, the use of a correlate of age like income to make credit decisions may be justified even if it causes applicants of one age group to be approved at a higher rate than another whereas the direct use of age or a correlate like zipcode may not be justified1. Causal analyses of natural systems often involve observing outcomes of specially created units, e.g. mice with genes altered. Such units may be atypical in natural populations. However, while performing causal analysis over machine learnt systems, a similar approach encounters an important challenge: machine learning systems are not expected to be evaluated on atypical or out-of-distribution units (datapoints), since they have not been exposed to such units during training. Standard methods for training classifiers that minimize empirical risk do not constrain the behavior of the classifier on such datapoints. As a result, training to minimize empirical risk does not distinguish among classifiers that agree on predictions in the training distribution but have unintended causal influences. We term this problem covariate shift in causal testing. In other words, typical machine learning algorithms are designed to make the right predictions but not necessarily for justifiable reasons. Returning to the example of credit decisions using age and income, consider a situation where the two are strongly correlated: young individuals have low income and older individuals have a higher income. This situation is illustrated in Figure 1a where all three predictors h1, h2, h3 have low predictive error, but they make similar predictions for very differ-\n1This is an example of a \u201cbusiness necessity defense\u201d under US aw on disparate impact [Burger, 1971].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0136/01363197-2738-4743-ba3c-242ec4310a72.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Three predictors with similar in-distribution predictions</div>\nFigure 1: The three predictors trying to separate white points from black have similar predictions on the distribution, but very different causal influences. Predictor h1 uses only income, h2 uses only age, and h3 uses a linear combination of the two. Figure (b) shows how counterfactual querying, by keeping age fixed and varying income allows the identification of causal influence and distinguishes between the three classifiers.\nent reasons. Since the three predictors have nearly identical predictions on the distribution, points from the distribution are not useful in distinguishing the causal influence of the two features. As a result, causal testing requires the creation of atypical units that break the correlations between features. For example, evaluating the predictor on the points on the red bar (Figure 1b) where age is fixed and income is varied informs whether income is used by a given predictor or not. However, since from an empirical risk minimization perspective the atypical points are irrelevant, an algorithm optimizing just for predictive accuracy is free to choose any of the three predictors. We formally characterize conditions that give rise to covariate shift in causal testing (Theorem 1). Intuitively, this result states that if the units used for measuring causal influence are sufficiently outside the data distribution, constraining the behavior of a predictor on the data distribution does not constrain the causal influences of the predictor. In order to address this issue, we introduce an active learning algorithm in Section 4. This algorithm provides an accountability mechanism for data processors to examine important features, and if their influences are suspicious, to collect additional information that constrains the feature influences. This additional information could steer the influences toward more acceptable values (e.g., by reducing the influence of age in h2 in Figure 1a). Alternatively, it could provide additional evidence that the influence values convey appropriate predictive power and the suspicions are unfounded (e.g., by preserving influences in h1 in Figure 1a). The active learning process is assisted by two oracles. The first is a feature selection oracle that examines the causal influences of different features, and chooses the feature for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5b2/e5b22a1f-93b6-4318-8e94-c3f49a8b8351.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Age (b) Causal testing for income</div>\n<div style=\"text-align: center;\">(b) Causal testing for income</div>\nwhich counterfactuals queries should be answered. We envision this oracle to be an auditor who can identify problematic causal influences based on background knowledge of causal factors or ethical norms governing classification. The second is similar to a standard active learning oracle, and labels atypical points to answer counterfactual queries. For example, for predictor h2 in our running example, the feature selection oracle might notice that age has an unduly high influence, and can instruct the algorithm to focus on instances that vary age while keeping income fixed. While the direct use of age may be obviously problematic, in common applications the system designer may not have apriori knowledge of which attribute uses are problematic. The feature selection oracle may be able to spot suspiciously high or low influences and guide the counterfactual queries that get sent to the labeler to better inform the learning. We evaluate the counterfactual active learning algorithm for linear, decision tree, and random forest models on a number of datasets, using a synthetic labeler. In particular, we demonstrate that after counterfactual active learning, the trained classifier has similar causal influence measures to the labeler. We also show that the classifier can generalize better to out-of-distribution points. This is an important consequence of having causal behavior similar to the labeler. Finally, we demonstrate that the accuracy on the data distribution does not degrade as a result of this additional training.\nRelated Work. Prior work on causal learning learns the structure of causal models [Spirtes et al., 2000; Hyttinen et al., 2013], or given the structure of models, the functional relationship between variables. In this context, active learning has been used to aid both the discovery of causal struc-\ntures [Tong and Koller, 2001; He and Geng, 2008] and their functional relationships [Rubenstein et al., 2017]. In this work we don\u2019t attempt to learn true causal models. Instead, our work focuses on constraining the causal behavior of learnt models. In doing so, we provide an accountability mechanism for data processors to collect additional data that guides the causal influences of their models to more acceptable values or justifies the causal influences of the learnt model.\nContributions. In summary, the contributions of this paper are as follows. \u2022 A formal articulation of the covariate shift in causal testing problem. \u2022 A novel active learning algorithm that addresses the problem. \u2022 An empirical evaluation of the algorithm for standard machine learning predictors on a number of real-world datasets.\n# 2 Background\nA predictor h is a function X \u2192Y that operates on an input space X \u2286Rn to a space of predictions Y. The input space X has a probability distribution P associated with it, where P(X = x) is the frequency of drawing a particular instance x.\n# 2.1 Risk Minimization\nGiven random variables X \u2208X, and Y \u2208Y, and a loss function l, the risk associated with predictor h is given by\nR(h) = E [l(h(X), Y )] .\nThe goal of supervised learning algorithms under a risk minimization paradigm is to minimize R(h). In general, the distributions over X and Y are unknown. As a result, learning algorithms minimize empirical risk over a sample {(xi, yi)}1..N\nNote that the risk minimization paradigm only constrains the behavior of a predictor on points from the distribution and treats any two predictors that have identical behavior on points from the distribution interchangeably. For ease of presentation, we focus on binary classification tasks where Y is binary, and use the 0 \u22121 loss function l0-1(\u02c6y, y) = I(\u02c6y \u0338= y) = |\u02c6y \u2212y|.\n# 2.2 Counterfactual Influence\nThe influence of a feature f for a predictor h is measured by comparing the outcomes of h on the data distribution to the outcomes of a counterfactual distribution that changes the value of f. We denote the data distribution over features as X and the counterfactual distribution with respect to feature f as Xf cf.\nA number of influence measures proposed in prior work can be viewed as instances of this general idea. For example, Permutation Importance [Breiman, 2001], measures the difference in accuracy between X and Xf cf, where Xf cf is chosen as X randomly permuted. In [Adler et al., 2018], Xf cf is chosen as the minimal perturbation of X such that feature f cannot be predicted. In this paper, we use Average Unary QII (auQII), an instance of Quantitative Input Influence [Datta et al., 2016], as our causal influence measure. The counterfactual distribution Xf cf for auQII is represented as X\u2212fUf, where the random variable X\u2212f represents features except f where Uf is sampled from the marginal distribution of f independently of the rest of the features X\u2212f. P(X\u2212fUf = x) = P(X\u2212f = x\u2212f)P(Uf = xf) Definition 1. Given a model h, the Average Unary QII (auQII) of an input f, written \u03b9f(h), is defined as \u03b9f(h) = EX,Uf [l0-1(h(X), h(X\u2212fUf))]\nP(X\u2212fUf = x) = P(X\u2212f = x\u2212f)P(Uf = xf) Definition 1. Given a model h, the Average Unary QII (auQII) of an input f, written \u03b9f(h), is defined as \u03b9f(h) = EX,Uf [l0-1(h(X), h(X\u2212fUf))]\n# 3 Covariate shift in Causal Testing\nIn this section, we discuss some of the theoretical implications of the covariate shift in causal testing. First, we show in Theorem 1 that risk minimization does not constrain influences when the data distribution diverges significantly from the counterfactual distribution. In other words, predictors trained under an ERM regime are free to choose influential factors. Further, in Theorem 2, we demonstrate predictors that agree on predictions on both the data distribution and the counterfactual distribution have similar influences. This theorem forms the motivation for our counterfactual active learning algorithm presented in Section 4 that attempts to minimize errors on both the data and the counterfactual distribution by adding points from the counterfactual distribution to the training set.\n# 3.1 Counterfactual divergence\nWe first define what it means for an influence measure to be unconstrained by its behavior on the data distribution. An influence measure \u03b9f is unconstrained for a predictor h, and data distribution X, if it is possible to find a predictor h\u2032 which has similar predictions on the data distribution but very different influences. More specifically, if the influence is high, then it can be reduced to a lower value, and vice versa. Definition 2. An influence measure \u03b9f is said to be (\u03f5, \u03b4)unconstrained, for 0 \u2264\u03b4 \u22641/2, for a predictor h, if there exists predictors h1, h2 such that for i \u2208{1, 2}, P(h(X) \u0338= hi(X)) \u2264\u03f5, and \u03b9f(h1) \u2265\u03b4 and \u03b9f(h2) \u22641 \u2212\u03b4. The following theorem shows that if there exist regions \u03d5 in the input space with low probability weight in the data distribution and high weight in the counterfactual distribution, i.e. the data distribution and counterfactual distribution diverge significantly, then any model will have unconstrained causal influences. As a result, predictors trained under an ERM regime are free to choose influential causal factors. Theorem 1. If there exists a predicate \u03d5, such that P(\u03d5(X)) \u2264\u03f5 and P(\u03d5(X\u2212fUf) \u2227\u00ac\u03d5(X)) = \u03b3, then for any h, \u03b9f(h) is (\u03f5, \u03b3/2)-unconstrained.\nProof. The proof proceeds via an averaging argument. Let \u03a0 be the set of all functions from X to {0, 1}. For i \u2208{1, 2} consider hi sampled uniformly from the set of deterministic functions that map values x satisfying \u03d5 to h(x) and according to some \u03c0 \u2208 \u03a0 otherwise: {x \ufffd\u2192h(x) when \u00ac\u03d5(x) and \u03c0(x) o.w. }\u03c0\u2208\u03a0. Notice that P(hi(X)|\u00ac\u03d5(X)) is therefore uniform in {0, 1}. As hi(x) = h(x) when \u00ac\u03d5(x), any such classifier satisfies P(h(X) \u0338= hi(X)) \u2264P(\u03d5(x)) \u2264\u03f5. Computing the expected influence over all h1, we have\n \u2208{} of deterministic functions that map values x satisfying \u03d5 to h(x) and according to some \u03c0 \u2208 \u03a0 otherwise: {x \ufffd\u2192h(x) when \u00ac\u03d5(x) and \u03c0(x) o.w. }\u03c0\u2208\u03a0. Notice that P(hi(X)|\u00ac\u03d5(X)) is therefore uniform in {0, 1}. As hi(x) = h(x) when \u00ac\u03d5(x), any such classifier satisfies P(h(X) \u0338= hi(X)) \u2264P(\u03d5(x)) \u2264\u03f5. Computing the expected influence over all h1, we have Eh1 [\u03b9f(h1))] = Eh1 \ufffd EX,Uf [I(h1(X) \u0338= h1(X\u2212fUf))] \ufffd Let \u03b8 = \u03d5(X\u2212fUf) \u2227\u00ac\u03d5(X). Then, P(\u03b8) = \u03b3 =\u03b3 EX,Uf [Eh1 [I(h1(X) \u0338= h1(X\u2212fUf))] | \u03b8] + (1 \u2212\u03b3) EX,Uf [Eh1 [I(h1(X) \u0338= h1(X\u2212fUf))] | \u00ac\u03b8] \ufffd if \u03b8, then Eh1 [I(h1(X) \u0338= h1(X\u2212fUf))] = 1 2 \ufffd \u2265\u03b3 1 2 + (1 \u2212\u03b3)0 =\u03b3/2. By an averaging argument, there exists an h\u2217 1 such that \u03b9f(h\u2217 1) \u2265\u03b3/2. Similarly, computing the expected influence over all h2, we have Eh2 [\u03b9f(h2))] = Eh2 \ufffd EX,Uf [I(h2(X) \u0338= h2(X\u2212fUf))] \ufffd Let \u03b8 = \u03d5(X\u2212fUf) \u2227\u00ac\u03d5(X). Then, P(\u03b8) = \u03b3 =\u03b3 EX,Uf [Eh2 [I(h2(X) \u0338= h2(X\u2212fUf))] | \u03b8] + (1 \u2212\u03b3) EX,Uf [Eh2 [I(h2(X) \u0338= h2(X\u2212fUf))] | \u00ac\u03b8] \u2264\u03b3 1 2 + (1 \u2212\u03b3)1 =1 \u2212\u03b3/2\nBy an averaging argument, there exists an h\u2217 1 such that \u03b9f(h\u2217 1) \u2265\u03b3/2. Similarly, computing the expected influence over all h2, we have Eh2 [\u03b9f(h2))] = Eh2 \ufffd EX,Uf [I(h2(X) \u0338= h2(X\u2212fUf))] \ufffd Let \u03b8 = \u03d5(X\u2212fUf) \u2227\u00ac\u03d5(X). Then, P(\u03b8) = \u03b3 =\u03b3 EX,Uf [Eh2 [I(h2(X) \u0338= h2(X\u2212fUf))] | \u03b8] + (1 \u2212\u03b3) EX,Uf [Eh2 [I(h2(X) \u0338= h2(X\u2212fUf))] | \u00ac\u03b8] \u2264\u03b3 1 2 + (1 \u2212\u03b3)1 =1 \u2212\u03b3/2\nAgain, by an averaging argument, there exists an h\u2217 2 such that \u03b9f(h\u2217 2) \u22641 \u2212\u03b3/2\n# 3.2 Relating counterfactual and true accuracies\nWe now show that if the two models agree on both the true and the counterfactual distributions, then they have similar influences.\nDefinition 3. Given a loss function l and predictors h and h\u2032, the expected loss of the h with respect to h\u2032, written err(h, h\u2032, X), is\nerr(h, h\u2032, X) = EX [l0-1(h(X), h\u2032(X))] .\nTheorem 2. If err(h, h\u2032, X) \u2264 \u03f51, and err(h, h\u2032, X\u2212fUf) \u2264\u03f52, then |\u03b9f(h) \u2212\u03b9f(h\u2032)| \u2264\u03f51 + \u03f52\nProof. |\u03b9(h, f) \u2212\u03b9(h\u2032, f)| = \ufffd\ufffdEX,Uf [h(X) \u0338= h(X\u2212fUf)] \u2212EX,Uf [h\u2032(X) \u0338= h\u2032(X\u2212fUf)] \ufffd\ufffd = \ufffd\ufffdEX,Uf [|h(X) \u2212h(X\u2212fUf)|] \u2212EX,Uf [|h\u2032(X) \u2212h\u2032(X\u2212fUf)|] \ufffd\ufffd\n\ufffd \u2212 by triangle inequality\n\ufffd \u2264EX,Uf [|h(X) \u2212h(X\u2212fUf) \u2212h\u2032(X) + h\u2032(X\u2212fUf)|] = EX,Uf [|h(X) \u2212h\u2032(X) + h\u2032(X\u2212fUf) \u2212h(X\u2212fUf)|] by triangle inequality \u2264EX,Uf [|h(X) \u2212h\u2032(X)|]\n\u2264| \u2212| + EX,Uf [|h\u2032(X\u2212fUf) \u2212h(X\u2212fUf)|] =err(h, h\u2032, X) + err(h, h\u2032, X\u2212fUf) \u2264\u03f51 + \u03f52\n\n# 4 Counterfactual Active Learning\nIn this section, we describe an active learning algorithm for training a model that pushes the model towards the desired causal influences. The learning is assisted by two oracles. The first is a feature selection oracle that examines the causal influences of input features, and chooses the feature for which counterfactuals should be labeled. We envision this oracle to be a domain expert that can identify problematic causal influences based on background knowledge of causal factors or ethical norms governing the classification task. The second oracle, similar to a standard active learning oracle, labels counterfactual points with their intended label.\nAlgorithm 1 Counterfactual active learning.\nAlgorithm 1 Counterfactual active learning.\nRequire: Labeling oracle O, Feature selection oracle F\nprocedure COUNTERFACTUALACTIVELEARNING(D, k)\nD: training dataset\nk: labeling batch size\nc \u2190train(D)\nrepeat\n\u20d7\u03b9 \u2190feature influences QII(c, D)\nf \u2190feature F(\u20d7\u03b9)\n\u02c6C\n$\n\u2190\u2212\nk D\u2212fUf\n\u02c6y \u2190oracle labels O( \u02c6C)\nD \u2190D \u222a\u27e8\u02c6C, \u02c6y\u27e9\nc \u2190train(D)\nuntil stopping condition reached\nreturn c\nend procedure\nThe active learning process (Algorithm 1), on every iteration, computes the influences of features of a classifier trained on the dataset. The feature selection oracle F picks a feature. Then k points \u02c6C are picked from the counterfactual distribution, where k is a pre-specified batch size parameter. The\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fbd2/fbd2b1d6-674a-4d7a-be1e-22e855b9a323.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ece0/ece02362-45cc-40ed-8b1f-621fd20f1a08.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Convergence of difference of influence</div>\n<div style=\"text-align: center;\">(b) Convergence of in-distribution error</div>\nFigure 2: Rates of convergence of counterfactual active learning with guided, random, and non \u2212counterfactual settings.\ns of convergence of counterfactual active learning with guided, random, and non \u2212counterfactual settings.\nparameter k can also be thought of as a learning rate for the algorithm. The k points in \u02c6C are then labeled by the oracle O and added to the training set. A new classifier is trained on the augmented dataset and this process is repeated until the stopping condition is reached. The stopping condition can either be a pre-specified number of iterations or a convergence condition when the classifier learnt does not show a significant change in influences. The choice of the feature selection oracle affect the speed of convergence of the algorithm. In our experiments, we consider two feature selection oracles (i) a baseline random oracle, that picks features at random for generating counterfactual queries, (ii) a guided oracle, that picks the feature that has the highest difference in influence from the true influence. In Section 5.2, we demonstrate that an oracle that deterministically picks the feature with the highest difference in influence converges faster than an oracle that picks a feature at random. The rationale for training the classifier on points from the counterfactual distribution is two-fold. First, by adding points from the counterfactual distribution, the algorithm reduces the divergence between the training distribution and the counterfactual distribution, as a result, constraining the feature in-\n<div style=\"text-align: center;\">(c) Convergence of out of distribution error</div>\nfluences of the learnt classifier according to Theorem 1. Additionally, by increasing accuracy of the classifier with respect to the labeler on the counterfactual distribution, the influences of the trained classifier are pushed closer to the influence of the labeler (Theorem 2).\n# 5 Evaluation\nIn this section, we evaluate the counterfactual active learning algorithm for linear, decision tree, and random forest models using a synthetic labeler as ground truth. In particular, we demonstrate that after counterfactual active learning, the trained classifier has similar causal influence measures to the labeler. We also show that the classifier can generalize better to out-of-distribution points. This is an important consequence of having causal behavior similar to the labeler. And finally, we demonstrate that the accuracy on the data distribution does not degrade as a result of this additional training.\n# 5.1 Methodology\nWe evaluate our algorithm by training two predictors. The first predictor provides the ground truth to be used by the oracles. The second predictor is trained on a biased version of the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9bc6/9bc687bc-66d9-496c-8937-63319e086950.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Convergence of difference of influence Mean Square Error of influence</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2057/20575516-12f5-4a67-98e5-8b5872f7dd1f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Convergence of difference of influence</div>\nFigure 3: Rates of convergence of counterfactual active learning with guided, random, and non \u2212counterfactual settings.\ndataset used to train the first predictor. This approach induces a difference in influence in the two predictors. In more detail, the following steps comprise our experimental methodology. \u2022 Given: D a dataset which is a sample from the original distribution, \u2022 Train ground truth model ht on D. This model is used by the labeling oracle to respond to counterfactual queries. \u2022 Select a random predicate \u03b8. \u2022 Construct Db by excluding points from D that satisfy \u03b8. \u2022 Train predictor hb on a random subset of Db, leaving the rest for testing and for use by the non \u2212counterfactual baseline described below. \u2022 Perform counterfactual active learning on predictor hb.\n\u2022 The random predicate \u03b8 is chosen by training a short decision tree on the dataset with random labels. Db is intended to simulate a biased data collection mechanism in order to induce artificial correlations in the dataset. The induced artificial correlations create a gap between the counterfactual distribution and the data distribution, thus making the feature influences unconstrained.\nWe run the active learning algorithm under the following settings: \u2022 guided: At each iteration, the feature selection oracle selects the feature with the highest difference in auQII with respect to the base model. \u2022 random: This is a baseline where the feature selection oracle selects a feature at random. \u2022 non \u2212counterfactual: This is another baseline where the labeling oracle labels fresh points from Db as opposed to from the counterfactual distribution. The experiments presented here are run on the following datasets: \u2022 adult: The benchmark adult dataset [Lichman, 2013], a subset of census data, is used to predict income from 13 demographic factors including age and marital status. \u2022 arrests: This data set is used to predict a history of arrests using 6 features extracted from the National Longitudinal Survey of Youth [Bureau of Labor Statistics, 1997] such as drug and alcohol use. \u2022 lending club: A data set of loans originated by Lending Club [Lending Club, 2016] is used to predict chargeoffs from 19 other financial variables about individuals. These datasets represent prediction tasks that could be potentially used in settings such as predictive policing or credit, and where data processors are accountable for reasons behind prediction. All experiments presented here are run with a batch size t of 200 for 20 epochs, and averaged over 50 runs of the algorithm.\nWe run the active learning algorithm under the following settings: \u2022 guided: At each iteration, the feature selection oracle selects the feature with the highest difference in auQII with respect to the base model. \u2022 random: This is a baseline where the feature selection oracle selects a feature at random. \u2022 non \u2212counterfactual: This is another baseline where the labeling oracle labels fresh points from Db as opposed to from the counterfactual distribution. The experiments presented here are run on the following datasets: \u2022 adult: The benchmark adult dataset [Lichman, 2013], a subset of census data, is used to predict income from 13 demographic factors including age and marital status. \u2022 arrests: This data set is used to predict a history of arrests using 6 features extracted from the National Longitudinal Survey of Youth [Bureau of Labor Statistics, 1997] such as drug and alcohol use. \u2022 lending club: A data set of loans originated by Lending Club [Lending Club, 2016] is used to predict chargeoffs from 19 other financial variables about individuals. These datasets represent prediction tasks that could be potentially used in settings such as predictive policing or credit, and where data processors are accountable for reasons behind prediction. All experiments presented here are run with a batch size t of 200 for 20 epochs, and averaged over 50 runs of the algorithm.\n# 5.2 Results\nFigure 2 shows the evolution of the active learning algorithm for the three datasets dataset with a logistic regression model. In particular, Figure 2a shows the the change of the mean square error of auQII between hb and ht. This figure shows that the feature influences converge to values close to that of ht with the guided oracle. The random oracle also converges but at a slower rate. This result is useful as it indicates that the process does not require the feature selection oracle to pick optimally. The non \u2212counterfactual algorithm does not affect the influence measures significantly. This is to be expected since it retrains using labeled points from within the biased distribution. In Figures 2b and 2c, we show the accuracy of the classifier on holdout sets from Db and D respectively. Figure 2b shows that the error on the data distribution does not increase due to this additional training. Further, Figure 2c shows that the error on the unbiased dataset D also decreases, even though parts of D are not in the training set. This can be viewed as a side-effect of the model becoming causally closer to the ground truth model. For all three datasets the guided oracle leads to faster convergence across the three metrics. The arrests dataset shows similar behavior for the random and guided oracles which can be attributed to the dataset only containing a small number of features.\nFigures 3a and 3b show the effect of counterfactual active learning on decision trees and random forests. They both show a similar trend to linear models of the causal influences of hb converging toward those of ht with counterfactual active learning.\n# 6 Conclusion and Future Work\nWe articulate the problem of covariate shift in causal testing and formally characterize conditions under which it arises. We present an algorithm for counterfactual active learning that addresses this problem. We empirically demonstrate with synthetic labelers that our algorithm trains models that (i) have similar causal influences as the labeler\u2019s model, and (ii) generalize better to out-of-distribution points while (iii) retaining their accuracy on in-distribution points. In this paper, we assume that the labeling oracle can label all points with equal certainty and cost. However, for points further from the distribution, the labeler might need to perform real experiments in order to label the points. This suggests two interesting directions for future work. The first studies mechanisms for answering counterfactual queries for points far away from the distribution. The second involves designing an algorithm that takes into account the cost of a labeler in the learning process.\n# Acknowledgments\nThis work was developed with the support of NSF grants CNS1704845 as well as by the Air Force Research Laboratory under agreement number FA9550-17-1-0600. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright notation thereon. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Air Force Research Laboratory, the National Science Foundation, or the U.S. Government.\n# References\n[Adler et al., 2018] Philip Adler, Casey Falk, Sorelle A. Friedler, Tionney Nix, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. Knowledge and Information Systems, 54(1):95\u2013122, Jan 2018. [Angwin et al., 2016] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias: There\u2019s software used across the country to predict future criminals. and its biased against blacks. ProPublica, May 2016. [Breiman, 2001] Leo Breiman. Random forests. Mach. Learn., 45(1):5\u201332, October 2001. [Bureau of Labor Statistics, 1997] Bureau of Labor Statistics. National longitudinal surveys. http://www.bls. gov/nls/, 1997. [Burger, 1971] Warren Burger. Griggs v. duke power company. Opinion of the United States Supreme Court, March 1971. [Byrnes, 2017] Nanette Byrnes. An ai-fueled credit formula might help you get a loan. ProPublica, 2017.\n[Datta et al., 2016] Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems. In Proceedings of IEEE Symposium on Security & Privacy 2016, 2016. [Datta et al., 2017] Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen. Proxy nondiscrimination in data-driven systems. Technical report, arXiv, July 2017. [European Commission, 2016] European Commission. General data protection regulation (GDPR). Regulation (EU) 2016/679, L119, May 2016. [He and Geng, 2008] Yangbo He and Zhi Geng. Active learning of causal networks with intervention experiments and optimal designs. 9:2523\u20132547, 11 2008. [Hyttinen et al., 2013] Antti Hyttinen, Frederick Eberhardt, and Patrik O. Hoyer. Experiment selection for causal discovery. Journal of Machine Learning Research, 14:3041\u2013 3071, 2013. [Ideal Inc., 2017] Ideal Inc. How ai can stop unconscious bias in recruiting. https://ideal.com/ unconscious-bias/, 2017. Accessed Nov. 22, 2017. [Kilbertus et al., 2017] N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch\u00a8olkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems 30, pages 656\u2013666. Curran Associates, Inc., 2017. [Lending Club, 2016] Lending Club. Lending club loan data. https://www.kaggle.com/wendykan/ lending-club-loan-data, 2016. [Lichman, 2013] M. Lichman. UCI machine learning repository, 2013. [Ribeiro et al., 2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201dwhy should i trust you?\u201d: Explaining the predictions of any classifier. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pages 1135\u2013 1144, New York, NY, USA, 2016. ACM. [Rubenstein et al., 2017] P. K. Rubenstein, I. Tolstikhin, P. Hennig, and B. Sch\u00a8olkopf. Probabilistic active learning of functions in structural causal models. 2017. [Spirtes et al., 2000] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2000. [Tong and Koller, 2001] Simon Tong and Daphne Koller. Active learning for structure in bayesian networks. In Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI\u201901, pages 863\u2013869, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.\n",
    "paper_type": "method",
    "attri": {
        "background": "Causal influence measures for machine learnt classifiers are essential for understanding classification reasons, identifying influential input features, and revealing biases. Previous methods have not effectively constrained classifier behavior on atypical datapoints, leading to a problem termed covariate shift in causal testing. This necessitates a new approach to improve causal influence assessments.",
        "problem": {
            "definition": "The problem is the inability of standard classifiers to distinguish among models that yield similar predictions on training data but have different causal influences, particularly when evaluated on atypical datapoints.",
            "key obstacle": "Existing methods do not account for the behavior of classifiers on atypical points, leading to potentially misleading causal influence measures."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to ensure that causal influence measures are reliable, particularly when classifiers are tested on atypical datapoints that differ from the training distribution.",
            "opinion": "The proposed solution is a novel active learning algorithm that constrains the influence measures of trained models to enhance their reliability.",
            "innovation": "The key innovation lies in the active learning approach that integrates feedback from feature selection and labeling oracles to refine causal influence measures."
        },
        "method": {
            "method name": "Counterfactual Active Learning",
            "method abbreviation": "CAL",
            "method definition": "A learning algorithm that adjusts the training process based on causal influences, leveraging counterfactual queries to refine model behavior.",
            "method description": "The method employs an iterative process where features are selected, counterfactual points are labeled, and the model is retrained to align its causal influences with those of a ground truth model.",
            "method steps": [
                "Train an initial model on a dataset.",
                "Compute feature influences using a causal influence measure.",
                "Select a feature based on its influence.",
                "Generate counterfactual points and label them.",
                "Augment the training set with labeled counterfactuals.",
                "Retrain the model and repeat until convergence."
            ],
            "principle": "The effectiveness of this method is based on the premise that constraining model behavior on counterfactual distributions will lead to more accurate causal influence assessments."
        },
        "experiments": {
            "evaluation setting": "The algorithm was evaluated on linear, decision tree, and random forest models using datasets like adult, arrests, and lending club, with synthetic labelers providing ground truth.",
            "evaluation method": "Performance was assessed by comparing causal influences and prediction accuracy on both in-distribution and out-of-distribution points after applying the active learning algorithm."
        },
        "conclusion": "The experiments validate that the proposed algorithm effectively aligns model causal influences with those of a synthetic labeler, enhances generalization to out-of-distribution points, and maintains accuracy on in-distribution points.",
        "discussion": {
            "advantage": "The main advantages include improved alignment of causal influences with ground truth models and better generalization capabilities.",
            "limitation": "The method assumes that the labeling oracle can provide accurate labels for all points, which may not hold true for points significantly different from the training distribution.",
            "future work": "Future research could explore mechanisms for labeling counterfactual queries for points far from the distribution and develop algorithms that consider the cost of labeling in the learning process."
        },
        "other info": {
            "acknowledgments": "This work was supported by NSF grants CNS1704845 and the Air Force Research Laboratory under agreement number FA9550-17-1-0600."
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "Causal influence measures for machine learnt classifiers are essential for understanding classification reasons, identifying influential input features, and revealing biases."
        },
        {
            "section number": "2.3",
            "key information": "The proposed solution is a novel active learning algorithm that constrains the influence measures of trained models to enhance their reliability."
        },
        {
            "section number": "4.2",
            "key information": "Existing methods do not account for the behavior of classifiers on atypical points, leading to potentially misleading causal influence measures."
        },
        {
            "section number": "4.5",
            "key information": "The effectiveness of the Counterfactual Active Learning method is based on the premise that constraining model behavior on counterfactual distributions will lead to more accurate causal influence assessments."
        },
        {
            "section number": "7.3",
            "key information": "Future research could explore mechanisms for labeling counterfactual queries for points far from the distribution and develop algorithms that consider the cost of labeling in the learning process."
        }
    ],
    "similarity_score": 0.4659193936041515,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0632_artif/papers/Supervising Feature Influence.json"
}