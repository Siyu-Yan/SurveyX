{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2002.02277",
    "title": "Interpretable Goal-based Prediction and Planning for Autonomous Driving",
    "abstract": "We propose an integrated prediction and planning system for autonomous driving which uses rational inverse planning to recognise the goals of other vehicles. Goal recognition informs a Monte Carlo Tree Search (MCTS) algorithm to plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS utilise a shared set of defined maneuvers and macro actions to construct plans which are explainable by means of rationality principles. Evaluation in simulations of urban driving scenarios demonstrate the system's ability to robustly recognise the goals of other vehicles, enabling our vehicle to exploit non-trivial opportunities to significantly reduce driving times. In each scenario, we extract intuitive explanations for the predictions which justify the system's decisions.",
    "bib_name": "albrecht2021interpretablegoalbasedpredictionplanning",
    "md_text": "#  Goal-based Prediction and Planning for Aut\n# retable Goal-based Prediction and Planning \nStefano V. Albrecht\u2217\u2020, Cillian Brewitt\u2217\u2020, John Wilhelm\u2217\u2020, Balint Gyevnar\u2217\u2020, Francisco Eiras\u2217\u2021, Mihai Dobre\u2217, Subramanian Ramamoorthy\u2217\u2020 \u2217Five AI Ltd., UK, {firstname.lastname}@five.ai \u2020School of Informatics, University of Edinburgh, UK \u2021Department of Engineering Science, University of Oxford, UK\nAbstract\u2014 We propose an integrated prediction and planning system for autonomous driving which uses rational inverse planning to recognise the goals of other vehicles. Goal recognition informs a Monte Carlo Tree Search (MCTS) algorithm to plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS utilise a shared set of defined maneuvers and macro actions to construct plans which are explainable by means of rationality principles. Evaluation in simulations of urban driving scenarios demonstrate the system\u2019s ability to robustly recognise the goals of other vehicles, enabling our vehicle to exploit non-trivial opportunities to significantly reduce driving times. In each scenario, we extract intuitive explanations for the predictions which justify the system\u2019s decisions.\n15 Mar 2021\n[cs.RO]\n# I. INTRODUCTION\nI. INTRODUCTION\nThe ability to predict the intentions and driving trajectories of other vehicles is a key problem for autonomous driving [1]. This problem is significantly complicated by the need to make fast and accurate predictions based on limited observation data which originate from coupled multi-agent interactions. To make prediction tractable in such conditions, a standard approach in autonomous driving research is to assume that vehicles use one of a finite number of distinct high-level maneuvers, such as lane-follow, lane-change, turn, stop, etc. [2], [3], [4], [5], [6], [7]. A classifier of some type is used to detect a vehicle\u2019s current executed maneuver based on its observed driving trajectory. The limitation in such methods is that they only detect the current maneuver of other vehicles, hence planners using such predictions are effectively limited to the timescales of the detected maneuvers. An alternative approach is to specify a finite set of possible goals for each other vehicle (such as road exit points) and to plan a full trajectory to each goal from the vehicle\u2019s observed local state [8], [9], [10]. While this approach can generate longer-term predictions, a limitation is that the generated trajectories must be matched relatively closely by a vehicle in order to yield high-confidence predictions of the vehicle\u2019s goals. Recent methods based on deep learning have shown promising results for trajectory prediction in autonomous driving [11], [12], [13], [14], [15], [16], [17]. Prediction models are trained on large datasets that are becoming available through data gathering campaigns involving sensorised vehicles (e.g. video, lidar, radar). Reliable prediction over several second horizons remains a hard problem, in part due to the difficulties\nS.A. is supported by a Royal Society Industry Fellowship. C.B., J.W., B.G. were interns at Five AI with partial financial support from the Royal Society and UKRI. IGP2 code: https://github.com/uoe-agents/IGP2\nin capturing the coupled evolution of traffic. In our view, one of the most significant limitations of this class of methods (though see recent progress [18]) is the difficulty in extracting interpretable predictions in a form that is amenable to efficient integration with planning methods that effectively represent multi-dimensional and hierarchical task objectives. Our starting point is that in order to predict the future maneuvers of a vehicle, we must reason about why \u2013 that is, to what end \u2013 the vehicle performed its past maneuvers, which will yield clues as to its intended goal [19]. Knowing the goals of other vehicles enables prediction of their future maneuvers and trajectories, which facilitates planning over extended timescales. We show in our work (illustrated in Figure 2) how such reasoning can help to address the problem of overlyconservative autonomous driving [20]. Further, to the extent that our predictions are structured around the interpretation of observed trajectories in terms of high-level maneuvers, the goal recognition process lends itself to intuitive interpretation for the purposes of system analysis and debugging, at a level of detail suggested in Figure 2. As we develop towards making our autonomous systems more trustworthy [21], these notions of interpretation and the ability to justify (explain) the system\u2019s decisions are key [22]. To this end, we propose Interpretable Goal-based Prediction and Planning (IGP2) which leverages the computational advantages of using a finite space of maneuvers, but extends the approach to planning and prediction of sequences (i.e., plans) of maneuvers. We achieve this via a novel integration of rational inverse planning [23], [24] to recognise the goals of other vehicles, with Monte Carlo Tree Search (MCTS) [25] to plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS utilise a shared set of defined maneuvers to construct plans which are explainable by means of rationality principles, i.e. plans are optimal with respect to given metrics. We evaluate IGP2 in simulations of diverse urban driving scenarios, showing that (1) the system robustly recognises the goals of other vehicles, even if significant parts of a vehicle\u2019s trajectory are occluded, (2) goal recognition enables our vehicle to exploit opportunities to improve driving efficiency as measured by driving time compared to other prediction baselines, and (3) we are able to extract intuitive explanations for the predictions to justify the system\u2019s decisions. In summary, our contributions are: \u2022 A method for goal recognition and multi-modal trajectory\n\u2022 A method for goal recognition and multi-modal trajectory prediction via rational inverse planning.\n\u2022 Integration of goal recognition with MCTS planning to generate optimised plans for the ego vehicle. \u2022 Evaluation in simulated urban driving scenarios showing accurate goal recognition, improved driving efficiency, and ability to interpret the predictions and ego plans.\n# II. PRELIMINARIES AND PROBLEM DEFINITION\nLet I be the set of vehicles in the local neighbourhood of the ego vehicle (including itself). At time t, each vehicle i \u2208I is in a local state si t \u2208Si, receives a local observation oi t \u2208Oi, and can choose an action ai t \u2208Ai. We write st \u2208S = \u00d7iSi for the joint state and sa:b for the tuple (sa, ..., sb), and similarly for ot \u2208O, at \u2208A. Observations depend on the joint state via p(oi t|st), and actions depend on the observations via p(ai t|oi 1:t). In our system, a local state contains a vehicle\u2019s pose, velocity, and acceleration (we use the terms velocity and speed interchangeably); an observation contains the poses and velocities of nearby vehicles; and an action controls the vehicle\u2019s steering and acceleration. The probability of a sequence of joint states s1:n is given by\nwhere p(st+1|st, at) defines the joint vehicle dynamics, and we assume independent local observations and actions, p(ot|st) = \ufffd i p(oi t|st) and p(at|o1:t) = \ufffd i p(ai t|oi 1:t). Vehicles react to other vehicles via their observations oi 1:n. We define the planning problem as finding an optimal policy \u03c0\u2217which selects the actions for the ego vehicle, \u03b5, to achieve a specified goal, G\u03b5, while optimising the driving trajectory via a defined reward function. Here, a policy is a function \u03c0 : (O\u03b5)\u2217\ufffd\u2192A\u03b5 which maps an observation sequence o\u03b5 1:n to an action a\u03b5 t. (State filtering [26] is outside the scope of this work.) A goal can be any subset of local states, G\u03b5 \u2282S\u03b5. In this paper, we focus on goals that specify target locations and \u201cstopping goals\u201d which specify a target location and zero velocity. Formally, define\n(2)\n\ufffd \ufffd\ufffd    \ufffd where s\u03b5 n \u2208G\u03b5 means that s\u03b5 n satisfies G\u03b5. The second condition in (2) ensures that \ufffd\u221e n=1 \ufffd \u2126n p(s1:n)ds1:n \u22641 for any policy \u03c0, which is needed for soundness of the sum in (3). The problem is to find \u03c0\u2217such that\n(3)\nwhere Ri(s1:n) is vehicle i\u2019s reward for s1:n. We define Ri as a weighted sum of reward elements based on trajectory execution time, longitudinal and lateral jerk, path curvature, and safety distance to leading vehicle.\nIII. IGP2: INTERPRETABLE GOAL-BASED PREDICTION AND PLANNING\nOur general approach relies on two assumptions: (1) each vehicle seeks to reach some (unknown) goal from a set of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4046/40463259-2557-46fe-b9fe-35bd91e27c89.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: IGP2 system overview</div>\npossible goals, and (2) each vehicle follows a plan generated from a finite library of defined maneuvers. Figure 1 provides an overview of the components in our proposed IGP2 system. At a high level, IGP2 approximates the optimal ego policy \u03c0\u2217as follows: For each non-ego vehicle, generate its possible goals and inversely plan for that vehicle to each goal. The resulting goal probabilities and predicted trajectories for each non-ego vehicle inform the simulation process of a Monte Carlo Tree Search (MCTS) algorithm, to generate an optimal maneuver plan for the ego vehicle toward its current goal. In order to keep the required search depth in inverse planning and MCTS shallow (and thus efficient), both plan over a shared set of macro actions which flexibly concatenate maneuvers using context information. We detail these components in the following sections.\n# A. Maneuvers\nWe assume that at any time, each vehicle is executing one of the following maneuvers: lane-follow, lane-change-left/right, turn-left/right, give-way, stop. Each maneuver \u03c9 specifies applicability and termination conditions. For example, lanechange-left is only applicable if there is a lane in same driving direction to the left of the vehicle, and terminates once the vehicle has reached the new lane and its orientation is aligned with the lane. Some maneuvers have free parameters, e.g. follow-lane has a parameter to specify when to terminate. If applicable, a maneuver specifies a local trajectory \u02c6si 1:n to be followed by the vehicle, which includes a reference path in the global coordinate frame and target velocities along the path. For convenience in exposition, we assume that \u02c6si uses the same representation and indexing as si, but in general this does not have to be the case (for example, \u02c6s may be indexed by longitudinal position rather than time, which can be interpolated to time indices). In our system, the reference path is generated via a Bezier spline function fitted to a set of points extracted from the road topology, and target velocities are set using domain heuristics similar to [27].\n# B. Macro Actions\nMacro actions specify common sequences of maneuvers and automatically set the free parameters (if any) in maneuvers based on context information such as road layout. Table I specifies the macro actions used in our system. The applicability condition of a macro action is given by the applicability\n<div style=\"text-align: center;\">Macro action: Additional applicability condition:</div>\nMacro action:\nAdditional applicability condition:\nManeuver sequence (maneuver parameters in brackets):\nContinue\n\u2014\nlane-follow (end of visible lane)\nContinue next exit Must be in roundabout and not in outer-lane\nlane-follow (next exit point)\nChange left/right\nThere is a lane to the left/right\nlane-follow (until target lane clear), lane-change-left/right\nExit left/right\nExit point on same lane ahead of car and in correct direction lane-follow (exit point), give-way (relevant lanes), turn-left/right\nStop\nThere is a stopping goal ahead of the car on the current lane\nlane-follow (close to stopping point), stop\ncondition of the first maneuver in the macro action as well as optional additional conditions. The termination condition of a macro action is given by the termination condition of the last maneuver in the macro action.\n# C. Velocity Smoothing\nTo obtain a feasible trajectory across maneuvers for vehicle i, we define a velocity smoothing operation which optimises the target velocities in a given trajectory \u02c6si 1:n. Let \u02c6xt be the longitudinal position on the reference path at \u02c6si t and \u02c6vt its target velocity, for 1 \u2264t \u2264n. We define \u03ba : x \u2192v as the piecewise linear interpolation of target velocities between points \u02c6xt. Given the time elapsed between two time steps, \u2206t; the maximum velocity and acceleration, vmax/amax; and setting x1 = \u02c6x1, v1 = \u02c6v1, we define velocity smoothing as\n(4)\n| \u2212| where \u03bb > 0 is the weight given to the acceleration part of the optimisation objective. Eq. (4) is a nonlinear nonconvex optimisation problem which can be solved, e.g., using a primal-dual interior point method (we use IPOPT [28]). From the solution of the problem, (x2:n, v2:n), we interpolate to obtain the achievable velocities at the original points \u02c6xt.\n# D. Goal Recognition\nWe assume that each non-ego vehicle i seeks to reach one of a finite number of possible goals Gi \u2208Gi, using plans constructed from our defined macro actions. We use the framework of rational inverse planning [23], [24] to compute a Bayesian posterior distribution over i\u2019s goals at time t\n(5)\nwhere L(s1:t|Gi) is the likelihood of i\u2019s observed trajectory assuming its goal is Gi, and p(Gi) specifies the prior probability of Gi. The likelihood is a function of the reward difference between two plans: the reward \u02c6r of the optimal trajectory from i\u2019s initial observed state si 1 to goal Gi after velocity smoothing, and the reward \u00afr of the trajectory which follows the observed trajectory until time t and then continues optimally to goal Gi, with smoothing applied only to the trajectory after t. The likelihood is defined as\n(6)\n<div style=\"text-align: center;\">Maneuver sequence (maneuver parameters in brackets):</div>\nwhere \u03b2 is a scaling parameter (we use \u03b2 = 1). This likelihood definition assumes that vehicles drive approximately rationally (i.e., optimally) to achieve their goals while allowing for some deviation. If a goal is infeasible, we set its probability to zero. Algorithm 1 shows the pseudo code for our goal recognition algorithm, with further details in below subsections. 1) Goal Generation: A heuristic function is used to generate a set of possible goals Gi for vehicle i based on its location and context information such as road layout. In our system, we include goals for the visible end of the current road and connecting roads (bounded by the ego vehicle\u2019s view region). In addition to such static goals, it is also possible to add dynamic goals which depend on current traffic. For example, in the dense merging scenario shown in Figure 2d, stopping goals are dynamically added to model a vehicle\u2019s intention to allow the ego vehicle to merge in front. 2) Maneuver Detection: Maneuver detection is used to detect the current executed maneuver of a vehicle (at time t), allowing inverse planning to complete the maneuver before planning onward. We assume a module which computes probabilities over current maneuvers, p(\u03c9i), for each vehicle i. One option is Bayesian changepoint detection (e.g. [29]). The details of maneuver detection are outside the scope of our paper and in our experiments we use a simulated detector (cf. Sec IV-B). As different current maneuvers may hint at different goals, we perform inverse planning for each possible current maneuver for which p(\u03c9i) > 0. Thus, each current maneuver produces its associated posterior probabilities over goals, denoted by p(Gi | s1:t, \u03c9i). 3) Inverse Planning: Inverse planning is done using A* search [30] over macro actions. A* starts after completing the current maneuver \u03c9i which produces the initial trajectory \u02c6s1:\u03c4. Each search node q corresponds to a state s \u2208S, with initial node at state \u02c6s\u03c4, and macro actions are filtered by their applicability conditions applied to s. A* chooses the next macro action leading to a node q\u2032 which has lowest estimated total cost1 to goal Gi, given by f(q\u2032) = l(q\u2032) + h(q\u2032). The cost l(q\u2032) to reach node q\u2032 is given by the driving time from i\u2019s location in the initial search node to its location in q\u2032, following the trajectories returned by the macro actions leading to q\u2032. A* uses the assumption that all other vehicles not planned for use a constant-velocity lane-following model after their observed trajectories. We do not check for collisions during inverse planning. The cost heuristic h(q\u2032) to estimate remaining cost from q\u2032 to goal Gi is given by the driving time from i\u2019s location in q\u2032 to goal via straight line at speed\n1Here we use the term \u201ccost\u201d in keeping with standard A* terminology and to differentiate from the reward function defined in Sec. II.\nAlgorithm 1 Goal recognition algorithm\nInput: vehicle i, current maneuver \u03c9i, observations s1:t\nReturns: goal probabilities p(Gi|s1:t, \u03c9i)\n1: Generate possible goals Gi \u2208Gi from state si\nt\n2: Set prior probabilities p(Gi) (e.g. uniform)\n3: for all Gi \u2208Gi do\n4:\n\u02c6si\n1:n \u2190A*(\u03c9i) from \u02c6si\n1 = si\n1 to Gi\n5:\nApply velocity smoothing to \u02c6si\n1:n\n6:\n\u02c6r \u2190reward Ri(\u02c6si\n1:n)\n7:\n\u00afsi\n1:m \u2190A*(\u03c9i) from \u00afsi\nt to Gi, with \u00afsi\n1:t = si\n1:t\n8:\nApply velocity smoothing to \u00afsi\nt+1:m\n9:\n\u00afr \u2190reward Ri(\u00afsi\n1:m)\n10:\nL(s1:t|Gi, \u03c9i) \u2190exp(\u03b2(\u00afr \u2212\u02c6r))\n11: Return p(Gi|s1:t, \u03c9i) \u221dL(s1:t|Gi, \u03c9i) p(Gi)\nlimit. This definition of h(q\u2032) is admissible as per A* theory, which ensures that the search returns an optimal plan. After the optimal plan is found, we extract the complete trajectory \u02c6si 1:n from the maneuvers in the plan and initial segment \u02c6s1:\u03c4. 4) Trajectory Prediction: Our system predicts multiple plausible trajectories for a given vehicle and goal. This is required because there are situations in which different trajectories may be (near-)optimal but may lead to different predictions which could require different behaviour on the part of the ego vehicle. We run A* search for a fixed amount of time and let it compute a set of plans with associated rewards (up to some fixed number of plans). Any time A* search finds a node that reaches the goal, the corresponding plan is added to the set of plans. Given a set of smoothed trajectories {\u02c6si,k 1:n|\u03c9i, Gi}k=1..K to goal Gi with initial maneuver \u03c9i and associated reward rk = Ri(\u02c6si,k 1:n), we compute a distribution over the trajectories via a Boltzmann distribution\n(7)\nwhere \u03b3 is a scaling parameter (we use \u03b3 = 1). Similar to Eq. (6), Eq. (7) encodes the assumption that trajectories which are closer to optimal are more likely.\n# E. Ego Vehicle Planning\nTo compute an optimal plan for the ego vehicle, we use the goal probabilities and predicted trajectories to inform a Monte Carlo Tree Search (MCTS) algorithm [25] (see Algorithm 2). The algorithm performs a number of closed-loop simulations \u02c6st:n, starting in the current state \u02c6st = st down to some fixed search depth or until a goal state is reached. At the start of each simulation, for each non-ego vehicle, we first sample a current maneuver, then goal, and then trajectory for the vehicle using the associated probabilities (cf. Section IIID). Each node q in the search tree corresponds to a state s \u2208S and macro actions are filtered by their applicability conditions applied to s. After selecting a macro action \u00b5 using some exploration technique (we use UCB1 [31]), the state in the current search node is forward-simulated based on the trajectory generated by the macro action \u00b5 and the sampled trajectories of non-ego vehicles, resulting in a partial trajectory \u02c6s\u03c4:\u03b9 and new search node q\u2032 with state \u02c6s\u03b9. Forward-simulation\nAlgorithm 2 Monte Carlo Tree Search algorithm\nReturns: optimal maneuver for ego vehicle \u03b5 in state st\nPerform K simulations:\n1: Search node q.s \u2190st (root node)\n2: Search depth d \u21900\n3: for all i \u2208I \\ {\u03b5} do\n4:\nSample current maneuver \u03c9i \u223cp(\u03c9i)\n5:\nSample goal Gi \u223cp(Gi | s1:t, \u03c9i)\n6:\nSample trajectory \u02c6si\n1:n \u2208{\u02c6si,k\n1:n | \u03c9i, Gi} with p(\u02c6si,k\n1:n)\n7: while d < dmax do\n8:\nSelect macro action \u00b5 for \u03b5 applicable in q.s\n9:\n\u02c6s\u03c4:\u03b9 \u2190Simulate \u00b5 until it terminates, with non-ego vehicles\nfollowing their sampled trajectories \u02c6si\n1:n\n10:\nr \u2190\u2205\n11:\nif ego vehicle collides during \u02c6s\u03c4:\u03b9 then\n12:\nr \u2190rcoll\n13:\nelse if \u02c6s\u03b5\n\u03b9 achieves ego goal G\u03b5 then\n14:\nr \u2190R\u03b5(\u02c6st:n)\n15:\nelse if d = dmax \u22121 then\n16:\nr \u2190rterm\n17:\nif r \u0338= \u2205then\n18:\nUse (8) to backprop r along search branches (q, \u00b5, q\u2032) that\ngenerated the simulation\n19:\nStart next simulation\n20:\nq\u2032.s = \u02c6s\u03b9; q \u2190q\u2032; d \u2190d + 1\nReturn maneuver for \u03b5 in st, \u00b5 \u2208arg max\u00b5 Q(root, \u00b5)\nof trajectories uses a combination of proportional control and adaptive cruise control (based on IDM [32]) to control a vehicle\u2019s acceleration and steering. Termination conditions of maneuvers are monitored in each time step based on the vehicle\u2019s observations. Collision checking is performed on \u02c6s\u03c4:\u03b9 to check whether the ego vehicle collided, in which case we set the reward to r \u2190rcoll which is back-propagated using (8), where rcoll is a method parameter. Otherwise, if the new state \u02c6s\u03b9 achieves the ego goal G\u03b5, we compute the reward for back-propagation as r \u2190R\u03b5(\u02c6st:n). If the search reached its maximum depth dmax without colliding or achieving the goal, we set r \u2190rterm which can be a constant or based on heuristic reward estimates similar to A* search. The reward r is back-propagated through search branches (q, \u00b5, q\u2032) that generated the simulation, using a 1-step offpolicy update function (similar to Q-learning [33])\n(8)\nwhere \u03b4 is the number of times that macro action \u00b5 has been selected in q. After the simulations are completed, the algorithm selects the best macro action for execution in st from the root node, arg max\u00b5 Q(root, \u00b5).\nWe evaluate IGP2 in simulations of diverse urban driving scenarios, showing that: (1) our inverse planning method robustly recognises the goals of non-ego vehicles; (2) goal recognition leads to improved driving efficiency measured by driving time; and (3) intuitive explanations for the predictions\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7942/79429c4c-f290-4a49-ae1c-ad1bbc7b723e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/02ce/02ce980b-2649-4fd1-828a-3545ebfa46b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Scenario 2 (S2)</div>\n<div style=\"text-align: center;\">(a) Scenario 1 (S1)</div>\nFig. 2: IGP2 in 4 test scenarios. Ego vehicle shown in blue. Bar plots show goal probabilities for non-ego vehicles. For each goal, up to two of the most probable predicted trajectories to goal are shown with thickness proportional to probability. (a) S1: Ego\u2019s goal is blue goal. Vehicle V1 is on the ego\u2019s road, V1 changes from left to right lane, biasing the ego prediction towards the belief that V1 will exit, since a lane change would be irrational if V1\u2019s goal was to go east. As exiting will require a significant slowdown, the ego decides to switch lanes to avoid being slowed down too. (b) S2: Ego\u2019s goal is blue goal. Vehicle V1 is approaching the junction from the east and vehicle V2 from the west. As V1 approaches the junction, slows down and waits to take a turn, the ego\u2019s belief that V1 will turn right increases significantly, since it would be irrational to stop if the goal was to turn left or go straight. Since the ego recognised V1\u2019s goal is to go north, it predicts that V1 will wait until V2 has passed, giving the ego an opportunity to enter the road. (c) S3: Ego\u2019s goal is green goal. As V1 changes from the inside to the outside lane of the roundabout and decreases its speed, it significantly biases the ego prediction towards the belief that V1 will take the south exit since that is the rational course of action for that goal. This encourages the ego to enter the roundabout while V1 is still in roundabout. (d) S4: Ego\u2019s goal is purple goal. With two vehicles stopped at the junction at a traffic light, vehicle V1 is approaching them from behind, and vehicle V2 is crossing in the opposite direction. When V1 reaches zero velocity, the goal generation function adds a stopping goal (orange) for V1 in its current position, shifting the goal distribution towards it since stopping is not rational for the north/west goals. The interpretation is that V1 wants the ego to merge in front of V1, which the ego then does.\ncan be extracted to justify the system\u2019s decisions. (Video showing IGP2 in action: https://www.five.ai/igp2.)\n# A. Scenarios\nWe use two sets of scenario instances. For in-depth analysis of goal recognition and planning, we use four defined local interaction scenarios shown in Figure 2. For each of these scenarios, we generate 100 instances with randomly offset initial longitudinal positions (\u223c[\u221210, +10] meters) and initial speed sampled from range [5, 10] m/s for each vehicle including ego vehicle. Here the ego vehicle observes the whole scenario. To further assess IGP2\u2019s ability to complete full routes with random traffic, we use two random town layouts shown in Figure 3. Each town spans an area of 0.16 square kilometers and consists of roads, crossings, and roundabouts with 2\u20134 lanes each. Each junction has one defined priority road. The ego vehicle\u2019s observation radius in towns is 50 meters. Non-ego vehicles are spawned within 25 meters outside the ego observation radius, with random road, lane, speed, and goal. The total number of non-ego vehicles within the ego radius and spawning radius is kept at 8 to maintain a consistent medium-to-high level of traffic. In each town we generate 10 instances by choosing random routes for the ego vehicle to complete. The ego vehicle\u2019s goal is continually updated to be the outermost point on the route within the ego observation radius. In all simulations, the non-ego vehicles use manual heuristics to select from the maneuvers in Section III-A to reach their goals. All vehicles use independent proportional controllers for acceleration and steering, and IDM [32] for automatic distance-keeping. Vehicle motion is simulated using a kinematic bicycle model.\n# B. Algorithms & Parameters\nWe compare the following algorithms in scenarios S1\u2013S4. IGP2: full system using goal recognition and MCTS. IGP2MAP: like IGP2, but MCTS uses only the most probable goal and trajectory for each vehicle. CVel: MCTS without\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3feb/3febaf5b-7dc2-4082-ac92-6bf7b903c853.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2bd0/2bd0cead-b509-46ab-bb47-2dfaecba72cc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Scenario 3 (S3)</div>\n<div style=\"text-align: center;\">(d) Scenario 4 (S4)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9451/94516b62-ff90-43b3-9cd8-ab5e96096431.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3: Town 1 and Town 2 layouts.</div>\ngoal recognition, replaced by constant-velocity lane-following prediction after completion of current maneuver. CVel-Avg: like CVel, but uses velocity averaged over the past 2 seconds. Cons: like CVel, but using a conservative give-way maneuver which always waits until all oncoming vehicles on priority lanes have passed. In the town scenarios we focus on IGP2 and Cons, and additionally compare to SH-CVel which works similarly to MPDM [5]: it simulates each macro action followed by a default Continue macro action, using CVel prediction for non-ego vehicles, then choosing the macro action with maximum estimated reward. (SH stands for \u201cshort horizon\u201d as the search depth is effectively limited to 1.) We simulate noisy maneuver detection (cf. Sec. III-D.2) by giving 0.9 probability to the current executed maneuver of the non-ego vehicle and the rest uniformly to other maneuvers. Prior probabilities over non-ego goals are uniform. A* computes up to two predicted trajectories for each nonego vehicle and goal. MCTS is run at a frequency of 1 Hz, performs K = 30 simulations with a maximum search depth of dmax = 5, and uses rcoll = rterm = \u22121. We set \u03bb = 10 for velocity smoothing (cf. Eq. (4)).\n# C. Results\n1) Goal probabilities: Figure 4 shows the average probability over time assigned to the true goal in scenarios S1\u2013 S4. In all tested scenario instances we observe that the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f355/f3553f39-19c9-41f4-9b44-fed1bb687b53.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4: Average probability given to true goal of selected vehicles in scenarios S1\u2013S4. Note: lines for S1/S3 are shorter than indicated in Tab. II since possible vehicle goals change after exit points are reached and we only show lines for initial possible goals.</div>\nprobability increases with growing evidence and at different rates depending on random scenario initialisation. Snapshots of goal probabilities (shown as bar plots) associated with the non-ego\u2019s most probable current maneuver can be seen in Figure 2. We also tested the method\u2019s robustness to missing segments in the observed trajectory of a vehicle. In scenarios S1 and S3 we removed the entire lane-change maneuver from the observed trajectory (but keeping the short lane-follow segment before the lane change). To deal with occlusion, we applied A* search before the beginning of each missing segment to reach the beginning of the next observed segment, thereby \u201cfilling the gaps\u201d in the trajectory. Afterwards we applied velocity smoothing to the reconstructed trajectory. The results are shown as dashed lines in Figure 4, showing that even under significant occlusion the method is able to correctly recognise a vehicle\u2019s goal. 2) Driving times: Table II shows the average driving times required of each algorithm in scenarios S1\u2013S4. Goal recognition enabled IGP2 and IGP2-MAP to reduce their driving times. (S1) All algorithms change lanes to avoid being slowed down by V1, leading to same driving times, however IGP2 and IGP2-MAP initiate the lane change before all other algorithms by recognising V1\u2019s intended goal. (S2) Cons waits for V1 to clear the lane, which in turn must wait for V2 to pass. IGP2 and IGP2-MAP anticipate this behaviour, allowing them to enter the road earlier. CVel and CVel-Avg wait for V1 to reach near-zero velocity. (S3) IGP2 and IGP2MAP are able to enter early as they recognise V1\u2019s goal to exit the roundabout, while CVel, CVel-Avg, and Cons wait for V1 to exit. (S4) Cons waits until V1 decides to close the gap after which the ego can enter the road. IGP2 and IGP2-MAP recognise V1\u2019s goal and merge in front. IGP2-MAP achieved shorter driving times than IGP2 on some scenario instances (such as S3 and S4). This is because IGP2-MAP commits to the most-likely goal and trajectory of other vehicles, while IGP2 also considers residual uncertainty about goals and trajectories which may lead MCTS to select more cautious actions in some situations. The limitation of IGP2-MAP can be seen when simulating unexpected (irrational) behaviours in other vehicles. To test this, we compared IGP2 and IGP2-MAP on instances from S3 and S4 which were modified such that V1, after slowing down,\nS1\nS2\nS3\nS4\nIGP2\n5.97 \u00b1 .02\n7.24 \u00b1 .05\n8.54 \u00b1 .05\n10.83 \u00b1 .03\nIGP2-MAP 5.99 \u00b1 .02\n7.23 \u00b1 .05\n8.36 \u00b1 .06\n10.40 \u00b1 .03\nCVel\n6.04 \u00b1 .03\n9.80 \u00b1 .17\n10.49 \u00b1 .09 12.83 \u00b1 .03\nCVel-Avg\n6.01 \u00b1 .02 11.31 \u00b1 .17 10.49 \u00b1 .09 13.59 \u00b1 .02\nCons\n6.01 \u00b1 .02 12.89 \u00b1 .03 10.90 \u00b1 .04 16.78 \u00b1 .02\nTABLE II: Average driving time (seconds) required to complete scenario instances from S1\u2013S4, with standard error.\n<div style=\"text-align: center;\">TABLE II: Average driving time (seconds) required to complete scenario instances from S1\u2013S4, with standard error.</div>\nTABLE II: Average driving time (seconds) required to complete scenario instances from S1\u2013S4, with standard error.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfe2/dfe26f3b-c92a-4124-9ba6-812218ee9927.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Driving times (seconds) of IGP2 and Cons for 10 routes in Town 1 and Town 2.</div>\nsuddenly accelerates and continues straight (rather than exiting as in S3, or stopping as in S4). In these cases we observed a 23% collision rate for IGP2-MAP (in all collisions, V1 collided into the ego) while IGP2 produced no collisions. These results show that IGP2 exhibits safer driving than IGP2-MAP by accounting for uncertainty over goals and trajectories. Figure 5 shows the driving times of IGP2 and Cons for the routes in the two towns. Both algorithms completed all of the routes. Goal recognition allowed IGP2 to reduce its driving times substantially by exploiting multiple opportunities for proactive lane changes and road/junction entries. In contrast, Cons exhibited more conservative driving and often waited considerably longer at junctions or before taking a turn until traffic cleared up. SH-CVel was unable to complete any of the given routes, as its short planning horizon often caused it to take a wrong turn (thus failing the instance). 3) Interpretability: We are able to extract intuitive explanations for the predictions and decisions made by IGP2. The explanations are given in the caption of Figure 2.\n# V. CONCLUSION\nWe proposed an autonomous driving system, IGP2, which integrates planning and prediction over extended horizons by reasoning about the goals of other vehicles via rational inverse planning. Evaluation in diverse urban driving scenarios showed that IGP2 robustly recognises the goals of nonego vehicles, resulting in improved driving efficiency while allowing for intuitive interpretations of the predictions to explain the system\u2019s decisions. IGP2 is general in that it uses relatively standard planning techniques that could be replaced with other techniques (e.g. POMDP-based planners [34]), and the general principles underlying our approach could be applied to other domains in which mobile robots interact with other robots/humans. Important future directions include goal recognition in the presence of occluded objects which can be seen by the non-ego vehicle but not the ego vehicle, and accounting for human irrational biases [35], [36].\n[1] W. Schwarting, J. Alonso-Mora, and D. Rus, \u201cPlanning and decisionmaking for autonomous vehicles,\u201d Annual Review of Control, Robotics, and Autonomous Systems, vol. 1, pp. 187\u2013210, 2018. [2] C. Dong, J. M. Dolan, and B. Litkouhi, \u201cSmooth behavioral estimation for ramp merging control in autonomous driving,\u201d in IEEE Intelligent Vehicles Symposium. IEEE, 2018, pp. 1692\u20131697. [3] C. Hubmann, J. Schulz, M. Becker, D. Althoff, and C. Stiller, \u201cAutomated driving in uncertain environments: Planning with interaction and uncertain maneuver prediction,\u201d IEEE Transactions on Intelligent Vehicles, vol. 3, no. 1, pp. 5\u201317, 2018. [4] B. Zhou, W. Schwarting, D. Rus, and J. Alonso-Mora, \u201cJoint multipolicy behavior estimation and receding-horizon trajectory planning for automated urban driving,\u201d in IEEE International Conference on Robotics and Automation. IEEE, 2018, pp. 2388\u20132394. [5] E. Galceran, A. Cunningham, R. Eustice, and E. Olson, \u201cMultipolicy decision-making for autonomous driving via changepoint-based behavior prediction: Theory and experiment,\u201d Autonomous Robots, vol. 41, no. 6, pp. 1367\u20131382, 2017. [6] C. Hubmann, M. Becker, D. Althoff, D. Lenz, and C. Stiller, \u201cDecision making for autonomous driving considering interaction and uncertain prediction of surrounding vehicles,\u201d in IEEE Intelligent Vehicles Symposium (IV). IEEE, 2017, pp. 1671\u20131678. [7] W. Song, G. Xiong, and H. Chen, \u201cIntention-aware autonomous driving decision-making in an uncontrolled intersection,\u201d Mathematical Problems in Engineering, vol. 2016, 2016. [8] B. D. Ziebart, N. Ratliff, G. Gallagher, C. Mertz, K. Peterson, J. A. Bagnell, M. Hebert, A. K. Dey, and S. Srinivasa, \u201cPlanning-based prediction for pedestrians,\u201d in Intelligent Robots and Systems, 2009, pp. 3931\u20133936. [9] J. Hardy and M. Campbell, \u201cContingency planning over probabilistic obstacle predictions for autonomous road vehicles,\u201d IEEE Transactions on Robotics, vol. 29, no. 4, pp. 913\u2013929, 2013. [10] T. Bandyopadhyay, K. S. Won, E. Frazzoli, D. Hsu, W. S. Lee, and D. Rus, \u201cIntention-aware motion planning,\u201d in Algorithmic Foundations of Robotics X. Springer, 2013, pp. 475\u2013491. [11] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y. Shen, Y. Shen, Y. Chai, C. Schmid, C. Li, and D. Anguelov, \u201cTNT: Targetdriven trajectory prediction,\u201d in Conference on Robot Learning, 2020. [12] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, \u201cPRECOG: prediction conditioned on goals in visual multi-agent settings,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 2821\u20132830. [13] Y. Chai, B. Sappm, M. Bansal, and D. Anguelov, \u201cMultiPath: multiple probabilistic anchor trajectory hypotheses for behavior prediction,\u201d in Conference on Robot Learning, 2019. [14] Y. Xu, T. Zhao, C. Baker, Y. Zhao, and Y. N. Wu, \u201cLearning trajectory prediction with continuous inverse optimal control via Langevin sampling of energy-based models,\u201d arXiv preprint arXiv:1904.05453, 2019. [15] S. Casas, W. Luo, and R. Urtasun, \u201cIntentNet: learning to predict intention from raw sensor data,\u201d in Conference on Robot Learning, 2018, pp. 947\u2013956. [16] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. Torr, and M. Chandraker, \u201cDESIRE: distant future prediction in dynamic scenes with interacting agents,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 336\u2013345. [17] M. Wulfmeier, D. Z. Wang, and I. Posner, \u201cWatch this: Scalable costfunction learning for path planning in urban environments,\u201d in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2016, pp. 2089\u20132095. [18] A. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, and R. Urtasun, \u201cPerceive, predict, and plan: Safe motion planning through interpretable semantic representations,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 414\u2013430. [19] S. V. Albrecht and P. Stone, \u201cAutonomous agents modelling other agents: A comprehensive survey and open problems,\u201d Artificial Intelligence, vol. 258, pp. 66\u201395, 2018. [20] J. Stewart, \u201cWhy people keep rear-ending self-driving cars,\u201d WIRED magazine, 2020, https://www.wired.com/story/ self-driving-car-crashes-rear-endings-why-charts-statistics/ (Accessed: 2020-10-31). [21] P. Koopman, R. Hierons, S. Khastgir, J. Clark, M. Fisher, R. Alexander, K. Eder, P. Thomas, G. Barrett, P. H. Torr, A. Blake, S. Ramamoorthy, and J. McDermid, \u201cCertification of highly automated vehicles for use\non UK roads: Creating an industry-wide framework for safety,\u201d 2019, FiveAI. [22] M. Gadd, D. De Martini, L. Marchegiani, P. Newman, and L. Kunze, \u201cSense\u2013Assess\u2013eXplain (SAX): Building trust in autonomous vehicles in challenging real-world driving scenarios,\u201d in 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2020, pp. 150\u2013155. [23] M. Ram\u00b4\u0131rez and H. Geffner, \u201cProbabilistic plan recognition using offthe-shelf classical planners,\u201d in 24th AAAI Conference on Artificial Intelligence, 2010, pp. 1121\u20131126. [24] C. Baker, R. Saxe, and J. Tenenbaum, \u201cAction understanding as inverse planning,\u201d Cognition, vol. 113, no. 3, pp. 329\u2013349, 2009. [25] C. Browne, E. Powley, D. Whitehouse, S. Lucas, P. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton, \u201cA survey of Monte Carlo tree search methods,\u201d IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1\u201343, 2012. [26] S. V. Albrecht and S. Ramamoorthy, \u201cExploiting causality for selective belief filtering in dynamic Bayesian networks,\u201d Journal of Artificial Intelligence Research, vol. 55, pp. 1135\u20131178, 2016. [27] C. G\u00b4amez Serna and Y. Ruichek, \u201cDynamic speed adaptation for path tracking based on curvature information and speed limits,\u201d Sensors, vol. 17, no. 1383, 2017. [28] A. W\u00a8achter and L. Biegler, \u201cOn the implementation of an interiorpoint filter line-search algorithm for large-scale nonlinear programming,\u201d Mathematical Programming, vol. 106, no. 1, pp. 25\u201357, 2006. [29] S. Niekum, S. Osentoski, C. Atkeson, and A. Barto, \u201cOnline Bayesian changepoint detection for articulated motion models,\u201d in IEEE International Conference on Robotics and Automation. IEEE, 2015. [30] P. Hart, N. Nilsson, and B. Raphael, \u201cA formal basis for the heuristic determination of minimum cost paths,\u201d in IEEE Transactions on Systems Science and Cybernetics, vol. 4, July 1968, pp. 100\u2013107. [31] P. Auer, N. Cesa-Bianchi, and P. Fischer, \u201cFinite-time analysis of the multiarmed bandit problem,\u201d Machine Learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002. [32] M. Treiber, A. Hennecke, and D. Helbing, \u201cCongested traffic states in empirical observations and microscopic simulations,\u201d Physical Review E, vol. 62, no. 2, p. 1805, 2000. [33] C. Watkins and P. Dayan, \u201cQ-learning,\u201d Machine Learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992. [34] A. Somani, N. Ye, D. Hsu, and W. S. Lee, \u201cDESPOT: online POMDP planning with regularization,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 1772\u20131780. [35] M. Kwon, E. Biyik, A. Talati, K. Bhasin, D. P. Losey, and D. Sadigh, \u201cWhen humans aren\u2019t optimal: Robots that collaborate with risk-aware humans,\u201d in Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction, 2020. [36] Y. Hu, L. Sun, and M. Tomizuka, \u201cGeneric prediction architecture considering both rational and irrational driving behaviors,\u201d in 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, 2019, pp. 3539\u20133546.\n",
    "paper_type": "method",
    "attri": {
        "background": "The ability to predict the intentions and driving trajectories of other vehicles is a key problem for autonomous driving. Previous methods have focused on detecting current maneuvers or specifying finite goals, but limitations exist in terms of accuracy and interpretability. A new method is necessary to enable better predictions and planning over extended timescales.",
        "problem": {
            "definition": "The problem addressed is the prediction of the intentions and trajectories of other vehicles in dynamic environments, which is complicated by limited observation data and multi-agent interactions.",
            "key obstacle": "Existing methods are limited to recognizing current maneuvers, which restricts the effectiveness of planning algorithms and often leads to overly conservative driving strategies."
        },
        "idea": {
            "intuition": "The idea is inspired by the notion that understanding the goals of other vehicles can enhance prediction accuracy and planning efficiency.",
            "opinion": "The proposed method, Interpretable Goal-based Prediction and Planning (IGP2), integrates goal recognition with Monte Carlo Tree Search (MCTS) to optimize maneuvers for the ego vehicle.",
            "innovation": "IGP2 differs from existing approaches by combining rational inverse planning with MCTS, allowing for the recognition of vehicle goals and the generation of explainable plans."
        },
        "method": {
            "method name": "Interpretable Goal-based Prediction and Planning",
            "method abbreviation": "IGP2",
            "method definition": "IGP2 is a system that uses rational inverse planning to recognize the goals of other vehicles and employs MCTS to plan optimal maneuvers for the ego vehicle.",
            "method description": "IGP2 integrates goal recognition and planning to enhance autonomous driving performance.",
            "method steps": [
                "Generate possible goals for non-ego vehicles.",
                "Perform inverse planning to compute goal probabilities and predicted trajectories.",
                "Use MCTS to simulate and select optimal maneuvers for the ego vehicle."
            ],
            "principle": "The effectiveness of IGP2 lies in its ability to reason about the goals of other vehicles, which informs more proactive and efficient planning."
        },
        "experiments": {
            "evaluation setting": "The evaluation involved simulations of urban driving scenarios, comparing IGP2 with baseline methods in terms of driving efficiency and goal recognition accuracy.",
            "evaluation method": "The performance was assessed through metrics such as driving time and the accuracy of goal recognition, alongside qualitative analysis of the system's interpretability."
        },
        "conclusion": "IGP2 demonstrated robust goal recognition and improved driving efficiency in various urban scenarios, providing intuitive explanations for its decisions, thus contributing to the field of autonomous driving.",
        "discussion": {
            "advantage": "The key advantages of IGP2 include enhanced prediction accuracy, improved driving efficiency, and the ability to provide intuitive explanations for decisions.",
            "limitation": "One limitation of the method is its reliance on the assumption that vehicles behave rationally, which may not hold in all scenarios.",
            "future work": "Future research directions include improving goal recognition in the presence of occluded objects and addressing irrational behaviors in other vehicles."
        },
        "other info": {
            "info1": "The system was evaluated using various defined local interaction scenarios.",
            "info2": {
                "info2.1": "The evaluation included 100 instances for each scenario.",
                "info2.2": "The method uses a combination of heuristic goal generation and Bayesian inference for goal recognition."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The ability to predict the intentions and driving trajectories of other vehicles is a key problem for autonomous driving."
        },
        {
            "section number": "1.2",
            "key information": "Understanding the goals of other vehicles can enhance prediction accuracy and planning efficiency in autonomous driving."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed is the prediction of the intentions and trajectories of other vehicles in dynamic environments, complicated by limited observation data and multi-agent interactions."
        },
        {
            "section number": "2.3",
            "key information": "IGP2 integrates goal recognition with Monte Carlo Tree Search (MCTS) to optimize maneuvers for the ego vehicle."
        },
        {
            "section number": "3.1",
            "key information": "IGP2 employs rational inverse planning to recognize the goals of other vehicles, which informs proactive planning."
        },
        {
            "section number": "5.1",
            "key information": "The method uses a combination of heuristic goal generation and Bayesian inference for goal recognition."
        },
        {
            "section number": "6.1",
            "key information": "IGP2 demonstrated robust goal recognition and improved driving efficiency, providing intuitive explanations for its decisions."
        },
        {
            "section number": "8",
            "key information": "Future research directions include improving goal recognition in the presence of occluded objects and addressing irrational behaviors in other vehicles."
        }
    ],
    "similarity_score": 0.5386608574377763,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/Interpretable Goal-based Prediction and Planning for Autonomous Driving.json"
}