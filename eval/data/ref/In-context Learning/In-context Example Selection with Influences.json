{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2302.11042",
    "title": "In-context Example Selection with Influences",
    "abstract": "In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use incontext influences to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a 16.3% performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.1",
    "bib_name": "nguyen2023incontextexampleselectioninfluences",
    "md_text": "# n-context Example Selection with Influ\nTai Nguyen TAING@SEAS.UPENN.EDU University of Pennsylvania Eric Wong EXWONG@CIS.UPENN.EDU University of Pennsylvania\n# Abstract\nIn-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use incontext influences to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a 16.3% performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.1\n# 1 Introduction\nLarge language models (LLMs) such as GPT-3 have recently become capable of in-context learning (ICL) [Bro+20]. In ICL, users provide the model with a few labeled examples as input before asking the model to make a prediction on a new example. This paradigm has enabled the rapid adaptation of LLMs to new tasks without requiring any modifications to the model. ICL has several advantages over traditional learning paradigms. First, the ability to do few-shot learning directly reduces the need for human-labeled data. Second, in contrast to other popular training paradigms such as finetuning a pretrained model [Rad+19; Dev+19], ICL enables inference without any gradient updates. Lastly, ICL also displays amazing versatility through different modes of prompting. Recent work shows that GPT-3 can do step-by-step reasoning when being demonstrated a few examples containing reasoning [Wei+22; Nye+22; Lyu+23]. Despite these promises, ICL performance is known to be highly variable. In particular, ICL volatility has been linked to biases such as the order of the examples [Lu+22], their templates [Lu+22; KT21], and example selection [Liu+22a]. Various mitigation methods were proposed to address this brittleness, such as model calibration [Zha+21] and template engineering [Liu+22b]. Given that not all in-context examples are equal, several others have focused on finding more optimal prompts. Liu et al. [Liu+22a] proposes a distance-based selection method, using semantic similarity to the validation query to rank candidate examples. Gonen et al. [Gon+22] finds a strong negative correlation between example perplexity and task performance. Similarly, Chen et al. [Che+22] suggests a sensitivitybased selection method which perturbs examples and chooses the ones with more robust predictions. While these methods have varying effectiveness, there lacks a consensus on which of these signals are most important in ICL. Motivated by this problem, our paper studies the relationship between influences and ICL, to better understand and quantify the impact of examples on ICL. Influences naturally lend to an offline example selection method that directly measures the effect of examples on ICL performance. In particular, we use in-context influences to measure and rank the impact of in-context examples on task performance. The framework can be customized to study different aspects of ICL, such as optimizing for the best classification accuracy or quantifying the impact of position.\nEric Wong\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5146/51463021-10bf-40ea-bf08-902cf70f1475.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Test accuracy increases when examples are selected in increasing in-context influence percen bins.</div>\n<div style=\"text-align: center;\">Figure 1: Test accuracy increases when examples are selected in increasing in-context influence percentile bins.</div>\nOn 8 language models and 9 natural language tasks, we demonstrate the efficacy of influence-based example selection in ICL at estimating the effect of training examples on downstream performance. We find that in-context influences outperform all other selection baselines at estimating ICL performance in both positive and negative selections. In-depth analysis exposes a significant gap between the most positive and most negative examples. For example, constructing prompts from the top influence bin improves ICL performance by up to 16.3% over the bottom influence bin on LLaMA-13B. Overall, our contributions are as follows:\n\u2022 We study in-context influence as a metric for selecting and analyzing in-context examples in few-shot ICL. In both positive and negative selections, our method outperforms several baselines at estimating ICL performance. \u2022 We demonstrate a substantial performance gap between positively and negatively influential examples. Our framework quantifies this gap, and further confirms the variability of example-selection in ICL. \u2022 While we focus on classification accuracy, our framework generalizes to any combination of performance metric, model, and task. For example, we leverage our framework to quantify emergent phenomena in LLMs, such as the impact of recency bias in example ordering.\n# 2 In-context Influences\nA variety of methods have been developed to understand how training data affects model performance. To estimate this effect, some methods use gradient information [KL17; Koh+19; HWT20; Pru+20] while others retrain models on subsets of the training data [GZ19; Ily+22]. These methods all aim to quantify how a training example affects the prediction of a test example after training. Inspired by these frameworks, our goal is to trace how ICL performance depends on the in-context examples and calculate the corresponding influences. Our setup follows the retraining-based influence frameworks, which have two main steps. Let S be a training set, and let f (S) be the validation performance after training on a dataset S. Retraining-based influences first collect a \u201cdataset\u201d of M training runs D = {(Si, f (Si)}M i=1 where Si \u2286S are random subsets of the original training dataset. The second step is to use this dataset to estimate the influence of each example x \u2208S, e.g. by learning a linear mapping [Ily+22].\nInfluences in k-shot prompting. To compute influences for in-context examples, we leverage the following key observation: in ICL, \u201ctraining\u201d a model on a subset S\u2032 reduces to prompting the model on a sequence containing S\u2032. Consequently, constructing the dataset D of training runs for ICL requires no gradient updates and is as costly as computing forward passes through the model. This drastically reduces the cost of calculating retraining-based influences, and can be calculated with only query-access to the model. Specifically, for the first step, we construct the dataset of training runs D by performing k-shot prompting with subsets S\u2032 \u2286S where |S\u2032| = k. For a fixed subset S\u2032, the performance of the resulting prompt containing S\u2032 is measured with a validation query appended to the end of the prompt. We repeat this inference over the entire validation set to compute the metric f (S\u2032), which measures the validation performance after prompting with S\u2032. This metric can be any evaluation method suitable for a natural language task\u2014here, we focus on classification accuracy. We repeat this process over multiple random subsets S\u2032 \u2286S until each example in S has been seen in multiple prompts, resulting in a dataset of prompting runs D = {(Si, f (Si)}M i=1. In the second step, we calculate the influence of each in-context example. We define the in-context influence I(xj) as the effect of an example xj on few-shot ICL performance. In other words, the influence is the difference between the average performance of prompts including xj and the average performance of prompts omitting xj. More formally, this can be written as:\nwhere Si is a specific uniformly sampled subset, M is the number of total subsets used to estimate influences, Nj is the total number of subsets containing example xj, and f (Si) is the performance metric when evaluated on the validation set. When f measures validation performance, a higher score for I(xj) corresponds to a higher average improvement in validation performance when including xj in the prompt, analogous to the meaning of influences in the classic, non-prompted setting. As the number of collected subsets grows, estimates of in-context influences become more accurate. A sufficiently large M is one with good coverage for each example\u2014this means that each xj \u2208S is seen multiple times. In our experiments, each xj gets seen at least 30 times on average.\nInfluence-based Example Selection. We use the proposed in-context influences to identify highly impactful in-context examples. Specifically, we can use the top influential examples to create the \u201cbest\u201d prompt for ICL (with respect to the influence scores). On the converse, we can also use the bottom influential examples to\ncreate the \u201cworst\u201d performing prompt for ICL. In summary, to do example selection for ICL, we carry out he following steps: 1. Prompt the model on random training subsets and measure validation performance to create the dataset of prompting runs D. 2. Calculate the in-context influence I(xj) for each example xj \u2208S following Equation 1 using D. 3. Select k examples with the most positive influences to use for k-shot prompting. The examples can be arranged in any ordering. A summary of the entire pipeline is shown in Algorithm 1.\n# 2.1 Cost Analysis & Hyperparameters\nTraining cost. Retraining-based influence frameworks [Ily+22; GZ19] can require training hundreds of thousands of models. This is necessary to collect a sufficiently large enough dataset D to accurately estimate influences. In contrast, the cost of computing in-context influences is relatively cheap, as we do not need to train an end-to-end model. Instead of training, we simply prompt the LLM using a randomly sampled S\u2032 from original training set S. Thus, the complexity of calculating the validation performance from a sampled subset is proportional to a forward pass through the LLM.\nSize of subsets. Our method has one parameter k, which controls the size of the random subsets S\u2032 \u2286S from which D is constructed. For ICL, k = |S\u2032| corresponds to the number of in-context examples given in the prompt. Unlike in the traditional setting, the context window length limit enforces a hard upper limit on the number of examples an LLM can be trained on via prompting. These context windows are typically limited to 2048 characters. Taking the context window into account, we select k to be the maximal number of examples that can be inserted into the context window. The value of k can vary by different choices of model (context window size) and the lengths of the individual examples in a dataset. Since the number of shots can impact ICL performance, we keep a consistent k for each model and task. Table 6 provides the precise k value associated with each task.\n# 3 Experiments\nWe conduct experiments to obtain in-context influences for 72 combinations of natural language tasks and LLMs. The goal is to a select a set of good ICL examples by running influences on the Dev set. At test time, such a set requires no further modification or computation.\nDatasets. We choose 9 datasets for our study, 5 of which are binary classification tasks and 4 are multichoice tasks.2 These datasets cover a wide range of common natural language tasks, including textual entailment (RTE), question-answering (PIQA), and text summarization (BoolQ). Each example instance has a definitively correct answer, making them convenient to be evaluated through classification accuracy. Beyond acquiring the original data, we subsample Train/Dev/Test sets with 400/200/500 in-context examples for each task.\nK-shot selection. We select examples uniformly by their label class. If a multi-choice task has 3 options, each option would make up roughly one-third of the examples. This balance prevents majority label bias [Zha+21] from skewing the model\u2019s inference.\nInference details. There are multiple ways to do inference on multi-choice tasks with autoregressive models [Hol+21]. We follow one popular approach, which ranks all possible continuations to a prompt and chooses the continuation with the highest log-likelihood. Thus, given a prompt x0:m and a possible prompt continuation xm:n, the score for xm:n can be defined as:\nwhere P(xj|x0:m) is the likelihood of token xj given the preceding context tokens x0:j. The prediction is then defined as the most likely continuation, arg maxxm:n \u2113(xm:n). We do not perform any token length or answer normalization tricks [Bro+20].\n# 3.1 Influence-based methods\nIn our main results, we evaluate the effectiveness of influence-based example selection using the following strategies:\nIn our main results, we evaluate the effectiveness of influence-based example selection using the following strategies:\n1. Influence (+/\u2212). We select examples with the most positive or negative influence scores according to Algorithm 1. If influence estimates are meaningful, we would expect examples with positive influences to perform the best among all baselines, while examples with negative influences would have the poorest performance. 2. In-context datamodels. We consider an alternative influence-based example selection based on the datamodels [Ily+22] framework that we adapt for ICL. Specifically, we fit a linear model3 g\u03b8 on the dataset D of input-output pairs from Section 2 to predict validation performance:\n2. In-context datamodels. We consider an alternative influence-based example selection based on the datamodels [Ily+22] framework that we adapt for ICL. Specifically, we fit a linear model3 g\u03b8 on the dataset D of input-output pairs from Section 2 to predict validation performance:\ng\u03b8(S\u2032) = \u03b8 \u00b7 1T S\u2032 + \u03b80\nwhere S\u2032 \u2286S is an example subset and 1S\u2032 is an indicator vector with the dimension of the training set S. A value of 1 at position i indicates that the example i is included in the subset S\u2032 and a value of 0 means otherwise. Following the datamodels framework, we can treat the parameters \u03b8 as influence estimates, and select in-context examples based on these estimates. Note that \u03b8 has a close connection to in-context influences as they both use the same training set D, but in-context datamodels assumes a linear model.\n# 3.2 Non influence-based methods\nWe compare influence-based example selection methods described in the previous section to the following baselines, which optimize various metrics for selection.\n3. One-shot. We do one-shot prompting (k = 1) on each Train example and rank them by their accuracy on the Dev set. One-shot selection assumes that we can extrapolate the performance of one-shot prompting to the few-shot setting.\n<div style=\"text-align: center;\">Table 1: Positive example selection methods on OPT-30B and their overall Rank Aggregation.</div>\nOPT-30B\nAll Models\nBinary Classification (Acc. \u2191)\nMulti-choice (Acc. \u2191)\nRank Agg (\u2193)\nPIQA\nBoolQ\nRTE\nWIC\nWSC\nARC-c\nARC-e\nHS\nOBQA\nAll Tasks\nPerplexity (+)\n76.80.0\n72.70.2\n61.90.3\n53.50.2\n43.50.6\n40.30.1\n76.30.1\n56.60.0\n28.50.1\n4.59\nRandom\n77.00.0\n71.10.2\n63.20.2\n54.80.1\n49.10.5\n41.50.1\n76.00.1\n55.40.1\n29.60.1\n4.37\nSimilarity (+)\n77.70.1\n70.10.4\n63.90.1\n53.30.1\n57.10.7\n42.00.1\n76.20.1\n56.70.0\n29.30.0\n4.33\nOne-shot (+)\n77.50.0\n76.50.1\n52.40.1\n51.10.2\n61.60.0\n41.50.0\n76.10.1\n56.60.1\n31.20.0\n4.24\nBest set\n76.90.0\n72.60.0\n64.10.3\n55.10.2\n54.80.4\n40.80.0\n75.80.1\n56.10.0\n31.50.0\n3.62\nIC Datamodels (+)\n78.10.0\n77.00.0\n65.90.1\n51.40.2\n56.40.1\n42.10.0\n76.60.0\n58.20.0\n31.70.1\n2.98\nInfluence (+)\n78.00.0\n74.10.1\n64.60.1\n52.50.1\n51.40.3\n41.60.1\n77.00.0\n57.40.0\n33.30.0\n2.96\n<div style=\"text-align: center;\">Table 2: Negative example selection methods on LLaMA-13B and their overall Rank Aggregation</div>\nLLaMA-13B\nAll Models\nBinary Classification (Acc. \u2193)\nMulti-choice (Acc. \u2193)\nRank Agg (\u2193)\nPIQA\nBoolQ\nRTE\nWIC\nWSC\nARC-c\nARC-e\nHS\nOBQA\nAll Tasks\nSimilarity (-)\n79.20.0\n83.20.0\n58.70.1\n54.60.2\n43.90.5\n51.10.0\n82.30.0\n62.10.0\n37.10.0\n5.19\nRandom\n78.50.1\n82.60.1\n61.10.3\n51.80.2\n42.90.4\n50.40.1\n82.70.0\n62.50.1\n35.70.1\n4.94\nWorst set\n78.80.0\n79.20.1\n54.10.2\n53.30.1\n45.70.6\n50.30.1\n83.00.0\n62.10.1\n33.60.1\n4.37\nPerplexity (-)\n74.90.0\n82.40.1\n57.90.1\n55.40.2\n42.80.4\n49.40.0\n81.40.0\n58.70.0\n33.10.1\n3.69\nOne-shot (-)\n78.70.0\n68.20.2\n53.90.1\n53.10.1\n55.40.7\n50.00.1\n81.40.0\n61.00.0\n26.10.1\n2.96\nIC Datamodels (-)\n78.50.0\n69.30.3\n50.00.0\n51.60.2\n38.90.1\n50.00.1\n82.80.1\n61.80.0\n22.00.1\n3.03\nInfluence (-)\n78.60.0\n68.30.3\n50.00.0\n50.60.2\n39.80.3\n49.30.1\n82.40.1\n61.60.0\n22.90.1\n2.90\n4. Semantic similarity. Examples close to the test queries in the embedding space can substantially improve ICL performance on semantic parsing tasks [Liu+22a]. We search for a set of examples with the closest distance to Dev set, then applying them on the unseen Test set. We use RoBERTa-large [Liu+19] sentence encoder implemented by Reimers and Gurevych [RG19]. 5. Perplexity. Perplexity measures the degree of uncertainty of the LLM when generating new tokens, where a lower perplexity means a higher confidence on the example. On perplexity, Gonen et al. [Gon+22] has linked prompt confidence to good ICL performance. We follow this insight to select examples based on their individual perplexity, which is more computationally friendly than calculating full prompt perplexity across many different example combinations. After selecting a set of k examples using the proposed baselines, we construct the prompt by ordering\nAfter selecting a set of k examples using the proposed baselines, we construct the prompt by ordering examples randomly. We compute Test accuracy and rank all baselines by single-task accuracy. We aggregate the ranks of each method my taking their average for both positive and negative example selection. In the main results, averages and standard errors are reported over 7 seeds.\n# 3.3 Results\nPositive selection. Table 1 shows results for all positive example selection baselines. Overall, influencebased selection methods outperform all non-influence counterparts. Specifically, across all models and tasks, Influence (+) and IC Datamodels (+) frequently select the best set of examples for an average rank of 2.98 and 2.96 (where both methods are considered in the ranking). Best set selection is our next most competitive baseline, with the rest of the methods trailing far behind. For Best set, strong performance on the task WIC\n(word sense disambiguation) contributes mostly to this success. Likewise, One-shot example selection sees exceptional performance on WSC, but does not work well for other tasks. We also note that random selection ranks better than Perplexity (+), although the latter outperforms random selection on many tasks (see Table 7 in Appendix).\nNegative selection. Similarly, our influence-based example selection methods can consistently identify low-performing examples. As shown in Table 2, results indicate that Influence (-) achieves the highest rank (2.90) among all other methods. Notably, in this context, One-shot slightly outperforms IC Datamodels (-), suggesting that selecting examples based on their individual validation performance can be effective Compared to positive selection, the ability to pinpoint negative examples is equally meaningful: we can avoid these examples to achieve better ICL performance, or further study them to identify the factors that make them ineffective.\nBinary vs. Multi-choice. Our experiments find influence-based example selection methods to work better on Multi-choice tasks compared to Binary classification tasks. In most models (see Table 7 in Appendix), Influence (+) and IC Datamodel (+) often outperform all other methods on multi-choice tasks, but less so on binary classification tasks. Specifically, for PIQA and WIC, the small gaps between the performance of Influence (-) and Influence (+) suggest that our influence-based method might not have captured example helpfulness well for these tasks. Many factors related to both the model and task could explain such disparity. For one, Zhao et al. [Zha+21] demonstrates that LLMs can have a strong bias towards selecting certain labels for ICL, which could hinder both model performance and influence attribution. We suspect that these biases can exacerbate when the LLM is asked to choose between binary labels (T/F) compared to many labels in the multi-choice setup. Furthermore, the inconsistent scaling of 3 OPT models on the SuperGLUE benchmark for few-shot ICL could play a factor [Zha+22a]. The capabilities of the models themselves factor majorly into the accuracy improvements of our selection methods.\n# 4 Analysis\nIn this section, we analyze in-context influences across a number of distinct axes. Specifically, we study (1) the cost of influence-based example selection, (2) distinguishing factors between examples with positive and negative influences, (3) influence agreement between model families, and (4) the scaling behavior of influence-based selection across the number of shots. We conclude our analysis with a case study quantifying the effect of recency bias in example ordering.\n# 4.1 Cost Comparison\nFigure 2 compares the cost of influence-based example selection against Best set and One-shot example selections on different tokens budgets for multi-choice tasks.4 Recall that computing our in-context influences on more training runs often leads to more accurate influence estimations. Our visualization demonstrates that in-context influences can realize favorable gains over other selection methods at a fraction of the full budget (20M tokens). In fact, in-context influences also scale well beyond this number, while the same effect does not guarantee for Best set. Note that One-shot selection scales linearly by the size of the Train set S and Dev set, while Influence (+) has more flexible scaling depends on compute budget.\n# 4.2 Do models agree on high-influence examples?\nThis section analyzes whether or not the best and worst examples on a task are shared across models. When considering the overlap between all 7 individual models, our work finds that they rarely agree on the most positive and negative influence examples (\u22644 for all tasks). However, within the same model families, we identify a decent overlap. Figure 3 plots the inter-family agreement between three families considered in our study. Compared to OPT, both GPT-NeoX and LLaMA models often identify a smaller set of top and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25cf/25cf0b61-540f-4724-a225-a8addb7bccc8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a60/2a6066f2-b99e-4092-9180-5f9f73ee91b6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Token budget comparison for different baselines evaluated on LLaMA-7B (|S| = 400).</div>\nID\nPrompt\nInfluence\nReason\nPIQA\n12444\nGoal: flashlight\nAnswer: shines a light\n-0.001854\nUnnatural\nWIC\n3890\nGo to the supermarket and buy some tea.\nWould you like some tea?\nquestion: Is the word \u2019tea\u2019 used in the same sense in the\ntwo sentences above?\nanswer: false\n-0.007068\nMislabeled\nOBQA\n3771\nContext: Single cell organisms can put an animal in the\nAnswer: emergency room\n-0.006058\nUnnatural\nbottom influence examples, while agreeing on these examples more often. This suggests that our in-context influences are picking up signals specific to the nature of these model families. These can include variations in model architecture, training data, training stability, tokenizers, and others [Zha+22a; Bla+22; Rad+19; Tou+23]. For instance, model performance has been linked to term frequencies in the pretraining data [Raz+22].\n# 4.3 Negative vs. Positive Examples\nPrior work has associated various characteristics with examples that are strongly positive or negative [KL17; HWT20; Ily+22]. Positive examples are found to sometimes be instances of data leakage during the training process, while negative examples are often mislabeled. For LLMs, we do not have access to the pretraining data to identify data leakage. However, we identify many instances in the bottom influence bin that appear as either \u201cunnatural\u201d or mislabeled. Table 3 shows instances of these negative examples and their associated potential cause. For PIQA example #12444, the overall plausibility of the statement could improve if the order of statements Goal and Answer gets switched. Alternatively, a better template could possibly help achieve better input-output coherence. We suspect that the prompt template might play an important role in determining the influence of an example. Additionally, we identify WIC example #3890 as a falselyannotated instance. Related to input-label mapping, Min et al. [Min+22b] has shown that label correctness is not important for good ICL performance.5 Quantitatively, we measure several metrics from the literature to compare examples with positive and negative influences. As Figure 4.3 illustrates, we find little to no association between in-context influences\n<div style=\"text-align: center;\">Figure 3: Model family agreement (overlap) when considering all examples (union) in the Top and Bottom 20th influence bins.</div>\nFigure 3: Model family agreement (overlap) when considering all examples (union) in the Top and Bottom 20th influence bins.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7e3/d7e31170-b8b1-480a-802d-7379f7852adb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: On Superglue-WIC, in-context influences do not correlate with any previously known e characteristics.</div>\n<div style=\"text-align: center;\">Figure 4: On Superglue-WIC, in-context influences do not correlate with any previously known example</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df1a/df1a55ac-be74-44d9-ae64-5b78743ae16d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: On LLaMA-7B, influence-based example selection scales well with increasing k-shot.</div>\nICL Sensitivity. Naturally, we can leverage in-context influences to quantify the gap between the most positive and negative examples in ICL. For example, on OpenBookQA, we observe an impressive 16.3% accuracy difference between the best and worst in-context examples on LLaMA-13B (See Table 4 in Appendix). Our framework adds to a list of previous works reporting ICL sensitivity [Liu+22a; Lu+22; Zha+21].\nInfluence bins. Additionally, we demonstrate that our influence framework can analyze example selection in more fine-grain. For this experiment, we group examples by their influence percentile, where each bin contains 20% of the Train set. From these bins, we randomly select a set of k examples in any ordering for 10 seeds for evaluation. If the influences are meaningful, we expect to see increasing performance gains as examples are selected in increasing percentile bins along the x-axis. Figure 1 identifies clear positive trends confirming our hypothesis on most models and tasks. This shows that in-context influences produce well-behaved results when examples are selected in specific influence regions. There are few exceptions, such as the BoolQ task on LLaMA-7B, where selecting examples in increasingly positive influences does not consistently improve validation accuracy.\n# 4.4 How do in-context influences generalize across k-shot?\nThus far, our estimation of in-context influences has assumed a many-shot setting where a maximal number of examples is packed into the context window. In this study, we are interested in knowing how in-context influences generalize to different numbers of in-context examples k. In comparing different selection methods, Figure 5 finds that the impact of in-context influences is most prevalent when k is many (generally \u22658). At one-shot and very few-shot, in-context influences can sometimes perform worse than other methods (RTE)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ef4/1ef4b722-e45e-4aeb-b93b-ec7025718e95.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec50/ec50bb0a-4ef7-42f6-af8a-39ee7698f3b7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Aggregated influences of each position in 4-shot prompting. Influence magnitudes are bigger at later positions.</div>\nbut steadily improve with increasing k. In contrast, the performance for One-shot and random selection not always improve (and sometimes decline) with increasing k (on Hellaswag and RTE).\n# but steadily improve with increasing k. In contrast, the performance for One-shot and random selection do not always improve (and sometimes decline) with increasing k (on Hellaswag and RTE).\n# 4.5 Case study: Example Ordering\nWe want to apply our influence-based framework to demonstrate its effectiveness for studying a phenomenon in ICL, which deals with recency bias in example ordering [Lu+22; Zha+21].\nSetup. To do this, we randomly choose 100 examples from another SuperGLUE task, CB, and assign them into 4 groups for 4-shot prompting. On OPT-6.7B, we compute an in-context influence estimate for each example-position pair over all possible ordering permutations (4! = 24). Given an arbitrary example, we are interested in quantifying its impact at any position in the ordering and the overall influence of each position. Results. Figure 6 confirms the presence of recency bias in ICL [Zha+21], showing that influence estimates of examples increase as their position ID moves down in the order. Between Position #0 and Position #3, there is a notable 2% difference in the estimated absolute influence. Figure 7 elaborates on this result: on the same set of in-context examples, the influence estimates computed in position #3 has the biggest spread among all positions. Once again, we observe a steadily increasing trend in the widths of the spread as an example is moved down in order.\nSetup. To do this, we randomly choose 100 examples from another SuperGLUE task, CB, and assign them into 4 groups for 4-shot prompting. On OPT-6.7B, we compute an in-context influence estimate for each example-position pair over all possible ordering permutations (4! = 24). Given an arbitrary example, we ar interested in quantifying its impact at any position in the ordering and the overall influence of each position\nResults. Figure 6 confirms the presence of recency bias in ICL [Zha+21], showing that influence estimates of examples increase as their position ID moves down in the order. Between Position #0 and Position #3, there is a notable 2% difference in the estimated absolute influence. Figure 7 elaborates on this result: on the same set of in-context examples, the influence estimates computed in position #3 has the biggest spread among all positions. Once again, we observe a steadily increasing trend in the widths of the spread as an example is moved down in order.\n# 5 Related Work\nExample selection. In parallel and independent work, Chang and Jia [CJ22] also study the use of influences for selecting in-context examples for k-shot prompting, and also find that influence-based selection outperforms baseline methods. While we both consider influence estimates based on datamodels and data shapley influences, there are some differences. Chang and Jia [CJ22] integrate the position of an in-context example into the datamodel to directly calculate the influence of position for each example. In contrast, we consider the vanilla datamodel that does not model position, but demonstrate positional bias in a case study in Section 4.5. Although the formulation of the CondAcc score from Chang and Jia [CJ22] may appear slightly different from our influence metric, Chang and Jia [CJ22] prove in their Appendix that the two quantities rank examples identically. The experimental setups cover two distinct use-cases \u2013 Chang and Jia [CJ22] focus on a smaller number of in-context examples (i.e. k = 4) and find that influences could greatly reduce the variance of ICL, while we study a large number of examples (i.e. k up to 52) that also leads to less variance and performance gains. Finally, the corresponding analyses complement each other well. Chang and Jia [CJ22] analyze the embedding distance of examples, while we analyze the scaling pattern of the number of in-context examples k and the level of influence agreement across model families. Both work find little\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8bdc/8bdc085c-0201-49ff-8fb6-dbc82dd760b3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Influence distribution of each position in 4-shot prompting. Bigger spreads are observed at later positions.</div>\nFigure 7: Influence distribution of each position in 4-shot prompting. Bigger spreads are observed at later positions.\ncorrelation between in-context influences and known signals such as example perplexity. The combined analyses present a more comprehensive understanding of influence-based example selection for ICL. Outside of in-context influences, examples have been found to be inequal when used in ICL. Liu et al. [Liu+22a] finds that the best in-context examples are the ones most semantically similar to the test sample, which translates well for semantic parsing tasks. Chen et al. [Che+22] links exemplars to a sensitivity measure, while Gonen et al. [Gon+22] recently shows a correlation between prompt perplexity and model performance. Others have improved ICL performance by focusing on prompt retrieval [RHB22], or applying reinforcement learning to improve ICL performance through prompt editing [Zha+22b]. Our in-context influence framework differs in its focus on identifying good examples from a Dev set that generalize well to any unseen evaluation, removing the need to perform any prompt editing or retrieval at test time. In-context learning. ICL comes with high volatility to factors beyond example selection. In the few-shot setting, models have shown a tendency to overly rely on the most frequent labels (majority bias) or labels that appear at late positions in a prompt (recency bias) [Zha+21]. The latter suggests that the ordering of examples can be optimized for performance gain [Lu+22]. The prompt template \u2013 the format in which the example is presented \u2013 also matters [Min+22a]. Other findings have discovered that correct input-label mapping has little relevance [Min+22b] and example diversity is more important [Su+22]. Recently, Aky\u00a8urek et al. [Aky+22] links the underlying computations of ICL to linear algorithms. Training data influence. Influence functions [KL17] have been used as a way to trace a model\u2019s output back to the training data. Influence of a specific training point measures the change in a model\u2019s performance when the point is removed from the training set. Data Shapley [GZ19] and Ilyas et al. [Ily+22] measure similar quantities via retraining the model on subsets of the dataset. Outside of individual attributions, influence functions have also been used to measure group effects, where prior work found the influence estimates of individual data points to be the lower bound of groups [Koh+19].\nTraining data influence. Influence functions [KL17] have been used as a way to trace a model\u2019s output back to the training data. Influence of a specific training point measures the change in a model\u2019s performance when the point is removed from the training set. Data Shapley [GZ19] and Ilyas et al. [Ily+22] measure similar quantities via retraining the model on subsets of the dataset. Outside of individual attributions, influence functions have also been used to measure group effects, where prior work found the influence estimates of individual data points to be the lower bound of groups [Koh+19].\n# 6 Conclusion\nOur work proposes in-context influences as a way to analyze and select examples for ICL. Influence-based example selection methods (in-context influences and in-context datamodels) outperform all baselines for both positive and negative selections, showing stronger results on multi-choice tasks compared to binary classification tasks. In-context influences can identify problematic examples, scale performance with the choice of k-shot, and generalize to many nidek families. In a case study, we further examine known biases found in ICL such as recency bias in example ordering. Our work adds to a growing body of work that aims to understand and debug different emerging phenomena in LLMs. One limitation of influence-based frameworks is that they predict ICL performance from a fixed training set. However, practitioners can generate original prompts and examples, which may not exist in the training set. One potential research direction is to predict the performance of any input example constructed on the fly, in addition to those in the training set. Our influence-based framework can also be leveraged to study ICL beyond classification performance. For example, future work can potentially calculate influences for other natural language tasks such as text generation, summarization, or other multi-task settings.\n# 7 Full Results\nWe provide more results, experimental, and discussion details that did not fit into the main paper\n# 7.1 Influence distribution\nTable 4: Mean difference of test accuracy (%) between the top 20 and bottom 20 percentile bin for ea model-task pair. The disparity between the two groups is clear, though it may vary by choice of model a task.\nGPT-J\nOPT-6.7B\nLLaMA-7B\nLLaMA-13B\nPIQA\n0.26\n-1.47\n-0.10\n0.90\nBoolQ\n0.20\n1.33\n4.30\n4.40\nRTE\n1.33\n5.10\n11.80\n10.07\nWIC\n4.23\n2.00\n3.57\n3.00\nWSC\n-5.84\n-8.38\n-1.69\n10.77\nArc (Chal.)\n-0.83\n1.16\n0.66\n4.03\nArc (Easy)\n0.23\n0.00\n2.70\n0.96\nHellaswag\n2.10\n1.54\n3.04\n3.32\nOBQA\n4.27\n5.96\n7.60\n16.34\nTable 4 shows the performance gaps between using the most positive and the most negative influence examples for ICL inference. Figure 9.4 plots the distribution of influence estimates for all models and tasks. Figure 9.4 visualizes the fine-grain behavior of influence-based example selection when examples are selected in increasing order of influences.\n# 7.2 Choice of k-shot for in-context influences\n# 8 Discussion\n# 8.1 Can linear datamodels predict in-context learn\nRecall from Section 3.1 that we train linear in-context datamodels to derive \u03b8 as another influence measure for example selection. To evaluate these datamodels, we hold out a fraction of the training pairs (arbitrarily selected) from the collection process and use them afterwards as the ground truth. We do this for each model and task combination. If the Pearson correlation (\u03c1) between the predicted outputs and actual outputs are strong and statistically significant, we say that the linear datamodels models have capably captured the relationship between the in-context examples and ICL performance. Figure 9.4 visualizes the correlation between the predicted and actual model outputs for all models and tasks. We observe strong linear trends across the board, implying that the fitted in-context datamodels can predict few-shot ICL performance on unseen subsets of examples. Among all tasks, SuperGLUE-WSC is the most difficult to predict, which can be explained by the high variance from having the smallest Test set.\n# Erratic behavior with OPT models on SuperGLUE\nAuthors of OPT report the model\u2019s erratic behaviors when evaluated on many SuperGLUE tasks [Zha+22a]. Specifically, on the task WSC, zero- and multi-shot performance do not improve with respect to scale. They suspect that the small size of the validation sets in these datasets can be a factor. There are also reported\naccidents during the training process related to hardware failures and loss divergences. These factors could partially explain signals found in our in-context influence estimates.\n# 9 Implementation Details\n# 9.1 Models\nLanguage models. All autoregressive models are downloaded from their HuggingFace checkpoints using the transformers module6. To conserve memory, we load all models in 16FP half precision. We thank the authors of these models for making their work available to the research community.\nSeed. By default, we keep a fixed seed=42. For experiments involving random example ordering, we also use other seeds in {51, 56, 67, 75, 82, 98}.\n# 9.2 Datasets\nAll datasets were downloaded using Huggingface\u2019s datasets module. Table 6 details the sizes of the ubsampled sets and the number of shots that fit in the in-context windows. \u2022 SuperGLUE [Wan+19] This benchmark includes 5 binary classification tasks: BoolQ, RTE, WIC, WSC, and CB. \u2022 PIQA [Cla+18] This benchmark includes 2 multi-choice tasks: AI2 Arc (Challenge) & AI2 Arc (Easy). \u2022 Hellaswag [Zel+19] Single multi-choice task: Hellaswag. \u2022 OpenBookQA [Mih+18] Single multi-choice task: OpenBookQA.\n<div style=\"text-align: center;\">Table 5: Models used in our work.</div>\nModel\nParameter #\nWindow\nOpen-source\nGPT-J\n6B\n2048\n\u221a\nGPT-NeoX\n20B\n2048\n\u221a\nOPT-6.7B\n6.7B\n2048\n\u221a\nOPT-13B\n13B\n2048\n\u221a\nOPT-30B\n30B\n2048\n\u221a\nLLaMA-7B\n7B\n2048\n\u221a\nLLaMA-13B\n13B\n2048\n\u221a\n# 9.3 Prompts\nTable 9 shows full prompt formats used the paper.\n# 9.4 Hardware\nWe run all experiments on the NVIDIA A100 and NVIDIA RTX A60\nTable 6: Datasets used in the paper. We sample 400 examples for train and 200 examples for  wherever possible. k denotes the number of demonstrations necessary to fill up the 2048 charac context windows.\nType\n| Train |\n| Dev |\n| Test |\nk\nPIQA\nBinary\n400\n200\n500\n38\nSuperglue BoolQ\nBinary\n400\n200\n500\n10\nSuperglue RTE\nBinary\n400\n200\n500\n12\nSuperglue WIC\nBinary\n400\n200\n500\n32\nSuperglue WSC\nBinary\n400\n104\n154\n32\nAI2 Arc (Challenge)\nMC\n400\n200\n500\n46\nAI2 Arc (Easy)\nMC\n400\n200\n500\n52\nHellaswag\nMC\n400\n200\n500\n18\nOpenBookQA\nMC\n400\n200\n500\n52\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0523/0523f75c-1714-4247-91b7-4bbb2ca6e94e.png\" style=\"width: 50%;\"></div>\nFigure 8: Linear in-context datamodels can predict ICL performance on arbitrary subset S\u2032. Pearson correlation is calculated over all tasks for the model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/853b/853b5acc-db57-4ff0-a034-a0c4917a003d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Influence distributions across all models and tasks. A wide spread signifies existence  high-influence points.</div>\n<div style=\"text-align: center;\">Figure 9: Influence distributions across all models and tasks. A wide spread signifies existence of many high-influence points.</div>\n<div style=\"text-align: center;\">9: Influence distributions across all models and tasks. A wide spread signifies existence of many fluence points.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/02b9/02b9923e-5a70-4bc5-ba74-78fdd5fd5e3b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: In most models and tasks, Test accuracy increases when in-context examples are selected increasing influence percentile bins. Many task and model observes linear trends outside of few exceptio (ie. WSC).</div>\n<div style=\"text-align: center;\">Figure 10: In most models and tasks, Test accuracy increases when in-context examples are selected in increasing influence percentile bins. Many task and model observes linear trends outside of few exceptions</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7edd/7edd4813-b2b3-43cc-982f-01bfb5c001b5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: How different positive example selection methods generalize with the number of k demonstrations.</div>\nPIQA\nBoolQ\nRTE\nWIC\nWSC\nARC-c\nARC-e\nHS\nOBQA\nRank (\u2193)\nGPT-J-6B\nOne-shot (+)\n75.00.0\n53.30.1\n50.00.0\n50.00.0\n61.70.0\n37.60.0\n73.70.0\n49.10.0\n30.60.0\n4.57\nRandom\n75.70.1\n61.70.3\n56.20.3\n51.90.3\n48.50.4\n37.70.1\n73.30.0\n49.10.1\n27.60.1\n4.32\nPerplexity (+)\n76.20.0\n64.40.1\n54.10.2\n50.10.0\n38.50.2\n38.40.1\n73.10.0\n49.10.0\n26.80.0\n4.22\nSimilarity (+)\n75.50.0\n61.80.1\n59.00.2\n51.90.2\n55.80.4\n37.60.1\n72.90.0\n49.70.0\n27.00.0\n4.06\nIC Datamodels (+)\n75.30.0\n57.50.1\n50.50.0\n55.70.2\n46.00.3\n38.50.1\n73.60.0\n50.50.0\n30.90.0\n3.46\nInfluence (+)\n75.40.0\n62.10.2\n50.40.1\n55.30.2\n49.70.6\n39.40.1\n73.50.1\n51.10.0\n30.50.1\n3.17\nBest set\n76.00.0\n65.00.1\n59.20.2\n51.70.2\n52.70.3\n37.70.1\n73.90.1\n49.40.0\n29.60.0\n3.14\nGPT-NeoX-20B\nPerplexity (+)\n76.60.0\n73.20.1\n63.20.2\n51.90.2\n42.30.4\n43.20.0\n78.00.0\n54.40.0\n29.50.0\n4.97\nRandom\n77.30.0\n67.00.4\n62.40.3\n51.70.2\n43.50.6\n43.90.1\n77.80.1\n55.10.1\n30.30.0\n4.41\nSimilarity (+)\n77.00.0\n64.10.5\n65.00.3\n51.20.1\n56.00.6\n43.50.1\n77.90.1\n54.80.1\n29.80.1\n4.35\nBest set\n77.60.0\n73.70.1\n63.70.2\n50.50.2\n46.30.5\n43.50.0\n77.60.1\n55.10.1\n31.30.1\n4.08\nOne-shot (+)\n76.50.0\n57.30.1\n53.50.2\n48.90.1\n61.70.0\n44.50.1\n78.50.1\n55.90.0\n32.80.0\n4.06\nInfluence (+)\n78.00.0\n73.60.2\n65.30.1\n51.90.2\n47.20.4\n43.80.1\n78.70.1\n54.90.1\n32.70.0\n2.95\nIC Datamodels (+)\n77.70.0\n75.20.0\n66.30.2\n51.60.2\n44.30.4\n44.10.0\n79.50.1\n55.50.1\n32.90.0\n2.52\nLLaMA-7B\nSimilarity (+)\n77.80.0\n79.60.1\n64.30.3\n52.50.3\n40.20.3\n44.70.1\n76.90.1\n58.20.0\n31.90.1\n4.60\nOne-shot (+)\n77.20.0\n78.20.1\n60.90.3\n51.60.1\n39.10.1\n43.60.0\n78.70.0\n59.90.0\n33.50.1\n4.52\nPerplexity (+)\n78.20.0\n78.10.1\n68.10.1\n50.40.1\n42.40.6\n44.20.1\n77.30.1\n59.30.0\n32.00.1\n4.32\nRandom\n78.00.1\n80.30.1\n63.40.2\n51.10.2\n41.70.3\n44.30.0\n78.70.1\n58.80.1\n32.30.0\n4.11\nInfluence (+)\n78.00.0\n72.70.3\n65.70.2\n54.00.3\n43.00.4\n44.80.0\n78.70.1\n60.00.0\n38.30.0\n3.25\nIC Datamodels (+)\n77.70.0\n75.40.3\n65.10.1\n52.90.3\n42.30.4\n45.40.0\n78.90.0\n60.20.1\n38.70.1\n3.19\nBest set\n78.20.0\n81.10.1\n67.60.1\n51.70.1\n44.50.3\n45.00.1\n77.70.0\n59.80.0\n35.10.0\n2.90\nLLaMA-13B\nSimilarity (+)\n78.50.0\n81.90.1\n57.30.3\n54.00.3\n42.20.6\n50.10.1\n82.80.0\n62.10.0\n36.30.1\n4.78\nPerplexity (+)\n79.10.0\n82.40.1\n61.80.3\n55.00.2\n40.80.1\n50.50.0\n82.50.0\n61.60.0\n35.60.1\n4.57\nRandom\n78.50.1\n82.60.1\n61.10.3\n51.80.2\n42.90.4\n50.40.1\n82.70.0\n62.50.1\n35.70.1\n4.51\nOne-shot (+)\n78.10.1\n84.50.1\n58.30.1\n50.00.0\n38.30.0\n52.60.0\n82.30.0\n63.30.0\n38.80.1\n4.43\nBest set\n78.70.0\n83.20.1\n69.40.3\n54.70.2\n46.90.5\n51.90.1\n82.50.0\n63.70.0\n36.40.1\n3.14\nIC Datamodels (+)\n78.80.0\n83.90.1\n66.60.1\n54.20.2\n41.90.5\n52.90.0\n82.90.0\n62.30.0\n42.50.0\n2.86\nInfluence (+)\n78.70.0\n84.30.1\n68.40.2\n54.50.1\n42.50.5\n53.10.0\n82.70.0\n62.60.0\n42.70.1\n2.70\nOPT-6.7B\nPerplexity (+)\n76.00.0\n69.80.1\n51.10.0\n49.30.1\n47.60.6\n37.60.1\n69.60.0\n53.20.0\n25.40.1\n4.78\nSimilarity (+)\n75.50.0\n66.90.2\n53.60.3\n50.80.1\n58.30.3\n39.00.1\n69.90.1\n52.10.1\n26.90.1\n4.38\nRandom\n75.50.1\n68.20.3\n55.40.3\n51.70.2\n49.40.5\n38.30.1\n70.40.1\n52.00.1\n27.70.1\n4.25\nOne-shot (+)\n76.20.0\n58.10.1\n55.40.3\n50.00.0\n61.70.0\n38.10.1\n68.90.1\n53.00.1\n30.50.1\n3.97\nBest set\n75.30.0\n69.10.1\n57.50.3\n50.70.1\n50.90.3\n38.30.1\n70.90.1\n52.60.0\n27.90.0\n3.89\nIC Datamodels (+)\n75.60.0\n68.50.1\n59.60.1\n55.00.2\n53.20.3\n37.60.0\n71.20.0\n53.70.1\n30.80.1\n3.02\nInfluence (+)\n75.90.0\n67.70.2\n62.70.1\n53.20.2\n52.90.3\n38.10.1\n70.60.1\n53.70.1\n31.30.1\n2.86\nSimilarity (+)\n75.90.1\n71.20.2\n52.90.2\n51.00.2\n58.60.1\n37.40.1\n73.10.1\n54.20.0\n28.90.1\n4.30\nRandom\n76.10.0\n69.50.2\n51.20.1\n53.60.3\n54.80.3\n37.60.1\n73.20.0\n53.60.1\n30.00.1\n4.27\nOne-shot (+)\n75.80.0\n69.00.1\n57.30.2\n50.00.0\n61.70.0\n39.60.0\n72.40.0\n53.30.1\n32.00.0\n4.17\nPerplexity (+)\n76.20.1\n71.80.1\n55.90.2\n53.10.3\n42.40.2\n38.00.0\n73.10.0\n54.10.1\n28.00.1\n4.14\nBest set\n75.80.0\n72.80.1\n53.30.3\n54.50.3\n50.30.4\n37.80.1\n73.10.0\n53.30.0\n31.40.1\n3.87\nIC Datamodels (+)\n75.90.0\n72.00.1\n65.10.2\n56.30.1\n48.10.4\n37.20.0\n72.60.0\n54.30.0\n34.40.1\n3.38\nInfluence (+)\n75.80.0\n71.90.1\n61.70.2\n55.70.1\n57.10.2\n36.80.1\n73.80.0\n54.40.0\n34.00.1\n2.89\nOPT-30B\nPerplexity (+)\n76.80.0\n72.70.2\n61.90.3\n53.50.2\n43.50.6\n40.30.1\n76.30.1\n56.60.0\n28.50.1\n5.16\nRandom\n77.00.0\n71.10.2\n63.20.2\n54.80.1\n49.10.5\n41.50.1\n76.00.1\n55.40.1\n29.60.1\n4.75\nBest set\n76.90.0\n72.60.0\n64.10.3\n55.10.2\n54.80.4\n40.80.0\n75.80.1\n56.10.0\n31.50.0\n4.30\nOne-shot (+)\n77.50.0\n76.50.1\n52.40.1\n51.10.2\n61.60.0\n41.50.0\n76.10.1\n56.60.1\n31.20.0\n3.92\nSimilarity (+)\n77.70.1\n70.10.4\n63.90.1\n53.30.1\n57.10.7\n42.10.1\n76.20.1\n56.70.0\n29.30.0\n3.84\nInfluence (+)\n78.00.0\n74.10.1\n64.60.1\n52.50.1\n51.40.3\n41.60.1\n77.00.0\n57.40.0\n33.30.0\n2.89\nIC Datamodels (+)\n78.10.0\n77.00.0\n65.90.1\n51.40.2\n56.40.1\n42.10.0\n76.60.0\n58.20.0\n31.70.1\n2.41\nSimilarity (+)\n75.90.1\n71.20.2\n52.90.2\n51.00.2\n58.60.1\n37.40.1\n73.10.1\n54.20.0\n28.90.1\n4.30\nRandom\n76.10.0\n69.50.2\n51.20.1\n53.60.3\n54.80.3\n37.60.1\n73.20.0\n53.60.1\n30.00.1\n4.27\nOne-shot (+)\n75.80.0\n69.00.1\n57.30.2\n50.00.0\n61.70.0\n39.60.0\n72.40.0\n53.30.1\n32.00.0\n4.17\nPerplexity (+)\n76.20.1\n71.80.1\n55.90.2\n53.10.3\n42.40.2\n38.00.0\n73.10.0\n54.10.1\n28.00.1\n4.14\nBest set\n75.80.0\n72.80.1\n53.30.3\n54.50.3\n50.30.4\n37.80.1\n73.10.0\n53.30.0\n31.40.1\n3.87\nIC Datamodels (+)\n75.90.0\n72.00.1\n65.10.2\n56.30.1\n48.10.4\n37.20.0\n72.60.0\n54.30.0\n34.40.1\n3.38\nInfluence (+)\n75.80.0\n71.90.1\n61.70.2\n55.70.1\n57.10.2\n36.80.1\n73.80.0\n54.40.0\n34.00.1\n2.89\nOPT-30B\nPerplexity (+)\n76.80.0\n72.70.2\n61.90.3\n53.50.2\n43.50.6\n40.30.1\n76.30.1\n56.60.0\n28.50.1\n5.16\nRandom\n77.00.0\n71.10.2\n63.20.2\n54.80.1\n49.10.5\n41.50.1\n76.00.1\n55.40.1\n29.60.1\n4.75\nBest set\n76.90.0\n72.60.0\n64.10.3\n55.10.2\n54.80.4\n40.80.0\n75.80.1\n56.10.0\n31.50.0\n4.30\nOne-shot (+)\n77.50.0\n76.50.1\n52.40.1\n51.10.2\n61.60.0\n41.50.0\n76.10.1\n56.60.1\n31.20.0\n3.92\nSimilarity (+)\n77.70.1\n70.10.4\n63.90.1\n53.30.1\n57.10.7\n42.10.1\n76.20.1\n56.70.0\n29.30.0\n3.84\nInfluence (+)\n78.00.0\n74.10.1\n64.60.1\n52.50.1\n51.40.3\n41.60.1\n77.00.0\n57.40.0\n33.30.0\n2.89\nIC Datamodels (+)\n78.10.0\n77.00.0\n65.90.1\n51.40.2\n56.40.1\n42.10.0\n76.60.0\n58.20.0\n31.70.1\n2.41\nPIQA\nBoolQ\nRTE\nWIC\nWSC\nARC-c\nARC-e\nHS\nOBQA\nRank (\u2193)\nGPT-J-6B\nSimilarity (-)\n76.00.0\n63.20.2\n53.50.2\n55.30.2\n48.90.5\n38.20.0\n73.50.0\n49.60.0\n27.20.0\n5.51\nRandom\n75.70.1\n61.70.3\n56.20.3\n51.90.3\n48.50.4\n37.70.1\n73.30.0\n49.10.1\n27.60.1\n4.98\nWorst set\n76.20.0\n60.20.1\n52.60.1\n52.10.2\n48.70.3\n37.70.0\n71.80.1\n48.50.1\n27.10.0\n4.22\nIC Datamodels (-)\n76.30.0\n58.80.2\n50.40.0\n50.20.1\n53.00.2\n38.30.1\n71.60.0\n47.10.0\n24.90.0\n3.43\nPerplexity (-)\n73.10.1\n60.10.1\n52.80.1\n51.40.1\n43.80.3\n37.10.1\n72.40.1\n46.30.0\n27.30.0\n3.32\nInfluence (-)\n75.60.0\n57.40.2\n50.60.0\n48.40.1\n47.90.4\n38.50.0\n71.10.0\n47.40.0\n26.20.1\n2.98\nOne-shot (-)\n76.40.0\n58.70.2\n50.00.0\n50.00.0\n38.30.0\n37.40.1\n72.80.1\n46.50.1\n25.30.1\n2.68\nGPT-NeoX-20B\nSimilarity (-)\n76.80.1\n62.50.2\n63.50.1\n52.40.2\n44.30.4\n43.70.1\n77.50.1\n55.30.0\n32.50.0\n5.08\nRandom\n77.30.0\n67.00.4\n62.40.3\n51.70.2\n43.50.6\n43.90.1\n77.80.1\n55.10.1\n30.30.0\n4.94\nPerplexity (-)\n76.30.0\n72.40.1\n61.20.3\n53.50.1\n46.00.6\n44.30.1\n77.40.1\n52.60.0\n30.70.1\n4.57\nWorst set\n76.40.0\n66.90.2\n61.50.4\n51.50.3\n45.50.3\n43.10.1\n77.10.1\n54.80.0\n30.50.1\n4.29\nOne-shot (-)\n77.40.0\n50.90.0\n62.20.2\n50.00.0\n60.90.0\n42.40.1\n76.50.0\n52.80.0\n28.10.1\n3.13\nInfluence (-)\n76.70.1\n53.90.1\n58.10.3\n50.80.1\n38.10.2\n41.80.1\n76.60.0\n53.90.0\n29.20.0\n2.68\nIC Datamodels (-)\n77.10.1\n54.30.1\n57.70.3\n49.40.1\n40.60.2\n41.40.1\n76.90.0\n53.10.0\n29.10.1\n2.52\nLLaMA-7B\nRandom\n78.00.1\n80.30.1\n63.40.2\n51.10.2\n41.70.3\n44.30.0\n78.70.1\n58.80.1\n32.30.0\n5.16\nSimilarity (-)\n77.80.0\n79.20.1\n59.10.1\n51.60.2\n42.60.3\n44.70.1\n78.40.0\n58.60.1\n31.90.1\n4.75\nWorst set\n78.30.0\n77.60.1\n58.10.2\n52.70.1\n41.90.4\n44.00.0\n79.00.0\n58.80.0\n30.10.1\n4.65\nOne-shot (-)\n78.40.0\n78.70.1\n69.10.1\n51.50.1\n61.80.0\n42.30.1\n74.70.1\n57.70.0\n32.00.1\n4.40\nPerplexity (-)\n75.50.0\n81.30.1\n57.90.2\n50.70.1\n41.10.5\n43.00.1\n77.70.0\n56.70.0\n28.70.1\n3.13\nIC Datamodels (-)\n78.20.0\n73.50.3\n53.50.1\n50.60.1\n45.60.8\n43.60.1\n76.90.0\n57.30.0\n27.90.1\n2.94\nInfluence (-)\n78.10.0\n71.90.1\n52.70.1\n49.30.1\n40.40.4\n43.10.1\n76.50.1\n57.20.0\n27.20.1\n2.16\nLLaMA-13B\nSimilarity (-)\n79.20.0\n83.20.0\n58.70.1\n54.60.2\n43.90.5\n51.10.0\n82.30.0\n62.10.0\n37.10.0\n5.51\nRandom\n78.50.1\n82.60.1\n61.10.3\n51.80.2\n42.90.4\n50.40.1\n82.70.0\n62.50.1\n35.70.1\n4.84\nWorst set\n78.80.0\n79.20.1\n54.10.2\n53.30.1\n45.70.6\n50.30.1\n83.00.0\n62.10.1\n33.60.1\n4.60\nPerplexity (-)\n74.90.0\n82.40.1\n57.90.1\n55.40.2\n42.80.4\n49.40.0\n81.40.0\n58.70.0\n33.10.1\n3.56\nOne-shot (-)\n78.70.0\n68.20.2\n53.90.1\n53.10.1\n55.40.7\n50.00.1\n81.40.0\n61.00.0\n26.10.1\n3.22\nIC Datamodels (-)\n78.50.0\n69.30.3\n50.00.0\n51.60.2\n38.90.1\n50.00.1\n82.80.1\n61.80.0\n22.00.1\n2.84\nInfluence (-)\n78.60.0\n68.30.3\n50.00.0\n50.60.2\n39.80.3\n49.30.1\n82.40.1\n61.60.0\n22.90.1\n2.43\nOPT-6.7B\nSimilarity (-)\n75.60.0\n65.40.2\n56.70.2\n52.70.0\n50.80.2\n38.00.1\n70.90.1\n53.00.0\n26.90.0\n4.94\nRandom\n75.50.1\n68.20.3\n55.40.3\n51.70.2\n49.40.5\n38.30.1\n70.40.1\n52.00.1\n27.70.1\n4.70\nWorst set\n76.10.0\n66.40.1\n53.00.3\n52.50.1\n55.30.3\n38.40.1\n69.60.1\n51.40.0\n26.80.0\n4.44\nPerplexity (-)\n75.10.0\n70.70.1\n51.90.2\n50.70.0\n59.60.2\n37.10.1\n69.50.0\n47.80.0\n27.50.0\n3.81\nInfluence (-)\n76.30.0\n61.90.3\n50.80.1\n50.50.1\n51.90.7\n37.20.1\n70.30.1\n51.40.1\n25.10.0\n3.38\nOne-shot (-)\n76.00.0\n65.10.2\n50.00.0\n50.00.0\n38.30.0\n37.60.1\n71.20.0\n50.60.1\n26.80.1\n3.13\nIC Datamodels (-)\n75.70.1\n66.30.1\n50.80.1\n48.10.2\n47.80.2\n36.90.1\n69.80.0\n51.20.0\n23.60.1\n2.65\nOPT-13B\nRandom\n76.10.0\n69.50.2\n51.20.1\n53.60.3\n54.80.3\n37.60.1\n73.20.0\n53.60.1\n30.00.1\n5.10\nSimilarity (-)\n76.00.1\n68.50.1\n50.60.1\n55.80.1\n52.70.3\n38.60.0\n73.60.0\n53.00.1\n29.50.1\n4.75\nPerplexity (-)\n73.70.1\n71.60.1\n50.30.0\n50.00.0\n52.00.5\n38.70.1\n72.50.1\n51.60.0\n30.40.0\n4.00\nIC Datamodels (-)\n77.00.0\n69.10.2\n50.30.0\n50.90.1\n50.70.3\n36.70.1\n72.60.1\n53.10.0\n28.80.1\n3.84\nWorst set\n76.10.0\n67.20.2\n50.40.0\n50.70.1\n48.80.4\n37.40.1\n72.90.0\n53.10.1\n28.60.0\n3.83\nInfluence (-)\n76.60.0\n69.40.1\n50.40.1\n49.20.1\n45.40.3\n36.50.1\n72.50.1\n52.70.0\n28.20.1\n3.05\nOne-shot (-)\n76.20.1\n52.10.0\n50.00.0\n50.00.0\n38.30.0\n35.70.1\n72.50.1\n50.20.0\n29.00.1\n2.14\nOPT-30B\nSimilarity (-)\n77.70.1\n67.30.2\n64.20.2\n54.60.1\n49.30.4\n42.30.1\n76.50.1\n56.30.0\n30.30.0\n5.79\nRandom\n77.00.0\n71.10.2\n63.20.2\n54.80.1\n49.10.5\n41.50.1\n76.00.1\n55.40.1\n29.60.1\n4.87\nWorst set\n77.60.0\n66.50.4\n64.30.2\n54.90.1\n46.50.3\n40.90.0\n75.10.1\n55.50.0\n29.80.0\n4.59\nInfluence (-)\n77.60.0\n61.00.4\n59.10.2\n51.70.1\n43.90.3\n41.30.1\n76.10.0\n54.40.0\n27.80.1\n3.59\nPerplexity (-)\n75.10.0\n73.70.1\n52.00.2\n51.60.1\n46.60.5\n40.90.1\n74.50.1\n53.50.0\n30.60.0\n3.48\nIC Datamodels (-)\n77.40.1\n59.10.1\n60.10.3\n51.10.1\n42.80.2\n40.40.1\n75.40.1\n55.20.0\n26.90.1\n2.98\nOne-shot (-)\n77.70.0\n51.70.0\n50.00.0\n50.00.0\n38.30.0\n40.30.0\n75.80.1\n51.70.0\n28.20.1\n2.02\nTask\nTemplate\nPIQA\nGoal: {goal}\nAnswer: {answer}\nBoolQ\n{passage}\nquestion: {question}?\nanswer: {answer}\nRTE\n{premise}\nquestion: {hypothesis}. true or false?\nanswer: {answer}\nWIC\n{sentence1}\n{sentence2}\nquestion: Is the word \u2018{word}\u2019 used in the same sense in the two sentences\nabove?\nanswer: {answer}\nWSC\nPassage: {text}\nQuestion: In the passage above, does the pronoun \u2018{span2}\u2019 refer to\n{span1}?\nAnswer: {answer}\nArc (Chal.)\nQuestion: {question}\nAnswer: {answer}\nArc (Easy)\nQuestion: {question}\nAnswer: {answer}\nHellaswag\nContext: {context}\nAnswer: {answer}\nOBQA\nContext: {context}\nAnswer: {answer}\nTask\nID\nInfluence\nPrompt\nPIQA\n2305\n0.004877\nGoal: sand paper\nAnswer: can be used to smooth wood for furniture\nRTE\n1439\n0.016923\nAs a real native Detroiter, I want to remind everyone that Madonna is from\nBay City, Mich., a nice place in the thumb of the state\u2019s lower peninsula.\nquestion: Madonna was born in Bay City, Mich.. true or false?\nanswer: true\nWIC\n2033\n0.01273\nEfface the memory of the time in the camps.\nEfface oneself.\nquestion: Is the word \u2018efface\u2019 used in the same sense in the two sentences\nabove?\nanswer: false\nWSC\n98\n0.031323\nPassage: The man lifted the boy onto his bunk bed.\nQuestion: In the passage above, does the pronoun \u2018his\u2019 refer to The man?\nAnswer: false\nArc (Chal.)\n684\n0.004829\nQuestion: Which energy resource is non-renewable?\nAnswer: oil\nArc (Easy)\n859\n0.003275\nQuestion: Which processes change magma into igneous rock?\nAnswer: cooling and crystallization\nHellaswag\n30980\n0.00546\nContext: Education and Communications: [header] How to calculate\nconsumer surplus [title] Understand the law of demand. [step] Most people\nhave heard the phrase \u201csupply and demand\u201d used in reference to the\nmysterious forces governing market economies, but many don\u2019t\nunderstand these concepts\u2019 full implications. \u201cdemand\u201d refers to the desire\nfor a good or service in the marketplace.\nAnswer: Generally, if all other factors are equal, demand for a product will\nfall as its price increases. [substeps] For example, let\u2019s say that a company is\nabout to release a new model of television.\nOBQA\n3640\n0.006248\nContext: Scavengers eat dead what?\nAnswer: fauna\nQuestion: Which processes change magma into igneous rock? Answer: cooling and crystallization\nTask\nID\nInfluence\nPrompt\nPIQA\n10777\n-0.001315\nGoal: baby wipe\nAnswer: Can be pierced by a fork Using the tines\nRTE\n2391\n-0.022086\nSince the fear of death is virtually a universal phenomenon, the death\npenalty is an unparalleled deterrent for people considering a crime.\nquestion: Capital punishment is a deterrent to crime.. true or false?\nanswer: true\nWIC\n4233\n-0.007281\nAfter the fire a still small voice. \u2013 1 Kings 19:12.\nConservatism has many voices.\nquestion: Is the word \u2018voice\u2019 used in the same sense in the two sentences\nabove?\nanswer: false\nWSC\n334\n-0.007789\nPassage: Sara borrowed the book from the library because she needs it for\nan article she is working on. She reads it when she gets home from work.\nQuestion: In the passage above, does the pronoun \u2018it\u2019 refer to the book?\nAnswer: true\nArc (Chal.)\n596\n-0.006895\nQuestion: A research scientist repeatedly observes a bird avoiding a specific\nbutterfly species even though it eats other types of butterflies. Which\nstatement most likely explains the behavior of the bird?\nAnswer: The behavior is learned over the lifetime of the bird.\nArc (Easy)\n1940\n-0.002372\nQuestion: The organisms that convert solar energy and raw materials into\nfood are\nAnswer: producers.\nHellaswag\n8891\n-0.005678\nContext: Surfing: People are surfing on a large wave in the water. A boat is\nin the water. a large wave\nAnswer: crashes in the water.\nOBQA\n978\n-0.004077\nContext: Do objects change size with distance for Stevie Wonder?\nAnswer: No\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) performance variability in large language models (LLMs), emphasizing the need for improved example selection methods to enhance performance.",
        "problem": {
            "definition": "The problem is the high sensitivity of ICL performance to the choice of input examples, which can lead to inconsistent and unpredictable outcomes.",
            "key obstacle": "The main difficulty lies in the lack of effective methods to identify which examples positively or negatively influence the model's performance in ICL."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that not all in-context examples have the same impact on model performance, prompting the need for a systematic way to evaluate these influences.",
            "opinion": "The proposed influence-based example selection method aims to quantify the impact of in-context examples on ICL performance, allowing for better selection of training examples.",
            "innovation": "The key innovation is the introduction of in-context influences as a metric to evaluate and select examples, which significantly outperforms existing selection methods."
        },
        "method": {
            "method name": "Influence-based Example Selection",
            "method abbreviation": "IES",
            "method definition": "IES is a framework that measures the impact of in-context examples on ICL performance by calculating their influences based on performance metrics.",
            "method description": "The method involves prompting the model with various subsets of examples and measuring their effects on performance to determine their influences.",
            "method steps": [
                "Prompt the model on random training subsets and measure validation performance.",
                "Calculate the in-context influence for each example based on the performance data.",
                "Select the top k examples with the highest positive influences for prompting."
            ],
            "principle": "The effectiveness of this method is based on the premise that examples with higher influences lead to better ICL performance, thus guiding the selection process."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using 9 datasets covering various natural language tasks, including binary classification and multi-choice tasks, with a focus on few-shot prompting.",
            "evaluation method": "The performance of the influence-based selection method was assessed by comparing its results against baseline methods using metrics like classification accuracy."
        },
        "conclusion": "The results demonstrate that influence-based example selection significantly improves ICL performance, particularly in multi-choice tasks, highlighting the importance of example selection in achieving better outcomes in LLMs.",
        "discussion": {
            "advantage": "The primary advantage of the proposed method is its ability to systematically identify and select impactful examples, leading to improved performance in ICL.",
            "limitation": "A limitation of the method is that it relies on a fixed training set, which may not account for the variability of examples generated on-the-fly during inference.",
            "future work": "Future research could focus on extending the influence-based framework to dynamically evaluate and predict the performance of examples not present in the training set."
        },
        "other info": {
            "additional notes": "The framework also allows for the exploration of biases in example ordering and the quantification of phenomena like recency bias in ICL."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning (ICL) performance variability in large language models (LLMs), emphasizing the need for improved example selection methods to enhance performance."
        },
        {
            "section number": "1.2",
            "key information": "The problem is the high sensitivity of ICL performance to the choice of input examples, which can lead to inconsistent and unpredictable outcomes."
        },
        {
            "section number": "3.3",
            "key information": "The proposed influence-based example selection method aims to quantify the impact of in-context examples on ICL performance, allowing for better selection of training examples."
        },
        {
            "section number": "3.4",
            "key information": "The method involves prompting the model with various subsets of examples and measuring their effects on performance to determine their influences."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of the influence-based example selection method is based on the premise that examples with higher influences lead to better ICL performance."
        },
        {
            "section number": "6.1",
            "key information": "The framework allows for the exploration of biases in example ordering and the quantification of phenomena like recency bias in ICL."
        },
        {
            "section number": "7",
            "key information": "The results demonstrate that influence-based example selection significantly improves ICL performance, particularly in multi-choice tasks, highlighting the importance of example selection in achieving better outcomes in LLMs."
        }
    ],
    "similarity_score": 0.7107445628436401,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-context Example Selection with Influences.json"
}