{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.12907",
    "title": "Meta-in-context learning in large language models",
    "abstract": "Large language models have shown tremendous performance in a variety of tasks. In-context learning \u2013 the ability to improve at a task after being provided with a number of demonstrations \u2013 is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model\u2019s priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.",
    "bib_name": "codaforno2023metaincontextlearninglargelanguage",
    "md_text": "# Meta-in-context learning in large language models\n1Max Planck Institute for Biological Cybernetics, 2University of T\u00fcbingen - T\u00fcbingen, Germany; 3Google DeepMind - London, United-Kingdom \u2217{julian.coda-forno@tuebingen.mpg.de}\n# Abstract\nLarge language models have shown tremendous performance in a variety of tasks. In-context learning \u2013 the ability to improve at a task after being provided with a number of demonstrations \u2013 is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model\u2019s priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.\n# 1 Introduction\nLarge language models (LLMs) are taking not only machine learning research but also society by storm [1, 2, 3]. Part of what makes these models so persuasive is that their abilities reach far beyond what we expected pure language models to do. They can, among other things, solve challenging reasoning problems, including university-level math questions [4] or analogical reasoning tasks [5], out-of-the-box and without additional training. Much of this power comes from what is known as in-context learning [6]. In-context learning (sometimes also called few-shot learning or few-shot prompting) refers to the ability of an LLM to improve at a given task after being provided with a number of task-relevant demonstrations. This ability sets LLMs apart from traditional models and led to a totally new paradigm \u2013 one which eschews finetuning of weights on task-specific data altogether and instead relies entirely on contextual information. In the present paper, we ask whether the learning algorithm implemented through in-context learning can be improved through in-context learning itself (without the need for any further finetuning of parameters). To study this question, we conducted several experiments where we presented an LLM with multiple learning tasks in a sequence. In three distinct settings, we find evidence for the idea that the in-context learning abilities of an LLM can be recursively enhanced via in-context learning, thereby displaying a form of meta-in-context learning. Figure 1 provides a high-level overview of our approach on an example supervised learning task. More specifically, we first investigate meta-in-context learning on two artificial domains: a supervised function learning task, and a two-armed bandit task. For both of them, we find that sequentially\nPreprint. Under review.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a06/7a064220-d7af-4ef9-99c3-8dc93225a9fb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Meta-in-context learning</div>\nFigure 1: High-level overview of our approach on an example of multiple three-shot regression tasks. We present an LLM with N learning tasks in a row. Improvement within a task indicates that the model is capable of in-context learning. If in-context learning improves across multiple learning tasks, the model is also capable of meta-in-context learning.\npresenting LLMs with multiple learning problems boosts their in-context learning abilities. We then use these idealized domains to identify the drivers behind meta-in-context learning. We find that meta-in-context learning adaptively modifies priors over latent variables, ultimately leading to priors that closely resemble the true statistics of the environment. Furthermore, our analysis reveals that meta-in-context learning can not only be used to change prior expectations but is also capable of reshaping an LLM\u2019s learning strategies. Lastly, we apply our approach to a realistic domain and demonstrate that meta-in-context learning can be used to obtain a learning algorithm that is competitive with traditional algorithms on a benchmark of real-world regression problems.\n# 2 Related work\nIn-context learning: Recent work has shown that LLMs can improve their performance after being shown a few task-relevant demonstrations \u2013 an ability referred to as in-context learning [6]. When and why in-context learning emerges is a matter of ongoing debate, with different theories being proposed. Chan et al. [7] argued that properties of the training data \u2013 such as burstiness and non-stationarity \u2013 are key drivers behind in-context learning. Min et al. [8], on the other hand, found that ground truth demonstrations can be replaced with random labels while barely hurting performance, suggesting that the role of demonstrations is more to prime the model for a particular task. Finally, Xie et al. [9] suggested that LLMs internally need to infer latent variables to make better predictions about future word occurrences, thereby implementing a form of Bayesian inference. In-context learning can solve classical learning tasks: If LLMs do apply some form of Bayesian inference, then one would expect an LLM to also be able to solve classical online learning tasks, such as regression or classification, purely through in-context learning. Previous research suggests that this is indeed the case. Lovre [10], for instance, tested GPT-3 on a range of low-dimensional classification and regression tasks and found that it was often on par with classical learning algorithms such as logistic regression. Likewise, Hegselmann et al. [11] tested an LLM\u2019s few-shot classification abilities on tabular data. They found that their approach outperforms prior deep-learning-based tabular classification methods on several benchmark datasets, and that performance further improves when the model is provided with semantic information about the data.1 LLMs are not only able to solve supervised learning problems but also simple reinforcement learning tasks. To provide one example,\nBinz & Schulz [12] evaluated GPT-3 on a two-armed bandit task and found that its performance exceeded that of human participants who did the corresponding psychological experiment. Meta-in-context versus classical meta-learning schemes: Meta-in-context learning stands in contrast to classical meta-learning schemes in which one adapts a neural network to a distribution over learning problems by adjusting its weights [13, 14]. Historically, this approach has relied on recurrent networks [15, 16, 17, 18] but, more recently, researchers have also started to use transformer-based architectures [19, 20, 21]. For example, Garg et al. showed \u201ctransformers can be trained from scratch to perform in-context learning of linear functions\u201d and that the resulting models achieve \u201cperformance comparable to the optimal least squares estimator.\u201d In a similar vein, von Oswald et al. argued that \u201ctransformers [can in principle] implement gradient descent in their forward pass\u201d and provide empirical evidence that they indeed do so [19]. In contrast to these approaches, our approach adapts a model to a distribution of learning problems entirely through the context itself as opposed to updating weights). Meta-in-context learning and psychological experiments: Human subjects in psychological experiments are typically evaluated using multiple successive learning problems. Lampinen [22] highlighted that this is in contrast to the common practice when evaluating LLMs, which involves probing each task in isolation. Meta-in-context learning addresses this issue by matching the testing procedure of psychological experiments. Therefore, our upcoming analyses also provide insights into\n# 3 Experimental analyses\nWe used the public OpenAI Python API [26] to run all our simulations. This API provides access to several LLMs from the Generative Pre-trained Transformer (GPT) family. We ran all our simulations on the TEXT-DAVINCI-002 model, which is also known as GPT-3. We set the temperature parameter to zero (leading to deterministic responses) unless otherwise noted and retained the default values for all other parameters. It is important to note that all experiments performed in this paper rely entirely on the in-context learning abilities of an LLM, and do not involve any form of finetuning.\n# 3.1 Learning one-dimensional functions\nIn our first set of experiments, we investigated GPT-3\u2019s ability for meta-in-context learning in a simple one-dimensional supervised regression task. In this setting, we provided GPT-3 with a list of input-target pairs from a given task and asked it to make accurate predictions for a new input value. This is one of the most fundamental machine learning problems, and its simplicity makes it an ideal testbed for an initial analysis. Every task n consisted of T = 5 input-target pairs (xt, f(xt)), where t \u2208[1, T] denotes the trial number. Each input-target pair was generated by a linear function of the form f(xt) = a(n)xt + b(n) + \u03b5t, where a(n) and b(n) are task-specific parameters drawn from a probability distribution p(T ). Inputs xt were sampled from U(0, 100) and the trial-specific additive noise \u03b5t was sampled from N(0, 1). For our meta-in-context learning simulations, we considered prompts that include data from up to five tasks, each corresponding to a different underlying function. Following the schema outlined in Figure 1, we iteratively presented each data-point together with the history of previous observations, starting from the first trial in task one up to the last trial in task five. In the box below, you can see an example prompt that we provided to GPT-3:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4438/44389d48-8283-44d2-bdf7-b059f8f86a1f.png\" style=\"width: 50%;\"></div>\nFunction learning prompt\nYou observe 5 machines that produce an output y for a given input x.\nEach machine implements a different function.\nMachine 1:\n[. . .]\nMachine 5:\nx = 52, y = \u2212209;\nx = 18, y = \u2212138;\nx = 60, y = [insert];\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a9d2/a9d2f13b-8a7a-42b4-a358-2f1ae466a98c.png\" style=\"width: 50%;\"></div>\nFigure 2: Meta-in-context learning on the one-dimensional regression task (100 simulations). Errors bars represent 95% confidence intervals. A: MSE across trials for different models. B: GPT-3\u2019s MSE averaged over trials for each task. C: Effects of trial and task for estimating the MSE. D: GPT-3\u2019s prior expectations across tasks (blue) compared to the true task distribution (orange).\n# 3.1.1 Results\nIn preliminary simulations, we found that GPT-3 has a strong bias toward increasing positive functions. To demonstrate the potential of our approach, we wanted to investigate whether it is possible to overwrite this bias via meta-in-context learning. In order to achieve this goal, we sampled taskspecific functions with a negative slope and intercept, i.e. a(n) \u223cN(\u22122, 1) and b(n) \u223cN(\u2212100, 1), and evaluated how observing multiple such tasks influences the model\u2019s behavior. GPT-3 does in-context learning: We first established that GPT-3 can learn within a single task without considering the effects of meta-in-context learning. To do so, we only examined the first task while ignoring all the subsequent ones. We found that performance as measured by the mean-squared error (MSE) improves with additional data-points as shown in Figure 2A (solid lines). GPT-3 matches (or even slightly outperforms) a Bayesian linear regression (BLR) model with default standard normal priors, indicating that in-context learning is able to solve the given problem. This is especially noteworthy since GPT-3 was never told that the underlying functions are linear, as opposed to BLR which has a built-in assumption of linearity. GPT-3 does meta-in-context learning: Next, we investigated whether GPT-3 is capable of meta-incontext learning. For this, we inspected how performance changes across the five tasks. Figure 2A demonstrates that meta-in-context learning is beneficial by comparing performance between the first and the final task (solid vs dashed lines). For reference, we also plotted the performance of a BLR model with access to the ground-truth data-generating distribution. Note that this model has access to privileged information and hence only serves as a performance upper-bound. Figure 2B shows a more detailed development of performance as we increase the number of tasks in the prompt.\nTo test whether the effects of in-context learning and meta-in-context learning are statistically meaningful, we fitted a linear regression model that included the trial and task number as independent variables on the MSE. We found statistically significant effects for both trial number (\u03b2 = \u221230.614\u00b1 3.08, t = \u221219.514, p < 0.001) and task number (\u03b2 = \u221213.26 \u00b1 3.08, t = \u22128.455, p < 0.001), confirming that GPT-3 is capable of both in-context and meta-in-context learning (see Figure 2C). Meta-in-context learning is driven by adaptation of priors: We speculated that GPT-3\u2019s performance improves during meta-in-context learning because it adapts its priors to true environmental statistics. In order to evaluate this hypothesis, we collected GPT-3\u2019s prior expectations before each task by asking it to make sequential predictions for 20 evenly spaced input values (using the same prompt template as above, but setting the temperature to one and feeding back the model\u2019s own predictions as the training data). We omitted outlier predictions that had absolute values of 10, 000 or larger for this analysis. The estimated priors indicate that GPT-3 has an initial bias toward increasing positive functions. However, already after two tasks it adapts its priors to decreasing negative functions, thereby closely matching the true environmental statistics (see Figure 2D). Furthermore, the prior for the intercept term is initially centered around zero but shifts toward the ground-truth value of \u2212100 after three tasks. Taken together, these simulations provide initial evidence that GPT-3 engages in meta-in-context learning, meaning that its in-context learning abilities can be recursively improved via in-context learning itself. We have suggested and empirically confirmed that GPT-3 accomplishes this by adapting its priors across multiple tasks.\n# 3.2 Experiments on two-armed bandit tasks\nIn a next step, we wanted to investigate whether our results from the supervised setting also transfer to a reinforcement learning paradigm. This setting adds an additional layer of complexity because it requires the agent to learn from its own experiences instead of having access to ground-truth solutions of previous tasks. In addition, it allows us to investigate learning strategies and how they evolve during meta-in-context learning. For our simulations, we considered a simple two-armed bandit task, in which an agent repeatedly interacts with two slot machines. In each trial, the agent can select one of two machines and is rewarded based on a probability distribution that is associated with that machine. The agent\u2019s objective is to maximize the total amount of acquired points. We used a cover story that involves a gambler visiting different casinos, which has been used in human experiments with similar tasks [27, 12], to generate our prompts:\n# Two-armed bandit task prompt\nYou are going to different casinos that own two slot machines.\nChoosing the same slot machine will not always give you the same points, but one slot\nmachine is always better than the other. Within a casino, your goal is to choose the slot\nmachine that will give you the most points over the course of 10 trials.\nEach casino owns a different pair of machine.\nYou have received the following points when playing in casino 1:\n[. . .]\nYou have received the following points when playing in casino 5:\n- Machine J delivered 4.2 points.\n- Machine F delivered -7.4 points.\n- Machine J delivered 3.2 points.\n- Machine J delivered 3.9 points.\nQ: We are now performing trial 5 in casino 5.\nWhich machine do you choose be-\ntween machine J and machine F?\nA: Machine [insert].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a1fa/a1fad21e-d7a8-45c4-9818-01054f3e772c.png\" style=\"width: 50%;\"></div>\n# 3.2.1 Results\nFor each task, we sampled independent mean rewards for each machine from N(0, \u221a 64). The actually obtained reward is generated using the mean reward that corresponds to the chosen machine plus some additive Gaussian noise sampled from N(0, \u221a 32). To stay consistent with the previous section, we considered prompts that include data from up to five different tasks for our meta-in-context learning simulations (each of these tasks consisted of ten trials). We used letters to indicate the different slot machines. For each task, we randomly sampled two different letters without replacement to cancel out biases towards certain letters.2 GPT-3 does in-context learning: We again first tested whether GPT-3 learns within a single task. Figure 3A confirms that this is indeed the case. Performance (measured in terms of regret) improves over the first four trials and plateaus afterward. However, when comparing GPT-3 to two baseline algorithms \u2013 a greedy policy and an upper confidence bound (UCB) algorithm \u2013 we observed that it lags in performance prior to meta-in-context learning. GPT-3 does meta-in-context learning: Like in our previous experiment, we found that GPT-3 improves during meta-in-context learning: as it observes more tasks, it gets better at solving new two-armed bandit problems (see Figure 3B). GPT-3\u2019s performance matches that of a greedy policy after having interacted with only four tasks and lags only slightly behind that of the UCB algorithm. We quantified the effects of in-context and meta-in-context learning in a statistical analysis. To do so, we fitted a linear regression model including the trial and task number as independent variables on the trial-wise regret. We found significant in-context learning (\u03b2 = \u22120.221 \u00b1 0.012, t = \u221235.828, p <\n0.001) and meta-in-context learning effects (\u03b2 = \u22120.031 \u00b1 0.012, t = \u22125.002, p < 0.001), thereby reproducing our results from the previous section (see Figure 3C). Meta-in-context learning is driven by adaptation of priors: Following our earlier analysis, we speculated that part of this performance boost arises because GPT-3 adapts its priors toward the statistics of the tasks that were encountered during meta-in-context learning. To verify this, we probed its prior expectations before starting each task by asking \u201chow rewarding do you expect machine X to be?\u201d We set the temperature parameter = 1 and repeated this question five times to reflect a sampling from a prior distribution. Figure 3D visualizes the change of priors across tasks. Before the initial task, GPT-3 expects rewards to be distributed around larger positive values (M = 545.22, SD = 6359). However, at the end of meta-in-context learning, its priors match the true data-generating distribution closely (M = 6.49, SD = 4.41). In addition to looking at the development of priors, the two-armed bandit setting also allows us to investigate whether any changes in strategies happen during meta-in-context learning. For this, we relied on an analysis originally proposed by Gershman [27]. The idea behind this analysis is to define a model that involves a parameterized combination of Boltzmann exploration, a UCB algorithm, and Thompson sampling, and then fit the parameters of this model to data generated by an agent (GPT-3 in our case). It is then possible to determine the extent to which the agent relied on a specific strategy by examining the resulting parameters. We assume that the agent\u2019s beliefs over expected rewards at trial t and action a are captured by the normal distribution p(ra,t) = N(\u00b5a,t, \u03c3a,t)3 and define the following probit regression model based on the parameters of these distributions:\n\ufffd with \u03a6 denoting the cumulative distribution function of a standard normal distribution. Equation 1 recovers a Boltzmann-like exploration strategy for [wBoltzmann, wUCB, wThompson] = [c, 0, 0], a variant of the UCB algorithm for [wBoltzmann, wUCB, wThompson] = [c, d, 0], and Thompson sampling for [wBoltzmann, wUCB, wThompson] = [0, 0, 1] [28]. In our analysis, we furthermore included an interaction effect with task number for each factor to investigate how the applied strategies change during meta-in-context learning. Meta-in-context learning reshapes learning strategies: We found a positive main effect of the value difference Vt (\u03b2 = 0.307 \u00b1 0.027, z = 21.759, p < 0.001), indicating that GPT-3 engages in Boltzmann exploration. GPT-3 becomes more greedy during meta-in-context learning as shown by the positive interaction effect between task number and Vt (\u03b2 = 0.128 \u00b1 0.025, z = 9.633, p < 0.001). Furthermore, we found negative main effects for both relative uncertainty RUt (\u03b2 = \u22120.160 \u00b1 0.010, z = \u221231.788, p < 0.001) and the uncertainty-scaled value difference Vt/TUt (\u03b2 = \u22120.400\u00b1 0.025, z = \u22125.393, p < 0.001), suggesting that GPT-3 avoids uncertain options by default. However, we also found a slight, but significant, increase in UCB-based decisions during meta-in-context learning (\u03b2 = 0.046 \u00b1 0.010, z = 9.279, p < 0.001). Figure 3E shows a visualization of all regression coefficients involved in this analysis. The results presented in this section corroborate those obtained from the supervised setting. GPT-3 generally performed better in a two-armed bandit task after meta-in-context learning. We again observed that GPT-3 accomplishes this by adapting its priors across tasks. In addition, we investigated changes in strategies during meta-in-context learning and found that GPT-3 learned to perform better by exploiting more consistently.\n# 3.3 Regression on real-world data\nIn our final experiment, we wanted to investigate whether our results obtained in the artificial domains scale up to real-world applications. To test this, we considered a multi-dimensional regression benchmark which consists of 60 different real-world datasets introduced in [29].\n(1)\nFor each simulation, we randomly selected five different tasks from the benchmark and then sampled five data-points without replacement for each task. We used five features for all tasks. If a dataset contained less than five features, we omitted it from our analysis, which yielded 42 remaining datasets. For tasks exceeding five features, we used a sub-selection procedure that retained only the top five features based on their F-value with respect to the target variable evaluated in a univariate linear regression. To maintain a consistent regression loss across all tasks, we normalized both the feature and target spaces to the interval of [\u22121, 1]. The resulting prompts follow the general template outlined earlier:\n# Regression on real world data\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6cbd/6cbd1af2-aee0-41fc-bfae-348cca6b0d7c.png\" style=\"width: 50%;\"></div>\n# 3.3.1 Results\nGPT-3 does in-context and meta-in-context learning: Following the previous experiments, we investigated the learning curves of GPT-3 and GPT-3 after meta-in-context learning. We additionally included two baselines in our analysis: BLR and a random forest model. We measured performance by the root-mean-squared error (RMSE). Figure 4A shows the learning curve for all these models. Like in the previous cases, we found that GPT-3 improves with additional data-points, confirming that it is capable of in-context learning in this setting. In addition, meta-in-context learning further improved performance. GPT-3 after meta-in-context learning matched the performance of BLR and was only slightly outperformed by the random forest model. Meta-in-context learning constrains predictions: Most of the improvement of meta-in-context learning seems to come from zero-shot performance. We hypothesized that this is the case because GPT-3 acquires an understanding of potential target value ranges during meta-in-context learning. To test this hypothesis, we plotted the proportion of GPT-3\u2019s outputs that fell at the extreme values of the target space or outside of it in Figure 4B. We found that meta-in-context learning substantially reduces the number of predictions that are outside of this range, indicating that meta-in-context learning helps to learn a better initial guess. Meta-in-context learning works better when task similarity is high: However, learning a good initial guess is not the full picture. We speculated that meta-in-context learning is more effective if there is a higher similarity to previously encountered tasks. To verify this, we fitted a linear regression model with trial and task similarity on the RMSE. We computed task similarity as the average similarity between the current data-point and the data-points from all previously observed tasks (each individual similarity measure was obtained using a radial basis function kernel). We found a significant effect of trial (\u03b2 = \u22120.089 \u00b1 0.020, t = \u22129.204, p < 0.001) as shown in Figure 4C, confirming that performance within a task improves with additional observations. Furthermore, we found a significant effect of task similarity (\u03b2 = \u22120.147 \u00b1 0.020, t = \u221215.223, p < 0.001), suggesting that meta-in-context learning works best if similarity to previously encountered tasks is high. The findings outlined in this section provide further evidence for meta-in-context learning in LLMs. Notably, GPT-3 exhibited superior performance in a real-world multi-dimensional regression task following meta-in-context learning. We identified two reasons for this. First, meta-in-context learning constrained initial model predictions to the range of plausible values, and second, GPT-3 was able to leverage similarities to previously encountered tasks in order to improve its predictions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb18/bb184a6b-9f5a-4bb1-a238-629dc6ad0a28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Meta-in-context learning on the real-world regression benchmark (42 \u00b7 50 simulations). Errors bars represent 95% confidence intervals. A: RMSE across trials for different models. B: Percentage of predictions outside or equal to the extremes of the squashed target range. C: Effects of trial and task similarities for estimating the RMSE.</div>\nNote that while we were running our model simulations, OpenAI released a new model \u2013 GPT4. We repeated the analysis from this section on this new model and described our results in the Supplementary Material. In general, GPT-4 reproduced the findings of the section. However, while GPT-4 was capable of meta-in-context learning, it performed slightly worse than GPT-3.\nWe have demonstrated that LLMs can improve their in-context learning abilities via in-context learning itself, i.e., that they are capable of meta-in-context learning. Meta-in-context learning was not only able to overwrite an LLM\u2019s priors but also changed its learning strategies, as demonstrated in two artificial domains. Finally, we applied our approach to a real-world benchmark of regression tasks where we found that meta-in-context learning leads to algorithms that are competitive with standard learning algorithms. Perhaps the most significant shortcoming of our model simulations is that they all relied on learning tasks with just a handful of observations. This limitation is mainly due to the practical constraint of a finite context window coupled with meta-in-context learning\u2019s rapid prompt length increase. To ensure that we remain within the allowed context length (and to keep the monetary costs for our simulations at a reasonable level), we had to make this design choice. However, we believe that \u2013 despite this restriction \u2013 our simulations were sufficient to illustrate the potential of meta-in-context learning. We hope that future LLM iterations with longer context lengths and lower inference costs will allow us to extend our simulations to larger datasets. In addition, the tasks we probed were rather simplistic in their nature. That being said, we think we have reasonably covered the space of fundamental learning paradigms, including a supervised problem, a reinforcement learning problem, and a collection of real-world datasets. With the increasing availability of multi-modal models, it will furthermore become feasible to apply our approach to other domains. In this context, two obvious candidates are classification tasks with visual stimuli or grid-based navigation tasks. In summary, our work has both near- and long-term consequences. In the near term, it indicates that it is not strictly necessary to engineer the perfect prompt for an LLM so that it can solve a given learning problem. Instead, LLMs are \u2013 to some degree \u2013 able to infer the required information from just a handful of related-tasks examples. In the long term, it could point to a paradigm where we adapt these models to the environment they are applied in purely through meta-in-context learning rather than finetuning them using traditional means.\n# Acknowledgments and Disclosure of Funding\nWe would like to thank Zeb Kurth-Nelson for the helpful discussions. Zeynep Akata acknowledges partial funding by the ERC (853489 - DEXIM).\n# 5 Supplementary Material\n# 5.1 Regression on real-world data using GPT-4\nIn this section we investigated the behavior of the newly released GPT-4 model for our last experiment. We proceeded in the same way as for GPT-3.4 We observed that GPT-4 actually performs slightly worse both before and after meta-in-context learning as shown in Figure 5A. Furthermore, we observed a slightly higher percentage of extreme predictions, particularly for GPT-4\u2019s first trial (see Figure 5B). Finally, our analysis also revealed significant effects for trial (\u03b2 = \u22120.133 \u00b1 0.024, t = \u221210.858, p < 0.001) and task similarity (\u03b2 = \u22120.196 \u00b1 0.024, t = \u221215.963, p < 0.001) as shown in Figure 5C.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee7e/ee7e2024-9e94-4877-b513-33935da9c64d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Meta-in-context learning on the regression on real-world data experiment (42 \u00b7 30 simulations). Errors bars represent 95% confidence intervals. A: RMSE across trials for different models. B: Percentage of predictions outside or equal to the extremes of the squashed target range. C: Effects of trial and task similarities for estimating the RMSE.</div>\n4It is worth noting that the API slightly changed and now provides the option to tailor the message with an assistant and a system. We did not use them except for the first trial where GPT-4 struggled to give a numerical output. Indeed, for some examples it instead produced messages such as \u201cunfortunately without any information about the relationship between the variables, the prediction is not possible.\u201d Therefore, only for that one case, we added the system functionality as follows: {\"role\": \"system\", \"content\": \"If no previous examples, sample y from your prior distribution. But do not give any non numerical answer! Even if you are unsure, try to predict y as well as possible.\"}.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving in-context learning abilities in large language models (LLMs) through a novel approach termed meta-in-context learning, highlighting the limitations of traditional methods and the need for a new paradigm.",
        "problem": {
            "definition": "The problem is the limited capacity of existing in-context learning methods to adaptively improve their performance across multiple tasks without the need for parameter finetuning.",
            "key obstacle": "The core obstacle is that traditional in-context learning lacks the recursive enhancement capability, which limits the model's adaptability and effectiveness in diverse tasks."
        },
        "idea": {
            "intuition": "The idea was inspired by the notion that LLMs might enhance their learning strategies through repeated exposure to varied tasks, similar to how humans learn from multiple experiences.",
            "opinion": "The proposed idea involves utilizing in-context learning itself as a mechanism to improve the learning capabilities of LLMs, thereby creating a feedback loop of learning.",
            "innovation": "The primary innovation lies in demonstrating that LLMs can recursively enhance their in-context learning abilities, contrasting with traditional methods that require weight updates."
        },
        "method": {
            "method name": "Meta-in-context learning",
            "method abbreviation": "MICL",
            "method definition": "Meta-in-context learning is defined as the recursive enhancement of an LLM's in-context learning abilities through repeated exposure to multiple learning tasks without finetuning.",
            "method description": "The core of the method involves presenting an LLM with a sequence of tasks to improve its learning strategies adaptively.",
            "method steps": [
                "Present multiple learning tasks sequentially to the LLM.",
                "Allow the LLM to learn from the demonstrations provided in each task.",
                "Evaluate the LLM's performance across tasks to identify improvements in learning strategies."
            ],
            "principle": "This method is effective because it enables the model to adapt its priors based on the observed statistics of the tasks, enhancing its predictive capabilities."
        },
        "experiments": {
            "evaluation setting": "The experimental setup included simulations on idealized domains (one-dimensional regression and two-armed bandit tasks) and real-world regression benchmarks using the OpenAI Python API with the GPT-3 model.",
            "evaluation method": "Performance was assessed through metrics such as mean-squared error (MSE) for regression tasks and regret for bandit tasks, with statistical analyses confirming the significance of improvements."
        },
        "conclusion": "The experiments demonstrated that meta-in-context learning effectively enhances the in-context learning abilities of LLMs, leading to performance competitive with traditional learning algorithms across various tasks.",
        "discussion": {
            "advantage": "Key advantages include the ability to improve learning without the need for finetuning, allowing for more efficient adaptation to new tasks.",
            "limitation": "A significant limitation is that all experiments relied on a limited number of observations due to context window constraints, which may affect the generalizability of the findings.",
            "future work": "Future research should explore extending meta-in-context learning to more complex tasks and multi-modal models, as well as addressing the limitations related to context length."
        },
        "other info": {
            "acknowledgments": "The authors thank Zeb Kurth-Nelson for helpful discussions and acknowledge partial funding by the ERC (853489 - DEXIM).",
            "model comparisons": {
                "GPT-3": "Outperformed traditional algorithms in various tasks after meta-in-context learning.",
                "GPT-4": "While capable of meta-in-context learning, it performed slightly worse than GPT-3 in similar tasks."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of improving in-context learning abilities in large language models (LLMs) through a novel approach termed meta-in-context learning."
        },
        {
            "section number": "1.2",
            "key information": "The limited capacity of existing in-context learning methods to adaptively improve their performance across multiple tasks without the need for parameter finetuning highlights the significance of in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "Meta-in-context learning is defined as the recursive enhancement of an LLM's in-context learning abilities through repeated exposure to multiple learning tasks without finetuning."
        },
        {
            "section number": "3.4",
            "key information": "The core of the method involves presenting an LLM with a sequence of tasks to improve its learning strategies adaptively."
        },
        {
            "section number": "4.1",
            "key information": "The proposed idea involves utilizing in-context learning itself as a mechanism to improve the learning capabilities of LLMs, thereby creating a feedback loop of learning."
        },
        {
            "section number": "6.1",
            "key information": "A significant limitation is that all experiments relied on a limited number of observations due to context window constraints, which may affect the generalizability of the findings."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrated that meta-in-context learning effectively enhances the in-context learning abilities of LLMs, leading to performance competitive with traditional learning algorithms across various tasks."
        }
    ],
    "similarity_score": 0.7147726199404331,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Meta-in-context learning in large language models.json"
}