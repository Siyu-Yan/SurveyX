{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.16822",
    "title": "Rethinking Certification for Trustworthy Machine Learning-Based Applications",
    "abstract": "Machine Learning (ML) is increasingly used to implement advanced applications with non-deterministic behavior, which operate on the cloud-edge continuum. The pervasive adoption of ML is urgently calling for assurance solutions assessing applications non-functional properties (e.g., fairness, robustness, privacy) with the aim to improve their trustworthiness. Certification has been clearly identified by policymakers, regulators, and industrial stakeholders as the preferred assurance technique to address this pressing need. Unfortunately, existing certification schemes are not immediately applicable to non-deterministic applications built on ML models. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues, and proposes a first certification scheme for ML-based applications.",
    "bib_name": "anisetti2023rethinkingcertificationtrustworthymachine",
    "md_text": "# Rethinking Certification for Trustworthy Machin Learning-Based Applications\nMarco Anisetti, Claudio A. Ardagna, Nicola Bena, Ernesto Damiani\nAbstract\u2014Machine Learning (ML) is increasingly used to implement advanced applications with non-deterministic behavior, which operate on the cloud-edge continuum. The pervasive adoption of ML is urgently calling for assurance solutions assessing applications non-functional properties (e.g., fairness, robustness, privacy) with the aim to improve their trustworthiness. Certification has been clearly identified by policymakers, regulators, and industrial stakeholders as the preferred assurance technique to address this pressing need. Unfortunately, existing certification schemes are not immediately applicable to non-deterministic applications built on ML models. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues, and proposes a first certification scheme for ML-based applications.\n22 Oct 2023\nModern applications consist of elastic server-side processes running in the cloud, implemented as micro- and nano-services developed with cloud-native technologies and orchestrated at run time. The availability of new orchestration platforms and programming frameworks is making it possible to execute these applications at line speed [1]. In the fullness of time, an intelligent cloud continuum will support autonomous, self-configuring applications that will request the activation of its services wherever they are needed, including innovative IoT devices [2]. While this new paradigm promises many advantages in terms of availability, elasticity, and, ultimately, quality of service, its complexity is much higher than its predecessors\u2019. This complexity leap is affecting the governance, risk, and compliance landscape, and even the procedures to guarantee security and safety of citizens. To understand why, let us discuss where we stand right now in terms of available solutions to assess and verify applications behavior. . . Traditionally, assurance techniques have been used to assess and verify the trustworthiness of a system or application [3]. Along the years, certification has become the most popular assurance technique, providing a way for a trusted authority to assert that a system or application supports a given (set of) non-functional property according to some evidence on its operation. Test-based certification schemes have been applied to software systems since the Eighties, with the release of the Orange book.1 Test-based schemes underwent a crisis in the middle of the past decade, when it became clear that certifying service-based applications, even with the low degree of autonomy available at the time, required monitoring and run-time re-verification, as different services were recruited at each execution. In 2012, on the\n\u2022 Marco Anisetti, Claudio A. Ardagna, Nicola Bena, Ernesto Damiani are with the Department of Computer Science, Universit\u00e0 degli Studi di Milano, Milan, Italy. \u2022 Ernesto Damiani is alo with with Khalifa University of Science and Technology, Abu Dhabi, UAE. 1. U.S. Department of Defense. \u201cDepartment of Defense Trusted Computer System Evaluation Criteria\u201d. Dec. 1985\ncrest of the service-oriented computing wave, the seminal ASSERT4SOA project2 led the way toward new generation of dynamic certification techniques with the following slogan: You live in a certified house, you drive a certified car, why would you use an uncertified service? At that time, certification enabled users to select and compose applications on the basis of their certified properties. Today, a second crisis of certification schemes is looming, as the massive adoption of machine learning is radically transforming applications behavior [4]. Opaque ML models pose new concerns on how to test and certify the trustworthiness, safety, and reliability of the applications. The challenges we faced in 2012 are back today, like the most classic of groundhog days. A new slogan is then emerging:\nYou use certified services, you hire certified professionals, why would you use an application driven by uncertified machine learning?\nThis question sheds some light on the motivations of the increasing push towards the definition of sound techniques for the non-functional certification of ML-based applications [5], [6]. What needs to be certified is the behavior of the application driven by ML models, rather than some theoretical notion on the ML models. For instance, fairness means there is no discrimination among users accessing an application; privacy and security mean that the application safely processes user data; robustness means that the application will keep operating reliably while under attack. Nevertheless, these properties depend on the ML models driving the application, and on the process/data used to train them. Training data can be partial or inaccurate (affecting fairness), poisoned (affecting robustness and security), and sensitive (affecting privacy and explainability). This article outlines the key elements of a sound certification scheme for ML-based applications. To this aim, we start from current certification schemes and analyze their limitations. We then give a first definition of a certification\n2. https://cordis.europa.eu/project/id/257351/results\nACCEPTED FOR PUBLICATION IN IEEE INTERNET COMPUTING; DOI: 10.1109/MIC.2023.3322327\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5f87/5f879568-4a80-4ac5-aa34-5a43cd2dc2b4.png\" style=\"width: 50%;\"></div>\nscheme for ML-based applications, based on simultaneous verification of three factors: i) the data used for training, ii) the training process, and iii) the ML model. We finally apply the scheme in a real-world example of its application.\n# 1 TRADITIONAL CERTIFICATION SCHEMES\nTraditional certification schemes evolved into dynamic schemes suitable for deterministic composite applications, where services are orchestrated at run time according to their non-functional properties [7]. Application properties are inferred from the ones of its components and continuously verified across service changes [8], [9]. Figure 1 shows the typical process of a certification scheme. The process starts with a Certification Authority (CA) defining the certification model, that is, a specification of the activities to be executed to certify an application. Formally, the certification model is a tuple CM= \u27e8p, ToC, E\u27e9, where p is the non-functional property to be certified (e.g., confidentiality), ToC the target of certification, that is, the application to be certified (e.g., a cloud-based application), and E the evidence collection model (e.g., a set of test cases). An accredited lab, delegated by the CA, executes E to collect the evidence that may support the awarding of a certificate proving p on ToC [3], [7]. Recently, certification schemes broaden the definition of target of certification [7] towards multi-factor certification.\nMulti-factor certification is a natural evolution of certification schemes to accomodate the peculiarities of a modern applications. The non-functional posture of an application, in fact, depends on its software artifacts as well as on the processes that brought it to operation (e.g., development and deployment processes).\nHowever, even dynamic, multi-factor certification schemes struggle with the latest evolution of modern applications towards ML.\n# 2 CHALLENGES IN ML-BASED APPLICATIONS CERTIFICATION\n# CERTIFICATION\nCERTIFICATION Today, the certification of ML-based applications (ML certification in the following) is more an art than a science [10], resulting in ad hoc solutions tailored to specific properties (e.g., explainability, fairness, and robustness [11], [12], [13]). Research is a standstill: no certification scheme for ML is available, despite the increasing push coming from society [5]; at the same time, none of the existing system certification schemes [3] can be adapted to the certification of ML. We argue that this standstill is due to four unsolved challenges we designate here as C1\u2013C4. To exemplify the challenges, we introduce a reference scenario that considers a malware detection application (malware detector in short). It is based on a Deep Learning model trained on real data collected from the field, as well as synthetic data generated according to a GAN [14]. Data are performance metrics retrieved from the underlying system. To operate in an adversarial environment, the malware detector must be certified for property robustness against inference-time attacks, that is, its ability to correctly operate in the presence of adversaries interfering with the malware detection process by perturbing collected data. C1: Target definition. A target of certification ToC is commonly defined as a list of components (i.e., endpoints, services, functions) with clear and unambiguous (i.e., deterministic) behavior. This approach does not suit applications whose components are not deterministic [13]. Current definitions of ToC are inapplicable to ML-based applications and must evolve to accomplish the uncertainty introduced by ML models. In our reference scenario, the behavior of the deep learner must be certified in terms of the robustness of the dataset and process used for training, as well as the characteristics of the learned model. C2: Property definition. The literature is rich of wellformalized non-functional properties (e.g., k-anonymity for privacy, confidentiality, integrity and availability for security), where property definition is decoupled from property verification. The latter is in fact left to the evidence collection model. ML certification instead lacks of commonly accepted and rigorous definitions of ML properties (e.g., explainability, fairness, and robustness), where property verification must be included in the property definition. Property verification in fact substantially characterizes the property itself and defines the means driving evidence collection [11], [12]. For instance, in our reference scenario, property ML robustness must specify how adversarial samples are crafted for run-time verification of the adversarial attacks. C3: Certification process. Certification process relies on evidence collection models executing test cases or monitoring rules to collect the evidence at the basis of a certificate award [3]. Traditional evidence collection statically assesses application interfaces, which might be insufficient to certify the behavior of an ML-based application. Evidence collection model for ML certification must consider three factors, namely, (training) data, (training) process, and the ML model itself. In particular, factor data is novel and must consider how a model is learned (i.e., developed), including the specific characteristics of the training set. The latter\nis completely neglected in traditional systems certification and would compare to the evaluation of the application developer (e.g., her experience and skills).\nis completely neglected in traditional systems certification and would compare to the evaluation of the application developer (e.g., her experience and skills). C4: ML pipelines. The structure of an ML-based application is recursive: each of its components can implement an ML pipeline orchestrating other components implementing an aspect of ML, from data ingestion to data processing. Certification schemes must support such a structure. In our reference scenario, the certification of property robustness should consider the robustness of the ML model against inference-time attacks, as well as the integrity of retrieved performance metrics. In some cases, this can be achieved by customizing existing solutions for certificate composition [8]. However, this challenge remains an open issue that we leave for our future work.\n# 3 ML-BASED APPLICATIONS CERTIFICATION\nTo address challenges C1\u2013C3, we need to reshape traditional certification schemes according to three main aspects: i) multi-factor certification of ML-based applications behavior (challenge C1), ii) ML-specific non-functional properties (e.g., fairness, explainability, robustness) definition (challenge C2), iii) ML-specific evidence collection models supporting non-functional properties verification at point ii) (challenge C3). Multi-factor certification of ML-based applications behavior. Multi-factor certification schemes [7] are the natural choice for certifying ML-based applications. Three factors f should be considered as follows: \u2022 factor data (fd), including information on the dataset used for model training/validation (e.g., characteristics of the samples); \u2022 factor process (fp), including information on the training process (e.g., adoption of boosting, transfer learning); \u2022 factor model (fm), including information on the behavior of the ML model in operation. Our three-factor certification model CM is a set of three independent certification models {CMfd, CMfp, CMfm}, where each CM\u2217is a tuple of the form \u27e8p\u2217,ToC\u2217, E\u2217\u27e9. Each certification model implements a certification process as follows. Certification model CMfd=\u27e8pfd, ToCfd, Efd\u27e9(factor data) evaluates the dataset used for training and its impact on the ML model. For instance, a poisoned training set negatively impacts on property robustness of malware detector, since it reduces its ability to distinguish between benign and malign samples. CMfd includes a property pfd specific for data, a target ToCfd modeling the dataset used for training/validation, and an evidence collection model Efd specifying the procedure for collecting evidence on the ToC, including evidence on dataset balancing and feature extraction. Certification model CMfp=\u27e8pfp, ToCfp, Efp\u27e9(factor process) expresses how the training process is implemented and its impact on the ML model. For instance, a sanitization technique for fixing poisoned samples in the dataset positively impacts on property robustness of malware detector,\nOur three-factor certification model CM is a set of three independent certification models {CMfd, CMfp, CMfm}, where each CM\u2217is a tuple of the form \u27e8p\u2217,ToC\u2217, E\u2217\u27e9. Each certification model implements a certification process as follows. Certification model CMfd=\u27e8pfd, ToCfd, Efd\u27e9(factor data) evaluates the dataset used for training and its impact on the ML model. For instance, a poisoned training set negatively impacts on property robustness of malware detector, since it reduces its ability to distinguish between benign and malign samples. CMfd includes a property pfd specific for data, a target ToCfd modeling the dataset used for training/validation, and an evidence collection model Efd specifying the procedure for collecting evidence on the ToC, including evidence on dataset balancing and feature extraction. Certification model CMfp=\u27e8pfp, ToCfp, Efp\u27e9(factor process) expresses how the training process is implemented and its impact on the ML model. For instance, a sanitization technique for fixing poisoned samples in the dataset positively impacts on property robustness of malware detector,\ndecreasing the shift in the learnt classification boundaries. CMfp includes a property pfp specific for the training process, a target ToCfp modeling the training process, and an evidence collection model Efp including the procedure for collecting evidence on the design and execution of the training process, such as the inspection of checkpoints generated during training. Certification model CMfm=\u27e8pfm, ToCfm, Efm\u27e9(factor model) evaluates the ML model in operation. It is a crucial target of certification and its verification is strongly intertwined with the property to be verified. For instance, property robustness of the malware detector can be actively tested assessing its ability to spot an adversary trying to inject adversarial samples to alter the ML model predictions. As another example, property privacy can be compromised by attacks that infer the presence of a sample in the training set. This is done by inspecting the ML model predictions [15]. However, if the application returns the predicted label only, the attack fails and privacy is preserved despite the lack of a specific protection. CMfm includes a property pfm specific for the ML model, a target ToCfm describing the ML model (e.g., its architecture and parameters), and an evidence collection model Efm including the procedure for collecting evidence on the behavior of the ML-based application, such as functions exercizing the ML model. ML-specific non-functional properties. ML certification requires the definition of ML specific non-functional properties (e.g., fairness, explainability, robustness, safety) and the redesign of traditional non-functional properties (e.g., confidentiality, integrity, availability \u2013 CIA). In general, these properties are the union of factor-specific properties pfd, pfp, and pfm, and must specify the verification means driving evidence collection (see challenge C2). We note that, depending on the property, some factors might not be relevant and verification means might be neglected. Let us consider property robustness in our reference scenario. It is the union of property robustness of the training set (pfd), the training process (pfp), and the ML model (pfm). For example, property robustness of the training set (pfd) is defined as the absence of targeted poisoning in the training set; its definition includes the function used to detect poisoned points. Let us then consider property integrity. It traditionally proves the integrity of the application and its artifacts. Property integrity of ML instead proves the integrity of the ML model behavior according to the three factors. Property integrity in factor data is the integrity of training data (e.g., verifying that the dataset cannot be altered). Property integrity in factor process is the integrity of the training process (e.g., a training process that includes adversarial training specification). Property integrity in factor model is the integrity of the generated ML model (e.g., verifying that the packaged ML model is tamper-proof). ML-specific evidence collection models. Evidence collection models are factor-specific, and describe how to collect evidence according to the three factors. All evidence and metadata collected during the certification of each factor must be stored in certificates, to ensure reproducibility and trustworthiness. Let us consider property robustness in our reference sce-\n<div style=\"text-align: center;\">Table 1 Summary of the certification scheme applied on our scenario.</div>\nf\np\nToC\nE\nOutcome\nfd\nRobustness against inference-time attacks\n(Absence of adversarial poisoning in the training set)\nTraining set\n\u2212Check the presence of poisoned samples\n\u2212evidence: poisoned samples=\u2205\n\u2713\nfp\nRobustness against inference-time attacks\n(Usage of randomized smoothing and adversarial train-\ning)\nTraining process\n\u2212Check the training process and checkpoints\n\u2212evidence: training process does not include randomized\nsmoothing and adversarial training\n\u2717\nfm\nRobustness against inference-time attacks\n(Ineffectiveness of adversarial attacks)\nMalware\ndetector\n(Deep\nLearning\nmodel)\n\u2212Craft and send adversarial samples to the ML model\n\u2212evidence: recall=0\n\u2717\nnario. Efp and Efm must be adapted to collect data coming from the training process and the ML model coping with their opaqueness. For instance, Efp verifies the robustness of the training process by ensuring the usage of strengthening techniques. Efm crafts and sends adversarial samples to verify robustness. In addition, let us consider an evidence collection process Efm for property privacy. This property can be verified in terms of the (in)ability to reverse-engineer training data while operating the ML model [15], [16].\n# 4 CERTIFICATION IN ACTION\nWe demonstrate our ML certification scheme in our reference scenario, which considers a malware detection application (see [14] for more details on the malware detector). We recall that the property of interest is robustness against inference-time attacks, that is, the ability of the malware detector to behave correctly in the presence of an adversarial perturbation aimed to hide malware activities. Table 1 summarizes the three factors and their outcome. Factor data. It defines a certification model CMfd=\u27e8pfd, ToCfd, Efd\u27e9as follows. \u2022 Property robustness pfd is defined as the absence of poisoned samples from the training set. They are samples injected in the training set to alter the classification boundaries learnt by the model, thus masking malware activities at inference time [17]. \u2022 Target ToCfd is the training set. \u2022 Evidence collection model Efd adapts the poisoning removal technique in [18] to flag possibly poisoned samples. In our scenario, no poisoned samples are retrieved, and the assessment for factor data fd is positive (\u2713). Factor process. It defines a certification model CMfp=\u27e8pfp,ToCfp, Efp\u27e9as follows. \u2022 Property robustness pfp is defined as the usage of two strengthening techniques during the training process: randomized smoothing and adversarial training. These techniques reduce the presence of poisoned samples and the impact of adversarial samples, respectively. \u2022 Target ToCfp is the training process. It does not include any strengthening techniques.\nFactor process. It defines a certification model CMfp=\u27e8pfp,ToCfp, Efp\u27e9as follows.\n\u2022 Property robustness pfp is defined as the usage of two strengthening techniques during the training process: randomized smoothing and adversarial training. These techniques reduce the presence of poisoned samples and the impact of adversarial samples, respectively. \u2022 Target ToCfp is the training process. It does not include any strengthening techniques.\n\u2022 Evidence collection model Efp inspects the training code, as well as the checkpoints created during training, to retrieve evidence on the usage of randomized smoothing and adversarial training. In our scenario, the process inspection fails due to the lack of both techniques and the assessment for factor process fp is negative (\u2717). Factor model. It defines a certification model CMfm=\u27e8pfm,ToCfm, Efm\u27e9as follows. \u2022 Property robustness pfm is defined as the effectiveness of adversarial attacks against the ML model. It specifies how samples of adversarial attacks are crafted and sent to the model. \u2022 Target ToCfm is the malware detector. \u2022 Evidence collection model Efm exercises the malware detector, sending adversarial (malware) samples and verifying whether the recall retrieved on such samples is larger than 0.95. In our scenario, recall on adversarial samples is 0, meaning that they are all misclassified as benign. The assessment for factor model fm is negative (\u2717). To conclude, the malware detector lacks of the proper robustness to operate in adversarial settings. Although it might appear that fm is the only relevant factor and traditional certification is sufficient, the certification of the three factors allows the final users to completely understand how the data, the training process, and the model in operation jointly contribute or not to the requested non-functional property. For instance, the failure in factor process motivates the failure in factor model. A certificate can be awarded for each factor independently, in case of positive outcome (\u2713), or a single certificate can be released by merging the three assessments according to predefined rules.\n\u2022 Evidence collection model Efp inspects the training code, as well as the checkpoints created during training, to retrieve evidence on the usage of randomized smoothing and adversarial training.\nIn our scenario, the process inspection fails due to the lack of both techniques and the assessment for factor process fp is negative (\u2717).\n# 5 CONCLUSIONS\nThis paper makes a first step towards the certification of ML-based applications. The road ahead is still long and impervious. Our evidence collection techniques based on analysis of the training set, training process, and ML model predictions (Table 1) must be complemented by other approaches such as abstract interpretation [19] or inspection of a surrogate white-box model [20]. The life cycle of ML models and their certificates must be carefully managed. ML models need in fact to be continuously adapted according\nto evolving system behavior, novel requirements, and drift in the incoming data, to name but a few. This requires certification to follow the changes of ML models along their life cycle. However, certification should not be limited to track changes; it should drive ML models evolution, so that ML models can evolve while supporting the desired properties in all factors. To conclude on an optimistic note, we are confident that certification will eventually succeed in supporting trust in ML-based application behavior. It is a good omen that ongoing regulatory discussions [5], [6] agree that certification should target all artifacts involved in ML training and operations.\n# ACKNOWLEDGMENTS\nThe work was partially supported by the projects i) MUSA \u2013 Multilayered Urban Sustainability Action \u2013 project, funded by the European Union - NextGenerationEU, under the National Recovery and Resilience Plan (NRRP) Mission 4 Component 2 Investment Line 1.5: Strengthening of research structures and creation of R&D \u201cinnovation ecosystems\u201d, set up of \u201cterritorial leaders in R&D\u201d (CUP G43C22001370007, Code ECS00000037); ii) SERICS (PE00000014) under the NRRP MUR program funded by the EU \u2013 NextGenerationEU.\n# REFERENCES\n[1] M.-J. Montpetit and N. Crespi, \u201cComputing in the Network: The Core-Edge Continuum in 6G Network,\u201d in Shaping Future 6G Networks: Needs, Impacts, and Technologies. Wiley, 2021. [2] Y. Alexeev, D. Bacon, K. R. Brown, R. Calderbank, L. D. Carr, F. T. Chong, B. DeMarco, D. Englund, E. Farhi, B. Fefferman, A. V. Gorshkov, A. Houck, J. Kim, S. Kimmel, M. Lange, S. Lloyd, M. D. Lukin, D. Maslov, P. Maunz, C. Monroe, J. Preskill, M. Roetteler, M. J. Savage, and J. Thompson, \u201cQuantum computer systems for scientific discovery,\u201d PRX Quantum, vol. 2, 2021. [3] C. A. Ardagna, R. Asal, E. Damiani, and Q. H. Vu, \u201cFrom Security to Assurance in the Cloud: A Survey,\u201d ACM CSUR, vol. 48, no. 1, 2015. [4] T. L. Duc, R. G. Leiva, P. Casari, and P.-O. \u00d6stberg, \u201cMachine Learning Methods for Reliable Resource Provisioning in EdgeCloud Computing: A Survey,\u201d ACM CSUR, vol. 52, no. 5, 2019. [5] E. Commission, \u201cProposal for a regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts,\u201d https://eur-lex.europa.eu/legal-content/ EN/TXT/?uri=CELEX:52021PC0206, 2021. [6] High-Level Expert Group on Artificial Intelligence, \u201cAssessment List for Trustworthy Artificial Intelligence (ALTAI) for selfassessment,\u201d European Commission, Tech. Rep., 2020. [7] M. Anisetti, C. A. Ardagna, and N. Bena, \u201cMulti-Dimensional Certification of Modern Distributed Systems,\u201d IEEE TSC, vol. 16, no. 3, 2023. [8] M. Anisetti, C. A. Ardagna, E. Damiani, and G. Polegri, \u201cTestBased Security Certification of Composite Services,\u201d ACM TWEB, vol. 13, no. 1, p. 3, 2019. [9] R. Faqeh, C. Fetzer, H. Hermanns, J. Hoffmann, M. Klauck, M. A. K\u00f6hl, M. Steinmetz, and C. Weidenbach, \u201cTowards Dynamic Dependable Systems Through Evidence-Based Continuous Certification,\u201d in Proc. of ISoLA 2020, Rhodes, Greece, October 2020. [10] E. Damiani and C. A. Ardagna, \u201cCertified Machine-Learning Models,\u201d in Proc. of SOFSEM 2020, Limassol, Cyprus, January 2020. [11] M. Anisetti, C. A. Ardagna, E. Damiani, and P. G. Panero, \u201cA Methodology for Non-Functional Property Evaluation of Machine Learning Models,\u201d in Proc. of MEDES 2020, Abu Dhabi, UAE, November 2020.\n[12] S. Sharma, J. Henderson, and J. Ghosh, \u201cCERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-Box Models,\u201d in Proc. of AAAI/ACM AIES 2020, New York, NY, USA, Feb. 2020. [13] G. Vidot, C. Gabreau, I. Ober, and I. Ober, \u201cCertification of embedded systems based on Machine Learning: A survey,\u201d arXiv preprint arXiv:2106.07221, 2021. [14] M. Anisetti, C. A. Ardagna, N. Bena, V. Giandomenico, and G. Gianini, \u201cLightweight Behavior-Based Malware Detection,\u201d in Proc. of MEDES 2023, Heraklion, Greece, May 2023. [15] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, \u201cMembership Inference Attacks Against Machine Learning Models,\u201d in Proc. of IEEE SP 2017, San Jose, CA, USA, May 2017. [16] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton, \u201cA Methodology for Formalizing Model-Inversion Attacks,\u201d in Proc. of IEEE CSF 2016, Lisbon, Portugal, 2016. [17] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, \u201cPoison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,\u201d in Proc. of NeurIPS 2018, Montr\u00e9al, Canada, December 2018. [18] N. Peri, N. Gupta, W. R. Huang, L. Fowl, C. Zhu, S. Feizi, T. Goldstein, and J. P. Dickerson, \u201cDeep k-NN Defense Against CleanLabel Data Poisoning Attacks,\u201d in Proc. of ECCV 2020, Glasgow, UK, August 2020. [19] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, \u201cAi2: Safety and robustness certification of neural networks with abstract interpretation,\u201d in Proc. of IEEE SP 2018, San Francisco, CA, USA, May 2018. [20] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi, \u201cA Survey of Methods for Explaining Black Box Models,\u201d ACM CSUR, vol. 51, no. 5, 2018.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c0bb/c0bbbcdb-07b2-49ba-9ced-c6d50eebf498.png\" style=\"width: 50%;\"></div>\nMarco Anisetti is Full Professor at the Universit\u00e0 degli Studi di Milano and co-founder of Moon Cloud srl. His research interests are in the area of computational intelligence and its application to the design and evaluation of complex systems. He received the Ph.D. degree in computer science from Universit\u00e0 degli Studi di Milano. Contact him at marco.anisetti@unimi.it.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8255/82557831-70ed-4d12-8119-e31bb86cd343.png\" style=\"width: 50%;\"></div>\nClaudio A. Ardagna is Full Professor at the Universit\u00e0 degli Studi di Milano, the Director of the CINI National Lab on Data Science, and co-founder of Moon Cloud srl. His research interests are in the area of cloud-edge security and assurance, and data science. He received the Ph.D. degree in computer science from Universit\u00e0 degli Studi di Milano. He is member of the Steering Committee of IEEE TCC, and secretary of the IEEE Technical Committee on Services Computing. Contact him at clau-\ndio.ardagna@unimi.it.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/668c/668c046d-7e00-49e1-b7ff-a3f3a35e5dea.png\" style=\"width: 50%;\"></div>\nNicola Bena is a Ph.D. student at the Universit\u00e0 degli Studi di Milano. His research interests are in the area of security of modern distributed systems with particular reference to certification, assurance, and risk management techniques. Contact him at nicola.bena@unimi.it.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c32e/c32ede1c-d0b2-446d-8558-8b50bef98a7b.png\" style=\"width: 50%;\"></div>\nErnesto Damiani is Full Professor at the Universit\u00e0 degli Studi di Milano and Founding Director of the Center for Cyber-Physical Systems, Khalifa University, UAE. His research interests include cybersecurity, big data, and cloud/edge processing. He received an Honorary Doctorate from Institute National des Sciences Appliqu\u00e9es de Lyon, France, in 2017. He is a Distinguished Scientist of ACM and was a recipient of the 2017 Stephen Yau Award. Contact him at ernesto.damiani@ku.ac.ae.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to address the pressing need for assurance solutions assessing non-functional properties of machine learning-based applications, particularly in terms of fairness, robustness, and privacy, by proposing a certification scheme tailored for such applications.",
            "scope": "The survey focuses on certification schemes for machine learning-based applications, particularly those that operate in dynamic, non-deterministic environments. It excludes traditional deterministic certification schemes as they are not suitable for the complexities introduced by machine learning."
        },
        "problem": {
            "definition": "The core issue explored is the inadequacy of existing certification schemes for assessing the trustworthiness and non-functional properties of machine learning-based applications, which exhibit non-deterministic behavior.",
            "key obstacle": "The primary challenges include defining appropriate targets for certification, establishing rigorous property definitions for machine learning, and developing evidence collection models that can effectively assess the behavior of such applications."
        },
        "architecture": {
            "perspective": "The survey introduces a three-factor certification model that includes the training data, the training process, and the machine learning model itself, emphasizing the need for multi-factor certification tailored to the unique characteristics of machine learning.",
            "fields/stages": "The survey organizes the current methods into three stages: 1) Certification of data used for training, 2) Certification of the training process, and 3) Certification of the operational machine learning model, each with specific properties and evidence collection models."
        },
        "conclusion": {
            "comparisions": "The survey compares existing certification schemes and highlights their shortcomings in the context of machine learning applications, emphasizing the need for new approaches that consider the unique non-functional properties of ML.",
            "results": "The overarching conclusion is that a structured certification scheme is necessary for machine learning applications, which must evolve alongside the models themselves to ensure ongoing trustworthiness and compliance with non-functional properties."
        },
        "discussion": {
            "advantage": "Existing research has made strides in defining non-functional properties and dynamic certification techniques, providing a foundation for developing tailored solutions for machine learning applications.",
            "limitation": "Current studies often lack rigorous definitions of ML-specific properties and fail to address the complexities of evidence collection in dynamic, non-deterministic environments.",
            "gaps": "There remain significant gaps in understanding how to effectively certify the behavior of ML models, particularly in terms of adapting traditional certification processes to accommodate the unique challenges posed by machine learning.",
            "future work": "Future research should focus on developing comprehensive certification frameworks that can adapt to evolving machine learning models, explore emerging trends in ML applications, and enhance the robustness of certification processes."
        },
        "other info": {
            "acknowledgments": "The work was partially supported by the MUSA project funded by the EU and the SERICS project under the NRRP MUR program."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The survey aims to address the pressing need for assurance solutions assessing non-functional properties of machine learning-based applications, particularly in terms of fairness, robustness, and privacy."
        },
        {
            "section number": "1.2",
            "key information": "The survey focuses on certification schemes for machine learning-based applications, particularly those that operate in dynamic, non-deterministic environments."
        },
        {
            "section number": "2",
            "key information": "The core issue explored is the inadequacy of existing certification schemes for assessing the trustworthiness and non-functional properties of machine learning-based applications."
        },
        {
            "section number": "2.1",
            "key information": "The primary challenges include defining appropriate targets for certification, establishing rigorous property definitions for machine learning, and developing evidence collection models."
        },
        {
            "section number": "3",
            "key information": "The survey introduces a three-factor certification model that includes the training data, the training process, and the machine learning model itself."
        },
        {
            "section number": "3.1",
            "key information": "The survey organizes the current methods into three stages: Certification of data used for training, Certification of the training process, and Certification of the operational machine learning model."
        },
        {
            "section number": "7.1",
            "key information": "Current studies often lack rigorous definitions of ML-specific properties and fail to address the complexities of evidence collection in dynamic, non-deterministic environments."
        },
        {
            "section number": "7.2",
            "key information": "Future research should focus on developing comprehensive certification frameworks that can adapt to evolving machine learning models."
        }
    ],
    "similarity_score": 0.6348051641082167,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Rethinking Certification for Trustworthy Machine Learning-Based Applications.json"
}