{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.00313",
    "title": "Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models",
    "abstract": "Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate how LLM embeddings and attention representations change following in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques such as representational similarity analysis (RSA) and propose novel methods for parameterized probing and measuring ratio of attention to relevant vs. irrelevant information in Llama-2 70B and Vicuna 13B. We designed two tasks with a priori relationships among their conditions: linear regression and reading comprehension. We formed hypotheses about expected similarities in task representations and measured hypothesis alignment of LLM representations before and after ICL as well as changes in attention. Our analyses revealed a meaningful correlation between improvements in behavior after ICL and changes in both embeddings and attention weights across LLM layers. This empirical framework empowers a nuanced understanding of how latent representations shape LLM behavior, offering valuable tools and insights for future research and practical applications.",
    "bib_name": "yousefi2024decodingincontextlearningneuroscienceinspired",
    "md_text": "# Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models\nSafoora Yousefi 1 Leo Betthauser 1 Hosein Hasanbeig 2 Rapha\u00a8el Milli`ere 3 Ida Momennejad 2\nAbstract\nLarge language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging taskspecific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate how LLM embeddings and attention representations change following in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques such as representational similarity analysis (RSA) and propose novel methods for parameterized probing and measuring ratio of attention to relevant vs. irrelevant information in Llama-2 70B and Vicuna 13B. We designed two tasks with a priori relationships among their conditions: linear regression and reading comprehension. We formed hypotheses about expected similarities in task representations and measured hypothesis alignment of LLM representations before and after ICL as well as changes in attention. Our analyses revealed a meaningful correlation between improvements in behavior after ICL and changes in both embeddings and attention weights across LLM layers. This empirical framework empowers a nuanced understanding of how latent representations shape LLM behavior, offering valuable tools and insights for future research and practical applications.\narXiv:2310.00313v4\n# 1. Introduction\nTransformer-based large language models (LLMs) such as GPT-3 (Brown et al., 2020) and Llama-2 (Touvron et al., 2023b) have achieved state-of-the-art performance on a wide range of tasks. One of the most intriguing aspects of modern Transformer-based models, especially LLMs, is their capacity for in-context learning (ICL) (Brown et al.,\n1Microsoft, Redmond, WA 2Microsoft Research, New York City, NY 3Macquarie University, Sydney, Australia. Correspondence to: Safoora Yousefi <sayouse@microsoft.com>, Ida Momennejad <idamo@microsoft.com>.\n2020). ICL enables the model to improve its performance on new tasks from a few examples provided in the input context (or prompt), without any parameter updates. ICL enables LLMs to flexibly adapt their behavior to taskspecific demands during inference without further training or fine-tuning. For instance, including examples of question-answer pairs in the prompt significantly improves performance on arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2022; Zhou et al., 2022). However, in spite of progress in this area, how ICL improves behavior remains mostly unknown and an active area of research.\never, in spite of progress in this area, how ICL improves behavior remains mostly unknown and an active area of research. Some prior studies have framed ICL as implicit optimization, providing theoretical and empirical evidence that Transformer self-attention can implement algorithms similar to gradient descent (Von Oswald et al., 2023; Aky\u00a8urek et al., 2022; Ahn et al., 2023). Other work has proposed a Bayesian perspective, suggesting pretraining learns a latent variable model that allows conditioning on in-context examples for downstream prediction (Xie et al., 2022; Wang et al., 2023; Ahuja et al., 2023). While these formal investigations offer valuable insights, they generally focus on toy models trained on synthetic data that may fail to capture the full richness of ICL behavior exhibited by large models trained on internet-scale data. To advance a more complete understanding of ICL capabilities, new perspectives are needed to elucidate how ICL arises in LLMs trained on naturalistic corpora. In this work, we introduce a neuroscience-inspired framework to empirically analyze ICL in two popular LLMs: Vicuna-1.3 13B (Chiang et al., 2023), and Llama-2 70B (Touvron et al., 2023a). We chose these models because they are open-source and provide access to embeddings and weights of all layers. We designed controlled experiments isolating diverse ICL facets including systematic generalization (e.g., in regression), and distraction resistance (e.g. in reading comprehension). To interpret how the LLMs\u2019 latent representations support ICL, we focus on changes in the LLMs\u2019 embeddings and attention weights following ICL. We adopt methods from neuroscience, namely representational similarity analysis (RSA), to interpret how model representations in various\nSome prior studies have framed ICL as implicit optimization, providing theoretical and empirical evidence that Transformer self-attention can implement algorithms similar to gradient descent (Von Oswald et al., 2023; Aky\u00a8urek et al., 2022; Ahn et al., 2023). Other work has proposed a Bayesian perspective, suggesting pretraining learns a latent variable model that allows conditioning on in-context examples for downstream prediction (Xie et al., 2022; Wang et al., 2023; Ahuja et al., 2023). While these formal investigations offer valuable insights, they generally focus on toy models trained on synthetic data that may fail to capture the full richness of ICL behavior exhibited by large models trained on internet-scale data. To advance a more complete understanding of ICL capabilities, new perspectives are needed to elucidate how ICL arises in LLMs trained on naturalistic corpora.\nIn this work, we introduce a neuroscience-inspired framework to empirically analyze ICL in two popular LLMs: Vicuna-1.3 13B (Chiang et al., 2023), and Llama-2 70B (Touvron et al., 2023a). We chose these models because they are open-source and provide access to embeddings and weights of all layers. We designed controlled experiments isolating diverse ICL facets including systematic generalization (e.g., in regression), and distraction resistance (e.g. in reading comprehension).\nTo interpret how the LLMs\u2019 latent representations support ICL, we focus on changes in the LLMs\u2019 embeddings and attention weights following ICL. We adopt methods from neuroscience, namely representational similarity analysis (RSA), to interpret how model representations in various\nhidden layers change as a result of ICL. We also propose Attention Ratio Analysis (ARA), a novel parameter-free method for computing the attention ratios between relevant and irrelevant information. Together, analyzing ICLinduced RSA and attention ratios enable us to systematically relate observed changes in latent representations and attention patterns to improvements in model behavior after ICL across three experiments. Our approach makes the following main contributions toward understanding the representational underpinnings of ICL.\n\u2022 We measured LLM embeddings for various prompts in the same task, and created hypothesis matrices about their expected similarity structure. We then measured the alignment of embedding similarities with the hypothesis using RSA. We found that ICL leads to changes in the prompt embedding similarity matrix to better encode task-critical information, which in turn improves behavior. We also decoded task-critical information prompt information from embeddings before and after ICL (using logistic regression classifiers) and showed that individual prompt embeddings also change after ICL to better encode this information. \u2022 We introduce attention ratio analysis (ARA) to measure changes in the attention weights after ICL. Specifically, we measure the ratio of attention between the models\u2019 response and the informative part of the prompt. We observed that increased allocation of attention to relevant content in the prompt correlates with behavioral improvements resulting from ICL. \u2022 We performed RSA and ARA across different layers\n\u2022 We performed RSA and ARA across different layers of various depths in Llama-2 and Vicuna and observed consistent patterns across layers.\nA neuroscience-inspired approach. We use a technique known as representational similarity analysis (RSA), which is widely used across the neurosciences for comparison of neural representations and behavior. We believe RSA is suited to our approach for two reasons. First, parameterfree methods like RSA offer notable benefits over parameterized probing classifiers routinely used to decode internal model representations (Belinkov, 2022). This is because parameterized probes run a greater risk of fitting spurious correlations or memorizing properties of the probing dataset, rather than genuinely extracting information inherent in the representations (Belinkov, 2022). RSA avoids this risk, since it directly uses model activations. Second, unlike causal interventions used in mechanistic interpretability research (Nanda et al., 2023; Conmy et al., 2023), RSA also scales efficiently to very large language models. Note that there is a potential trade-off between accuracy and complexity in probing methods, e.g., parameterized probes have the potential to achieve higher accuracy\nby fitting more complex patterns. In this work, we combine parameterized and parameter-free methods to leverage their respective strengths. This work demonstrates the promise of neuroscienceinspired analysis in advancing interpretability and design of robust, capable LLMs.\n# 2. Methods\nIn what follows we briefly describe the tasks, and the different steps of the latent representation analysis. This includes obtaining embedding vectors, performing representational similarity analysis (RSA) and attention ratio analysis (ARA), and training embedding classifiers. Tasks. We design two categories of experimental tasks throughout the paper. First, we demonstrate RSA as a tool to decode LLMs\u2019 hidden states in a linear regression (Section 3) task where a set of points from a line are given to the LLM and it is asked to generate another point on the same line. Then, we propose ARA as a tool to evaluate attention weights in a reading comprehension task (Section 4), where the LLMs are given a short text including informative and distracting content, and are asked a question about the text.\n# 2.1. Obtaining Prompt Embeddings and Attention Weights\n# 2.1. Obtaining Prompt Embeddings and Attention\nThe first step to perform RSA on LLM representations is to compute an embedding vector for each prompt. To this end, after feeding the prompt to the LLM, we first extract the n d-dimensional embeddings for n prompt tokens from the desired layer of the LLM. Then, we aggregate the token embeddings to reduce them into one d-dimensional vector per prompt. We experimented with both max-pooling and mean-pooling and verified that our findings are robust to the choice of embedding aggregation method. Following Timkey & van Schijndel (2021), we then standardize the embeddings before computing pairwise cosine similarities to reduces the effect of rogue dimensions dominating the similarities. We use the resulting embeddings as described in Sections 2.2 and 2.3.\nFor attention ratio analysis, we require the attention matrix between the response and the prompt. To obtain this, we first feed the concatenation of prompt and model response back to the model, then extract attention matrices from all h attention heads in each desired layer. We then aggregate the attention weights from the h attention heads into a single matrix for each prompt-response pair. We tried both maxpooling and mean-pooling to verify that our findings are robust to the choice of aggregation method.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9618/9618ae19-be76-4a82-bbfe-19b7d55b1910.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b376/b37633e1-3d36-4693-a81c-250712abd7aa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9cf4/9cf49180-3669-4630-8b45-9fe91b928b91.png\" style=\"width: 50%;\"></div>\nFigure 1. Experimental tasks and analyses. (a) We designed a linear regression task, in which the LLM is provided with a set of x and y coordinates that fall on the same line, is given a final x and is asked to provide a y so all points fall on the same line. (b) We designed a reading comprehension task with prompts about individuals doing different activities and a question about the activity of one of the individuals. Crucially, the prompts included both informative and distracting subsequences. (c) We constructed a hypothesis matrix about the similarity of different prompts for a given task. Regression: hypothesis was based on the line\u2019s slope. Reading comprehension: three hypothesis matrices were constructed based on name, activity, and their combination corresponding to the correct response. We demonstrated the correlation of the alignment between the hypothesis and the embedding similarity matrices with LLM behavior. (d) We computed the ratio of attention to informative components of the prompt before and after ICL and its correlation with LLM behavior.\n# 2.2. Embedding Representational Similarity Analysis\nWe designed a set of prompts with common components, such that representing those components is crucial in solving the prompt tasks. We hypothesize that in order to complete the tasks correctly, the LLMs must map prompts with similar components to similar representations.\nTo verify this, we obtained embeddings for each prompt as described in Section 2.1, and calculated cosine similarities for all pairs of prompts (M). We compared the resulting embedding similarity matrix with an a priori hypothesis matrix (H) representing the common components among prompts, arriving at a metric that we call hypothesis alignment. Subsequently, we measured whether and how these representations and their hypothesis alignment change after we introduce ICL examples to the same prompts.\n# 2.3. Embedding Probes\nWhile RSA allows studying the similarity structure among prompt embeddings, probing allows more targeted investigation of whether task-relevant features are encoded in them. As the prompt embeddings capture the LLMs\u2019 latent representations of the tasks, it is possible to decode various components of the task from prompt embeddings (Section 2.1). We use a probing classifier to study the effect of in-context-learning on the decodability of task components of interest from the embeddings. To achieve this, we\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7597/7597ce56-5294-47c5-958b-89c8a05d2ba1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">D. Attention Ratio Analysis (ARA)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7320/732008fa-7c76-4be6-8141-d01e71adb714.png\" style=\"width: 50%;\"></div>\ndivide the prompts into training and testing sets, train a logistic regression model to predict a task component in the prompt, and measure the decoding accuracy on the test set. For example, in Section 3, each prompt involves a line, and the slope of the line is decoded from the prompt embeddings. We report classification accuracy with 10 repetitions of Monte Carlo cross validation.\n# 2.4. Attention Ratio Analysis (ARA)\nWe examine representational changes in attention weights in various layers of the LLMs due to ICL. Namely, we compute the ratio of the attention between the tokens associated with the response and the tokens associated with different parts of the prompts. Using ARA, we example whether and increased attention to informative parts of the prompt underlies ICL.\nIntuitively, ARA measures how much of the response\u2019s attention is focused on a substring of interest in the prompt. Let\u2019s take a, s, and t to denote three subsets of substrings contained in x which represents the prompt concatenated with the response. For example, a can be the prompt, s the response, and t can be a part of the prompt which contains the necessary information to answer the question. To enable measuring attention ratios, we construct an input string x = p \u2322r by concatenating the prompt p and the model response r, and obtain the attention weights resulting from\nthis input from the last layer of the model. Let A be the aggregated attention weights across all of the desired layer\u2019s attention heads corresponding to x. We define the attention ratio A(a, s, t)x := 1 |t(x,s)|\u03a3(Ai,j)/ 1 |t(x,t)|\u03a3(Ai,k) where i \u2208t(x, a), j \u2208t(x, s), k \u2208t(x, t), and t(u, v) indicates the token indices corresponding to substring v after tokenization of string u. The attention ratio measure can be used to compare the attention of the response to relevant vs. irrelevant information in the prompt. We compared the distribution of this ratio before and after ICL in sections 4 across various Layers of Llama-2 and Vicuna. Two-sample t-test was used to measure the statistical significance of the shift in attention ratios before and after ICL.\n# 3. Linear Regression\nWe first apply RSA to LLMs in the task of linear regression where we perform an in-depth study of the effect of number of in-context examples on LLMs\u2019 representation of lines. We took inspiration from (Coda-Forno et al., 2023) in designing this task. We created 256 prompts, and 16 different lines. In each prompt, we provided two to eight (xi, yi) points for in-context learning and a test xT coordinate in the prompt, asking the LLM to predict the associated yT coordinate on the same line. The minimum number of points to determine a line is two, meaning we provide 0 to 6 additional examples for ICL. A sample prompt for this task with 1 additional example point is shown below:\nHere are a set of point coordinates that all fall on the same line: (0.86,22.2); (0.44,13.8); (0.63,17.6); (0.49,\nHere are a set of point coordinates that all fall on the same line: (0.86,22.2); (0.44,13.8); (0.63,17.6); (0.49,\nWe evaluated the models\u2019 behavioral error in calculating yT for xT as the absolute error between the response \u02c6 yT and the ground truth:\n(1)\nFigure 3(a) shows that the behavioral error of both models decreases as we increase the number of ICL examples. We studied representational changes in various layers of both LLMs that correlate with this behavioral change.\n# 3.1. Linear Regression RSA.\nOne underlying factor that could facilitate this behavioral improvement is better representation of the line\u2019s slope in the models\u2019 hidden states. Specifically, we hypothesize that prompts describing lines with the same slope should be encoded to similar embeddings in the models\u2019 representation space compared to those with different slopes. We built a similarity matrix H to capture this hypothesis (Figure 2(a)), where the entry Hi,j is 1 if the lines in the ith and\njth prompt have the same slope and 0 otherwise.\nNext, we calculated the cosine similarity matrix among embeddings of prompts with k ICL examples and we denoted it as Mk (Figures 2(b) and 2(c)). Finally, we computed hypothesis alignment of the embeddings of prompts with varying numbers of ICL examples as the correlation between H and each Mk.\nAs shown in Figure 3(d), hypothesis alignment increased as we increased the number of ICL example points in the prompt for both models. Embeddings from the middle layer of both models were used in Figure 3(d), while we repeated this analysis using embeddings from five different layers of various depths and observed a similar increase in hypothesis alignment with all other layers except the first layer (Figures 3(e) and 3(f)). We repeated this experiment using mean-pooling over prompt token embeddings instead of max pooling, and although correlations were generally lower, we consistently observed improvement in hypothesis alignment with more ICL examples (Appendix Fig. 8).\n# 3.2. Classifying Line Information from Embeddings.\nThe RSA results suggested that increasing the number of ICL examples increases the amount of slope information in the embedding similarity matrix. We then used probing classifiers to investigate if the same information can be decoded from prompt embedding directly. We trained a logistic regression classifier to predict the slope of the line (8 total slopes) from the prompt embeddings of five different layers of Llama-2 and Vicuna-1.3.\nIncreasing the number of in-context examples improved the classification accuracy of the slope classifiers in all layers except the first layer. (See Figure 3(b) for the last layer and Appendix Figure 7 for all layers.) This suggests that with more examples, LLMs were able to represent the line slopes more accurately. Importantly, behavioral improvement in both LLMs was correlated with the accuracy of this probing classifier. In other words, as the slope information embedded in the model\u2019s latent representations increased, the models had smaller behavioral error in predicting yT for xT . (Figure 3(c)). We observed similar results using mean-pooling over token embeddings instead of max-pooling (Appendix Figure 7(c)).\n# 4. Reading comprehension: Names and Activities\nWe designed a simple reading comprehension task that consists of clear and distinct components with a priori similarities, such that we can measure how each component is reflected in LLM embeddings and attention weights. Simple prompts. Specifically, we created 100 prompts of\nSimple prompts. Specifically, we created 100 prompts of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/747b/747b36c3-460d-4b40-a336-95db0a45a144.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/60d7/60d723bf-1157-409b-99fb-23d3d00dd8a9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Slope-based Hypothesis H</div>\n<div style=\"text-align: center;\">(b) Embedding similarity M0</div>\nFigure 2. Embedding similarity (M) and hypothesis (H) matrices for the regression task. (a) We constructed a hypothesis similarity matrix assuming prompts about lines that have the same slope would have similar embeddings. (b) and (c) We computed the actua prompt-to-prompt embedding similarity matrix (for a given layer, e.g., the last layer of Llama-2 here) for prompts with no ICL (M0) and after the addition of ICL examples (Mk). Each row and column represent the embedding of a regression task prompt. We then computed the alignment of the H and M similarity matrices before and after ICL for multiple layers of each model (Figure 3).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/835e/835e1f8a-9f45-4ca6-aef7-23141f2c7021.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b8ca/b8caa104-7a27-4435-88fe-a27f4e40bc61.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Embedding Probe</div>\n<div style=\"text-align: center;\">(d) Correlation(H, M)</div>\n<div style=\"text-align: center;\">(e) Vicuna H-M alignment across layers (f) Llama2 H-M alignment across </div>\nFigure 3. Behavior, hypothesis alignment, and embedding probes for the regression task before and after ICL. (a) Increasing the number of ICL examples decreased the absolute error between model\u2019s response \u02c6 yT and the ground truth yT (see equation 1) for Llama2 and Vicuna-1.3. (b) A logistic regression probing classifier was trained to predict the line slope of regression prompts from the last layer\u2019s embedding. Decoding accuracy increased with ICL in both Llama2 and Vicuna. (c) Behavior improvement in both models is correlated with the accuracy of the embedding classifier. The more slope information embedded in the model\u2019s representations, the smaller the model\u2019s mean absolute error in predicting yT . (d) The correlation between our slope-based hypothesis matrix and the embeddings similarity matrix increases with more ICL examples for both models (visualised for middle layer). (e) and (f) Hypothesis alignment improved consistently with more ICL examples across LLM layers of varying depths with the exception of the first layer.\nthe form \u201cname + activity\u201d, using 10 distinct names and 10 distinct activities. We refer to these prompts as \u201csimple prompts\u201d since we subsequently combine them to create more complex \u201ccomposite prompts\u201d. Here is an example\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/29f7/29f73304-12e1-4604-baf8-1534b4bbd343.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Embedding similarity M6</div>\n<div style=\"text-align: center;\">(c) Probe accuracy vs. behavior</div>\nPatricia is reading a book.\nComposite prompts. Next, we created composite prompts using two simple prompts. These composite prompts involve a simple reading comprehension task that requires the model to process information from the relevant simple prompt while ignoring the irrelevant one to come up with the correct answer. Here is an example composite prompt created with two simple prompts:\nQuestion: Patricia is reading a book. Joseph is swimming. Oliver is doing the same thing as Patricia. Oliver is a neighbor of Joseph. What is Oliver doing? Answer:\nBehavioral change after ICL: The correct response to the above prompt needs to include \u201cOliver is reading a book.\u201d or \u201cReading a book.\u201d Note that the prompt includes distracting statements that are not one of our simple prompts, e.g., \u201cOliver is a neighbor of Joseph\u201d, to make the task more challenging. Our goal was to study how ICL can improve LLMs\u2019 performance despite of distractors. We found that different distractors pose a challenge to Llama-2 and Vicuna-1.3: We provide examples of prompts used for each model in the supplementary materials (Section B.1). We created another 100 simple prompts with a different set of names and activities and made composite prompts to use as ICL examples. Llama-2 and Vicuna-1.3 performances on this task are reported before and after in-context examples were introduced (Figure 4(a)). We observe that adding an ICL example significantly improves the performance of both models. In the following subsections, we use these composite prompts to analyze representational and attention changes underlying this behavior improvement.\n# 4.1. Embedding similarity and hypothesis alignment.\nWe constructed a prompt-to-prompt embedding similarity matrix using cosine similarity of prompt embeddings obtained from the LLMs. (See section 2.1 for details).\nHypothesis alignment. We formed three hypothesis matrices about what the similarity structure should look like between prompt embeddings, and measured how close to our hypotheses the actual similarity matrices are.\n Activity-based hypothesis: We hypothesize if the correct answer to two prompts involves the same activity, then those prompts should have more similar embeddings compared to pairs that do not satisfy this condition. We construct the hypothesis matrix Hact so that Hact[i, j] = 1 if prompts i and j have the same activity in their correct answer, and 0 otherwise.\n\u2022 Name-based hypothesis: We hypothesize if two prompts inquire about the same person, then those prompts should have more similar embeddings compared to pairs that do not satisfy this condition. We construct the hypothesis matrix Hname so that Hname[i, j] = 1 if prompts i and j inquire about the same person, and 0 otherwise. \u2022 Combined hypothesis: We combine the above two hypotheses and expect prompt pairs that satisfy both conditions to be most similar (Hcomb[i, j] = 1), those that satisfy only one condition to be less similar (Hcomb[i, j] = 0.5), and those that satisfy none to be the least similar pairs (Hcomb[i, j] = 0). We computed the correlation between each hypothesis matrix and the prompt-to-prompt similarity matrix, and observed that when we add an ICL example to the prompt, the hypothesis alignment increases significantly for all hypotheses. This effect is statistically significant for all hypotheses in Vicuna, and for Hact and Hcomb hypotheses in Llama-2 (Figures 4(b) and 4(c)). We measured alignment with Hcomb for five layers of various depths in both models, and observed that the intermediate layers in Vicuna show consistent significant increase in hypothesis alignment after ICL (Figure 6(c)), while in Llama-2, only the 60th layer shows statistically significant increase. We used the Fisher z-transformation to compare correlations before and after ICL and used p < 0.01 as the significance threshold.\nWe measured alignment with Hcomb for five layers of various depths in both models, and observed that the intermediate layers in Vicuna show consistent significant increase in hypothesis alignment after ICL (Figure 6(c)), while in Llama-2, only the 60th layer shows statistically significant increase. We used the Fisher z-transformation to compare correlations before and after ICL and used p < 0.01 as the significance threshold.\n# 4.2. Attention Ratio Analysis of Composite Prompts.\nNext we applied the attention ratio analysis described in Section 2.4 to composite prompts. Each composite prompt consists of well-defined informative (sinf) and distracting simple prompts. For the example prompt above, \u201cJoseph is swimming.\u201d is a distracting simple prompt, while \u201cPatricia is reading a book\u201d is an informative one from which the correct answer can be inferred. We analyzed the attention of LLMs\u2019 response r on sinf to verify if an increased attention to the informative part of the prompt is underlying the behavior improvement after ICL. The simple prompts in composite prompts were shuffled before presenting the prompt to the models, so that the informative simple prompt does not always come first. For each composite prompt, we calculated the ratio of the response r attention to sinf over response attention to the the whole prompt p as A(r, sinf, p). In Figures 5(a) and 5(b), we compare the distribution of this value over composite prompts before and after introduction of ICL in both models. The addition of one ICL example significantly shifts attention to the informative parts of the prompt (i.e., larger values of attention ratio). We consistently observe this ICL-induced improvement of the attention ratio distribution across both LLMs, in five different layers of various\nlengths (See figures 6(b) and 6(a)). Interestingly, the middle layers in both models show this effect most strongly.\nlengths (See figures 6(b) and 6(a)). Interestingly, the middle layers in both models show this effect most strongly. Importantly, the attention ratio is significantly correlated with the behavior of both LLMs (Figures 5(c) and 5(d)). Using mean-aggregation over attention heads instead of max-aggregation, and observed the same increase in attention ratios after ICL (See Appendix Figure 9).\nImportantly, the attention ratio is significantly correlated with the behavior of both LLMs (Figures 5(c) and 5(d)). Using mean-aggregation over attention heads instead of max-aggregation, and observed the same increase in attention ratios after ICL (See Appendix Figure 9).\n# 5. Discussion and future directions\nWe investigated how ICL improves LLM behavior, studying how it impacts embeddings and attention weights across layers. We designed a linear regression task and a reading comprehension task and tested two open source LLMs: Vicuna-1.3 (13B) and Llama-2 (70B). We analyzed changes in latent representations of these LLMs before and after ICL, measuring representational similarity among embeddings, hypothesis alignment, and attention ratios as well as their correlation with behavior. We found that ICL improves task-critical representations and attention allocation to relevant content in the prompt. These representational changes were correlated with behavioral gains.\n# 6. Related Work\nOur use of RSA builds on prior work applying this technique to study neural representations in brains, neural networks, and NLP models. The latter includes examining relationships between sentence encoders and human processing (Abdou et al., 2019), correlating neural and symbolic linguistic structures (Chrupa\u0142a & Alishahi, 2019), analyzing biases in word embeddings (Lepori, 2020), comparing word embedding and fMRI data (Fereidooni et al., 2020), investigating encoding of linguistic dependencies (Lepori & McCoy, 2020), and assessing semantic grounding in code models (Naik et al., 2022). Uniquely, we apply RSA at scale to study ICL in large pretrained models like Llama-70B. Our work shows RSA can provide insights into task-directed representation changes during ICL. Concurrent lines of work also aim to elucidate ICL mechanisms. One hypothesis is that Transformers implement optimization implicitly through self-attention. For example, research shows linear self-attention can mimic gradient descent for regression tasks (Von Oswald et al., 2023). Other studies also suggest Transformers can implement gradient descent and closed-form solutions given constraints (Aky\u00a8urek et al., 2022), and specifically mimic preconditioned gradient descent and advanced techniques like Newton\u2019s method (Ahn et al., 2023). However, to our knowledge ours is the first study to use RSA at scale, studying ICL in large language models trained on naturalistic data rather than toy models.\nThis is important since insights from formal studies analyzing small toy models may not transfer to large pretrained models. A key advantage of our approach is the focus on larger LLMs like Llama-2: by scaling RSA and attention analysis our approach offers explanatory insights into real-world ICL capabilities. Our results show ICL improves embedding similarity according to experimental design (i.e., embeddings for tasks with cognitive similarity become more similar to each other), and shifts attention to relevant information, which also increases robustness to distractors. This aligns with the view that ICL relies on implicit optimization within the forward pass (Aky\u00a8urek et al., 2022; Ahn et al., 2023). Moreover, the changes we observe in representations and attention after more ICL examples imply the model optimizes its processing of prompts in context. Relatedly, some studies model ICL through a Bayesian lens, viewing pretraining as learning a latent variable model for conditioning (Xie et al., 2022; Wang et al., 2023; Ahuja et al., 2023; Wies et al., 2023). We empirically demonstrate that prompt embeddings become more task-aligned and attention more focused on critical task information. These observable changes could provide some additional support for the view that LLMs are effectively conditioning on salient factors implicit in prompts. In this sense, our results provide complementary real-world empirical evidence at the level of representations to supplement the theoretical insights from probabilistic perspectives. The emerging field of mechanistic interpretability aims to reverse engineer model computations, drawing analogies to software decompiling. The goal is to recover human-interpretable model descriptions in terms of learned \u201ccircuits\u201d implementing meaningful computations. For instance, recent work presents evidence that \u201cinduction heads\u201d are a key mechanism enabling ICL in Transformers, especially in small models (Olsson et al., 2022). While mechanistic interpretability is promising for validating causal claims, it remains challenging to scale up. Automating circuit discovery is an active area (Conmy et al., 2023), but not yet viable for models with tens of billions of parameters. Our approach provides complementary evidence by showing how relevant information becomes encoded in embeddings and attention. While we do not isolate causal circuits, we demonstrate the behavioral effect of improved task representations. Thus, we believe our proposed application of RSA and attention ratios could help evaluate proposals from mechanistic research in the future. LLMs have been shown to fail at multi-step planning and reasoning (Momennejad et al., 2023; Hasanbeig et al., 2023). A future direction is to study the effects of ICL on improving planning behavior in LLMs. Specifically, analyzing the latent representations of the different layers be-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6d3/d6d34dde-33e8-4b4e-8f0e-e4d09ada6d97.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/85ef/85ef3363-f842-49cc-a935-aaac49c26652.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Vicuna Hypothesis Alignment</div>\n<div style=\"text-align: center;\">(a) Behavioral Accuracy</div>\nFigure 4. Model behavior and hypothesis alignment before and after ICL for the reading comprehension task. (a): The accuracy of Llama-2 and Vicuna-1.3 behavior in the reading comprehension task significantly benefits from ICL examples. (b) ICL significantly improves the alignment between Vicuna\u2019s embedding similarity matrix and three hypothesis matrices, p < 0.05: name-based similarity (prompts inquiring about the same individual are more similar), activity-based similarity (prompts whose correct answer includes the same activity are more similar), and the combined name and activity similarity. (c) ICL significantly increases hypothesis alignment of Llama2 embeddings for activities and combined hypothesis matrices, p < 0.05.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f92c/f92c73c3-8508-46a1-bd73-b6be815ec4f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4cbe/4cbe4e40-e721-4341-becb-7943cd13c7d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c24/0c24af4b-3005-4a22-8a43-04a44e9a5c7c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Vicuna ARA vs behavior (d) Llama-2 ARA vs behavior</div>\n<div style=\"text-align: center;\">(b) Vicuna ARA</div>\n<div style=\"text-align: center;\">(a) Llama-2 ARA</div>\nFigure 5. Attention Ratio Analysis (ARA) before and after ICL for the reading comprehension task.. (a) and (b) The ratio of ttention to informative and uninformative information were measured for the reading comprehension task before and after ICL (blue nd orange respectively) for Llama-2 70B and Vicuna-1.3 13B. Attention ratio distributions concentrated toward larger numbers indicate more attention to the informative part of the prompt. Attention ratios corresponding to the middle layer of both models significantly shift o larger values with the introduction of ICL, indicating more attention to informative information after ICL. (c) and (d) Attention ratios x axis) in both models are significant indicators of each model\u2019s behavioral accuracy (y axis).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f91c/f91c285c-202a-4772-a502-346c9a3e6b44.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ad2/1ad2de05-9dd1-4d3a-8f51-dd1a193537ae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Vicuna ARA</div>\n<div style=\"text-align: center;\">(a) Llama-2 ARA</div>\nFigure 6. Hypothesis alignment across LLM layers before and after ICL. We measured the ratio of attention to informative vs. uninformative information, and hypothesis alignment of embeddings, across five layers (first, middle, last and quartiles) for each model.5(a) All layers of Llama2, with the exception of the last layer, demonstrate a statistically significant increase in ratio of attention to informative parts of the prompt after ICL. 5(b) All five Vicuna layers demonstrate the same pattern significantly. * indicates statistically significant increase (p < 0.001). 4(b) Vicuna\u2019s embeddings demonstrate a significantly higher alignment with the combined hypothesis Hcomb in the middle and deeper layers. 4(c) Llama\u2019s embeddings show this behavior across various layers although this effect is only statistically significant in layer 60. * indicates statistical significance with p < 0.01.\nfore and after ICL, and measuring the correlation between changes in representations and improvements in planning behavior on Markov decision processes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8e84/8e8433f3-55ad-44aa-b146-125569cd0bf9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Llama-2 Hypothesis Alignment</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/61bb/61bbfb38-d70b-4d61-8ce4-54eb6d098fce.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/23b3/23b3e122-07e6-429a-8b5f-92466f19b5ec.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Llama-2 alignment</div>\n<div style=\"text-align: center;\">(c) Vicuna alignment</div>\nIn sum, we show that ICL improves LLM behavior by better aligning its embedding representations and attention weights with task-relevant information. In future work, we\nintend to apply the method to better understand how LLMs work, and implement the methods offered here as a whitebox augmentation of LLMs.\n# 7. Impact Statement\nGiven that we have access to embeddings and attention weights of open-source LLMs, neuroscience-inspired methods can offer insights into the inner workings of LLMs in relation to expected functions, with or without ICL or chain of thought methods. This paper presents work inspired by neuroscience methods with the goal of advancing the field of machine learning.\nGiven that we have access to embeddings and attention weights of open-source LLMs, neuroscience-inspired methods can offer insights into the inner workings of LLMs in relation to expected functions, with or without ICL or chain of thought methods. This paper presents work inspired by neuroscience methods with the goal of advancing the field of machine learning. A cornerstone of our approach is the demonstration that interpretability methods inspired by neuroscience, such as RSA, can be effectively scaled to LLMs with tens of billions of parameters. As a result, such methods can be deployed on state-of-the-art LLMs, moving beyond the constraints of smaller toy models often used in interpretability research. Our work, therefore, bridges the gap between theoretical research and practical application, offering a path to scrutinize and understand the complex behaviors of production-level LLMs. The capacity to analyze and predict LLM behavior also offers potential societal benefits, such as protecting against misuse. For example, gaining a better understanding of the representational shift underpinning ICL is a meaningful step towards detecting and preventing prompt injection attacks. Finally, our approach demonstrates the value of interdisciplinary research, bridging insights from neuroscience and machine learning. This cross-pollination not only enriches our understanding of artificial systems but may also offer back novel insights for the study of neural representations in the brain. We encourage further exploration along these interdisciplinary lines to foster mutual advancements in AI and computational neuroscience.\nA cornerstone of our approach is the demonstration that interpretability methods inspired by neuroscience, such as RSA, can be effectively scaled to LLMs with tens of billions of parameters. As a result, such methods can be deployed on state-of-the-art LLMs, moving beyond the constraints of smaller toy models often used in interpretability research. Our work, therefore, bridges the gap between theoretical research and practical application, offering a path to scrutinize and understand the complex behaviors of production-level LLMs.\n# References\nMostafa Abdou, Artur Kulmizev, Felix Hill, Daniel M. Low, and Anders S\u00f8gaard. Higher-order comparisons of sentence encoder representations. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5838\u20135845, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1593. URL https: //aclanthology.org/D19-1593.\nSuvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=LziniAXEI9.\nSuvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=LziniAXEI9. Kabir Ahuja, Madhur Panwar, and Navin Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891, 2023. Ekin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is incontext learning? Investigations with linear models. In The Eleventh International Conference on Learning Representations, September 2022. Yonatan Belinkov. Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1):207\u2013219, April 2022. ISSN 0891-2017. doi: 10.1162/coli a 00422. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/202303-30-vicuna/. Grzegorz Chrupa\u0142a and Afra Alishahi. Correlating Neural and Symbolic Representations of Language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2952\u20132962, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1283. Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric Schulz. Metain-context learning in large language models. arXiv preprint arXiv:2305.12907, 2023. Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=89ia77nZ8u. Sam Fereidooni, Viola Mocz, Dragomir Radev, and Marvin Chun. Understanding and improving word embeddings\nKabir Ahuja, Madhur Panwar, and Navin Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891, 2023.\nYonatan Belinkov. Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1):207\u2013219, April 2022. ISSN 0891-2017. doi: 10.1162/coli a 00422.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/202303-30-vicuna/.\nJulian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric Schulz. Metain-context learning in large language models. arXiv preprint arXiv:2305.12907, 2023.\nthrough a neuroscientific lens. bioRxiv, pp. 2020\u201309, 2020.\nthrough a neuroscientific lens. bioRxiv, pp. 2020\u201309,\nHosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, and Ida Momennejad. ALLURE: Auditing and improving LLM-based evaluation of text using iterative in-context-learning. arXiv preprint arXiv:2309.13701, 2023.\nMichael Lepori. Unequal representations: Analyzing intersectional biases in word embeddings using representational similarity analysis. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 1720\u20131728, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main. 151. URL https://aclanthology.org/2020. coling-main.151.\nMichael Lepori and R. Thomas McCoy. Picking BERT\u2019s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 3637\u20133651, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.325.\nIda Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson. Evaluating cognitive maps and planning in large language models with cogeval. arXiv preprint arXiv:2309.15129, 2023.\nShounak Naik, Rajaswa Patil, Swati Agarwal, and Veeky Baths. Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis. In Weitong Chen, Lina Yao, Taotao Cai, Shirui Pan, Tao Shen, and Xue Li (eds.), Advanced Data Mining and Applications, Lecture Notes in Computer Science, pp. 395\u2013 406, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-22137-8. doi: 10.1007/978-3-031-221378 29.\nNeel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei,\nTom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. CoRR, abs/2109.04404, 2021. URL https://arxiv.org/ abs/2109.04404. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nTom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288), July 2023b. doi: 10.48550/arXiv.2307.09288.\nohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 35151\u201335174. PMLR, 23\u2013 29 Jul 2023. URL https://proceedings.mlr. press/v202/von-oswald23a.html.\nXinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. In\nWorkshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview. net/forum?id=HCkI1b6ksc.\nWorkshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview. net/forum?id=HCkI1b6ksc. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Advances in Neural Information Processing Systems, 35:24824\u201324837, December 2022. Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=f3JNQd7CHM. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https:// openreview.net/forum?id=RdJVFCHjUMI. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.\ndoing?\nWith ICL\n# A. Linear Regression\nA.1. Decodability Results\nIn Section 3, we studied the ability of probing classifiers to decode the line slope from the prompt embeddings and showed that the accuracy of these classifiers increases as we increase the number of ICL examples (see Figure 3(b)). Here, we extend that experiment to several layers of various depths in both models. We observed a consistent pattern of improvement in probe accuracy in all layers except the first one. See Figure 7.\nAdditionally, we repeated this experiment with prompt embeddings obtained by mean-pooling over token embeddings instead of max-pooling and observed similar results indicating more task-critical information (the line\u2019s slope) was encoded in the LLMs\u2019 latent representations after ICL examples were added to the prompts. See Figure 7(c).\n# A.2. RSA: Hypothesis Alignment\nWe measured alignment of the linear regression prompts\u2019 embedding similarity matrix with the slope-based hypothesis matrix using mean-pooling over prompt token embeddings instead of max-pooling. We still observed increased alignment as we increased the number of ICL examples in the prompts in both Vicuna1.3 13B (a) and Llama-2 70B (b), although with mean-pooling the hypothesis alignment was generally lower for Vicuna. See Figure 8.\n# B. Reading Comprehension: Names and Activities\nB.1. Prompt Samples\nHere we show examples of composite prompts used in Sec 4 for the reading comprehension task. Our goal was to study the underlying changes in latent states and attention patterns that lead to performance improvement in face of distractors, so it was important to find tasks that were challenging for the models without ICL, and that could be made easier with ICL. We found that different distractors pose a challenge to Llama-2 and Vicuna-1.3 where ICL can help:\nB.1.1. LLAMA-2 Sample prompts for Llama-2 with and without in-context examples: Without ICL\nOliver is baking a cake. Joseph is playing basketball. Ileana is sleeping. Natalie is doing the same thing as Ileana. Natalie carpools to work with Joseph. Natalie enjoys playing basketball with Joseph. What is Natalie\nWith ICL\nQuestion: Bob is playing the piano. Xavier is playing basketball. Tina is doing the same thing as Bob. Tina is a neighbor of Xavier. Tina enjoys rock climbing with Xavier. What is Tina doing? Answer: Tina is playing the piano. Question: Ileana is riding a bike. Harriet is watching TV. Joseph is doing the same thing as Harriet. Joseph is best friends with Ileana. Joseph enjoys riding a bike with Ileana. What is Joseph doing? Answer:\n# B.1.2. VICUNA-1.3\nSample prompts we used for Vicuna-1.3 with and without in-context examples:\nSample prompts we used for Vicuna-1.3 with and without in-context examples: Without ICL\nPatricia is baking a cake. Harriet is singing a song. Natalie is doing the same thing as Harriet. Natalie is a coworker of Patricia. What is Natalie doing?\n# With ICL\nQuestion: Alice is cooking a meal. Ursula is singing a song. Wendy is doing the same thing as Alice. Wendy carpools to work with Ursula. What is Wendy doing? Answer: Wendy is cooking a meal. Question: Larry is swimming. Mark is riding a bike. Natalie is doing the same thing as Larry. Natalie is a neighbor of Mark. What is Natalie doing? Answer:\n# B.2. Pairwise similarity matrix\nIn this section we provide embedding visualization and classification for the prompts introduced in Section 4. The pairwise similarity matrix of all simple prompt embeddings extracted from Llama-2 reveals high similarity between simple prompts involving the same name (bright blocks) and the same activity (bright diagonal lines). See Figure 10(a). Furthermore, t-SNE visualization of the same embeddings reveals that embeddings of prompts describing the same activity are clustered together. Each point corresponds to a simple prompt and is colored by the activity mentioned in the prompt. Most clusters in the t-SNE visualization homogeneously represent an activity (same color), one exception to this is the cluster on the left that corresponds to a specific name (\u201cIleana\u201d).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1a29/1a292e1b-ab8f-426d-9a43-1d068b521947.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Llama2 probe using max-aggregation (b) Vicuna probe using max-aggregation (c) Vicuna probe using mean-aggregation</div>\nFigure 7. Accuracy of probing classifiers trained on (a) max-aggregated Llama2 embeddings, (b) max-aggregated Vicuna embedding (c) and mean-aggregated Vicuna embeddings to predict the line slope in the linear regression task, using prompt embeddings taken from various hidden layers. There is a consistent improvement to the probes\u2019 performance as we add more ICL example points to the prompt This pattern is consistent across models, layers, and embedding aggregation methods.\nFigure 7. Accuracy of probing classifiers trained on (a) max-aggregated Llama2 embeddings, (b) max-aggregated Vicuna embeddings, (c) and mean-aggregated Vicuna embeddings to predict the line slope in the linear regression task, using prompt embeddings taken from various hidden layers. There is a consistent improvement to the probes\u2019 performance as we add more ICL example points to the prompts. This pattern is consistent across models, layers, and embedding aggregation methods.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9cad/9cadeb32-e72b-4050-818a-46df29b0f05e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Vicuna RSA with mean-aggregation in Linear Regression (b) Llama-2 RSA with mean-aggregation in Linear Regression</div>\nFigure 8. We measured alignment of the the linear regression prompts\u2019 embedding similarity matrix with the slope-based hypothesis matrix using mean-pooling over prompt token embeddings instead of max-pooling. Although with mean-pooling the hypothesis alignment was generally lower, we still observed increased alignment as we increased the number of ICL examples in the prompts in both Vicuna1.3 13B (a) and Llama-2 70B (b).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/557b/557b9dea-eaca-4c1b-9551-c02a4e865483.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68a4/68a4e595-6f3e-4750-b233-84aa857e8593.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Vicuna - ARA - mean aggregation</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8214/821470f2-88e9-4da3-b6e9-81a8739b5df1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/63f7/63f73555-b217-45d4-9d9e-aca3199cf949.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Attention Ratio (e) Llama2 - ARA - mean aggregation</div>\n<div style=\"text-align: center;\">0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Attention Ratio (e) Llama2 - ARA - mean aggregation</div>\nFigure 9. Attention Ratio Analysis (ARA) using mean aggregated attention weights over all attention heads. In the paper, we report ARA results using attention weights that were max-aggregated over attention heads. Here we verify if the results still hold using a different aggregation method. (a)and(d) The attention ratios significantly (p < 0.01) increase in all layers of both models except the last layer. (b)and(e) Attention ratios calculated from mean-aggregated attention weights are significantly correlated with the correctness of model response. (c)and(f) We show the distribution of attention ratios calculated from mean-aggregated attention weights of the middle layer of both models. A statistically significant to larger rations indicates more attention paid to the relevant content of the prompt. (See Section 2.4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/517a/517a4d4d-842a-4607-8615-f4e71f202c98.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Vicuna - ARA - mean aggregation</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/61a6/61a62525-d04a-4c33-9514-5e108e99e9a3.png\" style=\"width: 50%;\"></div>\nIn order to validate the probing classifier approach, we trained a logistic regression classifier to classify the ground truth activity from the embeddings of composite prompts that include one, two, or three simple prompts. See Figure 10(c). The task of predicting the ground truth activity from prompt embeddings becomes harder as we add more simple prompts to the composite prompt, which aligns with deteriorating behavioral accuracy (See Figure 10(c)).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d1b/4d1bc112-4f05-4e9e-97bf-b1580621379e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Simple prompt embeddings pairwise similarity matrix</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4f9/b4f956d6-de83-42d9-8c96-1f979c1ee204.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) t-SNE: Llama-2 Embs</div>\nFigure 10. Demonstration of Llama2 embedding analysis for the reading comprehension prompts (names and activities). (a) Pairwise embedding similarity matrix reveals a structure where simple prompts with the same name (bright squares), and prompts with the same activity (diagonal lines) are highly similar. For readability, only a subset of the prompts is included in this plot. See supplementary material for full set. (b) t-SNE visualization of the simple prompt embeddings reveals that embeddings of prompts describing the same activity are clustered together. Each point corresponds to a simple prompt and is colored by the activity mentioned in the prompt. (c) Accuracy of a logistic regression classifier trained to decode target activity from the embeddings of composite prompts that include one, two, or three simple prompts/activities and indirectly ask about one activity (target activity). The red line indicates random guessing accuracy. The task of predicting the target activity from prompt embeddings becomes harder as we add more simple prompts to the composite prompt. 16\n<div style=\"text-align: center;\">(c) Prompt Emb. Classifier</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of understanding in-context learning (ICL) in large language models (LLMs) by analyzing how their embeddings and attention representations change with ICL. Previous methods have struggled to explain the mechanisms behind ICL, necessitating a new approach to gain insights into these processes.",
        "problem": {
            "definition": "The problem defined in this paper is the lack of understanding of how ICL improves the performance of LLMs, particularly the changes in their internal representations and attention mechanisms.",
            "key obstacle": "The main obstacle is the complexity and opacity of LLMs, which makes it difficult to ascertain how ICL leads to behavioral improvements without clear empirical frameworks."
        },
        "idea": {
            "intuition": "The idea originates from the observation that neuroscience-inspired techniques can shed light on the internal workings of LLMs, particularly how they process information during ICL.",
            "opinion": "The proposed idea involves using representational similarity analysis (RSA) and attention ratio analysis (ARA) to systematically investigate the changes in LLM embeddings and attention patterns during ICL.",
            "innovation": "The innovation lies in the application of neuroscience-inspired methods at scale to analyze LLMs, allowing for a deeper understanding of their representational changes and attention allocation during ICL."
        },
        "method": {
            "method name": "Neuroscience-inspired analysis of LLMs",
            "method abbreviation": "NIA-LLMs",
            "method definition": "This method utilizes representational similarity analysis (RSA) and attention ratio analysis (ARA) to evaluate how LLM embeddings and attention weights change with in-context learning.",
            "method description": "The method focuses on measuring the representational shifts in LLMs' embeddings and attention weights before and after ICL, providing insights into their internal processing.",
            "method steps": [
                "Obtain embeddings for prompts fed into the LLM.",
                "Perform representational similarity analysis (RSA) to evaluate changes in embedding similarities.",
                "Conduct attention ratio analysis (ARA) to measure attention shifts towards relevant information in prompts."
            ],
            "principle": "The effectiveness of this method is based on the premise that improved alignment of embeddings and increased attention to relevant content will correlate with enhanced model performance."
        },
        "experiments": {
            "evaluation setting": "The experiments involved two tasks: linear regression and reading comprehension, using Llama-2 70B and Vicuna 13B models. Controlled conditions were set to isolate ICL effects.",
            "evaluation method": "The evaluation involved measuring hypothesis alignment of embeddings and attention ratios before and after ICL, using statistical analyses to assess the significance of observed changes."
        },
        "conclusion": "The experiments demonstrated that ICL significantly improves LLM behavior by enhancing task-critical representations and attention allocation to relevant content, indicating a clear relationship between representational changes and performance gains.",
        "discussion": {
            "advantage": "The proposed approach offers a robust framework for interpreting LLM behavior, leveraging insights from neuroscience to enhance understanding of model dynamics.",
            "limitation": "One limitation is the reliance on specific tasks that may not generalize across all possible applications of LLMs, potentially limiting the broader applicability of the findings.",
            "future work": "Future research could explore the effects of ICL on more complex reasoning tasks, as well as investigating the implications of these findings for improving LLM design and interpretability."
        },
        "other info": {
            "info1": "This research highlights the potential for interdisciplinary approaches, combining insights from neuroscience and machine learning.",
            "info2": {
                "info2.1": "The findings suggest a pathway to enhance the robustness of LLMs against prompt injection attacks.",
                "info2.2": "The methods developed in this study may inform future efforts in mechanistic interpretability of large models."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of understanding in-context learning (ICL) in large language models (LLMs) by analyzing how their embeddings and attention representations change with ICL."
        },
        {
            "section number": "1.2",
            "key information": "The problem defined in this paper is the lack of understanding of how ICL improves the performance of LLMs, particularly the changes in their internal representations and attention mechanisms."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Neuroscience-inspired analysis of LLMs (NIA-LLMs), utilizes representational similarity analysis (RSA) and attention ratio analysis (ARA) to evaluate how LLM embeddings and attention weights change with in-context learning."
        },
        {
            "section number": "3.2",
            "key information": "The innovation lies in the application of neuroscience-inspired methods at scale to analyze LLMs, allowing for a deeper understanding of their representational changes and attention allocation during ICL."
        },
        {
            "section number": "4.1",
            "key information": "The experiments demonstrated that ICL significantly improves LLM behavior by enhancing task-critical representations and attention allocation to relevant content."
        },
        {
            "section number": "6.2",
            "key information": "One limitation is the reliance on specific tasks that may not generalize across all possible applications of LLMs, potentially limiting the broader applicability of the findings."
        },
        {
            "section number": "7",
            "key information": "Future research could explore the effects of ICL on more complex reasoning tasks, as well as investigating the implications of these findings for improving LLM design and interpretability."
        }
    ],
    "similarity_score": 0.7025119130514756,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Decoding In-Context Learning_ Neuroscience-inspired Analysis of Representations in Large Language Models.json"
}