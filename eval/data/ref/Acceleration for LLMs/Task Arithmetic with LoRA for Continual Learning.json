{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.02428",
    "title": "Task Arithmetic with LoRA for Continual Learning",
    "abstract": "Continual learning refers to the problem where the training data is available in sequential chunks, termed \"tasks\". The majority of progress in continual learning has been stunted by the problem of catastrophic forgetting, which is caused by sequential training of the model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.",
    "bib_name": "chitale2023taskarithmeticloracontinual",
    "md_text": "# Task Arithmetic with LoRA for Continual Learning\nRajas Chitale\u2217, Ankit Vaidya\u2217, Aditya Manish Kane\u2217, Archana Ghotkar, Pune Institute of Computer Technology {rajaschitale, ankitvaidya1905, adityakane1}@gmail.com, aaghotkar@pict.edu\n 4 Nov 2023\n# Abstract\nContinual learning refers to the problem where the training data is available in sequential chunks, termed \"tasks\". The majority of progress in continual learning has been stunted by the problem of catastrophic forgetting, which is caused by sequential training of the model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.\narXiv:2311.02428v1\n# 1 Introduction\nThe problem of continual learning has grown in importance in recent times due to the large-scale nature of downstream datasets. Moreover, the high costs of data labelling pose challenge to conventional fine-tuning, where all of the downstream data can be used at once. In many settings, it is necessary for an agent to learn on-the-fly, by either learning from experiences or by external supervision from humans. Continual learning has seen many applications, ranging from climate change (Kane et al., 2022), medical AI (Yi et al., 2023) to real-time chatbots (Liu and Mazumder, 2021). The most promising methods in continual learning maintain a small set of past samples, called \"memory reservoir\" (Chaudhry et al., 2019b), and try to avoid catastrophic forgetting by augmenting the batch of the current task with samples from the memory reservoir. This method has achieved success to some extent, but is far from perfect. This method still suffers catastrophic forgetting, and thus cannot be deemed as a reliable method for continual learning in general. Vision Transformers (ViTs) (Dosovitskiy et al., 2021) have changed the way modern vision tasks are solved and have become ubiquitous in computer vision. Most of the prominent vision tasks have seen variants of ViTs reign supreme over the past couple of years. Efforts have been made to make ViTs, and transformers in general, run faster to support real-time applications. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is a method for efficient fine-tuning where the large weight tensors are augmented by low-rank counterparts, which are trained along with the frozen weights of the original model. Task arithmetic (Ilharco et al., 2023) is a simple method which leverages the semantics of weight spaces to manipulate the model weights to achieve compelling performance on tasks it was not trained on. It assumes that fine-tuning models pushes the weights away towards a semantically relevant\n\u2217first author, equal contribution\nWorkshop on Advancing Neural Network Training at 37th Conference on Neural Information Processing Systems (WANT@NeurIPS 2023).\nWorkshop on Advancing Neural Network Training at 37th Conference on Neural Information Processing Systems (WANT@NeurIPS 2023).\ndirection in the weight space, thus creating a \"task vector\" from the pretrained checkpoint to t fine-tuned checkpoint.\nWe make a simple observation: modern continual learning demands the model to efficiently learn all tasks irrespective to the order they were provided. This observation points to a straightforward combination of task arithmetic, low-rank adaptation and memory reservoirs. In this paper, we show that this simple combination is indeed a powerful one. Specifically, we fine-tune only the low-rank weights of LoRA-ViT on each task. After training on individual tasks, we combine these low-rank weights using task arithmetic rules and merge them to the pre-trained ViT. Finally, we fine-tune this ViT on a small set of samples from the dataset. To evaluate our method, we perform experiments on Flowers-102 (Nilsback and Zisserman, 2008), Oxford-IIIT Pets (Parkhi et al., 2012) and CIFAR10 (Krizhevsky, 2009) datasets. We conclusively show that this simple procedure brings the ViT very close its fully trained counterparts, thus cementing the effectiveness of our method.\n# 2 Related Work\nIn computer vision, CNN based models(He et al., 2015) have been the dominant architectures for tasks like classification, segmentation, and detection. After the application of the Transformer architecture (Vaswani et al., 2017) to these tasks in computer vision, Vision Transformers (ViTs) (Dosovitskiy et al., 2021) have surpassed the traditional models and have become the dominant paradigm. This architecture leverages the effectiveness of large-scale pre-training to surpass the previous state of the art performance in multiple vision tasks. A variant of this is the BEiT (Bao et al., 2021), which uses a similar pre-training method to BERT (Devlin et al., 2019) - masked image modelling. This model is then fine-tuned on downstream tasks, outperforming the ViT. There are also other variants like the DEiT (Touvron et al., 2021) which uses knowledge distillation to train Vision Transformers efficiently, ConViT (d\u2019Ascoli et al., 2022) which introduces gated positional self-attention (GPSA) which can be equipped with a soft convolutional inductive bias for improved performance, UViT (Chen et al., 2022) which shows that ViTs can perform better on segmentation and detection tasks without the addition of CNN-like designs and the Swin Transformer (Liu et al., 2021) which introduces an architecture that can be used as a general-purpose backbone for vision tasks. In continual learning, a sequence of contents like tasks or examples is provided incrementally over a period of time to the system, and it is expected that the system learns them as if they were provided simultaneously. One of the main hindrances in continual learning is catastrophic forgetting (French, 1999), where knowledge of previous tasks is lost when a system is trained on a new task. Some recent promising techniques to overcome catastrophic forgetting include as Experience Replay also called \"memory reservoir\" (Chaudhry et al., 2019b) in which a small set of past examples is maintained and the current task is augmented with these samples and AGEM (Chaudhry et al., 2019a) which stores the past examples and treats the losses on the past examples as an inequality constraint. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) is another method used to mitigate catastrophic forgetting in which we selectively decrease the plasticity of weights and protects the previous knowledge while training on new tasks. Another method, Learning without Forgetting (Li and Hoiem, 2018) uses a combination of knowledge distillation and fine-tuning to preserve the performance on old tasks. Deep Generative Replay (Shin et al., 2017) is a method which uses fake data that is generated to mimic former examples in training to enable flexible knowledge transfer between tasks. Fine-tuning models for downstream tasks is an inefficient process as all of the parameters are updated. One of the ways to overcome this inefficiency is to make use of parameter efficient fine-tuning, where we fine-tune only a small number of parameters with relatively unchanged performance. Some methods include Adapters (Houlsby et al., 2019) where we add a small number of parameters to the model which are trained for downstream tasks. However adapters introduce inference latency and bottlenecks which make the overall process more inefficient. Low-Rank Adaptation (LoRA) (Hu et al., 2021) uses low-rank matrix counterparts of the original weights during fine-tuning, and keeps the actual weights frozen. After training is done the model parameters are updated using the low-rank matrices at no inference cost or bottlenecks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/302f/302fac55-30dc-4682-be97-12194e19222f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d1b/4d1b52ef-e884-4114-90a3-a5bb1fecee49.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Step 1: Taskwise training</div>\n<div style=\"text-align: center;\">LoRA Transformer block</div>\nFigure 1: Our experimental setup. We only train the LoRA weights associated with the model. Each input example passes through the frozen and LoRA weights, and both the outputs are summed to get the final output. Each of these sets of LoRA weights is trained on the data of a particular task. We then calculate and merge the task vectors associated with each of these tasks. Finally, we fine-tune the model on a small set of samples, after which the model is ready for deployment.\n# 3 Method\n# 3.1 Problem setting\nWe address the problem of continual learning in a class-incremental setting. Concretely, there are N tasks, each denoted by T i = {Xi, Y i} where i \u2208{0, . . . , N \u22121} and Xi and Y i are image-label pairs for the ith task. Here each element in Y 0 \u222aY 1 \u222a. . . \u222aY N\u22121 \u2208{0, . . . , C \u22121}, and C is the number of total classes in the dataset. Note that X0 \u2229X1 \u2229. . . \u2229XN\u22121 = \u03d5, which means no samples are repeated across tasks. Each task encompasses a subset of classes from the total number of classes, which roughly equals C/N. A model is sequentially provided the data associated with each task T i, with the preceding task\u2019s data being inaccessible when a new task is presented. The goal, in our experimental setup, is to maximize performance on a hold-out set {Xtest, Y test} where elements from Y test \u2208{0, . . . , C \u22121}. The most well-known problem in this setting is \"catastrophic forgetting\", where the model performs poorly on initial tasks, since the weights are overridden by the updates while training on newer tasks.\n# 3.2 LoRA-adapted task-wise training\nThe first part of our approach is to take an off-the-shelf ViT backbone and augment it with LoRA. Specifically, we introduce an additional set of weights, corresponding to all query and value weight matrices in ViT. These new weights have a drastically low rank, and a single matrix W M\u00d7M is represented by two matrices of lower rank, A M\u00d7K and B K\u00d7M where K << M. In practice, M = 768 and K = 16. So, for an input tensor X b\u00d7s\u00d7M (b and s are batch size and sequence length, respectively):\nThe advantage of this method is that while training, W is frozen, and only A and B are updated. This results in significantly lesser computation, both in terms of FLOP count and wall-clock time. For inference, we modify the model weights such that\nThis effectively makes it the same as a conventional fine-tuned model. In our experiments, we fine-tune each LoRA-augmented ViT on a specific task. We finally merge the LoRA weights to the base model to get the task-specific model.\n# 3.3 Task arithmetic\nFor all experiments, we assume the \"task vector\" of a given task-specific model to be defined as follows: \u03c4i = \u03b8i \u2212\u03b8pre (4)\nFor all experiments, we assume the \"task vector\" of a given task-specific model to be defined as follows:\nFor all experiments, we assume the \"task vector\" of a given task-specific model to be defined a follows:\n\u03c4i = \u03b8i \u2212\u03b8pre\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/196d/196d3ad1-129a-4362-b9cf-7741f710487d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cda0/cda0bf82-8790-464a-b3d0-24207206acae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Step 3: Fine-tuning on memory</div>\n(1)\n(2) (3)\n(2)\n(4)\nHere, \u03b8i is the collection of weights of the task-specific model of task T i, and \"\u2212\" implies elementwise subtraction. After fine-tuning each LoRA-augmented model on its respective task, we combine the task vectors using the following:\nIntuitively, we get our final, task-agnostic model by doing the elementwise operation:\n\u03b8final = \u03b8pre + \u03c4\nThis, in itself provides a surprisingly strong benchmark on two of our three evaluation datasets.\n# 3.4 Finetuning on memory\nCommonly used methods like episodic memory (Chaudhry et al., 2019b) and experience replay (Rolnick et al., 2019) use a memory buffer to capture examples from individual tasks and to retrain the model later on these examples. To emulate this effect, we choose 10 samples per class from each dataset and fine-tune our final, task-agnostic model on this collated set. This approach has two advantages over the experience replay approach. Firstly, there is no requirement of augmenting every batch as done in reservoir sampling, for example. Secondly, training the model on a balanced dataset completely bypasses the inconsistencies associated with sampling. This not only improves the performance, but brings it very close to the full-shot supervised baseline. We present our experimental setup and results in the upcoming section.\n# 3.5 Enforcing feature distribution using KL-divergence loss\nOne of the intuitions behind training only LoRA weights is that this will result in small changes to the model, which will make more sense when we perform task arithmetic. Extending this thought, we also enforce the similarity of distribution from LoRA-adapted model to the one from vanilla ViT. This would enforce the task-wise LoRA features to be closer to the more generalized vanilla ViT features. Our loss function for KL-divergence experiments is as follows:\nLKL = KLDiv(softmax(f pre bb (x), softmax(f ft bb (x)) L = \u03bb1Lcls + \u03bb2LKL\nLKL = KLDiv(softmax(f pre bb (x), softmax(f ft bb (x))\nHere, fcls and fbb are the classifier head and backbone respectively, where the superscript denotes if the model is pre-trained or fine-tuned. We take a weighted sum of the losses, and we empirically choose \u03bb1 = 0.6 and \u03bb2 = 0.4. We have illustrated our complete training procedure in Figure 1.\n# 4 Results and discussion\nThe efficiency of our proposed method is demonstrated through varied experiments and comparisons. The experimental setup consists of three datasets namely, Oxford-IIIT Pets (37 classes), Flowers-102 (102 classes), and CIFAR10 (10 classes). Table 1 shows the details of the data samples used for training and testing. The task to class mapping for each dataset is shown in Appendix A.\n<div style=\"text-align: center;\">Table 1: Details of the datasets used for experimentation</div>\nDataset\nNo. of tasks\nTrain\nTest\nTotal\nAvg. samples/task\nTotal\nAvg. samples/task\nOxford-IIIT pets\n6\n3680\n613\n3669\n612\nFlowers-102\n10\n2040\n204\n6149\n615\nCIFAR10\n5\n50000\n10000\n10000\n2000\n(5)\n(6)\n(7)\n(8) (9)\nPretrained ViT and its LoRA counterpart were trained in the offline setting on the three datasets to get the baseline accuracy for each dataset. We observed that the resultant model of our continual learning approach, when trained on Flowers-102, has results almost similar to the offline counterparts on the same dataset. Our method showed lower but comparable accuracy with the offline learning benchmarks when trained on Oxford-IIIT Pets and CIFAR10 datasets. These results can be viewed and compared from Table 2.\n# 4.2 Comparison with continual learning benchmarks\nIn order to validate the efficiency of our proposed CL method, we benchmarked our results against the SOTA methods for CL like AGEM (Chaudhry et al., 2019a) and Experience Replay (ER) (Chaudhry et al., 2019b). The Table 2 shows the superior performance of the new approach as compared to the SOTA methods stated above. Moreover, as a consequence of training a LoRA-augmented ViT, we can observe a significant reduction in the training time and FLOPs.\nTable 2: Top-1 accuracy(%) of ViT, LoRA-ViT trained on entire datasets, AGEM and Experience Replay (ER) trained by class-incremental CL and our proposed class-incremental CL approach, trained on different datasets (50 epochs). \"XEnt\" and \"KLDiv\" stand for Crossentropy and KL Divergence losses respectively. The best scores for the continual setting have been highlighted in bold.\nOxford-IIIT Pets\nFlowers-102\nCIFAR10\nOffline learning\nbaselines\nViT\n94.03\n98.69\n99.05\nLoRA ViT\n93.75\n97.08\n98.55\nCL baselines\nAGEM\n73.18 (-20.85)\n33.39 (-65.3)\n62.3 (-36.35)\nReplay\n88.25 (-5.78)\n91.07 (-7.62)\n86.25 (-12.8)\nProposed method\nXEnt loss\n90.32 (-3.71)\n94.36 (-4.33)\n95.59 (-3.46)\nXEnt + KLDiv loss\n88.69 (-5.34)\n97.06 (-1.63)\n92.49 (-6.56)\n# 4.3 Computation analysis\nThe comparison of computational complexity of our proposed method and Experience replay measured in PFLOPs (PetaFLOPs) has been shown in Table 3. Our method uses 3 - 5 times less PFLOPs while significantly outperforming the existing methods. We calculated the FLOPs required for training using the fvcore library. The FLOPs required for the forward and backward pass were considered. In our LoRA-adapted ViT the FLOPs required for the backward pass were significantly lower as compared to a ViT as in a LoRA-adapted model the model weights are frozen and only the LoRA weights get updated during the backward pass. This contributes greatly to the lower complexity of our proposed methodology. Furthermore, we also note that memory replay essentially uses double the compute, since the model is trained on a combination of task data as well as data sampled from the memory.\n# 4.4 Implementation details\nFor performing extensive experiments on LoRA-augmented ViT, we had access to a Nvidia Tesla P100 GPU with 16GB HBM2 memory. We employed Adam optimizer with a learning rate of 5\u03f5 \u22126, weight decay of 1\u03f5 \u22126, and batch size of 32 for all the datasets. The LoRA parameters that were configured for ViT, using the PEFT library (Mangrulkar et al., 2022) from the Hugging Face API, were r = 16 and alpha = 16, where r is the dimension used by update matrices and alpha is the scaling factor. The bias parameters were set as non-trainable parameters. This configuration enabled us to train only 2.02% of the total 87.6M parameters of a ViT to get impressive results. The experiments were performed by incorporating KL Divergence loss and few-shot finetuning by training Oxford-IIIT Pets for 50 epochs, CIFAR10 for 50 epochs, and Flowers-102 for 30 epochs.\nable 3: Comparison of Computational Complexity (in PFLOPs) of proposed method with Experience eplay (ER). Reduction shows the how much the number of PFLOPs required on each dataset was\nTable 3: Comparison of Computational Complexity (in PFLOPs) of proposed method with Experience Replay (ER). Reduction shows the how much the number of PFLOPs required on each dataset was reduced by using our method.\nDataset\nReplay\nOur Method\nReduction\nOxford-IIIT Pets\n17.828\n3.493\n5.10\u00d7\nFlowers-102\n6.139\n1.599\n3.84\u00d7\nCIFAR10\n237.326\n44.880\n5.28\u00d7\n# The scaling factor used for adding the task vectors is \u03bb = 0.25 for all experiments.\nWe used Avalanche (Lomonaco et al., 2021) to calculate the CL baselines for AGEM and Experience Replay (ER) methods. For AGEM, 100 patterns per task were used and the memory size for ER was set to 200. The offline learning baselines were calculated using the above-given learning rate and weight-decay with 50 epochs for Oxford-IIIT pets and CIFAR10 while using 30 epochs for Flowers-102.\n# 4.5 Observations\nIn this section, we present some key observations from our experiments.\n1. Oxford-IIIT Pets and CIFAR10 have better pre-finetuning results: We observe that without finetuning, Flowers-102 has the worst performance. We speculate that this is because of the high variance and high number of classes in Flowers-102. We hypothesize that if the tasks have some underlying distributions, the task vector will be better directed, thus having a better task-agnostic model. 2. KL Divergence loss reduces variance amongst task vectors: We observe that KL Divergence loss was detrimental in the case of datasets with low number of classes. However, in the case of Flowers-102, it gave a significant boost. We speculate that this is because in the former case, KL Divergence loss was a too strong regularizer, which hindered actual learning. However, in the case of Flowers-102, it reduced the high variance in the resultant task vectors, hence exhibiting better performance. 3. The efficacy of few-shot finetuning makes the case for task arithmetic: As shown in Appendix B, we can see that finetuning greatly improves the performance of the model obtained using task-arithmetic. We consider this as a testimony to our original weight manipulation method. Since the model is able to achieve near offline results with very few samples, we deduce that the weights obtained by task-arithmetic are a good initialization for the model to learn task-agnostic representations.\n# 5 Conclusion\nIn this work, we introduce a novel approach to tackle continual learning. We use task arithmetic and low-rank adaption to mitigate catastrophic forgetting. We empirically show that the combination of these three seemingly unrelated methodologies outperforms classical baselines. Since a considerable part of vision community has started working on ViTs, we believe this work can serve as a simple yet strong baseline for all future works in the field of continual perception. A probable future work in this direction is to study the model combination logic in greater detail. Works like ZipIt! (Stoica et al., 2023) propose novel methods to manipulate the weights in order to support continual settings. Weight manipulation for continual and multi-task settings is still a nascent and little-understood field, which might provide greater insights and improvements to our method.\n# References\nHangbo Bao, Li Dong, and Furu Wei. 2021. BEiT: BERT pre-training of image transformers. Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. 2019a. Efficient lifelong learning with a-gem. Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K. Dokania, Philip H. S. Torr, and Marc\u2019Aurelio Ranzato. 2019b. On tiny episodic memories in continual learning. Wuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin, Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, and Denny Zhou. 2022. A simple single-scale vision transformer for object localization and instance segmentation. Stephane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. 2022. Convit: improving vision transformers with soft convolutional inductive biases. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):114005. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. Robert French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3:128\u2013135. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recognition. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. Aditya Kane, V Manushree, and Sahil S Khose. 2022. Continual vqa for disaster response systems. In NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526. Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical report. Zhizhong Li and Derek Hoiem. 2018. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935\u20132947. Bing Liu and Sahisnu Mazumder. 2021. Lifelong and continual learning dialogue systems: Learning during conversation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):15058\u2013 15063. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows.\nVincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graffieti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni. 2021. Avalanche: an end-to-end library for continual learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2nd Continual Learning in Computer Vision Workshop. Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github. com/huggingface/peft. Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing. Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. 2012. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. 2019. Experience replay for continual learning. Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual learning with deep generative replay. George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman. 2023. Zipit! merging models from different tasks without training. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. 2021. Training data-efficient image transformers and distillation through attention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Huahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, and Kang Li. 2023. Towards general purpose medical ai: Continual learning medical foundation model.\n# Appendix\n# A Dataset Details\nThe Oxford-IIIT Pets dataset, that has 37 classes, was split into 6 disjoint tasks as shown in Table 4. The Flowers-102 dataset, which has 102 classes, was split into 10 disjoint tasksas shown in Table 5. The CIFAR10 dataset, which has 10 classes, was split into 5 disjoint tasks as shown in Table 6.\n# B Task-wise results\nThe task-wise accuracy values obtained while performing experiments on each dataset are shown below in the following sections. Tables 7, 8, and 9 present the performance for Oxford-IIIT Pets, Flowers-102 and CIFAR10 datasets respectively.\nTask 0\namerican_bulldog, scottish_terrier, english_setter,\n6 classes\nnewfoundland, Maine_Coon, British_Shorthair\nTask 1\nPersian, boxer, english_cocker_spaniel,\n6 classes\nsaint_bernard, Russian_Blue, Bombay\nTask 2\njapanese_chin, Sphynx, german_shorthaired,\n6 classes\nbasset_hound, samoyed, shiba_inu\nTask 3\nstaffordshire_bull_terrier, Siamese, wheaten_terrier,\n6 classes\nAbyssinian, keeshond, havanese\nTask 4\nyorkshire_terrier, Bengal, great_pyrenees,\n6 classes\nEgyptian_Mau, pomeranian, beagle\nTask 5\namerican_pit_bull_terrier, Ragdoll, miniature_pinscher\n7 classes\npug, Birman, leonberger, chihuahua\n<div style=\"text-align: center;\">Table 5: Task to class mapping in the CL setting for Flowers-102</div>\nTask 0\nalpine sea holly, buttercup, fire lily,\n10 classes\nanthurium, californian poppy, foxglove,\nartichoke, camellia, frangipani, azalea\nTask 1\ncanna lily, fritillary, ball moss,\n10 classes\ncanterbury bells, garden phlox, yellow iris,\nballoon flower, cape flower, gaura, barbeton daisy\nTask 2\ncarnation, gazania, bearded iris, bird of paradise,\n10 classes\ncautleya spicata, germanium, bee balm,\nclematis, giant white arum lily, colt\u2019s foot\nTask 3\nglobe thistle, bishop of llandaff, great masterwort,\n10 classes\nglobe flower, black-eyed susan, common dandelion,\ngrape hyacinth, blackberry lily, corn poppy, columbine\nTask 4\nblanket flower, cyclamen, hard-leaved pocket orchid,\n10 classes\nbolero deep blue, daffodil, hibiscus, bougainvillea,\ndesert-rose, hippeastrum, bromelia\nTask 5\nenglish marigold, japanese anemone, king protea,\n10 classes\nstemless gentian, lenten rose, petunia, sunflower,\nlotus, peruvian lily, pincushion flower\nTask 6\nsweet pea, love in the mist, pink primrose,\n10 classes\nsweet william, magnolia, pink-yellow dahlia, sword lily,\nmallow, poinsettia, thorn apple\nTask 7\nmarigold, primula, tiger lily, mexican aster,\n10 classes\nprince of wales feathers, toad lily, mexican petunia,\npurple coneflower, tree mallow, monkshood\nTask 8\nred ginger, tree poppy, moon orchid, trumpet creeper,\n10 classes\nrose, morning glory, ruby-lipped cattleya,\nwallflower, orange dahlia, siam tulip\nTask 9\nwater lily, osteospermum, silverbush, watercress,\n12 classes\noxeye daisy, snapdragon, wild pansy, spring crocus,\npassion flower, spear thistle, windflower, pelargonium,\nTable 6: Task to class mapping in the CL setting for CIFAR10\n<div style=\"text-align: center;\">Table 6: Task to class mapping in the CL setting for CIFAR10</div>\nTask 0\nairplane, automobile\n2 classes\nTask 1\nbird, cat\n2 classes\nTask 2\ndeer, dog\n2 classes\nTask 3\nfrog, horse\n2 classes\nTask 4\nship, truck\n2 classes\nTable 7: Task-wise Top-1 Accuracy(%) on Oxford-IIIT Pets dataset for our proposed approach experimented on combination of Crossentropy loss, KL Divergence loss and memory fine-tuning . \"XEnt\" and \"KLDiv\" stand for Crossentropy and KL Divergence losses respectively. \"TARV\" and \"MemFT\" stand for task-agnostic resultant vector and few-shot fine-tuning on memory, respectively. The best scores for the continual setting have been highlighted in bold.\nTask\nXEnt loss\nXEnt + KLDiv loss\nTARV\nTARV+MemFT\nTARV\nTARV+MemFT\n0\n62.94\n83.97\n72.29\n85.64\n1\n91.31\n95.57\n76.32\n93.87\n2\n55\n91.17\n57.67\n92.67\n3\n85.67\n93.52\n64.51\n84.64\n4\n90.45\n95.31\n77.39\n90.45\n5\n75.43\n83.71\n82.43\n85.43\nEntire dataset\n76.8\n90.54\n71.77\n88.78\nTable 8: Task-wise Top-1 Accuracy(%) on Flowers-102 dataset for our proposed approach experimented on combination of Crossentropy loss, KL Divergence loss and memory fine-tuning . \"XEnt\" and \"KLDiv\" stand for Crossentropy and KL Divergence losses respectively. \"TARV\" and \"MemFT\" stand for task-agnostic resultant vector and few-shot fine-tuning on memory, respectively. The best scores for the continual setting have been highlighted in bold.\nTask\nXEnt loss\nXEnt + KLDiv loss\nTARV\nTARV+MemFT\nTARV\nTARV+MemFT\n0\n27.64\n92.86\n24.53\n98.14\n1\n27.95\n86.61\n25.17\n95.15\n2\n7.83\n93.21\n14.6\n99.22\n3\n21.45\n98.21\n17.96\n97.86\n4\n32.32\n92.13\n28.87\n93.92\n5\n41.49\n97.72\n39.09\n96.76\n6\n49.48\n98.45\n36.79\n98.45\n7\n9.09\n94.46\n5.09\n97.64\n8\n49.72\n97.22\n30.81\n98.99\n9\n29.50\n90.65\n25.75\n95.54\nEntire dataset\n29.646\n94.15\n24.867\n97.167\nTable 9: Task-wise Top-1 Accuracy(%) on CIFAR10 dataset for our proposed approach experimented on combination of Crossentropy loss, KL Divergence loss and memory fine-tuning . \"XEnt\" and \"KLDiv\" stand for Crossentropy and KL Divergence losses respectively. \"TARV\" and \"MemFT\" stand for task-agnostic resultant vector and few-shot fine-tuning on memory, respectively. The best scores for the continual setting have been highlighted in bold.\nTask\nXEnt loss\nXEnt + KLDiv loss\nTARV\nTARV+MemFT\nTARV\nTARV+MemFT\n0\n92.3\n97.35\n14.9\n93.9\n1\n84.9\n90.65\n96.55\n94.05\n2\n95.8\n95.35\n28.35\n81.6\n3\n96.1\n97.6\n26.1\n95.85\n4\n97.3\n97\n12.15\n97.05\nEntire Dataset\n93.28\n95.59\n35.61\n92.49\n",
    "paper_type": "method",
    "attri": {
        "background": "Continual learning has become increasingly important due to the large-scale nature of datasets and the high costs of data labeling. Traditional fine-tuning methods struggle with catastrophic forgetting, where training on new tasks leads to the loss of knowledge from previous tasks. Existing methods, such as memory reservoirs, have shown some success but are not reliable for continual learning.",
        "problem": {
            "definition": "The problem addressed is catastrophic forgetting in continual learning, particularly in class-incremental settings where models must learn from sequentially provided tasks without access to previous data.",
            "key obstacle": "The main challenge is that existing methods fail to maintain performance on earlier tasks when new tasks are introduced, leading to significant degradation in accuracy."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for models to efficiently learn from tasks presented in any order, leveraging low-rank adaptation and task arithmetic to mitigate forgetting.",
            "opinion": "The proposed method combines low-rank adaptation with task arithmetic and memory reservoirs to train vision transformers continually, aiming to improve efficiency and performance.",
            "innovation": "This method innovates by fine-tuning only low-rank weights and merging them using task arithmetic, which significantly reduces computational costs while maintaining high accuracy."
        },
        "method": {
            "method name": "LoRA-ViT",
            "method abbreviation": "Low-Rank Adaptation Vision Transformer",
            "method definition": "LoRA-ViT is a continual learning method that utilizes low-rank adaptation to fine-tune transformer-based models on sequential tasks while preventing catastrophic forgetting.",
            "method description": "The core of the method involves training low-rank weights on individual tasks and combining them with pretrained weights using task arithmetic.",
            "method steps": [
                "Fine-tune low-rank weights of the LoRA-ViT on each task.",
                "Combine the task vectors using task arithmetic rules.",
                "Fine-tune the model on a small set of samples from the dataset."
            ],
            "principle": "The method is effective because it minimizes the updates to the original model weights while allowing for significant task-specific learning through low-rank adaptation."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three datasets: Flowers-102, Oxford-IIIT Pets, and CIFAR10, with varying numbers of tasks and classes.",
            "evaluation method": "Performance was evaluated by comparing accuracy against state-of-the-art continual learning methods, measuring both training time and computational complexity."
        },
        "conclusion": "The proposed method successfully mitigates catastrophic forgetting in continual learning, demonstrating performance close to that of fully trained models while significantly reducing computational costs. This work establishes a strong baseline for future research in continual learning with vision transformers.",
        "discussion": {
            "advantage": "The key advantages include reduced computational requirements and the ability to achieve high accuracy with minimal data, making it efficient for real-time applications.",
            "limitation": "A limitation of the method is that it may still struggle with tasks that have high variance or require extensive data for training.",
            "future work": "Future research could explore more sophisticated weight manipulation techniques and further investigate the model combination logic to enhance continual learning capabilities."
        },
        "other info": {
            "info1": "The method achieved performance close to full-set fine-tuning with just 10 samples per class.",
            "info2": {
                "info2.1": "The model was trained using a Nvidia Tesla P100 GPU.",
                "info2.2": "Hyperparameters included a learning rate of 5e-6 and weight decay of 1e-6."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.3",
            "key information": "The proposed method, LoRA-ViT, addresses catastrophic forgetting in continual learning by fine-tuning low-rank weights and merging them using task arithmetic, which significantly reduces computational costs while maintaining high accuracy."
        },
        {
            "section number": "4.1",
            "key information": "LoRA-ViT is a continual learning method that utilizes low-rank adaptation to fine-tune transformer-based models on sequential tasks while preventing catastrophic forgetting."
        },
        {
            "section number": "3.5",
            "key information": "The key advantages of the proposed method include reduced computational requirements and the ability to achieve high accuracy with minimal data, making it efficient for real-time applications."
        },
        {
            "section number": "6.1",
            "key information": "The method achieves performance close to full-set fine-tuning with just 10 samples per class, demonstrating significant efficiency in model training."
        },
        {
            "section number": "6.4",
            "key information": "The evaluation method compared accuracy against state-of-the-art continual learning methods, measuring both training time and computational complexity."
        }
    ],
    "similarity_score": 0.6049107144266347,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Task Arithmetic with LoRA for Continual Learning.json"
}