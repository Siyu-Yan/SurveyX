{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2003.11838",
    "title": "Applying the Isabelle Insider Framework to Airplane Security",
    "abstract": "Avionics is one of the fields in which verification methods have been pioneered and brought a new level of reliability to systems used in safety critical environments. Tragedies, like the 2015 insider attack on a German airplane, in which all 150 people on board died, show that safety and security crucially depend not only on the well functioning of systems but also on the way how humans interact with the systems. Policies are a way to describe how humans should behave in their interactions with technical systems, formal reasoning about such policies requires integrating the human factor into the verification process. In this paper, we report on our work on using logical modelling and analysis of infrastructure models and policies with actors to scrutinize security policies in the presence of insiders. We model insider attacks on airplanes in the Isabelle Insider framework. This application motivates the use of an extension of the framework with Kripke structures and the temporal logic CTL to enable reasoning on dynamic system states. Furthermore, we illustrate that Isabelle modelling and invariant reasoning reveal subtle security assumptions. We summarize by providing a methodology for the development of policies that satisfy stated properties.",
    "bib_name": "kammller2020applyingisabelleinsiderframework",
    "md_text": "# Applying the Isabelle Insider Framework to Airplane Security\nFlorian Kamm\u00a8uller Middlesex University London and Technische Universit\u00a8at Berlin Manfred Kerber University of Birmingham, UK\nUniversity of Birmingham, UK\nAbstract\nAvionics is one of the fields in which verification methods have been pioneered and brought a new level of reliability to systems used in safety critical environments. Tragedies, like the 2015 insider attack on a German airplane, in which all 150 people on board died, show that safety and security crucially depend not only on the well functioning of systems but also on the way how humans interact with the systems. Policies are a way to describe how humans should behave in their interactions with technical systems, formal reasoning about such policies requires integrating the human factor into the verification process. In this paper, we report on our work on using logical modelling and analysis of infrastructure models and policies with actors to scrutinize security policies in the presence of insiders. We model insider attacks on airplanes in the Isabelle Insider framework. This application motivates the use of an extension of the framework with Kripke structures and the temporal logic CTL to enable reasoning on dynamic system states. Furthermore, we illustrate that Isabelle modelling and invariant reasoning reveal subtle security assumptions. We summarize by providing a methodology for the development of policies that satisfy stated properties. Keywords: Airplane safety and security, Insider threats, Interactive theorem proving, Security policies, Verification\n# 1. Introduction\nAirplanes offer a very safe way of travelling. Accidents and terror attacks are extremely rare. After the 2001-09-11 attacks stringent measures were taken and have been to the day of writing successful. The most recent major incident was an insider attack in which the copilot of Germanwings Flight 9525 on 2015-03-24 hijacked the aircraft by locking out the captain, who had left the cabin, and subsequently brought the aircraft to a crash in which all 150 persons on board died. As a consequence, airlines introduced a two-person rule that a pilot must never be on their own in the cockpit. The two-person rule has been rescinded\nMarch 27, 2020\nin 2017 only two years after it was introduced. The 2015-03-24 incident shows that insider attacks are an important issue and it motivated earlier work [1] of applying the existing Isabelle Insider framework [2] to verify airplane policies in the presence of insider attacks. This earlier work has revealed some major challenges for the Isabelle Insider framework: \u2022 Since the policies are dealing with actors and their possibilities of moving within the infrastructure, for example an airplane, a fixed association of actors with locations, roles, and credentials in the model must be extended to enable representing dynamic change. \u2022 We need to integrate dedicated logics into the framework enabling the expression of security and safety guarantees over the dynamically changing infrastructure state. We need to express global validity of logical properties of policies over all reachable states; for example, we want to express \u201cfor all states reachable from an acceptable initial state, a suicidal copilot cannot crash the plane\u201d. In the current paper, we provide solutions to these challenges and demonstrate them on the airplane case study. The main contribution of this paper are: \u2022 State transitions as well as rules for expressing changes to the state of infrastructures including locations, actors, their roles, credentials and behaviours are provided by Kripke structures. This allows modelling state change and state transition. \u2022 Temporal logic CTL is provided within the framework to formalize and prove logical properties. This enables (a) detecting attack paths through the graph of infrastructure state evolution and (b) from there identifying additional security assumptions that when met guarantee that the attack is not possible any more on any path. Another contributions of this work is to identify an improved methodology for policy invalidation and model refinement. After discussing related work in Section 2, we present in Section 3 a retrospective of the development of safety and security regulations for airplanes. We then present the existing Isabelle Insider framework in Section 4. Next, we use this framework to model an airplane scenario including an Insider attacker. We integrate Kripke structures into the model and express and interactively prove central security properties using the branching time temporal logic CTL (Section 5). Section 6 presents the analysis of those properties on the airplane scenario showing how the framework can be used to scrutinize the security policies and thereby reveal existing loopholes within their formal specifications. This procedure is summarized into our methodology before Section 7 concludes. The full Isabelle sources are available online [3]. In order to give an impression of the kind of formalization the most important definitions and theorems can be found in the Appendix.\n# 2. Related Work\nIn this section, we present some related work from the field of insider threats and work in which reasoning approaches similar to the one applied in our work are applied. Furthermore we discuss work related to the verification in avionics.\nThe insider threat patterns provided by CERT [4] use the System Dynamics model, which can express dependencies between variables. The System Dynamics approach is also successfully being applied in other approaches to insider threats, for example, in the modelling of unintentional insider threats [5]. Axelrad et al. [6] have used Bayesian networks for modelling insider threats in particular the human disposition. In comparison, the model we rely on for modelling the human disposition in the Isabelle Insider framework is a simplified classification following the taxonomy provided in [7]. In contrast to all these approaches, our work provides an additional model of infrastructures and policies allowing reasoning at the individual and organizational level. A major field of application of formal methods is avionics. Companies (such as Airbus and Boeing) and organizations (such as NASA) use formal methods to prove formal properties of aircrafts and spacecrafts. There is a large body of work, including work based on model checking and theorem proving, which we cannot give justice in this paper. We will mention only a few. [8] is mainly concerned with the relationship between software testing and formal verification, and Moy et al. argue that in many application areas formal verification outperforms testing, firstly in that the proofs show the correctness on all inputs and not just the ones tested, but secondly also in the person power required. [9] shows how a Z-based toolset is used to prove the correctness of embedded real time safety critical software for Eurofighter Typhoon. Khan et al. [10] argue that complexity of avionics has increased to a level that verification and validation of the systems need computer based approaches. They use model abstraction to simulate hardware and software interactions. In the domain of rigorous analysis of airplane systems, work often follows for practical and economic reasons a philosophy of using a mix of formal and systematic informal methods. An example from airplane maintenance procedures [11] uses a security evaluation methodology following the Common Criteria and a formal model and verification with the model checker AVISPA. In comparison, we use a more expressive logical model in the Isabelle Insider framework than the AVISPA specification. To our knowledge, the focus of work on formal methods in avionics is directed towards the correct functioning of the hardware and the software. However, it is very important to consider the human factor.1 We assume that our work is the first to consider insider threats within airplane safety and security in a formal way. Logical modelling and analysis of insider threats has started off by investigating insider threats with invalidation of security policies in connection with model checking by one of us\nin [12, 13]. This early approach also uses infrastructure models of organizations, actors and policies but was more restricted than the Isabelle Insider framework discussed in Section 4. The use of sociological explanation has been pioneered in [14] by one of us already with first formal experiments in Isabelle. Finally, one of us has established the Isabelle Insider framework in [2]. It has been validated on two of the main three insider patterns the Entitled Independent and Ambitious Leader. Relevant in the context of this application are other applications of the Isabelle Insider framework, and been applied to IoT Insiders [15, 16] by using in addition the extension of the framework to attack trees. Attack trees provide the possibility to refine attacks once they have been identified. This refinement is formalized together with the notion of attack trees as first introduced for insider models in general in [17]. In other work, we applied the insider framework to auction protocols [18]. In the CHIST-ERA project SUCCESS [19] we use the framework in combination with attack trees and the Behaviour Interaction Priority (BIP) component architecture model to develop security and privacy enhanced IoT solutions. In [20] Kamali et al. present reasoning that integrates deduction based reasoning and model checking for the formal verification of vehicle platooning. The idea is that vehicles move in platoons and can join and leave them under certain safety conditions. In order to model the hybrid aspects of the real-time system a hybrid system is used that makes use of discrete decision making (such as, initiating joining a platoon) and continuous control (of actually driving the vehicle). The formal discrete reasoning is translated to a timed automaton which can then be used to produce actual running code (in a simulator). The right level of abstraction is important in order to deal with complexity issues.\n# 3. Development of Airplane Safety and Security\nOn 2001-09-11, four terrorist attacks took place in the USA, two on the two towers of the World Trade Center, one on the Pentagon, and in a fourth attack the airplane crashed when passengers tried to overcome the hijackers.2 Before these attacks, aircraft hijacking typically meant that the hijackers had some negotiable demands. Because of the risk to life for the people on board the aircraft, the standard approach was to enter negotiations and to avoid a resolution by force while the aircraft was in the air. In particular, also there was no secured door between the passenger compartment and the cockpit in airplanes; actually the door was occasionally open, even allowing passengers to get a glimpse of the cockpit during the flight. In Western countries there were no airplane hijackings with major loss of life between the 1970s and the 2001-09-11 attacks. This may have created in the USA and other countries a false sense of security. In the wake of the attacks a serious rethink of the security provision has happened. In particular, the cockpit doors were reinforced and made bullet-proof, making it nearly impossible to open by intruders [24].\nThese (and other) changes seem to have had the wanted effect, since in the time since the introduction of secured cockpit doors there were only 17 airplane hijackings or attempted airplane hijackings3 (as listed on [23]), all but one of them could be prevented from causing fatalities, and the one that did result in fatalities was an insider attack. One nearly successful airplane hijacking has been caused by the copilot who forced Ethiopian Airlines Flight 702 to land at Zurich airport in an attempt to blackmail asylum for himself in Switzerland. Also this airplane hijacking can be characterized as an insider attack since the attacker was part of the crew. The one major exception to the rule was Germanwings Flight 9525 on 2015-03-24, which was on the way from Barcelona to D\u00a8usseldorf. The aircraft was hijacked by the copilot who locked out the captain who had left the cabin. The pilot tried to regain access to the cockpit but did not succeed. Subsequently, the copilot brought the aircraft to a crash in which all 150 people on board died. Let us now look more closely into the door and its release mechanism.4 The door is operated by a switch from inside the cockpit (with three positions: \u201cunlock\u201d, \u201cnorm\u201d, \u201clock\u201d) and a keypad outside the cockpit. In order to gain access to the cockpit normally a crew member would use the inter-phone to contact a pilot in the cockpit to request access, then presses the hash key on the keypad, which triggers a buzzer in the cockpit, and the pilot releases the door using the switch to open the door (by keeping it in the \u201cunlock\u201d position). In case the pilot(s) is/are incapacitated the crew member outside the cockpit can enter an emergency code to open the door. After 30 seconds (during which the buzzer sounds in the cockpit) of no reaction by the pilots the crew member can open the door for five seconds. Since this access method could be used by a hijacker to force a crew member to open the door from outside the cockpit, the pilots can, within the 30 seconds between entering the emergency code and the release of the door, lock the cockpit door by putting the toggle button into the \u201clock\u201d mode. In that case the keypad is disabled for five minutes and the door can be opened during this time only from inside the cockpit by putting the button in the position \u201cunlock\u201d. The mechanism can be described on different levels and each level requires certain assumptions (for instance, that the door itself will withstand any physical force that may be exerted by an attacker). According to Occam\u2019s razor, we try to give a representation that is as easy as possible and still describes the situation in sufficient detail that the important aspects are modelled. A first approximation can be given by the timed finite state machine in Figure 1 with three states \u201cN\u201d, \u201cU\u201d, and \u201cL\u201d for \u201cnormal\u201d, \u201cunlocked\u201d, and \u201clocked\u201d, respectively. While time plays a role and it makes a difference for humans whether the door is locked for 300ms, 300s, or 300 minutes, we will abstract from this in the following formalization. During the fatal flight, the copilot used this locking mechanism to lock out the captain from the cockpit. While the mechanism has been successful so far from preventing any fatal attempt by an outsider to hijack an aircraft, the same mechanism prevented the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8bf2/8bf2929e-4b05-4b64-b40c-885d5991e118.png\" style=\"width: 50%;\"></div>\nFigure 1: A finite timed automaton to describe the lock mechanism of the door. In the three states, Nt, Us, and Ls (for normal, unlocked, locked at times t or s, respectively), the pilots can lock the door, or unlock the door at any time with immediate effect, or do nothing with respect to the door \u2013 indicated by \u03b5. Cabin crew can enter the pin of the door; entering an incorrect PIN corresponds to the empty action \u03b5. Entering the correct PIN has an effect only in the state Nt after 30 seconds in a time window of five seconds unless the pilots take the lock action. After no action for 300 seconds the Ls state is transformed to the Nt state. captain from re-entering the cockpit and take action to rescue the aircraft in this case.\n# 4. Isabelle Insider Framework\nBefore we formalize the airplane scenario in the section 5, we give first a brief introduction to Isabelle in this section; describe the Isabelle Insider framework with infrastructures, policies, actors, and insiders; and describe how Kripke Structures and CTL are modelled.\n4.1. Isabelle and Modular Reasoning Isabelle/HOL is an interactive proof assistant based on Higher Order Logic (HOL). Application specific logics are formalized into new theories extending HOL. They are called object-logics. Although HOL is undecidable and therefore proving needs human interaction, the reasoning capabilities are very sophisticated supporting \u201csimple\u201d, i.e., repetitive, tedious proof tasks to a level of complete automation. The use of HOL has the advantage that it enables expressing even the most complex application scenarios, conditions, and logical requirements and HOL simultaneously enables the analysis of the meta-theory. That is, repeating patterns specific to an application can be abstracted and proved once and for all. As an example, we will see how general preservation theorems of the state transition relation over the system graph and over policies can be proved as part of the insider framework and applied in concrete applications like the airplane scenario (see Section 4.3). An object-logic contains new types, constants, and definitions. These items reside in a theory file, for instance, the file Insider.thy contains the object-logic for insider threats described in the following paragraphs. This Isabelle Insider framework is a conservative extension of HOL. This means that our object logic does not introduce new axioms and hence guarantees consistency. Conceptually, new types are defined as subsets of existing types and\nproperties are proved using a one-to-one relationship to the new type from properties of the existing type. This process of conservative extension has been greatly facilitated by the datatype package that offers a restricted sort of simple recursive type definitions. Inductive definitions are a similar tool to define new predicates by a set of rules. Both extension features offer the specification of model elements with a theory of induction and exhaustion properties necessary for the proof of theorems over the model. Besides datatypes and inductive definitions, we make also use of local assumptions within locales. This is the reasoning process we propose as part of our methodology: the insider condition in Section 5.3 is not an axiom but is locally assumed to analyze the infrastructure\u2019s policies. This process has been conceived as Modular Reasoning in Isabelle [26] and implemented in the locales mechanism. Locales have been motivated by case studies from abstract algebra where proofs about algebraic structures \u2013 like groups, rings, or fields \u2013 frequently use assumptions \u2013 like \u2200x.x \u25e61 = x \u2013 that are valid within these algebraic structures but not outside. Rather than repeating those local assumptions continuously in large numbers of property statements and proofs, locales realize contexts in which those assumption can be used. Insider threat modelling and analysis using logics shows the same needs, since assumptions about actors are specific to a certain application\u2019s infrastructure. Moreover, the definition and the assumption of a locale are accessible later on, whenever the locale is invoked. But since they are local assumptions and definitions they do not endanger HOL\u2019s principle of conservative extension. We are going to use Isabelle syntax and concepts in this paper and will explain them when they are used.\n4.2. Infrastructures, Policies, Actors, and Insiders In the Isabelle/HOL theory for Insiders, one expresses policies over actions get, move, eval, and put. An actor may be enabled to \u2022 get data or physical items, like keys, \u2022 move to a location, \u2022 eval a program, \u2022 put data at locations or physical items \u2013 like airplanes \u2013 \u201cto the ground\u201d. The precise semantics of these actions is refined in the state transition rules for the concrete infrastructure. The framework abstracts from concrete data \u2013 actions have no parameters: datatype action = get | move | eval | put The human component is the Actor which is represented by an abstract type actor and a function Actor that creates elements of that type from identities (of type string): typedecl actor type_synonym identity = string consts Actor :: string \u21d2actor\nNote that it would seem more natural and simpler to just define actor as a datatype over identities with a constructor Actor instead of a simple constant together with a type declaration like, for example, in the Isabelle inductive package [27]. This would, however, make the constructor Actor an injective function by the underlying foundation of datatypes therefore excluding the fine grained modelling that is at the core of the insider definition: In fact, it defines the function Actor to be injective for all except insiders and explicitly enables insiders to have different roles by identifying Actor images. Atomic policies of type apolicy describe prerequisites for actions to be granted to actors given by pairs of predicates (conditions) and sets of (enabled) actions:\nFor example, the apolicy pair (\u03bbx. True, {move}) specifies that all actors are enabled to perform action move. To represent the macro level view seeing the actor within an infrastructure, we define a graph datatype igraph for infrastructures containing: a set of location pairs \u2013 the actual \u201cmap\u201d of the infrastructure and a list of actor identities associated with each node (location) in that graph.5 Moreover, an igraph contains a function associating actors with a pair of string lists: the first list describes the credentials an actor has while the second list defines the roles that an actor can take on. Finally, an igraph has a component assigning locations to a string list describing the state of the component. Slightly adapting the original insider framework, we needed to integrate the credentials, roles, and location state into the infrastructure graph to enable the dynamic view of state transition and Kripke structures (see Section 4.3). For each of the components there exist corresponding projection functions and predicates has and role to express that actors have credentials or that they can perform in specified roles, respectively, and isin to express that locations are in a specified state (see Appendix).\ndatatype igraph = Lgraph (location \u00d7 location)set location \u21d2identity list actor \u21d2(string list \u00d7 string list) location \u21d2string list\nInfrastructures combine an infrastructure graph of type igraph with a policy function that assigns local policies over a graph to each location of the graph, that is, it is a function mapping an igraph to a function from location to apolicy set. The Isabelle type [igraph, location] \u21d2apolicy set abbreviates igraph \u21d2(location \u21d2apolicy set) hence the stepwise application to igraph to return a function is possible.\npolicy valued function. This higher order parameter represents local policies, that is, maps from graph locations to policies for that location. In the following section, we will see how this higher order function enables proof of general preservation properties. Policies specify the expected behaviour of actors of an infrastructure. We define the behaviour of actors using a predicate enables: within infrastructure I, at location l, an actor h is enabled to perform an action a if there is a pair (p,e) in the local policy of l \u2013 delta I l projects to the local policy \u2013 such that action a is in the action set e and the policy predicate p holds for actor h. enables I l h a \u2261\u2203(p,e) \u2208delta I l. a \u2208e \u2227p h For example, the statement enables I l (Actor\u2019\u2019Bob\u2019\u2019) move is true if the atomic policy (\u03bbx. True, {move}) is in the set of atomic policies delta I l at location l in infrastructure I. Double quotes as in \u2019\u2019Bob\u2019\u2019 create a string in Isabelle/HOL. The human actor\u2019s level is modelled in the Isabelle Insider framework by assigning the individual actor\u2019s psychological disposition6 actor state to each actor\u2019s identity.\nThe values used for the definition of the types motivations and psy state (see Appendix) are based on a taxonomy from psychological insider research [7]. The transition to become an insider is represented by a Catalyst that tips the insider over the edge so he acts as an insider formalized as a \u201ctipping point\u201d predicate. To embed the fact that the attacker is an insider, the actor can then impersonate other actors. In the Isabelle Insider framework, the predicate Insider must be used as a locale assumption to enable impersonation for the insider: this assumption entails that an insider Actor \u2019\u2019Eve\u2019\u2019 can act like their alter ego, say Actor \u2019\u2019Charly\u2019\u2019 within the context of the locale. This is realized by the predicate UasI:\nNote that this predicate also stipulates that the function Actor is injective for any other than the identities a and b. This completion of the Actor function to an \u201calmost everywhere injective function\u201d is needed in some proofs (for an example see Section 6.4). We generalize here from other approaches on formal security analysis used in particular in security protocol verification known as the Dolev-Yao attacker model [28]. Our approach is more flexible because it addresses not just one specific attacker with a set range of abilities (eavesdrop, intercept, fake in Dolev-Yao) but more generally an insider, that is, someone who can impersonate any other actor and thereby attain any ability or access rights that exist in the system. This flexibility also allows modeling an attacker that \u201cimpersonates\u201d more than one actor to analyze collusions of insiders. In an earlier application of the Isabelle Insider framework [18], we illustrated this by a \u201cringing attack\u201d on the Cocaine Auction protocol.\n4.3. Kripke Structures and CTL The expressiveness of Higher Order Logic allows formalizing the notion of Kripke structures as sets of states and a transition relation over those in Isabelle. Moreover, temporal logic can be directly encoded using Isabelle\u2019s fixpoint definitions for each of the CTL operators [29]. Combining the two, we can then apply them as generic tools to analyze dynamically changing infrastructures with Insiders: we consider snapshots of infrastructures as states, use the actors and their action based behaviour definition to define a state transition, to then use temporal logic to express safety and security properties over dynamically changing infrastructures. This application will be demonstrated on our case study in Section 5.1. We briefly introduce here the necessary facts of Kripke structures and CTL showing how they are instantiated for Insiders. The transition relation on system states is defined as an inductive predicate called state transition in. It introduces the syntactic infix notation I \u2192n I\u2019 to denote that system state I and I\u2019 are in this relation.\nThe specification of the behaviour of actors in the Insider framework allows defining the rules for the state transition relation of the Kripke structure for infrastructures for each of the actions. Here is the rule for put. The expression h @G l says that h is at location l in the graph G. The next state construction I\u2019 uses the projections gra, agra, cgra, lgra to select the graph itself, the actors-location association, the credentials and roles, and the location state map, respectively. The rule expresses that an actor \u2013 who is at location l and is \u201cput\u201d-enabled in the infrastructure I by its policy at location l \u2013 can \u201cput\u201d the location into a state z in the successor state I\u2019 of the state transition for infrastructures. The double brackets enclose the preconditions of the meta-implication =\u21d2in Isabelle. A proposition \ufffdA; B\ufffd=\u21d2C simply abbreviates A=\u21d2(B=\u21d2C).\nput: \ufffdG = graphI I; h @G l; enables I l (Actor a) put; I\u2019 = Infrastructure (Lgraph (gra G)(agra G)(cgra G)((lgra G)(l := [z]))) (delta I) \ufffd=\u21d2I \u2192n I\u2019\nWe illustrate this particular rule here because we use it in the case study to express that an actor can put the airplane to the ground (see Section 5.2). We can already develop some very useful theorems for the state transition relation and Kripke structures. For example, the following lemma motivates why we define infrastructures as higher order functions where the local policies map the graph to a function over its locations: precisely because of that generality of the infrastructure constructor we can prove that state transitions do not change the policy delta \u2013 as one would expect.\nThe relation \u2192n^* is the reflexive transitive closure \u2013 an operator supplied by the Isabelle theory library \u2013 applied to the relation \u2192n. The proof of this invariant illustrates why for policy verification as we show here a deductive framework like Isabelle is well suited. To deduce the above theorem, we first prove that single step state transitions preserve the policy.\n\u2200I I\u2019. I \u2192n I\u2019 \u2212\u2192 delta I = delta I\u2019\nThen we use this lemma within an application of the induction for reflexive transitive closure of relations that is provided in the Isabelle theory library to infer the above lemma init state policy. Note that it is the specification in HOL of the state transition relation that provides the case analysis rule and the induction scheme as sound rules automatically generated from the definition. Branching time temporal logic CTL has been integrated by one of us as part of the Isabelle Insider framework [29] built over Kripke structures. A generic type state including a transition \u2192i is defined there using the concept of type classes in Isabelle. This type class state is then instantiated to the type of infrastructures thereby instantiating the state transition relation to \u2192n defined in the insider theory presented above (see Appendix). Thereby, the theory constructed and proved for this state transition \u2192i over a generic type state are transferred automatically to infrastructures and their transition relation \u2192n. Summarizing, the CTL-operators EX and AX express that property f holds in some or all next states, respectively.\nAX f \u2261{ s. {f0. s \u2192i f0 } \u2286f } EX f \u2261{ s. \u2203f0 \u2208f. s \u2192i f0 }\nThe CTL formula AG f means that on all paths branching from a state s the formula f i always true (G stands for \u2018globally\u2019). It can be defined using the Tarski fixpoint theory by applying the greatest fixpoint operator.\nAG f \u2261gfp(\u03bb Z. f \u2229AX Z)\nIn a similar way, the other CTL operators are defined. The formal Isabelle definition of what it means that formula f holds in a Kripke structure M for insiders can be stated as: the initial states of the Kripke structure init M need to be contained in the set of all states states M that imply f.\nM \u22a2f \u2261 init M \u2286{ s \u2208states M. s \u2208f }\nGiven some example policy \u2013 a predicate over an infrastructure using actors, actions, and their behaviours \u2013 we can then for example try to prove that this property holds generally by attempting the following proof in Isabelle.\nexample Kripke \u22a2AG example_policy\nIf the proof fails, the failed attempt will reveal conditions describing a state in the Kripke structure as well as actions leading to this state that identify an attack possibility. In the example in Section 5.2, this will be illustrated. Additionally, the failed attempts to prove the global validity also lead to identifying invariants of the system helping to establish decisive side conditions as well as identifying loopholes. The loopholes lead to a deeper insight into problems with the policy. By defining new locale assumptions and re-proving global properties, the newly found assumptions can be refined until the proof succeeds. This procedure will be illustrated on the airplane case study as well in Section 6.4. Note that all the definitions in the locale airplane that we use in Section 5 have been implemented as locale definitions using the locale keywords fixes and defines [30]. Thus they are accessible whenever the locale airplane is invoked. But since definitions are essentially abbreviations, they adhere to the principle of conservative extension of HOL not endangering consistency.\n# 5. Formalizing the Airplane Scenario\nIn this section we first provide the necessary infrastructure, then specify global and loca policies, and finally formalize insider attacks and safety and security.\n5.1. Formalization of Airplane Infrastructure and Properties We restrict the Airplane scenario to four identities: Bob, Charly, Alice, and Eve. Bob acts as the pilot, Charly as the copilot, and Alice as the flight attendant. Eve is an identity representing the malicious agent that can act as the copilot although not officially acting as an airplane actor. The identities that act legally inside the airplane infrastructure are listed in the set of airplane actors.\nfixes airplane_actors :: identity set defines airplane_actors_def: airplane_actors \u2261{\u2019\u2019Bob\u2019\u2019, \u2019\u2019Charly\u2019\u2019, \u2019\u2019Alice\u2019\u2019}\nIn the above locale definition we use the fixes keyword to introduce a locale constant with its type which is then specified by defines. In the following, we drop all these elements but the actual definition to make the exposition shorter and clearer. To represent the layout of the airplane, a simple architecture is best suited for the purpose of security policy verification. The locations we consider for the graph are cockpit, door, and cabin. They are defined as locale definitions and assembled in a set airplane locations\ncockpit \u2261Location 2 door \u2261Location 1 cabin \u2261Location 0 airplane_locations \u2261{ cabin, door, cockpit }\nThe actual layout and the initial distribution of the actors in the airplane infrastructure is defined by the following graph in which the actors Bob and Charly are in the cockpit and Alice is in the cabin.\nex_graph \u2261Lgraph {(cockpit, door),(door,cabin)} (\u03bb x. if x = cockpit then [\u2019\u2019Bob\u2019\u2019, \u2019\u2019Charly\u2019\u2019] else (if x = door then [] else (if x = cabin then [\u2019\u2019Alice\u2019\u2019] else [])))\nex_creds ex_locs\nThe two additional inputs ex creds and ex locs for the constructor Lgraph are the credential and role assignment to actors and the state function for locations introduced in Section 4.2, respectively. For the airplane scenario, we use the function ex creds to assign the roles and credentials to actors. For example, for Actor \u2019\u2019Bob\u2019\u2019 the following function returns the pair of lists ([\u2019\u2019PIN\u2019\u2019], [\u2019\u2019pilot\u2019\u2019]) assigning the credential PIN to this actor and designating the role pilot to him.\n(if x = Actor \u2019\u2019Bob\u2019\u2019 then ([\u2019\u2019PIN\u2019\u2019], [\u2019\u2019pilot\u2019\u2019]) else (if x = Actor \u2019\u2019Charly\u2019\u2019 then ([\u2019\u2019PIN\u2019\u2019],[\u2019\u2019copilot\u2019\u2019]) else (if x = Actor \u2019\u2019Alice\u2019\u2019 then ([\u2019\u2019PIN\u2019\u2019],[\u2019\u2019flightattendant\u2019\u2019]) else ([],[])))))\"\nex_locs \u2261\u03bb x. if x = door then [\u2019\u2019norm\u2019\u2019] else (if x = cockpit then [\u2019\u2019air\u2019\u2019] else [])\n5.2. Initial Global and Local Policies\n# 5.2. Initial Global and Local Policies\nIn the Isabelle Insider framework, we define a global policy reflecting the global safety and security goal and then break that down into local policies on the infrastructure. The verification will then analyze whether the infrastructure\u2019s local policies yield the global policy. Globally, we want to exclude attackers to ground the plane. In the formal model, landing the airplane results from an actor performing a put action (see Section 4.3) in the cockpit and thereby changing the state from air to ground. Therefore, we specify the global policy as \u201cno one except airplane actors can perform put actions at location cockpit\u201d by the following predicate over infrastructures I and actor identities a.\nWe next attempt to define the local policies for each location as a function mapping locations to sets of pairs: the first element of each pair for a location l is a predicate over actors specifying the conditions necessary for an actor to be able to perform the actions specified in the set of actions which is the second element of that pair. The local policy functions are additionally parameterized over an infrastructure graph G since this may dynamically change through the state transition.\nput: to perform a put action, that is, put the plane into a new position or put the lock, a actor must be at position cockpit, i.e., in the cockpit;\nmove: to perform a move action at location cockpit, that is, move into it, an actor must b at the position cabin, must be in possession of PIN, and door must be in state norm\nAlthough this policy abstracts from the buzzer, the 30 sec delay, and a few other technica details, it captures the essential features of the cockpit door. The graph, credentials, and features are plugged together with the policy into the infras tructure Airplane scenario.\n5.3. Insider Attack, Safety, and Security\nWe now first stage the insider attack and introduce basic definitions of safety and security for the airplane scenario. To invoke the insider within an application of the Isabelle Insider framework, we assume in the locale airplane as a locale assumption with assumes that the tipping point has been reached for Eve which manifests itself in her actor state assigned by the locale function astate\nastate x = (case x of \u2019\u2019Eve\u2019\u2019 \u21d2Actor_state depressed {revenge, peer_recognition} | _ \u21d2Actor_state happy {})\nIn addition, we state that she is an insider being able to impersonate Charly by locally assuming the Insider predicate. This predicate allows an insider to impersonate a set of other actor identities; in this case the set is singleton.\nassumes Eve_precipitating_event: tipping_point(astate \u2019\u2019Eve\u2019\u2019) assumes Insider_Eve : Insider \u2019\u2019Eve\u2019\u2019 {\u2019\u2019Charly\u2019\u2019}\nassumes Insider_Eve : Insider \u2019\u2019Eve\u2019\u2019 {\u2019\u2019Charly\u2019\u2019} Next, the process of analysis uses this assumption as well as the definitions of the previous section to prove security properties interactively as theorems in Isabelle. We use the strong insider assumption here up front to provide a first sanity check on the model by validating the infrastructure for the \u201cnormal\u201d case. We prove that the global policy holds for the pilot Bob. To illustrate a proof in Isabelle, we show the statement of the theorem including the Isabelle proof script. The system replies of the interaction with Isabelle are omitted but can be simply recreated by running that script. lemma ex inv: global_policy Airplane_scenario \u2019\u2019Bob\u2019\u2019 by (simp add: Airplane scenario def global policy def airplane actors def) The proof is finished with one complex step: unfold the definitions of the scenario given by Airplane scenario def and two other definitions and then apply the simplifier, an automated technique that applies equational (including conditional) rewriting to solve a goal. We can prove the same theorem for Charly who is the copilot in the scenario (omitting the proof and accompanying Isabelle commands).\nBut Eve is an insider and is able to impersonate Charly. She will ignore the global policy. This insider threat can now be formalized as an invalidation of the global company policy for \u2019\u2019Eve\u2019\u2019 in the following \u201cattack\u201d theorem named ex inv3: theorem ex_inv3: \u00ac global_policy Airplane_scenario \u2019\u2019Eve\u2019\u2019\nBut Eve is an insider and is able to impersonate Charly. She will ignore the global policy This insider threat can now be formalized as an invalidation of the global company policy for \u2019\u2019Eve\u2019\u2019 in the following \u201cattack\u201d theorem named ex inv3:\nThis theorem can be proved by first invoking the above insider assumption about Eve unfolding the corresponding underlying definitions provided in the Isabelle Insider framework but finally then again using the powerful simplification tactic simp. The attack theorem is proved in Isabelle: it says that Eve can get access to the cockpit and put the position to ground. In other words, Eve can crash the plane. The proof is very similar to proofs of comparable theorems in other applications of the Isabelle Insider framework, for instance, for the IoT [16] or for auctions [15], and can basically be copied from there just replacing local definition names. Summarizing, the insider assumption allows modeling that actors may be the same as other actors. Policies that are expressed according to roles thus apply to those insiders which \u2013 given that they are attackers \u2013 are harmful. Safety and security are sometimes introduced in textbooks as complementary properties, see, e.g., [31]. Safety expresses that humans and goods should be protected from negative effects caused by machines while security is the inverse direction: machines (computers) should be protected from malicious humans. Similarly, the following descriptions of safety and security in the airplane scenario also illustrate this complementarity: one says that the door must stay closed to the outside; the other that there must be a possibility to open it from the outside.\nSecurity I a \u2261isin (graphI I) door \u2019\u2019locked\u2019\u2019 \u2212\u2192\u00ac(enables I cockpit (Actor a) move)\nemma Security: Security Airplane_scenario \u2019\u2019Bob\u2019\nThe simple formalizations of safety and security enable proofs only over a particular state of the airplane infrastructure at a time but this is not enough since the general airplane structure is subject to state changes. For a general verification, we need to prove that the properties of interest are preserved under potential changes. Since the airplane infrastructure permits, for example, that actors move about inside the airplane, we need to verify safety and security properties in a dynamic setting. After all, the insider attack on Germanwings Flight 9525 appeared when the pilot had moved out of the cockpit. Furthermore, we want to redefine the policy into the two-person policy and examine whether safety and security are improved. For these reasons, we next apply the general Kripke structure mechanism introduced in Section 4.3 to the airplane scenario.\n# 6. Analysis of Safety and Security Properties\nIn this section we first introduce a Kripke structure to model state transitions in the airplane scenario. Then we formalize the two-person rule and look how this rule is related to the property that the airplane is not in danger with respect to an insider attack. We show that an additional assumption is necessary to prove this property. We conclude the section by summarizing the methodology.\n6.1. Kripke Structure for Airplane Scenario The state transition relation \u2192i introduced in Section 4.3 is generally defined for a type class state. Therefore, we can instantiate the state transition for the type infrastructure as \u2192n. Consequently, we can define the set of all states that are in the reflexive transitive closure of the infrastructure transition relation when starting in the infrastructure Airplane scenario as a locale definition Air states.\nFrom there, we can define a corresponding Kripke structure by applying the constructo Kripke to the above state set and the singleton set of Airplane scenario as the (only initial state.\nWe now illustrate how we can use this Kripke structure to explore and potentially inval idate the policy. The state of the infrastructure that represents the fatal state is when th pilot has moved out and the door is locked. We introduce a locale definition aid graph t represent the graph for this infrastructure.\nLgraph {(cockpit, door),(door,cabin)} (\u03bb x. if x = cockpit then [\u2019\u2019Charly\u2019\u2019] else (if x = door then [] else (if x = cabin then [\u2019\u2019Bob\u2019\u2019, \u2019\u2019Alice\u2019\u2019] else []))) ex_creds ex_locs\u2019\nraph \u2261Lgraph {(cockpit, door),(door,cabin)} (\u03bb x. if x = cockpit then [\u2019\u2019Charly\u2019\u2019] else (if x = door then [] else (if x = cabin then [\u2019\u2019Bob\u2019\u2019, \u2019\u2019A ex_creds ex_locs\u2019\n{(cockpit, door),(door,cabin)} (\u03bb x. if x = cockpit then [\u2019\u2019Charly\u2019\u2019] else (if x = door then [] else (if x = cabin then [\u2019\u2019Bob\u2019\u2019, ex_creds ex_locs\u2019\nFor the analysis of security, we need to ask whether this new infrastructure state Airplane in danger is reachable via the state transition relation from the initial state. It is. We can prove the following as a theorem in the locale airplane. theorem step_allr: Airplane_scenario \u2192n^* Airplane_in_danger\nAs the name of this theorem suggests it is the result of lining up a sequence of steps that lead from the initial Airplane scenario to that Airplane in danger state. In fact there are three steps via two intermediary infrastructure states Airplane getting in danger0 and Airplane getting in danger (see Appendix). The former encodes the state where Bob has moved to the cabin and the latter encodes the successor state in which additionally\nthe lock state has changed to locked. The definitions of these states are very similar to the above definition of Airplane in danger (see Appendix). The proof of the theorem step allr correspondingly lines up lemmas for each of the state transitions between the involved states. Once provided with these lemmas, the main proof is just one simplification with the underlying definition of the reflexive transitive closure of a relation. This is the advantage of using a richly equipped proof assistant: the theory library is well equipped with standard mathematics and the tactics work well on this basis. The only real work has to be done to prove the individual steps. However, although the proof scripts are a bit lengthy, this is just simple step by step unfolding of definitions and simplification. The only reason why it is not done in one step fully automatically is that some instantiations under existential quantifiers have to be inserted in the application of the state transition rules, like for example the rule put we have seen in Section 4.3. Using the formalization of CTL over Kripke structures introduced in Section 4.3, we can now transform the attack sequence represented implicitly by the above theorem step allr into a temporal logic statement. This attack theorem states that there is a path from the initial state of the Kripke structure Air Kripke on which eventually the global policy is violated by the attacker.\nThe proof uses the underlying formalization of CTL and the lemmas that are provided to evaluate the EF statement on the Kripke structure. However, the attack sequence is already provided by the previous theorem. So the proof just consists in supplying the step lemmas for each step and finally proving that for the state at the end of the attack path, i.e., for Airplane in danger, the global policy is violated. This proof corresponds precisely to the proof of the attack theorem ex inv3. It is not surprising that the security attack is possible in the reachable state Airplane in danger when it was already possible in the initial state. However, this statement is not satisfactory since the model does not take into account whether the copilot is on his own when he launches the attack. This is the purpose of the two-person rule which we want to investigate in more detail in this paper. Therefore, we next address how to add the two-person role to the model.\n# 6.2. Introduce Two-Person Rule\nTo express the rule that two authorized personnel must be present at all times in the cockpit, we define a second set of local policies. The following function realizes the twoperson constraint. It requests that the number of actors at the location cockpit in the graph G given as input must be at least two to enable actors at the location to perform the action put. Formally, we can express this here as 2 \u2264length(agra G cockpit) since we have all of arithmetic available (remember agra G y is the list of actors at location y in G introduced in Section 4.3).\n\u2200h \u2208set(agra G y). h \u2208airplane_actors), {put}), (\u03bb x. (\u2203n. n @G cabin \u2227Actor n = x) \u2227has (x, \u2019\u2019PIN\u2019\u2019) isin G door \u2019\u2019norm\u2019\u2019), {move})} else (if y = door then {(\u03bb x. ((\u2203n. n @G cockpit \u2227Actor n = x) \u22273 \u2264length(agra G cockpit)), {move})} else (if y = cabin then {(\u03bb x. \u2203n. n @G door \u2227Actor n = x), {move})} else {})))\nNote that the two-person rule requires three people to be at the cockpit before one of them can leave. This is formalized as a condition on the move action of location door. A move of an actor x in the cockpit to door is only allowed if three people are in the cockpit. Practically, it enforces a person, say Alice to first enter the cockpit before the pilot Bob can leave. However, this condition is necessary to guarantee that the two-person requirement for cockpit is sustained by the dynamic changes to the infrastructure state caused by actors\u2019 moves. A move to location cabin is only allowed from door so no additional condition is necessary here. What is stated informally above seems intuitive and quite easy to believe. However, comparing to the earlier formalization of this two-person rule [1], it appears that the earlier version did not have the additional condition on the action move to door. One may argue that in the earlier version the authors did not consider this because they had neither state transitions, Kripke structures, nor CTL to consider dynamic changes. However, in the current paper this additional side condition only occurred to us when we tried to prove the following invariant which is needed in a subsequent security proof.\nlemma two_person_inv1: Airplane_not_in_danger_init \u2192n^* I =\u21d22 \u2264length (agra (graphI I) cockpit)\nThis proof requires an induction over the state transition relation starting in the infras tructure state Airplane not in danger init with Charly and Bob in the cockpit and th two-person policy in place.\nAirplane_not_in_danger_init \u2261Infrastructure ex_graph local_policies_four_eyes\nThe corresponding Kripke structure of all states originating in this infrastructure state is defined as Air tp Kripke. Within the induction for the proof of the above two person inv1, a preservation lemma is required that proves that if the condition 2 \u2264length (agra (graphI I) cockpit) holds for I and I \u2192n I\u2019 then it also holds for I\u2019. The preservation lemma is actually trickier to prove. It uses a case analysis over all the transition rules for each action. The rules for put and get are easy to prove for the user as they are solved by the simplification tactic automatically. The case for action move is the difficult case. Here we actually need to use the precondition of the policy for location door in order to prove that the two-person invariant is preserved by an actor moving out of the cockpit. In this case, we need for example, invariants like the following lemma that shows that in any\ninfrastructure state originating from Airplane not in danger init actors only ever appear in one location and they do not appear more than once in a location \u2013 which is expressed in a predicate nodup (see Appendix). The following lemma is an instantiation of a similar general lemma proved for all Kripke structures \u2013 similar to the lemma init state policy mentioned in Section 4.3.\nlemma actors_unique_loc_aid_step: Airplane_not_in_danger_init \u2192n^* I =\u21d2\u2200a. (\u2200l l\u2019. a @graphI I) l \u2227a @graphI I l\u2019 \u2212\u2192l = l\u2019 \u2227(\u2200l. nodup a (agra (graphI I) l))\n6.3. Revealing Necessary Assumption by Proof Failure We would expect \u2013 and this has in fact been presented in [1] \u2013 that the two-person rule guarantees the absence of the insider attack. This is indeed a provable fact in the following state Airplane not in danger defined similar to Airplane in danger from Section 6.1 but using the two-person policy.\nSo, in the state Airplane not in danger with the two-person rule, there seems to be no danger. But this is precisely the scenario of the suicide attack! Charly is on his own in the cockpit \u2013 why then does the two-person rule imply he cannot act? The state Airplane not in danger defined in the earlier formalization is mis-named: it uses the graph aid graph to define a state in which Bob has left the cockpit and the door is locked. Since there is only one actor present, the precondition of the local policy for cockpit is not met and hence the action put is not enabled for actor Charly. Thus, the policy rule for cockpit is true because the precondition of this implication \u2013 two people in the cockpit \u2013 is false, and false implies anything: seemingly a disastrous failure of logic. Fortunately, the above theorem has been derived in a preliminary model only [1] in which state changes were not integrated yet and which has been precisely for this reason recognized as inadequate. Now, with state changes in the improved model, we have proved the two-person invariant two person inv1. Thus, we can see that the system \u2013 if started in Airplane not in danger init \u2013 cannot reach the mis-named state Airplane not in danger in which Charly is on his own in the cockpit. However, so far, no such general theorem has been proved yet. We only used CTL to discover attacks using EF formulas. What we need for general security and what we consider next is to prove a global property with the temporal operator AG that proves that from a given initial state the global policy holds in all (A) states globally (G). As we have seen in the previous section when looking at the proof of two person inv1, it is not evident and trivial to prove that all state changes preserve security properties. However, even this invariant does not suffice. Even if the two-person rule is successfully enforced in a state, it is on its own still not sufficient. When we try to prove\nAir_tp_Kripke \u22a2AG {x. global_policy x \u2019\u2019Eve\u2019\u2019} for the Kripke structure Air tp Kripke of all states originating in Airplane not in danger i we cannot succeed. In fact, in that Kripke structure there are infrastructure states where the insider attack is possible. Despite the fact that we have stipulated the two-person rule as part of the new policy and despite the fact that we can prove that this policy is preserved by all state changes, the rule has no consequence on the insider. Since Eve can impersonate the copilot Charly, whether two people are in the cockpit or not, the attack can happen. What we realize through this failed attempt to prove a global property is that the policy formulation does not entail that the presence of two people in itself actually disables an attacker. This insight reveals a hidden assumption. Formal reasoning systems have the advantage that hidden assumptions must be made explicit. In human reasoning they occur when people assume a common understanding, which may or may not be actually the case. In the case of the rule above, its purpose may lead to an assumption that humans accept but which is not warranted. We use again a locale definition to encode this intentional understanding of the twoperson rule. The formula foe control encodes for any action c at a location l that if there is an Actor x that is not an insider, that is, is not impersonated by Eve, then the insider is disabled for that action c.\n6.4. Proving Security in Refined Model Having identified the missing formulation of the intentional effects of the two-person rule, we can now finally prove the general security property using the above locale definition. We assume in the locale airplane an instance of foe control for the cockpit and the action put. assumes cockpit_foe_control: foe_control cockpit put With this assumption, we are now able to prove that for all infrastructure states of the system airplane originating in state Airplane not in danger init Eve cannot put the airplane to the ground. theorem Four_eyes_no_danger: Air_tp_Kripke \u22a2AG {x. global_policy x \u2019\u2019Eve\u2019\u2019} The proof uses as a key lemma that within Kripke structure Air tp Kripke there is always someone in the cockpit who is not the insider. lemma tp_imp_control: Airplane_not_in_danger_init \u2192n^* I =\u21d2\u2203x. x @I cockpit \u2227Actor x \u0338= Actor \u2019\u2019Eve\u2019\u2019 This lemma can be proved by using the invariant that always two people are in the cockpit. However, the invariant two person inv1 cannot be used directly since it is a lemma over lists rather than sets. Instead of re-formulating the model with sets, we use a simple fact about sets and lists.\n6.4. Proving Security in Refined Model Having identified the missing formulation of the intentional effects of the two-person rule, we can now finally prove the general security property using the above locale definition. We assume in the locale airplane an instance of foe control for the cockpit and the action put.\n6.4. Proving Security in Refined Model Having identified the missing formulation of the intentional effects of the two-person rule, we can now finally prove the general security property using the above locale definition. We assume in the locale airplane an instance of foe control for the cockpit and the action put. assumes cockpit_foe_control: foe_control cockpit put With this assumption, we are now able to prove that for all infrastructure states of the system airplane originating in state Airplane not in danger init Eve cannot put the airplane to the ground. theorem Four_eyes_no_danger: Air_tp_Kripke \u22a2AG {x. global_policy x \u2019\u2019Eve\u2019\u2019}\n# 7. Discussion and Conclusions\nIn this section, we briefly discuss limitations and approaches to developing airplane policies, summarize the contributions of the paper, and present some concluding remarks.\nIn order to prove consequences of policies certain assumptions have to be made and it is important to analyze the assumptions, since any consequences hold only with respect to the assumptions. An important assumption is that the airplane is initially not in danger, Airplane not in danger init. That is, if the assumption is violated initially (before the airplane leaves the ground) then we cannot conclude that the airplane will not be in danger later. Current policies do not assume that the cockpit door must be locked before passengers board the airplane. Actually, often it is still open and closed only later. This means that an attack by an outsider during this phase cannot be ruled out. For airlines it is an important question whether they should follow a two-person rule and as a consequence of the events on 2015-03-24 with the Germanwings flight 9525 a number of countries recommended the rule and a number of airlines7 introduced them \u2013 without consideration of possible negative consequences. In a more recent development, some German airlines have rescinded the two-person rule,8 since the introduction has also the disadvantage that it takes considerably longer for one person to leave and another to enter the cockpit than just for one person to leave. This means that with the two-person policy, each time a pilot/co-pilot leaves the cockpit the door is open for much longer than without the policy, hence increasing the risk of a hostile attack. Up to now no good improvement on the protocol for the door has been found, since any change seems to be paired with substantial disadvantage as well. We have not formally modelled the situation and the reasoning behind this. We do this informally here. If we assume p0, the probability that one pilot is an insider; p1, the probability that a terrorist can use the time the door is open to enter the cockpit following the one-person rule and take over the plane; and p2, the corresponding probability that a terrorist can enter the cockpit following the two-person rule. Fortunately all these probabilities are very small. This means, however, that there is no reliable way to determine their values. It seems obvious that p2 > p1, it can be assumed that p2 is considerably bigger than p1.9 With these probabilities we get that an aircraft is in danger according to the one-person rule: probability(insider OR terrorist) = p0 + p1 \u2212probability(insider AND terrorist) \u2248p0 + p1 With the two-person rule: probability(insider OR terrorist) = 0 + p2 \u22120 \u00b7 p2 = p2 The second equation of the first case assumes that the events that a pilot is an insider and that a terrorist can use the one-person rule to enter the cockpit are independent. The approximate equality follows since both p0 and p1 are very small, that is, the size of p0 \u00b7 p1 is\n7This is reported, for instance, in an article of 2015-03-26 by Reuters, http://www.reuters.com/article/france-crash-cockpits-idUSL6N0WS6GR20150326. 8See https://phys.org/news/2017-04-german-airlines-scrap-two-person-cockpit.html and https://www.swiss.com/corporate/EN/media/newsroom/press-releases/media-release-20170428. 9See, https://www.easa.europa.eu/newsroom-and-events/news/minimum-cockpit-occupancy-easaissues-revised-safety-information-bulletin\nnegligible compared to either p0 or p1. In the second case it is assumed that the probability that an insider can harm the plane if not on their own is 0. In order to follow a rational policy, an airline should look at the relationship of the probabilities in the two cases, that is, between p2 and p0 + p1. It should go for the smaller probability. If the probability of a terrorist getting in following the two-person rule is greater than that of getting in following the one-person rule plus the probability of an insider doing harm then follow the one-person rule, else the two-person rule. However, as we have mentioned above it is very difficult to determine these probabilities. Hence, when it comes to defining policies, it looks much more fruitful to consider possibilistic specifications of systems, actors, and their possible behaviours in order to understand better the shortcomings and possible glitches when imposing policies as security rules than to apply probabilistic reasoning.\n# 7.2. Advantages and drawbacks of the approach\nWhile the detection of attacks is a very useful feature, the use of a heavier, that is, a more labour intensive analysis, like interactive theorem proving with Isabelle may seem to be an academic exercise. Particularly in the light of related logical analysis techniques like model checking or SMT (Satisfiability Modulo Theory) solving, the interaction might appear like an unnecessary limitation. However, as the foundations of logic and computation theory teach us, properties may become undecidable as soon as higher order elements are in the models. And this is the case when we want to express policies over infrastructures, and prove properties that often necessitate proofs of invariants which can only be proved by induction. Invalidation of policies of infrastructures to detect insider threats [12] uses model checking to discover paths to system states in which the security policy is violated. However, the restrictions on the description of infrastructure models in model checkers renders them insufficient for our purposes: we need to consider a variety of actors and restrictions like the number of people in locations and changing configurations created by actors moving about between them. Model checking explores the entire state space of systems for all possible instantiations of all state variables. This process \u2013 if implemented as a decision procedure \u2013 requires finite models and is exponential in the number of state components \u2013 a problem known as state explosion. Due to the resulting restrictions on the state specification it is not possible to use general arithmetic expressions \u2013 for example using state variables over infinite data domains like x < 2 for x being an integer \u2013 nor to describe security policies using higher order predicates \u2013 for example using expressions like \u201ca is at location l\u201d as an input for a graph based policy model generalizing over actors a and locations l of an infrastructure. Similarly, SMT solvers use a complete enumeration of all possible interpretations of logical formulas and satisfiability checking has only recently been extended to higher order logic in an efficient way [32]. The application of model checkers would require to apply abstraction and considerable work would need to be done to find a suitable level of abstraction. The formalization in the rich language of Isabelle/HOL looks cognitively more adequate and allows to more easily experiment with different policies.\n# 7.3. Summary of Contributions\n7.3. Summary of Contributions The current paper presents a complete formalization and analysis of preliminary work previously presented as a workshop paper [1] on examining insider attacks on airplanes. The main improvements and additional contributions over this and other previous works with the Isabelle Insider framework are:\n# \u2022 We have identified a crucial implicit condition intentional in the two-person rule  malizing it as foe control in our model.\n\u2022 We have shown for the first time how invariant reasoning and induction can be used to prove that a global policy holds over a Kripke structure in the Isabelle Insider framework. By using an instance of foe control we showed that the two-person rule provides insider security.\nIsabelle and other HOL tools support a rich set of type definitions and inductive predicates. This work has shown the benefits of using these definition tools as a natural match for concepts in the application. Without such well-founded definitions, proof rules that are used on features of a model cannot be considered as mathematically sound. Datatypes and induction on predicates are derived from first principles like fixpoint induction and datatype isomorphism in HOL. This is known as the principle of conservative extension. It is this principle that adds a special quality of mathematical soundness to Isabelle formalizations. The complexity of the application domain of infrastructures including actors and policies necessitated the use of higher order functions to represent policies. We have illustrated this necessity by showing some meta-level invariants for the insider framework. The proof of invariants needs induction. As discussed in Section 2, the Isabelle Insider framework has been initially designed and validated on the insider threat patterns identified by CERT [4]. The present application of the Isabelle Insider framework is based on the same insider model but greatly enhances it by the generic state transition model based on Kripke structures and CTL. The definition of the airplane application uses earlier insider applications as a blueprint. Hence basic proofs can be reused. The current application additionally provides reusable proofs at the level of the insider theory itself (for example, preservation of the local policies by the state transition) and shows how proofs about the dynamic behaviour of the application are conducted. This can similarly inspire future applications allowing reusability of the Isabelle Insider framework.\n# 7.4. Conclusions\nThe current work has picked up on the challenging earlier application [1] on investigating airplane safety and security in the presence of insiders. We have successfully proved the major observation of that earlier paper: a thorough logical analysis of the airplane scenario requires the exploration of the state space for all possible changes to the state. Integrating the extensions to Kripke structures and CTL in our model we were now able to explore the airplane scenario thoroughly and completely. The analysis in the interactive theorem prover Isabelle has shown that earlier results were partly misleading because security results were only relating statically to one specific state at a time. In the current version, the use of an inductive state transition relation enables us to prove invariants and most prominently revealed a missing assumption when clarifying the policy specification. This shows that a rigorous validation as part of the process in the development of new airplane policy is very import. Finally, we were able to establish the proof of the global security property in presence of an insider. As a by-product, the extensive study has provided general improvements to the Isabelle Insider framework.\n# References\n31] D. Gollmann, Computer Security, Wiley, 2008. 32] H. Barbosa, A. Reynolds, D. El Ouraoui, C. Tinelli, C. Barrett, Extending SMT solvers to higher-order logic, in: P. Fontaine (Ed.), 27th International Conference on Automated Deduction, CADE-27, Vol. 11716 of LNAI, Springer, 2019, pp. 35\u201354.\n# Appendix A. Isabelle Code Extracts\n# Appendix A.1. Kripke Structures and CTL theory MC imports Main\ndefinition monotone :: (\u2032a set \u21d2\u2032a set) \u21d2bool where monotone \u03c4 \u2261(\u2200p q. p \u2286q \u2212\u2192\u03c4 p \u2286\u03c4 q ) lemma monotoneE: monotone \u03c4 =\u21d2p \u2286q =\u21d2\u03c4 p \u2286\u03c4 q \u27e8proof\u27e9 lemma lfp1: monotone \u03c4 \u2212\u2192(lfp \u03c4 = \ufffd{Z. \u03c4 Z \u2286Z}) \u27e8proof\u27e9 lemma gfp1: monotone \u03c4 \u2212\u2192(gfp \u03c4 = \ufffd{Z. Z \u2286\u03c4 Z}) \u27e8proof\u27e9 primrec power :: [\u2032a \u21d2\u2032a, nat] \u21d2(\u2032a \u21d2\u2032a) ((- \u02c6 -) 40) where power-zero: (f \u02c6 0) = (\u03bb x. x) | power-suc: (f \u02c6 (Suc n)) = (f o (f \u02c6 n)) lemma predtrans-empty: assumes monotone \u03c4 shows \u2200i. (\u03c4 \u02c6 i) ({}) \u2286(\u03c4 \u02c6(i + 1))({}) proof (rule allI , induct-tac i) show (\u03c4 \u02c6 0::nat) {} \u2286(\u03c4 \u02c6 (0::nat) + (1::nat)) {} by sim next show \ufffd(i::nat) n::nat. (\u03c4 \u02c6 n) {} \u2286(\u03c4 \u02c6 n + (1::nat))  =\u21d2(\u03c4 \u02c6 Suc n) {} \u2286(\u03c4 \u02c6 Suc n + (1::nat)) {} proof \u2212\n28\nfix i n assume a : (\u03c4 \u02c6 n) {} \u2286(\u03c4 \u02c6 n + (1::nat)) {} have (\u03c4 ((\u03c4 \u02c6 n) {})) \u2286(\u03c4 ((\u03c4 \u02c6 (n + (1 :: nat))) {})) using assms apply (rule monotoneE) by (rule a) thus (\u03c4 \u02c6 Suc n) {} \u2286(\u03c4 \u02c6 Suc n + (1::nat)) {} by simp qed qed lemma infchain-outruns-all: assumes finite (UNIV :: \u2032a set) and \u2200i :: nat. (\u03c4 \u02c6 i) ({}:: \u2032a set) \u2282(\u03c4 \u02c6 i + (1 :: nat)) {} shows \u2200j :: nat. \u2203i :: nat. j < card ((\u03c4 \u02c6 i) {}) \u27e8proof\u27e9 lemma no-infinite-subset-chain: assumes finite (UNIV :: \u2032a set) and monotone (\u03c4 :: (\u2032a set \u21d2\u2032a set)) and \u2200i :: nat. ((\u03c4 :: \u2032a set \u21d2\u2032a set) \u02c6 i) {} \u2282(\u03c4 \u02c6 i + (1 :: nat)) ({} :: \u2032a set) shows False \u27e8proof\u27e9 lemma finite-fixp: assumes finite(UNIV :: \u2032a set) and monotone (\u03c4 :: (\u2032a set \u21d2\u2032a set)) shows \u2203i. (\u03c4 \u02c6 i) ({}) = (\u03c4 \u02c6(i + 1))({}) \u27e8proof\u27e9 lemma predtrans-UNIV : assumes monotone \u03c4 shows \u2200i. (\u03c4 \u02c6 i) (UNIV ) \u2287(\u03c4 \u02c6(i + 1))(UNIV ) \u27e8proof\u27e9 lemma down-chain-reaches-empty: assumes finite (UNIV :: \u2032a set) and monotone (\u03c4 :: \u2032a set \u21d2\u2032a set) and (\u2200i :: nat. ((\u03c4 :: \u2032a set \u21d2\u2032a set) \u02c6 i + (1 :: nat)) UNIV \u2282(\u03c4 \u02c6 i) UNIV ) shows \u2203(j :: nat). (\u03c4 \u02c6 j) UNIV = {} \u27e8proof\u27e9 lemma lfp-loop: assumes finite (UNIV :: \u2032b set) and monotone (\u03c4 :: (\u2032b set \u21d2\u2032b set)) shows \u2203n . lfp \u03c4 = (\u03c4 \u02c6 n) {} \u27e8proof\u27e9\n# assumes finite (UNIV :: \u2032b set) and monotone (\u03c4 :: (\u2032b set \u21d2\u2032b set)) shows \u2203n . gfp \u03c4 = (\u03c4 \u02c6 n)(UNIV :: \u2032b set) \u27e8proof\u27e9\n(\ufffdx::a::state. x \u2208((\u03bb Z. (f ::\u2032a::state set) \u222aEX \u2032 Z)(EF f \u2229{x::\u2032a::state. (P::\u2032a::state \u21d2bool) x})) =\u21d2 P x) =\u21d2 P a \u27e8proof\u27e9 lemma EF-step-star-rev[rule-format]: x \u2208EF s =\u21d2(\u2203y \u2208s. x \u2192i\u2217y) \u27e8proof\u27e9 lemma EF-step-inv: (I \u2286{sa::\u2032s :: state. (\u2203i::\u2032s\u2208I . i \u2192i\u2217sa) \u2227sa \u2208EF s}) =\u21d2\u2200x \u2208I . \u2203y \u2208s. x \u2192i\u2217y \u27e8proof\u27e9 lemma AG-in-lem: x \u2208AG s =\u21d2x \u2208s \u27e8proof\u27e9 lemma AG-step: y \u2192i z =\u21d2y \u2208AG s =\u21d2z \u2208AG s \u27e8proof\u27e9 lemma AG-all-s: x \u2192i\u2217y =\u21d2x \u2208AG s =\u21d2y \u2208AG s \u27e8proof\u27e9 lemma AG-imp-notnotEF: I \u0338= {} =\u21d2((Kripke {s :: (\u2032s :: state). \u2203i \u2208I . (i \u2192i\u2217s)} (I :: (\u2032s :: state)set) \u22a2AG s)) =\u21d2 (\u00ac(Kripke {s :: (\u2032s :: state). \u2203i \u2208I . (i \u2192i\u2217s)} (I :: (\u2032s :: state)set) \u22a2EF (\u2212s))) \u27e8proof\u27e9 end Appendix A.2. Insider Framework theory AirInsider imports MC begin datatype action = get | move | eval |put typedecl actor consts Actor :: string \u21d2actor type-synonym identity = string type-synonym policy = ((actor \u21d2bool) \u2217action set) datatype location = Location nat datatype igraph = Lgraph (location \u2217location)set location \u21d2identity list actor \u21d2(string list \u2217string list) location \u21d2string list\n# definition tipping-point :: actor-state \u21d2bool where tipping-point a \u2261((motivation a \u0338= {}) \u2227(happy \u0338= psy-state a))\ninstantiation infrastructure :: state begin definition state-transition-infra-def : (i \u2192i i \u2032) = (i \u2192n (i \u2032 :: infrastructure)) instance by (rule MC.class.MC.state.of-class.intro) definition state-transition-in-refl ((- \u2192n\u2217-) 50) where s \u2192n\u2217s \u2032 \u2261((s,s \u2032) \u2208{(x,y). state-transition-in x y}\u2217) lemma move-graph-eq: move-graph-a a l l g = g by (simp add: move-graph-a-def , case-tac g, force) lemma delta-invariant: \u2200z z \u2032. z \u2192n z \u2032 \u2212\u2192delta(z) = delta(z \u2032) by (clarify, erule state-transition-in.cases, simp+) lemma init-state-policy: [[ (x,y) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217]] = delta(x) = delta(y) proof \u2212 have ind: (x,y) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 \u2212\u2192delta(x) = delta(y) proof (insert assms, erule rtrancl.induct) show (\ufffda::infrastructure. (\u2200(z::infrastructure)(z \u2032::infrastructure). (z \u2192n z \u2032) \u2212\u2192(delta z = delta z \u2032)) =\u21d2 (((a, a) \u2208{(x ::infrastructure, y :: infrastructure). x \u2192n y}\u2217) \u2212\u2192 (delta a = delta a))) by (rule impI , rule refl)\n# datatype doorstate = locked | norm | unlocked datatype position = air | airport | ground\nfixes ex-locs :: location \u21d2string list defines ex-locs-def : ex-locs \u2261(\u03bb x. if x = door then [\u2032\u2032norm \u2032\u2032] else (if x = cockpit then [\u2032\u2032air \u2032\u2032] else []))\n# else (if x = door then [\u2032\u2032Bob \u2032\u2032] else (if x = cabin then [\u2032\u2032Alice \u2032\u2032] else []))) ex-creds ex-locs\n# fixes local-policies :: [igraph, location] \u21d2policy set defines local-policies-def : local-policies G \u2261 (\u03bb y. if y = cockpit then {(\u03bb x. (? n. (n @G cockpit) \u2227Actor n = x), {put}), (\u03bb x. (? n. (n @G cabin) \u2227Actor n = x \u2227has G (x, \u2032\u2032PIN \u2032\u2032) \u2227isin G door \u2032\u2032norm \u2032\u2032),{move})\n# else (if y = door then {(\u03bb x. True, {move}), (\u03bb x. (? n. (n @G cockpit) \u2227Actor n = x), {put})} else (if y = cabin then {(\u03bb x. True, {move})} else {})))\nfixes Airplane-scenario :: infrastructure (structure) defines Airplane-scenario-def : Airplane-scenario \u2261Infrastructure ex-graph local-policies fixes Airplane-in-danger :: infrastructure defines Airplane-in-danger-def : Airplane-in-danger \u2261Infrastructure aid-graph local-policies\n# fixes Security :: [infrastructure, identity] \u21d2bool defines Security-def : Security I a \u2261(isin (graphI I ) door \u2032\u2032locked \u2032\u2032) \u2212\u2192\u00ac(enables I cockpit (Actor a) move)\nfixes astate:: identity \u21d2actor-state defines astate-def : astate x \u2261(case x of\n\u2032\u2032Eve \u2032\u2032 \u21d2Actor-state depressed {revenge, peer-recognition} | - \u21d2Actor-state happy {}) assumes Eve-precipitating-event: tipping-point (astate \u2032\u2032Eve \u2032\u2032) assumes Insider-Eve: Insider \u2032\u2032Eve \u2032\u2032 {\u2032\u2032Charly \u2032\u2032} astate assumes cockpit-foe-control: foe-control cockpit put begin lemma Safety: Safety Airplane-scenario (\u2032\u2032Alice \u2032\u2032) \u27e8proof\u27e9 lemma Security: Security Airplane-scenario s \u27e8proof\u27e9 lemma step0r: Airplane-scenario \u2192n\u2217Airplane-getting-in-danger0 \u27e8proof\u27e9 lemma step1r: Airplane-getting-in-danger0 \u2192n\u2217Airplane-getting-in-danger \u27e8proof\u27e9 lemma step2r: Airplane-getting-in-danger \u2192n\u2217Airplane-in-danger \u27e8proof\u27e9 theorem step-allr: Airplane-scenario \u2192n\u2217Airplane-in-danger \u27e8proof\u27e9 theorem aid-attack: Air-Kripke \u22a2EF ({x. \u00ac global-policy x \u2032\u2032Eve \u2032\u2032}) proof (simp add: check-def Air-Kripke-def , rule conjI ) show Airplane-scenario \u2208Air-states by (simp add: Air-states-def state-transition-in-refl-def ) next show Airplane-scenario \u2208EF {x::infrastructure. \u00ac global-policy x \u2032\u2032Eve  by (rule EF-lem2b, subst EF-lem000, rule EX-lem0r, subst EF-lem000, rule unfold state-transition-infra-def , rule step0, rule EX-lem0r, rule-tac y = Airplane-getting-in-danger in EX-step, unfold state-transition-infra-def , rule step1, subst EF-lem000, rule EX-lem rule-tac y = Airplane-in-danger in EX-step, unfold state-transition-infra-d rule step2, rule CollectI , rule ex-inv4) qed lemma actors-unique-loc-base: assumes I \u2192n I \u2032 and (\u2200l l \u2032. a @graphI I l \u2227a @graphI I l \u2032 \u2212\u2192l = l \u2032)\u2227 (\u2200l. nodup a (agra (graphI I ) l)) shows (\u2200l l\u2032. a @ l \u2227a @ l\u2032 \u2212\u2192l = l\u2032) \u2227\n# \u2032\u2032Eve \u2032\u2032 \u21d2Actor-state depressed {revenge, peer-recognition | - \u21d2Actor-state happy {}) assumes Eve-precipitating-event: tipping-point (astate \u2032\u2032Eve \u2032\u2032) assumes Insider-Eve: Insider \u2032\u2032Eve \u2032\u2032 {\u2032\u2032Charly \u2032\u2032} astate assumes cockpit-foe-control: foe-control cockpit put\nlemma actors-unique-loc-step: assumes (I , I \u2032) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 and \u2200a. (\u2200l l \u2032. a @graphI I l \u2227a @graphI I l \u2032 \u2212\u2192l = l \u2032)\u2227 (\u2200l. nodup a (agra (graphI I ) l)) shows \u2200a. (\u2200l l \u2032. a @graphI I \u2032 l \u2227a @graphI I \u2032 l \u2032 \u2212\u2192l = l \u2032) \u2227 (\u2200l. nodup a (agra (graphI I \u2032) l)) \u27e8proof\u27e9 lemma two-person-inv: fixes z z \u2032 assumes (2::nat) \u2264length (agra (graphI z) cockpit) and nodes(graphI z) = nodes(graphI Airplane-not-in-danger-init) and delta(z) = delta(Airplane-not-in-danger-init) and (Airplane-not-in-danger-init,z) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 and z \u2192n z \u2032 shows (2::nat) \u2264length (agra (graphI z \u2032) cockpit) \u27e8proof\u27e9 lemma airplane-actors-inv: assumes (Airplane-not-in-danger-init,z) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 shows \u2200h::char list\u2208set (agra (graphI z) cockpit). h \u2208airplane-actors \u27e8proof\u27e9 lemma Eve-not-in-cockpit: (Airplane-not-in-danger-init, I ) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217=\u21d2 x \u2208set (agra (graphI I ) cockpit) =\u21d2x \u0338= \u2032\u2032Eve \u2032\u2032 \u27e8proof\u27e9 lemma tp-imp-control: assumes (Airplane-not-in-danger-init,I ) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 shows (? x :: identity. x @graphI I cockpit \u2227Actor x \u0338= Actor \u2032\u2032Eve \u2032\u2032) \u27e8proof\u27e9 lemma Fend-2: (Airplane-not-in-danger-init,I ) \u2208{(x::infrastructure, y::infrastructure). x  y}\u2217=\u21d2 \u00ac enables I cockpit (Actor \u2032\u2032Eve \u2032\u2032) put by (insert cockpit-foe-control, simp add: foe-control-def , drule-tac x = I in spec, erule mp, erule tp-imp-control) theorem Four-eyes-no-danger: Air-tp-Kripke \u22a2AG ({x. global-policy x \u2032\u2032Eve \u2032\u2032}) proof (simp add: Air-tp-Kripke-def check-def , rule conjI ) show Airplane-not-in-danger-init \u2208Air-tp-states\n# lemma actors-unique-loc-step: assumes (I , I \u2032) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 and \u2200a. (\u2200l l \u2032. a @graphI I l \u2227a @graphI I l \u2032 \u2212\u2192l = l \u2032)\u2227 (\u2200l. nodup a (agra (graphI I ) l)) shows \u2200a. (\u2200l l \u2032. a @graphI I \u2032 l \u2227a @graphI I \u2032 l \u2032 \u2212\u2192l = l \u2032) \u2227 (\u2200l. nodup a (agra (graphI I \u2032) l))\n# xa \u2208Collect (state-transition x) =\u21d2 (Airplane-not-in-danger-init, x) \u2208{(x::infrastructure, y::infrastructure). x \u2192n y}\u2217 by (erule conjE, simp add: Air-tp-Kripke-def Air-tp-states-def state-transition-in-refl-def )+\ninterpretation airplane airplane-actors airplane-locations cockpit door cabin global-policy ex-creds ex-locs ex-locs \u2032 ex-graph aid-graph aid-graph0 agid-graph local-policies local-policies-four-eyes Airplane-scenario Airplane-in-danger Airplane-getting-in-danger0 Airplane-getting-in-danger Air-states Air-Kripke Airplane-not-in-danger Airplane-not-in-danger-init Air-tp-states Air-tp-Kripke Safety Security foe-control astate\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of airplane security, particularly in the context of insider threats, highlighting the need for improved verification methods that integrate human factors into security policies.",
        "problem": {
            "definition": "The problem involves verifying security policies in aviation to prevent insider attacks, specifically addressing the challenges posed by the dynamic nature of actor interactions and infrastructure states.",
            "key obstacle": "The main difficulty lies in the fixed association of actors with locations and roles, which does not accommodate the dynamic changes that can occur within an airplane's security environment."
        },
        "idea": {
            "intuition": "The idea is inspired by the critical need to enhance security measures in aviation following tragic incidents caused by insider threats, emphasizing the integration of logical modeling with human factors.",
            "opinion": "The proposed idea involves extending the Isabelle Insider framework to model insider attacks in aviation, enabling formal reasoning about security policies and their effectiveness.",
            "innovation": "This method distinguishes itself by incorporating Kripke structures and temporal logic CTL into the Isabelle Insider framework, allowing for dynamic reasoning about system states and security properties."
        },
        "method": {
            "method name": "Isabelle Insider Framework",
            "method abbreviation": "IIF",
            "method definition": "The Isabelle Insider Framework is a logical modeling tool that combines higher-order logic with interactive theorem proving to analyze security policies in dynamic environments.",
            "method description": "It enables the formal verification of security policies against insider threats by modeling actors, actions, and infrastructure states.",
            "method steps": [
                "Define the infrastructure and the actors involved.",
                "Model the security policies using logical predicates.",
                "Integrate Kripke structures to represent dynamic state transitions.",
                "Use temporal logic CTL to express and verify safety and security properties."
            ],
            "principle": "The effectiveness of this method stems from its ability to formalize complex interactions and transitions within security policies, allowing for rigorous proofs of their validity."
        },
        "experiments": {
            "experiments setting": "The experimental setup includes modeling insider attacks on an airplane scenario, using a defined set of actors and their roles, along with specific security policies to assess.",
            "experiments progress": "The evaluation involved applying the Isabelle Insider framework to prove the validity of the two-person rule and its implications for preventing insider attacks."
        },
        "conclusion": "The experiments demonstrated that the proposed method effectively identifies vulnerabilities in security policies related to insider threats, providing a robust framework for future policy development in aviation security.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to rigorously analyze security policies using formal methods, revealing hidden assumptions and potential vulnerabilities.",
            "limitation": "A limitation of the method is its reliance on specific assumptions about the initial state of the system, which may not always hold in real-world scenarios.",
            "future work": "Future research should focus on refining the model to account for a wider range of dynamic interactions and exploring additional security measures that can complement the two-person rule."
        },
        "other info": {
            "full source availability": "The complete Isabelle sources used in this study are available online.",
            "keywords": [
                "Airplane safety and security",
                "Insider threats",
                "Interactive theorem proving",
                "Security policies",
                "Verification"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of airplane security, particularly in the context of insider threats, highlighting the need for improved verification methods that integrate human factors into security policies."
        },
        {
            "section number": "2.1",
            "key information": "The problem involves verifying security policies in aviation to prevent insider attacks, specifically addressing the challenges posed by the dynamic nature of actor interactions and infrastructure states."
        },
        {
            "section number": "2.3",
            "key information": "The Isabelle Insider Framework is a logical modeling tool that combines higher-order logic with interactive theorem proving to analyze security policies in dynamic environments."
        },
        {
            "section number": "5.1",
            "key information": "The proposed idea involves extending the Isabelle Insider framework to model insider attacks in aviation, enabling formal reasoning about security policies and their effectiveness."
        },
        {
            "section number": "6.1",
            "key information": "The experiments demonstrated that the proposed method effectively identifies vulnerabilities in security policies related to insider threats, providing a robust framework for future policy development in aviation security."
        },
        {
            "section number": "8",
            "key information": "Future research should focus on refining the model to account for a wider range of dynamic interactions and exploring additional security measures that can complement the two-person rule."
        }
    ],
    "similarity_score": 0.5376614982277733,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/Applying the Isabelle Insider Framework to Airplane Security.json"
}