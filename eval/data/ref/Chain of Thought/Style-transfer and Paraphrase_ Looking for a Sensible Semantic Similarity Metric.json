{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2004.05001",
    "title": "Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric",
    "abstract": " Abstract\n3 Dec 202\nThe rapid development of such natural language processing tasks as style transfer, paraphrase, and machine translation often calls for the use of semantic similarity metrics. In recent years a lot of methods to measure the semantic similarity of two short texts were developed. This paper provides a comprehensive analysis for more than a dozen of such methods. Using a new dataset of fourteen thousand sentence pairs human-labeled according to their semantic similarity, we demonstrate that none of the metrics widely used in the literature is close enough to human judgment in these tasks. A number of recently proposed metrics provide comparable results, yet Word Mover Distance is shown to be the most reasonable solution to measure semantic similarity in reformulated texts at the moment.\n3 Dec \n[cs.C\narXiv:2004.05001v3\n# Introduction\nStyle transfer and paraphrase are two tasks in Natural Language Processing (NLP). Both of them are centered around the problem of an automated reformulation. Given an input text, the system tries to produce a new rewrite that resembles the old text semantically. In the task of paraphrase, semantic similarity is the only parameter that one tries to control. Style transfer usually controls more aspects of the text and could, therefore, be regarded as an extension of a paraphrase. Intuitive understanding of style transfer problem is as follows: if an input text has some attribute A, say, politeness, a system generates new text similar to the input semantically but with attribute A changed to the target \u02dcA. For example, given a polite sentence \u201dcould you be so kind, give me a hand\u201d and a target \u201dnot polite\u201d the system produces a rewrite \u201dGod damn, help me\u201d. The significant part of current works perform style transfer via an encoder-decoder architecture with one or multiple style discriminators to learn disentangled representations (Hu et al. 2017). This basic architecture can have various extensions, for example, can control ",
    "bib_name": "yamshchikov2020styletransferparaphraselookingsensible",
    "md_text": "# and Paraphrase: Looking for a Sensible Semantic Similarity Metric\n# Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric\nIvan P. Yamshchikov Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany ivan@yamshchikov.info Viacheslav Shibaev Ural Federal University Ekaterinburg, Russia Nikolay Khlebnikov Ural Federal University Ekaterinburg, Russia Alexey Tikhonov Yandex, Berlin, Germany\nmshchikov nstitute for the Sciences, Viacheslav Shibaev Ural Federal University Ekaterinburg, Russia Nikolay Khlebnikov Ural Federal University Ekaterinburg, Russia Alexey Tikhonov Yandex, Berlin, Germany\nIvan P. Yamshchikov Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany ivan@yamshchikov.info Viacheslav Sh Ural Federal Un Ekaterinburg, R\nIvan P. Yamshchikov Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany ivan@yamshchikov.info\n# Abstract\n3 Dec 202\nThe rapid development of such natural language processing tasks as style transfer, paraphrase, and machine translation often calls for the use of semantic similarity metrics. In recent years a lot of methods to measure the semantic similarity of two short texts were developed. This paper provides a comprehensive analysis for more than a dozen of such methods. Using a new dataset of fourteen thousand sentence pairs human-labeled according to their semantic similarity, we demonstrate that none of the metrics widely used in the literature is close enough to human judgment in these tasks. A number of recently proposed metrics provide comparable results, yet Word Mover Distance is shown to be the most reasonable solution to measure semantic similarity in reformulated texts at the moment.\n3 Dec \n[cs.C\narXiv:2004.05001v3\n# Introduction\nStyle transfer and paraphrase are two tasks in Natural Language Processing (NLP). Both of them are centered around the problem of an automated reformulation. Given an input text, the system tries to produce a new rewrite that resembles the old text semantically. In the task of paraphrase, semantic similarity is the only parameter that one tries to control. Style transfer usually controls more aspects of the text and could, therefore, be regarded as an extension of a paraphrase. Intuitive understanding of style transfer problem is as follows: if an input text has some attribute A, say, politeness, a system generates new text similar to the input semantically but with attribute A changed to the target \u02dcA. For example, given a polite sentence \u201dcould you be so kind, give me a hand\u201d and a target \u201dnot polite\u201d the system produces a rewrite \u201dGod damn, help me\u201d. The significant part of current works perform style transfer via an encoder-decoder architecture with one or multiple style discriminators to learn disentangled representations (Hu et al. 2017). This basic architecture can have various extensions, for example, can control POS-distance between input and output (Tian, Hu, and Yu 2018), or have additional discriminator or an extra loss term to improve the quality of the latent representations (Yamshchikov et al. 2019). There are also other approaches to this problem that do not use\nCopyright \u00a9 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nideas of disentangled latent representations but rather treat it as a machine translation problem; see, for example, (Subramanian et al. 2018). However, independently of a chosen architecture, one has to control the semantic component of the output text. It is expected to stay the same as the system changes the style of the input. This aspect makes the problem of style transfer naturally related to the problem of paraphrase (Prakash et al. 2016), (Gupta et al. 2018), (Roy and Grangier 2019). It also raises the question of how one could automatically measure the semantic similarity of two texts in these problems. As with every NLP task that is relatively new, the widely accepted baselines and evaluations metrics are still only emerging. There are ongoing discussions on which aspects of the texts are stylistic and could be changed by the style transfer system and which are semantic and therefore are technically out of the scope of the style transfer research (Tikhonov and Yamshchikov 2018). This paper refrains from these discussions. It instead attempts to systematize existing methods of quality assessment for the tasks of style transfer that are used in different state of art research results. We also put these methods into the perspective of paraphrase tasks. To our knowledge, that was not done before. The contribution of the paper is four-fold:\n\u2022 it compares more than a dozen of existing semantic similarity metrics used by different researchers to measure the performance of different style transfer methods;\n\u2022 using human assessment of 14 thousand pairs of sentences it demonstrates that there is still no optimal semanticpreservation metric that could be comparable with human judgment in context of paraphrase and textual style transfer, however Word Mover Distance (Kusner et al. 2015) seems to be the most promising one; \u2022 it proposes a simple necessary condition that a metric should comply with to be a valid semantic similarity metric for the task of style transfer;\n\u2022 it shows that some metrics used in style transfer literature should not be used in the context of style transfer at all.\n# Measuring semantic preservation\nStyle transfer, as well as a paraphrase, naturally demands the preservation of the semantic component as the input sen-\ntence is transformed into the desired output. Different researchers use different methods to measure this preservation of semantics. Despite its disadvantages (Larsson, Nilsson, and K\u02daageb\u00a8ack 2017), one of the most widely used semantic similarity metrics is BLEU. (Tikhonov et al. 2019) show that it could be manipulated in a way that the system would show higher values of BLEU on average, producing sentences that are completely detached from the input semantically. However, BLEU is easy to calculate and is broadly accepted for various NLP tasks that demand semantic preservation (Vaswani et al. 2017), (Hu et al. 2017), (Cohn-Gordon and Goodman 2019). Alongside BLEU, there are other, less broadly accepted metrics for semantic preservation. For example, (Zhang, Ding, and Soricut 2018) work with different versions of ROUGE. (Fu et al. 2018), (John et al. 2018) or (Romanov et al. 2018) compute a sentence embedding by concatenating the min, max, and mean of its word embeddings and use the cosine similarity between the source and generated sentence embeddings as an indicator of content preservation. (Tian, Hu, and Yu 2018) uses POS-distance alongside with BLEU and BLEU between human-written reformulations and the actual output of the system. One of the most recent contributions in this area (Mir et al. 2019) evaluates several of the metrics mentioned above as well as METEOR (Banerjee and Lavie 2005) and Word Mover\u2019s Distance (WMD). This metric is calculated as the minimum \u201ddistance\u201d between word embeddings of input and output (Kusner et al. 2015). In this paper, we use these metrics of content-preservation listed above alongside with several others that are used for semantic similarity in other NLP tasks recently. We put all these metrics into the context of paraphrase and style transfer. These metrics are: \u2022 POS-distance that looks for nouns in the input and output and is calculated as a pairwise distance between the embeddings of the found nouns; \u2022 Word overlap calculated as a number of words that occur in both texts; \u2022 chrF (Popovi\u00b4c 2015) \u2013 a character n-gram F-score that measures number of n-grams that coincide in input and output; \u2022 cosine similarity calculated in line with (Fu et al. 2018) with pre-trained embeddings by GloVe (Pennington, Socher, and Manning 2014); \u2022 cosine similarity calculated similarly but using FastText word embeddings (Joulin et al. 2016); \u2022 L2 distance based on ELMo (Peters et al. 2018) \u2022 WMD (Kusner et al. 2015) that defines the distance between two documents as an optimal transport problem between the embedded words; \u2022 BLEU (Papineni et al. 2002); \u2022 ROUGE-1 (Lin and Hovy 2000) compares any text to any other (typically human-generated) summary using a recall-oriented approach and unigrams;\ntence is transformed into the desired output. Different researchers use different methods to measure this preservation of semantics. Despite its disadvantages (Larsson, Nilsson, and K\u02daageb\u00a8ack 2017), one of the most widely used semantic similarity metrics is BLEU. (Tikhonov et al. 2019) show that it could be manipulated in a way that the system would show higher values of BLEU on average, producing sentences that are completely detached from the input semantically. However, BLEU is easy to calculate and is broadly accepted for various NLP tasks that demand semantic preservation (Vaswani et al. 2017), (Hu et al. 2017), (Cohn-Gordon and Goodman 2019). Alongside BLEU, there are other, less broadly accepted metrics for semantic preservation. For example, (Zhang, Ding, and Soricut 2018) work with different versions of ROUGE. (Fu et al. 2018), (John et al. 2018) or (Romanov et al. 2018) compute a sentence embedding by concatenating the min, max, and mean of its word embeddings and use the cosine similarity between the source and generated sentence embeddings as an indicator of content preservation. (Tian, Hu, and Yu 2018) uses POS-distance alongside with BLEU and BLEU between human-written reformulations and the actual output of the system. One of the most recent contributions in this area (Mir et al. 2019) evaluates several of the metrics mentioned above as well as METEOR (Banerjee and Lavie 2005) and Word Mover\u2019s Distance (WMD). This metric is calculated as the minimum \u201ddistance\u201d between word embeddings of input and output (Kusner et al. 2015). In this paper, we use these metrics of content-preservation listed above alongside with several others that are used for semantic similarity in other NLP tasks recently. We put all these metrics into the context of paraphrase and style transfer. These metrics are: \u2022 POS-distance that looks for nouns in the input and output and is calculated as a pairwise distance between the embeddings of the found nouns; \u2022 Word overlap calculated as a number of words that occur in both texts; \u2022 chrF (Popovi\u00b4c 2015) \u2013 a character n-gram F-score that measures number of n-grams that coincide in input and output; \u2022 cosine similarity calculated in line with (Fu et al. 2018) with pre-trained embeddings by GloVe (Pennington, Socher, and Manning 2014); \u2022 cosine similarity calculated similarly but using FastText word embeddings (Joulin et al. 2016); \u2022 L2 distance based on ELMo (Peters et al. 2018) \u2022 WMD (Kusner et al. 2015) that defines the distance between two documents as an optimal transport problem between the embedded words; \u2022 BLEU (Papineni et al. 2002); \u2022 ROUGE-1 (Lin and Hovy 2000) compares any text to any other (typically human-generated) summary using a recall-oriented approach and unigrams;\n# \u2022 ROUGE-2 that uses bigrams;\n\u2022 ROUGE-L (Lin and Och 2004) that identifies longest cooccurring in sequence n-grams; \u2022 Meteor (Banerjee and Lavie 2005) metric that is based on a harmonic mean of unigram precision and recall, with recall weighted higher than precision and some additional features, such as stemming and synonymy matching; \u2022 and the BERT score proposed in (Zhang et al. 2019) for the estimation of the generated texts. All these metrics are known to vary from dataset to dataset but show consistent results within one data collection. In the next section, we try to come up with a set of various paraphrases and style transfer datasets that would allow us to see qualitative differences between these metrics of semantic similarity.\n\u2022 and the BERT score proposed in (Zhang et al. 2019) for the estimation of the generated texts.\nAll these metrics are known to vary from dataset to dataset but show consistent results within one data collection. In the next section, we try to come up with a set of various paraphrases and style transfer datasets that would allow us to see qualitative differences between these metrics of semantic similarity.\n# Data\nThe task of paraphrasing a given sentence is better formalized than the task of style transfer. However, to our knowledge, there were no attempts to look at these two tasks in one context. There are several datasets designed to benchmark semantic similarity metrics. The most widely used is STS-B, see (Cer et al. 2017). (Zhang, Baldridge, and He 2019) provide a dataset of sentences that have high lexical overlap without being paraphrases. Quora Questions Paraphrase dataset1 provides paraphrases of Quora questions. However, these datasets do not include style transfer examples whereas the focus of this paper is to align semantic similarity metrics used for paraphrase with the one used in style transfer community. Here we intend to work with the metrics listed in the previous section and calculate them over three paraphrase and two style transfer datasets that are often used for these two NLP tasks. The paraphrase datasets include:\n\u2022 Dataset of politeness introduced in (Rao and Tetreault 2018) that we in line with the original naming given by the authors refer to as GYAFC later on; \u2022 Yelp! Reviews4 enhanced with human written reviews with opposite sentiment provided by (Tian, Hu, and Yu 2018). We suggest to work with these datasets, since they are frequently used for baseline measurements in paraphrase and style transfer literature.\n1https://www.kaggle.com/quora/question-pairs-dataset 2http://knowitall.cs.washington.edu/paralex/ 3http://paraphrase.org 4https://www.yelp.com/dataset\nOut of all these listed datasets we sample 1000 sentence pairs, where each pair of sentences consists of two paraphrases or two sentences with different style and comparable semantics. Experimental results that follow present averages of every measure of semantic similarity over these 1000 pairs for every dataset. Additionally to the paraphrases and style-transfer datasets we provide several datasets that consist of sentence pairs that have no common semantic component yet are sampled from the same datasets. We do it for several reasons: first, semantic similarity measure should be at least capable to distinguish sentence pairs that have no semantic similarity whatsoever from paraphrases or styletransfer examples, second, variation of the semantic similarity on random pairs for various corpora could show how a given metric depends on the corpus\u2019 vocabulary. These random datasets could be used as a form of a benchmark to estimate \u2019zero\u2019 for every semantic similarity metric. All the metrics that we include in this paper already have undergone validation. These metrics hardly depend on the size of the random data sample provided it is large enough. They are also known to vary from one dataset to another. However, due to the laborious nature of this project, we do not know of any attempts to characterize these differences across various datasets.\n# Assessment\nThis paper is focused on the applications of semantic similarity to the tasks of style transfer and paraphrase, however there are more NLP tasks that depend on semantic similarity measures. We believe that the reasoning and measurements presented in this paper are general enough to be transferred to other NLP tasks that depend upon a semantic similarity metric. Table 1 and Table 2 show the results for fourteen datasets and thirteen metrics as well as the results of the human evaluation of semantic similarity. It is essential to mention that (Rao and Tetreault 2018) provide different reformulations of the same text both in an informal and formal style. That allows us to use the GYAFC dataset not only as a style transfer dataset but also as a paraphrase dataset, and, therefore, extend the number of datasets in the experiment. To stimulate further research of semantic similarity measurements, we publish5 our dataset that consists of 14 000 different pairs of sentences alongside with semantic similarity scores given by the annotators. Each sentence was annotated by at least three humans independently. There were 300+ English native speakers involved in the assessment. Every annotator was presented with two parallel sentences and was asked to assess how similar their meaning is. We used AmazonTurk with several restrictions on the turkers: these should be native speakers of English in the top quintile of the internal rating. Humans were to assess \u201dhow similar is the meaning of these two sentences\u201d on a scale from 1 to 5. This is a standard formulation of semantic-similarity assessment task on AmazonTurk. Since annotators with high performance scores already know this task, we didn\u2019t change this standard formulation to ensure that gathered data is representa-\n5https://github.com/VAShibaev/semantic similarity metrics\ntive for standard semantic similarity problems. We publish all scores that were provided by the annotators to enable further methodological research. We hope that this dataset could be further used for a deeper understanding of semantic similarity.\n# Discussion\nLet us briefly discuss the desired properties of a hypothetical ideal content preservation metric. We do understand that this metric can be noisy and differ from dataset to dataset. However, there are two basic principles with which such metrics should comply. First, every content preservation metric that is aligned with actual ground truth semantic similarity should induce similar order on any given set of datasets. Indeed, let us regard two metrics M1 and M2 both of which claim to measure semantic preservation in two given parallel datasets Da and Db. Let us assume that M1 is the goldstandard metric that perfectly measures semantic similarity. Let us then assume that under the order that M1 induces on the set of the datasets the following holds\n# M1(Da) \u2264M1(Db).\n# Then either\nThen either\nM2(Da) \u2264M2(Db)\n \u2264 would be true in terms of the order induced by M2 as well or M2 is an inferior semantic similarity metric. Since style is a vague notion it is hard to intuitively predict what would be the relative ranking of style transfer pairs of sentences Ds, and paraphrase pairs Dp. However, it seems more than natural to disqualify any metric that induces such an order under which a randomized dataset ends up above the paraphrase or style transfer dataset. Under order induced by an ideal semantic preservation metric one expects to see both these datasets to be ranked above the dataset Dr that consists of random pairs\n(1)\nTable 1 and Table 2 show resulting values of every metric across every dataset with standard deviations of the obtained scores. Table 3 summarizes order induced on the set of the paraphrase datasets, style transfer datasets, and datasets consisting of random pairs of sentences. One can see that humans rank random pairs as less semantically similar than paraphrases or style-transfer rewrites. Generally, human ranking corresponds to the intuition described in Inequalities 1. Majority of the metrics under examination are also in agreement with Inequalities 1. What is particularly interesting is that humans assess GYAFC reformulations (the sentences with supposedly similar semantic but varying level of politeness) as the most semantically similar sentence pairs. However Yelp! rewrites that contain the same review of a restaurant but with a different sentiment are ranked as the least similar texts out of all non-random sentence pairs. This illustrates the argument made in (Tikhonov and Yamshchikov 2018) that sentiment is perceived as an aspect of semantics rather than style by\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/38b3/38b3653c-d8ec-41a7-84dc-f37dcfe2dc16.png\" style=\"width: 50%;\"></div>\nDataset\nHuman Labeling\nPOS-distance\nWord\nchrF\nCosine\nCosine\nWMD\noverlap\nSimilarity\nSimilarity\nWord2Vec\nFastText\nBibles\n3.54 \u00b1 0.72\n2.39 \u00b1 3.55\n0.47 \u00b1 0.18\n0.54 \u00b1 0.18\n0.04 \u00b1 0.04\n0.04 \u00b1 0.02\n0.57 \u00b1 0.29\nParalex\n3.28 \u00b1 0.8\n2.91 \u00b1 4.28\n0.43 \u00b1 0.18\n0.48 \u00b1 0.18\n0.13 \u00b1 0.09\n0.09 \u00b1 0.04\n0.62 \u00b1 0.3\nParaphrase\n3.6 \u00b1 0.79\n2.29 \u00b1 2.85\n0.31 \u00b1 0.2\n0.41 \u00b1 0.23\n0.29 \u00b1 0.17\n0.21 \u00b1 0.12\n0.77 \u00b1 0.34\nGYAFC formal\n3.63 \u00b1 0.75\n2.27 \u00b1 3.97\n0.5 \u00b1 0.22\n0.53 \u00b1 0.22\n0.06 \u00b1 0.04\n0.05 \u00b1 0.03\n0.57 \u00b1 0.35\nGYAFC informal\n3.41 \u00b1 0.78\n3.79 \u00b1 4.54\n0.32 \u00b1 0.17\n0.34 \u00b1 0.17\n0.09 \u00b1 0.05\n0.09 \u00b1 0.04\n0.76 \u00b1 0.31\nYelp! rewrite\n2.68 \u00b1 0.83\n1.11 \u00b1 2.34\n0.45 \u00b1 0.25\n0.51 \u00b1 0.23\n0.08 \u00b1 0.06\n0.08 \u00b1 0.06\n0.61 \u00b1 0.31\nGYAFC rewrite\n3.83 \u00b1 0.75\n2.32 \u00b1 3.91\n0.47 \u00b1 0.21\n0.53 \u00b1 0.22\n0.06 \u00b1 0.04\n0.06 \u00b1 0.04\n0.54 \u00b1 0.35\nBibles random\n2.32 \u00b1 0.69\n11.21 \u00b1 8.08\n0.10 \u00b1 0.04\n0.17 \u00b1 0.05\n0.10 \u00b1 0.07\n0.1 \u00b1 0.03\n1.23 \u00b1 0.07\nParalex random\n1.95 \u00b1 0.71\n10.31 \u00b1 4.84\n0.13 \u00b1 0.08\n0.14 \u00b1 0.05\n0.24 \u00b1 0.09\n0.18 \u00b1 0.04\n1.3 \u00b1 0.07\nParaphrase random\n1.97 \u00b1 0.65\n7.47 \u00b1 2.5\n0.02 \u00b1 0.06\n0.1 \u00b1 0.05\n0.58 \u00b1 0.18\n0.46 \u00b1 0.13\n0.34 \u00b1 0.07\nGYAFC random\n2.13 \u00b1 0.73\n10.61 \u00b1 7.2\n0.05 \u00b1 0.05\n0.13 \u00b1 0.04\n0.15 \u00b1 0.05\n0.15 \u00b1 0.04\n1.24 \u00b1 0.08\ninformal\nGYAFC random\n2.12 \u00b1 0.74\n10.82 \u00b1 8.64\n0.08 \u00b1 0.05\n0.14 \u00b1 0.04\n0.15 \u00b1 0.04\n0.14 \u00b1 0.03\n1.26 \u00b1 0.07\nformal\nGYAFC random\n2.07 \u00b1 0.7\n10.58 \u00b1 8.03\n0.06 \u00b1 0.05\n0.13 \u00b1 0.04\n0.15 \u00b1 0.04\n0.14 \u00b1 0.03\n1.25 \u00b1 0.07\nrewrite\nYelp! random\n2.14 \u00b1 0.79\n8.97 \u00b1 4.35\n0.06 \u00b1 0.06\n0.14 \u00b1 0.04\n0.19 \u00b1 0.06\n0.17 \u00b1 0.05\n1.26 \u00b1 0.08\nrewrite\nTable 1: Various metrics of content preservation with standard deviations calculated across three paraphrase datasets, dataset of rewrites and various randomized datasets. GYAFC Formal and Informal correspond to the content preservation scores fo GYAFC data treated as paraphrases in a formal or informal mode respectively. GYAFC and Yelp! rewrite correspond to th score between an input and a human-written reformulation in a different style. GYAFC and Yelp! random stand for the score calculated on samples of random pairs from the respective dataset.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a772/a7722c2a-afc6-4dd0-a4f5-aed8d78bbe56.png\" style=\"width: 50%;\"></div>\nTable 2: Various metrics of content preservation with standard deviations calculated across three paraphrase datasets, dataset of rewrites and various randomized datasets. GYAFC Formal and Informal correspond to the content preservation scores fo GYAFC data treated as paraphrases in a formal or informal mode respectively. GYAFC and Yelp! rewrite correspond to th core between an input and a human-written reformulation in a different style. GYAFC and Yelp! random stand for the score alculated on samples of random pairs from the respective dataset.\nhuman assessors. Therefore, addressing the sentiment transfer problem as an example of the style transfer problem can cause systemic errors in terms of semantic similarity assessment. Unfortunately this often happens in modern style transfer research and should be corrected. Closely examining Table 3 one can make several conclusions. First of all, cosine similarity metrics based on Word2Vec or on FastText do not seem to be useful as metrics of semantic preservation since they do not satisfy Inequality 1 and also have the lowest correlation with human assesment, shown in Table 4. All the other metrics induce relatively similar orders on the set of the datasets. Figure 1 illustrates that. Table 4 shows correlation of the metric values with human assessments as well as correlations between human-induced order and the orders that other semantic similarity metrics induce. Table 4 also demonstrates variability of the semantic similarity metrics. The intuition behind variability is to show how prone is\nthe metric to fluctuations across different texts. Since on the datasets of random pairs the metric ideally should show very low semantic similarity, it is suboptimal if it assumes a large range of values on this datasets. The ratio between the range of values on random datasets and the range of values on all datasets is always between 0 and 1 plus and could intuitively characterize how noisy the metric is. If R is a set of all datasets of random pairs and A is set of all datasets in question, one can introduce a measure of metrics variability V as\nFor human labelling variability is relatively high V = 19.7% which means that humans often vary in their assessment of sentences that have no common semantic component. Lower variability on random pairs could be beneficial if one is interested in some form of binary classification that\nMetric\nBibles\nParalex\nParaphrase\nYelp!\nGYAFC\nGYAFC\nGYAFC\nYelp!\nGYAFC\nGYAFC\nGYAFC\nBibles\nParalex\nParaphrase\nrandom\nrandom\nrandom\nrandom\nrandom\nrandom\nrandom\nrewrite\nrewrite\ninformal\nformal\nrewrite\nrewrite\ninformal\nformal\nPOS\n14\n10\n8\n9\n11\n12\n13\n1\n4\n7\n2\n5\n6\n3\nWord overlap\n10\n9\n14\n11\n12\n13\n8\n4\n3\n6\n1\n2\n5\n7\nchrF\n9\n10\n14\n11\n12\n13\n8\n4\n2\n7\n3\n1\n5\n6\nWord2Vec\n8\n12\n14\n11\n7\n10\n9\n4\n2\n5\n3\n1\n6\n13\nFastText\n7\n12\n14\n11\n9\n10\n8\n4\n3\n6\n2\n1\n5\n13\nWMD\n8\n13\n14\n11\n10\n9\n12\n4\n1\n6\n3\n2\n5\n7\nELMo L2\n8\n13\n14\n12\n11\n10\n9\n4\n3\n5\n2\n1\n6\n7\nROUGE-1\n10\n9\n14\n11\n13\n12\n8\n5\n3\n6\n1\n2\n4\n7\nROUGE-2\n10\n9\n14\n13\n12\n8\n11\n4\n2\n6\n1\n3\n5\n7\nROUGE-L\n9\n10\n14\n11\n13\n12\n8\n4\n3\n7\n2\n1\n5\n6\nBLEU\n10\n11\n14\n12\n13\n8\n9\n4\n3\n5\n2\n1\n6\n7\nMeteor\n10\n9\n14\n11\n12\n13\n8\n4\n3\n7\n2\n1\n5\n6\nBERT score\n10\n9\n14\n8\n12\n13\n11\n3\n4\n7\n1\n2\n5\n6\nHuman Labeling\n9\n14\n13\n8\n12\n10\n11\n7\n1\n5\n2\n4\n6\n3\nTable 3: Different semantic similarity metrics sort the paraphrase datasets differently. Cosine similarity calculated with Word2Vec or FastText embeddings do not comply with Inequality M(Dr) < M(Dp). All other metrics clearly distinguish randomized texts from style transfers and paraphrases and are in line with Inequalities 1. However, none of the metrics i completely in line with human evaluation.\nMetric\nCorrelation\nCorrelation\nVariability\nof the metric\nof the induced\nof the metric\nwith human\norders with\non random\nevaluation\nhuman-induced order\nsentences\nPOS\n0.87\n0.72\n37.0%\nWord overlap\n0.89\n0.80\n23.8%\nchrF\n0.9\n0.83\n17.2%\nWord2Vec\n0.46\n0.64\n88.6%\nFastText\n0.52\n0.65\n86.3%\nWMD\n0.92\n0.89\n12.3%\nELMo L2\n0.82\n0.86\n53.3%\nROUGE-1\n0.9\n0.82\n33.5%\nROUGE-2\n0.84\n0.81\n4.5%\nROUGE-L\n0.89\n0.83\n33.4%\nBLEU\n0.72\n0.84\n0.2%\nMeteor\n0.91\n0.80\n19.5%\nBERT score\n0.89\n0.82\n23.1%\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebdc/ebdcb77a-d854-43e3-91d6-d0a33c88512e.png\" style=\"width: 50%;\"></div>\nTable 4: WMD shows the highest pairwise correlation with human assessment similarity scores. The order on fourteen datasets, induced by WMD also has the highest correlation with human-induced semantic similarity order. Variability on random sentences is a ratio of the difference between the maximal and minimal value of a given metric on the datasets of random pairs and difference of the maximal and minimal value of the same metric on all available datasets.\nwould distinguish pairs of sentences that have some information in common and the ones that do not. In this context BLEU seems to be superior to all other metrics of the survey. However, if we want to have some quantitative estimation of semantic similarity that resembles human judgement, than Meteor, chrF, and WMD seem to be more preferable. One can also introduce several scoring systems to estimate how well every metric performs in terms of Inequalities 1. For example, we can calculate, how many datasets get the same rank in the metric-induced order as in the humaninduced one. Another possible score could be a number of swaps needed to produce the human-induced order out of the metric-induced one. Table 5 shows these scores for the the semantic similarity metrics in question. Looking at the results listed above we can recommend\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e78c/e78c53ab-162e-40f8-80ef-af76d6e4b61e.png\" style=\"width: 50%;\"></div>\nMetric\nNumber of ranks\nNumber of swaps\ncoinciding with\nneeded to reconstruct\nhuman-induced ranking\nhuman-induced ranking\nPOS\n3\n16\nWord overlap\n1\n15\nchrF\n2\n14\nWord2Vec\n3\n16\nFastText\n2\n17\nWMD\n1\n11\nELMo L2\n4\n11\nROUGE-1\n0\n15\nROUGE-2\n2\n13\nROUGE-L\n2\n14\nBLEU\n3\n13\nMeteor\n2\n15\nBERT score\n3\n13\nTable 5: Scores for the orders induced by different semantic similarity metrics.\nthe following. First of all, one has to conclude that there is no \u201dsilver bullet\u201d for semantic similarity yet. Every metric that is used for semantic similarity assessment at the moment fails to be in line with human understanding of semantic similarity. It is important to add here that in terms of standard deviation human assessment is far more concise than some of the metrics under study. Though human scores vary from dataset to dataset the variance of them is relatively small when compared to the mean on any given dataset. Second, judging by Table 4 and Table 5 there are two metrics that seem to be the most promising instruments for the task. These are: WMD that induces the order with minimal amount of swaps needed to achieve human-induced order, shows the highest correlation with human assessment values, and the highest correlation with human-induced order; and ELMO L2 distance that has the highest number of coinciding ranks and is as well only eleven swaps away from a human-induced order, it also has the second highest in correlation for the induced order with the human-induced one, yet is relatively inferior in terms of pairwise correlation with human assessment. Finally, let us look at Figure 1. There is a clear correlation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f5da/f5dab27a-5298-47f0-96ec-c9a432f7e8d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Pairwise correlations of the orders induced by the metrics of semantic similarity.</div>\nbetween all orders induced by the metrics listed in Table 4. This correlation of induced orders is not only a consistent result that shows that the majority of semantic preservation metrics are aligned to a certain extent. This correlation could also be regarded as a justification of an order theory inspired methodology that we propose here for comparative analysis of metrics. Looking at Figure 1 one should also mention that POSdistance, as well as Word2Vec and FastText cosine similarities seem to be less aligned with every other metric that was tested. One could also see that WMD and ELMO L2 induce very similar orders. Taking this into consideration and revisiting results in Table 4 and Table 5 we can conclude that if one has to choose one metric of semantic similarity for a task of paraphrase or style transfer, WMD is the preferable metric at the moment. The observed correlation of the induced orders gives hope that there is a universal measure of semantic similarity for texts and that all these metrics proxy this potential metric to certain extent. However, it is clear that none of them could model human judgement. There are several reasons that account for that. One is the phenomenal recent success of the semantic extraction methods that are based on local rather than global context that made local information-based metrics dominate NLP in recent years. Humans clearly operate in a non-local semantic context yet even state of art models in NLP can not account for this. The fact that BERT score that theoretically could model inner non-local semantics still does not reproduce human semantic similarity estimations is a proof for that. Second reason is the absence of rigorous, universally accepted definition for the problem of style transfer. We hope further research of disentangled semantic representations would allow to define semantic information in NLP in a more rigorous way, especially in context of several recent attempts to come up with unified notion of semantic information, see for example (Kolchinsky and Wolpert 2018).\n# Conclusion\nIn this paper, we examine more than a dozen metrics for semantic similarity in the context of NLP tasks of style transfer and paraphrase. We publish human assessment for semantic similarity of fourteen thousand short text pairs and hope that this dataset could facilitate further research of semantic similarity metrics. Using very general order theory reasoning\nand human assessment data, we demonstrate that Word2Vec and FastText cosine similarity based metrics should not be used in context of paraphrase and style transfer. We also show that the majority of the metrics that occur in style transfer literature induce similar order on the sets of data. This is not only to be expected but also justifies the proposed order-theory methodology. POS-distance, Word2Vec and FastText cosine similarities are somehow less aligned with this general semantic similarity order. WMD seems to be the best semantic similarity solution that could be used for style transfer problems as well as problems of paraphrase at the moment. There is still no metric that could distinguish paraphrases form style transfers definitively. This fact is essential in the context of future style transfer research. To put that problem in the context of paraphrase, such semantic similarity metric is direly needed.\n# References\n[Banerjee and Lavie 2005] Banerjee, S., and Lavie, A. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 65\u201372. [Carlson, Riddell, and Rockmore 2017] Carlson, K.; Riddell, A.; and Rockmore, D. 2017. Zero-shot style transfer in text using recurrent neural networks. arXiv preprint arXiv:1711.04731. [Cer et al. 2017] Cer, D.; Diab, M.; Agirre, E.; LopezGazpio, I.; and Specia, L. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 1\u201314. [Cohn-Gordon and Goodman 2019] Cohn-Gordon, R., and Goodman, N. 2019. Lost in machine translation: A method to reduce meaning loss. arXiv preprint arXiv:1902.09514. [Fu et al. 2018] Fu, Z.; Tan, X.; Peng, N.; Zhao, D.; and Yan, R. 2018. Style transfer in text: Exploration and evaluation. AAAI. [Gupta et al. 2018] Gupta, A.; Agarwal, A.; Singh, P.; and Rai, P. 2018. A deep generative framework for paraphrase generation. In Thirty-Second AAAI Conference on Artificial Intelligence. [Hu et al. 2017] Hu, Z.; Yang, Z.; Liang, X.; Salakhutdinov,\nR.; and Xing, E. P. 2017. Toward controlled generation of text. In International Conference on Machine Learning, 1587\u20131596. [John et al. 2018] John, V.; Mou, L.; Bahuleyan, H.; and Vechtomova, O. 2018. Disentangled representation learning for text style transfer. In arXiv preprint. [Joulin et al. 2016] Joulin, A.; Grave, E.; Bojanowski, P.; Douze, M.; J\u00b4egou, H.; and Mikolov, T. 2016. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651. [Kolchinsky and Wolpert 2018] Kolchinsky, A., and Wolpert, D. H. 2018. Semantic information, autonomous agency and non-equilibrium statistical physics. Interface focus 8(6):20180041. [Kusner et al. 2015] Kusner, M.; Sun, Y.; Kolkin, N.; and Weinberger, K. 2015. From word embeddings to document distances. In International conference on machine learning, 957\u2013966. [Larsson, Nilsson, and K\u02daageb\u00a8ack 2017] Larsson, M.; Nilsson, A.; and K\u02daageb\u00a8ack, M. 2017. Disentangled representations for manipulation of sentiment in text. arXiv preprint arXiv:1712.10066. [Lin and Hovy 2000] Lin, C.-Y., and Hovy, E. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics-Volume 1, 495\u2013501. Association for Computational Linguistics. [Lin and Och 2004] Lin, C.-Y., and Och, F. J. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, 605. Association for Computational Linguistics. [Mir et al. 2019] Mir, R.; Felbo, B.; Obradovich, N.; and Rahwan, I. 2019. Evaluating style transfer for text. arXiv preprint arXiv:1904.02295. [Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, 311\u2013318. Association for Computational Linguistics. [Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 1532\u20131543. [Peters et al. 2018] Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365. [Popovi\u00b4c 2015] Popovi\u00b4c, M. 2015. chrf: character n-gram fscore for automatic mt evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, 392\u2013 395. [Prakash et al. 2016] Prakash, A.; Hasan, S. A.; Lee, K.; Datla, V.; Qadir, A.; Liu, J.; and Farri, O. 2016. Neural\nJ.;\nparaphrase generation with stacked residual lstm networks. In arXiv preprint. [Rao and Tetreault 2018] Rao, S., and Tetreault, J. 2018. Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer. arXiv preprint arXiv:1803.06535. [Romanov et al. 2018] Romanov, A.; Rumshisky, A.; Rogers, A.; and Donahue, D. 2018. Adversarial decomposition of text representation. In arXiv preprint. [Roy and Grangier 2019] Roy, A., and Grangier, D. 2019. Unsupervised paraphrasing without translation. In arXiv preprint. [Subramanian et al. 2018] Subramanian, S.; Lample, G.; Smith, E. M.; Denoyer, L.; Ranzato, M.; and Boureau, Y.-L. 2018. Multiple-attribute text style transfer. arXiv preprint arXiv:1811.00552. [Tian, Hu, and Yu 2018] Tian, Y.; Hu, Z.; and Yu, Z. 2018. Structured content preservation for unsupervised text style transfer. In arXiv preprint. [Tikhonov and Yamshchikov 2018] Tikhonov, A., and Yamshchikov, I. P. 2018. What is wrong with style transfer for texts? In arXiv preprint. [Tikhonov et al. 2019] Tikhonov, A.; Shibaev, V.; Nagaev, A.; Nugmanova, A.; and Yamshchikov, I. P. 2019. Style transfer for texts: Retrain, report errors, compare with rewrites. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 3927\u20133936. [Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems, 5998\u20136008. [Yamshchikov et al. 2019] Yamshchikov, I. P.; Shibaev, V.; Nagaev, A.; Jost, J.; and Tikhonov, A. 2019. Decomposing textual information for style transfer. In Proceedings of the 3rd Workshop on Neural Generation and Translation, 128\u2013137. [Zhang, Baldridge, and He 2019] Zhang, Y.; Baldridge, J.; and He, L. 2019. Paws: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1298\u20131308. [Zhang et al. 2019] Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. [Zhang, Ding, and Soricut 2018] Zhang, Y.; Ding, N.; and Soricut, R. 2018. Shaped: Shared-private encoder-decoder for text style adaptation. arXiv preprint arXiv:1804.04093.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This paper provides a comprehensive analysis of semantic similarity metrics used in natural language processing tasks like style transfer and paraphrase generation, aiming to identify gaps and improve the evaluation of these metrics.",
            "scope": "The survey focuses on semantic similarity metrics applied to style transfer and paraphrase tasks, explicitly excluding discussions on stylistic aspects that do not pertain to semantics."
        },
        "problem": {
            "definition": "The core issue explored is the lack of effective semantic similarity metrics that align with human judgment for evaluating paraphrase and style transfer tasks.",
            "key obstacle": "The primary challenge is that existing metrics either fail to accurately reflect human assessments or can be manipulated to produce misleading results."
        },
        "architecture": {
            "perspective": "The survey categorizes existing semantic similarity metrics based on their effectiveness and alignment with human judgment, proposing a necessary condition for valid metrics in style transfer.",
            "fields/stages": "Metrics are organized by their application in style transfer and paraphrase tasks, including POS-distance, Word Mover's Distance, BLEU, ROUGE, and cosine similarity, among others."
        },
        "conclusion": {
            "comparisons": "The survey compares various metrics, highlighting that while many metrics induce similar rankings, some like WMD show the highest correlation with human assessments.",
            "results": "The findings suggest that Word Mover's Distance is currently the most promising metric for measuring semantic similarity in style transfer and paraphrase tasks."
        },
        "discussion": {
            "advantage": "Existing research has made strides in developing metrics that can assess semantic similarity, with some metrics showing consistent results across datasets.",
            "limitation": "Current metrics often fail to align with human judgment, leading to potential misinterpretations of semantic similarity.",
            "gaps": "There remains a significant gap in identifying a universal metric that can reliably differentiate between paraphrases and style transfers.",
            "future work": "Future research should focus on refining semantic similarity metrics and exploring new methodologies that better capture human semantic understanding."
        },
        "other info": {
            "dataset": "The paper introduces a new dataset consisting of 14,000 human-labeled sentence pairs for semantic similarity assessment.",
            "methodology": "The authors employ order theory reasoning and comparative analysis to evaluate the effectiveness of different semantic similarity metrics."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper provides a comprehensive analysis of semantic similarity metrics used in natural language processing tasks like style transfer and paraphrase generation."
        },
        {
            "section number": "2.1",
            "key information": "The core issue explored is the lack of effective semantic similarity metrics that align with human judgment for evaluating paraphrase and style transfer tasks."
        },
        {
            "section number": "2.3",
            "key information": "The survey categorizes existing semantic similarity metrics based on their effectiveness and alignment with human judgment."
        },
        {
            "section number": "5.3",
            "key information": "The primary challenge is that existing metrics either fail to accurately reflect human assessments or can be manipulated to produce misleading results."
        },
        {
            "section number": "6.3",
            "key information": "Future research should focus on refining semantic similarity metrics and exploring new methodologies that better capture human semantic understanding."
        },
        {
            "section number": "7.2",
            "key information": "The findings suggest that Word Mover's Distance is currently the most promising metric for measuring semantic similarity in style transfer and paraphrase tasks."
        }
    ],
    "similarity_score": 0.5193456519826353,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/Style-transfer and Paraphrase_ Looking for a Sensible Semantic Similarity Metric.json"
}