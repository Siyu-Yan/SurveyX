{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2006.01304",
    "title": "Rethinking Empirical Evaluation of Adversarial Robustness Using First-Order Attack Methods",
    "abstract": "We identify three common cases that lead to overestimation of adversarial accuracy against bounded first-order attack methods, which is popularly used as a proxy for adversarial robustness in empirical studies. For each case, we propose compensation methods that either address sources of inaccurate gradient computation, such as numerical instability near zero and non-differentiability, or reduce the total number of back-propagations for iterative attacks by approximating secondorder information. These compensation methods can be combined with existing attack methods for a more precise empirical evaluation metric. We illustrate the impact of these three cases with examples of practical interest, such as benchmarking model capacity and regularization techniques for robustness. Overall, our work shows that overestimated adversarial accuracy that is not indicative of robustness is prevalent even for conventionally trained deep neural networks, and highlights cautions of using empirical evaluation without guaranteed bounds.",
    "bib_name": "lee2020rethinkingempiricalevaluationadversarial",
    "md_text": "# Rethinking Empirical Evaluation of Adversarial Robustness Using First-Order Attack Methods\nKyungmi Lee 1 Anantha P. Chandrakasan 1\n# Abstract\nWe identify three common cases that lead to overestimation of adversarial accuracy against bounded first-order attack methods, which is popularly used as a proxy for adversarial robustness in empirical studies. For each case, we propose compensation methods that either address sources of inaccurate gradient computation, such as numerical instability near zero and non-differentiability, or reduce the total number of back-propagations for iterative attacks by approximating secondorder information. These compensation methods can be combined with existing attack methods for a more precise empirical evaluation metric. We illustrate the impact of these three cases with examples of practical interest, such as benchmarking model capacity and regularization techniques for robustness. Overall, our work shows that overestimated adversarial accuracy that is not indicative of robustness is prevalent even for conventionally trained deep neural networks, and highlights cautions of using empirical evaluation without guaranteed bounds.\narXiv:2006.01304v1\n# 1. Introduction\nRobustness against adversarial examples (Szegedy et al., 2013; Biggio & Roli, 2018) is becoming an important factor for designing deep neural networks, resulting in increased interest in benchmarking architectures (Su et al., 2018) and regularization techniques (Madry et al., 2017; Jakubovitz & Giryes, 2018) for robustness. An essential but challenging methodology for understanding adversarial robustness is precise evaluation of adversarial robustness. For a bounded adversarial example (i.e., within an \u03f5-ball in Lp norm from an unperturbed input sample x), adversarial robustness boils down to the existence of a perturbation r (\u2225r\u2225p < \u03f5) such that a deep neural network\u2019s predictions on x and x + r\n1Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA. Correspondence to: Kyungmi Lee <kyungmi@mit.edu>.\nare different, which is difficult to (dis)prove for a highdimensional input sample. Consequently, empirically testing the prediction using r generated by bounded first-order attack methods such as Fast Gradient Sign Method (Goodfellow et al., 2014) and Projected Gradient Descent (Madry et al., 2017), which are algorithms that generate r efficiently with few back-propagations, became a popular approach. Then, accuracy against samples generated by those attack methods is taken as a proxy for adversarial robustness for the entire dataset (e.g., test set). Nevertheless, this approach only yields an upper bound of robustness; that is, failure of attack methods to find adversarial examples might not imply true robustness. Notably, gradient masking (Papernot et al., 2017; Tramr et al., 2017; Athalye et al., 2018) inflates adversarial accuracy by inducing gradients used by attack methods to be inaccurate. Therefore, it is important to understand when failure of attack methods is not indicative of robustness in order to obtain more precise metric for empirical evaluation of adversarial robustness. In this work, we identify three common cases in which bounded first-order attack methods are unsuccessful due to superficial reasons that do not indicate robustness: 1) cross-entropy loss close to zero resulting in inaccurate computation of gradients, 2) gradient shattering (Athalye et al., 2018) induced by non-differentiable activation and pooling operations, and 3) certain training conditions inducing deep neural networks to be less amenable to first-order approximation, increasing the number of iterations for iterative attacks to successfully find adversarial examples. We observe these phenomena are prevalent in various conventionally trained deep neural networks across different architectures and datasets, not only confined to specific defenses intentionally designed to cause gradient masking. For each case, we propose compensation methods to address the cause (Section 3). We demonstrate the impact of these phenomena using case studies on model capacity and regularization techniques (Section 4). We further analyze transferability of compensation methods for black-box scenarios (Section 5), and whether these phenomena can explain the gap between empirical adversarial accuracy and verified lower bounds of robustness (Wong & Kolter, 2017; Tjeng et al., 2019) (Sec-\nWe demonstrate the impact of these phenomena using case studies on model capacity and regularization techniques (Section 4). We further analyze transferability of compensation methods for black-box scenarios (Section 5), and whether these phenomena can explain the gap between empirical adversarial accuracy and verified lower bounds of robustness (Wong & Kolter, 2017; Tjeng et al., 2019) (Sec-\ntion 6). We conclude this paper by linking our finding back to related work (Section 7), and with a short remark on future work (Section 8).\n# 2. Preliminaries\n# 2.1. Notations and definitions\nThroughout this paper, we use x to denote an unperturbed (clean) input and f(\u00b7) to represent a neural network. Also, we denote output pre-softmax logits of a neural network given x as z = f(x). We only consider the classification task, and use cross-entropy loss for training and generating adversarial samples. Naturally, the predicted label is y = argmax i zi where zi represents the value of ith logit. Given logits z and the ground truth label t, we express the loss as l(z, t). Gradients of the loss with respect to x are denoted as g := \u2202l(z,t) \u2202x = \u2202l(f(x),t) \u2202x .\nWe consider adversarial robustness within an \u03f5-ball defined for Lp norm around an input sample x that is correctly predicted by a neural network f. That is, if there exists a perturbation r such that \u2225r\u2225p < \u03f5 and f(x + r) \u0338= f(x), we claim that f is not robust for x.\n# 2.2. Experimental setup\nDataset We use CIFAR-10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and TinyImageNet (a down-sampled dataset from Russakovsky et al. (2015)) to analyze and benchmark adversarial accuracy. The images are normalized to the range [0, 1] for both training and testing, and further pre-processing includes random crop and flips during training. We randomly split 10% of training samples for validation purpose. For Section 6, we additionally use MNIST (LeCun et al., 1998) and follow pre-processing of Wong & Kolter (2017) for MNIST and CIFAR-10.\nNeural network architectures We examine various design choices, including different model capacity, usage of batch normalization (Ioffe & Szegedy, 2015), and residual connections (He et al., 2016). For CIFAR-10 and SVHN, we consider a Simple model with 4 convolutional layers and 2 fully connected layers, a Simple-BN model that has a batch normalization layer following each convolutional layer in a Simple model, and a WideResNet (WRN) (Zagoruyko & Komodakis, 2016) with depth of 28 layers. For TinyImageNet, we use a VGG-11 (Simonyan & Zisserman, 2014) with and without batch normalization, and a WRN with depth of 50 layers. Details of architectures and their training hyperparameters are described in Appendix A. Adversarial attack methods We examine bounded firstorder attack methods that are popularly used to study adversarial robustness for L2 and L\u221enorms, for their untargeted adversarial examples, primarily under full white-\nbox setting. In particular, we consider Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), Random-FGSM (R-FGSM) (Tramr et al., 2017), and Projected Gradient Descent (PGD) (Madry et al., 2017). FGSM computes a perturbation r as r = \u03f5 \u00b7 sign(g) for L\u221enorm using a single back-propagation to obtain g. R-FGSM modifies FGSM by adding a random perturbation before computing gradients. PGD uses iterative update to compute a perturbation after random initialization, and each iteration evaluates gradients as in FGSM. For L2 norm, sign is replaced with dividing by \u2225g\u22252 to produce a unit vector.\nImplementation1 All experiments are implemented with PyTorch (Paszke et al., 2017). We use AdverTorch (Ding et al., 2019) as a framework to implement attack methods.\n# 3. Analysis on failure modes of attacks\nIn this section, we analyze when bounded first-order attack methods fail to find adversarial examples, and identify three cases in which such failure does not indicate robustness. We provide compensation methods for each case to improve the evaluation metric.\nFirst-order attack methods use gradients to compute the perturbation direction. However, when the value of loss function becomes zero, gradients are naturally zero and do not provide meaningful information about the perturbation direction. Similar phenomenon occurs when the value of loss function is not exactly zero, but very small so that limited numerical precision and stability contaminate gradients to be no longer useful. Here we analyze this (near) zero loss phenomenon and propose simple methods to compensate for this phenomenon.\nAnalysis Cross-entropy loss gets small when pre-softmax logits z have large \u201cmargin\u201d, the gap between logits corresponding to the most likely and the second most likely labels, and often becomes zero due to limited numerical precision. However, logit margins can be simply inflated by weight matrices with large values, for instance when no regularization that can penalize large weights is applied. To illustrate, if a matrix W for a linear equation u = Wv gets multiplied by a constant c > 1, the difference between ith and jth element of u will also be inflated by c. Taking exponential on z with large margin to compute loss can easily result in near zero loss.\nFrom an experiment using a Simple model trained without explicit regularization on CIFAR-10, we also observe a symptom of gradient masking, specifically a black-box\nTable 1. Accuracy of neural networks against first-order attack methods in the order of FGSM/R-FGSM/PGD for \u03f5 = 4 255 in L\u221enorm. We apply compensation methods for zero loss and innate non-differentiability discussed in Sections 3.1 and 3.2. As compensation methods are applied in cascading manner (i.e., samples on which baseline attacks fail are subjected to compensation methods), we set the number of random starts for baseline R-FGSM and PGD to be same as the total number of evaluations compensation methods use for fair comparison. All models are trained without explicit regularization on the specified dataset.\nDataset\nArchitecture\nAccuracy (%)\nClean\nAttack Baseline\nZero loss\nNon-differentiability\nBoth\nCIFAR-10\nSimple\n84.75\n19.50 / 29.81 / 2.55\n10.79 / 29.11 / 1.68\n18.41 / 29.17 / 2.54\n8.74 / 28.31 / 1.67\nSimple-BN\n87.09\n28.66 / 28.39 / 6.26\n9.93 / 19.03 / 0.10\n26.92 / 28.05 / 6.11\n6.89 / 17.93 / 0.07\nWRN 28\n91.65\n21.90 / 20.79 / 0.02\n11.34 / 13.46 / 0\n15.87 / 19.76 / 0.02\n5.94 / 11.25 / 0\nSVHN\nSimple-BN\n94.29\n28.60 / 43.36 / 2.80\n23.81 / 42.21 / 2.59\n24.35 / 42.60 / 2.60\n19.10 / 41.31 / 2.38\nWRN 28\n95.42\n49.74 / 58.22 / 4.06\n45.46 / 57.04 / 3.73\n39.69 / 56.70 / 3.79\n34.30 / 55.05 / 3.69\nTinyImageNet\nVGG 11\n50.32\n11.32 / 20.16 / 7.94\n7.44 / 16.56 / 4.10\n11.30 / 20.62 / 7.98\n7.44 / 16.86 / 4.22\nVGG-BN 11\n50.72\n4.16 / 11.68 / 0.64\n3.22 / 10.70 / 0.48\n3.82 / 11.80 / 0.68\n3.06 / 11.00 / 0.48\nWRN 50\n57.24\n19.78 / 23.40 / 2.02\n3.36 / 9.86 / 0.40\n18.24 / 23.48 / 1.58\n2.88 / 10.02 / 0.36\nattack being stronger than a white-box attack (Athalye et al., 2018) (full detail in Appendix B.1). While this implies that adversarial accuracy of the model could have been overestimated, we also find many samples on which an attack (e.g., FGSM) fails have (near) zero loss (< 10\u22128) induced by large logit margin. Overall, we cannot conclude that failure of attacks to find adversarial examples in case of (near) zero loss is indicative of robustness. Compensation A straightforward way to account for this (near) zero loss phenomenon is to ensure that the value of loss is sufficiently large so that gradients can be computed accurately. First, we consider rescaling pre-softmax logits z by dividing with a fixed constant T; that is, we compute gradients for loss on rescaled logits, l( f(x) T , t) where t is ground truth labels. When T > 1, absolute value of logits decrease, leading to larger cross-entropy loss. Alternatively, we consider changing t from ground truth labels to other classes when computing loss, which essentially gives same expression as targeted attacks. Since loss with respect to ground truth labels is small, changing target class will increase loss. Note that still our interest is to find untargeted adversarial examples although we change loss function to be targeted to ensure large value. After increasing the value of loss using either approach, we apply first-order attack methods as usual. We find that changing target labels to be the second most likely classes generally gives the best compensation method for white-box attacks compared to rescaling logits and other possible target labels (e.g. randomly chosen classes or the least likely classes), increasing success rate of attack methods up to 11% for FGSM and 4.5% for PGD (\u03f5 = 8 255, L\u221e) for the above-mentioned Simple model. Quantitative comparison of these methods is presented in Appendix B.1. Impact We benchmark how this phenomenon affects evalu-\nattack being stronger than a white-box attack (Athalye et al., 2018) (full detail in Appendix B.1). While this implies that adversarial accuracy of the model could have been overestimated, we also find many samples on which an attack (e.g., FGSM) fails have (near) zero loss (< 10\u22128) induced by large logit margin. Overall, we cannot conclude that failure of attacks to find adversarial examples in case of (near) zero loss is indicative of robustness.\nation against first-order attack methods by comparing adversarial accuracy against baseline attacks (i.e., vanilla FGSM, R-FGSM, and PGD) and attacks with the compensation method using the second most likely classes as targets for computing loss. We apply the compensation method to samples on which baseline attacks fail and report resulting accuracy (Table 1, Column \u2018Zero loss\u2019). Gradient masking induced by zero loss was previously observed by Carlini & Wagner (2017) for their analysis of defensive distillation (Papernot et al., 2016b), which used temperature softmax (same as rescaling logits) during the distillation step. However, we find that this phenomenon is not confined to specific defenses that deliberately caused gradient masking; we can see prevalence of this phenomenon among many conventionally trained models as shown in Table 1. We can expect that this phenomenon will affect models trained without explicit regularization that have weight matrices with large magnitudes more severely compared to models trained with regularizations that penalize large weights such as weight decay. For a Simple model trained on CIFAR-10, we verify that compensating for this phenomenon decreases adversarial accuracy by 4.48% and 0.19% for FGSM and PGD (\u03f5 = 4 255, L\u221e) when weight decay is applied, compared to 8.71% and 0.87% when no regularization is used.\n# 3.2. Innate non-differentiability\nNon-differentiability of functions that are in the computation graph for back-propagation causes gradient shattering, a type of gradient masking identified by Athalye et al. (2018). In this section, we analyze how innate non-differentiability induced by popularly used Rectified Linear Unit (ReLU) and max pooling subtly affects attack methods, and how\nto compensate for this phenomenon using Backward Pass Differentiable Attack (BPDA) (Athalye et al., 2018).\nto compensate for this phenomenon using Backward Pass Differentiable Attack (BPDA) (Athalye et al., 2018). Analysis A layer using ReLU as an activation function passes gradients only through non-negative valued neurons, as negative valued neurons are set to be zero after ReLU. However, adding perturbations r can \u201cswitch\u201d some of negative valued neurons, which did not contribute to gradients originally, to take non-negative values (or vice versa) during forward-propagation. In such cases, gradients are no longer accurate as the effective neurons contributing to the final prediction are changed. A similar problem exists for max pooling when perturbations change the maximum valued neurons in each pooling window. We analyze how often this switching happens for ReLU and max pooling when perturbations are added to inputs. On a Simple model trained without explicit regularization on CIFAR-10, we observe that more neurons switch for larger perturbation size \u03f5, and the fraction of neurons that switch can be significant, especially for max pooling (7.73% of ReLU neurons and 20.60% of max pooling neurons switch for a FGSM attack with \u03f5 = 8 255 in L\u221enorm). Compensation BPDA provides a method to approximate gradients for non-differentiable functions, by substituting such functions with similar but differentiable functions during back-propagation. Although BPDA was originally used to break defenses that relied on non-differentiability, we use BPDA to smoothly approximate behaviors of ReLU and max pooling around their non-differentiable points. Substituting ReLU with softplus and max pooling with Lp norm pooling with p = 5 can be used as a default setting for BPDA especially when compensating for both this and the zero loss phenomena. Detailed comparison of substitute functions are shown in Appendix B.2. Impact Similar to the zero loss phenomenon, we benchmark how innate non-differentiability affects adversarial accuracy using softplus and p = 5 or 10 as the compensation method (Table 1, Column \u2018Non-differentiability\u2019). We observe that non-differentiability generally gives subtle difference (\u223c1%) especially for R-FGSM and PGD. Nevertheless, there are models significantly affected by nondifferentiability, such as a WRN 28 trained on SVHN, which shows more than 10% difference for FGSM. Also, FGSM seems to be more affected than other attack methods, which can be explained by smaller effective step size of R-FGSM and PGD, since smaller \u03f5 results in less frequent switching of neurons. For example, PGD uses small step size (< \u03f5) for each iteration, and R-FGSM typically takes a step size of \u03f5 2 instead of \u03f5 due to added random perturbation. We also report adversarial accuracy when attacks are compensated for both zero loss and non-differentiability (Table 1, Column \u2018Both\u2019). Compensating for both phenomena ad-\nWe also report adversarial accuracy when attacks are compensated for both zero loss and non-differentiability (Table 1, Column \u2018Both\u2019). Compensating for both phenomena ad-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7cc3/7cc3ee97-013f-4d72-9d2a-b959828e13ee.png\" style=\"width: 50%;\"></div>\nFigure 1. (Above) Adversarial accuracy against a PGD attack (\u03f5 = 0.5, L2, compensated for zero loss and non-differentiability) of WRN 28 models trained on CIFAR-10 with different regularization conditions, as a function of the number of iterations the attack uses. Shown in log-log scale. (Below) Comparison of different compensation methods (Eigen and BFGS) discussed in Section 3.3 on a WRN 28 trained with excessive weight decay (same as in the above figure), under the same number of back-propagations required to find adversarial examples.\nditionally accounts for up to 5% (e.g., WRN 28 models trained on CIFAR-10 and SVHN evaluated against FGSM) compared to only compensating for one of them.\n# 3.3. Require more iterations\nWe observe that certain training conditions increase the number of iterations required to find adversarial examples for iterative attack methods, resulting in increased adversarial accuracy when evaluated against attacks using small fixed number of iterations. We propose to incorporate second-order information to reduce the total number of backpropagations when this phenomenon occurs.\nAnalysis We find that applying weight decay excessively (increasing strength of weight decay as training progresses) for a WRN 28 trained on CIFAR-10 improves adversarial accuracy against PGD (\u03f5 = 0.5, L2, compensated for zero loss and non-differentiability) using small number of iterations, but eventually does not manifest in a benefit compared to a model trained without explicit regularization for large number of iterations (Figure 1, above). For example, a model with excessive weight decay is evaluated to have 7.11% of\nadversarial accuracy compared to only 0.12% of a model with no regularization when PGD uses 5 iterations, but it ultimately shows less than 0.1% of adversarial accuracy when the number of iterations is increased to 100. This observation can be concerning when one uses attacks with a small number of iterations, which can be typical when evaluating complex neural networks where back-propagations are computationally expensive, to compare different training conditions and concludes that simply applying weight decay excessively can provide advantage. Compensation While using a large number of iterations is an uncomplicated way to prevent overestimation of adversarial accuracy, we investigate methods to reduce the number of back-propagations needed and to better understand why certain conditions require more iterations. A couple of plain observations are that random initialization of PGD affects the success of subsequent first-order iterations, which is the well-known reason for using multiple random starts when attack methods have stochasticity, and that successful initialization (from which first-order iterations find adversarial examples) leads to larger increase of loss and the size of gradients (in L2 norm) per iteration. Although the observations themselves can be trivial, they hint that initializing to a point where gradients change rapidly, thus with high curvature, might help subsequent first-order iterations to find adversarial examples easily. However, exact curvature of loss is second-order information that is computationally expensive and numerically unstable to obtain. Thus we consider a method that can approximate the principal eigenvector (corresponding to the largest eigenvalue) of the Hessian, which provides information on which direction loss changes fastest. We adopt a single step of power iteration (Eq (1)) and finite difference method (Eq (2)) of Miyato et al. (2019), which roughly approximates the principal eigenvector u of Hessian H as:\n(1)\n(2)\n\ufffd \ufffd where d is randomly sampled from N(0, I) and normalized to be a unit vector. This method uses two additional backpropagations to compute u, which we use as a direction for initialization of PGD (PGD + Eigen) instead of a random\nWe also examine Quasi-Newton method, specifically BFGS, which approximates the inverse of the Hessian used to compute Newton direction. To simplify computational overhead, we only use a single iteration to update the Hessian, and omit line search and instead use perturbation size \u03f5 to obtain initialization (PGD + BFGS). This method also adds two additional back-propagations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0203/0203f50f-649e-46b1-b046-674cfb07e9be.png\" style=\"width: 50%;\"></div>\nImpact We test the effectiveness of these two methods on the above-mentioned WRN 28 trained with excessive weight decay, by comparing adversarial accuracy against PGD attacks using these methods as initialization, under the same total number of back-propagations used for both initialization and first-order iteration (Figure 1, below). We find that both methods provide stronger attack; for example, adversarial accuracy against PGD + Eigen and PGD + BFGS are 4.82% and 3.60% compared to 7.11% of baseline PGD when using only 5 back-propagations (equivalent to 5 iterations for baseline PGD, and 3 first-order iterations for PGD + Eigen and PGD + BFGS). Thus, utilizing approximate second-order information reduces the total number of backpropagations to achieve similar success rate of attacks when this phenomenon occurs.\n# 4. Case study\nIn this section, we investigate how the three phenomena inducing overestimation of adversarial accuracy affect practically important cases, such as benchmarking the trade-off between model capacity and adversarial robustness, and comparison of regularization techniques.\n# 4.1. Model capacity\n4.1.1. TRAINING MODELS WITH DIFFERENT WIDTH\nFirst, we consider comparing adversarial accuracy among models with the same architecture but with different width (number of output neurons in each layer). Several studies postulate that models with larger width provide better adversarial robustness (Madry et al., 2017; Deng et al., 2019), by showing better adversarial accuracy of those models against FGSM or PGD. We examine this claim by measuring adversarial accuracy of Simple models with different relative width trained on CIFAR-10 with fixed weight de-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59e3/59e31af4-7ab3-47fb-83a9-7b1b97e37629.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">100.0 75.0 56.2 42.2 31.6 23.7 17.8 13.3 10.0 7.5 Surviving weights (%)</div>\nFigure 3. Change of clean and adversarial accuracy through iterative pruning on an over-parameterized WRN 28 (trained on CIFAR-10), trained and finetuned without explicit regularization (above) and with weight decay (below). Dashed and solid lines represent accuracy against baseline attacks and compensated attacks, respectively. The total number of back-propagations is fixed to 9.\ncay, against both baseline and compensated attacks under the same number of evaluations (i.e., random starts) and back-propagations (i.e., gradient computations for either iterations of PGD or initialization of methods in Sec 3.3) (Figure 2). We find that although models with larger width indeed show better adversarial accuracy, their benefit over smaller models could have been overstated; for example, a PGD attack (\u03f5 = 1.0, L2) gives 10.14% difference in adversarial accuracy between the models with width 1 and 16, but compensating for zero loss and non-differentiability results in only 0.41% difference. Although for other attacks we observe less extreme gaps between baseline and compensated attacks, we generally find that adversarial accuracy of models with larger width tends to be overestimated especially due to the zero loss phenomenon.\n# 4.1.2. WEIGHT PRUNING\nAs another approach to benchmark the trade-off between model capacity and adversarial robustness, we consider pruning an over-parameterized model and its effect on adversarial accuracy. We iteratively prune weights of a large WRN 28 (trained on CIFAR-10; details in Appendix C.2) along with finetuning (Han et al., 2015), both with and without fixed weight decay, and measure adversarial accuracy\nas in Section 4.1.1 (Figure 3). Without weight decay during training and finetuning, adversarial accuracy against baseline attacks drops significantly (> 25%) as more weights are pruned away. However, applying compensation methods show that adversarial accuracy actually drops less than 1%, similar to that of clean accuracy. The major source of discrepancy is the zero loss phenomenon, in that the original dense model\u2019s adversarial accuracy is overestimated due to this phenomenon and the pruned sparse models in fact rely less severely on this phenomenon. On the other hand, adversarial accuracy against baseline attacks increases by 3.5% through iterative pruning with weight decay, but compensating results in less than 0.4% of increase. We find that weight decay used during finetuning, which adds a large number of epochs (e.g., 10 epochs for finetuning \u00d7 10 pruning iteration \u2192100 additional epochs), can act similar to excessive weight decay discussed in Section 3.3. As a result, sparser models show higher adversarial accuracy against baseline PGD, and using initialization methods of Section 3.3 (e.g., PGD + Eigen) gives a more accurate evaluation in this scenario. This example illustrates how compensating for the phenomena discussed in Section 3 can prevent misleading conclusions. For example, for WRN 28 models and L2 PGD attacks we tested here, pruning does not affect adversarial accuracy significantly. However, without proper compensation, one might conclude that pruning negatively affects adversarial accuracy only observing the case without weight decay, or that pruning improves adversarial accuracy while also reducing the model size after experimenting with weight decay.\nThis example illustrates how compensating for the phenomena discussed in Section 3 can prevent misleading conclusions. For example, for WRN 28 models and L2 PGD attacks we tested here, pruning does not affect adversarial accuracy significantly. However, without proper compensation, one might conclude that pruning negatively affects adversarial accuracy only observing the case without weight decay, or that pruning improves adversarial accuracy while also reducing the model size after experimenting with weight decay.\n# 4.2. Regularization\nWe compare popular regularization techniques proposed for better generalization or robustness, on whether their adversarial accuracy is affected by the three phenomena we discussed. We train WRN 28 models with different regularization techniques on CIFAR-10, and report adversarial accuracy against baseline and compensated attacks (Table 2; details of hyperparameter in Appendix C.3). We observe that input gradient regularization (Ross & DoshiVelez, 2017) and adversarial training (Madry et al., 2017) are least affected by compensation methods, indicating that their robustness does not rely on the phenomena discussed in Section 3. Adversarial accuracy of other regularization methods, such as weight decay and spectral normalization (Miyato et al., 2018), partly seems to be overestimated by those phenomena although some of them show better adversarial accuracy even after compensation (e.g., a model trained with spectral normalization shows 21.64% and 5.25% decrease in adversarial accuracy for FGSM and PGD, respectively, when compensation methods are applied). Nevertheless,\nTable 2. Clean and adversarial (FGSM/PGD, \u03f5 = 4 255, L\u221e, total 9 back-propagations for PGD) accuracy (%) of WRN 28 models trained using different regularization techniques on CIFAR-10. We compare adversarial accuracy against baseline and compensated attacks.\nRegularization\nClean\nAttack Baseline\nCompensated\nNone\n91.65\n21.90 / 0.02\n5.94 / 0\nWeight decay\n93.64\n31.47 / 0.02\n20.31 / 0.02\nWeight decay excess\n91.52\n47.83/ 1.85\n36.46 / 0.63\nSpectral norm1\n87.34\n31.50 / 7.50\n9.86 / 2.25\nOrthonormal2\n93.47\n27.19 / 0.03\n13.25 / 0.01\nInput gradient3\n89.75\n24.93 / 12.80\n23.18 /12.79\nAdversarial training4\n82.67\n67.62 / 65.58\n66.46 / 64.82\n1 \nModel\nLower bound\nAdversarial accuracy\nBaseline\nCompensated\nMNIST-A, \u03f5 = 0.4\n52.401\n54.96\n54.09\nMNIST-B, \u03f5 = 0.3\n75.812\n78.96\n77.35\nCIFAR-A, \u03f5 =\n2\n255\n49.802\n51.76\n51.18\nCIFAR-B, \u03f5 =\n8\n255\n22.402\n23.49\n22.86\n1 \nthese regularization methods show different behavior for other model architectures and datasets, and those cases are reported in Appendix C.3.\n# 5. Transferability of compensated attacks\nWe focus on a black-box attack scenario in which adversarial examples are crafted using a surrogate model that is trained on the same dataset as a target model, but without access to parameters of a target model. We observe that the three phenomena inflating adversarial accuracy discussed in Section 3 can also affect evaluations in black-box setting when a surrogate model suffers from those phenomena. We also show that compensation methods produce transferable adversarial examples, partly accounting for overestimated black-box adversarial accuracy.\nWe first examine transferability of examples generated using each compensation method. For the zero loss phenomenon, we find that rescaling logits can provide examples with better transferability, especially when we assume attacks cannot access pre-softmax logits of a target model. Also, examples generated with initialization methods of Section 3.3 only show limited transferability, resulting in less than < 1% of difference for black-box adversarial accuracy when using PGD + Eigen.\nTo illustrate transferability, we measure average accuracy against black-box attacks among WRN 28 models trained on CIFAR-10 using different regularization techniques as in Secton 4.2, by using one of them as a surrogate and measuring accuracy on others. On average, compensating for zero loss and non-differentiability gave 11.16% and 2.90%\nTable 3. Comparison of lower bounds of robustness obtained with MILP (Tjeng et al., 2019) and empirical adversarial accuracy against both baseline and compensated PGD attacks (5 random starts for both; the total number of back-propagations is 50 for MNIST and 10 for CIFAR-10). Models are trained to be provably robust (Wong & Kolter, 2017) in stated \u03f5-ball for L\u221enorm. For each model, attacks use the same \u03f5 the model has been trained for as the maximum perturbation size.\ndifference when the models with no explicit regularization and excessive weight decay are used as surrogate models, respectively, for a PGD attack (\u03f5 = 0.5, L2, fixed to total 9 back-propagations). When the model with excessive weight decay is a surrogate, using PGD + Eigen accounted for an additional 0.88% on average. More results on transferability is shown in Appendix D.\n# 6. Comparison with verified lower bounds\nRecently proposed methods for provably robust adversarial training (Wong & Kolter, 2017; Sinha et al., 2017; Raghunathan et al., 2018) provide the guaranteed lower bound of robustness, although computational efficiency and scalability are challenging issues. Nevertheless, there is usually a gap between the guaranteed lower bound and adversarial accuracy that serves as the natural upper bound of robustness. We investigate whether this gap can be explained by the three phenomena responsible for overestimated adversarial accuracy.\nWe experiment on models from Wong & Kolter (2017) that are trained to be provably robust, and obtain lower bounds of those models with the verification approach based on MILP (Tjeng et al., 2019) that produces tight bounds for deep neural networks with ReLU activation. Details of these models are explained in Appendix E. Then, we compare adversarial accuracy against baseline and compensated PGD (Table 3). We find that the three phenomena at least partially explain the gap between empirical adversarial accuracy and verified lower bounds; compensation methods result in 0.58 \u22121.61% difference in adversarial accuracy, and can bring adversarial accuracy within 0.46% of the lower bound (e.g., CIFAR-B). The major source contributing to this gap is non-differentiability for models trained on MNIST, and zero loss for those trained on CIFAR-10.\n# 7. Related work\nAttack methods In this work, we use FGSM, R-FGSM, and PGD for the analysis on their failure cases. There are notable modifications to PGD, such as Basic Iterative Method (Kurakin et al., 2016) that omits random initialization of PGD or Momentum Iterative Method (Dong et al., 2017) that updates each iteration using the momentum term. Our compensation methods are based on these bounded first-order attacks, and provide more accurate gradient computation (Sec 3.1, 3.2) and efficient initialization for PGD (Sec 3.3). Although not examined in this work, Jacobian Saliency Map Attack (Papernot et al., 2016a) provides L0 norm attack method and uses gradients to find each pixel\u2019s importance. Alternatively, unbounded attack methods find adversarial examples with minimum perturbation size with optimization-based approaches (Szegedy et al., 2013; Carlini & Wagner, 2017; Chen et al., 2017). For unbounded attack methods, empirical evaluation typically compares the average perturbation size instead of measuring adversarial accuracy.\nBenchmarking robustness There has been increased interest in understanding adversarial robustness for various\ndesign choices, such as architectures (Su et al., 2018; Deng et al., 2019), activation quantization (Lin et al., 2019) for hardware efficiency, and regularization techniques for robustness (Madry et al., 2017; Cisse et al., 2017). These studies use empirical evaluation against adversarial attacks for numerical experiments.\n# 8. Conclusion\nOverestimated adversarial accuracy has been mainly investigated for defenses that often explicitly capitalized on obfuscated gradients (Athalye et al., 2018) or zero loss (Papernot et al., 2016b; Carlini & Wagner, 2017). In this work, we demonstrate that sources of overestimated adversarial accuracy exist for many conventionally trained deep neural networks, across different architectures and datasets. The three common cases are 1) zero loss that induces gradient computation to be inaccurate due to numerical instability, 2) innate non-differentiability of ReLU and max pooling that can \u201cswitch\u201d when perturbations are added, resulting in a different set of effective neurons contributing to the final prediction for back-propagation (to compute gradients) and forward-propagation (perturbations are tested for effectiveness), and 3) requiring more iterations to successfully find adversarial examples for models trained with certain conditions, such as excessive application of weight decay, thus inflating adversarial accuracy against iterative attacks with small fixed number of iterations. We analyze consequences of these three cases with experiments on different model capacity and regularization techniques, by comparing adversarial accuracy before and after compensating for these three cases. Moreover, we show how these three cases can influence black-box adversarial accuracy, and partially account for the gap between empirical adversarial accuracy and verified lower bounds of robustness. Nevertheless, the three cases we identified might not be an exhaustive list responsible for overestimated adversarial accuracy. We still observe a gap between compensated adversarial accuracy and the exact robustness obtained by MILP (Tjeng et al., 2019) for MNIST-A in Table 3. This implies that there might exist other phenomena accounting for this gap or that better compensation methods exist for the cases we proposed here. We think future work on investigating other types of gradient masking and compensation methods will benefit empirical studies by further sharpening the metric for adversarial robustness. Additionally, future work on theoretical analysis of our observations, such as models with larger width tend to rely more on zero loss (Sec 4.1.1) or excessive weight decay makes a WRN 28 model less amenable to first-order approximation (Sec 3.3, 4.1.2), can clarify whether they have fundamental implications or are artifacts.\n# References\n# Chen, P.-Y., Sharma, Y., Zhang, H., Yi, J., and Hsieh, C.J. Ead: Elastic-net attacks to deep neural networks via adversarial examples, 2017.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2016. doi: 10.1109/cvpr.2016.90. URL http:// dx.doi.org/10.1109/CVPR.2016.90.\nJakubovitz, D. and Giryes, R. Improving dnn robustness to adversarial attacks using jacobian regularization, 2018.\nRoss, A. S. and Doshi-Velez, F. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients, 2017.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.\n# A. Details of experimental setups\nSimple and Simple-BN architectures briefly described in Section 2.2 are explained in detail in Table A.1. For WRN 28, we modify the number of output channels and pooling window size to fit with smaller input dimension of CIFAR-10 and SVHN compared to ImageNet. For VGG and WRN used for TinyImageNet, we use the architecture defined as in TorchVision, and only modify final pooling and fully connected layer\u2019s dimension to fit downscaled TinyImageNet (3 \u00d7 64 \u00d7 64 with 200 classes).\nTable A.1. Description of neural network architectures used in this paper. Convolution layers are specified as (output channel, input channel, kernel height, kernel width, stride, padding). Maxpool layers are in (kernel height, kernel width, stride, padding), and fully connected (FC) layers are in (output channel, input channel).\nTable A.1. Description of neural network architectures used in this paper. Convolution laye channel, kernel height, kernel width, stride, padding). Maxpool layers are in (kernel height connected (FC) layers are in (output channel, input channel).\nModel Type\nDescription (w: width scaling factor)\nSimple\nConv1 : (w \u00d7 8, 3, 3, 3, 1, 1)\nConv2 : (w \u00d7 8, w \u00d7 8, 3, 3, 1, 1)\nMaxPool: (2, 2, 2, 0)\nConv3 : (w \u00d7 16, w \u00d7 8, 3, 3, 1, 1)\nConv4 : (w \u00d7 16, w \u00d7 16, 3, 3, 1, 1)\nMaxPool: (2, 2, 2, 0)\nFC1 : (w \u00d7 128, w \u00d7 16 \u00d7 8 \u00d7 8)\nFC2 : (10, w \u00d7 128)\nSimple-BN\nConvolution and FC layers are same as in Simple,\nbut Batch Normalization layer follows each\nConvolution layer.\nAs a default setting, we train for 100 epochs using Stochastic Gradient Descent (SGD) with momentum of 0.9 for CIFAR-10 and SVHN. For Simple-BN and WRN 28, we use starting learning rate of 0.1 and decay it by factor of 10 for every 40 epochs. For Simple, we start with learning rate of 0.01. Models for TinyImageNet are trained with Adam (\u03b21 = 0.9, \u03b22 = 0.99), with starting learning rate of 0.001. Learning rate decay is applied in the same manner. Default batch size is 128, unless GPU memory is insufficient. Different training conditions deviating from the default setting, including specific regularizations, are described when they are introduced in Appendix C.\n# A.3. Equations for attack methods\nhen, Random-FGSM (R-FGSM) modifies FGSM by adding a random perturbation before computing gradients:\nhere r is randomly sampled from normal distribution N(0, I). Projected Gradient Descent (PGD) updates xadv iteratively, nd the equation for step s + 1 can be expressed as:\n(A.1)\n(A.4)\n(A.5)\nhere r is random initialization vector as in the equation (A.2), and \u03b1 is a step size. Clip operation ensures the perturbation  be within the \u03f5-ball around the sample x. For L2 norm, sign operation is replaced with dividing by the size \u2225g\u22252 to tain a unit vector. That is, for FGSM:\n\u2225\u2225 Other attacks can be similarly modified. Since sign operation no longer exists, we can drop \u2018S\u2019 from  (e.g., FG(S)M and R-FG(S)M).\n# B. Additional analysis on failure of attacks\nB.1. Zero loss\nB.1.1. OMITTED ANALYSIS\nFirst, we train a Simple model (w = 4) without explicit regularization on CIFAR-10, and analyze the average loss and size of gradients (in L2 norm) (Table B.1). We characterize those statistics for samples on which a FGSM attack (\u03f5 = 8 255) succeeds and fails. Observe that samples on which the attack fails have smaller loss and size of gradients induced by large logit margin.\nTable B.1. Characterizing samples on which the white-box attack using FGSM (\u03f5 = 8 255) succeeds and fails for the value of loss, size of gradients, and logit statistics including margin and variance, for a Simple model with width scale factor of 4 trained without explicit regularization on CIFAR-10.\nTable B.1. Characterizing samples on which the white-box attack using FGSM (\u03f5 = 8 255) succeeds and fails for the value of loss, size of gradients, and logit statistics including margin and variance, for a Simple model with width scale factor of 4 trained without explici regularization on CIFAR-10.\nAttack succeed\nAttack fail\nLoss\n0.0643\n0.0011\nGradient\n2.0686\n0.0339\nLogit margin\n7.20\n18.73\nLogit variance\n73.44\n142.19\nFurthermore, we observe that a black-box attack on the above-mentioned model is stronger than a white-box attack: when the model with the same architecture, width, and initialization but trained independently using weight decay regularization (with hyperparameter 5 \u00d7 10\u22124) is used as a surrogate, a black-box FGSM attack (\u03f5 = 8 255) on the above-mentioned model results in 13.22% accuracy (in other words, 86.78% attack success). However, a white box FGSM attack with the same \u03f5 results in 14.04% accuracy, higher than that against the black-box attack. This observation indicates that a black-box attack is stronger than a white-box attack for this model, signaling a possible gradient masking. Additionally, we visualize how loss changes as a perturbation is added (Fig B.1) on a randomly chosen clean sample x on which the white-box attack fails but the black-box attack succeeds. This sample has zero cross-entropy loss. We can observe that moving along the direction of g (while \u03f52 = 0) does not increase loss even when the perturbation size is increased to \u03f51 = 16 255. However, notice that the black-box attack\u2019s gradients easily increase loss (Fig B.1 (a)). We also visualize loss when \u03f52 moves along the direction of gradients produced by changing the target label for loss to be the second most likely class (Fig B.1 (b)). This illustrates how gradients computed from zero loss do not give meaningful perturbation direction, and why failure of attacks in such case does not indicate robustness (i.e., existence of adversarial examples in other perturbation directions, such as gradients of the black-box attack).\n# B.1.2. COMPARISON OF COMPENSATION METHODS\nWe compare the effectiveness of compensation methods discussed in Section 3.1, by comparing accuracy against compensated attack methods on a Simple model trained without explicit regularization on CIFAR-10 (Table B.2). Generally, changing the target label to be the second most likely class (corresponding to the second largest logit) gives the best compensation for this model.\nIn addition to comparing models trained with and without weight decay for how they are affected by the zero los phenomenon, as in Section 3.1, we additionally compare models with larger width or batch normalization (Table B.3). Fo\n(A.6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8bc9/8bc9b277-fc67-4cdd-a916-88b10b795bac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure B.1. Visualization of the value of loss (z-axis) evaluated for points x\u2217= x + \u03f51 \u00b7 sign(g) + \u03f52 \u00b7 sign(g\u2032), where g is gradient of loss with respect to the input sample computed using the target model itself, and g\u2032 is (a) gradients computed from a surrogate mode used for a black-box attack, and (b) gradients computed using the second most likely class as the label for loss instead of the ground truth</div>\nthe Simple architecture and CIFAR-10 dataset we investigate here, models with larger width or batch normalization ar affected more severely than a model with smaller width and no batch normalization.\nHere we present different choices of substitute functions and their effectiveness. First, as a substitute for ReLU, we consider softplus (\u03b1 = 2, threshold set to 2), CELU (\u03b1 = 2), and ELU (\u03b1 = 1). While softplus and CELU are continuously differentiable at zero regardless of the hyperparameter \u03b1 that controls the slope, \u03b1 for ELU has to be fixed to 1 to ensure differentiability at zero. Second, if a model uses max pooling, we substitute max pooling with Lp norm pooling. As p \u2192\u221e, Lp norm pooling gets closer to max pooling. Thus, sufficiently large but finite p can provide differentiable approximation of max pooling. Tables B.4 and B.5 show the effectiveness of each choice for a Simple model and a WRN 28 model. Tables B.6 and B.7 compare each choice when combined with zero loss compensation. We observe that using softplus and p = 5, 10 generally gives the most decrease in adversarial accuracy, when compensating for both this and the zero loss phenomena. However, when only compensating for this phenomenon, other substitute functions often give better compensation.\n# B.3. Require more iterations\nEigenvector approximation (PGD + Eigen) Miyato et al. (2019) proposed a method to approximate the principle eigenvector u of the Hessian H for a semi-supervised learning problem. Although their objective function for which they computed u and H is different from ours, we adopt their general principle to use power iteration and finite difference method to approximate u of our H = \u2207\u2207xl(f(x), t). Power iteration starts with a randomly sampled vector d, with an assumption that d has non-zero projection on the principle eigenvector u. Then, d is updated as d \u2190 H\u00b7d \u2225H\u00b7d\u22252 until it converges. Finite difference method is used to approximate H \u00b7 d when exact H is hard to obtain; since only the product H \u00b7 d is necessary, finite difference method computes the difference between first-order derivatives measured for x and x + \u03b4 \u00b7 d (\u03b4 > 0) as an approximation for H \u00b7 d. Furthermore, we use u produced by this approximation method as a direction for initialization instead of the perturbation r itself. The motivation behind this approach is to initialize PGD\u2019s first-order iterations to a point with high curvature, so that gradients and loss can increase rapidly (Sec 3.3). Since u is a unit vector in L2 norm, we use \u03f5 \u00b7 u as initialization for PGD in L2 norm, and clip( \ufffd dx \u03c0 \u00b7 \u03f5 \u00b7 u, min = \u2212\u03f5, max = \u03f5) for L\u221enorm where dx is the dimension of input samples. Note that this method requires back-propagation twice when approximating H \u00b7 d using finite difference method. Quasi-Newton method (PGD + BFGS) BFGS (BroydenFletcherGoldfarbShanno algorithm) is a second-order optimization method that approximates the Quasi-Newton direction for update H\u22121 \u00b7 g. Since we are only interested in the direction for initialization, rather than fully iterate with second-order method, we only consider approximating the inverse of the Hessian\nRescaling\nTargeted\nEvaluation type\nBaseline\nT = 10\nT = 50\nT = 100\nRandom\nLeast\nSecond\nGap\nClean\n84.75\n-\n-\n-\n-\n-\n-\n-\nFGSM\n19.50\n13.77\n15.24\n15.38\n14.59\n17.62\n10.79\n8.71\nR-FGSM\n29.81\n30.35\n30.44\n30.46\n30.02\n30.73\n29.11\n0.70\nL\u221e, \u03f5 =\n4\n255\nPGD\n2.55\n2.06\n2.26\n2.26\n2.03\n2.67\n1.68\n0.87\nFGSM\n14.04\n3.51\n4.40\n4.51\n4.20\n6.52\n3.08\n10.96\nR-FGSM\n15.03\n9.58\n11.29\n11.50\n11.12\n13.78\n7.23\n7.80\nL\u221e, \u03f5 =\n8\n255\nPGD\n0.24\n0\n0.01\n0.01\n0.01\n0.08\n0\n0.24\nFGM\n23.18\n20.45\n21.46\n21.54\n20.77\n22.67\n18.10\n5.08\nR-FGM\n39.64\n40.09\n40.23\n40.26\n39.81\n40.33\n39.08\n0.56\nL2, \u03f5 = 0.5\nPGD\n6.67\n5.06\n5.68\n5.75\n5.25\n6.85\n3.83\n2.84\nFGM\n17.75\n7.75\n9.37\n9.56\n9.03\n12.32\n6.14\n11.61\nR-FGM\n19.84\n17.88\n18.96\n19.04\n18.36\n20.32\n15.24\n4.60\nL2, \u03f5 = 1.0\nPGD\n3.82\n0.11\n0.28\n0.28\n0.25\n1.39\n0.03\n3.79\nwhere d is randomly sampled from N(0, I) and scaled to have a small size \u03b4. Note that the update in Eq (B.2) is sam as typical BFGS inverse update for the first iteration where H\u22121 0 is initialized as the identity matrix. Then, we us H\u22121 \u00b7 g = H\u22121 \u00b7 \u2202l(f(x),t) \u2202x as the direction for initialization.\nwhere d is randomly sampled from N(0, I) and scaled to have a small size \u03b4. Note that the update in Eq (B.2) is same as typical BFGS inverse update for the first iteration where H\u22121 0 is initialized as the identity matrix. Then, we use H\u22121 \u00b7 g = H\u22121 \u00b7 \u2202l(f(x),t) \u2202x as the direction for initialization. Similar to PGD + Eigen, this method requires two additional back-propagations for computing \u2206g. However, this method consumes more memory as it directly computes H\u22121 instead of the matrix-vector product H \u00b7 d. Since H\u22121 is a dx \u00d7 dx matrix, batch size might have to be reduced to accommodate significant additional memory of H\u22121.\n# C. Additional experiments on case studies\nAccuracy against compensated attacks is measured in a cascading manner, in which samples that survived the previous stage (i.e., an attack fails on that sample to find adversarial perturbation) are subjected to the next compensation method. For a single-step attack, such as FGSM or R-FGSM, we cascade compensation methods in following steps:\n2. Apply a compensation method for the zero loss phenomenon (default: change target labels to the second most likely classes) 3. Apply a compensation method for the non-differentiability phenomenon (default: BPDA with softplus as a substitute for ReLU and Lp norm pooling with p = 5 for max pooling)\nThis scheme results in 4 effective evaluations, and baseline attacks with stochasticity (e.g. R-FGSM) are set to have 4 random starts (i.e., a sample has to survive all four attempts to be considered accurate; in other words, this is equivalent to 4 cascading attacks but without compensation methods) for fair comparison. For iterative attacks, such as PGD, we cascade as\n(B.1)\n(B.2)\nTable B.3. The gap in adversarial accuracy (in %) when compensating for the zero loss phenomenon by changing target labels to be the second most likely classes. \u2018No Reg\u2019 represents a Simple model with width scale factor of 4 trained without explicit regularization. \u2018Weight Decay\u2019 represents the same model as \u2018No Reg\u2019, but trained with weight decay of 5 \u00d7 10\u22124. \u2018Width x4\u2019 represents a model with 4 times more neurons per layer compared to \u2018No Reg\u2019, thus is a Simple model with width scale factor of 16. \u2018Batch Norm\u2019 indicates a Simple-BN model, which is same as \u2018No Reg\u2019 except for batch normalization following each convolutional layer.\nEvaluation type\nNo Reg\nWeight Decay\nWidth x4\nBatch Norm\nL\u221e, \u03f5 =\n4\n255\nFGSM\n8.71\n4.48\n19.48\n18.73\nR-FGSM\n0.70\n0.59\n5.78\n9.36\nPGD\n0.87\n0.19\n4.51\n6.16\nL\u221e, \u03f5 =\n8\n255\nFGSM\n10.96\n8.12\n18.43\n19.99\nR-FGSM\n7.80\n3.89\n17.75\n15.68\nPGD\n0.24\n0.15\n1.20\n2.05\nL2, \u03f5 = 0.5\nFGM\n5.08\n2.41\n16.54\n16.78\nR-FGM\n0.56\n0.63\n2.72\n5.64\nPGD\n2.84\n0.57\n15.29\n12.68\nL2, \u03f5 = 1.0\nFGM\n11.61\n7.91\n27.34\n21.02\nR-FGM\n4.60\n1.73\n16.24\n15.51\nPGD\n3.79\n2.14\n19.62\n9.45\n1. Apply an attack without any compensation methods (e.g., plain PGD). 2. Apply PGD with an initialization method proposed in Sec 3.3 (default: PGD + Eigen) 3. Apply a compensation method for the zero loss phenomenon with plain PGD (default: chan second most likely classes)\n4. Apply both compensation methods in 2 and 3 together\n5. Apply a compensation method for the non-differentiability phenomenon along with compensation methods in 2 and 3 (default: BPDA with softplus as a substitute for ReLU and Lp norm pooling with p = 5 for max pooling)\nNote that for iterative attacks, we do not test for every possible combination of compensation methods for the three phenomena. Resulting scheme has 5 effective evaluations, and baseline attacks are set to have 5 random starts. For PGD attacks in subsequent experiments, we use total 9 back-propagations (9 iterations without initialization methods of Sec 3.3 or 7 iterations with those initialization methods) as a default value. When a compensation method is not effective for a given model, just using another random start can be more effective than using that compensation method. In such case, accuracy against baseline attacks can be lower than accuracy against compensated attacks.\n# C.1. Training models with different width\nIn this section, we present additional results on benchmarking the trade-off between adversarial accuracy and model widths. Details of training hyperparameters for models used in this section are described in Table C.1. Additional figures for CIFAR-10 dataset We measure how accuracy difference among Simple models with different width is affected by the perturbation size \u03f5 of attacks (Fig C.1 (a, b)). For both FG(S)M and PGD attacks, we observe that attacks with larger \u03f5 are affected more by compensation methods; in other words, accuracy against attacks with larger \u03f5 tends to be more overestimated for Simple models and CIFAR-10 dataset we consider in this example. We also measure adversarial accuracy for other architectures, Simple-BN models (Fig C.1 (c)) and WRN 28 models (Fig C.1 (d)). Similar to Simple models, the difference in adversarial accuracy between models with small and large widths is overestimated, and that difference reduces when compensation methods are applied. SVHN For SVHN dataset, we consider Simple-BN and WRN 28 models (same architectures as those for CIFAR-10 dataset) (Fig C.2). In contrast to models trained on CIFAR-10, models trained on SVHN show less accuracy difference even for baseline attacks. Interestingly, adversarial accuracy of larger models often drops (e.g. accuracy against FGSM attacks for Simple-BN with width=16), and compensating can increase accuracy difference between small and large models in such\nTable B.4. Adversarial accuracy (in %) of a Simple model, same as in Table B.2, for different attack types compensated fo non-differentiability using BPDA. We investigate three differentiable functions for substituting ReLU: Softplus (\u03b1 = 2, threshold CELU (\u03b1 = 2), and ELU (\u03b1 = 1) while fixing Lp-norm pool\u2019s p to be 5. Then, we sweep for p by fixing ReLU substitute functi the one achieved the best performance.\nReLU Substitute\nLp-norm Pool\nEvaluation Type\nBaseline\nSoftplus\nCELU\nELU\np = 2\np = 5\np = 10\nGap\nClean\n84.75\n-\n-\n-\n-\n-\n-\n-\nFGSM\n19.50\n18.41\n18.93\n18.68\n18.58\n18.41\n18.44\n1.09\nR-FGSM\n29.81\n29.17\n30.51\n29.92\n30.26\n29.17\n29.15\n0.66\nL\u221e, \u03f5 =\n4\n255\nPGD\n2.55\n2.54\n2.54\n2.50\n2.57\n2.50\n2.47\n0.08\nFGSM\n14.04\n12.33\n12.06\n12.15\n11.85\n12.06\n12.11\n2.19\nR-FGSM\n15.03\n14.88\n14.95\n14.72\n15.03\n14.72\n14.78\n0.31\nL\u221e, \u03f5 =\n8\n255\nPGD\n0.24\n0.21\n0.15\n0.16\n0.22\n0.15\n0.13\n0.11\nFGM\n23.18\n21.07\n22.43\n22.22\n21.66\n21.07\n20.59\n2.59\nR-FGM\n39.64\n38.85\n40.05\n39.72\n39.77\n38.85\n35.67\n3.97\nL2, \u03f5 = 0.5\nPGD\n6.67\n6.68\n6.57\n6.55\n6.56\n6.55\n6.64\n0.12\nFGM\n17.75\n16.07\n16.04\n16.25\n15.87\n16.04\n15.78\n1.97\nR-FGM\n19.84\n18.69\n19.66\n19.39\n19.06\n18.69\n17.98\n1.86\nL2, \u03f5 = 1.0\nPGD\n3.82\n3.90\n3.64\n3.63\n3.58\n3.63\n3.67\n0.24\ncase. Nevertheless, general magnitude of accuracy difference between small and large models is moderate (\u223c6%) compare to that for models trained on CIFAR-10 (\u223c25%). Results on SVHN indicate that even the same architectures and trainin conditions can show different behaviors depending on datasets.\ncase. Nevertheless, general magnitude of accuracy difference between small and large models is moderate (\u223c6%) compared to that for models trained on CIFAR-10 (\u223c25%). Results on SVHN indicate that even the same architectures and training conditions can show different behaviors depending on datasets. TinyImageNet We consider VGG 11, VGG-BN 11, and WRN 50 models with different relative widths for TinyImageNet (Fig C.3). VGG 11 and VGG-BN 11 models show similar pattern as models trained on CIFAR-10; adversarial accuracy of models with larger widths is overestimated, resulting in large accuracy difference between models for baseline attacks. However, behavior of WRN 50 models is more similar to that of models trained on SVHN, in that there is not significant difference in accuracy among models with different width.\nTinyImageNet We consider VGG 11, VGG-BN 11, and WRN 50 models with different relative widths for TinyImageNet (Fig C.3). VGG 11 and VGG-BN 11 models show similar pattern as models trained on CIFAR-10; adversarial accuracy of models with larger widths is overestimated, resulting in large accuracy difference between models for baseline attacks. However, behavior of WRN 50 models is more similar to that of models trained on SVHN, in that there is not significant difference in accuracy among models with different width.\n# C.2. Weight pruning\nFor weight pruning, we initially train a WRN 28 model with width scale factor of 10. The model is trained for 100 epochs using SGD with momentum of 0.9 as an optimizer, with starting learning rate of 0.1, which is decayed by factor of 10 every 40 epochs. We use early stopping based on the validation accuracy. Batch size is fixed to 128. To compare the impact of using weight decay, we train two models with and without weight decay of 5 \u00d7 10\u22124 with otherwise same training conditions as stated.\nusing SGD with momentum of 0.9 as an optimizer, with starting learning rate of 0.1, which is decayed by factor of 10 every 40 epochs. We use early stopping based on the validation accuracy. Batch size is fixed to 128. To compare the impact of using weight decay, we train two models with and without weight decay of 5 \u00d7 10\u22124 with otherwise same training conditions as stated. We iteratively remove weights with small magnitude as in typical weight pruning. To be specific, in each pruning iteration, we remove 25% of total weights from convolution layers based on their magnitude, and finetune for 10 epochs with learning rate of 0.001. Otherwise, finetuning conditions are same as training conditions. However, note that the optimizer is initialized at each pruning iteration so that the momentum term from previous iteration (which contains information on removed weights) does not affect current finetuning. We iterate this process for 9 cycles, and the proportion of surviving weights at the final stage is 7.5%.\nWe iteratively remove weights with small magnitude as in typical weight pruning. To be specific, in each pruning iteration, we remove 25% of total weights from convolution layers based on their magnitude, and finetune for 10 epochs with learning rate of 0.001. Otherwise, finetuning conditions are same as training conditions. However, note that the optimizer is initialized at each pruning iteration so that the momentum term from previous iteration (which contains information on removed weights) does not affect current finetuning. We iterate this process for 9 cycles, and the proportion of surviving weights at the final stage is 7.5%.\n# C.3. Regularization\nRegularization techniques are developed for better generalization, as for weight decay and spectral normalization (Miyato et al., 2018), or robustness against perturbations, as for orthonormal regularization (Cisse et al., 2017), Jacobian regularization (Ross & Doshi-Velez, 2017; Jakubovitz & Giryes, 2018), and adversarial training (Goodfellow et al., 2014; Madry et al., 2017). In this section, we present these techniques in detail, and provide additional experimental results.\nTable B.5. Adversarial accuracy (in %) of a WRN 28 (width scale factor: 2), trained on CIFAR-10 without explicit regularization, compensated for innate non-differentiability using BPDA. Details are same as in Table B.4, except for that this model does not use max\nTable B.5. Adversarial accuracy (in %) of a WRN 28 (width scale factor: 2), trained on CIFAR-10 without e compensated for innate non-differentiability using BPDA. Details are same as in Table B.4, except for that this mo pooling.\nReLU Substitute\nEvaluation Type\nBaseline\nSoftplus\nCELU\nELU\nGap\nClean\n91.65\n-\n-\n-\n-\nFGSM\n21.90\n15.87\n21.36\n20.06\n6.03\nR-FGSM\n20.79\n19.76\n23.71\n23.30\n1.03\nL\u221e, \u03f5 =\n4\n255\nPGD\n0.04\n0.04\n0.04\n0.04\n0.00\nFGSM\n17.57\n10.93\n15.38\n13.83\n6.64\nR-FGSM\n5.41\n4.43\n7.28\n6.50\n0.98\nL\u221e, \u03f5 =\n8\n255\nPGD\n0\n-\n-\n-\n-\nFGM\n47.30\n44.73\n45.46\n42.76\n4.54\nR-FGM\n45.13\n44.11\n46.87\n45.29\n1.02\nL2, \u03f5 = 0.5\nPGD\n22.58\n21.92\n18.15\n15.78\n6.80\nFGM\n45.33\n42.57\n40.75\n38.26\n7.07\nR-FGM\n36.18\n35.89\n37.05\n33.78\n2.40\nL2, \u03f5 = 1.0\nPGD\n16.29\n15.81\n11.09\n11.95\n5.20\nWeight decay penalizes the Frobenius norm of weight matrices, inducing weights to be smaller. Typically, weight decay is directly incorporated to optimizers. However, when written as an additional regularization term to loss function, weight decay can be expressed as:\nwhere f is a deep neural network model with total L layers, x is an input sample, and t is the ground truth label. l(\u00b7, \u00b7) is a standard cross-entropy loss, and \u03bb controls the strength of weight decay term. In this work, we consider fixed \u03bb throughout training process and increasing \u03bb as training progresses to excessively apply weight decay. Wi represents a weight matrix of the ith layer. Typically, we use \u03bb = 5 \u00d7 10\u22124 for fixed weight decay, and start with \u03bb = 1 \u00d7 10\u22124 and multiply it by factor of 10 every 40 epochs for excessive weight decay. We choose to increase \u03bb instead of using large fixed \u03bb to study excessive weight decay, because larger \u03bb (e.g. 10\u22122) at earlier epochs resulted in poor training that often did not escape starting loss and accuracy level. Spectral normalization (Miyato et al., 2018) was proposed to set the Lipschitz constant of each layer to be 1, by dividing weights of each layer with the estimated largest eigenvalue of that layer, so that training can be stabilized especially for Generative Adversarial Networks. For a weight matrix Wi corresponding to the ith layer, Miyato et al. (2018) computed the largest singular value of this layer \u03c3(Wi) using a single-step power iteration per a single forward-backward pass during training, then divide the weight matrix by \u03c3(Wi) to set the large singular value to be 1. We use spectral normalization layer after each convolution and linear layer (except for the final linear classifier), and remove batch normalization if it is originally used. Also, we do not use spectral normalization for convolution layers used for residual connections in WRN 28. Orthonormal regularization was used as a part of Parseval network (Cisse et al., 2017) that improved adversarial robustness of deep neural networks. In particular, orthonormal regularization induces each weight matrix to be orthonormal, so that the eigenvalues of W T i Wi become 1. The motivation behind this is similar to that of spectral normalization, in that both aim to control the Lipschitz constant of each layer. The resulting loss function to be optimized is:\nwhere I is an identity matrix with the same size as W T i Wi. Note that this formulation is simplified from what Cisse et al. (2017) used in their work, which included sampling and other regularization for convexity. We follow orthonorma\n(C.1)\n(C.2)\n<div style=\"text-align: center;\">Table B.6. Adversarial accuracy (in %) of a Simple model, same as in Table B.4, when compensated for both zero loss and innate non-differentiability. The zero-loss phenomenon is compensated by changing the targe labels to be the second most likely classes.</div>\nReLU Substitute\nLp-norm Pool\nEvaluation Type\nZero Loss Only\nSoftplus\nCELU\nELU\np = 2\np = 5\np = 10\nGap\nClean\n84.75\n-\n-\n-\n-\n-\n-\n-\nFGSM\n10.79\n8.74\n10.05\n9.45\n9.40\n8.74\n8.86\n2.23\nR-FGSM\n29.11\n28.31\n29.78\n29.11\n29.45\n28.31\n28.39\n0.80\nL\u221e, \u03f5 =\n4\n255\nPGD\n1.68\n1.70\n1.73\n1.70\n1.70\n1.70\n1.70\n-0.02\nFGSM\n3.08\n1.25\n1.62\n1.56\n1.13\n1.25\n1.43\n1.95\nR-FGSM\n7.23\n6.25\n7.17\n6.47\n6.64\n6.25\n6.28\n0.98\nL\u221e, \u03f5 =\n8\n255\nPGD\n0\n-\n-\n-\n-\n-\n-\n-\nFGM\n18.10\n14.94\n17.01\n16.56\n15.94\n14.94\n13.74\n4.36\nR-FGM\n39.08\n38.29\n39.47\n39.10\n39.26\n38.29\n34.33\n4.75\nL2, \u03f5 = 0.5\nPGD\n3.83\n3.90\n3.90\n3.88\n3.93\n3.88\n3.91\n-0.05\nFGM\n6.14\n3.11\n4.47\n4.41\n3.15\n3.11\n3.08\n3.06\nR-FGM\n15.24\n13.16\n14.95\n14.48\n13.90\n13.16\n12.16\n3.08\nL2, \u03f5 = 1.0\nPGD\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.04\n0.00\n<div style=\"text-align: center;\">ble B.7. Adversarial accuracy (in %) of a WRN 28, same as in Table B.5, compensated for both zero loss and innate nondifferentiabili</div>\nReLU Substitute\nEvaluation Type\nZero Loss Only\nSoftplus\nCELU\nELU\nGap\nClean\n91.65\n-\n-\n-\n-\nFGSM\n11.34\n5.94\n11.08\n10.58\n5.40\nR-FGSM\n13.46\n11.25\n15.21\n15.01\n2.21\nL\u221e, \u03f5 =\n4\n255\nPGD\n0\n-\n-\n-\n-\nFGSM\n7.89\n3.36\n6.87\n6.01\n4.53\nR-FGSM\n2.49\n1.3\n3.55\n3.27\n1.19\nL\u221e, \u03f5 =\n8\n255\nPGD\n0\n-\n-\n-\n-\nFGM\n21.32\n11.72\n21.1\n20.49\n9.60\nR-FGM\n27.74\n21.93\n30.15\n29.4\n5.81\nL2, \u03f5 = 0.5\nPGD\n0\n-\n-\n-\n-\nFGM\n15.55\n7.3\n14.59\n13.62\n8.25\nR-FGM\n12.51\n7.73\n14.91\n14.39\n4.78\nL2, \u03f5 = 1.0\nPGD\n0\n-\n-\n-\n-\nregularization used as in Lin et al. (2019) that adopted this simple formulation for convolution layers to improve adversarial robustness of models with activation quantization. Jacobian regularization (Jakubovitz & Giryes, 2018) or input gradient regularization (Ross & Doshi-Velez, 2017) can be thought as reducing the first-order term in Taylor\u2019s expansion when a small perturbation r is added to an input x: l(f(x + r), t) \u2243l(f(x), t) + rT \u00b7 \u2207xl(f(x), t) + O(\u2225r\u22252) (C.3) Jacobian regularization computes gradients for every logit zi (where z = f(x)) with respect to the input x, and the resulting loss function is:\nregularization used as in Lin et al. (2019) that adopted this simple formulation for convolution layers to improve adversarial robustness of models with activation quantization. Jacobian regularization (Jakubovitz & Giryes, 2018) or input gradient regularization (Ross & Doshi-Velez, 2017) can be thought as reducing the first-order term in Taylor\u2019s expansion when a small perturbation r is added to an input x: l(f(x + r), t) \u2243l(f(x), t) + rT \u00b7 \u2207xl(f(x), t) + O(\u2225r\u22252) (C.3)\nThis method can require significant additional back-propagations when the total number of classes C is large, as in the cas of TinyImageNet with 200 classes. More simply, input gradient regularization computes Jacobian not from logits z, bu\n(C.3)\n(C.4)\nDataset\nArchitecture\nTraining condition\nCIFAR-10,\nSVHN\nSimple\nEpochs: 100, Batch size: 128\nOptimizer: SGD with momentum of 0.9\nLearning rate: start with 0.01, decay by factor of 10 every 40 epochs\nRegularization: fixed weight decay of 5 \u00d7 10\u22124\nSimple-BN,\nWRN 28\nEpochs: 100, Batch size: 128\nOptimizer: SGD with momentum of 0.9\nLearning rate: start with 0.1, decay by factor of 10 every 40 epochs\nRegularization: fixed weight decay of 5 \u00d7 10\u22124\nTinyImageNet\nVGG 11\nEpochs: 100, Batch size: 128 (96 for scale factor 4)\nOptimizer: Adam (\u03b21 = 0.9, \u03b22 = 0.99)\nLearning rate: start with 10\u22124. decay by factor of 10 every 40 epochs\nRegularization: none\nVGG-BN 11\nEpochs: 100, Batch size: 128 (96 for scale factor 4)\nOptimizer: Adam (\u03b21 = 0.9, \u03b22 = 0.99)\nLearning rate: start with 10\u22123. decay by factor of 10 every 40 epochs\nRegularization: none\nWRN 50\nEpochs: 100, Batch size: 128\nOptimizer: Adam (\u03b21 = 0.9, \u03b22 = 0.99)\nLearning rate: start with 10\u22123. decay by factor of 10 every 40 epochs\nRegularization: fixed weight decay of 5 \u00d7 10\u22124\nIn this work, we consider the later form of input gradient regularization. For more details including theoretical justification of Jacobian regularization in relation to Lipschitz stability, we refer readers to Jakubovitz & Giryes (2018). Adversarial training (Goodfellow et al., 2014; Madry et al., 2017) achieved robustness by directly train a model on samples crafted by attack methods. Madry et al. (2017) analyzed that adversarial training can be thought as an optimization for min-max problem when a model f is parameterized with \u03b8:\n\u03b8\u2217= arg min \u03b8 l(f\u03b8(xadv), t) = arg min \u03b8 max x\u2032 l(f\u03b8(x\u2032), t)\nand that attack methods, such as PGD, are approximation for the inner maximization.\nC.3.2. ADDITIONAL EXPERIMENTS\nIn addition to the result presented in Section 4.2, we show how adversarial accuracy of regularization techniques is affected by the three phenomena, for different architectures and datasets. Details of training conditions for models used in thi section are shown in Table C.2.\nby the three phenomena, for different architectures and datasets. Details of training conditions for models used in this section are shown in Table C.2. First, we present more detailed results for WRN 28 models trained on CIFAR-10 using different regularization techniques in Table C.3. We measure adversarial accuracy against both baseline and compensated attacks for different perturbation sizes \u03f5 in L\u221eand L2 norm. Generally, adversarial accuracy against smaller perturbation \u03f5 tends to be less affected by compensation methods, manifested by relatively small gap between baseline and compensated adversarial accuracy for \u03f5 = 2 255 for L\u221enorm and \u03f5 = 0.3 for L2 norm. Then, we consider different architectures, Simple and Simple-BN, for CIFAR-10 dataset (Table C.4). We observe that spectral normalization and orthonormal regularization are less affected by compensation methods for these architectures, in contrast to WRN 28 architecture. For example, adversarial accuracy (PGD, \u03f5 = 4 255) of a WRN 28 with spectral normalization decreases by 5.25% when compensation methods are applied, but that for a Simple model only decreases by\n(C.6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b52/1b5212e1-10f9-44ea-b986-7a8f9c47a077.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c)</div>\n<div style=\"text-align: center;\">gure C.1. (a, b) Accuracy difference (with respect to the accuracy of the model with width=1) of Simple models with different relative idths, measured for different perturbation sizes \u03f5 in L2 norm using a single-step FG(S)M attack (a) and an iterative PGD attack (b). (c,  Accuracy difference (with respect to the accuracy of the model with width=1 for Simple-BN and width=2 for WRN 28) of Simple-BN odels (c) and WRN 28 models (d) with different relative widths. For all plots, dashed and solid lines represent accuracy against baseline nd compensated attacks, respectively.</div>\n0.02%. For a Simple model with orthonormal regularization, compensation methods with PGD turn out to be not effective and result in even higher adversarial accuracy compared to baseline PGD. Therefore, the effectiveness of regularization methods, especially whether they rely on the three phenomena inflating adversarial accuracy, might be dependent on model\n0.02%. For a Simple model with orthonormal regularization, compensation methods with PGD turn out to be not effective and result in even higher adversarial accuracy compared to baseline PGD. Therefore, the effectiveness of regularization methods, especially whether they rely on the three phenomena inflating adversarial accuracy, might be dependent on model architectures. Finally, we run similar experiments for different datasets, SVHN (Table C.5) and TinyImageNet (Table C.6). Models trained on SVHN generally show much less gap between adversarial accuracy against baseline and compensated attacks compared to those trained on CIFAR-10, even when architectures are exactly same for both SVHN and CIFAR-10. Also, adversarial accuracy of models using weight decay and spectral normalization is affected less severely by compensation methods for SVHN. WRN 50 models trained on TinyImageNet are benchmarked for limited regularizaton techniques. In contrast to CIFAR-10 and SVHN, Jacobian regularization seems to be affected by compensation methods by 6% of accuracy difference against a PGD attack. Overall, these observations imply that whether adversarial accuracy of regularizaton techniques is overestimated can be complicated by model architectures and datasets.\nFinally, we run similar experiments for different datasets, SVHN (Table C.5) and TinyImageNet (Table C.6). Models trained on SVHN generally show much less gap between adversarial accuracy against baseline and compensated attacks compared to those trained on CIFAR-10, even when architectures are exactly same for both SVHN and CIFAR-10. Also, adversarial accuracy of models using weight decay and spectral normalization is affected less severely by compensation methods for SVHN. WRN 50 models trained on TinyImageNet are benchmarked for limited regularizaton techniques. In contrast to CIFAR-10 and SVHN, Jacobian regularization seems to be affected by compensation methods by 6% of accuracy difference against a PGD attack. Overall, these observations imply that whether adversarial accuracy of regularizaton techniques is overestimated can be complicated by model architectures and datasets.\n# D. Additional experiments on transferability\nWe experiment on how compensation methods affect black-box adversarial accuracy, where we craft adversarial examples from a surrogate model without accessing parameters of a target model. Gap between adversarial accuracy against baseline and compensated attacks can indicate whether black-box adversarial accuracy is overestimated originally. We measure black-box transferability of adversarial examples generated using compensation methods among Simple models with different relative width (Table D.1), WRN 28 models with different regularization techniques (Table D.2), and different\n<div style=\"text-align: center;\">(d)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb3a/eb3a9889-f035-42dd-bf4b-050f4715fdd1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3437/3437d6c3-70df-4434-ac3b-97e928fd9168.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure C.3. Accuracy difference (with respect to the model with width=1) of VGG 11 models (a), VGG-BN 11 models (b), and WR 50 models (c) trained on TinyImageNet with different relative widths. Dashed and solid lines represent accuracy against baseline an compensated attacks, respectively.</div>\narchitectures for CIFAR-10 dataset (Table D.3). Note that we rescale logits (T = 10) to compensate for the zero loss phenomenon, instead of changing target labels, as mentioned in Section 6.\n# E. Experimental details on verified lower bounds\nHere we elaborate on experimental setup for Section 6. We introduce each model considered, and methods to obtain the lower bound of each model. We follow data preprocessing of Wong & Kolter (2017) for MNIST and CIFAR-10 dataset, which additionally includes normalization in case of CIFAR-10; the size of perturbation \u03f5 is scaled according to the normalization so that the pixel level perturbation size (which assumes 0-255 RGB image encoded with 8-bit) can be preserved.\n\u2022 MNIST-A, \u03f5 = 0.4 : this model uses \u2018small\u2019 model of Wong & Kolter (LPd-CNNA of Tjeng et al.), and is trained with the code publicly available made by Wong & Kolter (https://github.com/locuslab/convex adversarial). Training hyperparameters are: cascade=1, epochs=200, schedule length=20, norm type=l1 median, norm eval=l1, starting epsilon=0.01, verbose=200, batch size=20, test batch size=10, eps=0.4. To obtain the lower bound of this model,",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Adversarial robustness has gained significant attention as deep neural networks are increasingly deployed in safety-critical applications. However, the empirical evaluation of adversarial robustness using first-order attack methods often leads to overestimation of the robustness of models, which can mislead researchers and practitioners.",
            "purpose of benchmark": "The benchmark aims to provide a more accurate empirical evaluation of adversarial robustness by addressing the common pitfalls encountered with existing first-order attack methods."
        },
        "problem": {
            "definition": "The benchmark is designed to address the problem of overestimated adversarial accuracy when using bounded first-order attack methods to evaluate the robustness of deep neural networks.",
            "key obstacle": "Existing benchmarks often fail to account for issues such as gradient masking and numerical instability, leading to misleading conclusions about the robustness of models."
        },
        "idea": {
            "intuition": "The benchmark was inspired by observed discrepancies between empirical adversarial accuracy and verified robustness, particularly in cases where models were not truly robust despite high reported accuracy.",
            "opinion": "The authors emphasize that accurate evaluation metrics are crucial for understanding adversarial robustness, as overestimation can lead to the deployment of vulnerable models.",
            "innovation": "The benchmark introduces compensation methods that improve the accuracy of empirical evaluations by addressing the identified failure modes of existing first-order attack methods.",
            "benchmark abbreviation": "AR-Benchmark"
        },
        "dataset": {
            "source": "The benchmark utilizes datasets such as CIFAR-10, SVHN, and TinyImageNet, which are commonly used in adversarial robustness research.",
            "desc": "The datasets are pre-processed and normalized to ensure consistency in evaluation across different model architectures.",
            "content": "The datasets consist of images that are used to test the adversarial robustness of neural networks under various attack conditions.",
            "size": "60,000",
            "domain": "Computer Vision",
            "task format": "Image Classification"
        },
        "metrics": {
            "metric name": "Adversarial Accuracy, Robustness Gap",
            "aspect": "The metrics measure the effectiveness of models against adversarial attacks and the difference between empirical accuracy and verified robustness.",
            "principle": "The metrics are chosen to reflect both the performance of models under adversarial conditions and the reliability of the evaluation process.",
            "procedure": "Models are evaluated using various first-order attack methods, both with and without compensation techniques, to assess their adversarial accuracy."
        },
        "experiments": {
            "model": "Various deep neural network architectures, including Simple, Simple-BN, and WideResNet, are tested in the benchmark.",
            "procedure": "Models are trained and evaluated using a combination of standard training techniques and the proposed compensation methods to assess their adversarial robustness.",
            "result": "The results indicate that applying compensation methods significantly reduces the overestimation of adversarial accuracy, revealing a clearer picture of model robustness.",
            "variability": "Variability in results is accounted for by conducting multiple trials and using different random initializations for attacks."
        },
        "conclusion": "The benchmark successfully identifies and mitigates common pitfalls in the empirical evaluation of adversarial robustness, providing a more reliable framework for assessing the true robustness of deep learning models.",
        "discussion": {
            "advantage": "The benchmark improves the understanding of adversarial robustness by providing clearer evaluation metrics and addressing overestimation issues.",
            "limitation": "Despite its advancements, the benchmark may not capture all factors influencing adversarial robustness, and further research is needed to explore additional phenomena.",
            "future work": "Future research could focus on refining compensation methods and exploring other factors that contribute to the gap between empirical and verified adversarial robustness."
        },
        "other info": {
            "info1": "The benchmark highlights the importance of rigorous empirical evaluation in the field of adversarial machine learning.",
            "info2": {
                "info2.1": "The authors suggest that other evaluation frameworks could benefit from incorporating similar compensation techniques.",
                "info2.2": "The findings may have implications for the design of more robust neural network architectures."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "3.2",
            "key information": "The benchmark aims to provide a more accurate empirical evaluation of adversarial robustness by addressing the common pitfalls encountered with existing first-order attack methods."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark is designed to address the problem of overestimated adversarial accuracy when using bounded first-order attack methods to evaluate the robustness of deep neural networks."
        },
        {
            "section number": "3.3",
            "key information": "The results indicate that applying compensation methods significantly reduces the overestimation of adversarial accuracy, revealing a clearer picture of model robustness."
        },
        {
            "section number": "4.2",
            "key information": "The authors emphasize that accurate evaluation metrics are crucial for understanding adversarial robustness, as overestimation can lead to the deployment of vulnerable models."
        },
        {
            "section number": "6.2",
            "key information": "Despite its advancements, the benchmark may not capture all factors influencing adversarial robustness, and further research is needed to explore additional phenomena."
        }
    ],
    "similarity_score": 0.5305310894186711,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Rethinking Empirical Evaluation of Adversarial Robustness Using First-Order Attack Methods.json"
}