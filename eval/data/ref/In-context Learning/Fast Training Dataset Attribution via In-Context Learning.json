{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.11852",
    "title": "Fast Training Dataset Attribution via In-Context Learning",
    "abstract": "We investigate the use of in-context learning and prompt engineering to estimate the contributions of training data in the outputs of instruction-tuned large language models (LLMs). We propose two novel approaches: (1) a similarity-based approach that measures the difference between LLM outputs with and without provided context, and (2) a mixture distribution model approach that frames the problem of identifying contribution scores as a matrix factorization task. Our empirical comparison demonstrates that the mixture model approach is more robust to retrieval noise in in-context learning, providing a more reliable estimation of data contributions.",
    "bib_name": "fotouhi2024fasttrainingdatasetattribution",
    "md_text": "# Fast Training Dataset Attribution via In-Context Learning\nMilad Fotouhi, Mohammad Taha Bahadori, Oluwaseyi Feyisetan, Payman Arabshahi, David Heckerman\nAmazon and University of Washington {mfotouhi, bahadorm}@amazon.com August 23, 2024\nAbstract\nWe investigate the use of in-context learning and prompt engineering to estimate the contributions of training data in the outputs of instruction-tuned large language models (LLMs). We propose two novel approaches: (1) a similarity-based approach that measures the difference between LLM outputs with and without provided context, and (2) a mixture distribution model approach that frames the problem of identifying contribution scores as a matrix factorization task. Our empirical comparison demonstrates that the mixture model approach is more robust to retrieval noise in in-context learning, providing a more reliable estimation of data contributions.\n# 1 Introduction\nTraining Data Attribution (TDA) refers to the task of quantifying contributions of different data sources on the outputs of a model (Park et al., 2023; Nguyen et al., 2023). This task is essential for debugging the processes of curating corpora for training and for improving the training of neural networks. Understanding the contribution of data sources allows us to assess the monetary value of proprietary training data, which is crucial for fair compensation and data management (Ghorbani & Zou, 2019; Nohyun et al., 2022). Existing methods for TDA, primarily fall into two categories: retraining-based methods and influence function-based methods, as detailed in recent surveys (Hammoudeh & Lowd, 2024; Worledge et al., 2024). Retraining approaches such as those by (Feldman & Zhang, 2020; Ghorbani & Zou, 2019) involve retraining the model without the target data source. However, this method is computationally expensive. Influence function approaches (Koh & Liang, 2017; Pruthi et al., 2020; Chen et al., 2021; Park et al., 2023), relax the need for full retraining by requiring only a few gradient calculations with respect to the data. Despite their efficiency, these methods rely on a linear approximation of the neural network around the target data point, which can be inaccurate. Critically, the influence function approaches compute the attribution score for a dataset as a linear function (usually an average or sum) of the attribution scores for each data point in the dataset (Hammoudeh & Lowd, 2024; Park et al., 2023). This approach fails to provide a holistic view of the contributions of an entire dataset to the model\u2019s output. Additionally, both methods require access to the internals of LLMs, which is not feasible for some popular models. A related technique, Machine Unlearning (Ginart et al., 2019; Sekhari et al., 2021) is still expensive for obtaining the contribution scores. We explore the use of in-context learning and prompt engineering to estimate the contributions of each dataset as a whole in the outputs of instruction-tuned LLMs. We propose two approaches: (1) A similaritybased approach, which posits that providing a dataset as context to an LLM trained on that dataset changes its output less compared to when the LLM was not trained on the dataset. (2) A mixture distribution model approach, where we model the behavior of LLMs using a new mixture distribution. This approach transforms the problem of identifying contribution scores into a matrix factorization problem, which we solve using the\nalternating projected least squares method. Both approaches utilize Retrieval Augmented Generation (RAG) (Lewis et al., 2020) to accommodate large data sources. In the experiments, we evaluate four instruction-tuned LLMs: Mistral 7B (Jiang et al., 2023), Bloomz (Le Scao et al., 2023), Microsoft/Phi-3-mini (Abdin et al., 2024) and GPT 4.0 (Achiam et al., 2023) on a set of binary Q&A datasets, boolq (Clark et al., 2019). We demonstrate that the mixture model approach is more robust to the retrieval noise inherent in RAG systems, providing more accurate estimations of data contributions. Additionally, we conduct an evaluation using Trak (Park et al., 2023) to compare our results with those obtained by the Trak method, validating the effectiveness and efficiency of our proposed approaches.\n# 2 Methodology\nAn LLM processes knowledge from different sources. Our goal is to examine different prompts and see if we can uncover the sources of this knowledge. In the binary outcome setting, we have tuples in the format of question, context, and outcome: (q, c, y). When we don\u2019t use any context, we denote c = \u2205. Our model also outputs p(y|q, c). Our goal is to quantify the contributions of the training datasets D1, . . . , Dn in p(y|q, c). We assume that we have a query set Q = {q1, . . . , qm}. We assume that we have k, k = 1, . . . , K relevant datasets about a topic and we want to quantify their contributions in the generation of the output by our LLM.\n# 2.1 The Non-parametric Approach: The Shapley Context Method (SCM)\nThe key idea of this approach is that if an LLM uses the information from the kth dataset, providing the k dataset as context will not change the output much. Conversely, if adding a dataset as context changes t output significantly, it was likely not used for generation of the output.\nwhere ck is the context provided from the kth dataset. Usually, desired information can be found in multiple datasets (Ghorbani & Zou, 2019). To consider the impact of datasets in presence of other datasets, we define the following residual scores to be used in the Shapley formula (Shapley, 1953):\nk  Usually, desired information can be found in multiple datasets (Ghorbani & Zou, 2019). To consider the impact of datasets in presence of other datasets, we define the following residual scores to be used in the Shapley formula (Shapley, 1953):\nThe Shapley values are computed as follows:\nThis formula finds the residual increase in the similarity by including Dk, when we already have included another set S \u2286{D1, . . . , DK} \\ {Dk}. Algorithm 1 in Appendix B describes the details of our Shapley Context Method (SCM).\nThis formula finds the residual increase in the similarity by including Dk, when we already have included another set S \u2286{D1, . . . , DK} \\ {Dk}. Algorithm 1 in Appendix B describes the details of our Shapley\nWe propose a model for summarizing the behavior of LLMs. Our model explicitly contains attribution scores and captures the entirety of the datasets used for its training. We use a mixture distribution approach, which\nWe propose a model for summarizing the behavior of LLMs. Our model explicitly contains attribution scor and captures the entirety of the datasets used for its training. We use a mixture distribution approach, whi defines: p(y|q) = \u03c00\ufffdp0(y|q) + K \ufffd k=1 \u03c0k\ufffdpk(y|q), (\n(1)\n(2)\n(4)\nwhere \ufffdp0 denotes a general-purpose language model and \ufffdpk denote the language models specialized on each of the relevant datasets k = 1, . . . , K. The distributions \ufffdpk, k = 0, . . . , K are imaginary and we do not intend to explicitly estimate them. Remark 1: Given the modularity of LLM structures, this assumption is not fully realistic. However, this assumption provides a holistic view of the contributions of each dataset, captured by distributions \ufffdpk, k = 1, . . . , K. Thus, Model (4) serves as a useful tool to statistically summarize the behavior of the LLM. Remark 2: Model (4) can capture the scenarios where an LLM uses data from multiple sources, but does not model the scenarios where the LLM uses the interaction of data from multiple sources. We model the impact of providing context from a dataset k \u2208{1, . . . , K} as an intervention in the probability distribution:\np(y|q, ck) = \u03c00\ufffdp0(y|q) + (1 \u2212\u03c00)\ufffdpk(y|q).\n\ufffd\ufffd The key assumption is that both Eq. equation 4 and equation 5 do not have context terms in the right-hand side quantities. See a weaker version of Assumption equation 5 in the appendix. Goal: Our goal is identifying \u03c0k, k = 1, . . . , K. Which probability distance metrics help identify these contributions? We want to do this without explicitly estimating \ufffdpk, k = 1, . . . , K.\n#  \ufffd Formulating as a Matrix Factorization Problem. For each of the m queries, we perform K +1 prompts (or 2K prompts) and write the results in a linear equation as follow:\n \ufffd mulating as a Matrix Factorization Problem. For each of the m queries, we perform K +1 prompts 2K prompts) and write the results in a linear equation as follow:\np(j) = \u03a0\ufffdp(j) k , for j = 1, . . . , m.\n\ufffd We observe the quantity on the left-hand side, but none of the quantities in the right-hand side. The matrix \u03a0 has a specific structure. By defining the matrix P = [p(1), . . . , p(m)], we can write our problem in the\n\ufffd We observe the quantity on the left-hand side, but none of the quantities in  \u03a0 has a specific structure. By defining the matrix P = [p(1), . . . , p(m)], w following matrix form:\n \ufffd where P \u2208[0, 1](K+1)\u00d7m, \u03a0 \u2208[0, 1](K+1)\u00d7(K+1), and \ufffdP \u2208[0, 1](K+1)\u00d7m. This is a matrix factorization problem with a special structure. We assume that \ufffdpk(y|q) can be obtained via some clever prompts. We can make assumptions about \ufffdpk(y|q) that allow recovery of \u03c0 mixture parameters. Remark 3: Instead of K + 1 prompts, we can have up to 2K prompts. However, for the prompts that use multiple datasets, we need to assume the form of the resulting distribution, similar to Eq. (5). An alternative is to impose priors on \u03c0 and \ufffdP to improve identifiability. We pursue the second approach in the next section. Alternating Projected Least Squares. We can have multiple estimates for \u03c0 from Eq. (6). We can resolve this issue be encouraging solutions that have lower variance. We achieve this by using two regularizers: an entropy regularizer for \u03c0 to assume that the sources contribute equally and a regularizer that encourages \ufffdP to be less informative.\n \ufffd Alternating Projected Least Squares. We can have multiple estimates for \u03c0 from Eq. (6). We can resolve this issue be encouraging solutions that have lower variance. We achieve this by using two regularizers: an entropy regularizer for \u03c0 to assume that the sources contribute equally and a regularizer that encourages \ufffdP to be less informative.\n\ufffd where \u2225\u00b7 \u2225F and H(\u00b7) denote the Frobenius norm and Shannon\u2019s entropy. We use entropy regularization on \u03c0 encouraging the null hypothesis of \u201cequal contributions of all sources\u201d. The Frobenius norm regularization implies that unless there is strong evidence, the outputs of the latent probabilities \ufffdP should be 1/2. Note the regularizers are vital for obtaining a non-trivial solution, and in absence of the them, there are many solutions for the problem. The problem in Eq. equation 7 is biconvex; i.e., fixing either of \u03c0 or \ufffdP, the problem is convex (Gorski et al., 2007). Thus, we solve it by the alternating least squares method. We describe the procedure in\n(5)\n(6)\n(7)\nAlgorithm 2 in Appendix B. We further assist the regularization terms by randomly initializing \ufffdP to be around 1/2 and \u03c0 to be around 1/(K + 1). We obtain the confidence intervals for both SCM and CMF by bootstrapping. In Appendix A we study the impact of regularization coefficients on the uncertainty of the estimation using synthetic datasets.\n# 3 Implementation\n# 3.1 Prompt Engineering\nFor simplicity of evaluation and without loss of generality, we used BoolQ (Clark et al., 2019) Q&A dataset, where the answers are binary Yes/No. To instruct the LLMs to provide direct boolean responses, we used prompt engineering. Initially, we tested various prompts without explicitly instructing the model to answer with \u201dYes\u201d or \u201dNo.\u201d Diverse examples used in this process are provided in Appendix C.3. Through iterative testing, we found that responses improved when the model was explicitly instructed to provide a boolean answer. This led to our final prompt: Prompt: \u201dGiven the context below, answer the question that follows with only \u2019Yes\u2019, \u2019No\u2019, or \u2019I don\u2019t know\u2019 if the context is insufficient. {question}? The answer to this question is\u201d While this final prompt worked well for GPT-4, Bloomz, and Mistral 7B, generating straightforward \u201dYes,\u201d \u201dNo,\u201d or \u201dI don\u2019t know\u201d responses, it was harder to instruct Phi-3-mini. Even with the final prompt, Phi-3-mini often generated more text than just a simple boolean response. Therefore, calculating similarities was straightforward for GPT-4, Bloomz, and Mistral 7B, but we had to devise another solution for Phi-3-mini. The embedding similarity API on GPT-4 was not precise enough as it did not focus primarily on the context of the generated response. To calculate the similarity for Phi-3-mini, we created a Zero-shot classification layer (which takes 1000 characters) between the prediction and the result to measure similarity more accurately.\n# 3.2 Using RAG\nGiven the limitations of LLM context windows, fitting entire datasets directly into the context is impractical. To address this, we utilized Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) to enhance the context by retrieving relevant documents from databases before generating responses. The process involves splitting the documents into semantically relevant chunks using the RecursiveCharacterTextSplitter from the Hugging Face Transformers library, computing embeddings for all chunks with a model like thenlper/gte-small, and storing these embeddings in a vector database using FAISS (Facebook AI Similarity Search). When a question is posed, it is embedded and a similarity search is performed against the vector database to find the closest matching documents. These retrieved documents are then provided as context to the LLMs along with the original question, allowing the LLMs to generate responses augmented with additional context. We used a chunk size of 512 and a top-k value of 3, ensuring the context was trimmed to 2000 characters for conciseness.\n# 4 Preliminary Experiments\nSimplified setup to demonstrate our methodology: Step 1: Task Selection We use the BoolQ Q&A dataset, which consists of tuples in the form (question, relevant context, binary answer) for each question. Step 2: LLM Selection We examined four instruction-tuned LLMs: GPT-4, Bloomz, Mistral 7B, and Phi-3-mini. We report the accuracy of these LLMs on the Q&A task in Table 5. Given that the dataset is binary, we prompted the LLMs to answer \u201dYes\u201d or \u201dNo\u201d to each question, or to say \u201dI don\u2019t know\u201d if they could not provide a definite response (see Section C.1).\n<div style=\"text-align: center;\">Bloomz GPT-4 Mistral 7B Phi-3</div>\n\u03d5BoolQ\n0.48\n0.59\n0.57\n0.50\n\u03d5Chemistry\n0.10\n0.08\n0.09\n0.10\n\u03d5Natural Science\n0.12\n0.09\n0.10\n0.11\n\u03d5History\n0.11\n0.10\n0.10\n0.11\n\u03d5Biology\n0.10\n0.07\n0.08\n0.10\n\u03d5Law\n0.09\n0.07\n0.08\n0.08\nTable 1: Shapley Values (\u03d5k) using SCM Algorithm\nStep 3: Dataset Collection We collected five datasets on different topics. The corpora were sourced from a subset of the Wikipedia Field of Science dataset available on Hugging Face, specifically the fields of Chemistry, Natural Science, History and Archaeology, Biology, and Law. Each of these datasets contains more than a million samples across five categories. Step 4: Evaluation Clearly, the context from BoolQ is more related to the questions. Thus, the successful methods should estimate higher weight for BoolQ, as the proxy for the relevant data used during training. For SCM, we report \u03d5k for 6 sources. Similarly, we report 7-dimensional \u03c0 vector for CMF. The first dimension \u03c00 represents the contribution of all other data sources.\nResults and Analysis Tables 1 and 3 show the attribution results obtained by SCM and CMF algorithms, respectively. Both algorithms successfully identify the BoolQ dataset as the most influential dataset. This is because BoolQ context is more directly related to the questions. Chemistry, Natural Science, History and Archaeology, Biology, and Law have lower \u03d5k values, showing that while they contribute to the context, their impact is less significant compared to BoolQ. Note that in CMF, we need to calculate \u03c0BoolQ 1\u2212\u03c0Base to directly compare it with \u03d5BoolQ estimated by SCM. Comparing the results of SCM and CMF, we see that CMF places higher weight on BoolQ. With all LLMs, the quantity \u03c0BoolQ 1\u2212\u03c0Base is larger than \u03d5BoolQ obtained by SCM. We attribute this to the noise in RAG when we pull context from multiple sources. Moreover, CMF provides the contributions of the base LLM \u03c0Base. We can see that GPT-4 has the highest contribution from the base language model. This observation is inline with the observation that GPT-4 has the highest accuracy without any context, as reported in Table 5 in the appendix. We also conducted an evaluation using Trak (Park et al., 2023) as a popular baseline. Trak provides a different perspective on dataset attribution by scoring the impact of training data on model predictions. The Trak scores for Bloomz and Phi-3-mini across the same datasets are shown in Table 2 . Our results align well with Trak, and improve upon it, reinforcing the robustness of our methods.\n<div style=\"text-align: center;\">Model BoolQ Chemist Natural Sci History Biology Law</div>\nPhi-3\n0.58\n-0.08\n0.20\n0.18\n-0.05\n0.07\nBloomz\n0.61\n-0.10\n0.22\n0.19\n-0.06\n0.09\nTable 2: TRAK Scores for Different Sources using Phi-3 and Bloomz models. Positive scores indicate datasets that contribute positively to the model\u2019s output, while negative scores indicate a lesser or inverse influence. By comparing our results with Trak, we observe that both methods identify BoolQ as the most significant contributor. However, our CMF approach provides a more detailed and accurate attribution of dataset contributions, particularly in quantifying the base model\u2019s influence and managing the noise inherent in RAG systems.\nRuntime Comparison The CMF algorithm is faster than the SCM algorithm as it requires fewer queries with shorter context sizes. Utilizing an AWS EC2 G6 instance (g6.16xlarge), the total runtime for CMF\n<div style=\"text-align: center;\">Bloomz GPT-4 Mistral 7B Phi-3</div>\n\u03c0Base\n0.05\n0.08\n0.06\n0.05\n\u03c0BoolQ\n0.60\n0.62\n0.61\n0.59\n\u03c0Chemistry\n0.09\n0.07\n0.08\n0.09\n\u03c0Natural Science\n0.07\n0.08\n0.07\n0.06\n\u03c0History\n0.08\n0.10\n0.09\n0.07\n\u03c0Biology\n0.06\n0.06\n0.05\n0.05\n\u03c0Law\n0.05\n0.10\n0.08\n0.06\n\u03c0BoolQ\n1\u2212\u03c0Base\n0.63\n0.67\n0.65\n0.62\nTable 3: \u03c0 and \u03c0BoolQ 1\u2212\u03c0Base values using CMF algorithm\n# Table 3: \u03c0 and \u03c0BoolQ 1\u2212\u03c0Base values using CMF algorithm\ninvolving 7 runs, ranges from 77 to 94 minutes for all LLMs. In contrast, the SCM method, which requires 25 runs, results in a total runtime of 352 to 384 minutes. Both algorithms\u2019 runtimes are dominated by the RAG search time. This substantial reduction in runtime demonstrates the efficiency of the CMF method, making it more suitable for scenarios demanding both accuracy and computational efficiency. For TRAK, the runtime is significantly higher due to its high memory requirements. TRAK requires about 20 GB of GPU memory for a model with 1 million parameters. Scaling this to larger models, TRAK\u2019s memory requirements become impractical for large LLMs with modest computing resources. Running TRAK on our LLMs would necessitate approximately 600 GB of GPU memory and significantly more computational time, making CMF and SCM more feasible for our use case. Deep Dive into RAG Noise Effect We compute the mean similarities and residuals for each model as shown in Table 4. The high residual for Bloomz (-0.32) indicates that BoolQ contributes significantly to the output. When the BoolQ context is added, the model\u2019s performance changes markedly, demonstrating the influence of this dataset. The low residual for GPT-4 (-0.03) suggests that GPT-4 has been pre-exposed to similar data. The minimal change in performance with the addition of BoolQ context indicates that GPT-4 already possesses substantial knowledge from similar datasets. The positive residual for Mistral 7B (0.06) implies that Mistral 7B relies on the added context. This reliance on context suggests that the model benefits greatly from the additional information provided by BoolQ, indicating that it has not been exposed to similar data during training. However, this also introduces RAG noise, as the model\u2019s output can be significantly influenced by the context. The small residual for Phi-3 (0.03) shows partial exposure to similar data during training. The model benefits from the added BoolQ context but to a lesser extent, suggesting that while it improves with additional context, it already has some degree of relevant knowledge.\nDeep Dive into RAG Noise Effect We compute the mean similarities and residuals for each model as shown in Table 4. The high residual for Bloomz (-0.32) indicates that BoolQ contributes significantly to the output. When the BoolQ context is added, the model\u2019s performance changes markedly, demonstrating the influence of this dataset. The low residual for GPT-4 (-0.03) suggests that GPT-4 has been pre-exposed to similar data. The minimal change in performance with the addition of BoolQ context indicates that GPT-4 already possesses substantial knowledge from similar datasets. The positive residual for Mistral 7B (0.06) implies that Mistral 7B relies on the added context. This reliance on context suggests that the model benefits greatly from the additional information provided by BoolQ, indicating that it has not been exposed to similar data during training. However, this also introduces RAG noise, as the model\u2019s output can be significantly influenced by the context. The small residual for Phi-3 (0.03) shows partial exposure to similar data during training. The model benefits from the added BoolQ context but to a lesser extent, suggesting that while it improves with additional context, it already has some degree of relevant knowledge.\nMetric\nBloomz\nGPT-4\nMistral 7B\nPhi-3\nsS\n0.86 (0.03)\n0.76 (0.02)\n0.73 (0.03)\n0.60 (0.03)\nsS\u222a{Dk}\n0.54 (0.02)\n0.72 (0.02)\n0.69 (0.03)\n0.63 (0.02)\nrk,S\n-0.32 (0.02) -0.03 (0.02)\n0.06 (0.02)\n0.03 (0.02)\nTable 4: Mean similarities sS and residuals rk,S for different LLMs (with standard deviations) To evaluate the effectiveness of context provision using RAG, we designed an experiment to measure the accuracy of various LLMs when answering questions from the BoolQ dataset. The experiment compared the models\u2019 performance across different scenarios: without any context, with only the BoolQ context, with contexts from five other datasets, and with all datasets combined. The results are summarized in Table 5. The baseline setting (No Context) reveals inherent differences in the LLMs\u2019 capabilities. GPT-4 has the highest baseline accuracy at 0.73, followed by Phi-3 at 0.70 and Mistral 7B at 0.68, indicating robust\n<div style=\"text-align: center;\">Bloomz GPT-4 Mistral 7B Phi-3</div>\n<div style=\"text-align: center;\">Context</div>\nContext\nBloomz\nGPT-4\nMistral 7B\nPhi-3\nNo Context\n0.43\n0.73\n0.68\n0.70\nBoolQ as RAG\n0.74\n0.87\n0.85\n0.82\nFive Datasets Only\n0.35\n0.64\n0.60\n0.45\nAll Datasets + BoolQ\n0.73\n0.84\n0.82\n0.83\nTable 5: Accuracy of LLMs with Different Contexts\npre-training for these models. Bloomz shows lower accuracy at 0.43, highlighting its dependency on contextual data. When the BoolQ context is provided using RAG, all models show significant accuracy improvements, with GPT-4 reaching 0.87, Mistral 7B at 0.85, and Phi-3 at 0.82. Bloomz also improves to 0.74, though it remains lower than the others. Providing context from five datasets (excluding BoolQ) leads to accuracy drops for all models, with Bloomz at 0.35, GPT-4 at 0.64, Mistral 7B at 0.60, and Phi-3 at 0.45. This indicates that less relevant data is less effective for understanding BoolQ queries. Combining all datasets with BoolQ context results in slight decreases in accuracy for GPT-4 (0.84) and Mistral 7B (0.82), suggesting that additional datasets introduce noise. Bloomz and Phi-3 show minimal changes, indicating that extra data does not significantly impact their performance once BoolQ context is included. These results emphasize the importance of relevant contextual information in enhancing LLM performance, with GPT-4 consistently outperforming other models due to its extensive training.\n# 5 Conclusion and Discussion\nOur results show that both of our proposed algorithms successfully attribute the output of LLMs to the BoolQ dataset (as the proxy for related knowledge). Comparing LLMs GPT-4 showed minimal change in the similarities when BoolQ context was added, suggesting prior exposure to similar data, while Bloomz exhibited a high residual, indicating substantial influence from BoolQ. The CMF algorithm provides further insights by quantifying the contributions of the base LLM. Comparing our two methods, we conclude that CMF is computationally less expensive and more robust to the RAG noise. In this paper, we considered several datasets as proxies for the datasets used for training of LLMs. For future work, we will try to avoid this approximate method and train our LLMs on specific datasets and test our algorithms with ground truth contributing datasets obtained by retraining.\n# Acknowledgment\nWe would like to thank Layne Price and Bilal Fadlallah for their helpful comments.\n# References\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv:2303.08774, 2023. Yuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, and Chunyan Miao. Hydra: Hypergradient data relevance analysis for interpreting deep neural networks. In AAAI, volume 35, pp. 7081\u20137089, 2021.\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report arXiv:2303.08774, 2023. Yuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, and Chunyan Miao. Hydra: Hypergradient data relevance analysis for interpreting deep neural networks. In AAAI, volume 35, pp. 7081\u20137089, 2021.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL-HLT, 2019. Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. NeurIPS, 33:2881\u20132891, 2020. Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In International conference on machine learning, pp. 2242\u20132251. PMLR, 2019. Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data deletion in machine learning. NeurIPS, 32, 2019. Jochen Gorski, Frank Pfeuffer, and Kathrin Klamroth. Biconvex sets and optimization with biconvex functions: a survey and extensions. Mathematical methods of operations research, 66(3):373\u2013407, 2007. Zayd Hammoudeh and Daniel Lowd. Training data influence analysis and estimation: A survey. Machine Learning, pp. 1\u201353, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pp. 1885\u20131894. PMLR, 2017. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00b4e, Alexandra Sasha Luccioni, Fran\u00b8cois Yvon, Matthias Gall\u00b4e, et al. Bloom: A 176b-parameter open-access multilingual language model. 2023. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. NeurIPS, 2020. Elisa Nguyen, Minjoon Seo, and Seong Joon Oh. A bayesian perspective on training data attribution. arXiv preprint arXiv:2305.19765, 2023. Ki Nohyun, Hoyong Choi, and Hye Won Chung. Data valuation without training of a model. In The Eleventh International Conference on Learning Representations, 2022. Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: attributing model behavior at scale. In ICML, 2023. Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. NeurIPS, 33:19920\u201319930, 2020. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. NeurIPS, 34:18075\u201318086, 2021. L S Shapley. A value for n-person games. Contributions to the Theory of Games, pp. 307\u2013317, 1953. Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, and Carlos Guestrin. Unifying corroborative and contributive attributions in large language models. In 2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pp. 665\u2013683. IEEE, 2024.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7fa1/7fa1da47-f792-42c2-a8f0-4e1d9638f30e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The Logarithm of the variations in the solutions for \u03c0. The variation is measured as the sum of th eigenvalues of the covariance matrix of the solutions (i.e., nuclear norm). The shaded area shows the [5, 95 percentile.</div>\nAlgorithm 1 Shapley Context Method (SCM)\nRequire: An instruction-tuned LLM M that generates outputs y for each query q and context c.\nRequire: A set of queries Q = {q1, . . . , qm}.\nRequire: A set of datasets that we need to compute their contributions D1, . . . , DK.\n1: for q \u2208Q do\n2:\nCompute output without context: y = M(q).\n3:\nfor S \u2282{D1, . . . , DK} \\ {Dk} do\n4:\nUse RAG to create contexts cS and cS\u222a{Dk} from the datasets in S and Dk.\n5:\nCompute the output with the context y|cS = M(q|cS).\n6:\nCompute the output with the context including Dk: y|cS\u222a{Dk} = M(q|cS\u222a{Dk}).\n7:\nCompute the similarities sS and sS\u222a{k} using Eq. (2).\n8:\nCompute the residual gain rk,S = sS\u222a{k} \u2212sS.\n9:\nend for\n10:\nUse Eq. (3) and rk,S to compute \u03d5k(q).\n11: end for\n12: Return average \u03d5k over m queries.\n# A Appendix\nA Synthetic Experiments with Matrix Factorization\n# A Synthetic Experiments with Matrix Factorization\nTo understand the impact of regularization terms in Eq. (7), we perform a simple synthetic experiment. We create a matrix of random p(y|q) (uniformly between 0 and 1) for m = 5 queries and decompose it to identify the impact of 3 sources. We vary the regularization terms \u03bb\u03c0 = \u03bbPK \u2208[0, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 1]. We repeat the experiments with randomly initialized bm\u03c0 and PK for 100 times and report the variations in the solutions for \u03c0. The variation is measured as the sum of the eigenvalues of the covariance matrix of the solutions (i.e., nuclear norm). Figure 1 recommends choosing the regularization coefficients larger than 0.1 to have stable solutions.\nTo understand the impact of regularization terms in Eq. (7), we perform a simple synthetic experiment. W create a matrix of random p(y|q) (uniformly between 0 and 1) for m = 5 queries and decompose it to identif\n# B Algorithms\nAlgorithm 2 Context Mixture Factorization (CMF)\nRequire: An instruction-tuned LLM M that generates outputs y for each query q and context c.\nRequire: A set of queries Q = {q1, . . . , qm}.\nRequire: A set of datasets that we need to compute their contributions D1, . . . , DK.\n1: for q \u2208Q do\n2:\nCompute output without context: p[y|q, c0] = M(q).\n3:\nfor k = 1, . . . , K do\n4:\nUse RAG to create context ck from the dataset Dk.\n5:\nCompute the output with the context p[y|q, ck] = M(q|ck).\n6:\nend for\n7: end for\n8: Build matrix P, where Pk,j = p[y|qj, ck].\n9: Solve Eq. (7) via alternating least squares and to compute \ufffd\u03c0.\n10: Return the contribution vector \ufffd\u03c0.\n#  \ufffd C Implementation Details\n# C.1 Prompt Engineering\nFor simplicity of evaluation and without loss of generality, we used BoolQ (Clark et al., 2019) Q&A dataset, where the answers are binary Yes/No. To instruct the LLMs to provide direct boolean responses, we used prompt engineering. Initially, we tested various prompts without explicitly instructing the model to answer with \u201dYes\u201d or \u201dNo.\u201d Diverse examples used in this process are provided in Appendix C.3. Through iterative testing, we found that responses improved when the model was explicitly instructed to provide a boolean answer. This led to our final prompt: Prompt: \u201dGiven the context below, answer the question that follows with only \u2019Yes\u2019, \u2019No\u2019, or \u2019I don\u2019t know\u2019 if the context is insufficient. {question}? The answer to this question is\u201d While this final prompt worked well for GPT-4, Bloomz, and Mistral 7B, generating straightforward \u201dYes,\u201d \u201dNo,\u201d or \u201dI don\u2019t know\u201d responses, it was harder to instruct Phi-3-mini. Even with the final prompt, Phi-3-mini often generated more text than just a simple boolean response. Therefore, calculating similarities was straightforward for GPT-4, Bloomz, and Mistral 7B, but we had to devise another solution for Phi-3-mini. The embedding similarity API on GPT-4 was not precise enough as it did not focus primarily on the context of the generated response. To calculate the similarity for Phi-3-mini, we created a Zero-shot classification layer (which takes 1000 characters) between the prediction and the result to measure similarity more accurately.\n# C.2 Using RAG\nGiven the limitations of LLM context windows, fitting entire datasets directly into the context is impractical. To address this, we utilized Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) to enhance the context by retrieving relevant documents from databases before generating responses. The process involves splitting the documents into semantically relevant chunks using the RecursiveCharacterTextSplitter from the Hugging Face Transformers library, computing embeddings for all chunks with a model like thenlper/gte-small, and storing these embeddings in a vector database using FAISS (Facebook AI Similarity Search). When a question is posed, it is embedded and a similarity search is performed against the vector database to find the closest matching documents. These retrieved documents are then provided as context to the LLMs along with the original question, allowing the LLMs to generate responses augmented with additional context. We used a chunk size of 512 and a top-k value of 3, ensuring the context was trimmed to 2000 characters for conciseness.\n# C.3 Prompts\nGeneral Question Prompt: \u201cRead the context provided and answer the following question: {question}\u201d Contextual Understanding Prompt: \u201cBased on the information in the context, what can you conclude about the following question? {question}\u201d Summarization Prompt: \u201cAfter considering the context below, summarize your answer to this question: {question}\u201d Opinion-Based Prompt: \u201cGiven the details in the context, what is your opinion on the following question: {question}\u201d Detail Extraction Prompt: \u201cExtract relevant information from the context to answer this question: {question}\u201d Fact-Checking Prompt: \u201cUsing the context provided, verify the accuracy of the following statement: {question}\u201d Table 5 shows the average accuracy calculated by comparing the predictions with the ground truth from BoolQ.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of Training Data Attribution (TDA), which quantifies the contributions of different data sources on the outputs of instruction-tuned large language models (LLMs). Existing methods for TDA, such as retraining-based and influence function-based methods, are computationally expensive or rely on inaccurate linear approximations, necessitating new approaches to improve efficiency and accuracy.",
        "problem": {
            "definition": "The problem is to accurately estimate the contributions of various training datasets to the outputs generated by LLMs, particularly in the context of instruction-tuned models.",
            "key obstacle": "The main challenge is that existing methods require either full retraining of models or rely on linear approximations that do not capture the holistic contributions of datasets."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that providing context from a dataset to an LLM trained on that dataset should minimally affect the output, thereby indicating the dataset's contribution.",
            "opinion": "The proposed idea involves two novel approaches: a similarity-based approach and a mixture distribution model approach, which transform the problem of identifying contribution scores into a more manageable form.",
            "innovation": "The key innovation lies in framing the contribution estimation as a matrix factorization problem, allowing for more robust handling of retrieval noise and better accuracy in estimating data contributions."
        },
        "method": {
            "method name": "Context Mixture Factorization (CMF)",
            "method abbreviation": "CMF",
            "method definition": "CMF is defined as a method that utilizes matrix factorization to estimate the contributions of training datasets to the outputs of LLMs by analyzing the effects of providing different contexts.",
            "method description": "CMF estimates the contribution of datasets to LLM outputs by performing matrix factorization on the model's outputs given various contexts.",
            "method steps": [
                "Compute output without context.",
                "For each relevant dataset, create context and compute outputs with that context.",
                "Build a matrix from the outputs and solve the matrix factorization problem.",
                "Return the contribution estimates."
            ],
            "principle": "The method is effective because it leverages the modularity of LLMs and the statistical properties of the outputs to derive meaningful contribution scores without requiring full model retraining."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized four instruction-tuned LLMs (Mistral 7B, Bloomz, Microsoft/Phi-3-mini, and GPT-4) and a binary Q&A dataset (BoolQ) to evaluate the performance of the proposed methods.",
            "evaluation method": "The evaluation involved comparing the outputs of LLMs with and without specific contexts, measuring the contributions of datasets through similarity scores and residual gains."
        },
        "conclusion": "The results indicate that both proposed algorithms successfully attribute model outputs to the BoolQ dataset, with the CMF method being more robust and computationally efficient compared to the SCM method, particularly in managing RAG noise.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include its computational efficiency and robustness to noise in retrieval-augmented generation, allowing for more accurate attributions of data contributions.",
            "limitation": "One limitation of the method is its reliance on assumptions regarding the independence of dataset contributions, which may not hold in all scenarios.",
            "future work": "Future research will focus on training LLMs on specific datasets to obtain ground truth contributions and further refine the attribution methods."
        },
        "other info": {
            "acknowledgments": "The authors thank Layne Price and Bilal Fadlallah for their helpful comments.",
            "dataset source": "Datasets were sourced from a subset of the Wikipedia Field of Science dataset available on Hugging Face."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of Training Data Attribution (TDA), which quantifies the contributions of different data sources on the outputs of instruction-tuned large language models (LLMs)."
        },
        {
            "section number": "1.2",
            "key information": "The significance of accurately estimating the contributions of various training datasets to the outputs generated by LLMs is emphasized, particularly in the context of instruction-tuned models."
        },
        {
            "section number": "3.1",
            "key information": "The proposed Context Mixture Factorization (CMF) method utilizes matrix factorization to estimate the contributions of training datasets to the outputs of LLMs by analyzing the effects of providing different contexts."
        },
        {
            "section number": "3.3",
            "key information": "The CMF method estimates the contribution of datasets to LLM outputs by building a matrix from the outputs and solving the matrix factorization problem, transforming the contribution estimation into a more manageable form."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the method is its reliance on assumptions regarding the independence of dataset contributions, which may not hold in all scenarios."
        },
        {
            "section number": "6.2",
            "key information": "The key advantages of the proposed approach include its computational efficiency and robustness to noise in retrieval-augmented generation, allowing for more accurate attributions of data contributions."
        }
    ],
    "similarity_score": 0.7514904484534622,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Fast Training Dataset Attribution via In-Context Learning.json"
}