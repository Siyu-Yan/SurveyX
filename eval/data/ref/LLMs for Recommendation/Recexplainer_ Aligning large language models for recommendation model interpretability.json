{
    "from": "google",
    "scholar_id": "BR18PUJF5IEJ",
    "detail_id": null,
    "title": "Recexplainer: Aligning large language models for recommendation model interpretability",
    "abstract": " Abstract\n\nRecommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs\u2019 own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model\u2019s behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model\u2019s behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.\n\n# \u2022 Information systems \u2192 Recommender systems; \u2022  Computing methodologies \u2192 Natural language generation.\n\n\u2217 Corresponding authors.\n\nPermission to make digital or hard copies of all or part of this work for personal o",
    "bib_name": "lei2023recexplainer",
    "md_text": "# cExplainer: Aligning Large Language Models for Explaining Recommendation Models\n\nJianxun Lian \u2217\nMicrosoft Research Asia Beijing, China jianxun.lian@outlook.com\nYuxuan Lei University of Science and Technology of China Hefei, China leiyuxuan@mail.ustc.edu.cn\n\nDefu Lian \u2217\nXing Xie Microsoft Research Asia Beijing, China xing.xie@microsoft.com\nXu Huang University of Science and Technology of China Hefei, China xuhuangcs@mail.ustc.edu.cn\nUniversity of Science and Technology of China Hefei, China liandefu@ustc.edu.cn\n\nDefu Lian \u2217\nXu Huang University of Science and Technology of China Hefei, China xuhuangcs@mail.ustc.edu.cn\nUniversity of Science and Technology of China Hefei, China liandefu@ustc.edu.cn\n\n# Abstract\n\nRecommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs\u2019 own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model\u2019s behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model\u2019s behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.\n\n# \u2022 Information systems \u2192 Recommender systems; \u2022  Computing methodologies \u2192 Natural language generation.\n\n\u2217 Corresponding authors.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD \u201924, August 25\u201329, 2024, Barcelona, Spain \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671802\n\nCCS Concepts\n\u2022 Information systems \u2192 Recommender systems; \u2022  Computing methodologies \u2192 Natural language generation.\n\n# CCS Concepts\n\u2022 Information systems \u2192 Recommender systems; \u2022  Computing methodologies \u2192 Natural language generation.\n\nKeywords\nLarge Language Models, Recommender Systems, Model Explainability\n\nACM Reference Format: Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. 2024. RecExplainer: Aligning Large Language Models for Explaining Recommendation Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \u201924), August 25\u201329, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10. 1145/3637528.3671802\n\n# 1 Introduction\n\nRecommender systems provide the appropriate information to the right individual based on comprehending users\u2019 preferences and intentions [19, 20, 25]. These systems have become an essential component in various online services, including e-commerce, news, and television & movies. Embedding-based recommender models, such as collaborative filtering based on latent factors [20, 26] and sequential recommenders [16, 22], showcase their remarkable expressiveness in representing complex signals, and have thus been extensively utilized in recommender systems. However, embeddingbased models typically function in a black-box manner, resulting in a lack of explainability. Model explainability is a crucial aspect of building reliable and trustworthy recommender systems. It offers multiple advantages, including insights into the underlying logic of systems, identification of bugs, detection of biases, and providing clues for model improvement. One mainstream category of techniques for model explanation involves training a surrogate model to align with original black-box model [21, 36, 37, 54]. This surrogate model must be both human-interpretable and maintain (local) fidelity to the original model. Once trained, it serves as an effective explainer for the original model. However, existing surrogate models typically employed, such as sparse linear models and decision trees, are inherently explainable but usually compromise fidelity due to their simplicity. Furthermore, the explanations generated are often\n\nlimited to basic styles, such as additive weights or multiple decision rules, and lack semantic interpretation from human-readable perspective.\n\nlimited to basic styles, such as additive weights or multiple decision rules, and lack semantic interpretation from human-readable perspective. Recently, large language models (LLMs) have exhibited exceptional versatility and proficiency in handling complex tasks such as question answering, code comprehension, reasoning, and instruction following [2, 8, 17, 33, 34]. The remarkable capabilities of LLMs present new opportunities to revolutionize various research fields within machine learning, including explainable AI. With an extensive repository of world knowledge embedded in their memory and powerful multi-step reasoning skills, LLMs are renowned for generating high-quality, human-readable explanations. As a result, the traditional paradox that self-explainable surrogate models must be simple and low-complexity may no longer hold true. In this paper, we investigate the potential of utilizing an LLM as a surrogate model for explaining recommender systems. We begin with the traditional training approach, which primarily involves aligning an LLM with a target recommendation model. The recommendation model is pre-trained and remains unaltered during this process. The LLM is then trained to emulate the recommendation model\u2019s predictive patterns\u2014given a user\u2019s profile as input, the LLM is fine-tuned to predict the items that the recommendation model would suggest to the user. We refer to this approach as behavior alignment. However, similar to traditional surrogate model-based approach, behavior alignment merely mimics predictive observations from outside the model, attempting to deduce what is happening within the black-box. We argue that a more profound way to explain the execution logic of models involves enabling the LLM to directly comprehend the neural layers of the recommender model. Therefore, we propose an alternative approach called intention alignment, wherein the embeddings (i.e., activations of neural layers) of the recommender model are incorporated into the LLM\u2019s prompts to represent user and item information, and the LLM is fine-tuned to understand these embeddings. This approach can be considered as a multimodal model, with textual words and recommendation model embeddings representing two distinct data modalities. Take [15] from the series of vision-language multimodal models [23, 31, 43, 44] as an example. The image pixels are transformed into embeddings by a pre-trained vision model. The language model is then trained to comprehend the contents of the original image by incorporating these embeddings into the input context. Eventually, the language model aligns itself with the vision model\u2019s space, acquiring the ability to understand and respond to questions about the image. Merging the advantages of both approaches, we introduce a novel method called hybrid alignment. This strategy effectively counteracts the hallucination issues associated with the intention alignment approach by incorporating both explicit titles and implicit embeddings in the prompt during training and inference stages. Thus, hybrid alignment facilitates a more robust and comprehensive understanding of the recommender model, ultimately enhancing the LLM to generate highly credible explanations. To validate the effectiveness of our proposed approaches, we conduct extensive experiments on three public datasets, examining their alignment effect and explanation generation ability. Empirical evidence demonstrates that LLMs can be successfully aligned to\n\naccurately reflect and comprehend the behavior of recommender models, highlighting their potential as a new type of surrogate model for explaining complex systems. Our contributions can be summarized as follows:\n\n\u2022  We propose to align LLMs for explaining recommender models, presenting a significant potential to advance explainable AI research by overcoming the traditional dilemma of requiring surrogate models to be simple for self-explainability.\n\u2022 To enable efficient model alignment, we introduce two distinct approaches: behavior alignment and intention alignment, each providing unique benefits. Additionally, we present hybrid alignment, a method that combines the advantages of both approaches.\n\u2022 We rigorously evaluate these alignment approaches on three publicly available datasets, demonstrating their effectiveness in both comprehension and explanation, highlighting the potential of LLMs as a new type of surrogate model for explaining recommender models.\n\n# 2 Methodologies 2.1 Problem Formulation\n\nIn recommender systems, users are represented by their behavioral sequences: x \ud835\udc62 = \u27e8 \ud835\udc4e 1,\ud835\udc4e 2, ...,\ud835\udc4e | x \ud835\udc62 | \u27e9, where \ud835\udc4e \u2217 represents an item that user \ud835\udc62 has interacted with in the past, and items are listed in chronological order. A recommender model \ud835\udc53 () learns to assign a higher score to items that users favor over those they don\u2019t: \ud835\udc53 (x \ud835\udc62,\ud835\udc4e \ud835\udc56)> \ud835\udc53 (x \ud835\udc62,\ud835\udc4e \ud835\udc57), where \ud835\udc4e \ud835\udc56 denotes a positive item and \ud835\udc4e \ud835\udc57 denotes a negative item for the user. To scale large systems, the twotower model paradigm has been extensively employed in industrial recommender systems, particularly in initial steps such as item recall. In this paradigm, users and items are separately encoded into embeddings: e \ud835\udc62 = \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f (x \ud835\udc62), e \ud835\udc56 = \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a (a \ud835\udc56), and the preference score is determined by the similarity between e \ud835\udc62 and e \ud835\udc56. This paper focuses on the two-tower model paradigm and leaves other paradigms such as the single-tower paradigm for future work. Given a trained recommender model \ud835\udc53 (), our objective is to tune an LLM \ud835\udc54 () to explain the decision-making process within \ud835\udc53 (). In next sections, we detail our methodologies for tuning LLMs into recommendation model explainer (RecExplainer), covering three styles: behavior alignment, intention alignment, and hybrid alignment, which are denoted as RecExplainer-B, RecExplainer-I, and RecExplainer-H, respectively.\n\n# 2.2 Behavior Alignment\n\nIn this approach, we fine-tune an LLM \ud835\udc54 () such that its predictive behavior aligns with the recommender model \ud835\udc53 (). The hypothesis is that if an LLM ideally aligns with a target model\u2019s predictions, it can imitate the execution logic of the target model and make corresponding predictions, then the LLM can leverage its inherent knowledge and reasoning capabilities to generate an explanation for its predictions. Fine-tuning tasks include: Task 1: Next item retrieval. Given item titles of user history x \ud835\udc56, this task teaches the LLM about the recommendations the target model would make to the user. It is important to note that there are two major differences between Task 1 for training \ud835\udc54 and the traditional next item prediction task for training model \ud835\udc53. First, the label in Task 1 is based on the predicted items from the target model\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e8c4/e8c47128-b68e-4229-b7c6-3334a041fc78.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">illustrations for aligning LLM with different tasks.\n</div>\n<div style=\"text-align: center;\">Figure 1: Graphical illustrations for align\n</div>\n<div style=\"text-align: center;\">Figure 1: Graphical illustrations for aligning LLM with different tasks.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3781/3781c5e5-f9cc-4ec0-9dd5-858a4ebbe114.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Recommender model\n</div>\n<div style=\"text-align: center;\">Explanations\n</div>\n\ud835\udc53, rather than the ground truth in the original dataset. Second, the input and output content do not contain item IDs but are replaced with textual titles. These modifications ensure that the LLM focuses on understanding the target model\u2019s decision-making patterns. Task 2: Item ranking. Given the item titles of user history x \ud835\udc56 and a short list of item candidates p \ud835\udc56 = \u27e8 \ud835\udc4e 1,\ud835\udc4e 2, ...,\ud835\udc4e \ud835\udc58 \u27e9, this task teaches the LLM to reorder p \ud835\udc56 to reflect the order provided by \ud835\udc53 (). Retrieval and ranking are two of the most crucial tasks for recommender systems; therefore, Task 1 and Task 2 are specifically designed to align an LLM with the recommendation process of the recommender model. Task 3: Interest classification. Given the item titles of user history x \ud835\udc56 and one candidate item \ud835\udc4e \ud835\udc57, the LLM must generate a binary label: like or dislike, reflecting whether user \ud835\udc56 likes item \ud835\udc57 or not from the perspective of \ud835\udc53 (). To prepare training data samples, we set up two thresholds \ud835\udc61 + and \ud835\udc61 \u2212, selecting items with \ud835\udc53 (x \ud835\udc56,\ud835\udc4e \ud835\udc57)> \ud835\udc61 +\nas positive samples, and items with \ud835\udc53 (x \ud835\udc56,\ud835\udc4e \ud835\udc57) <\ud835\udc61 \u2212 as negative samples. Task 3 serves as a complement to Task 2: while Task 2 teaches the LLM to recognize relative orders between items, it lacks the ability to discern the absolute sentiment of \ud835\udc53 () towards items. Task 4: Item discrimination. A pretrained LLM may not possess sufficient knowledge about all items in a recommendation domain. This insufficiency can arise due to various reasons, such as the\n\npresence of fresh items and domain-specific items that appear less frequently in general knowledge sources. To address this issue of missing item knowledge, we design the item discrimination task: given an item title, let the LLM describe item details, including tags, descriptions, and related items 1. This task helps the LLM to better understand the characteristics of items. Task 5: ShareGPT training. To mitigate catastrophic forgetting, which leads to a decline in the LLM\u2019s general intelligence during fine-tuning, we also incorporate a general-purpose instruction tuning dataset. ShareGPT 2 is a publicly available dataset that contains conversations between users and ChatGPT, which is gathered from ShareGPT.com with public APIs. This dataset has been used for training numerous popular LLMs, such as Vicuna [5] and MPT 3. Incorporating ShareGPT training helps preserve the LLM\u2019s general intelligence and adaptability while fine-tuning on specific tasks. This task is important because when generating explanations, not only does LLM need to understand the target recommender model, but it also needs to combine its own reasoning, instruction following and other general intelligence abilities. All five tasks play a crucial role in generating training samples for behavior alignment. After fine-tuning, the LLM is prompted to produce model explanations. For example, given a prompt like\"[some system prompts here] Given a user with history: item title 1, item title 2, ..., will you recommend item xx to the user and why?\", the LLM can mimic the execution logic of the recommendation model and generate a well-informed and coherent explanation, demonstrating its understanding of the underlying recommendation process and user preferences.\n\n# 2.3 Intention Alignment\n\nNonetheless, LLMs exhibit capabilities far beyond mere behavior cloning. Recently, cross-modality training has shown remarkable success in enabling LLMs to comprehend multimodal content. For example, vision-language models treat text and images as two distinct modalities. By aligning perceptions derived from text and images, the resulting LLM can effectively understand the content of images. Consequently, by leveraging the inherent reasoning\n\n1 For each item, we select the top k items as its related items based on item embeddin similarities generated by the target recommender model. 2 https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered 3 https://www.mosaicml.com/blog/long-context-mpt-7b-8k\n\nabilities of the LLM, it becomes capable of providing linguistic explanations for images, such as answering the question, \" Explain why this photo is funny\". Building upon these insights, we treat the user and item embeddings generated from \ud835\udc53 as a unique modality. This data modality captures the characteristics of items and users\u2019 preferences. Consequently, we aim to align the LLM\u2019s perceptions with those originating from the user and item embeddings. We term this approach\"intention alignment\", and its underlying hypothesis is that if an LLM can comprehend the neurons of the target model while retaining its multi-step reasoning capabilities, it holds the potential to elucidate the decision-making logic of the target model. To establish an effective connection between the LLM and embeddings from \ud835\udc53, we modify the training data for Tasks 1 through 4 by replacing item names in the query prompts with their corresponding embeddings. For instance, a prompt of Task 1 becomes\"[some system prompts here] Given a user with history: [a vector of user embedding], generate the next most likely item title.\". This forces the LLM to generate accurate responses based on user and item embeddings. Specifically, for Tasks 1, 2, and 3, we replace the sequence of item titles in user history with a special token <user> and map it to a single projected user embedding \ufffd e \ud835\udc62:\n\n# \ufffd e \ud835\udc62 = \ud835\udc3a\ud835\udc38\ud835\udc3f\ud835\udc48 (e \ud835\udc62 \ud835\udc4a 1 + b) \ud835\udc4a 2\n\n\ufffd e \ud835\udc62 = \ud835\udc3a\ud835\udc38\ud835\udc3f\ud835\udc48 (e \ud835\udc62 \ud835\udc4a 1 + b) \ud835\udc4a 2\n\n(1)\n\n\ufffd\nThe projection operation aims to extend the original user embeddings generated by f (e.g., with a dimension of 32) to match the length of token embeddings in the LLM (e.g., with a dimension of 4096). For Tasks 2, 3, and 4, we substitute the candidate item title with a special token <item> and map it to the projected item embedding \ufffd e \ud835\udc56, using a projection similar to 1 but with a new set of parameters. In addition to Tasks 1 through 5, we design an auxiliary task for intention alignment to enhance the information fidelity between user embeddings and the users\u2019 true history: Task 6: History reconstruction. Given a projected user embedding \ufffd e \ud835\udc62, this task recovers the titles of items in the user\u2019s history or the preference summary of the user history. We leverage GPT-4 4\n[29] to generate a preference summary for each user based on user history titles in advance. It is important to note that Tasks 2 and 3 primarily concentrate on understanding the relationships between user embeddings and item embeddings, but they do not sufficiently explore the self-contained information within user embeddings. Task 6 is designed to address this limitation. For better illustration, Figure 1 provides a comparison among the processes of different tasks to highlight the distinctions, while Figure 2 shows the model architecture of the intention alignment.\n\n# 2.4 Hybrid Alignment\n\nThe intention alignment approach entirely depends on user/item embeddings to decode preference-related information, which may lead to a too strict hypothesis. During the training of model \ud835\udc53, a certain degree of information will inevitably be lost, such as it is hard to fully identify every item in user history from the encoded user embeddings. To mitigate the information lost, we design the\n\n4 The snapshot of GPT-4 is gpt-4-0314.\n\nthird approach called \"hybrid alignment\", combining both the previous approaches. All Task 1 through 6 are included. for tasks that involve user history or item candidates, hybrid alignment not only include both data forms of behavior alignment and intention alignment, but also add a new data form: simultaneously put both user history/item candidates and user/item embeddings in the query prompt. Thus, a prompt may look like: \"[some system prompts here] Given a user with history: [a vector of user embedding], item title 1, item title 2, ..., generate the next most likely item title.\"\n\n# 3 Experiments\n\n# 3.1 Evaluation Strategies and Metrics\n\nIn measuring performance of our RecExplainer on explaining recommendation models, we evaluate from two perspectives:\n\n3.1.1 Alignment Effect. We first assess the LLM\u2019s alignment effect, that is, to what extent LLMs can understand neurons and predictive patterns of the target recommender model. Following the previous work [52], we apply the leave-one-out strategy for evaluation. We take the last item of each user\u2019s interaction sequence as the test data and use the other records for training the LLM and target recommender model. It should be noted that when training the target recommender model, we use labels from the original dataset, but when training the LLM, we use labels inferred by the welltrained recommendation model. We evaluate four alignment tasks, including task1 (next item retrieval), task2 (item ranking), task3 (interest classification), and task6 (history reconstruction). For next item retrieval, we adopt the top-K hit ratio (HR) and top-K normalized discounted cumulative gain (NDCG) for evaluation, where we set K to 5. For item ranking, we calculate the NDCG@5. For interest classification, we use classification accuracy. For history reconstruction, we define a history coverage ratio (HCR) metric, which calculates the proportion of items in the user history that appear in the predicted sequence. During the inference of LLM, we use greedy decoding to generate texts and consider a successful output when there is a strict string match between generated item names and their ground-truth names.\n3.1.2 Explanation Generation Ability. Considering that there is no available ground truth for the explanation, we need a new evaluation system to demonstrate the effectiveness of our method on model explainability. Specifically, we design an instruction to prompt the LLM to first evaluate the target item and then generate a coherent explanation:\"The user has the following purchase history: {USER HISTORY} . Will the user like the item: {ITEM} ? Please give your answer and explain why you make this decision from the perspective of a recommender model. Your explanation should include the following aspects: summary of patterns and traits from user purchase history, the consistency or inconsistency between user preferences and the item.\" Following [46], we implement a four-level scoring system to quantitatively evaluate the response from the LLM. Complete criteria can be found in Appendix A.1. \u2022  RATING0: Incorrect classification. \u2022  RATING1: Correct classification, insufficient explanation. LLM provides irrelevant explanations or provide explanations with hallucination.\n\n3.1.1 Alignment Effect. We first assess the LLM\u2019s alignment effect, that is, to what extent LLMs can understand neurons and predictive patterns of the target recommender model. Following the previous work [52], we apply the leave-one-out strategy for evaluation. We take the last item of each user\u2019s interaction sequence as the test data and use the other records for training the LLM and target recommender model. It should be noted that when training the target recommender model, we use labels from the original dataset, but when training the LLM, we use labels inferred by the welltrained recommendation model. We evaluate four alignment tasks, including task1 (next item retrieval), task2 (item ranking), task3 (interest classification), and task6 (history reconstruction). For next item retrieval, we adopt the top-K hit ratio (HR) and top-K normalized discounted cumulative gain (NDCG) for evaluation, where we set K to 5. For item ranking, we calculate the NDCG@5. For interest classification, we use classification accuracy. For history reconstruction, we define a history coverage ratio (HCR) metric, which calculates the proportion of items in the user history that appear in the predicted sequence. During the inference of LLM, we use greedy decoding to generate texts and consider a successful output when there is a strict string match between generated item names and their ground-truth names.\n\n3.1.2 Explanation Generation Ability. Considering that there is no available ground truth for the explanation, we need a new evaluation system to demonstrate the effectiveness of our method on model explainability. Specifically, we design an instruction to prompt the LLM to first evaluate the target item and then generate a coherent explanation:\"The user has the following purchase history: {USER HISTORY} . Will the user like the item: {ITEM} ? Please give your answer and explain why you make this decision from the perspective of a recommender model. Your explanation should include the following aspects: summary of patterns and traits from user purchase history, the consistency or inconsistency between user preferences and the item.\" Following [46], we implement a four-level scoring system to quantitatively evaluate the response from the LLM. Complete criteria can be found in Appendix A.1. \u2022  RATING0: Incorrect classification. \u2022  RATING1: Correct classification, insufficient explanation. LLM provides irrelevant explanations or provide explanations with hallucination.\n\n\u2022  RATING2: Correct classification, acceptable explanation with minor imperfections such as lack of persuasiveness or informativeness.\nRATING3: Correct classification, satisfying explanation.\n\n\u2022  RATING2: Correct classification, acceptable explanation with minor imperfections such as lack of persuasiveness or informativeness.\n\n\u2022  RATING3: Correct classification, satisfying explanation.\n\nThe evaluation criteria are formulated with a two-step approach: initially assessing correctness of the classification, followed by the assessment of explanation quality. The correctness of classification holds significant importance as it serves as an indicator of whether the LLM is formulating explanations based on an accurate understanding of the target model, rather than relying on conjecture. Considering that human annotation is extremely time-consuming and labor-intensive, we adopt a combined approach using both human annotators and LLM annotators. Some studies [4, 7] have already demonstrated that LLM can to some extent replace manual annotations. Since GPT-4 4 is currently the most powerful LLM with strong abilities to follow instructions and perform reasoning tasks, we adopt both GPT-4 scoring and human scoring strategies to evaluate our generated explanations. More specifically, for GPT-4, we use the aforementioned evaluation criteria as prompts, inputting them into GPT-4 to generate scores. We sample 500 test cases for each dataset, calculating the mean score of each LLM on each dataset. Regarding human scoring, due to cost considerations, we select a sample of 120 test cases from a single dataset. Given that there are five different LLMs for text generation, this results in a total of 600 generated texts for human evaluation. For more details about human and GPT-4 annotations, please refer to Appendix A. These human evaluation results effectively complement the GPT-4 evaluation results, providing a comprehensive assessment.\n\n# 3.2 Experimental Setup\n\n3.2.1 Datasets. We evaluate our model on three public datasets: Video Games and Movies & TV dataset released in Amazon platform 5 [28], and Steam 6 [16, 30]. For the specific tasks, we generate data as follows: for next item retrieval, we treat the top-1 prediction of the target recommender model as the ground truth; for item ranking, we sample five items from the entire item set for each sample, and use the ranking order produced by the target model as the ground truth; for interest classification, we set the \ud835\udc61 + and \ud835\udc61 \u2212 threshold as the top 20%, bottom 50% respectively, and sample one positive and one negative item for each user for training and testing. The dataset details can be found in Appendix B.\n\n3.2.2 Implementation details. Our backbone LLM is vicuna-v1.3-7b [5] with a maximum context length of 1024. We employ LoRA [14] for parameter-efficient tuning and leverage DeepSpeed\u2019s ZeRO-2 [35] to further reduce gpu memory consumption. We use 8 NVIDIA V100-32GB GPUs and fp16 for training 10 epochs, with a total batch size of 64. The peak learning rate is 1e-4 with a linear warmup for the first 10% steps. We train our model using the standard language modeling objective and only compute loss on the response tokens. For the target recommender model, we adopt the powerful transformer-based model SASRec [16], which is lightweight and effective. Hyperparameter tuning is performed to train SASRec on three datasets, obtaining a well-trained sequential recommender.\n\n5 https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/ 6 https://github.com/kang205/SASRec\n\nSpecifically, the embedding size is set to 256, 192, 256 on Video Games, Movies & TV and Steam datasets, respectively, and the max sequence length is set to 9, 13, 16 respectively. During LLM\u2019s training, the target SASRec is fixed, which is used only for inferring user embeddings and item embeddings.\n3.2.3 Baselines. For evaluating the alignment effect, we employ two statistical models, a raw LLM and three aligned models.\n\u2022 Random: Sample k items uniformly from the item set for retrieval. Random shuffle the candidate items for ranking.\n\u2022 Popularity: Sample k items based on the item popularity distribution for retrieval. Sort the candidate items according to the item popularity.\n\u2022 Vicuna-v1.3-7B [5]: An open-source LLM obtained by finetuning the LLaMa model on ShareGPT data. This model serves as the base model which is not fine-tuned on our in-domain dataset.\n\u2022 Vicuna-v1.3-7B-ICL [5]: In-context learning is an effective approach to align LLMs to do specific tasks. Specifically, when performing a specific task, we randomly select two instances from the training set of that task and place them at the beginning of the LLM\u2019s context.\n\u2022 GPT4-ICL [29]: Same in-context learning strategy but with the currently most powerful closed-source LLM from OpenAI.\n\u2022 SASRec [16]: a mainstream traditional sequential recommender model. To enable the SASRec to align to the target recommender model and complete several alignment tasks, we use knowledge distillation outlined in EMKD[10]. Specifically, we minimize the Kullback-Leibler divergence between the teacher logits and the student logits, where the logits represent the scores given by users for the entire item set.\nFor evaluating the explanation generation ability, we use  Vicunav1.3-7B and ChatGPT 7 (gpt-3.5-turbo-0301), as other aforementioned methods either are not text generation models or lack readily available examples for in-context learning. Additionally, the three proposed alignment methods themselves are suitable for mutual comparisons on both evaluation settings.\n\nSpecifically, the embedding size is set to 256, 192, 256 on Video Games, Movies & TV and Steam datasets, respectively, and the max sequence length is set to 9, 13, 16 respectively. During LLM\u2019s training, the target SASRec is fixed, which is used only for inferring user embeddings and item embeddings.\n\n# 3.3 Performance w.r.t. Alignment\n\nTo investigate the alignment effect of LLM after training, we evaluate model\u2019s performance on four recommendation-related tasks, with the results presented in Table 1. It should be noted that for Tasks 1, 2 and 3 in Table 1, we use the inference results of the target recommender model as ground truth labels and the SASRec baseline in Table 1 is actually another model aligned with the target recommender model. We have the following observations: RecExplainer-H can achieve comparable performance with the powerful SASRec, and often performs better in retrieval (task1) and classification (task3) tasks. This demonstrates that our RecExplainer\u2019s alignment training is sufficiently effective, as only thorough alignment can ensure the reliability of subsequent explanation generation. For the vicuna-7b model without alignment, the performance is unsatisfactory across all tasks. This suggests that there is still a significant gap between the target model and the LLMs, demonstrating the necessity of alignment training. In comparison to vicuna-7B, vicuna-7B-ICL with the adoption of the in-context\n\n7 https://chat.openai.com/\n\nformance w.r.t Alignment to the target recommender model. \"N/A\" represents that the method can not be applied t\n\nTable 1: Performance w.r.t Alignment to the target recommender model. \"N/A\" represents that the method can not b corresponding task.\n\nTable 1: Performance w.r.t Alignment to the target recommender model. \"N/A\" represents that the method ca corresponding task.\n\nTable 1: Performance w.r.t Alignment to the target recommender model. \"N/A\" re corresponding task.\n\nDataset\nAmazon Video Games\nAmazon Movies and TV\nSteam\nTask\nTask1\nTask2\nTask3\nTask6\nTask1\nTask2\nTask3\nTask6\nTask1\nTask2\nTask3\nTask6\nMethods\nH@5\nN@5\nN@5\nACC\nHCR\nH@5\nN@5\nN@5\nACC\nHCR\nH@5\nN@5\nN@5\nACC\nHCR\nRandom\n0.0023\n0.0015\n0.6153\n0.5030\nN/A\n0.0050\n0.0030\n0.6100\n0.4987\nN/A\n0.0060\n0.0039\n0.6139\n0.4977\nN/A\nPopularity\n0.0077\n0.0047\n0.6683\nN/A\nN/A\n0.0150\n0.0088\n0.7044\nN/A\nN/A\n0.0321\n0.0201\n0.7971\nN/A\nN/A\nVicuna-7B\n0.0026\n0.0014\n0.2391\n0.5026\nN/A\n0.0091\n0.0062\n0.2706\n0.5011\nN/A\n0.0028\n0.0015\n0.3229\n0.5000\nN/A\nVicuna-7B-ICL\n0.0379\n0.0304\n0.2661\n0.5070\nN/A\n0.0144\n0.0104\n0.3005\n0.5079\nN/A\n0.0461\n0.0332\n0.2907\n0.5076\nN/A\nGPT4-ICL\n0.1105\n0.0864\n0.6492\n0.6338\nN/A\n0.0886\n0.0692\n0.5954\n0.5964\nN/A\n0.3008\n0.2525\n0.6750\n0.5886\nN/A\nSASRec\n0.6736\n0.5234\n0.8759\n0.7768\nN/A\n0.6217\n0.5025\n0.8252\n0.6541\nN/A\n0.9751\n0.8780\n0.9577\n0.8914\nN/A\nRecExplainer-B\n0.7460\n0.6260\n0.7521\n0.8365\nN/A\n0.8106\n0.7027\n0.7033\n0.7818\nN/A\n0.9310\n0.8100\n0.8699\n0.9554\nN/A\nRecExplainer-I\n0.8436\n0.6994\n0.8299\n0.9385\n0.1162\n0.9039\n0.7709\n0.8290\n0.8396\n0.1201\n0.9615\n0.8122\n0.9083\n0.9904\n0.0659\nRecExplainer-H\n0.8057\n0.6922\n0.8458\n0.9189\n0.1325\n0.8773\n0.7750\n0.7638\n0.8109\n0.1461\n0.9358\n0.8242\n0.9036\n0.9815\n0.0707\nlearning method shows some improvements, but its performance remains relatively low. On the other hand, gpt4-ICL demonstrates significantly higher performance than vicuna-7B-ICL, showcasing its strong general intelligence. However, the performance of gpt4-ICL still lags far behind RecExplainer-H, as alignment training enables the LLM to thoroughly learn the recommendation paradigm of the target model on the entire dataset. Regarding our three alignment methods, RecExplainer-B performs the worst across all tasks and datasets, suggesting that merely imitating the recommendation behavior of the target model is not an optimal solution for understanding the target model. Given that the neurons of the target recommender model (such as user and item embeddings) can inherently reflect the recommendation paradigms and collaborative signals in the target model, we can see that the performance of RecExplainer-I improves significantly when these embeddings are used as part of prompts for the LLM. For RecExplainer-H, its performance is superior to all other approaches except on next item retrieval (task1) and interest classification (task3) tasks, which are slightly lower than that of RecExplainer-I in some datasets. A possible reason is that when both neuron signals and textual signals are added to LLM\u2019s inputs, the LLM may overly rely on the text and, to some extent, neglect the role of neurons. Overall, The performance of RecExplainer-H is very powerful, indicating that textual and neuron signals can complement each other, jointly enhancing the LLM\u2019s understanding of the target model. In conclusion, compared to models without alignment, LLMs with alignment training significantly enhance their predictive ability for the pre-trained target model. They can achieve comparable performance with existing alignment-based recommendation models, indicating that LLMs with alignment training have effectively learned the paradigm and neurons of the target model, making it suitable for subsequent recommendation explanation tasks.\n\n# Performance w.r.t. Explanation\n\n3.4.1 Overall Ratings. Evaluation results from GPT-4 and human experts are shown in Table 2 and Figure 3 respectively. We have the following observations: The trend of the two evaluation strategies are the same, validating the credibility of our evaluation method. Among which,\n\nRecExplainer-H achieves the highest scores on all three datasets, indicating that it can mimic the execution logic of the target model well and give satisfying explanations for that logic. RecExplainer-B comes next, suggesting that behavioral imitation is also helpful in understanding the execution paradigm of target models. For the two unaligned LLMs, Vicuna-7B and ChatGPT, they can generate reasonably good explanatory texts in some cases through their powerful reasoning capabilities and internal knowledge. However, since they are unrelated to the target model and are not aware of the target models\u2019 predictive patterns, they are not sure whether the target model would recommend the item or not. As a result, their explanations tend to be ambiguous and lack persuasiveness and their scores tend to fall under the RATING-2. Another point worth mentioning is that we find RecExplainer-I has the lowest evaluation scores. By examining specific examples, we discover that it generates explanations with hallucination, such as mistaking other items as the current user\u2019s history. This indicates that there might be a certain gap in directly reconstructing textual content from neuron signals, as the information may be insufficient. This is also demonstrated by the relatively low metrics of the history reconstruction task in the previous section.\n\n3.4.2\n\n3.4.2 Distinction and Coherence.  To further verify whether RecExplainer are indeed explaining its own predictions, we conduct validation from two perspectives: (1) Is RecExplainer\u2019s explanations distinct from other LLM\u2019s explanations? (2) Do RecExplainer\u2019s explanations reflect RecExplainer\u2019s predictions? We generate 2500 explanations for each of Vicuna-7B, ChatGPT, and RecExplainer-H, and divide them into training and testing sets at 4:1 ratio. Firstly, we train a discriminator to prove that the explanations generated by RecExplainer possess sufficient distinctiveness and are different from those produced by models without alignment training. The experimental results are illustrated in Figure 4, revealing that this discriminator can easily differentiate explanations from RecExplainer and other models. Secondly, we develop score predictors to assess the alignment between the explainer\u2019s textual explanations and the target recommender model\u2019s predictions. Specifically, for a given user-item pair (\ud835\udc62,\ud835\udc56), \ud835\udc53 (x \ud835\udc62,\ud835\udc4e \ud835\udc56) represents the prediction made by the target recommender model. The score\n\npredictors are then trained using the explainer\u2019s textual explanations as input, with the goal of closely approximating \ud835\udc53 (x \ud835\udc62,\ud835\udc4e \ud835\udc56). This is a regression task, so we evaluate this using Mean Squared Error (MSE). The results, as presented in Table 3, indicate that explanations produced by RecExplainer offer a significant advantage when used to predict scores of the target model, confirming that RecExplainer effectively utilizes the understanding of the target model\u2019s behavior patterns during the explanation generation process. Both the discriminator and the score predictor employed in this study are based on the base version of BERT[17].\n\nTable 2: Performance w.r.t. Explanation (GPT-4). The higher score represents the better performance in explanation, ranging from 0 to 3.\n\nMethods\nGames\nMovies\nSteam\nVicuna-7B\n2.0703\n2.0261\n2.0341\nChatGPT\n1.9320\n1.8360\n1.9560\nRecExplainer-B\n2.3240\n2.1360\n2.4660\nRecExplainer-I\n1.6653\n1.4689\n1.3394\nRecExplainer-H\n2.5240\n2.2204\n2.4920\nTable 3: Performance w.r.t score predictors. The metric is Mean Squared Error (MSE).\n\n<div style=\"text-align: center;\">Table 3: Performance w.r.t score predictors. The metric is Mean Squared Error (MSE).\n</div>\nMethods\nGames\nMovies\nSteam\nVicuna-7B\n3.6970\n2.8373\n0.9687\nChatGPT\n3.4803\n2.8393\n1.0288\nRecExplainer-H\n1.2786\n1.9190\n0.3248\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec24/ec24fa43-88e4-44e4-ae9c-313be8a6ef7e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e 3: Performance w.r.t Explanation (Human experts) on\n</div>\n<div style=\"text-align: center;\">Figure 3: Performance w.r.t Explanation (Human experts) o Amazon Video Games dataset.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd1f/cd1ff5d6-ab70-4224-8057-116ca35d5f29.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Confusion matrix of explanation discriminatio\n</div>\n# 3.5 Case Study\n\n3.5.1 Explanation Quality. We show cases of each method for straightforward effect of explanation, illustrated in Figure 5. Both RecExplainer-B and RecExplainer-H give convincing explanations, which point out that firstly the user prefers gaming accessories and devices instead of games, and secondly the user has no explicit engagement in Xbox-360 platform, exhibiting high consistency with the output of the target recommender model. Nevertheless, Vicuna and ChatGPT do not give a satisfying and persuasive explanation. This is because they do not align with the target model and can only rely on their own knowledge and logic to make certain conjectures about user preferences, which may cause errors. Notably, RecExplainer-I exhibits hallucination in giving non-existing games. This illustrates that although the alignment is effective, solely relying on hidden neurons to recover the domain information of user history/items and make explanations are not enough due to the information compression loss in embeddings of the target model.\n3.5.2 Controllable Explanations.  Benefit from the powerful instruction following capability and multi-step reasoning ability of LLM, our RecExplainer possesses interactive capabilities, enabling itself to understand user requirements and dynamically adjust the content of its explanations accordingly. Concretely, we instruct RecExplainer to predict and explain from two different view, i.e. the game platforms and game genres, the cases are shown in Figure 6. The model could generate consistent predictions with the target model in each case, and the two explanations indeed vary corresponding to the instructions, demonstrating the remained instruction following capability and the controllability of our RecExplainer.\n\n# 3.6 Ablation Study w.r.t Explanation\n\nIn our method, we design multiple training tasks to help the LLM better understand target recommender models and domain-specific data. To explore the impact of each task on LLM\u2019s explanation generation ability, we remove each task to train a RecExplainer and evaluate the explanation quality using GPT-4. As shown in Figure 7, each training task contributes to the final explanation quality. Specifically, task3 (interest classification) and task 6 (history reconstruction) have shown to have the most substantial impacts. On the one hand, compared to item embedding, information in user embedding is more severely compressed, and history reconstruction effectively facilitates learning this information. On the other hand, during explanation generation, the model needs to justify the prediction of the target model. Therefore, the classification task also has a significant impact on the model\u2019s explanation performance.\n\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af64/af640462-d4f3-4479-a9a5-42faf9497dcc.png\" style=\"width: 50%;\"></div>\n# \n<div style=\"text-align: center;\"></div>\n\n# \n\n# \n\n\nFigure 5: Case study for explanation on Amazon Video Games dataset. Fonts highlighted in red signify the presence of errors within the generated explanations, while those highlighted in green denote accurate and logical explanations.\n\n# 4 Related Work\n\n# 4 Related Work 4.1 Model Explainability\n\n# 4.1 Model Explainability\n\nDeep neural models have demonstrated state-of-the-art (SOTA) performance in various machine learning tasks, and explaining the decision-making logic behind these black-box models has become a significant area of research. Existing literature in this field can be broadly divided into two categories [49]. The first category focuses on identifying the most salient parts of input features that contribute to the decision result of the target model, with the primary challenge being the effective implementation of score attribution. [11] present early work investigating the explainability of deep models by visualizing what a neuron computes in the input space, using several alternative methods such as sampling from a neuron and maximizing the activation of a neuron. Input space visualization\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ea7/5ea7c29c-9da9-4123-a270-1d4cbe715263.png\" style=\"width: 50%;\"></div>\n# \nFigure 6: Case study for controllable explanation on Amazon Video Games dataset.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afbb/afbbb0a2-e668-442b-98ab-4e98178563f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">igure 7: Ablation study on Amazon Video Games dataset.\n</div>\nFigure 7: Ablation study on Amazon Video Games da\n\nhas since been widely applied in vision tasks [50, 51] to facilitate human understanding of deep models. [3, 12] propose models that select salient input features by maximizing the mutual information between salient features and response variables. Additionally, other popular feature selection methods include neuron contribution difference [38], integrated gradients [40], influence functions [18], and Shapley value estimation [6, 27, 39]. The second category of methods involves training a surrogate model to explain the target model. Surrogate models should maintain high fidelity to the target model\u2019s predictive patterns while also possessing explainable properties. Consequently, linear models and decision trees are the most commonly used surrogate models. [21, 37, 54] develop methods for generating a small number of compact decision rules as surrogates to explain the target model. [36] proposes a general model explanation framework that allows for the flexible selection of various surrogate model forms, such as linear models or decision trees. However, surrogate models need\n\nto be structurally simple for explaining complex models, which creates a dilemma with model fidelity. Recently, LLMs have demonstrated strong versatility in terms of predictive accuracy and reasoning/explanation ability, providing an opportunity to overcome this dilemma. As a novel advancement in the first category of related works, [1] utilizes GPT-4 to automatically generate explanations for neurons based on the attributed input features. To the best of our knowledge, this paper is the first to discuss leveraging LLMs for the second category of methods.\n\n# 4.2 LLMs and Multimodality\n\nIn recent years, the Transformer architecture [42] has become a fundamental building block for language models. Researchers have discovered that by pretraining large Transformer-based language models on extensive open data sources, these models exhibit enhanced capabilities in knowledge accumulation, multi-task learning, and few-shot or even zero-shot predictions [2, 8, 17, 33, 34]. Remarkably, when the model size reaches a certain scale (e.g., 170 billion parameters), language models exhibit emergent abilities [47], which are unforeseen phenomena, such as instruction following, reasoning, and problem-solving skills, indicating a preliminary step towards AGI. Consequently, researchers have begun to investigate whether emergent abilities can also be present in smaller-scale models (e.g., 7B models) if trained effectively. This inquiry has led to the development of several popular models, including OPT [53], Llama [41], Vicuna [5], WizardLM [48], and phi-1.5 [24]. The intelligence of large models is not solely confined to text; other data modalities, such as images [9, 13] and audio [32], can also benefit from large-scale pretraining. Multimodal models [23, 31, 43, 44] seek to break boundaries and bridge different modalities, as humans can simultaneously comprehend multimodal knowledge and perform complex tasks using all available information. Fundamentally, multimodal models align the models\u2019 representations across different modalities in the form of latent embeddings [15, 45], enabling them to not only perceive each individual modality but also the interactions across modalities. With this perspective in mind, this paper considers the embeddings of recommender models as a new data modality and proposes a novel model explainer based on aligning LLMs.\n\n# 5 Conclusion\n\nIn this paper, we investigate the potential of employing large language models (LLMs) as surrogate models to enhance the explainability of recommender systems. LLMs, known for generating highquality, human-readable explanations, offer a promising solution to the traditional dilemma of necessitating simple models for selfexplainability, thus paving the way for more advanced and transparent AI systems. We introduce three innovative alignment approaches \u2014 behavior alignment, intention alignment, and hybrid alignment \u2014 to facilitate effective model alignment. Each of these approaches offers unique advantages in terms of explainability and fidelity. Through rigorous evaluation on three publicly available datasets, we demonstrate the effectiveness of our proposed alignment approaches in both comprehension and explanation. Empirical evidence highlights the potential of LLMs as a new type of surrogate model for explainable recommender systems. As an\n\ninitial attempt, our research contributes to the ongoing efforts in explainable AI, paving the way for future work on leveraging LLMs for a wide range of explainability applications in complex systems.\n\n# Acknowledgments\n\nThe work was supported by grants from the National Key R&D Program of China (No. 2021ZD0111801) and the National Natural Science Foundation of China (No. 62022077).\n\n# References\n\n[1] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023) (2023).\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.\n[3] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. 2018. Learning to explain: An information-theoretic perspective on model interpretation. In International conference on machine learning. PMLR, 883\u2013892.\n[4] David Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 15607\u201315631. https://doi.org/10.18653/V1/2023.ACL-LONG.870\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) (2023).\n[6] Anupam Datta, Shayak Sen, and Yair Zick. 2016. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In 2016 IEEE symposium on security and privacy (SP). IEEE, 598\u2013617.\n[7] Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. Is GPT-3 a Good Data Annotator?. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 11173\u201311195. https://doi.org/10.18653/V1/2023.ACL-LONG.626\n[8] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pretraining for natural language understanding and generation. Advances in neural information processing systems 32 (2019).\n[9]  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In  9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=YicbFdNTTy\n10] Hanwen Du, Huanhuan Yuan, Pengpeng Zhao, Fuzhen Zhuang, Guanfeng Liu, Lei Zhao, Yanchi Liu, and Victor S. Sheng. 2023. Ensemble Modeling with Contrastive Knowledge Distillation for Sequential Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023. ACM, 58\u201367. https://doi. org/10.1145/3539618.3591679\n11]  Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2009. Visualizing higher-layer features of a deep network. University of Montreal 1341, 3 (2009), 1.\n12] Chaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin Chen, Di He, and Xing Xie. 2019. Towards a Deep and Unified Understanding of Deep Neural Models in NLP. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2454\u20132463. https://proceedings.mlr.press/ v97/guan19a.html\n13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 16000\u201316009.\n14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id=nZeVKeeFYf9\n15]  Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023.\n\nLanguage is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045 (2023).\n[16]  Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Recommendation. In IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018. IEEE Computer Society, 197\u2013206. https: //doi.org/10.1109/ICDM.2018.00035\n[17] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, Vol. 1. 2.\n[18] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 1885\u20131894. https://proceedings.mlr. press/v70/koh17a.html\n[19] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. 426\u2013434.\n[20]  Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30\u201337.\n[21]  Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2017. Interpretable & explorable approximations of black box models. arXiv preprint arXiv:1707.01154 (2017).\n[22] Yuxuan Lei, Xiaolong Chen, Defu Lian, Peiyan Zhang, Jianxun Lian, Chaozhuo Li, and Xing Xie. 2023. Practical Content-aware Session-based Recommendation: Deep Retrieve then Shallow Rank. In Amazon KDD Cup 2023 Workshop.\n[23] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019).\n[24] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).\n[25] Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing Xie. 2020. Lightrec: A memory and search-efficient recommender system. In Proceedings of The Web Conference 2020. 695\u2013705.\n[26] Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong Rui. 2014. GeoMF: joint geographical modeling and matrix factorization for point-ofinterest recommendation. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 831\u2013840.\n[27] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural information processing systems 30 (2017).\n[28] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 188\u2013197.\n[29] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [30]  Apurva Pathak, Kshitiz Gupta, and Julian McAuley. 2017. Generating and personalizing bundle recommendations on steam. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1073\u20131076.\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748\u20138763.\n[32] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning. PMLR, 28492\u201328518.\n[33] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).\n[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\n[35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, Christine Cuicchi, Irene Qualters, and William T. Kramer (Eds.). IEEE/ACM, 20. https://doi.org/10.1109/SC41405.2020.00024\n[36] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.\n[37] Gregor PJ Schmitz, Chris Aldrich, and Francois S Gouws. 1999. ANN-DT: an algorithm for extraction of decision trees from artificial neural networks. IEEE Transactions on Neural Networks 10, 6 (1999), 1392\u20131401.\n[38]  Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating Activation Differences. In Proceedings of the\n\n34th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 3145\u20133153. https://proceedings.mlr.press/v70/shrikumar17a.html\n[39] Erik \u0160trumbelj and Igor Kononenko. 2014. Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems 41 (2014), 647\u2013665.\n[40] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328. https://proceedings.mlr.press/v70/ sundararajan17a.html\n[41]  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[43] Teng Wang, Wenhao Jiang, Zhichao Lu, Feng Zheng, Ran Cheng, Chengguo Yin, and Ping Luo. 2022. Vlmixer: Unpaired vision-language pre-training via cross-modal cutmix. In International Conference on Machine Learning. PMLR, 22680\u201322690.\n[44] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. 2023. Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 19175\u201319186. https://doi.org/10.1109/CVPR52729.2023.01838\n[45] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023. Visually-Augmented Language Modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id= 8IN-qLkl215\n[46] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13484\u201313508. https://doi.org/10.18653/v1/2023.acl-long.754\n[47] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).\n[48] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).\n[49] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. 2019. Gnnexplainer: Generating explanations for graph neural networks.  Advances in neural information processing systems 32 (2019).\n[50] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. 2015. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579 (2015).\n[51]  Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer, 818\u2013833.\n[52]  Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and JiRong Wen. 2023. Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. CoRR abs/2305.07001 (2023). https://doi.org/10.48550/arXiv.2305.07001 arXiv:2305.07001\n[53] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).\n[54] Jan Ruben Zilke, Eneldo Loza Menc\u00eda, and Frederik Janssen. 2016. Deepred\u2013rule extraction from deep neural networks. In Discovery Science: 19th International Conference, DS 2016, Bari, Italy, October 19\u201321, 2016, Proceedings 19. Springer, 457\u2013473.\n\n# A Details for human and GPT-4 annotations A.1 Criteria for GPT-4 and Human Experts\n\n# A Details for human and GPT-4 annotations\n\n# A.1 Criteria for GPT-4 and Human Experts\n\nWe ask human experts and GPT-4 to score explanations generated by all the LLMs with the same criteria. The prompt is shown in Figure A1.\n\nWe ask human experts and GPT-4 to score explanations generated by all the LLMs with the same criteria. The prompt is shown in Figure A1.\n\nPlease act as an impartial judge and evaluate the AI assistant\u2019s recommendation decision as well as decision explanation based on the user\u2019s purchase history, target item, and ground truth label. Assign a score according to the following four levels:\nRATING-0: Incorrect classification - The assistant fails to generate a correct recommendation decision.\nRATING-1: Correct classification, insufficient explanation - The assistant correctly makes the recommendation decision but provides no, few, or irrelevant explanations, or provides explanations with hallucination, some of which do not conform to the actual situation.\nRATING-2: Correct classification, acceptable explanation - The assistant correctly makes the recommendation decision and provides an explanation that is logically consistent and aligns with the user\u2019s history and target item. But the explanation still has minor imperfections such as lack of persuasiveness or informativeness.\nRATING-3: Correct classification, satisfying explanation - The assistant correctly makes the recommendation decision and provides a satisfactory explanation, including a summary of the user\u2019s historical behavior patterns and characteristics, as well as a thorough analysis of the consistency or inconsistency between user preferences and the target item.\nPlease give your score in the form of <br>RATING</br>, for example, if the rating is 1, output <br>RATING-1</br>. Do not allow the length of the explanation to influence your evaluation. Be as objective as possible.\nKnown information: User history: {USER HISTORY}, Target item: {ITEM}, Label: {YES/NO}. Assistant\u2019s output: {EXPLANATIONS}\n\nFigure A1: Prompt for the evaluation criteria\n\n<div style=\"text-align: center;\">Table B1: Statistics of the datasets.\n</div>\nDataset\n#Users\n#Items\n#Inters\nSparsity\nGames\n3,901\n1,864\n31,672\n99.564%\nMovies\n3,194\n1,170\n28,105\n99.248%\nSteam\n2,493\n986\n27,498\n98.881%\n<div style=\"text-align: center;\">Table B2: Dataset details for each task.\n</div>\nDataset\nVideo Games\nMovies and TV\nSteam\nSplit\n# Train\n# Test\n# Train\n# Test\n# Train\n# Test\nTask 1\n23,870\n3,901\n21,717\n3,194\n22,512\n2,493\nTask 2\n23,870\n3,901\n21,717\n3,194\n22,512\n2,493\nTask 3\n7,802\n2,294\n6,388\n1,888\n4,986\n1,456\nTask 4\n9,177\n0\n4,363\n0\n3,856\n0\nTask 5\n10,000\n1,000\n10,000\n1,000\n10,000\n1,000\nTask 6\n27,771\n3,901\n24,911\n3,194\n25,005\n2,493\nTotal\n102,490\n14,997\n89,096\n12,470\n88,871\n9,935\n# A.2 Human Evaluation Setup\n\nWe ask three experts, all of whom are master students majoring in recommender systems and are not involved in the co-author list, to evaluate the generated results of all LLMs. These three experts coordinate the standards of the 4-level rating system before starting annotations and then each of them rates all the instances independently. During the evaluation process, they are presented with the target label, user history, target item, and model responses. Model responses are listed in random order, with all the model information anonymized, ensuring that the experts are unaware of the specific LLM responsible for generating each text.\n\n# A.3 Human and GPT-4 Evaluation Agreement\n\nWe have also included calculations for both inter-human agreement and gpt4-human agreement. When calculating inter-human agreement, we conduct pairwise comparisons among the three human\n\nannotators and compute the average metric. For the gpt4-human agreement calculation, we separately compute the metrics for the three gpt4-human pairs and then average them. We first report Cohen\u2019s \ud835\udf05, which is commonly used to measure inter-rater agreement for categorical items. When calculating this, we treat the 4-level rating (1-4) as a categorical variable. The \ud835\udf05 for inter-human and gpt4-human is 0.366 and 0.316 respectively, which both show a moderate agreement. We also compute the Spearman correlation coefficient \ud835\udf0c between the ratings of our two evaluators (human or gpt4) by treating the rating as an ordinal variable (4>3>2>1). The coefficient for interhuman and gpt4-human is 0.563 and 0.714 respectively, which both indicate a high correlation between the two evaluators.\n\n# B Details about Dataset generation B.1 Templates for Data Generation\n\nTo better align the LLM to the target recommendation model, we design several training tasks for the LLM to understand the predictive behaviors of the target model and the domain-specific data. Following [5], All tasks are formed into USER-ASSISTANT format. We list all the templates used in our datasets in Figure B2.\n\n# B.2 Statistics of the Datasets\n\nConsidering that each dataset has millions of interaction records, we reduce their sizes to avoid unacceptable training costs. Concretely, we filter each dataset by first selecting items with top frequency, and retaining users\u2019 interaction history on this item set. Finally, we randomly select a subset of users as our dataset. Following prior work [52], we also apply a 5-core filter, further removing users and items with fewer than five interactions from the dataset. We show the overall data statistics and statistics for each task in Table B1 and Table B2, respectively.\n\nTask Name: Next Item Retrieval Assistant Response:\"{ITEM TITLE}\" User Question: 1.\"Given the user purchase history: {USER HISTORY} , generate the next most likely clicked item title.\" 2.\"What is the next most likely clicked item title for the purchase history: {USER HISTORY} ?\" 3.\"Predict the item that the user with this history: {USER HISTORY} might like next.\" 4.\"Considering the purchasing history: {USER HISTORY} , what will be the next item the user click on?\" 5.\"Based on the buying history {USER HISTORY} , what item is the user likely to click on next?\" 6.\"With the given purchase records {USER HISTORY} , can you determine the next item the user will click?\" 7.\"What item is expected to be clicked next by a user who has this purchase history: {USER HISTORY} ?\" 8.\"Generate the next probable clicked item for a user with the purchase history: {USER HISTORY} .\" 9.\"For a user with the following purchase background: {USER HISTORY} , which item will he most likely click next?\"\n\nTask Name: Item Ranking Assistant Response:\"{SORTED ITEM TITLES}\" User Question: 1.\"Given the user history: {USER HISTORY} and next items to be ranked: {ITEMS} , generate the sorted item titles from the user\u2019s favorite to least favorite.\" 2.\"Considering user: {USER HISTORY} and some items he might like next: {ITEMS} , provide a ranking list of them according to the user preference.\" 3.\"Please rank the following items: {ITEMS} from what the user likes to dislikes. Here is the user history: {USER HISTORY} .\" 4.\"For user with purchase history: {USER HISTORY} , please arrange these items in order of preference: {ITEMS} .\" 5.\"Taking into account user\u2019s history: {USER HISTORY} , create a list of the items: {ITEMS} ranked by the user\u2019s interests.\" 6.\"With the user\u2019s purchase history given: {USER HISTORY} , sort the items: {ITEMS} based on the user\u2019s taste from best to worst.\" 7.\"Based on the purchase history: {USER HISTORY} , please provide a ranking of the following items: {ITEMS} according to the user\u2019s preferences.\" 8.\"Given user\u2019s past history: {USER HISTORY} , rank these items: {ITEMS} from most to least appealing.\" 9.\"Using the provided user purchase history: {USER HISTORY} , generate a ranked list of items: {ITEMS} in accordance with the user\u2019s likes and dislikes.\"\n\nTask Name: Interest classification Assistant Response:\"{YES/NO}\" User Question: 1.\"The user has the following purchase history: {USER HISTORY} . Will the user like the item: {ITEM} ?\" 2.\"Considering user: {USER HISTORY} and item: {ITEM} , will the user like the item?\" 3.\"Here is the user history: {USER HISTORY} . Do you think the user will prefer the item: {ITEM} ?\" 4.\"User\u2019s purchase records are: {USER HISTORY} . Can you tell if the user will enjoy item: {ITEM} ?\" 5.\"Given the purchase background of the user: {USER HISTORY} , would the user appreciate the item: {ITEM} ?\" 6.\"The buyer has this purchase history: {USER HISTORY} . Would the user be interested in the product: {ITEM} ?\" 7.\"With the following purchasing history for the user: {USER HISTORY} , can we predict if the user will like item: {ITEM} ?\" 8.\"Here\u2019s the customer\u2019s buying log: {USER HISTORY} . Would you say the user might favor the item: {ITEM} ?\"\nTask Name: Item discrimination Assistant Response:\"{TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}\" User Question: 1.\"What is the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} ?\" 2. Given the item: {ITEM} , generate its {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}.\" 3. For the item: {ITEM} , what is its {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}?\" 4.\"Can you tell me the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} ?\" 5.\"Please generate the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} .\" 6. {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} ?\" 7.\"Item: {ITEM} , what is its {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}?\" 8.\"Could you generate the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} for the item: {ITEM} ?\"\nTask Name: History reconstruction\n\n# \"The user has the following purchase history: {USER HISTORY} . Will the user like the item: {ITEM} ?\"\"Considering user: {USER HISTORY} and item: {ITEM} , will the user like the item?\"\n\nFigure B2: Prompt templates for data generation.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Recommender systems are essential in online services, particularly embedding-based models, which are effective in representing complex signals. However, their black-box nature limits transparency and reliability. Recent advancements in large language models (LLMs) present new opportunities for enhancing explainability in these systems. This paper introduces methods to leverage LLMs as surrogate models for explaining black-box recommender models, aiming to improve model transparency and user trust.",
        "problem": {
            "definition": "The paper addresses the challenge of explainability in recommender systems, where existing models often operate as black boxes, making it difficult to understand their decision-making processes.",
            "key obstacle": "The main difficulty lies in the lack of transparency and interpretability in current embedding-based models, which limits users' and developers' ability to understand and trust the recommendations made."
        },
        "idea": {
            "intuition": "The inspiration comes from the observed capabilities of LLMs in understanding and generating human-readable explanations, suggesting they can serve as effective surrogates for explaining complex models.",
            "opinion": "The proposed idea involves training LLMs to comprehend and emulate the behavior of target recommender models, thereby generating explanations that are both high-quality and tailored to user preferences.",
            "innovation": "The innovation lies in the introduction of three distinct alignment methods (behavior alignment, intention alignment, and hybrid alignment) that enhance the LLM's ability to explain the underlying logic of recommender models while maintaining fidelity to their predictive patterns."
        },
        "method": {
            "method name": "RecExplainer",
            "method abbreviation": "RecExplainer",
            "method definition": "RecExplainer is a framework that aligns large language models with recommender systems to produce human-readable explanations for recommendations.",
            "method description": "The method combines behavior, intention, and hybrid alignment approaches to enable LLMs to effectively mimic and explain the decision-making logic of recommender models.",
            "method steps": [
                "Behavior alignment: Fine-tune LLMs to predict items based on user history and mimic the target model's behavior.",
                "Intention alignment: Incorporate user and item embeddings into the LLM's prompts to enhance understanding of the model's decision logic.",
                "Hybrid alignment: Combine both textual and embedding inputs to improve the robustness of explanations."
            ],
            "principle": "The effectiveness of this method stems from the LLM's ability to leverage its extensive knowledge and reasoning capabilities to generate coherent and contextually relevant explanations based on the alignment with the recommender model."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three public datasets, including Video Games and Movies & TV from Amazon and Steam, using various metrics to assess the performance of the RecExplainer methods in alignment and explanation generation tasks.",
            "evaluation method": "The evaluation involved a leave-one-out strategy to test the LLM's alignment with the recommender model, measuring performance across tasks such as next item retrieval, item ranking, interest classification, and history reconstruction."
        },
        "conclusion": "The results demonstrate that LLMs can be effectively aligned to reflect the behavior of recommender models, yielding high-quality explanations. The proposed alignment methods significantly enhance the explainability of recommender systems, paving the way for more transparent AI applications.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved explainability, the ability to generate user-specific explanations, and higher fidelity in reflecting the decision-making logic of recommender models compared to traditional methods.",
            "limitation": "A limitation of the method is its reliance on the quality of the embeddings and the potential for information loss during the alignment process, which may affect the accuracy of generated explanations.",
            "future work": "Future research could explore further enhancements in the alignment techniques, the integration of additional modalities for richer explanations, and the application of the framework to other complex AI systems."
        },
        "other info": {
            "acknowledgments": "The work was supported by grants from the National Key R&D Program of China and the National Natural Science Foundation of China.",
            "keywords": [
                "Large Language Models",
                "Recommender Systems",
                "Model Explainability"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommender systems are essential in online services, particularly embedding-based models, which are effective in representing complex signals."
        },
        {
            "section number": "1.2",
            "key information": "Recent advancements in large language models (LLMs) present new opportunities for enhancing explainability in these systems."
        },
        {
            "section number": "2.1",
            "key information": "The paper addresses the challenge of explainability in recommender systems, where existing models often operate as black boxes."
        },
        {
            "section number": "4.1",
            "key information": "The proposed idea involves training LLMs to comprehend and emulate the behavior of target recommender models, thereby generating explanations that are both high-quality and tailored to user preferences."
        },
        {
            "section number": "4.2",
            "key information": "RecExplainer is a framework that aligns large language models with recommender systems to produce human-readable explanations for recommendations."
        },
        {
            "section number": "6.1",
            "key information": "The main difficulty lies in the lack of transparency and interpretability in current embedding-based models, which limits users' and developers' ability to understand and trust the recommendations made."
        },
        {
            "section number": "10.1",
            "key information": "A limitation of the method is its reliance on the quality of the embeddings and the potential for information loss during the alignment process."
        },
        {
            "section number": "11",
            "key information": "The results demonstrate that LLMs can be effectively aligned to reflect the behavior of recommender models, yielding high-quality explanations."
        }
    ],
    "similarity_score": 0.7786698261784757,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e8c4/e8c47128-b68e-4229-b7c6-3334a041fc78.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3781/3781c5e5-f9cc-4ec0-9dd5-858a4ebbe114.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec24/ec24fa43-88e4-44e4-ae9c-313be8a6ef7e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd1f/cd1ff5d6-ab70-4224-8057-116ca35d5f29.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af64/af640462-d4f3-4479-a9a5-42faf9497dcc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ea7/5ea7c29c-9da9-4123-a270-1d4cbe715263.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afbb/afbbb0a2-e668-442b-98ab-4e98178563f9.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Recexplainer_ Aligning large language models for recommendation model interpretability.json"
}