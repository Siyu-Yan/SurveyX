{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.18358",
    "title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
    "abstract": "The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.",
    "bib_name": "song2023communicationtheoryperspectiveprompting",
    "md_text": "# A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models\n1AI Group, WeBank Co., Ltd, China\nE-mail: {yfsong, yuanqinhe, summerzhao, allengu, dijiang, navyyang, lixinfan, qiangyang}@webank.com;\nmmerzhao, allengu, dijiang, navyyang, lixinfan, qiangyang}@weban\nAbstract The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods. Keywords Prompting Methods, Large Language Models\n# 1 Introduction\nLarge Language Models (LLMs) (e.g., GPT-3 [1], GPT-4 [2], LLaMa [3]) make it possible for machines to understand users\u2019 attention accurately, thus revolutionizing the human-computer interaction (HCI) paradigm. Compared to traditional machine systems like databases and search engines, LLMs demonstrate impressive capability in understanding, generating, and processing natural language, facilitating a series of services ranging from personal assistants [4], healthcare [5] to e-commercial tools [6] via a unified natural language interface between users and machine. The research paradigm around LLM has shifted from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning approach. Along this line of research endeavors, LLM-based prompting engineering (PE) methods [7, 1] have attracted much attention, partially be-\ncause they are the key techniques in making full use of the superior capabilities of LLMs via constructing appropriate prompts. PE refers to the process of crafting effective instructions to guide the behavior of LLMs, and it greatly helps in bridging the gap between the pre-training tasks used to construct the LLM with the down-streaming tasks queried by the end users. Through careful prompt designing, users can steer LLM\u2019s output in the desired direction, shaping its style, tone, and content to align with their goals. To this end, numerous prompt engineering (PE) methods have been explored with the notable progress of LLM advancement and technologies [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]. A common theme of PE development lies in continuously improving accuracy and responsiveness of designed prompts, which often include components like Role, Context, Input, Output Format, and Examples (see Fig. 1). Specifically, prompt template and answer-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8eef/8eef34fd-ccdd-4516-b065-6717828c1838.png\" style=\"width: 50%;\"></div>\nA typical prompt usually has: \u2022 Role: Define the identify that the LLM is emulating, like a  doctor or a customer service agent.  \u2022 Context: Describe the situation and relevant facts to frame  the task or question for the LLM. Providing background  context helps guide the response. \u2022 Input: Clearly explain the task or information being requested  of the LLM. Concise, direct prompts work best. \u2022 Output Format: Indicate the desired output format and  specify the type of output expected focuses the LLM. Typical  output includes a conversational response, a summary, or a  series of instructional steps, etc.  \u2022 Examples (optional): Illustrative examples further clarify the  appropriate response style and content. This \"few-shot  learning\" helps steer the capabilities of the LLMs.\nFig.1. Components of A Good Prompt.\ning engineering has evolved from solely utilizing discrete prompts to continuous prompts, and even to exploring hybrid prompts that combine continuous and discrete elements, which provides a larger optimization space to achieve better performance. With the emergent capability of LLM, LLMs are leveraged to plan and use external tools via its in-context learning capability, which significantly enhanced its ability in specialized domains and broadened its application fields. Following these studies, one can summarize representative PE methods in a chronological overview as illustrated in Fig. 2. These methods can be categorized as three groups that respectively correspond to three prompting tasks proposed to improve the qualities of LLMs\u2019 outputs, namely prompt template engineering, prompt answer engineering, and multi-turn prompting and multi-prompt learning. An example of the input and output for the above-mentioned tasks can be found in Table 1.\n\u2022 First, prompt template engineering methods aim to carefully design a piece of \u201ctext\u201d that guides\nthe language models to produce the desired outputs. For example, in Table 1, to finish a classical sentiment detection for a input A=\u201cGreat places to eat near my location!\u201d, the prompt template engineering designs a template \u201c[A] Overall, it was a [Z] restaurant\u201d to enforce the LLM to fill the desired comments in the blank i.e. [Z]. Essentially this type of template engineering method induces LLM to focus on word embeddings that are relevant to the questions. A common designing principle of existing prompt template engineering methods is to better align information between users and LLMs. Such a trend is manifested by the evolution from using discrete prompts (e.g., a piece of human-readable text) [11, 9] to continuous ones (e.g., a continuous task-specific vector) [13, 20].\n\u2022 Second, prompt answer engineering [7] refers to the process of searching for an answer space and a map to the original output, which enhances users\u2019 understanding of the information encapsulated\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a568/a568f809-dfb4-40b2-a8a3-1e2660365cc8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Chronological overview of representative studies in prompting methods from four aspects: prompt template engineerin [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], prompt answering engineering [25, 9, 26, 11, 10, 12, 27, 17, 28, 29, 30 and multi-turn prompting and multi-prompt learning [10, 31, 17, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41].</div>\nwithin the LLM. For the same example in Table 1, the prompt answer engineering aims to find a mapping from the result \u201cgood\u201d obtained from the LLM to the desired answer \u201cpositive\u201d. The field of prompt answer engineering is currently witnessing a notable development trend characterized by the pursuit of models that excel in decoding model information from simple mapping to complex mapping to enhance human comprehension.  Third, multi-prompting methods mainly applied ensemble techniques [10] to mitigate the sensitivity of LLM to different formulations and to obtain a more stable output. In Table 1, the multiprompting methods combine three different templates (i.e., 1. \u201cIt was a [Z]\u201d; 2. \u201cJust [Z]\u201d; 3. \u201cAll in all, it was [Z]\u201d;) and their inference results (i.e., 1. \u201cgood\u201d 2. \u201cgreat!\u201d 3. \u201cokay\u201d) to obtain\nthe final desired one (i.e., \u201cpositive\u201d). Later, as LLMs become more capable, multi-turn prompt methods attract more attention that aims to provide more context to LLM by leveraging information either from LLM itself or external tools [37, 33]. In the field of multi-prompting methods, researchers are endeavoring to develop adaptive strategies that enhance LLM\u2019s ability to task planning and the utilization of tools.\nthe final desired one (i.e., \u201cpositive\u201d). Later, as LLMs become more capable, multi-turn prompt methods attract more attention that aims to provide more context to LLM by leveraging information either from LLM itself or external tools [37, 33]. In the field of multi-prompting methods, researchers are endeavoring to develop adaptive strategies that enhance LLM\u2019s ability to task planning and the utilization of tools. In this article, we summarize the prompting meths from a communication theory perspective with ich the ultimate goal of PE is to reduce the informan misunderstanding between the users and the LLMs. erefore, as delineated in Section 2, the communican theory perspective provides a coherent explanation different PE methods in terms of their objectives and derlying principles. Moreover, this novel perspective o offers and presents insights into scenarios where\nIn this article, we summarize the prompting methods from a communication theory perspective with which the ultimate goal of PE is to reduce the information misunderstanding between the users and the LLMs. Therefore, as delineated in Section 2, the communication theory perspective provides a coherent explanation of different PE methods in terms of their objectives and underlying principles. Moreover, this novel perspective also offers and presents insights into scenarios where\n<div style=\"text-align: center;\">Table 1. Running Examples for PE Methods</div>\nStage\nInput\nOutput\nPrompt Template\nEngineering\nGreat places to eat near my location!\nGreat places to eat near my location!\nOverall, it was a [Z] restaurant.\nLarge Language\nModel\nGreat places to eat near my location!\nOverall, it was a [Z] restaurant.\nGreat places to eat near my location!\nOverall, it was a good restaurant.\nPrompt Answering\nEngineering\ngood\npositive\nMulti-Prompt\n1. It was a [Z]; 2. Just [Z]; 3. All in all,\nit was [Z];\n1. good 2. great! 3. okay\nexisting prompting methods come short. The remainder of the article is structured as follows: Section 2 details the overview of the prompting methods from the communication theory perspective. Sections 3, 4, and 5 review and summarize the recent progresses, respectively, from four PE tasks namely prompt template engineering, answer engineering, and multi-turn prompting methods. Section 6 discusses other related surveys and potential research directions. Finally, we conclude this article in Section 7 by summarizing significant findings and discussing potential research directions.\n# 2 A Communication Theory Perspective of Prompting Methods\nThe study of modern communication theory, which dates back to the 1940s and the following decades, gave rise to a variety of communication models including both linear transmission models and non-linear models such as interaction, transaction, and convergence models [42, 43, 44]. A common theme of these early studies is to analyze how individuals utilize verbal and nonverbal interactions to develop meaning in diverse circumstances. Conceptually, the communication process is often modeled as a chain of information processing steps involving encoding, transmitting, and decoding of messages, between a sender and a receiver. To give a better illustration, Fig. 3a depicts the classical Model of Communication in communication the-\nory, which includes a sender encoding a message and transmitting it to the receiver over a channel. Then, the receiver decodes the message and delivers some type of response. During the transmission process, the message may be distorted due to noise, leading to the necessity of multi-turn interaction. The original communication theory is widely utilized to examine factors including social [45], cultural [46], and psychological [47] that influence human communication. The overall goal of communication theory is to reveal and clarify the common human experience of interacting with others through information exchange. Among early studies of various communication models, we are particularly inspired by two influential works, namely, Shannon-Weaver\u2019s mathematical model of communication [48] and Schramm\u2019s communication model [49]. Shannon-Weaver\u2019s pioneering work, first published in 1948, provides a strong mathematical foundation to analyze information flow between an active sender and a passive receiver. It is however oversimplistic in the sense that it does not take into account of complexities involved in interactive communication between active senders and receivers, who may respond by sending their message as a form of feedback. The interaction models of communication were first studied by Scharmm and published in his 1954 book [49], which pictorially illustrated the feedback loop as depicted in Fig. 3a. Nevertheless, Scharmm\u2019s model falls\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f9a/9f9a7a94-77fb-4428-82b8-966fa766d835.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) The classical Interaction Model of Communication.</div>\n<div style=\"text-align: center;\">Fig.3. Prompting methods from the communication theory perspective</div>\nshort of rigorous theoretical and mathematical formulation to accommodate quantitative analysis e.g. information gain or mutual information between senders and receivers. Various prompting engineering methods for LLM, in our view, can be understood from Scharmm\u2019s model point of view (see Fig. 3b). In the same vein of Shannon-Weaver\u2019s analysis, we, therefore, delineate a mathematical formulation of Prompting Engineering Systems for interactive user-LLM communication as follows: Definition 1. A Prompt Engineering System (PES) consists of a processing chain\n# Definition 1. A Prompt Engineering System (PES) consists of a processing chain\nDefinition 1. A Prompt Engineering System (PES) consists of a processing chain\n(1)\nwhere g\u03c9T represents the mapping from the input X to the prompt PT , f\u03b8 denotes the mapping from the prompt PT to the answer PA and h\u03c9A denotes the mapping from the answer PA to the output Y (see Fig. 3b for an illustration). Definition 2 (Goal of PES). PES aims to maximize the mutual information between the inputs X and outputs Y , i.e., max \u03c9T ,\u03c9A I(X, Y ) = max \u03c9T ,\u03c9A I(X, h\u03c9A \u25e6f\u03b8 \u25e6g\u03c9T (X)) (2) where f \u25e6g(x) = f(g(x)).\nmax \u03c9T ,\u03c9A I(X, Y ) = max \u03c9T ,\u03c9A I(X, h\u03c9A \u25e6f\u03b8 \u25e6g\u03c9T (X)) where f \u25e6g(x) = f(g(x)).\n(2)\n<div style=\"text-align: center;\">(b) Different Aspects of Existing Prompting Methods.</div>\nIt\u2019s worth noting that prompt engineering is consistently divided into two procedures: Prompt Template Engineering and Prompt Answer Engineering. Each procedure has specific goals similar to Eq. (2) that align with its intended purpose. While the capacity in Def. 2 is well-known in information theory [50], how to reach the maximum of Eq. (2) for large language models illustrated in Fig. 3b remains an unexplored research direction. There exists a large variety of prompting engineering methods, which, in our view, essentially aim to reduce information misunderstanding between users and LLMs. In other words, they aim to reach the capacity of PES as defined. This connection between PES and the communication models has never been explicitly stated before. Moreover, the existing work can be divided into three categories: prompt template engineering (X g\u03c9T \u2212\u2192 PT ), prompt answer engineering (PA h\u03c9A \u2212\u2192Y ), and multi-prompt and multi-tune prompting as shown in Fig. 3b. Specifically, the prompt template engineering aims to reduce the encoding error/ look for the prompt that is easily understood by the machine, while the prompt answering engineering aims to reduce the decoding error/ look for the prompt that easily understood by the human. The development of LLMs aims to enhance the capability of the receiver that could better\nhandle users\u2019 information needs, and most importantly, the multi-turn prompting and multi-prompt engineering aim to constantly reduce the information misunderstanding via multi-turn interactions.\n# \u2022 Prompt template engineering aims to optimize\nmax \u03c9T I(X, PA) = max \u03c9T I(X, f\u03b8 \u25e6g\u03c9T (X)),\n(3)\n\u03c9T A \u03c9T \u03b8 \u25e6\u03c9T  which looks for an additional piece of text, namely a prompt, to steer the LLMs to produce the desired outputs for downstream tasks. From the communication theory perspective, it acts as an \u201cencoder\u201d to bridge the gap between the users and the LLMs by encoding the messages in a way that the model can understand and then elicit knowledge from LLMs (see details in Section 3). In the encoding process, the challenge lies in the accurate understanding of the user\u2019s intention by LLM with limited instruction following capability. Template engineering aims to reduce this mismatch by translating the user\u2019s request to a format that could be better understood by LLM.\n# \u2022 Prompt answer engineering aims to optimize\nmax \u03c9A I(PT , Y ) = max \u03c9A I(PT , h\u03c9A \u25e6f\u03b8(PT )), (\n    \u25e6 which focuses on developing appropriate inputs for prompting methods, has two goals: 1) search for a prompt answer PA; 2) look for a map to the target output Y that will result in an accurate predictive model. In the decoding process, LLMgenerated output often carries redundant information in addition to the expected answer due to its unlimited output space. Answer engineering aims to confine the output space and extract the target answer. The field of prompt answer engineering is currently witnessing a notable develop-\nment trend characterized by the pursuit of effective answer engineering such that ultimate outputs (i.e. Y ) are well aligned with that of end users\u2019 expectations (see details in Section 4) \u2022 To further reduce the information misunderstanding, the user could conduct multi-interaction according to Eq. (3) and Eq. (4), called multi-prompt/multi-turn PE. Multiprompting methods aims to optimize\nment trend characterized by the pursuit of effective answer engineering such that ultimate outputs (i.e. Y ) are well aligned with that of end users\u2019 expectations (see details in Section 4)\n\u2022 To further reduce the information misunderstanding, the user could conduct multi-interaction according to Eq. (3) and Eq. (4), called multi-prompt/multi-turn PE. Multiprompting methods aims to optimize\n(4),\n(5)\nwhich mainly applied ensemble techniques [10] to mitigate the sensitivity of LLM to different formulations and to obtain a more stable output. Later, as LLMs become more capable, multiturn prompt methods focus to provide more context to LLM by leveraging multiple communication procedures between the machine and person [37, 33]. In the field of multi-prompting methods, researchers are endeavoring to develop adaptive strategies that enhance LLM\u2019s ability to task planning and the utilization of tools. The adaptive and iterative nature of multi-prompting methods is by the communication theory (see Section 5 for an elaborated explanation).\n[10]\n# 3 Prompt Template Engineering\nGiven the information chain X \u2192PT \u2192PA, the answer PA is determined by the prompt-processed PT and model M with pre-trained weights \u03b8. Suppose that \u00af PA is the targeted prediction, the key problem of prompt template engineering is to find a good prompt that maximizes the probability p( \u00af PA|M, PT , \u03b8) on diverse downstream tasks with limited data. To obtain the optimal prompt, current works [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24] can be formulated into\nthree categories: constructing PT , ranking PT and tuning PT , as shown in Fig. 4.\n# 3.1 Constructing PT\nThe basic motivation of constructing PT is to transform the specific task to make it align with the pretraining objective (i.e., next-word prediction, masked LM) of the LM. As shown in Table 2, existing prompt constructing methods [9, 10, 10, 51, 9, 8, 15, 52, 53, 53, 11] could be categorized into five different approaches, which are discussed in detail as follows.\n# 3.1.1 Manually-designed\nInitially, the prompt templates are manually designed in natural language based on the user\u2019s experience, and they have been validated to be able to improve the performances of downstream tasks, especially in a zero-shot setting [1, 8]. The most frequent style is to reformulate the original task as a \u2019fill-in-blank\u2019 cloze one [9, 10], and the answer is obtained by predicting the words in the given [mask] place. For example, as illustrated in Fig. 4 and Table 2, Petroni et al. [9] manually designed prompts to re-structure the relational knowledge, while studies like [10, 51] are dedicated to solving the text classification and language understanding tasks by several self-defining prompt patterns and propose a new training procedure named PET. Another line of work involves developing prefix prompts for generation tasks, which provide instructions and steer the LLMs to finish the sentence. For example, a summarization task can be handled by adding \u2018TL;DR:\u2019 [8], and a translation task can be conducted into \u2019Eng. Translate to Spanish: Span\u2019 [52]. Even though manually designed prompts show some effectiveness [53], they are also criticized for being time-consuming and unstable [15]. A subtle difference in the designed prompts may result in a substantial performance decrease. As such, how to explore the prompt space and construct prompts more\nthoroughly and more effectively becomes an important and challenging issue.\n# 3.1.2 Heuristic-based\nThe heuristic-based methods focus on finding acceptable prompts by some intuitive strategies. For example, to construct more flexible and diverse prompts for different examples (rather than the fixed ones), Jiang et al. [11] propose to use the most frequent middle words and the phrase spanning in the shortest dependency path that appeared in the training data as a prompt. This method shows a large performance gain compared to the manually-designed prompts. Han et al. [19] tries to form task-specific prompts by combining simple human-picked sub-prompts according to some logic rules. Different from the above methods, Logan et al. [54] uses an extremely simple uniform rule by null prompts, which only concatenates the inputs and the [mask] token, and it\u2019s able to gain a comparable accuracy with manually-defined prompts.\n# 3.1.3 Paraphrasing-based\nThe paraphrasing-based methods are widely used in data augmentation, aiming at generating augmented data that is semantically related to the original text, and this could be achieved in various ways using machine translation, model-based generation, and rulebased generation [58]. The paraphrasing-based methods could naturally be used to construct prompt candidates based on the original text, and we could further select the best one or integrate them to provide better performance. Representative studies includes [11, 55, 14]. Specifically, Jiang et al. [11] uses backtranslation to enhance the lexical diversity while keeping the semantic meaning. Yuan et al. [55] manually creates some seeds and finds their synonyms to narrow down the search space. Haviv et al. [14] uses a BERTbased model to act as a rewriter to obtain prompts that\n<div style=\"text-align: center;\">Ranking !</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7579/7579e23e-f878-4e68-9613-8d862da39671.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig.4. Overview of the PE Methods</div>\n<div style=\"text-align: center;\">Table 2. Summary of the prompt construction methods</div>\nMethod\nAutomated\nGradient-Free\nFew-shot\nZero-shot\nStability\nInterpret-ability\nManually-design [8, 9, 10]\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\nHeuristic-based [11, 54, 19]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nParaphrasing-based [11, 55, 14]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\nGeneration-based [17, 56]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nOptimization-based [12, 22, 57]\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nLLMs can understand better.\n# 3.1.4 Generation-based\nThe generation-based methods treat prompt searching as a generative task that can be carried out by some LMs. For example, Gao et al. [17] first make use of the generative ability of T5 [52] to fill in the placeholders as prompts, and then the prompts could be further improved by encoding domain-specific information [56].\n# 3.1.5 Optimization-based\nTo alleviate the weakness of insufficient exploration space faced by existing methods, the optimized-based methods try to generate prompts guided by some optimization signals. For example, Shin et al. [12] employs gradients as the signals, and then searches for discrete trigger words as prompts to enrich the candidate space. Deng et al. [22] generates the prompt using\na reinforced-learning approach that is directed with the reward function.\n# 3.2 Ranking PT\nAfter obtaining multiple prompt candidates with the above-mentioned methods, the next step is to rank them to select the most effective one. Existing studies solve this problem by finding prompts that are close to the training samples to reduce the information mismatch between the pre-training and inference phases.\n# 3.2.1 Execution Accuracy\nSince the objective of the designed prompts is to fulfill the downstream tasks, it\u2019s intuitive and straightforward to evaluate the performance by execution accuracy over the specific tasks [57, 17, 11].\n# 3.2.2 Log Probability\nThe log probability criterion prefers the prompt that delivers the correct output with higher probability, rather than being forced to give the exact answer. For example, a prompt template that can work well for all training examples is given the maximum generated probability in [17]. Furthermore, language models can also be utilized to evaluate the quality of prompts. In [59], the prompt with the highest probability given by an LM is selected, which indicates closer to the general expression that appears in the training dataset.\n# 3.2.3 Others\nOther criteria can be used to select the top one or the top-k prompt. For example, Shin et al. [12] regards the words that are estimated to have the largest performance improvement as the most crucial elements.\n# 3.3 Tuning PT\nDue to the continuous nature of LLMs, searching over discrete space is sub-optimal [15]. How can we further improve the performance once we obtain a prompt? Recent studies turn to optimizing the prompt as continuous embeddings. The main idea is to learn a few continuous parameters, referred to as soft prompts, and these continuous parameters can be optionally initialized by the previously obtained discrete prompt. Li et al. [13] first introduces a continuous task-specific \u2018prefix-tuning\u2019 for generative tasks. Studies like [20] and [15] adopt a similar strategy and prove its effectiveness in various natural language understanding tasks. Following the abovementioned studies, many improvements have been conducted to find better prompts, such as better optimizing strategies [16], better vector initialization [21, 23], indicative anchors [15] etc. Furthermore, studies like [60, 13, 20] further point out that prompt position,\ngth, and initialization all affect the performance of tinuous prompts [60, 13, 20] (Table 3). In this secn, we summarize these factors as follows: \u2022 Different Position. There are three different positions for autoregressive LM that the prompt can be inserted into, that is, the prefix [PREFIX; XT ; Y ], the infix [XT ; INEFIX; Y ], and the hybrid one [PREFIX; XT ; INFIX; Y ]. There is no significant performance difference between those positions. [13] shows that prefix prompt sightly outperforms infix prompt, and the hybrid one is much more flexible than the others. \u2022 Different Length. There is no optimal length for all tasks, but there is always a threshold. The performance will increase before reaching the threshold, then it will either plateau or slightly decrease. \u2022 Different Initialization. A proper initialization is essential for the performance of the prompts and the performance of random initialization is usually unsatisfactory. Typical methods include initialized by sampling real word [13, 20], using class label [20], using discrete prompt [16], and using pre-trained based vector [21, 23]. Furthermore, the manually designed prompts can provide a good starting point for the following search process.\n# 3.4 Trends for Prompt Template Engineering\nThere are two trends in prompt template engineering: \u2022 Tend to have less human involvement, using automated methods rather than designing manually when constructing prompts. \u2022 Tend to develop optimization-based techniques. The gradient-based searching method shows better performance than the derivative-free one in\n<div style=\"text-align: center;\">Table 3. Summary of the prompt tuning methods</div>\nWork\nPosition\nLength\nInitialization\nprefix tuning [13]\nprefix, infix\n200 (summarization), 10 (table-to-text)\nrandom, real words\nprompt tuning [20]\nprefix\n1, 5, 20, 100, 150\nrandom, sampled vocab, class label\np-tuning [15]\nhybrid\n3 (prefix), 3 (infix)\nLSTM-trained\nDART [18]\ninfix\n3\nunused token in vocabulary\nOPTIPrompt [16]\ninfix\n5, 10\nmanual prompt\ndynamic [60]\nhybrid, dynamic\ndynamic\nsampled vocab\nhard prompt construction while the soft prompt is more promising than the hard one.\nFrom the communication theory perspective, the development history of prompting template engineering reflects the trends of utilizing prompts with stronger expressive ability to better capture the user\u2019s intent.\n# 4 Prompt Answering Engineering\nAs illustrated in Fig. 3(b), prompt answer engineering (PAE) aims to align LLMs outputs with the intended purpose. The use of PAE is motivated by the need to mitigate the gap between the capabilities of pretrained LLMs and a large variety of requirements of different downstream tasks (see more discussion in Sect. 2). Technology-wise, PAE involves a set of methods that control admissible answer space and optimization mechanisms of LLMs\u2019 output (see overview in Table 4).\n# 4.1 Search for an Answer Space\n# 4.1.1 Pre-defined Answer Space\nThis involves a set of pre-defined answers for the question-answering task, e.g., pre-defined emotions (\u201chappiness\u201d, \u201csurprise\u201d, \u201cshame\u201d, \u201canger\u201d, etc.) for the sentiment classification task. The model can then be trained to select the best answer from this predefined space. As an illustration, the answer space PA can be defined as the set of all tokens [9], fixed-length spans [65], or token sequences [8]. Furthermore, in certain tasks like text classification, question answering,\nor entity recognition, answers are crafted manually as word lists that pertain to relevant topics [25, 27].\n# 4.1.2 Discrete Answer Space\nDiscrete answer space refers to a set of specific and distinct answer options that a language model can choose from when generating a response to a given prompt. Specifically, the possible answers are limited to a fixed set of choices, such as a small number of named entities or keyphrases (e.g., the total choice of planet in the solar system is eight). The model can then be trained to identify whether the correct answer is among this set of possibilities [63, 61, 12].\n# 4.1.3 Continuous Answer Space\nContinuous answer space refers to a scenario where the possible answers or responses are not restricted to a predefined set of discrete options. Instead, the answers can take on a range of continuous values or be any text, number, or value within a broader, unbounded spectrum [28, 66]. The model can then be trained to predict a point in this continuous space that corresponds to the correct answer.\n# 4.1.4 Hybrid Approach\nThis involves combining multiple methods to design the answer space, such as using a pre-defined list of entities for certain types of questions, but allowing for free-form text answers for other types of questions [67].\n<div style=\"text-align: center;\">Table 4. Summary for the prompt answer engineering methods</div>\nAnswer Space Type\nAnswer Mapping Method\nWork\nTask Type\nOptimizing the mapping\nDiscrete Answer Space\n[26, 61, 12, 62]\nClassification & regression\nContinuous Answer Space\n[28]\nClassification\nBroadening the output\ndiscrete Answer Space\n[63]\nGeneration\nDecomposing the output\ndiscrete Answer Space\n[64]\nClassification\nManually Mapping\nPre-defined answer\n[9, 25, 27]\nGeneration\nRemark 1. Answer shapes summarized as follows are also needed in prompt answer engineering. In practice, the choice of answer shape depends on the desired outcome of the task. \u2022 Tokens: individual tokens within the vocabulary of a pre-trained Language Model (LLM), or a subset of the vocabulary.. \u2022 Span: short sequences of multiple tokens, often comprising a phrase or segment of text. \u2022 Sentence: A longer segment of text that can encompass one or more complete sentences.\n\u2022 Sentence: A longer segment of text that can encompass one or more complete sentences.\n# 4.2 Search for an Answer Mapping\nThere are several strategies to search for an answer mapping.\n# 4.2.1 Manually Mapping\nIn many cases, the mapping from potential answers space PA to output Y is obvious such that this mapping can be done manually. For instance, the answer is output itself for the translation task [9] such that the mapping is identity mapping; In addition, Yin et al [25] designed related topics (\u201chealth\u201d, \u201cfood\u201d, \u201cfinance\u201d, \u201csports\u201d, etc.), situations (\u201cshelter\u201d, \u201cwater\u201d, \u201cmedical assistance\u201d, etc.), or other possible labels. Cui et al. [27] manually proposed some entity tags such as \u201dorganization\u201d, \u201dperson\u201d and \u201dlocation\u201d, etc. for the Named Entity Recognition problem.\n# 4.2.2 Broadening the answer PA\nBroadening PA (P \u2032 A = B(PA)) is expanding the answer space to obtain a more accurate mapping. Jiang et al. [63] proposed a method to paraphrase the answer space PA by transferring the original prompt into other similar expressions. In their approach, they employed a back-translation technique by first translating prompts into another language and then translating them back, resulting in a set of diverse paraphrased answers. The probability of the final output can be expressed as P(Y |x) = \ufffd y\u2208B(PA) P(y|x), where B(Y ) represents the set of possible paraphrased answers.\n# 4.2.3 Decomposing the output\nDecomposing Y (D(Y )) aims to expand the information of Y , which makes it easier to look for a mapping g\u03b8. For example, Chen et al. [64] decomposed the labels into several words and regarded them as the answer. Concretely, they decomposed label/output \u201cper:city of death\u201d into three separated words {person, city, death}. The probability of final output can be written as P(y|x) = \ufffd y\u2208D(Y ) P(y|x).\n#  \ufffd 4.2.4 Optimizing the mapping.\nThere exist two approaches to optimize the mapping function. The first approach is to generate the pruned space \u02dc PA and search for a set of answers within this pruned space. Schick et al. [26, 61] introduced a technique for generating a mapping from each label to a singular token that represents its semantic meaning. This mapping, referred to as a verbalizer v, is designed\nto identify sets of answers. Their approach involves estimating a verbalizer v by maximizing the likelihood w.r.t. the training data conditioned on the verbalizer v. Shin et al. [12] proposed an alternative approach for selecting the answer tokens. They employed logistic classifiers to identify the top-k tokens that yield the highest probability score, which together form the selected answer. In addition, Gao et al. [62] constructed a pruned set \u02dc PA c containing the top-k vocabulary words based on their conditional likelihood for each class c. As for the second approach, it investigates the potential of utilizing soft answer tokens that can be optimized through gradient descent. Hambardzumyan et al. [28] allocated a virtual token to represent each class label and optimized the token embedding for each class along with the prompt token embedding using gradient descent.\n# 4.3 Trends for Prompt Answer Engineering\nThere are two trends in prompt answer engineering: \u2022 Developing more robust and generalizable question-answering models that can handle more complex tasks and a broader range of inputs. For example, the answer space is some discrete spans at the beginning (see Sect. 6) and developed to the complex continuous space (see Sect. 4.1.3). \u2022 There is also a focus on improving the quality and relevance of prompts to improve model performance. Specifically, several techniques have been explored, such as paraphrasing and pruning, after the direct mapping approach. More recently, optimization methods using gradient descent have been proposed to enhance accuracy.\nThe prompt answering engineering also shows a trend of exploring prompts to decode the machine language with less information loss, i.e., has a better understanding of the machine.\n# 5 Multiple Prompting Methods\nMultiple prompts can be utilized to further reduce the information mismatch during the encoding and decoding process. These methods can be categorized into two main types, namely \u201cmulti-prompt engineering\u201d and \u201cmulti-turn prompt engineering\u201d, depending on the interrelationship of prompts (see Fig. 5). Multi-prompt engineering is akin to an ensemble system, whereby each response serves as a valid answer, and responses from multiple prompts are aggregated to produce a more stable outcome. This type of method can be thought to extend the use of prompts in the spatial domain. On the other hand, multi-turn PE entails a sequence of prompts, whereby subsequent prompts depend on the response generated from previous prompts or the obtaining of the final answer relies on multiple responses. Consequently, this type of method can be viewed as an extension in the temporal domain.\n# 5.1 Multi-prompt Engineering Methods\nMulti-prompt methods employ multiple prompts with similar patterns during the inference aiming to enhance information preservation. This method is closely associated with ensembling techniques [93, 94, 95]. Although the primary motivation is to exploit the complementary advantages of different prompts and reduce the expenses associated with PE, it can also be integrated with prompt-engineering techniques to further improve efficacy. From a communication theory perspective, multi-prompt engineering can be considered as sending multiple copies of the message to ensure the authentic delivery of data.\n# 5.1.1 Expanding PT\nExpanding PT aims to cover a larger semantic space around the sender\u2019s true intention, and a more stable approximation of the target output, \u00af XA, can be ob-\nTable 5. Summary of the PE methods involving multiple prompts. NLU: Natural Languag Generation.\n 5. Summary of the PE methods involving multiple prompts. NLU: Natural Language Understanding, NLG: Natural Language\nMethod\nNLU\nNLG\nReasoning\nMulti-prompt\nExpanding PT\n[11, 20, 28, 68]\n[55]\n-\nDiversifying PA\n-\n-\n[69, 70, 71, 72, 73]\nOptimizing \u03b8\n[10, 74]\n[75, 17]\n-\nMulti-turn prompt\nDecomposing PT\n-\n[31, 76, 77]\n[38, 78, 79, 80, 81, 82, 83]\nRefining PT\n-\n[84, 85]\n[32, 37, 85, 69, 86, 87]\nAugmenting PT\n-\n[39, 88]\n[40, 41, 89]\nOptimizing \u03b8\n-\n[31, 76]\n[89, 90, 91, 92]\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d2e/7d2eea2a-bf3a-4e6c-b4ad-c5f3b9bb10a2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig.5. Overview of multiple prompting methods. (a) Multi-prompt methods utilize several similar prompts to produce a more s esult. (b) Multi-turn prompt methods produce the final result by aggregating responses from a sequence of prompts.</div>\ntained by aggregating the responses. Jiang et al. [11, 20, 28] propose to combine outputs of different prompts to get the final result for classification tasks. Qin et al. [68] incorporates multi-prompt ideas with soft ppromptsand optimizes weights of each prompt together with prompt parameters. Yuan et al. [55] propose to use text generation probability as the score for text generation evaluation, and aggregate multiple results of different prompts as the final score. 5.1.2 Diversifying PA Different from expanding PT whose main goal is to leverage the input space around PT , diversifying PA aims to exploit the various \u201dthinking paths\u201d of the LLM through sampling its decoder. This is especially effective for handling complex tasks, such as mathematical and reasoning problems. Wang et al. [69] propose a self-consistency method based on the Chain-of-thoughts (CoT) which samples multiple reasoning paths and selects the most consistent answer by majority voting or weighted averaging. Lewkowycz [70] applied a similar idea to quantitative\ntained by aggregating the responses. Jiang et al. [11, 20, 28] propose to combine outputs of different prompts to get the final result for classification tasks. Qin et al. [68] incorporates multi-prompt ideas with soft ppromptsand optimizes weights of each prompt together with prompt parameters. Yuan et al. [55] propose to use text generation probability as the score for text generation evaluation, and aggregate multiple results of different prompts as the final score.\n# 5.1.2 Diversifying PA\nDifferent from expanding PT whose main goal is to leverage the input space around PT , diversifying PA aims to exploit the various \u201dthinking paths\u201d of the LLM through sampling its decoder. This is especially effective for handling complex tasks, such as mathematical and reasoning problems. Wang et al. [69] propose a self-consistency method based on the Chain-of-thoughts (CoT) which samples multiple reasoning paths and selects the most consistent answer by majority voting or weighted averaging. Lewkowycz [70] applied a similar idea to quantitative\nproblems by combining multiple prompts and output sampling. Wang et al. [71] investigated various ensemble variants in reasoning problems and found that rational sampling in the output space is more efficient. These methods solely used the final answer as the selection criterion and did not exploit the generated rationals from various sampling paths. To take advantage of these intermediate results, Li et al. [72] proposed to generate more diversified reasoning paths with multiple prompts and used a model-based verifier to select and rank these reasoning paths. Fu et al. [73] introduced a complexity-based metric to evaluate reasoning paths and prioritize those with higher complexity in the aggregation. Weng et al. [96] employed LLM to self-verify various reasonings by comparing predicted conditions using the generated reasonings to original conditions. The consistency score is then used to select the final result. Yao et al. [97] proposed the \u201dTree of Thoughts\u201d to explore the intermediate steps across various reasoning paths, and used the LLM to evaluate the quality of each possible path.\n# 5.1.3 Optimizing \u03b8\nThis line of work treats multiple prompts as a label generator to address the sample deficiency problem. Schick et al. [10] first proposes pattern-exploiting training (PET) that employs a knowledge distillation strategy to aggregate results from multiple promptverbalizer combinations (PVP). They first utilize PVP pairs to train separate models that generate pseudolabels for unlabeled datasets. This extended dataset is then used to train the final classification model. Schick et al. [98] extends this idea to the text generation task by using the generation probability of decoded text as the score. [17] uses a similar method for automatic template generation. Schick et al. [74] further expands PET with multiple verbalizers. This is achieved by introducing sample-dependent output space.\n# 5.2 Multi-turn Prompt Engineering Methods\nMulti-turn prompt engineering methods involve decomposing the full prompting task into several subtasks, each addressed by a corresponding prompt. This process typically entails a sequence of encoding and decoding operations, where subsequent prompts may depend on the decoded message from previous prompts or each prompt is responsible for a sub-task. The outcome can be obtained either from the result of the last prompt or by aggregating the responses generated by all prompts. This strategy is designed to tackle challenging tasks, such as complex mathematical questions or reasoning tasks. It mainly involves two components: 1) decomposing PT into sub-tasks to reduce the difficulty of each sub-task; and 2) modifying PT to generate better intermediate results for later steps. These two components can help to bridge the gap between complex X and Y .\n# 5.2.1 Decomposing PT\nDecomposing PT is the first step in handling complex tasks, and a proper decomposition requires a good understanding of both the target task and the user\u2019s intention. Yang et al. [99] decomposed SQL operations using fine-tuned few-shot models and untrained zero-shot models combined with predefined rules. However, ruled-based decomposition heavily relies on human experiences, so it is desirable to automate this step with LLMs. Min et al. [76] proposed an unsupervised method that utilizes a similarity-based pseudodecomposition set as a target to train a seq2seq model as a question generator. The decomposed simple question is then answered by an off-the-shelf single-hop QA model. Perez et al. [31] treats the decomposition in multi-hop reading comprehension (RC) task as a span prediction problem which only needs a few hundreds of samples. For each task, various decomposition paths are generated, with each sub-question answered by a single-hop RC model. Finally, a scorer model is used to select the top-scoring answer based on the solving path. Khot et al. [77] proposed a text modular network leveraging existing models to build a next-question generator. The training samples are obtained from sub-task models conditioned on distant supervision hints. With the emergent general ability of LLMs, instead of training a task-specific decomposition model, LLMs are used to fulfill decomposition tasks. Zhou et al. [38] proposed the least-to-most prompting method where hard tasks are first reduced to less difficult sub-tasks by LLM. Then answers from previous sub-problems are combined with the original task to facilitate subsequent question solving. Dua et al. [78] employs a similar idea and appends both question and answer from the previous stage to the subsequent prompt. Creswell et al. [79] proposed a selection-inference framework. It uses LLM to alternatively execute selecting relevant information\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/28e6/28e68212-c170-4d44-8b6e-e20cb77da3c0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4bfc/4bfc03aa-378e-4134-b833-c88e187f07cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f59/1f5940a2-d85d-49a2-974d-7e94fcab704d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig.6. Schematic illustrations of multi-prompting methods. (a) Multi-prompt methods mainly employ ensemble-based methods. ( Multi-turn prompt methods mainly leverage LLMs or external tools to provide clearer and more helpful context.</div>\nfrom a given context and inferring new facts based on the selected information. Arora et al. [80] proposed to format the intermediate steps as open-ended questionanswering tasks using LLMs. It further generates a set of prompt chains and uses weak supervision to aggregate the results. Khot et al. [81] proposed a modular approach for task decomposition with LLMs by using specialized decomposition prompts. Drozdov et al. [100] introduced a dynamic least-to-most prompting method for semantic parsing tasks by utilizing multiple prompts to build a more flexible tree-based decomposition. Ye et al. [82] uses LLMs as the decomposer for table-based reasoning tasks. LLMs are used for both sub-table extraction and question decomposition. Press et al. [101] proposed Self-Ask which decomposes the original task by repeatedly asking LLM if follow-up questions are needed. Wu et al. [83] proposed to build an interactive chaining framework with several primitive operations of LLM to provide better transparency and controllability of using LLMs.\n# 5.2.2 Refining PT\nRefining PT aims to construct a better representation of PT based on the feedback from previous prompt-\ning results. This is especially important for multi-step reasoning, where the quality of generated intermediate reasonings has a critical impact on the final answer. Following the success of the few-shot chain-ofthoughts (CoT) prompting method, Kojima et al. [37] proposed a zero-shot CoT method that utilizes the fixed prompt \u2019Let\u2019s think step by step\u2019 to generate reasonings. These intermediate results are then fused with the original question to get the final answer. To select more effective exemplars, various methods are proposed. Li et al. [84] uses LLMs to first generate a pseudo-QA pool, then a clustering method combined with similarity to the question is adopted to dynamically select QA pairs from the generated QA pool as demonstration exemplars. Shum et al. [32] leveraged a high-quality exemplar pool to obtain an exemplar distribution using a variance-reduced policy gradient estimator. Ye et al. [85] employs self-consistency method[69] to generate pseudo-labels of an unlabeled dataset. The accuracy of these silver labels serves as the selection criterion of exemplars. To further reduce the search complexity of various combinations, additional surrogate metrics are introduced to estimate the accuracy. Diao et al. [86]\naddresses this problem by using hard questions with human annotations as exemplars. The hardness is measured by the disagreement of results obtained by multiple sampling of the LLM. Zhang et al. [87] proposed automatic CoT methods. They introduced question clustering and demonstration sampling steps to automatically select the best demonstrations for the CoT template.\n# 5.2.3 Augmenting PT\nDifferent from refining PT which mainly focuses on finding prompts that generate better intermediate results, augmenting PT leverages the exploitation of external information, knowledge, tools, etc. in the prompting. We present some examples in this field below, for more details we refer the reader to the specific survey [102]. Yang et al. [39] proposed a recursive reprompting and revision (3R) framework for long story generation leveraging pre-defined outlines. In each step, the context of the current status and the outline of the story is provided to the prompt to ensure better content coherence. Yang et al. [88] proposed to use more detailed outlines so that the story generation LLM can focus more on linguistic aspects. Information retrieved from other sources is also often used to augment PT . Yao et al. [103] gives the LLM access to information from Wikipedia. Thoppilan et al. [34] taught the LLM to use search engines for knowledge retrieval. More broadly, Paranjape et al. [33] introduces a task library to enable the LLM using external tools. Schick et al. [40] trained the LLM to use various external tools via API. Shen et al. [41] utilized LLM as a central controller to coordinate other models to solve tasks.\n# 5.2.4 Optimizing \u03b8\nGeneral LMs are not optimized for producing intermediate rationals or decomposing a complex task or question. Before the era of LLMs, these tasks require\nspecifically trained LMs. Min et al. [76, 31] trained an LM model for decomposing the original task into subtasks. Nye et al. [90] trains the LLM to produce intermediate steps stored in a scratch pad for later usage. Zelikman et al. [91] utilized the intermediate outputs that lead to the correct answer as the target to finetune the LLM. Wang et al. [89] proposed an iterative prompting framework using a context-aware prompter. The prompter consists of a set of soft prompts that are prepared for the encoder and decoder of the LLMs respectively. Taylor et al. [92] employed step-by-step solutions of scientific papers in the training corpus, which enables the LM to output reasoning steps if required.\n# 5.3 Trends for Multiple Prompting Methods\nEnsemble-based methods are easy to implement and flexible to incorporate with various strategies, e.g. expanding input space and aggregating output space. However, this brings limited advantages for complex problems whose final answers are hard to obtain directly, but rely heavily on the intermediate thinking steps. Therefore, multi-turn PE methods emerged. It essentially adjusts its input dynamically during the interaction based on the knowledge and feedback from the LLM or external tools. In this way, LLM can leverage more context and understand better the true intention of the user. Initially, specialized LLM are trained to handle planning and solving specific subtasks, this not only introduces extra training effort but also constrains the generalization capability of LLM. With the increasing understanding ability and larger input length of LLM, in-context learning becomes the preferred paradigm, which utilizes embedded knowledge and the capability of LLM to handle various tasks via prompting. This paradigm soon dominated because of its efficiency and flexibility. There are two trends in multiple prompting engi-\nneering:\n# neering:\n\u2022 Developing an enhanced adaptive prompting strategy for LLM-based task decomposition is imperative. The extensive range and intricacy of tasks render human-based or rule-based task decomposition infeasible. While some studies have explored the use of LLM prompting to generate intermediate questions or actions for specific tasks, a comprehensive strategy is currently lacking. \u2022 Enabling LLM to leverage tools without the need for fine-tuning is a crucial objective. By incorporating external tools, LLMs can address their limitations in specialized domains or capabilities. Previous studies have employed fine-tuning-based approaches to train LLMs in utilizing web search or other tools accessible through APIs. From the communication theory perspective, muliple prompting methods evolved from the extension n the spatial domain (ensemble-based methods) into he temporal domain (mulit-turn), to better align the user\u2019s intention and LLM\u2019s capability by decomposing he user\u2019s request and leveraging external tools.\nFrom the communication theory perspective, multiple prompting methods evolved from the extension in the spatial domain (ensemble-based methods) into the temporal domain (mulit-turn), to better align the user\u2019s intention and LLM\u2019s capability by decomposing the user\u2019s request and leveraging external tools.\n# 6 Discussion\nResearchers have proposed several surveys to recapitulate the rapid advancements in the field of PE methods [7, 104, 105, 106, 107, 108]. To name a few, Liu et. al proposed a comprehensive survey about existing PE methods, which covers common aspects like template engineering, answering engineering, training strategies, applications, and challenges [7]. They reveal the development history of prompting learning and describe a set of mathematical notations that could summarize most of the existing studies. Furthermore, they consider prompt-based learning as a new paradigm\nthat revolves around the way we look at NLP. In another survey [104] that mainly focuses on the reasoning abilities (e.g., arithmetic, commonsense, symbolic reasoning, logical, multi-modal) of LLMs, Qiao et. al summarized the studies that harness these reasoning abilities via advanced PE methods like chain-of-though and generated knowledge prompts. Additionally, some focused surveys cover specific topics like parameterefficient fine-tuning (PEFT) LLMs using PE methods [105]. Different from the above-mentioned studies, we try to interpret existing PE methods from a communication theory perspective. Following this line of research, we also would like to discuss some potential challenges and future directions for PE methods, which could be divided into four categories including Reducing the Encoding Error, Reducing the Decoding Error, and Interactive and Multi-turn Prompting.\n# 6.1 Reducing the Encoding Error\n Better Ranking Criteria. One of the points of discrete prompts is that it\u2019s difficult to design and choose an optimal prompt, causing its instability. Although soft prompts partly addressed this problem, the discrete prompt is still very important because it has good interpretability and has been proven to be able to help soft prompts search effectively. Looking through the existing methods, we can find that accuracy-based criteria are resource-consuming, while LM-based log probability is not sufficient to evaluate the prompt. So a well-designed ranking criterion combined with a mass of auto-based generated prompts may be a good direction for the future.  Task-agnostic Prompt. Even though the prompt has been proven effective in many tasks such as classification and text generation, most of the\nexisting work has to design a specific prompt for a given task, which makes it complex and complicated [109]. So how to generate a taskagnostic prompt or transfer the prompt to other fields quickly may be a challenging problem. Discrete(meta-learning [110]) and continuous (decomposition [111]) prompts are applied to tackle this issue. However, they are not well-optimized and can serve unseen tasks.\nexisting work has to design a specific prompt for a given task, which makes it complex and complicated [109]. So how to generate a taskagnostic prompt or transfer the prompt to other fields quickly may be a challenging problem. Discrete(meta-learning [110]) and continuous (decomposition [111]) prompts are applied to tackle this issue. However, they are not well-optimized and can serve unseen tasks.  Interpretability Issue. Recent studies show that those methods learning optimal prompts in continuous space can achieve better performance than in discrete space [7]. However, the generated \u2018soft\u2019 prompts are difficult to read and understand, namely poor interpretability. Therefore, designing and improving soft prompts can be tough. Existing work [20] tries to use the nearest words in embedding space to probe the effect. However, the reasons excavated are not obvious. It remains to explore why this kind of prompt can work well and what causes the performance differences between different prompts.\n Interpretability Issue. Recent studies show that those methods learning optimal prompts in continuous space can achieve better performance than in discrete space [7]. However, the generated \u2018soft\u2019 prompts are difficult to read and understand, namely poor interpretability. Therefore, designing and improving soft prompts can be tough. Existing work [20] tries to use the nearest words in embedding space to probe the effect. However, the reasons excavated are not obvious. It remains to explore why this kind of prompt can work well and what causes the performance differences between different prompts.\n# 6.2 Reducing the Decoding Error\n\u2022 Privacy-preserving Methods [112, 113, 114]. To address privacy concerns on output, future research could focus on developing methods that preserve the privacy of the data used for training and inference. This could include techniques such as differential privacy, homomorphic encryption, and federated learning.\n Human-in-the-loop Methods [115]. To improve the accuracy and relevance of prompt answer engineering methods, future research could focus on developing methods that incorporate human feedback and interaction. This could enable users to\nprovide feedback and corrections to the generated answers and to refine the model over time.\n# 6.3 Interactive and Multi-turn Prompting\n Transparency and Explainability. Despite the recent popularity of LLMs, the lack of explainability of the outputs and transparency of the working mechanism makes LLMs less attractive in complex tasks that require high stability. The success of the chain-of-thoughts methodology shows the \u201dthinking path\u201d of LLMs can be evoked with proper indication. This property can be exploited to generate step-by-step task-solving procedures like scratch paper in exams so that the final answer can be better justified. [83] builds an interactive framework based on this idea and further involves human interaction for better controllability of the process. In addition to this online paradigm, LLMs can also be asked to explain the answer or decision afterward. [116] demonstrated that GPT-3 generates more favorable free-text explanations than crowdsourced.\n Interactive Multi-turn Prompt. Though automation in prompting methods is highly wanted, humans in the loop can bring more controllability and supervision over the process, producing more reliable results. However, frequent human intervention will diminish the efficiency gained by using LLMs. Therefore, in addition to the granularity of decomposed tasks, it is also required to determine when to involve human feedback. This could be designed manually for each task, but it would be much more efficient if LLMs could plan these stages by themselves.\n# 7 Conclusion\nThis paper tries to provide an overview of existing prompting methods from a communication theory perspective. Towards this objective, we consider LLMs as a unified interface to achieve various NLP tasks and examine these prompt-based studies to reduce the information misunderstanding that appears in the different stages between users and LLMs during their interactions. We hope this survey will inspire researchers with a new understanding of the related issues in prompting methods, therefore stimulating progress in this promising area.\n# References\n[1] Brown T, Mann B, Ryder N, Subbiah M, Kaplan J D, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A et al. Language models are few-shot learners. Advances in neural information processing systems, 2020, 33:1877\u20131901. [2] OpenAI. Gpt-4 technical report. 2023. [3] Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M A, Lacroix T, Rozi`ere B, Goyal N, Hambro E, Azhar F et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [4] Cheng K, Li Z, Li C, Xie R, Guo Q, He Y, Wu H. The potential of gpt-4 as an ai-powered virtual assistant for surgeons specialized in joint arthroplasty. Annals of Biomedical Engineering, 2023, pp. 1\u20135. [5] Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios. Journal of Medical Systems, 2023, 47(1):33.\n[6] George A S, George A H. A review of chatgpt ai\u2019s impact on several business sectors. Partners Universal International Innovation Journal, 2023, 1(1):9\u201323. [7] Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 2023, 55(9):1\u201335. [8] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I et al. Language models are unsupervised multitask learners. OpenAI blog, 2019, 1(8):9. [9] Petroni F, Rockt\u00a8aschel T, Lewis P, Bakhtin A, Wu Y, Miller A H, Riedel S. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019. [10] Schick T, Sch\u00a8utze H. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020. [11] Jiang Z, Xu F F, Araki J, Neubig G. How can we know what language models know? Transactions of the Association for Computational Linguistics, 2020, 8:423\u2013438. [12] Shin T, Razeghi Y, Logan IV R L, Wallace E, Singh S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 4222\u20134235. [13] Li X L, Liang P. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 4582\u20134597. [14] Haviv A, Berant J, Globerson A. Bertese: Learning to speak to bert. arXiv preprint arXiv:2103.05327, 2021. [15] Liu X, Zheng Y, Du Z, Ding M, Qian Y, Yang Z, Tang J. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021. [16] Zhong Z, Friedman D, Chen D. Factual probing is [mask]: Learning vs. learning to recall. arXiv preprint arXiv:2104.05240, 2021. [17] Gao T, Fisch A, Chen D. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. [18] Zhang N, Li L, Chen X, Deng S, Bi Z, Tan C, Huang F, Chen H. Differentiable prompt makes pre-trained language models better fewshot learners. arXiv preprint arXiv:2108.13161, 2021. [19] Han X, Zhao W, Ding N, Liu Z, Sun M. Ptr: Prompt tuning with rules for text classification. AI Open, 2022, 3:182\u2013192. [20] Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 3045\u20133059. [21] Gu Y, Han X, Liu Z, Huang M. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021.\n[22] Deng M, Wang J, Hsieh C P, Wang Y, Guo H, Shu T, Song M, Xing E P, Hu Z. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022. [23] Hou Y, Dong H, Wang X, Li B, Che W. Metaprompting: Learning to learn better prompts. arXiv preprint arXiv:2209.11486, 2022. [24] Wang Z, Panda R, Karlinsky L, Feris R, Sun H, Kim Y. Multitask prompt tuning enables parameter-efficient transfer learning. In The Eleventh International Conference on Learning Representations, 2023. [25] Yin W, Hay J, Roth D. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. arXiv preprint arXiv:1909.00161, 2019. [26] Schick T, Schmid H, Sch\u00a8utze H. Automatically identifying words that can serve as labels for few-shot text classification. arXiv preprint arXiv:2010.13641, 2020. [27] Cui L, Wu Y, Liu J, Yang S, Zhang Y. Templatebased named entity recognition using bart. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 1835\u2013 1845. [28] Hambardzumyan K, Khachatrian H, May J. WARP: Word-level Adversarial ReProgramming. DOI: 10.48550/arXiv.2101.00121. [29] Chen Y, Liu Y, Dong L, Wang S, Zhu C, Zeng M, Zhang Y. Adaprompt: Adaptive model training for prompt-based nlp. arXiv preprint arXiv:2202.04824, 2022. [30] Kim E, Yoon H, Lee J, Kim M. Accurate and prompt answering framework based on customer\nreviews and question-answer pairs. Expert Systems with Applications, 2022, 203:117405.\nreviews and question-answer pairs. Expert Systems with Applications, 2022, 203:117405. [31] Perez E, Lewis P, Yih W t, Cho K, Kiela D. Unsupervised Question Decomposition for Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8864\u20138880. DOI: 10.18653/v1/2020.emnlp-main.713. [32] Shum K, Diao S, Zhang T. Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. DOI: 10.48550/arXiv.2302.12822. [33] Paranjape B, Lundberg S, Singh S, Hajishirzi H, Zettlemoyer L, Ribeiro M T. ART: Automatic multi-step reasoning and tool-use for large language models. DOI: 10.48550/arXiv.2303.09014. [34] Thoppilan R, De Freitas D, Hall J, Shazeer N, Kulshreshtha A, Cheng H T, Jin A, Bos T, Baker L, Du Y et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. [35] Ahn M, Brohan A, Brown N, Chebotar Y, Cortes O, David B, Finn C, Fu C, Gopalakrishnan K, Hausman K, Herzog A, Ho D, Hsu J, Ibarz J, Ichter B, Irpan A, Jang E, Ruano R J, Jeffrey K, Jesmonth S, Joshi N J, Julian R, Kalashnikov D, Kuang Y, Lee K H, Levine S, Lu Y, Luu L, Parada C, Pastor P, Quiambao J, Rao K, Rettinghouse J, Reyes D, Sermanet P, Sievers N, Tan C, Toshev A, Vanhoucke V, Xia F, Xiao T, Xu P, Xu S, Yan M, Zeng A. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, August 2022. DOI: 10.48550/arXiv.2204.01691.\n5] Ahn M, Brohan A, Brown N, Chebotar Y, Cortes O, David B, Finn C, Fu C, Gopalakrishnan K, Hausman K, Herzog A, Ho D, Hsu J, Ibarz J, Ichter B, Irpan A, Jang E, Ruano R J, Jeffrey K, Jesmonth S, Joshi N J, Julian R, Kalashnikov D, Kuang Y, Lee K H, Levine S, Lu Y, Luu L, Parada C, Pastor P, Quiambao J, Rao K, Rettinghouse J, Reyes D, Sermanet P, Sievers N, Tan C, Toshev A, Vanhoucke V, Xia F, Xiao T, Xu P, Xu S, Yan M, Zeng A. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, August 2022. DOI: 10.48550/arXiv.2204.01691.\n[36] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A, Zhou D. Selfconsistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [37] Kojima T, Gu S S, Reid M, Matsuo Y, Iwasawa Y. Large Language Models are Zero-Shot Reasoners. DOI: 10.48550/arXiv.2205.11916.\n[36] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A, Zhou D. Selfconsistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [37] Kojima T, Gu S S, Reid M, Matsuo Y, Iwasawa Y. Large Language Models are Zero-Shot Reasoners. DOI: 10.48550/arXiv.2205.11916. [38] Zhou D, Sch\u00a8arli N, Hou L, Wei J, Scales N, Wang X, Schuurmans D, Cui C, Bousquet O, Le Q, Chi E. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. DOI: 10.48550/arXiv.2205.10625. [39] Yang K, Tian Y, Peng N, Klein D. Re3: Generating Longer Stories With Recursive Reprompting and Revision. DOI: 10.48550/arXiv.2210.06774. [40] Schick T, Dwivedi-Yu J, Dess`\u0131 R, Raileanu R, Lomeli M, Zettlemoyer L, Cancedda N, Scialom T. Toolformer: Language Models Can Teach Themselves to Use Tools, February 2023. [41] Shen Y, Song K, Tan X, Li D, Lu W, Zhuang Y. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace, April 2023. DOI: 10.48550/arXiv.2303.17580. [42] Narula U. Handbook of communication models, perspectives, strategies. Atlantic Publishers & Dist, 2006. [43] Chandler D, Munday R. A dictionary of media and communication. OUP Oxford, 2011. [44] Cobley P, Schulz P J. Theories and models of communication, volume 1. Walter de Gruyter, 2013.\n[45] Latan\u00b4e B. Dynamic social impact: The creation of culture by communication. Journal of communication, 1996, 46(4):13\u201325. [46] Orbe M P. From the standpoint (s) of traditionally muted groups: Explicating a co-cultural communication theoretical model. Communication Theory, 1998, 8(1):1\u201326. [47] Segrin C, Abramson L Y. Negative reactions to depressive behaviors: a communication theories analysis. Journal of abnormal psychology, 1994, 103(4):655. [48] Shannon C E. A mathematical theory of communication. The Bell system technical journal, 1948, 27(3):379\u2013423. [49] Schram W E. The process and effects of mass communication. 1954. [50] Cover T M. Elements of information theory. John Wiley & Sons, 1999. [51] Schick T, Sch\u00a8utze H. It\u2019s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020. [52] Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu P J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 2020, 21(1):5485\u20135551. [53] Zhou Y, Zhao Y, Shumailov I, Mullins R, Gal Y. Revisiting automated prompting: Are we actually doing better? arXiv preprint arXiv:2304.03609, 2023. [54] Logan IV R L, Bala\u02c7zevi\u00b4c I, Wallace E, Petroni F, Singh S, Riedel S. Cutting down on prompts and\nparameters: Simple few-shot learning with language models. arXiv preprint arXiv:2106.13353, 2021. [55] Yuan W, Neubig G, Liu P. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 2021, 34:27263\u201327277. [56] Ben-David E, Oved N, Reichart R. Pada: A prompt-based autoregressive approach for adaptation to unseen domains. arXiv preprint arXiv:2102.12206, 2021. [57] Zhou Y, Muresanu A I, Han Z, Paster K, Pitis S, Chan H, Ba J. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022. [58] Li B, Hou Y, Che W. Data augmentation approaches in natural language processing: A survey. AI Open, 2022, 3:71\u201390. [59] Davison J, Feldman J, Rush A M. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), 2019, pp. 1173\u20131178. [60] Yang X, Cheng W, Zhao X, Petzold L, Chen H. Dynamic prompting: A unified framework for prompt tuning. arXiv preprint arXiv:2303.02909, 2023. [61] Schick T, Sch\u00a8utze H. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. DOI: 10.48550/arXiv.2001.07676.\n[59] Davison J, Feldman J, Rush A M. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), 2019, pp. 1173\u20131178. [60] Yang X, Cheng W, Zhao X, Petzold L, Chen H. Dynamic prompting: A unified framework for prompt tuning. arXiv preprint arXiv:2303.02909, 2023. [61] Schick T, Sch\u00a8utze H. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. DOI: 10.48550/arXiv.2001.07676.\n[62] Gao T, Fisch A, Chen D. Making pre-trained language models better few-shot learners. In Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021, 2021, pp. 3816\u20133830. [63] Jiang Z, Xu F F, Araki J, Neubig G. How Can We Know What Language Models Know? DOI: 10.48550/arXiv.1911.12543. [64] Chen Y, Liu Y, Dong L, Wang S, Zhu C, Zeng M, Zhang Y. AdaPrompt: Adaptive model training for prompt-based NLP. In Findings of the Association for Computational Linguistics: EMNLP 2022, December 2022, pp. 6057\u20136068. [65] Jiang Z, Anastasopoulos A, Araki J, Ding H, Neubig G. X-factr: Multilingual factual knowledge retrieval from pretrained language models. arXiv preprint arXiv:2010.06189, 2020. [66] Nickel M, Kiela D. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In International conference on machine learning, 2018, pp. 3779\u20133788. [67] Hou Y, Che W, Lai Y, Zhou Z, Liu Y, Liu H, Liu T. Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 1381\u20131393. [68] Qin G, Eisner J. Learning How to Ask: Querying LMs with Mixtures of Soft Prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\npp. 5203\u20135212. DOI: 10.18653/v1/2021.naaclmain.410. [69] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A, Zhou D. SelfConsistency Improves Chain of Thought Reasoning in Language Models. DOI: 10.48550/arXiv.2203.11171. [70] Lewkowycz A, Andreassen A, Dohan D, Dyer E, Michalewski H, Ramasesh V, Slone A, Anil C, Schlag I, Gutman-Solo T, Wu Y, Neyshabur B, Gur-Ari G, Misra V. Solving Quantitative Reasoning Problems with Language Models, June 2022. [71] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Zhou D. Rationale-Augmented Ensembles in Language Models. DOI: 10.48550/arXiv.2207.00747. [72] Li Y, Lin Z, Zhang S, Fu Q, Chen B, Lou J G, Chen W. On the Advance of Making Language Models Better Reasoners. [73] Fu Y, Peng H, Sabharwal A, Clark P, Khot T. Complexity-Based Prompting for Multi-Step Reasoning. DOI: 10.48550/arXiv.2210.00720. [74] Schick T, Sch\u00a8utze H. It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners. DOI: 10.48550/arXiv.2009.07118. [75] Schick T, Sch\u00a8utze H. Few-Shot Text Generation with Pattern-Exploiting Training. DOI: 10.48550/arXiv.2012.11926. [76] Min S, Zhong V, Zettlemoyer L, Hajishirzi H. Multi-hop Reading Comprehension through Question Decomposition and Rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6097\u2013 6109. DOI: 10.18653/v1/P19-1613.\n[76] Min S, Zhong V, Zettlemoyer L, Hajishirzi H. Multi-hop Reading Comprehension through Question Decomposition and Rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6097\u2013 6109. DOI: 10.18653/v1/P19-1613.\n[77] Khot T, Khashabi D, Richardson K, Clark P, Sabharwal A. Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 1264\u20131279. DOI: 10.18653/v1/2021.naacl-main.99. [78] Dua D, Gupta S, Singh S, Gardner M. Successive Prompting for Decomposing Complex Questions, December 2022. DOI: 10.48550/arXiv.2212.04092. [79] Creswell A, Shanahan M, Higgins I. SelectionInference: Exploiting Large Language Models for Interpretable Logical Reasoning, May 2022. DOI: 10.48550/arXiv.2205.09712. [80] Arora S, Narayan A, Chen M F, Orr L, Guha N, Bhatia K, Chami I, Sala F, R\u00b4e C. Ask Me Anything: A simple strategy for prompting language models, November 2022. DOI: 10.48550/arXiv.2210.02441. [81] Khot T, Trivedi H, Finlayson M, Fu Y, Richardson K, Clark P, Sabharwal A. Decomposed Prompting: A Modular Approach for Solving Complex Tasks. [82] Ye Y, Hui B, Yang M, Li B, Huang F, Li Y. Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. DOI: 10.48550/arXiv.2301.13808. [83] Wu T, Terry M, Cai C J. AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. DOI: 10.48550/arXiv.2110.01691.\n[77] Khot T, Khashabi D, Richardson K, Clark P, Sabharwal A. Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 1264\u20131279. DOI: 10.18653/v1/2021.naacl-main.99.\n[78] Dua D, Gupta S, Singh S, Gardner M. Successive Prompting for Decomposing Complex Questions, December 2022. DOI: 10.48550/arXiv.2212.04092.\n[83] Wu T, Terry M, Cai C J. AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. DOI: 10.48550/arXiv.2110.01691.\n[84] Li J, Zhang Z, Zhao H. Self-Prompting Large Language Models for Open-Domain QA. DOI: 10.48550/arXiv.2212.08635. [85] Ye X, Durrett G. Explanation Selection Using Unlabeled Data for In-Context Learning. DOI: 10.48550/arXiv.2302.04813. [86] Diao S, Wang P, Lin Y, Zhang T. Active Prompting with Chain-of-Thought for Large Language Models. DOI: 10.48550/arXiv.2302.12246. [87] Zhang Z, Zhang A, Li M, Smola A. Automatic Chain of Thought Prompting in Large Language Models. DOI: 10.48550/arXiv.2210.03493. [88] Yang K, Klein D, Peng N, Tian Y. DOC: Improving Long Story Coherence With Detailed Outline Control. [89] Wang B, Deng X, Sun H. Iteratively Prompt Pretrained Language Models for Chain of Thought. DOI: 10.48550/arXiv.2203.08383. [90] Nye M, Andreassen A J, Gur-Ari G, Michalewski H, Austin J, Bieber D, Dohan D, Lewkowycz A, Bosma M, Luan D, Sutton C, Odena A. Show Your Work: Scratchpads for Intermediate Computation with Language Models, November 2021. DOI: 10.48550/arXiv.2112.00114. [91] Zelikman E, Wu Y, Mu J, Goodman N D. STaR: Bootstrapping Reasoning With Reasoning. [92] Taylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Saravia E, Poulton A, Kerkez V, Stojnic R. Galactica: A Large Language Model for Science, November 2022. [93] Ting K M, Witten I H. Stacked Generalization: When does it work?\n[94] Zhou Z H, Wu J, Tang W. Ensembling neural networks: Many could be better than all. 137(1):239\u2013263. DOI: 10.1016/S00043702(02)00190-X. [95] Duh K, Sudoh K, Wu X, Tsukada H, Nagata M. Generalized Minimum Bayes Risk System Combination. In Proceedings of 5th International Joint Conference on Natural Language Processing, pp. 1356\u20131360. [96] Weng Y, Zhu M, Xia F, Li B, He S, Liu K, Zhao J. Large Language Models are Better Reasoners with Self-Verification, May 2023. [97] Yao S, Yu D, Zhao J, Shafran I, Griffiths T L, Cao Y, Narasimhan K. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, May 2023. DOI: 10.48550/arXiv.2305.10601. [98] Schick T, Sch\u00a8utze H. Few-shot text generation with natural language instructions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 390\u2013402. [99] Yang J, Jiang H, Yin Q, Zhang D, Yin B, Yang D. SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models, May 2022. DOI: 10.48550/arXiv.2205.07381. 100] Drozdov A, Sch\u00a8arli N, Aky\u00a8urek E, Scales N, Song X, Chen X, Bousquet O, Zhou D. Compositional Semantic Parsing with Large Language Models. 101] Press O, Zhang M, Min S, Schmidt L, Smith N A, Lewis M. Measuring and Narrowing the Compositionality Gap in Language Models. DOI: 10.48550/arXiv.2210.03350. 102] Mialon G, Dess`\u0131 R, Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R, Rozi`ere B, Schick T,\nDwivedi-Yu J, Celikyilmaz A, Grave E, LeCun Y, Scialom T. Augmented Language Models: A Survey. DOI: 10.48550/arXiv.2302.07842. [103] Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y. ReAct: Synergizing Reasoning and Acting in Language Models. DOI: 10.48550/arXiv.2210.03629. [104] Qiao S, Ou Y, Zhang N, Chen X, Yao Y, Deng S, Tan C, Huang F, Chen H. Reasoning with Language Model Prompting: A Survey. DOI: 10.48550/arXiv.2212.09597. [105] Lialin V, Deshpande V, Rumshisky A. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647, 2023. [106] Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [107] Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B, Sun X, Xu J, Sui Z. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. [108] Lou R, Zhang K, Yin W. Is prompt all you need? no",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to provide a comprehensive overview of prompting engineering (PE) methods for Large Language Models (LLMs) through the lens of communication theory, addressing the evolving nature of these methods and their implications for NLP tasks.",
            "scope": "The survey focuses on various PE methods categorized into prompt template engineering, prompt answering engineering, and multi-prompt/multi-turn prompting. It excludes non-PE methods and specific applications of LLMs outside the prompting context to maintain clarity and coherence."
        },
        "problem": {
            "definition": "The core problem explored is the effective communication between users and LLMs through prompting methods, particularly how to reduce information misunderstandings in this interaction.",
            "key obstacle": "A primary challenge is the inherent complexity and variability in user intentions and LLM responses, which can lead to encoding and decoding errors during the prompting process."
        },
        "architecture": {
            "perspective": "The survey introduces a communication theory perspective that categorizes existing PE methods based on their objectives and principles, emphasizing the importance of reducing information misunderstanding.",
            "fields/stages": {
                "prompt template engineering": "Focuses on designing prompts that effectively guide LLM outputs.",
                "prompt answering engineering": "Involves mapping generated outputs to user expectations.",
                "multi-prompting methods": "Utilizes multiple prompts to enhance the robustness and stability of LLM responses."
            }
        },
        "conclusion": {
            "comparisions": "The survey compares various prompting methods, highlighting their strengths and weaknesses in terms of effectiveness, adaptability, and user alignment.",
            "results": "Key findings suggest that a nuanced understanding of communication theory can significantly enhance the design and application of prompting methods, leading to improved LLM performance."
        },
        "discussion": {
            "advantage": "Current research has achieved substantial advancements in prompting methods, allowing for better alignment between user queries and LLM outputs.",
            "limitation": "Despite progress, many prompting methods still struggle with interpretability and the ability to generalize across diverse tasks.",
            "gaps": "Unanswered questions remain regarding the optimal design of prompts that can adapt to varying user needs and contexts.",
            "future work": "Future research should focus on developing adaptive prompting strategies and exploring the integration of external tools to enhance LLM capabilities."
        },
        "other info": {
            "info1": "The survey emphasizes the interdisciplinary nature of prompting methods, bridging insights from communication theory and NLP.",
            "info2": {
                "info2.1": "The paper discusses the evolution of PE methods and their implications for future research directions.",
                "info2.2": "It highlights the importance of user-centered design in the development of effective prompting strategies."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.4",
            "key information": "Prompt engineering is critical for enhancing in-context learning in LLMs, as it focuses on designing prompts that effectively guide LLM outputs."
        },
        {
            "section number": "2",
            "key information": "The survey provides a comprehensive overview of prompting engineering (PE) methods for Large Language Models (LLMs) through the lens of communication theory."
        },
        {
            "section number": "3.1",
            "key information": "Current research has achieved substantial advancements in prompting methods, allowing for better alignment between user queries and LLM outputs."
        },
        {
            "section number": "4.1",
            "key information": "Key findings suggest that a nuanced understanding of communication theory can significantly enhance the design and application of prompting methods, leading to improved LLM performance."
        },
        {
            "section number": "6.1",
            "key information": "Despite progress, many prompting methods still struggle with interpretability and the ability to generalize across diverse tasks."
        },
        {
            "section number": "6.4",
            "key information": "Unanswered questions remain regarding the optimal design of prompts that can adapt to varying user needs and contexts."
        }
    ],
    "similarity_score": 0.7235264928831643,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models.json"
}