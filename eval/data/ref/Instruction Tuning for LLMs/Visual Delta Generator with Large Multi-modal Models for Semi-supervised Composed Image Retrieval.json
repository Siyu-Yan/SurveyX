{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.15516",
    "title": "Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval",
    "abstract": "Composed Image Retrieval (CIR) is a task that retrieves images similar to a query, based on a provided textual modification. Current techniques rely on supervised learning for CIR models using labeled triplets of the <reference image, text, target image>. These specific triplets are not as commonly available as simple image-text pairs, limiting the widespread use of CIR and its scalability. On the other hand, zero-shot CIR can be relatively easily trained with image-caption pairs without considering the image-toimage relation, but this approach tends to yield lower accuracy. We propose a new semi-supervised CIR approach where we search for a reference and its related target images in auxiliary data and learn our large language modelbased Visual Delta Generator (VDG) to generate text describing the visual difference (i.e., visual delta) between the two. VDG, equipped with fluent language knowledge and being model agnostic, can generate pseudo triplets to boost the performance of CIR models. Our approach significantly improves the existing supervised learning approaches and achieves state-of-the-art results on the CIR benchmarks.",
    "bib_name": "jang2024visualdeltageneratorlarge",
    "md_text": "# Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval\nYoung Kyun Jang*1, Donghyun Kim*2, Zihang Meng1, Dat Huynh1, and Ser-Nam Lim3 Meta AI1, Korea University2, University of Central Florida3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/035e/035ece74-53e1-4f83-b5b9-340b0d54e061.png\" style=\"width: 50%;\"></div>\n# Abstract\nComposed Image Retrieval (CIR) is a task that retrieves images similar to a query, based on a provided textual modification. Current techniques rely on supervised learning for CIR models using labeled triplets of the <reference image, text, target image>. These specific triplets are not as commonly available as simple image-text pairs, limiting the widespread use of CIR and its scalability. On the other hand, zero-shot CIR can be relatively easily trained with image-caption pairs without considering the image-toimage relation, but this approach tends to yield lower accuracy. We propose a new semi-supervised CIR approach where we search for a reference and its related target images in auxiliary data and learn our large language modelbased Visual Delta Generator (VDG) to generate text describing the visual difference (i.e., visual delta) between the two. VDG, equipped with fluent language knowledge and being model agnostic, can generate pseudo triplets to boost the performance of CIR models. Our approach significantly improves the existing supervised learning approaches and achieves state-of-the-art results on the CIR benchmarks.\n# 1. Introduction\nImage-to-image or text-to-image retrieval, where a query image/text is used to retrieve similar ones from a gallery, has grown into a pivotal research field with many practical applications [38]. However, relying solely on image queries is limiting, as they primarily retrieve similar images, making it challenging to understand the user\u2019s intent for modifications in the results. On the other hand, relying solely on text queries can also be restrictive, as it may not effectively convey the user\u2019s desired detailed visual contents. To address this, Composed Image Retrieval (CIR) was introduced [2, 36, 42, 52]. CIR seeks to retrieve images using a query that combines both an image and a textual description of the user\u2019s intent (referred to as the visual delta), which allows more flexible retrieval. Due to the convenience and diverse\n*Authors contributed equally.\n<div style=\"text-align: center;\">(a) CIR triplet generation with human supervision (expensive). (b) Visual Delta Generator for generating pseudo triplets.</div>\nFigure 1. An illustration of the data preparation process of (a) conventional supervised Composed Image Retrieval (CIR) vs. (b) our proposed semi-supervised CIR. While supervised CIR struggles to scale up due to high annotation costs, our semi-supervised method offers a cost-effective and scalable solution. It augments training samples efficiently by generating pseudo triplets through our Large Language Model (LLM)-based Visual Delta Generator.\napplicability of CIR, it has attracted increased attention recently for a variety of real-world applications. Existing research on CIR has been developed under two major settings: (1) Supervised CIR: Learning with supervised triplets (i.e. <reference image, visual delta, target image >) [2, 11, 28, 35, 50] as shown in Fig. 1 (a), and (2) Zero-shot CIR: Learning with massive noisy <image, textual caption > pairs [3, 42, 49], without any CIR supervision. Supervised CIR would obviously yield much higher accuracy in retrieval but requires expensive two-stage data collection processes - collecting pairs of related reference and target images, and then annotating them with visual delta that depicts the difference between them. On the other hand, zero-shot CIR does not incur additional labeling costs and utilizes web-collected noisy image text caption pairs directly. However, it has a much lower performance bar compared to supervised approaches and lacks the ability to specialize in specific CIR domain tasks. In this paper, we investigate a class of CIR called semi-supervised CIR, blending supervised and unsupervised samples to enhance generalization (Fig. 1 (b)). This\nmethod focuses on boosting CIR performance in specific retrieval domains by creating new triplets from unsupervised data. Building on this concept, we introduce a novel technique, Visual Delta Generator (VDG), designed to tap into the extensive natural language capabilities of Large Language Models (LLMs) [5, 21, 47, 48]. Our approach involves projecting reference and target images from supervised CIR triplets into the language embedding space, making them suitable inputs for the LLM. We then fine-tune the model by using prompts such as \u2018Describe the differences between images.\u2019 to induce it to yield human-like visual delta as illustrated in Fig. 2. Furthermore, we employ a parameter-efficient fine-tuning technique, LoRA [19], on the LLM. This choice of design effectively enhances the quality of visual deltas while also preserving the LLM\u2019s original capabilities, without harming its inherent knowledge. After the VDG is trained, it knows how to distinguish between a given reference and target image and produce visual delta as a textual response. If we forward two similar images with different compositions, we can thus obtain the corresponding visual delta easily with the VDG. This allows us to achieve two purposes. First, we can now augment existing CIR triplets by adding generated visual deltas to pairs of reference and target images from the training set. Second, we can also harvest new reference-target pairs from an unlabeled database based on visual similarity, after which we forward these images to VDG to configure new pseudo triplets for CIR training. Note that our VDG is model agnostic \u2013 it simply increases the number of triplet candidates for training any given supervised CIR baselines. This strategy strikes a balance between maintaining the integrity of supervisory concepts derived from a supervised dataset and the capacity for effortless expansion using new, unlabeled image samples. It\u2019s a cost-effective and scalable solution, ensuring uniformity in annotations across extensive datasets. By generating pseudo triplets with VDG, we significantly reduce annotation costs and enhance the performance of CIR models trained solely on supervised learning, as well as those trained without supervised triplets. Our approach leads to state-of-the-art results in CIR benchmarks. The key advantages of our semi-supervised CIR include: \u2022 To the best of our knowledge, we are the first to transfer knowledge from Large Language Models (LLMs) and connect it with semi-supervised Composed Image Retrieval (CIR). \u2022 We propose a novel Visual Delta Generator (VDG) that generates synthetic visual deltas for augmenting the supervised dataset and allowing the integration of an auxiliary image gallery for CIR model training. \u2022 Comprehensive experimental results confirm the effectiveness of our method, demonstrating state-of-the-art retrieval rankings and showcasing the potential of our work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3068/30684a8b-46df-482b-bea7-01356545ccf8.png\" style=\"width: 50%;\"></div>\nFigure 2. An overview of the VDG tuning process. It includes (a) a vision projector and (b) a Large Language Model (LLM). The VDG is trained to produce visual delta that accurately describes the difference between a reference image and its corresponding target image.\n# 2. Related Work\nComposed Image Retrieval. The field of image retrieval has captured the interest of many researchers in our community [6, 38]. One notable area that has seen much progress recently is Composed Image Retrieval (CIR), a problem that focuses on retrieving images that best match a given pair of a query image and textual intent. Supervised CIR methods [2, 11, 35, 50] are trained on human-annotated triplets, consisting of a reference image, a target image, and their textual difference. On the other hand, zero-shot CIR [3, 9, 16, 42] operates without relying on human-guided descriptions of the differences between the two images. Instead, it uses noisy image-text pairs, aiming to find a function that can translate images into words. This approach, designed to discriminate subtle differences between images based solely on text captions, poses challenges, but also scales well, making it easy to add more data for CIR training. Addressing the limitations of both approaches, we explore the field of semi-supervised learning-based retrieval in this work, leveraging the strengths of both labeled and unlabeled data to enhance retrieval performance.\nSemi-supervised Learning. Semi-supervised learning has been an active research topic in visual recognition for a long time [4, 18, 20, 25\u201327, 39, 44, 46, 51]. Semisupervised learning can be roughly categorized into two groups, consistency learning [25, 39, 46] and pseudolabeling based learning [27, 44]. Consistency-based methods, as the name implies, encourage consistency in the output of the model by adding noise to model weights or using Exponential Moving Average (EMA) from a teacher model to a student model. In pseudo-labeling based meth-\nods, hard/soft pseudo-labels obtained from a pretrained model are assigned to unlabeled images [15, 44]. These pseudo-labels can be filtered using confidence thresholding or multi-view consistency. However, neither consistencybased nor pseudo labels methods are directly applicable to CIR. This is because the relative textual descriptions of the corresponding visual differences (visual delta) needed in CIR are not the intended outputs of these methods. In this work, we propose the Visual Delta Generator (VDG), a multi-modal pseudo-label generator. VDG processes two input images and generates text that describes their visual differences, making it an ideal candidate for constructing pseudo triplets for CIR.\n# Multi-modal Models for Image-Text Retrieval. Models\nlike CLIP [41] and BLIP [29] have showcased the advantages of training models on extensive image-text pairs, enabling precise alignment between language and vision representations that is crucial for image-text retrieval. Building on this, there have been significant advancements in utilizing Large Language Models (LLMs) for enhanced visionlanguage understanding [1, 10, 31, 33, 53]. Especially for CIR, Fromage [23] utilizes LLMs to directly produce embeddings for retrieval, allowing cross-modal compositional search with a single image and textual intent. CoVR [49] and SEARLE [3] apply LLMs to generate visual deltas using image captions without incorporating visual data, which limits the generation of accurate CIR training samples. Our proposed method overcomes these challenges by finetuning LLMs with the integration of a pretrained visionlanguage alignment module. This integration empowers LLMs to perceive and comprehend images, making them applicable even in scenarios with only image datasets. The method excels in generating accurate visual deltas, a key factor in training efficient external CIR models. These enhancements optimize the use of LLMs while ensuring computational efficiency, thereby expanding the versatility of LLMs in image-related tasks.\n# 3. Method\nOverview. Our goal is to establish a semi-supervised Composed Image Retrieval (CIR) system that merges image reference features with user textual descriptions to retrieve images from a large-scale database. We face a challenge in the limited availability of supervised triplets necessary for robust CIR model training. To overcome this limitation, we introduce a novel semi-supervised approach for CIR by leveraging an instruction-tuned Large Language Model (LLM), which we call Visual Delta Generator (VDG). The VDG learns to discriminate differences between two images and produces a textual response. This capability allows us to generate additional CIR training triplets, which in turn contributes to the development of more robust CIR models.\nSec. 3.1 provides detailed insights into the construction of the VDG. Sec. 3.2 describes the pseudo triplet generation process. Training of CIR models with pseudo triplets and our additional objective function for better optimization are described in Sec. 3.3.\n# 3.1. Visual Delta Generator Training\nWhile semi-supervised learning methods have been actively developed for standard visual recognition tasks, these cannot be directly applied to CIR. In CIR, pseudo-label generation requires a detailed semantic understanding of two separate images such that their difference can be automatically expressed in the form of text. We leverage vision-language pretraining models and LLMs to achieve the requirements. With the huge success of LLMs, there are approaches that aim to utilize their understanding of the language domain for improving vision tasks. Particularly, LLaVA [33] and InstructBLIP [10] which are trained on top of the chat-bot style instruction tuned LLM, Vicuna [7], have shown interesting results on vision-language tasks. Inspired by these, we propose VDG, which allows the LLM to take two images (reference, target) with similar contents and discriminate their difference in the form of text response (visual delta) as shown in Fig. 2. First, to enable the LLM to interpret images, we use the Querying Transformer (Q-Former) motivated by InstructBLIP [10] to prepare images for the LLM input (i.e., Vision Projector (VP) in Fig. 2(a)). Q-Former, a transformer encoder [12, 21], processes a fixed set of 32 learnable query tokens (embeddings). These tokens are modified through self-attention layers and interact with image features of Vision Transformer (ViT) [12] through cross-attention layers. As a result, the query tokens are infused with the visual information from the provided image, making them suitable for LLM processing.\nStage 1: Alignment. The outputs of the VP are inherently not aligned with the tokenized word embeddings of the inputs to the LLM, which we chose to be LLaMA2 [48] in this work. Alignment between the VP and LLM needs to be attained by fine-tuning trainable projection of VP in Fig. 2(a) as was done in LLaVA [33]. Specifically, we employ largescale image-caption pairs to foster alignment between the image representations understood by the VP and the LLM. This step includes minimizing standard next token prediction loss which is generally used to train the decoder-based LLM as:\n(1)\nM where wt denotes a token at time step t, T is the length of the sequence, P(\u00b7) is the probability assigned by the model\nto the actual token wt given with the image x, instruction Iinst and previous tokens w(1,..,t-1), and \u03b8proj is parameters of projection layer. Following standard practice, the prediction for each token is computed using a softmax over the vocabulary of the LLM, and the cross-entropy loss is computed between the predicted probabilities and the textual token labels as a classification.\nStage 2: Instruction Tuning. In this stage, we conduct instruction tuning to equip our LLM with the ability to understand image pairs and produce visual delta, as shown in Fig. 2(b). Referring to Fig. 3, we train the LLM with an instruction (i.e., \u201cRequest\u201d) to generate the visual delta in textual format (i.e., \u201cResponse\u201d). We forward two distinct images (i.e., \u201cReference\u201d and \u201cTarget\u201d) into the LLM via the VP. We employ a parameter-efficient fine-tuning technique, LoRA [19], directly on the LLM. This fine-tuning process is designed to provide \u201cextra room\u201d for the LLM to undertake new tasks, all without compromising its foundational capabilities. In this stage, we introduce a specific prompt structure to the LLM as below:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a06a/a06a657e-d0a3-4229-b776-636af1497b49.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3. A template prompt for VDG instruction tuning.</div>\nwhich guides the LLM to generate the corresponding visual delta. We leverage the same training loss from Eqn. 1 to train LoRA parameters.\n# 3.2. Pseudo Triplet Generation for CIR\nAfter training VDG, we can generate visual delta of two images, which can be used to form pseudo triplets for CIR. However, it is important to choose two images that not only share certain attributes and similarities but also present other distinct attributes (i.e., visual delta). To gather suitable pairs of reference and target images, we utilize an image encoder to select them from an auxiliary image gallery, which we denote as G\u2032. Note that, we notate upper strophe (\u2032) on samples and embeddings that are obtained from G\u2032, in the following. Following the strategy in CIRR [36], we start with an anchor image xa and retrieve the top 20 images from G\u2032 using cosine similarity scores between ResNet 152 [17] embeddings, pretrained on ImageNet [24]. We exclude images with scores above 0.94 and sequentially add images to a subgroup of size 6, skipping those within a 0.002 score of the previously included image. As depicted in Fig. 4, we establish dense connections between all pairs (both consecutive, represented by the outer circle, and non-consecutive,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a52/2a52de55-44ca-4d5c-8c7d-30944e94e872.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4. The process of pseudo triplet generation. First, an image subgroup is constructed based on visual similarity (left). Then, paired reference and target images are fed into the VDG to generate the visual delta, completing the triplet formation (right).</div>\nrepresented by the dotted inner connections), while ensuring no overlaps. The arrow\u2019s starting point denotes the reference, while its endpoint indicates the target. Given images x\u2032r i and x\u2032t j , VDG produces \u03b4\u2032 i,j. This allows us to formulate the pseudo triplet as {x\u2032r i , x\u2032t j , \u03b4\u2032 i,j}.\n# 3.3. Semi-supervised CIR Training\nPreliminaries. Suppose we have access to a CIR dataset of triplets: D = {(xr i , xt j, \u03b4n(i,j))}N n=1 where xr i , xt j denote reference and target images, respectively, \u03b4n(i,j) represents their visual delta, and N denotes the total triplet counts. In the pursuit of enhancing CIR through a semisupervised approach, our method\u2019s strength lies in its model-agnosticism, allowing for seamless integration with a variety of CIR models. Following recent trends, we opt to use the encoders from vision-language pretraining models [10, 29, 31, 41], notably CLIP and BLIP, as our baseline backbones. These encoders are naturally equipped to understand and convert both visual and textual elements into a joint embedding space, making them suitable for CIR tasks. The image encoder takes patchified image token embeddings x = [x1, ...xKimg] with the learnable image cls token embedding [xcls], and outputs the visual feature embeddings Eimg(xcls, x) = [\u02c6xcls, \u02c6x1, ..., \u02c6xKimg] where \u02c6x \u2208Rdi of di dimension, and Kimg denotes the number of generated tokens from each image. The text encoder processes tokenizer output embedding of visual delta \u03b4 = [z1, ...zKtxt] with the learnable text cls token embedding [zcls] to produce its textual feature embeddings Etxt(zcls, \u03b4) = [\u02c6zcls,\u02c6z1, ...,\u02c6zKtxt], where \u02c6z \u2208Rdt of dt dimension, and Ktxt denotes the number of generated text tokens from each visual delta.\nModel Architecture. To carry out CIR, we establish a fusion function f that takes a reference image xr and a visual delta \u03b4, producing a composed embedding c as: f(xr, \u03b4) = c, and notates its trainable components as f\u03b8. We utilize two well-known backbones, CLIP and BLIP, to configure f. In the case of CLIP, we employ the text encoder Etxt:\u03b8 and an\nadditional Combiner module [2] C\u03b8(\u02c6xr cls,\u02c6zcls) that outputs c to be the components of f\u03b8 (i.e., f CLIP \u03b8 = {Etxt:\u03b8, C\u03b8}). The Combiner is designed to optimally blend \u02c6xcls and \u02c6zcls carefully weighing their individual impacts while adeptly mixing them. In the BLIP case, we exclusively use BLIP\u2019s text encoder, grounded in the image, and designate it as the trainable Etxt:\u03b8 (i.e., f BLIP \u03b8 = Etxt:\u03b8, and c = \u02c6zcls), without incorporating additional modules. BLIP\u2019s text encoder inherently fuses image and text signals in its cross-attention layer, eliminating the need for a separate combining module. Notably, we freeze all vision encoders in our setup to ensure compatibility with existing image retrieval galleries and to enhance training efficiency.\nThe\nSupervised / Pseudo Separated Contrastive Loss. The training objective of CIR is to achieve strong alignment between the target image\u2019s embedding x (where x = \u02c6xt cls for simplicity), and the composed embedding c. On this purpose, we utilize HN-NCE [40] loss for a given training batch B \u223cD and B\u2032 \u223cD\u2032, where D\u2032 contains pseudo triplets. Additionally, to mitigate the impact of noise in pseudo triplets and ensure consistent contributions from supervised triplets, we compute a target-composed contrastive loss (tcc) as:\n  \\math cal  {L}_{ tc c }(\\m a thc al {B}, \\mathcal {B'}) =\\mathcal {L}_{c}(\\mathcal {B};\\tau )+\\mathcal {L}_{c} (\\mathcal {B} \\oplus \\mathcal {B'};\\tau ) \\label {eqn:tcc} \n(2)\nwhere Lc is defined as:\n   where \u03c4, \u03b1 denote hyper-parameters, and \u2295denotes concatenation along the batch axis, and wxi,cj, wxj,ci are set as in [40]. This design facilitates the independent yet concurrent investigation of both supervised and semi-supervised CIR embedding spaces in a contrastive manner.\nTarget-Delta Matching Loss. In the case of BLIP\u2019s image-grounded text encoder structure, we introduce a new target-delta matching loss (tdm). Our insight stems from the observation that, while all reference image patch tokens are considered in the cross-attention layer of the BLIP text encoder, target image patch tokens are overlooked when training solely with contrastive learning between cls token embeddings. As an example shown in Fig. 5, the visual delta can be seen as a weakly correlated caption to the target image. Thus, we aim to align the target image and visual delta to enable the BLIP text encoder to identify the image tokens related to textual input. The tdm loss is applied as:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5f73/5f739900-98d2-46ff-8009-3bf4d61fa39f.png\" style=\"width: 50%;\"></div>\nFigure 5. Illustration of our proposed adaptation of the BLIP image-grounded text encoder for CIR. Both reference (xr) and target image (xt) patch tokens are processed by the text encoder (f\u03b8).\n(3)\nwhere H is cross entropy, ytdm is a 2-dimensional one-hot vector label obtained through the hard negative mining process proposed in [30], where a pair of image and text is configured as matched or not. ptdm \u2208R2 generates both positive and negative target-delta matching scores and is computed as:\np where \u02c6zt i denotes the i-th token embedding in f\u03b8(xt, \u03b4) and p\u03b8 is a FC layer that outputs a 2-D vector. This loss ensures the text encoder fully processes the target image features, improving its understanding of the visual delta.\n# 4. Experiments\nSec. 4.1 details the setup and VDG generation. Sec. 4.2 covers quality checks of visual deltas. Evaluations and baseline comparisons are in Sec. 4.3, and further analyses in Sec. 4.4.\n# 4.1. Setup\nImplementation Details. We utilize InstructBLIP\u2019s pretrained weights [10], which have been trained with both a ViT-G/14 [13] and the Q-Former based on the Vicuna-13B model [7], without using prompts for Q-Former. For the stage 1 training described in Sec. 3.1, we employ 595K filtered image-text pairs from CC3M [43] provided by [33] to\nfind alignment with our baseline LLM, LLaMA2-13B [48]. In Stage 2, we implement instruction tuning with LoRA parameters [19], following the fixed prompt as outlined in Fig. 3. Visual deltas are generated in an autoregressive manner, predicted based on the LLaMA2 vocabulary. For the CLIP-based CIR training, we select the ViT-L/14 model combined with a Combiner [2]. For the BLIP-based model [29], we use a dedicated BLIP text encoder for image-text matching, paired with the ViT-L/16 model. Additional details can be found in the appendix.\nDatasets for CIR Evaluation. There are two standard benchmarks in CIR, one is CIRR [36] which deals with natural images, and the other is FashionIQ [52] which focuses on fashion domain images. Each presents unique challenges and datasets that help researchers push the boundaries of what\u2019s possible in CIR. Following the protocols utilized in benchmarks [36, 52], we report the CIR results with recall scores at top K retrieval results (R@K), or results under collected subset (Rs@K). Specifically, CIRR is configured with 4,351 subgroups (subsets), each containing six similar images, sourced from NLVR2 [45]. For experimental purposes these groups are distributed into train (3,345 subgroups of 16,742 images), validation (503 subgroups of 2,265 images), and test (503 subgroups of 2,178 images) sets. FashionIQ is divided into three categories of Dress, Shirt, and Toptee (Tops and Tees). The reference and target images are paired based on their category similarities. The 18,000 CIR triplets for training are pooled from 45,429 images of the training set, and 6,016 CIR triplets for the test are chosen from 15,415 images of the validation set.\nVisual Delta Generation. To produce pseudo triplets, we expand our dataset sources to configure an auxiliary gallery (G\u2032) which is built upon the grouping strategy introduced in Sec. 3.2, while excluding images that overlap with the benchmark sets. Once the subgroups are constructed, we further filter them to avoid heavy overlap. In total, we draw upon 42,390 unique subgroups from NLVR2 [45], and 79,427 from COCO [32]. Similarly, we build 27,957 individual subgroups from FashionIQ [52], and 30,880 from DeepFashion [34]. We mark upper strophe (\u2032) to these datasets. For the semi-supervised settings, we randomly select 3,345 groups from NLVR2\u2032 and COCO\u2032 for the CIRR case, as well as 3,600 groups from FashionIQ\u2032 and DeepFashion\u2032. We denote these datasets used for semi-supervised CIR with the subscript se, (e.g., NLVR2\u2032 se, COCO\u2032 se). Fig. 6 shows the comparison between human-annotated visual deltas and those generated by the VDG in both natural and fashion domains. We observe that the VDG is effective in generating high-quality visual deltas \u2013 additional results can be found in the appendix.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4b8e/4b8e6d2a-0554-46fe-8238-bbb2ffa77fe9.png\" style=\"width: 50%;\"></div>\nFigure 6. Qualitative comparison on visual deltas, human vs. VDG on CIRR and FashionIQ datasets. For both natural and fashion domain images, VDG can produce informative visual deltas. Table 1. Retrieval results on CIRR validation set. Human + VDG represents utilizing both human annotated and VDG-generated visual deltas when training the CIR model. BLIPtdm represents a model with target-delta matching loss. The best scores for each Val. set supervision are highlighted in bold.\n<div style=\"text-align: center;\">Figure 6. Qualitative comparison on visual deltas, human vs. VDG on CIRR and FashionIQ datasets. For both natural and fashion domain images, VDG can produce informative visual deltas.</div>\nFigure 6. Qualitative comparison on visual deltas, human vs. VDG on CIRR and FashionIQ datasets. For both natural and fashion domain images, VDG can produce informative visual deltas.\nTable 1. Retrieval results on CIRR validation set. Human + VDG represents utilizing both human annotated and VDG-generated visual deltas when training the CIR model. BLIPtdm represents a model with target-delta matching loss. The best scores for each Val. set supervision are highlighted in bold.\nTraining Set\nSupervision\nVal. Set\nSupervision\nBaseline\nR@1\nR@5\nR@10\nR@50\n(a) Human\nHuman\nCombiner\n37.98\n71.49\n82.52\n95.29\nBLIPtdm\n53.17\n82.09\n89.81\n97.54\n(b) VDG\nHuman\nCombiner\n35.47\n68.29\n80.29\n94.31\nBLIPtdm\n50.16\n80.03\n87.78\n96.75\n(c) Human + VDG\nHuman\nCombiner\n39.11\n73.02\n84.41\n95.96\nBLIPtdm\n53.67\n82.99\n89.97\n97.84\n(d) Human\nVDG\nCombiner\n38.96\n73.31\n84.36\n96.22\nBLIPtdm\n51.64\n83.33\n91.39\n97.90\n(e) VDG\nVDG\nCombiner\n41.59\n77.57\n87.35\n97.11\nBLIPtdm\n52.69\n85.17\n92.37\n98.59\n(f) Human + VDG\nVDG\nCombiner\n41.47\n77.90\n87.42\n97.30\nBLIPtdm\n52.95\n85.27\n92.54\n98.61\n# 4.2. Quality Check of VDG Responses\nTo assess the quality of VDG-generated visual deltas, we executed a series of experiments as outlined in Table 1. We use two backbones, Combiner, and BLIP with the tdm loss (denoted as BLIPtdm), as our baselines for evaluation. Initially, we compare the visual deltas generated by VDG with those annotated by humans. This is done by replacing the deltas in the training set with VDG-generated deltas for the same reference-target pairs. When comparing (a) and (b), we observe only a marginal drop in performance upon switching to VDG-generated deltas. This indicates that the deltas generated by VDG are as effective as those created by humans. More importantly, the improved performance observed when comparing (a) with (c) \u2014 which combines human-annotated and VDG-generated deltas \u2014 highlights the effectiveness of this hybrid approach as a robust data augmentation strategy to enhance CIR models. To evaluate VDG\u2019s performance on new image pairs, we conduct experiments by substituting the human-annotated\nTable 2. Retrieval results on CIRR test set. * denotes our baselines, \u2020 denotes VDG generated visual deltas are applied to augment original training set. We categorize into two distinct groups: one is Seen: Supervised / Supervised + External / Supervised + Aux. with VDG which utilize human-annotated visual delta for CIR model training, and the other is Unseen: Zero-shot / Aux. with VDG which does not utilize human-annotated visual delta for CIR model training. Within each category, best viewed with bold.\nMethod\nDress\nShirt\nToptee\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\n(a) Supervised\nARTEMIS [11]\n27.16\n52.40\n21.78\n43.64\n29.20\n53.83\nDCNet [22]\n28.95\n56.07\n23.95\n47.30\n30.44\n58.29\nFashionVLP [14]\n32.42\n60.29\n31.89\n58.44\n38.51\n68.79\nCombiner [2]\n31.63\n56.67\n36.36\n58.00\n38.19\n62.42\nCoVR [49]\n43.51\n67.94\n48.28\n66.68\n51.53\n73.60\n*Combiner\n31.95\n55.05\n39.21\n56.82\n38.55\n62.16\n*Combiner\u2020\n35.40\n59.99\n42.30\n61.63\n43.09\n66.96\n*BLIPtdm\n44.87\n66.83\n49.61\n66.93\n50.54\n72.26\n*BLIP\u2020\ntdm\n46.90\n68.86\n50.28\n68.04\n52.73\n74.45\n(b) Supervised + External Dataset for Pretraining\nCASE + LasCo.Ca. [28]\n47.44\n69.36\n48.48\n70.23\n50.18\n72.24\nCoVR + WebVid [49]\n44.55\n69.03\n48.43\n67.42\n52.60\n74.31\n(c) Supervised + Auxiliary Gallery with VDG\nCombiner\u2020 + DeepFashion\u2032\nse\n35.50\n60.09\n42.54\n62.86\n43.27\n67.86\nCombiner\u2020 + FashionIQ\u2032\nse\n36.30\n60.19\n43.98\n62.27\n44.33\n68.06\nBLIP\u2020\ntdm + DeepFashion\u2032\nse\n47.10\n69.10\n49.95\n69.96\n53.90\n74.35\nBLIP\u2020\ntdm + FashionIQ\u2032\nse\n47.89\n69.81\n51.36\n71.08\n53.29\n74.65\n(d) Zero-shot\nPic2Word by CC3M [42]\n20.00\n40.20\n26.20\n43.60\n27.90\n47.40\nSEARLE by ImageNet [3]\n20.32\n43.18\n27.43\n45.68\n29.32\n50.17\nCoVR by WebVid [49]\n21.95\n39.05\n30.37\n46.12\n30.78\n48.73\n(e) Auxiliary Gallery with VDG\nCombiner by DeepFashion\u2032\n23.30\n46.36\n30.86\n49.02\n31.87\n51.96\nCombiner by FashionIQ\u2032\n28.26\n51.46\n32.58\n51.28\n34.88\n55.79\nBLIP\u2020\ntdm by DeepFashion\u2032\n32.67\n54.39\n35.48\n55.05\n39.47\n59.92\nBLIP\u2020\ntdm by FashionIQ\u2032\n37.48\n58.70\n37.29\n57.11\n42.12\n62.32\nMethod\nR@1\nR@5\nR@10\nR@50\nRs@1\nRs@2\nRs@3\n(a) Supervised\nARTEMIS [11]\n16.96\n46.10\n61.31\n87.73\n39.99\n62.20\n75.67\nCIRPLANT [35]\n19.55\n52.55\n68.39\n92.38\n39.20\n63.03\n79.49\nCombiner [2]\n33.59\n65.35\n77.35\n95.21\n62.39\n81.81\n92.02\nCASE [28]\n48.00\n79.11\n87.25\n97.57\n75.88\n90.58\n96.00\nCoVR [49]\n48.84\n78.05\n86.10\n94.19\n75.78\n88.22\n92.80\nCombiner\n34.39\n66.22\n76.58\n91.04\n68.55\n86.36\n93.98\nCombiner\u2020\n36.91\n69.21\n79.54\n92.04\n70.00\n87.45\n94.39\nBLIPtdm\n48.94\n77.83\n86.15\n94.17\n75.71\n89.71\n95.81\nBLIP\u2020\ntdm\n49.08\n78.98\n86.89\n94.24\n76.18\n90.62\n95.86\n(b) Supervised + External Dataset for Pretraining\nCASE + LasCo.Ca. [28]\n49.35\n80.02\n88.75\n97.47\n76.48\n90.37\n95.71\nCoVR + WebVid [49]\n49.69\n78.60\n86.77\n94.31\n75.01\n88.12\n93.16\n(c) Supervised + Auxiliary Gallery with VDG\nCombiner\u2020 + COCO\u2032\nse\n38.77\n69.25\n79.21\n91.52\n71.25\n87.49\n94.34\nCombiner\u2020 + NLVR2\u2032\nse\n38.89\n69.84\n79.41\n91.18\n71.92\n87.89\n94.29\nBLIP\u2020\ntdm + COCO\u2032\nse\n49.37\n78.12\n85.52\n93.74\n76.68\n90.46\n96.05\nBLIP\u2020\ntdm + NLVR2\u2032\nse\n50.96\n80.15\n86.86\n94.46\n77.45\n90.65\n96.10\n(d) Zero-shot\nPic2Word by CC3M [42]\n23.90\n51.70\n65.30\n87.80\n-\n-\n-\nSEARLE by ImageNet [3]\n24.22\n52.41\n66.29\n88.63\n53.71\n74.63\n87.61\nCASE by LasCo.Ca. [28]\n35.40\n65.78\n78.53\n94.63\n64.29\n82.66\n91.61\nCoVR by WebVid [49]\n38.48\n66.70\n77.25\n91.47\n69.28\n83.76\n91.11\n(e) Auxiliary Gallery with VDG\nCombiner by COCO\u2032\n26.80\n54.05\n65.30\n83.88\n67.28\n85.42\n92.99\nCombiner by NLVR2\u2032\n31.57\n61.37\n72.10\n88.94\n67.98\n86.18\n93.18\nBLIPtdm by COCO\u2032\n43.49\n72.07\n81.59\n93.21\n72.36\n87.98\n94.72\nBLIPtdm by NLVR2\u2032\n45.74\n75.01\n82.52\n93.13\n72.60\n87.90\n94.77\ndeltas in the validation set with deltas generated by VDG. The comparison of (a) and (d) reveals comparable performance levels, suggesting that deltas generated by VDG are as effective as human annotations. When we incorporate VDG-generated deltas into the training set (cases (e) and (f)), we achieve a notable performance improvement over the human-annotated validation set across all metrics, with the exception of a small reduction in R@1. For example, there is a 5.14%p increase in R@5 between (b) and (e). The improvement observed may stem from the varied text descriptions by different human annotators, leading to inconsistencies in style and detail that potentially affect the distribution of visual differences between the annotated triplets. VDG offers a more consistent solution in generating visual deltas, thereby reducing errors associated with differences in human annotation. Our comprehensive testing demonstrates VDG\u2019s reliability as an annotation tool for CIR.\n# 4.3. Comparison with Other Methods\nTables 2 and 3 present comparative evaluations between our method and other CIR approaches on CIRR and Fash-\n<div style=\"text-align: center;\">Table 3. Retrieval results on FashionIQ validation set. We utilize the same notations and same categorizations as Table 2.</div>\nionIQ evaluation protocols. We categorize the experiments into two distinct groups. The first group, Seen, includes scenarios where the CIR model is trained with humanannotated training triplets. In contrast, the second group, Unseen, comprises scenarios in which the CIR model is trained without human-annotated triplets. Our implementations of Combiner and BLIP are denoted by grey box . Specifically, methods under (a) are trained with supervised triplets, while those notated with the symbol (\u2020) are additionally augmented with VDG-generated visual deltas from supervised image pairs (i.e., the same setting as Human + VDG in Table 1). The methods in (b) have been pretrained on external datasets for CIR tasks, resulting in enhanced performances. In our semi-supervised setup (c), the models are trained with augmented supervised triplets and pseudo triplets sub-sampled from the auxiliary gallery as described in Secs. 3.2 and 4.1. Methods in (d) are trained with largescale datasets with the intention of producing zero-shot performances. In (e), we only use pseudo triplets generated from the auxiliary gallery for training. The results clearly demonstrate that our VDG implementation significantly improves the performance of both Combiner and BLIPtdm baselines. In (a), a detailed comparison shows that models augmented with VDG samples (with \u2020) consistently outperform their counterparts (without \u2020) in all\nTable 4. Ablation results on BLIPtdm baseline for CIRR validation set with NLVR2\u2032 se as auxiliary gallery. Best viewed with bold.\n<div style=\"text-align: center;\">Table 4. Ablation results on BLIPtdm baseline for CIRR validation set with NLVR2\u2032 se as auxiliary gallery. Best viewed with bold.</div>\ngrouping\nconcat.\ntdm loss\nR@1\nR@5\nR@10\nR@50\n52.45\n81.18\n88.62,\n96.88\n\u2713\n55.66\n83.45\n91.21\n97.33\n\u2713\n\u2713\n56.28\n84.39\n91.41\n97.92\n\u2713\n\u2713\n\u2713\n57.88\n85.58\n93.21\n98.33\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3de9/3de91607-a3ab-4de4-9484-1afad286abcf.png\" style=\"width: 50%;\"></div>\n82.37\n82.76\n82.99\n84.76\n85.58\n85.10\n85.20\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nAdditional Data Ratio\n53.67\n53.77\n54.36\n57.28\n57.88\n57.12\n57.59\nR@1\nR@5\nRecall\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nAdditional Data Ratio\n50.28\n50.31\n50.46\n51.21\n51.36\n51.17\n51.20\n68.04\n68.41\n68.63\n70.30\n71.08\n70.63\n70.88\nR@10\nR@50\nRecall\n<div style=\"text-align: center;\">(a) CIRR validation</div>\n<div style=\"text-align: center;\">(b) FashionIQ-Shirt</div>\n<div style=\"text-align: center;\">Figure 7. Analysis on the scale for (c) Supervised + Auxiliary Gallery with VDG of NLVR2\u2032 se and FashionIQ\u2032 se. The x-axis denotes the ratio of additional images to training images. Table 5. Analysis on the scale for (e) Auxiliary Gallery with VDG. The dataset is scaled from one eighth (1/8) to the full set (1). Evaluated on CIRR Test set.</div>\nFigure 7. Analysis on the scale for (c) Supervised + Auxiliary Gallery with VDG of NLVR2\u2032 se and FashionIQ\u2032 se. The x-axis denotes the ratio of additional images to training images. Table 5. Analysis on the scale for (e) Auxiliary Gallery with VDG. The dataset is scaled from one eighth (1/8) to the full set (1). Evaluated on CIRR Test set.\nR@1\n1/8\n1/4\n1/2\n1\nR@10\n1/8\n1/4\n1/2\n1\nCOCO\n42.15\n42.89\n43.10\n43.49\nCOCO\n79.81\n81.01\n81.16\n81.59\nNLVR2\n44.31\n44.99\n45.13\n45.74\nNLVR2\n82.48\n82.75\n82.43\n82.52\nrecall metrics. Moreover, in (c), when we enhance our CIR baselines with additional pseudo triplets, there is a notable performance improvement. Specifically, when BLIPtdm is combined with the auxiliary gallery, it surpasses the previous state-of-the-art results in most recall metrics, showcasing the advantages of VDG. Additionally, it\u2019s important to highlight that our semi-supervised CIR method achieves these improvements with considerably fewer images than methods like CASE or CoVR, demonstrating its efficiency. In the unseen scenario, our implemented Combiner baseline not only outperforms Pic2Word, which uses the same CLIP backbone, but our BLIPtdm also significantly exceeds the former best-performing SEARLE or CoVR in most metrics. VDG\u2019s key advantage is its ability to generate visual deltas that align with the targeted domain, such as using COCO for natural images and DeepFashion for fashion images, by solely utilizing images without any accompanying captions. As a result, VDG significantly reduces the number of training samples required, yet achieves enhanced retrieval performances compared to zero-shot approaches like Pic2Word, CASE, or CoVR. This demonstrates that by focusing on similar domain, image-only datasets, we can substantially improve the efficacy of CIR model training.\n# 4.4. Further Analyses\nAblation study on Each Component. To showcase the effectiveness of our proposed learning strategies, we conducted an ablation study and presented the findings in Table 4. We consider three scenarios: (1) without grouping, where we randomly sample images from the auxiliary gallery to create pairs; (2) without concat., where we remove the concatenation step used in Eqn. 2 and use pseudo triplets only;\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c582/c582cdb9-37db-4422-9c53-32f80972d5fd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8. Retrieval results on auxiliary galleries, COCO, and DeepFashion. Actual user intents are used as text queries.</div>\nand (3) without tdm loss, where we exclude the tdm loss during training. The results indicate that each element sufficiently contributes to enhancing the performance. Impact of Generated Data Scale. Figs. 7a and 7b demonstrate the effect of the size of VDG-generated data. In a semi-supervised setup, we find that the performance peaks when the number of pseudo-selected images approximates the size of the training set, and then saturates. This saturation might arise because additional pseudo triplets fail to represent the test sample distribution, particularly as we rigorously remove overlapping images with the test set when forming G\u2032. For the unseen case as shown in Table 5, performance is enhanced with an increase in VDG-generated data. This suggests that even in the absence of supervised triplets for CIR model training, a larger set of pseudo triplets not only expands the model\u2019s comprehension of visual compositions but also aligns more closely with the distribution of supervised triplets. This alignment facilitates improved targeted domain retrieval performances, striking a balance between generalization and performance enhancement. Qualitative Retrieval Results. We facilitate retrieval by providing user intents along with query images, drawing from auxiliary galleries. The retrieval results, as depicted in Fig. 8, demonstrate accurate and relevant image retrieval. Additional results are detailed in the appendix.\n# 5. Conclusion\nIn this study, we investigate a novel semi-supervised learning approach in the context of Composed Image Retrieval (CIR). Our findings reveal that integrating human-annotated data with pseudo triplets generated by the Visual Delta Generator (VDG) significantly enhances the generalization capacity of CIR models. The VDG approach not only streamlines the generation of visual deltas but also emerges as a cost-efficient and effective alternative to extensive human annotation. This work paves the way for future research in semi-supervised learning and showcases VDG as a promising direction for advancing CIR systems.\nAcknowledgment: This work was partially supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program(Korea University)).\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 35: 23716\u201323736, 2022. 3 [2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Effective conditioned and composed image retrieval combining clip-based features. In CVPR, 2022. 1, 2, 5, 6, 7 [3] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-shot composed image retrieval with textual inversion. In ICCV, 2023. 1, 2, 3, 7 [4] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. NeurIPS, 32, 2019. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877\u2013 1901, 2020. 2 [6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, pages 104\u2013120. Springer, 2020. 2 [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 3, 5 [8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 2 [9] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. \u201cthis is my unicorn, fluffy\u201d: Personalizing frozen vision-language representations. In ECCV, pages 558\u2013577. Springer, 2022. 2 10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 3, 4, 5 11] Ginger Delmas, Rafael Sampaio de Rezende, Gabriela Csurka, and Diane Larlus. Artemis: Attention-based retrieval with text-explicit matching and implicit similarity. In ICLR, 2022. 1, 2, 7 12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 13] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual representation learning at scale. In CVPR, pages 19358\u201319369, 2023. 5 [14] Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh Chada, Yue Wu, Varsha Hedau, and Pradeep Natarajan. Fashionvlp: Vision language transformer for fashion retrieval with feedback. In CVPR, pages 14105\u201314115, 2022. 7 [15] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. NeurIPS, 17, 2004. 3 [16] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, and Sangdoo Yun. Compodiff: Versatile composed image retrieval with latent diffusion. arXiv preprint arXiv:2303.11916, 2023. 2 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016. 4 [18] Seunghoon Hong, Hyeonwoo Noh, and Bohyung Han. Decoupled deep neural network for semi-supervised semantic segmentation. NeurIPS, 2015. 2 [19] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2021. 2, 4, 6 [20] Young Kyun Jang and Nam Ik Cho. Generalized product quantization network for semi-supervised image retrieval. In CVPR, pages 3420\u20133429, 2020. 2 [21] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, pages 4171\u2013 4186, 2019. 2, 3 [22] Jongseok Kim, Youngjae Yu, Hoeseong Kim, and Gunhee Kim. Dual compositional learning in interactive image retrieval. In AAAI, pages 1771\u20131779, 2021. 7 [23] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. 2023. 3 [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 25, 2012. 4 [25] Samuli Laine and Timo Aila. Temporal ensembling for semisupervised learning. arXiv preprint arXiv:1610.02242, 2016. 2 [26] Semi-Supervised Learning. Semi-supervised learning. CSZ2006. html, 2006. [27] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, page 896. Atlanta, 2013. 2 [28] Matan Levy, Rami Ben-Ari, Nir Darshan, and Dani Lischinski. Data roaming and early fusion for composed image retrieval. arXiv preprint arXiv:2303.09429, 2023. 1, 7 [29] Junnan Li and et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. 3, 4, 6 [30] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learning with momentum distillation. NeurIPS, 34:9694\u20139705, 2021. 5 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. 2023. 3, 4 [32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755. Springer, 2014. 6 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 3, 5, 1 [34] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In CVPR, 2016. 6 [35] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life images with pretrained vision-and-language models. In ICCV, 2021. 1, 2, 7 [36] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life images with pretrained vision-and-language models. In CVPR, pages 2125\u2013 2134, 2021. 1, 4, 6 [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018. 1 [38] Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, and Junchi Yan. Image matching from handcrafted to deep features: A survey. IJCV, 129:23\u201379, 2021. 1, 2 [39] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. TPAMI, 41(8):1979\u20131993, 2018. 2 [40] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. In CVPR, pages 6967\u20136977, 2023. 5 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021. 3, 4 [42] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2word: Mapping pictures to words for zero-shot composed image retrieval. In CVPR, pages 19305\u201319314, 2023. 1, 2, 7 [43] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, pages 2556\u20132565, 2018. 5 [44] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. NeurIPS, 33:596\u2013608, 2020. 2, 3\n[45] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. In ACL, 2019. 6 [46] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NeurIPS, 30, 2017. 2 [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 3, 6 [49] Lucas Ventura, Antoine Yang, Cordelia Schmid, and G\u00a8ul Varol. CoVR: Learning composed video retrieval from web video captions. arXiv:2308.14746, 2023. 1, 3, 7 [50] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and image for image retrieval-an empirical odyssey. In CVPR, pages 6439\u2013 6448, 2019. 1, 2 [51] Haokun Wen, Xuemeng Song, Jianhua Yin, Jianlong Wu, Weili Guan, and Liqiang Nie. Self-training boosted multifaceted matching network for composed image retrieval. arXiv preprint arXiv:2305.09979, 2023. 2 [52] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion iq: A new dataset towards retrieving images by natural language feedback. In CVPR, pages 11307\u201311317, 2021. 1, 6 [53] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 3\n# Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval\nSupplementary Material\n# 6. Reproduction Guide\n6.1. More Implementation details\nVDG: Alignment. For stage 1 in Sec. 3.1, the focus is on training the projection layer in the vision projector and our baseline LLM, LLaMA2-13B, to achieve alignment. Training is executed over a single epoch with a learning rate of 1  \\times 10^{-3}, batch size 64 per GPU.\nVDG: Instructional Tuning. For stage 2 in Sec. 3.1, additional LoRA parameters (\u03b8lora) are applied, configured with \\ a lpha =16, \\pro t ect \\text  {rank}=64, and \\protec t  \\text  {dropout}=0.05. The tuning process begins at a learning rate of 2  \\times 10^{-4}, incorporating a warm-up phase over 100 iterations. The learning rate is reduced to one-tenth after reaching half of the total 10 epochs, batch size 8 per GPU.\nVDG: Augmentation. For VDG training, we simply apply basic random resized cropping to our images. This involves adjusting the scale of the images between 0.8 and 1.0 and their aspect ratios between 0.9 and 1.1.\nVDG: Visual Delta Generation The generation of visual deltas is conducted autoregressively, applying a temperature scaling of 0.2 to the top 50 token predictions.\nAlgorithm 2 Stage 2 - Instruction Tuning of VDG\n1: Load \u03b8proj from Stage 1, initialize \u03b8lora\n2: Input: D - Train set of CIRR or FashionIQ\n3: Input: pinst - prompt from Fig. 3\n4: \u2113inst \u2190LLLM with B \u223cD, pinst\n5: \u03b8proj \u2190\u03b8proj \u2212\u03b3 \u2202\u2113inst\n\u2202\u03b8proj\n6: \u03b8lora \u2190\u03b8lora \u2212\u03b3 \u2202\u2113inst\n\u2202\u03b8lora\nEnsure: Updated \u03b8proj, \u03b8lora\nCIR: Hyper-parameters For Eqn. 2, we set the hyperparameters as \u03c4 = 0.01, \u03b1 = 1.0, \u03b2 = 0.0 for CLIPbased Combiner, which makes loss as same as standard contrastive loss, and \u03c4 = 0.01, \u03b1 = 1.0, \u03b2 = 0.5 for BLIP baseline.\nCIR: Augmentation. CIR model training employs a standard data augmentation pipeline to enhance robustness. We start with a random resized crop, adjusting the scale of the images between 0.5 and 1.0. Further, a random horizontal flip, and random adjustments to image contrast, brightness, and sharpness are applied. We also incorporate different perspectives and angles of images by modifying translation and rotation.\nCIR: Model Training. Batch size is set as 64 per GPU for CIR model training. The CIR models begin with an initial learning rate of 1e \u22124, which follows a cosine decay schedule to zero for 6 and 10 epochs for BLIP and CLIP baselines, separately.\nModel Training and Optimization All models are optimized using AdamW optimizer [37], with \\b e ta _1=0.9, \\b e ta _2=0.99, and a consistent weight decay of 0.05. Training is performed on 8 NVIDIA A100 80GB GPUs using bfloat16 precision.\n# 6.2. Training procedure\nWe provide a training procedure for VDG in Algorithms 1 and 2, as well as a semi-supervised learning approach for the CIR model in Algorithm 3. We set the same batch size for B and B\u2032.\nAlgorithm 1 Stage 1 - Alignment of VDG\n1: Initialize \u03b8proj\n2: Input: D - Filtered image-text pairs from CC3M\n3: Input: P - Set of prompts from LLaVA [33]\n4: \u2113align \u2190LLLM with B \u223cD, p \u223cP\n5: \u03b8proj \u2190\u03b8proj \u2212\u03b3 \u2202\u2113align\n\u2202\u03b8proj\nEnsure: Updated \u03b8proj\nAlgorithm 3 Semi-supervised CIR training\n1: Load Eimg, f\u03b8\n2: Input: D - Train set of CIRR or FashionIQ (additional\nvisual deltas are applied with VDG)\n3: Input: D\u2032 - Pseudo triplet generated from auxiliary\ngallery with VDG\n4: \u2113tcc \u2190Ltcc with B \u223cD, B\u2032 \u223cD\u2032\n5: \u2113tdm \u2190Ltdm with B \u223cD, B\u2032 \u223cD\u2032 (BLIP only)\n6: f\u03b8 \u2190f\u03b8 \u2212\u03b3 \u2202(\u2113tcc+\u2113tdm)\n\u2202f\u03b8\nEnsure: Updated f\u03b8\nTable 6. Detailed dataset configurations for auxiliary galleries and CC3M-Filtered for alignment training (Stage 1 in Sec. 3). \u2018#\u2019 denotes the number of images in the dataset. \u2018#\u2019 ref-tar pairs denotes the number of unique reference-target image pairs.\nDataset\n# images\n# ref-tar pairs\nNLVR2\u2032\n63,788\n152,604\nCOCO\u2032\n102,436\n285,939\nFashionIQ\u2032\n33,994\n100,647\nDeepFashion\u2032\n38,237\n111,168\nCC3M-Filtered\n595,375\n-\n<div style=\"text-align: center;\">Table 7. Ablation study on VDG for CIRR validation set.</div>\nMethods\nR@1\nR@5\nR@10\nR@50\nOur Final Model\n50.16\n80.03\n87.78\n96.75\n(1) LLaMA2-7B\n50.04\n79.29\n86.94\n95.86\n(2) LLaMA2-13B-chat\n50.23\n79.67\n86.96\n96.03\n(3) LoRA rank=32\n49.03\n79.12\n86.80\n96.24\n(4) LoRA rank=128\n48.94\n79.05\n86.96\n95.86\n(5) Q-Former: BLIP-2\n49.63\n78.76\n86.68\n95.72\nTable 8. Experiment results on the CIRR test set using different scales of the CC3M-Filtered Dataset (\u223c1.4M pseudo CIR triplets). The dataset scales from one eight (1/8) to the full set (1).\nRatio\nR@1\nR@5\nR@10\nR@50\n1/8\n44.43\n73.18\n82.36\n92.28\n1/4\n45.34\n73.90\n82.82\n93.21\n1/2\n45.37\n74.24\n83.06\n93.25\n1\n45.42\n74.55\n83.28\n93.32\n# 7. Further Analysis\nData statistics. We provide detailed configuration of auxiliary gallery used for experiments in Table 6.\nDesign Choice. We investigate several configuration options, including: (a) the model size of the LLM, (b) the type of LLM used, (c) the rank of LoRA, and (4) the type of Q-Former. Based on our final model in the first row, we change the designated component in each row. We explore these options in Table 7 to assess their impact on CIR performance. Specifically, for (1) we experiment with the LLaMA2-7B model. For (2), we opt for the chat-bot style tuned LLaMA2-13B-chat model. Regarding (3) and (4), we experiment with varying the rank at 32 and 128, noting that our baseline is 64. Finally, for (5), we employ the Q-Former from BLIP-2, which is in line with FlanT5-XXL [8], rather than using the InstructBLIP one. It is important to note that our evaluations indicate that these options do not significantly affect performance. This underscores the general applicability and robustness of our VDG, demonstrating its effectiveness across a variety of configurations.\nLarger Scale Experiment. We further configure 1,431,135 pseudo triplets with the CC3M-Filtered dataset to explore the scalability of VDG to larger datasets and show the results in Table 8. It appears that as the dataset size increases, there\u2019s a gradual improvement in the recall metrics, suggesting that using more data improves the model\u2019s ability to retrieve relevant results.\nVDG Generation Results. We provide more generation results based on subgroups in Figs. 9 and 10. We notice that the VDG excels in generating high-quality visual deltas, with only a few errors.\nRetrieval Results. We provide more retrieval results for natural images in Figs. 11 and 12, and for fashion images in Figs. 13 and 14. In the domain of natural images, we chose query examples containing the word must. We observe that our CIR model effectively grasps the meaning of the text query and reflects this understanding in the retrieval results. In addition, the domain of fashion images is also well-represented in the retrieval results, accurately reflecting the user\u2019s text query while maintaining visual information of query image.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0446/0446c49d-573b-49a4-9a0b-fd7da384c4b6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9. Visual delta generation results with VDG on CIRR validation set.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/845b/845bf770-689c-4a12-a52b-2dcf313bc120.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10. Visual delta generation results with VDG on the DeepFashion dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c3db/c3dbbd99-dac1-4b7c-8f99-93cc8d30be79.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Change to show litter of puppies toppled on top of one  another instead of moving, must include red background</div>\n<div style=\"text-align: center;\">Change to a close-up photograph of a pure-white  Samoyed dog breed, must have tongue sticking out</div>\n<div style=\"text-align: center;\">Figure 11. Retrieval results on the CIRR test set.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1453/14532e83-f0f4-4f63-9eb2-f1d290813a9e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12. Retrieval results on the COCO dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1df4/1df426ea-7362-44a4-b63c-fa2f750b70e4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Black with free hugs written on it,  with colorful letters</div>\n<div style=\"text-align: center;\">Figure 13. Retrieval results on the FashionIQ dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c70/0c7076ab-a173-401f-af6c-c6dfe4cd8d23.png\" style=\"width: 50%;\"></div>\nFigure 14. Retrieval results on the DeepFashion dataset.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of Composed Image Retrieval (CIR), which seeks to retrieve images using a query that combines both an image and a textual description of the user\u2019s intent. Existing methods for CIR rely heavily on supervised learning, which requires labeled triplets of reference images, visual deltas, and target images. However, the scarcity of such labeled data limits the scalability and application of CIR. Zero-shot CIR approaches are less accurate as they utilize noisy image-caption pairs without supervision. This paper proposes a semi-supervised approach that generates pseudo triplets to enhance the performance of CIR models.",
        "problem": {
            "definition": "The problem addressed in this paper is the challenge of effectively retrieving images that match a user's query that combines visual and textual information, particularly in the context of limited availability of labeled data for supervised learning.",
            "key obstacle": "The main difficulty is the high cost and complexity of collecting and annotating supervised triplets, which restricts the scalability of existing CIR methods and limits their application in real-world scenarios."
        },
        "idea": {
            "intuition": "The proposed idea stems from the observation that large language models (LLMs) can effectively describe visual differences between images, suggesting that they can be leveraged to generate synthetic training data for CIR.",
            "opinion": "The Visual Delta Generator (VDG) is introduced as a method to generate textual descriptions of visual differences (visual deltas) between reference and target images, thus augmenting the training data for CIR.",
            "innovation": "The key innovation of this work lies in the use of VDG to create pseudo triplets that blend supervised and unsupervised samples, allowing for a more scalable and cost-effective training approach compared to traditional supervised methods."
        },
        "method": {
            "method name": "Visual Delta Generator",
            "method abbreviation": "VDG",
            "method definition": "VDG is a semi-supervised learning approach that generates visual deltas from pairs of images using a large language model, facilitating the creation of pseudo triplets for training CIR models.",
            "method description": "VDG generates textual descriptions of visual differences between pairs of images, enhancing the training dataset for CIR models.",
            "method steps": [
                "Align the visual features of reference and target images using a vision-language model.",
                "Train the VDG to generate visual deltas based on image pairs.",
                "Generate pseudo triplets by combining reference images, target images, and the generated visual deltas."
            ],
            "principle": "The effectiveness of VDG is based on its ability to leverage the language understanding capabilities of LLMs to accurately describe visual differences, which enhances the training process for CIR models."
        },
        "experiments": {
            "evaluation setting": "The experimental setup includes two standard benchmarks for CIR: CIRR, which utilizes natural images, and FashionIQ, which focuses on fashion domain images. The datasets are divided into training, validation, and test sets, with specific configurations for each.",
            "evaluation method": "The performance of the proposed method is evaluated using recall scores at various retrieval ranks (R@K) to compare against existing methods and assess the effectiveness of the generated pseudo triplets."
        },
        "conclusion": "The study concludes that integrating human-annotated data with pseudo triplets generated by VDG significantly enhances the generalization capacity of CIR models. The VDG approach is shown to be a cost-efficient alternative to extensive human annotation, paving the way for future research in semi-supervised learning for CIR.",
        "discussion": {
            "advantage": "The proposed approach stands out due to its ability to efficiently generate high-quality visual deltas, reducing the reliance on costly human annotations while improving retrieval performance.",
            "limitation": "A limitation of the method is the potential for lower performance in scenarios where the generated visual deltas do not accurately reflect the intended differences between images, which may occur in more complex retrieval tasks.",
            "future work": "Future research may explore further enhancements to the VDG, such as improving the quality of generated deltas or extending its application to other retrieval tasks beyond CIR."
        },
        "other info": [
            {
                "info1": "This work was partially supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)."
            },
            {
                "info2": {
                    "info2.1": "The proposed method achieves state-of-the-art results on CIR benchmarks.",
                    "info2.2": "The VDG is model agnostic, making it applicable to various CIR models."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2. Background and Definitions",
            "key information": "The problem addressed in this paper is the challenge of effectively retrieving images that match a user's query that combines visual and textual information, particularly in the context of limited availability of labeled data for supervised learning."
        },
        {
            "section number": "2.4 Transfer Learning",
            "key information": "The Visual Delta Generator (VDG) is introduced as a method to generate textual descriptions of visual differences (visual deltas) between reference and target images, thus augmenting the training data for CIR."
        },
        {
            "section number": "3. Fine-Tuning in NLP",
            "key information": "VDG is a semi-supervised learning approach that generates visual deltas from pairs of images using a large language model, facilitating the creation of pseudo triplets for training CIR models."
        },
        {
            "section number": "3.2 Benefits of Fine-Tuning",
            "key information": "The proposed approach stands out due to its ability to efficiently generate high-quality visual deltas, reducing the reliance on costly human annotations while improving retrieval performance."
        },
        {
            "section number": "8. Challenges and Future Directions",
            "key information": "A limitation of the method is the potential for lower performance in scenarios where the generated visual deltas do not accurately reflect the intended differences between images, which may occur in more complex retrieval tasks."
        },
        {
            "section number": "8.4 Future Research Directions",
            "key information": "Future research may explore further enhancements to the VDG, such as improving the quality of generated deltas or extending its application to other retrieval tasks beyond CIR."
        }
    ],
    "similarity_score": 0.5297006774280538,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0858_fine-/papers/Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval.json"
}