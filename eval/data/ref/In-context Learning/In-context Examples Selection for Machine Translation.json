{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.02437",
    "title": "In-context Examples Selection for Machine Translation",
    "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and outof-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
    "bib_name": "In-context3",
    "md_text": "# -context Examples Selection for Machine Tran\nSweta Agrawal1\u2217, Chunting Zhou2, Mike Lewis2, Luke Zettlemoyer2, Marjan Ghazvininejad 1 University of Maryland 2 Meta AI sweagraw@umd.edu {chuntinz,mikelewis,lsz,ghazvini}@meta.com\n# Abstract\nLarge-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and outof-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.\narXiv:2212.02437v1\n# 1 Introduction\nIn-context learning (Brown et al., 2020) has recently received a lot of attention from the NLP research community due to its remarkable ability to utilize only a few input-output examples to perform many NLP tasks (Liu et al., 2021). For example, Lin et al. (2021) demonstrate that a 7.5B multilingual generative model, XGLM, outperforms a supervised sequence-to-sequence baseline in 45 translation directions on the FLORES-101 machine translation benchmark (Goyal et al., 2022) using\njust 32 randomly sampled translation examples as demonstrations. While these results are compelling, recent work has also shown that the performance and capability of a pre-trained language model (PLM) can be highly sensitive to many factors, such as the choice of in-context examples (Liu et al., 2022b), their ordering (Lu et al., 2022) and the template (Jiang et al., 2020). Typically, in-context learning for MT uses examples that are randomly sampled from a small development set that resembles the domain of the test dataset. The effect of the aforementioned factors (such as the choice of the examples) on the translation quality of the PLM hence remains unclear and unexplored. Yet another crucial gap in using in-context learning for MT in the current literature is the effect of the domain of in-context examples on translation quality since out-of-domain generalization is a known and important challenge in MT (Koehn and Knowles, 2017). In this work, we systematically analyze how factors such as the choice and the number of few-shot in-context examples and their ordering impact MT output quality. We show that while noisy unrelated 1-shot example can have a significantly adverse effect on translation quality, a single prompt optimized to maximize the translation quality on a development set can sufficiently elicit task-based information from the PLM. Our analysis thus demonstrates the importance of selecting good examples for MT and raises the question: What are the properties of good in-context examples for MT? In that direction, our findings suggest that a well-formed meaning-equivalent translation example results in higher quality translation than randomly selected in-context examples. Furthermore, motivated by the use of Translation Memory in Computer-Aided Translation (Yamada, 2011) and its usage in computational approaches to Machine Translation (Somers, 1999; Koehn and Senellart, 2010; Khandelwal et al., 2020,\ninter alia), we retrieve similar examples to the test source from a datastore that includes pairs of source text and their corresponding translations via BM25, an unsupervised efficient retriever to provide additional context to the model. As the context window of the PLM is usually limited (\u223c3096 tokens, 16 \u221220 examples), we propose a novel in-contextexample selection and reranking strategy that maximizes the coverage of the source n-grams in the selected examples. Experiments on WMT\u201919 English\u2194German and English\u2194Russian datasets show that our proposed re-ranking strategy can consistently improve the translation quality over the outputs generated using BM25 retrieved examples. Combining optimized 1-shot task-level with example-specific in-context examples using a simple concatenation strategy further improves translation quality, outperforming state-of-the-art inference-adapted nearest-neighbor MT models (kNN-MT) on two out-of-domain datasets (Medical and IT) while being memory and compute efficient as our approach does not require constructing and querying a dense token-level datastore.\n# 2 Background: In-context Learning\nGenerating translations from large-scale multilingual language models like mGPT (Shliazhko et al., 2022), XGLM (Lin et al., 2021) or AlexaTM 20B (Soltan et al., 2022) requires conditioning the decoder-only language model with in-context parallel examples. These examples serve two purposes: a) providing the model with the format and knowledge of the task (task-level) and b) guiding the output generation via providing useful information about the unseen source sentence (example-specific). This is different from the standard sequence-to-sequence models, where the task is always known, and the model learns generalizable patterns from the input-output examples to perform the task (in this case, translation) for the unseen source text. Formally, given k in-context examples {xi, yi}k 1 the prefix input or the prompt, xp j, is generated by concatenating the demonstration examples {(xi, yi)}k 1 to the test input, xs j according to a template, P (see Table 1). The output, \u02c6y, is then generated via the PLM with parameters \u03b8 via greedy decoding as follows:\nTable 1: In-context Examples for Machine Translation.\n# 3 Prompt Selection\nIdeally, good in-context examples can trigger the pre-trained language model to generate the desired output and also elicit the information learned during pre-training (Jiang et al., 2020). Min et al. (2022) show that, for classification tasks, the incontext examples provide information about the task (the distribution of the input text, the label space, and the format of the task) and that the model does not rely on these examples to generate the final output. However, their analysis is limited to a) classification tasks and 2) randomly sampled in-context examples. Prior work has also shown that the order of these in-context examples can also lead to high variance in downstream performance (Zhang et al., 2022). However, less is understood about how these factors impact text generation tasks like MT. Do we need multiple incontext examples? What makes good in-context examples for MT? How sensitive is the model to the order of the prompts? In this work, we aim to better understand the impact of prompt selection on the translation quality of the outputs. Given a training dataset consisting of n parallel examples D = {xi, yi}n i=1, and a test source xj, we select a subset of m informative samples to form a prompt which either provides task-level and/or example-specific information as discussed below.\n# 3.1 Task-level In-context Examples\nA good task-level in-context example should be able to elicit information learned during pretraining from the PLM. One way to measure the efficacy of an example as a prompt is via computing the translation quality of the outputs generated when prompting the PLM given an example. Hence, we select the task-level prompt as follow: For a\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8bf3/8bf35f68-d3a5-44dc-a638-5012509c9224.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Our proposed strategy can cover all the terms from the input text,\u201cWelche Risiken sind mit Poulvac FluFend H5N3 RG verbunden?\u201d, in this case with just the two examples.</div>\ngiven example sampled from the training dataset, (xi, yi) \u2208DS, we create a prompt, xp i by concatenating the example {(xi, yi)} to each source in the development set. The system outputs are then generated using equation 1. We then rank examples from DS as task-level prompts based on the BLEU of the generated outputs against the references on this held-out development set, Ddev = {X, Y }:\n(2)\n# 3.2 Example-specific In-context Examples\nPrior work on retrieving good in-context examplespecific prompts for tasks other than MT (like question answering or knowledge retrieval) either trains a dense-retriever (Rubin et al., 2021) or utilizes samples that are closer to the test source in the embedding space of a PLM like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), or XLNET models (Liu et al., 2022b). While contextual models can generate a global sentence representation, they overlook rare lexicons which can be important for generating translations in unseen domains like medical or IT (Wrzalik and Krechel, 2021). However, for MT, overlapping n-grams between the source and the retrieved sentences ensures informativeness as the target associated with the retrieved sentence is likely to include partial translations of the source. We can thus use BM25 as an efficient unsupervised retrieval method to retrieve similar examples. However, as the examples are scored independently and BM25 favors rare\nword matches (Robertson et al., 2009), the top retrieved candidates might not cover all the terms in the source text (Figure 1). Given that the context window of the PLM is usually limited (\u223c3096 tokens, 16 \u221220 examples), maximizing the coverage of all the terms found in the test input might be favorable. Hence, we propose to re-rank the top 100 candidates retrieved from BM25 using our algorithm outlined in 1. We extract all the word n-grams, and their counts from the test source, xs j and source of the BM25 retrieved examples, {Pj(xi}k 1 (lines 2-4). Let S and Q denote the set of the source n-grams and the n-grams from a BM25 retrieved example, respectively. We compute a recall-based (R) n-gram overlap score (line 7) using the following equation:\n(4)\nThe example with the maximum score is then added to the set of selected prompts, and the found n-grams from the test source are then downweighted by a factor, \u03bb for the next iteration of selection (line 14). For example, setting \u03bb = 0 will select the example that covers the n-grams from the test source in the subsequent iteration that has not already been encountered. This process is then re-\nAlgorithm 1: An N-gram Recall-based Strategy to Re-rank In-context Examples\nInput: Prompts {Pj(xi, yi)}k\n1 for the test source xs\nj, \u03bb, Threshold\nOutput :Ordered Selected Prompts {T = Pj(xi, yi)}s\n1, s \u2264k\n1 T \u2190Empty Ordered List\n2 S \u2190EXTRACT_WORD_NGRAMS_WITH_COUNTS (xs\nj)\n3 for i \u2208{1..k} do\n4\nQ[i] \u2190EXTRACT_WORD_NGRAMS_WITH_COUNTS (Pj(xi))\n5 while True do\n6\nfor i \u2208{1..k} do\n7\nScore[i] \u2190NGRAM_OVERLAP_SCORE (S, Q[i])\n8\nif max(Score) < Threshold then\n9\nbreak\n10\nT.append(Pargmax(Score))\n11\nmatched_ngrams \u2190S \u2229Q[argmax(Score)]\n12\nQ[argmax(Score)] \u2190\u2205\n13\nfor ngram \u2208matched_ngrams do\n14\nCountS(ngram)\u00d7 = \u03bb\n15 Return T\npeated over the retrieved pool until a set threshold of the score is reached. Figure 1 shows the top-100 candidates retrieved via BM25 for the input: \u201cWelche Risiken sind mit Poulvac FluFend H5N3 RG verbunden?\u201d. The top few candidates provide the same information to the PLM, i.e., translation of the phrase \u201cPoulvac FluFend H5N3 RG\u201d. The examples including the other terms (\u201cWelche Risiken sind mit verbunden ?\u201d) from the input text, are ranked lower. On the other hand, our proposed re-ranking strategy can cover all the terms from the input text, in this case, with just the top-2 examples.\n# 4 Evaluation Settings\n# 4.1 Datasets and Evaluation Metric\nWe perform our in-domain evaluation on the WMT19 German (de) \u21d4English (en) and WMT-19 Russian (ru) \u21d4English (en) datasets (Barrault et al., 2019). For the out-of-domain evaluation, we use the multi-domain dataset from Aharoni and Goldberg (2020) for the following domains: Medical, Law, IT, and Koran. The dataset statistics are included in the Appendix (Table 11). We normalize punctuation using the Moses toolkit (Koehn et al., 2007) and remove sentences longer than 250 tokens as well as sentence pairs with a source/target length ratio exceeding 1.5 from the in-domain datasets.\nWe evaluate the detokenized length truncated outputs generated by the model using sacreBLEU (Post, 2018).1 The generated outputs from the PLM are truncated to twice the source length, as preliminary analysis suggested degeneration in a few (\u223c10-20) examples.\n# 4.2 Experimental Conditions\nLanguage Model We use the publicly available checkpoint of the XGLM7.5B, a decoder-only causal language multilingual model (Lin et al., 2021) for all our experiments, which has 32 layers and a hidden dimension of 4096.\n# Baselines and Comparisons We consider the following comparisons:\n\u2022 Random: p random few-shot examples sampled from the training dataset (number of trials=3).\n\u2022 Task-level: top-p examples that achieve the highest BLEU on the development set (\u00a7 3.1).\n\u2022 Retrieved In-context (BM25): qmax examples retrieved via BM25, since unlike task-level examples, there is no guarantee that exactly q similar examples will be found in the training dataset for each input.\nMethod\np + qmax\nEn-De\nDe-En\nRu-En\nEn-Ru\nAvg.\nTask-level\n1 + 0\n23.35\n32.16\n30.48\n25.04\n27.75\nBM25\n0 + 1\n19.17\n25.82\n24.54\n21.51\n22.76\nR-BM25\n0 + 1\n20.60\n28.19\n27.26\n21.92\n24.49\nRandom (Baseline)\n16 + 0\n24.48\n31.26\n30.38\n25.67\n27.95\nTask-level\n16 + 0\n23.72\n31.22\n30.89\n27.27\n28.28\nBM25\n0 + 16\n26.58\n32.16\n31.44\n28.54\n29.68\nR-BM25\n0 + 16\n27.07\n32.59\n31.85\n28.90\n30.10\nR-BM25\n0 + 17\n27.00\n32.68\n31.88\n28.80\n30.09\nTask-level + R-BM25\n1 + 16\n27.09\n33.24\n31.90\n29.50\n30.43\nTable 2: Results on WMT\u201919 test sets: Concatenating task-level prompt to R-BM25 consistently achieves the best BLEU scores across the board. p and qmax are the number of task-level and example-specific prompts respectively.\n\u2022 Retrieved Re-ranked In-context (R-BM25): qmax re-ranked examples using our proposed approach as detailed in \u00a7 3.2.\nWe additionally compare our results with kNNMT (Khandelwal et al., 2020) for out-of-domain evaluation. We use \u03bb = 0.1, threshold=1.0 and order the examples according to their similarity to the source, with the most similar examples on the left in all our experiments based on an initial hyperparameter search on the development dataset (Appendix Tables 12,13).\n# 5 Results\nTable 2 and 3 summarizes our main results for the in-domain evaluation when translating between English <-> German and English <-> Russian and the four German-English out-of-domain datasets, respectively.\n# 5.1 In-domain Evaluation\nA single task-level prompt is competitive with 16 random few-shot examples. Our experiment suggests that it is possible to elicit the task-level knowledge from the large-scale language model using a single prompt as opposed to using 16 random few-shot examples when translating into English (Table 2). Using a single task-level prompt (optimized on the development set) improves BLEU over using 16 random few-shot examples for 2 out of 4 translation directions (De-En, Ru-En). We hypothesize that when translating out of English, the model still benefits from getting exposed to multiple and diverse random few-shot examples as the target language model is relatively weaker.\nMultiple example-specific prompts are required to improve translation quality over a single task-level prompt. Using a single task-level (p = 1) prompt attains higher BLEU over using a single example-specific prompt (q = 1; BM25, RBM25) across the board. By contrast, using upto 16 BM25 prompts (qmax = 16) significantly improves output quality over using task-level prompts, with an average gain of 1.41 in BLEU.\nRe-ranking BM25 retreived examples improves BLEU. Our proposed re-ranking strategy consistently improves BLEU across the board over BM25 for both values of qmax = {1, 16} showing that both the order and the choice of the in-context examples matters. Both task-level and R-BM25 examples provide complementary advantages, as combining them using a simple concatenation strategy improve output quality over task-level or R-BM25 examples. We leave the exploration of optimizing the number and the joint order of task-level and example-specific prompts to future work.\n# 5.2 Out-of-domain Evaluation\nAs XGLM is trained on monolingual Common Crawl snapshots, translation in any domain and language could be considered an out-of-domain task from the model\u2019s perspective. However, we hypothesize that translation in specific domains like medical, law, or IT could still be challenging for the PLM as the model is less likely to have observed even sufficient monolingual datasets for these specialized domains, in contrast to the news text found in WMT. Examples from these domains might require translating rare terminologies\nMethod\nCorpus\np + qmax\nMEDICAL\nLAW\nIT\nKORAN\nAvg.\nTask-level\nDomain-specific\n1 + 0\n31.23\n32.10\n28.70\n14.68\n26.68\nWMT\n30.08\n31.10\n26.72\n13.19\n25.27\nR-BM25\nDomain-specific\n0 + 1\n52.62\n55.46\n40.54\n13.76\n40.60\nTask-level\nDomain-specific\n16 + 0\n32.65\n33.68\n28.81\n15.30\n27.61\nWMT\n30.14\n30.76\n26.19\n12.72\n24.95\nR-BM25\nDomain-specific\n0 + 16\n56.43\n59.57\n46.57\n17.49\n45.02\nR-BM25\nDomain-specific\n0 + 17\n56.65\n59.55\n46.64\n17.48\n45.08\nTask-level + R-BM25\n1 + 16\n56.76\n59.56\n47.50\n17.55\n45.34\nkNN-MT\n-\n-\n54.35\n61.78\n45.82\n19.45\n45.35\nTable 3: Results on the Multi-Domain Test Set: Prompting XGLM with R-BM25 in-context examples outpe forms kNN-MT on 2 out of 4 domains.\nand carry domain-specific idiosyncrasies, which is known to pose a challenge even for a well-trained supervised neural MT model (Koehn and Knowles, 2017). Hence, we also evaluate PLM under these specialized out-of-domain scenarios.\nDomain of few-shot in-context examples matter. Task-level in-context examples drawn from the domain of evaluation, i.e., domain-specific, obtain on an average higher BLEU scores across the board than using examples from a distant WMT corpus as expected (Table 3) in both 1-shot (p = 1: +1.4) and 16-shot (p = 16: +2.7) settings.\nExample-specific prompts significantly improve translation quality over task-level prompts. Unlike the in-domain evaluation, retrieved and re-ranked example-specific prompts (R-BM25) improve the translation quality significantly across the board with up to 23 BLEU gain in the Law domain using just a single example as a prompt over a task-level prompt. This can be attributed to the high lexical overlap in the examples retrieved from the training data for these domains (Table 8).\n# Task-level and R-BM25 prompts are comple-\nmentary. Both task-level and R-BM25 provide supporting information for a given test source sentence as concatenating these set of prompts improves output quality over using these methods independently, outperforming a strong kNN-MT baseline on 2 out of 4 domains (Medical and IT) without requiring access to a strong base MT model or token-level retrieval during inference. Our manual analysis suggests that the higher gain obtained in the IT domain (+0.86) when using both task-\nlevel and example-specific prompts can be explained by the observation that for 100 test source sentences, there are no training examples with any lexical overlap with the test source. The task-level prompt can still elicit learned information from the PLM over using no examples for these inputs.\n# 6 Analysis\n# 6.1 Task-level Example Selection\nChoice of Few-shot Examples We show the distribution of output quality as measured by BLEU when using 100 different examples as prompts in Figure 2. Across all four language pairs, there is a large variation in BLEU scores (up to 20 BLEU), where noisy or unrelated prompts can lead to significantly worse output quality. Given that most existing parallel corpora are web-crawled and the quality of bitext can vary significantly across different language pairs (Kreutzer et al., 2022), randomly sampled examples can under-estimate the translation quality attainable by prompting these PLM.\n# Impact of Pool Size on Task-level Prompt Selec-\ntion We select the best task-level prompt based on the translation quality on the development set from a random sample of 100 examples (pool) as detailed in Section 3.1. However, one concern regarding the selection of the best task-level prompt in this fashion could be that we might still be underestimating the PLM (s) performance, as a larger pool size could result in better output quality. We study the impact of using a larger pool size in Table 5 where increasing the number of examples from 100 to 1000 only leads to a gain of 0.5 points in the maximum BLEU. From the same table, we\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a701/a70145ba-29e7-4904-9a5a-35cc65a08dbb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: BLEU distribution on the WMT\u201918 test set for 100 randomly sampled 1-shot prompts from the training dataset. The same set of 100 random 1-shot prompts are used for x\u2192y and y \u2192x translation directions.</div>\nFeatures\nEn-De\nDe-En\nEn-Ru\nRu-En\n% (Aligned words)\nRandom (T=3)\n0.818 \u00b10.009\n0.837 \u00b1 0.015\n0.594 \u00b10.106\n0.663 \u00b10.051\nTask-level\n0.834\n0.926\n0.773\n0.886\nPrism-Src\nRandom (T=3)\n-1.027 \u00b10.068\n-1.081 \u00b10.016\n-2.214 \u00b10.139\n-1.767 \u00b10.111\nTask-level\n-0.843\n-0.847\n-1.557\n-1.206\nTable 4: Average scores obtained by top-10 1-best prompts and 10 Random 1-shot prompts on features quantifyi semantic equivalence/translation quality (higher is better).\n1-shot Prompts\n100\n1000\nMax\n35.82\n36.29\nMean\n34.06\n29.95\nStdev\n0.96\n9.55\nRandom 10 trials of best over 100 1-shot Prompts\nMean over Max\n-\n36.08\nStdev over Max\n-\n0.18\nTable 5: Task-level example selection from 1000 1-shot Prompts on the WMT\u201919 development dataset.\ncan also observe that for any subset of random 100 few-shot examples, we can extract a task-level prompt (BLEU: 36) with a small standard deviation in overall outpust quality (0.18).\nTranslation direction Figure 3 shows the correlation between output quality in forward (x \u2192y)\nand reverse (y \u2192x) translation directions when using 1-shot prompts \u2014 there is a moderate to high correlation in output quality for both language pairs suggesting that the best and worst 1-shot prompts in one direction exhibit similar behavior in the opposite translation direction.\nProperties of good Task-level prompts Our manual analysis on the best task-level prompts suggests that any well-formed and meaning-equivalent translation (Vyas et al., 2018; Briakou and Carpuat, 2020) could make a good task-level prompt (see examples in Appendix Table 14). To quantify the meaning equivalence of the 1-best task-level prompt against random 1-shot examples, we report the percentage of aligned words between the source and reference translation (\u201c% Aligned words\u201d) using fastAlign (Dyer et al., 2013) and the probability of generating the reference translation condi-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d09c/d09c4835-fd48-4e46-bbf2-04186feea9e4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Best 1-shot prompts work equally well for both translation directions.</div>\ntioned on the source using a pre-trained multilingual NMT model, Prism-src (Thompson and Post, 2020; Agrawal et al., 2021) in Table 4.2 Across all language pairs and both metrics, task-level examples achieve higher semantic similarity scores than random 1-shot examples suggesting that task-level examples are relatively more equivalent in meaning than random examples.\nPrompt\nBLEU\nBest\n32.14\nWorst\n1.12\nBest + Worst\n25.54\nWorst + Best\n31.43\nTable 6: BLEU when using a equivalent translation against a noisy unrelated parallel example on the WMT18 De-En Development Dataset.\nImpact of Noise Table 6 shows the impact on translation quality when using an unrelated translation example against a meaning-equivalent or wellformed source-translation pair as a prompt: a noisy unrelated in-context example leads to extremely low BLEU score of 1. While using both a good\ntask-level and a noisy prompt via concatenation (best-then-worst and worst-then-best) reduces the effect of noise, the best output quality achieved when using both prompts (31.43) is still lower than using just the best task-level prompt (32.14), highlighting the importance of both prompts selection and their ordering on translation quality. Impact of Ordering To further explore the sensitivity to the order of few-shot prompts on MT quality, we use all possible order permutations of four randomly sampled examples and the top four task-level examples as prompts (4!) and report the translation quality as measured by BLEU in Table 7: Task-level prompts are less sensitive to prompt order, as suggested by the lower standard deviation achieved in all settings, and result in higher translation quality than randomly selected examples. Across the three different runs of randomly sampled examples, there is a significant difference in BLEU, further corroborating that the choice of incontext examples and their ordering matters.\nEn-De\nDe-En\nEn-Ru\nRu-En\n34.43 \u00b10.25 25.19 \u00b10.26 12.48 \u00b15.72 15.56 \u00b10.50\nRandom\n35.63 \u00b10.48 25.85 \u00b10.15 24.99 \u00b10.21 19.04 \u00b10.39\n34.73 \u00b10.30 23.93 \u00b10.28 10.92 \u00b14.64 17.91 \u00b10.07\nOptimized 35.95 \u00b10.24 26.98 \u00b10.15 25.85 \u00b10.11 19.96 \u00b10.24\nTable 7: BLEU over all 24 permutations of 3 seeds of 4 randomly selected and top 4 task-level prompts.\n# 6.2 Informativeness of Example-specific\n# 6.2 Informativeness of Example-specific Prompts\nTo understand the benefit of retrieved examples in the out-of-domain evaluation, we measure the lexical overlap between the test input (x, y) and the prompts (Ix, Iy) using BLEU (Avg. BLEU (Ix, x), Avg. BLEU (Iy, y)), where Ix and Iy are the sources and target translations of the retrieved incontext examples. We also report the correlation against the translation quality BLEU(\u02c6y, y). Table 8 shows that the source lexical overlap is a good indicator of the informativeness of a prompt for 3 out of 4 domains, with Koran as an exception. For Koran, while the retrieved sentences have a high overlap with the source (36.03), the target associated with the prompts (Iy) does not get high BLEU with the reference (10.36) compared to other domains. We hypothesize that this might be due to a bias in the reference translations towards a particular output style. We provide an analysis of\nMedical\n35.785\n0.593\n32.101\n0.777\nLaw\n34.982\n0.677\n34.349\n0.786\nIT\n25.196\n0.497\n19.382\n0.669\nKoran\n36.033\n-0.016\n10.364\n0.676\nTable 8: Correlation between the degree of overlap as measured by BLEU and the translation quality of the outputs BLEU(\u02c6y, y), across different domains when using the top-1 prompt retrieved using BM25. Ix and Iy are the sources and the reference translations in the BM25 examples respectively.\nthe impact of this phenomenon on MT quality in Section 7.\n# 7 Output Analysis\nWe report two interesting findings when prompting PLM with task-level and example-specific prompts:\nStylistic Outputs One advantage of using a single task-level in-context example to prompt the PLM is that it allows us to systematically study how the choice of prompt influences the style of the generated translation. Table 9 illustrates one such example: we can observe that as the prompt includes a contraction (\u201cwe are\u201d vs. \u201cwe\u2019re\u201d), the outputs generated by the PLM also include contractions and can be incorrectly penalized by BLEU while being meaning equivalent.\nPrompt: Wegen des hei\u00dfen Sommers fangen wir erst sp\u00e4t an. = Because of the hot summer, we\u2019re late getting started. Source: Ja, ich bin sehr zufrieden mit dem Auftritt. Reference: Yes, I am very happy with the performance. PLM Output: Yes, I\u2019m very satisfied with the performance. Source: Es ist eine andere Unternehmenskultur. Reference: It is a different corporate culture. PLM Output: It\u2019s a different corporate culture.\nTemplate-based MT Template-based translation in medical, legal, it, or e-commerce domain can be preferable as they reduce the risk of generating errors in automatically generated translations. We present some examples in Table 10 on how PLM can seamlessly use retrieved prompts to synthesize a translation from the template provided.\nPrompt: WIE IST SINGULAIR ANZUWENDEN? = HOW TO TAKE SINGULAIR Source: WIE IST EVOLTRA ANZUWENDEN? PLM Output: HOW TO TAKE EVOLTRA Prompt: Zeigt die aktuelle Datei mit Opera an. = View the current file with Opera. Source: Zeigt die aktuelle Datei mit Lynx an (Textbasierter Browser). PLM Output: View the current file with Lynx (Text-based browser).\nTable 10: Outputs follow the template of the prompt.\n# 7.1 Size of the Datastore\nFigure 4 shows BLEU when varying the size of the datastore used to retrieve similar in-context examples using BM25 on the Medical dataset. As the size of the datastore increases, the likelihood of retrieving a more similar example increases. However, similar output quality in BLEU can be achieved by using multiple in-context examples when a smaller in-domain datastore is available as multiple examples can provide better coverage of the source terms \u2014 BLEU @q=16 with a datastore size of 100k is equivalent to BLEU @q=1 with twice as many examples (200k).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f03/6f0311b4-6ab2-42c6-a809-5daffa843a6d.png\" style=\"width: 50%;\"></div>\nFigure 4: BLEU on the Medical dataset when varying the size of the datastore and the number of BM25 incontext examples.\n# 8 Related Work\nIn-context Learning for MT Garcia and Firat (2022) use natural language prompts (e.g. Translate to {language_name}: {text}) to control the target language in multilingual MT and investigate the impact of scale, number of languages, and their similarity for this phenomena. Wang et al. (2022) utilize BM25 retrieved training examples in a supervised fashion to learn from similar examples during training. Contrary to prior work, we utilize similar examples to form a textual prompt which is used to guide the generation of translation output during inference and systematically study the properties of good in-context examples for MT. Domain Adaptation for MT Prior work on domain adaptation for machine translation uses outof-domain bilingual or monolingual datasets to improve the translation quality of a pre-trained neural sequence-to-sequence MT model either during training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Wang et al., 2017) or inference (Zheng et al., 2021; Khandelwal et al., 2020). Similar to past work, our work utilizes out-of-domain datasets during inference to adapt a pre-trained generative language model to improve the translation quality on unseen domains. However, our approach does not rely on creating a domain-specific tokenlevel datastore, but directly uses similar examples to provide additional context, hence is more compute and memory efficient. Prompt Selection The importance of selecting good in-context examples and their impact on downstream NLP task performance has been studied in prior work (Liu et al., 2022b; Lu et al., 2022; Jiang et al., 2020; Min et al., 2022; Zemlyanskiy et al., 2022; Rubin et al., 2021; Liu et al., 2022a). However, how these examples and their properties impact MT quality remains unexplored, which we investigate in our work.\n# 9 Conclusion\nWe investigate the choice of in-context examples selection for MT in both in-domain and out-ofdomain settings. We propose a novel recall-based re-ranking approach to utilize similar training examples as prompts and show their efficacy across multiple datasets and domains. Our findings show that task-level prompts can provide a complementary advantage to example-specific prompts, outperforming a strong kNN-MT baseline in 2 out\nof 4 out-of-domain datasets while being memory and compute efficient. Our manual analysis of the generated outputs reveals that the PLM can mimic the style of the in-context examples provided and can be used for template-based translation synthesis. These results open space for future research to evaluate the potential of generating diverse and style-specific outputs for MT.\n# References\nSweta Agrawal, George Foster, Markus Freitag, and Colin Cherry. 2021. Assessing reference-free peer evaluation for machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1158\u20131171.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7747\u2013 7763, Online. Association for Computational Linguistics.\nLo\u00efc Barrault, Ond\u02c7rej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1\u201361, Florence, Italy. Association for Computational Linguistics.\nacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\nglance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50\u201372.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021. Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022a. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638.\niachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022b. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nthan you think: A simple and effective method by retrieving from training data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3170\u20133179.\n# A Statistics of Datasets\nTable 11 includes statistics of training, development and test sets used for the experiments discussed in the paper.\nDataset\nTrain\nDev\nTest\nWMT-19 (de)\n42M\n2998\n2000\nWMT-19 (ru)\n10M\n3000\n2000\nMulti-Domain\nMedical\n248K\n2000\n2000\nLaw\n467K\n2000\n2000\nIT\n223K\n2000\n2000\nKoran\n17K\n2000\n2000\n# B Compute Infrastructure & Run time\nEach experiment is run on a single Nvidia Tesla V100 Volta GPU machine with 32G Ram. A single inference experiment on 2000 test examples using XGLM with 16 in-context examples takes around 3-4 hrs to complete.\n# C Results using Second Metric: Comet\nWe report translation quality using Comet (Rei et al., 2020) in Tables 15 and 16. We use the eamt22-cometinho-da model (Rei et al., 2022) to generate the scores as it was shown to achieve higher correlations with human judgments than lexical overlap metrics while being computationally efficient. Our re-ranking strategy (with qmax = 16) consistently performs the best across the board except for Koran, outperforming strong kNN-MT baselines on the multi-domain test set in 3 out of 4 settings. Adding a task-level prompt to 16 R-BM25 prompts via concatenation further improves quality in 5 out of 8 settings.\n# D Hyperparameter Search\n# D.1 Order of BM25 Retrieved Examples\nWe report the BLEU when using two different orderings of example-specific prompts on the development set for the medical domain. Ordering the examples with the most similar examples on the left attains higher BLEU than the right-to-left order. We note that the trend could vary depending on the noise in the training dataset, the degree of similarity, and the number of retrieved examples. We leave\nthe exploration of the ordering of example-specific prompts to future work.\n\u03bb\nBLEU\nLeft-to-right\n56.84\nRight-to-left\n54.97\nTable 12: BLEU using twi different orderings of the top-16 example-specific BM25 prompts on the Medical development Set.\n# D.2 Choice of \u03bb, Threshold\nTable 13 shows the BLEU and the average number of in-context examples selected when varying \u03bb and the threshold described in Section 3.2. We select \u03bb = 0.1 and threshold value of 1.0 as it achieves the best BLEU on the Medical development set as shown below:\n\u03bb\nThreshold\nBLEU\nAvg. # of Examples\n0.1\n0.1\n54.55\n14.16\n1.0\n54.56\n12.73\n5.0\n53.35\n8.83\n0.3\n0.1\n54.47\n15.06\n1.0\n54.51\n14.28\n5.0\n53.98\n10.32\n0.5\n0.1\n54.44\n15.44\n1.0\n54.39\n15.10\n5.0\n54.44\n11.85\nTable 13: BLEU using different values of \u03bb and threshold on the Medical Development Set (qmax = 16).\n# E Example Task-Level Prompts\nTable 14 shows the best task-level in-context example selected by our method described in \u00a7 3.1 and the respective BLEU scores on the development set for the German-English and Russian-English tasks.\nGerman: Beispielsweise der \u00c4nderungsantrag zu Artikel 5 in der Stellungnahme des Ausschusses f\u00fcr Landwirtschaft und l\u00e4ndliche Entwicklung weist klar und deutlich darauf hin, dass die Verschlechterung der Qualit\u00e4t des Bodens lokale oder regionale Ursachen und Wirkungen hat und daher unbedingt nationale statt europ\u00e4ischer Ma\u00dfnahmen ergriffen werden m\u00fcssen. English: For example, the amendment to Article 5 in the opinion of the Committee on Agriculture and Rural Development clearly indicates that the degradation of the soil has local or regional causes and effects and it is therefore essential to adopt national as opposed to European measures. Development BLEU: 35.82 Russian: \u0415\u0441\u043b\u0438 \u0432\u0430\u0448 \u0431\u0440\u0430\u0443\u0437\u0435\u0440 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0440\u0430\u043d\u0435\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0439 \u201ccookie\u201d, \u0442\u043e \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0439 \u0438\u043c \u043f\u043e\u0441\u0442\u0430\u0432\u0449\u0438\u043a \u0438\u043c\u0435\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u043e\u0435\u0434\u0438\u043d\u0438\u0442\u044c \u0430\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u0441\u0435\u0449\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u043c\u0438 \u043f\u043e\u0441\u0435\u0449\u0435\u043d\u0438\u044f\u043c\u0438, \u043d\u043e \u0442\u043e\u043b\u044c\u043a\u043e \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u0441\u0432\u043e\u0435\u0433\u043e \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044f. English: If the browser sends back an earlier saved cookie, then the service managing these can connect to the user\u00b4s earlier visit, but only in respect of their own content. Development BLEU: 25.63\nTable 14: Best task-level prompt For De-En and Ru-En Language Pairs according to the BLEU development set.\nMethod\np + qmax\nEn-De\nDe-En\nRu-En\nEn-Ru\nTask-level\n1 + 0\n0.354\n0.403\n0.428\n0.626\nBM25\n0 + 1\n0.107\n0.149\n0.139\n0.346\nR-BM25\n0 + 1\n0.204\n0.249\n0.244\n0.413\nRandom-Avg\n16 + 0\n0.387\n0.391\n0.424\n0.636\nTask-level\n16 + 0\n0.389\n0.381\n0.440\n0.662\nBM25\n0 + 16\n0.423\n0.410\n0.434\n0.673\nR-BM25\n0 + 16\n0.438\n0.420\n0.444\n0.677\nR-BM25\n0 + 17\n0.440\n0.421\n0.448\n0.676\nTask-level + R-BM25\n1 + 16\n0.434\n0.430\n0.447\n0.694\n<div style=\"text-align: center;\">Table 15: Comet Scores on WMT\u201919 test sets.</div>\nMethod\nCorpus\np + qmax\nMEDICAL\nLAW\nIT\nKORAN\nResults from Jiang et al. (2022)\nVanilla kNN-MT\n-\n-\n0.548\n0.662\n0.531\n-0.014\nTheir model\n-\n-\n0.578\n0.703\n0.585\n0.047\nTask-level\nDomain-specific\n1 + 0\n0.314\n0.320\n0.240\n-0.068\nWMT\n0.277\n0.345\n0.146\n-0.113\nR-BM25\nDomain-specific\n0 + 1\n0.464\n0.553\n0.389\n-0.216\nTask-level\nDomain-specific\n16 + 0\n0.369\n0.365\n0.222\n-0.047\nWMT\n0.297\n0.399\n0.098\n-0.131\nR-BM25\nDomain-specific\n0 + 16\n0.697\n0.697\n0.666\n-0.105\nR-BM25\nDomain-specific\n0 + 17\n0.699\n0.697\n0.667\n-0.104\nTask-level + R-BM25\n1 + 16\n0.701\n0.699\n0.721\n-0.095\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The paper addresses the challenge of how the choice and ordering of in-context examples affect the performance of machine translation (MT) models. It highlights the gap in understanding the impact of these factors on output quality, especially in the context of in-domain and out-of-domain translations.",
            "purpose of benchmark": "The benchmark aims to evaluate the effectiveness of different in-context example selection strategies on the translation quality of models, particularly focusing on their performance in various domains."
        },
        "problem": {
            "definition": "The benchmark is designed to assess the impact of in-context examples on machine translation quality, specifically examining how different selection and ordering of these examples affect translation outputs.",
            "key obstacle": "Existing benchmarks do not adequately address the variability introduced by the choice and order of in-context examples, leading to inconsistent translation quality."
        },
        "idea": {
            "intuition": "The creation of the benchmark stems from the observation that the quality of in-context examples can significantly influence the performance of pre-trained language models in MT tasks.",
            "opinion": "The authors believe that understanding the properties of effective in-context examples is crucial for improving machine translation systems and can lead to more reliable and higher-quality outputs.",
            "innovation": "This benchmark introduces a novel recall-based re-ranking strategy for selecting in-context examples, which is shown to improve translation quality compared to random sampling or existing methods.",
            "benchmark abbreviation": "ICE-MT"
        },
        "dataset": {
            "source": "The dataset was created using existing parallel corpora from the WMT19 English-German and English-Russian datasets, along with additional out-of-domain datasets from various fields like Medical, Law, IT, and Koran.",
            "desc": "The dataset consists of parallel translation examples, with specific attention to the quality and relevance of in-context examples for machine translation tasks.",
            "content": "The dataset includes text data in the form of source sentences and their corresponding translations, relevant to both in-domain and out-of-domain contexts.",
            "size": "54,000",
            "domain": "Machine Translation",
            "task format": "Translation"
        },
        "metrics": {
            "metric name": "BLEU",
            "aspect": "Translation quality",
            "principle": "BLEU was chosen as it is a widely accepted metric for evaluating the quality of machine translation outputs by comparing them to reference translations.",
            "procedure": "Model performance is evaluated by generating translations from the selected in-context examples and calculating BLEU scores against reference translations."
        },
        "experiments": {
            "model": "The experiments utilized the XGLM 7.5B model, a state-of-the-art multilingual generative language model.",
            "procedure": "Models were trained using a combination of task-level prompts and example-specific retrieved prompts, with various configurations explored to optimize translation quality.",
            "result": "Results demonstrated that combining task-level prompts with re-ranked in-context examples significantly improved BLEU scores compared to using either method alone.",
            "variability": "Variability was accounted for through multiple trials and by testing different subsets of the dataset to ensure robust evaluation."
        },
        "conclusion": "The benchmark findings suggest that carefully selected in-context examples can substantially enhance machine translation quality, providing a framework for future research to explore diverse and contextually relevant prompts.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by providing insights into the importance of in-context example selection, which can lead to more efficient and effective machine translation systems.",
            "limitation": "One limitation is that the benchmark may not fully capture the complexities of all possible translation scenarios, particularly in highly specialized domains.",
            "future work": "Future research could explore the development of more sophisticated methods for selecting and ordering in-context examples, as well as extending the benchmark to additional languages and domains."
        },
        "other info": {
            "info1": "The benchmark emphasizes the role of domain-specific prompts in improving translation quality, especially in specialized fields.",
            "info2": {
                "info2.1": "The authors suggest that further studies should investigate the impact of prompt ordering on translation outputs.",
                "info2.2": "The benchmark could also be adapted to assess other NLP tasks beyond machine translation."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the challenge of how the choice and ordering of in-context examples affect the performance of machine translation (MT) models."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark aims to evaluate the effectiveness of different in-context example selection strategies on the translation quality of models, particularly focusing on their performance in various domains."
        },
        {
            "section number": "3.1",
            "key information": "The creation of the benchmark stems from the observation that the quality of in-context examples can significantly influence the performance of pre-trained language models in MT tasks."
        },
        {
            "section number": "3.3",
            "key information": "This benchmark introduces a novel recall-based re-ranking strategy for selecting in-context examples, which is shown to improve translation quality compared to random sampling or existing methods."
        },
        {
            "section number": "5.2",
            "key information": "The benchmark findings suggest that carefully selected in-context examples can substantially enhance machine translation quality."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that the benchmark may not fully capture the complexities of all possible translation scenarios, particularly in highly specialized domains."
        }
    ],
    "similarity_score": 0.6976819536755251,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-context Examples Selection for Machine Translation.json"
}