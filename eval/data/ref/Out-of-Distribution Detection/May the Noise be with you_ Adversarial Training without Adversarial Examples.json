{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.08877",
    "title": "May the Noise be with you: Adversarial Training without Adversarial Examples",
    "abstract": "While the vulnerability of machine learning (ML) models to adversarial attacks has been thoroughly investigated by the community, it continues to be a persistent threat that undermines the trustworthiness of ML systems. Adversarial training (AT) remains the first line of defense used to harden ML models. It works by solving a complex min-max optimization when training the network on adversarial examples. On the other hand, stochastic-based certifiable defenses like \"randomized smoothing\" introduce randomness into the input to reduce the attack surface and provide probabilistic guarantees of robustness. In this paper, we investigate the following question: Can we obtain adversarially-trained models without training on adversarial examples? Our intuition is that training a model with inherent stochasticity, i.e., optimizing the parameters by minimizing a stochastic loss function, yields a robust expectation function that is non-stochastic. In contrast to related methods that introduce noise at the input level, our proposed approach incorporates inherent stochasticity by embedding Gaussian noise within the layers of the NN model at training time. We model the propagation of noise through the layers, introducing a closed-form stochastic loss function that encapsulates a noise variance parameter. Additionally, we contribute a formalized noise-aware gradient, enabling the optimization of model parameters while accounting for stochasticity. Our experimental results confirm that the expectation model of a stochastic architecture trained on benign distribution is adversarially robust. Interestingly, we find that the impact of the applied Gaussian noise\u2019s standard deviation on both robustness and baseline accuracy closely mirrors the impact of the noise magnitude employed in adversarial training. Our work contributes adversarially trained networks using a completely different approach, with empirically similar robustness to adversarial training. We hope that further",
    "bib_name": "arous2023noiseyouadversarialtraining",
    "md_text": "# May the Noise be with you: Adversarial Training without Adversarial Examples\nAyoub Arous1, Andr\u00e9s F L\u00f3pez-Lopera2, Nael Abu-Ghazaleh3, Ihsen Alouani4 1 Division of Engineering, New York University (NYU) Abu Dhabi, UAE 2 CERAMATHS, UPHF, Valenciennes, France 3 University of California Riverside, CA, USA 4 CSIT, Queen\u2019s University Belfast, UK\n12 Dec 2023\n# Abstract\nWhile the vulnerability of machine learning (ML) models to adversarial attacks has been thoroughly investigated by the community, it continues to be a persistent threat that undermines the trustworthiness of ML systems. Adversarial training (AT) remains the first line of defense used to harden ML models. It works by solving a complex min-max optimization when training the network on adversarial examples. On the other hand, stochastic-based certifiable defenses like \"randomized smoothing\" introduce randomness into the input to reduce the attack surface and provide probabilistic guarantees of robustness. In this paper, we investigate the following question: Can we obtain adversarially-trained models without training on adversarial examples? Our intuition is that training a model with inherent stochasticity, i.e., optimizing the parameters by minimizing a stochastic loss function, yields a robust expectation function that is non-stochastic. In contrast to related methods that introduce noise at the input level, our proposed approach incorporates inherent stochasticity by embedding Gaussian noise within the layers of the NN model at training time. We model the propagation of noise through the layers, introducing a closed-form stochastic loss function that encapsulates a noise variance parameter. Additionally, we contribute a formalized noise-aware gradient, enabling the optimization of model parameters while accounting for stochasticity. Our experimental results confirm that the expectation model of a stochastic architecture trained on benign distribution is adversarially robust. Interestingly, we find that the impact of the applied Gaussian noise\u2019s standard deviation on both robustness and baseline accuracy closely mirrors the impact of the noise magnitude employed in adversarial training. Our work contributes adversarially trained networks using a completely different approach, with empirically similar robustness to adversarial training. We hope that further\nexploration of this alternative may uncover to advantages in terms of both robustness and training time.\n# 1. Introduction\nWhile ML models achieved unprecedented success across a diverse spectrum of applications, including critical domains, their vulnerability to adversarial attacks [1, 2, 8, 15, 19] remains a significant concern. These attacks introduce bounded-magnitude perturbations into the model\u2019s input, that are maliciously tailored to force the output to a wrong label. Particularly, in safety-critical and security-sensitive contexts, these attacks represent a notable threat that undermines system security and safety and erodes the trustworthiness of ML. Several defenses have been proposed against adversarial attacks, which can be classified into heuristic defenses and certified defenses [3, 12, 21]. Heuristic defenses focus on practical effectiveness. The widely used state-of-the-art heuristic defense is adversarial training (AT) [16]. AT enhances the robustness of models intrinsically by exposing it to adversarial examples in the training data. Thus, adversarially trained models maintain integrity under adversarial attacks under a given noise magnitude. Mathematically, AT is formulated as a min-max problem, searching for the best solution to the worst-case optimum. Empirical results highlight the effectiveness of projected gradient descent (PGD) based adversarial training in achieving state-of-the-art accuracy against various L\u221eattacks. Another approach is to provide complete or incomplete verification bounds towards provably robust models. Some of these techniques involve model certification through the formulation of an adversarial polytype and the establishment of its upper bound using convex relaxations [20, 22]. Robustness verification approaches have worst-case exponential time complexity due to the hardness of verification\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dee4/dee40c35-54de-45b9-8188-b6280c7c0c01.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ad1/7ad1de3e-a494-48dc-ac21-7936878f7528.png\" style=\"width: 50%;\"></div>\n[13]. Within the body of certified robustness, we are particularly interested in probabilistic methods such as randomized smoothing based approaches [6, 9, 14]. These approaches involve input or feature transformations, mainly through additive noise [11, 18], to mitigate adversarial effects in the data or feature space. Liu et al. [14] suggest randomizing the entire DNN and predicting using an ensemble of multiple copies of the DNN. Lecuyer et al. [9] also propose adding random noise to the first layer of the DNN and estimating the output using a Monte Carlo simulation. All these approaches inject stochastic noise to the input or the model at inference time and require computational expensive simulations to infer the output which limits their practicality. Importantly, the main reason behind these limitations stems from the lack of analytical modeling for the noisy behavior of models. Motivated by bridging the gap between the two defense categories (i.e., AT and randomized techniques), we propose a new approach which trains adversarially robust models by leveraging random noise at training time. Our intuition is that training a model with inherent stochasticity, i.e., optimizing the parameters by minimizing a stochastic loss function, yields a robust expectation function (non-stochastic model). As illustrated in Figure 1, we are inspired by AT. However, instead of using the noise as an \u2113p-norm ball in the neighborhood of the data samples, we add the noise to the model itself during the training. We propose that training a stochastic model while taking noise into account converges to an expectation function that maximizes the decision boundary distance from the data samples. To do so without expensive Monte-Carlo simulations, we need a closed form of the Loss function, which takes into account the noise distribution. Therefore, we consider a zero-centered Gaussian noise in a layer pre-activation and analytically model its propagation through the NN. Through this propagation, we obtain a closed-form expression of a stochastic loss function, which is parameterized by the standard deviation of the initial noise. Additionally, we express a stochastic Jacobian distribution and back-propagate the noise-aware gradient to update the model parameters. At inference, the model is then inferred as the expectation of the stochastically-trained model. The proposed noise-aware training methodology is illustrated by Figure 2. Our experiments show that the expectation of the trained stochastic model is an adversarially-robust model. Interestingly, we observe the same trend in robustness as we vary the standard deviation (\u03c3) of the training stochasticity. This observation establishes that a higher noise distribution results in higher robustness against adversarial attacks, which draws a parallel with the impact of the training noise magnitude in AT. We believe that this represents a new methodology for adversarial hardening with attractive properties. Contributions. The contributions of this work are summarized as follows:\nFigure 1. Illustration of our intuition, in comparison to AT. While AT fits the model under an \u2113p-norm ball noise around the input samples to distance the decision boundary from the data distribution, we propose to optimize the model\u2019s parameters under stochastic behavior of the model itself. This will guarantee a distance from the expectation of the model and the data distribution.\n\u2022 We propose a new randomization-based technique to train adversarially robust models without min-max optimisation, i.e., without training on adversarial examples. At training time, we consider additive Gaussian noise injected to the first layer of the model with 0-mean and a standard deviation \u03c3. \u2022 To train the model under noise, we propose a closed-form expression of the loss as a function of the propagated noise. We address this problem by handling the non-linearity within the NN layers using Laplace approximation. By propagating the distribution through the layers, we finally integrate it in the expression of our loss function. This allows us to model the output distribution without the need of Monte Carlo simulation. The hardened model achieves similar robustness to AT. \u2022 We express a closed form of the gradient as a function of the noise distribution. Interestingly, this formulation allows the optimization of the noise parameter during training. Therefore, the second contribution of the paper relies on the consideration of the variance as a learnable parameter, rather than a hyper-parameter as it is often assumed in the state-of-the-art. \u2022 We show that using the closed form and gradient of the stochastic model, we can be leveraged to build adaptive attacks against defenses that rely on random noise at inference time.\n\u2022 We propose a new randomization-based technique to train adversarially robust models without min-max optimisation, i.e., without training on adversarial examples. At training time, we consider additive Gaussian noise injected to the first layer of the model with 0-mean and a standard deviation \u03c3. \u2022 To train the model under noise, we propose a closed-form expression of the loss as a function of the propagated noise. We address this problem by handling the non-linearity within the NN layers using Laplace approximation. By propagating the distribution through the layers, we finally integrate it in the expression of our loss function. This allows us to model the output distribution without the need of Monte Carlo simulation. The hardened model achieves similar robustness to AT. \u2022 We express a closed form of the gradient as a function of the noise distribution. Interestingly, this formulation allows the optimization of the noise parameter during training. Therefore, the second contribution of the paper relies on the consideration of the variance as a learnable parameter, rather than a hyper-parameter as it is often assumed in the state-of-the-art. \u2022 We show that using the closed form and gradient of the stochastic model, we can be leveraged to build adaptive attacks against defenses that rely on random noise at inference time.\n# 2. Proposed Approach\nTowards enhancing the robustness of ML models, we propose a novel approach that bridges the gap between adversarial training and stochastic-based defenses. Through the min-max optimization, AT in essence trains the model to fit a decision boundary that is as far as possible from the data distribution, while still providing the correct classification. Aiming for the same objective, our approach is illustrated in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aaa1/aaa17e5b-04a0-4bf8-b1ce-f9b034e444a8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. An overview of our proposed stochastic training methodology.</div>\nFigure 1 and is composed of two main parts: (i) At training time: we consider a model that has a stochastic component by injecting a Gaussian noise pre-activation of the first layer. We propagate the distribution through the model to have a loss function that is parametrized by the noise distribution\u2019s parameters (essentially the standard deviation \u03c3). Minimizing a loss under a stochastic decision boundary can be seen as transposing the AT problem from the data to the model. In fact, training a model with inherent stochasticity means finding the parameters that distances the decision boundary from the training samples. (ii) At inference time: Once the stochastic model converges, the inference is performed with the expectation of the trained model, i.e., withdrawing the Gaussian noise and keeping the model\u2019s parameters. This (deterministic) model has adversarial robustness that is dependant on the noise parameter \u03c3, which is analogous to the relation between robustness in AT and the training noise magnitude. The training problem for a stochastic model can then be expressed as follows:\n(1)\nWhere L\u03c3 is the stochastic loss function, W indicates the parameters of the classifier f(\u00b7), which is parametrized by the noise standard deviation \u03c3, and (x, y) \u223cD represents the training data sampled from a distribution D. The core part that enables our approach lies in modeling the propagation of noise through these layers, culminating in a closed-form stochastic loss function that encapsulates\na noise variance parameter. This departure from the norm not only introduces a layer of inherent stochasticity but also equips the model with the ability to adapt to perturbations in the input space. Additionally, we present a formalized stochastic gradient, enabling the optimization of model parameters while adeptly accounting for the model\u2019s stochasticity. The proposed approach not only enhances the model\u2019s robustness against adversarial attacks but also avoids the inference computational bottleneck associated with traditional stochastic-based defenses. Propagating uncertainties through an NN can be challenging, particularly when dealing with the non-linear layers of the network. The difficulty arises from the fact that the distribution of the network output depends not only on the input distribution but also on the weights of the network, which are typically unknown and need to be learned from data. Moreover, this problem is challenging due to the lack of explicit output modeling that takes the stochastic aspect into account.\n# 2.1. Forward propagation\nTo define the stochastic loss which includes explicitly within its expression the injected noise parameter, we need to propagate this injected noise which follows a Gaussian distribution, through the NN. To do so, we need to understand the transformations within our NN and handle the non-linear ones. Here, we consider convolution neural networks (CNNs), which include affine and linear transformations (convolution and\nfully connected layers) and a non-linear transformation for the activation function. For the former transformations, due to the properties of Gaussian distributions, we can show that the distribution of the outcome after applying them remains Gaussian [17]. In contrast, for the non-linear case, for instance, assuming the ReLU transformation, the outcome is not Gaussian anymore. However, it is still possible to promote Gaussianity by considering a Laplace approximation. Therefore, since Gaussianity is preserved in each layer of the NN model, the output will also be Gaussian where the variance will depend on the noise variance. In the following, for simplicity, we will consider that X \u2208Rd1\u00d7d2 (e.g. a one-channel 2D input image) but intuitions can be generalized to 3D tensor objects with a more cumbersome notation.\nConvolution layer. The convolutional transformation fW ,b(\u00b7), with weight and bias parameters W and b (respectively), is given by:\n# Convolution layer. The convolutional transformation fW ,b(\u00b7), with weight and bias parameters W and b (respectively), is given by:\nfW ,B(X) = X \u2217W + B,\n(2)\nwhere \u2217is the convolution operator. Here, W is an m1 \u00d7m2 matrix corresponding to the convolutional filter. The size of the matrix B will depend on the 2D convolution. Here, we assume that B is an p1 \u00d7 p2 matrix. The 2D convolution in (2) can be generally written in the following matrix form:\nwhere x := vect(X) \u2208Rd1\u00b7d2 and b := vect(B) \u2208Rp1\u00b7p2. The matrix A \u2208R(d1\u00b7d2)\u00d7(p1\u00b7p2) is constructed from W taking into account the multiplications involved in the 2D convolution. We seek to inject an additive Gaussian perturbation \u03bd \u223c N(0, \u03c32I) at the first layer of the convolutional architecture, i.e. Z = yconv + \u03bd. Because of the linearity, we can show that Z is also Gaussian-distributed:\n(3)\nwith \u00b5z = A\u22a4x + b. We observe that Z has elements Zi \u223cN(\u00b5i, \u03c32), for i = 1, . . . , p with p = p1 \u00b7 p2, where \u00b5i = a\u22a4 i x + bi with ai the i-th column of the matrix A. We must remark that a fully connected layer is also expressed by an affine transformation. Therefore, the aforementioned formulation can also be used for establishing the Gaussian distribution of the outcome.\nReLU Transformation. Now, our interest is to determine the distribution of the outcome after applying the ReLU transformation, i.e. the distribution of Yi = ReLU(Zi) := max(0, Zi), for i = 1, . . . , p. To simplify our analysis, we\ndrop the index i in the following, i.e. we consider Y = ReLU(Z) with Z \u223cN(\u00b5, \u03c32). Due to the non-linearity, y is not Gaussian but truncated Gaussian distributed with probability density function (pdf) given by [5]\n(4)\nwhere \u03c6 (\u03be) = 1 \u221a 2\u03c0 exp \ufffd \u2212\u03be2 2 \ufffd is the pdf of the standard normal distribution, and \u03b7 = 1\u2212\u03a6 \ufffd \u2212\u00b5 \u03c3 \ufffd with \u03a6(\u03be) = 1 2[1+ erf(\u03be/ \u221a 2)] the cumulative distribution function (cdf) and erf(\u00b7) the Gaussian error function. The indicator function 1\u03be>0 is equal to one if \u03be > 0 and zero otherwise. From the normalizing constant \u03b7, we can observe that: \u2022 \u03b7 \u2212\u2212\u2212\u2212\u2212\u2192 \u00b5\u2192+\u221e1, implying that fy (\u03be; \u00b5, \u03c3) tends to an untruncated Gaussian distribution. \u2022 \u03b7 \u2212\u2212\u2212\u2212\u2212\u2192 \u00b5\u2192\u2212\u221e 0, implying that fy (\u03be; \u00b5, \u03c3) \u2192\u03b4(0) (as a result of censoring the negative values) [4], which can be approximated by an untruncated Gaussian distribution with \u00b5 = 0 and \u03c32 \u21920. Numerically, these cases are exhibited when \u00b5 > 3\u03c3 or \u00b5 < \u22123\u03c3, respectively. For the case when |\u00b5| \u22643\u03c3, we consider a Laplace approximation to promote Gaussianity. Note that the interval [\u22123\u03c3, +3\u03c3] is also justified by the 99.7% coverage of the Gaussian distribution. Figure 3 illustrates the censoring effect of the ReLU function. To be able to apply a Laplace approximation on the pdf in (4), we need to focus only on the positive region. This will ensure continuity when considering the Taylor expansion. Suppose h(\u03be) = ln(fY (\u03be; \u00b5, \u03c3)). The second-order Taylor expansion around the maximum of h(\u03be0) is given by\n[1+\nwhere h\u2032\u2032(\u03be) = \u2212\u03c3\u22122. Then,\n(5)\nFor the mean \u03be0, we consider the mode of the pdf in (4) (see Figure 3 for an illustration):\n\u03be0 = arg max \u03be fY (\u03be; \u00b5, \u03c32) = \ufffd 0, if \u22123\u03c3 \u2264\u00b5 < 0, \u00b5, if 0 \u2264\u00b5 \u22643\u03c3.\nObserve that (5) has a closed-form which depends on the noise variance \u03c32 of the additive Gaussian perturbation z. By denoting yReLU the vector composed by the outcomes after applying the ReLU transformation, we can establish the distribution:\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5837/58376171-bb06-4b7a-a68e-d52a8ce5c914.png\" style=\"width: 50%;\"></div>\nwhere \u00b5ReLU is the mean vector with elements \u00b5ReLU,i = \u00b5i if \u00b5i \u22650, and zero otherwise. The variances \u03c32 i are equal to \u03c32 (noise variance) if \u00b5i \u2265\u22123\u03c3, and close to zero otherwise. For the next convolutional layer, we can follow the same procedure but considering the distribution in (6) rather than the one in (3). As a result, we will be able to propagate noise sequentially across the NN. We must remark that, while the weights of the NN model interplay a key role in the definition of the mean of the output distribution (see interaction in (3)), the variances will only depend on the noise variance \u03c32.\nMaxpooling transformation. Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarizes the features present in a region of the feature map generated by a convolution layer. To do so, we conserve features of the pooling layer by taking the maximum of the mean value in each region which we call the mean-pooling layer.\n# 2.2. Stochastic loss function\nThe injection of Gaussian noise in the NN motivates the construction of a loss function that can take into account the stochasticity in the model. For this reason, we seek to provide a loss function that jointly enhances the model\u2019s accuracy in the training phase and increases the score of predicting the true label. Let Y \u223cN((\u00b51, . . . , \u00b5n)), diag(\u03c32 1, . . . , \u03c32 n)) be the Gaussian distribution of the NN output, with n \u2208N corresponding to the number of neurons at the latest layer (i.e. the number of classes). Assume that k is the true label for the specific task prediction. Then, we define the stochastic loss function as:\nLoss = 1 n n \ufffd i=1 \ufffd y(i) \u2212\u00b5i \ufffd2 \u2212 n \ufffd i=1 i\u0338=k 1 n \u22121P (Yk > Yi) ,\nwhere, for i = 1, . . . , n, y(i) and \u00b5(i) are the ground truth label and the mean of the i-th neuron. Note that the loss is decomposed into two parts. The first term, which corresponds to the mean squared error (MSE), seeks to improve the accuracy of the model (i.e. the accuracy of the mean as a predictor). On the other hand, the second term seeks to maximize the probability of predicting the true label, which will also enhance the accuracy of the model. For the case of multivariate Gaussians, i.e. \ufffdX1, X2 \ufffd\u22a4\u223c N (\u00b5, \u03a3) with mean vector \u00b5 = \ufffd\u00b51, \u00b52 \ufffd\u22a4and covariance matrix (\u03a3)1\u2264i,j\u22642 = \u03c32 1,2, we have that\nP(X1 > X2) = CDFX2\u2212X1(0) \ufffd \ufffd\n\ufffd \ufffd \ufffd In our case, we need to consider the case where X1 and X2 are independent (see (6)). Therefore, using the aforementioned property for computing the probabilities P (Yi > Yk), and taking into account the independence between Y1, . . . , Yn, the stochastic loss can be written as:\nwith MSE = 1 n \ufffdn i=1(y(i) \u2212\u00b5i)2. We should note that our objective function involves the parameters of the Gaussian vector Y which have been previously computed in the forward propagation of the noise. As discussed in Section 2.1, the means \u00b51, . . . , \u00b5n will depend on the weights and biases of the NN, and the variances \u03c32 1, . . . , \u03c32 n will depend only on the noise variance \u03c32. Therefore, the stochastic loss can be written as a function of W , B and \u03c3, i.e. Loss(W , B, \u03c3), which allows establishing the optimization problem:\n# (W \u22c6, B\u22c6) = arg min W ,B Loss(W , B, \u03c3).\n(7)\nRemark. It is worth mentionning that the expressed loss\n# function also allows to define the follwing optimisation problem:\nfunction also allows to define the follwing optimisation problem:\n(W \u22c6, B\u22c6, \u03c3\u22c6) = arg min W ,B,\u03c3 Loss(W , B, \u03c3).\n(8)\nInterestingly, this formulation make the injected noise variance \u03c32 a learnable parameter that can be jointly optimised with the NN parameters. The optimization problem in (8) can be seen as an increase in the dimensionality of the (deterministic) NN model by adding a new dimension to the hyperspace. Further details and experiments about this perspective can be found in Section 7 of the supplementary material.\n# 2.3. Backward propagation of the noise-aware gradient\nThe next step is to find the partial derivatives of the stochastic loss function to enable the training by gradient backpropagation. To do so, need to find the stochastic Jacobian to find the noise-aware updates of the parameters. More precisely we are going to use the chain rule to compute the \u03c3-parametrised gradient.\n# Backpropagation through the last layer\nwe have a last fully connected layer with n neurons, weights W, inputs noise \u03c3out\u22121 and output noise \u03c3out , we have the expression of the loss after propagating the noise distribution, which is a function of \u03c3out. Computing its derivative gives us the final expression of \u2202Loss \u2202W which will be if we have kth neuron is the true label:\nIt is worth noticing that the gradient over the parameters is dependent on the standard deviation of the layer itself, which is the result of propagating the initial noise. Once we have the gradient over the last layer\u2019s parameters, the remaining process is a standard backpropagation via chain rule.\nBackpropagation through convolution layers. For the convolution layer, the calculation of the gradient of the output of the convolution with respect to its input is calculated as follows:\nLet consider the lth layer of the convolution, x with dimension H \u2217W, a filter w with dimensions k1 \u2217k2, bl is the bias, f is the activation function of the lth layer. Denote ol i,j = f \ufffd xl i,j \ufffd and \u03b4l i,j = \u2202Loss \u2202xl i,j . The backpropagation\n\ufffd \ufffd equations are as follows:\nFor the fully connected layer, this is also straightforward due to the linearity of the operation.\n# Backpropagation of the maxpooling and activation func-\nBackpropagation of the maxpooling and activation function. For the pooling layer, the process is the same as the conventional back-propagation the gradient. The gradient is only considered at the maximum of each region and then we are performing a padding task at each point to reshape the matrices and return to the same dimension at the input of this layer. The backpropagation through ReLU issimilar to conventional models.\n# 3. Empirical Evaluation\n# 3.1. Setup\n\ufffd We conducted experiments on both MNIST and CIFAR-10 datasets to empirically evaluate the performance of our methods. For MNIST dataset, we train a Lenet-5 (3 Convolution layers and 2 fully connected layers), with ReLU activation functions and max-pooling layers. Additionally, there were two fully connected layers, each with a size of 200. For CIFAR-10, we trained a CNN with five convolutional layers, each followed by ReLU and MaxPooling layers, and three fully connected layers. We proceeded with classification and accuracy measurement across various fixed standard deviation values, considering both forward and backward passes while accounting for the model\u2019s stochasticity within the weights optimization. We evaluate the model\u2019s robustness using Projected Gradient Descent (PGD) attack [15], as a state-of-the-art attack. Other results using FGSM can be found in the supplementary material.\n# 3.2. Results\nImpact on robustness. Figure 4 shows the adversarial robustness of the stochastically trained model with different \u03c3 levels comparatively with the baseline model for MNIST under PGD attack. 4 shows that higher \u03c3 results in more\nrobustness to adversarial noise. The same trend has been obseved in Figure 5 which depicts the adversarial robustness of the stochastically trained model comparatively with the baseline model for CIFAR10 under PGD attack.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/91b2/91b24714-2553-4ed6-8b2e-8adb167ee66e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d829/d8294260-d96a-42d4-834c-3fbc3dd7ee7c.png\" style=\"width: 50%;\"></div>\nFigure 4. Adversarial robustness of the stochastically trained model comparatively with the baseline model for MNIST under PGD attack.\n<div style=\"text-align: center;\">Figure 4. Adversarial robustness of the stochastically trained model comparatively with the baseline model for MNIST under PGD attack.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/71c3/71c33799-91a0-4dbb-b86f-8c921e31dfdf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5. Adversarial robustness of the stochastically trained model comparatively with the baseline model for CIFAR10 under PGD attack.</div>\nImpact on baseline accuracy\u2013 In this experiment we wanted to investigate the impact of the noise magnitude at training time on the baseline accuracy of the model. We compare these results with the (well known) impact of adversarial noise budget on AT on the model\u2019s accuracy. The results are shown in Figure 6, which illustrates a decline in baseline accuracy as the noise level increases, a trend consistently observed during adversarial training accordingly for adversarial noise. While the objective of this experiment is\nnot to quantitatively compare AT and stochastic training, it draws an interesting parallel which confirms the analogy we illustrated in Figure 1.\nFigure 6. Baseline accuracy of models trained with our approach while varying the standard deviation of the noise (top); and adversarially trained model while varying the adversarial noise budget used for AT (bottom) for MNIST.\n# 4. Adaptive attacks against Inference Time Randomization Techniques\nIn this section, we investigate if the stochasticity-aware loss function and gradient can be used to build adaptive attacks against defense strategies that use randomness at inference time as a defense. Given a randomized model \ufffdf\u03c3,W (\u00b7), which injects random noise to the first layer such as PixelDP [10] at inference time, our objective is to generate adversarial noise under a white-box setting; The attacker is assumed to have total access to the model\u2019s architecture, the parameters as well as to the defender\u2019s noise. We assume the attacker has access to the closed form of the stochastic loss and its gradient. The attacker implements a backpropagation of the gradient. The problem is therefore formalised as follows:\n(9)\n \ufffd The loss function contains the noise parameter \u03c3, and therefore, the adversarial example can be generated as follows:\n(10)\n\ufffd  \ufffd \ufffd To back-propagate the gradient to the input to implement the method described in Equation 10, the noise-aware gradient of the loss with respect to the input is expressed as follows:\nwhere for all i = 1, . . . n,\nif i = k,\n\ufffd \ufffd with \u03b1i = 2 n(\u00b5i \u2212y(i)) and c = (n \u22121) \u221a 2\u03c0.\nThis attack was carried out considering two different levels of noise, specifically \u03c3 = 0.6 and \u03c3 = 0.8 with one backward pass (FGSM method). Interestingly, the results depicted in Figure 7 revealed a vulnerability of the stochastic model to the adaptive attack. For \u03c3 = 0.6, the model is almost as vulnerable as a non-protected model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8702/8702ef79-2c0e-4d66-b976-dcf1f349faf7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7. Effect of the Adaptive attack on a inference-time randomised model for MNIST dataset</div>\n# 5. Related Work\nSeveral defense mechanisms were proposed to defend against adversarial attacks, we mainly distinguish: Adversarial Training (AT). AT is one of the most explored defenses against adversarial attacks. The main idea can be traced back to [8], in which models were hardened by including adversarial examples in the training data set of the model. Nonetheless, AT is much more computationally intensive than training a model on the training data set only because generating evasive samples needs more computation.\nRandomization-based Defenses. These techniques use random noise at inference time to defend against adversarial\nattacks [7, 9, 14]. Liu et al. [14] suggest to randomize the entire DNN and predict using an ensemble of multiple copies of the DNN. Lecuyer et al. [9] also suggest to add random noise to the first layer of the DNN and estimate the output by a Monte Carlo simulation. From a practical perspective, it is challenging for these works to scale and are limited with the necessity of MC simulation at inference time.\n# 6. Discussion and concluding remarks\nIn this paper, we propose a new approach to train adversarially robust models without the need of generating adversarial samples. Our proposition is based on a hybridation between adversarial training on the one hand, and randomization defenses on the other hand. In fact, while AT trains the model under an \u2113p-norm ball noise around the input samples to distance the decision boundary from the data distribution, we propose to optimize the model\u2019s parameters under stochastic behavior of the model itself to obtain the same objective. To enable noise-aware training, we derived a closed form loss function that encapsulates the noise distribution propagated through the model. Additionally, we formulated a noise-aware gradient, which backpropagated to update the model\u2019s parameters. Once the model is trained, we tested the expectation model, i.e., without noise, at inference time. We evaluate the model\u2019s accuracy under various adversarial attacks such as FGSM and PGD. Our experiments confirmed our initial intuition and showed that the proposed method trained robust models without adversarial examples, and without accuracy drop compared to baseline vanilla models. Interestingly, we also show that the proposed stochastic loss function can be used to generate efficient adversarial attacks against inference-time randomization based defenses. One potential limitation of this approach is that it may require more computational resources than conventional training, as it involves optimizing an additional parameter and approximations. However, in contradiction to existing randmization techniques, the inference is deterministic and that the benefits in terms of robustness are significant. Overall, the results of our study suggest that incorporating the noise variance as a parameter in the neural network can be an effective defense mechanism against adversarial attacks. Another finding we provide in the supplementary material suggests that the parameters of the injected noise within the model is also a learnable parameter that can be integrated in the model training. In fact, instead of fixing \u03c3, we consider it as a parameter of the model and we update it in the training process. Interestingly, the model did not converge to a deterministic model (\u03c3 = 0), but rather to an \"optimally stochastic\" model. More details can be found in the supplementary materials. Further research is needed to explore the full potential of this approach and its applicability to different types of neural\n# References\n[1] Ihsen Alouani. On the Challenge of Hardware Errors, Adversarial Attacks and Privacy Leakage for Embedded Machine Learning, pages 497\u2013517. Springer Nature Switzerland, Cham, 2024. 1 [2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, pages 274\u2013 283. PMLR, 2018. 1 [3] Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. Recent advances in adversarial training for adversarial robustness. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4312\u20134321. International Joint Conferences on Artificial Intelligence Organization, 2021. Survey Track. 1 [4] Maxime Beauchamp. On numerical computation for the distribution of the convolution of N independent rectified Gaussian variables. Journal de la Soci\u00e9t\u00e9 Fran\u00e7aise de Statistique, 159 (1):88\u2013111, 2018. 4 [5] Zdravko Botev and Pierre L\u2019Ecuyer. Simulation from the tail of the univariate and multivariate normal distribution, pages 115\u2013132. Springer International Publishing, Cham, 2019. 4 [6] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pages 1310\u20131320. PMLR, 2019. 2 [7] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In Proceedings of the 36th International Conference on Machine Learning, pages 1310\u20131320, Long Beach, California, USA, 2019. PMLR. 8 [8] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples, 2014. 1, 8 [9] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certified robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pages 656\u2013672, 2019. 2, 8 10] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2019. 7 11] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. Advances in Neural Information Processing Systems, 32, 2019. 2 12] L. Li, Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. Sok: Certified robustness for deep neural networks. 2023 IEEE Symposium on Security and Privacy (SP), pages 1289\u20131310, 2020. 1 13] L. Li, T. Xie, and B. Li. Sok: Certified robustness for deep neural networks. In 2023 IEEE Symposium on Security and Privacy (SP), pages 1289\u20131310, Los Alamitos, CA, USA, 2023. IEEE Computer Society. 2\n[14] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via random selfensemble, 2017. 2, 8 [15] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks, 2017. 1, 6 [16] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks, 2019. 1 [17] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning (adaptive computation and machine learning). The MIT Press, Cambridge, MA, 2005. 4 [18] Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer. Ensemble methods as a defense to adversarial perturbations against deep neural networks. arXiv preprint arXiv:1709.03423, 2017. 2 [19] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses, 2020. 1 [20] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. In Advances in Neural Information Processing Systems, pages 29909\u201329921. Curran Associates, Inc., 2021. 1 [21] Mingfu Xue, Chengxiang Yuan, Heyi Wu, Yushu Zhang, and Weiqiang Liu. Machine learning security: Threats, countermeasures, and evaluations. IEEE Access, 8:74720\u201374742, 2020. 1 [22] Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. General cutting planes for bound-propagation-based neural network verification. In Advances in Neural Information Processing Systems, pages 1656\u20131670. Curran Associates, Inc., 2022. 1\n# May the Noise be with you: Adversarial Training without Adversarial Examples\nSupplementary Material\n# 7. Can we optimize the noise as a parameter?\nThe core paper is interested in stochastically training ML models under a fixed noise parameter (standard deviation), and inferring the trained model in a deterministic fashion (by taking the expectation, i.e., \u03c3inference = 0). In this section, we explore the following question:\n# Q What would be the model\u2019s behavior if we consider \u03c3 as a learnable parameter?\nMore specifically, if we train the model under noise, while updating the noise parameter during training the same as weights and biases, we want to investigate the correctness of the following hypothesis:\nHypothesis \u2013 H: \"If we train the model while optimizing \u03c3, it converges to a deterministic model, i.e., finds that the minimization of the stochastic loss systematically converges to a \u03c3 = 0\"\nA way of conceptualizing this experiment is that we are expanding the dimensionality of the problem by introducing noise as a new dimension of the parameters\u2019 space. To investigate H, we train the Lenet-5 model under noise, while initializing \u03c3 randomly (we did not witness any specific difference made by the initialization). The update of \u03c3 is simply made by chain rule to find \u2202Loss \u2202\u03c3 . This will allow us to converge to the optimal values of parameters including noise standard deviation. If \u03c3 converges to 0 than H is verified. In a another subsequent setting, we update the loss function such that we minimize the stochastic loss under maximization of the noise itself. This setting is to explore the the maximum allowable noise while training the model. In this scenario, the expression of the loss function will be as follows:\n# Loss = MSE \u2212 n \ufffd i=1 i\u0338=k 1 n \u22121P (Yk > Yi) \u2212\u03b1\u03c32,\nWhile we use the previously expressed closed form of the stochastic loss and add \u03c32 multiplied by an empirical regularization factor \u03b1. In this analysis, we used \u03b1 = 0.25 for the Bimodel. The results are presented in Table 1, where \"Bimodel\" denotes the model with the maximization objective and \"Model\" refers to the stochastic model without maximization\nof the noise, \u03c30 is the initialisation and \u03c3\u2217is the value of noise standard deviation that the trained model converged to. Interestingly, even without maximisation of the noise, we noticed that the model converges to a non-zero \u03c3 value, which refutes the Hypothesis H.\nModel\n\u03c30\n\u03c3\u22c6\nModel\n1.9\n0.7\nBimodel\n1.9\n0.79\nTable 1. Values of converged noise: BiModel denote the model with the maximization objective and Model denote the one without any maximization objective.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the persistent vulnerability of machine learning models to adversarial attacks, highlighting the limitations of existing methods like adversarial training and the need for a new approach that does not rely on adversarial examples.",
        "problem": {
            "definition": "The issue at hand is the inherent susceptibility of machine learning models to adversarial attacks, which can mislead models into making incorrect predictions by introducing small, malicious perturbations to the input data.",
            "key obstacle": "The primary challenge is that existing defenses, such as adversarial training, require training on adversarial examples, which can be computationally intensive and may not generalize well to all types of attacks."
        },
        "idea": {
            "intuition": "The authors propose that incorporating inherent stochasticity into the training process can yield models that are robust to adversarial attacks without the need for adversarial examples.",
            "opinion": "The proposed method involves embedding Gaussian noise within the layers of the neural network during training, thereby creating a stochastic model that can better handle adversarial perturbations.",
            "innovation": "This approach differs from traditional adversarial training by not requiring adversarial examples, instead utilizing a closed-form stochastic loss function that accounts for noise propagation through the network."
        },
        "method": {
            "method name": "Noise-Aware Stochastic Training",
            "method abbreviation": "NAST",
            "method definition": "A training methodology that integrates Gaussian noise within the neural network layers to enhance adversarial robustness without the need for adversarial examples.",
            "method description": "The core of the method involves optimizing a stochastic loss function that incorporates noise, allowing the model to learn robust decision boundaries.",
            "method steps": [
                "Inject Gaussian noise into the first layer of the model during training.",
                "Propagate the noise through the network to derive a closed-form stochastic loss function.",
                "Optimize the model parameters using the noise-aware gradient derived from the stochastic loss."
            ],
            "principle": "The effectiveness of this method lies in its ability to model the propagation of noise through the network, which helps the model adapt to adversarial perturbations and maintain robustness."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on the MNIST and CIFAR-10 datasets using convolutional neural networks and evaluating their performance under Projected Gradient Descent (PGD) attacks.",
            "evaluation method": "The performance of the proposed method was assessed by comparing the adversarial robustness and accuracy of the models trained with noise against baseline models."
        },
        "conclusion": "The experimental results demonstrate that the proposed noise-aware stochastic training method yields models that are robust to adversarial attacks, achieving comparable performance to traditional adversarial training without the need for adversarial examples.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to train robust models without the computational burden of generating adversarial examples, while still maintaining high accuracy.",
            "limitation": "A potential limitation is the increased computational resources required for optimizing the additional noise parameter during training.",
            "future work": "Future research should explore the applicability of this method to different neural network architectures and its potential for further improving robustness against a wider range of adversarial attacks."
        },
        "other info": {
            "info1": "The noise standard deviation is treated as a learnable parameter, allowing it to be optimized alongside the model parameters.",
            "info2": {
                "info2.1": "The proposed method achieved similar robustness to adversarial training while reducing the reliance on adversarial examples.",
                "info2.2": "Further exploration is needed to understand the full implications of using noise as a learnable parameter in various contexts."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the persistent vulnerability of machine learning models to adversarial attacks, highlighting the limitations of existing methods like adversarial training and the need for a new approach that does not rely on adversarial examples."
        },
        {
            "section number": "2.4",
            "key information": "The issue at hand is the inherent susceptibility of machine learning models to adversarial attacks, which can mislead models into making incorrect predictions by introducing small, malicious perturbations to the input data."
        },
        {
            "section number": "3.4",
            "key information": "The proposed method involves embedding Gaussian noise within the layers of the neural network during training, thereby creating a stochastic model that can better handle adversarial perturbations."
        },
        {
            "section number": "5.2",
            "key information": "The key advantage of the proposed approach is its ability to train robust models without the computational burden of generating adversarial examples, while still maintaining high accuracy."
        },
        {
            "section number": "7.1",
            "key information": "The primary challenge is that existing defenses, such as adversarial training, require training on adversarial examples, which can be computationally intensive and may not generalize well to all types of attacks."
        },
        {
            "section number": "7.2",
            "key information": "Future research should explore the applicability of this method to different neural network architectures and its potential for further improving robustness against a wider range of adversarial attacks."
        }
    ],
    "similarity_score": 0.7038154385731188,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/May the Noise be with you_ Adversarial Training without Adversarial Examples.json"
}