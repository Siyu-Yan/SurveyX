{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.10825",
    "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities",
    "abstract": "Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.",
    "bib_name": "zhou2024largelanguagemodelllm",
    "md_text": "# Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities\nHao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun Wu, Dun Yuan, Li Jiang, Di Wu, Xue Liu, Fellow, IEEE, rlie Zhang, Fellow, IEEE, Xianbin Wang, Fellow, IEEE, Jiangchuan Liu, Fellow, IEEE.\nAbstract\u2014Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLMbased classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks. Index Terms\u2014Large language model, telecommunications, generation, classification, prediction, optimization.\n# I. INTRODUCTION\nWhile 5G networks have entered the commercial deployment stage, the academic community has started the exploration of envisioned 6G networks. In particular, 6G networks are expected to achieve terabits per second (Tbps) level data\nment stage, the academic community has started the exploration of envisioned 6G networks. In particular, 6G networks are expected to achieve terabits per second (Tbps) level data Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Can Chen, Yili Jin, Haolun Wu, Dun Yuan, Li Jiang, and Xue Liu are with the School of Computer Science, McGill University, Montreal, QC H3A 0E9, Canada. (emails:{hao.zhou4, chengming.hu, ye.yuan3, yufei.cui, can.chen, yili.jin, haolun.wu, dun.yuan, li.jiang3}@mail.mcgill.ca, xueliu@cs.mcgill.ca); Di Wu is with the School of Electrical and Computer Engineering, McGill University, Montreal, QC H3A 0E9, Canada. (email: di.wu5@mcgill.ca); Charlie Zhang is with Samsung Research America, Plano, Texas, TX 75023, USA. (email: jianzhong.z@samsung.com); Xiangbin Wang is with the Department of Electrical and Computer Engineering, Western University, London, ON N6A 3K7, Canada. (e-mail: xianbin.wang@uwo.ca); Jiangchuan Liu is with the School of Computing Science, Simon Fraser University, Burnaby, BC V5A 1S6, Canada. (e-mail: jcliu@sfu.ca).\nrates, 107/km2 connection densities, and lower than 0.1 ms latency [1]. To achieve these goals, the International Telecommunication Union (ITU) has defined six key use cases for envisioned 6G networks [2]. Specifically, three cases are extensions of IMT-2020 (5G), namely immersive communication, hyper-reliable and low-latency communication, and massive communication, and the other three novel usage cases are ubiquitous connectivity, integrated sensing and communication, and AI and communication. These novel techniques have shown satisfactory performance towards 6G requirements, but the complexity of network management also significantly increased. From 3G, 4G LTE to 5G and envisioned 6G networks, telecommunication (telecom) networks have become a complicated large-scale system, including core networks, transport networks, network edge, and radio access networks [3]. Moreover, 6G ubiquitous connectivity aims to address presently uncovered areas, e.g., rural and sparsely populated areas, by integrating other access systems such as satellite communications. In addition, 6G integrated sensing and communication is designed to improve applications requiring sensing capabilities, i.e., assisted navigation, activity detection, and environmental monitoring. Despite the potential benefits, such highly integrated network architecture and functions may lead to a huge burden on 6G network management, including network configuration and troubleshooting, product design and coding, standard specification development, performance optimization and prediction, etc. To handle such complexity, machine learning (ML) has become one of the most promising solutions, and there have been a large number of studies on artificial intelligence (AI)/MLenabled wireless networks, e.g., reinforcement learning-based network management [4], deep neural network-enabled channel state information (CSI) prediction [5], and federated learning for distributed model training in wireless environments [6]. For example, convex optimization has been applied to optimize network performance, but it requires problem-specific transformation for convexity. By contrast, reinforcement learning will transform the problem into a unified Markov decision process (MDP), and then interact with the environment to explore optimal policies. Compared with conventional optimization algorithms [7], reinforcement learning overcomes the complexity of dedicated problem reformulation, and can better\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/89ba/89bad7be-c247-42cf-addd-1905ee18f60a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Organization and key topics covered in this work.</div>\nhandle environmental uncertainties, e.g., the growing diversity of user preferences, and more distributed and heterogeneous resources in future telecom networks. These studies have demonstrated the importance of incorporating ML to improve the efficiency, reliability, and quality of telecom services. Recently, large language model (LLM) techniques have attracted considerable interest from both academia and industry. Unlike previous ML algorithms, these large-scale models with\nhandle environmental uncertainties, e.g., the growing diversity of user preferences, and more distributed and heterogeneous resources in future telecom networks. These studies have demonstrated the importance of incorporating ML to improve the efficiency, reliability, and quality of telecom services.\nRecently, large language model (LLM) techniques have attracted considerable interest from both academia and industry. Unlike previous ML algorithms, these large-scale models with\na huge amount of parameters have shown versatile comprehension and reasoning capabilities in various fields such as health care [8], law [9], finance [10], education, and so on [11]. For instance, Wu et al. introduced a BloombergGPT model that is trained on a wide range of financial data with 50 billion parameters, and the Med-PaLM2 developed by Google achieves 86.5% correct rate on the medical question answering dataset [8]. LLM technologies have many promising fea-\ntures such as in-context learning (ICL), step-by-step learning, and instruction following [12]. Existing studies have shown that LLMs can answer telecom-domain questions, generate troubleshooting reports, develop project code, and configure networks, which will significantly lower the difficulty of 6G ubiquitous connectivity management. Meanwhile, for 6G integrated sensing and communication, LLMs can understand and process multi-modal data, e.g., text, satellite or street camera images, 3D LiDAR maps and videos. It provides a promising approach to simulate and understand the 3D wireless signal transmission environment. Despite the great potential, LLM\u2019s real-world application is still at a very early stage, especially for domain-specific scenarios. For instance, telecom is a broad field that includes various knowledge domains, e.g., signal transmissions, protocols, network architectures, devices, and different standards. LLM is expected to properly understand and generate content that aligns with real-world details and specific requirements of telecom applications [13]. However, such specific telecomrelated requirements are rare in the existing knowledge base of general-domain LLMs. Therefore, applying a general-domain LLM directly to telecom tasks may lead to poor performance. Meanwhile, fine-tuning LLMs on telecom datasets may improve LLM\u2019s performance of domain-specific tasks, but the telecom-specific dataset collection and filtering still require careful design and evaluation. In addition, many telecom tasks require multi-step planning and thinking, e.g., a simple coding task can include multiple steps, indicating dedicated prompting and analyses from the telecom perspective [14]. Given the above opportunities and challenges, this work presents a comprehensive survey of LLM-enabled telecom networks. Different from existing studies that focus on one specific aspect such as edge intelligence [15], [16], grounding and alignment [17], this work provides a comprehensive survey on fundamentals, key techniques, and applications of LLM-enabled telecom. To be specific, this work focuses on generative models that were originally developed for language tasks, i.e., language models, and it also involves more diverse techniques and broad application scenarios such as optimization and prediction problems. In this survey, the term \u201cfoundation models\u201d refers to models specifically developed from scratch for applications that are beyond pure language-related tasks, such as the prediction foundation models in Section VIIB, while \u201cLLM-enabled\u201d or \u201cLLM-aided\u201d approaches denote methods that repurpose existing pre-trained language models for telecom tasks. Moreover, when referring to LLMs, it means that the inputs to the model are purely text, and the model generates purely text as outputs, even if the model can accept inputs in other modalities, such as GPT-4V and GPT-4o. When discussing the multi-modal inputs, we explicitly describe them as multi-modal large language models or multi-modal LLMs. Although LLM development is originally motivated by natural language tasks, it is worth noting that there have been diverse state-of-the-art explorations that are beyond the conventional language processing tasks, e.g., coding and debugging [18], recommendation [19], LLM-enabled agents\n[20], instruction-based optimization [21], network time-series prediction and decision making [22], etc. These LLM-inspired techniques have become crucial pillars of LLM studies, and exploring these techniques is crucial to take full advantage of LLM capabilities. Fig.1 presents the organization of this work, in which the left side indicates the telecom scenarios and demand, and the right side shows the LLM-enabled techniques. To better present the detailed application scenarios, the bottom of Fig.1 shows telecom environments that include radio access networks, network edge, central cloud, and other network elements such as regular users, malicious users, mmWave beam, environment image sensing, RISs, backhaul traffic, etc. Meanwhile, we categorize key telecom applications into generation, classification, optimization, and prediction problems to better distinguish different scenarios and customized designs1 In particular, we focus on the following topics: 1) LLM fundamentals: Understanding LLM fundamentals is the prerequisite for developing advanced applications in telecom networks [11]. Compared with existing studies [15]\u2013 [17], this work presents a more comprehensive overview of the model architecture, pre-training, fine-tuning, inference and utilization, and evaluation. Additionally, it presents different approaches to deploy LLMs in telecom networks, such as central cloud, network edge, and mobile LLM [16]. It further analyzes LLM fundamentals from the telecom application perspective, e.g., training or fine-tuning telecom-specific LLMs, and the importance of prompting and multi-step planning techniques for telecom tasks. 2) LLM for generation problems in telecom: Generating desired content is the most common usage of LLM, and here we investigate the applications to specific telecom scenarios. In particular, it involves answering telecom-domain questions, generating troubleshooting reports, project coding, and network configuration. It shows that LLM\u2019s generation capabilities are particularly useful in text and language-related telecom tasks to save human effort, e.g., automated code refactoring and design [14], recommending troubleshooting solutions [23], and generating network configurations [24]. 3) LLM-based classification for telecom: Classification is a common task in the telecom field, and we present LLMenabled network attack classification and detection, telecom text, image, and traffic classification problems. For instance, there have been many studies on visioned-aided blockage prediction and beamforming for 6G networks [25], and some LLM can provide zero-shot image classification capabilities, overcoming the training difficulties of conventional algorithms in complicated signal transmission environments [26]. 4) LLM-enabled optimization techniques: Optimization techniques are of great importance to telecom networks, e.g., resource allocation and load balancing [7], and LLM offers new opportunities [7]. In particular, we introduce LLM-aided automated reward function design for reinforcement learning,\n1Note that although the classification, optimization, and prediction capabilities are all based on the LLM\u2019s inference and generation capabilities, this organization can significantly reduce the reader\u2019s difficulty in understanding the LLM\u2019s potential for telecom applications.\nverbal reinforcement learning, LLM-enabled black-box optimizer, end-to-end convex optimization, and LLM-aided heuristic algorithm design. For example, reinforcement learning has been widely used for network optimization, but the reward functions are usually manually designed with a trial-and-error approach [27]. LLM can provide automated reward function designs, and such an improvement can significantly promote reinforcement learning applications in the telecom field. 5) LLM-aided prediction in telecom: Prediction techniques are crucial for telecom networks, such as CSI prediction [5], prediction-based beamforming [25], and traffic load prediction [36]. Existing studies have started exploring one-model-for-all time-series models. After pre-training on a large corpus of diverse time-series data, such a model can learn the hidden temporal patterns, and then generalize well across different prediction tasks without extra training. We will first introduce how to pre-train foundation models, and then present frozen pre-trained and fine-tune-based LLMs. In addition, the potential of multi-modal LLM is discussed for telecom prediction tasks. 6) Challenges and future directions: Finally, we identify the challenges and future directions of LLM-empowered telecom. The challenges focus on telecom-domain LLM training, practical LLM deployment, and prompt engineering for telecom applications. The future directions include LLM-enabled planning, model compression and fast inference, overcoming hallucination problems, retrieval augmented-LLM, and economic and affordable LLMs. In summary, the main contribution of this work is that we provide a comprehensive survey of the principles, key techniques, and applications for LLM-enabled telecom networks, ranging from LLM fundamentals to novel LLM-inspired generation, classification, optimization and prediction techniques along with telecom applications. This work covers nearly 20 telecom application scenarios and LLM-inspired novel techniques, aiming to be a roadmap for researchers to use LLMs to solve various telecom tasks. The rest of this paper is organized as Fig. 1. Section II discusses related surveys, and Section III presents LLM fundamentals. Sections IV, V, VI, and VII focus on generation, classification, optimization, and prediction problems and telecom applications, respectively. Finally, Section VIII identifies the challenges and future directions, and Section IX concludes this work.\n# II. RELATED SURVEYS\nTable I compares this work with existing studies [15]\u2013[17], [28]\u2013[35], including LLM fundamental techniques such as pretraining and fine-tuning, and other key topics ranging from question answering to multi-modality. Firstly, Table I shows that most existing studies focus on the fundamental techniques of LLMs, e.g., pre-training LLMs for telecom tasks in general [15], [16], [32]\u2013[35]. LLM deployment is discussed in many existing studies, including central cloud [15], [31], network edge [16], and mobile execution [17]. Due to the storage and computational resources constraint at the network edge, Lin et al. also summarized various techniques in [16] that can be used\nto improve the LLM training efficiency at the network edge, such as parameter-efficient fine-tuning, split edge learning, and quantized training. Meanwhile, researchers have investigated various network application scenarios for LLM and generative AI (GAI), such as integrated satellite-aerial-terrestrial networks [32], secure physical layer communication [37], semantic communication [38], and vehicular networks [39]. For instance, Javaid et al. studied the application of LLMs to integrated satelliteaerial-terrestrial networks, including resource allocation, traffic routing, network optimization, etc [32]. Huang et al. presents a general overview of LLM for networking, involving network design, configuration, and security [35]. Du et al. present a novel concept named \u201cAI-generated everything\u201d, discussing the interactions between (GAI) and different network layers [40]. In addition, sensing has become an important part of future 6G networks, and the multi-modal LLM are discussed in several existing studies, e.g., integrated sensing and communication with LLM [17], [29], multi-modal input to LLMs for intelligent sensing and communication [30], and multi-modal sensing [31]. These studies are very valuable explorations of LLM-enabled telecom networks by focusing on model training and deployment. However, LLM techniques are rapidly progressing and many LLM-inspired novel techniques and applications have been recently proposed. This work is different from existing studies in the following aspects: 1) In terms of LLM fundamentals, we provide comprehensive overviews and analyses, ranging from model architecture and pre-training to LLM evaluation and deployment. For instance, prompt engineering is of great importance for using LLM technology, but some crucial techniques such as chain-of-thought (CoT) [41] and step-by-step planning are not discussed in many existing studies [15]\u2013[17], [28]\u2013[31]. Understanding these prompt design skills is the prerequisite for advanced telecom applications. By contrast, this work provides detailed analyses of chain-of-thought along with telecom applications, e.g., LLM-aided automated wireless project coding with multi-step prompting and thinking [14]. Meanwhile, we also systemically analyzed the features of different LLM deployment strategies in telecom, while existing studies usually involve one single deployment [15]\u2013[17], [31]. 2) In terms of LLM-inspired techniques, this work presents the most state-of-the-art novel algorithms and designs. For instance, reinforcement learning has been widely applied to telecom optimization problems, but the reward function design requires considerable human effort [27]. Existing studies have shown that LLM can be used for automated reward function design, achieving a comparable performance as human manual designs [42]\u2013[44]. Such a technique may bring revolutionary changes to reinforcement learning techniques, which have great potential for telecom applications. In addition, timeseries LLM is also a promising technique for telecom, enabling one-model-for-all prediction [45]. However, these novel techniques are not mentioned in most existing studies. 3) In terms of telecom applications, we systematically summarize various LLM application scenarios, including question\nRef.\nLLM fundamental techniques\nGeneration applications\nClassification applications\nOptimization techniques\nPrediction\ntechniques\nArchit-\necture\nPre-\ntraining\nFine-\ntuning\nInfe-\nrence\nEvalu-\nation\nDeploy-\nment\nQuestion\nanswering\nTroubles-\nhooting\nCoding\nNetwork\nconfig.\nNetwork\nattacks\nText\nImage\nNetwork\ntraffic\nLLM\n-aided RL\nBlack-\nbox\nConvex\nHeuristic\nTime series\nLLM\nMulti-\nmodality\n[15]\n\u2713\n\u2713\n[16]\n\u2713\n\u2713\n[17]\n\u2713\n\u2713\n[28]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n[29]\n\u2713\n\u2713\n\u2713\n[30]\n\u2713\n\u2713\n[31]\n\u2713\n\u2713\n\u2713\n[32]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n[33]\n\u2713\n\u2713\n\u2713\n\u2713\n[34]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n[35]\n\u2713\n\u2713\n\u2713\n\u2713\nOur\nwork\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1 Multi-modality is discussed in several existing studies but not from the prediction perspective. Table I divides key topics from LLM fundamentals to\n Multi-modality is discussed in several existing studies but not from the prediction perspective. Table I divides key topics from LLM fundamentals to optimization and prediction to better align with the organization of our work.\n<div style=\"text-align: center;\">TABLE II SUMMARY OF EXISTING GENERAL AND DOMAIN-SPECIFIC LLMS.</div>\nDomain\nModel\nSize\nPre-train\nLatest Update\nGeneral\nGPT-4-Turbo\n-\n-\nMar 2024\nClaude-3 Opus\n-\n-\nMar 2024\nGemini-1 Ultra\n-\n-\nDec 2023\nMistral-Large\n-\n-\nFeb 2024\nLlama-2\n70B\n10T tokens\nJul 2023\nQwen-1.5\n72B\n-\nFeb 2024\nDeepSeek\n67B\n2T tokens\nJan 2024\nBaichuan-2 Turbo\n13B\n-\nSep 2023\nHealthcare\nMedGPT\n-\n-\nJul 2021\nChatDoctor\n7B\n100K\nJun 2023\nMed-PaLM\n540B\n760B\nDec 2022\nFinance\nfinBERT\n110B\n1.8M\nAug 2019\nFinMA\n30B\n1T tokens\nJun 2023\nBloombergGPT\n50B\n569-770B tokens\nDec 2023\nTime Series\nTabLLM\n3B\n50,000 rows\nMar 2023\nLLMTime\n70B\n-\nOct 2023\nTIME-LLM\n7B\n-\nJan 2024\nAutonomous\ndriving\nDriving with LLMs\n7B\n110k\nOct 2023\nDilu\n-\n-\nFeb 2024\nDriveGPT4\n70B\n112k\nMar 2024\nLaw\nLexiLaw\n6B\n-\nMay 2023\nJurisLMs\n13B\n-\nJuly 2023\nChatLaw\n13B\n980k\nJuly 2023\nRecommen-\ndation\nM6-Rec\n300M\n1G\nMay 2022\nTallRec\n7B\n100 samples\nOct 2023\nAgentCF\n175B\n20k samples\nOct 2023\nanswering, network troubleshooting, coding, network configuration, network attack classification and security, text and image classification, etc. Compared with existing studies, we presented more comprehensive overviews and analyses of using LLM techniques to solve various problems in the telecom domain. For each application, this work provides technical details such as framework, pre-training steps, and prompt designs, which are more informative than existing studies that focus on general system-level designs. Moreover, Table II summarizes various general and domain-\nspecific LLMs, demonstrating that LLMs have received considerable attention across many fields. Researchers have trained various domain-specific LLMs for their application scenarios, including healthcare [46], finance [10], time series [47], autonomous driving [48], and recommendation systems [49], etc. For instance, DriveGPT4 is designed to provide interpretable end-to-end autonomous driving [48]. LLM is also used for the automated design of reward functions in robot control [44], achieving better performance than human manual designs. Thus, given the rapid progress and great potential of LLMs, a comprehensive survey is expected to summarize the latest and potential applications of LLMs in the telecom field. To this end, this work answers such a question: What are the most state-of-the-art techniques inspired by LLMs, and how can these techniques be used to solve telecom domain problems? The answer to this question is crucial for building intelligent next-generation telecom networks.\n# III. LLM FUNDAMENTALS\nThis Section will introduce LLM fundamentals, and the overall organization is shown in Fig.2. It presents a thorough overview of LLM fundamentals, including the model architecture, pre-training, fine-tuning, inference and utilization, and model evaluation. We further discuss how LLMs can be deployed in telecom networks such as central cloud, network edge, and mobile devices. Finally, we analyze LLM fundamentals from the telecom application perspective, e.g., training or fine-tuning LLMs for the telecom domain.\n# A. Model Architecture\nThe fundamental component of contemporary LLMs is the transformer scheme [50], which leverages an attention mechanism to capture global dependencies between inputs and outputs. Transformers process raw inputs by tokenizing them and applying embeddings and positional encodings. The vanilla transformer architecture comprises two main components: the encoder and the decoder. The encoder\u2019s role is to\nextract features and understand the relationships among all input tokens. It employs self-attention, also referred to as bidirectional attention, allowing each token to attend to every other token in both directions. Conversely, the decoder is responsible for producing the output sequence while taking into account the input sequence and previously generated tokens. It initially applies a masked attention mechanism, known as causal attention, ensuring that the current token only attends to previously generated tokens. Additionally, the decoder employs crossattention, where the query comes from the decoder, and the key and value are from the encoder, enabling the decoder to integrate information from both the input sequence and the already generated tokens. Beyond the basic version of the attention mechanism, various variants are developed to capture the different relationships among tokens, such as multi-head attention [50], multi-query attention [51], and grouped-query attention [52]. Current architectures can be classified into three distinct categories: encoder-only architecture, encoder-decoder architecture, and decoder-only architecture. 1) Encoder-only architecture: Models with an encoderonly structure solely comprise an encoder. These models are tailored for language understanding tasks, where they extract language features for downstream applications such as classification. A prominent example is bidirectional encoder representations from transformers (BERT) [53]. BERT is pretrained with two main objectives: the masked language model objective, which aims to reconstruct randomly masked tokens, and the next sentence prediction objective, designed to ascertain if one sentence logically follows another. There have been many variants of this model, such as RoBERTa [54], which enhances the performance on downstream tasks2, and ALBERT [55] that introduces two parameter-reduction techniques to accelerate BERT\u2019s training process. 2) Encoder-decoder architecture: The foundational transformer block employs an encoder-decoder architecture, wherein the encoder relays keys and values generated by its self-attention module to the decoder for cross-attention processing. For example, the study in [56] introduces the text-to-text transfer transformer, a unified framework that reformulates all text-based language tasks into a text-to-text format, thereby facilitating the exploration of transfer learning within natural language processing (NLP). BART is another well-known model with standard transformer architecture [57], which employs a denoising autoencoder approach for pretraining sequence-to-sequence models. It introduces arbitrary noise into text and is trained to reconstruct the original content, effectively combining elements of BERT\u2019s bidirectional encoding and GPT\u2019s causal decoding methodologies. 3) Decoder-only architecture: Decoder-only architectures specialize in unidirectional attention, allowing each output token to attend only to its past tokens and itself. Both prefix and output tokens undergo identical processing within the decoder. Decoders are further distinguished based on their 2Here downstream tasks refer to a series of target tasks that can be solved\n2Here downstream tasks refer to a series of target tasks that can be solved by the pre-trained model, e.g., text classification, natural language inference.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a038/a0389e2c-fe1a-4b2c-a831-10453206cb5c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Organization and key topics of Section III.</div>\nattention mechanisms into causal and non-causal decoders. In causal decoders, every token is restricted to attending to its past tokens and itself; in non-causal decoders, prefix tokens can attend to all tokens within the prefix. Causal decoders are predominantly adopted in popular LLMs, such as the GPT series [58], PaLM [59], and LLaMA [60]. Noncausal decoders [61] resemble encoder-decoder frameworks in their ability to bidirectionally process the prefix sequence and autoregressively generate output tokens sequentially. Note that LLM is a complicated system, and there are multiple approaches to apply LLMs to the telecom field, ranging from pre-training and fine-tuning, to prompting. For instance, pre-training an LLM from scratch by using telecom-domain datasets, fine-tuning a general domain LLM for specific telecom tasks, or using general domain LLMs by prompting directly. The following will introduce the key procedures and features of each approach, in which Sections III-B, III-C, and III-D introduce the procedures of pre-training, fine-tuning, and prompting, respectively.\n# B. LLM Pre-training\nThe aim of pre-training language models is to predict the next word within a sentence. After being trained on extensive datasets, LLMs exhibit emergent capabilities in comprehension and reasoning. This subsection will introduce dataset collection, preprocessing, and model training techniques. 1) Dataset collection and preprocessing: Datasets for training language models fall into two primary categories: general and specialized. General datasets comprise a diverse range of sources, such as web pages, literature, and conversational corpora. For instance, web pages like Wikipedia [62] can contribute to a language model\u2019s broad linguistic\nunderstanding. Meanwhile, literary works also serve as a rich reservoir of formal and lengthy texts [63]. These materials are crucial for teaching LLMs complex linguistic constructs, facilitating the modelling of long-range dependencies. Specialized data involves scientific texts and programmingrelated data. For example, scientific literature comprises a wealth of formal writing imbued with domain-specific knowledge, encompassing academic papers and textbooks. On the other hand, programming data drawn from online questionanswering platforms like Stack Exchange [64], along with public software repositories such as GitHub, provide raw material rich with code snippets, comments, and documentation. Incorporating these specialized texts into the training of LLMs can significantly improve LLM\u2019s performance in reasoning and domain-specific knowledge applications. However, before pre-training, it is critical to preprocess the collected datasets, which often contain noisy, redundant, irrelevant, and potentially harmful data. The preprocessing procedure may include quality filtering, de-duplication [59], privacy redaction [65], and tokenization [66]. 2) Model training: In the model training process, two pivotal hyperparameters are the batch size and the learning rate. For the pre-training of LLMs, a substantial batch size is required, and recent studies suggest incrementally enlarging the batch size to bolster the stability of the training process [59]. In terms of learning rate adjustments, a widely used strategy is to start with a warm-up phase and then succeed with a cosine decay pattern. This approach helps in achieving a more controllable learning rate schedule. To enhance the training scalability, several key techniques are proposed. For instance, 3D parallelism encompasses data parallelism, pipeline parallelism, and tensor parallelism. Data parallelism involves the replication of the model\u2019s parameters and optimizer states across multiple GPUs [67], allocating to each GPU a subset of data to process and subsequently aggregate the computed gradients. Pipeline parallelism, as detailed in [68], assigns distinct layers of an LLM to various GPUs, allowing the accommodation of larger models within the confines of GPU memory. Tensor parallelism operates on a similar premise by decomposing the tensors [69], especially for the parameter matrices of LLM, facilitating the distribution and computation across multiple GPUs. Meanwhile, ZeRO is also a useful technique [70], which conserves memory by retaining only a portion of the model\u2019s data on each GPU. The remainder of the data is accessible across the GPU network, as needed, effectively addressing memory redundancy concerns.\n# C. LLM Fine-tuning\nFine-tuning refers to the process of updating the parameters of pre-trained LLMs to adapt to domain-specific tasks. Although the pre-trained LLM already has vast language knowledge, they lack specialization in specific areas. Fine-tuning overcomes this limitation by allowing the model to learn from domain-specific datasets, making the LLM more effective on specific applications. This subsection will introduce two finetuning strategies: instruction and alignment tuning.\n1) Instruction tuning: Instruction tuning is a method for fine-tuning pre-trained LLMs using a collection of natural language-formatted instances. This technique aligns closely with supervised fine-tuning and multi-task prompted training, enhancing the LLM\u2019s ability to generalize to unseen tasks, even in multilingual contexts [71]. The process involves collecting or constructing instruction-formatted instances and employing these to fine-tune LLMs in a supervised manner, typically using sequence-to-sequence loss for training. Models like InstructGPT and GPT-4 have demonstrated the effectiveness of instruction tuning in meeting real user needs and improving task generalization [72], [73]. Instruction-formatted instances usually consist of a task description, an optional input, a corresponding output, and possibly a few examples as demonstrations. These instances can originate from various sources, such as traditional NLP task datasets, daily chat data, and synthetic data. Existing research has reformatted traditional NLP datasets with natural language task descriptions to aid LLMs in understanding tasks, proving particularly effective in enhancing task generalization capabilities [71]. The design and quality of instruction instances significantly will impact the model\u2019s performance. Scaling the instructions, for instance, tends to improve generalization ability up to a certain point, beyond which additional tasks may not yield further gains [74]. Diversity in task descriptions and the number of instances per task are also critical, with a smaller number of highquality instances often sufficing for significant performance improvements [71]. 2) Alignment tuning: Alignment tuning aims to ensure LLMs adhere to human values, preventing outputs that could be harmful, biased, or misleading. This concept emerges from the realization that while the LLM excels in various NLP tasks, they may inadvertently generate content that deviates from ethical norms or human expectations [58]. Collecting human feedback is central to the alignment-tuning process. In particular, it involves curating responses from diverse human labellers to guide the LLM toward generating outputs that align with the predefined criteria. Approaches to collecting this feedback include ranking-based methods, where labellers evaluate the quality of model-generated outputs, and questionbased methods, where labellers provide insights on specific aspects of the outputs, such as their ethical implications [75]. A prominent technique in alignment tuning is reinforcement learning from human feedback (RLHF), where the model is fine-tuned using reinforcement learning algorithms based on human feedback. This process typically starts with supervised fine-tuning using human-annotated data, followed by training a reward model that reflects human preferences, and finally, fine-tuning the LLM using this reward model. Despite its effectiveness, RLHF can be computationally intensive and complex, necessitating alternative approaches for practical applications [76]. An alternative method for RLHF is direct optimization through supervised learning, which bypasses the complexities of reinforcement learning. This method relies on constructing a high-quality alignment dataset and directly finetuning LLMs to adhere to alignment criteria. Although less\nresource-intensive than RLHF, this approach requires careful dataset construction and may not capture the full range of human values and preferences as effectively as RLHF [77]. Additionally, researchers have introduced the direct preference optimization (DPO) technique [78], which eliminates the need for a reward model and allows the model to align directly with preference data. Some advanced LLMs, like Llama 3, utilize both RLHF with proximal policy optimization (PPO) and DPO. However, both RLHF and DPO depend on high-quality human preference data, which is limited and costly to acquire. To address this challenge, methods such as Constitutional AI [79] and RL from AI feedback (RLAIF) [80] have been developed to generate preference data using LLMs, enabling models to learn from AI feedbacks and facilitating knowledge transfer between models.\n# D. LLM Inference and Utilization by Prompting\nPrompt engineering is the process in which users design various inputs for AI models to generate desired outputs. Compared with fine-tuning, prompting has no requirements for extra training, producing output instantly based on user inputs. It indicates a straightforward approach to using LLMs, and the rapid response and training-free features make it a promising method for telecom applications. This subsection will introduce key techniques in prompt engineering, including ICL, CoT prompting, LLM for complex planning, and selfrefinement with iterative feedback. The comparisons among different prompt engineering techniques are shown in Fig. 3. 1) In-context learning (ICL): ICL, first introduced with GPT-3 [58], utilizes formatted natural language prompts and integrates task descriptions and examples to guide LLMs in task execution. This approach allows the LLM to recognize and perform new tasks by leveraging contextual information. The design of demonstrations is critical for ICL, encompassing the selection, format, and order of examples. The format of demonstrations involves converting selected examples into a structured prompt, integrating task-specific information and possibly incorporating reasoning enhancements like CoT [74], [82]. The ordering addresses LLM biases, arranging demonstrations based on similarity to the query or employing information-theoretic methods to optimize information conveyance [83], [84]. ICL\u2019s underlying mechanisms include task recognition and task learning, and then LLMs can use pretrained knowledge and structured prompts to infer and solve new tasks. Task recognition involves LLMs identifying the task type from the provided examples, leveraging pre-existing knowledge from pre-training data [85]. Task learning, on the other hand, refers to LLMs acquiring new task-solving strategies through the given demonstrations, a capability that becomes more pronounced with increasing model size [86]. Recent studies suggest that larger LLMs exhibit an enhanced ability to surpass prior knowledge and learn from the demonstrations provided in ICL settings [87]. 2) Chain-of-thought (CoT) prompting: CoT prompting is an advanced strategy to enhance LLM\u2019s performance on complex reasoning tasks, such as arithmetic, commonsense,\nand symbolic reasoning, by incorporating intermediate reasoning steps into prompts [82]. Differing from ICL\u2019s input-output pairing, CoT prompting enriches prompts with sequences of reasoning steps, guiding LLMs to bridge between questions and answers more effectively. Initially proposed as an ICL extension, CoT augments demonstrations from mere inputoutput pairs to sequences comprising inputs, intermediate reasoning steps, and outputs [82]. These steps help LLMs navigate complex problem-solving more transparently and logically, though they typically require manual annotation. However, creative phrasings such as \u201cLet\u2019s think step by step\u201d can trigger LLMs to generate CoTs autonomously, which significantly simplifies the CoT implementation. Despite improvements, CoT prompting faces challenges such as incorrect reasoning and instability. The enhancement strategies include better prompt design by utilizing diverse and complex reasoning paths, advanced generation strategies, and verification-based methods. These methods address generation issues by exploring multiple paths or validating reasoning steps, thus improving result accuracy and stability. Furthermore, extending beyond linear reasoning chains, recent studies propose tree- and graph-structured reasoning to accommodate more complex problem-solving processes [88]. In addition, CoT prompting significantly benefits large-scale LLMs (over 10B parameters) and tasks requiring detailed step-by-step solutions. However, it may underperform in simpler tasks or when traditional prompting is already effective [82]. 3) Planning for complex task solving: While ICL and CoT prompting provide a straightforward approach for task solving, they often fall short in complex scenarios like mathematical reasoning and multi-hop question answering [89]. To this end, prompt-based planning has emerged, breaking down intricate tasks into smaller and manageable sub-tasks and outlining action sequences for their resolution. The planning framework for LLMs encompasses three main components: the task planner, the plan executor, and the environment. The task planner devises a comprehensive plan to address the target task, which could be represented as a sequence of actions or an executable program [90]. This plan is then carried out by the plan executor, which can range from text-based models to code interpreters, within an environment that provides feedback on the execution results [88], [91]. In plan generation, LLMs can utilize text-based approaches to produce natural language sequences or code-based methods for generating executable programs, enhancing the verifiability and precision of the planned actions [91]. Feedback acquisition follows, where the LLM evaluates the plan\u2019s efficacy through internal assessments or external signals, refining the strategy based on outcomes from different environments [88]. In addition, the refinement process is crucial for optimizing the plan based on received feedback, and the corresponding methods include reasoning, backtracking, memorization, etc [88]. 4) Self-refinement with iterative feedback: Considering that LLMs may not generate correct answers initially, selfrefine has recently emerged to improve their outputs through iterative feedback and refinement. Among these studies, Madaan\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a980/a9804be3-1b49-47b7-b9f1-9f3f235023e5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3. Comparison of various prompt engineering techniques [11], [81]. Different from task-specific examples in ICL demonstrations, CoT promptin additionally incorporates intermediate reasoning steps in demonstrations. Prompt-based planning breaks down intricate tasks into manageable sub-tasks an outlines action sequences for their resolution. Self-refine enhances LLM\u2019s outputs through iterative feedback and refinement.</div>\net al. [81] first use an LLM to generate initial outputs, and then employ the same LLM to provide specific feedback on these outputs. Note that this feedback is actionable, containing concrete steps to further improve the initial outputs. With such specific and actionable feedback, the same LLM can iteratively refine its outputs until performance converges. Furthermore, Hu et al. [92] propose a self-refined LLM (named TrafficLLM) specifically designed for communication traffic prediction, which leverages in-context learning to enhance predictions through a three-step process: traffic prediction, feedback generation, and prediction refinement. Following the comprehensive feedback, refinement demonstration prompts enable the same LLM to refine its performance on target tasks.\n# E. Evaluation metrics of LLM\nEvaluating the performance of LLMs is a multifaceted task and receives increasing attention. This subsection focuses on the evaluation metrics that encompass various dimensions, including accuracy, hallucination, efficiency, and human alignment etc. Each of these aspects plays a crucial role in determining the overall applicability of LLMs in realworld scenarios such as telecom networks. Firstly, accuracy is paramount in evaluating LLM technologies as it directly impacts the model\u2019s reliability and trustworthiness. It measures how well an LLM can understand and process natural language queries, generate relevant and correct responses, and perform specific tasks like translation, summarization, or questionanswering. Benchmarks and standardized datasets are often used to quantitatively evaluate the model\u2019s accuracy. Secondly, hallucination refers to instances where the LLM generates incorrect or factual inconsistent information, often presenting it with a high degree of confidence. This phenomenon can significantly undermine the credibility of LLMgenerated content. Evaluating an LLM\u2019s tendency to hallucinate involves analyzing the model\u2019s responses for factual accuracy, consistency, and relevance to the input prompt. Recent studies show that traditional automatic metrics for summarization such as ROUGE [93] and BERTScore [94] show suboptimal performance on factual consistency measurement [95]. Thereafter, some novel metrics have been proposed to detect hallucination errors, such as AlignScore in [96].\nThen, the efficiency of LLMs indicates the computational resources required for training and inference, as well as the speed at which these models can generate responses. As the size of LLM expands, this expansion leads to significant issues regarding environmental sustainability and the ease of access to these technologies [97], [98]. Evaluating an LLM\u2019s efficiency involves a detailed assessment of its performance relative to the consumed resources. Key metrics for this assessment include the energy usage during operations, the time it takes to process information, and the financial burden associated with acquiring and maintaining the necessary hardware infrastructure. Additionally, it\u2019s important to consider the efficiency of data usage during training, as optimizing data can reduce computational requirements [99]. The last metric is human alignment. Manual evaluation for LLM alignment to human values generally offers a more holistic and precise assessment compared to automated evaluation [100]. This is supported by numerous studies, such as [101], [102], which incorporate human alignment evaluation to provide a more in-depth analysis of their methods\u2019 performance. Human alignment assesses the degree to which the language model\u2019s output aligns with human values, preferences, and expectations. It also considers the ethical implications of the generated content, ensuring that the language model produces text that respects societal norms and user expectations, promoting a positive interaction with human users.\nPractical deployment is the prerequisite for advanced applications of LLM technologies in telecom networks. In particular, it indicates how LLMs can be deployed within the current telecom network architecture, e.g., central cloud, network edge or even user devices. The LLM has great demands for computational and storage resources. For instance, GPT4 has 1.76 trillion parameters and the model size is 45 GB [73], posing a heavy burden on network storage capacities. Fine-tuning an LLM with 7 billion parameters, such as GPT4-LLM [109], could take nearly 3 hours on an 8\u00d780GB A100 machine, which is extremely time-consuming [110]. In addition, the inference time of LLMs will also contribute to overall network latency, which is related to hardware support,\nLLM\ndeployment\nstrategies\nMain features & Advantages\nPotential issues & Difficulties\nCloud\ndeployment\nCloud deployment is the most straightforward method\nfor LLM deployment. LLMs are usually\ncomputationally demanding, and cloud servers can\nprovide abundant computational and storage resources\nfor model training, fine-tuning, and inference.\nCloud deployment indicates higher end-to-end latency for implementing\nuser requests, since the inquiries have to be first uploaded and then\nprocessed and downloaded. It may prevent the application of some\nlatency-critical applications such as robot control, vehicle-to-vehicle\ncommunications, unmanned aerial vehicle (UAV) control, etc.\nNetwork edge\ndeployment\nNetwork edge deployment can be an appealing\napproach to shorten the response time and save\nbackhaul bandwidth to the central cloud. It enables\nrapid user request processing at edge servers or\ncloud, achieving shorter end-to-end delay than the\ncentral cloud-based approach.\nNetwork edge servers are usually resources-constrained, indicating\nlimited computational and storage resources for LLM fine-tuning and\ninference. Therefore, some techniques may be exploited, e.g., efficient\nparameter-efficient fine-tuning [103], split edge learning [104], and\nquantized training [105]. In addition, model compression is also a\npromising direction for edge LLM deployment.\nOn-device\ndeployment\nOn-device LLM is considered a very promising\ndirection to deploy LLMs directly at user devices. It\nenabled customized LLMs based on specific user\nrequests. Meanwhile, on-device LLMs have the\nlowest service latency by processing tasks locally.\nTherefore, it has great potential for implementing\nreal-time tasks.\nDespite the great advantages, on-device LLMs are still in the very early\nstages, and the main challenge is to overcome the very limited\ncomputational and storage resources at user devices. Apple has proposed\na technique to store LLM parameters on flash memory [106] and achieve\na 20 times faster inference speed. Qualcomm also announced a new\nmobile platform to support popular small-scale LLMs [107]. Therefore,\nhow to utilize limited computation resources to achieve faster inference\nis the key to on-device LLM deployment.\nCache-based\ndeployment\nCached-based approach is proposed by [16] based on\nmobile edge computing architecture. Specifically, the\nauthors propose to store the full-precision parameters\nin the central cloud, quantized parameters in the edge\ncloud, and the frozen parameters at the user devices,\nenabling more flexible model training and migration.\nSuch a distributed deployment approach is promising to save the model\nstore and migration cost. However, compared with on-device deployment,\nthe cache-based method also requires complicated coordination strategies\nfor model update and synchronization, e.g., model update and\nsynchronization frequency and the quantization bit version selection.\nCooperative\ndeployment\nCooperative deployment is proposed in [108], which\ninvolves the interactions between local small models\nand cloud-based large models. In particular, it\nassumes that the local model can collect and submit\nsensor data selectively to the large model, and the\nlarge model will update the small-scale local models\nbased on its domain-specific knowledge.\nThe cooperative deployment is a feasible solution to connect small-scale\nlocal LLMs to large cloud models. However, the local model updating\nfrequency should also be carefully determined to reduce the burden on\ncloud LLMs. In addition, note that the inference is still updated locally,\nand therefore the required computational resources are still challenges.\nTo this end, it may be combined with on-device LLMs to address the\nresource issues.\nbatch size, parallelism, model pruning, etc [111]. Therefore, it is of great importance to deploy LLMs appropriately to better serve the telecom network demand. We summarize the existing deployment schemes in Table III, including cloud, network edge, on-device, cache-based, and cooperative deployment. We present the details of each strategy as the following: 1) Cloud deployment: : Considering LLM\u2019s high demand for computational and storage resources, deploying LLMs in the central cloud is a straightforward solution, which can provide substantial computational resources to support the fine-tuning and inference of LLMs [15]. Shen et al. investigate LLM-enabled autonomous edge AI [15], in which the network edge devices can send the user request and datasets feature to LLMs in the cloud, and then the LLM can send back the task planning and AI model configuration to network edge devices through the backhaul. After that, the network edge and user devices can collaborate to make edge inferences. Cloud deployment can easily adapt to existing telecom network architecture, and a few pieces of extra hardware are needed since the LLM is deployed in the virtual cloud. However, cloud deployment suffers from long response time and high bandwidth costs since all data has to be transmitted to the cloud, and then LLM will process the request and finally\n<div style=\"text-align: center;\">Potential issues & Difficulties</div>\nThe cooperative deployment is a feasible solution to connect small-scale local LLMs to large cloud models. However, the local model updating frequency should also be carefully determined to reduce the burden on cloud LLMs. In addition, note that the inference is still updated locally, and therefore the required computational resources are still challenges. To this end, it may be combined with on-device LLMs to address the resource issues.\ndownload the LLM\u2019s output [16]. The long response time may prevent the applications on latency-critical tasks, e.g., vehicleto-vehicle networks and unmanned aerial vehicle control. In addition, the frequent multimodal information exchange, such as images and videos, between end users and cloud LLM will lead to extra bandwidth costs.\n2) Network edge deployment:: Here network edge refers to edge cloud or BSs that are closer to users than central cloud. Network edge deployment can be an appealing approach to shorten the response time and save bandwidth. However, compared with the central cloud, network edge devices usually have limited computational and storage capacities. To this end, multiple techniques can be exploited. For the storage capacity challenge, parameter sharing and model compression may be applied. In particular, LLMs for different downstream tasks may share the same parameters, which can be exploited to save the storage capacity. On the other hand, other technologies may be applied to reduce the computational resources demand in fine-tuning and inference, including parameter-efficient finetuning [103], split edge learning [104], and quantized training [105]. With these techniques, deploying LLMs at the network edge becomes a practical strategy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd25/fd2501fd-f938-4b0a-bca0-ed44cd0c7638.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4. Illustration of different LLM deployment strategies.</div>\n3) On-device deployment:: There are multiple benefits of deploying LLMs on user-side mobile devices, e.g., fast responses and local customization based on the user\u2019s specific requirements. However, such a deployment is also challenging since LLMs are usually storage- and computation-intensive. Xu et al. introduced a split learning approach based on collaborative end-edge-cloud computing, aiming to deploy LLM agents at mobile devices and network edge [17]. Specifically, the authors assume that LLMs with less than 10B parameters such as LLAMA-7B can operate on mobile devices, providing real-time inference services. Meanwhile, LLMs with more than 10B parameters such as GPT-4 are deployed on network edge servers, using global information and historical memory to assist the mobile LLM in processing complex tasks. Such a collaboration enables higher flexibility by exploiting mobile LLMs. However, the study of on-device LLM is still in a very early stage, and it requires considerable efforts to prove the feasibility of such a design [112]. For instance, Apple has proposed a technique to store LLM parameters on flash memory [106], achieving a 20 times faster inference speed than using GPU with limited dynamic random-access memory (DRAM) capacity. Similarly, Qualcomm has recently announced the Snapdragon 8s Gen 3 mobile platform, which supports popular small-scale LLM such as Llama 2 and Gemini Nano [107]. These studies may pave the way to effective inference of ondevice LLMs.\n3) On-device deployment:: There are multiple benefits of deploying LLMs on user-side mobile devices, e.g., fast responses and local customization based on the user\u2019s specific requirements. However, such a deployment is also challenging since LLMs are usually storage- and computation-intensive. Xu et al. introduced a split learning approach based on collaborative end-edge-cloud computing, aiming to deploy LLM agents at mobile devices and network edge [17]. Specifically, the authors assume that LLMs with less than 10B parameters such as LLAMA-7B can operate on mobile devices, providing real-time inference services. Meanwhile, LLMs with more than 10B parameters such as GPT-4 are deployed on network edge servers, using global information and historical memory to assist the mobile LLM in processing complex tasks. Such a collaboration enables higher flexibility by exploiting mobile LLMs. However, the study of on-device LLM is still in a very early stage, and it requires considerable efforts to prove the feasibility of such a design [112]. For instance, Apple has proposed a technique to store LLM parameters on flash memory [106], achieving a 20 times faster inference speed than using GPU with limited dynamic random-access memory (DRAM) capacity. Similarly, Qualcomm has recently announced the Snapdragon 8s Gen 3 mobile platform, which supports popular small-scale LLM such as Llama 2 and Gemini Nano [107]. These studies may pave the way to effective inference of ondevice LLMs. 4) Cache-based deployment: Lin et al. proposed a cachebased method in [16], which utilizes the mobile edge computing architecture to store, cache, and migrate models in edge networks. Specifically, they propose to store the fullprecision parameters in the central cloud, quantized parameters in the edge cloud, and finally the frozen parameters at the user devices. Such a separate model caching enables more flexible model training and migration. For instance, edge clouds or servers can apply low-precision computation by using quantized training, improving the edge training speed with limited computational resources. In addition, storing the frozen parameters on user devices can save the storage\ncapacities of the edge cloud, reducing the latency caused by full model migration. However, the cache-based method may require complicated coordination strategies for model update and synchronization, e.g., model update and synchronization frequency and the quantization bit version selection. 5) Cooperative deployment: Lin et al. proposed a novel EdgeFM approach in [108]. In particular, the edge devices will collect the sensor data from the environment, and then the local model can evaluate the uncertainty features of the collected data and the real-time network dynamics. After that, the local EdgeFM model will selectively upload the unseen data classes to query large models in the cloud, and the large models can periodically update a customized small-scale model at the network edge. Therefore, when the network environment changes, at the early stage, the local model can frequently query large models in the cloud, and then it can execute customized small models on edge devices at the late stage. Such a cooperative deployment can reduce the system overhead, and enable dynamic customization of local small models for edge devices. The experiment in [108] shows 3.2x lower end-to-end latency and achieve 34.3% accuracy improvement than the baseline. Finally, Fig. 4 illustrates different LLM deployment strategies. Note that LLM\u2019s requirements for storage and computational resources are the main motivations for developing various deployment strategies. For instance, the model size of Llama3-8b is around 5 GB, and therefore it is possible to be implemented at the network edge or even user devices, i.e., Snapdragon 8s Gen 3 mobile platform recently developed by Qualcomm. Similarly, Gemini Nano is less than 2 GB, and such a small size allows on-device deployment, e.g., Google plans to load Gemini Nano to its Pixel 8 smartphones. By contrast, large-scale LLMs require much more computation resources. For example, inference with Llama3-70b consumes at least 140 GB of GPU RAM. Using 2-bit quantization, the Llama3-70b can be implemented on a 24 GB consumer GPU, but such a low-precision quantization will significantly degrade the model accuracy. To this end, hybrid deployment\nmethods such as cache-based and cooperative deployment are proposed. The key objective is to take advantage of largescale LLM\u2019s high accuracy, while reducing the dependency on computational resources. On the other hand, these approaches may be combined, e.g., deploying small-scale on-device LLMs and then using larger cloud models to update the local models periodically. Given these deployment methods, many critical problems can then be investigated, e.g., service delay evaluation and task offloading, which still require more research efforts. For example, Chen et al. proposed a NETGPT scheme in [113], involving offload architecture, splitting architecture, and synergy architecture for cloud-edge collaboration.\n# G. Analyses of LLM Fundamentals in the Telecom Domain\nPrevious Sections III-A to III-F have covered the key techniques of LLM fundamentals, ranging from model architecture and pre-training to evaluation and deployment in telecom networks. This subsection will analyze how these fundamental techniques can be applied to the telecom domain. For telecom applications, pre-training an LLM from scratch can be time-consuming. It first requires extensive dataset collection, and the dataset preprocessing has to consider the format of complicated telecom equations and theories. Meanwhile, it also requires considerable computational resources to pre-train LLMs, leading to heavy burdens for telecom networks. By contrast, a more efficient approach is to finetune a general-domain LLM for specific telecom-domain tasks. Applying LLM technologies to the telecom domain requires an in-depth understanding of these fine-tuning techniques, such as instruction and alignment tuning methods. In particular, instruction tuning involves carefully constructing and selecting instruction datasets, employing strategic tuning methodologies, and considering practical implementation aspects. These strategies will significantly improve the performance, generalization, and user alignment of LLM technologies in the telecom domain. On the other hand, alignment tuning is a multifaceted process involving the setting of ethical guidelines, collection of human feedback, and application of advanced fine-tuning techniques such as RLHF. However, adapting these state-of-the-art fine-tuning techniques to telecom environments is still an open question. The fine-tuning process is usually task-specific, which requires professional knowledge of various telecom domain tasks. Instruction tuning can be a promising method for building a telecom-LLM by using existing telecom knowledge, but the dataset collection can be difficult due to privacy issues. Prompting techniques are especially useful for solving realtime telecom tasks with stringent delay requirements, e.g., resource allocation and user association. It means that LLMs can directly learn from the inputs and generate desired outputs without extra training, avoiding the tedious model training process in conventional ML algorithms. For instance, ICL provides a framework for leveraging the LLM in new task domains without explicit retraining, with its effectiveness heavily influenced by the design and structure of demonstrations. Meanwhile, CoT prompting has emerged as a potent method\nfor eliciting deeper reasoning capabilities in LLMs, applicable to a range of complex reasoning tasks. While still evolving, this approach opens new avenues for LLM application across diverse problem domains such as the telecom field. In addition, prompt-based planning represents a sophisticated approach to navigating complex tasks, enhancing LLM\u2019s problem-solving capabilities through structured action sequences, feedback integration, and continuous plan refinement. Such planning capabilities are very important for telecom applications since many telecom tasks involve multi-step thinking with complicated procedures. For instance, the resource allocation may include multi-layer controllers [4], and optimization problems can involve several agents and elements [114]. Therefore, multi-step planning and thinking should be carefully designed for LLM-enabled telecom applications. Evaluation metrics are critical to assess the LLM\u2019s performance in telecom environments. For instance, efficiency is one of the most important metrics that should be considered in telecom applications since many tasks require rapid or even realtime responses. Therefore, LLMs with long inference times may be inappropriate for these mission-critical applications, e.g., Ultra-Reliable Low Latency Communications (URLLC). In addition, evaluating the performance of LLMs should also include their proneness to hallucination and ethical standards, e.g., LLM may make misleading or even wrong decisions in network management. As LLM design and models continue to evolve and integrate more deeply into various aspects of society, the criteria for their evaluation will likely expand and become more sophisticated. Ensuring that LLMs are accurate, reliable, efficient, and ethically responsible is essential for their sustainable and beneficial integration into human-centric applications. Finally, practical deployment is the prerequisite for applying LLM to telecom networks. Compared with other domains such as education or healthcare, many telecom tasks have stringent requirements for delay and reliability, which require more efficient and reliable model output. Meanwhile, telecom devices usually have limited computational and storage resources. Therefore, efficient model training, fine-tuning, inference and storage techniques should be explored [16]. With previous knowledge and analyses, we will present detailed LLM-inspired techniques and applications in telecom tasks in terms of generation, classification, optimization, and prediction problems in the following sections.\n# IV. LLM FOR GENERATION PROBLEMS IN WIRELESS NETWORKS\nThe outstanding generation capability is one of the most attractive features of LLMs. This section first introduces the motivations for applying the LLM technique to telecom-related generation tasks, and then it presents detailed application scenarios, including telecom domain knowledge generation, code generation, and network configuration generation.\n<div style=\"text-align: center;\">TABLE IV MMARY OF LLM-AIDED GENERATION-RELATED STUDIES IN THE NETWORK FIELD.</div>\nTopics\nRefer-\nences\nProposed LLM-aided generation schemes\nKey findings & Conclusion\nTelecom application opportunities\nDomain\nknowledge\ngeneration\n[115]\nAdapting a BERT-like model to the telecom\ndomain and testing the model performance by\nquestion answering downstream task in the\ntarget domain.\nThe proposed technique achieved F1 score of\n61.20 and EM score of 36.48 on question\nanswering in a small-scale telecom question\nanswering dataset.\nTechniques such as customizing\nLLMs to understand\nand apply telecom-specific\nlanguage, evaluating their\ngenuine understanding of\ndomain knowledge generation\ncan contribute to more\nefficient, reliable, and secure\ntelecom service applications.\nExisting studies have\ndemonstrated the capability\nof LLM techniques to be applied in\ntelecom, including question\nanswering, literature review,\ngenerating troubleshooting report.\nIt shows great promises to build\nnext generation communication\nnetworks.\n[116]\nIt proposed a multi-stage BERT-based\napproach to understand the textual data of\ntelecom trouble reports, and then generate a\nranked solution list for new troubles based on\npreviously solved troubleshooting tickets.\n1) Presenting more information in the query can\nproduce a better list of recommended solutions;\n2) Creating a small candidate list is the key to\nreducing the model latency.\n[23]\nIt combines a BERT-like method with transfer\nlearning for trouble report retrieval,\nleveraging non-task-specific telecom data and\ngeneralizing the model to unseen scenarios.\nThe experiment includes nearly 18500 trouble\nreports, showing that combining pre-trained\ntelecom-specific language models with fine-tuning\nstrategies outperforms pure domain adaptation\nfine-tuning.\n[117]\nQuestion answering test on various LLMs,\ne.g., GPT 3.5, GPT 4, and Bard, including\ntelecom knowledge and product questions.\nBard and GPT4 show promise with respect to\naccuracy and could be useful for telecom domain\nquestion and answering. LLM\u2019s summarization\nrequires reliability tests.\n[118]\nIntegrating domain-specific grammars into\nLLMs to guide the generation of structured\nlanguage outputs, enhancing performance in\ndomain-specific tasks.\nDemonstrating the efficacy of integrating\ndomain-specific grammars with LLMs in\nenhancing their ability to generate structured\nlanguage outputs tailored to specific domains. It\nemphasizes the potential of this approach to\nsignificantly improve LLM performance in\ndomain-specific tasks.\nCode\ngeneration\n[14]\nUsing LLMs to generate Verilog code for\nwireless communication system development\nin FPGA. The experiment was implemented\nin the OpenWiFi project.\nThe LLM is capable of refactoring, reusing, and\nvalidating existing code. With proper design and\nprompting, LLMs can generate more complicated\nprojects with multi-step scheduling. LLM greatly\nreduced the coding time of undergraduate and\ngraduate students by 65.16% and 68.44%,\nrespectively.\nCode is the cornerstone of modern\ncommunication networks, and the LLM\nprovide promising opportunities to\nimprove the efficiency and reliability\nof codes, and meanwhile greatly save\nhuman effort.\na) The LLM can refactor and\nvalidate existing code.\nThis is very useful in\ntelecom filed, since\nthe network architecture is\nconstantly evolving and updated;\nb) With proper prompting, the LLM\ncan generate complicated projects\nwith multi-step scheduling\nrequirements, which is very\ncommon in telecom filed due to\ncomplicated network elements with\ndiverse functions.\n[119]\nIt proposed a framework to use LLM to\ngenerate task-specific code for traffic analyses\nand network life-cycle management.\nCombining the LLM with proper libraries, such\nas GPT-4 and NetworkX, can achieve 88% and\n78% coding accuracy for traffic analysis and\nnetwork lifecycle management tasks, respectively.\n[18]\nEmploying four students to reproduce the\nresults of existing network studies with the\nassistance of LLMs.\nThe students successfully reproduced networking\nsystems by prompting engineering ChatGPT.\nThey also achieve much lower lines of code by\nusing ChatGPT, e.g., one of them is only 20% of\nthe open-source existing version.\n[120]\nUsing LLM techniques for automated\nprogram repair of introductory level Python\nprojects.\nThe proposed scheme successfully repaired a\nlarger fraction of programs (86.71%) compared to\nthe baseline (67.13%), and adding few-shot\nexamples will raise the ratio to 96.50%.\n[121]\nFine-tuning pre-trained LLMs on Verilog\ndatasets collected from GitHub and Verilog\ntextbooks and then generating Verilog\nprojects.\nFine-tuning LLMs over a specific language can\nimprove the coding correct rate by 26%.\nNetwork\nconfiguration\ngeneration\n[122]\nIt proposed a three-stage LLM-aided\nprogressive policy generation pipeline for\nintent decomposition.\nThrough evaluating a service chain use case, the\npaper found LLMs could generalize to new\nintents through few-shot learning and concluded\nleveraging LLMs for policy generation is\npromising for automatic intent-based application\nmanagement.\nTelecom network operators\ncan leverage the LLM for network\nconfiguration generation in\nvarious ways. This includes automatic\nnetwork provisioning, optimization\nand performance tuning, security\nand compliance configuration,\nfault diagnosis and troubleshooting,\nand network virtualization. The LLM\nenables efficient, reliable, and\nsecure generation of network\nconfigurations, reducing manual\neffort and improving network\nmanagement in telecom\nenvironments.\n[123]\nIt proposed a multi-stage framework that\nutilizes LLMs to automate network\nconfiguration by taking in natural language\nrequirements and translating them into formal\nspecification, high-level configurations, and\nlow-level device configurations.\nThe results showed that state-of-the-art LLM\ntechnologies like GPT-4 are capable of generating\nfully working configurations from natural\nlanguage requirements without any fine-tuning.\n[124]\nIt proposed a framework that combines LLMs\nwith verifiers, using localized feedback from\nverifiers to automatically correct errors in\nconfigurations generated by the LLM.\nThe proposed scheme is able to synthesize\nreasonable though imperfect configurations with\nsignificantly reduced human effort, and coupling\nLLMs with verifiers providing localized feedback\nis necessary for real-world use configurations\ndespite requiring more testing.\nA. Motivations of Using LLM-based Generation for Telecom This subsection will introduce the key motivations of using LLM-enabled generation for telecom applications. Firstly, LLM can make telecom knowledge more accessible. LLMs have been pre-trained on many real-world datasets and equipped with considerable knowledge from various fields. Therefore, question-answering has become the most wellknown application of LLMs. With domain-specific datasets from websites and textbooks, the LLM can extract professional knowledge from existing publications and then generate appropriate answers based on users\u2019 requests. For instance, Maatouk et al. build a telecom knowledge dataset in [125], including 25,000 pages from research publications, overview, and standards specifications. With proper training and fine-tuning, such a dataset can greatly contribute to a Telecom-GPT, providing a systematic overview of hundreds of publications and standards. With reasoning and comprehension capabilities, professional telecom knowledge will become much more accessible to all researchers and even benefit the whole society. Meanwhile, LLM\u2019s generation capabilities can also automate many tasks that are usually time-consuming. For instance, developing new standard specifications usually requires considerable writing, discussions, and reviews. By contrast, given enough historical reports and proper prompts, the LLM can produce a draft standard instantly, and then the experts can review it accordingly. Moreover, the experts\u2019 comments can be fed directly to LLMs, and then the LLM can produce a new version efficiently, significantly saving human efforts on writing and revising paper works. Similarly, LLM technologies have been used to generate code in many existing studies, which is one of the most time-consuming tasks of modern industry [18], [120]. LLMs can refactor and improve existing codes, contributing to developing telecom projects. In addition, LLMs can easily learn from the provided existing examples, which is known as ICL. This capability is particularly useful in generation tasks, and LLMs can quickly generalize the given examples to related unseen scenarios. Meanwhile, if the initial generated output can not satisfy the requirements, users can also send the feedback directly to the LLM input, and then the LLM agent will revise the generation accordingly. This user-friendly generation approach will lower the difficulty of applying LLM techniques to generation tasks in telecom, which usually requires considerable professional knowledge and experience. Given the above motivations and advantages, it is crucial to exploit LLM\u2019s generation capabilities and apply them to telecom networks. Table IV summarized LLM-aided generationrelated studies and telecom application opportunities. In the following, we will introduce domain knowledge generation, code generation, and network configuration generation.\n# B. Domain Knowledge Generation\nGenerating domain-specific knowledge is an important application of LLM technologies in telecom. In particular, it refers to creating comprehensive summaries, overviews, and interpretations of telecom standards, technologies, and\nresearch findings. By leveraging vast datasets of technical documents, research papers, and standards specifications, LLM agents can produce detailed explanations and summaries that are tailored to the user\u2019s level of expertise and interest. This not only democratizes access to telecom knowledge but also serves as a bridge to fill the gap between experts and nonexpert users in the telecom field. 1) Understanding telecom domain knowledge: Telecom is a broad field, and there are various domains of knowledge such as signal transmissions, network architectures, communication protocols, and industry standards. For instance, signal transmission is fundamental telecom knowledge, involving the differences between amplitude, frequency, and phase modulation, as well as the distinctions between digital and analog signals. Meanwhile, communication protocols refer to sets of rules that ensure standardized data transmission, allowing for interoperability among diverse systems. Knowledge of these protocols is fundamental for the development and maintenance of robust communication networks. Additionally, telecom standards are equally important. Standards such as 3G, 4G, and the emerging 5G for mobile communications, as well as IEEE 802.11 for Wi-Fi, play a critical role in global telecom networks [126]. They facilitate the seamless operation of devices and services across different networks. A thorough understanding of the above telecom knowledge is not only vital for the development of new technologies and services, but also for ensuring that systems are interoperable and secure. The depth of understanding in telecom knowledge directly impacts the ability to innovate, secure, and solve problems within the telecom field. The integration of LLMs, trained with domain-specific datasets, offers promising avenues for automating knowledge generation and facilitating access to complex telecom content, thus bridging the gap between experts and general users. 2) Training LLMs with telecom-specific data: Training LLMs with telecom-specific data involves curating and preprocessing vast amounts of domain-specific information to fine-tune the models, aiming to generate accurate and relevant content within the telecom field. This process is crucial as it tailors the LLM\u2019s capabilities to understand and generate content that aligns with specific telecom requirements. It can be summarized by following steps: \u2022 The first step in training the LLM with telecom-specific data is the collection of datasets. These datasets may include technical documents, research papers, standards specifications, and other forms of professional literature prevalent in the telecom sector. For example, Holm et al. [115] created a small-scale TeleQuAD to train the question-answering capabilities of the build Bert-based model. Similarly, 185,000 trouble reports [23] are included to train a Bert-like model to generate automated troubleshooting tickets. However, these datasets are usually inaccessible to the public. By contrast, Maatouk et al. [125] introduced a large dataset of telecom knowledge to provide systematic overviews and detailed explanations of standards and research findings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14e5/14e54b54-2a93-4ee7-8046-05e0ecb04417.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. Using language models for automated troubleshooting in telecom fields [23].</div>\n\u2022 Following dataset collection, the preprocessing stage involves cleaning and organizing the data to make it suitable for training. This step may include removing irrelevant information, correcting errors, and converting the data into a format that is compatible with the ML model. The study [127] shows that preprocessing largescale datasets for LLM training can improve the model\u2019s learning efficiency and output quality. \u2022 Finally, it is worth noting that there are two main approaches to train LLMs, which are training the model from scratch or fine-tuning a general-domain LLM. In particular, training the model from scratch may produce better performance since the model can specialize in telecom language, but it is also time-consuming. On the other hand, the fine-tuning process adapts the pre-trained LLM to the telecom domain. This step involves training the model on the collected telecom-specific dataset, allowing it to adjust its parameters to better understand and generate telecom content. Fine-tuning enables the model to grasp the unique terminologies, concepts, and contexts of the telecom field, significantly enhancing its generation capabilities. Although fine-tuning a pre-trained LLM is much more efficient than training from scratch, the experiment in [115] proves that training the model on telecom-domain text from scratch can achieve better performance than fine-tuning a general-domain model.\nThe integration of telecom-specific data into LLM training is not just about enhancing the model\u2019s knowledge base; it\u2019s about equipping the LLM with the ability to understand the nuances and complexities of the telecom field. This tailored training approach ensures that the LLM can generate content that is not only informative but also practical and applicable to real-world telecom challenges.\n3) Using LLM to telecom knowledge-related generation tasks: After proper training or fine-tuning, using LLMs to generate telecom domain knowledge is a transformative approach that leverages the model\u2019s ability to process and synthesize vast amounts of information into coherent, accessible content tailored to the needs of various stakeholders in the telecom field. This capability extends from generating summaries of complex technical documents to answering specific queries with detailed explanations, thereby facilitating a deeper understanding of telecom technologies, standards, and practices. In the following, we present some existing applications of telecom knowledge-related generation tasks.\ning is one of the most well-known applications of LLM technologies. Using LLMs to answer domain-specific questions is grounded in the model\u2019s ability to interpret and articulate complex information in a manner that is both comprehensive and understandable. For example, Soman et al. evaluated the capabilities and limitations of existing pre-trained general domain LLMs in [117], including GPT-3.5, GPT-4, Bard, and LLaMA. For instance, one telecom-domain question is \u201dWhat are the different 5G spectrum layers?\u201d GPT-4 identifies the bands as below 1 GHz, 1-6 GHz and above 6 GHz, while LLaMA identifies the frequency bands as below 600 MHz, 600 MHz-24 GHz and above 24 GHz. These differences could be caused by different data sources of GPT-4 and LLaMA in the pre-training period. However, this could easily confuse or even mislead users without professional knowledge, which shows the importance of training a telecom-domain LLM specifically. Holm et al. [115] further investigate how various training methods can affect the model performance, e.g., pre-training a model using telecom knowledge from scratch or fine-tuning an existing general-domain model. In summary, LLM-enabled\nquestion answering democratizes access to advanced telecom knowledge, making it accessible to a broader audience, including researchers, practitioners, and the general public. In addition, LLM agents can also tailor the generated content based on the user\u2019s level of expertise and specific interests. There is an increasing number of commercial LLM products for generative question answering over business documents, e.g., nexocode and Caryon. By leveraging the comprehensive understanding and generation capabilities of LLM technologies, the telecom industry can enhance the accessibility of complex information, support educational endeavours, and streamline development processes. Generating troubleshooting solutions for telecom trouble reports: Telecom networks are complicated large-scale systems, and it is critical to identify, analyze and then resolve both software and hardware faults, which are known as trouble reports. The authors in [116] and [23] investigated using language models to understand previous trouble reports and then generate recommended solutions. Grimalt applied a BERTbased model to generate and rank multiple possible solutions for a given system fault in [116], which archives a nearly 55% correct rate. Then, Bosch [23] improved the model in [116] by including transfer learning and non-task-specific telecom data to improve the generalization capabilities on handling unseen trouble reports. Fig.5 summarizes the proposed scheme in [116] and [23]. One can observe that the analysis and correction phases can be time- and effort-consuming, which usually requires professional knowledge of telecom networks and devices. To this end, a language model-enabled method is proposed. It considers trouble report observation, headings, and fault areas as input and generates the top-K possible solutions. Then, the generated candidate solutions are sent back for verification. In particular, the fine-tuning process of the language model consists of three main steps, including the telecom language dataset, MS MARCO document ranking dataset [128], and trouble report dataset. Here, the MS MARCO dataset is included to train question-answering and ranking models, in which a large number of question-answer pairs are collected from search engines [128]. Fig.5 proves that using language models to generate solutions for automated troubleshooting can significantly improve overall efficiency, enabling faster response and repair for telecom. Finally, it is worth noting that these models may generate misleading or even wrong solutions, which can be caused by different data sources, training strategies, and so on [117]. For instance, the best correct rate in [23] is around 60%, and therefore, verification is crucial before real-world implementation.\n# C. Code Generation\nEfficient and reliable code is of paramount importance to intelligent communication networks. Recent studies have demonstrated the strong coding capability of LLMs, including commonly-used languages (e.g., Python [120], [129]) and hardware description languages (e.g., Verilog [14], [121]). For instance, Zhang et al. [120] apply the LLM to build an automatic program repair system for introductory Python\nprogramming assignments, and the experiment on 286 real student programs achieves a repair rate of 86.71%. For hardware description languages like Verilog for FPGA development, Du et al. [14] show that LLM can reduce nearly 50% of the coding time for undergraduate and postgraduate students and improve the quality by 44.22% for undergraduates and 28.38% for postgraduates. Existing studies [14], [120], [121], [129] have shown that LLM can refactor and improve existing codes. In addition, well-crafted prompts and designs can tackle complex, multi-step coding challenges encompassing multiple sub-tasks. Given these potentials, introducing LLMaided coding into telecom can greatly save human effort in coding, validating, and debugging while providing more efficient and reliable codes for telecom network scheduling and management projects. 1) LLM for code refactoring: Code refactoring is a common task that is frequently involved when developing wireless communication systems. Code refactoring aims to improve the readability, efficiency, and reliability of existing code [130]. For instance, good readabilities can lower the difficulty of long-term maintenance and reuse of existing code modules. Readability is also a critical requirement for wireless networks since the network architectures and protocols are constantly evolving and updated, e.g., from WiFi 6 to 6E and WiFi 7, and from RAN to cloud RAN and Open RAN. However, real-world projects usually include multiple contributors with different coding styles and mixed qualities. Such an issue could be very common in telecom, which are considered as complicated large-scale systems that include multiple modules with diverse functions. Therefore, improving code readability, efficiency, and reliability becomes more important for the telecom field. Fig. 6 shows an example from [14], which applies ChatGPT to revise the original code of an open-source FPGA-based project OpenWiFi [131]. The pink fonts indicate the changes made by ChatGPT. In particular, ChatGPT suggests using meaningful names for modules and variables, e.g., replacing the name \u201cDelayT\u201d with \u201cDelayBuffer\u201d. Meanwhile, four comments are added to improve the readability of the input and output. The input and output data type specification \u201cwire\u201d is added from line 2 to line 5, providing more explicit definitions and higher reliability. ChatGPT also recommends adding the negative edge of active-low reset signals in the \u201calways\u201d block in line 9 of the revised code. Du et al. [14] explained that such an asynchronous reset is more reliable and the system can make instant responses when detecting errors, without waiting for the rising edge of the clock signal. In addition, code validation is also an important task for telecom project development. Du et al. [14] utilized ChatGPT to generate an error-free testbench for effective OpenWiFi project validation. However, the fine-tuning process is not investigated, which can be a prerequisite for effectively generating hardware description languages. Different from the aforementioned studies, Thakur et al. [121] fine-tuned a pretrained LLM on Verilog datasets collected from GitHub and textbooks, demonstrating that fine-tuned LLMs can improve the coding correct rate by 26% on a specific language.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ee2/2ee26cc0-c309-466c-b8c9-7a43a45c80e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. Using ChatGPT to improve the code quality of OpenWiFi project [14] (The pink fonts show the main changes).</div>\n2) LLM-aided code generation with multi-step scheduling: Previous sections have shown that LLM can be used for fundamental coding tasks. However, real-world telecom project development is usually much more complicated by including multi-step scheduling and several sub-tasks. Xiang et al. applied LLM to regenerate the code of existing studies in [18], and the authors suggested that ChatGPT does not respond well to monolithic prompts like \u201dimplement this technique in the following steps\u201d. Instead, a more practical method is to send a detailed modular prompt each time. Such a step-by-step approach is also investigated in [119] and [14]. Specifically, Mani et al. [119] applied LLMs to network graph manipulation, and the prompt design is decoupled into the application prompt and code generation prompt. Specifically, the application prompt can provide task-specific prompts based on templates and user queries, and then the code generation prompt can use plugins and libraries to instruct LLMs. The experiment shows that combining the LLM with proper libraries, such as GPT-4 and NetworkX, can achieve 88% and 78% coding accuracy for traffic analysis and network lifecycle management tasks, respectively. Du et al. investigated a more complicated coding task in [14] by using Verilog to build a Fast Fourier Transform (FFT) module. A failure is first observed by using the following prompt:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5abe/5abe68ac-7993-41a2-8338-7ba246c0665f.png\" style=\"width: 50%;\"></div>\ncomplicated task with several sequential or parallel subtasks; b) The LLM lacks the capabilities of multi-step scheduling. To this end, the authors decouple the problem into four steps: \u2022 Step 1: Asking ChatGPT to generate two simple IP cores that are frequently used in the following FFT design:\ncomplicated task with several sequential or parallel subtasks; b) The LLM lacks the capabilities of multi-step scheduling. To this end, the authors decouple the problem into four steps:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/735d/735d42ac-e7f0-4881-b691-485caf19ac21.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\u2022 Step 2: Showing ChatGPT a simple 2-point FFT example with templates and suggestions and then asking ChatGPT to produce a 4-point FFT IP core:</div>\n\u2022 Step 2: Showing ChatGPT a simple 2-point FFT example with templates and suggestions and then asking ChatGPT to produce a 4-point FFT IP core:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5747/5747bed8-ae4f-49f1-9c7d-d0eab300bdd7.png\" style=\"width: 50%;\"></div>\n\u2022 Step 3: Asking ChatGPT to develop an eight-point FFT module based on the generated 4-point FFT in Step 3:\n\u201dI am writing an eight-point DIF-FFT on FPGA. Apart from IP cores Given in Question One,..., you can also use the fft 4 point IP core generated in Answer one. You need to look back to Question1 and Answer-1 for detailed input/output information on the four IP cores. Once again, I want to emphasize that:...\u201d\n Step 4: Finally, asking ChatGPT to generate a 16-point FFT using the 8-point FFT that has been generated in Step 3. This step is repeated in [14] by asking for a 2Npoint FFT module based on previously generated N-point FFT modules.\nSteps 1-4 is an obvious step-by-step CoT approach. Instead of asking for an 8-point FFT module directly, it starts from two simple IP cores and then provides examples of 2-point FFT modules with detailed suggestions. This is a very useful technique for LLM-aided project design in telecom networks, decoupling the objective into several steps with detailed examples and suggestions. Finally, we summarize some key lessons from existing studies on the use of LLM for code generation. Firstly, stepby-step prompt design is an important lesson that has been demonstrated in several existing studies [14], [18], [119]. Decoupling the complicated multi-step scheduling problem into several stages will lower the difficulty for LLM\u2019s understanding. For instance, in 5G cloud RAN simulation, we can divide the network into cloud, edge, and users, and then use LLM to generate the code for each part sequentially. Secondly, examples and pseudo-code are important for code generation. The LLM has excellent ICL capabilities, quickly learning from examples and generalizing to other scenarios. Xiang et al. [18] also reveal that implementation with pseudocode first can produce stabilized data types and structures, avoiding other changes when implementing the following components. There have been many codes for the telecom field in GitHub and textbooks, taking advantage of these existing examples is crucial to use LLM techniques. Then, a significant amount of human effort can be saved in code generation by using LLMs for debugging and testing. Xiang et al. [18] also shows that most errors can be solved by sending the error message to the LLM. Many of these errors are related to data types, which can be avoided by specifying key variables\u2019 data types. This lesson is also proved in [14], in which the LLM specified the data types of inputs and outputs to improve the reliability of existing code. Finally, LLM-aided coding can lower the requirement for professional knowledge [14], [18]. In particular, Du et al. [14] show that both undergraduate and graduate students can benefit from the assistance of LLMs, achieving comparable coding qualities. Xiang et al. [18] prove that undergraduate students can reproduce the results of some existing network studies by using the LLM.\nNetwork administrators orchestrate the flow of information within a network. They can guide data from source to destination by configuring a complex set of parameters for network elements. These configurations impact a wide range of devices and services, such as switches, routers, servers, and network interfaces. To ensure a reliable data stream, these settings require precise calibration across all network functionalities. Over the past ten years, both academic institutions and the commercial sector have embraced the concept of Software-Defined Networking (SDN) [132] as a means to streamline network management, marking a shift away from the older, more rigid networking models. SDN offers numerous advantages; nonetheless, adjusting network settings remains a task that often requires manual input. Such manual adjustments can be expensive, as they demand the skills of specialized developers familiar with various network protocols, and meanwhile such manual configurations are also intricate and prone to errors. Numerous initiatives have been launched with the aim of streamlining the translation of overarching network guidelines into individual settings for each network component. Such efforts focus on reducing human errors by creating verifiable and reliable configuration outputs through rigorous checks [133], [134]. Nonetheless, setting up network configurations is still considered as a labour-intensive, intricate, and costly endeavour for network operators. Recent advancements have demonstrated that the LLM possesses the ability to generate cohesive and contextually relevant content. They can answer questions and sustain in-depth conversations with users. Applications like GitHub Copilot and Amazon CodeWhisperer exemplify these advancements, assisting with a variety of programming-related tasks. These developments inspire confidence that the LLM can also be utilized to generate network configurations [24], [122]. One notable development of LLM-aided network configuration is CloudEval-YAML [24], a benchmark that provides a realistic and scalable assessment framework specifically for YAML configurations in cloud-native applications. This benchmark utilizes a hand-crafted dataset and an efficient evaluation platform to thoroughly examine the performance of LLMs within this context. Dzeparoska et al. [122] have introduced a pioneering method that employs the few-shot learning capabilities of the LLM to automate the translation of high-level user intents into executable policies. This approach facilitates dynamic, automated management of applications without the necessity for predefined procedural steps. In a related vein, Wang et al. [123] have developed NETBUDDY, a multi-stage pipeline that leverages LLMs to translate highlevel network policies specified in natural language into lowlevel device configurations. NETBUDDY first uses an LLM to convert the input into a formal specification, such as a data structure to express reachability. It then generates forwarding information and configuration scripts from the formal specification. Finally, NETBUDDY interacts with an LLM multiple times to sequentially provide topology, addressing details and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ded/1dedaf79-ea75-402c-897c-085d3ddf6ebf.png\" style=\"width:",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to provide a comprehensive overview of LLM-enabled telecom networks, addressing the potential of LLM techniques to automate various tasks in the telecommunication field and exploring their implications for future 6G networks.",
            "scope": "The survey encompasses LLM fundamentals, key techniques, and applications in telecom, including generation, classification, optimization, and prediction problems. It excludes in-depth discussions on non-telecom applications of LLMs."
        },
        "problem": {
            "definition": "The survey focuses on the integration of large language models (LLMs) in telecommunications, particularly their application in managing the complexity of next-generation networks (6G).",
            "key obstacle": "The primary challenges include the adaptation of general-domain LLMs to telecom-specific tasks, the requirement for domain-specific datasets, and the complexities involved in multi-step planning and reasoning."
        },
        "architecture": {
            "perspective": "The survey categorizes existing research into LLM fundamentals, applications in generation, classification, optimization, and prediction, highlighting the unique capabilities of LLMs in understanding and processing telecom-specific data.",
            "fields/stages": "The survey organizes current methods and research into categories such as LLM fundamentals, generation applications, classification tasks, optimization techniques, and prediction models, providing a structured view of the landscape."
        },
        "conclusion": {
            "comparisions": "The survey compares various LLM techniques across different applications, noting differences in effectiveness and approach, particularly in how LLMs can be tailored for specific telecom tasks versus general applications.",
            "results": "Key findings indicate that LLMs can significantly enhance efficiency in telecom operations, automate complex tasks, and provide valuable insights, although real-world applications are still in early stages."
        },
        "discussion": {
            "advantage": "The strengths of existing research include the versatility of LLMs in handling diverse telecom tasks, the ability to automate complex processes, and the potential to improve decision-making and operational efficiency.",
            "limitation": "Current research is limited by the availability of telecom-specific datasets, the challenges of fine-tuning LLMs for specific tasks, and the potential for inaccuracies in model outputs.",
            "gaps": "Unanswered questions include how to effectively collect and preprocess telecom-specific data for LLM training and the need for robust evaluation metrics tailored to the telecom domain.",
            "future work": "Future research should focus on developing specialized LLMs for telecom applications, improving fine-tuning techniques, and exploring multi-modal LLM capabilities to enhance performance in real-world scenarios."
        },
        "other info": {
            "authors": [
                "Hao Zhou",
                "Chengming Hu",
                "Ye Yuan",
                "Yufei Cui",
                "Yili Jin",
                "Can Chen",
                "Haolun Wu",
                "Dun Yuan",
                "Li Jiang",
                "Di Wu",
                "Xue Liu",
                "Charlie Zhang",
                "Xianbin Wang",
                "Jiangchuan Liu"
            ],
            "publication year": 2023,
            "keywords": [
                "Large language model",
                "telecommunications",
                "generation",
                "classification",
                "optimization",
                "prediction"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The survey provides definitions and explanations of key concepts such as large language models (LLMs) and their applications in telecommunications."
        },
        {
            "section number": "2.2",
            "key information": "The survey discusses the historical development and evolution of recommendation systems, particularly in the context of LLMs in managing the complexity of next-generation networks (6G)."
        },
        {
            "section number": "4.1",
            "key information": "The survey describes the capabilities of LLMs in processing and understanding telecom-specific data, highlighting their unique strengths in automation and decision-making."
        },
        {
            "section number": "4.2",
            "key information": "The integration of LLMs into telecommunications is analyzed, emphasizing how LLMs can enhance operational efficiency and automate complex tasks."
        },
        {
            "section number": "10.1",
            "key information": "The survey identifies challenges in adapting general-domain LLMs to telecom-specific tasks, including the need for domain-specific datasets and the complexities of multi-step planning."
        },
        {
            "section number": "10.2",
            "key information": "Future research opportunities include developing specialized LLMs for telecom applications and improving fine-tuning techniques."
        }
    ],
    "similarity_score": 0.7510846566938292,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Large Language Model (LLM) for Telecommunications_ A Comprehensive Survey on Principles, Key Techniques, and Opportunities.json"
}