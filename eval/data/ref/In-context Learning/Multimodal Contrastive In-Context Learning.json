{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.12959",
    "title": "Multimodal Contrastive In-Context Learning",
    "abstract": "The rapid growth of Large Language Models (LLMs) usage has highlighted the importance of gradient-free in-context learning (ICL). However, interpreting their inner workings remains challenging. This paper introduces a novel multimodal contrastive in-context learning framework to enhance our understanding of ICL in LLMs. First, we present a contrastive learning-based interpretation of ICL in real-world settings, marking the distance of the key-value representation as the differentiator in ICL. Second, we develop an analytical framework to address biases in multimodal input formatting for real-world datasets. We demonstrate the effectiveness of ICL examples where baseline performance is poor, even when they are represented in unseen formats. Lastly, we propose an on-the-fly approach for ICL (Anchored-byText ICL) that demonstrates effectiveness in detecting hateful memes, a task where typical ICL struggles due to resource limitations. Extensive experiments on multimodal datasets reveal that our approach significantly improves ICL performance across various scenarios, such as challenging tasks and resource-constrained environments. Moreover, it provides valuable insights into the mechanisms of in-context learning in LLMs. Our findings have important implications for developing more interpretable, efficient, and robust multimodal AI systems, especially in challenging tasks and resource-constrained environments.",
    "bib_name": "miyanishi2024multimodalcontrastiveincontextlearning",
    "md_text": "# Multimodal Contrastive In-Context Learning\nYosuke Miyanishi1,2, Minh Le Nguyen1\n# Yosuke Miyanishi1,2, Minh Le Nguyen1\n1Japan Advanced Institute of Science and Technology 2CyberAgent Inc. yosuke.miyanishi@jaist.ac.jp, nguyenml@jaist.ac.jp\n# Abstract\nThe rapid growth of Large Language Models (LLMs) usage has highlighted the importance of gradient-free in-context learning (ICL). However, interpreting their inner workings remains challenging. This paper introduces a novel multimodal contrastive in-context learning framework to enhance our understanding of ICL in LLMs. First, we present a contrastive learning-based interpretation of ICL in real-world settings, marking the distance of the key-value representation as the differentiator in ICL. Second, we develop an analytical framework to address biases in multimodal input formatting for real-world datasets. We demonstrate the effectiveness of ICL examples where baseline performance is poor, even when they are represented in unseen formats. Lastly, we propose an on-the-fly approach for ICL (Anchored-byText ICL) that demonstrates effectiveness in detecting hateful memes, a task where typical ICL struggles due to resource limitations. Extensive experiments on multimodal datasets reveal that our approach significantly improves ICL performance across various scenarios, such as challenging tasks and resource-constrained environments. Moreover, it provides valuable insights into the mechanisms of in-context learning in LLMs. Our findings have important implications for developing more interpretable, efficient, and robust multimodal AI systems, especially in challenging tasks and resource-constrained environments.\narXiv:2408.12959v1\n# Introduction\nUpon the explosive usage of the Large Language Model (LLM), in-context learning (ICL) characterizes LLM\u2019s reasoning process. Understanding its optimization mechanism is critical for reliable, evidence-based decision-making. Previous works have shown that LLMs could optimize the attention weights in the gradient-free inference. The research scope, however, is mostly limited to simple problems like linear regression or word-level natural language inference. The recent advances in multimodal LLM present us with more challenges. First, in addition to the linguistic format dependencies, which seem trivial to humans, multimodal ICL involves arbitrarily formatted multiple modalities. Although the research community proposes many approaches for solutions in different contexts, the impact of the multimodal ICL input formatting remains elusive. Second, exploring effective in-context examples is demanding due to the limited source of high-quality multimodal datasets com-\npared to those of single modality. To achieve a deeper understanding of LLM, as the gradient descent hinted at attention-based optimization, the existing gradient-based learning method could help interpret how it optimizes in ICL. Specifically, Contrastive Learning (CL), typically used for modality encoders and classic language models, could guide the model in mapping semantically similar inputs to a similar location in feature space. Based on the previous theoretical findings about the equivalence of LLM\u2019s learning process and CL, we show that CL helps interpret how LLM understands multimodal ICL semantics under unseen input formatting and/or resource shortage. Our contribution could be summarized as follows: 1. We propose a first CL-based interpretation of ICL in multimodal settings, suggesting that the semantically similar ICL examples trigger the representational shift dependent on the problem settings. 2. We propose a CL-based analytical framework for the bias of multimodal input formatting and show that semantically similar ICL examples could be helpful in challenging tasks even when presented in an unseen format. 3. We propose Anchored-by-Text ICL, an on-the-fly inference in which LLM first generates the ICL example and then performs the inference using the generated example as an anchor for extracting the input-label relationship. This approach has shown effectiveness in resourcelimited settings.\n# Related Work\nIn these few years, LLMs have been widely adapted to natural language processing (Zhao et al. 2023b), showing remarkable in-context learning (ICL) performance (Brown et al. 2020) with up to a few examples and without gradientbased training. Massive work has tested their multimodal capabilities (Zhang et al. 2024) centered on vision and language as a step toward general-purpose agents.\n# Interpreting Inner Workings\nTo achieve Trustworthy AI (Thiebes, Lins, and Sunyaev 2021), understanding how LLMs achieve high ICL performance is imminent. Various interpretations have been proposed to obtain theoretical and empirical grounding behind\nICL. Typically, the interpretation studies hire a specific algorithm to interpret the dynamics of LLM\u2019s representations: for example, Bayesian inference (Xie et al. 2022), kernel regression (Han et al. 2023), latent variable model (Wang et al. 2023c), algorithm selector (Li et al. 2023b), multi-state RNN (Oren et al. 2024), and gradient descent (von Oswald et al. 2023; Dai et al. 2023). Although these studies covered extensive theoretical aspects, most empirical findings are limited to simple problems like linear modeling or simple NLP tasks, let alone multimodal settings. To demystify LLM\u2019s remarkable multimodal ICL capabilities, its training and evaluation procedure should be a clue. Most LLMs are trained to maximize the predicted probability of the tokens in the training datasets (Shlegeris et al. 2024). In multimodal problems, non-language information (e.g., image) is encoded as captioned text (e.g. Miyanishi and Nguyen (2024)) or soft prompt (e.g. Bulat and Tzimiropoulos (2023)). At inference time, ICL frameworks mostly anchor the semantically similar examples to the test input (Liu et al. 2022; Wang, Yang, and Wei 2024; Li et al. 2023c), making it intuitive to hypothesize that the distance between ICL example and test input plays a crucial role in ICL. Here, we formally and empirically show that multimodal input distance, coded during the LLM\u2019s training procedure, plays an crucial role in understanding the ICL inputs. To provide such an distance-oriented view of ICL, Contrastive Learning (CL) (Le-Khac, Healy, and Smeaton 2020) could play an pivotal role. CL was initially developed as an unsupervised approach for training data distribution, and then Khosla et al. (2020) introduced supervised CL for labeled datasets. Before the paradigm shift to generative models, CL was a major pre-training objective of mainstream language models based on a Transformer (Vaswani et al. 2017) encoder like BERT (Devlin et al. 2019). In the LLM era, its main application is multimodal (e.g. vision and language) alignment (Hu et al. 2024). CL is rather straightforward for capturing the cross-input semantics since it is designed to map the inputs to the feature space based on conceptual similarity. Recently, Ren and Liu (2023) has theoretically analyzed the equivalence of ICL and supervised CL without negative examples and has shown its validity in simple mathematical problem-solving. In addition, we extend the analysis to the multimodal real-world datasets and propose that the semantically similar ICL examples trigger the representational shift in LLMs.\n# Input Formatting\nPrompt engineering (Bozkurt and Sharma 2023) has tackled the optimization of the instruction and task description. In addition to the textual information, multimodality (Wang et al. 2023b) poses a new challenge - how LLMs could understand the interleaved inputs of multiple information sources. Focusing on the image-text relationship, the most straightforward format is an image followed by a single instruction (e.g., a single visual question-answering entry), targetted by the most state-of-the-art multimodal models like LLaVA (Liu et al. 2023b). Another popular format is multi-turn conversation (Feng et al. 2023; Morgan et al.\n2023), with which the model should recognize at least the two lines of text interleaved by two images. Recent studies have tackled this problem with tailored pre-training protocol (Zheng, He, and Wang 2023) and/or instruction tuning (Li et al. 2023a; Tang et al. 2023). In line with these works, this paper quantitatively shows how the unseen format biases the LLM\u2019s comprehension of the ICL example, and that semantics-formatting balance works differently for the different tasks.\n# Resource Shortage\nLike the limited vision-and-language ability of humans with less visual experience (L\u00b4opez-Barroso et al. 2020; Mamus et al. 2023), the resource shortage is a significant challenge for vision-oriented models (Bai et al. 2023). Since typical ICL involves example selection from training subset of a given task, task-specific multimodal resources, like hateful memes detection datasets (Kiela et al. 2020; Gomez et al. 2020), constrains the ICL performance. One approach to this problem is to let the LLMs generate ICL examples for their own usage. For example, Wang, Yang, and Wei (2024) framed this problem into retrieval, and Coda-Forno et al. (2023) has shown that LLMs can perform meta-learning via ICL. Notably, in some cases like hateful memes, forcing state-of-the-art LLMs to generate positive examples is challenging for safety reasons. This paper shows that LLMgenerated negative examples shift the model\u2019s representation, and mitigate this positive example constraint.\n# Preliminaries\nTransformer\u2019s self-attention layer of depth d maps input document D to query Q, key K, value V with corresponding weight matrix W. An layer is written as:\n(1)\nIn case of generating the answer a for a set of the documents Dicl = {Dquery, Dex} consisting of the query Dquery with the ICL example Dex, the predicted most probable answer \u02c6y is obtained as:\n# ICL and CL\nICL and CL\nICL and CL CL typically utilizes contrastive loss (Hadsell, Chopra, and LeCun 2006) with which the document pair (D1, D2) is mapped to the representation space with the guidance of a binary yc (1 suggests that the documents are in a specific category, 0 otherwise). Given a distance function dist(\u00b7, \u00b7), the loss L with hyperparameter \u03f5 is defined as:\nL(D1, D2) = ycdD1/2 + (1 \u2212yc)max(\u03f5 \u2212dD1/2, 0) where dD1/2 = dist(D1, D2) (\n(3)\nIn inference time, the learned function fCL maps the new input Dtest to the representation space, and a dedicated function fproj projects that representation to \u02c6y.\n(4)\nRen and Liu (2023) has shown that ICL could be seen as CL without negative examples. They suggested that a selfattention layer could be seen as a contrastive learner. More specifically, with the help of a kernel function \u03d5, a single layer minimizes the distance between two different augmentations \u02c6xK, \u02c6xV of an identical training data point\u2019s representation h.\n(5)\nNote that the category label yc is omitted for the absence of the negative class. After the input passes through a single model layer, it gets the new representation h\u2032, embeds it to the same feature space using the query weight WQ, and obtains the inference output \u02c6y using the updated weight \u02c6W.\n(6)\nHereafter, we omit the learning rate \u03b7 for brevity. Since the weight update \u2206L is a function of key-value distance, we denote the update as \u2206L(K, V ), and its resulting (ICLoptimized) weight as Wicl. This paper factorizes the realworld learning process and empirically shows its significance.\n# Mixed Effect Model\nMixed effect model (Singmann and Kellen 2019) has been proposed to disentangle the dual effects of the variables within the same model. Specifically, in observation i, the effect of some variables X over the target variable yi is expected to be identical across all the observations (fixed effect), and another variables Z affect individual (group of) observation differently (random effect). Linear mixed effect model could be formalized as:\n(7)\nFor example, if we are to analyze the effect of a new teaching method on student performance across different schools in a city, the method should have a fixed effect since, in general, such a method aims for equal educational opportunities. In contrast, the school variable should have a random effect since each school must have a different educational policy. Note that various non-linear expressions of the mixed effect are proposed (e.g. Hajjem, Bellavance, and Larocque (2014); Sigrist (2023)), but we limit the scope to the linear model for brevity.\nICL Example Selection\nIn ICL, the example Dex is typically extracted from the training dataset or its subset \ufffdDtrain to obtain the closest example to the test input Dquery.\n(8)\nWe show the effectiveness of generating the example instead of selecting it and discuss how it is related to CL.\n# Methodology\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9f3/b9f3eb06-72c3-4aa9-ac0c-4429eea11795.png\" style=\"width: 50%;\"></div>\nFigure 1: Summary of the proposed method. Boxes represent data, while circles symbolize procedures and models. Images are taken from Hateful Memes (Kiela et al. 2020) and MMBench (Liu et al. 2023c) datasets.\n# Representational Shift Hypothesis\nIn CL interpretation (Eq.5-6), key-value distance contributes to attention-based optimization in ICL. Since this interpretation only presupposes the interaction of key-value pair, we could extend it to the arbitrary set of test-time input fractions (e.g. instruction prompt Kinst and the task given in zero-shot setting Vzsl). Since the zero-shot task consists of the instruction, the task, and LLM\u2019s prediction pred, and the model is trained to infer the latter tokens from the former, we propose that the distance among the zero-shot components affects the generation as follows:\n(9)\nSimilarily, the updated ICL weight Wicl could be formal ized as:\n(10)\nAssuming that the overall instruction affects each task equally \u2206W(Kinst, Vzsl) \u2243\u2206W(Kinst, Vicl), the weight\n(and resulting representation) shifts towards example-task distance.\n(11)\nIn summary, the ICL example first affects the representation of the zero-shot task, and the prediction is affected via the task-prediction representational shift. To test whether this hypothesis is correct, we analyze the distance-distance relationship.\n# Multimodal Input Formatting\nDisentangling Format and Semantics The semantics of the bimodal inputs and their format are entangled yet different concepts. Since CL aims to learn the inputs\u2019 similarity and variance, we could assume semantic similarity as its objective while formatting as a biasing factor. In other words, the formatting term Lfmt affects the actual loss L in parallel with its semantic term Lsem.\n(12)\nIntuitively, within a single dataset, the second term consistently biases all the ICL examples (fixed effect). In contrast, the first term should also reflect the variance of the individual test data points (random effect). Therefore, when the model faces a test input with an unseen format, the model output for input i should be interpreted as a mixed model.\n(13)\nWhen the ICL format is the same with the training process, \u2206Lfmt = 0.\nWhen the ICL format is the same with the training process, \u2206Lfmt = 0. Model Performance Analysis In the macroscopic view, the effect of the unseen format should be expressed as the impact of bias b \u2208{0, 1}, and that of ICL example presence e \u2208{0, 1}. Intuitively, the ICL examples have random effects due to the dependence on the content of each example. In contrast, the format bias should have a fixed effect, affecting the overall performance. Note that we use accuracy as the metric unless stated otherwise since all Visual Question Answering (VQA) datasets used in our analysis hire this metric. Together, the model accuracy acc of a data subset i could be modeled as:\n(14)\nTo analyze the impact across the models, the results of all the models are concatenated and the variables b and e are analyzed as an interaction term. Representation Analysis Since some of the widely used benchmarks like MMBench (Liu et al. (2023c)) require online submission for evaluation, which makes reproducible local evaluation challenging, we need the unsupervised approach. Under the our hypothesis, ICL is driven by the distance d\u00b7/\u00b7 between the representation of the key hk and that of value hv. In zero-shot VQA, the distance between question hq and answer ha would be the only clue to the model.\nTo analyze the impact across the models, the results of all the models are concatenated and the variables b and e are analyzed as an interaction term.\nIn contrast, the ICL example is concatenated to the question hicl = {hex, hq}, leading to the shift in the feature space and, therefore, distance with the new answer h\u2032 a. In the spirit of the linear representation hypothesis (Park, Choe, and Veitch (2023)), we implement a linear mixed effect model. Specifically, the random effect is modeled as the linear weight Wrandom, and the fixed effect is introduced via the product of hzcl and the embedded index representing the model and the dataset with the weights Wfixed.\n(15)\nFinally, we model the linear relationship between the queryanswer distance matrix and the shifted query-new answer matrix.\n(16)\n\u2032a   As a baseline, we use the model only with first term hicl = Wrandomhzcl, or a simple linear projection.\n# Anchored-by-Text ICL\nGeneration Strategy Two major blockers must be addressed for on-the-fly ICL example generation on hateful memes. First, it requires text-image bimodal generation. Since only limited models (e.g. Wu et al. (2023)) have such capability, we use the generated text as an anchor to cause a representational shift, and therefore the prediction. Hereafter we call it anchored-by-text ICL (AbT ICL). Second, most LLMs have safety limitations based on instruction tuning (Bianchi et al. (2023)). Since bypassing such limitations is neither desirable nor sustainable, we let the model generate negative examples. Together, given that document D consists of text T and image I (D = (T, I)) with a binary label y (0 for benign, 1 for hateful), our strategy is formalized as:\n(17)\n {} In short, the model generates text that fits with a given image to compose a benign meme and uses that meme as a benign example. Baselines include zero-shot and one-shot detection. Fig.2 shows a representative prompt aiming for this goal. Qu et al. (2023) introduced another workaround of using more general labels, which will be a part of our future work. Representation / Prediction Analysis Since hateful memes detection could be framed into binary classification in this experiment, we model the effect of the key-value distance (Eq. 13) over the predicted label y on three learning types lt (zero-shot zsl, ordinary ICL icl, and AbT ICL abt), and analyzed the difference of the weights W and the intercept W0 as an effect of the representational shift. For example, the effect of AbT over that of ordinary ICL could be formalized as:\n  (18)\n(18) The weights W lt and W lt 0 are estimated per layer dimension to perform the memory-efficient analysis.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c85d/c85dbd1a-69c1-47a9-9307-c43e15a3f94d.png\" style=\"width: 50%;\"></div>\nFigure 2: The representative Anchor-by-Text ICL prompt. The system prompt is truncated for illustrative purpose1.\n<div style=\"text-align: center;\">Figure 2: The representative Anchor-by-Text ICL prompt. The system prompt is truncated for illustrative purpose1.</div>\n# Experimental Settings\nExperiments are conducted on a single NVIDIA A100 80GB GPU with Linux OS. Unless stated otherwise, all codes are in Python 3.9. Statistical arguments are based on a t-test and bootstrapping with 1,000 resamples. We run the models once with a random seed of 1987.\n# Experiment I: Multimodal Input Formatting\nModel To disentangle the effect of input semantics and that of the formatting, the subject model in this paper should 1) have the expected maximum capability of understanding the semantics and 2) is NOT trained or fine-tuned on a multi-image setting. We primarily focus on LLaVA (Liu et al. 2023b) to satisfy this criterion. More specifically, we use two variants: LLaVA-Llama2 for its high performance of the linguistic backbone (Touvron, Martin, and Stone (2023)) and LLaVA 1.5 for its highest performance on vision-andlanguage tasks (Liu et al. (2023a)). 13 billion parameter models are used for memory constraints. We also use InternVL (1-8 billion) for their limited 2 yet tested multi-image capabilities by multi-image datasets like MMMU (Yue et al. 2024). To select ICL examples most similar to test inputs, CLIP (Radford et al. (2021), specifically HuggingFace clip-vitlarge-patch14) is used because of its relatively small computational cost and its high capability on similarity-related tasks (e.g., image aesthetics evaluation3). We take the last layer as a representation for its high correspondence with the generated tokens despite the presence of highly competitive short-cutting (Din et al. 2024; Fan et al. 2024). Dataset To cover various aspects of multimodal LLM\u2019s capabilities, we tested our approach with six VQA datasets, namely VQA v 2.0 (Goyal et al. (2017)), GQA (Hudson and Manning (2019)), VizWiz (Gurari et al. (2018)), TextVQA (Singh et al. (2019)), MMBench (Liu et al. (2023c)), and MM-Vet (Yu et al. (2023)).\n2https://github.com/OpenGVLab/InternVL/issues/419 3https://laion.ai/blog/laion-aesthetics/\nModel Accuracy Analysis Practically, the presence of the random and fixed effect (z and e in Eq. 13, respectively) is represented as a coefficient of the corresponding one-hot encodings. The performance of the mixed effect model is evaluated using the marginal/conditional R2 method (Nakagawa and Schielzeth (2013)). To maintain the experiment\u2019s integrity while utilizing a wide range of statistical tools, the R language\u2019s lmer package is called from the Python environment via rpy24 module. Representation Analysis The linear mixed model and the baseline linear model are implemented with PyTorch backend5 and trained to maximize the cosine similarity between the representation via Pytorch Metric Learning package6 and AdamW optimizer ((Loshchilov and Hutter 2019)). We extract 1,000 samples from each dataset and hold out 20% as a test set.\nModel Accuracy Analysis Practically, the presence of the random and fixed effect (z and e in Eq. 13, respectively) is represented as a coefficient of the corresponding one-hot encodings. The performance of the mixed effect model is evaluated using the marginal/conditional R2 method (Nakagawa and Schielzeth (2013)). To maintain the experiment\u2019s integrity while utilizing a wide range of statistical tools, the R language\u2019s lmer package is called from the Python environment via rpy24 module.\nRepresentation Analysis The linear mixed model and the baseline linear model are implemented with PyTorch backend5 and trained to maximize the cosine similarity between the representation via Pytorch Metric Learning package6 and AdamW optimizer ((Loshchilov and Hutter 2019)). We extract 1,000 samples from each dataset and hold out 20% as a test set.\n# Experiment II: AbT ICL\nIntuitively, the impact of AbT ICL may vary across datasets. The most influential scenario is 1) when the dataset size is small and suffers from high variance, making the example selection infeasible 2) when explicit and strong cross-modal interaction affects the dataset. Kiela et al. (2020) curated the Hateful Memes Challenge dataset, which perfectly fits this experiment\u2019s criteria. Initially, Laurenc\u00b8on et al. (2023) and Zhao et al. (2023a) have shown that ICL is not particularly effective unless the task is heavily tuned to the task. Moreover, Hee, Lee, and Chong (2022) and Miyanishi and Nguyen (2024) theoretically and empirically showed that the cross-modal interaction embedded in the hateful memes detection problem is fully reflected in this dataset. We leave more experiments on hateful meme detection (Gomez et al. (2020)) and other tasks to future work. Model To comply with Experiment I, we use LLaVALlama2 in this experiment. For ICL example selection, we\nModel To comply with Experiment I, we use LLaVALlama2 in this experiment. For ICL example selection, we use BM25 algorithm (Robertson et al. 1996).\nDataset We focus on the Hateful Memes Challenge dataset (Kiela et al. 2020) to test our framework in the context of complex multimodal interaction. Taking into account the presence of the image confounders (two memes with identical text and different images, resulting in different labels), the one-shot experiment adopts the ICL examples with most similar texts (one meme from hateful, one meme from benign) in the labeled training set, and use the two confounders as a single set of ICL example. Since the data size is small, we use f1 score to see the precision-recall balance.\n# Results & Discussion\nExperiment I: Multimodal Input Formatting Motivation If the representational shift hypothesis is c rect, the ICL examples could affect the prediction even\nMotivation If the representational shift hypothesis is correct, the ICL examples could affect the prediction even if\n4https://rpy2.github.io/doc.html 5https://pytorch.org/ 6https://kevinmusgrave.github.io/pytorch-metric-learning/\n4https://rpy2.github.io/doc.html 5https://pytorch.org/ 6https://kevinmusgrave.github.io/pytorch-metric-learning/\ngiven in a format different from that of the training. The preliminary analysis shows that LLaVA (Liu et al. 2023b), a model not trained by multi-image datasets, can explain multiple images per prompt separately under some constraints (Supplementary Fig.1). Based on this observation, our working hypothesis for Experiment I is that, although LLMs are heavily affected by the prompt format, they could interpret the semantics without solid inductive bias to some extent. We focus on ICL with a single example since we do not see any positive clue for further concatenation in the initial exploration. Performance Fig.3 and Supplementary Fig.4 summarize the performance of two LLaVA variants with or without the input of unseen format. Not surprisingly, LLaVA v1.5 outperforms v1 in all cases. Since the models are not trained with multiple-image datasets, the majority of the datasets show dropped performance in ICL. Interestingly, for LLaVA-Llama2, however, two image-text pairs boost the performance in some cases where the base performance is very low. This result supports the presence of semanticsbased ICL, particularly when the task is challenging. In the\ngiven in a format different from that of the training. The preliminary analysis shows that LLaVA (Liu et al. 2023b), a model not trained by multi-image datasets, can explain multiple images per prompt separately under some constraints (Supplementary Fig.1). Based on this observation, our working hypothesis for Experiment I is that, although LLMs are heavily affected by the prompt format, they could interpret the semantics without solid inductive bias to some extent. We focus on ICL with a single example since we do not see any positive clue for further concatenation in the initial exploration.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b2bb/b2bba0a7-6bd7-4da5-8320-44b93938fac8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Performance summary of LLaVA-Llama2. zsl and icl represent the corresponding learning type in the Methodology section.</div>\nFigure 3: Performance summary of LLaVA-Llama2. zsl and icl represent the corresponding learning type in the Methodology section.\ncase of InternVL, ICL generally resulted in decreased performance, potentially because of its high performance and multi-image resource shortage (Supplementary Fig.2). To see whether the task difficulty affects this trend, we see the performance by the number of reasoning steps, typically seen as the difficulty metric, and is provided in the GQA dataset. Divided by this subcategory, ICL performs slightly better in the larger number of steps, in contrast to the dramatically dropped performance in the smaller number of steps (Table 1, Supplementary Fig.3). Together with LLaVA, these results suggest that the semantics dominate the challenging tasks, while the formatting is more critical in established ones. Model Accuracy Analysis To quantify the impact of formatting and ICL examples, we model the linear mixed effect with or without the variables {z, e}, and the model variable m (Table 2). In general, m predominantly explains the\nN Steps\nN Samples\nZSL\nICL\n1-5\n12,153\n59.7 \u00b1 0.15\n52.5 \u00b1 0.31\n6-9\n65\n83.5 \u00b1 0.24\n84.6 \u00b1 0.27\nTable 1: Impact of multi-image ICL in GQA for InternVL 1b. N steps indicates the number of inference steps. The numbers with error indicates accuracy(%) in the corresponding setting.\naccuracy variation, reflecting much higher performance for LLaVA 1.5. In z-e comparison, e has a slightly higher explanatory power, implying the significance of individual ICL example. This is further supported by the highest power of random effect when combined with m.\nVariable\nR2*100\nFixed\nRandom\nFixed\nRandom\nm\nm\n22.6 \u00b1 3.0\n52.0 \u00b1 8.8\nz\ne\n0.3 \u00b1 0.1\n0.5 \u00b1 0.2\nm\ne\n33.5 \u00b1 2.4\n33.6 \u00b1 2.5\nz\nm\n0.2 \u00b1 0.1\n49.5 \u00b1 2.7\ncomb\ncomb\n23.7 \u00b1 4.4\n53.7 \u00b1 8.8\nTable 2: Fixed and Random Effects.R2 values are multiplied by 100 for brevity. m represents the model (LLaVA 1.5 or LLaVA-Llama2). z and e represents the formatting bias and the presence of ICL example, respectively. comb represents the combined effect of the two variables in the same column. R2 values are multiplied by 100 for brevity.\nRepresentation Analysis First, to see if the representation of the ICL model\u2019s question-answer dinstance vector can be linearly mapped onto a zero-shot vector, we applied a simple linear probe to get moderate explanatory power with an R2 value of 43.0 \u00b1 1.2, suggesting the presence of such mapping. Next, we applied the high-dimensional mixed effect model (Eq. 16), resulting in a much higher R2 59.2 \u00b1 2.3. This result suggests that the representational shift in the presence of formatting bias could be mapped linearly. Next, we attributed the shifted representation to the original one together with the bias information (model and dataset, Table 3). The original score shows much higher than the bias binaries themselves, suggesting that those bias are interactive with model representation. In summary, these results suggest the presence of the linear mapping before/after the representational shift, and its effect could be seen as a mixed effect together with model and dataset.\n# Experiment II: AbT ICL for Hateful Memes\nPerformance In comparison to the zero-shot setting, ICL significantly dropped the performance (Table 4). In contrast, AbT slightly improves the performance. These results suggest the capability of AbT in the absence of effective ICL examples. Further exploration for ineffective ICL problems will be the part of our future works.\nvariable\ncoef*100\n(Intercept)\n9.2 \u00b1 2.1\nmm-vet\n\u22120.75 \u00b1 0.7\nmmbench\n2.81 \u00b1 0.7\ntextvqa\n2.1 \u00b1 0.6\nvizwiz\n0.16 \u00b1 0.7\nvqav2\n\u22120.12 \u00b1 0.6\nmodel\n\u22120.39 \u00b1 0.4\noriginal score\n70.33 \u00b1 5.9\nTable 3: Mapping for the representational shift with bias information.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0631/06310c0e-a73e-4cff-86cc-f821df99b42c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 4: Hateful memes detection performance.</div>\nRepresentation / Prediction Analysis We applied a linear probe between the distance vector and the predicted label to test the explanatory power of the key-value distance over the model prediction. This resulted in a moderate AUC of 75.6 \u00b1 0.90, further supporting the contribution of keyvalue distance to the generation. Next, we extract each dimension\u2019s weight to see how d shifts across the three settings (Fig.4). Interestingly, AbT representation is close to that of ZSL, irrelevant of the labels, while ICL representation is distant. This result suggest that closer representation shift affects positively in case of hatetul memes detection.\n# Discussion\nUpon the previous pioneering study by (Ren and Liu 2023), our study on Multimodal Contrastive In-Context Learning (MCICL) has yielded several important findings that contribute to our understanding of in-context learning in LLMs. 1. Representational Shift Hypothesis: The representation analysis of two experiments supports our hypothesis. This finding provides insights into the mechanisms underlying ICL and suggests potential avenues for further optimization of ICL techniques. 2. Impact of Input Formatting: Our results show that balancing the formatting and semantics of ICL inputs plays a crucial role in ICL performance. 3. Anchored-by-Text ICL: The proposed Anchored-by-Text ICL approach demonstrates effectiveness in resourceconstrained hateful meme detection, important implication for real-world LLM applications.\n# Limitations and Future Work\nWhile our study provides valuable insights, there are several limitations and future research directions that warrant further investigation. Importantly, our experiments focused on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f81/7f81709b-242c-47b5-aaf9-991ca94bbb2c.png\" style=\"width: 50%;\"></div>\nFigure 4: Representational shift across the learning type. Suffixes 0 and 1 represents the weights for benign and hateful\na limited set of multimodal datasets and model architectures. Future work should explore the broader range of multimodal tasks and models, including but not limited to, multiimage tasks such as MMMU (Yue et al. 2024) and missing modality problem (Wang et al. 2023a; Zhao, Li, and Jin 2021). In addition, whether the representational shift causes the outcome variance is still elusive. One idea is to hire a mechanistic approach, such as path patching (Hanna, Liu, and Variengien 2023; Goldowsky-Dill et al. 2023). Training phase mechanisms such as grokking or double descent (Davies, Langosco, and Krueger 2022) should also be part of the research scope.\n# Conclusion\nMCICL enhances our understanding of in-context learning in LLMs by leveraging contrastive learning principles and addressing multimodal input challenges. It demonstrates improved performance in various scenarios, particularly in challenging settings. Our work provides valuable insights but also highlights the need for continued research in multimodal learning complexity. MCICL opens new avenues for enhancing LLM capabilities in multimodal settings, contributing to more robust, efficient, and responsible AI systems. As AI continues to evolve, approaches like MCICL will be crucial in creating more adaptable, interpretable, and effective multimodal AI systems for diverse real-world applications.\nMCICL enhances our understanding of in-context learning in LLMs by leveraging contrastive learning principles and addressing multimodal input challenges. It demonstrates improved performance in various scenarios, particularly in challenging settings. Our work provides valuable insights but also highlights the need for continued research in multimodal learning complexity. MCICL opens new avenues for enhancing LLM capabilities in multimodal settings, contributing to more robust, efficient, and responsible AI systems. As AI continues to evolve, approaches like MCICL will be crucial in creating more adaptable, interpretable, and effective multimodal AI systems for diverse real-world applications.\nTBD\n# References\nBai, Y.; Geng, X.; Mangalam, K.; Bar, A.; Yuille, A.; Darrell, T.; Malik, J.; and Efros, A. A. 2023. Sequential Modeling Enables Scalable Learning for Large Vision Models. arXiv:2312.00785. Bianchi, F.; Suzgun, M.; Attanasio, G.; R\u00a8ottger, P.; Jurafsky, D.; Hashimoto, T.; and Zou, J. 2023. Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models That Follow Instructions. arXiv:2309.07875. Bozkurt, A.; and Sharma, R. C. 2023. Generative AI and Prompt Engineering: The Art of Whispering to Let the Genie Out of the Algorithmic World. Asian Journal of Distance Education, 18(2). Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models Are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Bulat, A.; and Tzimiropoulos, G. 2023. LASP: Text-toText Optimization for Language-Aware Soft Prompting of Vision & Language Models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 23232\u201323241. Vancouver, BC, Canada: IEEE. ISBN 9798350301298. Coda-Forno, J.; Binz, M.; Akata, Z.; Botvinick, M.; Wang, J. X.; and Schulz, E. 2023. Meta-in-Context Learning in Large Language Models. In 37th Conference on Neural Information Processing Systems. New Orleans, LA, USA. Dai, D.; Sun, Y.; Dong, L.; Hao, Y.; Ma, S.; Sui, Z.; and Wei, F. 2023. Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as MetaOptimizers. In Findings of the Association for Computational Linguistics, 4005\u20134019. Association for Computational Linguistics. Davies, X.; Langosco, L.; and Krueger, D. 2022. Unifying Grokking and Double Descent. In MLSafety Workshop, 36th Conference on Neural Information Processing Systems (NeurIPS 2022). New Orleans, LA, USA. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of NAACL-HLT, 4171\u20134186. Din, A. Y.; Karidi, T.; Choshen, L.; and Geva, M. 2024. Jump to Conclusions: Short-Cutting Transformers with Linear Transformations. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, 9615\u20139625. Torino, Italia: ELRA and ICCL. Fan, S.; Jiang, X.; Li, X.; Meng, X.; Han, P.; Shang, S.; Sun, A.; Wang, Y.; and Wang, Z. 2024. Not All Layers of LLMs Are Necessary During Inference. arXiv preprint.\nFeng, J.; Sun, Q.; Xu, C.; Zhao, P.; Yang, Y.; Tao, C.; Zhao, D.; and Lin, Q. 2023. MMDialog: A Large-scale Multiturn Dialogue Dataset Towards Multi-modal Open-domain Conversation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 7348\u20137363. Toronto, Canada: Association for Computational Linguistics. Goldowsky-Dill, N.; MacLeod, C.; Sato, L.; and Arora, A. 2023. Localizing Model Behavior with Path Patching. arXiv preprint. Gomez, R.; Gibert, J.; Gomez, L.; and Karatzas, D. 2020. Exploring Hate Speech Detection in Multimodal Publications. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 1459\u20131467. Snowmass Village, CO, USA: IEEE. ISBN 978-1-72816-553-0. Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; and Parikh, D. 2017. Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In 2017 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Honolulu, HI, The United States of America: IEEE. Gurari, D.; Li, Q.; Stangl, A. J.; Guo, A.; Lin, C.; Grauman, K.; Luo, J.; and Bigham, J. P. 2018. VizWiz Grand Challenge: Answering Visual Questions from Blind People. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3608\u20133617. Salt Lake City, UT, USA: IEEE. ISBN 978-1-5386-6420-9. Hadsell, R.; Chopra, S.; and LeCun, Y. 2006. Dimensionality Reduction by Learning an Invariant Mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR\u201906), volume 2, 1735\u20131742. New York, NY, USA: IEEE. ISBN 978-0-76952597-6. Hajjem, A.; Bellavance, F.; and Larocque, D. 2014. MixedEffects Random Forest for Clustered Data. Journal of Statistical Computation and Simulation, 84(6): 1313\u20131328. Han, C.; Wang, Z.; Zhao, H.; and Ji, H. 2023. In-Context Learning of Large Language Models Explained as Kernel Regression. arXiv:2305.12766. Hanna, M.; Liu, O.; and Variengien, A. 2023. How Does GPT-2 Compute Greater-than?: Interpreting Mathematical Abilities in a Pre-Trained Language Model. In The Thirtyseventh Annual Conference on Neural Information Processing Systems. New Orleans, LA, USA. Hee, M. S.; Lee, R. K.-W.; and Chong, W.-H. 2022. On Explaining Multimodal Hateful Meme Detection Models. In Proceedings of the ACM Web Conference 2022, 3651\u20133655. Virtual Event, Lyon France: ACM. ISBN 978-1-4503-90965. Hu, W.; Xu, Y.; Li, Y.; Li, W.; Chen, Z.; and Tu, Z. 2024. BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. In The 38th Annual AAAI Conference on Artificial Intelligence. Vancouver, BC, Canada: arXiv. Hudson, D. A.; and Manning, C. D. 2019. GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition. Long Beach, CA, USA: IEEE. Khosla, P.; Tian, Y.; Teterwak, P.; Wang, C.; Isola, P.; Maschinot, A.; Krishnan, D.; and Sarna, A. 2020. Supervised Contrastive Learning. In Thirty-Fourth Annual Conference on Neural Information Processing Systems, volume 33, 18661\u201318673. Curran Associates, Inc. Kiela, D.; Firooz, H.; Mohan, A.; Goswami, V.; Singh, A.; Ringshia, P.; and Testuggine, D. 2020. The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes. In Thirty-Fourth Annual Conference on Neural Information Processing Systems. Red Hook, NY, USA. Laurenc\u00b8on, H.; Saulnier, L.; Tronchon, L.; Bekman, S.; Singh, A.; Lozhkov, A.; Wang, T.; Karamcheti, S.; Rush, A. M.; Kiela, D.; Cord, M.; and Sanh, V. 2023. OBELICS: An Open Web-Scale Filtered Dataset of Interleaved ImageText Documents. In Thirty-Seventh Annual Conference on Neural Information Processing Systems. New Orleans, LA, USA. Le-Khac, P. H.; Healy, G.; and Smeaton, A. F. 2020. Contrastive Representation Learning: A Framework and Review. IEEE Access, 8: 193907\u2013193934. Li, B.; Zhang, Y.; Chen, L.; Wang, J.; Yang, J.; and Liu, Z. 2023a. Otter: A Multi-Modal Model with In-Context Instruction Tuning. arXiv:2305.03726. Li, Y.; Ildiz, M. E.; Papailiopoulos, D.; and Oymak, S. 2023b. Transformers as Algorithms: Generalization and Stability in In-context Learning. arXiv:2301.07067. Li, Z.; Xu, P.; Liu, F.; and Song, H. 2023c. Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. arXiv:2307.05052. Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023b. Visual Instruction Tuning. In The Thirty-seventh Annual Conference on Neural Information Processing Systems. New Orleans, LA, USA. Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2022. What Makes Good In-Context Examples for GPT3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 100\u2013114. Dublin, Ireland and Online: Association for Computational Linguistics. Liu, Y.; Duan, H.; Zhang, Y.; Li, B.; Zhang, S.; Zhao, W.; Yuan, Y.; Wang, J.; He, C.; Liu, Z.; Chen, K.; and Lin, D. 2023c. MMBench: Is Your Multi-modal Model an Allaround Player? In WSDM \u201923: Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, 1128\u20131131. L\u00b4opez-Barroso, D.; Thiebaut De Schotten, M.; Morais, J.; Kolinsky, R.; Braga, L. W.; Guerreiro-Tauil, A.; Dehaene, S.; and Cohen, L. 2020. Impact of Literacy on the Functional Connectivity of Vision and Language Related Networks. NeuroImage, 213: 116722.\nLoshchilov, I.; and Hutter, F. 2019. DECOUPLED WEIGHT DECAY REGULARIZATION. In The Seventh International Conference on Learning Representations. New Orleans, LA, USA. Mamus, E.; Speed, L. J.; Rissman, L.; Majid, A.; and \u00a8Ozy\u00a8urek, A. 2023. Lack of Visual Experience Affects Multimodal Language Production: Evidence From Congenitally Blind and Sighted People. Cognitive Science, 47(1): e13228. Miyanishi, Y.; and Nguyen, M. L. 2024. Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: A Case Study on Hateful Memes. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, 2901\u20132916. Torino, Italia: ELRA and ICCL. Morgan, C.; Tonkin, E. L.; Masullo, A.; Jovan, F.; Sikdar, A.; Khaire, P.; Mirmehdi, M.; McConville, R.; Tourte, G. J. L.; Whone, A.; and Craddock, I. 2023. A Multimodal Dataset of Real World Mobility Activities in Parkinson\u2019s Disease. Scientific Data, 10(1): 918. Nakagawa, S.; and Schielzeth, H. 2013. A General and Simple Method for Obtaining R 2 from Generalized Linear Mixed-effects Models. Methods in Ecology and Evolution, 4(2): 133\u2013142. Oren, M.; Hassid, M.; Adi, Y.; and Schwartz, R. 2024. Transformers Are Multi-State RNNs. arXiv:2401.06104. Park, K.; Choe, Y. J.; and Veitch, V. 2023. The Linear Representation Hypothesis and the Geometry of Large Language Models. In The Thirty-seventh Annual Conference on Neural Information Processing Systems. New Orleans, LA, USA. Qu, Y.; He, X.; Pierson, S.; Backes, M.; Zhang, Y.; and Zannettou, S. 2023. On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning. In The 44th IEEE Symposium on Security and Privacy. San Francisco, CA, USA. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139. Ren, R.; and Liu, Y. 2023. In-Context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. arXiv:2310.13220. Robertson, SE.; Walker, S.; Beaulieu, MM.; Gatford, M.; and Payne, A. 1996. Okapi at TREC-4. In The Fourth Text REtrieval Conference (TREC-4), 73. Rubin, D. B. 2008. For Objective Causal Inference, Design Trumps Analysis. The Annals of Applied Statistics, 2(3). Shlegeris, B.; Roger, F.; Chan, L.; and McLean, E. 2024. Language Models Are Better Than Humans at Next-token Prediction. {Transactions on Machine Learning Research. Sigrist, F. 2023. Latent Gaussian Model Boosting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2): 1894\u20131905.\nSingh, A.; Natarajan, V.; Shah, M.; Jiang, Y.; Chen, X.; Batra, D.; Parikh, D.; and Rohrbach, M. 2019. Towards VQA Models That Can Read. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 8309\u2013 8318. Long Beach, CA, USA: IEEE. ISBN 978-1-72813293-8. Singmann, H.; and Kellen, D. 2019. An Introduction to Mixed Models for Experimental Psychology. In Spieler, D.; and Schumacher, E., eds., New Methods in Cognitive Psychology, 4\u201331. Routledge, 1 edition. ISBN 978-0-42931840-5. Tang, Z.; Yang, Z.; Khademi, M.; Liu, Y.; Zhu, C.; and Bansal, M. 2023. CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation. arXiv:2311.18775. Thiebes, S.; Lins, S.; and Sunyaev, A. 2021. Trustworthy Artificial Intelligence. Electronic Markets, 31(2): 447\u2013464. Touvron, H.; Martin, L.; and Stone, K. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention Is All You Need. In Thirty-First Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA. von Oswald, J.; Niklasson, E.; Randazzo, Ettore; Sacramento, Jo\\\u02dc{a}o; Mordvintsev, Alexander; Zhmoginov, Andrey; and Vladymyrov, Max. 2023. Transformers Learn InContext by Gradient Descent. In Proceedings of the 40th International Conference on Machine Learning, volume 1464, 24. Honolulu, HI, USA: JMLR.org. Wang, H.; Chen, Y.; Ma, C.; Avery, J.; Hull, L.; and Carneiro, G. 2023a. Multi-Modal Learning with Missing Modality via Shared-Specific Feature Modelling. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15878\u201315887. Vancouver, BC, Canada: IEEE. ISBN 9798350301298. Wang, J.; Liu, Z.; Zhao, L.; Wu, Z.; Ma, C.; Yu, S.; Dai, H.; Yang, Q.; Liu, Y.; Zhang, S.; Shi, E.; Pan, Y.; Zhang, T.; Zhu, D.; Li, X.; Jiang, X.; Ge, B.; Yuan, Y.; Shen, D.; Liu, T.; and Zhang, S. 2023b. Review of Large Vision Models and Visual Prompt Engineering. Meta-Radiology, 1(3): 100047. Wang, L.; Yang, N.; and Wei, F. 2024. Learning to Retrieve In-Context Examples for Large Language Models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, volume 1, 1752\u20131767. St. Julians, Malta: Association for Computational Linguistics. Wang, X.; Zhu, W.; Saxon, M.; Steyvers, M.; and Wang, W. Y. 2023c. Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning. In The Thirty-seventh Annual Conference on Neural Information Processing Systems. New Orleans, LA, USA. Wu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023. NExT-GPT: Any-to-Any Multimodal LLM. CoRR, abs/2309.05519.\nXie, S. M.; Raghunathan, A.; Liang, P.; and Ma, T. 2022. An Explanation of In-context Learning as Implicit Bayesian Inference. In The Tenth International Conference on Learning Representations. arXiv. Yu, W.; Yang, Z.; Li, L.; Wang, J.; Lin, K.; Liu, Z.; Wang, X.; and Wang, L. 2023. MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. arXiv:2308.02490. Yue, X.; Ni, Y.; Zhang, K.; Zheng, T.; Liu, R.; Zhang, G.; Stevens, S.; Jiang, D.; Ren, W.; Sun, Y.; Wei, C.; Yu, B.; Yuan, R.; Sun, R.; Yin, M.; Zheng, B.; Yang, Z.; Liu, Y.; Huang, W.; Sun, H.; Su, Y.; and Chen, W. 2024. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Seattle, WA, USA: arXiv. Zhang, D.; Yu, Y.; Li, C.; Dong, J.; Su, D.; Chu, C.; and Yu, D. 2024. MM-LLMs: Recent Advances in MultiModal Large Language Models. arXiv:2401.13601. Zhao, H.; Cai, Z.; Si, S.; Ma, X.; An, K.; Chen, L.; Liu, Z.; Wang, S.; Han, W.; and Chang, B. 2023a. MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. arXiv:2309.07915. Zhao, J.; Li, R.; and Jin, Q. 2021. Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2608\u20132618. Online: Association for Computational Linguistics. Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.-Y.; and Wen, J.-R. 2023b. A Survey of Large Language Models. arXiv:2303.18223. Zheng, K.; He, X.; and Wang, X. E. 2023. MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. arXiv:2310.02239.\n# Appendix\n# Supplementary Figures Additional Discussion on Causality\nWe leave the causal intervention to LLMs for future work. The nature of our framework, however, provides some causal explanation of the phenomena of interest, or the causality of the phenomena on the model. The causal effect could be helpful in quantitatively assessing how the phenomena of interest (e.g., unseen format, ICL example) affect the subject (LLM). For example, a widely used metric termed Average Treatment Effect (ATE) (Rubin (2008)) is defined as the average difference of outcome y where the treatment Z is given. Assuming binary treatment Z \u2208 {Z0, Z1}, ATE is formalized as:\n(19)\nSimilarly to Eq.1, the causal effect on the prediction y of the ICL example e under the presence of unseen format bias b in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e93c/e93cd2bc-1e5a-4a45-9b46-6df528717d24.png\" style=\"width: 50%;\"></div>\nFigure 5: Comparison of model responses to two-image inputs. Images obtained from official LLaVA repository. The LLaVA response is truncated for brevity (see our repository for the full output). The main difference is in the description (B) of the second image. LLaVA-LLama2 explained that the red toy stands beside a lake, confusing the two images. LLaVA 1.5, on the other hand, gave a description that only mentioned the content in the second image, suggesting that it could disentangle the two images.\ncomparison with the zero-shot setting could be defined as the difference of the expected prediction between ICL (b, e) = \u22aeand zero-shot (b, e) = \u22acsettings.\n(20)\nSince the accuracy metric acc is the ratio of correct prediction over the samples, acc is identical to E[y], where y is a binary for the correct prediction. Therefore, analyzing the accuracy difference provides us with insights into ATE.\n(21)\nSimilarly, the causal effect of ICL over the model on CL perspective is:\n(22)\n  We attribute accuracy acc or ICL-time question-answer distance dhicl/h\u2032a to the linearly weighted binary variables (b, e) or zero-shot distance dhq/ha, weight analysis is relevant to ATE.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0963/09634fa3-87ae-4a23-b055-8570bb8287cd.png\" style=\"width: 50%;\"></div>\nFigure 6: Performance summary of InternVL. 1b and 2b indicates the number of model parameters.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebd2/ebd2b7f2-1a4b-4d91-b668-184107238ed8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">internvl-2b-1-5</div>\nFigure 7: GQA performance of InternVL by the number of inference steps.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78a4/78a49cc5-9fac-47ce-b486-02ee72cdeddd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Performance summary of LLaVA 1.5.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "The rapid growth of Large Language Models (LLMs) usage has highlighted the importance of gradient-free in-context learning (ICL). Previous methods primarily focused on simple problems, leaving a gap in understanding multimodal ICL, which involves multiple modalities and formatting challenges.",
        "problem": {
            "definition": "The paper aims to solve the problem of interpreting in-context learning in LLMs, particularly in multimodal settings where input formatting and resource limitations complicate the learning process.",
            "key obstacle": "The main difficulty lies in the biases introduced by multimodal input formatting, which affects the model's ability to learn from semantically similar examples presented in unseen formats."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that contrastive learning can help interpret how LLMs understand multimodal ICL semantics under varying input formats.",
            "opinion": "The proposed method, Anchored-by-Text ICL, involves generating ICL examples on-the-fly to enhance the model's ability to detect hateful memes despite resource constraints.",
            "innovation": "This method differs from existing approaches by offering a new analytical framework based on contrastive learning principles, allowing for better handling of multimodal input biases."
        },
        "method": {
            "method name": "Anchored-by-Text ICL",
            "method abbreviation": "AbT ICL",
            "method definition": "AbT ICL generates ICL examples dynamically, using the generated examples as anchors to improve the model's performance in multimodal tasks.",
            "method description": "The method enhances ICL by generating text that fits with given images to create benign examples, which are then used to inform predictions.",
            "method steps": [
                "Generate text based on the image to create a benign example.",
                "Use the generated example as an anchor for ICL.",
                "Perform inference using the anchor to extract the input-label relationship."
            ],
            "principle": "The effectiveness of this method is rooted in its ability to shift the model's representation towards semantically relevant examples, thereby improving performance in resource-limited settings."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using multimodal datasets, particularly focusing on the Hateful Memes Challenge, with models such as LLaVA evaluated under varying conditions.",
            "evaluation method": "Performance was assessed using accuracy metrics and f1 scores, with statistical analysis conducted to evaluate the significance of results."
        },
        "conclusion": "MCICL demonstrates improved performance in understanding ICL in LLMs, particularly in challenging multimodal settings and resource-constrained environments, paving the way for more interpretable and robust AI systems.",
        "discussion": {
            "advantage": "The proposed approach effectively balances formatting and semantics, improving ICL performance and providing insights into model behaviors.",
            "limitation": "The study's limitations include a focus on a limited set of multimodal datasets, which may not generalize across all multimodal tasks.",
            "future work": "Future research should explore a broader range of multimodal tasks and investigate the causal mechanisms underlying representational shifts."
        },
        "other info": {
            "info1": "The experiments utilized an NVIDIA A100 80GB GPU and Python 3.9 for implementation.",
            "info2": {
                "info2.1": "Statistical arguments were based on t-tests and bootstrapping with 1,000 resamples.",
                "info2.2": "The dataset included six VQA datasets to cover various aspects of multimodal LLM capabilities."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper discusses the significance of gradient-free in-context learning (ICL) in the context of Large Language Models (LLMs) and highlights the importance of understanding multimodal ICL."
        },
        {
            "section number": "1.2",
            "key information": "The paper emphasizes the impact of ICL on interpreting LLMs, particularly in multimodal settings where input formatting poses challenges."
        },
        {
            "section number": "3.5",
            "key information": "The method, Anchored-by-Text ICL (AbT ICL), enhances ICL by generating examples dynamically to improve performance in multimodal tasks."
        },
        {
            "section number": "4.1",
            "key information": "The proposed method demonstrates how effective prompt design can significantly influence the outcomes of in-context learning, particularly by generating text that fits with given images."
        },
        {
            "section number": "6.1",
            "key information": "The paper identifies biases introduced by multimodal input formatting as a key challenge affecting the model's ability to learn from semantically similar examples."
        },
        {
            "section number": "7",
            "key information": "The conclusion highlights that the proposed method improves performance in understanding ICL in LLMs, particularly in challenging multimodal settings and resource-constrained environments."
        }
    ],
    "similarity_score": 0.6967202700459664,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Multimodal Contrastive In-Context Learning.json"
}