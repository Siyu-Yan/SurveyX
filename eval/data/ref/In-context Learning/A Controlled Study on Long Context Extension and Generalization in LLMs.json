{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.12181",
    "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
    "abstract": "Broad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within the range of their extension, whereas extrapolation remains challenging. All codebases, models, and checkpoints will be made available open-source, promoting transparency and facilitating further research in this critical area of AI development. 2",
    "bib_name": "lu2024controlledstudylongcontext",
    "md_text": "# A Controlled Study on Long Context Extension and Generalization in LLMs\nYi Lu2\u2217 Jing Nathan Yan1\u2217 Songlin Yang3 Justin T. Chiu1 Siyu Ren2 Fei Yuan2 Wenting Zhao1 Zhiyong Wu2+ Alexander M. Rush1 1Cornell University, 2Shanghai AI Lab, 3Massachusetts Institute of Technology\n# Abstract\nBroad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within the range of their extension, whereas extrapolation remains challenging. All codebases, models, and checkpoints will be made available open-source, promoting transparency and facilitating further research in this critical area of AI development. 2\n# 1 Introduction\nThe pretraining data scale of large language models (LLMs) has expanded greatly in recent years with open models trained up to 15T tokens [AI@Meta, 2024]. Implementation challenges make i difficult to fully train models with longer context windows during pretraining [Liu et al., 2023a]. Still long-context windows are considered central, as they enable LLMs to perform tasks that require more extensive textual understanding, such as utilizing information from textbooks [Tanzer et al., 2024] summarizing novels [Kry\u00b4sci\u00b4nski et al., 2022], and engaging in many-shot learning [Bertsch et al. 2024, Li et al., 2023a].\nThe pretraining data scale of large language models (LLMs) has expanded greatly in recent years with open models trained up to 15T tokens [AI@Meta, 2024]. Implementation challenges make it difficult to fully train models with longer context windows during pretraining [Liu et al., 2023a]. Still, long-context windows are considered central, as they enable LLMs to perform tasks that require more extensive textual understanding, such as utilizing information from textbooks [Tanzer et al., 2024], summarizing novels [Kry\u00b4sci\u00b4nski et al., 2022], and engaging in many-shot learning [Bertsch et al., 2024, Li et al., 2023a]. As a trade-off, researchers have proposed context extension, where an LLM initially pretrained on standard sequences is adapted for significantly longer context lengths [Chen et al., 2023a, Peng et al., 2023, Han et al., 2023, bloc97, 2023]. These methods differ in the type of attention used and in post-training adaptation techniques. They vary in complexity, training requirements, and qualitatively exhibit significantly different performance profiles. Unfortunately, there is a relatively poor understanding of the quantitative rankings of these different methodologies. Owing to the perceived challenges of evaluation, several new metrics, such as long context perplexity [Chen et al., 2023a,b, Han et al., 2023], and retrieval accuracy [Mohtashami and Jaggi, 2023, gkamradt, 2023] have been introduced [Bai et al., 2023, An et al., 2023]. However, the differences in long-context extension procedures make it hard to calibrate these metrics while controlling for other factors.\n\u2217Equal contribution. + Correspondence author 2https://github.com/Leooyii/LCEG\nPreprint. Under review.\nIn this work, we implement a controlled protocol for context extension. The aim is to compare context extension while removing spurious factors that impact LLM ability. Modeling: We standardize on the same base model for all experiments. Different base models behave significantly differently, making it challenging to draw general conclusions. For instance, past work evaluates LM-Infinite [Han et al., 2023] on LongBench [Bai et al., 2023] using different base models [Xiao et al., 2024, Lu et al., 2024]. Extensions: We implement a range of context extension methods within the same framework. We use a standardized recipe to eliminate potential gains from tailored hyperparameters. We additionally fix the post-training data for each method, utilizing an identical and open-sourced training corpus [Fu et al., 2024, Chen et al., 2023b]. Metrics: We look at both intrinsic metrics, such as perplexity, and extrinsic properties, such as downstream task performance [Hsieh et al., 2024, gkamradt, 2023, Bai et al., 2023]. We consider metrics within the extension length as well as an extrapolation to longer contexts. Our study identifies several takeaways for future research. First, contrary to some of the arguments for needing new metrics, our findings indicate a strong correlation between perplexity and downstream task performance for exact fine-tuned methods in controlled studies. While some efficient attention methods are unbalanced between perplexity and downstream task performance, there is a strong correlation between the two for most extension tasks. Second, we find relatively poor results for approximate attention methods. While they can handle longer length contexts, there generally is a trade-off in terms of accuracy for most of our benchmarks. Exact frozen methods also tend to degrade model performance, showing high sensitivity to hyperparameters and often failing with a general training recipe. Finally, continual fine-tuning with exact attention generally work well, particularly within the extended context length. Specifically, Dynamic NTK [emozilla, 2023] works best among these methods. Extrapolation to longer lengths remains a challenging problem. Our training code, models, and checkpoints will be open-sourced to support further research in this area.\n# 2 Related Work\nLong Context Methods We divide extension methods into three broad classes: exact attention, approximate attention, and context compression. Exact attention methods augment the parameterization of attention. Position interpolation (PI) [Chen et al., 2023a], NTK-aware [bloc97, 2023], Dynamic NTK [emozilla, 2023], YaRN [Peng et al., 2023], and CLEX [Chen et al., 2024], all based on RoPE [Su et al., 2021], design position embeddings for length extension. These methods may be applied with fine-tuning or to frozen models. Other exact attention methods focus on training-time improvements, such as contrastive training [Tworkowski et al., 2023] . Approximate attention methods introduce structured attention approximations that minimize the computational cost of length extension. Chen et al. [2023b] introduced the use of LoRA [Hu et al., 2021] and a specialized local attention mechanism to reduce further the computational overhead of further fine-tuning with long context. Other approaches break the text into chunks and utilize a well-designed \"chunk representation\" to retrieve relevant chunks for attention [Mohtashami and Jaggi, 2023, Xiao et al., 2024, Lu et al., 2024]. LM-Infinite and StreamLLM [Han et al., 2023, Xiao et al., 2023] retain only a few tokens from the beginning of the text and a local window to keep the attention window within the pretrained length. Xu et al. [2024] focuses on using retrievers to retrieve relevant blocks from long documents. Finally, context compression methods, which we do not explore in this work, reduce length extension to length compression via a summarization step [Jiang et al., 2023, Li et al., 2023b]. Long Context Evaluation Benchmarks The Long Range Arena (LRA) [Tay et al., 2020] is an early efforts in evaluating the proficiency of processing long contexts in different modalities. Since then, a growing number of benchmarks have emerged, including LongBench [Bai et al., 2023], LEval [An et al., 2023], and LooGLE [Li et al., 2023c]. These benchmarks are a mixture of diverse downstream tasks explicitly tailored to assess the capabilities of LLMs in understanding and generating lengthy contexts. Among these benchmarks, LongBench stands out for its inclusion of diverse sequences with varying lengths, distributions, patterns, languages, and domains, enabling a comprehensive, nuanced evaluation. In addition to evaluating LLMs\u2019 performance on downstream NLP tasks, there is\nanother line of benchmarks that specifically focuses on assessing particular aspects of long context processing ability [Liu et al., 2023b, Hsieh et al., 2024]. For instance, Mohtashami and Jaggi [2023] propose the passkey retrieval task to challenge a language model to accurately locate and retrieve a simple passkey (a five-digit random number) in a long context sequence. Similarly, the Needle in a Haystack [gkamradt, 2023] test requires the model to accurately recite the information from a specified sentence(the \"needle\") from a long document. However, most existing works mainly focus on evaluating mainstream commercial models (e.g. GPT-4 and Claude), open-source base models, or just perform individual evaluations of a few long context methods. There is a lack of comprehensive, yet controlled evaluation on long-context extension techniques themselves.\n# 3 Context Extension Methods\n# 3.1 Background: Attention and RoPE\nThe bottleneck in long context modeling in Transformers is attention. Attention is defined over C embeddings X = [x1, x2, . . . , xC]\u22a4\u2208RC\u00d7d where d is the model dimension. Learned weight matrices Wv \u2208Rd\u00d7dk, Wq \u2208Rd\u00d7dk, and Wk \u2208Rd\u00d7dk are used to transform these inputs where dk is the projected hidden dimension. The attention mechanism itself computes the attention matrix and applies it to produce a weighted sum of the value vectors:\nBasic attention was originally defined with: Q = XWq, K = XWk, V = XWv. However, this approach does not directly encode the relative position of keys and values.\nBasic attention was originally defined with: Q = XWq, K = XWk, V = XWv. However, this approach does not directly encode the relative position of keys and values. Rotary Position Embeddings (RoPE) [Su et al., 2024] encode positional information by applying a phase rotation to each element of the embedding vectors. Formally, we define a transformation f:\nRotary Position Embeddings (RoPE) [Su et al., 2024] encode positional information by applying a phase rotation to each element of the embedding vectors. Formally, we define a transformation f:\nHere xi \u2208Rdk is an embedding for position i, W is a projection matrix, and \u03b8 \u2208Rdk/2 is a frequency basis. The function is defined based on the rotary position matrix:\nDue to the arrangement of frequencies, this matrix has the property that R(\u03b8, n \u2212m) = R(\u03b8, m)\u22a4R(\u03b8, n) by Ptolemy\u2019s identity. We redefine the query-key product between two positions m and n as,\nIn this way, the relative positional information n \u2212m is implicitly injected into the query and key product, thus the attention score. The standard RoPE transformation, fW(xi, \u03b8), sets the components \u03b8j = b\u22122j dk with base b = 10000.\n(2)\n(3)\n(4) (5) (6)\nWe consider four methods for performing length extension on RoPE embeddings: Position Interpolation (PI) [Chen et al., 2023a], NTK-RoPE [emozilla, 2023], YaRN [Peng et al., 2023] and CLEX [Chen et al., 2024]. In this section our goal is to extend a method trained to extend position embeddings for context length C to length C\u2032 >> C. The methods in this section perform this extension by scaling the frequencies with the base scaling vector \u03b1 \u2208R dk 2 :\nLinear Position Interpolation (PI) decreases the frequencies of the basis functions so that more tokens fit within each period. PI is implemented by setting the components of the base scaling vector\nwhere t = C\u2032 C is target length ratio. PI has been integrated into many open-source models such as LLaMA2-7B-32K [Together.AI, 2023], Vicuna-7B-v1.5 [Chiang et al., 2023], and LongAlpaca [Chen et al., 2023b]. Neural Tangent Kernel Interpolation RoPE (NTK-RoPE) builds on linear position interpolation by introducing a per-dimension scaling factor. Inspired by findings from the NTK literature that show that high-frequency features are difficult for MLPs to learn, NTK-RoPE preserves high-frequency features while extending the period of low-frequency features. This is accomplished via a dimensiondependent base scaling vector \u03b1:\nwhere t = C\u2032 C is target length ratio. PI has been integrated into many open-source models such as LLaMA2-7B-32K [Together.AI, 2023], Vicuna-7B-v1.5 [Chiang et al., 2023], and LongAlpaca [Chen\nNeural Tangent Kernel Interpolation RoPE (NTK-RoPE) builds on linear position interpolation by introducing a per-dimension scaling factor. Inspired by findings from the NTK literature that show that high-frequency features are difficult for MLPs to learn, NTK-RoPE preserves high-frequency features while extending the period of low-frequency features. This is accomplished via a dimensiondependent base scaling vector \u03b1:\nwhere \u03ba = (t) k dk\u22122 so that the lowest frequency is scaled to match PI and the highest frequency remains the same as in RoPE.\nwhere \u03ba = (t) dk\u22122 so that the lowest frequency is scaled to match PI and the highest frequency remains the same as in RoPE. An extension to this approach, Dynamic NTK-RoPE suggests that instead of fixing scaling based on a set ratio s for all examples during inference, the formula should adapt to the current context length for a specific example. We followed the set up of Fu et al. [2024] for Dynamic NTK-RoPE. More details can be found in the Appendix 9.2. YaRN, another RoPE extension method, uses \u201cNTK-by-parts\" interpolation strategies across different dimensions of the embedding space and introduces a temperature factor to adjust the attention distribution for long inputs.\nWe use a ramp vector \u03b3 to determine the interpolation between the 1 t and the original frequency base. The interpolation gating is set based on the frequency for the dimension j.\n\uf8f4 \uf8f3 The values of p, q, T can be tuned as needed. Other methods such as CLEX Chen et al. [2024] models the scaling vectors as a dynamical system intending to learn target-length dependent scaling vectors.\n\uf8f4 \uf8f3 The values of p, q, T can be tuned as needed.\n# 3.3 Approximate Attention\nAn alternative approach is to modify the attention function itself. Instead of exactly computing each longer attention, these methods select a subset of positions to attend to. We consider four well-known methods based on three different attention mechanisms: sparse attention, sliding window attention, and retrieval attention.\n(7)\n(8)\n(9)\n(10)\n(11)\nLongLoRA [Chen et al., 2023b] avoids computing attention ranges over C\u2032 by only computing the block-diagonl part of attention. Formally, given a sequence length of C\u2032, LongLoRA divides it into M blocks of size B, resulting in a sparse attention matrix A \u2208RC\u2032\u00d7C\u2032 with a block-diagonal structure: \uf8ee \uf8f9\nwhere Ai \u2208RB\u00d7B is the attention matrix for the i-th block. In addition, they shift the blocks for half of the heads enabling the information flow between groups via shifting. Notably, while they employ local attention during the fine-tuning phase, full attention is still adopted during the inference stage. Landmark Attention [Mohtashami and Jaggi, 2023] addresses the challenge of attending over long sequences by breaking the input sequence into chunks and using trainable \u201clandmark\" tokens to summarize these chunks. The attention process is carried out in two stages. Given a sequence of C\u2032 embeddings, divided into M chunks, each of length B, the first step is to compute global attention between the query vectors Q \u2208RC\u2032\u00d7dk (corresponding to all input embedding) and the landmark vectors L \u2208RM\u00d7dk (which represent the chunks). From this global attention, a set of n-most attended-to chunks is selected for further processing. Next, a local attention mechanism is applied within each of the selected chunks. For the n-th selected chunk, the key matrix for the chunk is denoted as Kn \u2208RB\u00d7dk and Qn \u2208RB\u00d7dk. The attention matrices are then computed as follows: \ufffd \ufffd\nThe final attention for each embedding is a combination of these two attention. This method efficiently scales attention mechanisms for long sequences by focusing on landmark tokens that summarize large parts of the sequence, followed by local attention within the relevant chunks. LM-Infinite [Han et al., 2023] (a.k.a., Sliding Window Attention) maintains a sliding local window of size M along with a fixed global memory of G positions at the starting point of the given embedding. Given C\u2032 embeddings, attention is computed over the M embeddings in its local window and G embeddings in global memory. LM-Infinite replaces relative positional information n \u2212m with min(n \u2212m, C) while computing the query and key product in Eq 4. Altogether, LM-Infinite reduces the complexity from O((C\u2032)2) to O(C\u2032(M + G)) without the need to scale positional encoding. Self-Extend [Jin et al., 2024] maps the unseen positions in extended context length C\u2032 to positions within the pretraining context length C to avoid training. For each embeddings, Self-Extend chooses closest M embeddings and any embeddings beyond are divided into multiple groups. Each group contains N embeddings. When performing query-key product between two positions m and n in Equation 4, the relative positional information n \u2212m is replaced by r which is computed by scaling n \u2212m w.r.t M and N:\n\ufffd \ufffd \ufffd \ufffd where \u230a\u00b7\u230bdenotes the floor division. The maximum extended context length C\u2032 is (C \u2212M) \u00b7 N + M.\n\ufffd \ufffd \ufffd \ufffd ere \u230a\u00b7\u230bdenotes the floor division. The maximum extended context length \n# 4 Long-Context Extension Protocol\nBase Model All models start from an identical LLaMA2-7B base checkpoint [Touvron et al., 2023] This approach removes potential biases from the checkpoint basis. Note that to study the influence of base model, particularly the size of base model, we also use Phi-2-base checkpoint [Javaheripi et al. 2023] for context extension, to verify whether the trends and analyses we observed are consistent across different base models, thereby avoiding potential over-generalization. The results of Phi-2-base are reported in Appendix 9.1.\n(12)\n(13)\n(14)\n(15)\nFine-Tuning We sample 1B tokens from a long-context data mixture from Fu et al. [2024] to achieve state-of-the-art context extension performance. The details of the data are reported in Appendix 9.3. We focus on extending the context window from 4k to 32k, as most downstream tasks require contexts under 32k. We maintain a fixed training recipe to ensure consistency across all models [Chen et al., 2023b]. We follow existing practices by keeping an exponential moving average (EMA) of model weights with a constant decay. Most training hyperparameters are based on [Fu et al., 2024], with the learning rate set to 2 \u00d7 10\u22125. We use a linear learning rate warm-up and zero weight decay, utilizing 8 NVIDIA A100 GPUs. For LongLora, we fine-tune the weights of the LoRA adapter with trainable embedding and normalization, then merge these trainable weights with the LLaMA2 base model for evaluation. For Landmark Attention, the training context length is set to 512, with a block size of 64. For CLEX, we set the max scale factor to 32 and use the SiLU activation function. We reuse the original scale factor to maintain consistency for NTK, YaRN, and Position Interpolation methods. However, this base factor significantly degrades continual fine-tuned models, particularly causing performance deterioration in shorter sequences. Therefore, we conduct a grid search to determine a better scale factor for different input lengths for NTK-RoPE method. Based on our findings, we follow and improve upon Fu et al. [2024] to set the scale factor for NTK-RoPE method. The scale factor and its relationship with perplexity are reported in the Appendix 9.5. Please refer to the Appendix 9.2 for detailed hyperparameter setups. Metrics We consider two sets of intrinsic metrics. The first is based on perplexity. We use the book corpus PG19 [Rae et al., 2019] and the Proof-pile dataset [Azerbayev et al., 2023] to evaluate the long sequence language modeling performances. Following Press et al. [2022], all perplexity evaluations are calculated using a sliding window with a window size of 256. The second is based on retrieval. We focus on the needle in the haystack task [gkamradt, 2023](NIAH). NIAH involves identifying a specific, relevant piece of information (the \"needle\") within a large set of irrelevant data (the \"haystack\"). This task is commonly used to test the precision and recall capabilities of LLMs in scenarios where the relevant data is sparse and surrounded by a significant amount of noise. Additionally, we evaluate with RULER [Hsieh et al., 2024]. RULER enhances the standard NIAH test by incorporating variations with different types and quantities of needles. Additionally, it introduces new task categories, such as multi-hop tracing and aggregation, to evaluate behaviors beyond simple context-based searching. For extrinsic metrics, we consider a collection of tasks. LongBench [Bai et al., 2023] is a family of bilingual, multitask evaluations for long-context understanding widely used in measuring the longcontext abilities of LLMs [Jin et al., 2024, Xiao et al., 2024, Lu et al., 2024]. LongBench includes single-document question answering, multi-document QA, summarization, few-shot learning, and code completion. We follow Bai et al. [2023] to evaluate the models on 32k context window sizes by truncating the prompt from the middle when the task length exceeds a designated context window size. We also consider the ManyShots tasks, where the long-context model will be given several examples as prompts. We use the Trec News [Kontonis et al., 2024] dataset for this task.\n# 5 Experimental Results\n# 5.1 Result Overview\nTable 1 overviews the results across both types of evaluation. The main result demonstrate that fine-tuned exact attention methods for long contexts, such as NTK-32K and YARN, consistently outperform approximate attention methods and frozen methods by a significant margin. This suggests that trading accuracy for speed in approximate attention methods can result in a loss of important reasoning capabilities, particularly for retrieval-based tasks. The performance disparity highlights the importance of exact attention mechanisms in maintaining high accuracy over extended contexts, emphasizing the need for careful consideration of attention mechanisms in model design for longcontext tasks. We now consider each type of result in more detail.\n<div style=\"text-align: center;\">Table 1: Overview of results across different extension types.</div>\nAttention Mechanisms\nModel\nPPL\nNeedle\nMshots\nLongB\nRULER\nExact\nAttention\nFrozen\nNTK-F\n14.52\n18.8\n64.5\n25.54\n0.72\nFine-Tuned\nPI\n5.85\n42.1\n75.5\n33.48\n57.66\nYaRN\n5.85\n46.7\n75.0\n33.45\n36.95\nCLEX\n5.82\n71.1\n74.0\n33.48\n52.17\nNTK-32K\n5.79\n83.7\n71.0\n35.32\n59.42\nNTK-64K\n5.93\n69.1\n73.0\n34.30\n60.03\nApproxi.\nAttention\nFrozen\nLM-Infinite\n6.71\n23.9\n61.5\n25.84\n12.34\nSelf-Extend\n6.11\n25.8\n72.0\n33.62\n29.50\nFine-tuned\nLongLora\n9.89\n20.3\n55.5\n23.30\n3.53\nLandmark\n8.13\n50.9\n50.0\n28.19\n13.56\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4eed/4eede68f-8ce2-4d90-b327-f3c33f3f7eda.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Needle in a Haystack evaluation. Green squares indicates a high retrieval success rate, the white dashed line denotes the longest length examples seen at training or finetuning, and the Y-axis represents the distance to the retrieved target.</div>\n# 5.2 Intrinsic tasks\nPerplexity Table 2 shows perplexity scores across length. We see that continuous fine-tuning methods like PI, YaRN, and LongLora effectively keep low perplexity scores within the pre-training context length. However, when the context length exceeds perplexity scores escalate once the context surpasses the pre-trained window. Only NTK and CLEX can generalize to unseen sequence length in both pretraining and continual finetuning. Additionally, we find that exact attention maintains better perplexity than LoRA, which may reduce LongLora\u2019s ability. We also note that results on both PG19 and Proof-file gave nearly consistent conclusions.\nTable 2: Perplexity results of different methods on PG 19 and Proof-file. NTK-32K and NTK-64K refer to NTK-Dynamic, which requires finetuning on longer text. Len refers to the longest-length examples seen at training or fine-tuning. Ex refers to the exact attention. All results are produced by our experiments.\nModel Details\nEval Length\nLen\nEx\nMethods\n2k\n4k\n8k\n16k\n32k\n64k\nPG19\nFrozen\n4k\n\u2713\nLLaMA2\n6.61\n6.30\n-\n-\n-\n-\n4k\nLM-Infinite\n6.61\n6.30\n6.25\n6.45\n6.71\n8.49\n4k\n\u2713\nNTK-Frozen\n6.61\n6.30\n6.82\n7.94\n14.52\n-\n4k\nSelf-Extend\n6.61\n6.32\n6.15\n6.07\n6.11\n7.15\nFinetuned\n32k\n\u2713\nPI\n6.88\n6.52\n6.27\n6.08\n5.95\n-\n32k\n\u2713\nNTK-32K\n6.63\n6.32\n6.09\n5.92\n5.79\n5.76\n32k\n\u2713\nYaRN\n6.70\n6.39\n6.16\n6.01\n5.93\n-\n32k\n\u2713\nCLEX\n6.85\n6.62\n6.14\n5.93\n5.82\n5.79\n32k\nLongLora\n12.80\n11.52\n10.70\n10.18\n9.89\n-\n32k\nLandmark\n8.15\n8.14\n8.14\n8.11\n8.13\n8.15\n64k\n\u2713\nNTK-64K\n6.83\n6.49\n6.25\n6.07\n5.93\n5.85\nProof-file\nFrozen\n4k\n\u2713\nLLaMA2\n3.34\n3.04\n-\n-\n-\n-\n4k\nLM-Infinite\n3.34\n3.04\n2.94\n3.02\n3.11\n3.12\n4k\n\u2713\nNTK-Frozen\n3.34\n3.04\n2.91\n3.09\n4.06\n12.65\n4k\nSelf-Extend\n3.35\n3.06\n2.88\n2.78\n2.75\n2.90\nFinetuned\n32k\n\u2713\nPI\n3.34\n3.03\n2.83\n2.68\n2.58\n-\n32k\n\u2713\nNTK-32K\n3.27\n2.98\n2.78\n2.64\n2.54\n2.48\n32k\n\u2713\nYaRN\n3.29\n3.00\n2.81\n2.68\n2.59\n106.38\n32k\n\u2713\nCLEX\n3.37\n3.10\n2.80\n2.65\n2.55\n2.48\n32k\nLongLora\n5.97\n5.10\n4.58\n4.27\n4.13\n-\n32k\nLandmark\n4.51\n4.50\n4.48\n4.49\n4.49\n4.49\n64k\n\u2713\nNTK-64K\n3.33\n3.03\n2.83\n2.69\n2.58\n2.51\nNeedle-in-the-haystack NIAH results are shown in Figure 1. Continuous finetuning approaches such as NTK, PI, and YaRN have successfully retrieved the \"needle\" within the pretraining length. Yet, only the NTK and CLEX method can retrieve the needle beyond the pretraining length, aligning with the perplexity results. The performance of the Exact Attention Method generally surpasses that of the Approximate Attention Methods. LM-Infinite and Landmark Excel are only within the local window, and they struggle to retrieve the intermediate text accurately. Regarding the Dynamic NTK method, NTK-F exhibits weak generalization when not trained. When trained on the same amount of data(1B), NTK-32K outperforms NTK-64K. When trained on 2B tokens, NTK-64K demonstrated a significant performance improvement, details are in Appendix 9.4. RULER We test all models on 13 diverse tasks from the four RULER categories [Hsieh et al., 2024]. Each model is evaluated with 500 examples for lengths of 4k, 8k, 16k, 32k, and 64k. Results are compared with the Llama2-7B baseline in Table 3. We observed a similar trend as in the NIHK task, where NTK family methods performed the best. NTK-32k maintained relatively good performance compared to other methods finetuned with a length cap of 32k. Performance of models on different length and breakdown by 13 subtasks can be found in Appendix 9.8.\nNeedle-in-the-haystack NIAH results are shown in Figure 1. Continuous finetuning approaches such as NTK, PI, and YaRN have successfully retrieved the \"needle\" within the pretraining length. Yet, only the NTK and CLEX method can retrieve the needle beyond the pretraining length, aligning with the perplexity results. The performance of the Exact Attention Method generally surpasses that of the Approximate Attention Methods. LM-Infinite and Landmark Excel are only within the local window, and they struggle to retrieve the intermediate text accurately. Regarding the Dynamic NTK method, NTK-F exhibits weak generalization when not trained. When trained on the same amount of data(1B), NTK-32K outperforms NTK-64K. When trained on 2B tokens, NTK-64K demonstrated a significant performance improvement, details are in Appendix 9.4.\nRULER We test all models on 13 diverse tasks from the four RULER categories [Hsieh et al., 2024]. Each model is evaluated with 500 examples for lengths of 4k, 8k, 16k, 32k, and 64k. Results are compared with the Llama2-7B baseline in Table 3. We observed a similar trend as in the NIHK task, where NTK family methods performed the best. NTK-32k maintained relatively good performance compared to other methods finetuned with a length cap of 32k. Performance of models on different length and breakdown by 13 subtasks can be found in Appendix 9.8.\n# 5.3 Extrinsic tasks\nLongBench The evaluation results of most methods on LongBench are presented in Table 4, and results on all methods are presented in Appendix 9.7. Both LM-Infinite and Landmark Attention\nTable 3: RULER evaluation. Performance of models evaluated at length from 4k to 64k. Each sco is computed by averaging the accuracy of 13 tasks. Train Len refers to the longest-length exampl seen at continuous finetuning.\nable 3: RULER evaluation. Performance of models evaluated at length from 4k to 64k. Each score s computed by averaging the accuracy of 13 tasks. Train Len refers to the longest-length examples\nModels\nTrain\nLen\n4k\n8k\n16k\n32k\n64k\nFrozen\nLLaMA2\n4k\n80.94\n-\n-\n-\n-\nLM-Infinite\n4k\n81.05\n30.01\n18.02\n12.34\n10.56\nNTK-Frozen\n4k\n81.14\n44.45\n14.79\n0.72\n0.91\nSelf-Extend\n4k\n65.03\n50.73\n44.02\n29.50\n9.34\nFinetuned\nPI\n32k\n84.56\n76.04\n69.64\n57.66\n0.00\nNTK-32K\n32k\n86.58\n77.75\n70.01\n59.42\n46.26\nYaRN\n32k\n79.12\n65.60\n54.21\n36.95\n0.00\nCLEX\n32k\n50.18\n63.93\n64.35\n52.17\n30.61\nLongLora\n32k\n10.58\n6.37\n3.67\n3.53\n0.00\nLandmark\n32k\n22.37\n17.52\n16.31\n13.56\n14.15\nNTK-64K\n64k\n86.60\n76.34\n69.56\n60.03\n49.31\nTable 4: LongBench results. N-32 and N-64 refer to NTK finetuned on 32K and 64K context lengths respectively. SE refers to Self-Extend. YN refers to YaRN. CX refers to CLEX. LLR refers to LongLora. Len refers to average length of the datasets. Train Len refers to the longest length examples seen at training or finetuning. Eval Len refers to the maximum length of the input prompt. \u2713refers to whether the method is exact attention.\nFrozen\nFinetuned\nLen\nBase\nN-F\nSE\nPI\nN-32\nYN\nCX\nLLR\nLand\nN-64\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTrain Len\n4k\n4k\n4K\n32k\n32k\n32k\n32k\n32k\n32k\n64k\nEval Len\n4k\n32k\n32K\n32k\n32k\n32k\n32k\n32k\n32k\n32k\nNQA 18k\n21.09\n3.88\n23.49\n23.02\n23.73\n19.82\n24.19\n12.07\n12.47\n24.31\nQAP\n4k\n26.94\n26.79\n28.75\n25.85\n27.50\n26.98\n23.36\n20.15\n19.06\n24.97\nMQA 5k\n32.42\n29.82\n32.66\n35.10\n38.22\n37.11\n40.83\n24.50\n21.86\n40.60\nHQA\n9k\n31.23\n32.10\n37.63\n36.98\n41.56\n38.60\n35.59\n27.41\n33.66\n41.47\nWQA 5k\n25.75\n22.34\n30.70\n29.38\n31.58\n30.63\n28.24\n21.46\n24.94\n28.62\nMSQ 11k\n10.55\n8.84\n15.73\n16.80\n17.41\n22.08\n17.12\n11.46\n11.41\n18.24\nGR\n9k\n17.32\n17.87\n13.15\n25.61\n28.27\n20.98\n24.68\n24.05\n17.20\n24.37\nQSM 11k\n21.28\n15.35\n20.20\n21.19\n21.52\n20.66\n21.55\n17.66\n18.83\n21.65\nMWS 2k\n3.44\n9.30\n1.50\n10.55\n22.13\n8.91\n16.96\n21.19\n19.43\n25.02\nTRE\n5k\n66.00\n67.50\n69.00\n71.00\n69.00\n69.00\n67.50\n50.00\n49.00\n69.00\nTQA\n8k\n87.89\n18.69\n88.44\n88.55\n88.86\n89.63\n89.36\n12.28\n74.75\n88.65\nSMS\n6k\n41.70\n32.46\n43.76\n43.35\n42.21\n44.25\n43.02\n13.45\n40.38\n41.59\nPSC\n11k\n2.10\n2.67\n0.00\n1.50\n2.68\n1.05\n2.50\n4.57\n0.64\n2.09\nPSR\n9k\n9.00\n3.77\n4.50\n4.50\n4.62\n3.79\n8.50\n3.50\n2.50\n6.50\nLCC\n1k\n68.22\n63.64\n68.47\n55.05\n56.78\n54.06\n49.45\n57.12\n56.70\n52.04\nREP\n4k\n61.73\n53.69\n59.99\n47.26\n49.09\n47.60\n42.84\n51.92\n48.23\n39.68\nAvg.\n7k\n32.92\n25.54\n33.62\n33.48\n35.32\n33.45\n33.48\n23.30\n28.19\n34.30\nexhibit significant performance degradation compared to the base model. In contrast, the NTK, PI, and YaRN methods have successfully maintained their performance at 32k, demonstrating comparable results among these methods. This suggests that PI and YaRN perform similarly in downstream tasks, while the NTK family of models remains stable. Notably, the LongLoRA method, which utilizes LoRA, also experiences a performance decline relative to the base checkpoint, LLaMA2. We argue that this may be due to the sensitivity of the training procedures for LongLoRA, and we acknowledge this in our limitation discussion section.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c2c5/c2c59b20-31b3-4d71-af97-312fee261bdb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Perplexity and averaged downstream task accuracy for Needle in a haystack, LongBench and RULER.</div>\nFurthermore, the overall performance on LongBench has not shown significant improvement over LLaMA2.We hypothesize that this is due to the average length of LongBench test data (approximately 7.5k) being considerably shorter than the 32k context window of the long-context methods.\nMany-shot In-Context Learning with Trec News We evaluate TREC News [Li and Roth, 2002] with 1 to 1000 in-context examples. Performance improves with more examples in Figure 2. Exact Attention methods show significant gains from 10 to 50 examples (+44.0%) and 100 to 1000 examples (+25.9%), with slower growth from 50 to 100 examples (+5.7%). Approximate Attention methods consistently underperform. Performance gains align with model perplexity in longer contexts; NTK-Frozen excels with fewer examples but underperforms with more.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a6f5/a6f50f1d-7938-4bb7-a3f0-7e923aafee5c.png\" style=\"width: 50%;\"></div>\n# 6 Analysis\nPerplexity and Downstream Tasks existing studies [Sun et al., 2021, An et al., 2023] suggest that perplexity may not consistently correlate with performance on long-range tasks. In Figure 4, we plot the perplexity of models from our evaluated benchmarks against their performance. The figure shows a general correlation between perplexity and model performance across various tasks for exact attention methods. However, approximate attention methods, LongLora, and Landmark on RULER, exhibit slight deviations from this trend but still roughly fit into the linear relationship. We hypothesize that previous studies mentioned this disconnection due to the lack of controlled studies and the presence of noisy data.\nAs depicted in Figure 1, although LM-Infinite achieves a good perplexity score at 32k, it fails to generalize beyond 4k. This limitation arises because the LM-Infinite method focuses on a context window of only 4k, resulting in poor retrieval ability for longer contexts. This suggests that we should consider downstream tasks at different lengths when evaluating perplexity for approximate attention methods.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4076/4076f9db-9f63-4846-b8d2-7a3a24ea22e1.png\" style=\"width: 50%;\"></div>\nContext extension hurts in the short term and gains in the long term While context extension seems to improve perplexity, in Table 4, we do not notice a significant gain in performance. We hypothesize that while this dataset contains long tasks, the average length is much shorter than 32K. These methods seem to improve the ability to model language over the long term bu\n<div style=\"text-align: center;\">Figure 2: Many-shot in-context learning accuracy on TREC News.</div>\n<div style=\"text-align: center;\">Figure 3: Averaged negative log-likelihood of different models broken down by context position.</div>\ncompute the averaged negative likelihood of each position of YaRN, LLaMa2, and NTK-32K per position (with LLaMa2 seeing just tokens every 4k chunks) in Figure 3.\nNTK Generalizes Beyond 32k In Figure 1, we observe that NTK-32K successfully generalizes to unseen sequence lengths beyond 32k in both Needle-in-the-Haystack and RULER tasks, performing on par with NTK-64K. In contrast, NTK-F demonstrates generalization up to 8k but fails to extend further. This suggests that while NTK methods may possess the capability to generalize to longer unseen sequences, their effectiveness is contingent upon conditions such as continual fine-tuning. We find that up until 4K they all improve as expected with LLaMa2 having the best NLL. After 4K they all fluctuate in average, but we see a clear separation with Yarn and NTK taking into account the long context. At extremely long context NTK remains a strong model whereas Yarn becomes reverts to a similar performance as LLaMa2.\n# 7 Limitations and Broader Impacts\nLimitations As we are limited by computing budget, we only experiment with Llama-2-7B as our base model. The findings in this paper may not generalize to other, possibly larger, base models. We only fine-tuned models to context sizes of 32k, and generalization behaviors to longer contexts may differ when training contexts are longer. We also acknowledge that the standardized training recipe with fixed hyperparameters may bias some models more than the other models. Broader Impacts This paper provides an in-depth evaluation of various methods for extending the context lengths of LLMs, offering insights into their performance in long-range settings. By standardizing evaluations with a consistent base model and extension data, this work provides clearer benchmarks for future work. We also open-source code and models to promote transparency. Our models share similar risks as standard LLMs, where they may be used to generate harmful content and misinformation.\n# 8 Conclusion\nIn this paper, we used a standardized approach to assess the performance of various long-context methods in LLMs. Our study underscores the role of perplexity as a crucial, performance indicator at length and highlights the trade-offs inherent in different attention mechanisms. We shed light on the strengths and weaknesses of various approaches, providing valuable insights for future research. As part of our commitment to open science, all our resources, including codebases, models, and checkpoints, will be open-sourced upon acceptance, fostering future advancements in this pivotal area of AI research.\n# 9 Acknowledgements\nWe thank Yao Fu, Yue Yu, Tianyu Gao, Celine Lee, Woojeong Kim, Jack Morris, Junxiong Wang, and Oscar Yin for their suggestions and feedback. This work was supported by NSF CAREER #2037519 and NSF #1901030.\n# References\nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md.\nWojciech Kry\u00b4sci\u00b4nski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. Booksum: A collection of datasets for long-form narrative summarization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6536\u20136558, 2022. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples. arXiv preprint arXiv:2302.04931, 2023a. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation, 2023a. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models, 2023. bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_ rope_allows_llama_models_to_have/. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models, 2023b. Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers, 2023. gkamradt. Needle in a haystack - pressure testing llms, 2023. URL https://github.com/ gkamradt/LLMTest_NeedleInAHaystack. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2023. Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617, 2024. Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, and Xuanjing Huang. Longheads: Multi-head attention is secretly a long context processor. arXiv preprint arXiv:2402.10685, 2024. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context, 2024. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What\u2019s the real context size of your long-context language models?, 2024. emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/ dynamically_scaled_rope_further_increases/. Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. Clex: Continuous length extrapolation for large language models, 2024. URL https://arxiv.org/abs/2310. 16450. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Cornell University - arXiv,Cornell University - arXiv, Apr 2021.\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u00b4s. Focused transformer: Contrastive training for context scaling, 2023. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. URL https://arxiv.org/abs/2106.09685. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models, 2024. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression, 2023. Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models, 2023b. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers, 2020. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts?, 2023c. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023b. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Together.AI. Llama-2-7b-32k-instruct \u2014 and fine-tuning for llama-2 models with together api, 2023. URL https://www.together.ai/blog/llama-2-7b-32k-instruct. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. Mojan Javaheripi, S\u00e9bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models, 2023.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2023. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Vasilis Kontonis, Mingchen Ma, and Christos Tzamos. Active learning with simple questions, 2024. Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics -, Jan 2002. doi: 10.3115/1072228.1072378. URL http://dx.doi.org/10.3115/1072228.1072378. Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context?, 2021. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2023. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Vasilis Kontonis, Mingchen Ma, and Christos Tzamos. Active learning with simple questions, 2024. Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics -, Jan 2002. doi: 10.3115/1072228.1072378. URL http://dx.doi.org/10.3115/1072228.1072378. Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context?, 2021. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.\n# 9.1 Phi-2 for Context Extension\nWe use another open-weight model, Phi-2-base Javaheripi et al. [2023] as the base point for context extension, to verify whether the trends and analyses we observed are consistent across different base models. Using an identical training recipe, we re-train and re-evaluate seven models with Phi-2-base.\n# 9.1.1 Perplexity on Proof-file of Phi-2-base\nWe evaluate the perplexity score of Phi-2-base on seven models in Table 5. Consistent with our observations in the original submission, continuous fine-tuning methods like PI, YaRN, and LongLora effectively maintain low perplexity scores within the pre-training context length. However, perplexity scores escalate once the context length exceeds the pre-trained window. Notably, only NTK could generalize to unseen sequence lengths during both pre-training and continual fine-tuning.\nTable 5: Perplexity results of different methods on Proof-file with Phi-2-base. NTK-32K and NTK64K refer to NTK-Dynamic, which requires finetuning on longer text. Len refers to the longest-length examples seen at training or fine-tuning. Ex refers to the exact attention. All results are produced by our experiments.\nModel Details\nEval Length\nLen\nEx\nMethods\n2k\n4k\n8k\n16k\n32k\n64k\nFrozen\n2k\n\u2713\nPhi-2-base\n4.02\n25.72\n175.05\n-\n-\n-\n2k\n\u2713\nNTK-Frozen\n4.02\n3.73\n4.07\n5.49\n12.58\n36.68\n2k\nSelf-Extend\n4.08\n3.70\n3.48\n3.42\n3.48\n3.73\nFinetuned\n32k\n\u2713\nPI\n7.53\n6.75\n6.25\n5.97\n5.83\n45.00\n32k\n\u2713\nNTK-32K\n4.24\n3.81\n3.51\n3.32\n3.18\n3.20\n32k\n\u2713\nCLEX\n5.53\n4.32\n3.78\n3.51\n3.42\n3.60\n64k\n\u2713\nNTK-64K\n4.63\n4.14\n3.82\n3.61\n3.47\n3.38\n# 9.1.2 RULER of Phi-2-base\nWe test all models on 12 diverse tasks (except QA-2) from the four Ruler Hsieh et al. [2024] categories in Table 6. Consistently, NTK-32k maintains relatively strong performance compared to other methods fine-tuned with a length cap of 32k. The only exception was CLEX and NTK-32k at 64k, showing a slight drop in performance.\n<div style=\"text-align: center;\">Table 6: RULER evaluation on seven methods with Phi-2-base. Performance of models evaluated at length from 2k to 64k. Each score is computed by averaging the accuracy of 12 tasks. Train Len refers to the longest-length examples seen at continuous finetuning.</div>\nefers to the longest-length examples seen at continuous finetuning.\nModels\nTrain\nLen\n2k\n4k\n8k\n16k\n32k\n64k\nFrozen\nPhi-2-base\n2k\n83.73\n-\n-\n-\n-\n-\nNTK-Frozen\n2k\n83.98\n52.95\n18.09\n4.07\n0.06\n0.00\nSelf-Extend\n2k\n68.55\n50.82\n36.65\n22.00\n7.83\n2.32\nFinetuned\nPI\n32k\n25.51\n23.19\n16.88\n14.99\n4.78\n0.00\nNTK-32K\n32k\n81.18\n66.90\n52.57\n46.53\n32.06\n12.84\nCLEX\n32k\n75.33\n72.66\n53.56\n46.23\n25.46\n13.03\nNTK-64K\n64k\n78.73\n59.87\n47.56\n41.87\n25.66\n17.69\nTraining To maintain consistency across all models, we use a fixed training protocol [Chen et al., 2023b]. We adopt standard practices by applying an exponential moving average (EMA) to the model weights with a constant decay rate. The majority of our training hyperparameters are derived from [Chen et al., 2023b], including a learning rate of 2 \u00d7 10\u22125. We implement a linear warm-up for the learning rate and set the weight decay to zero, utilizing 8 NVIDIA A100 GPUs. For LongLora, we fine-tune the LoRA adapter weights along with trainable embeddings and normalization, subsequently integrating these trained weights into the LLaMA2 base model for evaluation. For Landmark Attention, the training context length is 512, with a block size of 64. For CLEX, we set the max scale factor to 32 and use the SiLU activation function. We present the hyperparameter settings for different methods during the training stage in Table 7. Inference We list the scale factors used for different length ranges during inference in Table 8. For NTK-RoPE, given the maximum observed length during training or inference, Ctest, and the scaling hyperparameter s, we follow Fu et al. [2024] in replacing t with s \u00b7 max(C\u2032,Ctest) C \u2212(s \u22121), and set the hyperparameter s to C\u2032 2C during both training and inference. For LM-infinite, we set the global memory G = 10 and the local window M = 4096. For Landmark Attention, the training context length is set to 512, with a block size of 64. For Self-Extend, we set the local window size M for neighbor tokens to 1024 and the group size N to 64.\nInference We list the scale factors used for different length ranges during inference in Table 8. For NTK-RoPE, given the maximum observed length during training or inference, Ctest, and the scaling hyperparameter s, we follow Fu et al. [2024] in replacing t with s \u00b7 max(C\u2032,Ctest) C \u2212(s \u22121), and set the hyperparameter s to C\u2032 2C during both training and inference. For LM-infinite, we set the global memory G = 10 and the local window M = 4096. For Landmark Attention, the training context length is set to 512, with a block size of 64. For Self-Extend, we set the local window size M for neighbor tokens to 1024 and the group size N to 64.\n<div style=\"text-align: center;\">Table 7: Hyperparameters for Different Long Sequence Methods in Training.</div>\nable 7: Hyperparameters for Different Long Sequence Methods in Train\nModels\nTrain\nLen\nTrain\nTokens\n\u03b1\nbsz\nlr\nPI\n32k\n1B\n8.0\n32\n2e-5\nNTK-32K\n32k\n1B\n29.0\n32\n2e-5\nYaRN\n32k\n1B\n8.0\n32\n2e-5\nLongLora\n32k\n1B\n8.0\n32\n2e-5\nLandmark\n32k\n1B\n-\n32\n2e-5\nNTK-64K\n64k\n1B\n57.0\n32\n2e-5\n<div style=\"text-align: center;\">Table 8: Hyperparameters for the Scale Factor Different Long-context Methods in Inference.</div>\nable 8: Hyperparameters for the Scale Factor Different Long-context Methods in Inference.\nameters for the Scale Factor Different Long-context Met\nModels\n4k\n8k\n16k\n32k\n64k\nLlama2\n-\n-\n-\n-\n-\nLM-Infinite\n-\n-\n-\n-\n-\nNTK-Frozen\n1.0\n3.0\n7.0\n15.0\n31.0\nPI\n8.0\n8.0\n8.0\n8.0\n8.0\nNTK-32K\n29.0\n29.0\n29.0\n29.0\n61.0\nYaRN\n8.0\n8.0\n8.0\n8.0\n8.0\nLongLora\n8.0\n8.0\n8.0\n8.0\n8.0\nLandmark\n-\n-\n-\n-\n-\nNTK-64K\n57.0\n57.0\n57.0\n57.0\n57.0\n# 9.3 Training Data Construction\nWe sample 1B tokens from a long-context data mixture following Fu et al. [2024]. We use the SlimPajama [Soboleva et al., 2023] dataset for continuous finetuning. This dataset serves as an open-source replication of the LLaMA [Touvron et al., 2023] pretraining data mixture. It comprises 82% web data (sourced 67% from CommonCrawl and 15% from C4), 4.5% code data (Github), 4.5% Wikipedia content, 4.5% books, 2.5% Arxiv papers, and 2.0% StackExchange content. We use per-source length-upsampling to sample 1B tokens from the datasets, which increases the portion of long sequences while keeping the domain mixture the same. We packed all sampled data into\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed76/ed76b2d2-4356-4189-a3f6-ac2d677faaf3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Needle in a Haystack evaluation. \u201cNTK-64-2B\u201d represents the NTK-64K model trained with 2B tokens. Green squares indicates a high retrieval success rate, the white dashed line denotes the longest length examples seen at training or finetuning, and the Y-axis represents the distance to the retrieved target.</div>\nchunks of the corresponding training length, regardless of document boundaries, following common practiceTouvron et al. [2023], Fu et al. [2024].\n# 9.4 Longer Model Needs more Training Tokens\nWe observe that the performance of NTK-64K is not as good as NTK-32K. Consequently, we further sample 2B tokens from a long-context data mixture from Fu et al. [2024] for training and evaluate the model on the \"Needle in A Haystack\" task, as shown in Figure 5. Our NTK-64K model demonstrates a significant performance improvement when trained with more tokens, indicating that longer models require more tokens for effective training.\n# 9.5 RoPE Scale Factor for Dynamic NTK\nWe observe that the scale factor significantly degrades NTK-Dynamic models, particularly causing performance deterioration in shorter sequences. Therefore, we conduct a grid search to determine a better scale factor for different input lengths. The scale factor and its relationship with perplexity on PG19 are reported in Table 9.\nnts of PG19 to calculate the perplexity.\nModels\nScale Factor\n4k\n8k\n16k\n32k\n64k\nNTK-Frozen\n1\n7.65\n118.82\nNaN\nNaN\nNaN\n3\n8.19\n7.99\n57.15\n386.02\nNaN\n7\n9.39\n9.26\n9.61\n72.62\n486.13\n15\n11.53\n12.04\n12.98\n20.15\n180.59\n31\n16.18\n20.66\n26.67\n40.06\n69.01\n63\n30.22\n48.78\n69.89\n89.75\n118.59\nNTK-32K\n1\n12.64\nNaN\nNaN\nNaN\nNaN\n5\n7.84\n7.638\n10.36\nNaN\nNaN\n13\n7.686\n7.459\n7.25\n8.35\nNaN\n29\n7.689\n7.457\n7.24\n6.82\n9.11\n61\n7.8\n7.565\n7.34\n6.91\n6.63\n125\n7.99\n7.774\n7.57\n7.13\n6.83\nNTK-64K\n1\n19.16\nNaN\nNaN\nNaN\nNaN\n9\n8.02\n7.79\n7.63\n22.6\nNaN\n25\n7.89\n7.65\n7.443\n7.04\n14.02\n57\n7.922\n7.67\n7.44\n7.01\n6.75\n121\n8.016\n7.75\n7.51\n7.06\n6.77\nTo validate our LongLora Chen et al. [2023b] implementation, we reproduced their Llama-2-7blonglora-32k model following LongLora\u2019s training data and training recipe. We evaluated the perplexity for the corresponding length on PG19 and Proof-file in Table 10.\n<div style=\"text-align: center;\">Table 10: Perplexity results of LongLora reported and our reproduction on PG 19 and Proof-file.</div>\nMethod\n2k\n4k\n8k\n16k\n32k\nPG19\nLlama-2-7b-longlora-32k\n8.29\n7.83\n7.54\n7.35\n7.22\nOur Reproduction\n8.10\n7.69\n7.43\n7.28\n7.32\nProof-file\nLlama-2-7b-longlora-32k\n3.35\n3.01\n2.78\n2.61\n2.50\nOur Reproduction\n3.33\n3.01\n2.80\n2.67\n2.61\n# 9.7 LongBench Results\nTable 11: LongBench results. N-32 and N-64 refer to NTK finetuned on 32K and 64K context lengths respectively. Inf refers to LM-Infinite. SE refers to Self-Extend. LLR refers to LongLora. AvgLen refers to average length of the datasets. Train Len refers to the longest length examples seen at training or finetuning. Eval Len refers to the maximum length of the input prompt. \u2713refers to whether the method is exact attention.\nExact\nAvgLen\nFrozen\nFinetuned\nBase\nInf\nN-F\nSE\nPI\nN-32\nYaRN\nCLEX\nLLR\nLand\nN-64\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTrain Len\n4k\n4k\n4k\n4K\n32k\n32k\n32k\n32k\n32k\n32k\n64k\nEval Len\n4k\n32k\n32k\n32K\n32k\n32k\n32k\n32k\n32k\n32k\n32k\nNQA\n18,409\n21.09\n10.39\n3.88\n23.49\n23.02\n23.73\n19.82\n24.19\n12.07\n12.47\n24.31\nQAPR\n3,619\n26.94\n22.58\n26.79\n28.75\n25.85\n27.50\n26.98\n23.36\n20.15\n19.06\n24.97\nMFQA\n4,559\n32.42\n26.19\n29.82\n32.66\n35.10\n38.22\n37.11\n40.83\n24.50\n21.86\n40.60\nHPQA\n9,151\n31.23\n16.13\n32.10\n37.63\n36.98\n41.56\n38.60\n35.59\n27.41\n33.66\n41.47\nWMQA\n4,887\n25.75\n20.64\n22.34\n30.70\n29.38\n31.58\n30.63\n28.24\n21.46\n24.94\n28.62\nMSQ\n11,214\n10.55\n5.26\n8.84\n15.73\n16.80\n17.41\n22.08\n17.12\n11.46\n11.41\n18.24\nGR\n8,734\n17.32\n13.43\n17.87\n13.15\n25.61\n28.27\n20.98\n24.68\n24.05\n17.20\n24.37\nQMSM\n10,614\n21.28\n6.10\n15.35\n20.20\n21.19\n21.52\n20.66\n21.55\n17.66\n18.83\n21.65\nMNWS\n2,113\n3.44\n3.63\n9.30\n1.50\n10.55\n22.13\n8.91\n16.96\n21.19\n19.43\n25.02\nTREC\n5,177\n66.00\n61.00\n67.50\n69.00\n71.00\n69.00\n69.00\n67.50\n50.00\n49.00\n69.00\nTRVQA\n8,209\n87.89\n81.40\n18.69\n88.44\n88.55\n88.86\n89.63\n89.36\n12.28\n74.75\n88.65\nSMSM\n6,258\n41.70\n15.07\n32.46\n43.76\n43.35\n42.21\n44.25\n43.02\n13.45\n40.38\n41.59\nPSC\n11,141\n2.10\n1.62\n2.67\n0.00\n1.50\n2.68\n1.05\n2.50\n4.57\n0.64\n2.09\nPSR\n9,289\n9.00\n4.00\n3.77\n4.50\n4.50\n4.62\n3.79\n8.50\n3.50\n2.50\n6.50\nLCC\n1,235\n68.22\n67.68\n63.64\n68.47\n55.05\n56.78\n54.06\n49.45\n57.12\n56.70\n52.04\nREPO\n4,206\n61.73\n58.27\n53.69\n59.99\n47.26\n49.09\n47.60\n42.84\n51.92\n48.23\n39.68\nAverage\n7,425\n32.92\n25.84\n25.54\n33.62\n33.48\n35.32\n33.45\n33.48\n23.30\n28.19\n34.30\n# 9.8 RULER Subtasks Result\nThe performance of models on different lengths and breakdowns by 13 subtasks are reported in Table 12(RULER on 4k), Table 13(RULER on 8k), Table 14(RULER on 16k), Table 15(RULER on 32k) and Table 16(RULER on 64k).\nTable 12: Ruler results on 4k context length. N-32 and N-64 refer to NTK finetuned on 32K and 64K context lengths respectively. Inf refers to LM-Infinite. SE refers to Self-Extend. LLR refers to LongLora. Train Len refers to the longest length examples seen at training or finetuning. Eval Len refers to the maximum length of the input prompt. \u2713refers to whether the method is exact attention.\nExact\nFrozen\nFinetuned\nBase\nInf\nN-F\nSE\nPI\nN-32\nYaRN\nCLEX\nLLR\nLand\nN-64\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTrain Len\n4k\n4k\n4k\n4k\n32k\n32k\n32k\n32k\n32k\n32k\n64k\nEval Len\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\nNIAH_S1\n100.00\n100.00\n100.00\n100.00\n98.00\n100.00\n99.60\n100.00\n0.00\n49.00\n100.00\nNIAH_S2\n100.00\n100.00\n100.00\n100.00\n99.80\n100.00\n88.60\n100.00\n0.00\n20.60\n100.00\nNIAH_S3\n99.20\n95.80\n98.80\n89.80\n99.80\n94.20\n53.00\n89.60\n0.00\n10.00\n97.20\nNIAH_M1\n99.20\n98.80\n99.20\n79.00\n99.20\n99.20\n62.60\n95.80\n0.00\n10.60\n98.00\nNIAH_M2\n88.00\n88.00\n88.20\n26.00\n95.40\n97.40\n14.00\n83.20\n0.00\n6.80\n97.00\nNIAH_M3\n61.40\n62.00\n61.60\n14.40\n78.00\n68.20\n8.20\n53.80\n0.00\n1.20\n84.80\nNIAH_MV\n83.55\n90.45\n86.60\n82.10\n95.45\n96.40\n50.25\n95.10\n0.05\n10.80\n96.15\nNIAH_MQ\n95.45\n96.15\n96.00\n90.70\n96.95\n97.00\n62.00\n96.20\n0.00\n5.35\n98.25\nVT\n57.72\n58.56\n56.48\n8.92\n96.64\n98.16\n25.68\n85.72\n0.00\n2.92\n97.00\nCWE\n78.20\n75.90\n78.20\n73.56\n81.38\n80.86\n58.78\n82.60\n64.70\n23.16\n74.26\nFWE\n84.33\n84.20\n84.93\n80.07\n58.40\n85.53\n26.20\n52.60\n18.53\n84.93\n81.40\nQA_1\n62.20\n60.40\n62.40\n60.60\n57.80\n62.40\n60.20\n55.80\n26.20\n37.20\n55.80\nQA_2\n43.00\n43.40\n42.40\n40.20\n42.40\n46.20\n43.20\n38.20\n28.00\n28.20\n46.00\nAvg.\n80.94\n81.05\n81.14\n65.03\n84.56\n86.58\n50.18\n79.12\n10.58\n22.37\n86.60\n<div style=\"text-align: center;\">Table 13: Ruler results on 8k context length.</div>\nExact\nFrozen\nFinetuned\nBase\nInf\nN-F\nSE\nPI\nN-32\nYaRN\nCLEX\nLLR\nLand\nN-64\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTrain Len\n4k\n4k\n4k\n4k\n32k\n32k\n32k\n32k\n32k\n32k\n64k\nEval Len\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\nNIAH_S1\n-\n46.00\n61.60\n100.00\n99.00\n99.80\n100.00\n100.00\n0.00\n46.00\n100.00\nNIAH_S2\n-\n36.60\n59.40\n98.80\n100.00\n100.00\n99.40\n100.00\n0.00\n7.20\n100.00\nNIAH_S3\n-\n20.80\n51.00\n88.60\n99.20\n94.20\n96.00\n97.80\n0.00\n3.80\n99.20\nNIAH_M1\n-\n27.80\n46.00\n69.40\n98.00\n94.20\n86.60\n90.20\n0.00\n7.60\n95.20\nNIAH_M2\n-\n4.40\n11.00\n8.20\n91.60\n86.20\n60.60\n66.00\n0.00\n1.60\n86.60\nNIAH_M3\n-\n2.60\n4.00\n3.20\n48.40\n52.20\n34.60\n11.80\n0.00\n0.00\n47.40\nNIAH_MV\n-\n30.35\n41.35\n52.95\n65.50\n85.95\n70.40\n61.25\n0.00\n6.25\n84.75\nNIAH_MQ\n-\n30.15\n50.40\n78.70\n93.25\n95.20\n92.45\n86.95\n0.00\n3.35\n94.95\nVT\n-\n4.88\n69.88\n1.48\n91.20\n96.16\n77.52\n48.16\n0.00\n3.08\n94.36\nCWE\n-\n65.08\n40.30\n30.82\n45.66\n45.76\n44.72\n32.72\n18.92\n22.08\n40.80\nFWE\n-\n56.73\n64.87\n59.00\n65.07\n70.13\n10.53\n45.40\n16.73\n76.60\n54.13\nQA_1\n-\n35.80\n44.40\n31.00\n50.80\n49.20\n43.20\n48.20\n22.80\n25.00\n50.20\nQA_2\n-\n29.00\n33.60\n37.40\n40.80\n41.80\n36.80\n42.60\n24.40\n25.20\n44.80\nAvg.\n-\n30.01\n44.45\n50.73\n76.04\n77.75\n65.60\n63.93\n6.37\n17.52\n76.34\n<div style=\"text-align: center;\">Table 14: Ruler results on 16k context length.</div>\nExact\nFrozen\nFinetuned\nBase\nInf\nN-F\nSE\nPI\nN-32\nYaRN\nCLEX\nLLR\nLand\nN-64\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTrain Len\n4k\n4k\n4k\n4k\n32k\n32k\n32k\n32k\n32k\n32k\n64k\nEval Len\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\n4k\nNIAH_S1\n-\n21.00\n14.20\n99.80\n97.20\n99.40\n100.00\n99.80\n0.00\n42.40\n99.80\nNIAH_S2\n-\n17.00\n17.40\n93.40\n100.00\n100.00\n99.20\n100.00\n0.20\n6.80\n100.00\nNIAH_S3\n-\n11.60\n8.20\n77.00\n99.60\n98.60\n89.60\n99.60\n0.00\n3.60\n100.00\nNIAH_M1\n-\n15.80\n9.20\n60.00\n97.80\n93.20\n83.40\n89.40\n0.00\n5.60\n90.80\nNIAH_M2\n-\n0.00\n0.60\n3.80\n82.80\n79.80\n19.60\n72.00\n0.00\n0.80\n67.60\nNIAH_M3\n-\n1.00\n0.00\n1.80\n34.20\n18.20\n7.40\n15.00\n0.00\n0.00\n29.60\nNIAH_MV\n-\n8.40\n6.90\n38.85\n77.55\n81.95\n58.75\n62.40\n0.00\n4.80\n83.50\nNIAH_MQ\n-\n8.85\n7.95\n59.30\n90.95\n86.20\n85.15\n81.60\n0.00\n2.75\n90.35\nVT\n-\n6.56\n11.28\n1.16\n68.84\n83.56\n47.12\n48.16\n0.00\n2.52\n88.68\nCWE\n-\n19.94\n28.36\n17.80\n27.26\n26.32\n23.72\n28.60\n0.62\n11.90\n21.20\nFWE\n-\n77.13\n25.80\n59.80\n47.93\n61.73\n10.13\n57.33\n12.93\n81.60\n51.73\nQA_1\n-\n22.80\n36.40\n28.00\n46.00\n45.20\n43.20\n49.20\n13.20\n23.00\n45.00\nQA_2\n-\n24.20\n26.00\n31.60\n35.20\n36.00\n37.40\n33.40\n20.80\n26.20\n36.00\nAvg.\n-\n18.02\n14.79\n44.02\n69.64\n70.01\n54.21\n64.35\n3.67\n16.31\n69.56\n<div style=\"text-align: center;\">Table 15: Ruler results on",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The rapid expansion of pretraining data for large language models (LLMs) has made it challenging to fully train models with longer context windows, which are essential for tasks requiring extensive textual understanding. Existing methods for extending context have shown varied performance, leading to uncertainty in evaluating long-context capabilities.",
            "purpose of benchmark": "The benchmark aims to provide a standardized evaluation protocol for comparing different context extension methods, facilitating clearer insights into their effectiveness and performance in long-context tasks."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of evaluating and comparing various long-context extension methods for LLMs, focusing on their performance in handling longer sequences of text.",
            "key obstacle": "Existing benchmarks lack a controlled evaluation framework that accounts for the variability in base models and extension techniques, making it difficult to ascertain the effectiveness of different methods."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the need to systematically evaluate long-context methods while controlling for confounding factors that could skew results.",
            "opinion": "The authors emphasize the importance of this benchmark in advancing the understanding of long-context capabilities in LLMs, as it provides a clearer framework for future research.",
            "innovation": "This benchmark introduces a controlled evaluation protocol that standardizes base models and extension methods, enabling a fair comparison of performance across different techniques.",
            "benchmark abbreviation": "LCEG"
        },
        "dataset": {
            "source": "The dataset was constructed by sampling 1 billion tokens from a long-context data mixture, ensuring a diverse representation of text types.",
            "desc": "The dataset includes a mixture of web data, code, Wikipedia content, books, academic papers, and StackExchange content, designed to evaluate long-context performance.",
            "content": "The dataset includes text data relevant to various tasks that require long-context understanding, such as summarization and question answering.",
            "size": "1,000,000,000",
            "domain": "Natural Language Processing",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "Perplexity, Needle in a Haystack",
            "aspect": "Accuracy, Retrieval Success",
            "principle": "Perplexity serves as an intrinsic metric for evaluating language modeling performance, while Needle in a Haystack assesses the precision and recall capabilities of LLMs in long contexts.",
            "procedure": "Models are evaluated based on their perplexity scores across various context lengths and their ability to retrieve specific information from long documents."
        },
        "experiments": {
            "model": "The models tested include state-of-the-art methods such as NTK, PI, and YaRN, as well as approximate attention methods like LM-Infinite and LongLora.",
            "procedure": "Models were fine-tuned on a standardized dataset with fixed hyperparameters to ensure consistency across evaluations.",
            "result": "Exact attention methods consistently outperformed approximate methods, demonstrating better accuracy in long-context tasks.",
            "variability": "Variability was accounted for through multiple trials and evaluations across different context lengths."
        },
        "conclusion": "The study concludes that a standardized approach to evaluating long-context methods reveals significant insights into their performance, highlighting the critical role of perplexity and the trade-offs in attention mechanisms.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive framework for evaluating long-context methods, promoting transparency and reproducibility in research.",
            "limitation": "The findings may not generalize to larger models or different training contexts, and the fixed hyperparameters could introduce bias.",
            "future work": "Future research should explore the implications of these findings on larger models and investigate the development of new metrics for long-context evaluation."
        },
        "other info": {
            "open source": "All codebases, models, and checkpoints will be made available open-source to support further research."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The rapid expansion of pretraining data for large language models (LLMs) has made it challenging to fully train models with longer context windows, which are essential for tasks requiring extensive textual understanding."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the challenge of evaluating and comparing various long-context extension methods for LLMs, focusing on their performance in handling longer sequences of text."
        },
        {
            "section number": "3.4",
            "key information": "This benchmark introduces a controlled evaluation protocol that standardizes base models and extension methods, enabling a fair comparison of performance across different techniques."
        },
        {
            "section number": "4.1",
            "key information": "Perplexity serves as an intrinsic metric for evaluating language modeling performance, while Needle in a Haystack assesses the precision and recall capabilities of LLMs in long contexts."
        },
        {
            "section number": "6.2",
            "key information": "The findings may not generalize to larger models or different training contexts, and the fixed hyperparameters could introduce bias."
        },
        {
            "section number": "7",
            "key information": "Future research should explore the implications of these findings on larger models and investigate the development of new metrics for long-context evaluation."
        }
    ],
    "similarity_score": 0.7006891659219535,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/A Controlled Study on Long Context Extension and Generalization in LLMs.json"
}