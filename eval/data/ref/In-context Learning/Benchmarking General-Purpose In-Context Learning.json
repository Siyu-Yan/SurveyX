{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.17234",
    "title": "Benchmarking General-Purpose In-Context Learning",
    "abstract": " Abstract\nAbstract\nIn-context learning (ICL) empowers generative models to address new tasks effectively and efficiently on the fly, without relying on any artificially crafted optimization techniques. In this paper, we study extending ICL to address a broader range of tasks with an extended learning horizon and higher improvement potential, namely General Purpose In-Context Learning (GPICL). To this end, we introduce two lightweight benchmarks specifically crafted to train and evaluate GPICL functionalities. Each benchmark encompasses a vast number of tasks characterized by significant task variance. These tasks are also crafted to promote long-horizon in-context learning through continuous generation and interaction, covering domains such as language modeling, decision-making, and world modeling. The benchmarks necessitate the models to leverage contexts and history interactions to enhance their capabilities, which we believe to be the key characteristics of GPICL. Our experiments indicate that the diversity of training tasks is positively correlated with the ability to generalize with ICL, but inversely correlated with zero-shot capabilities. Additionally, our findings indicate that the scale of parameters alone may not be crucial for ICL or GPICL, suggesting alternative approaches such as increasing the scale of contexts and memory states.\narXiv:2405.17234v6 \n# 1 Introduction\nThe success of large-scale generative language models can be primarily attributed to two key factors: 1. Zero-shot generalization derived from the extensive accumulation and storage of knowledge within the model parameters during the pre-training and fine-tuning stages (Kojima et al., 2022); 2. In-context learning(ICL) to distill knowledge from the context during the inference stage (Brown et al., 2020). The ability of zero-shot generalization remains unchanged post-training, which parallel the concept of \u201cinnate abilities\u201d (Koulakov et al., 2021) of biology. The ICL capability allows the a",
    "bib_name": "wang2024benchmarkinggeneralpurposeincontextlearning",
    "md_text": "# Benchmarking General-Purpose In-Context Learning\nFan Wang, Chuan Lin, Yang Cao, Yu Kang\n# Abstract\nAbstract\nIn-context learning (ICL) empowers generative models to address new tasks effectively and efficiently on the fly, without relying on any artificially crafted optimization techniques. In this paper, we study extending ICL to address a broader range of tasks with an extended learning horizon and higher improvement potential, namely General Purpose In-Context Learning (GPICL). To this end, we introduce two lightweight benchmarks specifically crafted to train and evaluate GPICL functionalities. Each benchmark encompasses a vast number of tasks characterized by significant task variance. These tasks are also crafted to promote long-horizon in-context learning through continuous generation and interaction, covering domains such as language modeling, decision-making, and world modeling. The benchmarks necessitate the models to leverage contexts and history interactions to enhance their capabilities, which we believe to be the key characteristics of GPICL. Our experiments indicate that the diversity of training tasks is positively correlated with the ability to generalize with ICL, but inversely correlated with zero-shot capabilities. Additionally, our findings indicate that the scale of parameters alone may not be crucial for ICL or GPICL, suggesting alternative approaches such as increasing the scale of contexts and memory states.\narXiv:2405.17234v6 \n# 1 Introduction\nThe success of large-scale generative language models can be primarily attributed to two key factors: 1. Zero-shot generalization derived from the extensive accumulation and storage of knowledge within the model parameters during the pre-training and fine-tuning stages (Kojima et al., 2022); 2. In-context learning(ICL) to distill knowledge from the context during the inference stage (Brown et al., 2020). The ability of zero-shot generalization remains unchanged post-training, which parallel the concept of \u201cinnate abilities\u201d (Koulakov et al., 2021) of biology. The ICL capability allows the agent to handle various unseen tasks without modifying the model parameters, only resulting in changes to the hidden states and memories. Currently, the ICL capabilities of large language models are confined to natural language-based tasks and relatively naive tasks such as following instructions and mimicking demonstrations. However, they generally lack the ability to continually interact with real-world environments and engage in reinforcement learning from contexts. Additionally, extremely complex tasks that require substantial amounts of context still pose significant challenges.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6cbd/6cbd2d11-1609-4a39-9d10-1ec31cf1fd5f.png\" style=\"width: 50%;\"></div>\nsource code will be available in the final version\nThe enormous lifelong learning and adaptation potential of biological neural networks that forms abrupt contrast with their limited innate abilities at the initial stages (e.g., the stage of mammalian infancy) are also used to inspire the design of machine intelligence systems (Zador, 2019; Wang et al., 2022; Schmidgall et al., 2024). Among those a promising topic is general purpose in-context learning (GPICL) (Kirsch et al., 2022; 2023). GPICL involves meta-training across a wide range of task classes and learning to interpret in context to generalize to new tasks. However, the characteristic of diversity in this context can be challenging to quantify. We aim to enrich the concept of GPICL by incorporating additional features, as shown in Figure 1. We consider the meta-learning framework in which a model is meta-trained in the outer loop to utilize contextual information for task adaptation in the inner loop. We identify a model with GPICL capability by three main characteristics: 1. Low zero-shot performance; 2. High ICL potential; 3. A long ICL horizon.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b67/0b67c16e-10bf-40bc-89d8-2077db8cfb3e.png\" style=\"width: 50%;\"></div>\nWe propose two lightweight benchmarks that adhere to the GPICL standard and are capable of generating unlimited synthetic data: 1. Meta-Language: This benchmark generates random yet consistent language patterns for meta-training the ICL functionality, enabling it to master a novel language from scratch. 2. Maze World: This benchmark consists of a variety of mazes with different targets that are randomly generated for navigation purposes. It is designed for training and evaluating models capable of navigating in various scenarios through observation, mapping, localization, and planning. We provide experiments of baseline models composed of language modeling, world modeling, and decision modeling based on the Transformer backbone and its variants, as described in Vaswani et al. (2017). It is shown that: 1. With auto-regressive models, both benchmarks exhibit low zero-shot performance and high potential for generalization with ICL; 2. Increasing the diversity of meta-training tasks can naturally lead to decreased zero-shot performance, improved generalization to unseen tasks, and an extended ICL horizon, implying that sufficiently diverse tasks can lead to effective GPICL; 3. The performance of GPICL is not strongly correlated with the scale of parameters once a certain threshold is exceeded. On the other hand, the size of the context significantly influences the model\u2019s performance, which aligns with the findings in Kirsch et al. (2022); Wang et al. (2022). The results strongly suggest that, in evaluations, the position-wise performance of the auto-regressive model should be investigated in detail, rather than averaging the performance across different context lengths. Our findings reveal a potential alternative approach to replace the current mainstream learning pipeline of large-scale language models, as shown in Figure 2. Typical Large Language Models (LLMs) go through three stages: Pre-training, Fine-tuning, and ICL to train and customize. With GPICL, it is possible to simplify this pipeline to just two stages: Meta-training and GPICL. Moreover, given the data efficiency of GPICL\ncompared with gradient-based methods, it is possible to base meta-training on low-quality synthetic data and use high-quality data more efficiently to master novel skills.\n# compared with gradient-based methods, it is possible to base meta-training on low-quality synthetic data and use high-quality data more efficiently to master novel skills.\n# 2 Learning to learn an randomized language: Meta-Language\n# 2.1 Problem Setting\nMost large language models (LLMs) are trained on specific, existing languages. Rather than synthesizing existing languages, we primarily consider learning a new language as a task to focus more on the ICL functionality. Although there are hundreds of languages in the world, treating each language as a sample for GPICL provides only a few hundred samples, which is insufficient. We introduce the \"Meta Language\" benchmark. This benchmark is designed to generate a vast number of new \"languages\" and assess the models\u2019 ability to learn an arbitrary new \"language.\" This pseudo-language is simulated using n-gram settings, generated by a randomized neural network f. The probability of the next token is determined by its preceding n tokens and a set of random parameters \u03b8.\np(xt|xt\u22121, xt\u22122, ...) = f(xt\u22121, ..., xt\u2212n; \u03b8),\nAlgorithm 1 Generating a length=T sequence of meta langauge\n1: Randomly Sample \u03b8 \u223cN(0, \u03c32), (\u03b8 is the parameter of the specified n-gram model f)\n2: for t in [1, T] do\n3:\nz1, ..., zN = f(xt\u22121, ..., xt\u2212n; \u03b8)\n4:\nFor i \u2208[1, N], zi = \u03bb[zi \u2212E(z)]/\n\ufffd\nV(z)\n5:\np(xt = xi) = exp(zi)/\ufffd\nj exp(zj)\n6:\nRandomly sample xt \u223cp(xt)\nThe generation process is as simple as Algorithm 1. To maintain the diversity of the generated meta language, we also normalize the mean and variance of the logits input to Softmax to keep the distribution being neither too stiff nor too flat. We set the hyper-parameter \u03bb in Algorithm 1 such that the perplexity of the ground truth generator E[\u2212logp(xt))] is kept between 0.5 and 1.0. The sequences generated by the randomized generator are meaningless and chaotic. However, as long as the generator\u2019s parameters remain static, a model can progressively learn and capture the underlying rules of the randomized generator. The complexity of the generated sequences can be bounded by N n, where N is the size of the vocabulary. Therefore, with a sufficiently large N and n\u2014parameters that can be adjusted as needed\u2014the complexity of the pseudo-language can be made arbitrarily high to test super-long-term dependencies.\n# 2.2 Baseline Solution\nAs the training data, we generated 500K sequences with a pre-defined vocabulary size of N = 32, and the complexity of the pseudo-language was uniformly sampled across n \u22083, 4, 5, 6. Each sequence comprises 4, 096 tokens, totaling 2B tokens. We apply the auto-regressive transformer (Vaswani et al., 2017) with rotary embedding (Su et al., 2024). The hyper-parameter settings include: a tiny-sized transformer (303k parameters), a small-sized transformer (9.5M parameters), and a standard-sized(151m parameters) model. The evaluation set consists of independently sampled sequences, totaling 4K samples (16M tokens) for each n \u2208{2, 3, 4, 5, 6, 7, 8}, sampled uniformly. We assessed the model\u2019s performance by plotting the averaged perplexity (\u2212log p(xt)) for each position across different n values. To assess how the trained model generalizes, we also included the PG-19 (Rae et al., 2019) test set as a natural corpus. In this set, words are broken down into 26 alphabetic characters and randomly mapped onto 32 pre-defined tokens, the left slot are given to punctuation marks. The entire test texts are concatenated into a long sequence and broken down into sentences of 4,096 characters each. It is important to note that the model has never been trained on any natural language corpora. Details of the model description can be found in Appendix A.1.\n(1)\n# 2.3 Evaluation Results\n# 2.3.1 Towards Meta-Language Model\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f2f/3f2f1bc3-f4c1-49cb-9005-701d5edca622.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparing evaluations on meta-language sampled by different language complexity. Horizontal axis represent the position or context length, and vertical axis represent the perplexity (the lower the better).</div>\nWe demonstrate that this pseudo-language trained model can be regarded as a meta-language model designed to adapt to any conceivable language through GPICL. See from basic evaluation results of 151M model on the test sets in Figure 3, several observations are noteworthy: First, perplexity improves with increasing context length across all values of n, indicating that the model is effectively leveraging in-context learning. Notably, for n = 7 and n = 8\u2014complexities on which the model was not explicitly trained\u2014we observed a consistent semi-logarithmic improvement in perplexity with increasing context length. Second, a higher language complexity, indicated by larger values of n, correlates with a more gradual improvement in perplexity relative to context length. This phenomenon confirms that increased language complexity introduces longer-term dependencies within contexts, establishing meta language as an ideal benchmark for evaluating long-dependency modeling capabilities. Most importantly, the model demonstrates a significant performance gain through in-context learning (ICL) in realistic natural language scenarios, despite never being exposed to a single real-world natural word, illustrating that the ICL capability acquired through meta-language is not restricted to a specific language type. We further test the ability of generation of the meta-language model on two rudimentary real-world language tasks, showing that in contrast to the absence of any natural language corpora, the model is able to learn to adapt to the real world language tasks through ICL only (Appendix A.3).\n# 2.3.2 Scaling Laws for GPICL\nWe have conducted further research into how the scale of parameters affects GPICL capability. As illustrated in Fig. 4, our findings reveal notable trends: 1. Language complexity (n) influences the relative performance of different-sized models and affects the dependence on context length. In simpler tasks (n < 4), performance rapidly converges to lower bounds asymptotically (t \u223cT = 4k. Here, with a slight abuse of notation, T does not necessarily represent the ICL horizon, but rather denotes the maximum context length used in our experiments.), and the performances of various models become indistinguishable, despite starting differently. 2. It should be noted that for tasks as straightforward as n = 2, where a 300K model achieves comparable asymptotic performance to a 150M model, we observe that larger models cannot attain higher performance without sufficient context. It is reasonable to infer that increasing model complexity further will not be beneficial. This observation aligns with a key characteristic of GPICL, where learning from context takes precedence over zero-shot generalization. 3. For greater complexity (n > 6), the ICL horizon clearly extends far beyond T = 4k, and the improvement in perplexity exhibits polynomial scaling law (depicted as a linear relationship on a semi-logarithmic axis) with increasing context length. However, it remains difficult to assert from current experiments whether, given sufficiently long contexts, models of varying scales will converge to similar asymptotic performances. Nevertheless, we can confidently conclude that for n < 8, a model with 10 million parameters has been adequate. 4. For the PG-19 test set, we were surprised to find that the tiny-sized model achieves comparable asymptotic performance to the other two models. We must consider\nthat the complexity of natural language far exceeds that of any conceivable n-gram language. Furthermore, the improvement does not exhibit a linear trend on a semi-logarithmic scale. It is reasonable to hypothesize that the GPICL is only capturing certain short-term and high-frequency patterns in natural language (such as frequently appeared words). Our experiments suggest a shift in the GPICL paradigm from increasing the parameters to increasing the context length, memory, and states provided the parameters are sufficient. In contrast, current large language models (LLMs) typically require at least billions of parameters, the majority of which we suspect are employed for zero-shot generalization, while the parameters dedicated to Incremental Context Learning (ICL) are relatively sparse. This perspective is consistent with the concept of the genomics bottleneck (Wang et al., 2022), which advocates for models that, while adequately parameterized, prioritize larger memory capacities to optimize GPICL. However, it is important to note that while the transformer architecture offers a \u201cfull memory\u201d capability for sequence modeling, it also incurs a computational cost of \u0398(T 2). This underscores the necessity for developing more efficient transformers, which would allow for the extension of context length to longer spans. We believe research in this area (Tay et al., 2022) could benefit from this benchmark.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59be/59be4896-f6bb-4f1c-89c5-4d42138a5003.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Comparing performances of language models of different sizes on the the same test set. Horizontal axis represent the position or context length, and vertical axis represent the perplexity (the lower the better).</div>\n# 3 Learning to navigate an randomized world: Maze World\n# 3.1 Problem Setting\nMazes are typical environments that can scale up to an unlimited number of layouts, features, targets, and tasks. Many previous works benchmark meta-learning with mazes, but these are limited to 2-D observation space (Wang et al., 2022; Morad et al., 2023; Nikulin et al., 2023), text-only (Ding et al., 2024) observation and action space, and limited number of layouts and tasks (Duan et al., 2016; Morad et al., 2023; Ding et al., 2024). To enhance the task diversity, we propose the Maze World benchmark. It tasks an agent with navigating a completely unknown environment, requiring it to explore, memorize its surroundings, perform simultaneous localization and mapping (SLAM), identify navigation targets, and plan its route. This setting could potentially reflect a wide range of real-world tasks, such as household robots adapting to unfamiliar indoor environments. As the agent gains more experience within a specific environment, it should progressively gather more information and navigate to specified targets more efficiently. The proposed benchmarks has the following features:\n\u2022 A number of Potential Navigation Targets (PNTs) are placed at random locations and remai fixed throughout the task. Each PNT is marked by a semi-transparent beam of light, as shown i Figure 5(a).\n\u2022 For each task, a randomized sequence of commands is generated, each directing the agent to wal to a specified target that is identifiable by color.\nThe agent receives a positive reward for reaching the specified target, while a minor step-cost is incurred for each step taken. Available actions include moving forward, moving backward, stopping, turning left, and turning right. Note that unlike previous works (Chen et al., 2021; Kirsch et al., 2023; Lin et al., 2023; Lee et al., 2024), we did not use rewards as inputs, nor did we explicitly predict rewards. In the \u201cNAVIGATION\u201d tasks of Maze World, instant rewards are easily inferred from observations. However, in other tasks within Maze World, such as \u201cSURVIVAL\u201d tasks (see details in the Appendix A.5), modeling rewards may be necessary. This should not impede the analysis of the benchmark and GPICL process. To provide a cost-effective and scalable reference dataset for meta-training GPICL, we have implemented rule-based agents with a robust simulated policy. These agents are allowed to access the global maps, but with limitations: They are allowed to record map areas within their line of sight into memory, while other areas remain hidden. These agents are referred to as privileged agents because they have access to additional information. The privileged agents efficiently navigates to their goals by utilizing an explorationthen-exploitation strategy. Although these agents may not consistently reach the global optimum due to shortcomings in its exploration strategy, it provides a relatively high standard reference policy for imitation learning. To simulate variant levels of intelligent agents, we equip the privileged agents with both Long-Term Memory (LTM) and Short-Term Memory (STM). The STM retains only the most recent three pieces of observation, while the LTM retains information until the episode concludes. Only part of the STM is allowed to be recorded to LTM. We introduce a hyper-parameter, p(STM \u2192LTM), to govern the probability of transferring information from STM to LTM. At p(STM \u2192LTM) = 0%, the agent operates without LTM, whereas p(STM \u2192LTM) = 100% allows for a fully functional LTM. For simplicity, a privileged agent with p(STM \u2192 LTM) = p is written as preiviledged agent (p) for short.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8afc/8afc92b4-98e5-4210-a726-e8ee25dcfd67.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) A demonstration of observations and a global map in MazeWorld</div>\nFigure 5: A demonstration of the maze world tasks alongside privileged agents with smart navigation policy. The privileged agent is provided with limited access to the environment\u2019s ground truth (b) under specific conditions. Only the areas within the agent\u2019s line of sight are revealed, while the unsighted regions remain obscured. The agent retains the visual information from the nearest three frames in its short-term memory, which is then transferred to long-term memory based on a specified probability. The privileged agent employs an exploration-then-exploitation strategy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca8e/ca8eabc3-87c2-451a-b9f0-75e12cd5a48a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) The LTM of a privileged agent</div>\n# 3.2 Baseline Solution\nThe baseline solution for the Maze World tasks is inspired by model-based imitation learning (Hu et al., 2022) and causal decision models (Chen et al., 2021). Although running meta-reinforcement learning (MetaRL) could be beneficial, we did not directly apply RL due to the significant challenges in scaling RL to meet the required needs. Following previous work that suggests imitating an expert as adequate to generate reinforcement learning capabilities (Lin et al., 2023; Zisman et al., 2023; Lee et al., 2024), we mainly focus on imitation learning augmented with world models. We use Variational Auto Encoder (VAE) to process each image to latent representation, and use a causal transformer (Chen et al., 2021) to encode the historical observation and action. The model ouputs the distribution of actions, and also the expected next frame. Specifically, given the reference trajectory (i1, a1, i2, a2, ..., iT , aT , iT +1), where it and at represent the observation and action respectively, our model is represented by:\nwhere zt is the latent embedding of the observed image it. Predicting the distribution of actions (\u03c0) and the next frame (\u02c6zt) are typically referred as Policy Model and World Model respectively. The learning losses are composed of three parts: 1. the reconstruction error (Lvae); 2. the cross entropy between the reference action and the predicted distribution (Lpm); 3. the next-frame prediction error (Lwm). The final loss would be:\nwhere zt is the latent embedding of the observed image it. Predicting the distribution of actions (\u03c0) and th next frame (\u02c6zt) are typically referred as Policy Model and World Model respectively.\nThe learning losses are composed of three parts: 1. the reconstruction error (Lvae); 2. the cross entropy between the reference action and the predicted distribution (Lpm); 3. the next-frame prediction error (Lwm). The final loss would be:\nThe choice of causal transformer includes a small-sized transformer (26M parameters) and a standard-sized transformer (237M parameters). A overview of the model architecture is in Fig. 6. To train our baseline models, we collect training data include 120K episodes of demonstration from demonstration of privileged agents for imitation learning. Each episode has 2, 048 steps, which gives 240M frames in total. To avoid the growing \u201ccompounding errors\u201d (Ross and Bagnell, 2010) associated with direct behavior cloning, we drew inspiration from dataset aggregation (DAgger) methods (Ross et al., 2011). We collected the trajectory of states by running a very noisy behavior policy, but used the privileged agent (100%) to label its action at each step (which is not actually executed). Additionally, to enhance the stability of auto-regression, we dropout and randomize 15% of the input observations to the causal transformer during training (Details in Appendix A.2). To investigate the impact of context length on performance, we introduce additional two variants that share the same structure as the small-sized causal transformer (with 26M parameters), but have limited access to the contexts. The partial-context causal transformer has an attention window of 2, providing effective context length up to 9. The context-free transformer has an attention window of 1, providing effective context length of 1. The context-free and partial-context transformers are trained by further fine-tuning on the foundation of the full-context version (Appendix A.1).\n# 3.3 Baseline Performances\nStatic Evaluation. We first conduct static evaluation using off-policy data from an independently sampled validation set comprising 1275 sequences (totaling 2.6M frames). To analyze the impact of context length on performance, we assessed the losses of the world model and policy model for each individual time step\n(2) (3) (4)\n(5)\n(6)\n(7)\n(8)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dde8/dde87b78-6928-40e9-afd6-f94818884251.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/655d/655dac64-0936-4740-86ea-0804252526c6.png\" style=\"width: 50%;\"></div>\nFigure 6: An overview of the model evaluated on the maze world tasks. Initially, a Variational Auto-Encoder (VAE) is trained to encode observed images into hidden vectors and vice versa, with its parameters subsequently frozen. The sequence of observations and actions is then utilized to train a causal model, specifically a transformer that employs only backward attention, using imitation learning and self-supervised learning techniques. For context-free and partial-context solutions, we intentionally mask some of the backward connections.\n<div style=\"text-align: center;\">Figure 6: An overview of the model evaluated on the maze world tasks. Initially, a Variational Auto-Encoder (VAE) is trained to encode observed images into hidden vectors and vice versa, with its parameters subse quently frozen. The sequence of observations and actions is then utilized to train a causal model, specifically a transformer that employs only backward attention, using imitation learning and self-supervised learning techniques. For context-free and partial-context solutions, we intentionally mask some of the backward con nections.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0bc2/0bc2a4f0-cf53-465e-9cf6-9b42110d415c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Static assessment of the ability to imitate previleged agents (Lpm) and predict the next fram (Lwm) across each time step (t).</div>\n(Lwm(t) and Lpm(t)). In Figure 7, We compare the performance of the four variants, with several noteworthy observations: First, the ability to encode long contexts significantly influences performance, while the scale of the parameters shows no observable effect, which is also consistent with the conclusion of Meta-Language. Second, both world-modeling and policy-modeling improve with increased context length, though they exhibit some subtle differences. The capability of world-modeling improves rapidly within the first 20 steps, after which the rate of improvement slows. This could be because single-step world modeling is primarily impacted by the localized environment, explaining the rapid initial improvement with short contexts, but is less influenced by distant layouts, leading to slower improvements over longer contexts. In contrast, the improvement in policy modeling with increased context length is not monotonic, with the loss remaining relatively unchanged between 10 and 500 steps. The decision modeling loss is closely related to the reference policy, which follows an exploration-then-exploitation strategy. These results may indicate that imitating the exploration strategy is more challenging. Interactive Evaluation. It is more beneficial to investigate the online performance of the policy model and world model by directly interacting with the environment. We first directly evaluate the performance of the policy model by interacting with a set of evaluation tasks. The evaluation set includes 64 tasks across\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/575e/575e7529-6bce-4800-b7fe-496c6a4d80a3.png\" style=\"width: 50%;\"></div>\nFigure 8: Interactive evaluation results of the randomized, previleged and casual-modeling based agents, depicted by plotting accumulated rewards against simulated time steps. The step minor cost is set to 0. Each plot summarizes the results from static 64 tasks. The shaded area represents the 95% confidence interval of the expectation.\nmazes of sizes 15 \u00d7 15, 25 \u00d7 25, 35 \u00d7 35. We plot the average accumulated rewards and their confidence over time (t). For comparative analysis, we also include the performance of a random policy, where actions are randomly sampled regardless of the observations, as well as privileged agents with p(STM \u2192LTM) of 5%, 25%, 100%. The results (Figure 8) indicate that the model with full contexts outperforms the random policy, context-free agents, and privileged agents (5%), but there is a significant gap compared to privileged agents with 25% and 100% LTM. However, when compared with partial-context causal transformers, the superiority of the full-context model is not obvious. Considering the conclusion in static evaluation, it is reasonable to consider that the performance of interactive evaluation is constrained by imitation learning itself. We believe it is beneficial to incorporate on-policy trajectories in the dataset in the future, either in reinforcement learning or DAgger (Ross et al., 2011). In-Context Improvement of World Modeling. In Figure 9, the world model is evaluated by comparing the ground truth future frames and predicted future frames at steps 1, 100, 1000, and 2000 of each interactive run. We demonstrate the performance of world models in predicting both the immediate future (k = 1) and a slightly more distant future (k = 4). The latter is more reliant on long-term memory, making it inherently more difficult. In the case of smaller 15 \u00d7 15 mazes, we notice a substantial enhancement in world modeling as the context length expands from 1 to 1000, which is validated by a consistent reduction of prediction errors for both k = 1 and k = 4. For larger-scale mazes, the challenge of predicting k = 4 is amplified, as retaining the entire map in memory becomes increasingly challenging. The partial-context model does not enhance its predictive capabilities as context length increases over 100, which aligns with expectations that it can not keep a long-term memory. The results indicate that the full-context causal transformer demonstrates a reasonable capacity to utilize contextual information for world modeling, yet there is considerable room for improvement, especially in solving larger-scale mazes and the efficient utilization of long-term dependencies. For further insights and cases, refer to Appendix A.4.\n# 4 Emergence of Generalizability\nTo illustrate the relationship between the two benchmarks and GPICL, we delve into how the ICL capability is linked to the diversity of meta-training data in these benchmarks. In this section, we pre-select a set of tasks to generate the sequences (dataset) for meta-training. Unlike the procedural generation of tasks for each sequence, this approach reduces the diversity of both the task-set and the dataset, by allowing multiple sequences to be derived from a single task. For example, in Meta-Language, each task is defined by the parameters of the n-gram generator (\u03b8), allowing for the generation of different sequences from a single task through softmax sampling. Similarly, in Maze World, each task is characterized by the layout, textures, and the position of the PNTs (potential navigation targets), with different trajectories being generated by sampling behavior policies. To examine the effect of task diversity on ICL capability, we create datasets of the same size but with varying numbers of pre-defined tasks. In Meta-Language, we generate 500,000\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d962/d962fbf4-a852-47e4-b1d6-7b76036b175a.png\" style=\"width: 50%;\"></div>\nFigure 9: Results regarding the in-context improvement of world modeling in Maze World. We assess the mean square error (MSE) for forecasting subsequent frames. When k = 1, it indicates the error associated with predicting the immediately following frame. When k = 4, it denotes the cumulative error for predicting 4 frames ahead in an autoregressive manner. The predictions are initiated from various context lengths (or time step), with t taking values from the set {1, 100, 1000, 2000}. The mean square error is averaged across 64 procedural generated tasks, and the shaded area provides a 95% confidence interval for these measurements.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a47f/a47f4da5-080f-44fc-8d1f-05628525e6a3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Performance of the meta-language model trained with 500,000 sequences generated from varying numbers of pre-selected tasks. The vertical axis represents the average perplexity in evaluation (lower values indicate better performance), and the horizontal axis represents the context length. Each point in the seen tasks evaluation is the average of 4,000 sequences generated from the pre-selected tasks. Each point in the unseen tasks evaluation is based on the PG-19 test set.</div>\nsequences (2 billion tokens) using 10, 1,000, and 100,000 pre-sampled tasks, respectively. In Maze-World, we synthesize 64,000 sequences (126 million frames) with 1, 10, 100, and 10,000 pre-sampled tasks for the 15 \u00d7 15 mazes. Given that the performance of standard-sized models and small-sized models was nearly indistinguishable in our previous experiments, we use only small-sized models for both benchmarks. Each meta-trained model is assessed on both \u2019seen\u2019 tasks (those used to generate the meta-training data) and \u2019unseen\u2019 tasks. The outcomes are displayed in Figures 10 and 11. Several conclusions have been validated across both benchmarks: First, we observe a continuously increasing asymptotic performance (the performance with sufficiently long context) in unseen tasks as the number of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45a1/45a1bc46-3014-4952-8523-b6c919abae5b.png\" style=\"width: 50%;\"></div>\nFigure 11: Performance of the causal transformer model trained with 64,000 Maze World sequences generated from varying numbers of pre-defined tasks. The left vertical axis represents the accumulated reward (higher values indicate better performance), while the right vertical axis represents the prediction error of world modeling in the interactive evaluation (lower values indicate better performance). The horizontal axis represents the context length or time steps. For the prediction error of world modeling, we evaluated the mean squared error (MSE) for predicting 1 and 4 steps into the future at context lengths of 1, 100, 1000, and 2000. Each evaluation point is the average of 64 runs, with the shaded area representing the 95% confidence interval.\nmeta-training tasks increases, a phenomenon also noted by Kirsch and Schmidhuber (2021). This finding confirms that the capability for task generalization is enhanced. Second, which is less discussed in the previous work, for seen tasks, performance does not consistently improve with the increase in the number of tasks; in contrast, it declines in most of the groups. Additionally, we notice an expansion of the ICL horizon, identified by the length of context during the performance improvement phase, and a reduction in zero-shot performance, identified by the level of performance at the first time step (where context length is 0), in both seen and unseen tasks. We hypothesize that this may be a consequence of the model transitioning from task-identification to learning-to-learn. In this process, the model retains less specific information about each task but gains improved generalizability and the ability to learn through ICL. The decline in performance for seen tasks could also be attributed to the insufficient context length we employ (up to 2,048 and 4,096) and the size of the dataset. Last, we observe a slower convergence in meta-training as the number of tasks increases, despite the dataset size remaining constant (Figure 15). This suggests that the process of learning to learn is more challenging than mastering specific tasks. It is important to note that the observed increase in generalizability, the expansion of the ICL horizon, and the reduction in zero-shot capability are all in line with the definition of GPICL that we previously mentioned.\n# 5 Related Work\n# 5.1 Meta-learning and GPICL\nMeta-learning encompasses a wide range of methodologies, including gradient-based adaptation (Finn et al., 2017), attention-based adaptation (Mishra et al., 2018), and memory-based adaptation (Santoro et al., 2016; Duan et al., 2016; Wang et al., 2022; Lu et al., 2024). Both attention-based and memory-based adaptations are closely related to in-context learning, especially general-purpose ICL. Since the discovery that large-scale\nlanguage models (LLMs) can perform few-shot learning (Ouyang et al., 2022), Transformer architectures and LLMs are increasingly becoming the state-of-the-art for meta-learning and in-context learning (ICL). This includes applications ranging from few-shot supervised learning (Min et al., 2021; Garg et al., 2022; Dai et al., 2023; Li et al., 2023), in-context reinforcement learning (Chen et al., 2021; Laskin et al., 2022; Lin et al., 2023), to multi-modal reasoning and embodied control (Reed et al., 2022; Szot et al., 2023). However, typical pre-training models only address relatively simple few-shot learning tasks, while more complex tasks still require task-specific or class-specific training. It is desirable to introduce general-purpose in-context learning (Kirsch et al., 2022; 2023) for generalizing across a wider variety of tasks, which necessitates benchmarks that can scale up in both quantity and diversity.\n# 5.2 Relative Benchmarks\nMeta-Learning Benchmarks. A variety of benchmarks have been utilized to evaluate meta-learning algorithms, encompassing domains such as image classification (LeCun et al., 1998), video games, and robot manipulation (Brockman et al., 2016). Many of these benchmarks were not initially designed for meta-learning but have been adapted to accommodate varying tasks through parametric adjustments in hyperparameters and by artificially hiding certain parameters from observation. This can be further advanced to include precedurely generated random targets of control (Finn et al., 2017; Mishra et al., 2018; Yu et al., 2020; Morad et al., 2023), video games of different difficulty level (Cobbe et al., 2019), geometry of locomotion (Najarro and Risi, 2020), rules of transition (Nikulin et al., 2023), and layouts of mazes (Mishra et al., 2018; Morad et al., 2023). However, some of these hidden configurations are relatively simple and fail to generate sufficient diversity, causing models to lean towards acquiring zero-shot and task identification abilities rather than in-context learning from scratch. For example, while many previous benchmarks include hiding the target of locomotion, an ideal learning algorithm should be able to locate the hidden target in at most three steps by comparing the rewards of walking in different directions, given that the location of the target is only in two dimensions. Another promising approach is to randomize labels (Kirsch et al., 2022; Wei et al., 2023), observations (Morad et al., 2023), and actions (Sinii et al., 2023), which tend to have higher dimensions. Based on the prior studies, it is plausible to infer that the diversity of tasks and the resulting generalization capabilities are related to the number of hyperparameters that can be varied. In contrast to the artificial randomization of rewards, observations, and targets, we believe that Meta-Language and MazeWorld offer environments that are inherently more complex and significant for benchmarking GPICL. Language Modeling. Recently, emphasis has been placed on long-term dependency modeling in large language models (LLMs). Most of the newly proposed benchmarks support context lengths over 8K tokens (Rae et al., 2019; Tay et al., 2020; Bai et al., 2023; Shen et al., 2023; Song et al., 2024). However, there are several limitations when using current language-based benchmarks for in-context learning (ICL) investigation. First, some benchmarks focus on one-step classification with very long feature descriptions (Tay et al., 2020; Chalkidis et al., 2019; Bai et al., 2023), which is not suitable for examining the interactive performance of the model. Second, it is unclear how much long-term dependency actually exists in these long contexts (Chen et al., 2024). Even if some long-term dependencies are present, they may be weak and relatively simplistic (such as retrieval and copy tasks), and could be overshadowed by the powerful zero-shot generalization capabilities of the models. This overshadowing can obscure crucial information important for GPICL. In practice, it is often found that scaling up model parameters to enhance zero-shot generalization typically yields better results than extending the ICL horizon. Visual Navigation. There are a few datasets featuring indoor and outdoor robot-environment interaction scenes, reconstructed from images taken in real-world settings (Song et al., 2017; Chang et al., 2017; Xia et al., 2018), or simulated environments (Szot et al., 2021). Currently, most of these environments primarily facilitate the learning of zero-shot generalization. A significant limitation of these datasets is the high data collection cost and the challenge of scaling up. Compared to these benchmarks, the proposed Maze World has a much larger sim-to-real gap, but offers greater diversity and complexity, and is much easier to scale up. Additionally, we suggest that reality-generated data could better serve as contexts in the GPICL stage, as illustrated in Figure 2(b), rather than during meta-training.\n# 6 Conclusions\nIn this paper, we introduce two benchmarks specifically designed for GPICL. Our preliminary investigations of these benchmarks have yielded inspiring results, demonstrating that scaling laws might exist between context length and performances, and that the performance is not entirely dependent on the scaling of parameters. It is important to note that although we are seeing the potential of application of those datasets in areas including language modeling and indoor navigation, the synthetic data generated by those benchmarks are not yet validated to be ready for a direct help to application. This work can be expanded along two directions: 1. The development of more realistic GPICL benchmarks, for instance, by including additional complexity to the meta-language; 2. Utilizing the benchmarks to uncover more sophisticated models (including memoryaugmented models) and optimization techniques (such as reinforcement learning) to further extend the\nIn this paper, we introduce two benchmarks specifically designed for GPICL. Our preliminary investigations of these benchmarks have yielded inspiring results, demonstrating that scaling laws might exist between context length and performances, and that the performance is not entirely dependent on the scaling of parameters. It is important to note that although we are seeing the potential of application of those datasets in areas including language modeling and indoor navigation, the synthetic data generated by those benchmarks are\nIn this paper, we introduce two benchmarks specifically designed for GPICL. Our preliminary investigations of these benchmarks have yielded inspiring results, demonstrating that scaling laws might exist between context length and performances, and that the performance is not entirely dependent on the scaling of parameters.\n# References\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in english. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317\u20134323, 2019. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In 2017 International Conference on 3D Vision (3DV), pages 667\u2013676. IEEE, 2017. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021. Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo, and Min Yang. Long context is not long at all: A prospector of long-dependency data for large language models. 2024. URL https://api. semanticscholar.org/CorpusID:270068255. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In International conference on machine learning, pages 1282\u20131289. PMLR, 2019. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn incontext? language models implicitly perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. Peng Ding, Jiading Fang, Peng Li, Kangrui Wang, Xiaochen Zhou, Mo Yu, Jing Li, Matthew R Walter, and Hongyuan Mei. Mango: A benchmark for evaluating mapping and navigation abilities of large language models. arXiv preprint arXiv:2403.19913, 2024. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn incontext? a case study of simple function classes. Advances in Neural Information Processing Systems, 35: 30583\u201330598, 2022. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 630\u2013645. Springer, 2016. Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. Advances in Neural Information Processing Systems, 35:20703\u201320716, 2022. Louis Kirsch and J\u00fcrgen Schmidhuber. Meta learning backpropagation and improving it. Advances in Neural Information Processing Systems, 34:14122\u201314134, 2021. Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022. Louis Kirsch, James Harrison, Daniel Freeman, Jascha Sohl-Dickstein, and J\u00fcrgen Schmidhuber. Towards general-purpose in-context learning agents. Workshop on Distribution Shifts, 37th Conference on Neural Information . . . , 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022. Alexei Koulakov, Sergey Shuvaev, Divyansha Lachi, and Anthony Zador. Encoding innate ability through a genomic bottleneck. BiorXiv, pages 2021\u201303, 2021. Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning with algorithm distillation. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594. PMLR, 2023. Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. In The Twelfth International Conference on Learning Representations, 2023. Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018.\nSteven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. Popgym: Benchmarking partially observable reinforcement learning. arXiv preprint arXiv:2303.01859, 2023. Elias Najarro and Sebastian Risi. Meta-learning through hebbian plasticity in random networks. Advances in Neural Information Processing Systems, 33:20719\u201320731, 2020. Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, Artem Agarkov, Viacheslav Sinii, and Sergey Kolesnikov. Xland-minigrid: Scalable meta-reinforcement learning environments in jax. arXiv preprint arXiv:2312.12044, 2023. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A, and Schulman J. Training language models to follow instructions with human feedback. Advances in neural information processing systems., 2022. German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural networks, 113:54\u201371, 2019. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Gabriel Barthmaron, Mai Gim\u00e9nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. 2022. St\u00e9phane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661\u2013668. JMLR Workshop and Conference Proceedings, 2010. St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning, pages 1842\u2013 1850. PMLR, 2016. Samuel Schmidgall, Rojin Ziaei, Jascha Achterberg, Louis Kirsch, S Hajiseyedrazi, and Jason Eshraghian. Brain-inspired learning in artificial neural networks: a review. APL Machine Learning, 2(2), 2024. Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. Viacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, and Sergey Kolesnikov. In-context reinforcement learning for variable action spaces. arXiv preprint arXiv:2312.13327, 2023. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024. Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1746\u20131754, 2017. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in neural information processing systems, 34:251\u2013266, 2021.\nAndrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, R Devon Hjelm, and Alexander T Toshev. Large language models as generalizable policies for embodied tasks. In The Twelfth International Conference on Learning Representations, 2023. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1\u201328, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Fan Wang, Hao Tian, Haoyi Xiong, Hua Wu, Jie Fu, Yang Cao, Yu Kang, and Haifeng Wang. Evolving decomposed plasticity rules for information-bottlenecked meta-learning. Transactions on Machine Learning Research, 2022. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Realworld perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9068\u20139079, 2018. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020. Anthony M Zador. A critique of pure learning and what artificial neural networks can learn from animal brains. Nature communications, 10(1):3770, 2019. Ilya Zisman, Vladislav Kurenkov, Alexander Nikulin, Viacheslav Sinii, and Sergey Kolesnikov. Emergence of in-context reinforcement learning from noise distillation. arXiv preprint arXiv:2312.12275, 2023.\n# A Appendix\n# A.1 Model Description\n<div style=\"text-align: center;\">Table 1: Comprehensive configurations of the baseline models</div>\nModel\nparameters\nlayers\ndmodel\nnheads\nbatch size\n(tokens)\nMeta-Language Model (tiny)\n303K\n6\n64\n4\n32K\nMeta-Language Model (small)\n9.5M\n12\n256\n8\n32K\nMeta-Langauge Model (standard)\n151M\n12\n1,024\n16\n64K\nCausal Transformer (small)\n26M\n8\n512\n8\n32K\nCausal Transformer (standard)\n237M\n12\n1,280\n16\n32K\nThe comprehensive configurations of the baseline models are outlined in Table 1. Notably, within the Maze World framework, each temporal increment is encoded by a pair of tokens: an observation and its corresponding action. For the optimization of the Meta-Language and Maze World baseline models, we\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48d6/48d648c5-c2ed-4e27-b852-8d68ca11acdc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Structures of the encoder and decoder in the Variational Autoencoder</div>\nemployed the Noam decay scheduler (Vaswani et al., 2017). This scheduler peaks at a learning rate of 10\u22123 following an initial warm-up phase spanning 1, 000 steps. For the image encoder (11M parameters) and image decoder (14M parameters), we employ convolutional and deconvolutional layers augmented with ResNet blocks (He et al., 2016). The detailed structure of the VAE is depicted in Figure 12. Initially, we train the VAE in isolation by setting \u03b1wm and \u03b1pm in Equation 8 to 0. Subsequently, we freeze the parameters of the encoder and decoder and train the causal transformer only. For partial-context and context-free causal transformers, we modify the attention window to 2 and 1, respectively, as shown in Figure 13. Since transformers can propagate information between layers, an attention window of 2 results in an effective context size of the number of layers plus 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8a65/8a659398-605a-4a6b-8f1d-483d3f442572.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Variant attention masks of causal modeling</div>\nFigure 14 presents the training loss versus the number of iterations for training the baseline in section 2 and A.5. During training baselines for Meta-Language tasks, we observed that standard-sized models converge relatively slowly on the training set. To address this, we conducted additional warm-up epochs on a simpler pre-training dataset with n = 2, 3 before transitioning to the main pre-training dataset. During training the baselines for Maze World tasks, we found there was mutual interference between the world modeling loss (Lwm) and policy modeling loss (Lpm), especially in the initial stages. Our meta-training underwent several\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f86/1f86fc59-c0f8-4591-8d8e-209d0eda401d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Lwm (partial-context))</div>\nFigure 14: The training loss with respect to the steps of optimization in training with procedural generated tasks and sequences. Notice that the partial-context and context-free causal transformers are initialized from the trained full-context transformers.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c526/c5260fee-49ed-4367-ae4a-46f4ec2affbf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: The training loss with respect to the steps of optimization in training with sequences generat from pre-selected tasks.</div>\n<div style=\"text-align: center;\">ure 15: The training loss with respect to the steps of optimization in training with sequences generated m pre-selected tasks.</div>\nphases where we continuously adjusted the loss weights \u03b1wm and \u03b1pm in Equation 8. To further accelerate and stabilize the convergence of the world model, in addition to these losses, we introduced an auxiliary loss Lz = \ufffd t ||\u02c6zt \u2212zt||2 with a weight of \u03b1z = 0.05. During the final stage, the hyperparameters are adjusted to \u03b1pm = 0.25 and \u03b1wm = 0.70. However, we recommend starting with \u03b1wm > 0.90 and \u03b1pm < 0.10 as the policy model training loss is more volatile initially. The context-free and partial-context causal transformers are initialized from the full-context causal transformer, and the hyperparameters remain unchanged. Figure 15 presents the training loss versus the number of iterations for the experiments in section 4. Both group are experimented on small-sized models.\n<div style=\"text-align: center;\">e) Lpm (partial-context)</div>\n<div style=\"text-align: center;\">(c) Lpm (Small-sized)</div>\n# A.2 Data Collection Strategy for Maze World\nFor dataset collection and static evaluation of Maze World, inspired by dataset aggregation (Ross et al., 2011) and noise distillation (Zisman et al., 2023), we utilize distinct behavior and reference policies. The behavior policy is determined by previleged agent where p(STM \u2192LTM) randomly sampled between [0, 50%]. Additionally, with the probability of \u03f5 \u2208[0, 80%] the behavior action is determined by random decision. With the states generated by behavior policy, we use the reference policy with p(STM \u2192LTM) = 100% to label the action. We\u2019ve also randomly dropped 20% of the input observations (zt) to enhance the robustness of auto-regression. Among these, 10% of the input observations are replaced with a special mask token with trainable embeddings, while the remaining 10% are perturbed with Gaussian noise N(0, 1).\n# A.3 Additional Results in Meta-Language Model\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/18a3/18a3bb95-5ef4-486a-8ea5-ae6c43ab9b1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) English vocabulary learning task.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ee1/0ee1c927-1448-4484-9c97-940853046da0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Mathematical operation task.</div>\nFigure 16: Input contexts were designed such that 15% contained errors. All mistakes are highlighted with red squares. The vocabulary for both tasks was limited to 32 pre-defined tokens, and there was no pretraining or further tuning applied to either task. Notice that this setup tests the MetaLM\u2019s ability to adapt and correct based on in-context learning alone, without relying on prior knowledge or adjustments. The model under evaluation has not been trained on any natural langauge corpora. Additionally, the vocabulary is randomly assigned to the predefined 32 tokens prior to inference.\nWe test the ability of generation of the meta-language model on two rudimentary real-world language tasks: English vocabulary learning and basic mathematical operations (addition and subtraction), which are typical for primary school education. For the English vocabulary task, the vocabulary consists of alphabet characters and common punctuation marks; for the math task, it includes digits and mathematical operators. The input contexts for testing included 40 English words and simple arithmetic operations below 5. To discourage direct replication, we intentionally introduced errors into 15% of the words or equations. We analyzed the outputs of the MetaLM across two different context lengths, with the shorter contexts comprising only the first half of the longer contexts. The results, depicted in Fig. 16, show that MetaLM is capable of learning to memorize and correct both English vocabulary and mathematical equations, validating that its ICL capability is independent of vocabulary settings. Furthermore, the longer the context provided, the fewer errors appeared in the outputs, indicating a clear trend of improvement.\nA.4 Additional Results in Maze World Tasks\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ef8f/ef8f4140-f2db-4d06-b4f9-37c0578db83c.png\" style=\"width: 50%;\"></div>\nFigure 17: Performances of different methods in 3 randomly picked 35\u00d735 tasks. Green trajectories indicate time steps t close to 0, while red trajectories represent t close to T. From left to right, the columns are generated by: (a) Random Policy (b) Context-free Model (c) Partial-context Model (d) Full-context Model (Small-sized) (c) Previleged agent with p(STM \u2192LTM) = 100%.\nIn Figure 17, We display trajectories generated by different policies on three randomly selected 35 \u00d7 35 tasks. Interestingly, we find that the main bottleneck of performance might arise from a lack of efficient exploration. Models with less context tend to explore less area. For instance, the random policy can only traverse a very small part of the maze. This observation could also explain the extraordinary challenge posed by the tasks: Traditional randomized exploration, a common technique in reinforcement learning, proves inadequate because it often revisits the same areas repeatedly without retaining information about which parts have been explored, thereby failing to cover broader regions effectively. Therefore, the agent must not only learn how to memorize paths and navigate but also how to explore more effectively, generating high-quality \u201ctraining data\u201d within its context to improve its future performance.\n(e) Privileged Agent\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/52a1/52a12322-21a2-4924-a5dd-219ec1a3c7dd.png\" style=\"width: 50%;\"></div>\nFigure 18: Cases regarding the in-context improvement of world modeling (with 15 \u00d7 15 mazes, small-sized full-context causal transformer). Each clip predicts 9 steps into the future, conditioned on a fixed sequence of actions, and based on varying lengths of context denoted by t. Each clip consists of two sections: the upper section displays the ground truth observations from the current step to 9 steps ahead (from left to right), while the bottom section shows the predicted future states up to 9 steps ahead.\nIn Figure 18, we illustrate a qualitative evaluation of the full-context causal transformer\u2019s world modeling capabilities. Across the majority of instances investigated, a noticeable enhancement in future predicting is evident as the context length grows, suggesting that the model progressively adapts to the intricacies of the maze. This observation indicates the promising applicability of GPICL to the realm of adaptive embodied intelligence.\n# A.5 Additional Diversity in the Maze World\nIt is worth noting that Maze World is a lightweight and fast engine with an easy-to-use Python API based on GYM (Brockman et al., 2016) for simulating navigation in various mazes. It runs smoothly at over 20 frames per second on a standard CPU. In addition to the aforementioned details, Maze World incorporates various configurations and hyper-parameters that can be adjusted to create a wide range of tasks. For example, it offers different types of observation spaces and action spaces, including discrete and continuous actions, as well as 2D and 3D observations (Figure 19a). Beyond navigation tasks, we introduce more challenging survival tasks, where the rewards provided by the PNTs are static within an episode but initially unknown (can be negative, Figure 19b). The basic environments can also be modified in terms of the size of the basic cell (Figure 19c,19d), the height of the ceiling (Figure 19e,19f), the density of obstacles (Figure 19g,19h), and the range of view (Figure 19i,19j).\n# A.6 Details of Task Configurations\nIn Table 2, we list the detailed settings for the tasks used in this paper. The rewards for reaching the target are by default related to the size of the maze. Additionally, we note that the density of obstacles controls the number of loops in the environments. We recommend using a density between 0.15 and 0.40. A density over 0.50 results in mazes without any loops, significantly reducing the difficulty of traversing the maze (or performing exploration), as an agent can easily traverse the entire world by always going right (or left). We believe this might be a crucial reason why, in Morad et al. (2023), where the agent is only required to navigate to a single target, context-free models can outperform memory-augmented models.\n<div style=\"text-align: center;\">Table 2: Setting of task configurations used in this paper</div>\nMeta-Langauge\nEmbedding size of the randomized langauge generator\n32\nHidden size of the randomized language generator\n64\nNormalize factor for softmax of the randomized language generator (\u03bb)\n5\nNumber of the hidden parameters (|\u03b8|, n = 2)\n7K\nNumber of the hidden parameters (|\u03b8|, n = 4)\n11K\nNumber of the hidden parameters (|\u03b8|, n = 8)\n19K\nMaze Tasks\nDensity of obstacles\n0.36\nNumber of Potential Navigation Targets (PNTs)\n10\nResolution of observation\n128 \u00d7 128\nRange of view\n12.0 (default)\nSize of basic cell\n2.0 (default)\nRewards of arriving target (15 \u00d7 15)\n0.57 (default)\nRewards of arriving target (25 \u00d7 25)\n1.24 (default)\nRewards of arriving target (35 \u00d7 35)\n2.06 (default)\nHeight of ceiling\n3.2 (default)\nHeight of agents\u2019 view points\n1.6 (default)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/124c/124c3796-d850-40a1-bd3e-c8a594c1eaf8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 19: Additional variants by provided configurations in Maze World.</div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The current state of in-context learning (ICL) capabilities in large language models is limited to natural language-based tasks and lacks the ability to interact with real-world environments. This limitation necessitates the development of benchmarks that can effectively evaluate and enhance general-purpose in-context learning (GPICL).",
            "purpose of benchmark": "The benchmarks are designed to train and evaluate GPICL functionalities across a wide range of tasks, promoting long-horizon in-context learning through continuous generation and interaction."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of enabling models to learn and generalize from diverse tasks without requiring modification of model parameters, focusing instead on the utilization of context and memory.",
            "key obstacle": "Existing benchmarks often lack the diversity and complexity needed to effectively evaluate ICL, leading to models that may excel in zero-shot performance but fail to generalize across varied tasks."
        },
        "idea": {
            "intuition": "The development of GPICL is inspired by the need for models to learn from diverse tasks and adapt their behaviors based on context, similar to biological learning processes.",
            "opinion": "The authors believe that the introduction of these benchmarks is crucial for advancing the field of machine learning, particularly in enabling models to achieve better generalization capabilities.",
            "innovation": "The benchmarks differ from previous ones by focusing on a broader range of tasks and allowing for synthetic data generation, thereby enhancing the complexity and variability of the learning scenarios.",
            "benchmark abbreviation": "GPICL"
        },
        "dataset": {
            "source": "The datasets for the benchmarks are synthetically generated, allowing for unlimited data creation tailored to specific tasks.",
            "desc": "The datasets consist of a vast number of sequences and tasks, designed to test models' abilities to learn and generalize from new contexts.",
            "content": "The dataset includes sequences of randomized languages and maze navigation tasks, incorporating features that require models to adapt and learn from their environments.",
            "size": "2,000,000,000",
            "domain": "Language Modeling",
            "task format": "Language Generation"
        },
        "metrics": {
            "metric name": "Perplexity",
            "aspect": "Model performance in terms of learning efficiency and generalization capability.",
            "principle": "Perplexity is chosen as it effectively measures how well a probability distribution predicts a sample, which is crucial for evaluating language models.",
            "procedure": "Models are evaluated based on their ability to minimize perplexity across generated sequences, with comparisons made against baseline performances."
        },
        "experiments": {
            "model": "Baseline models include various sizes of transformers, ranging from tiny to standard-sized models.",
            "procedure": "Models are trained on synthetic datasets generated from the benchmarks, with evaluations conducted on both seen and unseen tasks to assess generalization capabilities.",
            "result": "The experiments demonstrate that models trained with the benchmarks achieve significant improvements in generalization, especially when the diversity of tasks increases.",
            "variability": "Variability in results is accounted for by conducting multiple trials and using different subsets of the generated datasets."
        },
        "conclusion": "The introduction of the GPICL benchmarks shows promising results in enhancing model generalization capabilities and suggests that increasing task diversity can lead to better performance in unseen tasks.",
        "discussion": {
            "advantage": "The benchmarks provide a comprehensive framework for evaluating ICL and GPICL, facilitating advancements in model training and performance.",
            "limitation": "The synthetic data generated may not yet fully capture the complexities of real-world tasks, which could limit the benchmarks' applicability.",
            "future work": "Future research could focus on developing more realistic benchmarks and exploring advanced models and techniques that leverage the insights gained from these benchmarks."
        },
        "other info": {}
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) capabilities in large language models are currently limited to natural language-based tasks and lack the ability to interact with real-world environments."
        },
        {
            "section number": "1.2",
            "key information": "The benchmarks are designed to train and evaluate general-purpose in-context learning (GPICL) functionalities across a wide range of tasks, promoting long-horizon in-context learning through continuous generation and interaction."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark addresses the challenge of enabling models to learn and generalize from diverse tasks without requiring modification of model parameters, focusing instead on the utilization of context and memory."
        },
        {
            "section number": "3.3",
            "key information": "The benchmarks differ from previous ones by focusing on a broader range of tasks and allowing for synthetic data generation, thereby enhancing the complexity and variability of the learning scenarios."
        },
        {
            "section number": "6.4",
            "key information": "The synthetic data generated may not yet fully capture the complexities of real-world tasks, which could limit the benchmarks' applicability."
        },
        {
            "section number": "7",
            "key information": "The introduction of the GPICL benchmarks shows promising results in enhancing model generalization capabilities and suggests that increasing task diversity can lead to better performance in unseen tasks."
        }
    ],
    "similarity_score": 0.716352020684391,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Benchmarking General-Purpose In-Context Learning.json"
}