{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.00858",
    "title": "Can Large Language Models Understand Context?",
    "abstract": "Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models\u2019 ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.1",
    "bib_name": "zhu2024largelanguagemodelsunderstand",
    "md_text": "# Can Large Language Models Understand Con\n# Yilun Zhu1\u2217, Joel Ruben Antony Moniz2, Shruti Bhargava2, Jiarui Lu2 Dhivya Piraviperumal2, Site Li2, Yuan Zhang2, Hong Yu2, Bo-Hsiang Tseng2 1Department of Linguistics, Georgetown University\nyz565@georgetown.edu {joelrubenantony_moniz, shruti_bhargava, jiarui_lu, dhivyaprp}@apple.com {site_li, yzhang73, hong_yu, bohsiang_tseng}@apple.com\n# Abstract\nUnderstanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models\u2019 ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.1\narXiv:2402.00858v1\n# 1 Introduction\nDiscourse understanding, as one of the fundamental problems in NLP, focuses on modeling linguistic features and structures that go beyond individual sentences (Joty et al., 2019). Understanding discourse requires resolving the relations between words/phrases (coreference resolution) and discourse units (discourse parsing and discourse relation classification) in the previous context, iden-\n\u2217Work performed during an internship at Apple. 1The code is publicly available at https://github.com/ apple/ml-llm-contextualization-eval.\ntifying carry-over information for the following context (dialogue state tracking), and recognizing discourse-specific phenomena (ellipsis). LLMs have garnered substantial attention from both academia and the industry due to their remarkable capability in comprehending language and world knowledge. Their unparalleled performance across a diverse range of benchmarks and datasets has firmly established their significance in a relatively short period of time. As LLMs continue to push the boundaries of scale and capability, the evaluation of their multifaceted abilities becomes an equally vital endeavor. Consequently, the development of robust evaluation methodologies to assess specific aspects of LLMs becomes imperative. In addition, these methodologies should focus on helping achieve a comprehensive understanding of their advancement while clearly delineating their limitations. However, recently published LLMs, such as OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI, 2023), are only evaluated on limited benchmarks, and have a significant drawback: they neglect the inclusion of discourse-related datasets for evaluation, thereby limiting the comprehensive assessment of their language understanding capabilities. To provide a comprehensive evaluation, plenty of benchmarks and datasets address various facets of language understanding, including benchmarks that delve into common sense knowledge (Hendrycks et al., 2021a; Kwiatkowski et al., 2019), as well as linguistic capabilities like sentiment analysis, natural language inference, summarization, text classification, and more (Bang et al., 2023b; Liang et al., 2022). These general benchmarks and specific dataset evaluations exhibit certain limitations. Despite the requirement for contextual information in these benchmarks to effectively tackle tasks (for example, sentiment analysis requires an understanding of polarities within the given text), none of these benchmarks cater to tasks that de-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d66/2d6661c0-ceae-4d45-a25c-6c354c5814db.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Tasks and datasets in the context understanding benchmark.</div>\nmand a nuanced comprehension of linguistic features within a provided context. On the other hand, recent LLMs, by virtue of possessing billions of parameters, have led to an exponential surge in computational and storage costs (Brown et al., 2020b), which hinders the deployment of large models to personal devices and restricts the on-device performance of language understanding tasks. To address this challenge, model compression methods, which can reduce memory and disk requirements of both model training and inference, have gained attention. Existing compression techniques, such as 3-bit quantization (Frantar et al., 2022), have demonstrated the potential to reduce model sizes with only marginal performance trade-offs. However, the evaluation of quantization methods suffers from two deficiencies. Firstly, quantization methods are primarily evaluated on limited benchmarks and datasets, such as Lambada (Paperno et al., 2016), ARC (Boratko et al., 2018), PIQA (Tata and Patel, 2003), BoolQ (Clark et al., 2019), and StoryCloze (Mostafazadeh et al., 2017). It is not yet clear whether large, compressed models out- or under-perform their smaller counterparts when understanding context. Secondly, previous work has not delved into a linguistic analysis to identify where the model efficacy wanes. Given the above shortcomings, this paper evaluates LLMs on a context understanding benchmark constructed from varied discourse understanding datasets. We conduct an extensive analysis of LLM performance on this benchmark, including models of varying sizes and those subjected to compression techniques, aiming to provide a more comprehensive understanding of context understanding\n# capability of the LLMs. The contributions of this paper can be summarized as follows:\ncapability of the LLMs. The contributions of this paper can be summarized as follows:\n\u2022 We evaluate post-training compressed models in ICL settings and conduct an analysis of the reduction in context understanding capability compared to dense models.\n# 2 Related Work\n# 2.1 In-context Learning Evaluation\nThe paradigm of ICL (Brown et al., 2020a) is rapidly gaining importance. Studies have demonstrated that the generalization of LLMs to various downstream NLP tasks, such as MMLU (Hendrycks et al., 2021b), is significantly enhanced when provided with a small number of examples as prompts (Brown et al., 2020a; Chowdhery et al., 2022; Hoffmann et al., 2022; Rae et al., 2022; Anil et al., 2023; Touvron et al., 2023; OpenAI, 2022, 2023). Recent research has extensively evaluated the performance of LLMs across a spectrum of language-related tasks, spanning from text generation to understanding input sequences. This assessment contains a wide array of benchmarks, including SUPER-GLUE (Wang et al., 2019; Laskar et al.,\n2023), and tasks such as question answering, information retrieval, sentiment analysis (Bang et al., 2023b; Liang et al., 2022), dialogue (Heck et al., 2023), and text classification (Yang and Menczer, 2023).\n# 2.2 Model Compression for LLMs\nModel compression techniques can be broadly categorized into three main approaches: compression during training, compression associated with finetuning, and post-training methods. In terms of quantization during training, this technique enables LLMs to adapt to low-precision representations during the training process (Liu et al., 2023). Model compression with fine-tuning involves quantization awareness into the fine-tuning stage (Kim et al., 2023; Dettmers et al., 2023). Post-training techniques, on the other hand, are applied after the completion of an LLMs training phase and typically involve the use of calibration data. This category comprises two primary approaches: pruning, which removes redundant or non-salient weights to induce sparsity (Frantar and Alistarh, 2023), and quantization, which employs low-precision numeric representations of weights and activations (Nagel et al., 2020; Frantar et al., 2022; Yuan et al., 2023). Prior research shows that quantization outperforms pruning in several settings (Kuzmin et al., 2023), thus in this work, we focus on model quantization and its impact on the selected context-aware tasks.\n# 3 Task Selection & Design\nOur contextual understanding benchmark includes four tasks with nine datasets, as presented in Figure 1. In the following sections, we provide detailed explanations of each task and the corresponding datasets, along with the designed prompts for ICL evaluations.\n# 3.1 Coreference Resolution\nThe coreference resolution (CR) task contributes to achieving a coherent understanding of the overall meaning conveyed within the text. Thus, it plays a critical role in diving into language models\u2019 capability to grasp coreference relations as well as contextual nuances within documents. We select two coreference datasets: WSC273 (Levesque et al., 2012) and OntoNotes 5.0 (Pradhan et al., 2013). WSC273, which contains the first 273 examples from the Winograd Schema Challenge, is a dataset that requires the system to read a sentence with\nInstruction: Please carefully read the following passages. For each passage and the options, you must identify which option the mention marked in *bold* refers to. If the marked mention does not have any antecedent, please select \u201cno antecedent\u201d. Context: ... To express *its* determination ... the Chinese securities regulatory department ... this stock reform ... Choices: A. no antecedent B. the Chinese securities regulatory department C. this stock reform\nTable 1: An OntoNotes example of prompt and answer.\nan ambiguous pronoun and select the referent of that pronoun from two choices. OntoNotes is a human-annotated corpus of documents annotated with multiple layers of linguistic information including syntax, propositions, named entities, word sense, and in-document coreference. As it is one of the most frequently used datasets for training coreference models, prior research has achieved significant advancements under the supervised finetuning paradigm (Lee et al., 2017; Joshi et al., 2020; Bohnet et al., 2023). However, these model designs cannot be extended to generative models under ICL settings. Recently, Le and Ritter (2023) have leveraged document templates for LLMs; however, their evaluation is confined to prominent models such as InstructGPT (Ouyang et al., 2022), neglecting the fact that smaller models lack the generative capacity required to accomplish such tasks. Due to these limitations, we propose a novel multiple-choice task design. In this design, we provide the mentions and evaluate the model on resolution. Each option represents a potentially markable span.2 Table 1 presents an example of the input to the model3. The entire prompt consists of five parts: (1) an instruction that provides guidance to the model for the task, (2) a document containing plain text with a selected mention span highlighted using a bold symbol, (3) a list of choices, which includes all the gold mentions present in the document, (4) a question that directs the model\u2019s attention, and (5) a guiding word answer that prompts for the output. We experiment with multiple instructions and prompts and provide the one with the best performance. Linking scores are computed for each ques-\n2Considering the inferior performance of small models on the mention detection task, we utilize gold markable spans coreference linking. 3Detailed examples for each task design can be found in Appendix A.\nOntology: {\u201cslots\u201d: {\u201crestaurant-pricerange\u201d: \u201cprice budget for the restaurant\u201d, ... }, \u201ccategorical\u201d: {\u201crestaurant-pricerange\u201d: [\u2018cheap\u2019, \u2018expensive\u2019, \u2018moderate\u2019], ...} } Instruction: Now consider the following dialogue between two parties called the \u201csystem\u201d and \u201cuser\u201d. Can you tell me which of the \u201cslot\u201d was updated by the \u201cuser\u201d in its latest response to the \u201csystem\u201d? Present the updates in JSON format. If no \u201cslots\u201d were updates, return an empty JSON list. If you encounter \u201cslot\u201d that was requested by the \u201cuser\u201d then fill them with \u201c?\u201d. If a user does not seem to care about a discussed \u201cslot\u201d fill it with \u201cdontcare\u201d. [Previous Dialogue State] [Conversation]: \u201csystem\u201d: \u201c\u201d \u201cuser\u201d: \u201cI\u2019m looking for a moderately priced place to eat that\u2019s in the centre of town.\u201d Output: {\u201crestaurant-pricerange\u201d: \u201cmoderate\u201d, \u201crestaurant-area\u201d: \u201ccentre\u201d}\nTable 2: A DST example of prompt and answer\n<div style=\"text-align: center;\">Table 2: A DST example of prompt and answer.</div>\ntion and the results are subsequently aggregated for evaluation. We utilize the official evaluation metrics from the CoNLL-2012 shared task (Pradhan et al., 2012), which employs the CoNLL F1 score, derived from the averaging of three coreference metrics: MUC, B3, and CEAF\u03d54.\n# 3.2 Dialogue State Tracking\nDialogue state tracking (DST) is an important task in the area of task-oriented dialogue (TOD) modeling (Young et al., 2013), where the dialogue agent tracks the key information provided by the user as the conversation progresses. Table 2 provides an example from MultiWOZ (Budzianowski et al., 2018) where the user expresses the constraints when looking for a restaurant. The output of DST is typically maintained in slot-value pair format. Previous research has explored ICL capabilities on MultiWOZ and demonstrated promising results compared to fine-tuning models (Hu et al., 2022; Heck et al., 2023). However, these studies either involve partial training or are untested with smaller and quantized models. Here we adopt a straightforward and simplified ICL approach proposed by Heck et al. (2023), and test it on MultiWOZ v2.2 (Zang et al., 2020). The prompt to the model consists of domain knowledge from ontology, an instruction, previous dialogue state (the belief state accumulated until the previous user turn) and the conversation proceeding to the current turn. The ontology could be lengthy if considering all domains in the dataset. Thus, given the input length constraint of LLMs, only the knowledge relevant to the conversation is provided. Following literature,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c4ba/c4ba9343-a4da-4fad-80f4-a1bfdec28453.png\" style=\"width: 50%;\"></div>\nTable 3: A PDTB example of prompt and answer.\n# we report joint goal accuracy (JGA) (Mrk\u0161i\u00b4c et al., 2017) for evaluating the performance of DST.\nwe report joint goal accuracy (JGA) (Mrk\u0161i\u00b4c et al., 2017) for evaluating the performance of DST.\n# 3.3 Implicit Discourse Relation Classification\nDiscourse demonstrates its importance beyond individual sentences, which emphasizes the ways in which different segments of a text interconnect and structure themselves to convey a coherent and meaningful message. The PDTB-3 corpus, as introduced by Webber et al. (2019), annotates implicit discourse relations across elementary discourse units (EDUs)4. These relations imply connections between EDUs and may be made explicit by inserting a connective. Within the context of the understanding benchmark, we opt for the implicit discourse relation classification task for two primary reasons. Firstly, the order of the two EDUs is provided, enabling the model to directly utilize this information. Secondly, the connective triggering the relation is implicit, increasing the task\u2019s complexity. In this task, two EDUs are fed as input, and the objective is to correctly identify the relation between them. Due to the nuanced differences between each relation and the demand for annotators with rich linguistic knowledge and extensive annotation training, the classification task poses challenges to fine-tuned classification models. The PDTB3 corpus classifies discourse relations into four categories - Temporal, Contingency, Comparison, and Expansion. We convert this task into a multiple-choice question and experiment with classes as options. In the classes scenario, the task offers four options, each representing a distinct discourse relation class. Table 3 exhibits the components of the prompt. It includes an instruction at the beginning, followed by a concise description of each relation, a context with two arguments, a\n4EDU refers to the smallest segment of a text that conveys a complete and coherent meaning within larger discourse.\nInstruction: Rewrite the last query following interaction into a well-formed, context independent query. Resolve any disfluencies or grammatical errors in the query. Input: User: Try to reach Forbes now . Bot: Forbes at Washington Post ? Or Forbes of Publishing Division ? User: Publishing Division . Rewrite: Forbes of Publishing Division\nTable 4: A query rewriting example of prompt and answer.\nquestion along with answer choices, and a trigger word. We evaluate each model\u2019s performance on this dataset using accuracy as the metric.\n# 3.4 Query Rewriting\nWhile document-based CR (OntoNotes, Section 3.1) covers various types of coreference relations across multiple genres, it does not allow the ability to evaluate certain aspects which are important to understand context. Firstly, the CR task typically focuses on document-based coreference chains, neglecting mention resolution in dialogues. Secondly, ellipsis, which is the omission of one or more words from a clause, is a crucial linguistic phenomenon in speech and conversation. It is essential for language models to grasp and accurately identify ellipses within context. Incorporating these features into the benchmark is thus pivotal when evaluating context understanding. Query Rewriting (QR) is a task of rewriting the last utterance of a user in a conversation into a context-free, independent utterance that can be interpreted without dialog context. It requires the model to identify the entity or events references from context and further generate a complete utterance with resolved coreference or ellipsis. We incorporate five QR datasets in the proposed benchmark: MuDoCo with QR annotations (Martin et al., 2020; Tseng et al., 2021), QReCC (Anantha et al., 2021), InCar (Regan et al., 2019), GECOR (Quan et al., 2019), and CANARD (Elgohary et al., 2019). These datasets span multiple genres and domains in dialogues. We experiment with various prompts used for fine-tuning models and present the results with the best selections. Table 4 presents a concise prompt comprising an instruction along with context for each dialogue. To assess the quality of generated queries, we follow the metrics from previous research, particularly BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004).\n# 4 Experiments\n# 4.1 Implementation Details\nEvaluation was conducted on a computational infrastructure comprising 8 \u00d7 A100 GPUs. We experiment with three model families. For smaller models, we consider OPT (Zhang et al., 2022), ranging from 125M to 2.7B. Although OPT also offers larger models, we opt for LLaMA (Touvron et al., 2023) as the mid-sized LMs, spanning from 7B to 65B parameters, due to showcased superior performance by prior works. For large-scale LMs, we leverage GPT-3.5-turbo5. For each model, on every dataset, we assess five different settings: zero-shot, one-shot, 5-shot, 8-shot, and 10-shot. We randomly select the examples from the training set for the few-shot prompting.6\n# 4.2 Dense Model\nResults of the three model families are reported in Table 5, along with results of fine-tuned (FT) models to help better interpret how well the pretrained models behave with ICL. Figure 2 also visualizes the gap between various commercial/noncommercial language models and fine-tuning models that achieve the best performance on these tasks. For each, we present the N-shot setting that yields the highest score (see Appendix B for details). Overall, performance improves as the model size increases and pre-trained models with ICL struggle to catch up with FT models on most tasks.\nResults of the three model families are reported in Table 5, along with results of fine-tuned (FT) models to help better interpret how well the pretrained models behave with ICL. Figure 2 also visualizes the gap between various commercial/noncommercial language models and fine-tuning models that achieve the best performance on these tasks. For each, we present the N-shot setting that yields the highest score (see Appendix B for details). Overall, performance improves as the model size increases and pre-trained models with ICL struggle to catch up with FT models on most tasks. Coreference Resolution Larger models exhibit promising performance on the WSC273 task, indicating that LLMs can effectively handle \"simple\" coreference relations within limited contexts and mentions. However, when it comes to documentbased CR with complex clusters, their performance substantially drops 7. Even on providing groundtruth mentions, the highest-performing GPT is only on par with rule-based coreference systems (Manning et al., 2014) and is far from the end-to-end fine-tuned SpanBERT (Joshi et al., 2020). The gap\n5https://platform.openai.com/docs/models/ gpt-3-5 6WSC273 itself is a test set and thus has no fine-tuning results. We only report the zero-shot results. 7Note that the OntoNotes dataset is substantially larger than the others. We observe that inference on the entire test set becomes extremely time-consuming, particularly with the larger models; further, the cost of running inference on GPT3.5 starts becoming non-negligible. Consequently, we propose limiting the OntoNotes test set to a 10% sub-sample, which is the setting we consistently adopt.\nTask\nDataset\nMetrics\nOPT\nLLaMA\nGPT\nFT\n125M\n350M\n1.3B\n2.7B\n7B\n13B\n30B\n3.5-turbo\nCR\nWSC273\nAcc\n58.24\n66.67\n76.19\n77.66\n86.81\n89.38\n89.01\n88.64\nN/A\nOntoNotes\nMUC\n12.66\n7.58\n13.21\n8.29\n10.31\n31.80\n33.56\n56.32\n77.26\nB3\n53.80\n52.26\n53.54\n52.41\n52.20\n58.43\n58.66\n68.20\n73.43\nCEAF\u03d54\n31.09\n29.49\n31.40\n30.10\n32.63\n38.00\n39.27\n50.72\n74.46\nAvg. F1\n32.52\n29.78\n32.72\n30.27\n31.71\n42.74\n43.83\n58.41\n76.03\nDST\nMultiWOZ\nJGA\n11.11\n27.96\n26.61\n28.08\n32.30\n28.12\n42.24\n57.40\n63.79\nDisc.\nPDTB-3\nAcc\n10.04\n10.04\n10.04\n16.15\n17.16\n26.01\n39.77\n43.83\n76.23\nQR\nMuDoCo\nBLEU\n0.46\n0.36\n7.02\n49.20\n41.12\n61.15\n66.51\n57.14\n80.31\nROUGE\n1.52\n12.18\n10.98\n65.61\n56.07\n74.78\n77.88\n79.37\n92.01\nQReCC\nBLEU\n4.53\n31.27\n26.35\n40.09\n28.19\n38.64\n58.68\n55.24\n58.67\nROUGE\n13.91\n58.18\n53.10\n68.32\n48.27\n56.40\n78.74\n79.98\n81.75\nInCar\nBLEU\n0.00\n7.66\n12.71\n27.42\n28.20\n42.13\n48.58\n63.66\n88.45\nROUGE\n3.41\n28.76\n30.45\n49.63\n49.96\n56.73\n64.18\n83.51\n95.24\nGECOR\nBLEU\n0.20\n26.40\n26.32\n49.99\n53.27\n66.30\n73.80\n63.34\n82.56\nROUGE\n4.06\n42.13\n42.57\n65.89\n69.23\n80.99\n86.03\n79.00\n92.63\nCANARD\nBLEU\n2.61\n19.39\n24.24\n34.66\n21.34\n29.32\n47.24\n47.12\n57.46\nROUGE\n9.82\n45.63\n49.36\n62.73\n38.17\n46.61\n69.73\n74.61\n81.06\nTable 5: Few-shot results of two open-sourced models and GPT-3.5 on the context understanding benchmark. The results with the best number of few-shot examples are reported for each task. Fine-tuning (FT) results serves as a reference when evaluating LLMs\u2019 capability under ICL setup.\nbetween ICL and FT results highlights that under the ICL setting, LLMs struggle to build coreference chains without adequate domain-specific examples. Specifically, models except GPT perform significantly worse on the MUC metric. Error analysis reveals that these models are inclined to create more clusters, including singleton clusters. This implies that pre-trained LLMs encounter difficulties in understanding long-range contextual information.\nDST A similar trend is observed as CR where OPT and LLaMA models fall behind GPT-3.5 significantly. This suggests that these models fail to extract key information as the conversation proceeds, even with the provision of 5 to 10 demonstrations and the distilled relevant domain ontology in prompt. Our error analysis indicates that most of the errors happen due to the misdetection of slots or the wrong predicted value in a slot-value pair. Only GPT-3.5 reaches the level of FT results which is a fine-tuned T5 base model (Bang et al., 2023a).\nobserve an increase in scores when the model size exceeds 7B. However, even the best-performing LLM, GPT, performs worse than the SOTA finetuned model (Liu and Strube, 2023) with the drop of 32% accuracy. We carefully examine the predictions for each model and found that all models tend to predict the same relation class for every example, albeit with their individual preferences\nfor the selected relation. In addition, because of an imbalanced distribution of classes, these models potentially perform worse than random chance (25%). This suggests that the models struggle to distinguish the nuances between different relation classes and fail to correctly identify relations across EDUs within context.\nQuery Rewriting The gap between small and large models is significantly huge, compared to the other tasks. For instance, OPT-125M cannot even complete the rewriting task. Analysis on predictions of small models indicates that the model is not capable of following the instructions or learning patterns from the few-shot examples. We identify a few major error types: (1) generating the next sentence, instead of rewriting; (2) rewriting the wrong user turn from the conversation; (3) copying the last user utterance without any rewriting. These errors get reduced as the model size increases. However, similar to the previous three tasks, the best ICL results achieved by GPT is far from the fine-tuned models.8 It is worth noting that OPT-2.7B performs on par or notably better than LLaMA-7B, which is somewhat not aligned with the findings in Beeching et al. (2023) where LLaMA-7B even outperforms OPT-66B in many tasks, including ARC (Clark\n8In literature, the best FT results come from different models across five QR datasets, where some are not even LLM based. To ensure fair comparison, we fine-tuned a T5 large model on each QR dataset.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be98/be988d3d-6a20-411b-93d3-70b4f4a2879b.png\" style=\"width: 50%;\"></div>\nFigure 2: Comparison between commercial/noncommercial models and fine-tuning models for each task in the context understanding benchmark.\n# et al., 2018), HellaSwag (Zellers et al., 2019), and MMLU (Hendrycks et al., 2021b).\nAll in all, this section presents a holistic comparison of LLMs\u2019 behaviors on the target context understanding tasks. On the tasks with structured outputs such as CR or DST, even small models show a certain level of context understanding and seem to follow the task instruction. Classification tasks such as discourse relation selection are deemed the easiest among all tasks; however, the small models are even worse than a random guess (25%). As for the generative task, the ability to complete query rewriting can be only observed in the case of larger models, as the model has the freedom to generate arbitrary content that does not follow the prompt. We notice that OPT-2.7B outperforms LLaMA-7B in multiple QR datasets, including MuDoCo, QReCC, and CANARD. We carefully compare the outputs between the two models. As an example, QReCC, a QA-based conversational dataset, consists of several QA pairs as context and a last query to be rewritten. We observe that LLaMA7B tends to rewrite the question in context instead of rewriting the last target query, which is not frequent in OPT-2.7B. It is also noted that except for DST, FT models demonstrate marked superiority over pre-trained models, highlighting the potential for improving LLMs\u2019 competence on these context understanding tasks.\nDataset\nMetrics\n7B-D\n30B-Q\n30B-D\nWSC273\nAcc\n86.81\n87.18\n89.01\nOntoNotes\nMUC\n10.31\n25.37\n33.56\nB3\n52.20\n56.80\n58.66\nCEAF\u03d54\n32.63\n36.93\n39.27\nAvg. F1\n31.71\n39.70\n43.83\nMultiWOZ\nJGA\n32.30\n41.99\n42.24\nPDTB-3\nAcc\n17.16\n31.29\n39.77\nMuDoCo\nBLEU\n41.12\n59.22\n66.51\nROUGE\n56.07\n71.38\n77.88\nQReCC\nBLEU\n28.19\n53.72\n58.68\nROUGE\n48.27\n74.13\n78.74\nInCar\nBLEU\n28.20\n39.69\n48.58\nROUGE\n49.96\n56.32\n64.18\nGECOR\nBLEU\n53.27\n70.41\n83.36\nROUGE\n69.23\n73.80\n86.03\nCANARD\nBLEU\n21.34\n45.07\n47.24\nROUGE\n38.17\n67.15\n69.73\nTable 6: Comparison between dense and quantized models. Dense LLaMA-7B and 3-bit quantized LLaMA30B share similar memory and disk requirements. D represents dense model and Q denotes quantized model.\n# 4.3 Model Compression Technique\nAs we focus on evaluating context understanding of LLMs in an ICL setup, we evaluate models quantized using GPTQ (Frantar et al., 2022), which is an efficient one-shot weight quantization algorithm based on approximate second-order information that compresses the model post-training. It enables a reduction in memory and disk requirements by up to 80%, compared to the pre-quantized model.\n# 4.4 Quantized Model Results\nGPTQ (Frantar et al., 2022) has been shown to effectively reduce the model size to 3 bits without incurring substantial performance losses across a range of NLP tasks, such as MMLU, ARC, StoryCloze. However, whether this performance preservation can be extended to contextual understanding was unclear. Table 6 presents the comparison between the dense and 3-bit quantized LLaMA models. In contrast to previous studies on 3-bit quantization, we observed that quantization leads to fluctuated drops in performance across the four tasks. Specifically, in WSC273, MultiWOZ, and CANARD, post-training quantization incurs only a marginal performance drop (\u223c1.7 points). However, in the remaining datasets, quantization results in significant performance drops. The results further show that the quantized LLaMA-30B model consistently outperforms the\ndense LLaMA-7B model across all tasks despite being comparable in disk and memory requirements. For CR, the 30B quantized model achieves significantly higher scores on the OntoNotes dataset across all metrics. The MUC metric shows the most substantial improvement, indicating that the quantized 30B model partially overcomes the tendency to create small clusters for mentions. For DST on MultiWOZ, the 30B quantized model show a 30% relative improvement over the 7B model in JGA. On discourse parsing with PDTB-3, the accuracy of quantized 30B model is almost double, 17.16% vs 31.29%. Across all QR datasets, the quantized 30B model substantially improves NLG scores compared to the dense 7B model, with relative gains ranging from 15-50%. The largest gap is observed on GECOR. In general, we show that the quantized 30B LLaMA model consistently and significantly outperforms the dense 7B model as a result of the increased scale, despite using 3-bit quantization. The benefits of greater model scale thus outweigh the impacts of quantization in understanding discourse. We believe this finding would be beneficial when deploying LLMs in real-world applications with disk and runtime constraints.\n# 5 Case Study: Query Rewriting\nIn this section, we provide in-depth analysis by comparing the two open-sourced model families OPT and LLaMA, and the impact of quantization, using query rewriting as the target task. We conduct a careful inspection of the query rewriting task because of three reasons: (1) by the nature of the task, query rewriting is the only one with free-form generation, while the others effectively are either classification-based tasks or heavily constrained in their possible output predictions. The generation task allows us to explore the LLMs\u2019 output in more detail, and to provide more interesting insights; (2) the manual analysis of errors is a time-consuming process, making it challenging to conduct such an in-depth analysis across all four tasks; (3) the query rewriting task covers a diverse range of five datasets, enabling us to compare differences between each dataset and to thereby gain a deeper understanding.\n# 5.1 OPT vs. LLaMA\nPrior works (Beeching et al., 2023) have consistently shown that, under the same model size,\nDataset\n6.7/7B\n13B\n30B\nO.\nL.\nO.\nL.\nO.\nL.\nMudoco\n53.1\n41.1\n55.2\n61.1\n55.2\n66.5\n71.8\n56.0\n72.1\n74.7\n71.5\n77.8\nQReCC\n46.6\n28.1\n43.7\n38.6\n43.8\n58.6\n73.4\n48.2\n71.6\n56.4\n71.9\n78.7\nInCar\n40.3\n28.2\n41.9\n42.1\n44.6\n48.5\n64.8\n49.9\n62.6\n56.7\n65.3\n64.1\nGECOR\n58.8\n53.2\n60.9\n66.3\n58.2\n73.8\n75.7\n69.2\n78.3\n80.9\n76.1\n86.0\nCANARD\n43.8\n21.3\n37.5\n29.3\n41.3\n47.2\n72.0\n38.1\n66.0\n46.6\n69.3\n69.7\nTable 7: Comparison between OPT (O.) and LLaMA (L.) across five query rewrite datasets. For each dataset, the first and second rows represent BLEU and ROUGE scores respectively.\nContext\nUser: what is the name of india pakistan border line\nBot: The Radcliffe Line was the boundary demarcation\nline between the Indian and Pakistani portions of the\nPunjab and Bengal provinces of British India.\nUser: who created the radcliffe line\nBot: The Radcliffe Line was named after its architect, Sir\nCyril Radcliffe, who was the joint chairman of the two\nboundary commissions for the two provinces.\nUser: when was the line published\nGold answer: when was the radcliffe line published\nPrediction 1 (repeat the last query): when was the line\npublished\nPrediction 2 (language modeling): 1947\nTable 8: An example of two major types of errors found in the query rewriting task.\nLLaMA outperforms OPT. However, their performance on QR, as shown in Table 7, does not follow this pattern. When the model size is around 7B, OPT consistently performs better than LLaMA by a significant margin across the five QR datasets. The two models perform on par with each other at 13B. The superiority of LLaMA is only obvious with 30B model size. From another perspective, although we expect performance to improve as model size increases, we observe this trend on LLaMA, but not on OPT. These results suggest that it may not be correct to conclude the overall superiority between two model families by only comparing on a certain range of model sizes or on a certain set of tasks.\n# 5.2 Dense vs. Quantized\nWe conduct a quantitative analysis on the error types of query rewriting to investigate the performance gap between dense and quantized models.\nType\nDataset\n7B D\n30B Q\n30B D\nRepeat\nMuDoCo\n260\n247\n194\nQReCC\n86\n90\n26\nInCar\n17\n15\n8\nGECOR\n59\n62\n37\nCANARD\n47\n44\n32\nTotal\n469\n458\n297\nLM\nMuDoCo\n71\n29\n16\nQReCC\n80\n28\n16\nInCar\n19\n20\n15\nGECOR\n6\n1\n0\nCANARD\n127\n76\n59\nTotal\n232\n125\n106\nTable 9: Number of the major two types errors on three LLaMA models (7B dense, 30B quantized, and 30B dense) found in query rewriting. Repeat stands for repeat-the-last-query error and LM denotes language modeling error.\nAcross the five datasets, we identify two main error types that account for nearly 80% of the total errors, with examples shown in Table 8. First, the model repeats the last query without resolving any referred entity or ellipsis. In this case, the model seems to understand the instruction but fails at rewriting. This type of error can be primarily associated with the model\u2019s context understanding capability. Second, the model treats the task as a language modeling (LM) task, where it provides a response to the last query. In this scenario, the model appears to struggle to understand the task instruction, even with several few-shot examples. We classify this error type as more related to the model\u2019s ICL ability. We perform manual error annotations on the five QR datasets9. Table 9 illustrates the number of errors of the three selected models on each dataset. A consistent trend is observed across all QR datasets. In terms of repeat errors, the 30B dense model exhibits significantly fewer errors compared to the 7B dense model (297 vs. 469). However, 3-bit GPTQ quantization leads to an increase in this type of error, reaching a similar error count to the 7B dense model (458 vs. 469). This implies that 3-bit quantization reduces the model\u2019s ability to comprehend the context. Regarding LM errors, the 30B dense model also significantly outperforms the 7B dense model, with 106 errors compared to 232. It is to be noted that the quantized model generates only 125 LM errors, slightly more than the 30B dense model. However, it generates significantly fewer (around\n910% test data on QReCC and CANARD was graded.\n50%) errors compared to the 7B dense model (125 vs. 232). This indicates that 3-bit quantization maintains the ICL capability that allows models to rewrite the user query successfully rather than performing language modeling task.\n# 6 Conclusion\nThis paper introduces a contextual understanding benchmark designed to assess the performance of LLMs. We collect nine existing datasets spanning four tasks, each carefully tailored to suit generative models. This benchmark encompasses essential elements for assessing linguistic comprehension within context, including both document and dialog based contextual understanding. Experimental results reveal that LLMs under in-context learning struggle with nuanced linguistic features within this challenging benchmark, exhibiting inconsistencies with other benchmarks that emphasize other aspects of language. To the best of our knowledge, we are also the first to compare dense models and post-training quantization models in contextual understanding tasks. This comparison highlights that 3-bit post-training quantization reduces the general understanding capacity of context to different extent across the 4 tasks. The proposed contextual comprehension benchmark thus provides a unique perspective on the contextual dimension of language understanding and offers a valuable addition to existing LLM evaluations.\n# Limitations\nThis work provides an evaluation of various pretrained LLMs, including OPT, LLaMA, and GPT, on our understanding benchmark. However, we have not evaluated other LLMs designed for longer input scenarios, such as LongLLaMA (Tworkowski et al., 2023). In our evaluation, we focus on the GPTQ quantization method, analyzing its performance on our benchmark. We do not include other post-training quantization techniques, such as RPTQ (Yuan et al., 2023), in this work. Our evaluation concentrates on English datasets, primarily utilizing LLMs pre-trained with English data. All of the four tasks on our benchmark have datasets from other languages. The coreference dataset OntoNotes 5.0 contains annotations of Arabic and Chinese. In addition, recent releases such as CorefUD (Nedoluzhko et al., 2022) promote standardization of multilingual coreference anno-\ntations. In DST, CrossWOZ (Zhu et al., 2020) is a cross-domain wizard-of-oz task-oriented dataset. Long et al. (2020) develop TED-CDB, a Chinese discourse relation dataset. The query rewriting task also has datasets in other languages, such as REWRITE (Su et al., 2019) and Restoration-200K (Pan et al., 2019). Finally, specific LLMs optimized for individual languages, such as ChatGLM (Du et al., 2022), exist and are not a part of our evaluation.\n# Acknowledgements\nThe authors would like to thank Jeffrey Nichols, Russ Webb and the anonymous reviewers for their help and feedback.\n# References\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-domain question answering goes conversational via question rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 520\u2013534, Online. Association for Computational Linguistics.\nohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.\nMichael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, and Michael Witbrock. 2018. A systematic classification of knowledge, reasoning, and context within the ARC dataset. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 60\u201370, Melbourne, Australia. Association for Computational Linguistics.\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020a. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u00b4c. 2018. MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016\u20135026, Brussels, Belgium. Association for Computational Linguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335.\nAhmed Elgohary, Denis Peskov, and Jordan BoydGraber. 2019. Can you unpack that? learning to rewrite questions-in-context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5918\u20135924, Hong Kong, China. Association for Computational Linguistics. Elias Frantar and Dan Alistarh. 2023. SparseGPT: Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323. Michael Heck, Nurul Lubis, Benjamin Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, Hsienchin Lin, Carel van Niekerk, and Milica Gasic. 2023. ChatGPT for zero-shot dialogue state tracking: A solution or an opportunity? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 936\u2013950, Toronto, Canada. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2627\u20132643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64\u201377. Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Gabriel Murray. 2019. Discourse analysis and its applications. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12\u201317, Florence, Italy. Association for Computational Linguistics.\nJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. 2023. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Findings of the Association for Computational Linguistics: ACL 2023, pages 431\u2013469, Toronto, Canada. Association for Computational Linguistics.\n# Nghia T. Le and Alan Ritter. 2023. Are large language models robust zero-shot coreference resolvers?\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. 2017. End-to-end neural coreference resolution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188\u2013197, Copenhagen, Denmark. Association for Computational Linguistics.\nChaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models.\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models. Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics. Wei Liu and Michael Strube. 2023. Annotation-inspired implicit discourse relation classification with auxiliary discourse connective generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15696\u201315712, Toronto, Canada. Association for Computational Linguistics. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. Llm-qat: Data-free quantization aware training for large language models. Wanqiu Long, Bonnie Webber, and Deyi Xiong. 2020. TED-CDB: A large-scale Chinese discourse relation dataset on TED talks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2793\u20132803, Online. Association for Computational Linguistics. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In ACL 2014 System Demonstrations, pages 55\u201360. Scott Martin, Shivani Poddar, and Kartikeya Upasani. 2020. MuDoCo: Corpus for multidomain coreference resolution and referring expression generation. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 104\u2013111, Marseille, France. European Language Resources Association. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. LSDSem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46\u201351, Valencia, Spain. Association for Computational Linguistics. Nikola Mrk\u0161i\u00b4c, Diarmuid \u00d3 S\u00e9aghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. 2017. Neural belief tracker: Data-driven dialogue state tracking. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1777\u20131788. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training quantization. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920. JMLR.org.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\nNikola Mrk\u0161i\u00b4c, Diarmuid \u00d3 S\u00e9aghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. 2017. Neural belief tracker: Data-driven dialogue state tracking. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1777\u20131788.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training quantization. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920. JMLR.org.\nAnna Nedoluzhko, Michal Nov\u00e1k, Martin Popel, Zden\u02c7ek \u017dabokrtsk\u00fd, Amir Zeldes, and Daniel Zeman. 2022. CorefUD 1.0: Coreference meets Universal Dependencies. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4859\u20134872, Marseille, France. European Language Resources Association.\n# OpenAI. 2022. Optimizing language models for dialogue.\n# OpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.\nZhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou, and Xiaojiang Liu. 2019. Improving open-domain dialogue systems via multi-turn incomplete utterance restoration. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1824\u20131833, Hong Kong, China. Association for Computational Linguistics.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, Berlin, Germany. Association for Computational Linguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 1\u201340, Jeju Island, Korea. Association for Computational Linguistics.\nun Quan, Deyi Xiong, Bonnie Webber, and Changjian Hu. 2019. GECOR: An end-to-end generative ellipsis and co-reference resolution model for taskoriented dialogue. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4547\u20134557, Hong Kong, China. Association for Computational Linguistics.\nck W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling language models: Methods, analysis & insights from training gopher.\n# Michael Regan, Pushpendre Rastogi, Arpit Gupta, and Lambert Mathias. 2019. A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (cqr). ArXiv, abs/1903.11783.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.\nQi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and Minlie Huang. 2020. CrossWOZ: A large-scale Chinese cross-domain task-oriented dialogue dataset. Transactions of the Association for Computational Linguistics, 8:281\u2013295.\n# A Task Design Examples\nTable 10 presents the input example for each task. For CR, we only show examples from OntoNotes.\n# B Few-shot Settings\nTable 11 shows the number of examples for each dataset that yields the best scores. All datasets except WSC273 and PDTB3 use randomly selected examples from the training set. Since WSC273 does not include any train or validation set, we use the zero-shot setting, as scores presented in Table 5. For each class in PDTB3, we randomly select two examples from the training set for prompting. For some particular datasets, such as OntoNotes, experiments are only performed in the zero-shot and one-shot settings due to the limitation on input length.\n# C Reliability of Experiment Results\nFor each task, we have randomly run several experimental setups with multiple rounds, with over 10 settings in total. However, due to the challenges posed by limited time, budget, and computing resources, it is very difficult to run multiple rounds for every single experiment, given the complexity of our experimental setup. In addition, for existing experiments with multiple rounds, we empirically observe that there is low variance across the rounds, which leads us to assume that performing the remaining experiments with a single run does not significantly impact the arguments presented in this paper.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f367/f3673fbe-11a5-41b1-bf88-11ac6cad2a79.png\" style=\"width: 50%;\"></div>\n\u201ccategorical\u201d: {\u201crestaurant-pricerange\u201d: [\u2018cheap\u2019, \u2018expensive\u2019, \u2018moderate\u2019], \u201crestaurant-area\u201d: [\u2019centre\u2019, \u2019east\u2019, \u2019north\u2019, \u2019south\u2019, \u2019west\u2019], \u201crestaurant-bookday\u201d: [\u2019monday\u2019, \u2019tuesday\u2019, \u2019wednesday\u2019, \u2019thursday\u2019, \u2019friday\u2019, \u2019saturday\u2019, \u2019sunday\u2019], ... \u201chotel-internet\u201d: [\u2019free\u2019, \u2019no\u2019, \u2019yes\u2019], \u201chotel-area\u201d: [\u2018centre\u2019, \u2018east\u2019, \u2018north\u2019, \u2018south\u2019, \u2018west\u2019]\nNow consider the following dialogue between two parties called the \u201csystem\u201d and \u201cuser\u201d. Can you tell me which of the \u201cslot\u201d was updated by the \u201cuser\u201d in its latest response to the \u201csystem\u201d? Present the updates in JSON format. If no \u201cslots\u201d were updates, return an empty JSON list. If you encounter \u201cslot\u201d that was requested by the \u201cuser\u201d then fill them with \u201c?\u201d. If a user does not seem to care about a discussed \u201cslot\u201d fill it with \u201cdontcare\u201d.\n<div style=\"text-align: center;\">Table 10: Examples of task design for each task in the context understanding benchmark.</div>\nTask\nCoreference\nDST\nDiscourse\nquery rewriting\nDataset\nWSC273\nOntoNotes\nMultiWOZ\nPDTB3\nMuDoCo\nQReCC\nInCar\nGECOR\nCANARD\nN-example\nZero-shot\nOne-shot\n5-shot\n8-shot\n10-shot\n5-shot\n10-shot\n10-shot\n5-shot\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The evaluation of Large Language Models (LLMs) has largely overlooked their ability to understand contextual features, despite their impressive performance in various Natural Language Processing (NLP) tasks. Existing benchmarks do not adequately assess the linguistic capabilities required for discourse understanding, which involves resolving relationships between words and phrases across sentences.",
            "purpose of benchmark": "The benchmark aims to evaluate the context understanding capabilities of LLMs, particularly in the context of in-context learning scenarios and model compression techniques."
        },
        "problem": {
            "definition": "The benchmark is designed to address the challenge of evaluating LLMs' understanding of context, particularly in tasks that require nuanced comprehension of linguistic features across multiple dialogue turns and documents.",
            "key obstacle": "Existing benchmarks fail to provide a comprehensive assessment of LLMs' discourse understanding capabilities, particularly in evaluating models under in-context learning settings and assessing the impact of model compression."
        },
        "idea": {
            "intuition": "The development of this benchmark was inspired by the need to rigorously evaluate LLMs' contextual understanding, particularly as these models grow in size and complexity.",
            "opinion": "The authors believe that this benchmark is crucial for advancing the field of NLP by providing a more comprehensive evaluation of LLMs' linguistic capabilities.",
            "innovation": "This benchmark introduces a novel approach by incorporating multiple tasks and datasets specifically designed to evaluate context understanding, which has been largely neglected in previous benchmarks.",
            "benchmark abbreviation": "CUB"
        },
        "dataset": {
            "source": "The dataset is sourced from nine existing datasets that cover a range of tasks related to context understanding in both dialogue and document-based scenarios.",
            "desc": "The benchmark includes four distinct tasks and nine datasets, each tailored to assess various aspects of context understanding in LLMs.",
            "content": "The dataset includes tasks such as coreference resolution, dialogue state tracking, discourse relation classification, and query rewriting, encompassing both structured and unstructured data.",
            "size": "9",
            "domain": "Discourse Understanding",
            "task format": "Coreference Resolution"
        },
        "metrics": {
            "metric name": "Accuracy, JGA",
            "aspect": "Model performance in understanding and resolving contextual information.",
            "principle": "The selected metrics are based on their ability to effectively measure the nuanced performance of LLMs in context understanding tasks.",
            "procedure": "Models are evaluated based on their performance on the tasks using standard metrics such as accuracy and joint goal accuracy, with results aggregated across multiple trials."
        },
        "experiments": {
            "model": "The models tested include various sizes of OPT, LLaMA, and GPT-3.5-turbo, representing both small and large language models.",
            "procedure": "The models were evaluated in various settings, including zero-shot, one-shot, and few-shot prompting, across the selected tasks.",
            "result": "Results indicated that larger models generally performed better, although pre-trained models struggled to match the performance of fine-tuned models.",
            "variability": "Variability in results was accounted for through multiple trials and different prompting strategies."
        },
        "conclusion": "The benchmark reveals that LLMs exhibit significant challenges in understanding nuanced contextual features, highlighting the need for further research and development in this area.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive framework for evaluating LLMs' context understanding, addressing a critical gap in existing evaluations.",
            "limitation": "The benchmark focuses primarily on English datasets and does not consider other languages or additional model quantization techniques.",
            "future work": "Future research could explore the evaluation of LLMs in multilingual contexts and assess the impact of various post-training quantization methods."
        },
        "other info": [
            {
                "info1": "The benchmark includes tasks that require both generative and classification-based outputs."
            },
            {
                "info2": {
                    "info2.1": "The code and datasets are publicly available for further research.",
                    "info2.2": "The benchmark aims to facilitate a better understanding of LLMs' limitations in contextual comprehension."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The evaluation of Large Language Models (LLMs) has largely overlooked their ability to understand contextual features, despite their impressive performance in various Natural Language Processing (NLP) tasks."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark aims to evaluate the context understanding capabilities of LLMs, particularly in the context of in-context learning scenarios and model compression techniques."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark is designed to address the challenge of evaluating LLMs' understanding of context, particularly in tasks that require nuanced comprehension of linguistic features across multiple dialogue turns and documents."
        },
        {
            "section number": "3.2",
            "key information": "This benchmark introduces a novel approach by incorporating multiple tasks and datasets specifically designed to evaluate context understanding, which has been largely neglected in previous benchmarks."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark reveals that LLMs exhibit significant challenges in understanding nuanced contextual features, highlighting the need for further research and development in this area."
        },
        {
            "section number": "6.4",
            "key information": "Future research could explore the evaluation of LLMs in multilingual contexts and assess the impact of various post-training quantization methods."
        }
    ],
    "similarity_score": 0.732962499769019,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Can Large Language Models Understand Context_.json"
}