{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2306.03799",
    "title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models",
    "abstract": "Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt \"Let\u2019s think step by step\", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at https://github. com/YouBLEI/Prompt-Space",
    "bib_name": "shi2024promptspaceoptimizingfewshot",
    "md_text": "# Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models\nFobo Shi1*\u2020, Peijun Qing2,\u2217,\u2020, Dong Yang3,\u2217\u2021, Nan Wang3, Youbo Lei4,\u2020 Haonan Lu3,\u2021, Xiaodong Lin5, Duantengchuan Li1 1Wuhan University, 2Dartmouth College, 3OPPO Research Institute, 4Xi\u2019an Jiaotong University, 5Rugster Unversity\n# Abstract\nPrompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt \"Let\u2019s think step by step\", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at https://github. com/YouBLEI/Prompt-Space\n 28 Mar 2024\n[cs.CL]\n# 1 Introduction\nPrompt engineering becomes a relatively new and hot discipline for designing and optimizing prompts to effectively use large language models\n* Equal contribution. \u2020 The work was done during their internship in OPPO Research Institute. \u2021 Corresponding author (dongyang3-c@my.cityu.edu.hk); luhaonan@oppo.com.\n(LLMs) for a wide variety of applications and research domains (Brown et al., 2020; Thoppilan et al., 2022; Zhou et al., 2022; Sun et al., 2022; Dong et al., 2022). Researchers explore the use of simple and specific instructions to enhance the performance of LLMs on complex tasks, including arithmetic and commonsense reasoning, as well as question answering (Chowdhery et al., 2022; Scao et al., 2022; Ouyang et al., 2022; Bai et al., 2022). Developers strive to design robust and effective prompts either manually (Schick and Sch\u00fctze, 2020; Reynolds and McDonell, 2021) or automatically (Gao et al., 2020) that interface with LLMs and other tools (Wu et al., 2023; Xie et al., 2023). The goal is to uncover the full potential of LLMs across various domains, enabling them to tackle complex tasks with improved performance and accuracy. To elicit the reasoning ability of LLMs, (Wei et al., 2022) has proposed the concept of the chainof-thought (CoT) prompting. Unlike traditional input-output exemplars, the CoT prompting creates a series of intermediate reasoning steps that guide LLMs through a complex problem. This approach enables LLMs to develop a reasoning path that decomposes the complex problem into multiple reasoning steps. Notably, the CoT prompting demonstrates that the reasoning ability of LLMs perfectly matches the scaling laws, with the reasoning ability of LLMs significantly increasing with the size of the PaLM 540B model. Inspired by the CoT prompting, several works explore methods to enhance LLMs\u2019 reasoning abilities with simple techniques. (Kojima et al., 2022) introduces the \"Let\u2019s think step by step\" prompt, which helps LLMs adopt a step-by-step thinking approach, leading to the final answer. Their approach, known as Zero-shot-CoT, successfully generates a reasoning path in zero-shot reasoning scenarios. In practice, the CoT prompting has showed better performance than Zero-shot-CoT (Wei et al., 2022;\nKojima et al., 2022). However, the CoT prompting involves huge efforts in the manual design of both questions and related reasoning chains. To avoid the manual method, (Zhang et al., 2022) proposes an automatic CoT prompting, called Auto-CoT. It applies a clustering algorithm to identify representative questions for each cluster and generates reasoning chains using the Zero-shot-CoT method for each question. The previous works on CoT have greatly contributed to our understanding of effective prompts for improving the reasoning ability of LLMs. However, these works have certain limitations, such as the lack of guidance on finding optimal prompts for reasoning tasks. In this paper, we propose a novel approach called Prompt Space that overcomes these limitations and simultaneously leverages the strengths of previous works. Our approach starts by embedding questions and then utilizes matrix decomposition to yield basis vectors, or basis questions. These basis questions are used to construct a space that can represent all questions. With Zero-shot-CoT, we combine these basis questions with every question to automatically generate reasoning demonstrations for LLMs. Our approach offers a promising solution to find optimal prompts on reasoning tasks and significantly improves the few-shot reasoning of LLMs. Prompt Space surpasses the performance of current prompt paradigms on ten public reasoning benchmarks. Our work uncovers critical insights into the impact of the number of basis questions on reasoning tasks. Additionally, we identify the relationship between the selected questions and the reasoning ability of LLMs, and investigate how to determine the optimal number of exemplars for each reasoning task. Extensive experiments demonstrate that our approach establishes a reliable and mathematical methodology for selecting simple and effective prompts. Our goal is to not only design the robust and effective prompts for challenging reasoning tasks, but also highlight the significance of carefully exploring and analyzing the optimal prompts for unlocking the potential of LLMs in a wide variety of applications.\n# 2 Related Work\n# 2.1 Chain-of-thought Prompting\nChain-of-thought (CoT) prompting is an effective method to elicit the reasoning ability of LLMs through a chain of thought, where a series of inter-\nmediate reasoning steps are used to generate the answer (Wei et al., 2022). This approach has been shown to significantly improve the performance of LLMs on complex reasoning tasks. To further enhance their performance, self-consistency (SC) has been introduced, which replaces the standard greedy decoding of the LLM output with a stochastic output space ensemble (Wang et al., 2022b). Existing studies on the CoT prompting can be broadly divided into two categories: manually constructed the CoT prompting and automatically generated the CoT prompting. Our work aims at providing a robust and mathematical framework for selecting simple and effective prompts.\n# 2.2 Automatically Generated CoT Prompts\nFor enhancing CoT reasoning in LLMs, several previous works have explored the idea of selfgenerating a chain of thought (Kojima et al., 2022; Zhang et al., 2022; Zhou et al., 2022; Hebenstreit et al., 2023). (Kojima et al., 2022) finds that using specific phrases, like \"Let\u2019s think step by step\", as a prompt can guide LLMs to generate reasoning steps without any few-shot hand-crafting exemplars. Following this work, (Zhou et al., 2022) proposes a framework called Automatic Prompt Engineer (APE) for generating and selecting instructions automatically. APE addresses the instruction generation problem by using LLMs to generate and search for candidate solutions. Additionally, some studies implement Zero-shotCoT to generate the reasoning process in their demonstration (Kojima et al., 2022). (Zhang et al., 2022) proposes a novel method called Auto-CoT, for automatically creating the Chain of Thought (CoT) prompting in LLMs. This method samples diverse questions and reasoning chains to construct effective demonstrations for LLMs. It can elicit chain-of-thought reasoning without impairing performance and eliminating the need for handcrafting prompts. In contrast, (Shao et al., 2023) utilizes seed demonstrations to automatically synthesize more examples through forward and backward processes. Inspired by these works, we construct a space with text embeddings and the matrix decomposition to represent all questions. We also utilize Zero-shot-CoT to generate chains of thought for prompt examples (Kojima et al., 2022).\n# 2.3 Example Selection\nFor designing prompts, certain studies demonstrate that the performance of LLMs is influenced by var-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2c69/2c69cfdc-5a8b-4444-aac6-10cfec3d3571.png\" style=\"width: 50%;\"></div>\nFigure 1: The schematic of Prompt Space. Prompt Space consists of three steps, including embedding question finding basis vectors, and selecting basis questions. First, all questions in a reasoning dataset are encoded into th question matrix Qm\u00d7n. Second, the k basis vectors in the question matrix Qm\u00d7n are calculated by SVD and PCA Finally, the top k questions are selected to form the question space. The prompt exemplar is made up of the selecte k questions and the test question. LLMs could follow the prompt exemplar and then develop a chain of reasonin steps to get the final answer.\nious factors, such as tasks, prompts, and model structures (Zhao et al., 2021; Lu et al., 2021; Su et al., 2022; Griffin et al., 2023; Jiang et al., 2022). The main challenge is to develop selection criteria that are both effective and generalizable based on empirical experiments. (Wang et al., 2022b) shows that the sequence of reasoning steps is critical for achieving optimal performance. Additionally, (Rubin et al., 2021) proposes a similarity-based se-\nlection method, which retrieves the most similar training instances as a prompt for a given test case. Furthermore, another approach proposed by (Fu et al., 2022) suggests the selected prompts with more steps can significantly improve performance in the reasoning process. However, our method, Prompt Space, explores question embeddings to obtain basis questions in a reasoning task, which dramatically avoids using ineffective questions as\na demonstration. This approach provides an innovative mathematical solution for selecting effective prompts, yielding more generalizable and comprehensive reasoning chains. Our Prompt Space aims to develop a deep understanding of how to design the CoT prompting.\n# 3 Prompt Space\nIn this work, we propose a novel method called Prompt Space, which automatically creates demonstrations with questions and reasoning chains. Prompt Space seeks to design an appropriate space for identifying the basis questions for building prompt exemplars. For a vector space V , its vector basis is defined as a subset v1, v2..., vn in V . These basis vectors are linearly independent in span V . Consequently, if (v1, v2..., vn) is a list of vectors in V , then these vectors form a vector basis if and only if every x \u2208V can be uniquely written as\n(1)\nwhere c1, c2, ..., cn are elements of the base field. In Prompt Space, one vector represents a question embedding. By combining the basis questions with the test question, we create a demonstration that enables LLMs to effectively generate a chain of thought. Next, we will show how to select such basis questions for constructing Prompt Space. Selecting effective prompts as an exemplar can significantly enhance the reasoning abilities of LLMs. To solve arithmetic problems, humans tend to learn from previous question-answer pairs and generalize them to solve similar problems. Inspired by this thought mechanism, our work aims to select more representative questions as an exemplar to facilitate LLMs in developing a chain of reasoning steps. We assume that there exists a real prompt space P with k dimensional vectors, where the selected representative questions can serve as basis vectors of this space. These basis vectors provide an effective solution for LLMs to reason through the problem space. Principal component analysis (PCA) is a widely used algorithm for identifying the key components of extensive data features by geometric steps (Abdi and Williams, 2010). The implementation of PCA can efficiently compress a n-dimension matrix into a k-dimension matrix and obtain k principal vectors from the original space. Inspired by PCA, Prompt Space follows the below steps: 1. Embedding Questions. The question set of a task is Q = {q1, q2, ..., qm}, where m is the\nnumber of questions in a task. The MiniLML6-v2 model fMiniLM (Wang et al., 2020) encodes these questions as follows: qi = fMiniLM(qi) \u2208Rn, for i = 1, 2, ..., m. After the encoding process, the question matrix Q is created by putting together all question embeddings, i.e., Q = [q1, q2, ..., qm]T \u2208 Rm\u00d7n. Assuming the dimension (rank) of the prompt space P is k, the process of finding k basis vectors is identical to searching for the k principal components (questions) of the question matrix Q. 2. Finding basis vectors. We use Singular Value Decomposition (SVD) to calculate k basis vectors in the prompt space P (Wall et al., 2003). Using SVD, Q can be calculated as:\n(2)\nwhere U is denoted as a left singular matrix, U = [u1, u2, ..., um]T \u2208Rm\u00d7m, and ui \u2208R1\u00d7m is the eigenvector of QQT \u2208 Rm\u00d7m (for i = 1, ..., m). A complete proof is shown in Appendix A. Similarly, V is the right singular matrix, which can be written as V = [v1, v2, ...vm]T \u2208Rn\u00d7n, vi \u2208Rn\u00d71 (for i = 1, ...n) is the eigenvector of QT Q. Next, the k principal components of Q can be obtained:\n(3)\n(4)\n  where argmax(\u2022) is to calculate the maximum similarities between question embeddings and basis vectors (i.e, cosine similarity) (Sidorov et al., 2014). Finally, we can generate the prompt exemplar, including k basis questions and the original question in Q, to get the final output (answer).\nFigure 1 shows that an example of Prompt Space produces basis questions to solve an arithmetic problem. By following the three steps, we could select k basis questions and then combine them with the test question. To assist LLMs in generating he final output, we also use the prompt, \"Let\u2019s hink step by step\". Throughout the process, we still opt to automatically create the prompt rather han manually design it. As a result, LLMs can generate a step-by-step thought process for arriving at the answer. The algorithm of Prompt Space is shown in Appendix B. Prompt Space has several attractive properties as an approach for enhancing reasoning in LLMs. 1. Prompt Space enables LLMs to identify optimal prompts for a range of reasoning tasks and efficiently generate final outputs. 2. Prompt Space provides a robust mathematical framework for designing the prompt. It can suggest the optimal number of exemplars to improve the reasoning abilities of LLMs. Our method provides valuable insights into effective prompting strategies for achieving successful outcomes. 3. Prompt Space has the potential to be utilized in a variety of few-shot tasks through prompt engineering, including but not limited to translation, summarization, and expansion.\n# 4 Experiments\nWe briefly describe the experimental setup and highlight the main results. Additional experimental details and results can be found in Appendices C and D. Prompt Space is evaluated on three categories of reasoning tasks, namely arithmetic reasoning, commonsense reasoning, and symbolic reasoning. The experiment demonstrates Prompt Space on various tasks: 1. Prompt Space outperforms the state-ofthe-art baselines on these tasks. 2. Prompt Space can efficiently construct a space and find its basis questions for each task. 3. Prompt Space can determine the optimal number of basis questions for significantly improving the performance of LLMs on each dataset. 4. Prompt Space relies on the selection of embedding models.\n# 4.1 Experimental setup\nTasks and Datasets. Prompt Space is studied on ten standard datasets from three categories of reasoning tasks:\n1. Arithmetic reasoning contains six datasets: (1)AddSub (Hosseini et al., 2014), (2) MultiArith (Roy et al., 2015), (3) SingleEq (Koncel-Kedziorski et al., 2015), (4) AQUA-RAT (Ling et al., 2017), (5) SVAMP (Patel et al., 2021), (6) GSM8K (Cobbe et al., 2021). These datasets are sorted by release time. SingleEq and AddSub have plenty of easier problems, while MultiArith, AQUA-RAT, SVAMP, and GSM8K are more difficult and require multistep reasoning steps. 2. Commonsense reasoning: (1) CommonsenseQA (CSQA) (Talmor et al., 2019), (2) StrategyQA (STQA) (Geva et al., 2021). CSQA is a challenging dataset for commonsense question answering. Its questions contain complex semantics that often requires prior knowledge. STQA requires multi-step reasoning with an inferred strategy in the question. 3. Symbolic reasoning: (1) Last Letter Concatenation (Letter) and (2) Coin Flip (Coin) (Wei et al., 2022). Last Letter Concatenation asks the model to concatenate the last letters of words in a name. We generate full names by randomly concatenating names from samples. Coin Flip requires the model to answer whether a coin is still heads up after people either flip it or do not flip it. In this work, we consider an out-of-domain test set, where examples have more steps than those in exemplars. The detailed description of each dataset is shown in Appendix C.1. Baselines. We compare our Prompt Space with five baseline methods: Few-shot (Wei et al., 2022), Manual-CoT (Wei et al., 2022), Zero-shot (Kojima et al., 2022), Zero-shot-CoT (Kojima et al., 2022), and Auto-CoT (Zhang et al., 2022). Few-shot easily selects question-answer pairs as a demonstration for feeding to LLMs. Manual-CoT involves manually creating a series of reasoning chains as a demonstration to elicit the reasoning ability of LLMs. Zero-shot is a standard prompting technique for evaluating the abilities of LLMs. Zeroshot-CoT randomly selects questions as demonstrations, and then uses the prompt \"Let\u2019s think step by step\". Additionally, Auto-CoT utilizes clustering techniques to sample questions and generate demonstrations with the Zero-shot-CoT method. To ensure fair comparisons with the baselines, we run experiments with consistent in-context exemplars and a constant seed across all methods and datasets. Few-shot and Manual-CoT select the examples by human, while Auto-CoT select the examples by the K-means clustering algorithm. Our\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5aa1/5aa1d87b-9cb3-466f-a89c-6f968fa69523.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Prompt-Space-CoT-Zero (including the prompt, \u201cLet\u2019s think step by step\u201d prompt) and Prompt-Space-C (not including the prompt, \u201cLet\u2019s think step by step\u201d) with an input-output exemplar of an LLM.</div>\nPrompt Space uses the same rationales with ZeroCoT and Auto-CoT not Manual-CoT. Our CoT is generated by LLMs not humans. Figure E3 shows demonstrations of CSQA on difference methods including Random selection, Manual-CoT, AutoCoT and Our Prompt Space. Please refer to Appendix C.2 for detailed baselines. Implementation. We use the gpt-3.5-turbo-0301 version of the public ChatGPT model from the OpenAI API with 175 billion parameters (Brown et al., 2020; Ouyang et al., 2022). We select this LLM because it has better performance than the text-davinci-002 version of GPT-3, as reported in (OpenAI, 2023; Bai et al., 2022). In the decoding process, we set the temperature to 0 and use a greedy searching algorithm to obtain results. For zero-shot approaches, our results are deterministic. Following (Wei et al., 2022), we set the number of demonstrations k to 8, except for AQUA-RAT (4) and Letter (4), StrategyQA (6), and CSQA (7). However, Our Prompt Space can determine the optimal number of basis questions for each task. In the following sections, we will present a detailed analysis and provide further insights into the selection of basis questions. The selected embedding models are T5 models (base/large/XL/XXL) (Raffel et al., 2020), E5 models (small/base/large) (Wang et al., 2022a) and MiniLM-L6-v2 model (Wang et al., 2020). The embedding size of each question in all T5 models is 768, while for E5 models (small, base, large), their embedding sizes are 384, 768, 1024 respectively. Our MiniLM-L6-v2 model encodes questions with an embedding size of 384. Please refer to Appendix C.3 for detailed model descriptions. In our approaches, we investigate two types of Prompt Space shown in Fig. 2. The first type combines CoT with the \u201cLet\u2019s think step by step\u201d\nprompt, denoted as Prompt-Space-CoT-Zero. In contrast, the second type only uses CoT, namely Prompt-Space-CoT.\n# 4.2 Main Results\nIn the experiments, we evaluate Prompt Space on ten datasets from three categories of reasoning tasks. Due to the greedy decoding, the main results show deterministic results without error bars. Notably, Table 1 and 2 show that Prompt Space achieves superior performance over the state-ofthe-art methods on ten reasoning tasks, respectively. Compared to Auto-CoT, Prompt space with the optimal number of exemplars achieves up to average 3.2% in Table 2. Prompt Space vs Few-shot. Table 1 summarizes comparisons between our approach (Prompt Space) and two baselines (Zero-shot and Few-shot) for each dataset. In Table 1, Prompt Space doesn\u2019t include CoT and the \u201cLet\u2019s think step by step\u201d prompt, and just selects basis questions as a demonstration. Our results show that Prompt Space with the same settings achieves up to average 2.3%, 2% over Zero-shot and Few-shot on ten reasoning datasets, respectively. Especially, Prompt Space, with the optimal number of exemplars, achieves up to average 3.3%, 3% over Zero-shot and Few-shot on ten reasoning datasets, respectively. The most significant improvement is observed in the STQA and Letter datasets, with a relative increase of 13.5%, 112.5% over Few-shot, respectively. Moreover, Prompt Space outperforms two baselines on eight out of ten reasoning datasets. Arithmetic Reasoning. Our approach substantially outperforms the three baselines on five arithmetic reasoning tasks except for AddSub in Tab. 2. Importantly, our Prompt Space with the same set-\nTable 1: Accuracy (%) comparison of Prompt Space with two baselines on ten reasoning datasets. Two baselines are Zero-shot and Few-shot, respectively. Ten benchmark datasets contain three categories, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning. The last column shows average scores. See Appendix C\nfor a detailed setup.\nModel\nArithmetic\nCommonsense\nSymbolic\nAvg\nAddSub MultiArith SingleEq AQUA-RAT SVAMP GSM8K\nCSQA\nSTQA\nLetter\nCoin\nZero-shot\n87.6\n80.0\n87.4\n27.6\n74.0\n22.9\n73.6\n61.3\n0.8\n22.8\n53.8\nFew-shot\n85.8\n82.7\n89.4\n89.4\n89.4\n30.7\n76.1\n76.1\n76.1\n24.0\n79.3\n54.0\n1.6\n57.0\n58.1\nPrompt Space w/o CoT-Zero\n89.4\n89.4\n89.4\n83.7\n83.7\n83.7\n88.8\n32.7\n32.7\n32.7\n75.2\n25.2\n25.2\n25.2\n79.1\n61.3\n61.3\n61.3\n3.4\n3.4\n3.4\n62.0\n62.0\n62.0\n60.1\n60.1\n60.1\nPrompt Space w/o CoT-Zero (best) 89.9(10)\n89.9(10)\n89.9(10)\n86.3 (9)\n86.3 (9)\n86.3 (9)\n88.8 (8)\n32.7 (3)\n32.7 (3)\n32.7 (3)\n75.6 (6) 25.9 (6)\n25.9 (6)\n25.9 (6) 80.0 (8)\n80.0 (8)\n80.0 (8) 62.8 (10)\n62.8 (10)\n62.8 (10) 5.2 (9)\n5.2 (9)\n5.2 (9) 63.8 (6)\n63.8 (6)\n63.8 (6) 61.1\n61.1\n61.1\ntings achieves score gains of 1.8%, 1.2%, 2% and 2.1% over the previous state-of-the-art methods on MultiArith, SingleEq, SVAMP, and GSM8K, respectively. Although Prompt Space doesn\u2019t show competitive performance on AddSub, it is close to Auto-CoT. Additionally, Promp Space achieves the highest performance on AQUA-RAT, SVAMP, and GSM8K, indicating that it can solve more complex arithmetic reasoning. The difference between Prompt-Space-CoT-Zero and Prompt-Space-CoT is trivial, approximately 2%. Overall, the average score of Prompt-Space-CoT surpasses that of the three baselines on all arithmetic reasoning datasets, indicating its superior performance. Commonsense Reasoning. Prompt Space significantly outperforms the prior state-of-theart Auto-CoT over two commonsense reasoning datasets. Our approach with the same settings achieves respective improvements of 1.9%, 1.6% over Mannual-CoT and 1.4%, 0.9% over AutoCoT. Compared to Zero-shot, Zero-shot-CoT and Manual-CoT don\u2019t elicit better commonsense reasoning, while Prompt Space leverages the CoT method to dramatically increase performance instead of decreasing it. These results demonstrate that Prompt Space can improve performance on commonsense reasoning tasks requiring prior knowledge. Symbolic Reasoning. The performance of Prompt Space achieves a significant increase of 3.2% over Mannual-CoT and 9.4%, over Auto-CoT on the Letter dataset, respectively. Interestingly, the accuracy of Mannual-CoT, Auto-CoT, and our approach reaches to 100% on the Coin Flip dataset. The result indicates that Prompt Space dramatically enhances the reasoning abilities of LLMs on symbolic tasks.\n# 4.3 Effect of Embedding Models\nFigure 3 shows that the increase of embedding size cannot improve the performance of Prompt Space on various reasoning tasks. Besides, the appropriate embedding size could be 768 in T5 and E5\nmodels. As T5 models increase their model size, the performance of Prompt Space decreases significantly. Moreover, the solving rate of Prompt Space exhibits clear fluctuations on different embedding models.\n# 4.4 Further Analysis of Basis Questions\nFigue 4 illustrates the performance of Prompt Space with different basis questions on nine datasets. Our results reveal that the appropriate number of basis questions is 8 on arithmetic reasoning tasks except for AQUA-RAT, while that of basis questions is approximately 6 or 7 on commonsense reasoning tasks. Interestingly, the AQUA-RAT and Letter datasets exhibit a preference of a smaller number of basis questions, which indicates their space could be spanned by just four or five basis vectors. Overall, our findings demonstrate that the appropriate number of basis questions can significantly improve performance, which indicates that there exist basis vectors (questions) in the prompt space. However, there remains a challenge that we cannot automatically determine the optimal number of basis questions for each dataset. More analysis about basis questions of Prompt-Space-CoT is shown in Appendix D.1. Besides, we provide more visualizations of Prompt Space and the constructed demonstrations in Appendix D.2 and E, respectively.\n# 4.5 Effect of Question Sequence\nTable 3 shows that Prompt Space achieves better performance than other cases, when the basis questions are sorted in ascending order of their eigenvalues. However, the descending sort (original sequence) has superior performance over baselines on three out of four benchmarks. Furthermore, the difference between the original sequence and the reverse sequence is trivial (\u223c0.1%). Thus, these findings suggest that the descending sort is a acceptable approach used in our experiments.\n<div style=\"text-align: center;\">Table 2: Accuracy (%) comparison of Prompt Space with four state-of-the-art methods on ten reasoning datasets. These datasets contain three categories, such as arithmetic reasoning, commonsense reasoning, and symbolic reasoning. The last column records average scores. See Appendix C for a detailed setup.</div>\nreasoning. The last column records average scores. See Appendix C for a detailed setup.\nModel\nArithmetic\nCommonsense\nSymbolic\nAvg\nAddSub MultiArith SingleEq AQUA-RAT SVAMP GSM8K CSQA\nSTQA\nLetter\nCoin\nZero-shot-CoT\n82.5\n96.0\n90.4\n38.2\n76.5\n57.1\n72.0\n57.6\n71.0\n64.4\n69.9\nManual-CoT\n86.8\n97.0\n90.9\n45.3\n80.2\n75.8\n72.2\n61.7\n78.8\n100\n100\n100\n78.9\nAuto-CoT\n88.5\n88.5\n88.5\n96.0\n90.2\n46.5\n46.5\n46.5\n78.2\n74.1\n72.7\n62.4\n72.6\n100\n100\n100\n78.1\nPrompt-Space-CoT-Zero\n87.3\n98.0\n89.2\n36.2\n82.2\n82.2\n82.2\n72.4\n71.1\n63.3\n63.3\n63.3\n82.0\n82.0\n82.0\n100\n100\n100\n78.3\nPrompt-Space-CoT\n87.9\n98.8\n98.8\n98.8\n92.1\n92.1\n92.1\n40.6\n81.0\n77.9\n77.9\n77.9\n74.1\n74.1\n74.1\n62.5\n79.6\n100\n100\n100\n79.5\n79.5\n79.5\nPrompt-Space-CoT (best) 87.9 (8)\n98.8\n98.8\n98.8(8)\n92.5\n92.5\n92.5(4)\n48.8\n48.8\n48.8(7)\n82.6\n82.6\n82.6(10) 77.9\n77.9\n77.9(8) 77.9\n77.9\n77.9(8)\n64.4\n64.4\n64.4(9)\n84.4\n84.4\n84.4(1) 100\n100\n100(3) 81.3\n81.3\n81.3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/26fb/26fb996f-ec5b-484d-a9ab-ba9c72050169.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Accuracy (%) of Prompt-Space-CoT-Zero with different embedding models on three types of reasoning tasks: arithmetic reasoning (GSM8K, SVAMP), commonsense reasoning (CSQA), and symbolic reasoning (Letter The embedding models are T5 (base/large/XL/XXL), E5 (small/base/large), and MiniLM-L6-v2 (ours), respectively</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f0f3/f0f3d708-15b5-405d-a9c8-dc712a9497b9.png\" style=\"width: 50%;\"></div>\nFigure 4: Accuracy (%) of Prompt-Space-CoT-Zero with various numbers of basis questions on nine reasoning datasets. Table 3: Accuracy (%) of Prompt-Space-CoT-Zero with different question sequences on three types of reasoning tasks: arithmetic reasoning (GSM8K, SVAMP), commonsense reasoning (CSQA), and symbolic reasoning (Letter) Our model ranks the sequence of basis questions by their eigenvalue scores from largest to smallest. Three differen sequences are evaluated: (a) original sequence, (b) reversed sequence, (c) random sequence.\nSequence\nArithmetic\nCommonsense\nSymbolic\nAvg\nSVAMP GSM8K\nCSQA\nLetter\nRandom sequence\n81.4\n70.5\n73.1\n81.40\n76.6\nReversed sequence\n82.1\n70.9\n74.8\n74.8\n74.8\n82.0\n82.0\n82.0\n77.4\n77.4\n77.4\nOriginal sequence (ours)\n82.2\n82.2\n82.2\n72.4\n72.4\n72.4\n72.5\n82.0\n82.0\n82.0\n77.3\n# 5 Conclusion\nIn this paper, we propose a novel prompting method, namely Prompt Space, to explore the selection of prompts for enhancing reasoning in LLMs. For any dataset, Prompt Space can map its questions onto a real space for determining basis questions as a demonstration. Through experiments on arithmetic, commonsense, and symbolic reasoning\ntasks, we find that the demonstrations constructed by Prompt Space can significantly improve the reasoning abilities of LLMs on ten public benchmarks. Furthermore, without the help of the CoT method and the \"Let\u2019s think step by step\" prompt , Prompt Space also exhibits superior performance than fewshot and zero-shot learning in LLMs. Overall, Prompt Space could serve as an efficient tool for\nsolving reasoning tasks, but also has the potential to be a few-shot learner for a wide range of applications and tasks.\n# Limitations and Ethics Statement\nCompared to state-of-the-art methods, Prompt Space shows more competitive performance on three categories of reasoning tasks. Besides, it will significantly increases the capability and robustness of the chain-of-thought method on massive datasets. However, there are some potential limitations to consider. First, the optimal number of basis questions is observed by experimental results. Additionally, the performance of Prompt Space could be influenced by the selections of embedding models. Finally, we use an approximating method to obtain top k basis questions, which could increase the uncertainty of this method. Overall, we will continue to work on this problem to address these limitations and develop more effective and robust prompting methods. For reproducibility, all experiments are run by gpt-35-turbo version of the public ChatGPT model from the OpenAI API with 175 billion parameters. And these baseline methods are open-sourced implementation. To aid reviewing, we summarize the statistic of ten benchmark datasets, and include configures of different embedding models and experimental settings in the supplementary materials.\n# References\nHerv\u00e9 Abdi and Lynne J Williams. 2010. Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4):433\u2013459.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013 361. Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T Mai, Maria Vau, Matthew Caldwell, and Augustine Marvor-Parker. 2023. Susceptibility to influence of large language models. arXiv preprint arXiv:2303.06074. Konstantin Hebenstreit, Robert Praas, Louis P Kiesewetter, and Matthias Samwald. 2023. An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. arXiv preprint arXiv:2305.02897. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pages 523\u2013533. Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, and Carrie J Cai. 2022. Promptmaker: Prompt-based prototyping with large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1\u20138. Zhanming Jie and Wei Lu. 2023. Leveraging training data in few-shot prompting for numerical reasoning. In Findings of the Association for Computational Linguistics: ACL 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585\u2013597.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585\u2013597.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.\nOpenAI. 2023. Gpt-4 technical report.\n# OpenAI. 2023. Gpt-4 technical report.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\nTimo Schick and Hinrich Sch\u00fctze. 2020. It\u2019s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975.\nTian-Xiang Sun, Xiang-Yang Liu, Xi-Peng Qiu, and Xuan-Jing Huang. 2022. Paradigm shift in natural language processing. Machine Intelligence Research, 19(3):169\u2013183.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/76b0/76b0c0ee-e663-45c9-a23d-a23f1d807504.png\" style=\"width: 50%;\"></div>\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671. Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, Dong Yang, Fobo Shi, and Xiaodong Lin. 2023. Edit everything: A text-guided generative system for images editing. arXiv preprint arXiv:2304.14006. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493. Fang Zhao. 2022. Auto-correction dans un analyseur neuronal par transitions : un comportement factice ? (self-correction in a transition-based neural parser : a spurious behaviour ?). In Actes de la 29e Conf\u00e9rence sur le Traitement Automatique des Langues Naturelles. Volume 2 : 24e Rencontres Etudiants Chercheurs en Informatique pour le TAL (RECITAL), pages 20\u201332, Avignon, France. ATALA. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.\n# Appendix\nThe following is the supplementary Appendix for the paper. All the references are made in context of the main paper.\n# A Derivation of the matrix Q\nThe matrices QQT \u2208Rm\u00d7m and QT Q \u2208Rn\u00d7n are diagonalized, which can be rewritten as:\n(A.1)\n# B Algorithm of Prompt Space\nAlgorithm 1 describes the detailed algorithm of the proposed Prompt Space.\n# C Details of Experimental Setup\n# C.1 Datasets\nTable C1 summarizes the basic statistics of ten benchmark datasets.\n# C.2 Demonstrations of Baselines\nFigure C1 shows the illustrations of five baselines, including Zero-shot, Few-shot, Manual-CoT, Zeroshot-CoT, and Auto-CoT. It is clear to see the difference between five baselines in Fig. C1.\n# C.3 Models\nTable C2 shows the configurations of different embedding models.\n# D Complementary Experimental Results D.1 Effect of Basis Questions in Prompt-Space-CoT\n# D.1 Effect of Basis Questions in Prompt-Space-CoT\nWe additionally evaluated Prompt Space w/o CoTZero and Prompt-Space-CoT on ten tasks as the number of basis questions is increased from 0 to 10. The result shows that an appropriate number of basis questions is 8 for half of the arithmetic reasoning tasks (MultiArith, GSM8K, AddSub), while it varies in other arithmetic tasks. For commonsense reasoning tasks, the optimal numbers of basis questions are 6 and 9 on CSQA and STQA, respectively. Importantly, we only use one number of basis questions to achieve best performance on the Letter dataset, and that of basis questions is also small (3) on the Coin dataset. These findings\nAlgorithm 1 The detailed algorithm of the proposed Prompt Space.\nInput: Pre-trained text embedding models E, a question set Q = {qi}m i=1, and a large language model LLM Patameter: A conditional variable CoT-Zero for determining two cases: Prompt-Space-CoTZero and Prompt-Space-CoT (default CoTZero=True), and the initial number of basis questions is k Output: Answers {ai}m i=1 from LLM\n1: Embed each question qi with E to yield vectors qi. Then combine all question embeddings as a matrix Q = [q1, q2, ..., qm]T \u2208Rm\u00d7n. 2: Factorize the matrix Q with SVD: Q = U\u039bVT , where Q \u2208Rm\u00d7n. 3: Find k principal components of Q, i.e., Qk = UkQ, where Uk = [u1, u2, ...uk]T \u2208Rk\u00d7m, and Qk \u2208Rk\u00d7n. 4: Calculate the similarity between basis vectors and question embeddings, and then obtain the indices of the most similar problems, i.e., argmax(Qk, QT ) = [I1, I2, ..., Ik]T , for 1 <= Ij <= m. 5: Construct a demonstration with the following format: qI1, A: Let\u2019s think step by step. ... qI2, A: Let\u2019s think step by step. ... ... qIk, A: Let\u2019s think step by step. ... 6: for each question i = 1, ..., m do 7: if CoT-Zero then 8: Combine the demonstration from STEP 5 with the current test question qi: ... (demonstration in STEP 5) qi A: Let\u2019s think step by step. 9: else 10: Combine the demonstration from STEP 5 with the current test question qi: ... (demonstration in STEP 5) qi A: 11: end if 12: Generate the output answer ai with the prompt obtained from STEP 7 in the language model LLM 13: end for\n<div style=\"text-align: center;\">Table C1: Statistics of ten benchmark datasets.</div>\nDataset\nAnswer format\n# of Samples\nAverage words\n# of basis questions\nLicense\nAddSub\nNumber\n395\n31.5\n8\nUnspecified\nMultiArith\nNumber\n600\n31.8\n8\nUnspecified\nSingleEq\nNumber\n508\n27.4\n8\nNo license\nAQUA-RAT\nMultiple choices\n254\n51.9\n4\nApache-2.0\nSVAMP\nNumber\n1000\n31.8\n8\nMIT license\nGSM8K\nNumber\n1319\n46.9\n8\nMIT license\nCommonsenseQA (CSQA)\nMultiple choices\n1221\n27.8\n7\nUnspecified\nStrategyQA (STQA)\nYes or no\n2290\n9.6\n6\nApache-2.0\nLast Letter (Letter)\nFree format\n500\n15.0\n4\n-\nCoin Flip (Coin)\nYes or no\n500\n37.0\n8\n-\n<div style=\"text-align: center;\">Table C2: Configurations of different embedding models.</div>\nModel\nConfigurations\n# of Layers\nHidden size\n# of Parameters\nMiniLM-L6-v2\n6\n384\n22M\nE5-small\n12\n384\n33M\nE5-base\n12\n768\n110M\nE5-large\n24\n1024\n330M\nSentence-t5-base\n12\n768\n110M\nSentence-t5-large\n24\n768\n336M\nSentence-t5-xl\n24\n768\n1242M\nSentence-t5-xxl\n24\n768\n4866M\nindicate that our Prompt-Space-CoT needs a few basis questions to get the best performance on symbolic reasoning tasks, which dramatically reduces the cost of exemplar constructions in LLMs. For a fair comparison, we don\u2019t show the best results with optimal basis questions, while showing the results with the same number of exemplars in Table 2. Overall, these results further demonstrate that the existence of Prompt Space is significant for improving the reasoning abilities of LLMs, and reducing the cost of exemplar constructions.\n# D.2 Visualization of Prompt Space\nFigs. D2 visualizes Prompt Space via PCA projection on ten different datasets, namely AddSub, MultiArith, SingleEq, SVAMP, AQUA-RAT, GSM8K, CSQA, STQA, Letter, and Coin. The visualization demonstrates that there exists a real space including basis vectors (questions) on each dataset. The space on each dataset may be represented by different basis questions. From Figs. D2, basis questions are located on three base planes in the three-dimensional space.\n# D.3 Comparisons among Complex CoT, Dynamic Program Prompting, and Prompt Space.\nTable D5 shows the comparisons between baselines and Prompt Space on MultiArith, SVAMP, and GSM8K datasets.\n# E Demonstrations of Each Datasets\nThis section shows the specific demonstrations of each datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3083/3083501e-d5ea-4cc7-b9c2-fa8ce37e4030.png\" style=\"width: 50%;\"></div>\n# of Basis\nArithmetic\nCommonsense\nSymbolic\nAvg\nAddSub MultiArith SingleEq AQUA-RAT SVAMP GSM8K CSQA\nSTQA\nLetter Coin\ngpt-3.5-turbo-0301\n1\n87.1\n69.7\n84.1\n26.0\n70.8\n19.0\n77.4\n46.0\n3.2\n15.4\n49.9\n2\n87.1\n82.5\n86.8\n29.5\n70.4\n24.8\n77.7\n45.1\n2.8\n57.2\n56.4\n3\n89.1\n84.3\n88.4\n32.3\n74.2\n23.2\n78.2\n52.8\n4.2\n41.8\n56.9\n4\n89.4\n83.0\n88.8\n31.9\n74.5\n25.7\n78.7\n55.5\n3.4\n30.6\n56.1\n5\n89.1\n83.8\n88.4\n30.7\n75.4\n24.9\n78.7\n56.2\n4.4\n51.8\n58.3\n6\n89.6\n83.7\n88.2\n31.9\n75.6\n25.9\n79.0\n61.3\n4.2\n63.8\n60.3\n7\n88.6\n84.5\n88.6\n29.9\n75.6\n25.4\n79.1\n60.3\n4.0\n59.0\n59.5\n8\n88.4\n83.7\n88.8\n30.7\n75.5\n25.2\n80.0\n62.0\n3.8\n61.2\n59.8\n9\n89.9\n86.3\n88.4\n30.3\n74.7\n25.8\n79.9\n61.4\n5.2\n60.2\n60.2\n10\n89.6\n85.8\n88.2\n29.9\n75.0\n25.5\n79.0\n62.8\n4.2\n60.6\n60.1\nBest results\n89.9\n86.3\n88.8\n32.3\n75.6\n25.9\n80.0\n62.8\n5.2\n63.8\n61.1\ngpt-3.5-turbo-0125\n1\n81.4\n86.7\n88.5\n53.7\n80.8\n73.4\n72.1\n44.4\n53.1\n95.3\n72.9\n2\n84.4\n95.2\n92.5\n53.3\n80.3\n76.8\n73.7\n55.9\n68.3\n100.0 78.0\n3\n84.3\n94.8\n93.4\n54.7\n79.8\n77.4\n74.4\n58.1\n75.3\n93.9\n78.6\n4\n83.7\n95.1\n92.1\n55.1\n81.4\n78.4\n71.8\n60.9\n74.5\n100.0 79.3\n5\n83.5\n95.2\n93.5\n54.7\n81.8\n78.4\n71.5\n60.1\n76.6\n100.0 79.5\n6\n83.8\n94.7\n94.2\n55.6\n82.5\n77.5\n72.3\n60.7\n74.9\n97.0\n79.3\n7\n83.5\n94.0\n94.2\n54.5\n82.5\n78.2\n72.5\n61.8\n73.7\n97.0\n79.2\n8\n84.1\n94.4\n93.5\n52.0\n82.5\n77.6\n72.6\n60.9\n76.9\n97.8\n79.2\n9\n84.0\n95.6\n93.2\n53.7\n83.4\n76.9\n72.1\n62.2\n76.4\n92.0\n78.9\n10\n84.1\n95.1\n94.0\n50.1\n83.6\n76.8\n71.7\n62.9\n76.9\n78.7\n77.4\nBest results\n84.4\n95.6\n94.2\n55.6\n83.6\n78.4\n74.4\n62.9\n76.9\n100.0 80.6\nTable D4: Accuracy (%) comparison of Prompt-Space-CoT with different amounts of basis questions on ten reasoning datasets. Ten benchmark datasets contain three categories, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning.\n# of Basis\nArithmetic\nCommonsense\nSymbolic\nAvg\nAddSub MultiArith SingleEq AQUA-RAT SVAMP GSM8K CSQA\nSTQA\nLetter Coin\ngpt-3.5-turbo-0301\n1\n39.7\n58.8\n41.7\n15.0\n34.3\n24.7\n72.2\n44.6\n84.4\n61.8\n47.7\n2\n80.0\n94.3\n91.9\n32.3\n35.6\n69.3\n74.4\n59.7\n73.0\n88.0\n69.9\n3\n85.1\n96.2\n92.3\n36.2\n77.1\n71.1\n74.8\n61.7\n76.0\n100.0 77.1\n4\n86.6\n96.8\n92.5\n40.6\n80.4\n75.7\n74.0\n61.9\n74.2\n99.6\n78.2\n5\n86.1\n96.8\n92.5\n43.3\n81.7\n76.1\n72.5\n61.6\n75.6\n100.0 78.6\n6\n86.6\n97.3\n91.7\n39.8\n82.5\n75.6\n75.6\n62.5\n74.4\n97.8\n78.4\n7\n86.1\n98.0\n91.9\n48.8\n82.1\n76.6\n74.1\n63.6\n74.0\n84.2\n78.0\n8\n87.9\n98.8\n92.1\n47.6\n81.0\n77.9\n74.1\n62.5\n79.6\n100.0 80.2\n9\n87.8\n97.5\n92.1\n45.3\n82.1\n76.6\n74.3\n64.4\n76.2\n99.4\n79.6\n10\n87.6\n97.8\n91.5\n45.3\n82.6\n76.9\n74.4\n64.4\n77.6\n99.0\n79.7\nBest results\n87.9\n98.8\n92.5\n48.8\n82.6\n77.9\n75.6\n64.4\n84.4\n100.0 81.3\ngpt-3.5-turbo-0125\n1\n84.4\n90.1\n90.7\n53.5\n81.3\n73.7\n74.4\n36.1\n70.3\n96.5\n75.1\n2\n85.8\n97.0\n92.6\n56.3\n80.5\n79.8\n76.3\n62.5\n67.6\n99.0\n79.7\n3\n85.7\n96.3\n92.5\n52.2\n79.6\n79.8\n76.9\n64.8\n75.1\n100.0 80.3\n4\n85.1\n95.6\n91.7\n53.8\n80.3\n80.7\n74.6\n66.1\n73.3\n100.0 80.1\n5\n86.3\n95.6\n91.9\n57.2\n81.4\n79.9\n74.4\n63.3\n74.3\n99.9\n80.4\n6\n85.7\n95.2\n92.7\n55.9\n82.2\n79.6\n75.6\n61.7\n73.3\n99.9\n80.2\n7\n86.8\n95.4\n92.5\n56.0\n80.5\n79.5\n72.5\n67.9\n72.1\n99.9\n80.3\n8\n87.5\n95.2\n91.4\n54.6\n82.2\n80.2\n72.9\n65.6\n74.9\n99.9\n80.4\n9\n87.7\n95.9\n92.8\n50.7\n83.1\n79.6\n74.2\n66.8\n75.2\n99.7\n80.6\n10\n87.3\n96.2\n93.1\n53.8\n82.8\n79.1\n72.0\n68.0\n76.1\n100.0 80.8\nBest results\n87.7\n97.0\n93.1\n57.2\n83.1\n80.7\n76.9\n68.0\n76.1\n100.0 82.0\nTable D5: Enhanced comparisons between baselines and Prompt Space across MultiArith, Svamp, and GSM8K\nDatasets\nModel\nMultiArith\nSVAMP\nGSM8K\ntext-davinci-002\nComplex CoT (Fu et al., 2023)\n94.2\n-\n55.0\ncode-davinci-002\nManual-CoT (Wei et al., 2022)\n91.7\n68.9\n46.9\nAuto-CoT (Zhao, 2022)\n92.0\n69.5\n47.9\nComplex CoT\n95.8\n-\n66.6\nDynamic Program Prompting (Jie and Lu, 2023)\n-\n80.3\n76.6\ngpt-3.5-turbo-0301\nManual-CoT\n97.0\n80.2\n75.8\nAuto-CoT\n96.0\n78.2\n74.1\nComplex CoT(Random Sampling)\n-\n-\n77.1\nComplex CoT(Dialog In-Context Learning)\n-\n-\n74.9\nComplex CoT(Greedy Decoding)\n-\n-\n78.9\nPrompt-Space-CoT\n98.8\n82.6\n77.9\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bffd/bffd806c-1e7a-400c-965a-3c4df4d44eb8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Prompt Demonstrations on Last Letter</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f91d/f91d1849-5446-4532-a42b-4f995f46832b.png\" style=\"width: 50%;\"></div>\nFigure E3: The demonstrations of Last Letter on difference methods, including Random Auto-CoT, and Our Prompt Space.\nQ: As part of a lesson on earthquakes , a science class is researching the movement of a nearby fault line . The fault line moved 1.25 inches during the past year and 5.25 inches the year before . How far did the fault line move in all ? A: Let\u2019s think step by step. The fault line moved 1.25 inches during the past year and 5.25 inches the year before. That means it moved a total of 6.5 inches in two years. Q: There are 5 oak trees currently in the park . Park workers will plant 4 oak trees today . How many oak trees will the park have when the workers are finished ? A: Let\u2019s think step by step. There are 5 oak trees currently in the park. Park workers will plant 4 oak trees today. This means that when the workers are finished, there will be a total of 9 oak trees in the park. Q: There is 0.16666666666666666 cup of oil in Scarlett \u2019s measuring cup . If Scarlett adds 0.6666666666666666 cup more , how much oil will be in the measuring cup ? A: Let\u2019s think step by step. There is currently 0.16666666666666666 cups of oil in the measuring cup. If Scarlett adds 0.6666666666666666 more cups, that means she is adding (0.6666666666666666 + 0.16666666666666666) cups of oil, which equals 0.8333333333333334 cups of oil. Q: Fred picked 36 limes , Alyssa picked 32 limes , and Nancy picked 35 limes and 18 pears , at the farm . How many limes were picked in total ? A: Let\u2019s think step by step. First, we need to find out how many limes were picked in total. We can do this by adding up the number of limes that each person picked. 36 + 32 + 35 = 103 So, 103 limes were picked in total. Q: As part of a lesson on earthquakes , a science class is researching the movement of a nearby fault line . The fault line moved 1.25 inches during the past year and 5.25 inches the year before . How far did the fault line move in all ? A: Let\u2019s think step by step. The fault line moved 1.25 inches during the past year and 5.25 inches the year before. That means it moved a total of 6.5 inches in two years. Q: There are 41 short trees and 44 tall trees currently in the park . Park workers will plant 57 short trees today . How many short trees will the park have when the workers are finished ? A: Let\u2019s think step by step. There are 41 short trees and 44 tall trees currently in the park. Park workers will plant 57 short trees today. This means that when the workers are finished, there will be 41 + 57 = 98 short trees in the park. Q: Alyssa picked 17 plums and Jason picked 10 plums . Melanie picked 35 pears . How many plums were picked in all ? A: Let\u2019s think step by step. Alyssa picked 17 plums. Jason picked 10 plums. That means that, in total, 27 plums were picked. Q: Greg and Sharon own neighboring cornfields . Greg harvested 0.4 acre of corn on Monday and Sharon harvested 0.1 acre . How many more acres did Greg harvest than Sharon ? A: Let\u2019s think step by step. Greg harvested 0.4 acre of corn on Monday. That means Sharon harvested 0.1 acre of corn on Monday. That means Greg harvested 0.3 more acres of corn than Sharon.\nQ: A group of 3 friends went into a restaurant. The chef already had 6 chicken wings cooked but cooked 3 more for the group. If they each got the same amount how many would each person get? A: Let\u2019s think step by step. There are a total of 9 chicken wings. Each person would get 3 chicken wings. Q: In a video game, each enemy defeated gives you 9 points. If a level has 11 enemies total and you destroy all but 3 of them, how many points would you earn? A: Let\u2019s think step by step. Each enemy defeated gives you 9 points. If a level has 11 enemies total and you destroy all but 3 of them, you would have destroyed 8 enemies. 8 times 9 equals 72 points. Q: Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the rest into 9 different albums. How many pictures were in each album? A: Let\u2019s think step by step. First, we know that Wendy uploaded 45 pictures in total. Second, we know that Wendy put 27 pictures into one album. That means that Wendy put the remaining 18 pictures into 9 different albums. That means that each album would have 2 pictures. Q: Wendy was playing a video game where she scores 5 points for each treasure she finds. If she found 4 treasures on the first level and 3 on the second, what would her score be? A: Let\u2019s think step by step. On the first level, Wendy scores 5 points for each treasure, so she scores 20 points in total. On the second level, Wendy scores 5 points for each treasure, so she scores 15 points in total. To find Wendy\u2019s total score, we need to add the points she scored on each level together. So we take 20 + 15 and we get 35. Q: Faye was playing a video game where she scores 7 points for each treasure she finds. If she found 2 treasures on the first level and 6 on the second, what would her score be? A: Let\u2019s think step by step. On the first level, Faye found 2 treasures. So her score for the first level is 7*2=14. On the second level, Faye found 6 treasures. So her score for the second level is 7*6=42. To find her total score, we just need to add up her scores for each level. So her total score is 14+42=56. Q: A waiter had 9 tables he was waiting on, with 4 women and 3 men at each table. How many customers total did the waiter have? A: Let\u2019s think step by step. There are 9 tables. Each table has 4 women and 3 men. That means there are 36 women and 27 men in total. Therefore, the waiter had 63 customers in total. Q: While shopping for music online, Nancy bought 3 country albums and 5 pop albums. Each album came with a lyric sheet and had 8 songs. How many songs did Nancy buy total? A: Let\u2019s think step by step. Nancy bought 3 country albums. Each album has 8 songs. So she bought 24 songs from the country albums. Nancy bought 5 pop albums. Each album has 8 songs. So she bought 40 songs from the pop albums. Nancy bought a total of 24 + 40 = 64 songs. Q: In a video game, each enemy defeated gives you 9 points. If a level has 11 enemies total and you destroy all but 3 of them, how many points would you earn? A: Let\u2019s think step by step. Each enemy defeated gives you 9 points. If a level has 11 enemies total and you destroy all but 3 of them, you would have destroyed 8 enemies. 8 times 9 equals 72 points.\nQ: Mary, Sam, Keith, and Alyssa each have 6 baseball cards. How many baseball cards do they have in all ? A: Let\u2019s think step by step. There are four people, so we need to add up all of their baseball cards. Mary has 6 baseball cards, Sam has 6 baseball cards, Keith has 6 baseball cards, and Alyssa has 6 baseball cards. 6 + 6 + 6 + 6 = 24 So, they have 24 baseball cards in all. Q: As part of a lesson on earthquakes, a science class is researching the movement of a nearby fault line. The fault line moved 1.25 inches during the past year and 5.25 inches the year before. How far did the fault line move in all? A: Let\u2019s think step by step. The fault line moved 1.25 inches during the past year. The fault line moved 5.25 inches the year before. To find the total movement of the fault line, we need to add 1.25 + 5.25. The fault line moved 6.5 inches in all. Q: It rained 0.2 inches on Saturday and 0.4 inches on Sunday. How much did it rain on Saturday and Sunday combined? A: Let\u2019s think step by step. The first thing we need to do is add the two numbers together. This will give us the total amount of rain that fell over the two days. 0.2 + 0.4 = 0.6 Therefore, it rained a total of 0.6 inches on Saturday and Sunday combined. Q: Sandy has 8 dozen books. Fred has 5 times more books than Sandy. How many books does Fred have ? A: Let\u2019s think step by step. Sandy has 8 dozen books. This is 96 books. Fred has 5 times more books than Sandy. This means that Fred has 5 times 96 books, or 480 books. Q: As part of a lesson on earthquakes, a science class is researching the movement of a nearby fault line. The fault line moved 1.25 inches during the past year and 5.25 inches the year before. How far did the fault line move in all? A: Let\u2019s think step by step. The fault line moved 1.25 inches during the past year. The fault line moved 5.25 inches the year before. To find the total movement of the fault line, we need to add 1.25 + 5.25. The fault line moved 6.5 inches in all. Q: A DVD book holds 126 DVDs. There are 81 DVDs already in the book. How many more DVDs can be put in the book? A: Let\u2019s think step by step. There are already 81 DVDs in the book. That means there are 45 DVDs that can still fit in the book. Q: Sally bought 4 dozen eggs from the grocery store to bake some cakes. How many eggs did Sally buy ? A: Let\u2019s think step by step. 1 dozen = 12 eggs 4 dozen = 4 x 12 = 48 eggs So Sally bought 48 eggs from the grocery store. Q: Bert runs 2 miles every day. How many miles will Bert run in 3 weeks? A: Let\u2019s think step by step. In one week, Bert will run 14 miles. In two weeks, Bert will run 28 miles. In three weeks, Bert will run 42 miles.\nQ: 30 is subtracted from a number, it is reduced to its one third. What is the value of 50 A: Let\u2019s think step by step. First, we know that if 30 is subtracted from a number, it is reduced to one third of its original value. Next, we want to find 50% of that number. To do this, we can take one third of the number and multiply it by 2. Q: If Jill needed to buy 10 bottles of soda for a party in which 8 people attended, how many bottles of soda will she need to buy for a party in which 12 people are attending? Answer Choices: (A) 6 (B) 8 (C) 10 (D) 12 (E) 14 A: Let\u2019s think step by step. For 8 people, Jill needs 10 bottles of soda. For 12 people, Jill needs ? bottles of soda. Since Jill needs 2.5 bottles of soda per person, she will need 2.5 * 12 = 30 bottles of soda for 12 people. The answer is (E). Q: A computer routine was developed to generate two numbers (x,y) the first being a random number between 0 and 100 inclusive, and the second being less than or equal to the square root of the first. Each of the following pair satisfies the routine except Answer Choices: (A) (99,10) (B) (85,9) (C) (50,7) (D) (1,1) (E) (1,0) A: Let\u2019s think step by step. The first number, x, is a random number between 0 and 100 inclusive. The second number, y, is less than or equal to the square root of the first. That means that y can never be greater than 10. Therefore, (99,10) does not satisfy the routine. Q: If a man rows at the rate of 4 kmph in still water and his rate against the current is 2 kmph, then the man\u2019s rate along the current is: Answer Choices: (A) 15 kmph (B) 6 kmph (C) 12 kmph (D) 14 kmph (E) 6 kmph A: Let\u2019s think step by step. The man\u2019s rate in still water is 4 kmph. The man\u2019s rate against the current is 2 kmph. This means that the man\u2019s rate along the current must be faster than his rate in still water, but slower than his rate against the current. The answer must be between 4 kmph and 2 kmph. The answer must be 6 kmph.\nQ: 30 is subtracted from a number, it is reduced to its one third. What is the value of 50 A: Let\u2019s think step by step. First, we know that if 30 is subtracted from a number, it is reduced to one third of its original value. Next, we want to find 50% of that number. To do this, we can take one third of the number and multiply it by 2. Q: If Jill needed to buy 10 bottles of soda for a party in which 8 people attended, how many bottles of soda will she need to buy for a party in which 12 people are attending? Answer Choices: (A) 6 (B) 8 (C) 10 (D) 12 (E) 14 A: Let\u2019s think step by step. For 8 people, Jill needs 10 bottles of soda. For 12 people, Jill needs ? bottles of soda. Since Jill needs 2.5 bottles of soda per person, she will need 2.5 * 12 = 30 bottles of soda for 12 people. The answer is (E). Q: A computer routine was developed to generate two numbers (x,y) the first being a random number between 0 and 100 inclusive, and the second being less than or equal to the square root of the first. Each of the following pair satisfies the routine except Answer Choices: (A) (99,10) (B) (85,9) (C) (50,7) (D) (1,1) (E) (1,0) A: Let\u2019s think step by step. The first number, x, is a random number between 0 and 100 inclusive. The second number, y, is less than or equal to the square root of the first. That means that y can never be greater than 10. Therefore, (99,10) does not satisfy the routine. Q: If a man rows at the rate of 4 kmph in still water and his rate against the current is 2 kmph, then the man\u2019s rate along the current is: Answer Choices: (A) 15 kmph (B) 6 kmph (C) 12 kmph (D) 14 kmph (E) 6 kmph A: Let\u2019s think step by step. The man\u2019s rate in still water is 4 kmph. The man\u2019s rate against the current is 2 kmph. This means that the man\u2019s rate along the current must be faster than his rate in still water, but slower than his rate against the current. The answer must be between 4 kmph and 2 kmph. The answer must be 6 kmph.\nQ: During summer break 61619 kids from Lawrence county stayed home and the rest went to camp. The total number of kids in Lawrence county is 91676. About how many kids in Lawrence county went to camp? A: Let\u2019s think step by step. There are 91676 kids in Lawrence county. 61619 kids stayed home. That means that the rest, 91676-61619, went to camp. So about 30057 kids in Lawrence county went to camp. Q: During summer break 61619 kids from Lawrence county stayed home and the rest went to camp. The total number of kids in Lawrence county is 91676. About how many kids in Lawrence county went to camp? A: Let\u2019s think step by step. There are 91676 kids in Lawrence county. 61619 kids stayed home. That means that the rest, 91676-61619, went to camp. So about 30057 kids in Lawrence county went to camp. Q: Lewis earns $ 21 every week during the 216 weeks of harvest. If he has to pay $ 702 tax How much money will have at the end of the harvest season? A: Let\u2019s think step by step. First, let\u2019s calculate how much money he earns in total. He earns $21 per week, and there are 216 weeks in the harvest season. So he earns $21 * 216 = $4536 in total. Then, we need to calculate how much tax he needs to pay. He needs to pay $702 in tax. So the final answer is $4536 - $702 = $3834. Q: The grasshopper, the frog and the mouse had a jumping contest. The grasshopper jumped 25 inches. The frog jumped 18 inches farther than the grasshopper and the mouse jumped 2 inches farther than the frog. How far did the mouse jump? A: Let\u2019s think step by step. The grasshopper jumped 25 inches. The frog jumped 18 inches farther than the grasshopper. This means that the frog jumped a total of (25 + 18) = 43 inches. The mouse jumped 2 inches farther than the frog. This means that the mouse jumped a total of (43 + 2) = 45 inches. Therefore, the mouse jumped 45 inches. Q: Winter is almost here and most animals are migrating to warmer countries. There are 3 bird families living near the mountain. 26 new bird families came to live near the mountain from the arctic while 2 bird families flew away further south for winter. How many bird families were left near the mountain? A: Let\u2019s think step by step. There are 3 bird families living near the mountain. 26 new bird families came to live near the mountain from the arctic. 2 bird families flew away further south for winter. 3 + 26 - 2 = 27 bird families were left near the mountain. Q: Danny has 12 bottle caps in his collection. He found 53 bottle caps at the park. How many bottle caps does he have now? A: Let\u2019s think step by step. Danny has 12 bottle caps in his collection. He found 53 bottle caps at the park. How many bottle caps does he have now? He would have 12 + 53 = 65 bottle caps now. Q: The grasshopper and the frog had a jumping contest. The grasshopper jumped 13 inches. The grasshopper jumped 2 inches farther than the grasshopper. How far did the frog jump? A: Let\u2019s think step by step. The grasshopper jumped 13 inches. The grasshopper jumped 2 inches farther than the grasshopper. So the frog jumped 15 inches. Q: Lewis earns $ 21 every week during the 216 weeks of harvest. If he has to pay $ 702 tax How much money will have at the end of the harvest season? A: Let\u2019s think step by step. First, let\u2019s calculate how much money he earns in total. He earns $21 per week, and there are 216 weeks in the harvest season. So he earns $21 * 216 = $4536 in total. Then, we need to calculate how much tax he needs to pay. He needs to pay $702 in tax. So the final answer is $4536 - $702 = $3834.\nQ: During summer break 61619 kids from Lawrence county stayed home and the rest went to camp. The total number of kids in Lawrence county is 91676. About how many kids in Lawrence county went to camp? A: Let\u2019s think step by step. There are 91676 kids in Lawrence county. 61619 kids stayed home. That means that the rest, 91676-61619, went to camp. So about 30057 kids in Lawrence county went to camp. Q: During summer break 61619 kids from Lawrence county stayed home and the rest went to camp. The total number of kids in Lawrence county is 91676. About how many kids in Lawrence county went to camp? A: Let\u2019s think step by step. There are 91676 kids in Lawrence county. 61619 kids stayed home. That means that the rest, 91676-61619, went to camp. So about 30057 kids in Lawrence county went to camp. Q: Lewis earns $ 21 every week during the 216 weeks of harvest. If he has to pay $ 702 tax How much money will have at the end of the harvest season? A: Let\u2019s think step by step. First, let\u2019s calculate how much money he earns in total. He earns $21 per week, and there are 216 weeks in the harvest season. So he earns $21 * 216 = $4536 in total. Then, we need to calculate how much tax he needs to pay. He needs to pay $702 in tax. So the final answer is $4536 - $702 = $3834. Q: The grasshopper, the frog and the mouse had a jumping contest. The grasshopper jumped 25 inches. The frog jumped 18 inches farther than the grasshopper and the mouse jumped 2 inches farther than the frog. How far did the mouse jump? A: Let\u2019s think step by step. The grasshopper jumped 25 inches. The frog jumped 18 inches farther than the grasshopper. This means that the frog jumped a total of (25 + 18) = 43 inches. The mouse jumped 2 inches farther than the frog. This means that the mouse jumped a total of (43 + 2) = 45 inches. Therefore, the mouse jumped 45 inches. Q: Winter is almost here and most animals are migrating to warmer countries. There are 3 bird families living near the mountain. 26 new bird families came to live near the mountain from the arctic while 2 bird families flew away further south for winter. How many bird families were left near the mountain? A: Let\u2019s think step by step. There are 3 bird families living near the mountain. 26 new bird families came to live near the mountain from the arctic. 2 bird families flew away further south for winter. 3 + 26 - 2 = 27 bird families were left near the mountain. Q: Danny has 12 bottle caps in his collection. He found 53 bottle caps at the park. How many bottle caps does he have now? A: Let\u2019s think step by step. Danny has 12 bottle caps in his collection. He found 53 bottle caps at the park. How many bottle caps does he have now? He would have 12 + 53 = 65 bottle caps now. Q: The grasshopper and the frog had a jumping contest. The grasshopper jumped 13 inches. The grasshopper jumped 2 inches farther than the grasshopper. How far did the frog jump? A: Let\u2019s think step by step. The grasshopper jumped 13 inches. The grasshopper jumped 2 inches farther than the grasshopper. So the frog jumped 15 inches. Q: Lewis earns $ 21 every week during the 216 weeks of harvest. If he has to pay $ 702 tax How much money will have at the end of the harvest season? A: Let\u2019s think step by step. First, let\u2019s calculate how much money he earns in total. He earns $21 per week, and there are 216 weeks in the harvest season. So he earns $21 * 216 = $4536 in total. Then, we need to calculate how much tax he needs to pay. He needs to pay $702 in tax. So the final answer is $4536 - $702 = $3834.\nQ: A marketing company pays its employees on a commission-based salary system. If you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an additional 10% commission. Calculate the amount of money Antonella earned if she sold goods worth $2500. A: Let\u2019s think step by step. Antonella earned a 30% commission on the first $1000, so she earned $300. On the remaining $1500, she earned 10% commission. So she earned an additional $150. In total, she earned $450. Q: John buys 2 pairs of shoes for each of his 3 children. They cost $60 each. How much did he pay? A: Let\u2019s think step by step. John buys 2 pairs of shoes for each of his 3 children. That means he buys 6 pairs of shoes in total. Each pair of shoes costs $60. That means the total cost is 6 x $60 = $360. Q: A marketing company pays its employees on a commission-based salary system. If you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an additional 10% commission. Calculate the amount of money Antonella earned if she sold goods worth $2500. A: Let\u2019s think step by step. Antonella earned a 30% commission on the first $1000, so she earned $300. On the remaining $1500, she earned 10% commission. So she earned an additional $150. In total, she earned $450. Q: Daisy bought a bag of potatoes that weighed 5 pounds. She also bought a bag of sweet potatoes that weighed 2 times as much as the potatoes and a bag of carrots that weighed 3 pounds fewer than the sweet potatoes. How many pounds of carrots did Daisy buy? A: Let\u2019s think step by step. The bag of potatoes weighed 5 pounds. The bag of sweet potatoes weighed 2 times as much as the potatoes. So the sweet potatoes weighed 10 pounds. The bag of carrots weighed 3 pounds fewer than the sweet potatoes. So the carrots weighed 7 pounds. Q: A marketing company pays its employees on a commission-based salary system. If you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an additional 10% commission. Calculate the amount of money Antonella earned if she sold goods worth $2500. A: Let\u2019s think step by step. Antonella earned a 30% commission on the first $1000, so she earned $300. On the remaining $1500, she earned 10% commission. So she earned an additional $150. In total, she earned $450. Q: After scoring 14 points, Erin now has three times more points than Sara, who scored 8. How many points did Erin have before? A: Let\u2019s think step by step. Erin now has 3 times more points than Sara, who scored 8. That means that if we take away 8 points from Erin, she will still have 3 times more points than Sara. That means that Erin had 24 points before. Q: Dijana and Anis live near a lake, and every weekend they go out rowing into the lake. On a Sunday morning, both went out rowing, and Dijana rowed for 50 miles the whole day. Anis rowed 1/5 times more miles than Dijana. Calculate the total distance the two of them rowed on that day. A: Let\u2019s think step by step. Dijana rowed 50 miles, so Anis rowed 1/5 more than that, which is 10 miles. So the total distance they rowed is 60 miles. Q: Kyle bought last year\u2019s best-selling book for $19.50. This is with a 25% discount from the original price. What was the original price of the book? A: Let\u2019s think step by step. The original price of the book was $19.50 with a 25% discount. This means that the original price was $26. The original price was $26.\nQ: A marketing company pays its employees on a commission-based salary system. If you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an additional 10% commission. Calculate the amount of money Antonella earned if she sold goods worth $2500. A: Let\u2019s think step by step. Antonella earned a 30% commission on the first $1000, so she earned $300. On the remaining $1500, she earned 10% commission. So she earned an additional $150. In total, she earned $450. Q: John buys 2 pairs of shoes for each of his 3 children. They cost $60 each. How much did he pay? A: Let\u2019s think step by step. John buys 2 pairs of shoes for each of his 3 children. That means he buys 6 pairs of shoes in total. Each pair of shoes costs $60. That means the total cost is 6 x $60 = $360. Q",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of optimizing prompt engineering for large language models (LLMs), highlighting the limitations of current methods and the necessity for a robust mathematical framework to determine optimal prompts.",
        "problem": {
            "definition": "The problem defined in this paper is the lack of a solid mathematical solution for determining optimal prompts in prompt engineering for LLMs, which affects their performance on reasoning tasks.",
            "key obstacle": "The core obstacle preventing existing methods from effectively solving this problem is the manual design involved in creating prompts, which is time-consuming and often suboptimal."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea stems from the observed limitations of existing prompting methods and the need for a systematic approach to enhance reasoning capabilities in LLMs.",
            "opinion": "The proposed idea, called Prompt Space, involves using text embeddings and matrix decomposition to create a mathematical framework for generating optimal prompts, thereby improving reasoning tasks.",
            "innovation": "The key innovation of Prompt Space lies in its ability to automatically generate prompts from basis questions derived through matrix decomposition, which significantly enhances the few-shot reasoning capabilities of LLMs compared to existing paradigms."
        },
        "method": {
            "method name": "Prompt Space",
            "method abbreviation": "PS",
            "method definition": "Prompt Space is a method that utilizes text embeddings and matrix decomposition to construct a space representing all prompts, enabling the selection of optimal prompts for reasoning tasks.",
            "method description": "The core of the method involves embedding questions to identify basis vectors, which are then used to create prompt exemplars for LLMs.",
            "method steps": [
                "1. Embed the question set into a matrix of question embeddings.",
                "2. Use Singular Value Decomposition (SVD) to find basis vectors.",
                "3. Select basis questions to form the prompt exemplar.",
                "4. Combine basis questions with the test question to generate the final output."
            ],
            "principle": "The effectiveness of Prompt Space in solving the problem is supported by its mathematical foundation, which allows for the systematic selection of prompts based on their relationship to the reasoning tasks."
        },
        "experiments": {
            "evaluation setting": "Prompt Space was evaluated on ten standard datasets across three categories of reasoning tasks: arithmetic, commonsense, and symbolic reasoning.",
            "evaluation method": "The performance of Prompt Space was assessed by comparing its results with several baseline methods using consistent in-context exemplars and a constant seed across all datasets."
        },
        "conclusion": "The experiments demonstrated that Prompt Space significantly improves the reasoning abilities of LLMs across various benchmarks, outperforming state-of-the-art methods even without the assistance of traditional prompting techniques.",
        "discussion": {
            "advantage": "The primary advantages of Prompt Space include its ability to systematically generate effective prompts, its mathematical robustness, and its applicability to a wide range of reasoning tasks.",
            "limitation": "A notable limitation of the method is the dependency on the selection of embedding models, which can influence performance, and the challenge of determining the optimal number of basis questions for each dataset.",
            "future work": "Future research should focus on refining the approach to automatically determine the optimal number of basis questions and exploring additional applications of Prompt Space in various few-shot learning scenarios."
        },
        "other info": {
            "code availability": "The code for Prompt Space is publicly available at https://github.com/YouBLEI/Prompt-Space.",
            "date": "28 Mar 2024"
        }
    },
    "mount_outline": [
        {
            "section number": "1.4",
            "key information": "This paper addresses the issue of optimizing prompt engineering for large language models (LLMs), highlighting the limitations of current methods and the necessity for a robust mathematical framework to determine optimal prompts."
        },
        {
            "section number": "4",
            "key information": "The proposed idea, called Prompt Space, involves using text embeddings and matrix decomposition to create a mathematical framework for generating optimal prompts, thereby improving reasoning tasks."
        },
        {
            "section number": "4.1",
            "key information": "The primary advantages of Prompt Space include its ability to systematically generate effective prompts, its mathematical robustness, and its applicability to a wide range of reasoning tasks."
        },
        {
            "section number": "4.2",
            "key information": "A notable limitation of the method is the dependency on the selection of embedding models, which can influence performance, and the challenge of determining the optimal number of basis questions for each dataset."
        },
        {
            "section number": "3.3",
            "key information": "The core of the method involves embedding questions to identify basis vectors, which are then used to create prompt exemplars for LLMs."
        },
        {
            "section number": "6.1",
            "key information": "The problem defined in this paper is the lack of a solid mathematical solution for determining optimal prompts in prompt engineering for LLMs, which affects their performance on reasoning tasks."
        }
    ],
    "similarity_score": 0.6998156971412961,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models.json"
}