{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.13205",
    "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning",
    "abstract": "The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, fewshot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single \u201dbest\u201d prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.",
    "bib_name": "li2023practicalsurveyzeroshotprompt",
    "md_text": "# A Practical Survey on Zero-shot Prompt Design for In-context Learning\nYinheng Li Columbia University / New York City li.yinheng@columbia.edu\n# Abstract\nThe remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, fewshot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single \u201dbest\u201d prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.\narXiv:2309.13205v1\n# 1 Introduction\nIn recent years, transformer-based language models (such as (Raffel et al., 2019), (Lewis et al., 2019), (Brown et al., 2020), (Devlin et al., 2018)) have emerged as a transformative force in the field of artificial intelligence, revolutionizing Natural Language Understanding(NLU) and Generation(NLG). As model size and training data have evolved, the GPT series has exhibited extraordinary capabilities in a wide range of natural language tasks by relying on a paradigm known as in-context learning. According to (Brown et al., 2020), in-context learning harnesses the context provided by input data to generate appropriate responses or predictions, contrasting with traditional methods that necessitate\nexplicit task-specific training and fine-tuning on labeled datasets. In-context learning enables large language models to capitalize on vast amounts of data and adapt to various tasks in a flexible and dynamic manner. There are several categories of in-context learning, including zero-shot, one-shot, and few-shot learning. In all types of in-context learning, the key to success lies in effective prompt design, which is occasionally referred to as an \u201dart.\u201d This survey paper aims to categorize each type of in-context learning, discuss the core principles, examine state-of-the-art design techniques, and explore recent advancements in in-context learning, with a particular focus on zero-shot discrete incontext learning.\n# 2 Definition\nAlthough there is no formal definition for prompt design optimization, we follow the principle from (Brown et al., 2020) and provide the definition in (1) for prompt design in in-context learning:\nP \u22c6= arg max P Exi,yi\u2208D[S(f\u03b8(P, xi), yi)]\nP \u22c6= arg max P Exi,yi\u2208D[S(f\u03b8(P, xi), yi)] (1\n(1)\nHere, xi represents input sentences and features, while yi denotes the target labels. \u03b8 signifies the parameters for any Large Language Models (LLMs) or Pretrained Language Models (PLMs), which remain frozen in the case of in-context learning. f\u03b8 represents the output from LLMs given input xi and prompt P. S is a scoring function that measures the performance of the model output in relation to the ground truth label yi. The objective of in-context learning (or prompt engineering) is to identify the optimal prompt P \u2217that maximizes the score S in the test distribution. Based on the structure of P, in-context learning can be further classified into discrete (hard) prompt when P consists of a list of tokens or continuous\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03ab/03abad4b-e6f1-439e-9eec-98c4537d5561.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Prompt categorization by prompt form</div>\nprompt (soft) where P represents an embedding vector (see Figure 1). Additionally, for zero-shot incontext learning, P is independent of xi, whereas for one-shot or few-shot in-context learning, P can be a function of xi (from training data). This survey focuses on zero-shot in-context learning with discrete prompts and examines its application exclusively in decoder-only LLMs, such as the GPTx series.\n# 3 Relevant Work\n# 3.1 Prompts for Encoder-only Transformer Models (BERT)\nBefore the advent of in-context learning, some research efforts have been devoted to studying how to design effective prompts to enhance the performance of BERT models. As depicted in Figure 2, prompts in BERT are usually combined with input to form a cloze-style structure, while for transformer decoder-based models, prompts are more flexible. Numerous studies have investigated prompt design in BERT. In the work by (Jiang et al., 2020), the authors proposed heuristic-based approaches for designing discrete prompts. Dependency parsing is employed to identify useful prompts from Wikipedia. In (Gao et al., 2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts. They then used Ddev to select a single prompt with the best performance. In (Shin et al., 2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task. In addition to prompt designing strategies, other research work focuses on enriching the prompt can-\ndidates and ensembling the output from multiple prompts for the same input. To enrich prompts, (Jiang et al., 2020) employed back-translation to paraphrase prompts. Building on this work, (Haviv et al., 2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding. The concept of in-context learning originates from the work by (Brown et al., 2020). However, BERT models can also perform similar tasks by using a single token as output. For example,\nFrance\u2019s capital is [MASK].\nOnly the output for the [MASK] position is used for inference. This characteristic enables the ensemble of answers from different prompts, although it is not apparent for similar practices in GPT-style models. In (Jiang et al., 2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts. Among the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts). To the best of our knowledge, we did not find any work attempting to generate continuous prompts. In general, optimizing prompts in BERT brings only marginal improvements to the original model. Given the size and structure of BERT, it is more favorable to fine-tune on downstream tasks.\n# 3.2 Prompts for Decoder-only Transformer (GPT)\n# 3.2.1 Continuous Prompt\nAnother line of research has focused on optimizing soft prompts, which eliminate the constraint that prompts have to be natural language. Soft prompts can be learned and optimized directly within the same language model. The key difference between soft prompt tuning and fine-tuning is that prompt tuning typically fixes the weights of the language model and only performs gradient updates on the network that generates the prompt. Prefix-Tuning (Li and Liang, 2021) is one of the early works that tunes prompts on GPT-2 with a small amount of data per task, achieving comparable performance to the full data fine-tuning setting. Prefix-Tuning does not use a separate network; instead, it utilizes the same transformer network but only optimizes the input embedding of the prompt. In P-Tuning V1 (Liu et al., 2021b) and V2 (Liu et al., 2022),\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1dfa/1dfa1462-db4b-4cd5-935a-addc86cb4e59.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Prompt categorization by model types</div>\nthe authors employ a separate LSTM network to generate the input prompt for the language model. While using soft prompts provides more flexibility in prompt design, it requires access to either the weights of language models or the ability to input vectors into language models. As recent language models are hosted as cloud services and large language models are difficult to access via vector inputs, this practice becomes less feasible when using GPT-3 or PaLM (Chowdhery et al., 2022).\n# 3.2.2 Few-Shot Learning\nIn the GPT paper (Brown et al., 2020), fewshot learning demonstrates strong NLP capabilities across various benchmarks. As the title suggests, Language Models are Few-Shot Learners. In the few-shot setting, a task description along with a few examples are presented to the model, which is then asked to complete the task for an unseen example. Numerous studies have been conducted to optimize few-shot examples and prompts to enhance performance. In (Liu et al., 2021a), the authors discovered that GPT-3 generally performs better when in-context examples are similar to the test examples. As a result, they proposed an incontext example algorithm based on example similarities. Similarity is measured using RoBERTa embedding distance in Euclidean space or cosine distance. Other works, such as (Rubin et al., 2021) and (Gutierrez et al., 2022), have adopted similar example selection logic and demonstrated better performance over randomly selected examples. In addition to example selection methods, research efforts like (Wu et al., 2022) and (Kumar and Talukdar, 2021) have been made to optimize the rank\nand order of retrieved examples. While few-shot learning exhibits remarkable performance, according to the no free lunch(NFL) theorem (Wolpert and Macready, 1995, 1997), providing examples inevitably introduces bias to the prediction algorithm. In cases where out-ofdistribution samples occur, applying few-shot learning can hinder the inference process.\n# 4 Zero-Shot Discrete Prompts\nWith the recent success of Large Language Models such as GPTs, designing zero-shot discrete prompts has become increasingly popular in practice. In the experiments conducted by (Reynolds and McDonell, 2021), the authors demonstrate that carefully engineered zero-shot prompts can actually outperform few-shot prompts. They argue that providing examples does not always help because examples tend to be interpreted as part of a narrative rather than serving as categorical guidance. On the other hand, the advantages of using zeroshot discrete prompts can be listed as follows: (1) zero-shot prompts are highly interpretable, (2) few training data or examples are required, (3) the designing process is more straightforward as we only need to deal with task instructions, and (4) the prompt structure is flexible, allowing us to insert our input wherever needed. Zero-shot discrete prompts are also known as task instructions. There are two primary approaches to obtaining a good discrete prompt. The first is heuristic-based manual design, while the second relies on an optimization algorithm to find the optimal prompt. In this section, we focus on reviewing research on prompt\ndesign for transformer decoder style models (e.g., GPT), which has been the focus of a majority of research efforts.\n# 4.1 Manual Design\nIn their work (Reynolds and McDonell, 2021), the authors argue that GPT (or other LLMs) resemble a superposition of human authors. Therefore, it can be helpful to ask GPT to pretend to be a character in the prompt or use the prompt to signify a dialogue between people (i.e., task specification by memetic proxy). The authors also discuss the idea of MetaPrompts, which encapsulate a general intention that will develop towards specific meanings when additional information, such as a task question, is provided. The example prompts they provide, such as \u201dLet\u2019s solve this problem by splitting it into steps,\u201d have been proven to be significantly helpful by subsequent works. In the work (Mishra et al., 2021), the authors propose five principles for designing prompts for GPT-3 based on their observations of GPT-3\u2019s failures. These principles include: (1) using simple patterns to specify expected output, (2) using bulleted lists and assertions, (3) breaking down complex tasks into multiple simpler ones, (4) adding explicit textual statements of output constraints, and (5) customizing the instructions so that the model can directly output the results. These principles can be a good starting point for manual design. Another line of work focuses on improving the reasoning capabilities of large language models via prompt design. The work Chain-of-Thought (CoT) (Wei et al., 2022) was initially proposed in few-shot learning, where the reasoning steps were presented as part of the solution for several few-shot examples. The zero-shot version of CoT was later proposed in (Kojima et al., 2022), which demonstrates that inserting the single prompt \u201dlet\u2019s think step by step\u201d into the task instruction significantly improves performance on mathematical reasoning. The authors also experimented with different templates for prompts and found that instructive prompts help improve the model\u2019s performance in mathematical reasoning, while misleading or irrelevant prompts do not contribute to performance enhancement.\n# 4.2 Prompt Optimization\nFinding the optimal prompt can also be treated as an optimization process, where the goal is to optimize the performance of the target task. Similar\nto finding the best soft prompt or finding the optimal examples for few-shot learning, algorithms can be implemented to find the best zero-shot prompt. However, such work typically requires a small set of evaluation data to assess the prompt performance. In the work by (Zhou et al., 2022), the authors proposed Automatic Prompt Engineer (APE) for zero-shot prompt design. A LLM is used to generate a group of prompts given the task example or human description, and an iterative Monte Carlo search method is used to search for the optimal prompt given the objective function. In addition to using Monte Carlo search for prompt optimization, a gradient-free, edit-based search approach called Gradientfree Instructional Prompt Search (GRIPS) is introduced in (Prasad et al., 2022). GRIPS starts from a manually designed instruction and iteratively searches among generated prompts from four operations (delete, add, swap, paraphrase) to find the optimal prompt for a target task. Another line of research uses gradient-based methods but to generate discrete zero-shot prompts. The work FluentPrompt (Shi et al., 2022) follows the idea from AutoPrompt (Shin et al., 2020), using a gradient-based method to generate discrete prompts. They also use a fluency constraint to encourage human-readable prompt outcomes, which helps improve performance. Another gradientbased prompt generation method RLPROMPT is introduced in (Deng et al., 2022). This work uses a reinforcement learning structure to generate prompts that optimize the task-based reward function. The prompts generated from this framework are often incoherent gibberish but are claimed to achieve significant performance improvement.\n# 4.3 Evaluation\nEvaluating prompt design is very challenging. As there is no ground truth dataset for prompt generation, there is no \u201dbest\u201d prompt but only better prompts. Therefore, the evaluation of the prompt performance for in-context learning usually falls into the following categories. Conditional Probability (Likelihood): To evaluate the performance of a text generation model, we can measure the probability of the generated text. In our case, we can calculate the conditional probability of ground truth(y) given prompt (p), input(x) or calculate the joint probability of x, y, p averaging over the training data, as shown in (2)\nThis is a simple strategy because the models for in-context learning are generative language models which will generate the joint probability (likelihood) automatically. However, this metric sometimes fails to represent the actual performance of the downstream task. Execution Accuracy: A more direct method to measure the performance of a prompt is to use metrics from the target task (Zhou et al., 2022), as ultimately the performance on the task is what we care about. In addition to measuring the execution accuracy directly on the entire training set, there are ways to efficiently estimate the performance on a subset of training data to save computational cost (Zhou et al., 2022), (Li et al., 2022). Prompt Transferability is another evaluation metric reported in (Zhou et al., 2022), (Deng et al., 2022) which is used to prove the quality of the prompt generation methods. However, this metric is more useful in selecting the prompt designing method than evaluating the performance of a single prompt. General Metrics for Language Models should be used when using large language models via zeroshot in-context learning. It is also important to measure the performance from additional aspects. For example, if we are to build a Question-Answering system, we need to measure the risk of hallucination (Ji et al., 2022). If we are to build an email generation system, we may need to measure the toxicity and prevent generating any aggressive content. The work of Holistic Evaluation of Language Models (HELM) (Liang et al., 2022) provides a great example in evaluating the performance for language models via in-context learning. Although various metrics have been reported in HELM for existing models, it is worth noting that the design of our prompt will directly impact the models\u2019 performance.\n# 5 Conclusion\nThe rapid development of large language models (LLMs) has significantly influenced various NLP tasks. Among the techniques to harness their capabilities, in-context learning with different types of prompts\u2014discrete, continuous, few-shot, and zero-shot\u2014has shown remarkable promise. Discrete prompt engineering emphasizes humanreadable prompts that can enhance model performance, while continuous prompt optimization involves soft prompts that can be learned and opti-\nmized directly in the same language model. Fewshot learning leverages a small number of examples to guide the model in the right direction, whereas zero-shot discrete prompts only require task instructions, offering a more straightforward design process. Manual design of prompts can be guided by principles based on model behavior, and optimization algorithms can be used to find optimal prompts. Evaluating the performance of prompts is challenging, as there is no single \u201dbest\u201d prompt, and various metrics need to be considered. In conclusion, as LLMs continue to evolve, prompt design remains a crucial factor in harnessing their full potential across a wide range of applications. A combination of manual design, optimization techniques, and rigorous evaluation can lead to more effective and efficient use of LLMs in diverse NLP tasks.\n# References\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\neling with pathways. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Conference on Empirical Methods in Natural Language Processing. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. ArXiv, abs/2012.15723. Bernal Jimenez Gutierrez, Nikolas McNeal, Clay Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022. Thinking about gpt-3 in-context learning for biomedical ie? think again. In Conference on Empirical Methods in Natural Language Processing. Adi Haviv, Jonathan Berant, and Amir Globerson. 2021. Bertese: Learning to speak to bert. ArXiv, abs/2103.05327. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, and Pascale Fung. 2022. Survey of hallucination in natural language generation. ACM Computing Surveys. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916. Sawan Kumar and Partha P. Talukdar. 2021. Reordering examples helps during priming-based few-shot learning. ArXiv, abs/2106.01751. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs/1910.13461. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), abs/2101.00190. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4emi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4emi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, PoSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097.\nercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00b4e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What makes good in-context examples for gpt-3? In Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. ArXiv, abs/2112.08633. Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. 2022. Toward human readable prompt tuning: Kubrick\u2019s the shining is a good movie, and a good prompt too? ArXiv, abs/2212.10539. Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Eliciting knowledge from language models using automatically generated prompts. ArXiv, abs/2010.15980. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. David H. Wolpert and William G. Macready. 1995. No free lunch theorems for search. D.H. Wolpert and W.G. Macready. 1997. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1):67\u201382. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2022. Self-adaptive in-context learning. ArXiv, abs/2212.10375. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. ArXiv, abs/2211.01910.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/663c/663cf14d-4a54-41b9-8615-44b7a608f512.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to categorize different types of in-context learning techniques, particularly focusing on zero-shot discrete prompts, and to explore their impact on the performance of large language models (LLMs) in Natural Language Processing (NLP) tasks.",
            "scope": "The survey covers in-context learning techniques including discrete, continuous, few-shot, and zero-shot prompts, specifically examining their design, optimization, and evaluation methods. It excludes in-depth discussions on other machine learning paradigms not related to prompt design."
        },
        "problem": {
            "definition": "The core issue being explored is the optimization of prompt design in in-context learning to enhance the performance of LLMs.",
            "key obstacle": "Researchers face challenges in evaluating prompt performance due to the absence of a universally 'best' prompt and the need for multiple metrics to assess effectiveness."
        },
        "architecture": {
            "perspective": "The survey introduces a framework for understanding prompt design through categories of discrete and continuous prompts, and emphasizes the role of manual design versus optimization algorithms in prompt engineering.",
            "fields/stages": "The survey organizes current methods into categories based on the type of prompts: discrete prompts (hard prompts), continuous prompts (soft prompts), and various learning paradigms (zero-shot, few-shot)."
        },
        "conclusion": {
            "comparisions": "The survey compares the effectiveness of different prompt types, highlighting that zero-shot discrete prompts can outperform few-shot prompts in certain contexts, primarily due to their interpretability and straightforward design.",
            "results": "The survey concludes that effective prompt design is crucial for leveraging the full potential of LLMs, recommending a combination of manual design, optimization techniques, and rigorous evaluation."
        },
        "discussion": {
            "advantage": "Current research has achieved significant advancements in prompt design, demonstrating that well-engineered prompts can enhance the performance of LLMs across various NLP tasks.",
            "limitation": "The limitations of current research include challenges in prompt evaluation and the lack of a standardized approach to determine the best prompt.",
            "gaps": "There are gaps in understanding the long-term effects of prompt design on model performance and the need for more empirical studies to identify optimal strategies.",
            "future work": "Future research should focus on developing more robust evaluation metrics and exploring the interplay between prompt design and model architecture to improve LLM performance."
        },
        "other info": {
            "arXiv_id": "2309.13205v1",
            "authors": [
                {
                    "name": "Yinheng Li",
                    "affiliation": "Columbia University / New York City",
                    "email": "li.yinheng@columbia.edu"
                }
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The survey categorizes different types of in-context learning techniques, particularly focusing on zero-shot discrete prompts."
        },
        {
            "section number": "1.2",
            "key information": "The impact of in-context learning techniques on the performance of large language models (LLMs) in Natural Language Processing (NLP) tasks is explored."
        },
        {
            "section number": "1.4",
            "key information": "The survey introduces a framework for understanding prompt design through categories of discrete and continuous prompts, emphasizing the role of manual design versus optimization algorithms."
        },
        {
            "section number": "2",
            "key information": "The survey covers in-context learning techniques including discrete, continuous, few-shot, and zero-shot prompts, examining their design, optimization, and evaluation methods."
        },
        {
            "section number": "3.3",
            "key information": "The survey highlights the optimization of prompt design in in-context learning to enhance the performance of LLMs."
        },
        {
            "section number": "4.1",
            "key information": "Effective prompt design is crucial for leveraging the full potential of LLMs, as demonstrated by significant advancements in prompt engineering."
        },
        {
            "section number": "6.2",
            "key information": "Challenges in evaluating prompt performance arise from the absence of a universally 'best' prompt and the need for multiple metrics to assess effectiveness."
        },
        {
            "section number": "7",
            "key information": "The survey concludes that future research should focus on developing more robust evaluation metrics and exploring the interplay between prompt design and model architecture."
        }
    ],
    "similarity_score": 0.7719644602743575,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/A Practical Survey on Zero-shot Prompt Design for In-context Learning.json"
}