{
    "from": "google",
    "scholar_id": "p-hw8P6FTQYJ",
    "detail_id": null,
    "title": "Preserving in-context learning ability in large language model fine-tuning",
    "abstract": "\n\nA BSTRACT\n\nPretrained large language models (LLMs) are strong in-context learners that are able to perform few-shot learning without changing model parameters. However, as we show, fine-tuning an LLM on any specific task generally destroys its incontext ability. We discover an important cause of this loss, format specialization, where the model overfits to the format of the fine-tuned task and is unable to output anything beyond this format. We further show that format specialization happens at the beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that preserves in-context abilities of the pretrained model. ProMoT first trains a soft prompt for the fine-tuning target task, and then fine-tunes the model itself with this soft prompt attached. ProMoT offloads task-specific formats into the soft prompt that can be removed when doing other in-context tasks. We fine-tune mT5 XXL with ProMoT on natural language inference (NLI) and English-French translation and evaluate the in-context abilities of the resulting models on 8 different NLP tasks. ProMoT achieves similar performance on the fine-tuned tasks compared with vanilla fine-tuning, but with much less reduction of in-context learning performances across the board. More importantly, ProMoT shows remarkable generalization ability on tasks that have different formats, e.g. fine-tuning on a NLI binary classification task improves the model\u2019s in-context ability to do summarization (+0.53 Rouge-2 score compared to the pretrained model), making ProMoT a promising method to build general purpose capabilities such as grounding and reasoning into LLMs with small but high quality datasets. When extended to sequential or multi-task training, ProMoT can achieve even better out-of-domain generalization performance.\n\nNatural language processing (NLP) has recently been revolutionized by scaling up transformer based large langua",
    "bib_name": "wang2022preserving",
    "md_text": "# P RESERVING I N-C ONTEXT L EARNING ABILITY I L ARGE L ANGUAGE M ODEL F INE TUNING\n\nAnonymous authors Paper under double-blind review\n\nA BSTRACT\n\nPretrained large language models (LLMs) are strong in-context learners that are able to perform few-shot learning without changing model parameters. However, as we show, fine-tuning an LLM on any specific task generally destroys its incontext ability. We discover an important cause of this loss, format specialization, where the model overfits to the format of the fine-tuned task and is unable to output anything beyond this format. We further show that format specialization happens at the beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that preserves in-context abilities of the pretrained model. ProMoT first trains a soft prompt for the fine-tuning target task, and then fine-tunes the model itself with this soft prompt attached. ProMoT offloads task-specific formats into the soft prompt that can be removed when doing other in-context tasks. We fine-tune mT5 XXL with ProMoT on natural language inference (NLI) and English-French translation and evaluate the in-context abilities of the resulting models on 8 different NLP tasks. ProMoT achieves similar performance on the fine-tuned tasks compared with vanilla fine-tuning, but with much less reduction of in-context learning performances across the board. More importantly, ProMoT shows remarkable generalization ability on tasks that have different formats, e.g. fine-tuning on a NLI binary classification task improves the model\u2019s in-context ability to do summarization (+0.53 Rouge-2 score compared to the pretrained model), making ProMoT a promising method to build general purpose capabilities such as grounding and reasoning into LLMs with small but high quality datasets. When extended to sequential or multi-task training, ProMoT can achieve even better out-of-domain generalization performance.\n\nNatural language processing (NLP) has recently been revolutionized by scaling up transformer based large language models (LLMs) together with large scale pretraining (Vaswani et al., 2017; Devlin et al., 2019; Raffel et al., 2020a; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Smith et al., 2022). In addition to improved downstream performances, these pretrained LLMs can perform a broad array of unforeseen tasks when provided with a few examples in-context. This in-context few-shot capability allows users to flexibly re-purpose LLMs for specific tasks with a minimum amount of supervised data, making it extremely convenient for fast prototyping and experimentation, especially in the low data regime.\nHowever, even the largest and most advanced LLMs leave a lot to be desired. Grounding and eliminating hallucinations (Maynez et al., 2020), reasoning and logical clarity (Creswell & Shanahan, 2022), mathematics (Brown et al., 2020; Chowdhery et al., 2022; Noorbakhsh et al., 2021) are just a few examples where LLMs still lag behind the best human performances, or in some cases, the fine-tuned performances of the same model. While in many cases, scaling improve these qualities, it is possible that more than a few orders of magnitude in additional scale would be necessary to bridge these gaps, making a pure scaling solution impractical in the near term. In addition, it is possible that some of these properties may not improve indefinitely with larger scales. For example, Longpre et al. (2021) show that larger models have weaker context-based grounding properties as they tend to ignore the context more during reading comprehension question answering (QA).\n\nHow do we improve large language models beyond pretraining? The most common practice is to fine-tune it on a dataset specialized towards a downstream task - on most tasks, in-context learning does not achieve performances equivalent to supervised fine-tuning. As a result, the pretrainingfinetuning paradigm lives on even in the era of LLMs. However, fine-tuning causes specialization. As we show in this paper, a fine-tuned model usually forgets its ability to perform unseen tasks with few-shot in-context prompts. In fact, the in-context performance could drop to none within one thousand steps of fine-tuning.\n\nGround-Truth Output\nMercedes\u2019 Lewis Hamilton took the outright\nchampionship lead for the first time this season\nwith a dominant victory in the Italian Grand Prix.\nPretrained mT5\nHamilton won the British Grand Prix.\nTraditionally fine-tuned mT5 on RTE\nTrue\nProMoT fine-tuned mT5 on RTE\nLewis Hamilton won the Italian Grand Prix.\nTable 1: Output comparison of pretrained and traditionally fine-tuned mT5 models vs. ProMoT fine tuned on the RTE binary classification NLI dataset, performing an in-context 1-shot summarizatio task.\n\nLoss of in-context abilities during fine-tuning is problematic when we try to improve general qualities of LLMs such as grounding, reasoning, mathematics, utilizing external resources etc. For example, grounding is desirable in a large number of tasks, including natural language inference (NLI), summarization, QA, conversation, data-to-text (Ji et al., 2022) and unforeseen tasks that could be performed in-context. There is no shortage of quality data on a few specific instantiations of these general capabilities, e.g. high quality NLI datasets that in principle could teach the model the general concept of grounding, if there was no forgetting or specialization. On the other hand, it is impossible to cover all tasks that use grounding in any fine-tuning set. Forgetting makes it difficult to utilize these high quality datasets to build general capabilities into the model.\nIn this work, we discover that the loss of in-context abilities is, to a large extent, caused by format specialization: overfitting to the specific task format during fine-tuning. For example, if we finetune the model on NLI binary classification, then it learns that the output can only be \u201dTrue\u201d or\u201dFalse\u201d, making it lose its original ability to flexibly generate different output styles according to the in-context prompts of other tasks. In addition, we show that the format is usually learned first during fine-tuning, before the model fully learns the semantic content of the task.\nWe propose a simple solution to format specialization: PROmpt Tuning with MOdel Tuning (ProMoT). ProMoT off-loads format learning to a small amount of task-specific parameters that are external to the model. With ProMoT, we prefix a soft tunable prompt (Lester et al., 2021) to the input sequence during a 2-stage fine-tuning process: we first freeze the model and tune the prompt, then freeze the prompt and tune the model. Since format information is learned first, it mostly enters the soft prompt and is no longer required to be learned by the model. At inference time, we use the model with the tuned prompt if the inference task has the same format as the fine-tuning target task, otherwise we remove the tuned prompt and use the model directly.\nOur experiments show that this simple method can significantly alleviate the forgetting on in-context abilities during fine-tuning, and also results in surprising positive transfer across tasks with totally different formats. Fine-tuning the model only on an NLI binary classification dataset, we are able to improve the general grounding properties of a mT5 XXL model, thus improving its in-context performance on summarization. See Table 1 for a concrete example.\n\n\u2022 We show empirically that the in-context capabilities are lost during fine-tuning, and that format specialization is one important cause for this loss. We also discover that format specialization happens at the very beginning of the fine-tuning.\n\u2022 We propose a novel 2-stage fine-tuning framework: PROmpt Tuning with MOdel Tuning (ProMoT), where we utilize soft tunable prompts to absorb the task format during finetuning, thus alleviating in-context capabilities loss.\n\n\u2022 We show empirically that the in-context capabilities are lost during fine-tuning, and that format specialization is one important cause for this loss. We also discover that format specialization happens at the very beginning of the fine-tuning.\n\u2022 We propose a novel 2-stage fine-tuning framework: PROmpt Tuning with MOdel Tuning (ProMoT), where we utilize soft tunable prompts to absorb the task format during finetuning, thus alleviating in-context capabilities loss.\n\n\u2022 We evaluate ProMoT on 10 different NLP tasks including classification, summarization, translation and question answering tasks, and compare it with prompt tuning and classical fine-tuning.\n\u2022 Experimental results show the effectiveness of ProMoT: 1) ProMoT in general improves supervised performance beyond the limit of prompt tuning only. In certain tasks it matches or slightly outperforms the supervised performance of vanilla fine-tuning; 2) ProMoT significantly alleviates the forgetting of in-context capabilities on all tasks; 3) When finetuned on a single task, ProMoT results in positive task transfer across very dissimilar tasks when they share some semantic aspects; 4) When used in a sequential or parallel multi-task setting, ProMoT result in better models for most unseen in-context learning tasks. These properties makes ProMoT a promising method to build general purpose capabilities into LLMs with small fine-tuning datasets;\n\n\u2022 We evaluate ProMoT on 10 different NLP tasks including classification, summarization, translation and question answering tasks, and compare it with prompt tuning and classical fine-tuning.\n\n# 2 R ELATED W ORK\n\nLarge language models are shown to have the abilities to do in-context learning, where they learn a new task during the inference stage by conditioning on a few training examples (Raffel et al., 2020b; Xue et al., 2020; Radford et al., 2018; Chowdhery et al., 2022). Chan et al. (2022); Gao et al. (2020) studies the effect of pretraining data distribution on in-context learning abilities on image recognition tasks, where the tension between in-context learning tasks and fine-tuning tasks is discussed. They propose changing the data distribution to ease such tension, which could be difficult for generative NLP tasks. ProMoT is an orthogonal method that does not require changes in data distribution.\nRelated is the notion of catastrophic forgetting, usually analyzed with a sequence of image classification tasks. Ramasesh et al. (2022) found that as model size increases, the model becomes less prone to catastrophic forgetting. However these works are mostly restricted to tasks of similar format, e.g. classification with the same number of classes. In this work we are interested in vastly different tasks, e.g. classification vs long form generation where the format itself is critical.\nInstead of updating the model parameters, prompt-tuning (Lester et al., 2021; Zhang et al., 2021) appends continuous trainable embeddings (soft prompt) before the inputs and optimizes the prompt. Prompt-tuning underperforms fine-tuning in many cases, as shown in (Lester et al., 2021; Liu et al., 2021) and in our results. Prompt-tuning does not change the parameters of the pretrained model, thus cannot improve the pretrained model itself using the fine-tuning datasets. Similar as prompttuning, some other works propose to adapt large language models to a new task by only tuning a small set of additional parameters (Hu et al., 2021; He et al., 2021).\nAnother line of work uses fine-tuning to teach the model in-context few-shot or zero-shot abilities, providing a meta learning approach to in-context learning. Wei et al. (2021) fine-tunes the pretrained model on large-scale multitask datasets with diverse natural language prompts, improving the zeroand few-shot performance on unseen tasks. Min et al. (2021) incorporates the in-context learning objective into fine-tuning on multitask datasets with few-shot prompts. Such methods require finetuning on a large collection of different tasks to generalize, with each task being one datapoint of meta-learning. In contrast, ProMoT is not limited to this regime and shows positive task transfer with a single fine-tuning task. In addition, such approaches often require human engineered instructions or prompts for each task, a volatile process involving lots of trial and error. ProMoT does not require extensive prompt engineering as it optimizes the soft prompts with data. Finally, ProMoT is largely orthogonal to meta-learning style methods. They can be combined by applying ProMoT to a massively multitask setting where further improved generalization is expected. One could also apply ProMoT directly to the meta task of learning to solve in-context tasks. This, however, is beyond the scope of our current paper and will be left for future exploration.\n\nn this section, we show empirically that 1) in-context abilities are lost during fine-tuning; 2) format specialization is an important cause for such loss; 3) format specialization happens at the very beginning of supervised fine-tuning.\n\n3.1 L OSS OF IN CONTEXT CAPABILITIES DURING FINE TUNING\nPretrained LLMs achieve good performance with in-context learning. However, in many applications one may want to fine-tune LLMs. In this section, we show that the in-context learning performance usually drop significantly after vanilla fine-tuning.\nTo demonstrate this effect, we fine-tune a pretrained mT5 XXL model(13B parameters) (Xue et al., 2020) on the Recognizing Textual Entailment (RTE) dataset (Wang et al., 2019), which is a binary classification dataset predicting whether two given sentences are entailed. We fine-tune the model with default hyper-parameters for mT5 and task format used in PaLM (Chowdhery et al., 2022) where the output labels are True and False.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c88/8c88834f-2085-4177-88de-6696f38d29b0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5088/5088376d-9742-464e-ab20-8bf7850fd9b4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d87/0d8744a0-25b7-4478-87ae-5d4c26b4e377.png\" style=\"width: 50%;\"></div>\nFigure 1: Learning curve of a model finetuned on RTE dataset for 500 steps. Left axis: Accuracy on RTE dataset. Right axis: Exact match rate on two 1-shot QA tasks.\n\n<div style=\"text-align: center;\">Figure 1: Learning curve of a model finetuned on RTE dataset for 500 steps. Left axis: Accuracy on RTE dataset. Right axis: Exact match rate on two 1-shot QA tasks.\n</div>\nTo probe the model\u2019s in-context capabilities during fine-tuning, we evaluate with two openbook QA asks, trivia-qa (Joshi et al., 2017) and web questions (Berant et al., 2013) with 1-shot in-context prompts in a closed book setting. Figure 1 shows that when the accuracy on RTE dataset increases with fine-tuning, the in-context abilities drop drastically. We show that this phenomenon is generic and not a result of specific fine-tuning or evaluation tasks with more results in Section 5.2.\n\nLLMs such as mT5 XXL have the capacity to handle a large number of tasks. So why are the incontext few-shot abilities easily lost after a few hundred steps of fine-tuning? A natural hypothesis is that due to the homogeneity of output formats in fine-tuning datasets, the model quickly learns to follow this output format no matter what the input sequence is. This leads to loss of in-context learning abilities on other tasks that do not share the format of the fine-tuned task. To be more specific, by \u201dformat\u201d we refer to the characteristics of the fine-tuning labels as a subset of all possible output sequences. For example, the format of RTE is a set of two labels, \u201dTrue\u201d or \u201dFalse\u201d, among all possible sequences of tokens of various lengths. More generally, format might include factors such as the language used, typical output lengths and styles, special tokens or punctuation, upper/lower case styles etc. that are common among most data points of a fine-tuning task. Since, by definition, most data points share the same format, the model receives a strong gradient signal that the output should follow this format independent of the input signal, thus its in-context performance on tasks with any other format will drop drastically, even if they share important semantic similarities with the fine-tuned task.\nTo verify this hypothesis, we fine-tune the mT5 XXL model on RTE task (classification) and then evaluate on TriviaQA task (question answering task) with 1-shot prompt. In evaluation, we count the percentage of outputs which are \u2019True\u2019 or \u2019False\u2019 in the test set. Figure 2 shows that as the finetuning proceeds, the model outputs more \u2019True\u2019 or \u2019False\u2019 even with a 1-shot prompted input from TriviaQA. In particular, after 300 fine-tuning steps, 90% of the output becomes \u2019True\u2019 or \u2019False\u2019. The same phenomenon happens on other in-context tasks. For 1-shot WMT16 En-De translation, after 500 steps, more than 99% of the output becomes \u2019True\u2019 or \u2019False\u2019. This indicates that format specialization is an important reason for loss of in-context capabilities during fine-tuning.\n\n<div style=\"text-align: center;\">Figure 2: Change in statistics of True/False in outputs when evaluated on 1-shot in-context TriviaQA dataset. Left axis: Accuracy on RTE. Right axis: Ratio of True/False.\n</div>\nNext, we show experimental evidence that format learning happens first during standard finetuning. This is not surprising as the overwhelming majority of fine-tuning data points has very similar formats, causing a gradient signal that dominates over other, more nuanced elements such as the semantic content of the task.\nMore concretely, for the RTE dataset, the format refers to the fact that output \u2208 {True, False}, while the semantic content refers to the correlation between the input sequence and the output label. We isolate format learning from semantic learning by creating a randomized RTE dataset where the output labels are randomly shuffled, thus are no longer correlated with the input sequences. The gradients of format learning, g format, are then given by the gradients on the randomized RTE dataset. By comparing with the gradient g on the original RTE we can probe as to when format learning happens during fine-tuning. As we can see from Figure 3, at the very beginning of fine-tuning (step 0), The full RTE gradient g  is highly aligned with the format only gradient g format, signified by cos(\u27e8 g 0, g format,0 \u27e9) \u2248 1. Since randomized RTE and original RTE share\n\nFigure 3: Cosine similarity between the full gradient g and the format gradient g format  on different parts of the last decoder layer. We collect and show the cosine value for gradients on MLP kernel, Query, Key and Value on the attention module. The g and g format are much better aligned at the start of training, compared to at 400 steps.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7287/7287751f-1dd6-4ab5-9f9a-81d41d199b39.png\" style=\"width: 50%;\"></div>\nthe format information only and contain totally different semantic content, this alignment implies that the model is mostly learning the format. After 400 fine-tuning steps, this alignment disappears where the cosine similarity drops to around 0. 2. 1 Comparison between more steps can be found in Appendix Figure 8.\n\n4 P ROPOSED M ETHOD: PRO MPT TUNING WITH MO DEL T UNING (P RO M O T)\n\nThe observations from Section 3 inspire us to decompose format learning from fine-tuning, which can alleviate severe forgetting of in-context abilities during fine-tuning. The key idea is to offload format learning in a separate set of parameters external to the model during the earliest training stage, and then train the model which will now focus more on the semantic skills. We propose a two-stage fine-tune strategy called ProMoT, illustrated in Figure 4. At the first stage, ProMoT uses prompt-tuning to capture the format in a trainable soft prompt while the model itself is frozen. At the second stage, ProMoT freezes the learned soft prompt and fine-tunes the model itself to focus on semantic skills that might be more transferable.\n\nStage 1: Prompt-Tuning. Instead of updating the weight parameters, Prompt-tuning (Lester et al., 2021) prepends a continuous trainable prompt (soft prompt) before the embedded inputs and then updates this soft prompt during training. The soft prompt for a given fine-tuned task P e \u2208 R p \u00d7 e is a small set of free parameters taking the form of a few trainable embeddings, where p is the prompt length and e is the embedding size. Given an input sequence from the task (x 1, ..., x n), prompttuning first embeds the input sequence with the embedding layer in the pretrained model, and then concatenates the soft prompt before the embedded input. The soft prompt is then optimized to reduce the loss while the pretrained model is frozen. As indicated in Section 3.3, fine-tuning first learns the format. We expect that by prompt tuning first, the soft prompt will learn the format. Although we cannot guarantee that the soft prompts only learns the format, the small capacity can prevent the\n\n1 We compute the format gradient at 400 steps, g format, 400, by first fine-tuning the model on RTE for 400 steps, then computing the gradient on the randomized RTE dataset with the same batch of input sequences.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c5d/3c5d3c97-6c31-4a94-bcca-a87499c82d32.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Overview of our two-stage fine-tuning strategy. We run prompt-tuning at Stage 1 and model fine-tuning with trained prompt at Stage 2.\n</div>\nStage 2: Fine-tuning with trained prompt. After prompt-tuning, we expect the trained prompt to already store the format information. We then freeze the soft prompt and fine-tune the pretrained model. Importantly, as shown in Figure 4, the soft prompt is still prepended before the input during this stage, forcing the model to learn things not captured already by the soft prompt.\n\nto already store the format information. We then freeze the soft prompt and fine-tune the pretrained model. Importantly, as shown in Figure 4, the soft prompt is still prepended before the input during this stage, forcing the model to learn things not captured already by the soft prompt.\nEvaluation. After the two-stage fine-tuning, we obtain a fine-tuned model checkpoint and a trained soft prompt for this specific fine-tuning target task. We expect the soft prompt stores most of the format information, and we only use this prompt during inference when the inference task has the same format as the fine-tuned target task. Otherwise, we remove the learned prompt and simply feed the original input into the fine-tuned model.\nProMoT is a general framework that can be combined with different prompt-tuning and fine-tuning techniques. For example, we can use few-shot fine-tuning at the second stage where the inputs are appended with few-shot prompts, which can further improve the few-shot performance as proposed by Min et al. (2021).\n\nEvaluation. After the two-stage fine-tuning, we obtain a fine-tuned model checkpoint and a trained soft prompt for this specific fine-tuning target task. We expect the soft prompt stores most of the format information, and we only use this prompt during inference when the inference task has the same format as the fine-tuned target task. Otherwise, we remove the learned prompt and simply feed the original input into the fine-tuned model.\nProMoT is a general framework that can be combined with different prompt-tuning and fine-tuning techniques. For example, we can use few-shot fine-tuning at the second stage where the inputs are appended with few-shot prompts, which can further improve the few-shot performance as proposed by Min et al. (2021).\n\n# 5 E XPERIMENTS\n\n# 5.1 S ETTINGS\n\nDatasets. We use RTE (Wang et al., 2019) and WMT14 En-Fr as our two fine-tuning target tasks. RTE is a binary classification task and WMT14 En-Fr is a translation task. They are selected as examples of classification and generative tasks.\nTo evaluate in-context abilities, we use four types of tasks including natural language inference from superGLUE (Wang et al., 2019), translation from WMT, QA including triviaQA (Joshi et al., 2017) and web questions (Berant et al., 2013) that are evaluated in a closed book setting, and summarization with XSum (Narayan et al., 2018) and WikiLingua (Ladhak et al., 2020). For each task, we use 1-shot and 4-shots in-context prompts where the prompt template is the same with PaLM (Chowdhery et al., 2022). Input templates and output post-processing can be found in the Appendix.\nFor classification tasks, we report the accuracies. For QA tasks, we report the exact match ratio. For translation tasks, we report the BLEU score (Papineni et al., 2002). And for summarization tasks, we report the Rouge-2 score (Lin, 2004) . We report the results on development set for classification tasks (RTE, CB and WiC) and results on test set for all other tasks.\n\nModels. We use the mT5 (Xue et al., 2020) checkpoint for T5x XXL model (Raffel et al., 2020b) in all of our experiments, which contains 13B parameters. mT5 is pretrained on a large-scale multilingual dataset, which makes it a good choice for the translation tasks used in our experiments.\n\nomparing methods. In our experiments, we have several different training configurations and aselines, including\n\u2022 Traditional fine-tuning: Fine-tune the pretrained model without trainable prompt.\n\u2022 Prompt-tuning: Tune the trainable prompt with pretrained model frozen. The in-context learning performance of prompt-tuning is the same as the pretrained model.\n\u2022 ProMoT: Our proposed two-stage fine-tuning strategy.\n\u2022 Fine-tuning/Prompt-tuning/ProMoT with 1-shot: For each input/output pair during training and evaluation, we sample and prepend a 1-shot example from the training split.\n\n\u2022 Traditional fine-tuning: Fine-tune the pretrained model without trainable prompt.\n\u2022 Prompt-tuning: Tune the trainable prompt with pretrained model frozen. The in-context learning performance of prompt-tuning is the same as the pretrained model.\n\u2022 ProMoT: Our proposed two-stage fine-tuning strategy.\n\u2022 Fine-tuning/Prompt-tuning/ProMoT with 1-shot: For each input/output pair during training and evaluation, we sample and prepend a 1-shot example from the training split.\n\n\u2022\n\u2022 Fine-tuning/Prompt-tuning/ProMoT with 1-shot: For each input/output pair during training and evaluation, we sample and prepend a 1-shot example from the training split.\n\nHyper-parameters. For all mT5 models, we fine-tune with learning rate 0.001, drop rate 0.1 and label smoothing 0.1. For all prompt-tuning experiments, we use learning rate 0.2. For all tasks except summarization tasks, we choose the model input sequence length larger than the input length in datasets. For summarization, we cut each input to 1024 tokens. We use Adafactor optimizer and batch size 64 without data-packing across all experiments. In inference, we use beam search to decode the outputs with width 4. More experimental settings are provided in the appendix.\n\n# 5.2 S INGLE T ASK F INE TUNING\n\nhighest performance for each task with red and bold numbers, respectively.\nDatasets\nPretrained\nPrompt-tuning\nStandard Fine-tuning\nProMoT (Ours)\nProMoT + 1-shot (Ours)\nRTE\n-\n91.34\n92.06\n92.78\n93.86\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\nCB\n46.43\n51.790\n46.43\n51.790\n73.214\n82.143\n66.070\n67.860\n83.929\n82.143\nWiC\n49.687\n49.687\n49.687\n49.687\n50.000\n50.157\n51.411\n53.605\n51.254\n50.627\ntriviaQA\n17.582\n19.022\n17.582\n19.022\n0.150\n0.115\n17.643\n18.660\n17.820\n19.623\nweb questions\n9.695\n13.041\n9.695\n13.041\n0.049\n0.049\n11.073\n13.189\n10.138\n12.106\nWMT16 ende\n3.974\n8.830\n3.974\n8.830\n0.000\n0.000\n2.018\n3.694\n2.256\n4.894\nWMT16 enro\n1.818\n3.918\n1.818\n3.918\n0.000\n0.000\n0.704\n0.959\n0.870\n1.868\nXSum\n6.410\n2.353\n6.410\n2.353\n0.000\n0.000\n7.020\n7.006\n6.941\n3.935\nWikiLingua\n4.585\n1.329\n4.585\n1.329\n0.000\n0.000\n4.841\n4.903\n4.874\n3.434\nDatasets\nPretrained\nPrompt-tuning\nStandard Fine-tuning\nProMoT (Ours)\nProMoT + 1-shot (Ours)\nWMT14 En-Fr\n-\n39.28\n41.796\n41.3\n41.19\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\nCB\n46.430\n51.790\n46.430\n51.790\n16.071\n32.143\n41.071\n57.143\n41.071\n53.571\nWiC\n49.687\n49.687\n49.687\n49.687\n50.627\n49.060\n50.157\n50.313\n49.843\n50.627\ntriviaQA\n17.582\n19.022\n17.582\n19.022\n3.200\n3.147\n13.630\n15.204\n16.927\n18.191\nweb questions\n9.695\n13.041\n9.695\n13.041\n0.886\n6.152\n9.400\n7.923\n10.138\n12.008\nWMT16 ende\n3.974\n8.830\n3.974\n8.830\n0.808\n0.175\n15.517\n15.548\n16.139\n15.626\nWMT16 enro\n1.818\n3.918\n1.818\n3.918\n1.526\n0.424\n18.544\n17.799\n17.568\n16.808\nXSum\n6.410\n2.353\n6.410\n2.353\n0.045\n1.857\n1.493\n0.650\n3.407\n4.362\nWikiLingua\n4.585\n1.329\n4.585\n1.329\n0.030\n0.429\n1.142\n0.524\n4.215\n4.727\nWe fine-tune the pretrained model with ProMoT on two target tasks and evaluate on several few-shot evaluation tasks. At ProMoT stage 1, we run prompt-tuning for 5000 steps and save a checkpoint every 1000 steps, then select the prompt checkpoint with the best performance on target task. At ProMoT stage 2, we freeze the trained prompt and fine-tune the model for 1000 steps, checkpointing every 100 steps. We then pick the model checkpoint with highest performance on the finetuned task as our final checkpoint. For comparison, we run prompt-tuning and traditional fine-tuning for 5000\n\nand 1000 training steps respectively and report the performance of the best checkpoint. We explore finetuning with more steps in Appendix A.2.\nWe list the results for RTE in Table 9 and WMT14 En-Fr translation in Table 3. We see that after traditional fine-tuning on RTE, the performance on binary classification tasks (CB and WiC) with the same format may increase. However, the performance on other tasks drops to nearly zero. This further validates that fine-tuned models suffer from format specialization as discussed in Section 3. With ProMoT, the model achieves similar accuracy on RTE while forgetting much less on all fewshot evaluation tasks compared to standard fine-tuned models. More importantly, ProMoT models trained with only RTE outperform the pretrained model on summarization tasks like XSum and WikiLingua. Since ProMoT is a general framework, we can combine it with natural language fewshot prompts during fine-tuning stage, which is shown in Min et al. (2021) to improve in-context learning abilities. More concretely, training input of ProMoT + 1-shot is the concatenation of the soft prompt, the natural language 1-shot prompt, and then the original input sequence. Table 9 shows that ProMoT + 1-shot further boosts the performance over the original ProMoT on some tasks.\nWhen we apply ProMoT finetuning on English to French translation, we find similar trend (Table 3): Compared to standard fine-tuning, we observe significantly less forgetting on in-context learning tasks across the board. Compared to the pretrained model, we observe positive transfer on translation tasks for other language pairs as well as natural language inference binary classifications.\n\nAs we can see from Section 5.2, ProMoT alleviates forgetting of in-context abilities across the board and has positive transfer on some tasks. It is however not surprising that ProMoT on different tasks have different influences on in-context evaluation tasks. For example, ProMoT on RTE causes some forgetting on translation while improving classification and summarization. On the contrary, ProMoT on En-Fr has some forgetting on summarization while boosting other translation tasks. This inspires us to apply ProMoT on multiple datasets for a generally better model.\nIn this section, we explore injecting knowledge into a single model with ProMoT by training on multiple tasks with sequential training and multitask training. For sequential training, we first run ProMoT on the 1-shot RTE dataset, and then on 1-shot En-Fr translation dataset. For multitask training, we mix the data from different tasks in one batch and train a soft prompt for each task. All the training configurations are the same in Section 5.2. We illustrate the results in Figure 5 and Table 7.\nIn Figure 5, we first report the accuracy on two fine-tuning target datasets RTE and WMT14 En-Fr. For sequential two-stage training, we evaluate the RTE accuracy using the trained prompt for RTE together with the final model checkpoint after sequential training. After the second ProMoT on EnFr translation, the accuracy on RTE does not drop much, revealing the potential of our method in the continual-learning space. More importantly, after sequential ProMoT, we obtain a model with significantly improved in-context performances compared to the pretrained model on both unseen classification tasks (CB, WiC) and translation tasks (XSum, WikiLingua). The exact numbers in Figure 5 and more results on 4-shots evaluation can be found in Table 6 in the Appendix. Results for multi-task training and related analysis can be found in Appendix A.2.\n\n# 5.4 A BLATION S TUDY\n\nIn this section, we conduct several ablation study in Table 4 to show the effectiveness of our method. First, we jointly fine-tune both the soft prompt and the model at the same time. This results in severe forgetting of in-context learning abilities similar to that of standard fine-tuning. We thus show that the benefit of less forgetting and positive transfer comes from the two-stage nature of ProMoT instead of merely having the soft prompt. During joint training, soft prompt and the main model are trained together for 1000 steps and then we evaluate the in-context 1-shot performance on evaluation tasks. In addition, we also fine-tune the models while attaching a random soft prompt that is fixed during training. As we expected, a random prompt contains no format information and does not help the model offload format learning, resulting in severe forgetting. Another natural baseline is to finetune the model with natural language prompts instead of soft prompts in place - after all, the natural language prompts contain few-shot examples that also capture the task format to some extend. We compared this approach in a 1-shot scenario and did not observe significantly reduced forgetting\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/24ef/24ef8a0c-d591-4212-8cfc-644fa4d728b8.png\" style=\"width: 50%;\"></div>\nFigure 5: Results of sequential ProMoT fine-tuning compared with pretrained model and ProMoT fine-tuning on single tasks. Exact numbers and more results can be found in the Appendix. We scale up the Rouge-2 score for summarization tasks, the exact match rate for QA tasks and BLEU score for translation tasks by 5x to show all the tasks on the same figure.\n\n<div style=\"text-align: center;\">Figure 5: Results of sequential ProMoT fine-tuning compared with pretrained model and ProMoT fine-tuning on single tasks. Exact numbers and more results can be found in the Appendix. We scale up the Rouge-2 score for summarization tasks, the exact match rate for QA tasks and BLEU score for translation tasks by 5x to show all the tasks on the same figure.\n</div>\nTable 4: Here we show the ablation study results. Joint fine-tuning means fine-tuning the sof prompt and the main model together. Fine-tuning with 1-shot runs traditional fine-tuning on with a 1-shot natural language prompt attached to every input sequence. Fine-tuning with random promp fine-tunes the main model with a soft prompt randomly initialized with uniform distribution and kept fixed during finetuning. We compare these abalations with ProMoT + 1-shot where ProMoT is\n\nDatasets\nJoint\nFine-tuning\nFine-tuning\nProMoT\nFine-tuning\nwith 1-shot\nwith random prompt\n+ 1-shot (Ours)\nRTE\n90.97\n90.97\n92.06\n93.86\nCB\n83.929\n78.571\n83.929\n83.929\nWiC\n50.47\n51.411\n51.724\n51.254\ntriviaQA\n0.751\n0.027\n0.831\n17.82\nweb questions\n0.64\n0.000\n0.295\n10.138\nWMT16 ende\n0.000\n0.000\n0.000\n2.256\nWMT16 enro\n0.000\n0.000\n0.000\n0.87\nXSum\n0.000\n0.000\n0.000\n6.941\nWikiLingua\n0.000\n0.000\n0.000\n4.874\ncompared to standard fine-tuning. Most of in-context tasks still drop to zero performances. Th demonstrates that soft prompts in ProMoT works better than natural language prompts in capturin format information and alleviating the forgetting of in-context learning abilities during fine-tuning\n\nIn this paper, we found one important cause of the lose of in-context abilities during LLM finetuning: format specialization. Our analysis shows that format specialization happens at the very beginning of fine-tuning. This motivates us to develop ProMoT, a simple yet effective two-stage fine-tuning framework that utilizes soft trainable prompts to absorb the task format before finetuning the model. Our experimental results on a diverse set of NLP tasks show the effectiveness of ProMoT for preserving in-context capabilities during fine-tuning. ProMoT also shows surprising positive transfer across very different tasks, making it a promising method that could build general purpose capabilities into LLMs with small finetunig datasets. We also explored sequential training and multi-task training on two training tasks which can achieve even better generalization.\n\n# R EFERENCES\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1533\u20131544, 2013.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in NeurIPS, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nStephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent incontext learning in transformers, 2022. URL https://arxiv.org/abs/2205.05055.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL https://arxiv.org/abs/2208.14271.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, pp. 4171\u2013 4186, 2019. URL https://aclanthology.org/N19-1423.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. CoRR, abs/2202.03629, 2022. URL https://arxiv.org/abs/2202.03629.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, pp. 4034\u20134048, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https: //aclanthology.org/2020.findings-emnlp.360.\n\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. URL https://arxiv.org/abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020b.\nVinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=GhVS8_yPeEa.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00b4evry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. CoRR, abs/2110.08207, 2021. URL https://arxiv.org/abs/2110.08207.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022. URL https: //arxiv.org/abs/2201.11990.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in NeurIPS, 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. CoRR, abs/2109.01652, 2021. URL https://arxiv.org/abs/2109.01652.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. arXiv preprint arXiv:2108.13161, 2021.\n\n# A.1 E XPERIMENT D ETAILS\n\nTask\nTemplate\nRTE\n[premise] question: [hypothesis] Is it true or false? answer: {True, False}\nCB\n[premise] question: [hypothesis] Is it true or false or neither? answer: {True, False, Neither}\nWiC\n[sentence1] [sentence2] question: The word [word] is used in the same way in the two sentences.\nIs it true or False? answer: {True, False}\nQA\nQ: [question] A:\nTranslation\nTranslate [source language] to [target language]: [sentence 1]\nSummarization\nArticle: [article] One sentence summary:\n<div style=\"text-align: center;\">Table 5: Input template for each task\n</div>\nOutput post-processing For each task, we first extract the text after <extra id 0> and before <extra id 1>, then trim the text by locating and remove the text after the second prefix token (Q:, Translate, Article:). For classification tasks including RTE, CB and WiC, we check whether the first output token is True or False.\n\nA.2 A DDITIONAL E XPERIMENT R ESULTS\n\nMore results of sequential ProMoT training In Table 6 we show the exact numbers in Figure 5 and additional evaluation results on 4-shots datasets.\n\n<div style=\"text-align: center;\">Table 6: Results for sequentially training first on RTE and then on En-Fr with ProMoT + 1-shot.\n</div>\nTable 6: Results for sequentially training first on RTE and then on En-Fr with ProMoT + 1-shot.\nDatasets\nPretrained Model\nProMoT\nProMoT\nProMoT on 1-shot\n1-shot RTE\n1-shot En-Fr\nEn-Fr after RTE\nWMT14 En-Fr\n1.982\n0.92\n41.19\n41.33\nRTE\n47.653\n93.86\n53.43\n90.975\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\n1-shot\n4-shots\nCB\n46.43\n51.79\n83.929\n82.143\n41.071\n53.571\n82.143\n80.357\nWiC\n49.687\n49.687\n51.254\n50.627\n49.843\n50.627\n51.097\n57.367\ntriviaQA\n17.582\n19.022\n17.82\n19.623\n16.927\n18.191\n14.081\n16.98\nweb questions\n9.695\n13.041\n10.138\n12.106\n10.138\n12.008\n9.4\n11.713\nWMT16 ende\n3.974\n8.83\n2.256\n4.894\n16.139\n15.626\n18.132\n17.9\nWMT16 enro\n1.818\n3.918\n0.87\n1.868\n17.568\n16.808\n20.642\n21.512\nXSum\n6.410\n2.353\n6.941\n3.935\n3.407\n4.362\n3.185\n3.959\nWikiLingua\n4.585\n1.329\n4.874\n3.434\n4.215\n4.727\n3.422\n3.143\nTraining more steps: trade-off between fine-tuning target task and in-context learning abilities In Section 5.2, we report the results of the best checkpoints within 1000 steps of fine-tuning. With a longer training period, we can see a more clear trade-off between the performance on fine-tuning target task and the performance on in-context learning abilities. Here we show the long-term tradeoff between fine-tuning target task and in-context evaluation tasks by scattering the performance of different checkpoints within 20000 steps fine-tuning. In figure 6, and 7, we plot the trade-off on classification and translation tasks, respectively.\nAs we can see from the figures, datapoints for ProMoT is higher than traditional fine-tuning on the figures, which implies that with the same performance on fine-tuning target task, forgetting is alleviated with ProMoT fine-tuning.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e2b7/e2b77465-4144-48e1-a96a-ea166b15c5fa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Trade-off between BLEU score of En-Fr (horizontal axis) and average accuracy on classification tasks (vertical axis) when fine-tuning the model on En-Fr translation.\n</div>\nMulti-task training Another commonly-adopted method to train a model on multiple tasks is parallel multi-task training, especially when we known the training tasks beforehand. During multi-task training, data points from different tasks are sampled according to corresponding ratios to construct the training batches. Multi-task training has been shown to improve the generalization ability of large language models during finetuning, often with natural language prompts attached to distinguish the tasks (Sanh et al., 2021; Wei et al., 2021; Min et al., 2021). In this section, we apply ProMoT on RTE and WMT14 En-Fr in a multi-task finetuning setting. We show that ProMoT generalize substantially better on a subset of in-context tasks than traditional multi-task finetuning. At the first stage, we train a different soft prompt for each task. At the second stage, we prepend the corresponding trained and frozen prompt on each training example and tune the pretrained model on a mixture of data from different tasks. We use the same training and evaluation tasks and keep most of hyper-parameters the same except doubling training steps in the first stage. We show the results in Table 7, both on fine-tuned tasks and 1-shot out-of-domain evaluation tasks. With multi\nTable 7: Comparison of multi-task training on RTE and WMT14 En-Fr translation. We compare the evaluation results of pretrained mT5 model, traditional multi-task fine-tuning (FT), Sequential (Seq) ProMoT training and multitask (Multi) ProMoT training. 1-shot means we add a 1-shot natural language prompt before each training input (in addition to the soft prompts if we are using ProMoT). We report performance on two fine-tuning target tasks (RTE and WMT14 En-Fr) and eight 1-shot out-of-domain evaluation tasks.\n\nout-of-domain evaluation tasks.\nPretrained\nmulti-task FT\nSeq ProMoT\nMulti ProMoT\n1-shot multi-task\n1-shot Seq\n1-shot Multi\nFT\nProMoT\nProMoT\nRTE\n90.250\n92.419\n91.340\n91.700\n90.975\n93.140\nWMT14 En-Fr\n41.338\n40.880\n40.726\n40.869\n41.33\n40.553\nCB\n46.429\n80.357\n82.143\n83.929\n87.500\n82.143\n85.714\nWiC\n49.687\n51.097\n51.724\n51.411\n53.292\n50.627\n52.038\nTriviaQA\n17.582\n15.761\n12.906\n16.989\n16.53\n14.081\n17.184\nWeb questions\n9.695\n9.695\n8.661\n10.039\n9.400\n9.400\n10.384\nWMT16 En-De\n3.974\n0.882\n17.028\n18.833\n2.503\n18.132\n17.572\nWMT16 En-Ro\n1.818\n1.524\n19.657\n18.410\n5.625\n20.642\n18.574\nXSum\n6.41\n0.436\n2.933\n4.496\n1.816\n3.185\n4.321\nWikiLingua\n4.585\n0.717\n2.256\n2.892\n4.325\n3.422\n3.564\ntask ProMoT, forgetting on QA and summarization are alleviated compared to sequential ProMoT. Besides, although traditional multi-task fine-tuning can improve model generalization and thus alleviate the forgetting problem on some tasks, ProMoT multi-task training has much better transferred performance on translation tasks on unseen language pairs.\nAdditional experiments on T5 and FLAN-T5 To show the performance of our method on English-based pretrained model, we did two additional experiments on FLAN-T5 and T5 XXL with fine-tuning target task RTE.\n\ntask ProMoT, forgetting on QA and summarization are alleviated compared to sequential ProMoT. Besides, although traditional multi-task fine-tuning can improve model generalization and thus alleviate the forgetting problem on some tasks, ProMoT multi-task training has much better transferred performance on translation tasks on unseen language pairs.\n\nAdditional experiments on T5 and FLAN-T5 To show the performance of our method on English-based pretrained model, we did two additional experiments on FLAN-T5 and T5 XXL with fine-tuning target task RTE.\n\n<div style=\"text-align: center;\">Table 8: In-context performance of FLAN-T5 XXL finetuned on RTE and evaluated on 8 few-shot tasks. The accuracy on fine-tuned task (RTE) is in the first row. Prompt-tuning doesn\u2019t modify pretrained model parameters and has the same in-context performance as pretrained model.\n</div>\nretrained model parameters and has the same in-context performance as pretrained model.\nDatasets\nPretrained\nPrompt-tuning\nStandard Fine-tuning\nProMoT (Ours)\nRTE\n-\n93.86\n94.22\n94.22\nCB\n83.929\n83.929\n85.714\n83.929\nWiC\n61.755\n61.755\n67.085\n62.069\ntriviaQA\n33.926\n33.926\n36.241\n33.926\nweb questions\n34.744\n34.744\n33.612\n34.695\nWMT16 ende\n11.591\n11.591\n9.987\n11.719\nWMT16 enro\n16.806\n16.806\n14.816\n16.725\nXSum\n21.825\n21.825\n21.732\n21.788\nWikiLingua\n22.858\n22.858\n22.778\n22.848\n<div style=\"text-align: center;\">Table 9: In-context performance of T5 XXL finetuned on RTE and evaluated on 8 few-shot task The accuracy on fine-tuned task (RTE) is in the first row. Prompt-tuning doesn\u2019t modify pretrain model parameters and has the same in-context performance as pretrained model.\n</div>\nmodel parameters and has the same in-context performance as pretrained model.\nDatasets\nPretrained\nPrompt-tuning\nStandard Fine-tuning\nProMoT (Ours)\nRTE\n-\n91.7\n93.5\n93.14\nCB\n55.357\n55.357\n62.500\n73.214\nWiC\n49.843\n49.843\n50.000\n50.784\ntriviaQA\n34.147\n34.147\n0.018\n33.855\nweb questions\n16.043\n16.043\n0.000\n15.945\nWMT16 ende\n0.134\n0.134\n0.000\n0.018\nWMT16 enro\n0.055\n0.055\n0.000\n0.008\nXSum\n1.261\n1.261\n0.000\n1.791\nWikiLingua\n1.118\n1.118\n0\n2.251\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7220/7220beaf-0ebc-4ebb-9841-ea8cc6d45c86.png\" style=\"width: 50%;\"></div>\nFigure 8: Cosine similarity between the full gradient g and the format gradient g format on different parts of the last decoder layer. We collect and show the cosine value for gradients on MLP kernel, Query, Key and Value on the attention module.\n\nPlotting more steps for Figure 3 To further strengthen our conclusion in Figure 3, here we plot the gradient alignment from step 0 to step 400. As we can see from the figure, gradient alignment drops significantly after 300 steps which is matched with Figure 2 where the true and false ratio increases before 300 steps and then remains stable.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of preserving in-context learning abilities in large language models (LLMs) during fine-tuning. While LLMs excel in few-shot learning without parameter changes, fine-tuning typically compromises this capability due to format specialization, where models overfit to the fine-tuning task's format, losing flexibility in generating diverse outputs.",
        "problem": {
            "definition": "The problem is the degradation of in-context learning abilities in LLMs when they are fine-tuned on specific tasks, leading to a loss of performance on unseen tasks.",
            "key obstacle": "The main challenge is format specialization, which occurs early in the fine-tuning process, causing the model to rigidly adhere to the output format of the fine-tuning task."
        },
        "idea": {
            "intuition": "The idea arose from observing that fine-tuned models quickly lose their ability to perform in-context learning due to overfitting to specific output formats.",
            "opinion": "The proposed solution, Prompt Tuning with MOdel Tuning (ProMoT), involves a two-stage fine-tuning process that first trains a soft prompt to capture task-specific formats before fine-tuning the model itself.",
            "innovation": "ProMoT differs from existing methods by offloading format learning to a tunable soft prompt, which helps maintain the model's in-context learning capabilities across diverse tasks."
        },
        "method": {
            "method name": "Prompt Tuning with MOdel Tuning",
            "method abbreviation": "ProMoT",
            "method definition": "ProMoT is a two-stage fine-tuning framework designed to preserve the in-context learning abilities of pretrained LLMs by separating format learning from semantic learning.",
            "method description": "The method involves first tuning a soft prompt while keeping the model frozen, followed by fine-tuning the model with the prompt attached.",
            "method steps": [
                "Stage 1: Prompt-Tuning - Train a soft prompt while freezing the model.",
                "Stage 2: Fine-tuning - Freeze the soft prompt and fine-tune the model."
            ],
            "principle": "ProMoT is effective because it allows the model to focus on learning semantic skills while the soft prompt captures the necessary format information, thus alleviating the forgetting of in-context abilities."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using datasets such as RTE for binary classification and WMT14 for translation, evaluating the model's performance on various NLP tasks including classification, summarization, translation, and question answering.",
            "evaluation method": "The performance was assessed by comparing the results of ProMoT with traditional fine-tuning and prompt-tuning methods, measuring accuracy for classification tasks, exact match ratios for QA tasks, BLEU scores for translation, and Rouge-2 scores for summarization."
        },
        "conclusion": "The results demonstrate that ProMoT effectively preserves in-context learning abilities during fine-tuning, achieving comparable performance on fine-tuned tasks while minimizing performance loss on unseen tasks. The method shows promise for enhancing general-purpose capabilities in LLMs using limited datasets.",
        "discussion": {
            "advantage": "ProMoT significantly reduces forgetting of in-context capabilities and facilitates positive transfer across different tasks, making it a robust approach for fine-tuning LLMs.",
            "limitation": "While ProMoT improves in-context learning retention, its performance may vary depending on the specific tasks and the nature of the fine-tuning dataset.",
            "future work": "Future research could explore the application of ProMoT in multi-task settings and its integration with additional prompt-tuning techniques to further enhance model generalization."
        },
        "other info": {
            "additional findings": "ProMoT can achieve better out-of-domain generalization performance when extended to sequential or multi-task training, indicating its versatility.",
            "model used": "The experiments utilized the mT5 XXL model, which contains 13 billion parameters, for fine-tuning."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of preserving in-context learning abilities in large language models (LLMs) during fine-tuning."
        },
        {
            "section number": "1.3",
            "key information": "LLMs excel in few-shot learning without parameter changes, but fine-tuning typically compromises this capability due to format specialization."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge is format specialization, which occurs early in the fine-tuning process, causing the model to rigidly adhere to the output format of the fine-tuning task."
        },
        {
            "section number": "3.2",
            "key information": "ProMoT is a two-stage fine-tuning framework designed to preserve the in-context learning abilities of pretrained LLMs by separating format learning from semantic learning."
        },
        {
            "section number": "4.1",
            "key information": "The proposed solution, Prompt Tuning with MOdel Tuning (ProMoT), involves a two-stage fine-tuning process that first trains a soft prompt to capture task-specific formats before fine-tuning the model itself."
        },
        {
            "section number": "6.1",
            "key information": "ProMoT significantly reduces forgetting of in-context capabilities and facilitates positive transfer across different tasks, making it a robust approach for fine-tuning LLMs."
        },
        {
            "section number": "7",
            "key information": "The results demonstrate that ProMoT effectively preserves in-context learning abilities during fine-tuning, achieving comparable performance on fine-tuned tasks while minimizing performance loss on unseen tasks."
        }
    ],
    "similarity_score": 0.7204913230877241,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c88/8c88834f-2085-4177-88de-6696f38d29b0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5088/5088376d-9742-464e-ab20-8bf7850fd9b4.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d87/0d8744a0-25b7-4478-87ae-5d4c26b4e377.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7287/7287751f-1dd6-4ab5-9f9a-81d41d199b39.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c5d/3c5d3c97-6c31-4a94-bcca-a87499c82d32.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/24ef/24ef8a0c-d591-4212-8cfc-644fa4d728b8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e2b7/e2b77465-4144-48e1-a96a-ea166b15c5fa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7220/7220beaf-0ebc-4ebb-9841-ea8cc6d45c86.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Preserving in-context learning ability in large language model fine-tuning.json"
}