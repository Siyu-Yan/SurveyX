{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2105.03726",
    "title": "Mental Models of Adversarial Machine Learning",
    "abstract": "Although machine learning is widely used in practice, little is known about practitioners\u2019 understanding of potential security challenges. In this work, we close this substantial gap and contribute a qualitative study focusing on developers\u2019 mental models of the machine learning pipeline and potentially vulnerable components. Similar studies have helped in other security fields to discover root causes or improve risk communication. Our study reveals two facets of practitioners\u2019 mental models of machine learning security. Firstly, practitioners often confuse machine learning security with threats and defences that are not directly related to machine learning. Secondly, in contrast to most academic research, our participants perceive security of machine learning as not solely related to individual models, but rather in the context of entire workflows that consist of multiple components. Jointly with our additional findings, these two facets provide a foundation to substantiate mental models for machine learning security and have implications for the integration of adversarial machine learning into corporate workflows, decreasing practitioners\u2019 reported uncertainty, and appropriate regulatory frameworks for machine learning security.",
    "bib_name": "bieringer2022mentalmodelsadversarialmachine",
    "md_text": "# Industrial practitioners\u2019 mental models of adversarial machine learning\nLukas Bieringer\u2217 QuantPi Kathrin Grosse\u2217 University of Cagliari\nBattista Biggio University of Cagliari, Pluribus One Katharina Krombholz CISPA Helmholtz Center for Information Security\nMichael Backes CISPA Helmholtz Center for Information Security Battista Biggio University of Cagliari, Pluribus One\n# Abstract\nAlthough machine learning is widely used in practice, little is known about practitioners\u2019 understanding of potential security challenges. In this work, we close this substantial gap and contribute a qualitative study focusing on developers\u2019 mental models of the machine learning pipeline and potentially vulnerable components. Similar studies have helped in other security fields to discover root causes or improve risk communication. Our study reveals two facets of practitioners\u2019 mental models of machine learning security. Firstly, practitioners often confuse machine learning security with threats and defences that are not directly related to machine learning. Secondly, in contrast to most academic research, our participants perceive security of machine learning as not solely related to individual models, but rather in the context of entire workflows that consist of multiple components. Jointly with our additional findings, these two facets provide a foundation to substantiate mental models for machine learning security and have implications for the integration of adversarial machine learning into corporate workflows, decreasing practitioners\u2019 reported uncertainty, and appropriate regulatory frameworks for machine learning security.\n# 1 Introduction\nAdversarial machine learning (AML) studies the reliability of learning based systems in the context of an adversary [6, 12, 69]. For example, tampering with some features often\nsuffices to change the classifier\u2019s outputs to a class chosen by the adversary [9, 24, 80]. Analogously, slightly altering the training data enables the attacker to decrease performance of the classifier [11,72]. Another change in the training data allows the attacker to enforce a particular output class when a specified stimulus is present [20,36]. Most state-of-the-art attacks and mitigations are in an ongoing arms race [5,19,82]. Although machine learning (ML) is increasingly used in industry, very little is known about ML security in practice. At the same time, previous works show that practitioners are concerned about AML [47,60], and failures already occur [53], very little is known about ML security in practice. To tackle this question, we conduct a first study to explore mental models of AML. Mental models are relatively enduring, internal conceptual representations of external systems that originated in cognitive science [30,38]. In other security related areas, correct mental models have been found to ease the communication of security warnings [16] or enable users to implement security best-practices [81]. Mental models also serve to enable better interactions with a given system [87], or to design better user interfaces [29]. Our methodology builds upon these previous works by using qualitative methods to investigate the perception of vulnerabilities in ML applications. More concretely, we conducted 15 semi-structured interviews and drawing tasks with industrial practitioners from European start-ups and coded both drawings and the transcripts of the interviews. As the first work in this direction, we lay the foundations for practitioners\u2019 mental models of AML by describing two facets of these models. The first concerns the separation of ML related security (AML) and security unrelated to ML (non-AML security). In many cases, the borders between these two fields are blurry: a participant may start talking about evasion and finish the sentence with a reference to cryptographic keys. The second facet concerns the view of the ML model within a project. In contrast to the focus on an isolated model in AML research [5,6,12,19,69], our practitioners often describe one or more pipelines with potentially several applications of ML. Finally, we found more facets which are left for an in-depth\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e72c/e72cc1a5-8d3c-44a1-8ddd-2d17f083fb34.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: AML threats within the ML pipeline. Each attack is visualized as an arrow pointing from the step co point where the attack affects the pipeline.</div>\ninvestigation by future work. These include the application setting, prior education, and the perceived relevance of AML. Our interviews showed that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. At the same time, more than a third of our participants feels insecure about AML. These concerns seem justified as we found evidence for semi-automated fraud on ML systems in the wild. However, our findings have more practical implications. Our results allow us to address the current lack of understanding by (I) increasing awareness for AML and decreasing uncertainty about AML, (II) developing tools that help practitioners to assess and evaluate security of ML applications, and (III) drafting regulations that contain adequate security assessments and reduce insecurity about AML. However, more work is needed to understand the individual and shared mental models of practitioners and assess the real world security risks when applying ML.\n# 2 Background and related work\nIn this section, we review related work on AML and recall different attacks that have recently been discussed. We also review literature on mental models with regard to humancomputer interaction, usable security and ML.\n# 2.1 Adversarial machine learning\nAML studies the security of ML algorithms [6,12,69]. We attempt to give an informal overview of all attacks in AML, and additionally illustrate them in Figure 1. Poisoning/backdooring. Early works in poisoning altered the training data [72] or labels [11] to decrease accuracy of the resulting classifier, for example SVM. For deep learning, due to the flexibility of the models, introducing backdoors is more common [20,36]. Backdoors are chosen input patterns that reliably trigger a specified classification output. Defending such backdoors has lead to an arms race [82].\nEvasion/adversarial examples. Early work in evasion decreased the test-time accuracy of spam classification [24]. It was later shown that also more complex models change their output for small, malicious input perturbations [9, 80]. Albeit all classifiers are principally vulnerable towards evasion, recent works focus on the arms race in deep learning [5,19]. Membership inference. After first inferring attributes [4] of the training data, research later showed that entire points can be leaked from a model [74]. More concretely, the attacker deduces, given the output of a trained ML model, whether a data record was part of the training data or not. As for other attacks, numerous defenses are being proposed [37,64]. Model stealing. Tram\u00e8r et al. [83] recently introduced model stealing. During this attack, the attacker copies the ML model functionality without consent of the model\u2019s owner. The attacker, given black box access to the original model, tries to reproduce a model with similar performance. As for the previous attacks, mitigations have been proposed [39,68]. Weight perturbations. Fault tolerance of neural networks has long been studied in the ML community [17, 65]. Recently, maliciously altered weights are used to introduce a specific backdoor [35]. Few works exist to defend malicious change to the weights in general, not only related to backdoor introduction [78,88]. For the sake of completeness, we conclude with a description of additional, recent attacks, some of which are part of our questionnaires (see Appendix D.3). In adversarial initialization, the initial weights of a neural network1 are targeted to harm convergence or accuracy during training [32,54]. In adversarial reprogramming, an input perturbation mask forces the classifier at test time to perform another classification task than originally intended [27]. For example, a cat/dog classifier is reprogrammed to classify digits. In model reverse engineering, crafted inputs allow to deduce from a trained model the usage of dropout and other architectural choices [67]. Fi-\nnally, sponge attacks aim to increase energy consumption of the classifier at test time [75]. Practical Relevance of AML. In general, AML research has been criticized for the limited practical relevance of its threat models [28,31]. A possible reason is our lack of knowledge about AI security in practice. Few works attempt to tackle this gap, including for example Lin and Biggio [53]. They give an overview about AI attacks that were carried out in practice based on AI related incidents covered in newspapers. Furthermore, Boenisch et al. [14] conducted a survey and developed an awareness score, which however encompasses AML, privacy, and non-AML security. Concerning which threats are relevant in practice in industry, Kumar et al. [47] and Mirsky et al. [60] found that practitioners are most concerned about model theft and poisoning. Yet, in academia, most work focused on evasion so far. To shed more light on AML in practice, we interview industrial practitioners and take a first step towards a theory of mental models of AML. To this end, we now introduce and review mental models.\n# 2.2 Mental models\nMental models are relatively enduring and accessible, but limited, internal conceptual representations of external systems [26] that enable people to interact with given systems. Hence, the field of human computer interaction (HCI) studied this concept quite early [73]. Mental models, most recently, saw an increasing relevance in usable security. We now recall prior application scenarios and highlight relevant conceptual contributions in the context of security and ML. Mental models in HCI and usable security. The relevance of mental models has been subject to a lengthy debate in HCI research [76,85]. In many cases, the focus was to capture, depict and analyze mental models of specific objects of investigation. Examples of topics include, but are not limited to, the design of online search applications [7], interface design [44], and interfaces for blind people [25]. Research in usable security has recently focused on mental models of security in general [2,87], privacy in general [71], security warnings [16], incident response [70], the internet [41], the design of security dashboards [58], the Tor anonymity network [29], privacy and security in smart homes [81,90], encryption [1,89], HTTPS [45], and cryptocurrency systems [57]. With regard to the respective object of investigation, these contributions paved the way for improvements of user interface designs [29], adequate security communication [16], as well as the development of security policies and implementation of best-practices [81]. It has been argued that security mental models contain structural and functional properties [89]. For each application, users develop a cognitive representation of its inherent components, their interconnection and correspondingly possible security threats. This representation helps them to understand where threats could emerge and how they could take effect. Mental models evolve dynamically\nupon individual interaction with a given application [13]. Mental models in ML. In order to interact with an ML application, humans need a mental model of how it combines evidence for prediction [66]. This is all the more important for ML-based applications which often inherit a certain opacity. As Lage et al. [48] pointed out, the number of necessary cognitive chunks is the most important type of complexity in order to understand applications. During interaction with black-box processes, humans strive for reduced complexity which may lead to the development of inaccurate or oversimplified mental models [33,42]. A dedicated line of research therefore elaborates on the relevance and nature of mental models in the context of explainable artificial intelligence. Mental models have been found to serve as scaffolds not only for a given ML application [84], but also for its embedding in organizational practices [91]. For data science teams, these workflows usually consist of predefined steps (Figure 1) and necessitate interpersonal collaboration [62]. Following Arrieta et al. [3], we argue that individual collaborators within these teams (e.g., ML engineers, software engineers) develop separate internal representations of a given workflow or application. The need for appropriate mental models thereby increases with the enlarged scope of ML applications [49] and involved stakeholders [51,79]. Recent work in this line of research called for qualitative studies at the intersection of the HCI and ML communities, to better understand the cognitive expectations practitioners have on ML systems [8, 42]. Suchlike studies seem all the more relevant as various industry initiatives propagate a humancentric approach to AI, explicitly referring to mental models.2 However, the current scientific discourse lacks a dedicated consideration of cognition in AML. In order to fill this gap, we present the first qualitative study to elicit mental models of adversarial aspects in ML.\n# 3 Methodology\nThis section describes the design of our semi-structured interviews, the drawing task, our recruiting strategy, the participants, and the data analysis. Our methodology was designed to investigate the perception of ML security and is, to the best of our knowledge, the first mental model study of AML.\n# 3.1 Study design and procedure\nTo assess participants\u2019 perceptions, we conducted semistructured interviews enriched with drawing tasks. We draw inspiration from recent work in usable security which also investigated mental models [45,89]. Before the interview, participants were informed about the general purpose of our study and the applied privacy measures. We further assured each participant that their answers would\nTable 1: Participants with their random IDs. Capital letters denote that participants work in the same company. We denote the application domain and the working experience (Exp.) in years. Knowledge in ML, Security and AML is encoded as completed lectures (++), seminar/self-study (+) or none ().\nCompany\nEducation\nID\nApplication domain\nExp.\nML\nSec.\nAML\nDegree\n1\nHuman resources\n7\n++\n+\nPhD\n3\nA\nHealthcare\n0.4\nPhD\n4\nB\nCybsersecurity\n8\n++\n+\nPhD\n6\nC\nBusiness intelligence\n15\n++\n++\n+\nPhD\n7\nComputer vision\n12\n++\nBSc\n9\nComputer vision\n9\n++\nMSc\n10\nCybersecurity\nno questionnaire handed in\n11\nBusiness intelligence\n1\n++\nPhD\n12\nRetail and commerce\n1.4\n++\nPhD\n14\nAI as a service\n5\n++\n+\nPhD\n15\nComputer linguistics\n5\n+\n+\nMSc\n16\nC\nBusiness intelligence\n3\n++\n+\n+\nPhD\n18\nA\nHealthcare\n1.5\n++\nPhD\n19\nB\nCybersecurity\n15\n++\n++\n+\nMSc\n20\nA\nHealthcare\n1.2\n++\nMSc\nnot be judged. Participants were then instructed to complete a questionnaire on demographics, organizational background and a self-reflected familiarity with field-related concepts (Appendix D) before the interview. This questionnaire was filled with or without the authors\u2019 presence. The answers have later been used to put participants\u2019 perceptions in context to their organizational and individual background. The threefold structure of our interviews covered 1) a specification of a given ML project a participant was involved in, 2) the underlying ML pipeline of this project and 3) possible security threats within the project. We chose this approach as the different attack vectors form part of the ML-pipeline as shown in Section 2.1. The detailed interview guideline can be found in Appendix C. As a last step of our interviews, we confronted the participants with exemplary attacker models for some of the threats considered relevant in industrial application of ML [47]. To assess practitioners\u2019 understandings of these threats, study participants had to elaborate on these attack vectors within their specific setup (Appendix D.2). To assess the participants\u2019 knowledge about (A)ML in general, participants were asked to fill an additional questionnaire after the interview (Appendix D.3). In this questionnaire, we tested general knowledge in ML and independently asked for a self-reflected familiarity rating with some of the attacks we discussed in Section 2.1. This questionnaire was handed to the participants after the interview as to avoid priming. We conducted one pilot interview to evaluate our study design. This first participant met all criteria of our target population in terms of employment, education and prior knowledge. As his explanations and drawings matched our expectations, we only added a specific question regarding the collaborators within a given ML-based project.\nThe average interview lasted 40 minutes and was jointly conducted by the first two authors of this paper between April and July 2019. To minimize interviewer biases, we equally distributed the interviews, where one author was the lead interviewer and the other took notes. Due to the COVID-19 pandemic, interviews were conducted remotely and relied on a freely available digital whiteboard3.\n# 3.2 Recruitment\nRecruitment for a study on applied ML in corporate environments presents a challenge, as only a small proportion of the overall population works with ML. Furthermore, the topic touches compliance and intellectual property of participating organizations. Hence, many companies are skeptical about the exchange with third parties. Consequently, many current contributions with industrial practitioners as study participants are conducted by corporate research groups (e.g., [34,47]). We tried to initiate interviews with two large multinational companies. Unfortunately, both denied our request after internal risk assessments. Therefore, we focused on smaller companies where we could present our research project directly to decision-makers and convince them to participate in our study. We relied on the authors\u2019 networks (pilot participant, P11) and public databases for start-ups (more details in Appendix A) to find potential participants and used directmessaging on LinkedIn and emails to get in contact. Recruitment of study participants happened in parallel to interview conduction. Some participants forwarded our interview request to internal colleagues, so that we talked to multiple employees of some participating companies (see Table 1). We aimed to recruit experienced and knowledgeable participants and hence our requirements were a background in ML or computer science and positions such as data scientists, software engineers, product managers, or tech leads. We did not require any prior knowledge in security. After 8 interviews, no new topics (in our case for example new pipeline elements, whether defenses were mentioned, or how attacks were depicted in drawings) emerged. The research team thus agreed after 15 interviews that saturation was reached [15], and we stopped recruiting. The participants were randomly assigned an ID (a number between 1 and 20) which was used throughout our analysis. All participants were offered an euro 20 voucher as compensation for their time.\n# 3.3 Participants\nWe summarize demographic information in Table 1. One participant, P10, did not hand in the questionnaire and is consequently not included in the following statistics. 14 participants identified as male, one identified as female, our sample is thus skewed towards males when considering ML practitioners [40]. As previous work found security perception of\nwomen and men to exhibit only some differences [59], this bias is acceptable for a first exploration but should be studied in depth in future work. Our participants had an average age of 34 years (standard deviation (STD) 4.27). As intended for a first exploration of practitioners\u2019 perception of AML, our sample covered various application domains and organizational roles which we now describe in detail. Education and prior knowledge. The majority of participants (9 of 14) has a PhD, with all participants holding some academic degree. While our sample skews towards PhDs compared to the overall population of ML practitioners [40], previous work reports no correlation between overall education and security awareness [14]. Most participants (12 of 14) reported that they had attended lectures or seminars on ML. Roughly half (6 of 14) reported to have a similar background in security. To obtain a more objective measure we conducted a test about ML knowledge and asked participants to rate their familiarity with AML attacks (details in Appendix B). While we found that all participants were indeed knowledgeable in ML, we found that few attacks were well known to them. Employment. Regarding the size of the companies, four participants worked in companies with less than ten employees, five in companies with less than 50 and the remaining six participants in companies with less than 200 employees. The companies\u2019 application areas were as diverse as healthcare, security, human resources, and others. Most participants were working in their current positions 6 years (STD 4.9). Their roles were diverse: Most (8 of 15) were in managing positions. Three were software or ML engineers, three more researchers. One of the participants stated to be both a researcher and a founder. One participant did not report his role. Finally, we asked participants to report which goals were part of their companies\u2019 AI/ML checklist. Almost all participants (13 of 14) reported that performance mattered in their company. Half (7 of 14) stated that privacy was important. Slightly less than half (6 of 14) focused on explainability and security. Least participants (4 of 14) listed fairness as a goal in their products. To conclude, when interpreting these numbers, one should keep in mind that not all five goals apply equally to all application domains. Furthermore, our sample is too small to derive per area or per company insights, and we thus leave a detailed analysis for future work.\n# 3.4 Data analysis\nWe adopted an inductive approach, where we followed recent work in social sciences and usable security that constructed theories based on qualitative data [45,63]. To distill observable patterns in interview transcripts and drawings, we applied two rounds of open coding, e.g. we assigned one or several codes to sentences, words, or parts of the drawings. We then performed Strauss and Corbin\u2019s descriptive axial coding to group our data into categories and selective coding to relate these categories to our research questions [77]. Throughout\nthe coding process, we used analytic memos to keep track of thoughts about emerging themes. The final set of codes for interview transcripts and drawings is listed in Appendix E. As a first step, the first two authors independently conducted open coding sentence by sentence and sketch by sketch. This allowed for the generation of new codes without predefined hypotheses. Afterwards, the resulting codes were discussed and the research team agreed on adding specific codes for text snippets relating to the confusion of standard security and AML. As a second step, two coders independently coded the data again. After all iterations of coding, conflicts were resolved and the codebook was adapted accordingly. During axial coding, the obtained codes were grouped into categories. The first two authors independently came up with proposed categories which have then been discussed within an in-person meeting. While the grouping was undisputed for some of the categories presented in Appendix E (e.g. AML attacks, pipeline elements), for others the research team decided for (e.g. confusion, relevance) or against (e.g. type of ML model applied) the inclusion of a corresponding category only after detailed discussion. In addition, dedicated codes for the perception of participants (e.g. perceives AML as a feature, not a bug or security issue) were added to the codebook. Once the research team agreed on a final codebook, all transcripts and drawings were coded again using corresponding software.4 In doing so, we aimed for inferring contextual statements instead of singular entities. The codes and categories served as a baseline for selective coding. Independently, the researchers came up with observations and proposals for specific mental models. Every proposal included a definition of the observation, related codes, exemplary quotes and drawings. The first two authors then met multiple times to discuss the observations and the corresponding relations of codes and categories. The resulting code tree contains 77 interview codes in 12 groups, 44 for drawings (in 5 groups), as depicted in Appendix E. Over all interviews, the coders agreed on 989 codes while disagreeing on 136. Analogously, there were 275 codes on drawings in total, with 42 disagreements. We further calculated Cohen\u2019s kappa [23] to measure the level of agreement among the coders. For interview transcripts, we reached \u03ba = 0.71; for the codes assigned to drawings \u03ba = 0.85. These values indicate a good level of coding agreement since both values are greater than 0.61 [50]. Given the semi-technical nature of our codebook, we consider these values as substantial inter-coder agreement. Irrespective of this and in line with best practices in qualitative research, we believe that it is important to elaborate how and why disagreements in coding arose and disclose the insights gained from discussions about them. Each coder brought a unique perspective on the topic that contributed to a more complete picture. Due to the diverse background of our research team in AML, usable security and\neconomic geography, most conflicts arose regarding the relevance of technical and organizational elements of transcripts and drawings. These were resolved during conceptual and on-the-spot discussions within the research team.\n# 3.5 Expectations of mental models\nGiven previous work on mental models and ML, we designed our study in a way that participants would first visualize their pipeline and later add corresponding attacks and defenses. For the pipeline, we expected that participants would name basic steps or components, such as data (collection), training, and testing. In general, we assumed participants\u2019 descriptions would vary in technical detail. Regarding AML, one of our motivations to conduct this study was to learn which knowledge our participants had. As a recent phenomenon, AML might not be known at all in practice, although practitioners might be aware of attacks relevant to their specific application. In particular, we did not expect practitioners to depict attacks using a starting and target point, as done in Figure 1.\n# 3.6 Ethical considerations\nThe ethical review board of our university reviewed and approved our study design. We limited the collection of personal data as much as possible and used ID\u2019s for participants throughout the analysis. Since all participants were employed at existing companies and partially shared business-critical information, we aimed to avoid company-specific disclosures in this paper. Finally, we complied with both local privacy regulations and the general data protection regulation (GDPR).\n# 4 Empirical results\nIn this section, we discuss our findings from the interviews and drawings. Given the unexplored nature of mental models of AML, we focus on two main facets, and discuss additional findings that require a more in depth analysis (in the sense of future work) at the end of this section. The first of the two main facets is the (mingled) relationship between ML security (AML) and security unrelated to ML (non-AML security). We found that our participants, while not referring to AML and non-AML security interchangeably, still exhibited an often vague boundary between the two topics. The second facet concerns the view on ML as part of a larger workflow or product in industry, as opposed to the focus on an isolated model in academia. As a description of a high level workflow requires a high level perspective, we investigate whether it is equivalent to one, which we find not to be true. Afterwards, we then discuss potential facets requiring a more in depth investigation: the application setting, prior knowledge of the participant, and the perceived relevance of AML.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/763c/763c3fa0-1961-4e18-92bc-84a3bf648afc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">AML Research</div>\n<div style=\"text-align: center;\">Our Findings</div>\nFigure 2: High-level intuition Section 4.1. While in research, non-AML security and AML are rather distinct, our participants do not always clearly distinguish the two fields.\n# 4.1 Non-AML security and AML\nNon-AML security deals with the protection against digital attacks in general. In our case, it encompasses topics like access control, cryptography, malicious code execution, etc. Non-AML security provides sound solutions by deploying defenses or implementing design choices. In AML, threats are much more connected with the functioning of ML. For many AML attacks, it is unclear which defenses work due to the ongoing arms-race. Although both topics are conceptually different, we found that our participants did not distinguish between security unrelated to ML and AML, as visualized in Figure 2. In our interviews, on the one hand, the boundary between non-AML security and AML often appeared blurry or unclear, with the corresponding concepts intertwined. On the other hand, there were crucial differences in the perception between non-AML security and AML threats. One difference is that whereas security defenses were often clearly stated as such, AML mitigations5 were often applied without security incentives. Finally, we find a tendency to not believe in AML threats. Many participants denied responsibility, doubted an attacker would benefit, or stated the attack does not exist in the wild. There was no such tendency in non-AML security.\n# 4.1.1 Mingling AML and non-AML security\nWe first provide examples showing that non-AML security and AML were not distinguished by our participants. Afterwards, we investigate if non-AML security and AML are used interchangeably, by investigating the co-occurrence of codes. Vagueness of the boundary between security and AML. There are many examples for a vague boundary between nonAML security and AML. For example P20 reasoned about evasion: \u201cthis would require someone to exactly know how we deploy, right? and, where we deploy to, and which keys we use.\u201d At the beginning, the scenario seems unclear, but the reference to (cryptographic) keys or access tokens shows that the participant has moved to classical security. Analo-\ngously, when P18 reasoned about membership inference: \u201cbut that could be only if you break in [...] if you login in to our computer and then do some data manipulation.\u201d Again, this participant was reasoning about failed access control as opposed to an AML attack via an API. Sometimes, ambiguity in naming confused our participants. For example, P11 thought aloud: \u201cpoisoning [...] the only way to install a backdoor into our models would be that we use python modules that are somewhat wicked or have a backdoor.\u201d In this case, the term \u2018backdoor\u2019 in our questionnaire caused a non-AML security mindset involving libraries in contrary to our original intention to query participants about neural network backdoors. The same reasoning can also be seen in P11\u2019s drawing (compare Figure 3), where \u2018backdoor\u2019 points to python modules. Finally, P12 stated: \u201cmaybe the poisoning will be for the neural network. From our point of view you would have to get through the Google cloud infrastructure.\u201d From an AML perspective, the attack is carried out via data which is uploaded from the user. Yet, the infrastructure is perceived as an obstacle for the attack.\nCorrelations between non-AML security and AML attacks. In the previous paragraph, we showed that the boundaries between AML and non-AML security are blurred in our interviews. Another example is P6 reasoning about IP loss: \u201cwe are very much concerned I\u2019d say the models themselves and the training data we have that is a concern if people steal that would be bad.\u201d In this case, it is left out how the attack is performed. Analogously, P9 remarked: \u201cWe could of course deploy our models on the Android phones but we don\u2019t want anybody to steal our models.\u201d To investigate whether our participants are more concerned about some property or feature (data, IP, the model functionality) than about how it is stolen or harmed, we examined the co-occurrence of AML and non-AML security codes that refer to similar properties in our interviews. For example, the codes \u2018model stealing\u2019 and \u2018code breach\u2019 both describe a potential loss of the model (albeit the security version is broader). Both codes occur together six times, with \u2018code breach\u2019 being tagged one additional time. Furthermore, the code \u2018model reverse engineering\u2019, listed only two times, occurs both times with both \u2018model stealing\u2019 and \u2018code breach\u2019. However, not all cases are that clear. For example \u2018membership inference\u2019 and \u2018data breach\u2019 only occur together two times. The individual codes are more frequent, and were mentioned by three (\u2018membership inference\u2019) and eleven (\u2018data breach\u2019) participants. Analogously, attacks on availability (such as DDoS) in ML and non-AML security were only mentioned once together. Such availability attacks were brought up in an ML context twice, in non-AML security four times. Codes like \u2018evasion\u2019 and \u2018poisoning\u2019, in contrast, are not particularly related to any non-AML security concern. We conclude that AML and security are not interchangeable in our participants\u2019 mental models to refer to attacks with a shared goal.\nIn the previous subsection, we found that our participants did not distinguish non-AML security and AML. To show that this is not true in general, we now focus on the differences between the two topics. To this end, we start with the perception of defenses and then consider the overall perception of threats in AML and security. We conclude with a brief remark on the practical relevance of AML. Defenses. Out of fifteen interviews, in thirteen some kind of defense or mitigation was mentioned; whereas all corresponding interviewees mentioned a non-AML security defense (encryption, passwords, sand-boxing, etc). An AML mitigation appeared in eight. In contrast to security defenses, however, AML defenses were often implemented as part of the pipeline, and not seen in relation to security or AML. As an example, P9, P15, and P18 reported to have humans in the loop, however not for defensive purposes. P10 and P16 were aware that this makes an attack more difficult. For example, P16 stated: \u201cmaybe this poisoning of the data [...] is potentially more possible. There, we would have to manually check the data itself. We don\u2019t [...] blindly trust feedback from the user.\u201d Analogous observations hold techniques like explainable models (3 participants apply, 1 on purpose) or retraining (2 apply, additional 2 as mitigation). For example, P14 said: \u201cwhen we find high entropy in the confidences of the data [...] for those kind of specific ranges we send them back to the data sets to train a second version of the algorithm.\u201d In this case, retraining was used to improve the algorithm, not as a mitigation. We conclude that albeit no definite solution to vulnerability exists, many techniques that increase the difficulty for an attacker are implemented by our participants. At the same time, many practitioners are unaware which techniques potentially make an attack harder. Perception of threats. There is also a huge difference in the perception of threats in non-AML security and AML. In security, threats were somewhat taken for granted. For example, P9 was concerned about security of the server\u2019s passwords \u201cbecause anybody can reverse-engineer or sniff it or something.\u201d Analogously, P6 said to pay attention to \u201cthe infrastructure so that means that the network the machines but also the application layer we need to look at libraries.\u201d On the other hand, almost a third of our participants (4 of 15) externalized responsibility for AML threats. For example, P3 said their \u201cmain vulnerability from that perspective would probably be more the client would be compromised.\u201d Analogously, P1 remarked that ML security was a \u201cconcern of the other teams.\u201d In both cases, the participants referred to another entity, and reasoned that they were not in charge to alleviate risks. Other reasons not to act include participants not having encountered an AML threat yet, and concluded AML was not relevant. More concretely, P9 remarked: \u201cwe also have a community feature where people can upload images. And there could be some issues where people could try\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/029a/029acd19-9dfc-4fda-ae5e-1f9c7819d533.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Drawing of P11. Red markings were added by the participant before, blue after being confronted with selected attacks.</div>\nto upload not safe or try to get around something. But we have not observed that much yet. So it\u2019s not really a concern, poisoning.\u201d Roughly half of the participants (7 of 15) reported to doubt the attackers\u2019 motivation or capabilities in the real world. For example, P1 said: \u201cI have a hard time imagining right now in our use-cases what an attacker might gain from deploying such attacks.\u201d P20, who worked in the medical domain, stated: \u201cI\u2019m left thinking, like, why, what could you, achieve from that, by fooling our model. I\u2019m not sure what the benefit is for whoever is trying to do that.\u201d Finally, many participants (9 of 15) believed that they have techniques in place which function as defenses. As an in-depth evaluation of which mitigations are effective in which setting is beyond the scope of this paper, we leave it for future work. Practical relevance of AML. The fact that most participants did not consider AML threats relevant might be an expression of these threats being academic and not occurring in practice. Yet, our interviews showed that there are already variants of AML attacks in the wild. More concretely, P10 stated: \u201cWhat we found is [...] common criminals doing semiautomated fraud using gaps in the AI or the processes, but they probably don\u2019t know what AML, like adversarial machine learning is and that they are doing that. So we have seen plenty of cases are intentional circumventions, we haven\u2019t quite seen like systematic scientific approaches to crime.\u201d Our participants lack of concern might then be an indicator that harmful AML attacks are (still) rare in practice.\n# 4.1.3 Summary\nWe found that non-AML security and AML were mingled in our participants\u2019 mental models: the boundaries between the corresponding threats were often unclear. Yet, security and AML were not interchangeably used to refer to attacks with a shared goal. Furthermore, non-AML security threats were treated differently than AML threats: the latter were often considered less relevant. Whereas it remains an open question whether AML and non-AML security should be treated differently in practice, the fact that they are currently\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b94a/b94a0c81-fa6c-4cb0-9dd6-c408e64abfbc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Our Findings</div>\nFigure 4: High-level intuition of Section 4.2. While AML research studies individual models, our participants often describe workflows with potentially several models, sometimes even the embedding system of the ML project.\npoorly distinguished might due to low exposure to AML. At the same time, our interviews provided evidence for AML attacks in practice.\n# 4.2 ML models and ML workflows\nMany of our participants did not only refer to an ML model, but discussed a workflow or an entire system. This is in stark contrast to AML research, where models are often studied in isolation, possibly due to a lack of available data. This finding is visualized in Figure 4. In this subsection, we first discuss our participants view on ML models and the described systems. We then investigate whether such views are equivalent to a high level view on ML related projects, and conclude the section with a short discussion on some of our participants\u2019 struggles to assess threats at a high level.\n# 4.2.1 Model versus system view\nWe first focus on the description of the ML model itself. Afterwards, we describe practitioners\u2019 views of ML models within larger systems and conclude the section with relating both findings to the technical level of abstraction. ML model perspective. The general perception of the ML\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1ab/b1ab9b38-e0d1-4c9e-9cb0-8bd3184f7d5a.png\" style=\"width: 50%;\"></div>\npipeline (Figure 1) seems to affect mainly the relevance of ML-models as such within the pipeline. More concretely, participants talked about models as pipeline components. Many (11 of 15) of our participants presented their projects in chronological order or with an implicit flow. Examples are visible in Figure 3 or Figure 6. Moreover, 6 out of 15 participants explained a pipeline not only as being composed by several steps, but remarked potentially several applications of ML within, or that several (different) pipelines exist. For example, P14 reported that \u201cthe models are chained one after the other,\u201d and P7 stated that \u201cwe have both like unsupervised training and unsupervised training.\u201d We conclude that often there is not a single model deployed, but data may be processed by several models, potentially in sequential order. System perspective. Moreover, participants showed a strong focus on the surrounding or embedding of their MLbased project. In other words, not only the pipeline around the model was important, but also the surrounding infrastructure of the project. Out of 15 participants, 5 described their ML pipeline as a classifier as embedded into the larger project context (for example visible in Figure 3 or Figure 5). Related to this embedding, in two of the interviews, the topic of technical debt (or long-term maintenance) arose. In this context, P6 stated: \u201chow [...] we can also have to something that is maintainable in the long term.\u201d\n# 4.2.2 Technical abstraction level\nThe previous findings suggest a high level of technical abstraction in our interviews. While this is true on average, some (5 of 15) participants described their project minutely. For example P12 described their application almost at the code level: \u201c[...] we want to have for each node, that is basically the union of those two columns [...].\u201d However, whereas the same participants also described their project as a workflow, they did not talk about the embedding of the project. On the other hand, P18 remarked on their \u201csupervising\u201d (e.g., high level) perspective, yet provided no context. We conclude that our\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9e81/9e81cd70-4abd-4f37-92e8-b7fb6b6a4ada.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Drawing of P18. Red star indicates the most important component of the pipeline, not an attack.</div>\nsample does not allow conclusions about the level of technical abstraction and perspective on ML model, which is thus left for future work. We did find, however, that a high level perspective seemed to make threat assessment harder for at least some participants. Asked to specify a certain threat model, P19 stated for example: \u201cIt\u2019s like everywhere. Internal threats, external threats. Trying to mess with the communication, trying to mess if we model something.\u201d In a similar manner, P14 explained that an adversary could \u201ctry to put some pythons in non conforming ways to trigger networks.\u201d Both descriptions are hard to interpret in technical terms, although both participants seemed aware of security threats in general. The same problem persists for defenses that our participants apply to encounter AML-specific security threats. P18, for example, first explained that \u201cthe countermeasures are all in the API.\u201d After rechecking the documentation, the participant was able to provide further details on the applied defenses.\n# 4.2.3 Summary\nOur findings illustrate an important point which at the same time is very intuitive. Whereas most research papers focus on a single model when investigating ML security, in practice, models are trained and deployed in the context of other models or as components of larger workflows. At the same time, one pipeline may also contain several applications of ML. These views are not to be confused with the technical detail of a projects\u2019 description. We furthermore find evidence that the right level of detail is crucial to providing useful information.\n# 4.3 Additional facets of mental models\nEliciting mental models with only fifteen interviews seems ambitious, in particular in the context of a technique so versatile as ML. In the following, we thus discuss potential aspects of mental models that have to be studied in more depth in future work. These aspects include, but are not limited to the application setting, the effect of prior knowledge, and the perceived relevance of AML. We also found evidence of structural and functional components in our participant\u2019s mental models. As the occurrence of these in AML mental models can be anticipated from prior work in mental models [89], we leave the corresponding discussion to Appendix F.\nOur sample is too small to make general statements about the application area. However, since almost a third (4 of 15) participants work in cybersecurity, we attempt to investigate whether working in security affects sensitivity to AML. Hence, we first divide the participants into security and nonsecurity groups, starting with participants working in securityrelated fields. P10, who worked in a setting with cybersecurity reported: \u201cthere is some standard AML attacks on ML you can use, but we design our system knowing that very well; on the other hand, we know that there is no perfect security, so, again defense is in monitoring and vigilance, but it\u2019s not something that can be fully automated in our opinion.\u201d P10 was in general very sensitive towards AML. P4, also from a cybersecurity setting, was less concerned about evasion: \u201cI can\u2019t imagine yet how it can be applied for real life, for example [...] since we are pretty close on our development.\u201d Yet, P4 also stated the need to gather more information about AML. Hence, also participants who worked in security-related areas had diverse mental models with respect to concrete attacks. Participants from non-security fields have similarly diverse mental models. This diversity is also reflected in the drawings. P11 (Figure 3) added some attacks (in red) before we provided explanations of evasion, backdooring and membership inference (added in blue). P18 (Figure 6), on the other hand, did not add any threats in their drawing. Analogously, opinions also differ in the interviews; e.g., P15 who worked in an non-security setting, was aware of security issues: \u201cone interesting thing of course is that the solution is in some ways constraint by adversarial security considerations so for example you cannot use natural language generation very much because of potential adversarial behavior.\u201d On the other hand, and confirming the drawing, P18 reported that \u201cwe do not really protect the machine learning part.\u201d Investigating the diversity of mental models induced by the application area in more depth is thus left for future work.\n# 4.3.2 Prior knowledge\nAnother potential factor on a practitioner\u2019s mental model is knowledge about or exposure to the topic at hand. However, we find no strong relation between education and capability or knowledge about AML in our sample. For example, one participant self-reported high knowledge in AML, but also stated: \u201cmaybe the poisoning will be for the neural network.\u201d Here, a general attack, poisoning, is related to a specific model (neural networks). On the other end of the spectrum, P9 did not self-report any knowledge about security or AML, but correctly remarked: \u201cSomebody could send us 100.000 images and collect all the results and try to build a model from that.\u201d We conclude that in our sample, self-reported prior knowledge is not related to AML knowledge. Yet, more work is needed to understand more in depth the complex relationship between exposure, education, and mental models of AML.\nLast but not least, we found little awareness of AML in our sample. As already discussed in section 4.1.2, this might be a consequence of little exposure to AML attacks in the wild. On the other hand, we found all levels of concern about AML in our sample. More concretely, a third of our the practitioners (5 of 15) did not mention AML at all before we explicitly asked. Another third reported that they were not very concerned about AML. For example, P1 stated that evasion, or \u201cinjecting malicious data to basically make the model [...] predict the wrong things\u201d was \u201ca concern that is not as high on my priority list.\u201d P15, analogously, said: \u201cmainly the machine learning pipeline this is the less critical security problem,\u201d reasoning that \u201csimply a performance would be unexpected.\u201d Yet, over a third (6 of 15) of the participants reported to feel insecure about AML when confronted with the topic. Of these six participants, two previously showed low priority on AML, and three did not mention AML at all. An example of insecurity is P4, who stated they needed \u201csome more research on it.\u201d Some participants, like P19, were concerned about specific attacks: \u201cI maybe need to learn more about this membership.\u201d In summary, some practitioners consider AML threats important, whereas some participants did not know AML well, and yet others did not consider it an important threat. From each of these three groups, there was at least one participant that felt not well informed. After the interviews (e.g., off the record) some participants stated that their awareness for AML had increased due to the interview. Many also inquired about defenses against specific threats, further confirming that they were indeed concerned about specific attacks.\n# 5 Future work\nOur findings expose the lack of knowledge about AML in practice, and thus show the need for additional research at the intersection of AML and cognitive science. In this section, we summarize these potential directions of future work. We first discuss theoretical research on mental models of AML and secondly more practical research that applies findings derived from mental models to AML.\n# 5.1 A theory on mental models of AML\nOur work is a first step to describe mental models of AML. For well-grounded mental models, more research is needed to investigate different aspects, as discussed in the previous section about the technical detail, application area and prior knowledge, for example. However, more research is also required concerning the development of mental models, and how a user based threat taxonomy (as opposed to a research based taxonomy) could look like. Temporal evolvement of mental models of AML. A better understanding about the development of individual mental\nmodels could help to assess necessary steps to make practitioners take into account AML. In addition, research on how mental models are shared between various AI practitioners might help to implement adequate defenses within and across corporate workflows. Corresponding starting points can be found in cognitive science [61], where the convergence of mental models has been studied as a three-phase process of orientation, differentiation and integration [43]. Inherent threat taxonomies of mental models. Whereas academia has proposed clear threat models in ML security, it is unclear whether or to which degree these are also used or useful in practice. In this context, it could be interesting to consider existing taxonomies by Biggio et al. [10] and Barreno et al. [6]. These frameworks seem promising to investigate which specific structural elements practitioners consider relevant for specific attack vectors and how they perceive the causal evolution of these attacks. In line with recent work by Wang et al. [86], such user-centric attack taxonomies might help to understand practitioners\u2019 reasoning on AML.\n# 5.2 Applying mental models to AML\nSecondly, but not less important, is the question how AML research can benefit from the study of mental models and which problems could be tackled in this context. Examples include the usability of AML tools and libraries, a more realistic threat modelling in AML research as well as a general assessment of AML attacks in the wild. Utility and usability of AML tools and libraries. We found that practitioners\u2019 mental models depend on available and provided information. Future research should therefore elaborate on the needed specificity of the available information. Furthermore, an evaluation of the available AML tools and libraries with regards to capabilities and needs of industrial practitioners might ease their usage across application domains. In line with recent work on fairness [52] and ethics [22], we consider this crucial for designing usable and accessible tools, corporate guidelines and regulations. Practical threat modelling for AML research. As stated in Section 2, AML research has been criticized for the limited practical relevance of its threat models [28,31]. Mental models could alleviate this issue in two ways. On the one hand, understanding which threats occur in which applications and how they are perceived helps to shift research towards designing practical and usable defenses. On the other hand, a deeper understanding of why non-AML security and AML are mingled allows us to adapt and improve current threat modelling. To this end, however, it is also important to know which threats need to be studied in the first place. AML in the wild. Given the previous insight and evidence of semi-automated, ML-related fraud, a more detailed assessment of which attacks are conducted in the wild would be beneficial. Future work could investigate this with a focus on different groups of ML practitioners, including for example\nML engineers, auditors, and researchers, or dependant on the application. Furthermore, our work outlines that the model perspective usually taken in AML is of limited use in practice. More work is needed to study AML in the context of entire ML pipelines and end-to-end workflows.\n# 6 Practical implications\nSimilar to Kumar et al. [47], we find that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. Given that we found only reports of semi-automated fraud in our sample in Section 4.1.2, the absence of strong AML in practice might explain this lack of knowledge. Yet, as discussed in Section 4.3.3, 6 of 15 participants felt insecure about ML security. We thus now discuss the diverse implications of our study on how to tackle these insecurities and the overall lack of knowledge. We start with the question how to raise awareness for AML. Afterwards, discuss the implications of our findings for the the embedding of AML in corporate workflows and finish with implications for regulatory frameworks of AML. Raising awareness of and increasing confidence about AML. Although we did not ask about privacy specifically, the general data protection regulation was often mentioned by our participants. For example, P6 stated: \u201cwe are also subject to GDPR so we cannot just ignore the security aspects of the process.\u201d Like other participants (P12, P18), P6 mentioned GDPR before we had asked about membership inference and thus privacy. Legislation might thus be a tool to increase awareness of AML. Independently, a third of our participants felt insecure about AML (Section 4.3.3). Given that several participants reported used software (P9, P14, for example \u201cTensorFlow\u201d), infrastructure (P14) or service provider (P3, P12, P20, for example \u201cGoogle\u201d), advertising tools to assess AML risks might be helpful for our participants. In particular as AML libraries6, but also overviews like the Adversarial ML Threat Matrix7 already exist. Our findings on the confusion between AML and non-AML security (Section 4.1.2) suggest these tools need to either enforce dedicated audits for both AML and non-AML security or combined countermeasures to address both areas jointly. Another solution to the feeling of insecurity, reported by our participants themselves (Section 4.3.3, P19: \u201cI maybe need to learn more about this membership\u201d), could be to provide materials for education. Embedding AML into corporate workflows. Whereas academia generally studies AML with the perspective of an individual model, in practice, the entire ML pipeline and broader AI workflow need to be considered. As discussed in Section 4.2, in our interviews, for example P6 and P16 (see Figure 5) described the entire workflow of their AI application, whereas other participants focused on the ML pipelines\n(for example P18, as visible in Figure 6). To successfully integrate AML into corporate workflows, however, more effort is needed. All actors working on an ML product need to be able to identify relevant and possible attacks and implementable defenses. Potential factors to consider here are for example different applications areas, as discussed in Section 4.3.1. Also the existing knowledge of the target audience should be considered, as the in Section 4.3.2 discussed variation of knowledge in our sample shows. Creating appropriate regulatory and standardization frameworks for AML. Lastly, our study has implications for regulatory approaches that enable appropriate security assessments. The differences in application (Section 4.3.1) and prior knowledge (Section 4.3.2) we found imply that regulatory frameworks need to find a way to formally encompass these differences with regards to necessary security measures. The currently proposed \u2018Legal Framework for AI\u2019 by the European Commission, for example, differentiates certain types of ML applications of which some are prohibited or classified as high-risk and thus require a certain risk management. Furthermore, as discussed in Section 4.2, our results indicate that it is essential to communicate such frameworks at the right technical abstraction level to encompass both technical ML practitioners and non-technical stakeholders. Standardization efforts could incorporate this requirement by providing adequate information at multiple mental abstraction levels [18]. For example, recently proposed frameworks like the NIST Taxonomy and Terminology of AML8 explicitly lists references that might help practitioners develop more complex mental models. As mentioned above, a similar regulatory approach to privacy, the European general data protection regulation, had served as a scaffold for their privacy perception.\n# 7 Limitations\nWe followed an inductive approach to investigate mental models through qualitative analysis. Hence, the data collected is self-reported and subjected to a coding process. We continued coding and refining codes until a good level of inter-coder agreement was reached. Nonetheless, all our findings are subject to interpretation and do not generalize beyond the sample, both of which is inherent to qualitative analyses. Finally, due to the COVID-19 pandemic, all interviews were conducted remotely and the interface limitations of the digital whiteboard might have impacted the participants\u2019 sketches. Given the qualitative approach and reached saturation, the small sample size of 15 is indeed acceptable [29, 89]. Due to the small sample size, however, several factors cannot be addresses in depth, as discussed in Section 4.3. Examples include, but are not limited to, the application setting and the perceived relevance. Ideally, future work provides a more in depth analysis of these topics in a larger quantitative study.\nAll participants were employed at European organizations with <200 employees. This is due to the fact that while several multinational companies stated great interest in our research, they denied participation after internal risk assessments. As mental models of ML systems are always embedded in organizational practices [91], we strongly encourage future research to assess our findings within larger samples including more variety, for example academics, small and large companies. Given that previous work found differences in general security behavior depending on gender [59], and cultural background [46], we also strongly encourage a more in depth analysis of these aspects. Furthermore, AML itself is a subject of study of which the perception evolves continuously. With an increasing awareness for security within applied machine learning, the findings presented can only be valid temporarily. Machine learning is applied in a wide range of settings. Consequently, not all attacks are relevant within each application domain. For example, a healthcare setting is subjected to other threats than a cybersecurity setting. For the sake of studying abstract facets of mental models, we did not consider the application in the present work. Yet, we would like to point out the necessity to study this aspect of AML in general.\n# 8 Conclusion\nBased on our semi-structured interviews with industrial practitioners, we take a first step towards a theory of mental models of AML. We described two facets of practitioners\u2019 mental models and sketched more facets as an anchor for in-depth investigation by future work. These include the technical abstraction level, application setting, prior education, and the perceived relevance of AML. We provided more details on the first facet, or the blurry relationship between AML and non-AML security. These two topics were often mingled, yet not used interchangeably by our participants. The second facet can be understood as a first step to refined threat models in AML research. As apposed to a single model, our participants instead described workflows and relationships between potentially several ML models in a larger system context. A clear understanding of the elicited mental models allows to improve information for practitioners and adjustments of corporate workflows. More concretely, our results help to raise awareness for AML, thus making practitioners feel less insecure. We further suggest that both application area and prior knowledge are considered when embedding AML into corporate workflows. Finally, regulatory frameworks might reduce uncertainty about AML and increase the awareness for possible AML threats. However, a wide range of subsequent research towards an encompassing theory of mental models in AML is still required. Last but nor least, we are convinced that the AML community will benefit from further practical assessment of attacks in practice, as our work already provides evidence of semi-automated fraud in the wild.\nThe authors would like to thank Antoine Gautier, Michael Schilling, the anonymous reviewers and the shepherd for the insightful feedback. This work was supported by the German Federal Ministry of Education and Research (BMBF) through funding for the Center for IT-Security, Privacy and Accountability (CISPA) (FKZ: 16KIS0753) and by BMK, BMDW and the Province of Upper Austria within the COMET program managed by FFG in the COMET S3AI module.\n# References\n# A Details on recruiting\nWe searched online databases like crunchbase 9, AIhubs10, and lists with promising AI start-ups (for example the list by Forbes11) to find potential participants.\n# B Participants\u2019 prior knowledge in (A)ML\nTo measure our participants\u2019 knowledge in ML, we constructed a questionnaire based on ML job interview questions12(Appendix D.3). Given that participants were not informed they had to take a test, we aimed to select a broad range of topics easy to query with multiple choice answers that were not too hard. The questionnaire had 8 questions, with the participants correctly answering on average 6.64 questions (STD 1.14). Guessing would yield an average of 2.66 correct questions. Thus, while we do not know how reliable our questionnaire estimates ML knowledge, we conclude that our participants are indeed knowledgeable in ML. We also investigated the familiarity of our participants with AML attacks. To avoid priming, we asked participants to rate their familiarity after the interview. As sanity checks,\nwe added two rather unknown terms, adversarial initialization [32] and neural trojans [56] (similar to backdoors). The results are depicted in Figure 7. Only one participant reported to be familiar with one attack (evasion). In general, most participants reported to have heard of most common attacks (evasion, poisoning, membership inference, and model stealing). As expected for the sanity check, adversarial initialization and neural trojans were largely unknown.\n# C Interview protocol\nThank you so much for taking the time to give us your perspective on security in machine learning. This study consists in III parts. Part I aims at exploring your role in ML-projects. Part II addresses the underlying machine learning pipeline. In part III, we want to know how you perceive the security of machine learning. In part II and III, please visualize the topics (and relationships between them) that we ask you about. There are no rules, no wrong way to do it, and don\u2019t worry about spelling things perfectly. Nothing is off limits and you can use any feature of the digital whiteboard. After this last part, we will ask you about your knowledge about security of machine learning before this study.\nPart I: Machine learning project \u2022 Can you briefly describe what AI- or machine learningbased project you are currently involved in? \u2022 Can you tell us a bit more about the goal of this project? \u2022 Who else is involved in this project? \u2022 What is your collaborators role in the project?\nPart III: Security within project and pipeline \u2022 Is security something you regularly incorporate into your workflow?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/717d/717d04c6-6797-43f5-b63a-2e6540305086.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Self-reported familiarity of interviewed participants with different attacks on ML. Total of participants is 14, as one participant did not hand in questionnaire.</div>\nFigure 7: Self-reported familiarity of interviewed participants with different attacks on ML. Total of participants is 14, as one participant did not hand in questionnaire.\n# D Questionnaires\n# D.1 Demographics questionnaire\nThank you for participating in our research study about security in machine learning. Please take a couple of minutes to respond to the following questions.\n\u2022 How old are you?\n\u2022 What gender do you identify with?\n\u25a1male\n\u25a1female\n\u25a1\n\u2022 What is your level of education? (please specify highest)\n\u25a1Highschool\n\u25a1Bachelor in\n\u25a1Master / Diploma in\n\u25a1Training / Apprenticeship in\n\u25a1PhD, area:\n\u2022 What is your profession?\n\u2022 What is your role in your team?\n\u2022 How long have you been working in your\ncurrent profession?\n\u2022 What is the number of employees at your\ncompany/organization?\n\u2022 What is the application domain of your product?\n\u2022 Which of these goals are part of your organization\u2019s\nAI/ML-model checklist?\n\u25a1Explainability\n\u25a1Fairness\n\u25a1Privacy\n\u25a1Security\n\u25a1Performance\n\u2022 In which of these areas have you taken a lecture or intense\ncourse? Please add the title of the course if applicable.\n\u25a1Machine Learning\n\u25a1Security\n\u25a1Adversarial Machine Learning\n\u2022 In which of these areas have you taken a seminar, or read\nup on? Please add the title of the seminar/book if applicable.\n\u25a1Machine Learning\n\u25a1Security\n# D.2 Attacks used in Interviews\nPlease read through the following selection of attack vectors and machine learning and explain whether you consider them relevant in your specific project. If yes, please add them to your sketch in a different color. Evasion/ Adversarial Examples. This attack targets a model during deployment. The goal of the attacker is to fool the model: changing its output significantly by altering the input only slightly. An example is to change a picture containing a dog, present it to a cat-dog-classifier, and the model\u2019s output changes from dog to cat. Poisoning. This attack targets the training or optimization phase of the model. The goal of the attacker is to either decrease accuracy significantly, or to install a backdoor. An example is a cat-dog classifier that always classifies images containing a smiley as cat. Privacy/ Membership Inference. This attack targets a model at test-time. The attacker\u2019s goal is to identify individual samples from or even the whole training set. An example is to measure the confidence on an input, as some algorithms tend to be more confident on data they have seen during training. Also over-fitting eases to determine what a classifier was trained on.\n# D.3 ML quiz\nPlease answer the following questions about ML. For each question, please tick at least one box. Question 1. Which loss is used to train DNN? \u25a10/1-loss. \u25a1Cross-entropy loss. \u25a1Hinge-loss. Question 2. What is the difference between classification and regression? \u25a1The kind of labels we fit: reals vs discrete classes. \u25a1Regression is the name of classification in psychology / medical science. \u25a1Regression is for discrete labels, classification for real valued ones. Question 3. What is the difference between L1 and L2 regularization? \u25a1L1 yields sparser solutions. \u25a1L2 yields sparser solutions. \u25a1none - they differ only in few practical applications. Question 4. In the bias-variance trade-off, what does high variance imply? \u25a1The analyzed data shows high variance. \u25a1The clf is overly complex and potentially overfits. \u25a1The data is likely to be classified fair (e.g., low bias). Question 5. Why is Naive Bayes naive? \u25a1Due to historic reasons.\n# E Final set of codes\nThe final set of codes for the interviews is depicted in Table 2, the codes for the drawings in Table 3.\nA. AML attacks\nD. security defenses\nG. organization\nL. perception\nA.1 poisoning\nD.1 sandboxing\nG.1 ML role in project\nL.1 security externalized\nA.2 evasion\nD.2 access control\nG.2 security role in project\nL.2 AML feature not bug\nA.3 model stealing\nD.3 development policy\nG.3 other role on project\nL.3 doubting attacker\nA.4 reverse engineering\nD.4 server register\nG.4 legal constraints\nL.4 believing defense is effective\nA.5 membership inference\nD.5 security testing\nG.5 technical dept of ML\nL.5 has not encountered threat\nA.6 availability\nD.6 data anonymization\nH. customer\nL.6 attacks too specific\nB. AML defenses\nD.7 input data format restrictions\nH.1 requirements\nL.7 insecurity about AML\nB.1 retraining\nE. pipeline elements\nH.2 privacy relevant data\nL.8 unspecific attack\nB.2 interpretability\nE.1 training\nI. cloud\nL.9 holistic attacker specificity\nB.3 basic models\nE.2 design\nI.1 used for security\nL.10 pipeline specific defense\nB.4 ensemble\nE.3 model\nI.2 used but potential security risk\nL.11 importance of data\nB.5 human in the loop\nE.4 data\nI.3 not used because of security\nL.12 high level perspective\nB.6 regularization\nE.5 data labelling\nI.4 neutral\nL.13 coding perspective\nB.7 own implementation\nE.6 data collection\nJ. relevance\nB.8 on purpose\nE.7 data preprocessing\nJ.1 mentioning AML\nC. security threats\nE.8 feature extraction\nJ.2 security low priority\nC.1 data capturing\nE.9 testing\nJ.3 AML low priority\nC.2 access\nE.10 deployment\nJ.4 encountered security issue\nC.3 data breach\nE.11 API\nK. confusion\nC.4 code breach\nE.12 database\nK.1 across ML attacks\nC.5 libraries\nF. pipeline properties\nK.2 security and AML\nC.6 denial of service\nF.1 iterative\nK.3 vagueness of concepts\nC.7 SDK\nF.2 several within project\nK.4 what security means\nC.8 customer\n# F Structural and functional components\nWe found structural and functional components in our participants\u2019 mental models. Structural components cover multiple, constituting entities that an individual perceives as relevant within a given application. In interaction with an ML system, functional components describe an individual\u2019s perception of the relations between the structural elements. As intended, the structure of our interview and drawing task (Appendix C) allowed to investigate these properties on the level of the ML pipeline, of the attack vectors as well as of the defenses.\n# F.1 ML pipeline\nAll participants distinguish clearly separable elements within their ML workflow. The specific composition of these steps defines the structure of a certain ML pipeline. For two participants, this structure reflects the ML pipeline that we introduced in Figure 1. When asked to sketch the kind of pipeline applied, P4 talked about \u201cdata\u201d, \u201ctraining\u201d, \u201ctesting\u201d, and \u201cvisualization\u201d. We argue that these structural components serve as a scaffold for an individual\u2019s mental model. Interestingly, the mental models of 12 out of 15 participants covered additional components that we did not expect prior to the study. The sketches of P3, P7, and P11 (Figure 3), for example, contain explicit elements for data capturing. P1, P9, P12, as well as P20 included dedicated elements representing a specific database to their drawing. Five participants also highlighted structural elements within the deployment environment during the interviews. P14, for example, specified on an API for deployment \u201con several kinds of hardware architectures\u201d.\nAnalogously, P1 described an API that \u201ccan be used to allow the user to interact with the models\u201d Hence, these structural elements concerning data and deployment seem to be of importance for the corresponding mental models. However, the perception of industrial practitioners does no only focus on these structural components but also covers functional aspects. P6 for instance stated that his ML pipeline \u201cforks into a number of different directions and there are also interactions between the different components\u201d. In the corresponding sketch, multiple arrows within and across specific ML models indicate this interconnection of single components. Other drawings include this functional perspective through straight lines connecting the structural components, arrows connecting some of the structural components in a subsequent manner (e.g. P14), and arrows connecting all structural components in a subsequent manner (P18 in Figure 6).\nF.1.1 Attack vectors\n# F.1.1 Attack vectors\nThe identified structural and functional components seem to be similarly relevant for mental models on attack vectors. For any kind of ML-specific threat, participants were able to precisely locate where they situated the corresponding, structural starting point. These have been specifically named during the interview and sketched via labelled arrows (e.g. Figure 3, P11), additional annotations (P11, P15), highlighted parts of potentially vulnerable pipeline components (e.g. Figure 8, P10) or as entire steps within a given ML workflow that have been marked as vulnerable (P9, P20). Strikingly, we saw a wide overlap in the perception of potential focal starting points for attack vectors. Study participants consid-\nA. pipeline elements\nB. pipeline properties\nC. named explicitly\nD. attacks\nE. drawing\nA.1 training\nB.1 iterative\nC.1 hardware\nD.1 no attacks\nE.1 boxes\nA.2 design\nB.2 linear\nC.2 software\nD.2 poisoning\nE.2 symbols\nA.3 model\nB.3 abstracted\nC.3 human\nD.3 evasion\nE.3 inner/outer\nA.4 data\nB.4 several\nC.4 privacy sanititzation\nD.4 membership inference\nE.4 flow within pipeline\nA.5 data labelling\nB.5 explainable\nC.5 output\nD.5 libraries\nE.5 workflow embedding\nA.6 data collection\nB.6 MLaaS\nC.6 classification\nD.6 data collection\nE.6 attacks graphical\nA.7 data preprocessing\nC.7 server\nD.7 input/output\nE.7 attacks words\nA.8 feature extraction\nD.8 unspecific attack\nE.8 attacks causal\nA.9 testing\nD.9 defenses\nE.9 attacks pointwise\nA.10 deployment\nD.10 exit points\nA.11 deployment environment\nD.11 input points\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff14/ff149040-12f7-4807-863d-0d42392db99d.png\" style=\"width: 50%;\"></div>\nFigure 8: Drawing of P10. Important components of the workflow added in blue, possible starting points for attacks in red.\nered the model itself, the input of their ML pipeline, or the deployment environment to be particularly vulnerable. Figure 5 (P16) shows this for the latter. When confronted with poisoning and reverse engineering attacks, P16 marked the input and output of his pipeline as possible starting points for threats (purple rectangles) and talked about how a competitor could \u201cscrew our labeled dataset\u201d or a customer might \u201cask a lot of questions to the API\u201d. However, the perception of attack vectors did also cover functional components. P1, for example, depicted the causal sequence of a \u201cdata injection attack\u201d as three consecutive red arrows connecting different components of his ML pipeline. This is all the more relevant, as P1 provided such a functional explanation and drawing for each of the attack vectors we presented to him. His mental models, hence, clearly seem to contain functional components. This is also the case for P16, who similarly provided explanations on the functional evolvement of certain attacks within his workflow and even added corresponding functional elements to his sketch (blue and red arrows in Figure 5).\n<div style=\"text-align: center;\"></div>\nAlthough we found participants\u2019 defenses explanations and sketches to be rather sparse, structural and functional properties are also relevant for the corresponding mental models. As visible in the sketch of P18, defenses are often thought of as structurally bound to specific components of a workflow/pipeline (Figure 6, P18). Data (P14), training (P6) and the models themselves (P10) have been specifically named as focal points for implementing defenses. In the case of defenses implemented at the model component, P14 stated to \u201cregularize in a way that makes it less sensitive to an adversary\u201d. Hence, these implemented defenses are cognitively attached to the classifier as a focal pipeline component. However, security mental models also contain functional properties. In the case of human-in-the-loop-defenses, for example, P14 stated to send certain classifications \u201cback to the data sets to train a second version of the algorithm\u201d if the output confidence for certain data exhibited high entropy. This is depicted in the corresponding sketch by an arrow pointing from a rectangle with the caption \u201cCPU\u201d at the end of the pipeline to \u201craw data\u201d (initial step of the pipeline). Similarly, P7, a participant working in video surveillance, explained the defense they had implemented to secure the transfer of\ninput data (from cameras and on-site computers) into their pipeline: \u201cThis can only go out, never go in. [...] Nothing from the internet can connect to that server\u201d. Industrial practitioners, hence, perceive defenses as containing functional components to unfold their full effect.\n# F.1.3 Summary\nWe conclude that mental models in AML contain of structural components which are cognitively put into (internal) relation.\nHowever, the specific unfolding of these internal conceptual representations seems to depend on the corresponding application and its underlying ML pipeline.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of practitioners' understanding of adversarial machine learning (AML) and the security challenges associated with machine learning (ML) applications. Despite the increasing use of ML in industry, there is a significant gap in knowledge regarding ML security, particularly concerning the mental models that practitioners hold about AML.",
        "problem": {
            "definition": "The main problem is the lack of adequate understanding among industrial practitioners about the security risks and vulnerabilities associated with ML systems, particularly in the context of adversarial machine learning.",
            "key obstacle": "A key obstacle is the confusion between AML and traditional security concepts, leading to misconceptions and inadequate risk assessment."
        },
        "idea": {
            "intuition": "The idea was inspired by previous studies in other security fields that utilized mental models to improve understanding and communication of security issues.",
            "opinion": "The authors propose that practitioners' mental models of AML need to be clarified and substantiated to enhance their understanding of ML security.",
            "innovation": "The primary innovation lies in the qualitative study that reveals the distinct ways practitioners perceive AML security, contrasting with the isolated model focus prevalent in academic research."
        },
        "Theory": {
            "perspective": "The theoretical perspective emphasizes understanding mental models as cognitive frameworks that shape how practitioners interact with and assess the security of ML systems.",
            "opinion": "The authors assume that mental models play a crucial role in determining how practitioners perceive and respond to AML threats.",
            "proof": "The study provides qualitative evidence through interviews and drawings that illustrate the participants' mental models and their interactions with AML concepts."
        },
        "experiments": {
            "evaluation setting": "The study involved 15 semi-structured interviews with industrial practitioners from various sectors, including healthcare and cybersecurity, to explore their perceptions of AML.",
            "evaluation method": "Interviews were conducted alongside drawing tasks where participants visualized their understanding of the ML pipeline and associated security threats."
        },
        "conclusion": "The study concludes that practitioners often lack a nuanced understanding of AML, leading to blurred boundaries between AML and non-AML security. It highlights the need for better education and awareness of AML to enhance security practices in ML applications.",
        "discussion": {
            "advantage": "The paper provides valuable insights into the mental models of practitioners, which can inform the development of better educational resources and tools for assessing ML security.",
            "limitation": "The study's findings are based on a small sample size, which may not fully represent the diverse perspectives of all ML practitioners.",
            "future work": "Future research should explore the development of mental models over time, the impact of prior knowledge on understanding AML, and the creation of user-centric threat taxonomies."
        },
        "other info": [
            {
                "info1": "The study emphasizes the importance of integrating AML considerations into corporate workflows."
            },
            {
                "info2": {
                    "info2.1": "The research highlights a disconnect between academic AML research and practical concerns faced by industry practitioners.",
                    "info2.2": "The authors suggest that regulatory frameworks could play a role in increasing awareness of AML threats."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The main problem is the lack of adequate understanding among industrial practitioners about the security risks and vulnerabilities associated with ML systems, particularly in the context of adversarial machine learning."
        },
        {
            "section number": "1.2",
            "key information": "The study emphasizes the importance of integrating AML considerations into corporate workflows, highlighting the interconnection between adversarial machine learning and broader security concepts."
        },
        {
            "section number": "5.2",
            "key information": "The study concludes that practitioners often lack a nuanced understanding of AML, leading to blurred boundaries between AML and non-AML security."
        },
        {
            "section number": "7.1",
            "key information": "A key obstacle is the confusion between AML and traditional security concepts, leading to misconceptions and inadequate risk assessment."
        },
        {
            "section number": "7.2",
            "key information": "Future research should explore the development of mental models over time, the impact of prior knowledge on understanding AML, and the creation of user-centric threat taxonomies."
        }
    ],
    "similarity_score": 0.6834620578710362,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Mental Models of Adversarial Machine Learning.json"
}