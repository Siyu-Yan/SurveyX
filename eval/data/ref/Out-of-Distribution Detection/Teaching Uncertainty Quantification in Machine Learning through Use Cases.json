{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2108.08712",
    "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases",
    "abstract": "Uncertainty in machine learning is not generally taught as general knowledge in Machine Learning course curricula. In this paper we propose a short curriculum for a course about uncertainty in machine learning, and complement the course with a selection of use cases, aimed to trigger discussion and let students play with the concepts of uncertainty in a programming setting. Our use cases cover the concept of output uncertainty, Bayesian neural networks and weight distributions, sources of uncertainty, and out of distribution detection. We expect that this curriculum and set of use cases motivates the community to adopt these important concepts into courses for safety in AI.",
    "bib_name": "valdenegrotoro2021teachinguncertaintyquantificationmachine",
    "md_text": "# Teaching Uncertainty Quantification in Machine Learning through Use Cases\nMatias Valdenegro-Toro 1\n# Abstract\nUncertainty in machine learning is not generally taught as general knowledge in Machine Learning course curricula. In this paper we propose a short curriculum for a course about uncertainty in machine learning, and complement the course with a selection of use cases, aimed to trigger discussion and let students play with the concepts of uncertainty in a programming setting. Our use cases cover the concept of output uncertainty, Bayesian neural networks and weight distributions, sources of uncertainty, and out of distribution detection. We expect that this curriculum and set of use cases motivates the community to adopt these important concepts into courses for safety in AI.\n# 1. Introduction\narXiv:2108.08712v\nNeural networks and machine learning models are ubiquitous in real-world applications, but in general model and data uncertainty are not well explored, and this propagates on how machine learning is taught at different levels. Uncertainty is an important concept that should be taught to all students interested in machine learning.\nOverall Uncertainty Quantification of machine learning models (Gawlikowski et al., 2021) is not part of the standard curricula at the undergraduate or graduate level, mostly being present in advanced summer schools (like MLSS, EEML, DeepLearn, SMILES, etc), with some exceptions at graduate courses aimed mostly at theory of Bayesian NNs (BNNs).\nIn this paper we aim to develop a concept for teaching uncertainty quantification in machine learning, first with a short curriculum, and then through different use cases, starting from why we need models with uncertainty and ending at out of distribution detection. We hope that this material can be used for easier planning of future courses.\n1German Research Center for Artificial Intelligence, Bremen, Germany. Correspondence to: Matias Valdenegro-Toro <matias.valdenegro@dfki.de>.\nProceedings of the 2 nd Teaching in Machine Learning Workshop, PMLR, 2021. Copyright 2021 by the author(s).\nProceedings of the 2 nd Teaching in Machine Learning Workshop, PMLR, 2021. Copyright 2021 by the author(s).\nTeaching with clear use cases can be beneficial for student\u2019s learning (Lynn Jr, 1999), specially when they are combined with practical experience.\nUncertainty in ML is a subject that is heavy on probability and statistics, and this is a topic that might not be easy for some students. We believe that having clear use cases for this purpose can help students learn and to clarify concepts. These use cases can be implemented in code using standard machine learning frameworks like Keras, TensorFlow, and PyTorch.\n# 2. Curricula for UQ in ML\nWe first introduce a short curricula template for a uncertainty in machine learning course. This could be a graduate-level course, requiring students to know basic neural networks, machine learning theory, and probability and statistics, as well as having appropriate coding skills in a programming language in order to understand and implement the use cases in a framework of their choice.\nThe overall curriculum is presented in Table 1. Any teacher should of course adapt this course to their institution or student body, and we encourage the teacher to also include seminar-style discussions including state of the art research in BNNs and uncertainty in ML, as this is still a very research heavy field.\nThe ultimate goal of this course is to enable students to perform research in this field, and to apply this knowledge into neighboring task field like Computer Vision, Reinforcement Learning, or Robotics.\n# 3. Use Cases\nIn this section we present a selection of use cases to teach concepts of uncertainty in machine learning settings. These represent what we think are the most difficult concepts for students to grasp, which motivate the application of use cases as teaching methodology.\n# 3.1. Output Uncertainty\nThe best use case to teach the concept of uncertainty at the output of a machine learning model is a simple regression setting, as the output mean can be associated to the output\n<div style=\"text-align: center;\">Teaching UQ in ML through Use Cases</div>\nUnit\nContent\nIntroduction to UQ\nPoint-wise outputs versus distribution outputs in ML models. Two-headed models for\nregression. Sources of uncertainty. Representations of output uncertainty. Applications\nand possible legal requirements. Relationship to Explainable AI. Connections to safety and\ntrustworthiness in AI.\nStatistical Methods\nCategorical, Gaussian, and Dirichlet Distributions. Predictive intervals, Quantile Regression.\nBayesian NNs\nDistribution over weights. Predictive posterior distribution. Inference using Bayes Rule.\nMethods for UQ\nDeep Ensembles (Lakshminarayanan et al., 2016), Monte Carlo methods like Dropout (Gal\n& Ghahramani, 2016) and DropConnect (Mobiny et al., 2019). For advanced courses,\nGaussian Processes (Rasmussen & Williams, 2005) and Markov Chain Monte Carlo methods\n(Betancourt, 2017) can also be included.\nMetrics and Evaluation\nLosses with uncertainty, Entropy, Calibration, Reliability plots, and related calibration metrics\n(Guo et al., 2017). Advanced topics can be proper scoring rules (DeGroot & Fienberg, 1983).\nOut of Distribution\nIn distribution and Out of distribution data. Evaluation protocol with standard datasets\nDetection\n(CIFAR10 vs SVHN, MNIST vs Fashion MNIST). Evaluation using histograms and ROC\ncurves.\nChallenges and\nScalability of BNNs, Generalization of out of distribution detection, Computational\nFuture Research\nperformance, Datasets with uncertainty, and Real-world applications (Valdenegro-Toro,\n2021).\nCurriculum for a graduate course in Uncertainty Quantification in Machine Learn\nof a classical model (without uncertainty), and the standard deviation of the output can be directly associated with the uncertainty in the output. In a classification setting with probabilities associated to each class, it is more difficult to directly see the effect of uncertainty in the model.\nLearning Objective. Students will learn about the difference between a classical machine learning model and one with output uncertainty.\nUse Case. Students will implement a standard neural network using a framework of their or the teacher\u2019s choice. Students will generate data by sampling the following func-\n(1) (2) (3)\nFor the range x \u2208[\u2212\u03c0, \u03c0]. Two neural network models can be used. One is a standard neural network and the other is a ensemble of 5 neural networks (Lakshminarayanan et al., 2016), which is a simple method to estimate uncertainty. An example of this setting can be seen in Figure 1, where output uncertainty is represented as confidence intervals. Students can the visually compare their results, and relate on how the standard neural network does not model the training data points variations, while the neural network with uncertainty does. This is specially noticeable as the standard deviation of the noise is variable, which is not captured with a standard neural network.\nA variation of this exercise is to use a deep ensemble, where each ensemble member has two output heads, one for the\ntask (\u00b5(x)) and another for uncertainty (\u03c32(x)), which can be trained with a negative log-likelihood loss that does not require labels for uncertainty (Eq 5). This exercise helps students see that a model needs to be \u201dadded something\u201d to estimate uncertainty properly, an output head in this case. The uncertainty head \u03c32(x) represents the variance of the output .\n# 3.2. Bayesian Neural Networks\nBayesian Neural Networks are difficult to understand conceptually since the formulation is heavy in probability, and weights are replaced by probability distributions. In this use case we simplify the concept for easy understanding.\nLearning Objective. Students will learn the conceptual differences between a standard and an Bayesian neural network and how it relates to produce uncertainty at the model output.\nUse Case. Students will implement the forward pass of a simple neural network using numpy or a similar linear algebra framework. For a standard neural network, scalar or point-wise weights are used, and for a BNN, weights will be drawn from a given Gaussian distribution (the actual weight values for this use case do not matter). Sampling can be used to produce predictions from a BNN, by sampling a set of weights and producing a forward pass with a given input. Students will compare the outputs given random weights for each of their networks, and compare how the BNN is a stochastic model, meaning that predictions vary with a given input, as different weights are sampled and propagate\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e11b/e11bf6f6-9430-472b-8880-6f9963a1cb6c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/073b/073b6f22-aa96-43cb-b96e-c812f4db75f7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Classic Neural Network</div>\nthrough the network to produce different outputs, but these predictions are not completely random, and are samples of the predictive posterior distribution. In comparison, the standard neural network has fixed predictions with a given input and weights, which cannot model uncertainty. An additional experiment is to vary the depth or width of the network, as a way to increase the number of weights, and see how predictions change in terms of stochasticity.\nthrough the network to produce different outputs, but these predictions are not completely random, and are samples of the predictive posterior distribution.\nIn comparison, the standard neural network has fixed predictions with a given input and weights, which cannot model uncertainty. An additional experiment is to vary the depth or width of the network, as a way to increase the number of weights, and see how predictions change in terms of stochasticity.\n# 3.3. Bayesian NN Intractability\nConnecting to the previous use case, it is well known that inference in BNNs is intractable, due to the high computational complexity required to estimate weight distributions, particularly for highly parameterized neural networks. In this use case we wish the student to form an intuition on why this is the case.\nLearning Objective. Students will learn an intuition on why BNNs are intractable with a thought experiment and validate it with a code implementation.\nUse Case. As a thought experiment, students should think about the predictive posterior distribution (Eq 4), which integrates a term over the weights of the network to produce a distribution output.\n(4)\nFor the experimental setting, students should implement a simple BNN using numpy, with randomly initialized weight distributions (Gaussian distributions can be used for simplicity), and then produce predictions with random data (similarly to the previous use case). But then students are asked to vary their network architectures, increasing depth from a few layer to over 50 layers, or the width from a small number to a large number (over 1024 neurons), and then estimate and plot the computation time as network depth and number of samples is varied.\n<div style=\"text-align: center;\">(b) Neural Network with Output Uncertainty</div>\ncability of BNNs for real-world applications, considering their computational costs. Additional experiments for discussion are the possibilities of computing the integral in Eq 4 with analytical or numerical methods.\n# 3.4. Aleatoric vs Epistemic Uncertainty\nDifferent sources of uncertainty (Der Kiureghian & Ditlevsen, 2009) and their separation (Kendall & Gal, 2017) are not always easy to see and learn intuitively. This use case tries to show the difference with a practical example in a regression setting.\nLearning Objective. Students will learn the difference between aleatoric and epistemic uncertainty through a simple regression problem, and how different parts of the model contribute to these sources of uncertainty.\nUse Case. We will use the same setting as the output uncertainty use case (Sec 3.1), but only a model with uncertainty. Since an ensemble is used to estimate uncertainty in this case, we will use the negative log-likelihood loss formulation to estimate aleatoric uncertainty:\nStudents should train an ensemble of 5 networks, and then plot the predictions separately. First students plot the predictions of the mean output of each ensemble member (which produces epistemic uncertainty), and then separately plot the standard deviation outputs of each ensemble member (which estimate aleatoric uncertainty). This concept is shown in Figure 2.\nStudents then compare both kinds of predictions and try to explain the differences, and how they relate to the epistemic and aleatoric sources of uncertainty. A plot of the training data might also help students visualize aleatoric uncertainty.\n# 3.5. Out of Distribution Detection\nOut of distribution detection entails detecting input samples outside of the training set distribution, through output un-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/014c/014c2519-4870-431c-9b13-d94b14aa9585.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Epistemic Uncertainty</div>\nFigure 2. Comparison between Epistemic and Aleatoric Uncertainty in the Toy Regression example.\ncertainty or other confidence measures. In this setting we present two use cases.\nLearning Objective. Students will learn how to perform and evaluate out of distribution using standard image classification datasets and in a regression toy example, and to get the intuitions on how uncertainty enables the out of distribution detection task.\nClassification Use Case. Using an appropriate neural network framework, students will implement and train a BNN (or an approximation) on the SVHN dataset (ID, indistribution), and evaluate in the train and test splits. Then students are asked to make predictions using their model on the CIFAR10 test set (OOD, out-of-distribution) and to look at the class probabilities that their model produces. Entropy can be used to obtain a single measure for each sample, and then compare the ID vs OOD entropy values using a histogram. The use case can be completed by obtaining a threshold between ID and OOD entropy distributions using an ROC curve, in order to perform out of distribution detection in the wild.\nRegression Use Case. For a toy example in regression, we use the same setting as Sec 3.1, keeping the training set x \u2208 [\u2212\u03c0, \u03c0], and introducing an OOD set of x \u2208[\u22122\u03c0, \u2212\u03c0] \u222a [\u03c0, 2\u03c0]. Students then plot the predictions from their model, noting the values on the two datasets (ID and OOD). A sample result can be seen in Figure 3.\nStudents should compare the output standard deviation produced by their model in the ID and OOD datasets. They should observe that uncertainty as predicted by the output standard deviation should be higher in the OOD data than in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d6b/2d6beebc-bfe9-4cbf-8e4d-e75cb3408e54.png\" style=\"width: 50%;\"></div>\nFigure 3. Out of Distribution Detection in the Toy Regression Example. Values x > \u03c0 and x < \u2212\u03c0 are out of distribution in this example, which triggers large epistemic uncertainty and can be used to detect this condition.\n<div style=\"text-align: center;\">Figure 3. Out of Distribution Detection in the Toy Regression Example. Values x > \u03c0 and x < \u2212\u03c0 are out of distribution in this example, which triggers large epistemic uncertainty and can be used to detect this condition.</div>\nthe ID data, which indicates that the model is extrapolating. Students can add additional evidence of extrapolation by plotting the function f(x) = sin(x) which is the true function that generated the training data, and confirm that the model predictions in the OOD data are very incorrect when compared to the true function, while predictions in the ID data are correct inside the training range ([\u2212\u03c0, \u03c0]). Error metrics like mean absolute error can be used to confirm this difference. The teacher can also show that uncertainty in the OOD set should be proportional to the distance (in input space) from the sample to the edge of the OOD set, and that this proportionality is expected for proper uncertainty quantification. Misconceptions. Students might be confused that some OOD examples have low uncertainty and are easily confused with ID examples. This can be explained with models are not perfect and make mistakes, and this also translates into mistakes in OOD. Another issue is the definition of out of distribution data can be very abstract, as it is an open set that corresponds to anything not in the training data distribution. Multiple OOD datasets can be used to show this.\nAnother issue is the definition of out of distribution data can be very abstract, as it is an open set that corresponds to anything not in the training data distribution. Multiple OOD datasets can be used to show this.\n# 4. Conclusions and Future Work\nIn this paper we have presented a small course curriculum and a selection of use cases to teach students about uncertainty quantification in machine learning models. We hope that this work can motivate the community about the importance of teaching uncertainty quantification and BNNs to students learning about machine learning, and how it relates to the concept of safety in artificial intelligence.\nFuture course contents and use cases can be centered in specific applications of machine learning and artificial intelligence, such as Computer Vision, Robotics, or Autonomous Systems. There is a good demand to connect theoretical fields (BNNs in particular) into practical applications as a way to lead future research.\n# References\nBetancourt, M. A conceptual introduction to hamiltonian monte carlo. arXiv preprint arXiv:1701.02434, 2017. DeGroot, M. H. and Fienberg, S. E. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12\u201322, 1983. Der Kiureghian, A. and Ditlevsen, O. Aleatory or epistemic? does it matter? Structural safety, 31(2):105\u2013112, 2009. Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050\u20131059. PMLR, 2016. Gawlikowski, J., Tassi, C. R. N., Ali, M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599, 2017. Kendall, A. and Gal, Y. What uncertainties do we need in bayesian deep learning for computer vision? arXiv preprint arXiv:1703.04977, 2017. Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016. Lynn Jr, L. E. Teaching and learning with cases: A guidebook. CQ Press, 1999. Mobiny, A., Nguyen, H. V., Moulik, S., Garg, N., and Wu, C. C. Dropconnect is effective in modeling uncertainty of bayesian deep networks. arXiv preprint arXiv:1906.04569, 2019. Rasmussen, C. E. and Williams, C. K. Gaussian processes for machine learning (adaptive computation and machine learning), 2005. Valdenegro-Toro, M. I find your lack of uncertainty in computer vision disturbing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 1263\u20131272, June 2021.\nValdenegro-Toro, M. I find your lack of uncertainty in computer vision disturbing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 1263\u20131272, June 2021.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This paper aims to address the lack of teaching uncertainty quantification in machine learning courses, providing a curriculum and use cases to enhance understanding of this important concept.",
            "scope": "The survey focuses on uncertainty quantification in machine learning, specifically through a proposed curriculum and use cases. It excludes broader topics in machine learning that do not pertain to uncertainty."
        },
        "problem": {
            "definition": "The survey explores the underrepresentation of uncertainty quantification in machine learning education and its implications for students' understanding of model reliability.",
            "key obstacle": "A primary challenge is the complexity of integrating uncertainty quantification into existing curricula, as well as students' difficulties with the probabilistic concepts involved."
        },
        "architecture": {
            "perspective": "The survey introduces a structured approach to teaching uncertainty quantification, categorizing existing research into practical use cases that facilitate learning.",
            "fields/stages": "The survey organizes methods into categories such as output uncertainty, Bayesian neural networks, methods for uncertainty quantification, and out-of-distribution detection."
        },
        "conclusion": {
            "comparisions": "The survey compares various teaching methodologies and their effectiveness in conveying uncertainty concepts, highlighting the benefits of use cases over traditional theoretical approaches.",
            "results": "Key takeaways include the necessity of incorporating uncertainty quantification in machine learning education to improve students' preparedness for real-world applications."
        },
        "discussion": {
            "advantage": "The current research successfully highlights the importance of uncertainty quantification, providing a framework that can enhance machine learning education.",
            "limitation": "A limitation is that the proposed curriculum may not be universally applicable due to varying institutional resources and student backgrounds.",
            "gaps": "There are gaps in the practical application of these concepts in real-world scenarios, as well as a need for further exploration of advanced topics in uncertainty quantification.",
            "future work": "Future research should focus on developing more specialized use cases and exploring practical applications in fields such as computer vision and robotics."
        },
        "other info": {}
    },
    "mount_outline": [
        {
            "section number": "6",
            "key information": "The survey focuses on uncertainty quantification in machine learning, specifically through a proposed curriculum and use cases."
        },
        {
            "section number": "6.1",
            "key information": "The survey explores the underrepresentation of uncertainty quantification in machine learning education and its implications for students' understanding of model reliability."
        },
        {
            "section number": "6.2",
            "key information": "The survey introduces a structured approach to teaching uncertainty quantification, categorizing existing research into practical use cases that facilitate learning."
        },
        {
            "section number": "7.1",
            "key information": "A primary challenge is the complexity of integrating uncertainty quantification into existing curricula, as well as students' difficulties with the probabilistic concepts involved."
        },
        {
            "section number": "7.2",
            "key information": "Future research should focus on developing more specialized use cases and exploring practical applications in fields such as computer vision and robotics."
        }
    ],
    "similarity_score": 0.7208092694528485,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Teaching Uncertainty Quantification in Machine Learning through Use Cases.json"
}