{
    "from": "google",
    "scholar_id": "IRFgfzZre8sJ",
    "detail_id": null,
    "title": "Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation",
    "abstract": " Abstract\n\nStudent modeling is central to many educational technologies as it enables predicting future learning outcomes and designing targeted instructional strategies. However, open-ended learning domains pose challenges for accurately modeling students due to the diverse behaviors and a large space of possible misconceptions. To approach these challenges, we explore the application of large language models (LLMs) for in-context student modeling in open-ended learning domains. More concretely, given a particular student\u2019s attempt on a reference task as observation, the objective is to synthesize the student\u2019s attempt on a target task. We introduce a novel framework, LLM for Student Synthesis (LLM-SS), that leverages an LLM for synthesizing a student\u2019s behavior. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs to boost their student modeling capabilities. We instantiate several methods based on LLM-SS framework and evaluate them using an existing benchmark, S TUDENT S YN, for student attempt synthesis in a visual programming domain. Experimental results show that our methods perform significantly better than the baseline method N EUR SS provided in the S TUDENT S YN benchmark. Furthermore, our method using a fine-tuned version of the GPT-3.5 model is significantly better than using the base GPT-3.5 model and gets close to human tutors\u2019 performance.\n\n# 1 Introduction\n\nStudent modeling refers to the process of representing the current state of a learner\u2019s knowledge, skills, preferences, and learning needs [1]. This is pivotal in developing educational systems as it allows for the personalization of learning experiences [2], catering specifically to each student\u2019s unique abilities and growth areas, and targeted instructional strategies that can significantly enhance the learning process [3]. By understanding student behavior, tutoring systems and educators can identify patterns and trends [4, 5], thereby predicting future learning outcom",
    "bib_name": "nguyen2023large",
    "md_text": "# Large Language Models for In-Context Student Modeling: Synthesizing Student\u2019s Behavior in Visual Programming\n\nUniversity of Vienna, Austria sebastian.tschiatschek@univie.ac.at\n\n# Abstract\n\nStudent modeling is central to many educational technologies as it enables predicting future learning outcomes and designing targeted instructional strategies. However, open-ended learning domains pose challenges for accurately modeling students due to the diverse behaviors and a large space of possible misconceptions. To approach these challenges, we explore the application of large language models (LLMs) for in-context student modeling in open-ended learning domains. More concretely, given a particular student\u2019s attempt on a reference task as observation, the objective is to synthesize the student\u2019s attempt on a target task. We introduce a novel framework, LLM for Student Synthesis (LLM-SS), that leverages an LLM for synthesizing a student\u2019s behavior. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs to boost their student modeling capabilities. We instantiate several methods based on LLM-SS framework and evaluate them using an existing benchmark, S TUDENT S YN, for student attempt synthesis in a visual programming domain. Experimental results show that our methods perform significantly better than the baseline method N EUR SS provided in the S TUDENT S YN benchmark. Furthermore, our method using a fine-tuned version of the GPT-3.5 model is significantly better than using the base GPT-3.5 model and gets close to human tutors\u2019 performance.\n\n# 1 Introduction\n\nStudent modeling refers to the process of representing the current state of a learner\u2019s knowledge, skills, preferences, and learning needs [1]. This is pivotal in developing educational systems as it allows for the personalization of learning experiences [2], catering specifically to each student\u2019s unique abilities and growth areas, and targeted instructional strategies that can significantly enhance the learning process [3]. By understanding student behavior, tutoring systems and educators can identify patterns and trends [4, 5], thereby predicting future learning outcomes [6] and providing timely support. Moreover, it allows them to detect if and when a student is losing interest or facing challenges [7], enabling them to intervene effectively [8]. In particular, student modeling is key in open-ended learning domains where creativity and exploratory behaviors are encouraged [9, 10]\nIn open-ended learning domains such as programming, students can take different learning paths and complete a task with different strategies [9]. This results in diverse behaviors and presents significant challenges to modeling a particular student\u2019s behavior [10]. In recent years, some efforts in student modeling for open-ended learning domains have been made, such as representing knowledge and forecasting future performance using deep learning [6], investigating students\u2019 problem-solving approaches using Natural Language Processing [12], early prediction of conceptual understanding [7], clustering-based methods for misconception discovery [4], students\u2019 attempts synthesis in blockbased visual programming [11], and predicting students\u2019 post-test performance and interest using multimodal predictive student modeling [13]. Existing works on student modeling in open-ended learning domains often require a large behavioral dataset from students or use a complex pipeline, and\n\nPreprint. Accepted at the EDM\u201924 conference.\n\nAdish Singla MPI-SWS, Germany adishs@mpi-sws.org\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/758e/758ebce0-4f10-491e-9fb8-c73e3ca792a2.png\" style=\"width: 50%;\"></div>\n\ufffd\nsometimes, a combination of both [6, 11, 13, 14]. In this paper, we seek to leverage recent advances in generative AI and large language models (LLMs) for student modeling in open-ended learning domains and address the above-mentioned shortcomings.\nIn particular, LLMs have demonstrated advanced capabilities for in-context learning in which a model learns to solve a downstream application scenario when prompted with appropriate contextual information [15, 16]. Notably, they have been used to simulate humans for replicating human subject studies [17] and to simulate students for training teaching assistants [18]. In this work, we investigate the potential of leveraging such capabilities of LLMs for in-context student modeling in open-ended learning environments. In our setup, an LLM observes a student\u2019s attempt on a reference task as the student\u2019s behavioral context, and the objective is to synthesize the student\u2019s attempt on a target task, reflecting the student\u2019s problem-solving style and misconceptions observed. In essence, we seek to address the following research question: Given a specific student\u2019s behavioral context, are LLMs capable of effectively modeling the student and subsequently synthesizing the student\u2019s attempt on a target task?\nTo this end, we introduce a novel framework, LLM for Student Synthesis (LLM-SS), that leverages LLMs for modeling and synthesizing a student\u2019s behavior. The design of our framework is inspired by Perturbation Student Model [19], based on the idea that a student\u2019s knowledge can be modeled as perturbations to expert knowledge. Our framework operationalizes this idea by providing a student\u2019s behavioral context in the prompt and improving the expert knowledge of a base LLM via fine-tuning. In summary, our main contributions are:\nI. We formalize the problem of using an LLM\u2019s in-context learning capabilities for student modeling and behavior synthesis in open-ended learning domains.\nII. We propose a novel framework LLM-SS for synthesizing student\u2019s behavior. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs to boost their student modeling capabilities.\nIII. We evaluate several methods instantiated from our framework on an existing benchmark, S TU DENT S YN, for student attempt synthesis in a visual programming domain. Our results highlight that our methods perform significantly better than baselines without requiring complex pipelines or extensive datasets.\nIV. We publicly release the implementation of LLM-SS to facilitate future research. 1\n\n# 2 Related Work\n\nStudent modeling and synthesis in open-ended domains. As discussed in the previous section, there have been recent developments on student modeling for open-ended learning domains, with techniques ranging from misconception discovery to identification of struggling students and investigating problem-solving strategies [4\u2013 7, 11\u2013 13]. Among these recent works, our work is closer to that of [11] as we are addressing the problem of synthesizing a student\u2019s behavior by focusing on misconceptions in observed attempts. In fact, our evaluation is based on the S TUDENT S YN benchmark from [11] that considers the problem of synthesizing a student\u2019s attempt in visual programming domains. As part of this benchmark, [11] proposed an automated method, N EUR SS, that requires extensive pre-training on expert data and continual training on real-world data from similar students. Our framework aims to avoid this complex training pipeline by leveraging the in-context learning capabilities of LLMs. Our work is also similar in spirit to contemporary works that use LLMs for simulating students to teach learners in conversational tutoring systems [20] or train human tutors [18].\nLLMs in programming education. Generative AI and LLMs hold great promise in enhancing the field of education through a complementary relationship between human teachers and generative models [21, 22]. Some of the earlier works applying LLMs in educational settings focused on computing and programming education domains and studied a various of scenarios, including generating high-precision feedback [23, 24], generating programming exercises [25], repairing bugs in programming assignments [26], task synthesis for visual programming [27, 28], and benchmarking LLMs capabilities with that of human tutors [28, 29]. Our work differs from these works, given our focus on leveraging LLMs for modeling a student and synthesizing students\u2019 attempts.\n\n# 3 Problem Setup\n\nIn this section, we formalize the problem of leveraging LLMs for in-context student modeling in open-ended domains. While we focus on LLM-based methods, we provide a generic setup that encapsulates various baseline methods that do not use LLMs (e.g., baseline N EUR SS used for comparison in Section 5). In particular, our problem setup is inspired by the work of [11] that will also be used later as a benchmark in our experiments.\nPreliminaries and synthesis objective. Given an open-ended learning domain, there is a student, henceforth referred to as STU, aiming to solve some tasks in the domain. We denote the space of all possible tasks by T, and the space of all possible solutions and attempts by C. In particular, we are given a reference task T ref \u2208 T of interest along with a solution C \u2217 T ref \u2208 C. 2 Our main goal is to develop a synthesizer that can model the student STU by observing how STU solves T ref, and subsequently synthesize an attempt on any similar target task T tar, imitating STU \u2019s behavior. More concretely, we consider the following two-step process:\n(1) First, the synthesizer observes a student\u2019s context tuple (T ref, C \u2217 T ref, C STU T ref), where C STU T ref \u2208 C is the student STU \u2019s attempt on solving the reference task.\nSTU would attempt T tar. 3\n(2) Next, given a target task T tar \u2208 T conceptually similar to T ref, along with a solution C \u2217 T tar \u2208 C, the synthesizer synthesizes a student\u2019s attempt \ufffd C STU T tar, which should be close to how the student\nQuality rubric for evaluation. We evaluate the performance of a synthesizer based on the quality of their synthesized student\u2019s attempt \ufffd C STU T tar. Based on existing literature [11, 29], we quantitatively measure the generative quality using expert-based assessments w.r.t. the following quality rubric:\n\u2022  QSTU. This attribute measures whether the synthesized attempt \ufffd C STU T tar captures the student STU \u2019s behavior (e.g., problem-solving strategy and underlying misconceptions).\n2 A task T can have multiple solutions, and C \u2217 T refers to any solution codes written by experts being provided as input. 3 There are different granularity levels at which we can synthesize the student STU \u2019s behavior, including: (a) a\n\n\ufffd\n2 A task T can have multiple solutions, and C \u2217 T refers to any solution codes written by experts being provided as input. 3 There are different granularity levels at which we can synthesize the student STU \u2019s behavior, including: (a) a coarse-level binary prediction of success/failure, (b) a medium-level prediction w.r.t. predefined misconceptions; (c) a fine-level synthesis of student\u2019s attempt. Here, we focus on this fine-level objective of synthesizing a student\u2019s attempt.\n\n\u2022  QTASK. This attribute measures whether the synthesized attempt \ufffd C STU T tar captures the characteristics of T tar (e.g., partially reflecting its solution C \u2217 T tar).\n\u2022  QOVERALL. This attribute measures whether the synthesized attempt \ufffd C STU T tar successfully captures both the student\u2019s behavior and the target task\u2019s characteristics at the same time. We will set QOVERALL =  QSTU \u00d7  QTASK.\nIllustrative example for visual programming domain. In our experimental evaluation (Section 5), we will consider an existing benchmark, S TUDENT S YN [11], for student attempt synthesis in a visual programming domain of Hour of Code: Maze Challenge by Code.org (HoCMaze) [30]. As an illustrative example, Figure 1 shows a concrete scenario for our problem setup.\n\n# 4 Our LLM-SS Framework\n\nIn this section, we propose a novel framework, namely LLM-SS, for in-context student modeling and synthesizing students\u2019 attempts. It is inspired by the Perturbation Student Model as discussed below (Section 4.1). Afterward, we delve into two components of LLM-SS: providing student\u2019s context (Section 4.2) and providing domain expertise (Section 4.3).\n\n# 4.1 Perturbation Student Model\n\nPerturbation Student Model is based on the idea that a student\u2019s knowledge can be modeled as perturbations to expert knowledge [19]. This model was introduced as an extension of the  Overlay Model [31, 32] \u2013 it allows modeling a student\u2019s misconceptions and \u201cbuggy\u201d knowledge that deviates from expert knowledge. It assumes that incorrect behaviors of a student can be caused by systematically applying a set of perturbations to domain expertise.\nIn our LLM-SS framework, we use an LLM to model a student in an open-ended learning domain following the same idea of Perturbation Student Model. More concretely, we provide a student\u2019s knowledge by a behavioral context in a prompt to LLM (Section 4.2), and provide domain-specific expertise through fine-tuning the LLM on expert data (Section 4.3).\n\n# 4.2 Providing Student\u2019s Context\n\nNext, we discuss how to provide a student\u2019s context to an LLM and leverage the LLM\u2019s capabilities of in-context learning. Again, the goal of a student\u2019s context is to give an LLM information about the student, which may include the student\u2019s background, preferences, learning history, and problemsolving trajectories on multiple tasks. This information can be provided to a given LLM as a context in a prompt \u2013 existing works have shown that LLMs can effectively learn from such contextual information without explicit training or further parameter updates [15, 16].\nIn our framework, the prompt includes a student\u2019s context in the form of a problem-solving attempt on a reference task, which is represented by an information tuple (T ref, C \u2217 T ref, C STU T ref); see Section 3. We expect the LLM to infer the student\u2019s misconceptions from the observed attempt along with the necessary perturbations to obtain C STU T ref from C \u2217 T ref. Subsequently, the LLM is asked to play the role of this student and synthesize an attempt for a target task T tar, which should reflect the student\u2019s behavior. This is when the LLM should apply the same perturbations to obtain \ufffd C STU T tar from C \u2217 T tar.\nFigure 2 shows an example of our main prompt template for providing the student STU \u2019s context and synthesizing the student\u2019s attempts. We note that our LLM-SS framework can accommodate multiple solutions for a task and richer representations of the student\u2019s context as input by appropriately adapting the prompt. In this template example, we have shown a single solution for a task and one student\u2019s attempt, as considered in our experimental evaluation; see Section 5.\n\n# 4.3 Providing Domain Expertise\n\nNext, we discuss how to provide domain-specific expertise to an LLM for student modeling. In general, datasets used for pre-training LLMs may not contain data coming from specialized open ended learning domains such as interactive educational games [10], physics simulations [33], o visual programming [30]. Consequently, LLMs could be far from experts in these domains; fo\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cee9/cee97cd4-5918-4891-8b0c-847067833efd.png\" style=\"width: 50%;\"></div>\nFigure 2: Prompt template used in LLM-SS framework. {pla for each scenario.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6b4/d6b4c69a-379e-4f06-b462-882f5b0d931f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Prompt for fine-tuning.\n</div>\n<div style=\"text-align: center;\">Figure 3: Fine-tuning an LLM using expert knowledge in LLM-SS framework.\n</div>\ninstance, even state-of-the-art models like GPT-4 perform poorly in synthesizing solutions for visual programming tasks [28]. In such settings, we need to enhance an LLM domain-specific knowledge to effectively model a student as per the Perturbation Student Model. In particular, we will enhance an LLM\u2019s domain expertise via fine-tuning \u2013 existing works have shown that pre-trained LLMs can be tailored to specific domains via fine-tuning [34, 35]\nIn our framework, we aim to improve a given LLM\u2019s capability of generating solutions C \u2217 T for any task T similar to the reference task T ref. Once the LLM acquires a better understanding of how to solve tasks in the domain, it is expected to better infer the student\u2019s behavior from the context provided in Section 4.2. More concretely, we use pairs of (task T, solution C \u2217 T) in the domain to create a fine-tuning dataset D ft = {x (k), y (k)}, where x (k) is an input prompt containing a task to be solved and y (k) is the desired solution generated by the LLM. We consider an LLM parameterized by \u03b8, with p \u03b8 denoting conditional probability distribution of sampling responses. We perform supervised fine-tuning to adjust \u03b8 through gradient descent, with the objective of minimizing the negative log-likelihood loss given by L ft (\u03b8) := \u2212 E (x (k), y (k)) \u223c D ft \ufffd log p \u03b8 (y (k) | x (k)) \ufffd [35].\nFigure 3 shows the pipeline overview of fine-tuning an LLM in our framework along with an example of fine-tuning prompt template. In each prompt x (k), we first start by describing the domain background (same as in Figure 2). Then, we use an instruction to steer the LLM\u2019s behavior to act as a domain expert and solve a task. The last part of the prompt is a representation of the task to be solved.\n\n# Experimental Evaluation\n\nThis section presents our experimental evaluation, including description of S TUDENT S YN benchmark with baseline methods from [11] (Section 5.1), evaluated methods (Section 5.2), evaluation procedure (Section 5.3), and results (Section 5.4).\n\nThis section presents our experimental evaluation, including description with baseline methods from [11] (Section 5.1), evaluated methods (Sec (Section 5.3), and results (Section 5.4).\n\n# 5.1 S TUDENT S YN Benchmark and Baselines\n\nWe use the S TUDENT S YN benchmark from [11], designed to evaluate student\u2019s attempt synthesis methods in the visual block-based programming domain of Hour of Code: Maze Challenge by Code.org (HoCMaze) [30]. This programming domain has been popularly used in several existing works [11, 36\u2013 38]. Figure 1 shows an example of task T ref along with a solution C \u2217 T ref\u2013 a task in HoCMaze is specified by a visual grid containing an avatar (blue arrow), a goal (red star), and some walls (gray cells); a solution code brings the avatar to the goal\u2019s location while avoiding hitting the walls. S TUDENT S YN is a challenging benchmark for our problem setup, as evidenced by the huge performance gap between human tutors and automated methods proposed in [11].\nBenchmark scenarios. This benchmark comprises two reference tasks T ref, namely HoCMaze-4 and HoCMaze-18 [30], and three target tasks T tar associated with each reference task. In our illustration of problem setup in Figure 1, we use HoCMaze-18 as T ref. The benchmark considers six types of misconceptions, such as confusion between left/right directions when turning, writing repetitive turn commands, and ignoring the If-Else/While structure. The benchmark provides a set of scenarios comprising a student STU with a specific misconception, one coding attempt C STU T ref on each reference task, and the STU \u2019s attempt on each target task C STU T tar serving as a ground-truth. In total, we evaluate on these 36 scenarios (2 T ref \u00d7 3 T tar \u00d7 6 STU).\nDataset for fine-tuning. Along with benchmark scenarios, [11] also provides a synthetic dataset consisting of (task, solution) pairs, where tasks are similar to either HoCMaze-4 or HoCMaze-18. Here, task similarity is measured conceptually by edit distance in solution codes. This synthetic dataset was created and used for pre-training models introduced in [11]. In our framework, we will use it to fine-tune a base LLM to boost its domain expertise. In total, there are 10, 000 training tasks and 500 validation tasks for HoCMaze-4, and 40, 000 training tasks and 500 validation tasks for HoCMaze-18.\nBaseline methods. We compare our framework with baseline method N EUR SS [11], an LSTM-based neural network pre-trained on expert knowledge and continually trained on real students\u2019 attempts. We also compare our framework with human tutors in the visual programming domain, referred to as T UTOR SS in [11]. Here, T UTOR SS can be considered an oracle that provides performance upper bounds. We re-use the students\u2019 attempts synthesized by N EUR SS and T UTOR SS from [11], and re-assess them w.r.t. our rubric in Section 5.3.\n\n# 5.2 Methods Based on LLM-SS Framework\n\nMethods using a base LLM without fine-tuning. Based on our LLM-SS framework, we develop the following concrete methods using base models without fine-tuning step described in Section 4.3: GPT-3.5-SS using GPT-3.5 [39], GPT-4-SS using GPT-4 [40], Llama2-7B-SS using Llama2-7B-Chat [35], and Llama2-70B-SS using Llama2-70B-Chat [35].\nMethods using a fine-tuned LLM.  We further develop the following three concrete methods by finetuning three models: GPT-3.5ft-SS using fine-tuned GPT-3.5 [39], Llama2-7Bft-SS using fine-tuned Llama2-7B-Chat [35], and Llama2-70Bft-SS using fine-tuned Llama2-70B-Chat [35]. We did not fine-tune the GPT-4 model as APIs are not publicly available. Details of our fine-tuning procedure are explained in Section 4.3. 4\n\nMethods using a base LLM without fine-tuning. Based on our LLM-SS framework, we develop the following concrete methods using base models without fine-tuning step described in Section 4.3: GPT-3.5-SS using GPT-3.5 [39], GPT-4-SS using GPT-4 [40], Llama2-7B-SS using Llama2-7B-Chat [35], and Llama2-70B-SS using Llama2-70B-Chat [35].\n\nMethods using a fine-tuned LLM. We further develop the following three concrete methods by fine tuning three models: GPT-3.5ft-SS using fine-tuned GPT-3.5 [39], Llama2-7Bft-SS using fine-tuned Llama2-7B-Chat [35], and Llama2-70Bft-SS using fine-tuned Llama2-70B-Chat [35]. We did not fine-tune the GPT-4 model as APIs are not publicly available. Details of our fine-tuning procedure are explained in Section 4.3. 4\n\n# 5.3 Evaluation Procedure\n\nFor each scenario from the S TUDENT S YN benchmark (see Section 5.1), we create a prompt following the template in Figure 2 to use it as input to an LLM. We use the domain background representation\n\nFor each scenario from the S TUDENT S YN benchmark (see Section 5 the template in Figure 2 to use it as input to an LLM. We use the d\n\n4 For fine-tuning Llama2-70B models, we used a cluster of 2 \u00d7 36 cores, 2. 40 GHz Intel Xeon Platinum Processor 8360 Y, and 8 \u00d7 Nvidia A 100 80 GB, with parallelization under a 64-bit Debian. We fine-tuned a model for each reference task separately, and one run on a reference task took up to 35 hours. For GPT-3.5, we fine-tuned the GPT-3.5-turbo-0613 model for each reference task separately, and one run on a reference task took up to 7 hours. We paid about 1000 $ in total for using fine-tuning APIs provided by OpenAI.\n\nHoCMaze-4\nHoCMaze-18\nQ-OVERALL\nQ-STU\nQ-TASK\nQ-OVERALL\nQ-STU\nQ-TASK\nGPT-3.5-SS\n0.28\n0.56\n0.50\n0.14\n0.61\n0.25\nGPT-4-SS\n0.61\n0.86\n0.72\n0.51\n0.81\n0.58\nGPT-3.5ft-SS\n0.64\n0.69\n0.75\n0.82\n0.92\n0.86\nLlama2-7B-SS\n0.08\n0.14\n0.44\n0.08\n0.25\n0.39\nLlama2-70B-SS\n0.36\n0.58\n0.50\n0.26\n0.56\n0.50\nLlama2-7Bft-SS\n0.52 (0.05)\n0.55 (0.07)\n0.90 (0.05)\n0.30 (0.08)\n0.66 (0.11)\n0.39 (0.09)\nLlama2-70Bft-SS\n0.65 (0.08)\n0.87 (0.05)\n0.73 (0.05)\n0.53 (0.03)\n0.83 (0.02)\n0.63 (0.03)\nNEURSS\n0.43\n0.56\n0.67\n0.25\n0.78\n0.36\nTUTORSS\n0.84\n0.92\n0.89\n0.85\n0.89\n0.95\nDetailed results w.r.t. to each attribute in the quality rubric: QOVERALL, QSTU, and QTASK. Fine-tuned dels are highlighted in green. Fine-tuning improves LLMs\u2019 capabilities of capturing both student\u2019s behavior\n\n(a) Detailed results w.r.t. to each attribute in the quality rubric: QOVERALL, QSTU, and QTASK. Fine-tu models are highlighted in green. Fine-tuning improves LLMs\u2019 capabilities of capturing both student\u2019s behav and target task\u2019s characteristics.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fc68/fc68e3eb-52e5-4128-80eb-a0d0cad392c5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">) Overall performance (QOVERALL). Green areas correspond to fine-tuning improvements. T UTOR SS (re nes) serves as a performance upper bound.\n</div>\nFigure 4: (a) shows performances of methods w.r.t. individual attributes in our quality rubric. (b) shows overall performance of capturing both student\u2019s behavior and target task\u2019s characteristics. Human tutors (T UTOR SS) serve as an oracle. For methods using a fine-tuned LLM, we report numbers averaged over three fine-tuning runs with standard errors (except GPT-3.5ft-SS with only one run, due to the high costs of using fine-tuning APIs from OpenAI).\nfor HoCMaze based on prompts in recent works [27]. Subsequently, all scenarios together with student attempts synthesized by the LLM are presented to two independent experts for assessment \u2013 these two experts have extensive knowledge in computer science and visual programming domains, and follow the evaluation rubric from Section 3. The annotation process is done in a blind condition, in which experts do not know from which method a coding attempt is synthesized. In total, there are about 500 codes to be annotated by each expert corresponding to different scenarios and methods.\nThese two experts annotated the synthesized codes by using binary values {0, 1} for annotation, i.e., each quality attribute could take a value of 0 (bad) or 1 (good). Concretely,  QSTU = 1 means that \ufffd C STU T tar captures the student STU \u2019s behavior in terms of the problem-solving strategy and underlying misconceptions, and otherwise  QSTU = 0; similarly,  QTASK = 1 means that \ufffd C STU T tar captures the characteristics of target task T tar, and otherwise  QTASK = 0. QOVERALL, defined as QSTU \u00d7 QTASK, takes values of {0, 1}. We validate the expert annotations w.r.t. QOVERALL using Cohen\u2019s kappa inter-agreement reliability [41], obtaining a value of 0. 71, indicating substantial agreement between two experts.\nNevertheless, further investigation into the annotations revealed that the majority of disagreements between two experts were borderline cases where the quality attribute value was unclear. This motivated us to refine the scale of assessment where QSTU  and QTASK would take values of {0, 0. 5, 1}, with 0. 5 now indicating partially capturing the student\u2019s behavior or the target task\u2019s characteristics. We note that QOVERALL, defined as QSTU \u00d7  QTASK, now takes values of {0, 0. 25, 0. 5, 1}. With this refined scale, one expert did the entire annotations again and the final results reported in Section 5.4 are based on these new annotations. We report averaged results in the range [0. 0, 1. 0] by aggregating across all scenarios for a given reference task.\n\n# 5.4 Results\n\nWithout fine-tuning: GPT-4-SS outperforms N EUR SS. Among our methods that use a base LLM model without fine-tuning, GPT-4-SS achieved the highest scores in all quality attributes across both\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8197/81978725-ef7a-489c-a583-54cb767cefe4.png\" style=\"width: 50%;\"></div>\nFigure 5: Losses and evaluations during fine-tuning our two best-performing methods GPT-3.5ft-SS and Llama2-70Bft-SS. We plot data per 0.1 epoch. Losses are plotted on log-scale for better visibility of dynamics. Validation BLEU/accuracy metrics are decided by the fine-tuning library/platform and shown as a sanity check, and are not used for optimization. For GPT-3.5ft-SS, the number of epochs depends on budget spent for OpenAI APIs; we spent roughly half of the total budget for each task. For Llama2-70Bft-SS, the number of epochs are determined by generative performance on a small validation set of examples.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aeb6/aeb61f0b-55bb-4133-a4f0-03dcd81db481.png\" style=\"width: 50%;\"></div>\nFigure 6: Student STU \u2019s attempts for the scenario shown in Figure 1. (a) shows ground-truth student STU \u2019s attempt C STU T tar provided in the S TUDENT S YN benchmark. (b-e) show synthesized student STU \u2019s attempts \ufffd C STU T tar provided by different methods.\n\n\ufffd\nreference tasks, followed by Llama2-70B-SS (see Figure 4a). Additionally, GPT-4-SS performs significantly better than the N EUR SS baseline w.r.t. QOVERALL in both reference tasks (p \u2264 0. 05), based on the \u03c7 2 test [42]. 5 QOVERALL scores of GPT-3.5-SS and Llama2-7B-SS are lower than that of the baseline N EUR SS (which also motivates why we need to do fine-tuning discussed in Section 4.3).\nFine-tuning shows significant improvements.  Our methods using fine-tuning, namely GPT-3.5ftSS, Llama2-7Bft-SS, and Llama2-70Bft-SS, demonstrate significant improvements compared to using their base versions without fine-tuning (p \u2264 0. 05), as shown in Figure 4b. Remarkably, for HoCMaze-18, there is no significant difference between the performances of GPT-3.5ft-SS and human tutors in T UTOR SS (p > 0. 05). We observe that fine-tuning enhances a base LLM\u2019s ability to capture the target task\u2019s structure (QTASK), as shown in Figure 4a\u2013 this improvement is expected given they are fine-tuned to generate solutions for tasks. More importantly, their ability to capture the student\u2019s behavior (QSTU) also increases across all reference tasks and fine-tuned models. Figure 5 provides insights into the fine-tuning process.\nExample of synthesized student\u2019s attempt. In Figure 6, we investigate the scenario for HoCMaze-18 from Figure 1. In this scenario, the student STU \u2019s misconception is ignoring conditionals when attempting to solve the given task. Figure 6 (a) shows student code C STU T tar for the target task provided in the benchmark. Figures 6 (b-e) show student codes \ufffd C STU T tar synthesized by different methods. Student code synthesized by GPT-3.5ft-SS has the same misconception observed in C STU T ref, while adapting to T tar (QOVERALL =1). Notably, it is very close to the code written by human tutors in T UTOR SS (QOVERALL =1). Llama2-70Bft-SS synthesized a code that captures the student\u2019s misconception, but only partially reflects the target task\u2019s characteristics (QOVERALL = 0. 5). In contrast, the N EUR SS baseline synthesized a code that overfits C STU T ref, failing to reflect the layout of T tar as it uses turnLeft blocks instead of turnRight  blocks (QOVERALL =0).\n\nhere are computed on aggregated data across both the reference t\n\n# 6 Concluding Discussions\n\nWe proposed a novel LLM-based framework, LLM-SS, for in-context student modeling in openended learning domains. The results showcase that methods instantiated from LLM-SS are capable of modeling a student\u2019s observed behavior and synthesizing the student\u2019s attempt on a target task. We also highlight that fine-tuning a base LLM using expert knowledge in a given open-ended learning domain significantly improves its effectiveness in student modeling. More importantly, our framework does not require building a complex training pipeline as existing works, making it broadly applicable to new domains. In summary, our work demonstrates the potential of using LLMs for in-context student modeling, especially in challenging open-ended learning domains.\nNext, we discuss some limitations of our current work and ideas to tackle them in the future. First, our framework was evaluated on one visual programming domain, and the scenarios we considered do not fully capture the wide spectrum of open-ended learning domains; it would be interesting to evaluate our framework in other open-ended learning domains (e.g., algebra or text-based programming). Moreover, it would also be useful to do a more systematic analysis to see which misconceptions or students\u2019 behaviors are not well captured by our framework. Second, we provided a student\u2019s context through only one example of a problem-solving attempt; it would be interesting to evaluate the effectiveness of our framework when the student\u2019s context contains richer information, including the student\u2019s background and attempts on different tasks. Third, we evaluated our framework on student modeling metrics but have not evaluated how this modeling helps improve the performance of downstream applications; as future work, it would also be important to investigate the usefulness of our modeling framework directly in downstream applications, such as performance prediction, task recommendation, or synthetic behavioral dataset generation for training data-intensive models. In particular, since our framework allows fine-grained synthesis of a student\u2019s attempts beyond binary performance prediction, it would be interesting to see how our framework can potentially be applied for providing finer-grained feedback to the student about possible misconceptions.\nFinally, we note that there are several ethical implications regarding the use of LLMs for student modeling. For instance, the attempts synthesized by LLMs may not accurately reflect a student\u2019s understanding or ability. Moreover, LLMs are prone to hallucination and might generate inaccurate information. Therefore, it is crucial to implement appropriate validation mechanisms and safeguards when deploying LLM-based student modeling techniques in classrooms.\n\n# Acknowledgments and Disclosure of Funding\n\nFunded/Co-funded by the European Union (ERC, TOPS, 101039090). Views and opinions expr are however those of the author(s) only and do not necessarily reflect those of the European Uni the European Research Council. Neither the European Union nor the granting authority can be responsible for them.\n\n# References\n\n[1] Kurt VanLehn. Student Modeling. Foundations of Intelligent Tutoring Systems, pages 55\u201378, 2013.\n[2] Konstantina Chrysafiadi and Maria Virvou. Student Modeling for Personalized Education: A Review of the Literature. Advances in Personalized Web-Based Education, 78:1\u201324, 2015.\n[3]  Anna N. Rafferty, Rachel Jansen, and Thomas L. Griffiths. Using Inverse Planning for Personalized Feedback. In Proceedings of the International Conference on Educational Data Mining (EDM), 2016.\n[4] Andrew Emerson, Andy Smith, Fernando J. Rodr\u00edguez, Eric N. Wiebe, Bradford W. Mott, Kristy Elizabeth Boyer, and James C. Lester. Cluster-Based Analysis of Novice Coding Misconceptions in Block-Based Programming. In Proceedings of the Technical Symposium on Computer Science Education (SIGCSE), 2020.\n[5] Yang Shi, Krupal Shah, Wengran Wang, Samiha Marwan, Poorvaja Penmetsa, and Thomas W. Price. Toward Semi-Automatic Misconception Discovery Using Code Embeddings. In  Proceedings of the International Learning Analytics and Knowledge Conference (LAK), 2021.\n\n[6] Lisa Wang, Angela Sy, Larry Liu, and Chris Piech. Learning to Represent Student Knowledge on Programming Exercises Using Deep Learning. In Proceedings of the International Conference on Educational Data Mining (EDM), 2017.\n[7] Jade Cock, Mirko Marras, Christian Giang, and Tanja K\u00e4ser. Early Prediction of Conceptual Understanding in Interactive Simulations. In Proceedings of the International Conference on Educational Data Mining (EDM), 2021.\n[8] Ahana Ghosh, Sebastian Tschiatschek, Sam Devlin, and Adish Singla. Adaptive Scaffolding in Block-Based Programming via Synthesizing New Tasks as Pop Quizzes. In Proceedings of the International Conference on Artificial Intelligence in Education (AIED), 2022.\n[9] Michael J. Hannafin, Craig Hall, Susan Land, and Janette Hill. Learning in Open-Ended Environments: Assumptions, Methods, and Implications. Educational Technology, 34(8):48\u201355, 1994.\n[10] Tanja K\u00e4ser and Daniel L. Schwartz. Modeling and Analyzing Inquiry Strategies in Open-Ended Learning Environments. International Journal of Artificial Intelligence in Education (IJAIED), 30(3):504\u2013535, 2020.\n[11] Adish Singla and Nikitas Theodoropoulos. From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. In Proceedings of the International Conference on Educational Data Mining (EDM), 2022.\n[12] ByeongJo Kong, Erik Hemberg, Ana Bell, and Una-May O\u2019Reilly. Investigating Student\u2019s Problem-solving Approaches in MOOCs using Natural Language Processing. In Proceedings of the International Learning Analytics and Knowledge Conference (LAK), 2023.\n[13] Andrew Emerson, Wookhee Min, Jonathan P. Rowe, Roger Azevedo, and James C. Lester. Multimodal Predictive Student Modeling with Multi-Task Transfer Learning. In Proceedings of the International Learning Analytics and Knowledge Conference (LAK), 2023.\n[14] Lauren Fratamico, Cristina Conati, Samad Kardan, and Ido Roll. Applying a Framework for Student Modeling in Exploratory Learning Environments: Comparing Data Representation Granularity to Handle Environment Complexity. International Journal Artificial Intelligence in Education (IJAIED), 27(2):320\u2013352, 2017.\n[15] Tom B. Brown et al. Language Models are Few-Shot Learners. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.\n[16] S\u00e9bastien Bubeck et al. Sparks of Artificial General Intelligence: Early Experiments with GPT-4. CoRR, abs/2303.12712, 2023.\n[17] Gati V Aher, Rosa I. Arriaga, and Adam Tauman Kalai. Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. In Proceedings of the International Conference on Machine Learning (ICML), 2023.\n[18] Julia M. Markel, Steven G. Opferman, James A. Landay, and Chris Piech. GPTeach: Interactive TA Training with GPT-based Students. In Proceedings of the Conference on Learning @ Scale (L@S), pages 226\u2013236, 2023.\n[19] Robert Kass. Student Modeling in Intelligent Tutoring Systems \u2014 Implications for User Modeling. In User Models in Dialog Systems, pages 386\u2013410, 1989.\n[20] Robin Schmucker, Meng Xia, Amos Azaria, and Tom Mitchell. Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems. NeurIPS\u201923 Workshop on Generative AI for Education (GAIED), 2023.\n[21] Paul Denny, Sumit Gulwani, Neil T. Heffernan, Tanja K\u00e4ser, Steven Moore, Anna N. Rafferty, and Adish Singla. Generative AI for Education (GAIED): Advances, Opportunities, and Challenges. CoRR, abs/2402.01580, 2024.\n[22]  Jaeho Jeon and Seongyong Lee. Large Language Models in Education: A Focus on the Complementary Relationship Between Human Teachers and ChatGPT. Education and Information Technologies, 28(12):15873\u201315892, 2023.\n[23] Tung Phung, Jos\u00e9 Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, and Gustavo Soares. Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. In Proceedings of the International Conference on Educational Data Mining (EDM), 2023.\n\n[24] Tung Phung, Victor-Alexandru P\u02d8adurean, Anjali Singh, Christopher Brooks, Jos\u00e9 Cambronero, Sumit Gulwani, Adish Singla, and Gustavo Soares. Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. In  Proceedings of the International Learning Analytics and Knowledge Conference (LAK), 2024.\n[25]  Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models. In Proceedings of the Conference on International Computing Education Research (ICER), 2022.\n[26] Jialu Zhang, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen. Repairing Bugs in Python Assignments Using Large Language Models. CoRR, abs/2209.14876, 2022.\n[27] Victor-Alexandru P\u02d8adurean, Georgios Tzannetos, and Adish Singla. Neural Task Synthesis for Visual Programming. Transactions on Machine Learning Research (TMLR), 2024.\n[28] Adish Singla. Evaluating ChatGPT and GPT-4 for Visual Programming. In Proceedings of the Conference on International Computing Education Research (ICER) - Volume 2, 2023.\n[29] Tung Phung, Victor-Alexandru P\u02d8adurean, Jos\u00e9 Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, and Gustavo Soares. Generative AI for Programming Education: Benchmarking Chatgpt, GPT-4, and Human Tutors. In Proceedings of the Conference on International Computing Education Research (ICER) - Volume 2, 2023.\n[30] Code.org. Hour of Code: Classic Maze Challenge. https://studio.code.org/s/ hourofcode, 2012.\n[31] Brian Carr and Ira P. Goldstein. Overlays: A Theory of Modelling for Computer Aided Instruction. https://hal.science/hal-00702959, 1977. AI Memo 406.\n[32] Gordon McCalla Jim E. Greer. Student Modelling: The Key to Individualized Knowledge-Based Instruction. Springer-Verlag, 1994.\n[33] Carl E. Wieman, Wendy K. Adams, and Katherine K. Perkins. PhET: Simulations That Enhance Learning. Science, 322(5902):682\u2013683, 2008.\n[34] Long Ouyang et al. Training Language Models to Follow Instructions with Human Feedback. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2022.\n[35] Hugo Touvron et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR, abs/2307.09288, 2023.\n[36]  Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas J. Guibas. Autonomously Generating Hints by Inferring Problem Solving Policies. In Proceedings of the Conference on Learning @ Scale (L@S), 2015.\n[37] Aleksandr Efremov, Ahana Ghosh, and Adish Singla. Zero-shot Learning of Hint Policy via Reinforcement Learning and Program Synthesis. In Proceedings of the International Conference on Educational Data Mining (EDM), 2020.\n[38] Umair Z. Ahmed, Maria Christakis, Aleksandr Efremov, Nigel Fernandez, Ahana Ghosh, Abhik Roychoudhury, and Adish Singla. Synthesizing Tasks for Block-based Programming. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.\n[39] OpenAI. OpenAI GPT-3.5. https://platform.openai.com/docs/models/ gpt-3-5-turbo, 2023.\n[40] OpenAI. GPT-4 Technical Report. CoRR, abs/2303.08774, 2023. [41] Jacob Cohen. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20:37 \u2013 46, 1960.\n[42] William G. Cochran. The \u03c7 2 Test of Goodness of Fit. The Annals of Mathematical Statistics, 23(3):315\u2013345, 1952.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenges in student modeling within open-ended learning domains, particularly the diverse behaviors and misconceptions that complicate accurate modeling. Previous methods often require large datasets or complex pipelines, highlighting the need for a more efficient approach.",
        "problem": {
            "definition": "The problem involves synthesizing a student's behavior in open-ended learning domains, specifically by modeling how a student approaches tasks and reflecting their problem-solving style and misconceptions.",
            "key obstacle": "The main difficulty lies in the diverse strategies students employ in open-ended tasks, which makes it challenging to model their behavior accurately without extensive data or complex methods."
        },
        "idea": {
            "intuition": "The idea is inspired by the capabilities of large language models (LLMs) to learn from contextual information, suggesting that they can effectively model student behavior based on observed attempts.",
            "opinion": "The proposed idea involves a framework called LLM for Student Synthesis (LLM-SS) that utilizes LLMs to synthesize a student's behavior in a target task based on their performance in a reference task.",
            "innovation": "The primary innovation is the introduction of LLM-SS, which leverages LLMs for in-context learning, eliminating the need for extensive pre-training on expert data and allowing for adaptability to various LLMs."
        },
        "method": {
            "method name": "LLM for Student Synthesis",
            "method abbreviation": "LLM-SS",
            "method definition": "LLM-SS is a framework that uses large language models to model a student's behavior and synthesize their attempts on tasks based on their previous performances.",
            "method description": "The method synthesizes a student's behavior by observing their attempts on a reference task and generating corresponding attempts on target tasks.",
            "method steps": [
                "Observe the student's behavior on a reference task.",
                "Identify the student's misconceptions and problem-solving strategies.",
                "Use this context to synthesize an attempt on a similar target task."
            ],
            "principle": "The method is effective because it relies on the LLM's ability to learn from contextual information, allowing it to adapt and model the student's behavior accurately."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the S TUDENT S YN benchmark, which includes reference tasks from the visual programming domain of Hour of Code: Maze Challenge by Code.org.",
            "evaluation method": "Performance was assessed by comparing synthesized attempts against expert assessments based on quality attributes that capture the student's behavior and task characteristics."
        },
        "conclusion": "The results demonstrate that the LLM-SS framework effectively models student behavior and synthesizes attempts on target tasks, significantly outperforming baseline methods and approaching human tutor performance, particularly when fine-tuned.",
        "discussion": {
            "advantage": "The key advantages include the ability to synthesize student attempts without complex training pipelines and the adaptability to different LLMs, which enhances its applicability across various domains.",
            "limitation": "The limitations include the evaluation being restricted to a single visual programming domain and potential inaccuracies in synthesized attempts that may not fully capture a student's understanding.",
            "future work": "Future research should explore applying the framework to a wider range of open-ended learning domains, incorporating richer student context, and investigating the impact of modeling on downstream applications like performance prediction and feedback generation."
        },
        "other info": {
            "funding": "Funded/Co-funded by the European Union (ERC, TOPS, 101039090).",
            "disclosure": "Views and opinions expressed are those of the authors and do not necessarily reflect those of the European Union or the European Research Council."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses challenges in student modeling within open-ended learning domains, highlighting the diverse behaviors and misconceptions that complicate accurate modeling."
        },
        {
            "section number": "1.2",
            "key information": "The significance of the proposed framework, LLM for Student Synthesis (LLM-SS), is its ability to synthesize student behavior based on observed attempts, which is relevant in the context of in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "LLM-SS synthesizes a student's behavior by observing their attempts on a reference task, identifying misconceptions and problem-solving strategies, and generating attempts on target tasks."
        },
        {
            "section number": "3.4",
            "key information": "The method relies on the LLM's ability to learn from contextual information, allowing it to adapt and model the student's behavior accurately."
        },
        {
            "section number": "4.1",
            "key information": "The design of the LLM-SS framework allows for effective modeling of student behavior without the need for extensive pre-training on expert data."
        },
        {
            "section number": "6.1",
            "key information": "Limitations include potential inaccuracies in synthesized attempts that may not fully capture a student's understanding, which relates to model bias."
        },
        {
            "section number": "7",
            "key information": "The results demonstrate that the LLM-SS framework effectively models student behavior, significantly outperforming baseline methods and approaching human tutor performance."
        }
    ],
    "similarity_score": 0.7154193336998103,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/758e/758ebce0-4f10-491e-9fb8-c73e3ca792a2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cee9/cee97cd4-5918-4891-8b0c-847067833efd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6b4/d6b4c69a-379e-4f06-b462-882f5b0d931f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fc68/fc68e3eb-52e5-4128-80eb-a0d0cad392c5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8197/81978725-ef7a-489c-a583-54cb767cefe4.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aeb6/aeb61f0b-55bb-4133-a4f0-03dcd81db481.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Large Language Models for In-Context Student Modeling_ Synthesizing Student's Behavior in Visual Programming from One-Shot Observation.json"
}