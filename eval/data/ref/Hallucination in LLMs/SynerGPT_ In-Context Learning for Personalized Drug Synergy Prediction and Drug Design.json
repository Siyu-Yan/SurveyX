{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.11694",
    "title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
    "abstract": "Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient\u2019s specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small \u201cpersonalized dataset\u201d of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to \u201cin-context learn\u201d common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn \u201cdrug synergy functions\u201d. Our model\u2014which does not use any textual corpora, molecular fingerprints, protein interaction or any other domainspecific knowledge\u2014 is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient\u2019s \u201cpersonalized dataset\u201d. Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.1",
    "bib_name": "edwards2023synergptincontextlearningpersonalized",
    "md_text": "# SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa9a/aa9ac303-6769-47b7-81e2-12cf2fdb3153.png\" style=\"width: 50%;\"></div>\nCarl Edwards Department of Computer Science University of Illinois Urbana-Champaign cne2@illinois.edu\nTushar Khot Allen Institute for Artificial Intelligence tushark@allenai.org\nn Tom Hope The Hebrew University of Jerusalem Allen Institute for Artificial Intelligence tomh@allenai.org\nHeng Ji Department of Computer Science University of Illinois Urbana-Champaign hengji@illinois.edu\nomputer Science nois Urbana-Champaign is.edu Tom Hope The Hebrew University of Jerusalem Allen Institute for Artificial Intelligence tomh@allenai.org\n# Abstract\nPredicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient\u2019s specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small \u201cpersonalized dataset\u201d of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to \u201cin-context learn\u201d common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn \u201cdrug synergy functions\u201d. Our model\u2014which does not use any textual corpora, molecular fingerprints, protein interaction or any other domainspecific knowledge\u2014 is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient\u2019s \u201cpersonalized dataset\u201d. Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.1\narXiv:2307.11694v2\nDrug combination therapy is a standard practice for diseases including cancer (Mokhtari et al., 2017) and HIV. It is based on identifying multiple single agent therapies that, when used together, lead to synergistic effects. Predicting such combinatorial synergies is challenging, especially given the wide range of multiple different mutations as well as different genetic backgrounds typically found in different patients\u2019 cancer cells (Mroz & Rocco, 2017). Many drug combinations can also cause increased toxicity (Zapata et al., 2020; Juurlink et al., 2004) in a manner that may depend on specific patient backgrounds (O\u2019Donnell & Dolan, 2009), adding further complexity to the problem. To enable the safest and most effective implementation of combination therapy in cancer care, it is thus important to personalize the prediction of drug synergies. Since the number of drug combinations scales exponentially, differentiating between synergistic and antagonistic pairings is very expensive to test in large quantities in laboratory conditions. Thus, 1Code will be made available upon publication.\n1Code will be made available upon publication.\n# Martin D. Burke Department of Chemistry University of Illinois Urbana-Champaign mdburke@illinois.edu\nMartin D. Burke Department of Chemistry University of Illinois Urbana-Champaign mdburke@illinois.edu\nconsiderable interest has recently grown in using machine learning for predicting synergistic and antagonistic effects between pairs of drugs in silico (Liu et al., 2020; Preuer et al., 2018; Rozemberczki et al., 2022a). These approaches are typically not evaluated in the few-shot setting, where only a few training examples are given. This is particularly relevant in the personalized setting described above, and more generally for cancer tissue types for which there is limited training data for synergy learning models. Additionally, these efforts use a variety of features to categorize the drugs, from molecular fingerprints (Preuer et al., 2018) to protein interactions (Yang et al., 2021). Obtaining these features often requires integrating external knowledge sources (e.g., from drug databases), which often results in findings being restricted to the limited subsets of drugs for which this information is available and also requires specialized engineering in model design. Finally, it is unclear if these external sources are actually needed for current models. In this work, we address these limitations by exploring the ability of transformer language models (LMs) to learn drug synergy relations. We devise approaches that leverage transformers (1) without any external knowledge required to be integrated into the model (i.e., no protein interaction networks or patient cell line features); (2) in the few-shot setting with an in-context learning approach that can generalize to novel unseen drugs and patient cell lines; and (3) for designing novel synergistic drug structures in the context of a specific patient\u2019s data.\nTransformer LMs are Strong Drug Synergy Learners\u2014Even Without Textual Representations First, we consider drug synergy prediction using transformer language models without enriching drugs/cells with information from external knowledge bases. We find these \u201cfeature-less\u201d models are able to achieve results that are better or competitive in comparison to knowledge-enhanced state-ofart drug synergy models (e.g., BERT models achieve 84.1% ROC-AUC to GraphSynergy\u2019s 83.4%) Furthermore, in contrast to recent work that uses language models pre-trained on scientific corpora (Nadkarni et al., 2021), we discover an intriguing finding: using randomized (i.e. uninformative) tokens instead of drug/cell names is able to rival models that use textual names of those entities. This suggests that external information coming from pre-training on scientific corpora (e.g., as in SciBERT (Beltagy et al., 2019)) or the web (e.g., Wikipedia) has negligible impact on fine tuned models in this setting. These findings motivate us to explore the power of transformer models without external information, and to study generalization beyond memorization capacity by evaluating on novel drugs/cells that were unseen during training.\nTransformer LMs are Strong Drug Synergy Learners\u2014Even Without Textual Representations First, we consider drug synergy prediction using transformer language models without enriching drugs/cells with information from external knowledge bases. We find these \u201cfeature-less\u201d models are able to achieve results that are better or competitive in comparison to knowledge-enhanced state-ofart drug synergy models (e.g., BERT models achieve 84.1% ROC-AUC to GraphSynergy\u2019s 83.4%) Furthermore, in contrast to recent work that uses language models pre-trained on scientific corpora (Nadkarni et al., 2021), we discover an intriguing finding: using randomized (i.e. uninformative) tokens instead of drug/cell names is able to rival models that use textual names of those entities. This suggests that external information coming from pre-training on scientific corpora (e.g., as in SciBERT (Beltagy et al., 2019)) or the web (e.g., Wikipedia) has negligible impact on fine tuned models in this setting. These findings motivate us to explore the power of transformer models without external information, and to study generalization beyond memorization capacity by evaluating on novel drugs/cells that were unseen during training. SynerGPT: A New In-Context Drug Synergy Setting & Model We take inspiration from recent work (Garg et al., 2022) that showed how a GPT model architecture can be trained to \u201cin-context learn\u201d function classes such as linear functions (e.g., linear regression/classification) and neural networks. We pre-train a GPT model from scratch on known drug synergies\u2014using no textual corpora\u2014and explore its ability to generalize in the few-shot setting to drugs and patient cell lines unseen during\nSynerGPT: A New In-Context Drug Synergy Setting & Model We take inspiration from recent work (Garg et al., 2022) that showed how a GPT model architecture can be trained to \u201cin-context learn\u201d function classes such as linear functions (e.g., linear regression/classification) and neural networks. We pre-train a GPT model from scratch on known drug synergies\u2014using no textual corpora\u2014and explore its ability to generalize in the few-shot setting to drugs and patient cell lines unseen during training. We find that our model, dubbed SynerGPT, is able to achieve strong competitive results without any external knowledge sources. In particular, we introduce a new setting of In-Context Learning for Drug Synergy (ICL-DS). In-Context Learning (ICL) (Dong et al., 2022) has emerged as a powerful paradigm for few-shot learning (Brown et al., 2020). In ICL, trained model parameters are never explicitly updated after pre-training, and adaptation to each task is done on the fly given contextual examples. This is particularly appealing in settings where it is prohibitively costly to perform parameter updates for each incoming new task and context (e.g., for each new patient in a hospital setting). We devise novel pre-training approaches for ICL-DS, including strategies for optimizing the language model prompt selection with a genetic algorithm. Prompts comprise specific combinations of drugs tested for synergy on specific patient cell lines; optimizing prompt selection in this setting has potential implications for the design of a standardized assay panel of drugs and cells to be tested for a patient\u2019s particular tumor. While specific patient data at this level is not readily available, we re-purpose existing drug combination data to lay the foundations for formalizing and studying our approaches from a machine learning perspective.\nDesigning New Molecules to be Synergistic in the Context of a Specific Patient Finally, in our third major contribution we propose an additional new task of Inverse Synergistic Drug Structure Design (ISDSD): using a GPT transformer model for generating or retrieving drug molecules that are synergistic in the context of a specific cancer patient\u2019s information (i.e., molecules that are synergistic with other drugs administered to a patient with specific cancer cells). This approach may in the future provide a new methodology for personalized drug candidate discovery.\n# Background and Problem Setting\nIn the last few decades, combination therapy has emerged as an effective method to target genetically unstable diseases (Mokhtari et al., 2017), with dramatic success in treating HIV (Moore & Chaisson, 1999) and more recently HCV(Liang & Ghany, 2013). Unlike HIV and HCV which encode only 10-15 proteins (Frankel & Young, 1998; Dubuisson, 2007), cancer is radically more complex. Since cancer has an unstable genome, combination therapy is often considered necessary (Mokhtari et al., 2017) and is commonly used in practice, with varying degrees of success. Generally, drugs work by affecting cellular pathways\u2013chain interactions of molecules which lead to changes in a cell. In drug synergy prediction, our goal is to predict whether combining drugs will have positive or negative outcomes in the complex system of these interacting pathways. Generally, synergy lab experiments are conducted in cell lines, which are a population of cells from a multi-cellular organism (for example, human lung cancer cells). In this work, we also investigate inverse design of drug molecules. Traditionally, the idea behind inverse design of molecules is to predict or retrieve a molecular structure which has some desired chemical property or protein target (Sanchez-Lengeling & Aspuru-Guzik, 2018). In our work, we seek to explore inverse design at a higher level\u2013 the \u201cinteractome\u201d of drug interactions in complex cellular pathways. General Problem Formulation Given k input drugs d1, d2, . . . , dk \u2208D along with a cell line c \u2208C, the goal of drug synergy prediction is to predict a synergy value y for the interactions between the drugs in the given cell line. In existing datasets, only the pairwise k = 2 setting is considered. Thus, we focus our experiments on pairwise drug synergy, the most commonly researched setting, but our methods can naturally be extended to n-ary synergies. This problem can be considered as either a regression (y \u2208R) or a binary classification problem (synergistic (True) or not (False); y \u2208[0, 1]). Synergy data comes from a dataset of tuples (d1, d2, c, y) \u2208D. Few-Shot In-Context Setting We also consider the few-shot setting in our formulation, which has applications for predicting synergies when there is scarce training data such as in tumor-specific synergy prediction, uncommon cancer tissues, or newly introduced single-agent therapies. In the few-shot setting, we assume there are n synergy tuples available which contain an unknown entity h (unknown cell line ch or unknown drug dh). Define these tuples as xi := (d1, d2, c, y)i for i \u2208[1..n] where one of d1, d2, or c is the unknown h. Each xi can then be used for training in addition to the existing training data. In our proposed method SynerGPT, we don\u2019t use these tuples xi in training\u2013 rather, we use them as the prompt for in-context learning. Here, we are particularly interested in synergy prediction based on extremely small datasets (e.g. tested synergies from a patient\u2019s specific cancer cells), which makes traditional supervised approaches less effective. In section 3.2.3, we detail our training strategies for in-context learning with unknown h from limited examples. Inverse Drug Design from Drug Synergy Context We propose a new task where the goal is to predict the structure of a molecule given a context of drug synergy tuples (e.g., we might be given 20 synergy tuples). We train a model to predict the structure of some unknown drug dh from its synergy relations with other drugs. This has two important uses. First, this may enable scientists to predict new molecules which have desirable synergies or similar synergies to existing drugs, which is a novel way to consider drug discovery. This can potentially enable the design of drugs that synergize specifically to target a given patient\u2019s unique cancer cells. Secondly, this can support explainability of the synergy prediction model as a function of the context it is fed, by \u201cvisualizing\u201d SynerGPT\u2019s understanding of the unknown drug given the context. Section 4.3 shows that we can observe the structure of the drug evolving towards the ground truth as more context examples are given. As this is a novel and difficult problem; we initially frame it as a retrieval task, effectively constraining the output space, though from an implementation perspective it is trivial to instead predict structures by using a pretrained generative model for molecules (Jin et al., 2020) with no architectural differences, as both the retrieval and generation of drug structures requires generating a latent vector.\n# 3 Methodology\nIn this section, we will consider the four components of our paper. First, we detail how drug synergy tuples are input to encoder-only language models (\u00a7 3.1). Next, we extend this idea to the few-shot\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a55/4a557ac0-afef-400a-b062-fc4250005e91.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Example of our prompt selection strategy (steps shown in black box). After training (step 0), we are given as input (step 1) a combination between three entities: a known drug Allopurinol ,</div>\n<div style=\"text-align: center;\">Figure 1: Example of our prompt selection strategy (steps shown in black box). After training (step 0), we are given as input (step 1) a combination between three entities: a known drug Allopurinol , unknown drug Vandetanib , and a known patient cell line 786-0 . We are given a small synergy graph G (bottom right). Nodes represent (drug, drug, cell line) tuples with synergy labels (from previous experiments). Edges represent shared entities; edge color indicates which entities (e.g. red , for sharing Mitotane ). Using different strategies, we adaptively select contextual examples (step 2) for in-context learning (step 3).</div>\nFigure 1: Example of our prompt selection strategy (steps shown in black box). After training (step 0), we are given as input (step 1) a combination between three entities: a known drug Allopurinol , unknown drug Vandetanib , and a known patient cell line 786-0 . We are given a small synergy graph G (bottom right). Nodes represent (drug, drug, cell line) tuples with synergy labels (from previous experiments). Edges represent shared entities; edge color indicates which entities (e.g. red , for sharing Mitotane ). Using different strategies, we adaptively select contextual examples (step 2) for in-context learning (step 3).\nICL setting and propose training methodologies to do so (\u00a7 3.2). We then discuss optimization of the \u201cprompt\u201d used for ICL (\u00a7 3.2.3). Finally, we extend our methodology to inverse drug design (\u00a7 3.3).\nInitially, we explore the efficacy of BERT-style language models (Devlin et al., 2019; Beltagy et al., 2019; Yasunaga et al., 2022) for drug synergy prediction. We modify the task input to be in natural language using a simple formulation:   \ntion: [CLS] d1 [SEP] d2 [SEP] c [SEP]\nwhere d1 and d2 are drug names (e.g., imatinib, 5-FU), and c is the name of a cell line (e.g., MCF2, Ishikawa). The model is then trained to predict the output value y from the [CLS] token representation.\nWe also investigate to what extent pretraining knowledge is responsible for the model\u2019s performance. To do so, we evaluate the impact on performance when the drug and cell names are replaced with \u2018random\u2019 tokens. Given the ordered (by frequency) vocabulary V of the LM, we select the tokens {vi \u2208V | i \u2208[k..(k + |C| + |D|)]} to represent our drug and cell lines. Note we start at a threshold k to avoid the most common tokens which might have specialized representations in the language model\u2019s latent space. We uniquely map each cell line and drug to a token in this set, which we use as input to the BERT LM. Essentially, this experiment is used to determine whether knowledge from pretraining or the transformer architecture itself is responsible for performance on the drug synergy task. An example input from this strategy is: [CLS] rabbit [SEP] fish [SEP] book [SEP].\n# 3.2 SynerGPT: In-Context Learning for Few-Shot Synergy Prediction\n3.2.1 In-Context Learning for Function Classes: Background\n# 3.2.1 In-Context Learning for Function Classes: Background\nRecent work trained transformer models to \u201cin-context learn\u201d function classes (Garg et al., 2022). A function class is a set of functions that satisfy specific properties, such as linear functions or neural networks. In-context learning of a function class F is defined as being able to approximate f(xquery) for \u201cmost\u201d functions f \u2208F given a new query xquery when conditioned on a prompt sequence (x1, f(x1), . . . , xn, f(xn), xquery). We define a prompt prefix Pn \ufffd(x1, f(x1), . . . , xn, f(xn), xn+1) as the\nfirst n in-context examples followed by the n + 1th input. A model M\u03b8 parameterized by \u03b8 is trained to minimize the loss averaged over all prefixes\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 \ufffd \ufffd \ufffd\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb given some appropriate loss function \u2113. Weights wn := 1 unless otherwise noted.\n3.2.2 Predicting Drug Synergy In-Context\nFor in-context prediction of drug synergy, we redefine\nPn = (d1 1, d2 1, c1, y1, . . . , d1 n, d2 n, cn, yn, d1 n+1, d2 n+1, cn+1\nas the prompt prefix (as discussed in Section 2, we refer to this as the \u201ccontext\u201d or \u201cinput context\u201d). Here, y can be considered the output of a function measuring synergy on (d1, d2, c). As in (Garg et al., 2022), we consider a GPT-2 family (Radford et al., 2019) decoder-only model, which we call SynerGPT. Here, the prediction of the synergy value yj is made using a linear transformation of the contextualized output representation of c j (note that this includes d1 j and d2 j due to self-attention). Model inputs\u2013drugs d, cell lines c, and labels y\u2013are initialized using a learnable embedding layer (i.e. no external features). To evaluate the model\u2019s ability to predict synergies of unknown entities, we hold out either m drugs or m cell lines and remove their synergy relations from the training set (see Section 4). We use a subset of the held out tuples as a pool of context examples. We now turn to selecting the context (prompt prefix) from this pool in a manner that increases predictive performance.\n3.2.3 How to sample the context?\nA central question about using language models without external features\u2014including textual names\u2014 is how to teach the model to understand unknown drugs or cell lines. We propose using a masking strategy\u2014every unknown drug dh or cell ch is represented by [UNKNOWN] and the model must use in-context learning to understand it based on contextually-related known drugs and cell lines. In this setting, we assume that we are given a set of synergy tuples to sample from to construct a prompt. During training, it\u2019s simply the training set. During evaluation, we consider a special held-out \u201ccontext\u201d set Dc \u2282D (thus named because we sample the context/prompt Pn from this set). To sample from this context set, we propose a context-selection strategy based on constructing a graph G on this Dc. Specifically, we construct G by creating a node for every synergy tuple x := (d1, d2, c, y) \u2208Dc. We construct a drug edge ed between two nodes x1 and x2 if they share drug d (i.e. d \u2208x1 \u2227d \u2208x2). Similarly, we construct a cell line edge ec if they share cell line c. See Figure 1 for an example and Appendix Figure 8 for more details. We employ the following context selection strategies to sample a context with n examples given some node x containing unknown h which is either drug dh or cell ch: 1. Random: Uniformly select n context examples from Dc.\n2. Graph: Uniformly select examples from the nodes adjacent to x in G. 3. Unknown-First: Uniformly select nodes adjacent to x which share an edge of type eh, i.e. prioritizing selection of nodes that contain the masked unknown h.\nNote that these strategies are hierarchical\u2013 Unknown-First falls back to Graph when there aren\u2019t enough examples which falls back to Random. Examples from Random are put earlier in the context than Graph which is again put before Unknown-First. In order to train the model to correctly use the [UNKNOWN] token, we need to artificially create unknown drugs or cells during training. Given training example x, we uniformly select d1 \u2208x or d2 \u2208x to be the hidden drug dh. For the unknown cell line setting, c \u2208x is always set to ch because there is just one cell line per example. We replace all occurrences of h in the prompt with [UNKNOWN]. We note that our sampling strategy is related to retrieval augmented models (Mialon et al., 2023). Here, however, we note that the model is also in-context learning synergy functions for unknown drugs based on Definition 1 in (Garg et al., 2022).\n# 3.2.4 Optimizing the Context\nWe further study whether the context can be optimized to best enable predictions for some unknown drug or cell line h (see Figure 7 for an example). The purpose of these experiments is to enable the\n(1)\neventual development of a standardized assay for drug synergy prediction. Thus, as output, these optimization algorithms produce a set of context tuples for each h. To do this optimization, we assume that we have four splits of data, which are constructed as follows. Given a set of p \u201cunknown\u201d drugs/cells H, all synergy tuples not containing any h \u2208H are put into a training set DTr. The remaining tuples are randomly partitioned into three equal sized sets: a context bank Dc, a validation set Dv, and a test set DTe. We first train a model on DTr following the Unknown-First strategy (where contexts are sampled from DTr itself). Following this, for each unknown entity hi, we select n context examples from Dc which maximize the model\u2019s score on the validation set Dv. This is a combinatorial optimization problem which can be considered related to the best subset selection problem (Bertsimas et al., 2015; Miller, 2002). We consider a genetic algorithm (Gad, 2021): a metaheuristic method which is useful for black box optimization of systems containing complex interacting parts (Mitchell et al., 2007), which is suitable for the complex interactions between cellular pathways required for drug synergy prediction. As output, we get a set of context tuples for each h. Optimization algorithm details are given in Appendix B.\n# 3.3 In-Context Learning for Inverse Design\nTo train the model to retrieve relevant drug structures in-context, we use the same architecture as for synergy prediction (\u00a7 3.2.2), so that we can use the same data split and optimized contexts from Section 3.2.4 to understand how the model interprets them. For effective retrieval, we need a strong base molecular representation that makes it possible to effectively distinguish molecules. So, we choose to use MegaMolBARTv2 (NVIDIA Corporation, 2022) representations, which were trained on 1.45 billion molecular SMILES strings and thus have a relatively comprehensive (in terms of drug classes) latent space. We train a SynerGPT model from scratch to predict representations using a linear transformation on the output [UNKNOWN] representation. We use this final representation to retrieve the desired drug using cosine similarity with the MegaMolBARTv2 representations of the drugs in our synergy dataset. The training context is selected using the Unknown-First strategy. Finally, we train the model using a minibatch contrastive loss (Radford et al., 2021; Edwards et al., 2021) between the L2-normalized ground truth representations Dg (here MegaMolBartv2) and predicted representations Dp (output from our model\u2019s prediction head):\n\u2113(Dg, Dp) = CE(e\u03c4DgDpT, Ib) + CE(e\u03c4DpDgT, Ib)\nwhere CE is categorical cross-entropy loss, b is the mini-batch size, Ib is the identity matrix, and \u03c4 is a learnable temperature parameter. We use this loss for \u2113in equation 1.\n# 4 Results\nBERT can do Drug Synergy? In this section, we experiment with finetuning BERT on drug synergy data where all drugs and cell lines are seen during training (data splits detailed in Appendix A.1). As discussed earlier, there has been recent work using external network datasets capturing interactions between drugs, proteins and cell lines (Yang et al., 2021) for synergy prediction.\nTo evaluate the impact of these external datasets, we compare against a strong and recent model, Graphsynergy (Yang et al., 2021) that uses over a dozen different network datasets and achieves state-of-the-art on its subset of DrugCombDB. We train four BERT-based (Devlin et al., 2019) language models (Beltagy et al., 2019; Yasunaga et al., 2022) and find that they outperform GraphSynergy\n<div style=\"text-align: center;\">Model</div>\nModel\nKB\nName\nROC-AUC\nPR-AUC\nDeepSynergy\n\u00d7\n84.3\n70.4\nMR-GNN\n\u00d7\n77.9\n62.6\nSSI-DDI\n\u00d7\n63.3\n41.4\nDeepDDS\n\u00d7\n87.2\n77.0\nSciBERT (random)\n86.9\n76.3\nBioLinkBERT (names)\n\u00d7\n86.4\n75.9\nTable 1: Classification results for four selected ChemicalX (Rozemberczki et al., 2022b) baselines and BERT on DrugCombDB (Liu et al., 2020). SciBERT and BioLinkBERT take random token and names as input, respectively. Values are average of five runs. Notably, SciBERT (random) outperforms four of the other five baselines. KB means external knowledge is used.\noutperform GraphSynergy in both name and random token settings. BioLinkBERT with random tokens, for example, achieves a\n<div style=\"text-align: center;\">KB Name ROC-AUC PR-AUC</div>\nROC-AUC score of 84.1% compared to GraphSynergy\u2019s 83.4% (p < 0.05 using paired t-test). In comparison, BioLinkBERT with drug names as input achieves 83.6%. We checked multiple BERT configurations, and details on other BERT models are shown in Appendix A.1 Table 4. A natural question here is whether the model has learnt the required knowledge during pre-training. Surprisingly, replacing drug and cell names with random tokens (\u00a7 3.1) resulted in no drop in performance. This suggests that the transformer architecture may be the dominant factor explaining BERT\u2019s performance on the task. However, if we use a randomly-initialized BERT model without any pre-training, we find the performance is worse (by 3 ROC-AUC pts). To verify our findings, we consider the ChemicalX framework (Rozemberczki et al., 2022b), which implements several baselines and provides a standardized subset of DrugCombDB (Liu et al., 2020) with drug and cell line features. This standardization allows us to compare different baseline methodologies on the same dataset.The ChemicalX DrugCombDB dataset has 2,956 drugs, 112 cell lines, and 191,391 synergy tuples. We compare against baselines DeepSynergy (Preuer et al., 2018), MR-GNN (Xu et al., 2019), SSI-DDI (Nyamabo et al., 2021), and DeepDDS (Wang et al., 2022), which we train using default hyperparameters from the original papers for 50 epochs as in (Rozemberczki et al., 2022b). These baselines (details in Appendix A.2) represent the most popular approaches to drug synergy prediction and allow us to compare against transformer architecture performance. Remarkably, SciBERT with random tokens outperforms all baselines except DeepDDS in this setting (Table 1). We see similar results on the DrugComb dataset (this database is larger but is continuously modified by volunteers; see Appendix J). We note that, while this performance is surprising in this domain, it follows from results from other domains. For example, language models are able to learn complex grammar and interactions just by observing how words co-occur. We conjecture this may be related to the observation that pre-training on a nonsense corpus (Krishna et al., 2021) can provide good weight initializiations for downstream tasks. We further discuss related work in Section 5.\n# 4.1 In-Context Learning for Few-Shot Drug Synergy\nWe now evaluate models on the few-shot and zero-shot setting, i.e, when a new drug or cell line is introduced with limited or no interaction data. We use the same architecture used in Garg et al. (2022): a GPT-2 (Radford et al., 2019) model with 256-dimensional embeddings, 12 layers, 4 attention heads, and batch size of 64. We use a learning rate of 2e-5. Model weights are initialized from scratch. To enable efficient experimentation in the few-shot setting, we construct a dataset split which contains multiple unknowns (i.e. m held-out drugs or cells: H := {hi | i \u2208[1..m]}). To construct our split, we remove all synergy tuples containing h \u2208H from the dataset D so that the remaining dataset only contains tuples with known drugs/cells (this is our training set DTr). Then, for each h, we select n synergy tuples randomly to form the \u201ccontext\u201d bank/split Dc. All other \u201cunknown\u201d synergy tuples are put into DTe. For comparison, we use the same baselines trained in zero-shot and few-shot settings. We also test SetFit (Tunstall et al., 2022) (a few-shot LM approach), k-nearest neighbors, off-the-shelf pre-trained GPT-2 (using entity names as input, similar to CancerGPT (Li et al., 2023a)), and MAML with DeepDDS (details in Appendix A.2). In the few-shot setting, the context bank Dc is considered part of the training set, and in the zero-shot setting it is not used. Our model, SynerGPT, however, is not trained on the context bank but uses it as context (prompt) examples for evaluation. Examples are selected using the Random, Graph, or Unknown-First strategies. We separately investigate the setting where drugs are unknown and where cell lines are unknown. Unknown Drugs To construct the dataset split, we set m = 50 unknown, i.e.,\u201cheld-out\u201d drugs and context n = 20 synergy tuples. Hence, our context bank contains 50 \u00d7 20 = 1, 000 tuples. Overall, we find that our SynerGPT can perform better in the few-shot setting than existing baselines on on this task, as shown in Table 2. Full results are in Appendix Table 6. SynerGPT is trained in the zero-shot setting, which means it can be evaluated both with context examples (few-shot) and without any examples (zero-shot). Each strategy performs roughly the same zero-shot (although since strategies are used in training there are small differences), but the performance with sampled context examples is much different. Without examples, SynerGPT performs worse than DeepDDS few-shot, but the same SynerGPT model outperforms DeepDDS when given the few-shot context. Overall, we outperform all prior models in the few-shot setting and zero-shot setting. In particular, Unknown-First is able to increase performance by 3.8% absolute ROC-AUC with context, whereas\n<div style=\"text-align: center;\">Unknown Drug Unknown Cell Line</div>\nUnknown Drug\nUnknown Cell Line\nMode\nModel\nROC-AUC\nPR-AUC\nROC-AUC\nPR-AUC\nZero-Shot\nDeepSynergy\n67.5\n47.7\n78.6\n63.6\nDeepDDS\n72.1\n53.2\n74.5\n59.8\nSciBERT (random)\n67.7\n47.4\n79.1\n64.4\nMAML-DeepDDS\n68.76\n50.05\n71.6\n54.6\nkNN-Features\n65.4\n45.9\n82.0\n70.3\nSynerGPT* (ours)\n74.0\n57.3\n83.5\n72.1\nFew-Shot\nDeepSynergy\n71.6\n53.9\n82.0\n68.7\nDeepDDS\n75.5\n57.4\n74.2\n60.4\nSciBERT (random)\n73.8\n56.9\n80.5\n66.4\nMAML-DeepDDS\n68.79\n50.00\n71.4\n54.6\nkNN-Features\n66.9\n47.7\n82.1\n70.5\nSetFit-S2\n58.8\n39.4\n63.3\n44.6\nGPT-2\n74.2\n56.8\n80.3\n66.6\nSynerGPT* (ours)\n77.7\n61.5\n83.8\n72.8\nTable 2: Few-shot and zero-shot results on ChemicalX DrugCombDB with 50 unknown drugs / 20 unknown cell lines. Our in-context methods perform better than baselines trained in the few-shot setting. Results are averaged over 5 runs. Zero-shot SynerGPT is evaluated without context. BERT models use random tokens. The difference between SynerGPT with and without context has p < 0.05 for both unknown drugs and cell lines based on a paired t-test. Similarly, both are statistically significant from the best baseline. *For simplicity, we report the best selection strategy (UnknownFirst for unknown drug and Interpolate for unknown cell line). Full results are in Appendix A.2.\nDeepDDS only increases 1.3% from zero- to few-shot. Our approach is able to leverage the few given examples more effectively as shown by this higher increase in ROC-AUC. It is also notable that Unknown-First outperforms Graph since the context contains more examples with the unknown drug which the model is able to utilize to produce better predictions. For example, the tuple (Vismodegib, Mithramycin A, NCI-H226) with unknown Vismodegib is True. Without examples, this is predicted as 0.46. For Graph with examples, it is predicted as 0.65\u2013closer to the ground truth. For Unknown-First, the prediction further increases to 0.79. In this example, Graph only sees 15 examples containing the unknown but Unknown-First sees a full 20. Few-shot DeepDDS predicts 0.47 for this example, which is quite similar to our method without examples. As another example, (Chlorambucil, Cylocide, SK-OV-3) consists of two unknown drugs and has label False. Without examples, it is predicted as 0.62. Graph improves this to 0.35 and Unknown-First improves to 0.23. Interestingly, few-shot DeepDDS exhibits high uncertainty and predicts 0.50. Unknown Cell Lines Since there are only 112 cell lines, we set m = 20 as unknown and use n = 10 context examples. Interestingly, we find that models perform worse with context examples. We believe this is caused by the relatively small number of patient cell lines in the data vs. 2,956 drugs, making it harder to learn higher-level types of drug-cell line interaction. In other words, we are trying to learn a complex function class (drug synergy in an unknown cell line) without a significant number of example functions f \u2208F . To alleviate this issue, we use 6 layers, batch size of 128, and only 30 epochs. Nonetheless, the issue still exists\u2013performance decreases for baselines DeepDDS and MR-GNN and our strategies Unknown-First and Graph. We experiment with interpolating between training initially with Random to Unknown-First at the end (see Appendix A.2.2), which helps in the unknown cell line case. We believe this creates an exploration-exploitation effect.\n# 4.2 Context Optimization\nAs we have shown in the previous section that the context selection strategy is very important for SynerGPT performance, the natural next question is to what extent the context can affect model performance. To test this, we conduct a different split. Like before, we select 50 unknown drugs and 20 cell lines; with their respective tuples, we create three uniform splits:\n<div style=\"text-align: center;\">g Unknown Cell Line</div>\n<div style=\"text-align: center;\">Unknown Cell Line</div>\n<div style=\"text-align: center;\">Unknown Drug</div>\n<div style=\"text-align: center;\">Unknown Cell Line</div>\nUnknown Drug\nUnknown Cell Line\nStrategy\nROC-AUC\nPR-AUC\nROC-AUC\nPR-AUC\nMean UF\n79.2\n63.8\n85.2\n74.9\nBest UF\n80.8\n66.4\n85.6\n75.7\nGA\n81.5\n66.9\n86.1\n76.5\nTable 3: Test-set context optimization results (p < 0.0001). Model parameters are fixed, only context is changed. UF indicates Unknown-First strategy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c8b/0c8b802a-3abb-45c1-9d48-117717596e0c.png\" style=\"width: 50%;\"></div>\nFigure 2: This figure shows the model\u2019s understanding of an unknown drug (Adavosertib) by retrieving candidates from the pool of held-out drugs. As the model sees more context synergy tuples n (shown at the bottom; selected by the GA), it retrieves structures closer to the target molecule until finding the ground truth after 12 context examples. Repeated structures are skipped for brevity.\n<div style=\"text-align: center;\">Figure 2: This figure shows the model\u2019s understanding of an unknown drug (Adavosertib) by retrieving candidates from the pool of held-out drugs. As the model sees more context synergy tuples n (shown at the bottom; selected by the GA), it retrieves structures closer to the target molecule until finding the ground truth after 12 context examples. Repeated structures are skipped for brevity.</div>\ncontext, validation, and test. We train a SynerGPT Unknown-First model using hyperparameters as in our above experiments\nIn context optimization, our goal is to select examples from the context and train splits which maximize some metric on the validation split. For our experiments, we maximize ROC-AUC for our trained model using the validation set. Overall, we consider two strategies: Unknown-First, and a genetic algorithm (GA). For the genetic algorithm, we use the implementation and hyperparameters from PyGAD (Gad, 2021) with a population of 8 for 50 epochs. Here, we consider each example in the context split to be a potential gene. For comparison, we also select the context at random according to the Unknown-First strategy. To ensure comparability, we evaluate Unknown-First the same number of times as the genetic algorithm and select the best context. Our results (Table 3) show that the genetic algorithm optimizes the context from a starting average AUC of 79.2% up to 81.5% for unknown drugs and from 85.2% to 86.1% for unknown cells. Appendix C visualizes this and shows error bars. We further analyze the results by different tissue types (Appendix E). For example, we find that for unknown drugs, synergy prediction in ovarian cancer is effective, but for both unknown drugs and cell lines predictive performance on bone cell lines is low.\nInverse drug design from synergy examples for discovery and explaina\nExplainability is one of the most challenging problems in deep learning. With transformer language models, the contrast between remarkable performance gain and lack of explainability becomes even more striking. Here we propose a novel drug design task to better understand the model\u2019s \u201cthought process.\u201d As shown in Figure 2, essentially, we look at SynerGPT\u2019s prediction as it gains more information via synergy tuples. While this is a useful step, we do recognize that retrieval doesn\u2019t fully address explainability and hope to inspire further work. We refer to Limitations (\u00a7 6) for more discussion. In this novel task, we evaluate SynerGPT\u2019s ability to retrieve the structure of an unknown drug. We use the same splits as before but replace the classification head with a vector output trained using the loss in Equation 2. Using the same splits allows us to visualize the optimized context from the genetic algorithm. Experimentally, we achieve the best performance with the weight value from equation 1 set to wi := i/k. Two examples of the model retrieving drugs which match the context synergy pairs are shown in Figures 2 and 5. These show the retrieved drug after i context examples have been observed by the model. Additionally, we show overall retrieval performance as the number of context examples shown to the model increases in Appendix D. Figure 4. For the weighted strategy, mean rank for seen drugs decreases from \u223c1,500 to \u223c400 as context increases. Qualitatively, we find that we can retrieve the relevant drug or a similar structure from synergy relationships in multiple cases. This is considerably more effective for drugs observed during training, but performance is also better than random for unknown drugs. This ability to visualize the model\u2019s understanding is helpful for explaining what the model predicts from observing a given context. Second, it enables retrieving drugs which have a desired set of synergies, which can help inform drug candidate discovery, including patient-specific scenarios. We note that we worked off a broad\nExplainability is one of the most challenging problems in deep learning. With transformer language models, the contrast between remarkable performance gain and lack of explainability becomes even more striking. Here we propose a novel drug design task to better understand the model\u2019s \u201cthought process.\u201d As shown in Figure 2, essentially, we look at SynerGPT\u2019s prediction as it gains more information via synergy tuples. While this is a useful step, we do recognize that retrieval doesn\u2019t fully address explainability and hope to inspire further work. We refer to Limitations (\u00a7 6) for more discussion.\nExplainability is one of the most challenging problems in deep learning. With transformer language models, the contrast between remarkable performance gain and lack of explainability becomes even more striking. Here we propose a novel drug design task to better understand the model\u2019s \u201cthought process.\u201d As shown in Figure 2, essentially, we look at SynerGPT\u2019s prediction as it gains more information via synergy tuples. While this is a useful step, we do recognize that retrieval doesn\u2019t fully address explainability and hope to inspire further work. We refer to Limitations (\u00a7 6) for more discussion. In this novel task, we evaluate SynerGPT\u2019s ability to retrieve the structure of an unknown drug. We use the same splits as before but replace the classification head with a vector output trained using the loss in Equation 2. Using the same splits allows us to visualize the optimized context from the genetic algorithm. Experimentally, we achieve the best performance with the weight value from equation 1 set to wi := i/k. Two examples of the model retrieving drugs which match the context synergy pairs are shown in Figures 2 and 5. These show the retrieved drug after i context examples have been observed by the model. Additionally, we show overall retrieval performance\nIn this novel task, we evaluate SynerGPT\u2019s ability to retrieve the structure of an unknown drug. We use the same splits as before but replace the classification head with a vector output trained using the loss in Equation 2. Using the same splits allows us to visualize the optimized context from the genetic algorithm. Experimentally, we achieve the best performance with the weight value from equation 1 set to wi := i/k. Two examples of the model retrieving drugs which match the context synergy pairs are shown in Figures 2 and 5. These show the retrieved drug after i context examples have been observed by the model. Additionally, we show overall retrieval performance as the number of context examples shown to the model increases in Appendix D. Figure 4. For the weighted strategy, mean rank for seen drugs decreases from \u223c1,500 to \u223c400 as context increases. Qualitatively, we find that we can retrieve the relevant drug or a similar structure from synergy relationships in multiple cases. This is considerably more effective for drugs observed during training, but performance is also better than random for unknown drugs. This ability to visualize the model\u2019s understanding is helpful for explaining what the model predicts from observing a given context. Second, it enables retrieving drugs which have a desired set of synergies, which can help inform drug candidate discovery, including patient-specific scenarios. We note that we worked off a broad\ndefinition of drug design as discovering new candidate medications. While retrieval is currently a challenging version of this, future work can expand the search space with generative models.\n# 5 Related Work\n# 5.1 Molecular Language Models\nIn recent years, advances in machine learning and NLP have been applied to molecule representations. Several efforts (Fabian et al., 2020; Chithrananda et al., 2020; Vaucher et al., 2021; Schwaller et al., 2021; NVIDIA Corporation, 2022; Tysinger et al., 2023) show excellent results training on string representations of molecules (Weininger, 1988; Weininger et al., 1989; Krenn et al., 2020; Cheng et al., 2023). Interest has also grown in multi-modal models (Edwards et al., 2022; Zeng et al., 2022) and multi-encoder models (Edwards et al., 2021; Vall et al., 2021; Xu & Wang, 2022; Su et al., 2022; Liu et al., 2022; Seidl et al., 2023; Xu et al., 2023b; Zhao et al., 2023) with applications to chemistry and biology. Existing work (Edwards et al., 2022; Su et al., 2022; Xu et al., 2023a; Christofidellis et al., 2023) also builds on this to \u201ctranslate\u201d between these modalities, such as MolT5 (Edwards et al., 2022), which translates between molecules and language.\n# 5.2 In-Context Learning\nWith the success of models such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023), interest has grown in the theoretical properties of in-context learning. (Garg et al., 2022), which we follow in this work, investigates the ability of transformers to learn function classes. (Olsson et al., 2022) investigates whether in-context learning is related to specific \u201cinduction heads\u201d. (von Oswald et al., 2022) shows that transformers do in-context learning by gradient descent. (Li et al., 2023b) frames in-context learning as algorithm learning to investigate generalization on unseen tasks.\nVery recently, considerable interest has grown in using language models, particularly GPT-4 (OpenAI, 2023), for uncovering chemical knowledge and molecular discovery (Hocky & White, 2022; White et al., 2022; Bran et al., 2023; Boiko et al., 2023; White et al., 2023; Castro Nascimento & Pimentel, 2023), including work in the few-shot setting (Ramos et al., 2023; Jablonka et al., 2023). CancerGPT (Li et al., 2023a), a related contemporaneous preprint, was recently released which explores a similar few-shot approach to drug-drug synergy prediction. It explores training literature-aware text-based GPT models on drug synergy data. The use of GPT models pretrained on massive textual corpora from the web also makes rigorous evaluation and comparison difficult. We believe our work is complementary, since we largely explore the transformer architecture without language and we consider in-context learning which they do not. We also consider extensions such as inverse design and context optimization. Due to the recency of (Li et al., 2023a), we leave additional comparisons beyond our real GPT2 baseline to future work. Applying language models to knowledge graphs has been investigated in the general (Yao et al., 2019; Kim et al., 2020; Youn & Tagkopoulos, 2022) and scientific domains (Nadkarni et al., 2021; Safavi et al., 2022). They can be considered similar to our tests of BERT language models applied to a drug synergy hypergraph (\u00a7 4).\n# 5.4 Drug Synergy Prediction\nAs discussed above, there are several approaches (Preuer et al., 2018; Xu et al., 2019; Nyamabo et al., 2021; Wang et al., 2022; Kuru et al., 2021; Sun et al., 2020; Rozemberczki et al., 2022b) which can predict synergy scores given cell line and drug features. There has also been interest in learning representations for these settings (Scherer et al., 2022). Recently, work (Yang et al., 2021; Rozemberczki et al., 2022a; Lin et al., 2022) has begun to incorporate additional data sources such as drug-protein interactions. This can help improve results, but it often requires creating a subset of the original synergy dataset which can bias results towards the proposed method. (Yang et al., 2023) extracts additional training data from the literature to improve synergy prediction results, which may relate to our results in Appendix F. Research also investigates the application of few-shot (Ma et al., 2021) and zero-shot (Huang et al., 2023) machine learning to drug response prediction\u2013we extend this idea to drug synergy prediction. (Yang et al., 2020) and (Kuenzi et al., 2020) are related but\nhave different focuses compared to our paper; neither compare against any other synergy baselines or do large-scale evaluation. (Yang et al., 2020) focuses on a mechanistic understanding of (drug, tumor) activity\u2013a different task. They use this understanding to rank subsystems and predict a limited number of drug combinations to evaluate. (Kuenzi et al., 2020) does database and experimental testing with small numbers of cell tissues and drugs.\n# 6 Conclusions and Future Work\nAs demonstrated by HIV, HCV, and now cancer, combination therapy is a critical option for disease treatment. Yet, difficulties arise in regards to understanding drug-drug interactions and patient-specific genetic differences. To tackle this, we show that encoder-only language models are effective for drug synergy prediction. We then build on these results by proposing SynerGPT, a decoder model with a novel training strategy for in-context learning which can produce strong results for few-shot drug synergy prediction. We additionally show that the model context can be optimized using non-linear black-box approaches, which has exciting implications for the design of a standardized drug synergy testing panel for creating patient-specific synergy datasets. Finally, we explore a novel task of inverse design using desired drug synergy tuples. Performance on this challenging task is low for unknown drugs; nonetheless, it shows promise for future work that may enable personalized drug discovery.\n# Limitations\nLimitations\nWhile we are able to achieve strong performance without additional cellular or drug data, our approach is very much a black box akin to most deep learning methods. To address this, we propose the task of inverse design from drug synergy examples, which allows the visualization of the model\u2019s structural understanding as it gains more information. While this is a useful step, we do recognize that further research on mechanistic explainability would be valuable. We hope our contribution on synergy-based inverse design can inspire further work on explainability and that SynerGPT\u2019s predictions can be useful inspiration for clinical researchers. We would also like to note that regardless of using deep learning models, pharmaceutical researchers are in many cases unable to explain the mechanisms of many important drugs on their own (e.g., Modafinil, Metformin, general anesthetics) (Stahl, 2020; Rena et al., 2013; Brown et al., 2011) \u2014 let alone explain their interactions with each other. These drugs are prescribed to hundreds of millions of patients. Recent studies (Lin et al., 2019) suggest that many purported protein drug targets may not be the actual target at all. Important progress with life-saving modern drugs can be made with limited visibility into underlying mechanisms, yet certainly improved mechanistic understanding would be highly useful. While we show that strong performance is possible without features, future work will still likely want to integrate external database features into drug synergy prediction; however, they will likely need to be integrated in a more thoughtful manner in order to ensure an actual benefit. It would also likely be interesting for future work to investigate the internal connections language models are learning and what it might mean for understanding the fundamental biology of how cellular pathways interact. It is also worth noting that designing molecules using drug synergy tuples is a somewhat atypical task, so there may exist a wall in terms of the information content inherent in the context. While we do analysis by separating model performance into different tissue types in this work (as done in multiple prior studies), we note that for future research it is likely too limiting and simplistic to separate cell lines into tissues types.\nReferences Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. arXiv preprint arXiv:2209.01712, 2022. Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019. Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern optimization lens. arXiv: Methodology, 2015. James RM Black and Nicholas McGranahan. Genetic and non-genetic clonal diversity in cancer evolution. Nature Reviews Cancer, 21(6):379\u2013392, 2021. Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023. Andrew P Bradley. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition, 30(7):1145\u20131159, 1997. Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023. Emery N Brown, Patrick L Purdon, and Christa J Van Dort. General anesthesia and altered states of arousal: a systems neuroscience analysis. Annual review of neuroscience, 34:601\u2013628, 2011. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Cayque Monteiro Castro Nascimento and Andr\u00e9 Silva Pimentel. Do large language models understand chemistry? a conversation with chatgpt. Journal of Chemical Information and Modeling, 63(6): 1649\u20131655, 2023. Austin H Cheng, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Al\u00e1n AspuruGuzik. Group selfies: a robust fragment-based molecular string representation. Digital Discovery, 2023. Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar. Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. Unifying molecular and textual representations via multi-task language modelling. arXiv preprint arXiv:2301.12586, 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Jean Dubuisson. Hepatitis c virus proteins. World journal of gastroenterology: WJG, 13(17):2406, 2007. Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2Mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 595\u2013607, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021. emnlp-main.47.\nCarl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 375\u2013413, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology. org/2022.emnlp-main.26. Benedek Fabian, Thomas Edlich, H\u00e9l\u00e9na Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. Molecular representation learning with language models and domain-relevant auxiliary tasks. arXiv preprint arXiv:2011.13230, 2020. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126\u20131135. PMLR, 2017. Alan D Frankel and John AT Young. Hiv-1: fifteen proteins and an rna. Annual review of biochemistry, 67(1):1\u201325, 1998. Ahmed Fawzy Gad. Pygad: An intuitive genetic algorithm python library, 2021. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997. Glen M Hocky and Andrew D White. Natural language processing models that automate programming will transform chemistry research and teaching. Digital discovery, 1(2):79\u201383, 2022. Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. arXiv preprint arXiv:2102.09548, 2021. Kexin Huang, Payal Chandak, Qianwen Wang, Shreyas Havaldar, Akhil Vaid, Jure Leskovec, Girish Nadkarni, Benjamin S Glicksberg, Nils Gehlenborg, and Marinka Zitnik. Zero-shot prediction of therapeutic use with geometric deep learning and clinician centered design. medRxiv, pp. 2023\u201303, 2023. Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Is gpt-3 all you need for low-data discovery in chemistry? ChemRxiv preprint, 2023. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International conference on machine learning, pp. 4839\u20134848. PMLR, 2020. David N Juurlink, Muhammad M Mamdani, Douglas S Lee, Alexander Kopp, Peter C Austin, Andreas Laupacis, and Donald A Redelmeier. Rates of hyperkalemia after publication of the randomized aldactone evaluation study. New England Journal of Medicine, 351(6):543\u2013551, 2004. Bosung Kim, Taesuk Hong, Youngjoong Ko, and Jungyun Seo. Multi-task learning for knowledge graph completion with pre-trained language models. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1737\u20131743, 2020. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic Acids Research, 51 (D1):D1373\u2013D1380, 2023. Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Onno Kranenburg. The kras oncogene: past, present, and future. Biochimica et biophysica acta, 1756 (2):81\u201382, 2005. Mario Krenn, Florian H\u00e4se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Selfreferencing embedded strings (selfies): A 100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020.\nKundan Krishna, Jeffrey Bigham, and Zachary C Lipton. Does pretraining for summarization require knowledge transfer? arXiv preprint arXiv:2109.04953, 2021. Brent M Kuenzi, Jisoo Park, Samson H Fong, Kyle S Sanchez, John Lee, Jason F Kreisberg, Jianzhu Ma, and Trey Ideker. Predicting drug response and synergy using a deep learning model of human cancer cells. Cancer cell, 38(5):672\u2013684, 2020. Halil Ibrahim Kuru, Oznur Tastan, and A Ercument Cicek. Matchmaker: a deep learning framework for drug synergy prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(4):2334\u20132344, 2021. Tianhao Li, Sandesh Shetty, Advaith Kamath, Ajay Jaiswal, Xianqian Jiang, Ying Ding, and Yejin Kim. Cancergpt: Few-shot drug pair synergy prediction using large pre-trained language models. arXiv preprint arXiv:2304.10946, 2023a. Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023b. T Jake Liang and Marc G Ghany. Current and future therapies for hepatitis c virus infection. New England Journal of Medicine, 368(20):1907\u20131917, 2013. Ann Lin, Christopher J Giuliano, Ann Palladino, Kristen M John, Connor Abramowicz, Monet Lou Yuan, Erin L Sausville, Devon A Lukow, Luwei Liu, Alexander R Chait, et al. Off-target toxicity is a common mechanism of action of cancer drugs undergoing clinical trials. Science translational medicine, 11(509):eaaw8412, 2019. Jiacheng Lin, Hanwen Xu, Addie Woicik, Jianzhu Ma, and Sheng Wang. Pisces: A combo-wise contrastive learning approach to synergistic drug combination prediction. bioRxiv, pp. 2022\u201311, 2022. Hui Liu, Wenhao Zhang, Bo Zou, Jinxian Wang, Yuanyuan Deng, and Lei Deng. Drugcombdb: a comprehensive database of drug combinations toward the discovery of combinatorial therapy. Nucleic acids research, 48(D1):D871\u2013D881, 2020. Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. arXiv preprint arXiv:2212.10789, 2022. Jianzhu Ma, Samson H Fong, Yunan Luo, Christopher J Bakkenist, John Paul Shen, Soufiane Mourragui, Lodewyk FA Wessels, Marc Hafner, Roded Sharan, Jian Peng, et al. Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients. Nature Cancer, 2(2):233\u2013244, 2021. Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023. Alan Miller. Subset selection in regression. CRC Press, 2002. Tom Michael Mitchell et al. Machine learning, volume 1. McGraw-hill New York, 2007. Reza Bayat Mokhtari, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. Combination therapy in combating cancer. Oncotarget, 8(23): 38022, 2017. Richard D Moore and Richard E Chaisson. Natural history of hiv infection in the_era of combination antiretroviral therapy. Aids, 13(14):1933\u20131942, 1999. Edmund A Mroz and James W Rocco. The challenges of tumor genetic diversity. Cancer, 123(6): 917\u2013927, 2017. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah Smith, Hannaneh Hajishirzi, and Tom Hope. Scientific language models for biomedical knowledge base completion: An empirical study. In 3rd Conference on Automated Knowledge Base Construction, 2021.\nNVIDIA Corporation. Megamolbart v0.2, 2022. URL https://catalog.ngc.nvidia.com/ orgs/nvidia/teams/clara/models/megamolbart_0_2. Arnold K Nyamabo, Hui Yu, and Jian-Yu Shi. Ssi\u2013ddi: substructure\u2013substructure interactions for drug\u2013drug interaction prediction. Briefings in Bioinformatics, 22(6):bbab133, 2021. Peter H O\u2019Donnell and M Eileen Dolan. Cancer pharmacoethnicity: Ethnic differences in susceptibility to the effects of chemotherapycancer pharmacoethnicity. Clinical Cancer Research, 15(15): 4806\u20134814, 2009. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. OpenAI. Gpt-4 technical report, 2023. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011. Kristina Preuer, Richard PI Lewis, Sepp Hochreiter, Andreas Bender, Krishna C Bulusu, and G\u00fcnter Klambauer. Deepsynergy: predicting anti-cancer drug synergy with deep learning. Bioinformatics, 34(9):1538\u20131546, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8748\u20138763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html. Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian optimization of catalysts with in-context learning. arXiv preprint arXiv:2304.05341, 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Graham Rena, Ewan R Pearson, and Kei Sakamoto. Molecular mechanism of action of metformin: old or new insights? Diabetologia, 56:1898\u20131906, 2013. Benedek Rozemberczki, Anna Gogleva, Sebastian Nilsson, Gavin Edwards, Andriy Nikolov, and Eliseo Papa. Moomin: Deep molecular omics network for anti-cancer drug combination therapy. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 3472\u20133483, 2022a. Benedek Rozemberczki, Charles Tapley Hoyt, Anna Gogleva, Piotr Grabowski, Klas Karis, Andrej Lamov, Andriy Nikolov, Sebastian Nilsson, Michael Ughetto, Yu Wang, et al. Chemicalx: A deep learning library for drug pair scoring. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3819\u20133828, 2022b. Tara Safavi, Doug Downey, and Tom Hope. Cascader: Cross-modal cascading for knowledge graph link prediction. arXiv preprint arXiv:2205.08012, 2022. Benjamin Sanchez-Lengeling and Al\u00e1n Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400):360\u2013365, 2018. Paul Scherer, Pietro Li\u00f2, and Mateja Jamnik. Distributed representations of graphs for drug pair scoring. arXiv preprint arXiv:2209.09383, 2022.\nPhilippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based neural networks. Nature Machine Intelligence, 3(2):144\u2013152, 2021. Philipp Seidl, Andreu Vall, Sepp Hochreiter, and G\u00fcnter Klambauer. Enhancing activity prediction models in drug discovery with the ability to understand human language. arXiv preprint arXiv:2303.03363, 2023. Amanpreet Singh, Mike D\u2019Arcy, Arman Cohan, Doug Downey, and Sergey Feldman. Scirepeval: A multi-format benchmark for scientific document representations. arXiv preprint arXiv:2211.13308, 2022. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. Stephen M Stahl. Prescriber\u2019s guide: Stahl\u2019s essential psychopharmacology. Cambridge University Press, 2020. Vaidotas Stankevicius, Gintautas Vasauskas, Rimante Noreikiene, Karolina Kuodyte, Mindaugas Valius, and Kestutis Suziedelis. Extracellular matrix-dependent pathways in colorectal cancer cell lines reveal potential targets for anticancer therapies. Anticancer Research, 36(9):4559\u20134567, 2016. Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481, 2022. Mengying Sun, Fei Wang, Olivier Elemento, and Jiayu Zhou. Structure-based drug-drug interaction detection via expressive graph convolutional networks and deep sets (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 13927\u201313928, 2020. John G Tate, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M Beare, Nidhi Bindal, Harry Boutselakis, Charlotte G Cole, Celestino Creatore, Elisabeth Dawson, et al. Cosmic: the catalogue of somatic mutations in cancer. Nucleic acids research, 47(D1):D941\u2013D947, 2019. Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat, Moshe Wasserblat, and Oren Pereg. Efficient few-shot learning without prompts. arXiv preprint arXiv:2209.11055, 2022. Emma P Tysinger, Brajesh K Rai, and Anton V Sinitskiy. Can we quickly learn to \u201ctranslate\u201d bioactive molecules with transformer models? Journal of Chemical Information and Modeling, 63 (6):1734\u20131744, 2023. Andreu Vall, Sepp Hochreiter, and G\u00fcnter Klambauer. Bioassayclr: Prediction of biological activity for novel bioassays based on rich textual descriptions. In ELLIS ML4Molecules workshop, 2021. Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, Vishnu H Nair, Anna Iuliano, and Teodoro Laino. Inferring experimental procedures from text-based representations of chemical reactions. Nature communications, 12(1):2573, 2021. Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. Jinxian Wang, Xuejun Liu, Siyuan Shen, Lei Deng, and Hui Liu. Deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations. Briefings in Bioinformatics, 23 (1):bbab390, 2022. David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31\u201336,\nDavid Weininger, Arthur Weininger, and Joseph L Weininger. Smiles. 2. algorithm for generation of unique smiles notation. Journal of chemical information and computer sciences, 29(2):97\u2013101, 1989. Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, et al. Do large language models know chemistry? ChemRxiv preprint, 2022. Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, et al. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368\u2013376, 2023. Hanwen Xu and Sheng Wang. Protranslator: zero-shot protein function prediction using textual description. In Research in Computational Molecular Biology: 26th Annual International Conference, RECOMB 2022, San Diego, CA, USA, May 22\u201325, 2022, Proceedings, pp. 279\u2013294. Springer, 2022. Hanwen Xu, Addie Woicik, Hoifung Poon, Russ B Altman, and Sheng Wang. Multilingual translation for zero-shot biomedical classification using biotranslator. Nature Communications, 14(1):738, 2023a. Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-modality learning of protein sequences and biomedical texts. arXiv preprint arXiv:2301.12040, 2023b. Nuo Xu, Pinghui Wang, Long Chen, Jing Tao, and Junzhou Zhao. Mr-gnn: Multi-resolution and dual graph neural network for predicting structured entity interactions. arXiv preprint arXiv:1905.09558, 2019. Cai Yang, Addie Woicik, Hoifung Poon, and Sheng Wang. Bliam: Literature-based data synthesis for synergistic drug combination prediction. arXiv preprint arXiv:2302.06860, 2023. Jiannan Yang, Zhongzhi Xu, William Ka Kei Wu, Qian Chu, and Qingpeng Zhang. Graphsynergy: a network-inspired deep learning model for anticancer drug combination prediction. Journal of the American Medical Informatics Association, 28(11):2336\u20132345, 2021. Mi Yang, Patricia Jaaks, Jonathan Dry, Mathew Garnett, Michael P Menden, and Julio Saez-Rodriguez. Stratification and prediction of drug synergy based on target functional similarity. npj Systems Biology and Applications, 6(1):16, 2020. Liang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193, 2019. Michihiro Yasunaga, Jure Leskovec, and Percy Liang. Linkbert: Pretraining language models with document links. arXiv preprint arXiv:2203.15827, 2022. Jason Youn and Ilias Tagkopoulos. Kglm: Integrating knowledge graph structure in language models for link prediction. arXiv preprint arXiv:2211.02744, 2022. Bulat Zagidullin, Jehad Aldahdooh, Shuyu Zheng, Wenyu Wang, Yinyin Wang, Joseph Saad, Alina Malyutina, Mohieddin Jafari, Ziaurrehman Tanoli, Alberto Pessia, et al. Drugcomb: an integrative cancer drug combination data portal. Nucleic acids research, 47(W1):W43\u2013W51, 2019. Lorenzo Villa Zapata, Philip D Hansten, Jennifer Panic, John R Horn, Richard D Boyce, Sheila Gephart, Vignesh Subbian, Andrew Romero, and Daniel C Malone. Risk of bleeding with exposure to warfarin and nonsteroidal anti-inflammatory drugs: a systematic review and meta-analysis. Thrombosis and haemostasis, 120(07):1066\u20131074, 2020. Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):862, 2022. Wenyu Zhao, Dong Zhou, Buqing Cao, Kai Zhang, and Jinjun Chen. Adversarial modality alignment network for cross-modal molecule retrieval. IEEE Transactions on Artificial Intelligence, 2023.\nShuyu Zheng, Jehad Aldahdooh, Tolou Shadbahr, Yinyin Wang, Dalal Aldahdooh, Jie Bao, Wenyu Wang, and Jing Tang. Drugcomb update: a more comprehensive drug sensitivity data repository and analysis portal. Nucleic acids research, 49(W1):W174\u2013W184, 2021.\n# A Full Results Tables\n# A.1 GraphSynergy Full Results\nA.1 GraphSynergy Full Results\nFull results for the BERT input method and GraphSynergy tests are in Table 4. We compare on the specific subset of DrugCombDB Liu et al. (2020) which was selected to match Graphsynergy\u2019s network data (i.e. selecting the subset of DrugCombDB with drugs/cells that can be matched with external protein-protein interaction, drug-protein association, and cell-protein association networks) and a 7:1:2 train:validation:test split. This data subset also contains useful surface names (the common natural language name of the drug; e.g. dasatinib), which allows us to compare the effect that drug names have on language model synergy prediction performance. We consider three BERT training variations: the original BERT Devlin et al. (2019), SciBERT Beltagy et al. (2019), and BioLinkBERT Yasunaga et al. (2022). SciBERT was trained on a corpus of scientific documents which would be considerably more focused on drugs than a general corpus. BioLinkBERT is a biomedical BERT model additionally trained using document relation prediction (e.g. citation links). We would like to reiterate the rather remarkable finding that pre-training on scientific literature does not necessarily help the model perform drug synergy prediction any better. Overall, these results indicate that models using external data may not be behaving how we think they are. ChemicalX Results We report full results on the subset of DrugCombDB Liu et al. (2020) used by ChemicalX Rozemberczki et al. (2022b) in Table 5. Previous work tested on dierent subsets of\nChemicalX Results We report full results on the subset of DrugCombDB Liu et al. (2020) used by ChemicalX Rozemberczki et al. (2022b) in Table 5. Previous work tested on different subsets of existing datasets (due to filtering for external features).\nInput\nModel\nROC-AUC\nF1\nPrecision\nRecall\nAccuracy\nGraphSynergy\n83.4\n72.7\n73.5\n71.9\n75.5\nName\nUnpretrained BERT-base\n80.6\n71.0\n71.7\n70.3\n74.0\nBioLinkBERT-base\n83.6\n73.1\n73.4\n72.8\n75.7\nSciBERT-base\n83.8\n73.8\n73.3\n74.3\n75.8\nBERT-base\n83.8\n73.3\n74.2\n72.4\n76.1\nBioLinkBERT-large\n84.7\n73.9\n74.7\n73.1\n76.7\nRandom\nToken\nBioLinkBERT-base\n84.1\n73.7\n73.6\n73.8\n76.2\nSciBERT-base\n83.8\n73.3\n74.2\n72.4\n76.2\nBERT-base\n84.0\n73.4\n74.1\n72.7\n76.1\nBioLinkBERT-large\n84.1\n73.8\n73.4\n74.2\n76.1\nTable 4: Performance of BERT models with names and random tokens and GraphSynergy on the custom subset of DrugCombDB Liu et al. (2020). Results are average of 5 runs. Name indicates that the common name of the drug is used as input, while Random Token uses the strategy described in Section 3.1.\nModel\nKB Info\nName Info\nROC-AUC\nPR-AUC\nDeepSynergy\n\u00d7\n84.3\n70.4\nMR-GNN\n\u00d7\n77.9\n62.6\nSSI-DDI\n\u00d7\n63.3\n41.4\nDeepDDS\n\u00d7\n87.2\n77.0\nSciBERT (random)\n86.9\n76.3\nBioLinkBERT (random)\n86.8\n76.4\nBioLinkBERT (name)\n\u00d7\n86.4\n75.9\nTable 5: Classification results for four selected ChemicalX Rozemberczki et al. (2022b) baselines and two BERT-base models on DrugCombDB Liu et al. (2020). First two BERT models use random token inputs and last model uses drug names as input. Values are average of five runs.\n# A.2 Few-Shot Full Results\n# A.2.1 Baseline Descriptions\nDeepSynergy is a popular feedforward model which uses cell line features and drug fingerprints. MRGNN is a graph convolutional network (GCN) Kipf & Welling (2016) fed into an LSTM Hochreiter & Schmidhuber (1997) which takes the drug structure into account. SSI-DDI uses a graph attention network (GAT) Veli\u02c7ckovi\u00b4c et al. (2017) with a final co-attention layer. DeepDDS uses both a GAT and GCN, which are fed into a fully connected feed forward network. Real GPT-2 We train a GPT-2 model2 in the few-shot setting (as opposed to SynerGPT\u2019s zero-shot) using random context and the same hyperparameters to mimic SynerGPT\u2019s training settings as much as possible. We use names of the drugs obtained from linking to PubChem Kim et al. (2023) as input in the form \u201cAre drugs [DRUG1] and [DRUG2] synergistic in cell line [CELL]?\u201d. SetFit Furthermore, we test finetuning a few-shot language-model baseline, SetFit Tunstall et al. (2022), on our few-shot data. We follow the original paper in using batch size 16, R = 20 text pairs generated for contrastive learning, and 1 epoch. Inputs to the model follow the same format as BERT in Section 3.1. We test using four models. 1. SetFit-SBERT: paraphrase-multilingual-mpnet-base-v2 from Reimers & Gurevych (2019) with names as input. This model was trained to create semantic embeddings via Siamese networks. 2. SetFit-C: recobo/chemical-bert-uncased-simcse from Recobo.ai 3 with names as input. This model was trained using SimCSE on chemistry text. 3. SetFit-S2: allenai/specter2 from Singh et al. (2022) with names as input. This model was trained on multiple scientific classification and regression tasks, such as MeSH descriptors classification. 4. SetFit-SMILES: DeepChem/ChemBERTa-77M-MTR from Ahmad et al. (2022) with SMILES strings as input. This model was pretrained by predicting 200 molecular properties for a molecule given its SMILES string.\nModel-Agnostic Meta-Learning We also consider a meta-learning formulation of our problem setting. We use MAML Finn et al. (2017) to train a DeepDDS model. Since MAML4 does few-shot classification using episodes sampled from different learning tasks, we reframe our problem to match this. We consider predicting synergy for each drug to be a task. Then, we sample an episode for training from a random task for each mini-batch. We aggregate rare drugs without enough samples to form an episode into the same task until there are enough samples for an episode. Additionally, since we are dealing with binary classification here, we use N = 2-way. We sample the \u201cvalidation\u201d portion of each episode from our training set like in SynerGPT. We use the same context bank (and context size) for \u201cadaptation\u201d during evaluation. The same learning rate (1e \u22123), batch size (512), and number of steps/epochs as DeepDDS is used. We report few-shot (first-order) and zero-shot (no adaptation) versions. Overall, we find that the MAML training procedure produces poor results, and adaptation produces insignificant performance increases. We attribute this to the episode-based sampling strategy neglecting important information in training.\nProtonets As another meta-learning baseline, we consider Protonets Snell et al. (2017). We use the same meta-learning framework as for MAML. Because we don\u2019t have drug task meta-data, we only consider the few-shot setting.\nk-Nearest Neighbors We also consider a",
    "paper_type": "method",
    "attri": {
        "background": "Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient\u2019s specific tumor via biopsied cells. Many drug combinations can cause increased toxicity in a manner that may depend on specific patient backgrounds, adding further complexity to the problem. To enable the safest and most effective implementation of combination therapy in cancer care, it is thus important to personalize the prediction of drug synergies.",
        "problem": {
            "definition": "The problem aims to predict whether combining drugs will have positive or negative outcomes in the complex system of interacting cellular pathways, focusing on the pairwise drug synergy setting.",
            "key obstacle": "The main challenge is the limited training data in the personalized setting, which complicates the prediction of drug synergies when there is scarce training data, particularly for specific tumor types."
        },
        "idea": {
            "intuition": "Inspired by recent developments in transformer language models that can learn function classes, the idea is to leverage these models for drug synergy prediction without requiring extensive external knowledge.",
            "opinion": "The proposed idea involves using a GPT model to in-context learn drug synergy functions from a small personalized dataset of drug combinations, thus predicting additional synergies in that context.",
            "innovation": "The primary innovation is the introduction of the SynerGPT model which operates without any external knowledge sources, achieving competitive results in predicting drug synergies."
        },
        "method": {
            "method name": "SynerGPT",
            "method abbreviation": "ICL-DS",
            "method definition": "SynerGPT is a transformer language model designed for in-context learning of drug synergy functions from limited personalized data.",
            "method description": "The method utilizes in-context learning to predict drug synergies based on contextual examples without requiring external features.",
            "method steps": [
                "Input drug synergy tuples into the model.",
                "Use the model to generate predictions based on held-out examples.",
                "Optimize the context examples using a genetic algorithm to improve predictions."
            ],
            "principle": "The effectiveness of this method lies in its ability to generalize from limited examples and adaptively select relevant context for predictions."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved using a dataset of 2,956 drugs, 112 cell lines, and 191,391 synergy tuples, comparing the performance of SynerGPT against existing state-of-the-art models in both few-shot and zero-shot settings.",
            "evaluation method": "The performance was assessed using ROC-AUC and PR-AUC metrics, with specific strategies for context selection during the evaluation phase."
        },
        "conclusion": "The experiments demonstrated that SynerGPT can effectively predict drug synergies in few-shot settings, outperforming existing models and suggesting a novel approach to personalized drug discovery.",
        "discussion": {
            "advantage": "The key advantages include the ability to predict drug synergies without relying on external knowledge sources and the effectiveness of in-context learning for few-shot predictions.",
            "limitation": "The limitations include the model's black-box nature, making it difficult to explain the underlying mechanisms of drug interactions.",
            "future work": "Future research could focus on integrating external features into the model and exploring the internal mechanisms that the language model learns regarding drug interactions."
        },
        "other info": {
            "additional details": {
                "impact": "The findings raise important questions about the role of non-textual pre-training for language models in drug discovery.",
                "potential applications": "The approach may lead to new methodologies for personalized drug candidate discovery."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "AI hallucination refers to instances where AI systems generate outputs that are not grounded in reality, which can lead to incorrect predictions or interpretations."
        },
        {
            "section number": "1.4",
            "key information": "Ethical concerns associated with AI hallucinations include the potential for misleading information in critical applications such as healthcare, where incorrect drug synergy predictions can have severe consequences."
        },
        {
            "section number": "2",
            "key information": "The problem aims to predict whether combining drugs will have positive or negative outcomes in the complex system of interacting cellular pathways, focusing on the pairwise drug synergy setting."
        },
        {
            "section number": "4.1",
            "key information": "The main challenge is the limited training data in the personalized setting, complicating the prediction of drug synergies when there is scarce training data, particularly for specific tumor types."
        },
        {
            "section number": "6.1",
            "key information": "SynerGPT is a transformer language model designed for in-context learning of drug synergy functions from limited personalized data, which can mitigate hallucination by relying on contextual examples."
        },
        {
            "section number": "7.1",
            "key information": "The findings raise important questions about the role of non-textual pre-training for language models in drug discovery, suggesting a need for interdisciplinary approaches in AI applications."
        }
    ],
    "similarity_score": 0.5645680172526957,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1250_AI_ha/papers/SynerGPT_ In-Context Learning for Personalized Drug Synergy Prediction and Drug Design.json"
}